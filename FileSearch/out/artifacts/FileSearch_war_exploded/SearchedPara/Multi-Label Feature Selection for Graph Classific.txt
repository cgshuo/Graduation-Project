
Due to the recent advances of data collection technology, many application fields are facing various data with complex structures, e.g. , chemical compounds, program flows and XML web documents. Different from traditional data in feature spaces, these data are not represented as feature vectors, but as graphs which raise one fundamental challenge for data mining research: the complex structure and lack of vector representations. An effective model for graph data should be able to extract or find a proper set of features for these graphs in order to perform analysis or management steps. Motivated by these cha llenges, graph mining research problems, in particular graph classification, have received considerable atten tion in the last decade.

In the literature, graph classification problem has been ex-tensively studied. Conventional approaches focus on single-label classification problems (binary classification) [22], [20], which assume, explicitly or implicitly, that each graph has only one label. However, in many real-world applica-tions, each graph can be assigned with more than one label. For example, in Figure 1, a chemical compound can inhibit the activities of multiple types of kinases, e.g. , ATPase and MEK kinase ; One drug molecular can have anti-cancer efficacies on multiple types of cancers. The selection and discovery of drugs or kinase inhibitors can be significantly improved if these chemical mo lecules are automatically tagged with a set of multiple labels or potential efficacies. This setting is also known as multi-label classification where each instance can be associated with multiple categories. It has been shown useful in many real-world applications such as text categorization [17], [19] and bioinformatics [5]. Multi-label classification is particularly challenging on graph data. The reason is that, in the single-label case, conventional graph mining methods can extract or find one set of discriminative subgraph features for the single label concept within the graph dataset. But in multi-label cases, each graph contains multiple label concepts, and multiple sets of subgraph features should be mined, one for each label concept, in order to decide all the possible categories for each graph using binary classifie rs (one-vs-all technique [3]). Thus the time and memory used for classifying multi-label graph data is much larger than for the single-label graphs. A major difficulty in performing multi-label classification on graph data lies in the complex structure of graphs and lack of features which is useful for multiple labels concepts. Selecting a proper set of features for graph data becomes an essential and important procedure for multi-label graph classification.

Despite its value and significance, the multi-label feature selection problem for graph data has not been studied in this context so far. If we consider graph mining and multi-label classification as a whole, the major research challenges on multi-label feature selection for graph classification are twofold:
Graph Data: One fundamental problem in multi-label feature selection on graph data lies in the complex structures and lack of feature representations of graphs. Conventional feature selection approaches in vector spaces assume, explic-itly or implicitly, that a full set of features is given before the feature selection. In the context of graph data, however, the full set of features for a graph dataset, are usually too large or even infeasible to obtain. For example, in graph mining, the number of subgraph features grows exponentially with the size of the graphs, which makes it impossible to enumerate all the subgraph features before the feature selection.
Multiple Labels: Another fundamental problem in multi-label feature selection on graph data lies in the multiple label concepts for each graph, i.e. how to utilize the multiple label concepts in a graph dataset to find a proper set of subgraph features for classification tasks. Conventional feature selection in graph cla ssification approaches focuses on single-labeled settings [15], [22], [20]. The mining strat-egy of discriminative subgraph patterns strictly follows the assumption that each graph has only one label. However, in many real-world applications, one graph can usually be assigned with multiple labels simultaneously. Directly applying single-label graph feature selection methods by adopting the popular one-versus-all binary decomposition (Figure 2 (a)), which performs f eature selection on each label concept, will result in different sets of subgraph features on different classes. Thus most state-of-the-art multi-label classification approaches in vector spaces cannot be used, since they assume that the instances should have a same set of features in the input space [19], [5].

In this paper, we introduce a novel framework to the above problems by mining subgr aph features using multiple labels of graphs. Our framework is illustrated in Figure 2 (b). Different from existing s ingle-label feature selection methods for graph data, our approach, called gMLC, can utilize multiple labels of graphs to find an optimal set of subgraph features for graph classification. We first derive an evaluation criterion for subgraph features, named gHSIC, based upon a given graph dataset with multiple labels. Then in order to avoid exhaustive enumeration of all subgraph features, we propose a branch-and-bound algorithm to ef-ficiently search for optimal subgraph features by pruning the subgraph search space using multiple labels of graphs. In order to evaluate our proposed model, we perform comprehensive experiments on real-world multi-label graph classification tasks. The experiments demonstrate that our feature selection approach can effectively boost multi-label graph classification perform ances. Moreover, we show that gMLC is more efficient by pruning the subgraph search space using multiple labels.

The rest of the paper is organized as follows. We start by a brief review on related work of graph feature selection and multi-label classification. Th en introduce the preliminary concepts, give the problem analysis and present the gHSIC criterion in Section III and Section IV; In Section V, we derive a branch and bound algorithm gMLC based upon gHSIC. Then Section VI reports the experiment results. In Section VII, we conclude the paper.

To the best of our knowledge, this paper is the first work addressing the multi-label feature selection problem for graph classification. Our work is related to both multi-label classification techniques and subgraph feature based graph mining. We briefly discuss both of them.

Multi-label learning deals with the classification prob-lem where each instance can belong to multiple different classes simultaneously. Conventional multi-label approaches are focused on instances in vector spaces. One well-know type of approaches is binary relevance (one-vs-all technique [3]), which transforms the multi-label problem into multiple binary classification problems, one for each label. M L -KNN [24] is one of the binary relevance methods, which extends the lazy learning algorithm, k NN, to a multi-label version. It employs label prior probabilities gained from each example X  X  k nearest neighbors and use maximum a posteriori (MAP) principle to determine label set. Elisseeff and Weston [5] presented a kernel method R ANK -SVM for multi-label classification, by minimizing a loss function named ranking loss . Extension of other traditional learning techniques have also been studied, such as probabilistic generative models [17], [21], decision trees [4], maximal margin methods [7], [13] and ensemble methods[6], etc.

Extracting subgraph features from graph data have also been investigated by many researchers. The goal of such approaches is to extract informative subgraph features from a set of graphs. Typically some filtering criteria are used. Upon whether considering the label information, there are two types of approaches: unsupervised and supervised. A typical evaluation criterion is frequency, which aims at collecting frequently appearing subgraph features. Most of the frequent subgraph feature extraction approaches are unsupervised. For example, Yan and Han develop a depth-first search algorithm: gSpan [23]. This algorithm builds a lexicographic order among graphs, and maps each graph to an unique minimum DFS code as its canonical label. Based on this lexicographic order, gSpan adopts the depth-first search strategy to mine frequent connected subgraphs efficiently. Many other frequent subgraph feature extraction approaches have been developed, e.g. AGM [11], FSG [16], MoFa [1], FFSM [10], and Gaston [1 8]. Supervised subgraph feature extraction approaches have also been proposed in literature, such as LEAP [22], CORK [20], which look for discriminative subgraph patterns for graph classifications, and gSSC[14] for semi-supervised classification.
Our approach is also relevant to graph feature selection approaches based on Hilbert-Schmidt independence criterion [2], but there are significant differences between them. Pre-vious graph feature selection approaches assume each graph object only has one label and they focus on evaluating sub-graph features effectively usi ng HSIC criterion and perform feature selection using frequent subgraph mining methods (gSpan) as black-boxes. However, our approach assumes that each graph can have multiple labels, and focuses on extracting good subgraph features efficiently by pruning the subgraph search space using branch and bound method inside gSpan. So, our method searches the pruned gSpan tree. In fact, we only generated and searched a much smaller tree than gSpan as the size of the search tree dominates the execution time.

Before presenting the feature selection model for multi-label graph classification, we first introduce the notations that will be used throughout th is paper. Multi-label graph classification is the task of automatically classifying a graph object into a subset of predefined classes. Let D = {
G 1 ,  X  X  X  ,G n } denote the entire graph dataset, which con-sists of n graph objects, represented as connected graphs . The graphs in D are labeled by { y 1 ,  X  X  X  , y n } ,where { 0 , 1 } Q denotes the multiple labels assigned to G i .Here Q is the number of all possible l abels within a label concept set C .

D EFINITION 1 (Connected Graph): A graph is repre-sented as G =( V ,E, L ,l ) ,where V is a set of vertices V = { v 1 ,  X  X  X  ,v n v } , E  X  X  X V is a set of edges, L is the set of labels for the vertices and the edges. l : V X  E  X  X  is a function assigning labels to the vertices and the edges. A connected graph is a graph such that there is a path between any pair of vertices.

D EFINITION 2 (Multi-label Graph): A multi-label graph is a graph assigned with multiple class labels ( G, y ) ,in which y =[ y 1 ,  X  X  X  ,y Q ]  X  X  0 , 1 } Q denotes the multiple labels assigned to the graph G . y k =1 iff graph G is assigned with the k -th class label, 0 otherwise. D EFINITION 3 (Subgraph): Let G =( V ,E , L ,l ) and G =( V ,E, L ,l ) be connected graphs. G is a subgraph of G ( G  X  G ) iff there exist an injective function  X  : V  X  V s.t. (1)  X  v  X  X  , l ( v )= l (  X  ( v )) ;(2)  X  ( u, v )  X  subgraph of G ,then G is a supergraph of G .

In our current solution, we focus on the subgraph-based graph classification problem, which assumes that a graph ob-ject G i is represented as a binary vector x i =[ x 1 i , associated with a set of subgraph patterns { g 1 ,  X  X  X  ,g Here x k i  X  X  0 , 1 } is the binary feature of G i corresponding to the subgraph pattern g k ,and x k i =1 iff g k is a subgraph of G i ( g k  X  G i ).

The key issue of feature selection for multi-label graph classification is how to find the most informative subgraph patterns from a given multi-label graph dataset. So, in this paper, the studied research problem can be described as follow: in order to train an effective multi-label graph classifier, how to efficiently find a set of optimal subgraph features using multiple labels of graphs?
Mining the optimal subgrap h features for multi-label graphs is a non-trivial task due to the following problems: (1) How to properly evaluate the usefulness of a set of subgraph features based upon multiple labels of graphs? (2) How to determine the optimal subgraph features within a reasonable amount of time by avoiding the exhaustive enumeration using multiple labels of the graphs? The subgraph feature space of graph objects are usually too large, since the number of subgraphs grows exponentially with the size of graphs. It is infeasible to completely enumerate all the subgraph features for a given graph dataset.

In the following sections, we will first introduce the optimization framework for selecting informative subgraph features from multi-label graphs, then propose an efficient subgraph mining strategy using branch-and-bound to avoid exhaustive enumeration.
 In this section, we address the problem (1) discussed in Section III by defining the subgraph feature selection for multi-label graph classification as an optimization problem. The goal is to find an optimal set of subgraph features based on the multiple labels of graphs. Formally, let us introduce the following notations:  X  S = { g 1 ,g 2 ,  X  X  X  ,g m } : a given set of subgraph features,  X  T  X  : the optimal set of subgraph features T  X   X  X  .  X  E ( T ) : an evaluation criterion to estimate the usefulness
We adopt the following optimization framework to select an optimal subgraph feature set: where t denotes the maximum number of feature selected, | X | is the size of the feature set. Similar optimization framework to select an optimal subgraph feature set has also been defined in the context of single-label graph feature selection in [20], [2]. In Eq. 1 the objective function has two parts: the evaluation criterion E and the subgraph features of graphs S .

For evaluation criterion, we assume that the optimal sub-graph features should have the following property, i.e. De-pendence Maximization : Optimal subgraph features should maximize the dependence between the subgraph features of graph objects and their multiple labels. This indicates that two graph objects with similar sets of multiple labels are likely to have similar subgraph features. Similar assumptions have also been used for multi-label dimensionality reduction in vector spaces [25].
 Many criteria that can be used as dependence evaluation. In this paper, we will derive an evaluation criterion for multi-label graph classification based upon a dependence evaluation criterion named Hilbert-Schmidt Independence Criterion (HSIC) [8]. In detail, by deriving from the def-inition of HSIC, we can rewrite the optimization problem in Eq. 1 as follow: where tr(  X  ) is the trace of matrix and H =[ H ij ] n  X  n H ij =  X  ij  X  1 /n ,  X  ij is the indicator function which takes 1 when i = j and 0 otherwise. K T denote the matrix of the inner product of graphs X  feature vectors corresponding to subgraph feature set T , which is a kernel matrix of graphs with the kernel function k ( G i ,G j )=  X  ( G i ) , X  ( G
D T x i ,D T x j .Here D T = diag(  X  T ) is a diagonal matrix indicating which features ar e selected into feature set T from S .And  X  T =[  X  1 T , X  2 T ,  X  X  X  , X  m T ]  X  X  0 , 1 } m indicator vector, and  X  i T =1 iff g i  X  X  . L =[ L ij ] n is a kernel matrix for the graph X  X  multiple labels with the kernel function l ( y i , y j )=  X  ( y i ) , X  ( y j ) . In our current implementation, l ( y i , y j )= y i , y j is used as the label kernel, and other kernels can also be directly used in this formulation.

The formula in Eq. 2 can be rewritten as follow: where M = HLH . By denoting function h ( g i ,M )= f i M f i , the optimization (2) can be written as
D EFINITION 4( gHSIC ): Suppose we have a multi-labeled graph dataset D = { ( G 1 , y 1 ) ,  X  X  X  , ( G n , be a kernel matrix defined on the multiple label vectors, and M = HLH . We define a quality criterion q called gHSIC, for a subgraph feature g as where f g =[ f (1) g ,  X  X  X  ,f ( n ) g ]  X  X  0 , 1 } n is the indicator vector for subgraph feature g , f ( i ) g =1 iff g  X  G i ( i =1 , 2 ,  X  X  X  ,n ). Since matrix L and M are positive semi-definite, for any subgraph pattern g , q ( g )  X  0 .
The optimal solution to the problem in Eq. 2 can be found by using gHSIC to forward feature selection on a set of subgraphs S . Suppose the gHSIC values for all subgraphs are denoted as q ( g 1 )  X  q ( g 2 )  X  X  X  X  X  X  q ( g m ) in sorted order. Then the optimal solution to the optimization problem in Eq. 3 is:
Now we address the second problem discussed in Sec-tion III, and propose an efficient method to find the optimal set of subgraph features from a given multi-label graph dataset.

Exhaustive enumeration: One of the most simple and straightforward solution for finding an optimal feature set is the exhaustive enumeration, i.e. , we first enumerate all subgraph patterns in a multi-label graph dataset, and then calculate the gHSIC values for all subgraph patterns. How-ever, in the context of graph classification, the number of subgraphs grows exponentially with the size of graphs, which makes the exhaustive enumeration approach usually impractical in real-world data.

Inspired by recent advances in graph classification ap-proaches, e.g. [22], [14], which put their evaluation criteria into the subgraph pattern mining steps and develop con-strains to prune search spaces, we take a similar approach by deriving a different constrain for multi-label cases. In order to avoid the exhaustive search, we proposed a branch-and-bound algorithm, named gMLC, which is summarized as follow: a) Adopt a canonical search space where all the subgraph patterns can be enumerated. b) Search through the space, and find the optimal subgraph features by gHSIC. c) Propose an upper bound of gHSIC and prune the search space.
 A. Subgraph Enumeration
In order to enumerate all subgraphs from a graph dataset, we adopted an efficient algorithm, gSpan, proposed by Yan et al[23]. We briefly review the general idea of gSpan approach: Instead of enumerating subgraphs and testing for isomorphism, they first build a lexicographic order over all the edges of a graph, and then map each graph to an unique minimum DFS code as its canonical label. The minimum DFS codes of two graphs are equivalent iff they are isomorphic. Details can be found in [23]. Based on this lexicographic order, a depth-first search (DFS) strategy is used to efficiently search through all the subgraphs in a DFS code tree. By a depth-first search through the DFS code tree X  X  nodes, we can enumerate all the subgraphs of a graph in their DFS code X  X  order. And the nodes with non-minimum DFS codes can be directly pruned in the tree, which saves us from performing an explicit isomorphic test among the subgraphs.
 B. Upper Bound of gHSIC
Now, we can efficiently enumerate all the subgraph pat-terns of a graph dataset in a canonical search space using gSpan X  X  DFS Code Tree. Then, we derive an upper bound for the gHSIC value which can be used to prune the search space as follow:
T HEOREM 1 (Upper bound of gHSIC): Given any two subgraphs g,g  X  X  , g is a supergraph of g ( g  X  g ). The gHSIC value of g ( q ( g ) ) is bounded by  X  q ( g ) ( i.e. , q ( g )  X   X  q ( g ) ), where  X  q ( g ) is defined as follow: where the matrix  X  M is defined as  X  M ij =max(0 ,M ij ) . f g = { I ( g  X  G i ) } n i =1  X  X  0 , 1 } n is a vector indicating which graphs in a graph dataset { G 1 ,  X  X  X  ,G n } contain the subgraph g , I (  X  ) is the indicator function. Suppose the gHSIC value of g is q ( g )= f g M f g .
 where G ( g )= { G i | g  X  G i , 1  X  i  X  n } .Since g is the supergraph of g ( g  X  g ), according to anti-monotonic property, we have G ( g )  X  X  ( g ) .Also  X  M ij =max(0 ,M we have  X  M ij  X  M ij and  X  M ij  X  0 .So, Thus, for any g  X  g , q ( g )  X   X  q ( g ) .
 C. Subgraph Search Space Pruning
In this subsection, we make use of the the upper bound of gHSIC to efficiently prune the DFS Code Tree using a branch-and-bound method, which is similar to [14] but un-der different problem context: In depth-first search through the DFS Code Tree, we maintain the temporally suboptimal gHSIC value (denoted by  X  ) among all the gHSIC values calculated before. If  X  q ( g ) &lt; X  , the gHSIC value of any supergraph g ( g  X  g ) is no greater than  X  .Now,we can safely prune the subtree from g in the search space. If  X  q ( g )  X   X  , we can not prune this space since there might exist a supergraph g  X  g ( q ( g )  X   X  ).

Figure 3 shows the algorithm gMLC. We first initialize the subgraphs T as an empty set. Then we prune the search space by running gSpan, while always maintaining the top-t best subgraphs according to q . In the course of mining, whenever we search to a subgraph g with  X  q ( g )  X  min g i  X  X  q ( g i ) , such that for any supergraph g ( q ( g )  X   X  q ( g ) ) according to the bound defined in Eq. (6), we can prune the branches of the search tree originating from g . In the other hand, as long as the resulting subgraph g can still improve the gHSIC value of any subgraph g i  X  X  , it is accepted into T and the last best subgraph is dropped off from T .

Note that in our experiments with the three datasets, the gHSIC criterion based on multiple labels provides such a bound that we can even omit the support threshold min sup and still find a set of optimal subgraphs within a reasonable time cost.
 A. Experimental Setup
Data Collections: In order to evaluate the multi-label graph classification perform ances, we tested our algorithm on three real-world multi-label graph classification tasks as follow: 1) Anti-cancer activity predictio n (NCI1): The first task is 2) Toxicology prediction of chemical compounds (PTC): 3) Kinase inhibition predi ction of chemical compounds
Evaluation Metrics: Multi-label classification require different evaluation metrics th an conventional single-label classification problems. Here we adopt some metrics used in [5], [24] to evaluate the multi-label graph classification performance. Assume we have a multi-label graph dataset D = { ( G 1 , y 1 ) ,  X  X  X  , ( G n , y n ) } , where graph G as y i  X  X  0 , 1 } Q .Let f ( G i ,k ) denote the classifier X  X  real-value outputs for G i on the k -th label ( l k ). We have the following evaluation criteria: a) Ranking Loss [5]: evaluates the performance of classi-b) Average Precision [24]: evaluates the average fraction
In our experiment, we will show the value of 1  X  AvePrec instead of Average Precision . Thus under all these evaluation criteria, smaller values are all indicating better performances. All experiments are conducted on machines with 4 GB RAM and Intel Xeon TM Quad-Core CPUs of 2.40 GHz.

Comparing Methods: In order to demonstrate the effec-tiveness of our multi-label graph feature selection approach, we test with following methods:  X  Top-k Frequent subgraph features + multi-label clas-B. Performances on Multi-label Graph Classification
In our experiment, we use 10 -round 10 -fold cross valida-tion to evaluate the multi-label graph classification perfor-mance. Each graph dataset is evenly partitioned into 10 parts. Only one part is used as testing graphs and the other nine are used as training graphs for frequent subgraph mining, feature selection and multi-label classification. We repeat the 10-fold cross validation 10 times and w e report the average results for the 10 rounds. The result of the feature selection methods for multi-label graph classification on NCI1, NCI2 and PTC datasets are displayed in Figure 4, Figure 5 and Figure 6. We show the number of selected subgraphs t among frequent subgraphs using min sup = 10% , together with evaluation metrics mentioned before.

Now, we first study the effectiveness of selecting sub-graph features by comparing two approaches: gMLC+SVM, Binary IG+ SVM, where the binary SVMs are used as base learners. It is worth noticing that, our gMLC is specially designed for conventional multi-label classification methods which require one set of features for all labels concepts. Thus gMLC only selects one set of subgraph features and uses it on multiple SVMs separately. However, Binary IG+ SVM selects a different set of subgraph features for each label concept and these feature sets are used on multiple SVMs separately. Hence, Binary IG+ SVM method has an advantage over our method by using different feature sets for different SVMs, while gMLC uses the same set of feature for all the SVMs. Figure 4, Figure 5 and Figure 6 indicate that gMLC+SVM can achieve compariable or even better performances than Binary IG+ SVM in most cases. This is because the multiple labels of the graphs usually have certain correlations, and the useful subgraph features on one label concept are also likely to be useful on some other label concepts. Thus our gMLC method can achieve better performances over Binary IG+ SVM even though we use a same set of feature for all binary SVMs. Utilizing the potential relations among multiple label concepts to select subgraph features are crucial to the success of our method in this case.

We further study the effectiveness of subgraph features using one of the general purposed multi-label classifi-cation methods, i.e. B OOS T EXTER , as the base classi-fier. It is also worth noticing that, to the best of our knowledge, gMLC is the first multi-label feature selec-tion method for graph data. Thus we cannot find any other baseline which select one set of feature for multiple label concepts in order to make a fair comparison. So our only choice is comparing the following two meth-ods: gMLC+B OOS T EXTER and Freq+B OOS T EXTER .We observe that on most tasks gMLC+B OOS T EXTER  X  X  per-formances are better than Freq+B OOS T EXTER , i.e. multi-label classification approaches without gHSIC subgraph feature selection. These results support our intuition that the gHSIC evaluation criterion in gMLC can find better subgraph patterns for multi-label graph classification than unsupervised top-k frequent subgraph approaches. The ex-ception is only the case on PTC dataset when the number of features selected is small (less than 15). Nonetheless, the Freq+B OOS T EXTER can never reach the best performance achievable by gMLC with a larger number of features. This is because the top 15 frequent features happen to be good classification features. However, the Freq cannot find other good features that are not that frequent.

We further observe that in all tasks and evaluation crite-ria, our multi-label feature selection algorithm with multi-label classification (gMLC+B OOS T EXTER ) outperforms the binary decomposition approach using single-label feature selections (Binary IG+ SVM). gMLC+B OOS T EXTER can achieve good performances with only a small number of features. Although this comparison is not quite fair, and the big improvement can both be counted on the good perfor-mance of gMLC feature selection and the state-of-the-art multi-label classification method, B OOS T EXTER .However, this result can just be used for a reference to the relative performances of the two types of multi-label graph classi-fication methods, binary deco mposition based and gMLC based. These results support the importance of the proposed multi-label feature selection method in the multi-label graph classification problems.
 C. Effectiveness of Subgraph Search Space Pruning
In our second experiment, we evaluated the effectiveness of the upper-bound for gHSIC proposed in Section V-B. So, in this section we compare the runtime performance of two versions of implementation for gMLC:  X  X ested gMLC X  versus  X  X n-nested gMLC X . The  X  X ested gMLC X  denotes the proposed method using the upper-bound proposed in Section V-B to prune the search space of subgraph enumer-ations; the  X  X n-nested gMLC X  denotes the method without the gHSIC X  X  upper-bound pruning, which first uses gSpan to find a set of frequent subgraphs, and then selects the optimal set of subgraphs via gHSIC. We run both approaches on the three tasks and record the average CPU time used on feature mining and selection. The result is shown in Figure 7.
In the NCI1, NCI2 and PTC dataset, we observe that as we decrease the min sup in the frequent subgraph mining, the un-nested gMLC would need to explore larger subgraph search spaces, and this size increases exponentially with the decrease of min sup . In the NCI1 dataset, when the min sup get too low ( min sup &lt; 4% ), the subgraph feature enumeration step in un-nested gMLC can run out of the computer memory. However, the nested gMLC X  X  running time does not increase as much, because the gHSIC can help pruning the subgraph search space using the multi-label information of the graphs. As we can see, the min sup can go to very low value in all datasets for the  X  X ested gMLC X .
Figure 8 shows the number of subgraph feature explored in the process of subgraph pattern enumeration in the three tasks. In all tasks, we observe that the number of searched subgraph patterns in nested gMLC is much smaller than that of un-nested gMLC (the gSpan step). In our experiments, we further noticed that on most datasets, nested gMLC provides such a strong bound that we may even allow nested gMLC to omit the minimum support threshold min sup and still receive an optimal se t of subgraph features within a reasonable time.

In this paper, we study the problem multi-label feature selection for graph classification, propose an evaluation criterion gHSIC to evaluate the dependence of subgraph features with the multiple labels of graphs, and derived an upper-bound for gHSIC to prune the subgraph search space. Then we propose a branch-and-bound algorithm to efficiently find a compact set of subgraph feature which is useful for the classification of graphs with multiple labels. Empirical studies shows that multiple labels can help selecting useful features for gra ph classification. Moreover, label correlations among the multiple labels can be very useful for multi-label feature selection problem in graph data. In our current implementation, we only calculate the inner-products to get the label kernel matrix. However, by adopting more advanced kernel s, the label correlations can also be considered under the current gMLC framework.
This work is supported in part by NSF through grants IIS 0905215, DBI-0960443, OISE-0968341 and OIA-0963278.
