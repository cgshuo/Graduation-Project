 Semi-supervised learning has been successfully applied to many fields such as knowledge management, information re-trieval and data mining. In this paper, we propose a gen-eral semi-supervised framework for multi-class categoriza-tion. Many classical supervised and semi-supervised method can be viewed as special cases of this framework. Based on this framework, we propose a novel method called multi-class unlabeled constrained SVM(MCUCSVM) and its spe-cial case: multi-class Laplacian SVM(MCLapSVM). We then put forward a general kernel version semi-supervised dual co-ordinate descent algorithm to efficiently solve MCUCSVM and makes it more applicable to problems with large number of classes and large scale labeled data. Both rigorous the-ory and promising experimental results on four real world datasets show the great performance and remarkable effi-ciency of MCUCSVM and MCLapSVM.
 I.2 [ ARTIFICIAL INTELLIGENCE ]: Learning ; H.3 [ INFORMATION STORAGE AND RETRIEVAL ]: Information Search and Retrieval Algorithms, Design, Performance Semi-supervised learning, Multi-class categorization, Multi-class UCSVM, Multi-class Laplacian SVM, Multi-class SVM, Dual coordinate descent method
Semi-supervised learning has received increasing attention during recent years since it can utilize both labeled and unla-beled data. Large amount of work demonstrate the effective-ness of semi-supervised learning over the traditional super-vised counterpart. In the literature, some semi-supervised methods [1] including inductive methods [7][6] and transduc-tive methods [4] have been proposed. Laplacian SVM [6] is a classical method basing on a However, there are several limi-tations for Laplacian SVM. First, although binary Laplacian SVM can deal with multiple classification using one versus rest or one versus one schemes, it is rather expensive espe-cially when number of classes is large. Second, it has cubic complexity which limits its practical application with large scale data. Third, it only consider the supervised error con-straint. Thus, it doesn X  X  make enouth use of the unlabeled data.

Coordinate descent method is a popular optimization ap-proach which has been broadly used in machine learning [2][3] [5]. Lin et al [3] proposed a dual coordinate descent method which can solve the dual quadratic programming of SVM in linear time. However, their method is limited to supervised scenario.
To unify the notations in this paper, we assume that there are l labeled data denoted by X L = { ( x i , y i ) } l i =1 y where C is the number of classes. We use K ( x i , x j ) to de-note the kernel function defined by K ( x i , x j ) =  X  ( x where  X  is the map from data space X into the Hilbert space of functions mapping X into R . We use  X ( X ) to denote the We define G = ( V, E ) as the graph associated with the sam-ples. V is the vertex set of graph, which is defined on the training set. E is the edge set containing the pairs of neigh-boring vertices ( x i , x j ). There are two typical methods to calculate the weight matrix or adjacency matrix W : (1) Gaussian kernel of width  X  : (2) K-nearest neighbors: Where N ( x ) denotes the set of K nearest neighbors of x . Let L be the graph Laplacian given by L = D  X  W where W ij is the edge weight in the data adjacency matrix defined in (1) or (2) and D is the diagonal matrix given by D ii = P j =1 W ij . One often use the normalized Laplacian defined by  X  L = D  X  1 / 2 LD  X  1 / 2 . K is the gram matrix defined by K =  X  T ( X ) X ( X ) . J is a label indicator matrix which is defined by J ij = 1 only if x i = X L j , J ij = 0 otherwise.
We first put forward a general semi-supervised multi-class framework as follows.
 F  X  = arg min Where V L is some loss function such as the hinge loss or square loss on labeled samples. V U is a loss function con-trolling the error on both labeled and unlabeled samples which utilizes the latent label F s of all samples. k F k nalizes the RKHS norm to impose smoothness conditions on possible solutions. k F k 2 I is a penalty term that reflects the geometric structure of samples which penalize the la-tent label F s . That means that we allow some difference between the decision value F ( x ) and its latent label F and we can enlarge the searching domain of F ( x ) and attain a better decision function. The decision function usually has a bias term b , one often deal with this term by appending each sample with an additional constant dimension: In this framework, we utilize the hinge loss function V L f function V U ( x i , f k , f s k ( x i )) = ( w k T  X  ( x f ( x i ) is the latent label of x i belonging to the k th class. We use the graph Laplacian: k F k 2 I = f ( x j )) 2 = trace ( F s T L F s ), to reflect the geometric struc-ture of samples, here f s k = [ f s k ( x 1 ) , . . . , f k F k 2 K we use the sum of the squared norm of projector P class unlabeled constrained SVM as follows:
The problem in Eq (5) is a linear inequality constrained quadratic convex optimization problem. Using the standard lagrange multiplier technique, we obtain: Define: We can express the projection matrix W by the combination of {  X  ( x i ) } l + u i =1 as follows: So we obtain the dual problem of Eq (5) as Where  X  Q is a semi-definite positive matrix given by  X  Once the variables  X  ipq are solved, the expansion coefficient matrix  X  could be calculated and the k th decision function f ( x ) could be represented by the kernel functions { g i ( x ) = K ( x i , x ) } l + u i =1 as follows: The label of sample x could be obtained by Simply making  X  3 approach to infinity, we propose the multi-class Laplacian SVM where A mclapsvm = ( I +  X  2  X  L  X  T )
Multi-class UCSVM is a general framework. First, it is a semi-supervised framework base on graph Laplacian. The traditional supervised method like SVM and RLS can be included as special case. Second, it introduce both the la-beled and unlabeled error loss, thus Laplacian SVM and LapRLS can be also included in our framework. Third, it is a multi-class framework which extents the binary classifi-cation method. Table 1 shows the connections with several classical methods.
The kernel version dual coordinate descent method for multi-class UCSVM is listed in Algorithm 1. Calculating  X  ipq g (  X  ) takes O ( n ) operations, updating  X  needs O ( n ) operations. Therefor, for each outer iteration(Updating  X  one time) is O ( n  X  n ), where n  X  = l ( C  X  1) is the number of lagrange multipliers in  X  . Note that for multi-class Lapla-cian SVM, we only need to set Q = J T K ( I +  X  2 LK )  X  1 and B = ( I +  X  2 LK )  X  1 J , for multi-class SVM, we need to set Q = J T KJ and B = J . Theorem 1 ensures the linear convergence of our algorithm.

Theorem 1.  X  generated by Algorithm 1 globally con-verges to an optimal solution  X   X  . The convergence rate is Algorithm 1 A kernel version dual coordinate descent method for multi-class UCSVM Require: Q = J T K
B =
Start with  X  = 0 and  X  = 0 while 1 do end while at least linear: there are 0 &lt;  X  &lt; 1 and an iteration k that
The overall complexity for multi-class UCSVM is O ( ln 2 + n n ) under nonlinear kernel. We sacrifices the speed for broad application to nonlinear classification with both la-beled and unlabeled data. However, if we exert the lin-ear kernel in supervised scenario, multi-class UCSVM takes O ( n  X   X  m ) operations, where  X  m is the number of the nonzero elements of training sample.
All the parameters are the best settings in grid search with respect to the mean error rates shown in Table 2 with the grid search. We constructed the adjacency matrix with 6-nearest neighbors using Eq (2) and build the normalized Laplacian matrix. Table 5 describes the number of labeled data, the number of classes and the corresponding applica-tion for the four real world datasets. As for the efficiency evaluation, we use a desktop computer with the configura-tion of Intel(R) Core(TM)2 Quad CPU Q6600 @2.40GHz and 3.25GB memory. For each class, we select 80% samples to build the train set and the remaining 20% to build the test set.
For Umist dataset 1 , we use linear kernel here. We do 10 in-dependent experiments. According to Table 3, MCUCSVM performs the best. From Table 4, we can conclude that: 1. Under the same precision, our algorithm is more efficient and stable than matlab quadprog function. 2. Although http://www.cs.toronto.edu/  X roweis/data.html Table 1: Connections of Multi-class Unlabeled con-straints SVM with traditional methods  X  3  X  X  X  , C  X  = 3 multi-class(MCSVM,RLS)  X  3  X  X  X  , C  X  = 3 multi-class(MCLapSVM,LapRLS) Umist  X  1 = 0 . 1  X  1 = 1 ,  X  2 = 0 . 1 Coil20  X  1 = 0 . 1  X  1 = 0 . 1 ,  X  2 = 1 20NG  X  1 = 0 . 01  X  1 = 10 ,  X  2 = 10 Umist  X  1 = 1 ,  X  2 = 0 . 1  X  1 = 10 ,  X  2 = 1 ,  X  3 = 1 Coil20  X  1 = 0 . 1 ,  X  2 = 1  X  1 = 1 ,  X  2 = 10 ,  X  3 = 10 20NG  X  1 = 10 ,  X  2 = 10  X  1 = 10 ,  X  2 = 10 ,  X  3 = 10 4 Method Umist Coil20 mcsvm 13.4(2.4) 45.2(6.6) 31.0(1.6) 37.0(8.6) lapsvm 11.5(3.1) 42.5(5.5) 26.7(2.4) 34.5(3.7) mclapsvm 10.8(3.3) 40.2(7.0) 26.4(2.2) 35.0(7.8) Method Isolet 20Newsgroup mcsvm 28.4(5.7) 36.5(6.0) 18.7(3.3) 20.0(2.9) lapsvm 25.5(4.7) 33.5(4.8) 6 . 2 ( 1 . 3 ) 17 . 3 ( 2 . 2 ) mclapsvm 26.9(5.4) 34.4(5.4) 6 . 2 ( 1 . 3 ) 17.4(2.2) mcucsvm 24 . 3 ( 4 . 4 ) 32 . 4 ( 4 . 9 ) 7.7(3.8) 20.6(5.0) Table 4: The mean time spent on the quadratic pro-gramming using the matlab quadprog function and the dual coordinate descent method respectively in seconds(std(seconds)).

Method mcsvm lapsvm Umist 20.9(1.6) 4 . 4 ( 0 . 7 ) 118.8(8.0) 1 . 1 ( 0 . 1 ) Coil20 160.1(10.6) 22 . 7 ( 5 . 1 ) 881.8(31.4) 3 . 8 ( 0 . 3 )
Isolet 5.0(0.4) 0 . 9 ( 0 . 1 ) 43.7(0.7) 0 . 1 ( 0 . 0 ) 20NG 52.5(3.5) 0 . 3 ( 0 . 0 ) 325.3(9.6) 78 . 3 ( 9 . 9 )
Method mclapsvm mcucsvm Umist 5.8(0.4) 1 . 4 ( 0 . 3 ) 5.8(0.5) 1 . 1 ( 0 . 1 ) Coil20 43.9(1.6) 3 . 2 ( 0 . 4 ) 41.8(1.8) 4 . 7 ( 0 . 4 )
Isolet 2.3(0.1) 1 . 3 ( 0 . 3 ) 2.3(0.1) 1 . 3 ( 0 . 3 ) 20NG 80.2(5.0) 45 . 4 ( 8 . 0 ) 75.8(7.8) 42 . 1 ( 8 . 6 ) Figure 1: The average time spent on the quadratic programming on Umist dataset using the matlab quadprog function and the dual coordinate descent algorithm with varying number of labeled data.
 Table 5: Datasets Descriptions. # C denotes the to-tal number of classes. # l/c denotes the number of labeled data in each class for experiments on perfor-mances.
 MCUCSVM has one more parameter than MCLapSVM, they nearly cost the same time. 3. MCLapSVM is more applicable than LapSVM(one v.s. rest).
For Coil-20 dataset 2 we use the linear kernel. 10 inde-pendent experiments are performed. Table 3 shows that MCUCSVM performs the best since it utilizes both the la-beled and unlabeled constraints. From Table 4, we can make the same conclusions to that made by experiment on Umist dataset.
We use the first 30 speakers(Isolet 1) for training and Iso-let 5 for testing. We do 30 independent experiments. In each experiment, all the spoken letters for one speaker are labeled with the other letters unlabeled. We consider the multiple classification problems with 26 classes. Here we use the rbf kernel with  X  = 10. From Table 3, we see that MCUCSVM performs better than all the other three methods. http://www1.cs.columbia.edu/CAVE/software/softlib/coil-20.php
We perform multi-class problems on four categories(christian, crypt, forsale and hockey) using cosine similarity. We took normalized TFIDF [4] as the feature vector. From Table 3, we can see 1. MCLapSVM is comparative to LapSVM(one v.s. rest) in terms of performance. 2. Unlabeled constraint isn X  X  always effective. 3. For problems with small number of classes, the speed improvement using dual coordinate de-scent algorithm is not noticeable.
In this section, we perform two experiments to evaluate the efficiency of our algorithm. From Figure 1 and Table 4, we can infer that dual coordinate descent algorithm is more applicable and competent for real problems with large num-ber of labeled data and large number of categories.
In this paper, we propose a general semi-supervised frame-work for multi-class categorization and a novel method called multi-class unlabeled constrained SVM(MCUCSVM) and its special case: multi-class Laplacian SVM(MCLapSVM). We then propose a general kernel version semi-supervised dual coordinate descent algorithm to efficiently solve the quadratic programming of MCUCSVM and MCLapSVM. Both rigor-ous theoretical analysis and practical evaluations on four real world datasets validate the high performance and great efficiency of MCUCSVM and MCLapSVM.
This work has been supported by NSFC (Grant No. 60835002 and No. 60675009). [1] O. Chapelle, B.Scholkopf, and A. Zien. Semi-Supervised [2] G. Chen, J. Zhang, F. Wang, C. Zhang, and Y. Gao. [3] C. Hsieh, K. Chang, C. Lin, S. Keerthi, and [4] T. Joachims. Transductive inference for text [5] S. Keerthi, S. Sundararajan, K. Chang, C. Hsieh, and [6] P. N. M. Belkin and V. Sindhwani. Manifold [7] V. Vapnik. Statistical Learning Theory .

