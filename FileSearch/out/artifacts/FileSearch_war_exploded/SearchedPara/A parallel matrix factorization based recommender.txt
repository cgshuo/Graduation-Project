 1. Introduction successful attitude when implementing recommender systems ( Adomavicius and Tuzhilin, 2005 ), can produce user specific recom-mendations based on historical user behaviors. Inside a CF recom-mender, the user interests on invol ved items (e.g., scores, clicks, purchase records, etc.) are quantized into a user-item rating matrix, where high ratings denote strong preferences. So the problem of CF can be regarded as the problem of missing data estimation, in which the main task is to estimate the unknown user-item pairs based on known entries with minimum accumulative error.
 successful kind of approach to CF is based on Matrix Factorization (MF). MF based recommenders work by transforming both items and users into the same latent f actor space, characterizing each entity with a feature vector inferred from the existing ratings, and then making predictions for unknown ratings using the inner products of the corresponding vector pairs. The earliest work of this kind is proposed by Sarwar et al. (2000) , employing the Singular
Value Decomposition (SVD). More recently, several MF techniques have been successfully applied to implementing CF recommenders, including the Probabilistic Latent Semantic Analysis by Hofmann (2004 ), the Maximum Margin Matrix Factorization by Srebro et al. (2005) , and the Expectation Maximization for Matrix Factorization by Kurucz et al. (2007 ). During the Netflix Prize, Brandyn Webb published the Regularized Matrix Factorization (RMF) ( Webb, 2006 ), which is accurate, highly efficient and easy to implement. Inspired by RMF, many researchers have further investigated MF based approaches. Paterek (2007 ), Taka  X  cs et al. (2009 ), Koren et al. (2009) and Koren and Bell (2011 ) all have proposed sophisticated MF based CF recommenders.

Although the aforementione d MF based recommenders have proven to be highly scalable and accurate, they are all serial models, where the building process cannot be parallelized. With the rapid development in computer hardware, large-scale PC clusters are becoming common; moreover, the progress in parallel programming techniques makes it possible to efficiently take use of this computa-tion power. Under such circumstance, the serial nature of most current recommenders is a great obstacle preventing them from sharing the efficiency brought by pa rallel computation. So it is very meaningful to parallelize the MF based recommender, especially for the practical use in real world applications.

Since parallel design can greatly enhance the practicability of the recommenders, there are many pioneering works focusing on this issue. Thomas and Srujana (2005) , proposed a parallel CF framework based on weighted co-clustering ( Gorrell and Webb, 2005 ); Srivatsava et al. (2009) , proposed a data-flow implementation of parallel CF based on weighted co-clustering ( Gorrell, 2006 ); Narang et al. (2010) , proposed a parallel CF algorithm based on the concept decomposition technique ( Thomas and Srujana, 2005 ). Actually, it is a popular and natural choice to implement the p arallel CF recommender based on clustering techniques, for the conve nience of parallelizing the cluster building process, along with fairly good prediction accuracy. How-ever, seeking for the parallel MF based recommenders is also mean-ingful because of the high recommendation performance and good extensibility. Besides, for constr ucting parallel he terogeneous CF ensembles which have proved to be superior to single models necessary to design the parallelism of different types of recommen-ders. For the parallel CF recommender based on MF techniques, one representative model is based on the Parallel Alternating Least Squares (hereafter referre d to as P-ALS) proposed by Srivatsava et al. (2009) . P-ALS can efficiently parallelize the training process, however, because of the frequent multiplication between high-dimensional matrices, it usually runs slower than the RMF model onasinglePC( Koren et al., 2009 ; Koren and Bell, 2011 ; Srivatsava et al., 2009 ).

In this work we aim at parallelizing the RMF based recom-mender, an MF based recommender with high recommendation performance and ease of implementation. As a matter of fact, because the construction of the RMF model can be regarded as a Stochastic Gradient Decent (SGD) training process, a more general choice to implement the parallelism is to employ the parallel SGD solver, e.g., the mini-batch scheduling based method proposed by Narang et al. (2010 ), and Zhou et al. (2008 ); and the parallelized SGD solver proposed by Delalleau and Bengio (2007) . However, as described in ( Zhou et al., 2008 ), with the number of employed processors increasing, the mini-batch scheduling might make the model unstable and not converge. And for the parallelized SGD solver described in ( Delalleau and Bengio, 2007 ), since it mainly focus on the bottom support mechanism of parallel data proces-sing while our objective is to parallelize the model construction, we do not adopt this method in this paper either.

In the following, we firstly carry out the theoretical analysis on the parameter updating process of RMF , whereby we can figure out that the main obstacle preventing the model from parallelism is the inter-dependence between item and user features. To remove the inter-dependence among parameters, we apply the Alternating Stochastic Gradient Solver (ASGD) solver to deal with the parameter training process. On this basis, we subseq uently propose the parallel RMF (P-RMF) recommender, which can parallelize the model building process through training different user/item features simultaneously. Afterwards, we have conducted experiments on two large, real datasets for validating the efficiency of P-RMF. The main contribu-tionsofthisworkinclude:  X  the theoretical analysis of the parameter updating process in
RMF model;  X  the parallel RMF (P-RMF) model, of which the parameter training can be proceeded in parallel to take advantage of the computing power provided by a cluster;  X  the empirical studies on two large, real datasets.

Section 2 gives the preliminaries. Section 3 describes our methods. The empirical validations are given in Section 4 . Finally, Section 5 discusses and concludes. 2. Preliminaries
The principle of CF is to make user specific recommendations of items matching the patterns discovered from historical user behaviors (e.g., scores, clicks, purchase records, etc.), which are usually represented by a user-item rating matrix. This rating matrix can be defined as follows:
Definition 1. The user-item rating matrix. Given an item set I and a user set U , then users X  preferences on items can be denoted by a 9 U 9 9 I 9 matrix R where each row vector denotes a specified user, each column vector denotes a specified item and each entry r denotes the user u  X  X  preference on item i .

Ordinarily, the entries with high values in R indicate strong user preferences on the corresponding items. So the problem of
CF can be regarded as missing data estimation, where the main task is to estimate the unknown user-item pairs based on known entries in R . Formally, this process can be defined as follows:
Definition 2. The problem of CF. Let R K denote the set consisting of known entries in R . Given a finite set of ratings T C training set, the objective is to construct a heuristic estimator, so-called recommender, which can estimate the unknown ratings with minimal accumulative error based on known ratings in T .
The accumulative error of the recommender is evaluated on a for avoiding overfitting.

According to recent progress on recommender systems, one of the most popular approaches to CF is based on matrix factoriza-tion. This should be primarily credited to Webb X  X  publication of the RMF (which is based on the SVD with Hebbian Learning ( Louppe and Geurts, 2010 ; Zinkevich et al., 2010 )) model ( Webb, 2006 ). Similar to the SVD based model proposed by Sarwar et al. (2000) , RMF also relies on building the low-rank approximation to
R ; however, by bounding this matrix approximation with a quadratic system, RMF can provide much higher scalability and accuracy when dealing with extreme sparse rating matrices as in most CF problems.

The first step of implementing RMF is to transform the users and items into the same latent factor space. Let f denote the dimension of the latent factor space, P A R m f denote the latent feature matrix of users where each row represents a specified user, and Q A R f n denote the item latent feature matrix where each column represents a specified item (naturally f 5 min  X  m , n  X  ), then the estimation to r ui can be confined to the inner product of the corresponding user-item feature vector pair, given by: ^ r  X  p T u U q i :  X  1  X 
Thus, the accumulative error between the approximation matrix ^ R and R can be bounded by: RSE  X  where 99 U 99 denotes the standard Euclidean norm. Note that the bulk behind the parameter l is the Tikhonov regularizing term as the penalty factors for avoiding overfitting. Then the task con-centrates on appropriately estimating the values of parameters in
P and Q . This can be achieved by searching the global minimum of the quadratic system (2) by the Stochastic Gradient Decent (SGD) solver, which loops through all ratings in T and modifies the involved parameters by moving in the opposite direction of the gradient for each training example. This parameter training process can be formulated by:  X  P , Q  X  X  argmin ) 8 &lt; : where Z denotes the learning rate. By a standard SGD solver, all involved features will be trained according to Eq. (3) on the given training instances sequentially. To achieve a global minimum, this training process will be repeated for several times, each of which is called a training epoch. Due to its accuracy, efficiency and ease of implementation, RMF is further investigated by many research-ers; several sophisticated MF based recommenders are proposed based on this research ( Paterek, 2007 ; Koren and Bell, 2011 ). be highly efficient. However, motivated by the rapid progress in parallel computing, it is meaningful to further improve the scalability via parallelization. In this paper, we mean to imple-menting a parallel RMF based recommender which can imple-ment the parallelism as well as maintaining reasonable time cost and high prediction accuracy. In the following section, we will introduce our method in detail. 3. Method model. By combining the prediction rule (1), RSE (2) and feature updating rule (3), we see that RMF is an iterative learning model, where during each training epoch the training examples are learned sequentially, and the parameters are dependent on each other. Let
R ( u ) denote the rating set by a specified user u ,theneachtraining instance in R ( u ) can be orderly marked as r u , 1 , r u , 2 p u ,and p  X  1  X  u denote the value of p u after the example r by applying the feature updating rule (3), we get: where h 1 denotes the temporal state of q 1 .Let c  X  1 Z l ,  X  5  X  then Eq. (4) is transformed into:
By analogy, we can derive the expression of p u after the sequential training on the instances from r u, 2 to r u,K
Then we can obtain the expression of p u after one training epoch, by combining Eqs. (6) and (7): where q  X  h k  X  k denotes the temporal state of q k when the system is going to learn the k th example rated by user u . Similarly, let R ( i ) denote the rating set on item i , and sequentially mark each training instance in R ( i ) with r 1 , i , r 2 , i , ::: , r we can derive the expression of q i after one training epoch as: q i  X  c
From Eqs. (8) and (9), we see that the parameter training of the original RMF model possesses the characteristics that the training result of a latent feature is not only dependent on the initial value, but also on the temporal state of related features; e.g., the value of p u depends on the value of q  X  h k  X  k . Because of the inter-dependence of user features and item features, the training process of RMF is  X  X locked X  and cannot be separated into independent parts, as shown in Fig. 1 (a). So, once we remove the inter-dependence among parameters, we can parallelize each epoch smoothly, as shown in Fig. 1 (b).

Actually, the final outputs after a training epoch are only relevant to the initial values of user/item features, if they are not dependent on the temporal state of each other. This can be achieved by separating one epoch into two parts, during each of which training one kind of latent features while fixing the values of the other. This strategy leads to solving the latent factors in RSE(2) with the Alternating Stochastic
Gradient Decent (ASGD) solver, which also iterates over the given training set and updates the parame ters using the same learning rule with SGD, but in each iteration it only trains one kind of parameters (the user features/item features in our case). Based on this principle, we can summarize the training process employing the ASGD solver in Table 1 .

Compared with SGD, which updates both the specified user feature and item feature on each instance within a single itera-tion, ASGD iterates twice over the training dataset during each training epoch, during each of which the system only trains one kind of parameters. Thus, the final outputs are only connected to the initial value of each latent feature and the corresponding training examples. Then we can simply parallelize each epoch by training different user/item features at the same time. Based on the inference above, we design the training process of the parallel RMF (hereafter referred to as P-RMF) model using ASGD in Table 2 .

From Table 2 , we can find that the parallelism of P-RMF is implemented by splitting the training data into separate rating sets by each user/on each item, and training the corresponding latent features simultaneously. However, since this parallel train-ing process relies on a different solver, it is necessary to validate whether ASGD would result in negative effects on the prediction accuracy and the convergence rate. Moreover, since ASGD requires iterating twice on the training dataset, it would take more time for each training epoch. So, it is important to check if the parallelization could provide enough acceleration rate. 4. Experiments 4.1. Dataset description
The experiments were conducted on two datasets. The first dataset is the MovieLens 1M dataset (hereafter referred to as the ML1M dataset). This dataset contains 1 million ratings applied to 3900 movies by 6040 users with a rating scale on the [0,5] interval. The data density of ML1M is 4.25%. The other dataset used in our experiments is the Netflix dataset. This dataset contains over 100 million ratings from 480 users over 17 K movie titles. The rating scale lies in the range of [0,5]. In our experiment, we extracted a subset consisting of the ratings on the first 1000 movies. This subset is hereafter referred to as the NF5M dataset, which contains 5,010,199 ratings by 404,555 users. The data density of NF5M is 1.24%. In our experiments, we applied the 10-fold cross-validation experiment setting on both datasets. 4.2. Evaluation metrics
The recommenders X  prediction accuracy is measured by root mean squared error (RMSE) ( Herlocker et al., 2004 ), which is given by:
RMSE  X  where V denotes the validation set. For a given recommender, low
RMSE means high prediction accuracy. The parallel performance of evaluated models is evaluated by the speedup and parallel efficiency ( Lester, 1993 ). Formally, the speedup is computed by
S  X  N  X  X  T  X  1  X  = T  X  N  X  ,  X  11  X  where N denotes the number of nodes; T (1) and T ( N ), respectively denote the execution time of the algorithm on 1 node and N nodes. Moreover, since the essential purpose of this work is to provide a faster solution to build MF based recommender, we also recorded the speedup of the parallel model to the serial model (the RMF model in our case), which is given by
S  X  N  X  X  T seq = T  X  N  X  ,  X  12  X  where T seq denotes the running time of the sequential algorithm.
As to the parallel efficiency, it is computed by dividing the speedup by the employed node count. In our experiment, we recorded the efficiency of the parallel model to its one-core version, which is computed as:
E  X  N  X  X  T  X  1  X  =  X  N U T  X  N  X  X  ;  X  13  X  and the efficiency of the parallel model to the serial model, which is computed as:
E  X  N  X  X  T seq =  X  N U T  X  N  X  X  :  X  14  X  4.3. Experiment settings The tested models included the RMF, P-RMF and the Parallel
Alternating Least Squares (hereafter referred to as the P-ALS) model by Srivatsava et al. (2009) . All tested models were implemented on MATLAB R2010b. For RMF and P-RMF, the learning rate Z and the regularizing paramter l were set at 0.005 and 0.007 on ML1M, and 0.003 and 0.005 on NF5M. For
P-ALS, the regularizing parameter l 1 was set at 0.007 on NF1M, and 0.0055 on NF5M. The parameters were all decided through cross validation.

The experiments consisted of two steps. In the first step, we compared the performance of RMF and P-RMF, for valida-ting if P-RMF has any disadvantage in convergence rate and prediction accuracy. In the second step, we tested the parallel performance of the compared models. All experiments were conducted on a Microsoft Windows cluster consisting of 8 nodes, each of which is equipped with two 2.53 GHz dual-core CPUs and 8 GB RAM.
 4.4. Comparison between RMF and P-RMF and P-RMF. For a fair comparison, both RMF and P-RMF were tested on a single node without parallel settings. By fixing the learning rate and regularizing parameter, we repeated comparing the prediction accuracy, the converging epochs and the running time of both models, with the latent space dimension f raging from 20 to 500. The experimental results on the ML1M and NF5M datasets are respectively depicted in Figs. 2 and 3 . From these two figures, we have some findings:
III) Since P-RMF iterates twice on the training data in each the employment of the ASGD solver in P-RMF will not cause negative effect in prediction accuracy; however, since P-RMF iterates twice on the training data during each training epoch, the total running time is a bit higher than that of RMF on a single machine. Thus, we need to validate the parallel performance of
P-RMF in the next part of experiment. 4.5. The parallel performance of P-RMF support at most 32-core-parallelism. Based on the algorithm flow summarized in Table 2 , we simply implement P-RMF through the parfor settings provided by Matlab 2010b. The P-ALS model is implemented according to the description in ( Srivatsava et al., 2009 ). For a more detailed comparison, we fixed the value of f at 500 and repeated training the parallel recommenders with the number of employed cores increasing, and recorded the running time and RMSE of both models. Afterwards, we can compare the parallel performance of P-ALS and P-RMF, and validate if the parallelism could lead to a faster solution to building the MF based recommender ( Tables 3 and 4 ).
 respectively depicted in Figs. 4 and 5 . Based on the experiment results, we can find that the effect of the parallelism in P-RMF is obvious. When employing all 32 cores to build the P-RMF model on the ML1M dataset, the required training time was 247 s, which indicates a speedup at 10.87 to RMF, and at 16.54 to the serial version of P-RMF. On the NF5M dataset, the required training time of P-RMF was 2562 s, and the speedup to RMF and the serial verision of P-RMF was 13.13 and 18.77, respectively. Note that on NF5M the speedup of P-RMF to RMF is higher than that on ML1M. This is mainly caused by the much less converging epochs of P-RMF (42 epochs) compared to that of RMF (52 epochs). On the other hand, although the speedup and efficiency of
P-ALS to its one-core version was a bit higher than P-RMF (see the panels (c) and (d) in Figs. 4 and 5 ), the P-ALS model required much more training time compared to P-RMF. When training
P-ALS with 32 cores on the ML1M datasets, the required training time was 528 s, which indicates a speedup at 18.76 to the one-core version of P-ALS, but at 5.08 to RMF, while the P-RMF model can obtain a speedup at 10.87 to RMF; and on the NF5M dataset, the required training time of P-ALS was 7104, indicating a speedup at 4.74 to RMF, which is also lower than that of P-RMF at 13.13.

In the meanwhile, the prediction accuracy of RMF and P-RMF is very close, as shown in Tables 3 and 4 . On ML1M, the RMSE of
P-RMF (at 0.8427) was about 0.56% lower than that of RMF (0.8476); while on NF5M, P-RMF could obtain an RMSE (at 0.9226) 0.19% higher than that of RMF (0.9208). For comparison, the RMSE values of P-ALS on ML1M and NF5M were 0.8489 and 0.9235, respectively.

From the experiment results, we can summarize that the parallelism of P-RMF is very effective. When dealing with extre-mely sparse rating matrices as in most CF problems, P-RMF can provide a very fast solution for building MF based recommender, without loss in prediction accuracy. 5. Conclusion In this work we focus on parallelizing the RMF recommender.
To achieve this objective, we firstly analyze the parameter training process of RMF, and figure out the expression of the training results after each training epoch. Based on this analysis, we find that the main reason why the training process of RMF cannot be parallelized is that the user latent features and item latent features are dependent on the temporal state of each other during each training epoch. So, the key to implement the parallelism is to remove the inter-dependence between the user features and item features. For addressing this problem, we choose to apply the ASGD solver to training the latent features. This solver trains the features by iterating multiple times over the training dataset, during each of which only updates one kind of features while fixing the values of the other. By employing the ASGD solver, we successfully remove the parameter inter-depen-dence, and then parallelize the training process by dispatching different users/items to different nodes and training the corre-sponding features simultaneously. On this basis, we present the P-RMF model, which provides a parallel solution to build the RMF recommender.
 Two concerns should be validated before applying this design. The first is that whether the employment of the ASGD solver will result in negative effect on the convergence rate and prediction accuracy. Based on the experiment results on two common CF datasets, we find that RMF and P-RMF can obtain nearly the same prediction accuracy; while P-RMF requires fewer epochs to converge. Since the parameter update frequency is the same in both models, this phenomenon may be explained with the different parameter update orders. The other concern is that whether our parallel model P-RMF can provide a fast solution for building one MF based recommender. Through empirical studies, we find that the speedup of P-RMF to RMF is obvious, given that the additional iteration over the training data makes it a bit slower to build P-RMF on a single node than to build RMF.
When compared to another parallel model P-ALS, P-RMF has advantages in two aspects: (I) P-RMF only requires an additional iteration during each training epoch based on RMF, which makes it very easy to implement; (II) when dealing with the extreme sparse rating matrices as in most CF tasks, P-RMF is faster than P-ALS when provided with the same computation power.

Nonetheless, since the parallelism of P-RMF is implemented by simultaneously training different user/item features, the para-meter training for each user/item is serial. When dealing with extreme unbalance dataset, this might lead to a performance bottleneck. For dealing with this problem, it is interesting to investigate how to further parallelize the parameter updating process for a single user/item in the future work.
 Acknowledgment
Foundation under Grant 2011M501392, and the National Key Tech-nology R&amp;D Program of China under Grant no. 2011BAH25B01. References
