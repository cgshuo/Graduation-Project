 Histogram construction is a fundamental problem in data management, and a good histogram supports numerous min-ing operations. Recent work has extended histograms to probabilistic data [5 X 7]. However, constructing histograms for probabilistic data can be extremely expensive, and exist-ing studies suffer from limited scalability [5 X 7]. This work designs novel approximation methods to construct scalable histograms on probabilistic data. We show that our meth-ods provide constant approximations compared to the op-timal histograms produced by the state-of-the-art in the worst case. We also extend our methods to parallel and distributed settings so that they can run gracefully in a clus-ter of commodity machines. We introduced novel synopses to reduce communication cost when running our methods in such settings. Extensive experiments on large real data sets have demonstrated the superb scalability and efficiency achieved by our methods, when compared to the state-of-the-art methods. They also achieved excellent approxima-tion quality in practice.
 H.2.4 [ Information Systems ]: Database Management X  Systems histogram; scalable method; probabilistic database
In many applications, uncertainty naturally exists in the data due to a variety of reasons. For instance, data inte-gration and data cleaning systems produce fuzzy matches [10, 21]; sensor/RFID readings are inherently noisy [4, 9]. Numerous research efforts were devoted to represent and manage data with uncertainty in a probabilistic database management system [18, 21]. Many interesting mining prob-lems have recently surfaced in the context of uncertain data e.g., mining frequent pattern and frequent itemset [1, 3, 24]. In the era of big data, along with massive amounts of data from different application and science domains, uncertainty in the data is only expected to grow with larger scale.
Histograms are important tools to represent the distribu-tion of feature(s) of interest (e.g., income values) [15, 19]. Not surprisingly, using the possible worlds semantics [8, 21], histograms are also useful tools in summarizing and working with probabilistic data [5 X 7]. Given that answering queries with respect to all possible worlds is in #P-complete com-plexity [8], obtaining a compact synopsis or summary of a probabilistic database is of essence for understanding and working with large probabilistic data [5 X 7]. For example, they will be very useful for mining frequent patterns and itemsets from big uncertain data [1, 3, 24].

Cormode and Garofalakis were the first to extend the well-known V-optimal histogram (a form of bucketization over a set of one dimension values) [15], and wavelet histogram [16] to probabilistic data [6,7], followed by the work by Cormode and Deligiannakis [5]. Note that histogram construction can be an expensive operation, even for certain data, e.g., the ex-act algorithm for building a V-optimal histogram is based on a dynamic programming formulation, which runs in O ( Bn 2 for constructing B buckets over a domain size of n [15]. Not surprisingly, building histograms on probabilistic data is even more challenging. Thus, existing methods [5 X 7] do not scale up to large probabilistic data, as evident from our analysis and experiments in this work.

Thus, this work investigates the problem of scaling up his-togram constructions in large probabilistic data. Our goal is to explore quality-efficiency tradeoff, when such tradeoff can be analyzed and bounded in a principal way. Another objec-tive is to design methods that can run efficiently in parallel and distributed fashion, to further mitigate the scalability bottleneck using a cluster of commodity machines.
 Overview. A probabilistic database characterizes a prob-ability distribution of an exponential number of possible worlds, and each possible world is a realization (determinis-tic instance) of the probabilistic database. Meanwhile, the query result on a probabilistic database essentially deter-mines a distribution of possible query answers across all possible worlds. Given the possible worlds semantics, es-pecially for large probabilistic data, approximate query an-swering based on compact synopsis (e.g., histogram) is more desirable in many cases, e.g., cost estimations in optimizers and approximate frequent items [3, 5 X 7, 21, 24, 26].
Conventionally, histograms on a deterministic database seek to find a set of constant bucket representatives for the data distribution subject to a given space budget of buckets and an error metric. Building histograms on determinis-tic databases has been widely explored and understood in the literature. In probabilistic databases, building the cor-responding histograms need to address the following prob-lems: (I) how to combine the histograms on each possible world; (II) how to compute the histogram efficiently without explicitly instantiating all possible worlds.

One meaningful attempt is building histograms that seek to minimize the expected error of a histogram X  X  approxima-tion of item frequencies across all possible worlds, using an error metric, which was first proposed in [6, 7]. One con-crete application example might be estimating the expected result size of joining two probabilistic relations based on the corresponding histograms, or evaluating queries asking for an expected value approximately.

It is important to note that for many error metrics, this histogram is not the same as simply building a histogram for expected values of item frequencies; and the latter always provides (much) worse quality in representing the proba-bilistic database with respect to a number of commonly used error metrics, as shown in [6, 7]
Based on this definition, a unified dynamic programming (DP) framework of computing optimal histograms on the probabilistic data was proposed in [6, 7] with respect to var-ious kinds of error metrics. Specifically, for the widely used sum of square error (SSE), it costs O ( Bn 2 ) time where B is the number of buckets and n is the domain size of the data. Immediately, we see that the optimal histogram con-struction suffers from quadratic complexity with respect to the domain size n . For a domain of merely 100 , 000 values, this algorithm could take almost a day to finish and render it unsuitable for many data sets in practice.
 Summary of contributions. Inspired by these observa-tions, we propose constant-factor approximations for his-tograms on large probabilistic data. By allowing approxi-mations, we show that it is possible to allow users to adjust the efficiency-quality tradeoff in a principal manner.
We propose a novel  X  X artition-merge X  method to achieve this objective. We introduce  X  X ecursive merging X  to improve the efficiency, while the histogram quality achieved will not significantly deviate from the optimal version. We also de-vise novel synopsis techniques to enable distributed and par-allel executions in a cluster of commodity machines, to fur-ther mitigate the scalability bottleneck. To that end,
In addition, we survey other related works in Section 7 and conclude the paper in Section 8. Unless otherwise specified , proofs of theorems and lemmas were omitted due the space constraint and for brevity; they are available in Appendix B of our online technical report [25]. Uncertain data models. Sarma et al. [20] describes var-ious models of uncertainty, varying from the simplest basic model to the (very expensive) complete model that can de-scribe any probability distribution of data instances. Basic model is a over-simplification with no correlations. Existing work on histograms on uncertain data [5 X 7] adopted two popular models that extend the basic model, i.e., the tuple model and the value model , and compared their prop-erties and descriptive abilities. The tuple and value models are two common extensions of the basic model in terms of the tuple-and attribute-level uncertainty [20], that were ex-tensively used in the literature (see discussion in [5 X 7]).
Without loss of generality, we consider that a probabilistic database D contains one relation (table). We also concen-trate on the one dimension case or one attribute of interest. Definition 1 The tuple model was originally proposed in TRIO [2]. An uncertain database D has a set of tuples  X  = { t } . Each tuple t j has a discrete probability distribution function (pdf) of the form ( t j 1 ,p j 1 ) ,..., ( t j` j ifying a set of mutually exclusive (item, probability) pairs. Any t jk , for k  X  [1 ,` j ], is an item drawn from a fixed domain and p jk is the probability that t j takes the value t jk j th row of a relation.

When instantiating this uncertain relation to a possible world W , each tuple t j either draws a value t jk with prob-ability p jk or generates no item with probability of 1  X  P k =1 p jk . The probability of a possible world W is sim-ply the multiplication of the relevant probabilities. Definition 2 The value model is a sequence  X  of indepen-dent tuples. Each tuple gives the frequency distribution of a distinct item of the form  X  j : f j = (( f j 1 ,p j 1 ), ..., ( f  X  . Here, j is an item drawn from a fixed domain (e.g., source IP) and its associated pdf f j describes the distribution of j  X  X  possible frequency values.

In particular, Pr[ f j = f jk ] = p jk where f jk is a frequency value from a frequency value domain V ; f j is subject to the constraint that P jk p jk  X  1 for k  X  [1 ,` j ]. When it is less than 1, the remaining probability corresponds that the item X  X  frequency is zero . When instantiating this un-certain relation to a possible world W , for an item j , its frequency f j either takes a frequency value f jk with proba-bility p jk or takes zero as its frequency value with probability 1  X  P ` j k =1 p jk . So the probability of a possible world W is computed as the multiplication of the possibilities of f taking the corresponding frequency in each tuple. 2.1 Histograms on probabilistic data
Without loss of generality, in both models, we consider the items are drawn from the integer domain [ n ] = { 1 ,...,n } and use W to represent the set of all possible worlds. Let N be the size of a probabilistic database, i.e., N = |  X  | .
For an item i  X  [ n ], g i is a random variable for the distri-bution of i  X  X  frequency over all possible worlds , i.e, where g i ( W ) is item i  X  X  frequency in a possible world W and Pr( W ) is the possibility of W .
 Example 1 Consider an ordered domain [ n ] with three items { 1 , 2 , 3 } for both models, i.e., n = 3.

The input  X  = { X  (1 , 1 2 ) , (3 , 1 3 )  X  ,  X  (2 , 1 4 ) , (3 , model defines eight possible worlds:
The input  X  = { X  1 : (1 , 1 2 )  X  ,  X  2 : (1 , 1 3 )  X  ,  X  3 : ((1 , in the value model defines eight possible worlds:
Consider the tuple model example from above and denote the eight possible worlds (from left to right) as W 1 ,..., W It X  X  easy to see that g 3 ( W ) = 1 for W  X  { W 4 ,W g ( W ) = 2 for W  X  { W 8 } and g 3 ( W ) = 0 on the rest. Thus, the frequency random variable g 3 of item 3 is g 3 Meanwhile, it X  X  also easy to see g 3 = { (1 , 1 2 ) , (2 , W in the value model example from above.
 Definition 3 A B-bucket representation partitions domain [ n ] into B non-overlapping consecutive buckets ( s k ,e k  X  [1 ,B ], where s 1 = 1, e B = n and s k +1 = e k + 1. Fre-quencies within each bucket b k are approximated by a single representative b b k and we represent it as b k = ( s k ,e
The B -bucket histogram achieving the minimal SSE error for approximating a deterministic data distribution is known as the V-optimal histogram [14]. It can be found using a dy-namic programming formulation in O ( Bn 2 ) time [15], where n is the domain size of the underlying data distribution. We denote this method from [15] as the OptVHist method.
To extend histogram definitions to probabilistic data, we first consider a single possible world W  X  W for a prob-abilistic data set D , where W is a deterministic data set . Hence, the frequency vector of W is given by G ( W ) = { g 1 ( W ) ,...,g n ( W ) } (recall that g i ( W ) is item i  X  X  frequency in W ). Given a B -bucket representation for approximat-ing G ( W ), the SSE of a bucket b k in the world W is given as: SSE ( b k ,W ) = P e k j = s B -bucket representation in W is simply P B k =1 SSE ( b k
Cormode and Garofalakis have extended B -bucket his-togram to probabilistic data [6, 7] by asking for the minimal expected SSE. Formally, Definition 4 Given the (uncertain) frequency sequence of random variables { g 1 ,...,g n } as defined in (1), the problem seeks to construct a B -bucket representation (typically B n ) such that the expected SSE over all possible worlds is minimized, i.e., the histogram with the value given by:
In (2), the expectation of the sum of bucket errors is equal to the sum of expectations of bucket errors [6, 7], i.e.,
Consequently, the optimal histogram could be derived by a dynamic programming formulation as follows: where H ( i,j ) represents the minimal error from the optimal j -buckets histogram on interval [1 ,i ]; min minimal bucket error for the bucket spanning the interval [ ` + 1 ,i ] using a single representative value b b .
Previous work [6, 7] showed that the cost of the optimal histogram is O ( Bn 2 ) and min in constant time using several precomputed prefix-sum ar-rays which we will describe in the following subsection. We dub this state-of-art method from [7] the OptHist method . 2.2 Efficient computation of bucket error
Cormode and Garofalakis [7] show that, for SSE, the min-imal error of a bucket b = ( s,e, b b ) is achieved by setting the representative b b = 1 e  X  s +1 E W P e i = s g i . The corresponding bucket error is given by:
In order to answer the min ( s,e ) values in constant time, prefix-sum arrays of E W and E W [ g i ] in equation (5) are precomputed as follows (de-tails can be found in [7]): tuple model: E W [ g i ] = P t P t j  X   X  Pr[ t j = i ](1  X  Pr[ t j = i ]). value model: E W [ g i ] = P v = P v
Set A [0] = B [0] = 0, then the minimal SSE min for both models is computed as:
In both models, in addition to the O ( Bn 2 ) cost as shown in last subsection, it also takes O ( N ) cost to compute the A,B arrays ( N = |  X  | , number of probabilistic tuples).
The state-of-the-art OptHist method from [7] is clearly not scalable, when given larger domain size.
 A baseline method. A natural choice is to consider com-puting a B -bucket histogram for the expected frequencies of all items . Note that this histogram is not the same as the desired histogram as defined in equation (2) and (3) since in general E[ f ( X )] does not equal f (E[ X ]) for arbitrary func-tion f and random variable X .

However, we can show in our histogram, the SSE error of a bucket [ s,e ] using b b as its representative is:
On the other hand, if we build a B -bucket histogram over the expected frequencies of all items, the error of a bucket [ s,e ] using  X  b as its representative is:
When using the same bucket configurations (i.e., the same boundaries and b b =  X  b for every bucket), the two histograms above differ by P e j = s (E W [ g 2 i ]  X  (E W [ g i ]) 2 on a bucket [ s,e ]. Hence, the overall errors of the two his-tograms differ by P i  X  [ n ] Var W [ g i ] which is a constant. Given this and computing the expected frequencies of all items can be done in O ( N ) time, computing the optimal B -bucket his-togram for them (now a deterministic frequency vector) still requires the OptVHist method from [15], taking O ( Bn for a domain of size n , which still suffers the same scalabil-ity issue.
 A natural choice is then to use an approximation for the B -bucket histogram on expected frequencies (essentially a V-optimal histogram), as an approximation for our histogram. The best approximation for a V-optimal histogram is an (1 +  X  )-approximation [23] (in fact, to the best of our knowl-edge, it is the only method with theoretical bound on ap-proximation quality). But when using approximations, one cannot guarantee that the same bucket configurations will yield the same approximation bound with respect to both histograms. So its theoretical guarantee is no longer valid with respect to our histogram. Nevertheless, it is worth comparing to this approach as a baseline method, which is denoted as the EF-Histogram method.
Hence, we search for novel approximations that can pro-vide error guarantees on the approximation quality and also offer quality-efficiency tradeoff, for the histograms from [6,7] as defined in (2). To that end, we propose a constant approx-imation scheme, Pmerge , by leveraging a  X  X artition-merge X  principle. It has a partition phase and a merge phase . Partition. The partition phase partitions the domain [ n ] into m equally-sized sub-domains, [ s 1 ,e 1 ] ,..., [ s m s 1 = 1 ,e m = n and s k +1 = e k + 1. For the k th sub-domain [ s k ,e k ], we compute the A,B arrays on this domain as A k ,B k for k  X  [1 ,m ]. A k and B k are computed using [ s ,e k ] as an input domain and equation (6) for the value and the tuple models respectively,
Next, for each sub-domain [ s k ,e k ] ( k  X  [1 ,m ]), we apply the OptHist method from [7] (as reviewed in Section 2.1) over the A k ,B k arrays to find the local optimal B-buckets histogram for the k th sub-domain. The partition phase pro-duces m local optimal B -bucket histograms, which lead to mB buckets in total.
 Merge. The goal of the merge phase is to merge the mB buckets from the partition phase into optimal B buckets in terms of the SSE error using one merging step. To solve this problem, naively, we can view an input bucket b = ( s,e, b as having ( e  X  s + 1) items with identical frequency value b b . Then, our problem reduces to precisely constructing an V-optimal histogram instance [15]. But the cost will be O ( B ( P mB i =1 ( e i  X  s i + 1)) 2 ) using the OptVHist method, which is simply O ( Bn 2 ).

A critical observation is that a bucket b = ( s,e, b b ) can also be viewed as a single weighted frequency b b with a weight of ( e  X  s + 1), such that we can effectively reduce the domain size while maintaining the same semantics. Formally, let Y = mB . A weighted frequency vector { f 1 ,f 2 , ...,f on an ordered domain [ Y ] has a weight w i for each f i . It implies w i items with a frequency f i at i . The weighted version of the V-optimal histogram seeks to construct a B -bucket histogram such that the SSE between these buckets and the input weighted frequency vector is minimized. This problem is the same as finding: where s 1 = 1 and e B = Y . The optimal B buckets can be derived by a similar dynamic programming formulation as that shown in equation (4). The main challenge is to compute the optimal one-bucket min [ s,e ] now in the weighted case. We show in Appendix A how to do this efficiently using several prefix sum arrays.
Thus the weighted optimal B -bucket histogram can be derived by filling a Y  X  B matrix, and each cell ( i,j ) takes O ( Y ) time. Thus, the weighted B -bucket histogram is com-puted in O ( BY 2 ) = O ( m 2 B 3 ) time, which is much less than O ( Bn 2 ) since both B and m are much smaller than n . An example. An example of Pmerge is given in Figure 1, where n = 16, B = 2, and m = 4. To ensure clarity, we show only two possible worlds W 1 (blue circle) and W 2 (green tri-angle) from the set of possible worlds W of this database. In the partition phase, each sub-domain of size 4 is approx-imated by 2 local optimal buckets. In total, the partition phase has produced 8 buckets in Figure 1. In the merge phase, each input bucket maps to a weighted frequency as discussed above. For example, the first bucket covering fre-quencies in [1 , 2] represents a weighted frequency of 1 . 8 with weight 2. These 8 buckets were merged into two buckets as the final output.
 Complexity analysis. In the partition phase, it takes lin-ear time to compute the corresponding A k ,B k arrays within each sub-domain [ s k ,e k ] for k  X  [1 ,m ], following the results from [7]. The size of sub-domain [ s k ,e k ] is roughly n/m for k  X  [1 ,m ]. It takes O ( Bn 2 /m 2 ) to run the OptHist method on A k ,B k to find the k th local optimal B -bucket histogram. Next, the merge phase takes only O ( B 3 m 2 ) time as analyzed above. Hence, Lemma 1 Pmerge takes O ( N + Bn 2 /m + B 3 m 2 ) .
In order to evaluate the absolute value of the histogram approximation error, we adopt the ` 2 distance (square root of SSE error) between the data distribution and the his-togram synopsis. Next, we show the approximation quality of Pmerge compared to the optimal B -bucket histogram found by OptHist in terms of the ` 2 distance. Theorem 1 Let kH ( n,B ) k 2 and kH Pmerge ( n,B ) k 2 be the ` norm of the SSE error of B -bucket histogram produced by OptHist and Pmerge respectively on domain [ n ] . Then, kH Pmerge ( n,B ) k 2 &lt; 3 . 17  X kH ( n,B ) k 2 .
Note that the problem size of mB in the merge phase of Pmerge may still be too large to be handled efficiently by a DP method. Fortunately, we can further improve the efficiency by doing  X  X ecursive merging X  as follows.
First of all, the partition phase will partition the input domain into m ` equal-sized sub-domains, instead of only m sub-domains, for some integer ` (user specified).

The merge phase now recursively merges the m ` B buckets from the partition phase into B buckets using ` iterations. Each iteration reduce the number of input buckets by a fac-tor of m by applying a sequence of merging steps . Specif-ically, each merging step merges mB consecutive buckets (from left to right) from the current iteration into B buck-ets in the next iteration, which is done using the same merg-ing step from the standard Pmerge method (i.e., using the weighted B -bucket histogram idea). We dub the recursive Pmerge methods RPmerge .

Extending the analysis from Lemma 1 and Theorem 1 gives the following result, w.r.t the ` 2 norm of the SSE: the RPmerge method gives a 3 . 17 ` approximation of the optimal B -bucket histogram found by OptHist .

It is important to note that the approximation bounds in both Theorems 1 and 2 reflect the worst-case analysis . The extreme cases leading to the worst-case bounds are almost impossible in real data sets. In practice, Pmerge and its recursive version RPmerge always provide (very) close to optimal approximation quality (much better than what these worst-case bounds indicate), as shown in our experiments.
Pmerge allows efficient execution in a distributed and parallel framework. In the partition phase, each sub-domain can be handled independently in parallel.

The recursive Pmerge offers even more venues for paral-lelism. In this case, its merge phase can also run in a dis-tributed and parallel fashion, since each merging step from every iteration can be processed independently.
 Next, we X  X l address the challenge on computing the local A ,B k arrays efficiently for each sub-domain [ s k ,e k ] in a dis-tributed and parallel setting . For both models, we assume that the underlying probabilistic database has been split into  X  chunks {  X  1 ,..., X   X  } and stored in a distributed file system (DFS). It is important to note that the input data is not necessarily sorted by the values of the items when stored into chunks in a DFS.
Recall that in the value model, f i is a pdf describing item i  X  X  possible frequency values and their associated probabili-ties. We first show that: Lemma 2 In the value model, Pr[ g i = v ] = Pr[ f i = v ] for any frequency value v  X  V ( V is the domain of all possible frequency values).
 Lemma 2 and equation (6) imply that: Lemma 3 The A,B arrays for the value model also equal: A [ j ] = P j i =1 E[ f 2 i ] , B [ j ] = P j i =1 E[ f i
Without loss of generality, we assume  X   X  X ata nodes (aka processes) X  to consume the input data chunks, and also m  X  X ggregate nodes/processes X  to produce the local optimal B -bucket histograms. Each data chunk is processed by one data node in parallel. Each data node produces m parti-tions, each of which corresponds to a sub-domain of size (roughly) n/m , using a partition function h : [ n ]  X  [ m ], h ( i ) = ( d i/ d n/m ee ).

The ` th data node processing chunk  X  ` reads in tuples in  X  in a streaming fashion. For each incoming tuple ( i,f i ) found h ( i )th aggregate node will collect the h ( i )th partitions from all  X  data nodes, the union of which forms the h ( i )th sub-domain of the entire data.

Thus, the k th ( k  X  [1 ,m ]) aggregate node will have all the key-value pairs ( i, (E[ f i ] , E[ f 2 i ])) for all i  X  [ s k th sub-domain, if item i exists in the database; otherwise it simply produces a ( i, (0 , 0)) pair for such i  X  [ s k That said, the k th aggregate node can easily compute the A ,B k arrays for the k th sub-domain using Lemma 3. It then uses the OptHist method on A k ,B k to produce the k th local optimal B -bucket histogram. Clearly, all m aggregate nodes can run independently in parallel.
In the tuple model, the tuples needed to compute Var W [ g and E W [ g i ] for each item i are distributed over  X  tuple chunks. Hence, we rewrite equation (6) for computing A,B arrays in the tuple model as follows: Lemma 4 The A,B arrays in the tuple model can also be computed as: E
A similar procedure as that described for the value model could then be applied. The difference is that the ` th data node processing chunk  X  ` emits a key-value pair ( i, (E W ,` Var W ,` [ g i ])) instead, for each distinct item i from the union of all possible choices of all tuples in  X  ` . Thus, the k th aggregate node will reconstruct A k ,B k arrays according to Lemma (4) and then use the OptHist method on A k ,B k arrays to produce the local optimal B -bucket histogram for the k th sub-domain in the partition phase.
For RPmerge , we carry out the partition phase for each model using the method from Section 4.1 and Section 4.2 respectively. In the merge phase, we can easily invoke mul-tiple independent nodes/processes to run all merging steps in one iteration in parallel. In the following, we denote the distributed and parallel Pmerge and RPmerge methods as parallel-Pmerge and parallel-RPmerge respectively.
A paramount concern in distributed computation is the communication cost. The parallel-Pmerge method may in-cur high communication cost for large domain size.
This cost is O ( n ) in the value model. Given a set  X  of tuples in a value model database with size N = |  X  | ;  X  is stored in  X  distributed chunks in a DFS. Each tuple will produce a key-value pair to be emitted by one of the data nodes. In the worst case N = n (one tuple for each item of the domain), thus O ( n ) cost. On the other hand, this cost is O (  X n ) in the tuple mode. The worst case is when possible choices from all tuples in every distributed tuple chunk have covered all distinct items from the domain [ n ].

There are only O ( Bm ) bytes communicated in the merge phase of parallel-Pmerge for both models, where every ag-gregate node sends B buckets to a single node for merging. Thus, the communication cost of parallel-Pmerge is domi-nated by the partition phase.

We present novel synopsis to address this issue. The key idea is to approximate the A k ,B k arrays at the k th aggre-gate node ( k  X  [1 ,m ]) with unbiased estimators b A k , structed by either samples or sketches sent from the data nodes. Since parallel-Pmerge and parallel-RPmerge share the same partition phase, hence, the analysis above and the synopsis methods below apply to both methods . The VS method. One way of interpreting E[ f 2 i ] and E[ f is treating each of them as an count of item i in the arrays A k and B k respectively. Then A k [ j ] and B k [ j ] in Lemma 3 can be interpreted as the rank of j , i.e. the number of appearance of items from [ s k ,e k ] that are less than or equal to j in array A k ,B k respectively. Using this view, we show how to construct an estimator b B k [ j ] with the value model sampling method VS . The construction and results of b A k are similar.

Considering the ` th data node that processes the ` th tuple respectively if ( i,f i )  X   X  ` ; otherwise we assign them as 0. We then define A k,` ,B k,` as follows, for every k  X  [1 ,m ]:
Using  X  ` , the ` th data node can easily compute A k,` ,B locally for all k and j values. It X  X  easy to get the following results at the k th aggregate node for any j  X  [ s k ,e k
We view B k,` [ j ] as the local rank of j from  X  ` at the ` th data node. By (7), B k [ j ] is simply the global rank of j that equals the sum of all local ranks from  X  nodes. We also let M
For every tuple ( i,f i ) from  X  ` , data node ` unfolds (con-ceptually) E[ f i ] copies of i , and samples each i independently with probability p = min {  X ( copy of i is sampled, it is added to a sample set S k,` where k = h ( i ), using the hash function in Section 4.1. If c of i are sampled, we add ( i, 1) ,..., ( i,c i ) into S k,` of values in S k,` are sorted by the item values from the first term, and ties are broken by the second term. Data node ` sends S k,` to the k th aggregate node for k  X  [1 ,m ].
We define the rank of a pair ( i,x ) in S k,` as the number of pairs ahead of it in S k,` , denoted as r (( i,x )). For any j  X  [ s k ,e k ] and `  X  [1 , X  ], aggregate node k computes an estimator b B k,` [ j ] for the local rank B k,` [ j ] as: r (( j,c j )) /p + 1 /p , if item j is present in S k,` .
If an item j  X  [ s k ,e k ] is not in S k,` , let y be the predecessor of j in S k,` in terms of item values , then b B k,` [ j ] = 1 /p . If no predecessor exists, then b B k,` [ j ] = 0.
It then estimates the global rank B k [ j ] for j  X  [ s k Lemma 5 b B k [ j ] in (8) is an unbiased estimator of B and Var[ b B k [ e ]] is O ((  X M k ) 2 ) .
 The communication cost is P `,j p = O (min { for `  X  [1 , X  ] and j  X  [ s k ,e k ] for aggregate node k in the worst case. Hence, the total communication cost in the par-tition phase of Pmerge with VS is O (min { m Note that { M 1 ,...,M m } can be easily precomputed in O ( m X  ) communication cost. The TS (tuple model sketching) method. Observe that we can rewrite equations in Lemma 4 to get: local rank of j in a separate local array computed from  X  Similarly, estimation of the global rank, i.e., the first term of A [ j ] and B k [ j ] in (9), can be addressed by the VS method.
The challenge is to approximate P j i = s ond term of A k [ j ] in (9). It is the second frequency moment ( F 2 ) of { E W [ g s k ] ,..., E W [ g j ] } . Given that each E distributed sum and j varies over [ s k ,e k ], we actually need a distributed method to answer a dynamic F 2 (energy) range query approximately on a sub-domain [ s k ,e k ].

The key idea is to build AMS sketches [17] for a set of intervals from a carefully constructed binary decomposition on each sub-domain locally at every data node.
 For a sub-domain [ s k ,e k ] at the k th aggregate node, let M k = P composition partitions [ s k ,e k ] into 1 / X  intervals, where each interval X  X  F 2 equals  X M 00 k . An index-level (recursively) con-catenates every two consecutive intervals from the level be-low to form a new interval (thus, the height of this binary decomposition is O (log d 1  X  e ). Figure 2(a) illustrates this idea. (a) binary decomposition (b)localQ-AMS Figure 2: Binary decomposition and local Q-AMS.

Once the ( 1  X   X  1) partition boundaries {  X  k, 1 ,..., X  k, at the leaf-level were found, aggregate node k sends them to all  X  data nodes. Each data node builds a set of AMS sketches, one for each interval from the binary decomposi-tion (of all levels), over its local data. We denote it as the local Q-AMS sketch (Queryable-AMS).
In other words, data node ` builds these AMS sketches Then data node ` sends its Q-AMS sketch for [ s k ,e k ] to the k th aggregate node, which combines  X  local Q-AMS sketches into a global Q-AMS sketch for the k th sub-domain [ s k ,e leveraging on the linearly-mergeable property of each indi-vidual AMS sketch [12, 17]. The global Q-AMS sketch is equivalent to a Q-AMS sketch that is built from { E W [ g E W [ g e k ] } directly; recall that E W [ g i ] = P  X  ` =1
For a query range ( s,e ), s k  X  s &lt; e  X  e k , we find the intervals that form the canonical cover of ( s,e ) in the global Q-AMS sketch, and approximate (E W [ g s ]) 2 +  X  X  X  +(E W by the summation of the F 2 approximations (from the AMS sketches) of these intervals. If ( s,e ) is not properly aligned with an interval at the leaf-level of an Q-AMS sketch, we snap s and/or e to the nearest interval end point.

The error from the snapping operation in the leaf-level is at most O (  X M 00 k ). By the property of the AMS sketch [17], the approximation error of any AMS sketch in the global Q-AMS sketch is at most O (  X F 2 ( I )), with at least probability (1  X   X  ), for an interval I covered by that AMS sketch. Also F ( I )  X  M 00 k for any I in the global Q-AMS sketch. Fur-thermore, there are at most O (log 1  X  ) intervals in a canon-ical cover since the height of the tree in Q-AMS is log d Hence, the approximation error for any range F 2 query in the global Q-AMS sketch is O (  X M 00 k log 1  X  ) with probability at least (1  X   X  ), for  X , X   X  (0 , 1) used in the construction of the Q-AMS sketch. Finally, the size of an AMS sketch is O ( 1  X  2 log 1  X  ) [12, 17]. Thus, we can show that: Lemma 6 Given the partition boundaries {  X  k, 1 ,..., X  k, for a sub-domain [ s k ,e k ] , for any s,e such that s e  X  e k , Q-AMS can approximate (E W [ g s ]) 2 + (E W [ g  X  X  X  +(E W [ g e ]) 2 within an additive error of O (  X M 00 probability  X  (1  X   X  ) using space of O ( 1  X  3 log 1  X  ) . Communication cost and partition boundaries. Each aggregate node needs to send ( 1  X   X  1) values per sub-domain to all  X  data nodes, and there are m sub-domains in total. So the communication cost of this step is O ( m X / X  ). Then, each data node needs to send out m local Q-AMS sketches, one for each sub-domain. The communication cost of this step is O ( m X   X  3 log 1  X  ). Hence, the total communication is O ( log 1  X  ), which caters for the worst-case analysis.
But the above method and analysis depend on the calcu-lation of the partition boundaries {  X  k, 1 ,..., X  k, 1 sub-domain [ s k ,e k ], for k  X  [1 ,m ]. To calculate this exactly we need { E W [ g s k ] ,..., E W [ g e k ] } at the k th aggregate node, which obviously are not available (unless using O ( n X  ) total communication for  X  data nodes for all sub-domains, which defeats our purpose). Fortunately, given that VS can esti-mate each B k [ j ] with an  X  error efficiently, each E W be estimated as ( b B k [ i ]  X  b B k [ i  X  1]) By (9). We implemented all methods in Java. We test OptHist , EF-Histogram, Pmerge and RPmerge methods in central-ized environment without parallelism, and parallel-Pmerge and parallel-RPmerge methods (with and without synop-sis) in distributed and parallel settings. The centralized ex-periments were executed over a Linux machine running a single Intel i7 3 . 2GHz cpu, with 6GB of memory and 1TB disk space. We then used MapReduce as the distributed and parallel programming framework and tested all methods in a Hadoop cluster with 17 machines (of the above configu-ration) running Hadoop 1 . 0 . 3. The default HDFS (Hadoop distributed file system) chunk size is 64MB.
 Datasets. We executed our experiments using the World-Cup data set and the SAMOS data set. The WorldCup data set is the access logs of 92 days from the 1998 World Cup servers, composed of 1.35 billion records. Each record con-sists of client id, file type and time of access etc. We choose the client id as the item domain, which has a maximum pos-sible domain size of 2 , 769 , 184. We vary the domain size of client ids from 10 , 000 up to 1 , 000 , 000. Records in the entire access log are divided into continuous but disjoint groups, in terms of access time. We generate a discrete frequency distribution pdf for items within each grouping interval and assign the pdf to a tuple in the tuple model . For the value model , we derive a discrete pdf for each client id based on its frequency distribution in the whole log with respect to 13 distinct requested file types and assign the pdf to the tuple with that client id in the value model. The SAMOS data set is composed of 11.8 million records of various atmospheric measurements from a research vessel and we care about the temperature field, which has a domain size of about 10,000 (by counting two digits after the decimal point of a fraction reading). In a similar way, we form the tuple model and value model data on the SAMOS data.
 Setup. The default data set is WorldCup. To accommo-date the limited scalability of OptHist , we initially vary the value of n from 10 , 000 up to 200 , 000 and test the effects of different parameters. The default values of parameters are B = 400 and n = 100 , 000. For RPmerge , the recursion depth is ` = 2. We set m = 16 and m = 6 as the default values for Pmerge and RPmerge respectively. We then ex-plore the scalability of our methods, up to a domain size of n = 1 , 000 , 000. The running time of all methods are only linearly dependent on N , number of tuples in a database. Hence, we did not show the effect of N ; all reported running time are already start-to-end wall-clock time.

In each experiment, unless otherwise specified, we vary the value of one parameter, while using the default values of other parameters. The approximation ratios of our approx-imate methods were calculated with respect to the optimal B -buckets histogram produced by OptHist [6, 7]. Effect of m . Figure 4 shows the running time and approx-imation ratio when we vary m from 4 to 20 on the tuple model data sets. Recall that Pmerge will produce m sub-domains, while RPmerge will produce m ` sub-domains, in the partition phase. Hence, RPmerge gives the same num-ber of sub-domains using a (much) smaller m value. For both methods, a larger m value will reduce the size of each (a) Tuple model: running time. (c) Tuple model: approx. ratio. (a) Tuple model: running time. (c) Tuple model: approx. ratio. sub-domain, hence, reducing the runtime of the OptHist method on each sub-domain and the overall cost of the par-tition phase. But a larger m value increases the cost of the merge phase. As a result, we expect to see a sweet point of the overall running time across all m values. Figure 4(a) reflects exactly this trend and the same trend holds on the value model data set as well. They consistently show that m = 16 and m = 6 provide the best running time for Pmerge and RPmerge respectively. Note that this sweet point can be analytically analyzed, by taking derivative of the cost function (partition phase + merge phase) with re-spect to m .

Figure 4(b) shows their approximation ratios on the tuple model data set. The approximation quality of both meth-ods fluctuates slightly with respect to m ; but they both pro-duce B -buckets histograms of extremely high quality with approximation ratio very close to 1. The quality is much bet-ter than their worst-case theoretical bounds, as indicated by Theorems 1 and 2 respectively.

The results of varying m from the value model are very similarly, and have been omitted for brevity. Also, we have investigated the results of varying the recursive depth ` from 1 to 3. They consistently show that ` = 2 achieves a nice balance between running time and approximation quality. For brevity, we ommited the detailed results.
 Effect of n . Figure 5 shows the results with respect to n on both value and tuple models. In both models, the running time of OptHist increases quadratically with respect to n . In contrast, both Pmerge and RPmerge are much more scalable, and have outperformed OptHist by at least one to two orders of magnitude in all cases. For example, in Figure 5(b), when n = 100 , 000, OptHist took nearly 14 hours while RPmerge took only 861 seconds. RPmerge further improves the running time of Pmerge by about 2-3 times and is the most efficient method.

Meanwhile, both Pmerge and RPmerge achieve close to 1 approximation ratios across all n values in Figures 5(c) and Figure 5(d). The approximation quality gets better (ap-proaching optimal) as n increases on both models.
 Effect of B . We vary the number of buckets from 100 to 800 in Figure 6. Clearly, RPmerge outperforms OptHist by two orders of magnitude in running time in both models, as see in Figures 6(a) and 6(b). Figures 6(c) and 6(d) show the approximation ratios in each model respectively. The approximation ratio of both Pmerge and RPmerge slightly increases when B increases on both models. Nevertheless, the quality of both methods are still excellent, remaining very close to the optimal results in all cases. (a) Running time: WorldCup. (c) Running time: SAMOS. Figure 7: Comparison against the baseline method.
 Comparison with the baseline. Lastly, we compare the running time and approximation ratios of our meth-ods against the baseline EF-Histogram method (with  X  = 0 . 1 , X  = 0 . 05 and  X  = 0 . 01 respectively) on two data sets. Our methods used their default parameter values on the WorldCup data set. For the SAMOS data set, we set n = 10 , 000 and B = 100. Clearly, small  X  values does help im-prove the approximation quality of EF-Histogram as shown in Figure 7(b) and Figure 7(d). But our methods have pro-vided almost the same approximation quality on both data sets, while offering worst-case bounds in theory as well. Note that EF-Histogram only provides the (1 +  X  ) approximation bound with respect to the B -buckets histogram on expected frequencies, but not on the probabilistic histograms.
Meanwhile, the running time of EF-Histogram increases significantly (it is actually quadratic to the inverse of  X  value, (c) Tuple model: vary B . i.e., 1 / X  2 ), especially on the much larger WorldCup data set. In all cases our best centralized method, RPmerge , has sig-nificantly outperformed the EF-Histogram as shown in Fig-ure 7(a) and Figure 7(c). Furthermore, the distributed and parallel fashion of Pmerge and RPmerge further improves the efficiency of these methods, as shown next. Effect of size of the cluster. Figure 9 shows the run-ning time of different methods when we vary the number of slave nodes in the cluster from 4 to 16. For reference, we have included the running time of centralized Pmerge and RPmerge . We can see a (nearly) linear dependency between the running time and the number of slave nodes for both parallel-Pmerge and parallel-RPmerge methods. The speed up for both methods is not as much as the increas-ing factor of the number of slave nodes used. The reason is that Hadoop always includes some extra overhead such as job launching and tasks shuffling and IO cost of intermediate HDFS files, which reduces the overall gain from parallelism. Scalability. Next, we investigate the scalability of RP-merge (the best centralized method), parallel-Pmerge and parallel-RPmerge on very large probabilistic data sets. We used all 16 slave nodes in the cluster, and varied either the values of n from 200,000 to 1000,000 when B = 400, or the values of B from 100 to 800 when n = 600 , 000. We omit OptHist and Pmerge methods in this study, since they are too expensive compared to these methods.
 Figures 8(a) and 8(b) show that with recursive merging RPmerge can even outperform parallel-Pmerge as n in-creases. But clearly parallel-RPmerge is the best method and improves the running time of RPmerge by 8 times on the value model and 4 times on the tuple model when n = 1000 , 000. It becomes an order of magnitude faster than parallel-Pmerge in both models when n increases.
Figures 8(c) and 8(d) show the running time when we vary B and fix n = 600 , 000. Running time of all methods increase with larger B values. This is because large B values increase the computation cost of the merging step, especially for recursive Pmerge . Nevertheless, parallel-RPmerge sig-nificantly outperforms both parallel-Pmerge and RPmerge in all cases on both models.
Lastly, we study the communication saving and approx-imation quality of parallel-Pmerge and parallel-RPmerge with synopsis. The default values are n = 600 , 000, B = 400 and  X  = 0 . 002 for VS and  X  = 0 . 1 for TS . We have omit-ted the results for the running time of Parallel-Pmerge and Parallel-RPmerge with synopsis, since they are very close to that of Parallel-Pmerge and Parallel-RPmerge re-spectively (since the running time of all these methods are dominated by solving the DP instances in the partition and merging phases). Comparing effects of synopsis in both models. Here we use parallel-PmergeS (parallel-RPmergeS ) to denote a parallel-Pmerge (parallel-RPmerge ) method with a synop-sis in either model. In value model, the synopsis is VS ; and in tuple model, the synopsis is TS .

Figure 10(a) shows that parallel-PmergeS outperforms parallel-Pmerge and parallel-RPmerge by more than an order of magnitude in communication cost for both mod-els. Parallel-RPmergeS has much higher communication cost than parallel-PmergeS since the sampling cost in the partition phase has increased by an order of m using m 2 sub-domains (when ` = 2). Nevertheless, it still saves about 2-3 times of communication cost compared to that of parallel-Pmerge and parallel-RPmerge for both models.
 Figure 10(b) shows that parallel-PmergeS and parallel-RPmergeS have excellent approximation quality on the value model (very close to optimal histograms). They give less op-timal approximations in the tuple model, since Q-AMS in the TS method has higher variances in its estimated A,B arrays in the tuple model, compared to the estimations on A,B arrays given by VS in the value model.
 Remarks. The communication cost of all of our synopsis methods are independent of n , whereas the communication cost of both parallel-Pmerge and parallel-RPmerge are lin-early dependent on n , as shown from our analysis in Section 5. This means the synopsis methods introduce even more savings when domain size increases.
We have reviewed the most relevant related work in Sec-tion 2. That said, extensive efforts were devoted to con-structing histograms in deterministic data, motivated by the early work in [14 X 16,19]. An extensive survey for histograms on deterministic data is in [13]. There are also numerous efforts on modeling, querying, and mining uncertain data; see [1, 3, 21, 24]. A good histogram for large probabilistic data is very useful for many such operations, e.g, finding frequent items, patterns, and itemsets [1, 3, 24, 26].
However, little is known about histograms over proba-bilistic data till three recent studies [5 X 7]. Cormode and Garofalakis have extended the bucket-based histogram and the wavelet histogram to probabilistic data by seeking to minimize the expectation of bucket errors over all possible worlds [6, 7]. The details of which can be found in Section 2. Cormode and Deligiannakis then extend the probabilistic histogram definition to allowing bucket with a pdf repre-sentation rather than a single constant value [5]. A main limitation of these studies is the lack of scalability, when the domain size of the probabilistic data increases.

Allowing some approximations in histogram construction is also an important subject on deterministic data, e.g., [11, 22, 23] and many others. One possible choice is to run these methods on expected frequencies of all items, and sim-ply use the output as an approximation to our histogram. But the theoretical approximation bound with respect to the deterministic data (in our case, the expected frequencies of all items) does not carry over to probabilistic histogram def-inition with respect to n random variables (frequency distri-butions of every item i ). To the best of our knowledge, the (1+  X  ) approximation from [23] is the best method with theo-retical guarantees for histograms over deterministic data (in fact, to the best of our knowledge, other methods are mostly heuristic-based approaches). We did explore this approach as a baseline method in our study.
This paper designed novel approximation methods for con-structing optimal histograms on large probabilistic data. Our approximations run much faster and have much bet-ter scalability than the state-of-the-art. The quality of the approximate histograms are almost as good as the optimal histograms in practice. We also introduced novel techniques to extend our methods to distributed and parallel settings, which further improve the scalability. Interesting future work include but not limited to how to extend our study to probabilistic histograms with pdf bucket representatives [5] and how to handle histograms of other error metrics. Mingwang Tang and Feifei Li were supported in part by NSF Grants IIS-1251019 and IIS-1200792. Fast computation of bucket Error. We can show that in the weighted case the min b bucket b is as follows: SSE ( b, b b ) = P e j = s w j ( f prefix sum arrays need to be precomputed are:
Given these arrays, min
