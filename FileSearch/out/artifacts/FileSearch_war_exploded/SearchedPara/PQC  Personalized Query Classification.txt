 Query classification (QC) is a task that aims to classify Web queries into topical categories. Since queries are usually short in length and ambiguous, the same query may need to be classified to different categories according to different people X  X  perspectives. In this paper, we propose the Per-sonalized Query Classification (PQC) task and develop an algorithm based on user preference learning as a solution. Users X  preferences that are hidden in clickthrough logs are quite helpful for search engines to improve their understand-ings of users X  queries. We propose to connect query clas-sification with users X  preference learning from clickthrough logs for PQC. To tackle the sparseness problem in click-through logs, we propose a collaborative ranking model to leverage similar users X  information. Experiments on a real world clickthrough log data show that our proposed PQC algorithm can gain significant improvement compared with general QC as well as natural baselines. Our method can be applied to a wide range of applications including personal-ized search and online advertising.
 H.3.3 [ Information Search and Retrieval ]: Information filtering, Search process; H.3.4 [ Systems and Software ]: Performance evaluation,performance measures Algorithms, Experimentation, Performance Personalized Query Classification, Clickthrough log, Collab-orative ranking
With the exponentially increasing amount of information available on the Internet, Web search has become an indis-pensable tool for Web users to gain their desired informa-tion. Typically, Web users submit a short query consisting of a few words to search engines and expect to satisfy their in-formation need. However, because these queries are usually short and ambiguous, how to interpret them is not trivial. This problem has become a major research issue in IR com-munity. In this work, we refer to the problem of classifying queries to a set of topical categories as the query classifica-tion problem, or QC for short. The importance of QC is emphasized by many services provided by Web search. A direct application of QC is to provide better search result pages for users with interests of different categories. Search result pages can be grouped according to the categories pre-dicted by a QC algorithm to give better user experiences. Online advertising services can be enhanced by QC results to promote different products more accurately.

Since Web queries are usually short and ambiguous, one same query may belong to different categories for different people. This difficulty is illustrated in Table 1, which shows the labeling results from three labelers in a query classifi-cation competition (ACM KDDCUP X 05). In the table, the three human labelers, denoted as L1, L2 and L3, respec-tively, gave each query at most five labels in a ranked order. The average precision and F1 score values of each labeler are evaluated against the labeling results of the other two label-ers. The average values among the three labelers are around 0.50, which indicates that the categories of the same queries are different for different people. From this example, we can see that it is difficult for humans and search engines to pre-dict to which categories the query belongs for a particular user.

Due to the difficulty in solving the QC problem, search engines often treat queries as if they belong to all probable popular categories, in order to make the provided services fit different needs. For example, the search results are often reorganized to make returned Web pages diverse in topics (as shown in Figure 1). In online advertising, for example, when a user submits a query  X  X pple X , the advertisements re-lated to different interpretations of the term are returned, such as  X  X Pod X ,  X  X acintosh X  and  X  X pple (fruit) X . In many search services, no matter whether the user is a farmer or a programmer, there is no difference in the types of advertise-ments provided. This will cause many of the search results to be irrelevant and the delivered advertisements not to be targeted.

In order to optimize the quality of search services, it would be desirable to provide personalized query classification ser-vices according to the interests of each individual user. An effective way to improve personalized query understanding is to ask a user to explicitly specify which documents are relevant to the user X  X  information need. However, previous studies have shown that the vast majority of users are re-luctant to provide explicit feedback which can further reveal their preferences [5]. Therefore we need to rely on the im-plicit feedback information such as clickthrough data. Pre-vious studies [20, 16, 15, 22] have shown that it is possible to learn users X  interests over Web pages from clickthrough data for personalized search. However, it is still unclear how to achieve personalized query classification . One difficulty is that how to connect a user X  X  understanding of a query to his clickthrough log. Another difficulty is that clickthrough data for a single user can be rather sparse, resulting in insuf-ficient training data to learn a user X  X  preference for personal-ized query classification. In this paper, we investigate these problems by proposing a unified model that can integrate search queries X  categories and users X  relevance feedbacks col-laboratively. Experimental results on a real search engine X  X  clickthrough log demonstrate that our approach is effective in tackling the personalized query classification problem. Figure 1: An example result page from a commer-cial search engine with query  X  X ffice X . The returned Web pages contain different topics ranging from Mi-crosoft software product Office to TV show and com-mon office products.

The rest of this paper is organized as follows. In Section 2, we will first survey the related work. In Section 3, we will introduce our overall model for PQC as well as the algo-rithm for learning user preferences. Then in Section 4, we Table 1: The score of each labeler evaluated based on scores of the other two labelers as ground truth on the KDDCUP X 05 dataset.
 will present our experimental results on real world datasets. Section 5 concludes this paper and also provides the future work we plan to investigate on this problem.
Query classification (QC) is the task of classifying search queries into categories. It was used as the task of ACM KD-DCUP in 2005 [14], an annual competition hosted by ACM KDD conference. In that year, 37 teams over the world ran their algorithms to classify 800,000 queries and submit-ted their results to compete. Since then, QC has received more and more attention in both academic and industry communities. Most existing QC research focused on find-ing new features associated with search queries to tackle the sparseness problem, or utilizing various Web resources to obtain additional training data. In [17], the winning team of ACM KDDCUP X 05 proposed a novel method to classify queries to target categories without labeled data. With their method, a mapping between an intermediate taxonomy (e.g., Open Directory Project) and the target taxonomy is first constructed. Then the training data from the intermediate taxonomy are extracted to train a classification model for the target taxonomy. In [18], the authors improved their previous method by building a bridging classifier on an in-termediate taxonomy and then use it to map user queries to the target categories via the above intermediate taxonomy. In [2], researchers proposed to find selection preferences be-tween queries and query categories in the query log data to improve query classification. They also trained classifiers for QC with different features and then used these classifiers to vote for final results. To our best knowledge, no previ-ous work has ever addressed the issue of personalized query classification (PQC). Recently, Cao et al. in [4] considered the QC problem enhanced using context information. They use linear chain conditional random field (CRF) model with context-aware features to utilize session information. How-ever, their method only works when there is sufficient session information. They also ignored the long term history for in-dividuals.
Personalized search has become an active research topic recently due to the fact that different people often search for different things with the same query. Sugiyama et al. pro-posed a user profiling method for personalized search [21]. They constructed user profiles with a collaborative filtering model based on a user X  X  browsing history in one day. Lee et al. proposed two types of features for personalized search: features constructed from user-click behavior and from the anchor-link distribution, respectively [13]. Dou et al. pro-posed an evaluation method for personalized search using large scale clickthrough logs, based on which they found that both long-term and short-term contexts are very im-portant in improving search performance for profile-based personalized search algorithms [9]. Tan et al. considered long-term search history for personalized search where a lan-guage model was used [23]. Recently, Chirita et al. pro-posed a personalized query enrichment method by accessing the desktop data of users [6]. In this paper, we address a different aspect of the personalization problem of query understanding, which is more general since many other per-sonalized services, such as query suggestion or advertising, can be constructed based on PQC.
Many efforts have been made to utilize the implicit feed-back information of clickthrough logs to improve search per-formance. Shen et al. in [19] used previous queries and click-through information to reduce the ambiguity in queries to find the most relevant documents. Craswell and Szummer tried to solve the sparseness problem of clickthrough logs by using a random-walk model on a bipartite graph constructed by clicked &lt; query, url &gt; pairs [7]. Similar click-based models are also used in [25]. However, such click-based models may be noisy and biased (position bias) [12], causing suboptimal performance. To reduce the noise and bias in clickthrough log, there are some approaches proposed to leverage extra information such as page dwell time, users X  behaviors after clicks, etc [1]. An alternative way to avoid position bias is to use the preference information contained in user clicks [11]. We notice that, such preference information can be natu-rally represented through a ranking-based model. For this reason, we propose a collaborative ranking model for user preference learning from clickthrough logs.
In this section, we first introduce the overall framework of PQC. Then we present our collaborative ranking model for learning user preferences. Finally, we discuss how to estimate users X  short-term and long-term preferences, as well as the method of combining them to improve PQC.
The overall model can be illustrated by a graphical model, as shown in Figure 2. Assume that a user u wants to search for the information that belongs to a category c . A query q belonging to c is first generated and submitted to a search engine. The search engine may interpret q as being from a set of categories where the returned search result pages may fall into these candidate categories. If we view the query from user u  X  X  perspective, this query should belong to category c and the clicked pages should be generated from c as well. Take the query  X  X pple X  as an example, the returned pages may be from different categories such as  X  X omputer hardware X , X  X S X , X  X P3 player X  X r X  X ruit X . If the clicked pages are about X  X Pod X  X n particular, then it indicates that the user may be searching for MP3 players.

Formally, QC aims to estimate p ( c | q ), namely the condi-tional probability of searching information about the cat-egory c given the query q . PQC targets at the problem of estimating p ( c | q, u ), where the user u is also considered. p ( c | q, u ) is the probability that the query q belongs to the Figure 2: A graphical model representation for PQC. category c for the user u . Using the Bayes formula, The first factor is the prior probability of the category c . The second factor is the probability of generating the query q from the category c . The third factor is the probability of occurrence of the user u conditioned on the category c and the query q . We can simplify this term by assuming that This assumption means that user u has stable interests that are independent of the current query q . Therefore the condi-tion on q is unnecessary. This assumption may hold in both long term and short-term perspectives. For example, from long term perspective, a computer scientist may have sta-ble interests in computer science related information. From short-term perspective, any user aimed at searching certain information may have stable interests in the topics related to that information within the search session. Although there are counterexamples which violate this assumption, hereby we assume such an assumption holds most of the time.
Then the simplified equation becomes This quantity is proportional to the probability that q is generated from c and the probability that u has interest in c .

Estimating p ( q | c ) can be obtained from general QC. We use the method from [17], which has been shown to achieve promising classification accuracy. Then by Bayes rules,
To estimate p ( u | c ), we also apply the Bayes rules by which we convert the problem into estimating the user X  X  preference on categories p ( c | u ). Since the majority of users may be reluctant to provide explicit feedback information on their search preferences [5]. Therefore, it is desirable if the user preferences can be learned automatically from the historical clickthrough logs. We will address this problem in the following sections.
An intuitive idea is that the historical queries submitted by a user can be used to help learn user preferences: The above equation shows that we can treat the problem of estimating user preferences as a query classification prob-lem. The difference from the previous QC work is that we need to classify a group of queries collectively rather than an individual query.

The approach above can be used to learn the short-term user preferences within a short time period, e.g., within a search session. Suppose that a user wants to find some re-sources on X  X achine learning X  X nd submits a query X  X achine learning tools X . The next query may be  X  X eka X . Obviously, user preferences with only a query  X  X eka X  is not clear since the query itself is ambiguous. We can utilize the previous queries to help make better prediction.

However, this method has a major problem for preference learning. The sessions containing the current query may not reflect all search interests of a given user. Also, the method does not utilize the user X  X  click history information. Additionally, the sessions of one user may be too limited to infer user preferences. These reasons provide the motivation for a collaborative ranking model, which we present next, to solve the PQC problem.
Clickthrough data is often quite sparse, meaning that the information for inferring search interests of a particular user may be very limited [9]. Our key observation is that users who clicked the same pages for the same query tend to have similar interpretations for the query. Therefore, a personal-ized query classification algorithm can use the clickthrough data of many similar users to improve the query classifica-tion performance.

Therefore, to estimate p ( c | u ), we introduce a collaborative ranking model to solve the sparseness problem. Consider the above example of the query  X  X pple X . Even if a user who submitted this query did not search for results related to  X  X p3 players X  before, his previous search profile may indi-cate that he is similar to some other users who have queried  X  X p3 players X . In other words, we can exploit the correla-tions between users and their interested categories. Next, we will formalize this intuition using a latent factor model.
A major difficulty faced by click-based models as indi-cated by [12], is that user clicks are biased by the displayed position. Therefore, pairwise preferences are suggested in-stead of click information, which can avoid the problem of position bias. Besides, user preference can be naturally ex-pressed as a ranking relation. For example, we may say that we prefer finding information on  X  X omputers X  more than on  X  X ruits X , but we may find it difficult to attach a numerical score to indicate how much we prefer an individual cate-gory. Therefore, in this section, we propose a collaborative ranking model for user preferences learning.
Figure 3: An example on preferences extraction.
Since search queries are usually too short to specify the exact search intent, a general query classifier may classify a query q to several categories C q = { c 1 , c 2 ,  X  X  X  , c ever, for a particular user u , the query q may only refer to fewer categories. The preferences of the user u may be im-plicitly reflected by u  X  X  clickthrough logs. If the user u only clicked on Web pages of category c i , we can generate the preference c i  X  c j , j 6 = i for user u , which states that c more preferred than c j .

More generally, for each clicked page, we can obtain its generate the preferences by the following formula where  X  and  X  are the set operations of intersection and complement, respectively. An example can be found in Fig-ure 3. In the given example, the query is  X  X pple X  and the related categories returned by QC including  X  X nformation \ Companies X  and  X  X iving \ Food X , etc. Since the user only clicked the Web page about the fruit, we can generate the pairwise preference such as  X  X iving \ Food  X  Information \ Companies X .
To model the pairwise preferences, we adopt the likeli-hood cost function used in [3]. Let r ui be the preference score for the user u and the category i . For the observed pairwise preference such that the category i is preferred to the category j , we use the Bradley-Terry model [3] to define its likelihood.
 where  X  ( x ) = 1 1+ e  X  x . Notation r ui  X  r uj means that the category i is preferred to the category j for the user u . For each user u , we may have a set of pairwise preferences &lt; i, j &gt; u .

Latent factor models are able to uncover the underlying factors that determine the generation of observed data. For example, PLSA [10] models how documents are generated from latent semantic topics. Considering the problem in a probabilistic framework, we model r ui as the joint proba-bility p ( u, c ), which represents the probability of the event that a user u is interested in a category c . The probability will be high if they are both related to some latent factor f , which can be formulated as p ( u, c ) = These latent factors may represent topics or other forms of semantic clusters. We can further use matrix notations to simplify the problem. Due to the equivalence between PLSA and nonnegative matrix factorization [8], the model can be expressed as where R is the matrix of relevance score with rows repre-senting users and columns representing queries. The proba-bilistic perspective helps us to understand the interpretation of the model. However, it would be more convenient to use a matrix factorization based model which is easier to handle from computation perspective.

Unifying the Bradley-Terry model and the matrix factor-ization model, we can obtain the log-likelihood function as where d i  X  j is a vector with element i being 1 and j being  X  1, otherwise being 0. u u is the u th row of the matrix U .
For the matrices U and C , we also assign them prior dis-tributions with the following form where u ij and c ij are the elements of U and C .  X  u and  X  are parameters controlling the confidence of prior. The prior distributions have the same effect as regularization terms in maximum margin learning [24], which can prevent over-fitting. Then, we obtain the posterior distribution as our objective function, l =
Gradient based optimization algorithms can be used to optimize the above objective function. The gradients of the cost function with respect to U and C are shown in Equa-tion (6) and (7).
When  X  x is large, e  X  x may overflow. In order to avoid this issue, the following equivalent equation is used. When | x | is large,  X  0 ( x ) is equal to 0.

There are different gradient-based algorithms available for such a non-constraint optimization problem. In this paper, we use the gradient-descent algorithm to optimize the objec-tive function. For each iteration, the complexity of calculat-ing the gradient is in the order of N , where N is the number of preference pairs in the training data. It is independent of the size of the matrix. Therefore, our algorithm can handle large-scale data with millions of users. After obtaining the preference score matrix, we can normalize the matrix R with respect to each user to get p ( c | u ).
Users typically have long-term preferences as well as short-term preferences. For example, a computer scientist usually submits many queries related to computer science (long-term preference) while he may also submit some queries not related to computer science sometimes (short-term prefer-ence). We show how to convert preferences learning as a query classification problem in Section 3.2. We can use the method with queries in a session to learn the short-term pref-erence. In Section 3.3 we proposed a collaborative ranking model, which can be used for long-term preferences learn-ing. In this section, we will discuss how to combine these two kinds of preferences to do personalized query classifica-tion. A simple, yet effective, method is to directly add the two preference models together as where  X  is an importance weight for short-term preferences. A user session, for example, can be regarded as a kind of short-term preferences. One problem of the session based methods is that they are unable to deal with very short sessions well, since very short sessions do not provide much contextual information. When there are very limited data available, it is hard to infer the real interest of users. This problem can be solved by using the long-term preferences, which is verified in our experimental section.

With the learned user preferences, we can classify the queries into categories from a user X  X  perspective. The classi-fication process can be achieved by using a Bayes rule given in Equation 2.
In this section, we will demonstrate the effectiveness of our proposed PQC method through a series of experiments. We first introduce our experimental setup, including some basic descriptions of the dataset and the evaluation met-ric we will use in our experimental settings. Next we will show the result of our proposed collaborative ranking model on the dataset to demonstrate the effectiveness in learning the user preferences. Finally, we evaluate the learned user preferences through improvement of query classification per-formance.
In order to validate the effectiveness of our proposed so-lution to PQC, we have carried out extensive experiments on a real world clickthrough log dataset obtained from a commercial search engine. This clickthrough log dataset is recorded from November 1, 2007 to November 15, 2007. We randomly sampled 10,000 users for our experiments. There are 22,696 queries and 51,366 urls involved in the dataset. Figure 4 shows the histogram of the number of queries sub-mitted by the users in the log. We found that in the log, there are only 22 users whose historical query lengths are longer than 50. These users are not shown in the figure for better display of the results. In this dataset, most users have fewer than 10 historical queries, which supports our claim that short sessions are the majority from which little contex-tual information can be obtained. The average number of Figure 4: Histogram of number of historical queries for users. historical queries amongst all users is 5.3. This average num-ber of queries is not large since the time-span of the original clickthrough log is not long; users would not issue too many queries during such a short time period. We plan to inves-tigate the experiment on long time-span data in our future work. Although the time-span is not quite long, we will see our proposed method still have significant performance gain compared to the baselines.
Performing evaluation for large-scale PQC results is a non-trivial task, since it is impossible to ask users to label their own queries during their searching behaviors. Therefore, we resort to implicit feedbacks based on the clickthrough logs, to reflect important information about users X  real informa-tion needs. In this section, we propose an unsupervised eval-uation method based on clickthrough logs. The evaluation can be done in an automatic way without manual labeling.
Our basic idea is as follows. We observe that the cate-gories of user-clicked pages reflect the user X  X  real intention after the user submits a query. Thus, PQC results should be consistent with the category label of the clicked pages. As a result, we can check the consistency of the prediction from PQC and user X  X  clicking behavior. For example, if the PQC algorithm classifies a query q for the user u into a category c , but the user X  X  clicks show what he/she wanted is a Web page that is classified into a category c 0 different from c , then the PQC system must have predicted wrongly. This rule can be generalized for consistency measurement, by defining a new metric hit @ k , as follows: where C k q is the top k categories predicted by the algorithm for the query q , while C q , without superscripts, is all the categories the PQC algorithm predicts for the query q . C is the categories obtained from the user X  X  clicked pages p , which is regarded as the ground truth results of PQC. The value of hit @ k will be high if the PQC results are more consistent with the categories of the clickthrough logs.
For the classification labels, we use the categories defined in the ACM KDDCUP X 05 data set 1 . There are a total of 67 categories that form a hierarchy. Table 2 shows several examples of queries and their categories in the test data of ACM KDDCUP X 05 competition. As we can see, for each query, it may belong to more than one category. Thus, as we mentioned before, it is important to disambiguate be-tween these candidate categories and select the  X  X rue labels X  amongst them.
 Table 2: Example Queries and Their Categories Queries Categories FIFA 2006 apple office
In order to obtain the categories of clicked pages, we need to build a classifier to classify Web pages into the same label space as the queries. We crawled the contents of Web pages that have been clicked. Since we do not have labeled data to train the classifier, we use bridging classifier introduced in [18] to construct the classification model.

We import the supervision information from a third party knowledge base to leverage the connection between a clicked page p and the target label C T in the following manner: where C I j is some intermediate category in the third party knowledge base (in our case, Wikipedia is used as the knowl-edge base). We adopt a uniform distribution for the class prior P ( C I j ). The likelihood P ( C T | C I j ) and P ( p | C be estimated with the statistics P ( w i | C I j ) provided by the knowledge base We selected the top 100 relevant articles from Wikipedia to form C I . Since the articles in Wikipedia are organized in a network structure, we added the neighboring articles to the corresponding C I j . We use mutual information as the measure for relevance.

We use the KDDCUP X 05 data set to test our bridging classifier implemented with Wikipedia. The comparison is 9 1 http://www.sigkdd.org/kdd2005/kddcup.html Table 3: Performance on ACM KDDCUP X 05 Test Bed shown in Table 3, and the baseline performance is the win-ning solution of KDDCUP X 05 competition [17]. From the ta-ble, we can observe that our implementation achieved com-parable results.

We split the data into training and test subsets according to the following method. First, we set a threshold value for the length of historical queries. The records for each user within this length are treated as the training data and future queries are treated to be test data for evaluation. For example, suppose that we set the threshold value to 5. All of the first 5 queries that are submitted by a user are used as the training data, while the remaining queries are used as the test data. For users who submitted no more than five queries, we simply used all their queries for training and do not include their queries in the testing phase. The QC algorithm in [17] will assign a query to each category with a probability, which is p ( c | q ). We use the top five categories as the query category list. Since the QC algorithm does not consider the user information, therefore the ranking of the five categories are the same for all users.

For our PQC method, we learn user preferences and calcu-late p ( c | u, q ) to re-rank the top five categories. For both the QC and PQC results, we can evaluate them by the hit @ k metric introduced above. It is easy to see that, the higher the value of hit @ k is, the better the performance of the al-gorithm will be.
Table 4 summarizes our experimental results of PQC and compares it with the basic QC approach, using the hit @ k metric introduced above. From the table, we can observe that the hit @1 value is improved significantly by applying the PQC approach. Recall that the hit @1 value measures whether the top predicted category reflects the user X  X  click-ing behavior. For hit @2 and hit @3 the improvements are also significant. It can also be observed that with the in-crease of k , the amount of improvement we can achieve drops. Such a phenomenon is reasonable, since the more categories we predict, the more possible it would be to in-clude the category of the clicked URL into the predicted category set of the queries. However, since in practical ap-plications, the top ranked category is the most important output, such experimental result indicates personalization is a very promising way to achieve better query classifica-tion performance. We also can compare our proposed PQC model with a memory-based approach that does not use col-laboration. The method is one special case when we choose  X  = 1 in Equation 8.
Figure 5 shows the change of hit @ k when the number of historical queries used for training varies. To conduct this experiment, we use the records that are after the tenth queries of users X  as test data. Then we fix the test data and change the number of previous queries used as training data. Table 4: Comparison between QC and PQC. The QC algorithm that achieves comparable results as the solution of ACM KDDCUP X 05 championship team. The method Mem is a memory-based ap-proach without collaboration.
 Figure 7: Compare collaborative ranking model with collaborative filtering.

As shown in Figure 5, it is reasonable that with more users X  historical queries in our system, our prediction be-comes more accurate. This is a clear trend for hit @1. For hit @ k when k 6 = 1, there seems to be no improvement from a length of six to ten. However, the variance is greatly re-duced. It is also surprising to see that even with only one previous query as training data, we can still significantly im-prove the accuracy of PQC results as compared to the QC result in Table 4. This indicates the effectiveness of using collaboration ranking for PQC.
Figure 6 shows the change of hit @ k when  X  varies. The trend reflects the effect of long-term preferences and short-term preferences. The figure shows the best performance is achieved when both long-term and short-term preferences are considered. For hit @1 and hit @2, we can find that long-term preferences are more useful than short-term pref-erences. This observation validates our previous assumption that clickthrough log in one session is not sufficient to infer users X  preferences.
Understanding the underlying user preferences is a key part for our PQC algorithm. We use an example to show that our PQC algorithm could make more reasonable predic-tions on query categories while taking the user preferences into consideration. Table 5 shows two users who are inter-ested in two different categories of the query  X  X ideo X . The table shows the results produced by both the QC and our Figure 5: Performance with different length of training queries. PQC approaches. It can be seen that, for the PQC out-puts, categories involving  X  X omputers X  categories are ranked higher for user A and categories involving  X  X ntertainment X  categories are ranked higher for user B. To evaluate the ef-fectiveness of our model more precisely, we also calculated the accuracy of user preference prediction, which can be cal-culated on the held-out test data. Collaborative ranking is learned according to our algorithm in the previous section. Figure 7 shows the comparison between collaborative filter-ing method and our proposed collaborative ranking method on the accuracy of prediction. It can be seen that our collab-orative ranking method could outperform the collaborative filtering method, especially when the iteration number is small.
In this paper, we developed a personalized query clas-sification model PQC which can significantly improve the accuracy of query classification. The PQC solution uses a collaborative ranking model for users X  preference learning to leverage many similar users X  preferences. Preference learning is based on the relevance feedback and collaborative infor-mation. We also proposed an evaluation method for PQC using clickthrough logs, based on which, we have evaluated our PQC system with ranking results from human subjects and clickthrough logs.

In the future, we plan to investigate other types of person-alized query classification problems that are different from topical queries, such as functional output, like categorizing queries into commercial/non-commercial ones. Since users X  interests may change with time, we can also take time into consideration. For example, for a user, more recent pref-erences may be more important. Besides, we also plan to apply the PQC solution to other personalized services such as personalized search and advertising.
Bin Cao, Derek Hao Hu and Qiang Yang are supported by a grant from MSRA (MRA07/08.EG01). [1] E. Agichtein, E. Brill, and S. Dumais. Improving web [2] S. M. Beitzel, E. C. Jensen, D. D. Lewis, [3] C. Burges, T. Shaked, E. Renshaw, A. Lazier, [4] H. Cao, D. H. Hu, D. Shen, D. Jiang, J.-T. Sun, [5] J. M. Carroll and M. B. Rosson. The paradox of the [6] P. A. Chirita, C. S. Firan, and W. Nejdl. Personalized [7] N. Craswell and M. Szummer. Random walks on the [8] C. Ding, T. Li, and W. Peng. NMF and PLSI: [9] Z. Dou, R. Song, and J. Wen. A large-scale evaluation interested in the entertainment
Query QC PQC(User A) PQC(User B) video [10] T. Hofmann. Probabilistic latent semantic indexing. In [11] T. Joachims, L. Granka, B. Pan, H. Hembrooke, and [12] T. Joachims, H. Li, T.-Y. Liu, and C. Zhai. Learning [13] U. Lee, Z. Liu, and J. Cho. Automatic identification of [14] Y. Li, Z. Zheng, and H. K. Dai. Kdd cup-2005 report: [15] Z. Ma, G. Pant, and O. R. L. Sheng. Interest-based [16] F. Qiu and J. Cho. Automatic identification of user [17] D. Shen, R. Pan, J. Sun, J. Pan, K. Wu, J. Yin, and [18] D. Shen, J. Sun, Q. Yang, and Z. Chen. Building [19] X. Shen, B. Tan, and C. Zhai. Context-sensitive [20] X. Shen, B. Tan, and C. Zhai. Implicit user modeling [21] K. Sugiyama, K. Hatano, and M. Yoshikawa. Adaptive [22] J.-T. Sun, H.-J. Zeng, H. Liu, Y. Lu, and Z. Chen. [23] B. Tan, X. Shen, and C. Zhai. Mining long-term [24] M. Weimer, A. Karatzoglou, Q. Le, and A. Smola. [25] G.-R. Xue, H.-J. Zeng, Z. Chen, Y. Yu, W.-Y. Ma,
