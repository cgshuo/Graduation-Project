 Multivariate Time-Series (MTS) are ubiquitous, and are gen -erated in areas as disparate as sensor recordings in aerospa ce systems, music and video streams, medical monitoring, and financial systems. Domain experts are often interested in searching for interesting multivariate patterns from these MTS databases which often contain several gigabytes of data . Surprisingly, research on MTS search is very limited. Most of the existing work only supports queries with the same length of data, or queries on a fixed set of variables. In this paper, we propose an efficient and flexible subsequence search framework for massive MTS databases, that, for the first time, enables querying on any subset of variables with arbitrary time delays between them. We propose two al-gorithms to solve this problem  X  (1) a L ist B ased S earch ( LBS ) algorithm which uses sorted lists for indexing, and (2) a R *-tree B ased S earch ( RBS ) which uses Minimum Bounding Rectangles (MBR) to organize the subsequences. Both algorithms guarantee that all matching patterns withi n the specified thresholds will be returned (no false dismissa ls). The very few false alarms can be removed by a post-processing step. Since our framework is also capable of Univariate Time-Series (UTS) subsequence search, we first demonstrate the efficiency of our algorithms on several UTS datasets pre-viously used in the literature. We follow this up with exper-iments using two large MTS databases from the aviation do-main, each containing several millions of observations. Bo th these tests show that our algorithms have very high prune rates ( &gt; 99%) thus needing actual disk access for only less than 1% of the observations. To the best of our knowl-edge, MTS subsequence search has never been attempted on datasets of the size we have used in this paper.  X  This work was done when the author was vising NASA Ames.
 permission and/or a fee.
 KDD  X 10 Washington DC, USA Copyright 200X ACM X-XXXXX-XX-X/XX/XX ...$10.00.
 H.2.8 [ Database Management ]: Database applications X  Data Mining Algorithms, Experimentation, Performance Similarity Search, Multivariate Analysis, Large Scale Min ing
Many data mining application domains generate large mul-tivariate time series (MTS) databases. Examples of such do-mains include Earth sciences, music, video, medical monito r-ing, and aeronautical and aerospace systems, and financial systems. Domain experts are often interested in searching for particular patterns X  X aveforms over subsets of variabl es which may occur within some window of time of each other.
The motivation for this research comes from applications in Aviation Safety. Consider a typical problem that a safety analyst at an airline might want to address. Suppose that the airline has a large database of one million flights of mul-tivariate time series that show the settings of the control s ur-faces (usually discrete signals), the pilot inputs (discre te), as well as the heading, speed, and readings from the propulsion systems (all usually continuous). In many such databases, the number of recorded parameters from a modern aircraft is nearly 1000. The safety analyst may want to find all situa-tions in the database that correspond to a X  X o-around X  X hich means that a landing has been aborted and the aircraft is directed to circle back for another landing.

Such a situation would correspond to a query on a sub-set of the fields in the time series database where the event LANDING GEAR RETRACTED occurs just after AL-TITUDE descends below 2000 feet. This event is typical of what happens when a landing attempt must be aborted and the plane has to circle back to an appropriate point and attempt to land again. Another search for indicators of an  X  X nstable approach X  may include searching on parameters including speed, descent rate, vertical flight path, and sev -eral cockpit configuration parameters. Again, this search would be done on about a dozen parameters out of the 1000 parameters that may be recorded on the aircraft. The events would be separated in time and may or may not occur on a particular flight.
 Figure 1 shows an MTS from a real aviation dataset of CarrierX 1 . Each MTS contains the data collected from mul-tiple sensors of an aircraft during a flight. In the figure, the x -axis refers to the different parameters while the y -axis refers to time of sampling the values. Typically, an ana-lyst may be interested in only searching a subset of all the variables available. Queries by the analyst may look like: 1. Return all the flights (a subset of the MTS) where the 2. Return all the flights where the aircraft is climbing at
Time Figure 1: Sample MTS dataset and query Q . x -axis refers to different parameters and y -axis refers to time. Components of query and time delays are also shown.

Current research in MTS search [16][19][15][7] does not support the types of queries described here. Current algo-rithms in this area require that the query be of the same length as all the MTS in the database and that all queries be on a fixed set of variables (usually all the variables). Ad-ditionally, current algorithms do not allow for any time lag between the variables in the query.

Our primary application of interest is in the area of avi-ation. Given a large database of flight recorded data we wish to provide a search technology that allows analysts to rapidly identify flights with particular characteristics ( as de-fined across a set of events on a subset of the multivariate time series). Thus, user supplies a query consisting of wave -forms over several variables  X  typically substantially few er than the total number of variables present in the database.
We cannot release the name of the carrier due to the data sharing agreement.
 The user may choose how many and which variables to query over every time, i.e. , this need not be fixed in advance. Also the query may cover any desired length of time up to the maximum length of the available time series. The wave-forms have some (possibly zero) time shifts between them. The user also supplies a threshold for each variable describ -ing the maximum allowable difference between the query variable and the corresponding variable in any matches that are returned. This threshold is in the same units as the cor-responding variable to make threshold selection easier for domain experts. The MTS search algorithm must return all matches (with no false dismissals or false positives), cons ist-ing of the matching MTS in the database and where within the MTS the matching pattern was found (offset from the beginning of the MTS), such that the time shift constraints and the threshold constraints are satisfied.
 There has been substantial research in making Univariate Time Series (UTS) search very fast on very large databases [5][14]. Therefore, one obvious approach to the MTS search problem is to search for each query variable separately with in the database and then join the results while taking into ac-count the time shift constraints. However, this may lead to much more searching than is required, leading to a substan-tial amount of processing time. For example, if the query consists of five variables, but searching on two variables le ads to a small set of candidate matches, then a brute force search on the remaining three variables within the small candidate set would be much faster than a UTS search on the remain-ing three variables in the entire database. We exploit this fact in the novel algorithm that we present in this paper. However we still leverage UTS search X  X y doing so, we uti-lize existing work and advance the area of fast UTS search and also retain the flexibility of allowing queries over any desired subset of variables and with any desired time shifts among the variables, unlike existing MTS search algorithms . The specific contributions of this paper are as follows:  X  We propose two algorithms a list based search algo- X  Using these algorithms as the building blocks, we pro- X  To the best of our knowledge, the datasets that we
The rest of the paper is organized as follows. In Section 2, we discuss in more detail related work in the areas of MTS and UTS search. In Section 3, we describe the notations and give a more precise definition of the MTS search problem. In Section 4 we describe the UTS search algorithm that we use as the core of our MTS search algorithm. This leads into Section 5 where we explain our MTS search algorithm. Section 7 describes our experiments with this algorithm and comparisons with some existing work. We provide conclu-sions and descriptions of future work in Section 8.
We divide this section into related work on UTS search and MTS search.
 UTS search : The topic of subsequence matching of time se-ries has been an active area of research in the database/data mining community. Depending on the application, time se-ries matching can be of the following categories: (1) full time series matching in which the queries are entire time series sequences, and (2) time series subsequence matching in which the queries can be of any size. Popular techniques for performing entire length time series search include the ones proposed by Keogh and Ratanamahatana [6], Sakurai et al. [12], Shou et al. [13] and the references therein. Since these techniques cannot be adopted to perform subsequence search easily, we do not consider them further in our discus-sion.
 One of the early works of subsequence matching is by Faloutsos et al. [2] in which the authors have proposed a DFT/R-tree based indexing scheme. Input time series is first broken into overlapping window sequences of fixed length w and then six DFT coefficients are extracted from each sequence. These 6-dimensional representations are th en packed into a minimum bounding rectangle (MBR) and in-dexed using an R-Tree data structure. On receiving a query, the same process is applied (extracting DFT coefficients) and then the search is performed on R-Tree. Candidate MBRs are then checked with the actual database to remove the false alarms. A dual approach to this one, proposed by Moon et al. [8], is to decompose the input time sequence into disjoint sequences and the query sequence into sliding windows. As a result, this technique can index data points directly instead of MBRs and thereby reduce false alarms. However, as the size of the time series increases to millions of points, storing all the points in the index may still be challenging.

To alleviate this problem, Traina et al. [14] recently pro-posed a technique of using multiple reference points to spee d up the search. The idea is to randomly select multiple global reference points from the dataset, find the distances of all points from this reference point and index these distances i n a tree or other index method. It has been verified that us-ing multiple reference points, the candidate set of the sear ch process can be significantly reduced. While our algorithm resembles this philosophy, it has the following significant differences: (1) [14] only talks about nearest neighbor and range query on the database, we show how it can be used for arbitrary subsequence matching, and (2) unlike [14] which only works for univariate time series, we adopt it for mul-tivariate subsequence search with arbitrary number of vari -ables and arbitrary time delays among those variables.
Several other techniques exist for subsequence matching in univariate time series databases. Due to shortage of spac e we only present the references here  X  ranked subsequence matching by Han et al. [4], disk resident pattern discovery by Mueen et al. [9], subsequence retrieval under DTW [11], and approximate embedding-based matching [1].
 MTS search : There does not exist much work on multivari-ate time series (MTS) search. Yang and Shahabi [16] present a PCA-based similarity technique for comparing two MTS X  X . Given a database of MTS X  X  this technique first computes the covariance matrix between two MTS. Then eigenvectors and eigenvalues of the covariance matrix are used as a measure of similarity between the MTSs. This work was extended in [18] in which the authors proposed the use of kernel PCA instead of traditional PCA which suffers from the curse of dimensionality. The kernel trick helps to solve this problem by not requiring one to explicitly compute the dot product among feature vectors.

Distance-based index structure for MTS has been dis-cussed by Yang and Shahabi [17]. The proposed indexing scheme, known as Muse , builds a multi-level index struc-ture. Unlike the PCA-based similarity index, Muse does not use any weights ( e.g. eigenvalues) while constructing the index, obviating the need for changing the index whenever the weight changes. At query time, the levels are combined with the weights to generate the lower bound on the query distance to the candidates.

The work by Lee et al. [7] addresses the problem of search-ing in multi-dimensional sequences. The multi-dimensiona l sequence is partitioned into sequences, packed into MBR and then indexed using the R-tree scheme. The query is processed in a similar fashion to find the intersecting MBR X  X  after which exact calculation is done. Vlachos et al. [15] pro-poses an index structure for multi-dimensional time series (2-D trajectory data) which can handle multiple distance functions such as LCSS and DTW. Similar to our proposed technique, the index is built using R-tree and queried using the minimum bounding envelope. This indexing and query-ing scheme can, however, only address the nearest neighbor query and not subsequence query which is the main focus of our work.

To the best of our knowledge, there does not exist any multi-dimensional search technique which (1) can perform search on any arbitrarily chosen subset of variables, and (2 ) take into consideration time delay between the variables in the query, both important to our particular application.
Let D be a database consisting of multivariate time series datasets M T 1 , . . . M T  X  D  X  where each M T i can be represented as a matrix of row sequences M T i = h y (1 ,  X  ) i y (2 ,  X  ) rows correspond to time instances and columns correspond to attributes or features. Each y ( j,  X  ) i = h y ( j, 1) suming there are n features v 1 , . . . , v n consistent across all the M T  X  X . It is also assumed that each y ( j, X  ) i  X   X  or { 0 , 1 } . For consistency, the set of values across the  X  -th column y the i -th MTS. Whenever appropriate we will drop the index i . Let y (  X  , X  ) and x (  X  , X  ) be two UTS sequences. Then, Let w be the size of a sliding window containing w consec-utive samples of a UTS. We now define -nearest neighbors ( -NN).

Definition 3.1 ( -NN). Given a user defined thresh-old , and a univariate sequence q of length w , (which we call the query), -NN returns all the subsequences s of length w from the dataset, such that, d ( s, q ) &lt; .
Before we present the formal problem definition, we present the definition of a query.

Definition 3.2 (Multi-variate Query). A multivari-ate query Q consists of the following components:  X  Values specified for a subset of attributes V q = { v 1  X  time delays 1 , 2 , 3 , . . . such that t 3  X  t 2 = 1
Definition 3.3 (Multi-variate Search (MTS)). Given a database of multi-variate time series D , a query Q and a user-defined threshold , a MTS returns all the M T  X  X  such that for all j  X  V q ,  X  the variables are delayed by 1 , 2 , 3 , . . .
When a query Q defined in Section 3.2 contains only one variable, it becomes a univariate time series search. For clarity and ease of exposition, we will start with solving th is problem. We assume there is a minimal length for all queries and it is set to w . This value depends on different applica-tions as we discuss in the experimental section. We first discuss the L ist B ased S earch ( LBS ) algorithm in details and then discuss the salient differences with our R  X  -tree al-gorithm ( RBS ).
For a univariate query q v on the v -th variable, the brute-force method to find all its -NN is to compare it with all subsequences of length L ( q v ) for every offset of time series y i (  X  i = 1 , 2 , . . . ,  X  D  X  ), which is time consuming and im-practical.

A classic data mining solution to speed up this process is to find a lower bound of distance measure and use this bound to prune unpromising candidates. This lower bound should be: (1) cheaper to compute than computing the distances between all subsequences, otherwise we would spend more time; (2) tight with respect to the original distance measur e, otherwise we cannot prune enough.

One such technique for deriving a lower bound, also used in the literature [14][10], is by using a reference subseque nce and the triangle inequality. We will show later that our framework to find the -NN even does not require calculating the lower bound one by one . Figure 2 il-lustrates the basic idea of the pruning. First, we randomly pick a subsequence R (of the same length as w ), and calcu-late its distance to all the remaining subsequences. Then, we order them by their distance to R . S 1 and S 2 are only shown for clarity in the figure. Note that these two steps are done before the query q v comes and only need to be done once. When a query q v comes, we calculate the dis-tance d ( q v , R ). All candidates whose distances are not in the range [ d ( q v , R )  X  , d ( q v , R )+ ] ( e.g. S 2 be pruned. This is due to the triangular inequality: Finally, for all candidates in this range ( e.g. S 1 in Figure 2), we do an exact calculation to remove the false alarms. In order to reduce the number of false alarms, we use multiple reference points to build several indices and then join the candidates from these indices to get the final set of candi-dates. We discuss this in details in the next section. Figure 2: Candidate subsequences ( S 1 , S 2 ) ordered by their distance to a reference subsequence R .
 When a query Q comes, a range based on d ( Q, R ) can be used to prune candidates.
We first discuss the index building algorithm followed by the search algorithm. Alg. 1 presents the pseudo-code of LBS build index. The inputs are UT S Database and length of the sliding window w . The output is a set of sorted lists. In the first step, we select r subsequences R 1 , . . . , R size w from UT S Database which we call reference points . Then, for each overlapping subsequence S of length w from the i -th UTS in UT S Database , we find the Euclidean dis-tance of S from the ri -th reference point R ri . We store these distances (as the key) along with the offset and UTS-id which generated this distance into a list called Index Thus at the end of this process, we build  X  r  X  number of lists Index 1 , . . . , Index r , one corresponding to each refer-ence point. In the next step we simply sort these lists and store them in the disks either as one long list or in parts, depending on the size of the index.
 Algorithm 1 : Build Index for L ist B ased S earch ( LBS ) Input : UT S Database , w Output : Sorted lists Index 1 , . . . , Index r
Initialization : Select r reference points R 1 , . . . , R begin end
When a query Q of length w is provided, we use the search code shown in Alg. 2. The input in this case are the query Q , the UT S Database , the set of indices, the set of refer-ence points, w , and . The output of -NN search returns all subsequences of length w such that the distance of this with Q is less than . First, for each reference point R i , we find the distance Dist i of the query from it. Then we collect those candidates from Index i whose key (distance) lies in the range Dist i  X  . We call this step the first level of prun-ing since we apply the triangle inequality directly here. St ill many false alarms may be generated because the triangle inequality is essentially a one-sided test i.e. if the distance of any subsequence to any reference point is greater than , we can discard the former, but not otherwise, irrespective o f the actual distance of the subsequence to the query. In the second level of pruning, we intersect the candidates found similarly using different reference points. This reduces th e number of false alarms dramatically as we show in out ex-periments. Once a compact candidate set is found, we do a disk access to retrieve those candidates and remove false alarms. Note that we obtain a different candidate set if we use a different reference sequences. The size of candidate set is crucial to the running time, since we have to access the disk and perform the exact calculation to remove false alarms and return all matching candidates.
 Algorithm 2 : LBS -NN Search on UTS Input : UT S Dataset , Q , Index 1 , . . . , Index r ,
Output : Set of nearest neighbors -NN of Q begin end
We now discuss now LBS handles queries longer than w in the following two cases: L ( Q ) = nw ( n &gt; 1) : We first divide Q into n disjoint sub-L ( Q ) = nw + v (0 &lt; v &lt; w ) : Since we have solved the
Our detailed experimental results demonstrate that the proposed LBS algorithm offers a high prune rate even with a moderate number of reference points ( e.g. 3). However the index, being a sorted list of time series points, is often huge (of the order of the number of points in the time se-ries). This increases the storage costs. Our R  X  -based search algorithm solves this problem by avoiding the need to store and index each point separately. Once a set of distances to a reference point are computed as before, we store them together into a Minimum Bounding Rectangle (MBR) and index the minimum and maximum bounds of this rectangle using a spatial indexing scheme such as R  X  -tree. We have used two packing methods proposed in [2]: (1) the I -fixed method which combines a fixed number of points, and (2) the I -adaptive method which optimizes a cost function to find the optimal number of points per MBR. These result-ing trees using multiple reference points become the Index  X  X  for the RBS algorithm. When searching on Q , we perform the same transformation as LBS and search for Dist i  X  in the R  X  -tree. This returns a set of candidate MBRs (for each tree) which then needs to be joined to reduce false positives . Each element from the joined candidate set is retrieved from the disk to remove the false alarms. We do not present the pseudo-code here due to shortage of space. Algorithm 3 : MTS Build Index using LBS Input : M T S Database ( D ), w Output : Index for MTS search
Initialization : Select R (  X  ) 1 , . . . , R (  X  ) r for uts  X  ; begin end Algorithm 4 : MTS -NN Search using LBS Input : D , Q , Index , R 1 , . . . , R r , w , [ 1 , . . . , ]
Output : Set of nearest neighbors -NN of Q begin end State the diff between this method and FRM (FRM uses DFT for 6-d indexing while we use in 2-d..so it is cheaper and scales well)
We combine ref pt with R-tree for better performance and accuracy...also increasing the no of ref pts increases t he prune rate which they cannot do
To validate the performance of the LBS and RBS algo-rithms, we ran a variety of tests on different datasets, both univariate and multivariate. All experiments were run on a 64-bit 2.33 GHz quad core dell precision 690 desktop running red hat enterprise linux version 5.4 having 2GB of physical memory. The algorithms were implemented in Matlab and run on version R2007b. In all our experiments we have mea-sured the following four quantities:  X  ! = set of nearest neighbors within radius of query  X  C = candidate set returned by F RM , LBS and RBS  X  L = length (# samples) of any UTS  X  T = total number of sliding window sequences of any Using these, we derive and report the following quantities as done in the literature [2][8]:  X  Selectivity S =  X  !  X  T  X  Prune rate = 1  X   X  C  X  T Intuitively, selectivity refers to the true fraction of nea rest neighbors for the chosen query Q and threshold , while prune rate refers to the fraction of candidates that can be ignored as having distance greater than even without accessing the database. Note that for an MTS of size  X  D  X  (files), the following two relations hold between any UTS of total length L (over all MTSs) and total number of sliding window sequences T :
L = where L i is the length of any UTS in the i -th MTS.
In order to keep the comparison independent of the imple-mentation across different platforms, we have not measured the actual running time of these algorithms. Note that all of these algorithms guarantee no false dismissals (but fals e alerts). Thus, actual running time is going to be propor-tional to the number of candidates returned, since each of these candidates need to be retrieved from the actual time series databases, and checked if their actual distance to th e query ( Q ) is less than .

We first present results on univariate datasets, followed by results on multivariate datasets. Table 1: Description of the univariate datasets used for comparing the performance of FRM, LBS and RBS.
We have used three univariate datasets for testing our al-gorithms shown in Table 1. These datasets have been used in the literature ([2] and [8]) for finding subsequences from ti me series databases. Figure 3 shows a plot of these datasets.
The first dataset is the stock market dataset having 329,112 entries 2 . The second dataset is the random walk dataset generated synthetically. The first value is set to 1.5 and the subsequent values are obtained by adding a random value in the range (-0.001, 0.001) to the previous one. The last dataset is a pseudo periodic time series dataset 3 in which each value is between -0.5 and +0.5. This dataset appears highly periodic, but never repeats itself.

For all the univariate experiments, we have used the length of sliding window w =512 and length of query sequences the same as the length of sliding window. We perform experi-ments with several selectivities ranging from 10  X  6  X  10 [2]. The desired selectivities were achieved by modifying t he threshold of each query. We tested three algorithms on these datasets: (1) the FRM algorithm using the adaptive MBR approach, details of which can be found in [2], (2) list based method ( LBS ), and (3) the R  X  -tree based method ( RBS ), the last two introduced in this paper. The number of reference points used is given as an argument for LBS and RBS e.g. RBS(5). To avoid the effects of noise and gen-erate statistically significant results, we experimented w ith ten randomly generated queries each having length of w . Unless otherwise stated, we have used three to five reference points for the LBS and RBS methods. In the next section we present the thresholds, selectivities, and results on th ese three datasets.
We summarize the results of FRM, LBS and RBS in Ta-ble 2 for the stock dataset. We varied from 0.01 to 1.0 to generate selectivities in the range of 10  X  6  X  10  X  2 . The table shows the prune rates and the number of nearest neighbors found in the datasets for each of these selectivities averag ed over ten queries. Also shown in this table are the number of MBRs and average points per MBR for the RBS (5) algo-rithm. We have used the I-adaptive MBR creation heuristic as discussed in [2], in which more points can be packed in a single MBR with larger , thereby reducing the total num-ber of MBRs. For all the thresholds, we see that the prune rate of LBS (3) is the best for all the thresholds. Also, the prune rates of RBS (5) tend to be very close to the F RM algorithm.
 We have similar results for the random walk dataset in Table 3. In this case also, the selectivity ranges from 10
Available from ftp://ftp.santafe.edu/
Available from http://archive.ics.uci.edu/ml/ datasets/Pseudo+Periodic+Synthetic+Time+Series to 10  X  2 . As before, LBS (3) performs the best, for all the thresholds while RBS (5) performs better than F RM as the threshold is increased. For the RBS algorithm, we note that prune rate for threshold 0.01 is very close to the maximum value of 1.0 and differs from the F RM prune rate only in the third place of decimal.

The results on the periodic dataset present an interesting phenomenon. As before, the snapshots of the results are presented in Table 4. The LBS approach has the highest prune rate for all the thresholds. However, the RBS (5) tech-nique in this case performs poorly compared to the F RM technique. This can be explained noting that F RM builds MBR X  X  in the DCT domain while RBS directly works in the input space. It is well-known that for periodic signals, DCT/DFT can extract most of the energy in the first few coefficients. Thus, the MBR X  X  constructed by the F RM algorithm using only the first three DFT coefficients are highly condensed and informative. On the other hand, as pointed out by Moon et al. [8], adjacent values of the pe-riodic dataset are relatively large. Hence adjacent window s have large distance to the reference points. When these are combined to form MBR, many windows far apart can be included in the same MBR, thereby increasing the number of candidates and false alarms. Note that the number of candidates can be reduced by increasing the number of ref-erence points. Using around eight reference points, RBS (8) has lesser number of candidates compared to F RM for this dataset. However, for fairness of comparison, we have used three reference points for both LBS and RBS when exper-imenting with the multivariate datasets.

To sum up, both the LBS algorithm and the RBS al-gorithms offer an excellent prune rate for univariate time series search. LBS offers the best prune rate of all the three algorithms compared here, but as discussed before, suffers from large storage cost  X  O ( n ), where n is the number of elements in the timeseries. For a large n , this may be very expensive. On the other hand, RBS uses MBR X  X  to group similar points and hence can reduce the storage cost dramatically. For example, the number of MBR X  X  for the periodic dataset having 1 million data points is only about 6000, thereby reducing the search space by several orders of magnitude. However, since the unit of search is an MBR (containing several points) and not individual points, all the points in the selected MBR X  X  need to be visited. Hence, the prune rate of RBS is lower than LBS . Nevertheless, both these algorithms have a better prune rate compared to F RM .
We have used two large multivariate datasets for demon-strating the search capabilities of LBS and RBS in the multivariate domain. These datasets that are relevant to the NASA Integrated Vehicle Health Management (IVHM) project. To the best of our knowledge, these multivariate datasets are by far much bigger compared to the datasets used in the literature for multi-dimensional time series se arch. The datasets are described next.
 C-MAPSS dataset : The first dataset is simulated com-mercial aircraft engine data. This data was generated us-ing the Commercial Modular Aero-Propulsion System Sim-ulation (C-MAPSS) [3]. C-MAPSS is a high-fidelity sys-tem level engine simulator designed to simulate nominal and fault engine degradation over a series of flights. The datase t contains 6875 full flight recordings sampled at 1 Hz with 29 engine and flight condition parameters recorded over a 90 minute flight that includes ascent to cruise at 35000 feet and descent back to sea level. This dataset has over 32 mil-lion tuples. Since some of the variables do not show much variability, we have tested our algorithm on a subset of 16 variables only. Table 5 presents the salient features of thi s dataset. Interested readers can download this dataset from : Dashlink 4 .
 US Regional carrier dataset (CarrierX): The second dataset is a real life commercial aviation dataset of a US re-gional carrier consisting of 3573 flights. Each flight contai ns 46 variables. Domain experts identified a subset of 9 vari-ables combination of which are critical to assess the health of such aircraft systems. The entire dataset contains more than 22 million tuples.

For all the multivariate experiments, we have used sliding window size and length of query equal to 256. For LBS we have used three reference points, while for RBS we have used five reference points for building the indices. These choices are based on the prune rates of these algorithms on univariate datasets.
 Table 5: Description of the multivariate datasets used for demonstrating performance of LBS and RBS.
Table 6 presents three sets of thresholds for each of the variables of the CMAPSS and CarrierX dataset. The choice https://dashlink.arc.nasa.gov/data/ c-mapss-aircraft-engine-simulator-data/ Table 6: Thresholds for the variables of CMAPSS and CarrierX dataset. of these thresholds is such that the selectivities of each va ri-able independently ranges from 10  X  6  X  10  X  2 .

The performance results of LBS and RBS on CMAPSS and CarrierX are presented in Table 7. The first column denotes the dataset. The second column refers to the five different queries we have run along with the variables for each query. We have run each query with three different thresholds (hence the three rows for each query) presented in the table in increasing order. For example, using Ta-ble 6, it can be concluded that 1 = (0 . 2 , 2 , 1) for the first query of CMAPSS. The next three columns show the num-ber of candidates generated for the first variable ( C 1 ), the second variable ( C 2 ), and after joining these two candidate sets C 12 both for LBS and RBS . The join on the candidate set is performed based on two criterion: (1) the time delay between any two candidates must conform to the ones spec-ified in the query, and (2) they should be generated from the same MTS. Generating C 1 and C 2 is the first level of filtering while generating C 12 refers to the second level of filtering. Column C e is the actual number of these candidates which are found to be less than the threshold after doing the ex-act calculation. So smaller the size of C 12 , the lesser the number of actual disk elements that need to be accessed. ! column refers to the actual number of nearest neighbors of the query after taking all the variables and time delays into consideration. The last two columns show the prune rate = C 12 /T and selectivity S = !/T respectively.

These results show that for the two large multivariate datasets, querying with different queries and thresholds, t he prune rates are very high  X  99% implying that only less than 1% of the candidates need to be retrieved from the database for exact calculation. Also, we notice that the siz es of the candidate sets are smaller for LBS than RBS for all the queries thereby raising lesser number of false positive s. However, the storage requirements of LBS is non-trivial. For example, for CarrierX, we need to index approximately 22 million distances using each reference point per UTS. The total storage requirement for the index will be 22 , 000 , 000  X  (4 + 4 + 4) / (1024  X  1024)  X  250 MBytes , for each UTS, assuming we store { distance, MTS id, Off-set } for each window sequence as a float of (4+4+4) bytes. For RBS , let X  X  assume that (1) we have M MBR X  X  on av-erage for each reference point, and (2) we store { min MBR, max MBR, MTS id, Offset } for each MBR. In our experi-ments we have M = 5174619. Then the total storage re-quirements (assuming 4 bytes for each) will be: 5 , 174 , 619  X  (4 + 4 + 4 + 4) / (1024  X  1024)  X  78 MBytes . Hence the storage requirements of RBS is much less than LBS . From these results we conclude that:  X  query execution time of LBS is expected to be much  X  RBS has relatively higher rate of false positives com- X  the index storage requirements of LBS may be signif-
In this paper we present two algorithms LBS and RBS for finding multivariate subsequences from large MTS datasets. Both these algorithms guarantee no false dismissals. To demonstrate the prune rate, first we have run experiments on several UTS datasets used in the literature for subse-quence search. The results show that LBS offers the best prune rate of all the three algorithms compared in this pa-per. RBS has a lower prune rate due to the search unit being an MBRs and not individual points, but builds smaller in-dices. Experiments on two commercial aviation related MTS datasets each having millions of tuples show that both these algorithms offer excellent prune rates (greater than 0.9) &gt; The latter implies that we need to retrieve only a small per-centage ( &lt; 1%) of all the candidates for post processing in order to eliminate false positives. To the best of our knowl-edge, this is the first algorithm which allows extremely fast and flexible pattern/subsequence search in massive multi-variate time series datasets on any subset of variables with time delays between them. Also the CMAPSS and Carri-erX datasets that we have tested are the much bigger than any of the MTS datasets used in the literature for multi-variate subsequence search. As a future work, we plan to develop a parallel and fully decentralized implementation of this MTS search technique on a Map-Reduce framework for better scalability.
 This work was supported by the NASA Aviation Safety Pro-gram, Integrated Vehicle Health Management Project and a Google Annex project. [1] V. Athitsos, P. Papapetrou, M. Potamias, G. Kollios, [2] C. Faloutsos, M. Ranganathan, and Y. Manolopoulos. [3] D. K. Frederick, J. A. DeCastro, and J. S. Litt. User X  X  [4] W. Han, J. Lee, Y. Moon, and H. Jiang. Ranked [5] J. J. Shieh and E. Keogh. iSAX: Disk-aware Mining [6] E. Keogh and C. A. Ratanamahatana. Exact Indexing [7] S. Lee, S. Chun, D. Kim, J. Lee, and C. Chung. [8] Y. Moon, K. Whang, and W. Loh. Duality-Based [9] A. Mueen, E. Keogh, and N. Bigdely-Shamlo. Finding significantly lesser number of candidates i.e. false alarms C 12 compared to RBS . [10] A. Mueen, E. Keogh, Q. Zhu, S. Cash, and [11] S. Park, S. Kim, and W. Chu. Segment-based [12] Y. Sakurai, M. Yoshikawa, and C. Faloutsos. FTW: [13] Y. Shou, N. Mamoulis, and D. Cheung. Fast and [14] C. Traina, R. Filho, A. Traina, M. Vieira, and [15] M. Vlachos, M. Hadjieleftheriou, D. Gunopulos, and [16] K. Yang and C. Shahabi. A PCA-based Similarity [17] K. Yang and C. Shahabi. A Multilevel Distance-Based [18] K. Yang and C. Shahabi. A PCA-based Kernel for [19] K. Yang and C. Shahabi. An Efficient k Nearest
