 Jiangxi Univ. of Finan. &amp; Eco. Hong Kong Univ. of Sci.&amp;Tech.
 Recently, the RFID technology has been widely used in many kinds of applications. However, because of the interference from environmental factors and limita tions of the radio frequency technology, the data streams coll ected by the RFID readers are usually contain a lot of cross-read s. To address this issue, we propose a KerneL dEnsity-bAsed Probability cleaning method (KLEAP) to remove cross-read s within a sliding window. The method estimates the density of each tag using a kernel-based function. The reader corresponding to the micro-cluster with the largest density will be regarded as the position that the tagged object should locate in current window, and the readings derived from other readers will be treated as the cross-reads. Experiments verify the effectiveness and efficiency of the proposed method. H2.8 [ Data Mining ]: Database Application  X  data mining Algorithms; Performance; Design; Experimentation Cross-reads, Kernel density estimator, RFID data cleaning, RFID data streams Recently, the radio frequency identification (RFID) technology has been used in many kinds of applications such as retail supermarkets and supply chain, but the issue of false reads on the tagged items [1]. Basically, the false reads in RFID streams can be classified into two categories [2]: 1) Missing-Reads. Though a RFID tag indeed locates in the range of a reader, it might not be read at all, thus, leading to a false belief that the tag is not present. This may be caused by the weakness of RF signal, shortage of power, shie ld of signal between the tag and also referred as false negatives. 2) Cross-Reads. When a RFID tag locates outside the range of a reader, but it might be captured by this reader which leads another Cross-reads may be arisen by the reflection of metal items, the abrupt strengthen of RF, and the change of antenna directions. This type of errors is al so called false positives. The presences of missing-reads and cross-reads in the RFID streams collected by RFID readers can result in the location uncertainty of tags. For exampl e, when a tagged object is not detected by a reader at some time, we are not sure whether the tag doesn X  X  locate in the region of the reader. Similarly, when a tagged object is captured by a reader, we are also not sure whether the tag indeed locates in the region of th e reader. Therefore, the existence of missing-reads and cross-reads make the tasks such as real-time monitoring and tracing difficult. People have already noticed the negative effects brought by the false reads, and some works have been conducted to eliminate the defects. Among these works, mo st focus on reducing the number of missing-reads, such as using a smooth filter to interpolate the missing reads within a window[3], applying a sample-based approach to recovery the missing-reads[4], using Bayesian inference to clean RFID raw data[5]. We know in an application of r eal-time monitoring and tracking, the presence of missing-reads only losses the locations of the tagged objects temporarily, which can be resolved by repeatable readings[3]. However, cross-reads often result in providing wrong information, and the existence of cross-reads will limit the wide usage of RFID technology. However, due to the dynamic and massive properties of RFID streams, it is not trivial to remove cross-reads due to the unique characteristics: 1) The uncertainty of the reading . Generally, an item in a store readers sometimes might move it to another shelf. Therefore, when a tag is captured by a reader, due to the uncertainty of position information about the tag and th e randomness about the generation of cross-reads, we cannot accurately determine whether the reading about the tag is caused by moving the tag into the detection range or by the cross-reads. 2) The process of real-time stream. As we know, a RFID reader keeps on sending RF signals to tags and receives the reflected signals. Therefore, the captured RFID data by a reader is often presented in the form of stream, which requires the real-time processing, that is, the process speed for the RFID data steam must be as fast as the data arriving sp eed. Otherwise, the useful data will get lost. SMURF [5] can adapt the size of window to reduce false positives, but it can not eliminate the cross-reads generated by physical factors. Wang et al. [6] removed the cross-reads using a simple counting method based on the hy pothesis that cross-reads generally have a low occurrence rate compared to normal true readings. However, the effectiveness of the method is directly do not have much domain knowledge . Furthermore, if the number of the cross-reads is more than th at of the normal true readings, the approach may produce wrong filter results. Motivated by the necessity to remove cross-reads, in this paper, we design an effective KerneL dEnsity-bAsed Probability cleaning method (KLEAP) to remove cross-reads over a RFID data stream. The rest of the paper is arranged as follows. Section 2 gives the problem definition. In Section 3, we describe a kernel-based cleaning method through estimate co mputing kernel density We discuss experiments results in S ection 4. Finally, the paper is concluded. A RFID stream can be segmented into a sequence of disjoint time windows, and in each window, it is assumed the probability that a tagged item moves from a site to another (i.e., the position change) is very low. Because a window may contain multiple read cycles in the real RFID application environment, it may produce multiple readings about a tag in a window. In a time window, a tag may be reported appearing within the detection ranges of multiples different readers. In order to determine the real position of the tags, we need to get a position distribution of the tag in a window. The reader near the tag will have more readings and receive stronger signals than the reader far from the tag. 1) The reader whose detection range covers T will produce more observations than the reader whos e detected range does not cover T . 2) For the observations on T , the average signal strength of the observations produced by the reader whose detection range covers T will be higher than that of the observations produced by the reader whose detection range does not cover T . We can represent an observation on a tagged object T i collected by reader R j in window W k as a k ij V : Where  X  k is the logic timestamp of observations produced in W and SS represents the receiving signal strength. The greater the value of SS is, the stronger the signal strength is. We can group the observations of tags based on their IDs in each window when a new observation arrives. The gr ouping results on each tag can be represented as a stream. For example, all of observations on T collected in window W k form a data stream as follows. of the tag produced by different readers in each window. In order to save the memory space and reduce the cost of computation, we group all of observations k ij V about T i collected by R micro-cluster k ij C , which can be defined as following: where SS is the average signal strength of T i captured by R W ; k ij D is total number of observations that R j P represents the probability that T i locates in the reading range of R when it is detected by R j . By now, the problem of detecting cross-reads can be transferred into deriving the true location of T i from all micro-clusters of T The difficulty for filtering cross-reads is how to identify cross-detecting outliers on data streams. The density-based methods often perform well than the distance-based one [7], so we apply the density-based methods to detect cross-reads. To obtain the location, we exploit the method of kernel density estimator to com pute the density of k ij C . Based on the densities of all micro-clusters of T i , we can infer that T range of the reader corresponding to the micro-cluster who has the both indicate that tag T i should locate within the detection range of R . Note that, when more than two micro-clusters have the same density, we will choose the reader which has the largest probability that a tag should locate. In our study, in order to acquire enough sample observations, the time of actual observation is char acterized by the window. It is assumed the cross-reads are irrelevance in different windows. According to the method of kernel density estimator, the kernel density of all observed sample points is defined as follows. Definition 1. Let X 1 , X 2 , ..., X n be the sample points of independent random variables with the same distribution in space R following: where K is a kernel function, h is a smoothing parameter(i.e., number of sample points. In this paper, we use a standard Gaussian function as K, which mean is zero and variance is 1. Thus the variance is controlled indirectly through the parameter h . Generally, the read cycle of a read er is very small (about 0.5ms). If a window size is 1s, the reader will produce about 2000 observation readings for one tag without including the cross-reads. If the number of tags is large, it is very time-consuming to estimate the kernel density due to the huge volume of data. tags, we borrow the idea of micro-clusters in clustering algorithms to compress the data set. Firstly, a centroid is chosen for each tag to maintain a micro-cluster based on reader IDs. corresponding centroid of the micro-cluster that it should belong to. Thus, the centroid of a micro-cluster can be used to represent all data and reduce largely the overh ead to compute kernel density. In the end, each micro-cluster is regarded as a pseudo-point. At this time, we only need to compute the contribution of each centroid in the micro-cluster to other data points. Now, we can define the density es timator as the sum of weights of the kernel functions, where each weight equals the number of observations that the tag is detected by the reader in different micro-clusters (i.e., k ij D ). Therefore, if micro-clusters k im k i CC K 1 contain all the data points of T , the density estimator is defined as follows: where and XC k ij  X  represents the distance between X and the centriod of C . This distance is calculated on the attributions &gt;&lt; SSR Formula (8): In Formula (6), because only R j and SS bar is used to calculate the kernel function, so the value of d is 2. The parameter h only affects the smoothness of kernel density estimator. For it is not a key factor in our algorithm, we didn X  X  discuss the details about it in the paper. In the experiment s discussed in Section 4, the value of h is set as 0.3. p AAA ,,, 21 L ++ are categorical attributions (e.g., the IDs of readers). d ( X,Y ) is a float number made up of two parts: the integer part is the sum of Hamming distances computed according to Formula (9), and the decimal part is the corresponding attributions. After we calculate the distributed densities of all the sample points, we can query the micro-cluster which has the largest sum of the sample densities. As a conseque nce, the reader corresponding to the micro-cluster with largest dens ity is the position that the tagged object should locate in the current window. The readings derived from other readers will be treated as cross-reads. Because we have m readers, the computation cost of Formula (6) is O ( m ). In addition, we should compute the density for each tag through formula (6), and there are n tags. Then, the computational complexity of kernel density estimator is O ( m*n ). In the paper, we focus on cleaning cross-reads from the uncertain RFID data stream caused by physical factors, e.g., the reflection of metal items. Throughput and precision ratio are selected as the metrics to measure the performance. Throughput = Total number of observation events processed/Total processing time; Precision=The number of detected cross-reads/The number of actual cross-reads. We developed a simulator for a retail store scenario that produces synthetic RFID streams. Under this scenario, we can control various factors, such as the distance between tags and readers, antenna direction, signal reflection strength, etc. Table 1 lists main experimental parameters. Experiment 1. Test the precision if tags move using KLEAP and Na X ve methods. In order to show the advantage of the KDE-based method, an experiment is designed to test the precision comparing with two na X ve methods. The first na X ve method (Na X ve1) is a simple and intuitive method which only counts the number of times a tag is should locate. The second na X ve method (Na X ve2) only compares the average signal strength which a tag is captured by each reader within one window to determine th e position of tags. For a tag, the reader with the largest average signal strength is the reader that the tag should belong to. The precision results of the three methods are shown in Figure 2, which show the precision of KLEAP method is much higher than those of the na X ve approaches. The reason is that the two na X ve approaches cannot map data point into a multiple dimension space. Experiment 2. Test the scalability in th e case of multiple tags This experiment is designed to test the scalability of KLEAP method in the situation with mu ltiple tags. Assumed there are M tags are evenly spread in three detected range of readers. When M is varying from 50 to 500 and W k =50, WS =30s, the throughput and precision of the method are shown in Figures 3 and 4. We can see from Figure 3, the throughput will decrease with the increase of tag number. This is because the volume of steam data in the windows will increase as the number of tags increases. However, the precision changed little (See Figure 4), which shows that KLEAP can apply into the s ituation with large volume of tags. Due to the existence of cross-reads in RFID streams, in this paper, we propose an efficient kernel density-based probability cleaning method, called KLEAP, to filter the cross-reads in RFID data streams. KLEAP detects the exact positions of tags over the RFID data streams through examining the kernel densities of each tag captured by multiple readers. The computational complexity of kernel density estimator is O ( m*n ). In the future, we will enhance KLEAP performance further in two aspects: use probability information to implement increment computation and adjust window size adaptively. [1] S. S. Chawathe, V. Krishnamurthy, S. Ramachandran, et al. Managing [2] C.Floerkmeier. A probabilistic appr oach to address uncertainty of [4] J. Xie, J. Yang, Y. Chen, et al. A Sampling-Based Approach to [5] Haiquan Chen and Wei-Shinn Ku and Haixun Wang. Leveraging [6] Y. Bai, F. Wang, and P. Liu. Efficiently Filtering RFID Data Streams. 
