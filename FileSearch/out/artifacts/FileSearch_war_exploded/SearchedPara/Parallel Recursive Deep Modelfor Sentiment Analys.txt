 Sentiment analysis, also called opinion mining, is the field of study that analyzes peoples opinions, sentiments, evaluations, appraisals, attitudes, and emotions towards entities such as products, services, organizations, individuals, issues, events, topics, and their attributes [ 1 ].
 popular research problem to tackle. In contrast to Web sites where people are limited to the passive viewing of content, Web 2.0 may allow users to more easily express their views and opinions on social networking sites, such as Twitter and Facebook. The opinion information they leave behind is of great value. For exam-ple, by collecting movie reviews, film companies can decide on their strategies for making and marketing movies. Customers can make a better decision which movie is worth watching. Hence, in recent years, sentiment analysis has become a popular topic for many research communities, including artificial intelligence and natural language processing. In this paper, we focus on the task of predicting sentiment label distribu-tions, which aims to predict sentiment label distributions on phrase and sentence level. There has recently been considerable progress in predicting sentiment label distributions. Some existing supervised learning approaches [ 2 ] employed anno-tated corpora of manually labeled documents. Several unsupervised learning approaches have also been proposed [ 3 , 4 ] based on given sentiment lexicons. Various joint sentiment and topic models [ 5  X  7 ] were proposed to analyze sentiment in detail. Recently, some models based on recursive neural networks have considerable representational power, such as RNN [ 8 ], MV-RNN [ 9 ]and RNTN [ 10 ]. All these models, i.e., RNN and its variant, predict sentiment label of phrase or sentences, based mainly on its vector representation, while missing some valuable local information for the global judgment.
 Therefore, we consider that the local information has contributed to the global analysis, i.e. the sentiment distribution of the words in a phrase or sentence is important, and can impact the sentiment analysis for the phrase or sentence. So based on this hypothesis, we propose a Parallel Recursive Deep Model (referred as PRDM), by introducing a sentiment Recursive Neural Networks (sentiment-RNN) to cooperate with RNTN. In our model, each node in neural networks corresponds to a sentiment node, which is represented as sentiment label vector. All the sentiment nodes form a sentiment-RNN. Sentiment-RNN and RNTN composed of a parallel yet interacted structure. As a result, the sentiment of a phrase (or sentence) is calculated by utilizing not only the vector for the phrase or sentence but also the sentiment distribution of its children.
 Moreover, we find that our PRDM is able to be seamlessly integrated with Stanford Sentiment Treebank 1 , which is the first benchmark with fully labelled parsing trees. This Treebank is a valuable resource for a complete analysis of the compositional effects of sentiment in language [ 10 ]. And our PRDM is able to integrate not only the structure information of parsing trees, but also provide a potential network structure over the manually labels in the tree.i.e., PRDM pro-vides a fully supporting platform for Stanford Sentiment Tree bank, to facilitate the analysis of the compositional effects of sentiment in language. In order to illustrate our models effectiveness in predicting sentiment label distributions, we compare to several models such as RNN [ 8 ], MV-RNN [ 9 ]and RNTN [ 10 ], and baselines such as Naive Bayes (NB), bi-gram NB and SVM. Experimental evaluation demonstrates that our model outperforms previous state of the art.
 The rest of the paper is organized as follows: In Section 2, we introduce some related work including word representations, recursive deep learning and Stanford Sentiment Treebank. Section 3 introduces our Parallel Recursive Deep Model and describes the details of parameter learning. Section 4 presents the experiments on predicting sentiment label distributions on both sentences and phrases level. The conclusion and future work are presented in Section 5. All the datasets, code and all relevant parameter specifications are publicly available This work is mainly connected to two areas of NLP research: word represen-tations and recursive deep learning; and one new corpus: Stanford Sentiment Treebank. 2.1 Word Representations A word representation is a mathematical object associated with each word, often a vector. Each dimensions value corresponds to a feature and might even have a semantic or grammatical interpretation, so we call it a word feature [ 11 ]. hopefully capturing useful syntactic and semantic properties [ 11 ]. Many approa-ches have been proposed to learn good performance word embeddings. There are some embeddings datasets publicly available for evaluation, such as SENNAs embeddings [ 12 ], Turians embeddings [ 11 ], HLBLs embeddings [ 13 ], Huangs embeddings [ 14 ].
 of language models. It was recently proposed using the distributed Skip-gram or continuous Bag-of-Words (CBOW) models. These models learn word repre-sentations using a simple neural network architecture that aims to predict the neighbors of a word. Due to its simplicity, the Skip-gram and CBOW models can be trained on a large amount of text data. There is one parallelized imple-mentation can learn a model from billions of words in hours [ 15 ]. 2.2 Recursive Deep Learning Recursive neural networks (RNNs) [ 8 ] are able to process structured inputs by repeatedly applying the same neural network at each node of a directed acyclic graph (DAG). The recursive use of parameters is the main difference with stan-dard neural networks. The inputs to all these replicated feed forward networks are either given by using the childrens labels to look up the associated represen-tation or by their previously computed representation. RNNs [ 8 ] are related to auto-encoder models such as [ 16 ].
 ous vector and a matrix of parameters. It assigns a vector and a matrix to every node in a parse tree: the vector captures the inherent meaning of the constituent, while the matrix captures how it changes the meaning of neighboring words or phrases. The composition function employed in MV-RNN can be referred to [ 17 ]. nonlinear function. In MV-RNN, the total number of parameters to learn is large. RNTN [ 10 ] aims to build greater interactions between the input vectors. It takes as input phrases of any length. And it utilizes the same tensor-based composition function for all nodes. RNTN has been successfully utilized in predicting phrase or sentence sentiment label. 2.3 Stanford Sentiment Treebank In order to better express the meaning of longer and variable length phrases, richer supervised training and evaluation resources are necessary. The Stanford Sentiment Treebank is the first corpus with fully labeled parse trees that allows for a complete analysis of the compositional effects of sentiment in language [ 10 ]. The corpus consists of 11,855 single sentences, which are movie reviews. The Stanford parser [ 18 ] is used to parse all sentences. The Stanford Sentiment Tree-bank includes a total of 215,154 unique phrases from those parse trees. And each phrase is labeled by Amazon Mechanical Turk. This new dataset allows us to bet-ter predict sentiment label of any-length phrases or sentences based on supervised and structured machine learning techniques. The dataset will enable community to train and evaluate compositional models. Stanford Sentiment Treebank has been successfully utilized in predicting movie review [ 10 ]. More details about Stanford Sentiment Treebank dataset can be referred to http://nlp.stanford. edu/sentiment/treebank.html . In this section, we firstly introduce our Parallel Recursive Deep Model (PRDM). And then we describe the details of parameter learning. 3.1 Parallel Recursive Deep Structure Our Parallel Recursive Deep Model can strengthen compositional vector repre-sentation ability for phrase of any length and sentence, as well as the ability of sentiment label prediction via introducing a sentiment recursive deep structure. Fig.1 shows our model. The key difference between our model and RNTN is that RNTN use the sentiment label of word or phrase only for local sentiment label training and prediction, while we introduce a sentiment-RNN to propagate local sentiment label information to the whole network, i.e., in the process of training and predicting, any local calculation includes remote linked sentiment label information. In this way, we take more full usage of sentiment labels, and also in this way, we can easily take the RNTN as the special instance of our model. For ease of exposition, we used a tri-gram not very good to explain our model.
 We first make some definitions. Each word is represented as a vector. All the word vectors are stacked in the word embedding matrix R d  X | M | , | M | is the size of the vocabulary. The word embeddings can be seen as a parameter that is trained jointly with the model. There are two kinds of units in PRDM, term node and sentiment node. Term node, corresponding to a word or phrase or sentence, is represented as a representation vector; sentiment one is represented as the sentiment label vector of word (or phrase, sentence). When an n-gram is given to the model, it is parsed into a binary tree and each leaf node, corresponding to a word. In Fig.1, term node vector representations for each word respectively.
 timent node. The sentiment label vector is C -dimensional.
 sentiment classification matrix; W L  X  R C  X  2 C is the sentiment transformation matrix; W  X  R d  X  2 d is the transformation matrix. In our model, we omit the bias for simplicity.
 RNTN and another is sentiment-RNN. RNTN will compute parent vectors in a bottom up fashion using compositionality function g . The parent vectors are again given as features to next layer and sentiment classification matrix. nodes as inputs to compute parent sentiment node. As a result, the final sen-timent label of each phrase or sentence is computed through its vector and its children nodes sentiment nodes. From Fig.1, we can see that RNTN and sentiment-RNN are parallel in structure. It is also the reason that we call our model as Parallel Recursive Deep Model.
 tensor-based compositional function is that it can directly relate input vectors. node.
 resulting in a R 2 d  X  1 vector. And the output of each slice product computed as formula (2). V i ]  X  R 2 d  X  2 d is the i th slice of the tensor. The initialization of slices is random. The slices will subsequently be modified to enable a sequence of inputs to compose a new vector.
 More details about tensor-based composition can be referred to [ 17 ]. Based on the definition of compositionality function, we give the general function as formula (3) to compute a parent vector V p .
 W  X  R d  X  2 d is the transformation matrix and also the main parameter to learn. When V is set to 0, the compositionality function is the same as used in standard Recursive Neural Networks [9]. f = tanh is a standard element-wise nonlinearity. In our example, the vector of p 1 in Fig.1, the parent node of computed through formula (4). After computing the first two nodes, the network is shifted by one position and takes as input vectors and again computes a potential parent node. The next parent vector p 2 in Fig.1 will be computed as formula (5). Note that the parent vectors must be of the same dimensionality to be recur-sively compatible and be used as input to the next composition. RNTN model uses the same, tensor based composition function.
 When comes to the sentiment-RNN, it repeats the similar process. However, the inputs become the sentiment nodes. Sentiment-RNN model uses standard composition function as formula (6) to compute parent sentiment node Where W L  X  R C  X  2 C is the sentiment transformation matrix. When 0, RNTN [10] is the special case of our Parallel Recursive Deep Model. And similarly we employ formula (7) to compute next parent sentiment node We not only use each node as features to a sentiment classification matrix , but also use its children nodes sentiment nodes as inputs to sentiment transformation matrix W L . Then we use the obtained vectors as inputs to a soft classifier. For classification into C classes, we compute posterior over labels given the node and its corresponding sentiment node. For example, we compute the sentiment label of phrase not very good in Fig.1 as formula (8). Where W s  X  R C  X  d represents sentiment classification matrix. The prediction for other phrases of the given tri-gram, for example very good, is similar to formula (8). We skip description for computing other phrases sentiment labels. 3.2 Backprop Through Parallel Recursive Deep Structure In this section, we describe how to train our model. Given a sentence, we aim to maximize the probability of correct prediction, or minimize the cross-entropy error between the predicted and target sentiment labels at all nodes. The pre-dicted sentiment label at node i is represented as y i , and the target sentiment label at node i is represented as t i , which is labelled by humans. regularization hyper parameters for all model parameters. The error as a function of the PRDM parameters for a sentence is represented as formula (9). j means the j th sentence in corpus. Num is the total number of sentences in corpus.
 More details about process of compute can be referred to [ 19 ]. The derivative for the weights of sentiment transformation for sentiment node computed as formula (10).  X  is the Hadamard product between the two vectors. f is the element-wise derivative of function f , which in standard case of using computed as f ( x )=1  X  f 2 ( x ).
 sentiment node S bc is computed as formula (11). for the weights of sentiment transformation W L as formula (12). The derivative for the weights of the sentiment classification matrix node p The derivative for the weights of the sentiment classification matrix other nodes is similar to formula (13) and we skip the details here. The final result of the derivative for the weights of the sentiment classification matrix is the sum of all the error from each node. It is computed as formula (14). T is the total number of nodes in the sentence parse tree.
 The full derivative for W and V is the sum of the derivatives at each node. We can use formula (15) to compute the full derivative for  X  k and  X  ,full and  X  p 1 ,full are full incoming errors for node p the optimization we use AdaGrad [ 20 ] to find optimal solution. The process of computing W is similar. More details about learning W and to [ 10 ]. For the experiment, we follow the experimental protocols on previous state-of-the-art RNTN model as described in [ 10 ]. We employed the same initialized parameters such as learning rate and word embeddings and so on.
 The sentences in the Stanford Sentiment Treebank were split into three sets: train set (including 8544 sentences), dev set (including 1101 sentences) and test set (includ-ing 2210 sentences). We use dev set and cross validation over legal-ization of word vector size, learning rate as well as the weights and minibatch size for AdaGrad.
 We compare to the previous state of the art based on RNTN. And we also com-pared to commonly used methods that use bag of words features with Naive Bayes and SVMs, as well as Naive Bayes with bag of bigram features. We abbre-viate these with NB, SVM and biNB. We also compare to a model that averages neural word vectors and ignores word order (VecAvg), as well as Recursive Neu-ral Networks (RNN), MV-RNN and RNTN.
 4.1 Sentiment Label Prediction 0 means very negative; 1 means negative; 2 mean neutral; 3 means positive; 4 means very positive.
 ex-ample is A fascinating and fun film.. Note that the full stop . is also taken as a word. The right one is sentiment tree labeled by humans and the left one is predicted result by our model. In this example, we can see that there is only one prediction, (sentiment label for phrase fascinating and), different to sentiment label annotated by humans.
 positive; If lv &lt; 2, then it will be classified into negative; If classified into neutral. All neutral sentences are removed from the Treebank, and finally we get 6920 sentences in train set, 872 sentences in dev set and 1821 sentences in test set.
 mance of positive/negative on phrases or sentences ignoring the neutral classes. label values based on our model. The sentiment labels are classified into binary classification of positive/negative according to the values.
 both all phrases (All) and full sentences (Root). The results of other models or approaches in Table 2 are cited from [10].
 a large margin, especially on all phrases level. The previous state of the art for all phrases was 87 . 6% based on RNTN [10]. PRDM pushes the previous state of the art on full sentences up to 93 . 1%. PRDM achieves state of the art on both full sentence and all phrases level. It is expected since that RNTN is the special case of our PRDM when the sentiment transformation matrix Based on the experimental result, we can make a safe conclusion that our model is more reliable in predicting sentiment label distributions on both phrase and sentence level. We introduce a novel Parallel Recursive Deep Model (PRDM) for predicting senti-ment label distributions. It can express the sentimental semantics of any length phrases and sentences. The main property of our model is to introduce a novel sen-timent-RNN cooperating with RNTN. PRDM not only utilizes the information of the vector for word and phrases, but also utilizes the information of the corresponding sentiment nodes. This strengthens the ability to predict sentiment label on both sen-tence and phrase level. Our model achieves state of the art performance on both full sentences level and all phrases level. Meanwhile, as our model is seamlessly integrat-ed with Stanford Sentiment Tree bank, our PRDM provides a fully supporting plat-form for this Tree bank, to facilitate the analysis of the compositional effects of sen-timent in language. In the future work, we will expand our model for more complicat-ed tasks in sentiment analysis, such as sentiment summarization.

