 Choosing between two learning algorithms given a sin-gle dataset is not a trivial task [10]. The most straight-forward approach is to use a hypothesis test of some kind to decide whether we can reject the null hypoth-esis that the two algorithms perform the same. How-ever, because there is often just a limited amount of data available, we have to reuse the data more than once to get a number of samples x . Here x provides an indication of the difference in accuracy of two algo-rithms. We can use the values of x to calculate some sort of statistic T . For that statistic to be used in a hypothesis test we normally assume that the sam-ples x on which T is based are independent. However, we know that they are not be completely independent because they are based on partly the same data. In practice, the result is that when performing a test the Type I error differs considerably from the desired sig-nificance level, as observed in previous work [2, 7]. One of the effects of having dependence between sam-ples is that the estimated variance is lower than the actual variance and a way to overcome this defect is to compensate the variance estimate [7]. In this ar-ticle, we look at he effect of the degrees of freedom being lower than the theoretically expected number. We can compensate for this by calibrating the degrees of freedom.
 In the following section, various well known hypothe-sis tests and their variations are introduced in detail, followed by tests based on repeated cross validation in Section 3. Section 4 sets out the procedure for cali-brating the hypothesis tests by measering the actual degrees of freedom for the tests. In Section 5, we study empirically the behavior of the various tests by varying parameters of the environment in which the tests are applied. We finish with some concluding remarks, rec-ommendations and some pointers to further research. First, we describe hypothesis tests based on k-fold cross validation, resampling and corrected resampling. The 5x2 cross validation test [2] was not considered because of the undesirable low replicability (see Table 5), making it hard to justify the extra work involved in setting up experiments. We make the degrees of free-dom explicit in the formulas (usually indicate by df ), so that it is clear what changes when the df is fixed to a value that differs from the default one. 2.1. k-fold cross validation In k-fold cross validation, the data is split in k approx-imately equal parts and the algorithms are trained on all data but one fold for each fold. The accuracy is esti-mated by using the data of the fold left out as test set. This gives k accuracy estimates for algorithms A and B , denoted P A,i and P B,i where i (1  X  i  X  k ) is the fold left out. Let x i be the difference x i = P A,i  X  P B,i then the mean of x i is normally distributed if the al-gorithms are the same and the folds are sufficiently large (at least containing 30 cases) [6]. The mean x . is simply estimated using x . = 1 k biased estimate of the variance is  X   X  2 = We have a statistic approximating the t distribution with df = k  X  1 degrees of freedom Dietterich [2] demonstrated empirically that the k-fold cross validation test has slightly elevated Type 1 error when using the default degrees of freedom. In Section 4, we will calibrate the test by using a range of values for df and selecting the one that gives the desired Type I error. 2.2. Resampling and corrected resampling Resampling is repeatedly splitting the training data randomly in two, training on the first fraction and testing on the remaining section and applying a paired t-test. This used to be a popular way to determine al-gorithm performance before it was demonstrated to have unacceptable high Type I error [2].
 Let P A,j and P B,j be the accuracy of algorithm A and algorithm B measured on run j (1  X  j  X  n ) and x j the difference x j = P A,j  X  P B,j . The mean m of x j is estimated by m = 1 n first estimated using  X   X  2 = unbiased estimate, this is multiplied with 1 n  X  1 . Nadeau and Bengio [7] observe that the high Type I is due to an underestimation of the variance because the samples are not independent. They propose to make a correction to the estimate of the variance, and multiply used for training and n 2 the fraction used for testing. Altogether, this gives a statistic approximating the t distribution with df = n  X  1 degrees of freedom: for resampling, and for corrected resampling. As our experiments will demonstrate, tests from the previous section suffer from low replicability. This means that the result of one researcher may differ from another doing the same experiment with the same data and same hypothesis test but with different random splits of the data. Higher replicability can be expected for hypothesis tests that utilize multiple runs of k-fold cross validation. There are various ways to get the most out of the data, and we will describe them with sults of a 3 run 3-fold cross validation experiment in-side the box at the left half, though in practice a ten run 10-fold cross validation is more appropriate. Each cell contains the difference in accuracy between two algorithms trained on data from all but one folds and measured with the data in the single fold that was left out as test data.
 The use all data approach is a naive way to use this data. It considers the 100 outcomes as independent samples. The mean folds test first averages the cells for a single 10-fold cross validation experiment and considers these averages as samples. In Figure 1 this test would use the numbers in the most right column. These averages are known to be better estimates of the accuracy [5]. The mean runs test averages over the cells with the same fold number, that is, the num-bers in the last row of the top matrix in Figure 1. A better way seem to be to sort the folds before averag-ing. Sorting the numbers in each of the folds gives the matrix at the bottom of Figure 1. The mean sorted runs test then uses the averages over the runs of the sorted folds.
 The mean folds average var test uses an alternative way to estimate the variance from the mean folds test. Instead of estimating the variance directly from these numbers, a more accurate estimate of the variance may
P be obtained by averaging variances obtained from the data of each of the 10-fold cv experiments. The same idea can be extended to the mean run and mean sorted run test, giving rise to the mean run averaged var and mean sorted run averaged var tests.
 The mean folds average T test uses a similar idea as the mean folds averaged var test, but instead of averaging just the variances, it averages over the test statistic that would be obtained from each individual 10 fold experiment. This idea can be extended to the mean run and mean sorted run tests as well, giving the mean run averaged T and mean sorted run averaged T tests. More formally, let there be r runs ( r &gt; 1) and k folds ( k &gt; 1) and two learning schemes A and B that have accuracy a ij and b ij for fold i (1  X  i  X  k ) and run j (1  X  j  X  r ). Let x be the difference between those accuracies, x ij = a ij  X  b ij . We use short notation x Then we use  X   X  2 i. (  X  ) for an overview of the various tests and the way they are calculated. For many of the tests, the mean m and variance  X   X  2 of x are estimated with which the test statistic Z is calculated. As evident from Table 1, the averaged T tests have a slightly different approach. The tests are calibrated on the Type I error by gen-erating 1000 random data sets with mututal indepen-dent binary variables. The class probability is set to 50%, a value that typically generates the highest Type I error [2]. On each of the training sets, two learning algorithms are used, naive Bayes [4] and C4.5 [8] as im-plemented in Weka [11]. The nature of the data sets ensures that none can be outperformed by the other. So, whenever a test indicates a difference between the two algorithms, this contributes to the Type I error. Each test is run with degrees of freedom ranging from 2 to 100. The degrees of freedom at which the Type I error measured is closest and less than or equal to the significance level is chosen as the calibrated value. Note that in our experiments we use stratification for the different runs. Stratification ensures that class val-ues are evenly distributed over the various folds, and tends to result in more accurate accuracy estimates [5]. The left half of Figure 2 shows the degrees of freedom thus obtained for 5, 10, 15, 20, 25, 30, 40, 50, 60, 70, folds where chosen to calibrate because these values are fairly often found in the literature. The right half of Figure 2 shows the corresponding Type I error and Figure 3 shows the legend.
 The following can be observed from Figure 2:  X  The calibrated degrees of freedom for resampling  X  The df for resampling is up to 90% lower than the  X  The df for 10 fold cross validation is constant over  X  Comparing results for 1%, 5% and 10% signifi- X  The df for all other tests varies most for 5 and  X  The df for mean folds is very low (at most 7) and  X  The mean runs is very low, (except for a glitch at  X  The df for sorted runs averaged var and average  X  The df X  X  for  X  X se all data X ,  X  X olds varT X , A number of tests will not be considered further since the low or erratic values for the calibrated df indicate that the df is not the main issue with those tests (ex-amining the results in the following experiments con-firm this, however for clarity of presentation they are omitted here). These tests are mean folds, mean runs, sorted runs averaged var, and sorted runs averaged T. In this section, we study the behavior of the various tests empirically by measuring Type I error for varying class probability, number of runs and significance level. Also Type II error and replicability are measured and some other learning algorithms are considered. 5.1. Vary class probabilities Table 2 shows the Type I error measured using cal-ibrated tests on 1000 syntetic datases with 10 inde-pendent binary variables and a class probability in the range 0.05, 0.1, 0.2, 0.3, 0.4 and 0.5. So, there are a total of 1000 x 6 = 6000 data sets with 300 instances each. Naive Bayes and C4.5 was trained on each of those data sets. The number in the table indicates the number of data sets on which the test indicates that the performance of the algorithms differs. Ten run ten-fold cross validation data was used at 0.05 significance level.
 Except for the averaged T tests, none of the tests have a Type I error exceeding the expected level. In fact, the Type I error tends to be considerably lower with the lower class probabilities. 5.2. Vary number of runs In this and the following sections, four randomly gen-erated Bayesian networks were used to generate 1000 data sets with 300 instances each (see [1] for details) resulting in different performances of naive Bayes and C4.5. Table 3 shows the properties of these data sets when learned on the data sets and measured on a sin-gle 10.000 instances test set. Set 1 is the set used for calibrating the tests.
 Figure 4 shows the Type I error for various numbers of runs of 10 folds cross validation at 0.05 significance level. Since the tests were calibrated on this data set, it comes as no surprise that the Type I error is close and below the targeted 50.
 The power of the tests was measured on data sets where C4.5 outperforms naive Bayes with an increas-ing margin (see Table 3). Figure 4 also shows the power of the tests for data sets 2, 3, and 4. Some observations:  X  Note that the k-fold cv test does not vary with  X  The power of resampling and corrected resam- X  Interestingly, all other calibrated tests that use all Type I error on Set 1 Power on Set 2
Power on Set 3 Power on Set 4  X  Those tests show very little sensitivity to the num-5.3. Vary significance level Datasets 1 to 4 were used to measure the sensitivity of the tests on the significance level. The tests are for 10 run and 10 folds. Table 4 shows the Type I error for significance levels 1%, 2.5%, 5%, and 10%. Not surprisingly, the Type I error is close and below the expected levels, since the tests were calibrated on this set.
 Some observations:  X  Increasing the significance level increases the  X  There is a large gap between the resampled, cor- X  The runs averaged T test is almost always best  X  The use all data test is always in the top 3 of the  X  Overall, all tests (except the first three) have a 5.4. Measuring replicability This section shows results for measuring replicability. Each test was run ten times for each data set with dif-ferent random splits. Table 5 shows the overall repli-cability, defined as the number of times the ten tests all reject or all not reject the null hypothesis. Also, the minimum for the four data sets is listed, which is meaningful because the location where replicability is lowest is typically the area in which decisions need to be made which are not completely obvious.
 Table 6 shows the minimum columns as calculated for Table 5 for significance levels 0.01, 0.025, 0.05 and 0.10.
 Some observations:  X  Again, there is a big difference between the first  X  Judging from the last column of Table 5, use all  X  There is little variance between the replicability  X  As Table 6 shows, the significance level does not The 5x2 cv test has a particular low replicability, which appears clearly for set 4. The conservative nature of the test makes set 4 the set on which the outcome is not always clear (unlike the more powerful tests where set 3 has this role). The reason for the low replicability is that the outcome of the test is dominated by an accuracy estimate based on one outcome of a single run of a 2cv experiment, which can vary greatly (as Table 5 shows). Calibration cannot repair this problem. 5.5. Other learning algorithms Naive Bayes and C4.5 were chosen for calibration be-cause these algorithms are based on completely differ-ent principles so that dependence of learning algorithm influences a test minimally. Further, these algorithms are sufficiently fast to perform a large number of ex-periments. To see how well tests calibrated on those two algorithm perform on other algorithms, we com-pared nearest neighbor, tree augmented naive Bayes and voted perceptron with default settings as imple-mented in Weka [11]. Table 7 shows the Type 1 error at 5% significance level for the 10 run 10 fold use all data test. This table is fairly representative for the other 10x10 fold cv based tests: these tests have a Type 1 error that differs less than 1% in absolute value from that in Table 7. Overall, the Type 1 errors are accept-able, suggesting that the calibrated test can be used on a wider range of algorithms. We studied calibrating the degrees of freedom of hy-pothesis tests as a way to compensate for the difference between the desired Type I error and the true Type I error. Empirical results show that some tests are not calibratable. However, the ones that are calibratable show surprisingly similar behavior when varying pa-rameters such as the number of runs and significance level, which is an indication that an incorrect numer of degrees of freedom indeed is the cause of a difference in observed and theoretical Type I error.
 Furthermore, the calibrated tests show a pleasently high replicability in particular compared to 5x2 cross validation [2], (corrected) resampling [7] and k-fold cross validation.
 For choosing between two algorithms, we recommend using the 10 time repeated 10 fold cross validation test where all 100 individual accuracies are used to esti-mate the mean and variance and with 10 degrees of freedom for binary data. This is conceptually the sim-plest test and has the same properties as the other calibrated repeated k-fold cross validation tests. Fur-thermore, it empirically outperforms 5x2 cross valida-tion, (corrected) resampling and k-fold cross validation on power and replicability. Further emperical research is required to confirm whether this test performs well on non-binary data.
 There are many ways the techniques presented in this article can be generalized. For example, when there is not sufficient data in the data set to justify the nor-mality assumption used for a t-test, a sign test may be applicable. The benefit of sign tests is that no as-sumptions need to be made about the distribution of the variable sampled, which is the difference between accuracies in our case. Calibrated tests could be used for this purpose.
 The main limitation of the pairwise approach is that a choise between only two algorithms can be made. However, in practice multiple algorithms will be avail-able to choose from. This gives rise to the so called multiple comparison problems [3] in choosing learning algorithms. Suppose that all algorithms perform the same on the domain, then the probability that one of those algorithms seem to outperform the others sig-nificantly increases, just like flipping a coin multiple times increases the change of throwing head 5 times in a row. Similar issues arise when comparing two al-gorithms on multiple data sets, or multiple algorithms on multiple data sets.
 Acknowledgements I want to thank Eibe Frank for his very stimulating discussions on this topic and the other members of the machine learning group at the University of Waikato for their encouragements.

