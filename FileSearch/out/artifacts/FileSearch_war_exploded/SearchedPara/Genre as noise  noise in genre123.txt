 ORIGINAL PAPER Andrea Stubbe  X  Christoph Ringlstetter  X  Klaus U. Schulz Abstract Given a specific information need, documents of the wrong genre can be considered as noise. From this pers-pective, genre classification helps to separate relevant docu-ments from noise. Orthographic errors represent a second, finer notion of noise. Since specific genres often include documents with many errors, an interesting question is whe-ther this  X  X icro-noise X  can help to classify genre. In this paper we consider both problems. After introducing a com-prehensive hierarchy of genres, we present an intuitive method to build specialized and distinctive classifiers that also work for very small training corpora. Special empha-sis is given to the selection of intelligent high-level features. We then investigate the correlation between genre and micro noise. Using special error dictionaries, we estimate the typi-cal error rates for each genre. Finally, we test if the error rate of a document represents a useful feature for genre classifi-cation.
 Keywords Genre hierarchies  X  Features  X  Genre classification  X  Error dictionaries  X  Noisy corpora 1 Introduction The technical term  X  X enre X  refers to the partition of docu-ments into distinct classes of texts with similar function and form. When analyzing documents, genre represents an inde-pendent dimension, ideally orthogonal to topic. Traditionally, most of the work in the area of text classification has concen-trated on the problem of how to recognize thematic domains. However, since the genre of a document often gives strong hints on its value for a given user, genre classification also helps to distinguish between  X  X oise X  and  X  X usic, X  X  X etween wanted and unwanted documents.

In the context of documents and genres, the technical term  X  X oise X  has two possible meanings. In a narrower sense, it refers to data contaminated, for example by spelling/typing errors or by errors resulting from OCR recognition. In a wider sense, depending on the task at hand, each genre can represent a class of noisy documents. For example, cooking recipes and forums on fishing represent a kind of  X  X acro-noise X  if someone collects scientific articles on fish. Obviously, clas-sifying genre helps to recognize  X  X acro-noise X . Observing web pages of certain genres, with an eye-catching number of orthographic errors, for example forums, it is a natural ques-tion whether this  X  X icro-noise X  can help to classify genre. Our main contributions are the following: 1. We introduce a fine-grained hierarchy of genres with 2. We present a collection of hand-crafted high-level tex-3. We present a detailed evaluation of the distribution of 4. We show that for a number of genres an automated ana-Our genre hierarchy extends previous work by [ 2 , 4 ]. We tried to reach maximal completeness with regard to general search applications, at the same time avoiding fuzzy and overlap-ping genre classes. With the use of two levels and 32 leaf categories in the genre hierarchy we want to guarantee suf-ficient granularity for practical applications, simultaneously offering the possibility to return to a coarser scheme where this is preferable. Our main application scenarios that moti-vated the construction of the hierarchy were genre-qualified search and genre-specific corpus collection.

Our work on features and classifiers is motivated by the practical experience that standard classifiers based on learning (e.g., support vector machines [ 7 ]) do not lead to satisfactory results if only a small amount of training data is available. In our test, a total of 1,280 files in the com-plete corpus is composed of 40 documents available for each genre. When using 20 documents for training of a genre, standard classifiers and uniform feature sets produced poor results. We were then interested to see if a heuristic classifier based on a small set of features motivated by class-specific knowledge would lead to better results. Considerable effort was put into the selection of powerful features. As another refinement, several methods for combining the classifiers for distinct genres have been tested. For the given scenario, our classifiers in fact outperform standard methods from machine learning. 1
We illuminate the correlation between the genre of a docu-ment and the percentage of orthographic errors found in the texts focusing on spelling and typing errors. For detecting this  X  X icro noise X , we use huge special error dictionaries that capture the main part of errors introduced by the res-pective noisy channels. In fact, the results show a strong correlation between genre and the number of orthographic errors, with a significant trend towards higher error rates in documents that belong to the more private oriented genres. As one application, genres and documents with high error rates can be excluded from corpus construction.

Since some genres typically come with a particularly low or high error rate, it is natural to assume that the error rate of a given document can provide valuable hints on its genre. We use error dictionaries to derive additional classification features and integrate them into our classifiers. Our experi-ments show that in fact for some of the genres the precision of classifiers is improved when using the error rate as a new feature.

The paper is structured as follows. In Sect. 2 , we describe our hierarchy of document genres and introduce the corpora used for our experiments. Section 3 addresses the extrac-tion of genre-specific features and their contribution to the classifiers. In Sect. 4 , we consider strategies for combining the individual classifiers into a decision network. Section 5 describes the construction and application of error dictiona-ries. In Sect. 6 , we describe our experimental results. First we evaluate our genre classifiers over a test corpus of 640 annotated documents, comparing the new technique with tra-ditional methods from machine learning. Two case studies exemplify how genre classification can be fruitfully used in different application scenarios. We then present an evaluation that characterizes the distribution of error rates for orthogra-phic errors in distinct genres. In a final series of experiments we report on the effect of using error rates as an additional feature for genre classifiers. The conclusion summarizes the results and comments on the future work. 2 A hierarchy of genres Starting from a preceding system [ 4 ], we developed a new, finer-grained hierarchy of genres, meeting the demands of genre focused corpus construction and in particular, the fil-tering of noise from a macro perspective. The 11 classes proposed by Dewe et al., were rearranged to eight container classes. We split up the class other running text into the litera-ture genres (B), mail (F.1), and diverse genres for knowledge communication (C); interactive web pages together with dis-cussions and letters were assigned to the container class com-munication (F); private and public homepages were merged into presentation (C.7); error messages, empty pages, and frame sets were put into class  X  X othing X  (G.1). Concerning the second level of the hierarchy, several new genres below the container classes are meant to increase the coverage of the classification. Because of their functional similarity the journalistic genres were additionally scrutinized by an expert leading to minor rearrangements. The final hierarchy is pre-sented in Table 1 .

To better judge its quality and transparency, the hierarchy was evaluated by a non-expert not involved into the construc-tion process. The test person had the task to classify two docu-ments of each genre according to the classification schema. For 76.8% of the texts the human choose the originally tag-ged class. 2.9% of the classifications were also correct, since the documents were mixed documents for example a presen-tation that contains a lot of programming code. 2 For 13.1% the class selected by the test person joined the same container class and was very similar as, for example, a reportage and a feature document. Only 7.2% of the texts were classified in a completely wrong manner. The following confusions were observed: F.4  X  G, C.7  X  G, A.7  X  A.8, A.4  X  F.1 and E.2  X  G. 3
The containers of the hierarchy define a first classification level usable for coarse corpus partition. With regard to clas-sification errors, a hierarchical classification schema helps to evaluate the severity of a false classification: depending on the application, errors that happen within a container can be defined to cause lower loss of accuracy than those that cross the top-level classes.

For each of the 32 genres, 20 English HTML web docu-ments for training and 20 documents for testing were collec-ted, leading to a corpus with 1,280 files. 4
We were careful to gather a broad distribution of topics, authors, and sources for each genre in order to avoid a bias towards specific values of these dimensions. The balanced data set for training and for testing allows us to compare the performance of the different classifiers and abstracts from the specifics of document spaces for different applications with their individual distribution of genres. 3 Genre specific classifiers As we argued above, genre classification helps to recognize unwanted documents. A kernel issue behind document clas-sification is the selection of features. While [ 4 ] and others use global feature sets, we decided to use specialized fea-tures for each genre. The goal was to allow only a small set of significant and natural features for each single classifier. Since training corpora were small, we used human know-ledge on the given genre and tried to avoid effects caused by accidental similarities between documents of distinct genres that result in overfitting. In an iterative process, we investi-gated all training documents for the given genre, identifying important characteristics and sometimes defining clues. 5 evaluated these features for all classes and tried to separate the training files of the chosen genre from the other files by determining thresholds that maximized the F1-value for those features and their combinations. These intuitive hypo-theses (e.g., catalogs indeed contain a lot of prices) were tested on the complete training collection. For classification, features were arranged into a simple decision tree. If the use of a certain feature led to a performance improvement, it was added, otherwise it was discarded. During this process, when a previously acknowledged feature became degraded it was removed. For practical reasons the iteration was terminated when the classifier reached values for recall and precision of about 90% on the training corpus. For some genres which are exceedingly difficult to identify, a threshold for precision of 75% was set. 6 The final result of this procedure is a form of hand-crafted decision tree for each genre.

Many different kinds of features were considered inclu-ding form, vocabulary and parts of speech, complex patterns, and combinations of all these. Fo r m features can be further divided into statistical clues such as average line length or number of sentences, document structure, formatting of the text and HTML meta-information such as content-to-code-ratio. Vocabulary includes specialized word lists as well as dictionaries, for example positive adjectives or the 200,000 most common English words. Also multi word lexemes, bigrams, signs (emoticons) or phrases (such as  X  X o whom it may concern X  in letters) were applied. Patterns include more complex units such as repetitions of characters, dates or bibliographic references. Combinations of these features result in high-level structures. For example a casual style of writing can be recognized by the number of contractions (e.g.,  X  X on X  X  X ) and the use of vague, informal and generalizing words. The occurrence of some kind of agents can be recogni-zed through quotation marks (as only agents can speak), pro-nouns, names and living entities. Sometimes it was necessary to distinguish different styles of writing or structure within genres. Commentaries, for example, can either be polemic pamphlets or could show the pros and cons of a topic. In these cases, we had to construct rules of the form feature-set-1 feature-set-2 . To avoid misclassification, special features that help to separate between similar genres were used.

The classifiers were then constructed as a conjunction of single rules. As an example the classifier of the genre reportage is defined by the following conjunction. 7 Difficulties. The limits of the described method are reached for text documents that neither possess specific structure nor specific vocabulary. Such texts often can only be recognized by POS-characteristics or by the kind of language used. Still, the stylistic differences between two authors can be more severe than those between two genres. Another problem is that certain genres have strong similarities. Examples are commentaries and marginal notes, which both express the opinion of an author in a somewhat casual manner. 4 Classifier combination Endowed with specialized classifiers for each genre, we had to fix their interplay and their global behavior. An evaluation of 10% of our training corpus (two files per class) showed that 22% of the documents show aspects of more than one genre. Therefore, depending on the application it could be better to allow multiple classification. On the other hand, sometimes it might be desirable to have an unequivocal classification, and thus, a decision on the most probable class has to be made. 4.1 Multiple classification The default case of multiple classification is an independent application of all classifiers to an input document. Since each classifier can make a positive decision, a document can end up in more than one class.

Filtering. A variant of multiple classification that exploits knowledge about the interdependencies of the classifiers is filtering. To remove erroneously classified texts of a certain genre from another class, filters can be used. These filters improve the precision of an individual classifier, restricting the set of hits. The filter rules operate as a disqualification cri-terion: if a text has been recognized as A, it cannot be simul-taneously classified as B. This approach is highly efficient if A texts are often erroneously assigned to B, but conver-sely only a few B texts are recognized as A. In order to find appropriate rules, one may compute a confusion matrix on the training data. All classes that are only misclassified in an unidirectional way are suitable for filtering. 4.2 Mono classification One solution is to compute the results for each single clas-sifier and apply well-known techniques such as the behavior knowledge space(BKS) method, to determine the best class [ 6 ]. Instead of computing in advance all the classifications and then filtering the results afterwards, a more efficient alter-native is to determine an evaluation sequence a text has to go through. As soon as a text is classified, the process stops. This procedure prevents multi-classifications, but a poor ordering of the applied classifiers can lead to deterioration of precision and recall. For example, if the classifier for class A leads to wrong classifications of documents that belong to class B, its use before the classifier of class B will lead to lower preci-sion for the classifier of class A and to lower recall for the classifier of class B.

Ordering by F1 value. A possible solution to set up a reasonable unequivocal classification is to use the classifiers in order of their F1 values reached at the training set. 8 underlying idea is that a higher F1 value indicates a higher probability that the classifier will make the correct decision.
Ordering by dependencies and recall. Ordering only by the F1 values misses the possible advantages that result from a reordering triggered by the local dependencies between the classifiers. To determine an improved ordering, a dependency graph is generated. In a first round each docu-ment of the training corpus is classified by all classifiers. By that we get for each classifier N i a value for recall and precision. Additionally we get a confusion matrix for the classifiers.

A first version of the classification sequence is establi-shed by declining recall values with precision as a secondary ordering criterion:
Then with the help of the confusion matrix a dependency graph is generated. When finding texts of class N j misclas-sified as class N i , we create a directed edge from N i to N labelled by the number of missclassified texts. The final ver-sion of the classification sequence is received by rearranging classifiers in the sequence according to their dependencies with their successors: if a classifier N i is followed by N has a dependency edge with N j , N j is put before N i .The procedure is not applied recursively: that means if rearran-ged the dependencies of N j are not taken into account.
For the case of a cycle in the dependency graph, N i is only rearranged if the label of the outgoing edge to N j is higher than that of the incoming edge from N j . An illustrating cut-out of the dependency graph is shown in Fig. 1 . From our training corpus, the following sequence arose.
 Compared to the F1 model, the ordering by dependencies lead to improvements an precision and recall on the training and on the test collection. 9 In our experiments (cf. Sect. 6 ), we used mono-classification, ordered by dependencies and multiple classification with filtering. 5 Finding errors with error dictionaries Our method to investigate the correlation between genre and orthographic errors is based on error dictionaries [ 1 , 13 ]. Assuming that errors in texts result from a structured and elucidable process, it is possible to generate and store errors in a systematic way, applying a generative algorithm to a lan-guage base. In [ 13 ] huge error dictionaries including typing errors, spelling errors and OCR-errors have been employed to estimate the number of orthographic errors in web docu-ments. These dictionaries were found to capture most of the orthographic errors found in the web. In our present study, OCR-errors did not play any role. Hence, regarding the corre-lation between genre and noise we concentrated on the error channels typing and wrong cognitive representation .
Typing errors. Ignoring less important classes, typing errors can be divided into transpositions, deletions, inser-tions, and substitutions [ 10 ]. While transpositions and dele-tions may affect arbitrary symbols, insertions occur when pressing two keys together instead of one. Therefore, any inserted letter is neighbored on the keyboard to one of its adjoint letters in the text. Similarly substitutions only affect two symbols neighbored on the keyboard. Taking these res-trictions into account, we created a rule set for producing typing errors from correct words. These rules were applied to a conventional English dictionary with 100,000 entries (high-frequency words of a larger dictionary). We did not simultaneously apply two rules to a correct word; thus, the tokens that are produced contain exactly one error. We never modified the first letter of a given word. On average, 135 mutations were produced per English input token.

Obviously, when applying an error pattern, a correct word may be produced and thus an additional filtering step is requi-red (s.b.). After deletion of duplicates and correct words (fil-tering), the dictionary of typing errors D err ( English,typing contains 9 , 427 , 051 entries.

Cognitive errors. We define cognitive errors as ortho-graphic errors that result from an incorrect cognitive repre-sentation, caused, for example, by a disagreement between phonetic and orthographic form of a word. To find charac-teristic patterns for such errors, a bootstrapping method was used. Starting from a small set of prominent errors, we collected error prone documents from the web. From these documents, new high-frequent errors were extracted. The bootstrapping was terminated when no new errors with a reasonable frequency were found. From the list of errors, we derived patterns for cognitive errors and built up a produc-tion program for cognitive error dictionaries. Applying the error rules to a standard English dictionary D ( English ) 315 , 300 entries, we obtained a list containing 1 , 223 , garbled tokens. After the standard filtering step (s.b.), the dictionary for cognitive spelling errors, D err ( English,spell is composed of 1 , 202 , 997 entries.
Filtering step. The filtering procedure needs as input an unfiltered error dictionary and in addition a filtering dictio-nary D Filter . For our experiments, D Filter represents the union of diverse conventional dictionaries presented in Table 2 . Each garbled token found in D Filter is excluded from the respective error dictionary. Note that the classification of a token as an error is always related to the applied filter lexicon. This can have profound effects on the values of precision and recall. For example, when analyzing the orthography of mul-tilingual documents, the overgeneration of an error dictionary can be reduced drastically by adding a missing lexicon of one of the involved languages to the filter procedure.
 Detecting and counting errors with error dictionaries. In a study (cf. [ 13 ]) on 1,000 real errors and on 4,000 tokens recognized by the error dictionaries, we found a recall of 62.4% and a precision of 85% for error recognition using our error dictionaries. For the worst documents ( &gt; 10 errors per 1,000 tokens), recall (66.93%) and precision (95.00%) turned out to be higher. The results show that the number of hits of the error dictionary could be seen as a lower approximation of the real number of errors. For English texts, the ratio between both numbers is  X  1 . 4(cf.[ 13 ]). The approximation is more reliable for  X  X ad X  documents with a large number of hits. In what follows, the error rate of a text is defined as the average number of hits of the error dictionary in 1,000 tokens of the text. 6 Experimental results In this section, we present the experimental results. In the first part we evaluate the behaviour of our specialized classifiers for genre classification, comparing them with conventional statistical classifiers. In the second subsection, we study the correlation between genre and the percentage of orthographic errors found in the texts. Finally, we describe experiments of using error rates as an additional feature for genre classifica-tion. 6.1 Genre classification by a combination of specialized In our experiments for genre classification initially we applied the unequivocal classification model (cf. Sect. 4.2). Each document of an evaluation corpus is treated by specialized binary classifiers until a classifier makes a positive decision. The application sequence of the classifiers is controlled by a dependency graph.

Precision and recall are used as evaluation measures for genre classification. An evaluation corpus D ={ D 1 , ..., is a multi set of documents of T different genres, with D { d cision of a classifier for genre i , according to the set of reco-gnized documents C i , are defined as follows:
Precision is the number of documents of genre i in the result set C i divided by the total number of documents in the result set. Precision = | C i  X  D i | | C
Recall is the fraction of documents of genre i in the result set C i divided by the total number of documents of genre i in the evaluation corpus. Recall = | C i  X  D i | | D
In Table 3 , we show a survey of the classification results using the genre-specialized classifiers combined by the dependency graph. The precision of the classification into original classes is 72.2% with an overall recall of 54.0%. The quality of classification differs considerably between certain classes, ranging from an F1 value of 14.7% for mar-ginal notes (A.4) to 100% for  X  X othing X  (G.1). Genres with a definite structural appearance such as directories, poems, FAQ and forums involve certain form features and because of that are better recognized than average. If we consider documents as correctly classified that do not end up in their original class, but in a class that is also well-justified in the sense of a multi-classification (cf. Sect. 4.1) the precision rises to 80.5%. We regard a document as well-justified to suit for a class if it either is a mixture of genre (like a presentation in form of a timeline) or contains a certain amount of mate-rial that belongs to a different genre: for example, a scientific report with a great part of statistical information that has been classified to statistics or a presentation with a great amount of programming code. Reducing the hierarchy to the more coarse-grained first level, we obtain a precision of 77.8%, showing clearly the effect of improvements in classification when using fewer genres.

An analysis of the confusion matrix shows a high quantity of minor classification errors where true class and classifica-tion result are close neighbors. For example, marginal notes are confused with features (4) or commentary (6) X  X ll of them fall into the journalism container and express somehow the view of the author. An excerpt of the confusion matrix pre-sented in Table 4 shows frequently confused genres that lead to more serious classification errors. The given examples of confusion errors show the direction to possible improvements of the classifiers by either maintaining separative ranges of feature values or introducing additional separative features.
Trends on bigger samples. During an application expe-riment (cf. bellow) and a classifier adaption experiment [ 16 ] trends for precision and recall on bigger samples were inves-tigated. These data are to be seen as a complement to the given results since the used corpora are not carefully balanced either with regard to unbiased sources (recall) or to the genre distribution of the corpus (precision). On a 160-document set for each of 4 selected genres we got the following recall values: blog(57.50), catalog(40.00), faq(52.50), interview (55.00). 11 With regard to precision on a corpus of 30,000 webpages measuring on random test samples of 50 docu-ments we got the following values: blog(64.00%), forum (72.00%), interview(56.00%).

Comparison with Machine Learning Methods. For the sake of comparison, several machine-learning (ML) methods have been applied to the data, using as a global feature set the union of all feature sets introduced for the specialized classifiers. 12 The first ML method is the Naive Bayes Clas-sifier using the maximum likelihood expectation criterion to make a decision. The term  X  X aive X  refers to the assump-tion of statistical independence of features, which leads to a simple multiplication of probabilities obtained for the single features. The second method is the decision tree J.48 ,a variant of C4.5, that turns the feature combination into a series of if-then-tests [ 12 ]. With the k-nearest-neighbor algo-rithm (KNN), an object is assigned to the nearest cluster in the feature space. 13 Finally, we applied Support Vector Machines (SVMs) [ 7 ], which divide the data into classes by a separating hyper plane. 14 The SVM was trained by the WEKA implementation of John Platt X  X  sequential minimal optimization algorithm [ 11 ]. Multi class problems are conver-ted to a set of 1-vs-1 classifications (pairwise classification) and combined using pairwise coupling [ 5 ]. In comparison to statistical methods (cf. Table 5 ), our method is superior by 39% in precision and 13% in recall. This result, of course, depends on the small training corpus and we claim superio-rity only under this condition. 15 Still, for many classification tasks it is not realistic to annotate thousands of training docu-ments. Here we consider the proposed method as a strong alternative.
 Comparison with previous work on genre classification. Comparing our results to previously published work, the small size of our training corpora and the high number of possible classes should be emphasized. In [ 3 ], using a trai-ning corpus with 10,000 documents and only 7 genres, an F1 value of 89.1% is reached that sharply decreases with the reduction of training documents. In [ 18 ], a Bayes classifier is used to classify documents into nine classes of the Brown corpus. Recall of 57.8% and precision of 62.2% are reported. In [ 9 ] the influence of the number of genres on classification quality is documented with a decline from 73% precision using four different genres to 52% using 15 Brown catego-ries.

In two application studies, we further tested the strength of our method to filter noise by classifying and excluding undesired genres.

Application scenario 1: Collecting scientific articles on fish. The first study deals with the improvement of the ran-king of a search engine by genre classification. As an applica-tion scenario we assume a user who is interested in scientific articles on fish, which he hopes to extract from the Internet by sending queries like e.g., cod  X  habitat to a search engine. The evaluation runs over the 30 highest-ranked documents of each query. We used ten different queries. In Table 6 ,we present the macrovalues for recall and precision on the ran-ked document sets at cut points 5, 10, 15, 20 and the complete set of 30 documents. 16 We compare the findings of the search engine to the sets reranked by genre recognition. To mark the upper bound, we give values for precision as achieved with a perfect ranking. It turns out that both precision and recall are improved by the genre classification. On the other hand, as the perfect ranking shows, the improvements by far do not reach the upper bound. This gap is caused by the weak recall (40%) of our classifier for science documents.

Application scenario 2: supporting the construction of language models for speech recognition. In a second appli-cation experiment, we collected a corpus for the improve-ment of language models for speech recognition. A serious problem in this domain is that training corpora of spoken language are notoriously sparse. A widely used technique is to extend the spoken material by documents of written text, thus boosting the language models [ 14 ]. A shortfall of this method is that arbitrary written documents are collected, ignoring matters of language style. In our experiments, we collected documents of written text from genres where the use of language is similar to that found in spoken corpora. We approved forum/guestbooks, interviews and blogs using lan-guage similar to that in speech, and tried to exclude all other documents as noise. Sending 200 combined utterances (3 g) of the Verbmobil spoken language corpus [ 17 ] to a search engine, we collected ca. 30,000 web pages. From these, 1,631 were classified as forum/guestbook, 1,327 as interview, and 1,355 as blog. For each genre, a random sample of 50 answer documents was annotated by hand to estimate precision. For forum/guestbook, we obtained a precision of 72%. With 6 blog documents in the sample, this increases to a value of 84% desired documents. By the term secondary precision , we denote the ratio of all desired documents in a sample divided by the sample size. For the interview class, we achieved 56% primary precision and, with 6 forum docu-ments and 7 blog documents, a secondary precision of 82%. The blog genre comes with 64% primary precision containing 13 forum documents and 1 interview document leading to a secondary precision of 92%. Compared to the above results for our test collection, the genre classifiers on average show slightly lower precision, but taking desired genres into account ( interview, blog, forum ), the classifiers work remarkably well. If we approximate the recall for the three desired classes by the recall values obtained for the test collection of our genre corpus, we obtain a reduction of noise in absolute values of 24,000 excluded files or a residue of only 2.5%. 6.2 Correlation between genre and orthographic errors Table 7 shows the mean rate of orthographic errors ( err ) for each of our 32 genres. As we argued earlier, the error rate represents a lower approximation for the real number of errors. In addition, values for the eight container classes are given. We find extraordinary high differences between the genres, and also that significant deviations within the container classes exist. Error rates reach from 0.23 for law to 6.89 for forum/guestbook. In the journalism class, the sub-classes review and interview come with values er r &gt; 2 the container literature, poems are exceptionally erroneous with er r &gt; 5 . 0. In the information class, the two lexica genres have higher error rates. For the documentation contai-ner class, the subclass law X  X ith a mean error rate of only 0.23 X  X s a candidate for classifier tuning by error rate. For the communication container class, the guestbook/forum sub-class has an outstanding error rate. Somewhat surprizingly, the value for blog is nearly as high as the former. Evidently, for some of the blogs, no spellcheckers have been used (s. b.). These two classes also hold the highest rates over the whole classification. Naturally, the guestbook/forum genre is a can-didate for improvement of genre classification by using the error rate of a document as an additional feature.

Since the error rate for blogs was very high, we collec-ted another corpus of 200 blog documents using Google. We found that due to the page-ranking mechanism, we had now much more professional blogs in our selection. Here the mean error rate was 3.03 (standard deviation 2.26). This indicates that for some genres details of the corpus collection have significant influence on the kind of documents that are attracted.

In the right columns of Table 7 we show the mean error rate for 80% of the documents with the lowest error rate. This cut will help to eliminate the outliers with a high devia-tion of the error rate compared to the rest of the class. The relative order between the genres is not changed too drasti-cally. In the  X  X nformation X  container, the FAQ genre moves to a more prominent position, which makes sense since FAQs are usually dynamic, technically oriented web pages, pos-sibly not well maintained from an orthographic point of view.

Figure 2 shows the deviation of error rates between trai-ning and test corpora with remarkable stability for all corpora except  X  X ode X  (C.9). 17
Topicality and Genre. Thinking of corpus collection for computer-aided language learning (CALL), it is important to know whether the mean error rate for documents of a given genre depends on the topics that are covered by the document. If, for example, the error rates of a highly error-prone genre are acceptable for more professional topics, its exclusion from a pedagogical corpus as noise only by genre is not justi-fied. We conducted a prestudy for the genre forum/guestbooks on corpora that cover four distinct topics ranging from hob-bies to science: fish, neurology, mushrooms, and holocaust. With a range of 4.11 X 6.89, the forum genre seems to have high mean error rates for all topics. The corresponding pic-ture for all other genres remains to be studied. 6.3 Using error rate for classification Observing a significant correlation between genre and mean error rate, we tried to exploit this for the improvement of classification. We used the error rate as an additional fea-ture for our genre classifiers. Not surprisingly, an impro-vement was obtained only for some of the genres. For the genres features(A.7), persons(E.1), timeline(E.4), the preci-sion of classifiers could be improved without any loss of recall, see Table 8 . For three other genres that were tested (portrait(A.3), reportage(A.7), presentation(C.7)), classifica-tion results even became worse. A partial explanation is the high variance of error rates. For the statistical classifiers we obtained a similar picture. For example, SVM classification improved for class prosa (B.2) from 65.2 to 71.4% precision. But again for other classes a negative effect was obtained. 7 Conclusion In this paper we showed that genre classification can be successfully applied to compute meaningful partitionings of document repositories. As we indicated in two case studies, a division of documents into genre classes can help to better satisfy the needs of a user or support special corpus construc-tion tasks. We introduced a new fine-grained hierarchy of genres which offers an adequate granularity for a wide range of applications. With the focus on hand-crafted high level fea-tures, a system of classifiers for the hierarchy was designed. We think that the manual and careful design of special fea-tures deserves much more attention in the literature on text classification and machine learning. Our specialized genre classifiers are extremely easy to implement and they work even for very small training corpora.

We also showed that a significant correlation exists bet-ween the genre of a document and its percentage of orthogra-phic errors. Using this knowledge we could further improve the behavior of the classifiers for some genres by using the mean error rate as an additional feature.

In our future work we intend to deepen this picture. For genres where the error rate has a high variance it might be interesting to see if further subdivision into  X  X rofessional-public X  versus  X  X on-professional-private X  subgenres makes sense. We also intend to look at further application scenarios, from ranking of search results to focused corpus construction. References
