 1. Introduction
With the rise of online services such as YouTube, 1 the widespread use of video databases has become a reality. The need has
TRECVid 2007 the best performing automatic run had a Mean Average Precision (MAP) score of 0.088, returning just over 2 rel-the best performing MAP score in ad-hoc text retrieval was 0.374, with on average half of the documents at rank position 10
In recent years there has been an increasing consideration of how the retrieval context , which may include many factors outside of those typically considered in information retrieval, can affect the text and multimedia information retrieval pro-main goal of this paper is to consider a range of contextual factors, and determine the degree to which the different factors contexts: contexts which allow a classifier to more easily learn a model of relevance may then be considered as being more about which contexts make the classification task easier (e.g. enable a more cohesive learning process).
To do this, we consider the retrieval problem as a classification problem and we work with databases constructed from classification problem with a binary class attribute ( X  X  X elevant X  and  X  X  X on Relevant X ), where the documents which will be classified are a set of instances, each one representing a video shot described by a set of features. Formally the problem can be defined as a set of instances C train ={( s 1 , l ), ... ,( s vance; that is, the value of the class attribute for each instance.

In order to avoid problems of over-fitting and erroneous conclusions, evaluations are made over several different configurations:
As it will be explained in more detail in Section 3.2 , we use four different kind of features to represent shots with which the classifier is fed: features representing behavior of users during user study, features representing physical and meta-data information about shots, vocabulary features and windowed vocabulary features extracted from Automatic Speech
Recognition (ASR) in shots. We first measure how well each representation performs, and then we compare those base-lines against the performance when splitting the dataset into contexts.
 We work on two databases generated from the interaction logs of two user studies, which are introduced in Section 3.1 . We use three different classifiers: probabilistic (Naive Bayes), distance-based ( k NN) and vectorial-based (SVM). In one of these databases two different kind of relevance are used: official relevance and relevance as perceived by users. The structure of this paper is as follows: Section 2 summarizes previous work related to context in information retrieval.
Section 3 presents our approach to creating different kinds of features, our selection method, the different contexts created and how we perform the evaluation. The results for the experiments are presented in Section 4 , followed by a discussion of the results in Section 5 . The paper finishes with conclusions and future work. 2. Related work 2007 ). The advances reported in the forums ranged from theoretical such as creating a taxonomy of contextual features, to empirical, such as deriving new context from environments, to constructive such as new applications that exploit context.
Our work aims to make a methodological advance in this area by developing a framework to measure the impact of contex-tual factors. Therefore, this section discusses different approaches taken to measure the impact of context for modeling doc-ument relevancy.

One way to examine the impact of contextual features is to investigate the factors that influenced people X  X  relevance judgements. For example, Barry and Schamber (1998) discussed two sets of semi-structured interviews carried out to estab-lish the criteria used to judge document relevancy. The study identified 10 criterion categories common to both the inter-views. Their results highlighted that people employed non-topical factors such as quality of sources for relevance judgements. Tombros, Ruthven, and Jose (2005) observed interactive search sessions to extract the factors that influenced people X  X  relevance judgements. They identified five groups of influential factors based on 24 participants performing three different search tasks on the Web. Their results suggest that non-textual elements in documents such as structure and visual features are affective to people X  X  relevance assessments.

Another way to examine the impact of contextual features is to investigate their effect on searching behavior. For exam-ple, Kelly and Belkin (2004) studied the effect of tasks and searchers on the reading time of retrieved documents. Their experiments show a significant correlation between the contextual features and searching behavior. Reading time was found to vary across search tasks as well as individual searchers. This suggests that reading time can be unreliable to model rele-vance without context. A similar approach was taken by White, Ruthven, and Jose (2005) who studied the effect of topic complexity, search experience, and search stage in the performance of implicit relevance feedback. Implicit feedback was used to suggest expansion terms in the study. A mixture of measures such as subject assessments, take-up rate of suggested factors affect the utility of implicit relevance feedback.

A distinct approach taken by Fox, Karnawat, Mydland, Dumais, and White (2005) was to model document relevancy predictive Bayesian models. It can be seen that the accuracy of relevance prediction of the models was used as a measure of impact in their study. An advantage of their approach is the number of variables that can be investigated. While the other approaches can examine two or three factors at a time, the classifiers can allow us to investigate a large number of poten-tially effective factors. A disadvantage is that the dependency of features in the generated models is not always clear or interpretable.

Another way to find a dependency between contextual features is to measure the frequency of their co-occurrence in search environments. For example, Freund, Toms, and Clarke (2005) looked at two contextual features, document genres and work tasks, to find the dependency between them. The use of documents in a software engineering workplace was ana-lysed in their study. The experiments show that there is a significant correspondence of document genres with the types of work tasks, suggesting that one can learn relevant genres by understanding the roles and tasks in an organisation.
Compared to these existing studies, we use a set of classifiers to predict video shot relevance. The main focus of this work is the search for training subsets which improve classifier performance, where we perform this search based on different user contexts. Contexts which result in an improvement in classification performance can then be considered as a potential influence on the users retrieval process. 3. Methodology used to split the data into smaller context-dependent data sets for testing (Sections 3.3 X 3.5 then describe the evaluation method used, and the two notions of relevance used. 3.1. Dataset creation
To create the datasets which will be used in the evaluation, we used the interaction logs from two previously conducted an experiment in which several users interacted with a video retrieval system, searching for different topics under different tained verbose data recording the actions each user performed, such as which actions were performed with a shot, of action performed, timestamp, user condition, topic of search, etc.
 which represent the shot, and the class feature itself, i.e.
 Class features are either Relevant or Non Relevant, which refers to the relevance of the shot in the corresponding tuple. appropriate. Since two different user studies are being used, different datasets have been constructed due to the different types of features which were tested to predict relevance (see Section 3.2 ). Additionally since one of the studies did not use the official TRECVid topic descriptions, two different types of relevance have been used (see Section 3.5 ).
In the Collaborative study ( Villa et al., 2008 ), users where grouped into pairs (userA, userB) and searched for relevant shots in a game-like scenario, under four different awareness conditions: 1. Mutual: mutual awareness, where both users could see each other X  X  search screen. 2. Watching: the user would be able to see the other remote user X  X  screen, but not vice-versa. 3. Watched: the inverse of the previous condition, where the user cannot see the remote user, but knows they can see him or her. 4. Independent, where there was no awareness, and both users searched independently.

Thus the four conditions are called Watching, Watched, Mutual, Independent. The Tuple Features constructed from the log of this study are: User, Topic, Condition, and Shot. We remind the reader that Tuple Feature are used to make the context splits, but at the time of performing the evaluation on a dataset they are removed in order to predict relevance solely based upon the constructed Relevance Prediction Features . This study used four of the 24 standard TRECVid topics, and so official TRECVid relevance judgements could be used.

The second user study was the Storyboard study. In this, users interacted with two different interfaces (a common base-line interface and a storyboard-style interface), to search for relevant shots for two different non-TRECVid topics. The
Tuple Features created from these logs are: User, Topic, System, Run and Shot, where System is either the Baseline interface or the experimental Storyboard interface. The Run feature indicates which of the two system was used the first, and which second. 3.2. Relevance prediction features the evaluation and metrics computed were different. As mentioned above, an instance in the final dataset will follow the pattern
We use four different kinds of Relevance Prediction Features : 1. User Behavior Features: user interaction with the interface and the video shots. 2. Object Features: low-level visual features and metadata about the individual video shots. 3. Vocabulary Features: bag-of-word features extracted from Automatic Speech Transcripts (ASR). 4. Windowed-Vocabulary Features: these are very similar to vocabulary features, but where the ASR text used to represent a shot is extended to include neighbouring shots.

Thus, for the logs from each user study four final datasets DS 1, DS 2, DS 3 and DS 4 are derived, where the four datasets contain the same values for Tuple Features and Class Feature but each one contains one of the four types of relevance predic-tion features constructed.

Our User Behavior Features were designed similar to Agichtein, Brill, &amp; Dumais (2006) , and give information about how the user interacts with a document. In our case, the information is related to the actions the user performed through shots suggested by the information retrieval system after he/she has run a query under a topic and condition. Behavior Features used in this work are shown in Table 1 and they can be split into three groups: Click-Through features , which represent information about the clicks the user performed on shots.

Browsing features , which show different metrics about the time spent on shots. and Query-Text features , which count words in the current text query and make comparisons with other text queries.
Note that the values for these features are computed for each tuple from the user X  X  logs.

Object Features are not extracted from the logs. They represent both Low-Level Features extracted for the corresponding video-shot from the most representative key-frame (provided by TRECVid), and Metadata . They are shown in Table 2 .
Using these features, the Relevance Prediction Features describe the shot appearing in Tuple Features . Metadata records information about the length of shots and also information related to the Automatic Speech Recognition (ASR) transcript generated from the shots audio, as provided by TRECVid. It should be noted that text transcripts are filtered through a stop-word list and the Porter stemming algorithm ( Porter, 1997 ) before other text statistics are generated.
Vocabulary Features represent a bag of words created from the ASR. In this case the text is used to create a vocabulary of stemming algorithm to generate the final bag of words. The resulting text is transformed into the Weka format using a tool based on Lucene. 3 For this type of feature, video relevance classification becomes a problem of text classification.
It is expected that video relevance classification based on ASR works relatively well due to the fact that text has more descriptive power than, for example, low-level visual features. However, in the literature some complaints about using
ASR can be found, such as in Yan &amp; Hauptmann (2004) , where the authors state that some speech might not have anything in common with their respective shots.

Finally, Windowed Vocabulary Features refer to a common technique in video retrieval systems which use ASR. This uses the same procedure performed when using Vocabulary Features but in this case the text used to construct the bag of words does not come only from the ASR of the corresponding shot, but also from the n previous and subsequent shots in time. This is called n-Windowed ASR and in our case we use a 6-Windowed Vocabulary. It is expected that 6-Windowed Vocabulary features perform better than creating a bag of words from only the ASR of a single shot. The window size of 6 was based on the work carried out in Urban et al. (2006) .
 When we use ASR to create a bag of words and evaluate using a bayesian classifier, we do not use Naive Bayes but the
Naive Bayes Multinomial, which is recommended for text classification ( McCallum, 1998 ). For the case of Behavior Features, which are continuous values and user dependent, it is unlikely that we will construct a dataset with repeated instances. But, when using Object or Vocabulary Features, the same shot can appear in different tuples so the Relevance Prediction Features are repeated; then we would have several repeated instances in the dataset where the class feature is sometimes set as Rel-most frequent value. 3.3. Splitting data into contexts Each instance in a dataset is made up of where tuple features describe some context information such as: userID, search topic and search condition. All these are nominal features and, by selecting one of these tuple features the dataset can be split into as many possible values as the selected nominal attribute might take. This is what we call an attribute-based context split.

Another context split method is possible, which we refer to as property-based context . For example, the experience con-text can be made up of two subgroups: less experienced users in the user study and more experienced users. This kind of information needs to be obtained via a questionnaire completed by the users. Thus, if a user study is performed with four people and we learn from their answers in the questionnaire that users {id1,id3} are the least experienced meanwhile users {id2,id4} are the most experienced, we can split our dataset based on those two subgroups, using the userID tuple feature.
 uct of the possible splits for each of the chosen contexts. In Tables 3 and 4 contexts constructed for each user study are shown and described.
 3.4. Evaluation method
The baseline in this paper is performed without using the Tuple Features so that the evaluation is totally free of context the different contexts defined for the Collaborative and Storyboard study, respectively; for example, in Table 3 we see that the users in the study who are less experienced, and the other containing shots accessed by more experienced users.
Thus, for each context shown in these tables, each dataset is split into as many possible splits as the corresponding con-text contains. The results obtained for each context will be compared against the previously computed baseline to find how context affect classification. In order to compute what kind of constructed feature works best, baselines with each kind of feature are compared between themselves.

As explained in Fig. 1 , datasets (whole and split) are evaluated by performing 5 2 CV (as suggested in Demsar (2006) for best configuration found) and k NN ( k = 3). As happens in information retrieval systems, datasets are very skewed due to there being a far larger number of non-relevant documents than relevant. Training sets are balanced by randomly deleting as many non-relevant documents as needed so that the classifier is trained with the same number of relevant and non-rel-evant documents. Although not a sophisticated way to balance datasets, in several works ( Japkowicz, 2000 ) it is stated that random re-sampling performs as well as, or even better, than many other kinds of directed balancing.

The performance of classifiers is not computed using MAP since we are not tackling a retrieval problem but rather clas-sification; our aim is not to compare with other retrieval systems but instead compare which predictive features and con-texts better suit, in general, classifiers. Thus, for each 5 2 CV, we compute the F  X  measure represents the harmonic mean of precision and recall. This is computed for relevant documents because for an evant documents.

Although training sets are balanced, test sets are not: if a classifier always marks documents as belonging to the majority class value, accuracy would be incredibly high but documents belonging to the minority class values would never be cor-rectly predicted. For information retrieval systems, documents belonging to the minority class value (relevant documents) should be predicted correctly, so accuracy on its own is not an appropriate metric. 3.5. Types of relevance We have used two sources of information to decide if a shot is relevant for a topic or not: Official Relevance and User
Relevance. The evaluation of a dataset may then be performed twice, once for each kind of relevance. Official Relevance uses the relevant shots for each topic based on the standard information retrieval pooling method of relevance assessment. User
Relevance is computed based on the shots selected explicitly by users during the experiments (users could explicitly mark shots as relevant to the topic, in both the user studies used). A shot can then be considered relevant if the user marks it as such.
 One of the user studies this work (Storyboard) is based on did not use the official TRECVid 2006 topics, and so Official
Relevance for that study cannot be used. The Storyboard dataset is therefore evaluated using only User Relevance, while the Collaborative dataset is evaluated using both User and Official relevance.
 Relevance predictions have a different meaning depending on the kind of features and relevance used. Predicting User
Relevance using User Behavior Features can be seen as predicting explicit user feedback because users marked videos (or not) after interacting with them. Predicting Official Relevance using User Behavior Features predicts the relevance of a shot when feeding our classifier with Object or Vocabulary Features values, we are assuming that low-level features (such as Col-or Layout) are meaningful enough to cross the semantic gap concepts. Similarly, when predicting User Relevance using Object Features, some influence between low-level features and metadata in user perception is assumed. When using Vocabulary Features, relevance prediction is similar to when Object Fea-tures are used, but in this case there is no semantic gap, the problem becoming a text classification task.
Four datasets were created from the Collaborative user study and two datasets from the Storyboard user study. For each for each kind of possible context ( Tables 3 and 4 ). 4. Results
Experiments were carried out on datasets constructed from the logs obtained in two user studies. For each constructed dataset, four new datasets are derived using either User Behavior, Object, Vocabulary or Windowed-Vocabulary Features (see evaluation results for datasets context splits (see Section 4.2 ). 4.1. Results for different types of feature representation
In this section we show the results obtained when performing classification with three different classifiers. Evaluation is performed over the two databases created from the two users studies. For each database, evaluation is performed using one of the four kinds of features, with the Collaborative users study represented twice, one for each kind of relevance. We show the results of the evaluations in Tables 5 X 7 .
 The mean F 1  X  measure for databases using the Behavior Features representation always performs best. We ran a paired Wilcoxon signed rank test ( Wilcoxon, 1945; Demsar, 2006 ) with a = 0.05, using, for each classifier in each table, Behavior
Features as control and comparing it to each other database for the same classifier and table. Input, for each comparison, was the 10 values coming from the 5 2 CV evaluation. When the F nificantly worse than the baseline (Behavior Features with the same classifier), the corresponding cell is marked with a symbol.
 Behavior Features work so well suggests that collaborative information retrieval systems are a good way of improving the relevance prediction performance of IR systems. That is, comparing the behavior of present users with previous users who found a relevant (or not) document, the relevance of this document can be predicted. The tables also suggest that represent-ing documents using extended vocabulary usually improves performance compared with the single-document vocabulary representation.

Considering the results for each classifier we find that SVM does not always perform the best, although we could expect it ulary features representation). We have found the explanation in the balancing process applied during the training stage:
SVM is known to be robust against imbalance situations ( Japkowicz &amp; Shaju, 2002 ), therefore pre-processing the training data before constructing the classifier does not improve the SVM X  X  performance while improving the performance of both
NB and k -NN. Thus NB and k -NN outperform SVM in some occasions. 4.2. Evaluation using context-split datasets
In this section we show the results obtained when performing classification with three different classifiers. Evaluation is performed over two datasets created from the two users studies. For each database, evaluation is performed using one of the three kinds of features, with the Collaborative users study represented twice, once for each kind of relevance. Windowed
Vocabulary features have not been used because they add an extra context factor and we wish to avoid any contextual bias we are trying to compute context influence without any extra factors.

Results are shown in two tables. Table 8 shows results obtained on the Collaborative user study logs, while Table 9 shows results for the Storyboard user study logs. The first row in both tables shows the mean F The remaining rows show results for each dataset split based on the corresponding context.

Independent t -tests with a = 0.05 were performed to compare the baseline with each context result. A comparison is therefore made between two vectors containing a total of 30 samples, where 10 samples come from the results obtained along the performed 5 2 CV using one of the three classifiers. Thus, 30 samples are joint (we are comparing averages) which are discussed in next section, show that contexts clearly affect classifiers X  performance.
 5. Discussion
In this section we discuss the results obtained after splitting and evaluating the presented datasets under different contexts. 5.1. Is it a statistical improvement what we expect for every context split?
There is not a statistical improvements in classifier performance for every context split. On the one hand, it is expected latter type of context can be regarded as a  X  X  X ohesive X  context. Thus, some contexts make the classifier X  X  task easier, while others do not. The aim of this paper is to find what contexts are more cohesive and thus lead to a better classification performance. 5.2. When a split dataset performs statistically better, is it due to context influence or to its smaller size?
When randomly reducing the number of instances from a dataset, we are keeping the same number of properties (attri-and thus the evaluation would commonly decrease. However, if the split is based on some relevant attribute, the entropy of the database is decreased and a better model can be expected to be learned. Thus, splitting a database based on the correct is, a context split will perform statistically better when it is a cohesive context. 5.3. Context influence
This subsection will discuss the results presented in Tables 8 and 9 , pointing out the most interesting results from the experiments. 5.3.1. User experience
It can be seen from Tables 8 and 9 that  X  X  X ore Experienced X  is a cohesive context which helps the classifier learn a better model, for most of the feature types.
 sifier is worse. For the Object/Vocabulary features, shots selected by more experienced users are more cohesive than with less experienced users, the statistics being learned from similar instances thus producing a greater performance.
Considering next official relevance in the Collaborative experiment, only Object features perform statistically different from the baseline, although it is worth mentioning that the F the baseline.
 For the Storyboard experiment ( Table 9 ) we again find a statistical improvement for the  X  X  X ore Experience X  context.
Although this is just for one feature type, again the other two features have a low p -value, still greater than baseline, although not statistically significant. 5.3.2. Task difficulty without context differentiation. This suggests that shots accessed while carrying out easy tasks are more cohesive and pre-dictable for predicting relevance. 5.3.3. Condition
Interesting results were found for this type of context, in both Tables 8 and 9 . For the Collaborative data, results suggest that searching for shots is more cohesive when the user who bookmark X  X  the shot is searching alone (for user relevance).
When using official relevance, performance is better for contexts composed of shots accessed by users helping each other or watching somebody X  X  actions (i.e. the collaborative conditions).

For the Storyboard data, the condition feature can take two values, corresponding to the use of the baseline search inter-face or the storyboard interface. Results suggest that the use of an interface that users are familiar with results in a more coherent and compact search than searches performed using a new novel interface, even though the new interface is in-tended to improve the user interaction. 5.3.4. Other singular contexts
The  X  X  X un X  context (Storyboard study only, Table 9 ) is a quite random and non deterministic context since this is simply the random order in which users used the two possible interfaces. As a consequence, no consistent conclusion can be found nor suggested.

It is surprising that no statistical improvement has been achieved for any of the following context splits:  X  X  X ess Experi-are datasets constructed under contexts which decrease classifiers X  performance. 5.3.5. Mixed contexts
Mixed contexts are more difficult to interpret, but if we focus on searching for corroboration of the conclusions stated above, we find that in fact the same conclusions can be found again in mixed contexts. Thus, in the Collaborative user study, we find that the statistical improvements are again achieved mostly when one of the mixed split contexts is  X  X  X ore Experi-enced X  or  X  X  X asy X . Additionally, in the Storyboard user study, a statistical improvement is achieved for all types of feature when we find together a mix of two cohesive contexts: i.e.  X  X  X aseLine and TaskB X ,  X  X  X aseLine and More Experienced X  and  X  X  X askB and More Experienced X . 6. Conclusions and future work
We have tested four different types of features in the classification domain (with three different classifiers) where the class attribute is binomial with the values {Relevant, Non Relevant}. Additionally, we have searched for cohesive contexts which positively influence the performance of classifiers. In order to not bias the conclusions, we have run our experiments best, which is a promising result for collaborative information retrieval; followed by the Windowed Vocabulary features, although vocabulary is not always available in video documents.
 the authors of this paper presents a comparison of low-level features fusion.

Concerning context, results support the hypothesis that classifiers learn a better model when datasets on which they are evaluated have been constructed using shots accessed in cohesive contexts. Several cohesive contexts have been found; shots accessed by more experienced users, or during an easy task, or using a search interface users are used to, generate cohesive sets of shots which learn a better model.
 tures such as SIFT and SURF features, which are known to perform well. To the best knowledge of the authors X , it is still not known how robust these other types of feature are against different contexts. Finally, since we have shown that there exists searcher? Experienced or a beginner?) from the performance of a classifier which receives as input for training the set of documents the user accesses.
 Acknowledgments
This work has been partially supported by the Junta de Comunidades de Castilla-La Mancha (JCCM) under Project (PCI08-0048-8577), Ministerio de Educacin y Ciencia (MEC) under Project (TIN2007-67418-C03-01), FEDER funds, MIAUCE Project (FP6-033715), and SALERO Project (FP6-027122).
 References
