 Incremental decremental algorithm for online learning of Support Vector Machine (SVM) was pre-viously proposed in [1], and the approach was adapted to other variants of kernel machines [2 X 4]. When a single data point is added and/or removed, these algorithms can efficiently update the trained model without re-training it from scratch. These algorithms are built on an optimization technique called parametric programming [5 X 7], in which one solves a series of optimization prob-lems parametrized by a single parameter. In particular, one solves a solution path with respect to the coefficient parameter corresponding to the data point to be added or removed. When we add and/or remove multiple data points using these algorithms, one must repeat the updating operation learning. In what follows, we refer this conventional algorithm as single incremental decremental algorithm or single update algorithm .
 In this paper, we develop a multiple incremental decremental algorithm of the SVM. The proposed algorithm can update the trained model more efficiently when multiple data points are added and/or removed simultaneously. We develop the algorithm by introducing multi-parametric programming [8] in the optimization literature. We consider a path-following problem in the multi-dimensional space spanned by the coefficient parameters corresponding to the set of data points to be added or removed. Later, we call our proposed algorithm as multiple incremental decremental algorithm or multiple update algorithm .
 The main computational cost of parametric programming is in solving a linear system at each break-point (see Section 3 for detail). Thus, the total computational cost of parametric programming is roughly proportional to the number of breakpoints on the solution path. In the repeated use of single update algorithm for each data point, one follows the coordinate-wise solution path in the multi-dimensional coefficient parameter space. On the other hand, in multiple update algorithm, we the path becomes much shorter than the coordinate-wise one. Because the number of breakpoints in the shorter path followed by our algorithm is less than that in the longer coordinate-wise path, we idea.
 This paper is organized as follows. Section 2 formulates the SVM and the KKT conditions. In Sec-tion 3, after briefly reviewing single update algorithm, we describe our multiple update algorithm. In section 4, we compare the computational cost of our multiple update algorithm with the sin-gle update algorithm and with the LIBSVM (the-state-of-the-art batch SVM solver based on SMO algorithm) in numerical experiments on synthetic and real data sets. We close in Section 5 with concluding remarks. y discriminant function: where  X ( x ) denotes a fixed feature-space transformation. The model parameter w and b can be obtained by solving an optimization problem: where C  X  R + is the regularization parameter. Introducing Lagrange multipliers  X  i  X  0 , the optimal discriminant function f : X  X  R can be formulated as f ( x ) = where K ( x i , x j ) =  X ( x i ) T  X ( x j ) is a kernel function. From the Karush-Kuhn-Tucker (KKT) optimality conditions, we obtain the following relationships: Using (1a)-(1c), let us define the following index sets: In what follows, the subscription by an index set, such as v I for a vector v  X  R n , indicates a subvector of v whose elements are indexed by I . Similarly, the subscription by two index sets, such as M M ; O for a matrix M  X  R n  X  n , denotes a submatrix whose rows are indexed by M and columns are indexed by O . If the submatrix is the principal submatrix such as Q M ; M , we abbreviate as Q M . 3.1 Single Incremental Decremental SVM In this section, we briefly review the conventional single incremental decremental SVM [1]. Using the SV sets (2b) and (2c), we can expand y i f ( x i ) as conditions (1b) and (1d), we need parameters. We maximize the  X   X  c under the constraint that no element moves across M , I and O . optimality condition. Decremental algorithm can be derived similarly, in which the target parameter moves toward 0. 3.2 Multiple Incremental Decremental SVM Suppose we add m new data points and remove  X  data points simultaneously. Let us denote the index set of new adding data points and removing data points as respectively, where |R| =  X  . We remove the elements of R from the sets M , I and O (i.e. M  X  M\R , I  X  X \R and O  X  X \R ). Let us define y = [ y our method corresponds to the conventional single incremental decremental algorithm. We initially them from A because these points already satisfy the optimality condition (1a). Similarly, we can remove the points { i |  X  i = 0 , i  X  X } because they already have no influence on the model. Unlike single incremental decremental algorithm, we need to determine the directions of  X   X  A and  X   X  R . shortest path to 0 , i.e., where  X  is a step length. For  X   X  A , we do not know the optimal value of  X  A beforehand. To determine this direction, we may be able to use some optimization techniques (e.g. Newton method). However, such methods usually need additional computational burden. In this paper, we simply take This would become the shortest path if  X  i = C,  X  i  X  X  , at optimality.
 When we move parameters  X  i ,  X  i  X  X  X  X  , the optimality conditions of the other parameters must be kept satisfied. From y i f ( x i ) = 1 , i  X  X  , and the equality constraint (1d), we need Using matrix notation, (5) and (6) can be written as where satisfied: Since we removed the indices { i | f ( x i )  X  1 } from A , we obtain i , we can append the point to M and remove it from A . On the other hand, if (9) holds until  X  i and (9) is called critical region (CR) .
 We decide update direction by the linear system (7) while monitoring inequalities (8) and (9). Sub-stituting (3) and (4) to (7), we obtain the update direction [ hadamard product  X  (element-wise product [9]), we can write and we define this set as H . We determine the step length as follows: If  X  becomes 1 , we can terminate the algorithm because all the new data points in A and existing points in M , O and I satisfy the optimality conditions and  X  R is 0 . Once we decide  X  , we can update  X  M and b using (10), and  X  A and  X  R using (3) and (4). In the path-following literature, point reaches bound of any one of the constraints (8) and (9) we need to update M , O and I . After updating, we re-calculate  X  ,  X  to determine the next step length. 3.3 Empty Margin We need to establish the way of dealing with the empty margin M . In such case, we can not obtain the bias from y i f ( x i ) = 1 , i  X  X  . Then we can only obtain the interval of the bias from To keep these inequality constraints, the bias term must be in where and If this empty margin happens during the path-following, we look for the new data points which re-enter the margin. When the set M is empty, equality constraint (6) becomes determined when u (  X  ) and l (  X  ) intersect. where We take two different strategies depending on  X  (  X  ) .
 If i low , i up  X  X  X  X  , we can update b and M as follows: By setting the bias terms as above, equality condition is satisfied. If i low  X  X  or i up  X  X  , we can put either of these points to margin. term can be uniquely determined). If we increase  X  , g i changes linearly: boundaries: Figure 1 shows an illustration of these functions. We can trace the upper bound and the lower bound until two bounds become the same value. 3.4 The number of breakpoints The main computational cost of incremental decremental algorithm is in solving the linear system the discussion, let us introduce the following assumptions: Each polygonal region enclosed by dashed lines represents the region in which M , I , O and A The update of matrices and vectors at the breakpoints are the main computational cost of path-algorithm can trace shortest path to optimal point from the origin (left plot). On the other hand, shortest path to  X  1 = 0 ,  X  2 = C (left plot), while single incremental algorithm again moves one coordinate at a time (right plot).
 The first assumption means that the breakpoints are uniformly distributed on the path. The second assumption holds for the removing parameters  X  R because we know that we should move  X  R to 0 . On the other hand, for some of  X  A , the second assumption does not necessarily hold because we do not know the optimal  X  A beforehand. In particular, if the point i  X  X  which was located inside the margin before the update moved to M during the update (i.e. the equality (9) holds), the path with respect to this parameter is not really the shortest one.
 same discussion holds for other cases too). In this simplified scenario, the ratio of the number of breakpoints of multiple update algorithm to that of repeated use of single update algorithm is If we consider only the case of  X  i = C,  X  i  X  X  , the ratio is simply We compared the computational cost of the proposed multiple incremental decremental algorithm (MID-SVM) with (repeated use of) single incremental decremental algorithm [1] (SID-SVM) and with the LIBSVM [10], the-state-of-the-art batch SVM solver based on sequential minimal opti-mization algorithm (SMO).
 In LIBSVM, we examined several tolerances for termination criterion:  X  = 10  X  3 , 10  X  6 , 10  X  9 . When we use LIBSVM for online-learning, alpha seeding [11, 12] sometimes works well. The basic idea of alpha seeding is to use the parameters before the update as the initial parameter. In alpha seeding, we need to take care of the fact that the summation constraint  X   X  y = 0 may not be satisfied after removing  X  s in R . In that case, we simply re-distribute assume that the kernel matrix K is positive definite. If the kernel matrix happens to be singular, which typically arise when there are two or more identical data points in M , our algorithm may not work. As far as we know, this degeneracy problem is not fully solved in path-following literature. Many heuristics are proposed to circumvent the problem. In the experiments described below, we points are generated from a mixture of two Gaussian while the circle points come from a single Gaussian. Two classes have equal prior probabilities. use one of them: adding small positive constant to the diagonal elements of kernel matrix. We set size enough large to store the entire matrix. 4.1 Artificial Data and/or removing points. We generated data points ( x , y )  X  R 2  X { +1 ,  X  1 } using normal distri-butions. Figure 3 shows the generated data points. The size of initial data points is n = 500 . As discussed, adding or removing the data points with  X  i = 0 at optimal can be performed with al-most no cost. Thus, to make clear comparison, we restrict the adding and/or removing points as those with  X  i = C at optimal. Figure 4 shows the log plot of the CPU time. We examined several The horizontal axis is the number of adding and/or removing data points. We see that MID-SVM is significantly faster than SID-SVM. When m = 1 or  X  = 1 , SID-SVM and MID-SVM are identi-cal. The relative difference of SID-SVM and MID-SVM grows as the m and/or  X  increase because MID-SVM can add or remove multiple data points simultaneously while SID-SVM merely iterates the algorithm m +  X  times. In this experimental setting, the CPU time of SMO does not change largely because m and  X  are relatively smaller than n . Figure 5 shows the number of breakpoints of SID-SVM and MID-SVM along with the theoretical number of breakpoints of the MID-SVM in Section 3.4 (e.g., for scenario (a), the number of breakpoints of SID-SVM multiplied by The results are very close to the theoretical one. 4.2 Application to Online Time Series Learning We applied the proposed algorithm to a online time series learning problem, in which we update the model when some new observations arrive (adding the new ones and removing the obsolete ones). daily flow of the river increases or decreases using the previous 7 days temperature, precipitation and flow ( x i  X  R 21 ). This data set contains the observations from Jan 1 1988 to Dec 31 1991. The size of the initial data points is n = 1423 and we set m =  X  = 30 (about a month). Each dimension of x is normalized to [0 , 1] . We add new m data points and remove the oldest  X  data the adding or removing data points by its parameter. Figure 6 shows the elapsed CPU times and C . Figure 6 shows that our algorithm is faster than the others, especially in large C . It is well known that the computational cost of SMO algorithm becomes large when C gets large [14]. Cross-is especially low for the hyperparameters with good generalization performances in this application problem. We proposed multiple incremental decremental algorithm of the SVM. Unlike single incremen-tal decremental algorithm, our algorithm can efficiently work with simultaneous addition and/or removal of multiple data points. Our algorithm is built on multi-parametric programming in the optimization literature [8]. We previously proposed an approach to accelerate Support Vector Re-gression (SVR) cross-validation using similar technique [15]. These multi-parametric programming frameworks can be easily extended to other kernel machines.
