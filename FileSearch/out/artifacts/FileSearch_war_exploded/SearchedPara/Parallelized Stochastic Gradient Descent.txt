 Over the past decade the amount of available data has increased steadily. By now some industrial scale datasets are approaching Petabytes. Given that the bandwidth of storage and network per on communication has become ever more pressing. A simple example illustrates the dilemma. At algorithms from cubic or quadratic to linear in the sample size. However, whenever we have more matter of hours rather than days.
 coupling of the processing units, thus ensuring extremely low latency between the processors. In the number of processors, both in the case of bounded gradients and in the strongly convex case. with considerable latency and bandwidth constraints between the computers.
 A more MapReduce friendly set of algorithms was proposed by [3, 9]. In a nutshell, they rely on distributed computation of gradients locally on each computer which holds parts of the data and terms of computation and network is very high: it requires many passes through the dataset for convergence. Moreover, it requires many synchronization sweeps (i.e. MapReduce iterations). In other words, this algorithm is computationally very wasteful when compared to online algorithms. amount of communication. Unfortunately their proposed algorithm has a number of drawbacks: increasing sample size the guarantees become increasingly loose in practice as we see more data. function indexed by i and with parameter w . Then each processor carries out stochastic gradient Algorithm 1 SGD( { c 1 , . . . , c m } , T,  X  , w 0 ) for t =1 to T do end for return w T .
 On top of the SGD routine which is carried out on each computer we have a master-routine which aggregates the solution in the same fashion as [7].
 Algorithm 2 ParallelSGD( { c 1 , . . . c m } , T,  X  , w 0 ,k ) for all i  X  { 1 , . . . k } parallel do end for
Aggregate from all computers v = 1 Algorithm Latency tolerance MapReduce Network IO Scalability Distributed subgradient [3, 9] moderate yes high linear Distributed convex solver [7] high yes low unclear Multicore stochastic gradient [5] low no n.a. linear This paper high yes low linear A direct implementation of the algorithms above would place every example on every machine: and drawing with replacement (as the theory here implies) and drawing without replacement on the Algorithm 3 SimuParallelSGD( Examples { c 1 , . . . c m } , Learning Rate  X  , Machines k ) Define T = $ m/k %
Randomly partition the examples, giving T examples to each machine. for all i  X  { 1 , . . . k } parallel do end for
Aggregate from all computers v = 1 an anytime algorithm via stochastic gradient descent. The algorithm requires no communication between machines until the end. This is perfectly suited to MapReduce settings. Asymptotically, the error approaches zero. The amount of time required is independent of the number of examples, only depending upon the regularization parameter and the desired error at the end. in the appendix. Before delving into details we briefly outline the proof strategy: 2.1 Loss and Contractions In this paper we consider estimation with convex loss functions c i : # 2  X  [0 ,  X  ) . While our for convenience. For instance, in the case of regularized risk minimization we have where L is a convex function in w  X  x i , such as 1 for binary classification. The goal is to find an approximate minimizer of the overall risk To deal with stochastic gradient descent we need tools for quantifying distributions over w . Lipschitz continuity: A function f : X  X  R is Lipschitz continuous with constant L with respect Lipschitz seminorm: [10] introduce a seminorm. With minor modification we use H  X  older seminorm: Extending the Lipschitz norm for  X   X  1 : Contraction: For a metric space ( M, d ) , f : M  X  M is a contraction mapping if ( f ( In the following we assume that ( L ( x, y, y " ) ( ( x, y )  X  X  X  Y and for all values of w within a suitably chosen (often compact) domain. Theorem 1 (Banach X  X  Fixed Point Theorem) If ( M, d ) is a non-empty complete metric space, then any contraction mapping f on ( M, d ) has a unique fixed point x  X  = f ( x  X  ) . Corollary 2 The sequence x t = f ( x t  X  1 ) converges linearly with d ( x  X  ,x t )  X ( f ( t Our strategy is to show that the stochastic gradient descent mapping Lemma 3 Let c  X   X  ( # # We prove this in Appendix B. If we choose  X   X  X ow enough X , gradient descent uniformly becomes a contraction. We define 2.2 Contraction for Distributions known regarding the number of iterations required until the asymptotic regime is assumed. We now distributions. For this we introduce the Monge-Kantorovich-Wasserstein earth mover X  X  distance. butions over the space. The Wasserstein distance between two distributions X, Y  X  P ( M, d ) is is a contraction mapping on ( P ( M, d ) ,W z ) with constant c .
 is a mapping on distributions.
 Lemma 6 Given a Radon space ( M, d ) , if p 1 . . . p k are contraction mappings with constants c . . . c k with respect to W z , and tion mapping with a constant of no more than [ ! This is proven in Appendix C. We apply this to SGD as follows: Define p  X  = 1 stochastic operation in one step. Denote by D 0 drawn and by D t Then the following holds: tion rate (1  X   X  X  ) . Moreover, there exists a unique fixed point D  X  if This is proven in Appendix F. The contraction rate (1  X   X  X  ) can be proven by applying Lemma 3, Lemma 5, and Corollary 6. As we show later, w t  X  G/  X  with probability 1, so Pr w  X  D  X  W stationary distribution D  X  of c ( w ) . What the result does, though, is establish a guarantee that each computer carrying out good bounds if we can bound the  X  X ias X  and  X  X ariance X  of D  X  2.3 Guarantees for the Stationary Distribution averaged over many samples, the error would be low.
 Proven in Appendix G using techniques from regret minimization. Secondly, we show that the squared distance from the optimal point, and therefore the variance, is low.
 Theorem 10 The average squared distance of D  X  In other words, the squared distance is bounded by O (  X  G 2 /  X  ) . Proven in Appendix I using techniques from reinforcement learning. In what follows, if x  X  M , probability of 1 at x . Throughout the appendix, we develop tools to show that the distribution over the output vector of the algorithm is  X  X ear X   X  D  X  particular, if D T,k of k machines with a learning rate  X  , then W 2 (  X  D  X  that is near to this mean.
 Theorem 11 Given a cost function c such that ( c ( that  X  D and is bounded, then, for any v :
E w  X  D [ c ( w )]  X  min  X  ( W 2 ( v, D )) This is proven in Appendix K. The proof is related to the Kantorovich-Rubinstein theorem, and the main theorem: This is proven in Appendix K. 2.4 Discussion of the Bound dency on the sample size. This is to be expected since we obtained a bound in terms of risk min-imization of the given corpus rather than a learning bound. Instead the runtime required depends only on the accuracy of the solution itself.
 conditioning number  X  = 1 In order to make our error order  X  , we must set k = 1 time.
 roughly doubling T , you can halve the error. Also, the bound captures how much paralllelization Data: We performed experiments on a proprietary dataset drawn from a major email system with normalized to unit length for the experiments. the resulting model. That way, we obtained the performance for the algorithm after each machine studying the performance of the optimization approach.
 Evaluation measures: We report both the normalized root mean squared error (RMSE) on the test set and the normalized value of the objective function during training. We normalize the RMSE value of a single, full sequential pass over the data reaches the value 1 . 0 . Configurations: We studied both the Huber and the squared error loss. While the latter does not 3.1 Results and Discussion 100 machines with  X  =1 e  X  3 . In terms of wall clock time , the models obtained on 100 machines clearly outperform the ones obtained on 10 machines, which in turn outperform the model trained improvement over using one machine than using 100 machines is over 10 .
 Predictive Performance: Figure 2 shows the relative test RMSE for 1 , 10 and 100 machines with gain in performance between 1 and 10 machines is much higher than the one between 10 and 100 . empirical evidence for the behaviour predicted by our theory. is based on a novel technique that uses contraction theory to quantify finite-sample convergence rate of stochastic gradient descent. We show worst-case bounds that are comparable to stochastic constant) benefits more from the parallelization. [1] Shun-ichi Amari. A theory of adaptive pattern classifiers. IEEE Transactions on Electronic [2] L. Bottou and O. Bosquet. The tradeoffs of large scale learning. In Advances in Neural [3] C.T. Chu, S.K. Kim, Y. A. Lin, Y. Y. Yu, G. Bradski, A. Ng, and K. Olukotun. Map-reduce for [4] John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning [5] J. Langford, A.J. Smola, and M. Zinkevich. Slow learners are fast. In Neural Information [6] J. Langford, A.J. Smola, and M. Zinkevich. Slow learners are fast. arXiv:0911.0491, 2009. [7] G. Mann, R. McDonald, M. Mohri, N. Silberman, and D. Walker. Efficient large-scale dis-[8] N. Murata, S. Yoshizawa, and S. Amari. Network information criterion  X  determining the [9] Choon Hui Teo, S. V. N. Vishwanthan, Alex J. Smola, and Quoc V. Le. Bundle methods for [11] M. Zinkevich. Online convex programming and generalised infinitesimal gradient ascent. In
