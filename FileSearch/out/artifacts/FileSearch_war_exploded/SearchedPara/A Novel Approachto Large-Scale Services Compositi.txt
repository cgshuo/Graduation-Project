 As a common understanding, Web services are self-describing and open building blocks for rapid, low-cost composition of distributed applications [8]. In prac-tice, a single Web service may not be suffi cient at performing complex tasks. We usually need to combine multiple existin g services together to meet customers X  complex requests. With today X  X  SOC technology, such composition is usually per-formed by human engineers. However, th e Web environment is highly dynamic, and most Web services are evolving at all time. A service engineer cannot al-ways foresee all the changes that could happen in the future. A manual service composition can also be too rigid to adapt to a dynamic environment. There-fore, dynamic service composition is regarded as a crucial functionality for the Web of the future. Different technologies of computational intelligence have been investigated for solving the probl em of dynamic service composition.
AI planning is a typical type of techniques used to automate Web services composition [9] [2]. Doshi [3] and Gao [4] have studied the application of MDPs (Markov Decision Processes) in Web ser vice composition. MDPs assume a fully observable world and require explicit reward functions and state transition func-tions. Such requirements are too strict to a real world scenario. Wang et al. [11] proposed to use reinforcement learning (RL) for service composition, so as to avoid complex modeling of the real world. Although it has been proved to be effective for small scale service compositions, it can be overly computationally expensive when working on a large number of services.

In this paper, we present a novel mechanism based on multi-agent reinforce-ment learning to enable adaptive service composition. The model proposed in this paper extends the reinforcement l earning model that we have previously introduced in [11]. In order to reduce t he time of convergence, we introduce a sharing strategy to share the policies among the agents, through which one agent can use the policies explored by the others. As the learning process con-tinues throughout the life-cycle of a se rvice composition, the composition can automatically adapt to the change of the environment and the evolvement of its component services. Experimental evaluat ion on large scale service compositions has demonstrated that the proposed model can provide good results. In this section we present the reinforcem ent learning model that we have previ-ously introduced in [11] for solving the d ynamic web service composition prob-lem. The RL model introduced in [11] will be extended in this section towards a distributed architecture.

Reinforcement learning is the problem faced by an agent that must learn behavior through trial-and-error interactions with a dynamic environment [7]. One key aspect of reinforcement learnin g is a trade-off between exploitation and exploration. To accumulate a lot of reward, the learning system must prefer the best experienced actions, however, it has to try new actions in order to discover better action selections for the future. One such method is  X  greedy, when the agent chooses the action that it believes has the best long-term effect with probability 1  X  , and it chooses an action uniformly at random, otherwise. We formally define the key concepts used in the model.
 Definition 1. (Web Service). A Web service is modeled as a triple WS = &lt; Pr ; E ; QoS &gt; ,where  X  Pr represents the precondition of WS , which specifies the states of the world  X  E represents the effect of WS , which describes how WS changes the state  X  QoS is a n -tuple &lt;att 1 ; att 2 ; :::; att n &gt; ,whereeach att i denotes a QoS As mentioned earlier, we use Multi-agent Markov Decision Process (MAMDP) to model service composition. A MAMDP involves multiple actions and paths for each agent to choose. For agent m , we call our service composition model WSC-MDP, which simply replaces the a ctions in a MDP with Web services. Definition 2. (Web service composition MDP (WSC-MDP). AWeb service composition MDP is a 7-tuple WSC-MDP = &lt;m,s m 0 ,S m ,A m ( . ) ,P m ,R m , s r &gt; ,where  X  m : denote the agent m .  X  S m : a finite set of state spaces of agent m .  X  s m 0  X  S m : is the initial state of agent m and also an execution of the service  X  s m r  X  S m : is the set of terminal states for agent m . Upon arriving at one of  X  A m ( . ) : represents the set of Web services that can be executed in state s  X  S m  X  P m :[ p m iaj ] S m  X  A m  X  S m  X  [0 , 1] : The transitions function for the agent.  X  R m :[ r m iaj ] S m  X  R : defines the rewards that agent m receives if it is in A WSC-MDP can be visualized as a transition graph. As illustrated by Fig. 1, the graph contains two kinds of nodes, i.e. state nodes and service nodes, which are represented by open circles and solid circles respectively. s 0( m )represents the initial state node. The terminal states nodes are sr ( m ). A state node can be followed by a number of service nodes, representing the possible services that can be invoked in the state. There is at least one arrow pointing from a service node to the next state node. Each arrow is labeled with a transition probability p iaj , and the expected reward for that transition r the labels in Fig. 1.) The transition probabilities on the arrows rooted at a single action node always sum to one.
 Definition 3. Service Workflow. Let wf be a subgraph of a WSC-MDP. wf is a service workflow if and only if there is at most one service that can be invoked at each state wf . In other words, a service workflow is actually equivalent to a deterministic state machine. A tradition service composition usually builds on a single such workflow.
 Example 1. Fig. 2 shows two of multiple service workflows. Which workflow to be executed is determined by the policy of the Markov decision process. Definition 4. (Policy): A policy  X  is a mapping from state s  X  S to a service ws  X  A , which tells which service ws =( s ) to execute when in state s. Each policy of a WSC-MDP can determine a single workflow. By executing a workflow, the service customer is suppos ed to receive a certain amount of reward, which is equivalent to the cumulative reward of all the executed services. Given a WSC-MDP, the task of our service composition system is to identify the optimal policy or workflow that offers the best cumu lative reward. As the environment of a service composition keeps changing, the transition function P and the reward function R of a WSC-MDP change too. As a result, the optimal policy changes with time. If our system is able to identify the optimal policy at any given time, the service composition will be highly adaptive to the environment. The proposed Distributed Approach The Q-learning is the most popular an d seems to be the effective model-free algorithm about the RL problems. It does not, however, address any of the issues involved in generalizing over large state and/ or action spaces [7]. That is why, in order to speed up the training process, we extend the proposed approach towards a distributed one, in which multiple cooperative agents learn to coordinate in order to find the optimal policy in their environment.

Experience sharing can help agents with similar tasks to learn faster and better. For instance, agents can exchange information using communication [10]. Furthermore, by design, most multiagent systems also allow the easy insertion of new agents into the system, leading to a high degree of scalability [1].
From agent-m X  X  standpoint, its control task could be thought of as an ordinary reinforcement problem except that their action selecting strategy is dependent on other agent X  X  optimal policies at the beginning of the learning. So at a cer-tain state, one agent X  X  policy may be useful to other agents which can help them to find optimal strategy quickly. But assume that each agent can simultane-ously send its current policy to other agents, the communication information is huge. For the purpose of reducing the communication information we don X  X  let the agent to communicate with each others, but introduce supervisor agent which supervises the learning process and synchronizes the computations of the individual agents. In our algorithm, each agent m use the global Q-values esti-mations stored in the blackboard which stores the global Q-values estimations and communicate to the supervisor agent their intention to update a Q-value estimation.

So we have two types of agents in our architecture:  X  WSCA (Web Service Composition Agents). Each WSCA agent runs in a  X  a WSCS(Web Service Composition Supervisor) agent which supervises the In this study, each reinforcement-learning agent uses the one-step Q-learning algorithm. Its learning decision policy is determined by the central Q-table and the state/action value function which estimates long-term discounted rewards for each state/action pair. In order to get more knowledge for the agent and jump out of the sub-superior strategy trap, searching strategy was introduced into the Q-learning. The agent is allowed to take the action which isn X  X  the superior at the current view, so  X   X  greedy strategy was proposed. Thus the agent can explore the state-action space by choose the via ble action randomly a t some degree, and avoid arriving at the local superior solution via only choosing the action with the maximal Q-value.

For agent m , given a current state s and available actions A ( m ), a Q-learning agent selects action a i with a probability given by the rule below: In each time step, the agent m updates Q ( s, a ) by recursively discounting future utilities and weighting them by a positive learning rate  X  : Here  X  (0  X   X &lt; 1) is a discount parameter.

If Q ( s, a ) is the biggest among the actions in state s , than the WSCS will update Q ( s, a ) in the blackboard.

The training process consists of three phases and will be briefly described in the following.
 Phase 1. Initial phase The WSCS supervisor agent initializes the Q-values from the blackboard. Phase 2. Training phase of each WSCA agent During some training episodes, the individual WSCA agents will experiment some paths from the initial to a final state, using the  X  greedy mechanism and updating the Q-values estimations according to the algorithm described below (algorithm 1). We denote in following by Q ( s, a ) the Q-value estimate associated to the state s and a , as stored by the blackboard of the WSCS agent. Algorithm 1. Multiagent Q-learning Algorithm for Agent m Phase 3. Executing phase After the training of the multi-agent system has been completed, the solution learned by the WSCS supervisor agent is constructed by starting from the initial state and following the Greedy mechanism until a solution is reached. The system applies the solution as a service workflow to execute. At the same time, the execution is also treated as an episode of the learning process. The Q-functions are updated afterwards, based on the newly observed reward.

By combining execution and learning, ou r framework achieves self-adaptively automatically. As the environment chang es, service composition will change its policy accordingly, based on its new observation of reward. It does not require prior knowledge about the QoS attributes of the component services, but is able to achieve the optimal execution policy through learning. In order to evaluate the methods, we conducted simulation to evaluate the prop-erties of our service composition mecha nism based on the methods discussed in this paper. The PC configuration: Intel Xeon E7320 2.13GHZ with 8GB RAM, Windows 2003, jdk1.6.0.

We considered two QoS attributes of se rvices. They ware service fee and ex-ecution time. We assigned each service node in a simulated WSC-MDP graph with random QoS values. The value followed normal distribution. To simulate the dynamic environment, we periodically varied the QoS values of existing ser-vices based on a certain frequency. We applied the algorithms introduced in Section 3 to execute the simulated serv ice compositions. The reward function used by the each learner was solely bas ed on the two QoS attributes. After an execution of a service , the learners get a reward , whose value is: The reward was always positive, however service consumers always prefer low execution time and service fee.

We will show that such cooperative agents can speed up learning, measured by the average cumulative values in training, even though they will eventually reach the same asymptotic performance as independent agents. Scalability with respect to the number of services In this stage of our evaluation, we studied the effect of distributed RL approach with varied number of services in each state. We fix the state number on 500 and vary the services from 20 to 200. As the Fig. 3 shows, the distributed RL learns more quickly than Q-learning, an d when the number of services increases, the reducing of convergence time is mo re considerable, because they may have explored the different parts of a state space and share their knowledge. If agents perform the similar task, two agents can complement each other by exchanging their policies or use what the other agent had already learned for its own benefit. Assume that each agent can simultaneously send its current policy at some state to blackboard Q-table by WSCS agent, if some agent finds a better choice, it may update blackboard Q-table through WSCS agent, then other agents can adopt that policy with certain probability in that state.

The results in all cases clearly indicate that distributed RL approach presented in this paper learns more quickly and reduces the overall computational time compared againest the Q-learning. This paper studied a novel framework for large scale service composition. In or-der to reduce the time of convergence we introduce a sharing strategy to share the policies among agents in a team. The experimental results show that the strategy of sharing state-action space improves the learning efficiency signifi-cantly. Additionally, the problem that has to be further investigated is how to reduce the communication cost between the WSCA agents and WSCS agent and explore other local search mechanisms. Next, we will concentrate on these issues and improve our algorithm further.

