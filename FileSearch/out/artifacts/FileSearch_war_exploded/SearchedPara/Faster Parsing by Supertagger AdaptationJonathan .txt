 stephen.clark@cl.cam.ac.uk c  X  In many NLP tasks and applications, e.g. distribu-tional similarity (Curran, 2004) and question an-swering (Dumais et al., 2002), large volumes of text and detailed syntactic information are both critical for high performance. To avoid a trade-off between these two, we need to increase parsing speed, but without losing accuracy.

Parsing with lexicalised grammar formalisms, such as Lexicalised Tree Adjoining Grammar and Combinatory Categorial Grammar ( CCG ; Steed-man, 2000), can be made more efficient using a supertagger . Bangalore and Joshi (1999) call su-pertagging almost parsing because of the signifi-cant reduction in ambiguity which occurs once the supertags have been assigned.

In this paper, we focus on the CCG parser and supertagger described in Clark and Curran (2007). Since the CCG lexical category set used by the su-pertagger is much larger than the Penn Treebank POS tag set, the accuracy of supertagging is much lower than POS tagging; hence the CCG supertag-ger assigns multiple supertags 1 to a word, when the local context does not provide enough infor-mation to decide on the correct supertag.

The supertagger feeds lexical categories to the parser, and the two interact, sometimes using mul-tiple passes over a sentence. If a spanning analy-sis cannot be found by the parser, the number of lexical categories supplied by the supertagger is increased. The supertagger-parser interaction in-fluences speed in two ways: first, the larger the lexical ambiguity, the more derivations the parser must consider; second, each further pass is as costly as parsing a whole extra sentence.

Our goal is to increase parsing speed without loss of accuracy. The technique we use is a form of self-training , in which the output of the parser is used to train the supertagger component. The ex-isting literature on self-training reports mixed re-sults. Clark et al. (2003) were unable to improve the accuracy of POS tagging using self-training. In contrast, McClosky et al. (2006a) report im-proved accuracy through self-training for a two-stage parser and re-ranker.

Here our goal is not to improve accuracy, only to maintain it, which we achieve through an adap-tive supertagger. The adaptive supertagger pro-duces lexical categories that the parser would have used in the final derivation when using the base-line model. However, it does so with much lower ambiguity levels, and potentially during an ear-lier pass, which means sentences are parsed faster. By increasing the ambiguity level of the adaptive models to match the baseline system, we can also slightly increase supertagging accuracy, which can lead to higher parsing accuracy.
Using the parser to generate training data also has the advantage that it is not a domain specific process. Previous work has shown that parsers typically perform poorly outside of their train-ing domain (Gildea, 2001). Using a newspaper-trained parser, we constructed new training sets for Wikipedia and biomedical text. These were used to create new supertagging models adapted to the different domains.

The self-training method of adapting the su-pertagger to suit the parser increased parsing speed by more than 50% across all three domains, with-out loss of accuracy. Using an adapted supertagger with ambiguity levels tuned to match the baseline system, we were also able to increase F-score on labelled grammatical relations by 0.75%. Many statistical parsers use two stages: a tag-ging stage that labels each word with its gram-matical role, and a parsing stage that uses the tags to form a parse tree. Lexicalised grammars typ-ically contain a much smaller set of rules than phrase-structure grammars, relying on tags (su-pertags) that contain a more detailed description of each word X  X  role in the sentence. This leads to much larger tag sets, and shifts a large proportion of the search for an optimal derivation to the tag-ging component of the parser.

Figure 1 gives two sentences and their CCG derivations, showing how some of the syntactic ambiguity is transferred to the supertagging com-ponent in a lexicalised grammar. Note that the lexical category assigned to with is different in each case, reflecting the fact that the prepositional phrase attaches differently. Either we need a tag-ging model that can resolve this ambiguity, or both lexical categories must be supplied to the parser which can then attempt to resolve the ambiguity by eventually selecting between them. 2.1 Supertagging Supertaggers typically use standard linear-time tagging algorithms, and only consider words in the local context when assigning a supertag. The C&amp;C supertagger is similar to the Ratnaparkhi (1996) tagger, using features based on words and POS tags in a five-word window surrounding the target word, and defining a local probability distribution over supertags for each word in the sentence, given the previous two supertags. The Viterbi algorithm Figure 1: Two CCG derivations with PP ambiguity. can be used to find the most probable supertag se-quence. Alternatively the Forward-Backward al-gorithm can be used to efficiently sum over all se-quences, giving a probability distribution over su-pertags for each word which is conditional only on the input sentence.

Supertaggers can be made accurate enough for wide coverage parsing using multi-tagging (Chen et al., 1999), in which more than one supertag can be assigned to a word; however, as more su-pertags are supplied by the supertagger, parsing efficiency decreases (Chen et al., 2002), demon-strating the influence of lexical ambiguity on pars-ing complexity (Sarkar et al., 2000).

Clark and Curran (2004) applied supertagging to CCG , using a flexible multi-tagging approach. The supertagger assigns to a word all lexical cate-gories whose probabilities are within some factor,  X  , of the most probable category for that word. When the supertagger is integrated with the C&amp;C parser, several progressively lower  X  values are considered. If a sentence is not parsed on one pass then the parser attempts to parse the sentence again with a lower  X  value, using a larger set of categories from the supertagger. Since most sen-tences are parsed at the first level (in which the av-erage number of supertags assigned to each word is only slightly greater than one), this provides some of the speed benefit of single tagging, but without loss of coverage (Clark and Curran, 2004).
Supertagging has since been effectively applied to other formalisms, such as HPSG (Blunsom and Baldwin, 2006; Zhang et al., 2009), and as an in-formation source for tasks such as Statistical Ma-chine Translation (Hassan et al., 2007). The use of parser output for supertagger training has been explored for LTAG by Sarkar (2007). However, the focus of that work was on improving parser and supertagger accuracy rather than speed. 2.2 Semi-supervised training Previous exploration of semi-supervised training in NLP has focused on improving accuracy, often for the case where only small amounts of manually labelled training data are available. One approach is co-training , in which two models with indepen-dent views of the data iteratively inform each other by labelling extra training data. Sarkar (2001) ap-plied co-training to LTAG parsing, in which the su-pertagger and parser provide the two views. Steed-man et al. (2003) extended the method to a variety of parser pairs.

Another method is to use a re-ranker (Collins and Koo, 2002) on the output of a system to gener-ate new training data. Like co-training, this takes advantage of a different view of the data, but the two views are not independent as the re-ranker is limited to the set of options produced by the sys-tem. This method has been used effectively to improve parsing performance on newspaper text (McClosky et al., 2006a), as well as adapting a Penn Treebank parser to a new domain (McClosky et al., 2006b).

As well as using independent views of data to generate extra training data, multiple views can be used to provide constraints at test time. Holling-shead and Roark (2007) improved the accuracy of a parsing pipeline by using the output of later stages to constrain earlier stages.

The only work we are aware of that uses self-training to improve the efficiency of parsers is van Noord (2009), who adopts a similar idea to the one in this paper for improving the efficiency of a Dutch parser based on a manually constructed HPSG grammar. The purpose of the supertagger is to cut down the search space for the parser by reducing the set of categories that must be considered for each word. A perfect supertagger would assign the correct cat-egory to every word. CCG supertaggers are about 92% accurate when assigning a single lexical cate-gory to each word (Clark and Curran, 2004). This is not accurate enough for wide coverage parsing and so a multi-tagging approach is used instead. In the final derivation, the parser uses one category from each set, and it is important to note that hav-ing the correct category in the set does not guaran-tee that the parser will use it.

Figure 2 gives an example sentence and the sets of lexical categories supplied by the supertagger, for a particular value of  X  . 2 The usual target of the supertagging task is to produce the top row of categories in Figure 2, the correct categories. We propose a new task that instead aims for the cat-egories the parser will use, which are marked in bold for this case. The purpose of this new task is to improve speed.

The reason speed will be improved is that we can construct models that will constrain the set of possible derivations more than the baseline model. We can construct these models because we can obtain much more of our target output, parser-annotated sentences, than we could for the gold-standard supertagging task.

The new target data will contain tagging errors, and so supertagging accuracy measured against the correct categories may decrease. If we ob-tained perfect accuracy on our new task then we would be removing all of the categories not cho-sen by the parser. However, parsing accuracy will not decrease since the parser will still receive the categories it would have used, and will therefore be able to form the same highest-scoring deriva-tion (and hence will choose it).

To test this idea we parsed millions of sentences in three domains, producing new data annotated with the categories that the parser used with the baseline model. We constructed new supertagging models that are adapted to suit the parser by train-ing on the combination of these sets and the stan-dard training corpora. We applied standard evalu-ation metrics for speed and accuracy, and explored the source of the changes in parsing performance. In this work, we consider three domains: news-wire, Wikipedia text and biomedical text. 4.1 Training and accuracy evaluation We have used Sections 02-21 of CCGbank (Hock-enmaier and Steedman, 2007), the CCG version of the Penn Treebank (Marcus et al., 1993), as train-ing data for the newspaper domain. Sections 00 and 23 were used for development and test eval-uation. A further 113,346,430 tokens (4,566,241 sentences) of raw data from the Wall Street Jour-nal section of the North American News Corpus (Graff, 1995) were parsed to produce the training data for adaptation. This text was tokenised us-ing the C&amp;C tools tokeniser and parsed using our baseline models. For the smaller training sets, sen-tences from 1988 were used as they would be most similar in style to the evaluation corpus. In all ex-periments the sentences from 1989 were excluded to ensure no overlap occurred with CCGbank.
As Wikipedia text we have used 794,024,397 tokens (51,673,069 sentences) from Wikipedia ar-ticles. This text was processed in the same way as the NANC data to produce parser-annotated train-ing data. For supertagger evaluation, one thousand sentences were manually annotated with CCG lex-ical categories and POS tags. For parser evalua-tion, three hundred of these sentences were man-ually annotated with DepBank grammatical rela-tions (King et al., 2003) in the style of Briscoe and Carroll (2006). Both sets of annotations were produced by manually correcting the output of the baseline system. The annotation was performed by Stephen Clark and Laura Rimell.

For the biomedical domain we have used sev-eral different resources. As gold standard data for supertagger evaluation we have used supertagged GENIA data (Kim et al., 2003), annotated by Rimell and Clark (2008). For parsing evalua-tion, grammatical relations from the BioInfer cor-pus were used (Pyysalo et al., 2007), with the Table 1: Statistics for sentences in the supertagger training data. Sentences containing more than 250 tokens were not included in our data sets. same post-processing process as Rimell and Clark (2009) to convert the C&amp;C parser output to Stan-ford format grammatical relations (de Marneffe et al., 2006). For adaptive training we have used 1,900,618,859 tokens (76,739,723 sentences) from the MEDLINE abstracts tokenised by McIn-tosh and Curran (2008). These sentences were POS -tagged and parsed twice, once as for the newswire and Wikipedia data, and then again, us-ing the bio-specific models developed by Rimell and Clark (2009). Statistics for the sentences in the training sets are given in Table 1. 4.2 Speed evaluation data For speed evaluation we held out three sets of sen-tences from each domain-specific corpus. Specif-ically, we used 30,000, 4,000 and 2,000 unique sentences of length 5-20, 21-40 and 41-250 tokens respectively. Speeds on these length controlled sets were combined to calculate an overall pars-ing speed for the text in each domain. Note that more than 20% of the Wikipedia sentences were less than five words in length and the overall dis-tribution is skewed towards shorter sentences com-pared to the other corpora. We used the hybrid parsing model described in Clark and Curran (2007), and the Viterbi decoder to find the highest-scoring derivation. The multi-pass supertagger-parser interaction was also used.
The test data was excluded from training data for the supertagger for all of the newswire and Wikipedia models. For the biomedical models ten-fold cross validation was used. The accuracy of supertagging is measured by multi-tagging at the first  X  level and considering a word correct if the correct tag is amongst any of the assigned tags.
For the biomedical parser evaluation we have used the parsing model and grammatical relation conversion script from Rimell and Clark (2009).
Our timing measurements are calculated in two ways. Overall times were measured using the C&amp;C parser X  X  timers. Individual sentence measurements were made using the Intel timing registers, since standard methods are not accurate enough for the short time it takes to parse a single sentence.
To check whether changes were statistically sig-nificant we applied the test described by Chinchor (1995). This measures the probability that two sets of responses are drawn from the same distribution, where a score below 0.05 is considered significant.
Models were trained on an Intel Core2Duo 3GHz with 4GB of RAM . The evaluation was per-formed on a dual quad-core Intel Xeon 2.27GHz with 16GB of RAM . 5.1 Tagging ambiguity optimisation The number of lexical categories assigned to a word by the CCG supertagger depends on the prob-abilities calculated for each category and the  X  level being used. Each lexical category with a probability within a factor of  X  of the most prob-able category is included. This means that the choice of  X  level determines the tagging ambigu-ity, and so has great influence on parsing speed, ac-curacy and coverage. Also, the tagging ambiguity produced by a  X  level will vary between models. A more confident model will have a more peaked distribution of category probabilities for a word, and therefore need a smaller  X  value to assign the same number of categories.

Additionally, the C&amp;C parser uses multiple  X  levels. The first pass over a sentence is at a high  X  level, resulting in a low tagging ambiguity. If the categories assigned are too restrictive to enable a spanning analysis, the system makes another pass with a lower  X  level, resulting in a higher tagging ambiguity. A maximum of five passes are made, with the  X  levels varying from 0.075 to 0.001.
We have taken two approaches to choosing  X  levels. When the aim of an experiment is to im-prove speed, we use the system X  X  default  X  levels. While this choice means a more confident model will assign fewer tags, this simply reflects the fact that the model is more confident. It should pro-duce similar accuracy results, but with lower am-biguity, which will lead to higher speed.

For accuracy optimisation experiments we tune the  X  levels to produce the same average tagging ambiguity as the baseline model on Section 00 of CCGbank. Accuracy depends heavily on the num-ber of categories supplied, so the new models are at an accuracy disadvantage if they propose fewer categories. By matching the ambiguity of the de-fault model, we can increase accuracy at the cost of some of the speed improvements the new mod-els obtain. We have performed four primary sets of exper-iments to explore the ability of an adaptive su-pertagger to improve parsing speed or accuracy. In the first two experiments, we explore performance on the newswire domain, which is the source of training data for the parsing model and the base-line supertagging model. In the second set of ex-periments, we train on a mixture of gold standard newswire data and parser-annotated data from the target domain.

In both cases we perform two experiments. The first aimed to improve speed, keeping the  X  levels the same. This should lead to an increase in speed as the extra training data means the models are more confident and so have lower ambiguity than the baseline model for a given  X  value. The second experiment aimed to improve accuracy, tuning the  X  levels as described in the previous section. 6.1 Newswire speed improvement In our first experiment, we trained supertagger models using Generalised Iterative Scaling ( GIS ) (Darroch and Ratcliff, 1972), the limited mem-ory BFGS method ( BFGS ) (Nocedal and Wright, 1999), the averaged perceptron (Collins, 2002), and the margin infused relaxed algorithm ( MIRA ) (Crammer and Singer, 2003). Note that these are all alternative methods for estimating the lo-cal log-linear probability distributions used by the Ratnaparkhi-style tagger. We do not use global tagging models as in Lafferty et al. (2001) or Collins (2002). The training data consisted of Sec-tions 02 X 21 of CCGbank and progressively larger quantities of parser-annotated NANC data  X  from zero to four million extra sentences. The results of these tests are presented in Table 2. Sections 02-21 of CCGbank and 4,000,000 NANC sentences.
Using the default  X  levels we found that the perceptron-trained models lost accuracy, disqual-ifying them from this test. The BFGS , GIS and MIRA models produced mixed results, but no statistically significant decrease in accuracy, and as the amount of parser-annotated data was in-creased, parsing speed increased by up to 85%.
To determine the source of the speed improve-ment we considered the times recorded by the tim-ing registers. In Table 3, we have aggregated these measurements based on the change in the pass at which the sentence is parsed, and how the tag-ging ambiguity changes on that pass. For sen-tences parsed on two different passes the ambigu-ity comparison is at the earlier pass. The  X  X otal Time Change X  section of the table is the change in parsing time for sentences of that type when pars-ing ten thousand sentences from the corpus. This takes into consideration the actual distribution of sentence lengths in the corpus.

Several effects can be observed in these re-sults. 72% of sentences are parsed on the same pass, but with lower tag ambiguity (5th row in Ta-ble 3). This provides 44% of the speed improve-ment. Three to six times as many sentences are parsed on an earlier pass than are parsed on a later pass. This means the sentences parsed later have very little effect on the overall speed. At the same time, the average gain for sentences parsed earlier is almost always larger than the average cost for sentences parsed later. These effects combine to produce a particularly large improvement for the sentences parsed at an earlier pass. In fact, despite making up only 7% of sentences in the set, those parsed earlier with lower ambiguity provide 50% of the speed improvement.

It is also interesting to note the changes for sen-tences parsed on the same pass, with the same ambiguity. We may expect these sentences to be parsed in approximately the same amount of time, and this is the case for the short set, but not for the two larger sets, where we see an increase in pars-ing time. This suggests that the categories being supplied are more productive, leading to a larger set of possible derivations. 6.2 Newswire accuracy optimised Any decrease in tagging ambiguity will generally lead to a decrease in accuracy. The parser uses a more sophisticated algorithm with global knowl-edge of the sentence and so we would expect it to be better at choosing categories than the su-pertagger. Unlike the supertagger it will exclude categories that cannot be used in a derivation. In the previous section, we saw that training the su-pertagger on parser output allowed us to develop models that produced the same categories, despite lower tagging ambiguity. Since they were trained on the categories the parser was able to use in derivations, these models should also now be pro-viding categories that are more likely to be useful.
This leads us to our second experiment, opti-Table 5: Cross-corpus speed improvement, models trained with able from it. mising accuracy on newswire. We used the same models as in the previous experiment, but tuned the  X  levels as described in Section 5.1.

Comparing Tables 2 and 4 we can see the in-fluence of  X  level choice, and therefore tagging ambiguity. When the default  X  values were used ambiguity dropped consistently as more parser-annotated data was used, and category accuracy dropped in the same way. Tuning the  X  levels to match ambiguity produces the opposite trend.
Interestingly, while the decrease in supertag ac-curacy in the previous experiment did not translate into a decrease in F-score, the increase in tag accu-racy here does translate into an increase in F-score. This indicates that the supertagger is adapting to suit the parser. In the previous experiment, the supertagger was still providing the categories the parser would have used with the baseline supertag-ging model, but it provided fewer other categories. Since the parser is not a perfect supertagger these other categories may in fact have been incorrect, and so supertagger accuracy goes down, without changing parsing results. Here we have allowed the supertagger to assign extra categories, which will only increase its accuracy.

The increase in F-score has two sources. First, our supertagger is more accurate, and so the parser is more likely to receive category sets that can be combined into the correct derivation. Also, the su-pertagger has been trained on categories that the parser is able to use in derivations, which means they are more productive.

As Table 6 shows, this change translates into an improvement of up to 0.75% in F-score on Section Table 6: Evaluation of top models on Section 23 of CCGbank. All changes in F-score are statistically significant. 23 of CCGbank. All of the new models in the table make a statistically significant improvement over the baseline.
 It is also interesting to note that the results in Tables 2, 4 and 6, are similar for all of the train-ing algorithms. However, the training times differ considerably. For all four algorithms the training time is proportional to the amount of data, but the GIS and BFGS models trained on only CCGbank took 4,500 and 4,200 seconds to train, while the equivalent perceptron and MIRA models took 90 and 95 seconds to train. 6.3 Annotation method comparison To determine whether these improvements were dependent on the annotations being produced by the parser we performed a set of tests with supertagger, rather than parser, annotated data. Three extra training sets were created by annotat-ing newswire sentences with supertags using the baseline supertagging model. One set used the one-best tagger, and two were produced using the most probable tag for each word out of the set sup-plied by the multi-tagger, with variations in the  X  value and dictionary cutoff for the two sets. Table 8: Comparison of annotation methods for extra data. The multi-taggers used  X  values 0.075 and 0.001, and dictionary cutoffs 20 and 150, for taggers a and b respectively.
 Table 9: Cross-corpus speed for the baseline model on data sets balanced on sentence length.
As Table 8 shows, in all cases the use of supertagger-annotated data led to poorer perfor-mance than the baseline system, while the use of parser-annotated data led to an improvement in F-score. The parser has access to a range of infor-mation that the supertagger does not, producing a different view of the data that the supertagger can productively learn from. 6.4 Cross-domain speed improvement When applying parsers out of domain they are typ-ically slower and less accurate (Gildea, 2001). In this experiment, we attempt to increase speed on out-of-domain data. Note that for some of the re-sults presented here it may appear that the C&amp;C parser does not lose speed when out of domain, since the Wikipedia and biomedical corpora con-tain shorter sentences on average than the news corpus. However, by testing on balanced sets it is clear that speed does decrease, particularly for longer sentences, as shown in Table 9.

For our domain adaptation development ex-periments, we considered a collection of differ-ent models; here we only present results for the best set of models. For speed improvement these were MIRA models trained on 4,000,000 parser-annotated sentences from the target domain.
As Table 5 shows, this training method pro-duces models adapted to the new domain. In par-ticular, note that models trained on Wikipedia or the biomedical data produce lower F-scores 3 than the baseline on newswire. Meanwhile, on the target domain they are adapted to, these models achieve a higher F-score and parse sentences at least 45% faster than the baseline.

The changes in tagging ambiguity and accuracy also show that adaptation has occurred. In all cases, the new models have lower tagging ambi-guity, and lower supertag accuracy. However, on the corpus of the extra data, the performance of the adapted models is comparable to the baseline model, which means the parser is probably still be receiving the same categories that it used from the sets provided by the baseline system. 6.5 Cross-domain accuracy optimised The ambiguity tuning method used to improve ac-curacy on the newspaper domain can also be ap-plied to the models trained on other domains. In Table 7, we have tested models trained using GIS and 400,000 sentences of parsed target-domain text, with  X  levels tuned to match ambiguity with the baseline.

As for the newspaper domain, we observe in-creased supertag accuracy and F-score. Also, in almost every case the new models perform worse than the baseline on domains other than the one they were trained on.

In some cases the models in Table 7 are less ac-curate than those in Table 5. This is because as well as optimising the  X  levels we have changed training methods. All of the training methods were tried, but only the method with the best results in newswire is included here, which for F-score when trained on 400,000 sentences was GIS .

The accuracy presented so far for the biomedi-Table 10: Performance comparison for models us-ing extra gold standard biomedical data. Models were trained with GIS and 4,000,000 extra sen-tences, and are tested using a POS -tagger trained on biomedical data. cal model is considerably lower than that reported by Rimell and Clark (2009). This is because no gold standard biomedical training data was used in our experiments. Table 10 shows the results of adding Rimell and Clark X  X  gold standard biomedi-cal supertag data and using their biomedical POS -tagger. The table also shows how accuracy can be further improved by adding our parser-annotated data from the biomedical domain as well as the additional gold standard data. This work has demonstrated that an adapted su-pertagger can improve parsing speed and accu-racy. The purpose of the supertagger is to re-duce the search space for the parser. By train-ing the supertagger on parser output, we allow the parser to reach the derivation it would have found, sooner. This approach also enables domain adap-tation, improving speed and accuracy outside the original domain of the parser.

The perceptron-based algorithms used in this work are also able to function online, modifying the model weights after each sentence is parsed. This could be used to construct a system that con-tinuously adapts to the domain it is parsing.
By training on parser-annotated NANC data we constructed models that were adapted to the newspaper-trained parser. The fastest model parsed sentences 1.85 times as fast and was as accurate as the baseline system. Adaptive train-ing is also an effective method of improving per-formance on other domains. Models trained on parser-annotated Wikipedia text and MEDLINE text had improved performance on these target do-mains, in terms of both speed and accuracy. Op-timising for speed or accuracy can be achieved by modifying the  X  levels used by the supertagger, which controls the lexical category ambiguity at each level used by the parser.

The result is an accurate and efficient wide-coverage CCG parser that can be easily adapted for NLP applications in new domains without manu-ally annotating data.
 We thank the reviewers for their helpful feed-back. This work was supported by Australian Re-search Council Discovery grants DP0665973 and DP1097291, the Capital Markets Cooperative Re-search Centre, and a University of Sydney Merit Scholarship. Part of the work was completed at the Johns Hopkins University Summer Workshop and (partially) supported by National Science Founda-tion Grant Number IIS-0833652.

