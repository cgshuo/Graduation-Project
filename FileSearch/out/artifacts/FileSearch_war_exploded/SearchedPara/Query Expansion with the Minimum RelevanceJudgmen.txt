 Query expansion is a simple but very useful technique to improve search perfor-mance by adding some terms to an initial query. While many query expansion techniques have been proposed so far, a standard method of performing is to use relevance information from a user [1]. If we can use more relevant documents in query expansion, the likelihood of selecting query terms achieving high search improvement increases. However it is impractical to expect enough relevance information. Some researchers said that a user usually notifies few relevance feedback or nothing [2].
 under the condition that we can utilize little relevance information, especially we only know a relevant document and a non-relevant document. To overcome the lack of relevance information, we tentatively increase the number of relevant documents by a machine learning technique called Transductive Learning .Com-pared with ordinal inductive learning approach, this learning technique works even if there is few training examples. In our case, we can use many documents in a hit-list, however we know the relevancy of few documents. When applying query expansion, we use those increased documents as if they were true relevant ones.
 of relevance information in practical situations. There are several researches which deal with this problem. Pseudo relevance feedback which assumes top n documents as relevant ones is one example. This method is simple and relatively effective if a search engine returns a hit-list which contains a certain number of relative documents in the upper part. However, unless this assumption holds, it usually gives a worse ranking than the initial search. Thus several researchers propose some specific procedure to make pseudo feedback be effective [3,4]. In another way, Onoda [5] tried to apply one-class SVM (Support Vector Machine) to relevance feedback. Their purpose is to improve search performance by using only non-relevant documents. Though their motivation is similar to ours in terms of applying a machine learning method to complement the lack of relevance information, the assumption is somewhat different. Our assumption is to utilizes manual but the minimum relevance judgment.
 [6]. In this research, they proposed a transductive method called the manifold-ranking algorithm and showed its effectiveness by comparing with active learning based Support Vector Machine. However, their setting of relevance judgment is not different from many other traditional researches. They fix the total number of images that are marked by a user to 20. As we have already claimed, this setting is not practical because most users feel that 20 is too much for judgment. We think none of research has not yet answered the question. For relevance judgment, most of the researches have adopted either of the following settings. One is the setting of  X  X nough relevant documents are available X , and the other is  X  X o relevant document is available X . In contrast to them, we adopt the setting of  X  X nly one relevant document is available X . Our aim is to achieve performance improvement with the minimum effort of judging relevancy of documents. two fundamental techniques for our query expansion method. Section 4 explains a technique to complement the smallness of manual relevance judgment. Section 5 introduces a whole procedure of our query expansion method step by step. Section 6 shows empirical evidence of the effectiveness of our method compared with two traditional query expansion meth ods. Section 7 investigates the exper-imental results more in detail. Finally, Section 8 summarizes our findings. 2.1 Query Expansion The main objective of query expansion is to select additional terms of achieving better search results. From where and how to choose such terms differentiate many query expansion techniques which have been proposed so far. For example, a method for domain specific search prepares documents in a certain domain and pick up terms from them as a batch procedure [7,8]. In another case, a method for ad-hoc search usually selects terms from documents at the head of an initial search result. This approach further branches off to the utility of manual or automatic feedback. Anyway, most of the methods first score each term in a certain set of documents and then choose some best scored terms for expansion. search with manual feedback. In this approach, there is a well-known query expansion method called the Robertson X  X  wpq method [1] which is used in many researches [3,4]. Our method is based on this one. The wpq selects expansion terms using the following scoring function. where r t is the number of seen relevant documents containing term t . n t is the number of documents containing t . R is the number of seen relevant documents for a query. N is the number of documents in the collection. The second term of this formula is called the Robertson/Spark Jones weight [9] which is the core of the term weighting function in the Okapi system [10]. This function is originated in the following formula. where p t is the probability that a term t appears in relevant documents. q t is the probability that a term t appears in non-relevant documents. If we estimate p for q t . In contrast, it is not so easy to give a good estimation for p t . Since users in practical situations do not give much feedback, R tends to be very small and this fact produces two problems. One is the lack of term variety. Candidates for expansion terms are limited. The other is the scoring ability of r t R .Since r t is small if R is small, many terms come to have the same score. The challenge is to increase the number of R . Although pseudo feedback which automatically assumes top n documents as relevant is one solution, its performance heavily depends on the quality of an initial search. As we show later, pseudo feedback has limited performance.
 smallness of R with a transductive learning technique, which is used to find doc-uments possibly being relevant based on a set of training examples 1 . Because we want to consider an assumption not far from practical situations, we restrict the number of training examples to the minimum -a relevant document and a non-relevant document. Of course, manual judgment is an advantage to the pseudo one. However this minimum information has no utility for the wpq method. Per-formance improvement depends on the accuracy of the judgment assigned by the learning to each document with no manual judgment. 2.2 Transductive Learning Transductive learning is a machine learning technique based on the transduction which directly derives the classification labels of test data without making any approximating function from training data [11]. This learning technique is based on an assumption that two similar data are likely to have the same class label. If we can define a reasonable similarity between each element of a data set, this learning works well even if the number of training examples is small. l u . The purpose of the learning is to assign a label to each element in U under the condition that the label of each element in L are given.
 rithms which are based on the solution for graph cutting problems [12,13,14]. According to the experimental results in [14], these algorithms do not have so much performance difference that we select an algorithm called  X  Spectral Graph Transducer (SGT) X  for our query expansion method. The SGT formalizes a learning task as an optimization problem of the constrained ratiocut. By solving the relaxed problem, it produces an approximation to the original solution. documents in a hit-list. Because the number of documents in a collection is usu-ally too huge 2 , n should be set to a moderate number. L corresponds to two documents with manual judgments, a relevant document and a non-relevant document. Furthermore, U corresponds to the documents of X  X  L whose rele-vancy is unknown. In the learning process, first SGT makes an undirected graph where a vertex corresponds to a document in X and an edge represents similarity between vertices. For each vertex, edges to most k similar vertices are created in the graph. The problem here is how to partition it to two parts where one part includes only positive examples (relevant documents) and the other includes only negative examples (non-relevant documents). SGT formalizes it as the following constrained ratiocut problem. (3) represents a cut of a k -nearest graph described above. Although we can solve the learning task as a simple mincut problem, there is a risk that an unbalanced label assignment is produced as Joachims points out. Because such an assignment is not likely to be a good solution, SGT introduces a constraint in the denominator of the formula (3) to produce a more balanced label assignment. This new problem is hard to solve as it is, thus SGT gives an approximation to the solution by solving its relaxed problem. We omit the details about its concrete solution (See more details in [12]). At final stage of SGT, it assigns a value around  X   X  + =+ 1  X   X  p  X  p for examples possibly being positive and  X   X   X  =+  X  p 1  X   X  p for examples possibly being negative. Here  X  p is an estimate for the fraction of positive examples in X . According to Joachims, SGT has several parameters which give large influence to its learning performance. In particular, performance of our query expansion method is very sensitive to  X  p . We next explain how to relax this sensitivity. If we use the scoring function in the formula (1), we have to assign a binary label (1 for a relevant document and 0 for a non-relevant) to each document based on a value assigned by SGT. This means that it is necessary a certain threshold to make hard class assignment. SGT now tentatively use a threshold  X  =  X   X  + + X   X   X  2 . However,  X   X  + and  X   X   X  are sometimes not reliable because  X  p is difficult to estimate in our setting. Accordingly, instead of b inary labels, we use a scoring function in formula (2) with another estimation of  X  p t . SGT finally assigns a value z i = X   X  +  X   X  or  X   X   X   X   X  which distributes around 0 to each document d i .If z i is positive, the corresponding document seems to be relevant with strong possibility. Similarly, if the value is negative, the corresponding document seems to be non-relevant with strong possibility.
 arealnumberoflessthan1.0toeachexamplein X . Because it is important to make a loose threshold allowing examples to which SGT acutually assigns negative values, following functions are tested as representatives in our research. Figure 1 show the shape of each function. 1. Step function (SGT-step) 2. Partially Linear function (SGT-linear) 3. Partially normal distribution function (SGT-ndist) The first function just shifts the original threshold. The second and the third ones set a loose threshold which includes s ome examples assigned negative values by SGT. Using values produced by one of these functions, our method estimates p in the following way. The difference As described before, we estimate q t with n t  X  r t N  X  R where R is a set of documents d i whose z i is positive. We here explain a whole procedure of our query expansion method step by step. 1. Initial Search : A retrieval starts by inputting a query for a topic to an IR 2. Relevance Judgment for Documents in a Hit-List: The IR system 3. Finding More Relevant Documents by Transductive Learning: Be-4. Selecting Terms to Expand the Initial Query: Our query expansion 5. The Next Search with an Expanded Query: The expanded query is This section provides empirical evidence on how our query expansion method can improve the performance of information retrieval. We compare our method with other traditional methods. 5.1 Environmental Settings We use the Okapi [10] as a retrieval system and a data set for the TREC-8 adhoc task [15].
 each document in a collection based on the following formula.
 where Q is a query containing terms T , tf is the frequency of occurrence of the term within a document, qtf is the frequency of the term within the topic from which Q was derived. K is calculated by (12), where dl and avdl denote the document length and the average document length measured in some suitable unit, such as word or sequence of words. In our experiments, we set k 1 =1 . 2 ,k 3 = 1000 ,b =0 . 75, and avdl = 135 . 6. w (1) is the Robertson/Spark Jones weight introduced in section 2. When doing an initial search for each topic, this weight is calculated by the following formula. a list of manual relevance judgment. The document collection contains about 520,000 news articles. Each document is preprocessed by removing stopwords and stemming. Query terms for an initial search are nouns extracted from the title tag in each topic X  X  description. Some topics have few relevant documents or too much relevant documents. We remove such topics having none of relevant or non-relevant document within top 10 documents because we cannot apply our query expansion method for such topics. There are 8 such exceptive topics in our experiments. 5.2 Query Expansion Methods to Compare We compared our query expansion method with the following two others. Normal : This method simply uses only one relevant documents judged by Pseudo : This method is called pseudo relevance feedback , which assumes top According to the difference of term scoring scheme, we test four types of SGT-based query expansion methods. They are represented by SGT-step-0 , SGT-step- X  , SGT-linear and SGT-ndist respectively. SGT-step-0 and SGT-step- X  differs in each value of  X  .  X  = 0 for the former, and  X  in the latter is the 30th largest value in all of z i . The latter is another kind of Pseudo method mixed by SGT.
 5.3 Results We evaluated the results in two ways. One is 11 points average precision (in Table 1). The other is precision-recall curve (in Fig 3). The number of expan-sion terms we tested are 5,10,15 and 20. Precision and recall are calculated on residual collection where documents with manual judgment (2 documents in our case) are removed from an original collection. The value in the table is aver-aged over 42 topics. The number of documents used for SGT is 100. Since 2 documents are given as training examples, the rest 98 documents are used as test examples. SGT has several parameters to be set such as fraction of relevant documents, number of nearest neighbor in a graph, number of eigen values to use and so on. Although a default value or an automatic calculation procedure is prepared for each parameter, the parameter of fraction of relevant documents does not work well without manual setting because the number of training exam-ples is too few. We set this parameter 0.1 for all topics based on our preliminary test.
 precision compared with Normal and Pse udo methods. As for the usefuleness of functions for the estimation of p t , SGT-step-0 is slightly less than the other three SGT-based methods. Thus we can say that the functions are effective. However any distinctive advantage could not be seen among three functions. Since the number of expansion terms did not affect retrieval performance, we only make precision-recall curves when 5 expansion terms are added as shown in Figure 3. Curves of SGT based methods did not across over other curves at any point. This indicates the superiority of our query expansion method. Figure 4 shows a bar graph of difference of 11 points average precision between SGT-step- X  and Pseudo for each topic when 5 expansion terms are added. SGT-step- X  is superior to Pseudo if the difference is positive, and vice versa. As shown in the figure, there are some topics for which SGT-step- X  gives worse performance than Pseudo. Because this result is related to the number of rel-evant documents used for query expansion, we also investigated a difference of the number of relevant documents used for query expansion in Figure 5. When SGT-step- X  has less relevant documents than Pseudo, the performance is also less than Pseudo except for some topics. The reason SGT-step- X  has less rele-vant documents than Pseudo is a miss setting of the fraction of relevant docu-ments which is a parameter of SGT. It is better to set a correct value for the fraction of relevant documents to SGT, however estimating the value is not so easy.
 traditional methods from several points of view. However we have another ques-tion that query expansion procedure is necessary because SGT can re-order documents by itself. In order to answer this question, we compare recall of top n ( n =10  X  100) documents in a hit-list re-ordered by SGT itself and SGT-based query expansion method (SGT-step- X  ) as shown in Table 2. As shown in the table, recall rates of SGT itself is lower than the SGT-based query expansion method. This shows that SGT itself cannot work well. Under the condition of the minimum relevance judgment, SGT is effective if being mixed with our specific procedure.
 In this paper we proposed a novel query expansion method which only use the minimum manual judgment. To complement the lack of relevant documents, this method utilizes the SGT transductive learning algorithm to predict the relevancy of unjudged documents. Since the performance of SGT much depends on an estimation of the fraction of relevant documents, we introduced a modified term scoring scheme which actually changes the thresholding procedure of SGT. The experimental results showed our method outperforms other traditional methods in the evaluations of precision and recall criteria. Though our modified term scoring scheme could relax SGT X  X  parameter sensitivity described above in some degree, we have more chance to improve the performance by removing more SGT X  X  parameter dependencies.

