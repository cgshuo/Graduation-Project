 As a fundamental and effective tool for efficient organization, summarization, naviga-tion, and retrieval of large amount of documents, document clustering has been exten-sively studied for decades. Most of the research efforts on document clustering have been focused on developing efficient and effective algorithms and on algorithm-level improvements. In particular, many new algorithms have been developed to address the challenges in clustering documents such as large volume, high dimensionality, sparsity, and complex semantics and to improve clustering performance [Andrews and Fox 2007].

However, relatively few research efforts have been reported on evaluating and un-derstanding document clustering results. To evaluate the document clustering per-formance, existing methods typically compute and output some quantitative validity measures [Tan et al. 2006; Halkidi et al. 2001]. These measures include (1) internal indices which use only the information present in the dataset, such as cluster sepa-ration and cohesion; and (2) external indices which use the externally derived label information (the ground truth) and evaluate the  X  X greement X  between the clustering results and the ground truth.

Although these numeric measures are useful to assess the clustering quality and to compare different clustering algorithms, they are quite limited in the scope of cluster-ing evaluation. In particular, clustering evaluation solely based on these quantitative measures ignores the following important information necessary for understanding document clustering results: (1) associations between individual documents and clus-ters are not provided; (2) relationships among different clusters are not represented; (3) the correspondence between the clustering results and the ground truth is not demonstrated; and (4) text content information is not provided and it is difficult to glean insights from the documents.

In this article, to address the aforesaid limitations, we present DClusterE , a compre-hensive and effective framework for Document Clustering E valuation using informa-tion visualization since visualization techniques are widely recognized as effective and powerful tools for data exploration. DClusterE integrates cluster validation with user interactions and offers rich visualization tools for users to examine document cluster-ing results from multiple perspectives. In addition to performance view which provides quantitative measurements of clustering results, DClusterE contains three-level views ( results overview , cluster view , document view ) and allows users for easy, direct, and interactive access of the documents at different levels: (1) results overview including force-directed layout view and matrix view provides an overall picture of the whole dataset. Using color schemes, it efficiently helps users glean insights of the cluster patterns. (2) cluster view presents the associations between individual documents and clusters; (3) document view provides the text content for each document and allows users to view and browse the details.

The contribution of this article can be summarized as: (1) a comprehensive and effective document clustering visualization framework which includes three-level in-formative views and supports user interactions, allowing users to examine document clustering results from multiple perspectives; (2) a novel Multiplicative Update Algo-rithm (MUA) for matrix reordering to generate narrow-banded (or clustered) nonzero patterns from documents. Combined with coarse seriation, MUA is able to provide better visualization of the cluster structures; (3) a Mallows-distance-based algorithm for establishing the relationship between the clustering results and the ground truth, which serves as the basis for coloring schemes; (4) an integrated visual environment where the relationships among clusters or individual documents can be visualized di-rectly and clearly.

The remainder of this article is organized as follows: Related prior work is discussed in Section 2 and the system overview is presented in Section 3. The main techniques used in DClusterE are described in Section 4. Experiments are conducted in Section 5 to evaluate our proposed document clustering framework. The user interface of DClusterE is discussed in Section 6 and a user study is conducted in Section 7. Finally, Section 8 concludes.
 Traditional clustering evaluation methods are based on quantitative measurements which include: (1) internal criteria which use only the information present in the dataset such as cluster separation and cohesion; and (2) external criteria that present the  X  X greement X  between the clustering results and the ground truth. Popular external criteria include entropy, purity, accuracy [Li and Ding 2006], normalized mutual infor-mation [Fred and Jain 2003], F-score, and rand index [Milligan and Cooper 1986]. Accuracy assigns each cluster to the ground-true class that is most frequent in this cluster and calculates the number of correctly assigned documents divided by the total number of documents. Normalized mutual information measures the shared informa-tion between cluster indicators and ground truth, based on their joint and marginal probability distribution. Rand index gives an equal weight to false positives and false negatives and F-score can penalize false negatives more strongly than false positives by selecting a proper weight for combination.

Besides the aforementioned measures, it is often desirable to access the correspond-ing  X  X greement X  between two clusters from different clusterings. Most of existing clus-tering comparison methods are based on (1) the memberships of objects to clusters, such as pair counting [Ben-Hur et al. 2002] which counts the pairs of objects from the same or different clusters in two clusterings, setting matching [Dongen 2000], which finds a match between clusters from two different clusterings in a stepwise manner without a global optimization objective, and variation of information [Meila 2002] which calculates the amount of information that is lost or gained in changing from one cluster to the other using conditional entropies or (2) the similarity between pairs of cluster representatives [Zhou et al. 2005] where a globally optimal matching approach is employed. In real-life document clustering applications, documents are represented as vectors using the vector space model where each dimension corresponds to a separate term. If a term appears in the document, its value in the vector is then nonzero. It should be pointed out that there are many schemes for determining the value such as binary weighting and tf-idf weighting. Typically, there are thousands of terms used in the documents. Hence document datasets are generally of high dimensions. Due to  X  X urse of dimensionality X , 2D displays such as force-directed layout and matrix view, based on suitable pairwise similarities, became a novel presentation for high-dimensional datasets. They not only locate nodes but also are widely used in potential pattern extraction. 2.2.1. Force-Directed Layout View. Force-directed graph drawing methods try to cal-culate layouts for undirected graphs based on an objective function, and they are widely used for visualizing the community construct in networks. One of the first force-directed layout algorithms is based on barycentric representation [Tutte 1963]. The methods such as Eades [1984] and Fruchterman-Reingold [Fruchterman and Reingold 1991] modeled a graph as a physical system of rings and springs. Kamada-Kawai [Kamada and Kawai 1989] uses spring forces proportional to the graph-theoretic distances and has its own variant on Eades X  algorithm. Davidson and Harel [1996] apply simulated annealing to produce a highest-quality figure. The LinLog model [Noack 2007] is used to represent the community structure.
 2.2.2. Matrix View. Another relationship-based visualization is matrix view, in which each cell presents a pairwise similarity value between two instances. Matrix re-ordering provides a complete picture and illustrative visualization of all the data in glance [Mueller 2004; Tran-Luu and Declaris 1997]. Given a proximity matrix A  X  R N  X  N indicating pairwise similarities between N documents, matrix reordering seeks a permutations of { 1 , 2 ,..., N } such that the reordered matrix is near block-diagonal form where all the nonzero values fall within a band in each row and column. In particular, the nonzero elements should all fall near the main diagonal of the per-mutated matrix. Such a block-diagonal structure facilitates the identification of the latent structures more convenient without having to consider high-dimensional repre-sentations [Berry et al. 1996]. Many different methods have been developed for matrix reordering over the years, including symbolic and spectral methods [Barnard et al. 1993; Chan and George 1980]. Later, the matrix visualization based on relationships for high-dimensional data mining was proposed to explore cluster structures [Strehl and Ghosh 2003]. The cluster heat map as a nice matrix representation was also used to reveal hierarchical cluster structures [Wilkinson and Friendly 2009; Wu et al. 2010]. 2.2.3. Others. Besides the visualization techniques mentioned before, some novel researches about clustering validation and document visualization have been done in recent years. VISTA [Chen and Liu 2004] is developed for clustering, validating, and refining by 2D star-coordinates space. A rich set of user-friendly interactive ren-dering operations based on the domain knowledge is provided. Following this work, SSVC [Chidlovskli and Lecerf 2008] is proposed to learn an optimal projection distance metric for the star and spherical coordinate visualization systems. SSVC couples visual clustering with the automatic setting beyond the conventional manual setting by the learned projection distance metric from the user feedbacks. As for document visual-ization, due to large number of terms in document vectors, Latent Semantic Indexing (LSI) combined with MultiDimensional Scaling (MDS) is proposed to reduce dimen-sions for document visualization in a 2D display [Fortuna et al. 2008]. LSI first ex-tracts hidden semantic concepts to find main topics appearing in the set of documents, and MDS is then used on the semantic space to map documents to a 2D display. Based on the semantic representation of textual datasets, another document visualization technique is proposed for textual data analysis using a directed semantic graph which is derived from subject-verb-object triplets [Rusu et al. 2009]. This technique could also be used for generating the document summary with graphical description. For an extremely large text corpus, exemplar-based visualization is presented to under-stand the relationships among documents [Chen et al. 2009] through a probabilistic multidimensional projection model. Many works have been reported on visual text analysis and they can be broadly di-vided into four different categories: (a) metadata visualization focusing on visualizing the metadata of text documents. For example, TileBars is used to visualize document length and query term frequency [Hearst 1995]. (b) document visualization methods focus on displaying document relationships such as the Galaxy of News [Rennison 1994], Jigsaw [Stasko et al. 2008], and ThemeRiver [Havre et al. 2002]. (c) word visualization methods show the text information at the text level such as TextArc (www.textarc.org), WordTree [Wattenberg and Vi  X  egas 2008], and FeatureLens [Don et al. 2007]. (d) text visual analytic methods integrate visualization with some text analytic methods such as visualization-based summarization [Allan et al. 1998; Ando et al. 2005; Jiao et al. 2010; Zhang et al. 2010, 2011] and interactive text analysis [Liu et al. 2009]. Different from early work on visual text analytics, our DclusterE focuses on document clustering evaluation. A screen shot of DClusterE is presented in Figure 1. In addition to providing a performance view which presents quantitative measurements of clustering results, DClusterE contains three-level views: results overview, cluster view, and document view. Results overview contains force-directed layout view and matrix view. Force-directed layout view utilizes node colors, background colors, and node positions to pro-vide an informative overview of the clustering results. Matrix view uses a matrix plot to visualize the similarity relationships between documents. In matrix view, we de-velop a novel Multiplicative Update Algorithm (MUA) for matrix reordering to gener-ate clustered patterns from documents. Combined with coarse seriation, MUA is able to provide better visualization of the cluster structures. In both force-directed lay-out view and matrix view, a Mallows-distance-based scheme is used to establish the correspondence between the clustering results and the ground truth. Color schemes are then devised based on the established correspondence to enhance visualization re-sults. In particular, in force-layout view, the document nodes are colored based on the clustering results and the background Voronoi cells [Arya et al. 2002] are colored ac-cording to the true class labels. In matrix view, the color of each grid in the matrix is scaled by the pairwise similarities and an attached color bar indicates the true class la-bel information. Cluster view displays the associations between individual documents and clusters and document view provides document details. The three-level views are complementary to each other and allow the users to examine document clustering results from multiple perspectives. In addition, DClusterE supports general user in-teractions such as zoom in/out, browsing, and interactive access of the documents at different levels. We will describe main techniques used in DClusterE in the following section.
 An application scenario. Figure 2 shows an application scenario of DClusterE . Take the DBLP dataset which is described in Section 5 as an example. A user first computes the traditional performance measures to evaluate the clustering results (e.g., results shown in Table VI). Then he/she may wonder how those clustering results are gener-ated or how they can interpret or explain the results. The user can then use DClus-terE to visualize the shape of clusters, the relationships among clusters, or among instances, or between instances and clusters, or between clusterings and the ground truth, through Linlog layout and matrix views (the details are described in Section 4.). In addition, using DClusterE , one can clearly observe the documents that are difficult to cluster or having ambiguous cluster labels (more details are described in Section 5.). This can help users to better understand and interpret clustering results. Force-directed layout algorithms try to obtain optimal positions for each document in a 2D plane by minimizing the energy function [Noack 2009] to display the document relationships. In our system, we applied LinLog [Noack 2009], a special case of force-directed layout algorithm, to position documents. The Euclidean distance between two objects is used to measure their dissimilarity. By positioning the documents on a 2D plane, force-directed layout is able to show some crucial details that are not included in cluster indicators such as distances between two clusters, distances between two objects, distances between a object and a cluster, cluster shapes, and cluster densities.
Specifically, the LinLog layout algorithm introduces the weights of nodes and edges and it formulates the attractive force between node u and v as where w u ,v is the edge weight which is calculated by the similarity between node u and v . And the repulsive force exerted on node u by v is where w u and w v are the weights for node u and v , it is the sum of the total edge weights which connects to the node. Then, the force model can be transformed into an energy model as The optimal positions p of all nodes are obtained by minimizing the energy function E . Layout view can be naturally used to evaluate document clustering quality since the documents in the same cluster should be placed in the positions that near to each other. 4.2.1. Matrix Reordering. Matrix reordering aims to generate narrow-banded (or clustered) nonzero patterns from documents and provides a complete picture and illustrative visualization of all the data in a glance. Here we propose a Multiplicative Updating Algorithm (MUA) for performing matrix reordering. We first explain some basic concepts before introducing MUA. Firstly given a proximity matrix A  X  R N  X  N , list can then be represented as { T (1) ,  X  X  X  , T ( N ) } .Let P be the permutation matrix Then the reordered matrix B can be represented as The idea of matrix reordering is to find P so that the B is as close to a block-diagonal form as possible.

Secondly we use Bond Energy (BE) to quantify matrix blockness. BE measures the correlations between neighboring matrix elements [Tran-Luu and Declaris 1997] and is defined as the BE by rearranging the rows/columns of A so that each entry is as closely related numerically as possible to its four neighboring entries, and render the matrix into a pattern whose underlying structure could be interpreted by inspection [Arabie and Hubert 1990].
 Let We have the following proposition.

P ROPOSITION 1. The matrix reordering problem can be formulated as
The function Tr is the trace of a matrix. The proof of Proposition 1 only requires some algebraic manipulations and is omitted here for space limit. The main step of MUA is using the following multiplicative update rules to solve Eq. (7).
 The correctness and convergence to a locally optimal solution in MUA can be rigorously proven using the auxiliary function approach [Nocedal and Wright 1999]. We sketch the proof here. The Lagrangian function is The KKT complementary condition for the nonnegativity constraint is It can be easily verified that the fixed points of the update rule Eq. (8) satisfy Eq. (10). P is initialized as an arbitary permutation matrix.

At convergence, the elements of the solution P  X  are not generally { 0 , 1 } -valued and rounding is necessary. We propose to carry out this rounding via the Hungarian algo-rithm for the bipartite graph matching. This algorithm can be formulated as 4.2.2. Coarse Seriation Matrix Reordering. Matrix ordering via MUA does not use the ground truth (i.e., cluster label information). In our work, we show that combined with coarse seriation [Strehl and Ghosh 2003], MUA is able to provide better visualization of the cluster structures.

Coarse seriation aims to rearrange the documents according to the label information so that different clusters can be well separated [Strehl and Ghosh 2003]. Given a n  X  n permuted similarity matrix A 1 obtained after performing MUA, and a reordered true label vector L , the binary cluster indicator CI can be obtained by Anew n  X  n permutation matrix P is defined as Note n l is the number of documents in l -th cluster. From Eq. (12), P i , j is 1 if j is the sum of the number of points among the first i that belong to the same cluster and the number of points in the first L i  X  1 clusters [Strehl and Ghosh 2003]. Now, the final permuted similarity matrix A 2 and the corresponding label vector L are Thus, the generation of a  X  X anded matrix X  according to the true label information can well represent cluster patterns, where large entries should all fall near the diagonal line from the upper left to the low right. A simple matrix view is shown in Figure 3. A cluster view presents grid visualization for all documents in the d -neighborhood of a cluster center, that is, the set of all documents that are at distance less than d from the cluster center. In our framework, d is set to be the largest distance from any document in a cluster to the cluster center. Users are allowed to divide d into n intervals, and the y-axis is used to indicate the number objects which are located in the specific range of the distance defined by the x-axis. As shown in Figure 4, from the left to right, the first column shows the objects which are located in the distance less than d / n from the cluster center, and the second column shows the objects with the distance between d / n and 2 d / n , and so on. The cluster view offers a visual representation of the association between documents and the cluster, and shows the document distribution ultimately correlated with a particular cluster. Figure 4 presents an example of cluster views. For the left figure of Figure 4, we can observe that there is no object within the distance  X  6 d / n . For the distance between 6 d / n and 7 d / n , there is only one object in the red cluster and for the distance between ( n  X  1) d / n and d , there are four objects in the red cluster, four objects in the green cluster, and one object in the blue cluster. And from the right figure of Figure 4, all objects near the cluster center are from the same cluster. It is evident that the clustering results in the right figure are better than the clustering results in the left figure. In results overview, a Mallows-distance-based scheme is used to establish the corre-spondence between the clustering results and the ground truth. Color schemes are then devised based on the established correspondence to enhance visualization results. 4.4.1. Mallows Distance. Mallows distance is used to establish the correspondence be-tween the clustering results and the ground truth. Mallows distance is a complete and globally optimal matching scheme between clusters in two clustering results [Mallows 1972; Zhou et al. 2005]. Given a dataset X = { x 1 , x 2 ,..., x N } and two clustering results, C 1with J clusters and C 2with K clusters. C 1 i , j is the probability of the i -th object be-belonging to the k -th cluster in C 2. The two clusters C 1 j and C 2 k can be represented probabilities of each object x i belonging to them, respectively. Then we can assign weights  X  j to C 1 j ,and  X  k to C 2 k . Note that the weights reflect the significance of the clusters and J j =1  X  j =1, K k =1  X  k = 1. Thus adopting Mallow distance, the distance between C 1and C 2 can be described as [Zhou et al. 2005] C 2 k are indicated by  X  j , k and can be computed using linear programming. 4.4.2. Coloring and Background. Coloring schemas are designed for layout and matrix views as follows.  X  For layout view, after establishing the correspondence between the clustering re-sults and the ground truth, we use the following mechanism to demonstrate their re-lationships: (1) the background area is constructed by calculating a Voronoi cell for each node [Arya et al. 2002]; (2) two groups of colors are defined for nodes (darker) and node background areas (lighter) respectively. Node colors indicate the cluster-ing results, and node background colors represent the ground truth; and (3) the node colors would be changed after a clustering algorithm is executed, while the background color is static to represent the ground truth. Figure 5 presents a simple example of Voronoi diagram which partitions a plane with n points ( n is the num-ber of documents) into convex polygons such that each polygon contains exactly one generating point and every point in a given polygon is closer to its generating points than to any other. Using the Voronoi diagram colored by the ground truth, we can easily check whether the Linlog layout (based on the calculated pairwise similarity) is consistent with the ground truth or not.  X  For the matrix view, we add an extra color bar for displaying the true class label information (on the left and the bottom of matrix view). The color of the grid of the i -th row and j th column is determined by the cluster indicators and the shade of color is scaled by the similarity value between the corresponding two documents from dark to light. If these two documents are from two different clusters, the grid is colored by gray. Otherwise, it is painted by the color of the cluster that the documents belong to. A simple example is shown in Figure 3. The grid colored by dark green means that the corresponding two documents are in the same cluster (corresponding to the 3rd class in the ground truth), and the similarity between them is high.  X  The colors are consistent across the views at different levels. This means that the color for a cluster in layout view is the same as the color that describes the corre-sponding cluster in other views. These three different views have their own advantages and limitations and integrating them together in DClusterE makes the whole system more efficient and user-friendly. Linlog layout is more suitable for small and sparse datasets while matrix-based rep-resentations are better suited for large and dense datasets [Keller et al. 2006]. Also, working as the alternative view of the Linlog layout, matrix view needs to set the threshold for the similarities (i.e., if the similarity value is larger than the threshold, it would be replaced by 1, otherwise, it is set as 0.). Matrix views and Linlog layout views are both dealing with the whole dataset, and cluster views focus on subsets of objects with high similarities. So the use of multiple views provides a rich yet comple-mentary set of visualization forms to the users. In this section, firstly, we introduce the data preprocessing procedure. Then we conduct the following four sets of experiments to evaluate our proposed framework: (1) com-pare MUA with other five popular matrix reordering algorithms on textual datasets to validate its efficacy; (2) compare the matrix views generated by coarse seriation matrix reordering; (3) compare Mallows distance with other measures in clustering results comparison; (4) compare DClusterE with 2D/3D scatter plot views and cluster-ing validation methods to demonstrate its ability in clustering evaluation; (5) compare the advantages and limitations of matrix views and Linlog layout views; and (6) inves-tigate the flexible user interaction features supported in DClusterE and perform user studies to evaluate user sophistication. Here, we use three common document datasets named CSTR, Log, and DBLP. These datasets are summarized in Table I.

CSTR Dataset. This dataset contains the abstracts of technical reports (TRs) pub-lished in the Department of Computer Science at University of Rochester from 1991 to 2007. There are 550 abstracts and they are divided into four research areas: Nat-ural Language Processing (NLP), Robotics/Vision, Systems, and Theory. We also use the category information of terms obtained from ACM Keywords Taxonomy as prior knowledge [Li and Ding 2006].

Log Dataset. This dataset contains 1367 text messages of system log from different desktop machines describing the status of computer components. These messages are divided into 8 different situations [Li and Ding 2008].

DBLP Dataset. This dataset is obtained from DBLP Computer Science Bibliography and contains the paper titles published by 202 productive researchers from 4 cate-gories: data mining, theory, operating system, and networking. The text datasets are processed through removing the stop-words and unnecessary tags and heads using rainbow package [McCallum 1996]. Each document is transferred to a word vector to build a document-word matrix. Nor-mally, this kind of matrix is with more than 10 thousands columns(words). If all fea-tures are used together to explore pairwise similarities, no pattern could be found due to the large volume of noise. To keep enough information and avoid too much noise being involved, in this article, we use the most popular feature extraction method for textual dataset preprocessing: latent semantic analysis to do feature reduction. Then a cosine similarity measurement is applied to calculate the similarity between two documents. Using these similarities, force-directed layout and matrix reordering are used to locate documents in the 2D display. 5.3.1. Performance of MUA. In this section, we compare MUA with five other most pop-ular algorithms such as reverse Cuthill-McKee (RCM) [Chan and George 1980], Mod-ified Minimum Degree (MMD), Column Count (CC), Dendrogram (Den), and Fiedler Ordering (FO) [Barnard et al. 1993] in the mentioned three textual datasets.
We compare the performance of various algorithms using Bond Energy as described in Section 4.2. In addition, we also use bandwidth and envelope size as performance measures for matrix reordering. Bandwidth is defined as the maximum bandwidth value over all rows in the permutated matrix and envelope size is the average band-width value through all rows [Chan and George 1980]. Both of them have been widely used in matrix reordering. The experimental results of the three different performance measures are shown in Table II, Table III, and Table IV, respectively. From these tables, we observe that: (1) matrix reordering methods can efficiently minimize band-width and envelope size, and (2) our MUA algorithm outperforms the other five algo-rithms on most counts.
 5.3.2. Coarse Seriation Matrix Reordering. Using the DBLP dataset, we also use visual similarity matrices to evaluate the matrix ordering algorithms. Figure 6 presents the matrix view of the original dataset and the reordered matrix views by three differ-ent algorithms (coarse seriation, MUA, and coarse seriation matrix reordering (i.e., MUA+coarse-seriation)). From Figure 6, we can observe that: (1) In the matrix view ordered by coarse seriation, the documents in each cluster can be well separated, and four cluster patterns based on the ground truth can be clearly observed. However, similar documents in the same cluster are not placed near each other. (2) Using MUA, documents are reordered based on their pairwise similarities, and similar documents would be placed nearby to minimize the bandwidth or envelope size. However, in the view generated by MUA, the cluster patterns (as identified by the ground truth) are not well separated. (3) Combining MUA with coarse seriation can better visualize both the inter-and intracluster structures. Coarse seriation matrix reordering can distin-guish different clusters, and also place similar documents close each other within each cluster. It has been shown that Mallows distance outperforms other clustering compari-son measures such as the Variation of Information (VI) method [Meila 2002] and Rand [Rand 1971] for comparing soft and hard clustering results [Zhou et al. 2005]. In particular, it has been shown that Mallows distance can identify how the clusters are separated and can differentiate the distances between clusters. In this section, we empirically compare Mallows distance with VI and Rand for clustering results compar-ison. We generate synthetic datasets D as follows: D 1 contains three clusters, each of them with 100 instances; and the distance between the centroids of Cluster1 and Cluster3 is two times the distance between Cluster1 and Cluster2. Then we compare the clustering results generated by Kmeans and the ground truth on D 1 using the three different measures. To demonstrate the advantage of Mallows distance, we con-sider the following two scenarios for D : S 1 : label 1/3 of the instances in Cluster1 to Cluster2; and S 2 : label 1/3 of the instances in Cluser1 to Cluser3. Note that in these two scenarios, the ground truth has been changed. We then the compare clustering re-sults generated by Kmeans with the new labels. The comparison results are shown in Table V. It can be observed that both VI and Rand methods produce the same results for S 1 and S 2 , however, Mallows distance gives larger distance to S 2 since Cluster3 is farther than Cluster2. This demonstrates the benefits of Mallows distance in consid-ering similarities among different clusters. Table V also lists the results on the three document datasets. We observe that for the datasets in which the clusters are well separated, the distance calculated by Mallows distance is much smaller than others, such as the Log and DBLP datasets. Two case studies are performed to demonstrate the effectiveness of DClusterE for doc-ument clustering evaluation using the Log and DBLP dataset.
 5.5.1. DBLP Dataset. As for the DBLP dataset, for comparison purposes, we first visu-alize the two and three most important eigenvectors extracted from Latent Semantic Analysis (LSA) using 2D and 3D scatter plot views, which are shown in Figure 7. In this figure, it is evident that the clusters in green and yellow can not be well separated and it is difficult to find a clear pattern in the dataset even though we already rotate the axis for better representation.

Then, we use the Linlog layout algorithm to position each document, and use differ-ent colors to represent different classes in the ground truth. The 2D display by Linlog is shown in Figure 8. From this figure, it can be seen that the documents in each cluster can be well separated and similar documents are placed close to each other. Now, we present how to evaluate clustering results using the layout algorithm. Here, three clustering algorithms, Clustering with Maximum Modularity (CMM) [Brandes et al. 2008], Hierarchical Clustering (HC), and Kmeans clustering, are used to perform document clustering. Figure 9 presents the results overview gener-ated by DClusterE and Table VI shows the quantitative evaluation measurements including accuracy, Normalized Mutual Information (NMI), F-Score (FS), and Rand Index (RI). The following can be observed.  X  The results overviews generated by DClusterE are consistent with quantitative measurements in the sense that better clustering results typically have better quan-titative measurements and lead to better visualization. Note that CMM has the best quantitative performance. In the results overview, CMM has the largest agreement between the node colors and the background colors and the smallest variation of node colors in neighboring nodes.  X  Some interesting documents with ambiguous cluster labels can be observed easily.
For example, as shown in Figure 10, Prof. Matthew Andrews is labeled as a re-searcher in the theory field, but in layout view, that name is surrounded by network researchers. By using the document view, we can see that he has published a lot of papers on network research.
  X  Using visualization, the extra information about the cluster shape, density, and size can assist users for further exploration.
 The results of matrix reordering views for the same dataset are shown in Figure 11. We can observe that: (1) for Kmeans and hierarchical algorithms, some documents in the clusters in green and red are wrongly clustered; a few documents in the yellow cluster are assigned to the red cluster; and few documents in the blue cluster are clustered into the red cluster. (2) MMC has the maximum  X  X greement X  with the ground truth. And (3) all these observations are consistent with the layout views.

To further explore the cluster structures, users can check the cluster view as shown in Figure 12. In Figure 12, the top four views represent the document distributions for the four clusters in the ground truth, and the views in the second, third, and forth levels show the document distributions in the results from CMM, HC, and Kmeans clustering, respectively. 5.5.2. Log Dataset. To further analyze the advantages/limitations of matrix views and Linlog layout views, we perform a detailed study using the Log dataset which consists of 9 clusters with more than one thousand instances. Figure 13 shows the matrix view of the original Log dataset and Figure 14 provides the matrix view and the Linlog layout view for the datset. In Figure 14, the left figure is the matrix view using the MUA + coarse seriation algorithm and the right figure is generated by the Linlog layout algorithm. We observe the following.  X  Linlog layout views could not avoid node overlapping. Taking the Linlog layout view in Figure 14 as an example, the nodes in different clusters are heavily overlapped with each other.  X  Matrix views could depict the hierarchical structure for the given dataset. Taking a look at the selected rectangle area in the matrix view, three subclusters in the cluster in dark blue can be easily observed.  X  It is much easier to focus on the detail information and detect outlier/ambiguous nodes in the Linlog layout views than in the matrix views. For example, the nodes in one cluster that are surrounded by many nodes from other clusters can be easily identified in the Linlog layout views, such as the selected nodes using red circles in Figure 14.
 From the preceding observations, it is evident that by combining the matrix views and Linlog layout views, a better representation for clustering structures and clustering evaluation can be achieved. DClusterE also supports flexible user interactions which include: (1) zoom in/out in each level view; (2) tracking and highlighting individual documents or document sets; (3) browsing documents or words; (4) interactive access of the documents at different levels.

As an efficient data analysis tool, DClusterE also provides some useful functions in dimension reduction and clustering as follows. (1) Latent Semantic Analysis (LSA) [Landauer 1997] and MultiDimensional Scaling (2) general feature selection methods including reliefF [Marko and Igor 2003], mRMR (3) three most popular clustering algorithms -Kmeans, maximum modularity, and To better evaluate the performance of DClusterE in clustering evaluation and data analysis, we conduct a user survey. In this survey, we compare the capability of doc-ument clustering visualization of DClusterE with four well-known data analysis tools (WEKA [Witten and Frank 2005], CLUSION [Strehl and Ghosh 2003], GAP [Wu et al. 2010], and VISTA [Chen and Liu 2004]) as well as traditional numerical performance measures (Accuracy, NMI, Random Index and so on).

Firstly, we give a brief introduction on various data analysis tools used in the user study. (1) WEKA. WEKA is a collection of data mining methods including data preprocess-(2) CLUSION. CLUSION is a visualization tool for clustering representation using (3) GAP. GAP is a matrix visualization and clustering toolkit for high-dimensional (4) VISTA. VISTA is an interactive system to visualize cluster rendering and validate
Forty students who have some data mining background are chosen as the subjects for conducting this survey. Firstly, we install DClusterE on the computers at the computing lab in the School of Computer Science at Florida International University. Thirty-three of the subjects were enrolled in the data mining course and took the user study in the computing lab in the last class (about one-and-a-half hours) in Fall 2010. As for the other 7 students who are graduate students doing research in the areas of data mining and information retrieval, we distributed DClusterE to them and asked them to conduct the user study and return the feedback at their convenience. Three datasets (CSTR, Log, and DBLP) as described in Section 5.1 are used in the user study. The participants are asked to use DClusterE , WEKA, CLUSION, GAP, and VISTA to visualize and evaluate the different clustering results on these three datasets.
Note that the systems are presented to the user in a randomized fashion and there is no fixed order in which these systems are used. The users are asked to rate the different aspects as well as the overall impression of systems. In particular, they are asked to assign a score of 1 (the least satisfaction) to 10 (the highest satisfaction), according to their satisfaction of the use of different systems in five major aspects as well as overall sophistication which include the following. (1) Aspect 1: whether the system could present the clustering quality. (2) Aspect 2: whether the cluster structure could be visualized meaningfully. (3) Aspect 3: whether the visualization features help users further understand docu-(4) Aspect 4: whether the system supports flexible user interactions. (5) Aspect 5: whether the system provides efficient data analysis tools in clustering (6) Overall Rating: the overall sophistication of the system.
 The average scores (of all the users) for each aspect are shown in Table VII. In addition to the quantitative evaluation, students also provided some feedback and comments. Feedback from the users are overwhelmingly positive and suggest that our system can be used to perform document clustering evaluation.

From Table VII, we observe that most participants think DClusterE is an effective tool in document clustering evaluation. WEKA presents cluster sizes, cluster centroid information, and a 2D scatter plot, while CLUSION estimates the clustering results using matrix view with coarse seriation. GAP is an enhanced version of CLUSION which introduces a hierarchical tree to present cluster structures. These evaluation features in both WEKA and CLUSION are already incorporated in DClusterE .How-ever, WEKA and CLUSION do not provide many important pieces of information for clustering evaluation such as the relationships among clusters, the relationship be-tween clustering results and the ground truth, and the detailed information of each document. In addition, they do not support user interactions. GAP is the only one which is comparable to DClusterE . Note that WEKA achieves the best performance in Aspect 5 because it contains many more feature selection and clustering algorithms than any other tools. VISTA outperforms others in Aspect 4 because it is able to dis-cover hidden patterns and make effective use of user interactions.

Statistical Significance. Also, to statistically compare the performance of DClusterE with other clustering evaluation methods, we apply a t-test which is one of the most widely used statistical significance tests to determine the significance based on the values of overall rating. The results are shown in Table VIII. From Table VIII, we observe that DClusterE outperforms most other evaluation methods significantly. We have described DClusterE as a clustering evaluation and understanding frame-work using visualization techniques. DClusterE consists of three-level informative views and supports interactive access of the documents at different levels. A novel Mul-tiplicative Update Algorithm (MUA) for matrix reordering and a Mallows-distance-based algorithm for establishing the relationship between the clustering results and the ground truth are proposed. Experiments and user studies are conducted to demon-strate the effectiveness and efficiency of DClusterE .

