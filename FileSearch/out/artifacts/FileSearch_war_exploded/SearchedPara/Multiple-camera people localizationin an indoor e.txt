 SHORT PAPER Valery A. Petrushin  X  Gang Wei  X  Anatole V. Gershman Abstract With the rapid proliferation of video cameras in public places, the abil-ity to identify and track people and other objects creates tremendous opportunities for business and security applications. This paper presents the Multiple Camera Indoor Surveillance project which is devoted to using multiple cameras, agent-based technology and knowledge-based techniques to identify and track people and summarize their activities. We also describe a people localization system, which identifies and localizes people in an indoor environment. The system uses low-level color features  X  a color histogram and average vertical color  X  for build-ing people models and the Bayesian decision-making approach for people local-ization. The results of a pilot experiment that used 32 h of data (4 days  X  8h) showed the average recall and precision values of 68 and 59% respectively. Aug-menting the system with domain knowledge, such as location of working places in cubicles, doors and passages, increased the average recall to 87% and precision to 73%.
 Keywords Multi-camera surveillance  X  Indoor surveillance  X  Bayesian inference  X  People localization 1 Introduction The proliferation of video cameras in public places, such as airports, streets, park-ing lots, hospitals and shopping malls creates many opportunities for business and public safety applications, including surveillance for threat detection, monitoring parking lots and streets, tracking customers in a store, assets movement control, detecting unusual events in a hospital, and monitoring elderly people at home. These applications require the ability to automatically detect, recognize and track people and other objects by analyzing video or image data. While video surveil-lance has been in use for decades, the development of systems that can automati-cally detect and track people is still an active research area. Many approaches have been proposed in recent years. They differ in various aspects, such as number of cameras used, type of cameras (grayscale or color, mono or stereo, etc.) and their speed and resolution, type of environment, area covered and location of cameras. There are a vast number of papers that are devoted to this research area and we review some of them in the next section.
 applications due to three major reasons. First, the capability of today X  X  video and image analysis techniques is dwarfed by the complexity of the identification and localization problem. Second, most of such systems are developed and tested in controlled  X  X imulated X  environments, e.g., a person deliberately walking back and forth in the room, instead of actual public areas. Finally, the efforts have been mostly focused on analyzing the video data without taking into consideration the domain knowledge about the environment.
 algorithms, we propose to:  X  Build simple and efficient agents that collaboratively analyze and aggregate the video data from multiple cameras, instead of developing sophisticated algo-rithms that rely on a single video stream.  X  Take advantage of the data redundancy to improve the accuracy of tracking and recognition by fusing data from the cameras with overlapping fields of view.  X  Take into account knowledge about the environment by building  X  X nvironment-specific X  systems.  X  Develop systems that work in a real-world environment.
 (MCIS) project. The backbone of the project consists of 30 AXIS-2100 web-cams that observe the whole Accenture Technology Labs X  floor and a meeting room located on another floor (see Fig. 1 ). The webcams cover the total area of 18,000 ft 2 (1,670 m 2 ). The backbone architecture also includes several computers. Each computer receives signals from 3 to 4 webcams, detects  X  X vents X  (any essen-tial movement in the camera X  X  field of view) and records event pictures in JPEG format. The average signal sampling frequency is about three frames per second. Another computer collects event pictures, converts them into MPEG-1 movie and creates an event record in an SQL database. The event database serves as a com-mon repository for both the people who are doing manual searches of events and for automated computer analysis.
  X  Create a realistic multi-sensor indoor surveillance environment.  X  Create an around-the-clock working surveillance system that accumulates data in a database for three consecutive days and has a GUI for search and browsing.  X  Use this surveillance system as a base for developing more advanced event analysis algorithms, such as people recognition and tracking, using collaborat-ing agents and domain knowledge.
 presents experimental results of the system X  X  performance. 2 Related works There are many papers devoted to single and multiple camera surveillance. Here, we shall focus on the research that deals with indoor people identification and tracking.
 subway stations that uses the luminance contrast in YUV color space to separate people blobs from the background. The coordinates and geometric features of the blobs are estimated, and a two-way matching matrix algorithm is used to track (overlapping) blobs.
 It uses several interacting modules to increase tracking robustness. The modules are a motion tracker that detects moving regions in each frame, a region tracker that tracks selected regions over time, a head detector that detects heads in the tracked regions, and an active shape tracker that uses models of people shape to detect and track them over time. The modules predict a person X  X  position and moving direction, and recover from occlusions.
 real-time identification and tracking of up to three people in a rather small room (5 m  X  5 m). The system evaluates 3D models of blobs and clusters them to fit the people-shaped blob models. Then the centroids of the blobs are projected into the room ground plan. The quantized RGB color histogram and histogram intersection are used for person X  X  identity maintenance. The person tracker module keeps the history of the person X  X  past locations and uses it to predict current location. Despite a low image-processing rate (about 3.5 Hz) the system works well with up to three people, who are not moving too fast and are not wearing similarly colored outfits. edge about the topology of the paths between the cameras. It probabilistically models the chain of observation intervals for each tracked person using Bayesian formalization of the problem. To estimate the optimal chain of observations, the authors transform the maximum a posteriori estimation problem into a linear pro-gramming optimization.
 grayscale cameras for tracking people and for selecting a camera that gives the best view. The system consist of three modules: single view tracking, multiple view transition tracking and automatic camera switching. The system uses the follow-ing features for each person: locations of selected feature points, intensity of the selected feature points and geometric information related to a coarse 2D human body model. The multivariate Gaussian models and Mahalonobis distances are used for people modeling and tracking. The class-conditional distribution for spa-tial and spatial-temporal matching is used for the multiple view transition track-ing for matching predicted locations and body model sizes. The automatic camera switching is necessary if the person is moving out of the current camera X  X  field of view, or the person moves too far away, or the person is occluded by another person. The system selects a camera that will contain the person over the largest time according to the current prediction of the person X  X  movement. The experi-ments with three cameras in various indoor environments showed high robustness of people tracking (96 X 98%).
 lapping and/or nonoverlapping uncalibrated color cameras for people tracking. The system uses spatial and color Gaussian probability distributions for each per-son to identify and track people in one camera view. The person identification is based on the voting of the foreground pixels. If two or more people receive a certain percentage of votes from the same region, the systems assumes partial occlusion of between these people. In case of complete occlusion, a linear veloc-ity predictor is used for disambiguation. In order to track people across multiple cameras the system learns the field of view lines of each camera as viewed in the other cameras. This information and the knowledge of cameras X  location are used for identification of moving people. The experiments with three cameras and three different camera setups gave promising results. 3 Person detection and localization 3.1 Visual features A person X  X  most distinguishable visual identity is his or her face. However, in many practical applications, traditional face recognition algorithms fail to work reliably because of the small size and poor quality of images. Other salient char-acteristics of a person include the size of the body, color of the hair and color of the clothes that person is wearing. At any given day, a person usually wears the same clothes, and thus the color of person X  X  clothes is consistent and is a good discriminator (unless everybody wears the same uniform).
 sufficient to distinguish different people based on their clothes: a color histogram and the average vertical color, both in the RGB color space. For the detected body region R , we extract three 16 bin histograms, one for each color component. The histogram value of component c and bin m is defined as: where | R | is the number of pixels of region R and c ( x , y ) is the quantized value of the color component at pixel ( x , y ) .
 count for this variation, we used the average vertical color, which is calculated as follows. The top 15% (the head) and bottom 20% (the feet that also often include shadow) of the detected body blob R are discarded and the rest of the region is divided into M horizontal stripes of equal width. The average color is calculated for each stripe and for each color component giving a 3-by-M feature vector. For color component c and stripe region R ( m ) the average value is calculated using the following formula: where | R ( m ) | is the number of pixels of the region R ( m )and c ( x , y )isthevalueof the color component at pixel ( x , y ). We varied the number of stripes from 2 to 16 and found that the optimal number was 8.
 identification. The similarity between the current evidence E and a person X  X  model M ( H i ) is based on the linear combination of Euclidean distances D between cor-responding features and a threshold T . The threshold is estimated from two em-pirical distributions:distances to the modeled person and to all other people. The same distributions are used in converting the distance D into a probabilistic mea-sure P ( E | H i ) using linear interpolation (3) with the estimated threshold T and distances r 1 and r 2 . Figure 2 illustrates this conversion.
 Since people may wear different clothes from day to day, the person X  X  models must be updated daily. At the entrance of the floor, we installed a fingerprint recogni-tion system and a camera viewing it. Everyone who participated in the experiment registered at the entrance and the system created this person X  X  model based on several pictures taken by this camera. Unfortunately, due to varying illumination conditions at different locations these initial models are not enough for reliable performance. They need to be adapted for each camera. Currently we do not have an automatic solution to this problem and we adapt the models by selecting pic-tures manually. 3.2 People localization We use the term  X  X ocalization X  rather than  X  X racking X  because we are finding only the rough locations of people (up to a part of a room, a hallway or a cubicle), while the term  X  X racking X  tends to imply a more precise 3D or 2D positioning of an ob-ject in the x , y , z coordinates as well as the calculation of the object X  X  trajectory. Ourtaskistolocalize N people in an indoor environment of known geometry with stationary cameras. The fields of view for some cameras may overlap. A  X  X amera cluster X  is as a set of cameras that have a nonempty intersection of their field of views.
 the environment that either has different illumination conditions or belongs to an intersection of field of views of two or more cameras. A location is large enough to contain one or more persons. To synchronize views from different cameras, time is sampled into ticks. The tick duration is selected depending on the sampling frequencies of the cameras. It should be large enough to serve as a synchronization unit and small enough so that people either stay in the same location or only move to an adjacent one.
 streams. Due to the different luminosity conditions at different locations, we define several models for each person  X  one or more for each location viewed from each camera. The initial models are defined (through training) prior to the surveillance task and then change incrementally during the task. The current state of the world is specified by a probability distribution of objects being at particular locations at each time tick.
 we call a restricted localization problem, assumes that we localize only a restricted number of people as opposed to all the people in the environment. For example, we localize the employees of a store, or assign people at locations to known/unknown classes. In this case we have to create models only for known people plus a model for the  X  X nknown X  person, which is based on the features of all other people. This currently in the environment.
 that we localize all the people in the environment. In this case we have to create people X  X  models dynamically as we do not know who might enter the environment under observation. The second problem is a generalization of the first and below we describe our approach for solving it. We start with a simple case of a single camera and then proceed to more general cases. 3.2.1 Single camera, one location Suppose we have only one camera that watches one location. It captures events, which are sequences of consecutive frames that contain motion. Each event has a background image that captures the scene without motion. We assume that we have obtained models for each of the N people who might enter the field of view. We also assume that we have prior probabilities P ( H i ) i = 1 : N for the person i being in front of the camera. In general case, the prior probabilities depend on time. Let P ( H i | T j ) be a probability to see a person H i at time interval T j at this location. In our experiments we used 1 h intervals from 9 a.m. to 5 p.m. for T j j = 1 ,..., 8. However, for brevity we omit time from the prior probabilities. The prior probabilities can be estimated from data or assumed to be equal if no data is available.
 Step 1. Detect moving objects by subtracting the background image and apply post-processing steps, such as thresholding and morphological operations to remove noise and shadows. The result is a set of regions (blobs) R ={ R j } j = 1 : M .
 Step 2. Extract features from the detected blobs using formulas (1) and (2), com-pare them to all N models and convert them into conditional probabilities P ( R j | H i ) i = 1 : N , j = 1 : M using formula (3).
 Step 3. Calculate posterior probabilities using Bayes formula.
 R . The result of this step is an N  X  M matrix of posterior probabilities of the i -th person being represented by the j -th region.
 Step 4. Merge the probabilities for the blobs to obtain the probability P ( H i )forthe person H i being observed by the camera.
 3.2.2 Single camera, multiple locations In this case we have the camera X  X  field of view divided into nonoverlapping loca-tions. This partitioning allows more precise localization, and people models that are specific for each location can improve identification precision. Unfortunately, the latter is not always possible because many people do not visit all locations dur-ing the day. In this case the model from the closest location is used. Each person the probability that person would make transition from the i -thtothe j -th location during a single time tick.
 Step 1. Extract the set of moving regions R ={ R j } j = 1 : M .
 Step 2. Map regions into locations. The result is a binary matrix that shows which regions belong to a given location.
 Step 3. For each location L k k = 1 : K that has one or more regions do the following steps.
 Step 3.1. Extract features from the detected regions using formulas (1) and (2), compare them to all N models for this location, and convert them into condi-(3).
 Step 3.2. Calculate posterior probabilities using Bayes formula.
 where Step 3.3. Merge the probabilities for the regions to obtain the probability P ( H i | L k ) for the person H i being observed by the camera in location L k . Step 4. Normalize probabilities for all people across all locations observed by the camera. Step 5. Factor in transitions between locations to obtain the posterior probabilities: 3.2.3 Multiple cameras localization One of the major concerns for a multi-camera environment is the accuracy of frame synchronization between different cameras. In general, surveillance cam-eras are not synchronized. However, the clocks of the computers that obtain video frames from the cameras can be synchronized and each frame can be time-stamped. This allows us to identify all frames that belong to the same short time interval that we call a time tick. The time tick should be large enough to contain at least one frame from each camera, but be small enough so that we can assume that during a time tick people either stay inside the same location or move only to an adjacent location. In our experiments we used 1 second time ticks.
 shows an example of two cameras with overlapping fields of view. The graph on the right represents transitions among locations. (Here location L 0 corresponds to the  X  X ut-of-view X  space.) person H i at location L j P ( H i | L j ) , i = 1 : N , j = 1 : K ; location and camera-specific models for each person, and transition probabilities T ( H k ) ={ t ij ( H k ) } k = 1 : N , i , j = 1 : K .
 Step 1. Get all frames for the next time tick.
 Step 2. Extract the set of moving regions R ={ R j } j = 1 : M .
 Step 3. For each of Q cameras, map regions into locations. Because several frames from the same camera can be taken during the time tick, the algorithm identifies which regions represent the same object. It does this using simple heuristic of the region in the previous frame. During the time tick a region can move across several locations, that is why the result of this step is not a set of binary matrices, but matrices with values in the range from 0 to 1, that reflects the  X  X oft membership X  mapping of regions into locations.
 Step 4. Extract features from regions using formulas (1) and (2). Calculate dis-tances from them to all models for the corresponding locations and cameras.
Convert distances into conditional probabilities P ( R j , L k , C q | H i ) i = 1 : N , j = 1 : M , k = 1 : K , q = 1 : Q using formula (3).
 Step 5. For clusters of cameras merge the regions from different cameras that represent the same objects using region features and knowledge about camera locations. The result is a set of objects O ={ O r } and a matrix W ={ w jr } k = 1 : K , r = 1 : M 0 ,where M 0 is the number of objects (merged regions). Each w kj is the average of membership values of merged regions.
 Step 6. Unite the conditional probabilities that belong to the same object. The result is a sequence of probabilities S r ={ P ( R j , L k , C q | H i ,) } associated with the object O r , r = 1 : M 0 .
 Step 7. For each location L k , k = 1 : K that has one or more objects do the following steps.
 Step 7.1. Calculate the posterior probabilities using Bayes formula.
 where Step 7.2. Merge the probabilities for objects to obtain the probabilities for persons at locations P ( H i | L k ) .
 Step 8. Normalize the probabilities for all people across all locations using formula (8).
 Step 9. Calculate the probability distribution taking into account transitions be-tween locations using (9).
 single camera and multiple locations. First is the use of time ticks as synchroniza-tion units. Second is the use of soft membership for mapping moving regions into locations. Third is the integration of regions from images taken by different cam-eras and the use of multiple pieces of evidence to calculate posterior probabilities (10).
 3.2.4 Localization using domain knowledge Using static cameras in a slowly changing office environment has certain advan-tages that can be used to improve the performance of the system. For exam-ple, people are likely to be found in such areas as cubicles, armchairs in halls, doorways, hallway passages, etc. Other areas such as blinking computer screens, each camera to guide our feature extraction process. Figure 4 shows an example of camera layout. This layout is for the camera that is watching cubicles. Rectangles mark the important areas; and crossed rectangles mark the areas to ignore. This extension required more prior probabilities and more location-specific models for people, but it fit nicely into the probabilistic framework that has been described above. 4 Experimental results For our pilot experiment we used eight cameras integrated into four clusters. Figure 1 shows the cameras and the locations that were used. The camera C 0 ,lo-cated at the entrance to the floor, was used only for the initial model acquisition as we described in Sect. 3.1. The camera cluster K 1 ={ C 1 } contains only one camera that views a meeting room ( L 1 ) on another floor. Cluster K 2 ={ C 2 , C 3 , C 4 , C 5 } consists of four cameras that view a hallway ( L 2 ), a discussion area ( L 4 )andtheir intersection ( L 3 ). Cluster K 3 ={ C 6 } has a camera that watches a laboratory. And Cluster K 4 ={ C 7 , C 8 } has two cameras that view an open-space cube farm. people served as volunteers to be localized in the experiment. The color features of several dozen people were used for building the  X  X nknown person X  model. To evaluate the accuracy, we recorded four days of video data, 8 h per day. One day worth of data was used for the prior probability and transition matrices estima-tions, and the rest for testing. We used precision ( P ) and recall ( R ) as measures for evaluating the performance of the system (12).
 where C is the number of events where people were correctly localized by the system, A is the number of events where people were actually visible (ground truth), and T is the number of events that the system claimed to contain a person in a location.
 68.23% and the precision of 59.16%. Using the domain knowledge the system im-proved the performance with the average recall and precision increased to 87.21 and 73.55%, respectively. 5 Conclusion and future work This paper presents the Multiple Camera Indoor Surveillance project and reports on the preliminary results of the people localization system performance. The sys-tem uses low-level color features for people identification and localization. Merg-ing results from multiple cameras and augmenting the system with domain knowl-edge proved to increase the system X  X  accuracy. The current version of the system uses rather large locations and rough transition matrices. For future research we plan to use smaller locations and more person models. We expect that this will improve the precision of people localization. We also plan to experiment with dif-ferent visual and geometric features such as color features in different color spaces and sizes of people X  X  bodies.
 References Author Biographies
