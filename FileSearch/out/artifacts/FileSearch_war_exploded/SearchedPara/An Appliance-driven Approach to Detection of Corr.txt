 Load curve data in power systems refers to users X  electrical energy consumption data periodically collected with meters. It has become one of the most important assets for modern power systems. Many operational decisions are made based on the information discov-ered in the data. Load curve data, however, usually suffers from corruptions caused by various factors, such as data transmission er-rors or malfunctioning meters. To solve the problem, tremendous research efforts have been made on load curve data cleansing. Most existing approaches apply outlier detection methods from the sup-ply side ( i.e. , electricity service providers), which may only have aggregated load data. In this paper, we propose to seek aid from the demand side ( i.e. , electricity service users). With the help of readily available knowledge on consumers X  appliances, we present an appliance-driven approach to load curve data cleansing. This approach utilizes data generation rules and a Sequential Local Op-timization Algorithm (SLOA) to solve the Corrupted Data Identi-fication Problem (CDIP). We evaluate the performance of SLOA with real-world trace data and synthetic data. The results indicate that, comparing to existing load data cleansing methods, such as B-spline smoothing, our approach has an overall better performance and can effectively identify consecutive corrupted data. Experi-mental results also show that our method is robust in various tests. H.5.m [ Information Systems ]: Information interfaces and presen-tation X  Miscellaneous Corrupted data identification; Data analysis; Optimization
Electricity usage data, on the one hand, plays an important role in big data applications, and on the other hand, has been severely under explored. A recent news article appeared in Forbes [23] said,  X  But for the most part, utilities have yet to realize the potential of the flood of new data that has begun flowing to them from the power grid, . . . , And in some cases, they may not welcome it.  X  Yet, exist-ing power grid is facing challenges related to efficiency, reliability, environmental impact, and sustainability. For instance, the low ef-ficiency of current electric grid could lead to 8% of electric energy loss along its transmission lines, and the maximum generation ca-pacity is in use only 5% of the time [14].

The emerging smart grid technology aspires to revolutionize tra-ditional power grid with state-of-the-art information technologies in sensing, control, communications, data mining, and machine learning [6, 14]. Worldwide, significant research and development efforts and substantial investment are being committed to the nec-essary infrastructure to enable intelligent control of power systems, by installing advanced metering systems and establishing data com-munication networks throughout the grid. Consequently, power networks and data communication networks are envisioned to har-monize together to achieve highly efficient, flexible, and reliable power systems.

Among the various types of data transmitted over the smart grid, load curve data, which refers to the electric energy consumption pe-riodically recorded by meters at points of interest across the power grid, has become the critical assets for utility companies to make right decisions on energy generation, billing, and smart grid oper-ations. Load curve data, which  X  X s beginning to give us a view of what the customer is actually experiencing, something that we X  X e never ever seen before X  [23], is precious user behavior data, and is an important type of big data.

Load curve data collected and reported from smart meters at end-users X  premises is especially important for both energy supply and energy demand sides. On the demand side, it has direct im-pact on customers X  energy bills and their trust on the still nascent smart grid technology. It can also provide important information for domotics [17, 26]. On the energy supply side, inaccurate load data may lead to large profit losses and wrong business decisions. In 2012 , 126 . 8 million residential customers in the US used over 1 , 374 billion kWh, which counts to over 33% of the total elec-tric energy in the US [12]. The importance of this huge amount of energy and its financial implication cannot be over emphasized.
Nevertheless, it is unavoidable that load curves contain cor-rupted data and missing data, caused by various factors, such as malfunctioning meters, data packet losses in wireless networks, un-expected interruption or shutdown in electricity use, and unsched-uled maintenance [5]. Due to the huge volume of load curve data, it is hard for utilities to manually identify corrupted load curve data. Unfortunately, problems caused by corrupted data are usually re-alized only after it is too late, such as after a customer receiving a suspicious yet hard to rebut high energy bill.

As a concrete example, according to the news reports [20, 24], some customers in the province of British Columbia, Canada, were baffled by energy bills that are more than double what they were charged before the smart meter installation. While the problem could be identified by common sense and certain agreement might be reached by good faith negotiations [20, 24], fixing the ques-tionable bill is another head-scratching and embarrassing issue to the utility. As a response to customer complaints, the utility nor-mally took remedy actions, such as replacing the smart meters or taking back the smart meters for lab testing [24]. Such a remedy, however, can hardly be effective. According to CBC News [25],  X  X overnment estimates indicate there are about 60,000 smart me-ter holdouts (in the province). X  Overall, the users and the utility company have the well-aligned interest and should work together to tackle this critical problem plaguing the electric power industry.
Techniques of load data cleansing have been proposed to deal with load data corruption problem recently [5]. Most existing load data cleansing methods are designed for the supply side ( i.e. , elec-tricity service providers), to help the utility companies find the cor-rupted data and protect their profits. From the supply side, the col-lected load data is usually aggregated data, i.e. , the energy con-sumption of a billing unit such as a house or a commercial build-ing. When performing data cleansing on the supply side, due to the difficulty of obtaining extra knowledge behind the aggregated load data, most existing approaches apply outlier detection meth-ods, i.e. , the data that deviates remarkably from the regular pat-tern is identified as corrupted data. Various assumptions about the data generation mechanism are required for outlier detection, but due to limited information, those assumptions are usually based on empirical knowledge or statistic features of the data. Such outlier detection methods are oblivious of appliances X  various energy con-sumption models and may not be accurate or fair to customers. We call these methods appliance-oblivious . Such methods suffer from a few important deficiencies.

For example, the regression-based outlier detection methods find statistical patterns of load data and claim the data significantly de-viating from the patterns as corrupted data. Nevertheless, such resulted outliers are not necessarily corrupted data. In addition, without the knowledge of appliances X  energy consumption models, some  X  X idden" corrupted data is hard to detect. To be specific, the energy consumption of a group of appliances in a house or a build-ing is a stochastic process. The stochastic feature makes it hard to establish a fixed pattern. Turning on/off any high-power appli-ance may lead to a steep change in load curve. Using appliance-oblivious data cleansing methods, the data generated under such a condition is likely to be captured as outliers.

As another example, appliance-oblivious methods cannot deal with  X  X idden X  corrupted data. Fig. 1 shows an example of three appliances, A 1 , A 2 , and A 3 , which have power ranges of [2 , 4] , [10 , 12] and [30 , 32] , respectively. The load data within some ranges such as (4 , 10) , (16 , 30) , and (36 , 40) cannot be generated by any combination of the three appliances. Nevertheless, such data may not be identified by existing outlier detection as corrupted data.
Recently, with the emergence of fine-grained in-house energy monitoring systems, customers now have the capability to moni-tor their own energy usage more closely and more accurately [30]. As such, users may possess more knowledge behind the data, e.g. , the decomposition of total energy consumption according to main appliances. Even if in-house energy monitoring system is not avail-able, users should know the rated power of appliances X , which are easily accessible from the appliances X  manual, technical specifica-Figure 1: An example showing hidden corrupted data gener-ated with three appliances tion or public websites, such as [13]. This knowledge presents new opportunities to perform load data cleansing on the customer side directly or on the supply side with auxiliary information from cus-tomers. This new angle of tackling the corrupted load data problem can greatly improve the quality of load data.

In this paper, we tackle the practical problem of corrupted load curve data identification by developing an appliance-driven ap-proach. Specifically, we make the following contributions: The rest of the paper is organized as follows. In Section 2, we review the related work. In Section 3, the corrupted data identifica-tion problem (CDIP) is formulated. To solve CDIP, an optimization model is developed in Section 4, and SLOA is applied to find an approximate solution in Section 5. We evaluate the performance of our method in Section 6 and test its robustness in Section 7. The as-sumptions is further discussed in Section 8. The paper is concluded in Section 9.
Most related literature treats the corrupted data the same as out-liers in load pattern and focuses on outlier detection. A broad spec-trum of techniques for outlier detection in load data have been de-veloped, which include regression-based time series analysis, uni-variate statistical methods, and data mining techniques.

Regression-based time series analysis is the most widely used approach for outlier detection in load data [1, 5, 21, 22]. Mateos and Giannakis [22] developed a nonparametric regression method that approximates the regression function via ` 0 -norm regulariza-tion. Chen et al. [5] proposed a nonparametric regression method based on B-spline and kernel smoothing to identify corrupted data. Abraham and Chuang [1] analyzed residual patterns from some re-gression models of time series and used the patterns to construct outlier indicators. They also proposed a four-step procedure for modeling time series in the presence of outliers. Greta et al. [21] considered the estimation and detection of outliers in time series generated by a Gaussian auto-regression moving average (ARMA) process, and showed that the estimation of additive outliers is re-lated to the estimation of missing observations. ARMA is also uti-lized in [2, 1] as a fundamental model to identify outliers.
Univariate statistical methods are another type of techniques for outlier detection. Univariate statistical methods deal with outliers in load data by processing load data as one-dimensional real val-ues [9, 15, 10]. Most univariate methods for outlier detection as-sume an underlying a prior distribution of data. The outlier detec-tion problem is then translated to finding those observations that lie in the so-called outlier region of the assumed distribution, which is defined by a confidence coefficient value [10]. Since the statistical methods are susceptible to the number of exemplars, a simple but effective method named Boxplot or IQR is proposed in [31] to deal with small-sized exemplars.

In addition to the above methods, data mining techniques are also applied to identify outliers, such as k-nearest neighbor [29], k-means, k-medoids [4], and DBSCAN [18]. As a type of clustering methods, they group data with similar features, and identify data items that do not strongly belong to any cluster or far from other clusters as outliers. Recently, Aggarwal [3] provided a thorough survey on outlier detection.

Nevertheless, all the above methods do not consider the special physical laws behind the load data. Regression-based methods as-sume that the data follows a certain pattern, which can be modeled by a function governed by a set of parameters; univariate methods assume that the data is sampled from a certain known distribution; clustering methods assume that the data is well structured as clus-ters and the corrupted data deviates significantly from the normal structure. Obviously, the underlying assumptions in the existing methods are quite general and do not capture the specific features of load data well. Our paper fills the gap and differs from the ex-isting literature by offering a completely new angle to address the load data corruption problem.
In this section, we present a formal problem definition. Before that, we first describe an energy consumption model and discuss the generation rules of load data.
Load data is time series data that records users X  energy consump-tion. It is collected by smart meters periodically at a certain sam-pling frequency. Without loss of generality, we assume that the time is slotted, with each timeslot equal to the sampling interval time. In the rest of the paper, we thus use the terms  X  time  X ,  X  timeslot  X  and  X  sampling interval  X  interchangeably.

We denote the load data from timeslot t = 1 to timeslot t = n in a column vector as where each value y i in the vector represents the aggregated energy consumption of all appliances in a property, say a house at timeslot i . The energy consumption at a time instant depends on the appli-ances X  on-off states and their individual power level.

We assume that a house includes m appliances in total, and the power of the k -th appliance is p k ( watts ). At any time instant, if we record the power level of each individual appliance, we can define an m dimensional column power vector to capture energy consumption of the house:
Note that the power level of an appliance normally does not re-main at a fixed value but changes in a certain range. For this reason, we define two m dimensional column vectors, denoted as P l P , respectively: where l i and u i represent the lower and upper bounds of the power level of the i -th appliance, respectively. A power vector P is called valid if, for each value p i in P , l i  X  p i  X  u i .
 At any instant, the state of an appliance may be either on or off . We use an n  X  m 0 -1 state matrix , S = [ S ij ] n  X  m the states of the m appliances from time t = 1 to t = n , where S i,k = 1 indicates that the k -th appliance is on at time i , and 0 otherwise. In addition, we call the i -th row of S a state vector at time i , denoted by:
We have the following observations. First, a valid load data ele-ment y i (in watt-hours ) should be equal to the inner product of the state vector and the power vector at t = i , multiplied by the sam-pling interval time. This is a basic physical law for load curve data generation. Second, since the sampling interval is typically small, we assume that the probability that an appliance has more than one on-off switch events during a timeslot is negligible. In addition, the total number of on-off state switches of all appliances during a timeslot should be small. This feature is called the temporal spar-sity of on-off switching events. Intuitively, this feature means that in normal operation it is unlikely that the household turns on/off many appliances in a short time. Based on the above observations, we define the generation rules of load data.

D EFINITION 1 (G ENERATION R ULES ). Assume that the ini-tial state of appliances is S 0 . We claim that each valid load data, y , must satisfy the following rules: where f is data sampling frequency, 1  X  i  X  n , and  X  is the upper bound on the total number of on-off state switches for m appliances during a sampling interval.

Note that the energy consumption value (watt-hours) is calcu-lated with power value (watt) multiplied by time 1 /f (hour). To keep our discussion simple, we assume that a valid initial state vec-tor S 0 is given at this moment. We will relax this assumption later and show that the impact of an inaccurate initial state vector quickly becomes negligible as long as the system runs for just a little while (Section 7).

Based on the above generation rules, corrupted data is the values that break any of the rules.

D EFINITION 2 (CDIP). The corrupted data identification problem (CDIP) is, given load data Y = { y 1 ,y 2 ,  X  X  X  ,y bound vectors P l ,P u , and a sampling frequency f , find corrupted data items that violate any of the generation rules, i.e., C  X  { y y violates ( Equation 6) , for 1  X  i  X  n } .
To solve CDIP, a na X ve idea is to find all the solutions satis-fying the constraints in (Equation 6), by brute-force, exhaustive search for all possible appliance states. This method is very time-consuming. Even for a small data set it is very costly to find the answer. Since the generation rules can be considered as constraints in an optimization problem, we will show how the problem can be transformed to an optimization problem, which sheds light on a fast solution to an approximate problem.

D EFINITION 3 (V IRTUAL A PPLIANCE ). Besides the real ap-pliances, we introduce a virtual appliance into the system. Its as-sociated power is called virtual power, and we record the values of virtual power from time t = 1 to t = n in a virtual power vector where v i  X  (  X  X  X  , +  X  ) denotes the virtual power at time t = i .
By introducing the virtual appliance, we can develop the follow-ing optimization model to solve CDIP: subject to ( S i  X  P l + v i ) /f  X  y i  X  ( S i  X  P u + v
To understand the rationale behind the formulation of Equa-tion (8), it is worthwhile to point out that v i  X  V will come into play whenever S i cannot satisfy the generation rules, i.e. , the vir-tual appliance is  X  X urned on X  when the load data y i is corrupted. Thus, v i essentially makes a record to the corrupted data. After obtaining the final solution to Equation (8), the v i variables with non-zero values indicate the corrupted data, i.e. , We try to minimize ` 1 -norm, because it is proven that for most large under-determined systems of linear equations the minimal ` solution is also the sparsest solution ( i.e. , resulting in the minimal number of non-zero values of v i ) [11]. In addition, a larger v means that a corrupted y i is farther away from a valid range. In this sense, v i can be also regarded as the corrupted degree of y
It can be proved that CDIP is NP-hard by reducing the Travel-ing Salesperson Problem (TSP) to CDIP. We omit the proof due to space limit. By investigating the special structure of the prob-lem, however, we can develop an effective heuristic algorithm in-troduced in the next section.
 Algorithm 1 Sequential Local Optimization Algorithm Input: Load data { y 1 ,y 2 ,  X  X  X  ,y n } , power bounds P Output: Corrupted data set C , corrupted degree v i , 1  X  i  X  n 1: v 0 = 0 2: C =  X  3: for k = 1 : n do 4: Solve Problem (Equation 10), and obtain v i and S i where 5: if v k 6 = 0 then 6: C = C  X  X  y k } 8: end if 9: end for 10: return C, { v 1 ,v 2 ,  X  X  X  ,v n }
In this section, we propose a Sequential Local Optimization Al-gorithm (SLOA) and develop a quantitative strategy to estimate the minimum local window size.
The temporal sparsity of corrupted load data suggests that we can perform optimization in a smaller, local time window. By con-sidering the correlation between consecutive timeslots, we design a Sequential Local Optimization Algorithm (SLOA). Without loss of generality, we take a load data from time t = 1 to t = n as an example to show the major steps of SLOA.

Step 1. Consider a small time window with size of w, 1  X  w &lt; n , which starts from time k to time k + w  X  1 . Given the state vector at time k  X  1 , i.e. , S k  X  1 , we consider the following optimization problem: minimize subject to ( S i  X  P l + v i ) /f  X  y i  X  ( S i  X  P u + v By setting w n , we can significantly reduce the search space. Actually, we can show that the computational complexity to solve the above problem is O ( M w ) , where M = m 0 + m 1 +  X  X  X  + Since m is the total number of appliances and  X  m , the problem can be solved quickly using tools such as CVX 2 . 0 with a Gurobi engine [8].

Step 2. For the k -th time window that starts from time k , we use the following strategy to handle consecutive corrupted data: if the data point at time k is identified to be corrupted, i.e. , v recover the current state vector from the previous one, i.e. , set S S
Step 3. Repeat above two steps from k = 1 to k = n , and solve problems in form of Equation (10) sequentially. After n iter-ations, we can get a sequential solution v 1 ,v 2 ,  X  X  X  ,v corrupted data set is C = { y i : v i 6 = 0 , 1  X  i  X  n } , in which v the corrupted degree of load data y i .

Algorithm 1 shows the pseudo code of SLOA.
Clearly, one key question in SLOA is to determine a suitable size of the local window. In principle, we want the size to be as small as possible to speed up the calculation, but a size too small may result in a poor solution largely deviating from the global optimal one. For example, in the extreme case of w = 1 , SLOA becomes a simple greedy search algorithm, where the final solution may not be good. On the other hand, if w = n , the problem becomes the same as Equation (8), which is hard to solve. What is the minimum local window size that leads to a nearly global optimal solution?
Since it is hard to obtain a strict proof that the local optimal solutions together lead to the global optimal solution, we use the following heuristics to estimate the minimum local window size: within the local window, it should be possible that one state vector can be transited to any other state vectors in the vector space. In other words, within the local window, we should cover all possible state vectors in the search. This heuristics sheds light on finding the minimum local window size, as formulated below.

D EFINITION 4 (O VERLAP I NDEX ). Consider m appliances denoted by a set { R 1 ,R 2 ,  X  X  X  ,R m } , where R i  X  [ l represents the i -th appliance X  X  power range. The overlap index of m appliances is defined as: where p max and p min stand for the maximum and minimum power of all appliances, respectively, and I ( x ) is an indicator function
Note that the denominator R P max P cludes all valid power values, i.e. , the ones that can be covered by at least one appliance X  X  power range. We can see that the overlap index represents the number of appliances whose power range cov-ers a valid power value, averaged over the whole power range of all appliances. In particular, O = 1 indicates that no pair of appliances have overlapped power, and O = m means that all appliances have the same power range. Intuitively, when O is large, there is a good chance of finding multiple local optimal solutions to Equation (10), since there are multiple equivalent choices to turn on/off appliances in each iteration.

With the heuristics in estimating the minimum local window size, we have the following result.

L EMMA 1. Given the overlap index O of m appliances and the upper bound  X  on the total number of on-off state switches in a timeslot, in order to get the nearly global optimal solution to Equation (8) via Equation (10), the minimum local window size w = max { m  X   X  O , 1 } .

P ROOF . We prove the following condition holds: within the lo-cal window, we can cover all possible state vectors in the search.
First, the value of w relates to upper bound  X  (  X  m ) on the total number of on-off switches in one timeslot. It is obvious that from time t = i to t = i + 1 , the state vector S i can only change to another state vector S i +1 , with k S i +1  X  S i k 1  X   X  . If  X  = m , then within one step, a state vector is allowed to change to any other state vector. On the other hand, if  X  = 1 , within one step, a state vector can only change one value in the vector. In other words, from one state vector, it requires at least m  X  timeslots to reach any other state vector in the state vector space, i.e. , w  X  Figure 2: Energy monitoring platform and appliances X  power ranges
Second, the overlap index O can reduce the value of w . Based on the meaning of O , m appliances with overlap index O are equiv-alent to [ m/O ] appliances without overlapped power. Replace m with m/O , we can get w  X  m  X   X  O . Considering w  X  1 , we con-clude: Since we want w to be as small as possible, w = max { m  X   X  O 2
Note that, although the minimum local window size obtained above is an estimation, it works effectively in our experiments with real-world data as well as with synthetic data.
Given n load values, m appliances, and the upper bound  X  (  X  m ) on the total number of on-off state switches in a timeslot, the computational complexity of the original problem (Equation 8) is O ( M n ) , where M = m 0 + m 1 +  X  X  X  + m  X  . Using SLOA, solving the optimization problem (Equation 10) for n times results in the time complexity of O ( n  X  M w ) , where w  X  Z + and w n . Please note that the appliance number m is a constant value and w is also a small constant.

Obviously, the larger the value of w , the higher the computa-tional complexity. Fortunately, the overlap index of appliances in a house/building is usually high, as observed in our real-world testbed. This fact allows us to select a small local window size fol-lowing Lemma 1. Therefore, in the application scenarios, SLOA can approach the NP-hard problem heuristically and efficiently. We will show that this algorithm indeed can provide a good solution with abundant experimental results in the following sections.
We evaluate our method with real-world trace data from a real-world energy monitoring platform. We monitored the appliances X  energy consumption of a typical laboratory and a lounge room on the fifth floor of the Engineering/Computer Science Building at University of Victoria. The real-time power of laptops, desktops and some household appliances was recorded. Each appliance X  X  power level was measured every 10 seconds and the measurement results were transmitted with ZigBee radio to a server that stores the data. The monitored appliances and their regular power shown in Fig. 2.

We collect the data over a two-month period. Fig. 3 demon-strates one-week and one-day load data collected by our platform. For clear illustration, we only show one-day data as an example. Note that even in a lab setting like ours, there indeed exists some apparent corrupted data, indicated by the dashed red dots in Fig. 3 Figure 3: One-week and one-day load data collected via the energy monitoring platform.

In order to introduce more corrupted data, we asked three stu-dents to distort the load data with  X  X alsification X , i.e. , they were asked to arbitrarily modify the aggregated load data within the range of [0 ,  X  ) . These changed data together with the original corrupted ones were labeled and used as the ground-truth to verify the performance of our method.

Since the existing appliance-oblivious load data cleansing meth-ods, such as B-spline smoothing, detect outliers and consider out-liers as corrupted data, we use the terms  X  X utliers X  and  X  X orrupted data X  interchangeably hereafter. For outlier detection, four statis-tical results can be obtained: (1) true positive ( TP ), the number of points that are identified correctly as outliers; (2) false positive ( FP ), the number of points that are normal but are identified as outliers; (3) true negative ( TN ), the number of points that are nor-mal and are not identified as outliers; (4) false negative ( FN ), the number of points that are outliers but are not identified. Using TP,FP,TN and FN , we evaluate the following three broadly-used performance metrics: precision, recall, and F-measure. Pre-cision is the ratio of the number of correctly detected corrupted values over the total number of detected values; recall is the ratio of the number of correctly detected values over the number of pre-labeled corrupted values; and the F-measure is a harmonic mean of precision and recall, i.e. ,
For comparison, we implement and test an appliance-oblivious data cleansing method, B-spline smoothing, which is introduced in [5] to identify corrupted load data. In the B-spline smoothing
An appliance X  X  regular power is an approximate range around the rated power where this appliance works.
The corrupted data mainly comes from some incorrect power val-ues from the laptop that occasionally reports impossible values such as hundreds of Watts. method, we set the confidence coefficient  X  = 0 . 05 , which results in a confidence interval of 95% . We treat the degree of freedom ( df ) as a variable, whose value is trained when smoothing the load curve data. For our method, the overlap index is obtained as O  X  2 , and the upper bound of on-off switching events of appliances within the sampling interval is set to 2 , i.e. ,  X  = 2 . According to Equa-tion (13), the local window size, i.e. , the value of w in Algorithm 1, is set to 3 . Since the value of local window size is an estimation, in order to obtain more comprehensive performance evaluation for our method, we also vary the local window size in a range. Table 1 summarizes some of the results from the two methods. Furthermore, Fig. 4 and Fig. 5 illustrate one of the outcomes from our appliance-driven method and the B-spline smoothing method, respectively. From the results, we have the following interesting observations.
In addition to the data collected from university facilities, we also evaluate our method with real-world trace data from some typ-ical households. Load data of four houses (each house possesses 20  X  40 appliances) were collected by Belkin Inc. and made on Kaggle [16]. With the data from one of the four households, we evaluate the performance of our appliance-driven method. The ex-perimental process is similar to that in Case 1, and the results are summarized in Table 2.
 Table 2: Results of corrupted data identification on household data ( w = 1 )
To thoroughly test our method, we evaluate its performance us-ing large-scale synthetic data that simulates a large number of ap-pliances and much diverse energy patterns. With different synthetic datasets, we can also test the robustness of our method. the virtual appliance (Section 4.)
There is no standard model for the load curve data of a house, since the data actually results from a complex process related to human activities. We thus use the Monte Carlo simulation to gen-erate the load data using the following method:
Poisson distribution is a good model for situations where the to-tal number of items is large and the probability that each individ-ual item changes its state is small. It has been broadly adopted to simulate events related to human behavior, such as the number of telephone calls in a telephone system.
To introduce some corrupted data and test the effectiveness of our method, we  X  X orrupt" some data values by replacing them with random values uniformly distributed between [0 ,Max ] , where Max is a given large constant. The time interval of introducing corrupted data is assumed to follow an exponential distribution with the mean value of  X  .
The parameters used to generate the synthetic data and the cor-rupted data are listed in Table 3.
 Table 3: Parameter settings for load data generation and cor-ruption Figure 6: Result of corrupted data identification on synthetic data with our appliance-driven method ( w = 1 , X  = 5 ) Figure 7: Result of corrupted data identification on synthetic data with B-spline smoothing method( df = 160 )
We treat the bound on the total number of on-off switches in a sampling interval  X  as a variable. To speed up the processing, we set the local window size to 1 . The small local window size may not lead to the best performance of SLOA. However, as shown in our experimental results, even with this setting, our method already performs better than B-spline smoothing. For the B-spline smooth-ing method, the degree of freedom ( df ) is set as a variable and is trained when smoothing the synthetic data.

The performance results of our method and the B-spline smooth-ing method are summarized in Table 4. Fig. 6 and Fig. 7 illustrate one of the outcomes from our method and the B-spline smoothing method, respectively.

From the results, we can see that the our method works effec-tively on large-scale synthetic data. In particular, the precision of our method increases with increase of  X  , and can even reach 100% . This result indicates that our method can provide exactly correct identification when  X  is large enough. Regarding the overall per-formance in view of F-measure , our method works better with a smaller  X  value and outperforms B-spline smoothing clearly.
In practice, we often meet the situation that all data within a small time interval are corrupted or lost. Consecutive corrupted data poses a big challenge to regression-based methods, as will be illustrated in this subsection. Figure 8: Identification of consecutive corrupted data with our appliance-driven method Figure 9: Identification of consecutive corrupted data with B-spline smoothing method ( df = 100 )
To introduce consecutive corrupted data, we replace the load data in a small time window as 0 s, as shown in the upper part of Fig. 8. We then use our method and the B-spline smoothing method to test the data. Fig. 8 and Fig. 9 illustrate one outcome from our appliance-driven approach and the B-spline smoothing method, re-spectively.

From the results, we can see that our method does much bet-ter than B-spline smoothing for consecutive corrupted data iden-tification. With  X  = 5 , our method can correctly identify all the corrupted data. On the other hand, even though we regulate the parameters for B-spline smoothing, it almost failed every time to identify even half of the corrupted data.

An interesting phenomenon can be found around the consecutive corrupted data in Fig. 9. There is an apparent trend with B-spline smoothing to fit the corrupted data. This is mainly because the B-spline smoothing method tries to fit the curve pattern and reduce the total bias error with global optimization, indicating that it cannot deal with consecutive corrupted data well.
One may question whether the performance of SLOA relies on a correct initial state vector, accurate information regarding appli-ances power ranges, and an accurate estimation on appliances X  on-off states. all of such information may be hard to obtain in practice. To answer this question, we test the robustness of SLOA. We use the synthetic data created using the same parameters in Table 3. We first disclose the test results and then explain the reasons.
For this test, we change the initial state of an appliance to a ran-dom 0 -1 value, and perform multiple tests. Fig. 10 shows one of the outcomes. We find that, even with an incorrect initial state, our method can always recover to correct load data after a few steps. This result indicates that our SLOA method is robust against inac-curate initial power state setting. Figure 10: Fast recovery of estimated load starting from a ran-dom initial state
In practice, we may not precisely know the power ranges of ap-pliances. Based on this consideration, we run extra simulations to test the robustness of our method when the power ranges of appli-ances is inaccurate. We carry out two kinds of tests as follow. We do not consider the situation where appliances X  power ranges are narrowed, since intuitively a user can always widen an appli-ance X  X  power range if she/he is not sure about the right values. The test results are summarized in Table 5. The results clearly indicate that, with inaccurate or even wrong power ranges of appliances, our method can still identify corrupted data with high precision.
We have seen that our method can give correct bounds for energy consumption most of the time. Accordingly, we might infer that the estimated states of the appliances should be the same with the real situation, or at least quite close.

In order to verify this conjecture, we calculate the difference (one-norm distance) between the estimated state S e and real state S r at each time instance. Fig. 11 shows the result.

To our surprise, the estimated states are not close to the real states, and actually deviate remarkably from the real states. We can Figure 11: Difference between estimated state S e and real state S see that in Fig. 11, the mean distance between S e and S r 25 , indicating that nearly half of the appliances are not estimated with correct states. This shows that the solution to the CDIP prob-lem is not unique but multiple, and our method can provide right load data without need to always find the right states of appliances.
In real life, a lot of appliances are with similar or overlapped power range. In this sense, we indeed do not need to know the exact state for similar appliances as long as we can give a good approximation for their total consumption. In addition, due to the temporal sparsity of on-off switch events in the short sampling in-terval and the fact that only some appliances are on at any time instant, the negative impact of inaccurate power range estimation on one appliance can be offset by the negative impact of incorrect state estimation of another appliance. The offsetting is enforced au-tomatically by the optimization objective function that minimizes the gap between the actual load data and the estimated value.
Our approach is based on the assumption that customers are will-ing to collaborate and provide their appliances X  information. This naturally causes privacy concerns. Nevertheless, this assumption is nothing special in real-world applications, since customers have to put certain trust on service providers. Commercial energy moni-toring platforms, such as PlotWatt [27] and PlugWise [28], indeed require users X  appliance information.

Another concern of our solution is that users may not be able to know the power model of each appliance. Nevertheless, the information needed in our solution is simple and could be found from users X  manual, technical specification or public websites such as [13, 19]. In addition, cheap per-appliance monitoring devices [7] can be used to obtain the required information. In our above test, it can be seen that it is unnecessary to precisely know the power ranges of appliances, and our solution can tolerate up to 20% of estimation errors in appliances X  power ranges.
To answer the industrial call for improving quality of load data, we developed a new appliance-driven approach for corrupted data identification that particularly takes advantage of information avail-able on the demand side. Our appliance-driven approach considers the operating ranges of appliances that are readily available from users X  manual, technical specification, or public websites [13]. It identifies corrupted data by solving a carefully-designed optimiza-tion problem. To solve the problem efficiently, we developed a se-quential local optimization algorithm (SLOA) that practically ap-proach the original NP-hard problem approximately by solving an optimization problem in polynomial time. We evaluated our method using real trace data from a university facility and four typ-ical households, and large-scale synthetic data generated by Monte Carlo simulation. Test results indicate that our method can pre-cisely capture corrupted data. In addition, SLOA is robust under various test scenarios, and its performance is resilient to inaccurate power range information or inaccurate power state estimation.
Our method greatly augments the arsenal of existing load data cleansing tools to minimize human effort in identifying corrupted data. How to replace aberrant values and missing values is out of our focus, because this issue is relevant to utilities X  internal rules and thus requires human interaction. Considering various policies and methods for load data imputation will be our future work. This work was partially supported by the Natural Sciences and Engineering Research Council of Canada, the National Natural Sci-ence Foundation of China (No.61373152), the Key Technologies R&amp;D Program of China (No.2012BAH08B01), and a BCIC NRAS Team Project. All opinions, findings, conclusions and recommen-dations in this paper are those of the authors and do not necessarily reflect the views of the funding agencies.
