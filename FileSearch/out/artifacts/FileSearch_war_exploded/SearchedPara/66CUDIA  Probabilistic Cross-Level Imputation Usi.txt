 In healthcare-related studies, individual patient or hospital data may contain private or confidential information, such as medical conditions. Public disclosure of raw data potentially introduces ethical or legal issues. Thus, in many cases, disclosure of sensitive raw data requires legally authorized protocols, such as the Health Insur-ance Portability and Accountability Act (HIPAA) Privacy Rule [HIPAA Compliance Assistance 2003], which includes several de-identification techniques [Emam and Fineberg 2009], or data providers X  consent to share. Adherence to these regulations can consume considerable time and effort and may result in only a limited number of attributes or features being provided to researchers. On the other hand, such measures may be provided publicly at higher or more aggregated levels, such as state-level, county-level summaries or averages over health zones, such as hospital referral regions (HRR) or hospital service areas (HSA) (e.g., see http://www.data.gov/health or http://www.cdc.gov/datastatistics/ ). Averaged statistics over such levels protect privacy by blending individual information in group summaries, thereby making individual data subjects invisible or unidentifiable over sensitive data. This kind of aggregate information may provide richer feature spaces compared to publicly available individual-level data without infringing privacy and legal issues.
Aggregate information has been frequently used across various domains, such as po-litical science and ecological and healthcare-related studies due to its relatively easier access. A common practice for dealing with such information is to regard group sum-maries as representatives for the individuals in the same group. Individual-level fea-ture interactions can be inferred by applying several statistical tools, known as  X  X ross-level inferences X  [Achen and Shively 1995; King 1997], in which partitions based on such levels consist of homogeneous characteristic individuals (the constancy assump-tion). However, this assumption is valid usually under restrictive conditions, such as very small-sized partitions. Even worse, the constancy assumption is difficult to verify in real applications. In fact, many partitions derived from such levels are composed of a heterogeneous population, which causes the ecological fallacy. Thus, the applicability and the effectiveness of cross-level inference are still controversial [Freedman 1999].
Aggregate variables in healthcare data are not the only problem that hinders individual-level statistical analyses or data mining research. Many healthcare datasets have limited scope of variables or features, as the survey has a pre-defined purpose. In such cases, designing additional sets of surveys might be inappropriate due to cost or temporal dynamics. Although combining multiple datasets from different sources can be an alternative solution, their aggregation levels generally differ in their geograph-ical regions or administrative units. For example, routinely collected administrative datasets, such as national registers, aim to collect information on a limited number of variables for the whole population, while survey and cohort studies contain more detailed data from a sample of the population. To the best of our knowledge, system-atic integration of multiple data sources with different aggregation levels has not been studied thoroughly, and we first propose a theoretical framework on combining and utilizing such datasets. We expect that a proper utilization of such data might (i) help to reduce any extra cost or (ii) extend the scope of current healthcare research.
In this article, we seek a better utilization of such aggregated information for aug-menting individual-level data. Suppose two datasets from possibly multiple sources are available for research, where their aggregation levels are also different. Figure 1 shows an example of data presented at different levels of aggregation: state-level and county-level. We refer to the dataset with a finer granularity as the individual-level dataset and to the other dataset as the aggregate-level dataset. Assuming that the dataset of interest is generated by a mixture model that represents underlying heterogeneous groups, we introduce a novel generative process that captures the underlying distribu-tions using a Bayesian directed graphical model and the central limit theorem. Despite the limited nature of given aggregated information, our clustering algorithm provides not only reasonable cluster centroids, but also imputes the unobserved individual fea-tures more effectively. These cross-level imputed features better reflect the underlying distribution of the data, thus a subsequent predictive model using such extended in-formation shows improved performance. Many datasets in the healthcare domain are divided into multiple tables containing different levels of aggregation (sometimes ob-tained from different sources), and the suggested methodology in this article could be useful in increasing the utility in such scenarios.

The rest of the article is organized as follows: We begin by reviewing traditional statistical cross-level imputation techniques and then outline various inference mech-anisms that will be used extensively in our approach. In Section 3, we approach the problem by modeling the data generation process. We start from a generic Bayesian clustering model, then step-by-step, we impose additional constraints and transform the simple model to suit the problem setting. After presenting the final model, its model parameter estimation technique is explained in Section 4. Due to the complex-ity of the model, a new approximate Monte Carlo expectation maximization (MCEM) algorithm is developed, which is more computationally efficient than a generic MCEM technique. Moreover, a deterministic algorithm that can be used as a parameter initial-ization method is derived as a valuable artifact of our probabilistic approach. Using the learned model parameters, in Section 4, we propose a cross-level imputation formula, which basically enables us to estimate the masked individual values for the aggregate features. The imputation procedure is shown to be an unbiased estimator, and its sta-tistical properties are analyzed in detail. To highlight the effectiveness of our approach, we first examine the imputation quality using a simulated dataset in Section 6.1. Then we demonstrate various possible settings in real applications using the Behavioral Risk Factor Surveillance System (BRFSS) dataset in Section 6.2. Finally, we discuss the possible constraints of our framework and future work in Section 7. In this section, we summarize three bodies of related work, starting from traditional imputation techniques in statistics. This is followed by ecological study techniques, where aggregated and individual information are both available. Finally, we briefly discuss various approaches that are used to make inferences in Bayesian graphical models.

Imputation Techniques in Statistics. In statistics, there is a vast literature on im-putation techniques that are mainly used to substitute missing values in data [Rubin 2004]. A once-common method is cold-deck imputation, where a missing value is im-puted from randomly selected similar records from another dataset. More sophisticated techniques, such as the nearest neighbor imputation and the approximate Bayesian bootstrap, have been developed to supersede this original method. As a special case, when geographical information is missing in the data, geo-imputation techniques are widely used, where the imputation is taken from approximate locations derived from associated data [Henry and Boscoe 2008]. Regression estimation [Tabachnick and Fidel 2001] is another widely used imputation technique in statistics. In regression estima-tion, the variable with missing data is treated as the dependent variable, while the other variables are treated as the independent variables. A normal regression is per-formed based on this setting, then the missing values are replaced by the regression results. Regression estimation assumes a sufficient number of complete individual samples, which is not the case in our setting. If missing values are rather sparse, a Bootstrap technique can be used to improve a subsequent predictive modeling per-formance [Brownstone and Valletta 2001]. However, these traditional techniques are based on individual-level data, and some of them have limited applicability.
Ecological Studies. In ecological studies, aggregated information is usually the unit of analysis, as individual information is usually not available due to expensive acquisition costs or legal issues. Most of the ecological analyses are based on ecological regression, which uses Goodman X  X   X  X onstancy assumption X  [Goodman 1953, 1959; King 1997]. The constancy assumption states that behavior within an ecological group does not depend on the other specific characteristics of the group, that is, a group consists of homogenous individuals. Although ecological studies have been used frequently across multiple domains, such as social science and healthcare analysis, the validity of the studies is still controversial because of the difference between ecological correlation and individual correlation [Robinson 1950], which is also known as the  X  X cological fallacy X . In many cases, the constancy assumption may not hold because regional and contextual effects on ecological groups cannot be overlooked, and one ecological group is rarely homogeneous in its behavior.

Ecological regression analysis based on the constancy assumption is vulnerable to confounding factors and aggregation bias. Traditionally, the aggregation bias has been tackled in two ways: (i) by assuming a quadratic model rather than a linear model, or (ii) by calculating interval estimates for unobserved individual features rather than point estimates. In the first method, a quadratic model is obtained by relaxing the constancy assumption [Achen and Shively 1995]. In this framework, an individual in a specific ecological group is no longer independent of the group, and this relationship is specified by a linear model, resulting in a quadratic model at aggregation level. However, the added assumption is not verifiable in most of the cases, and the inter-pretation of the results becomes harder. In the second method, unobserved individual features are bounded to satisfy aggregated information constraints. This technique is also known as the  X  X ethod of bounds X  [Duncan and Davis 1953]. But the bounds are too broad to be informative in practice and are typically only used as a sanity check tool.
Despite their theoretical instabilities, ecological analyses continue to be used due to relatively easier access to the aggregate data [Freedman 1999]. Fortuitously, in recent years, it has been reported that auxiliary individual-level information can help to reduce the ecological fallacy [Wakefield and Salway 2001]. In the hierarchical related regression (HRR) framework, auxiliary individual-level information represents a small fraction of the individual samples that constitute the aggregate information [Jackson et al. 2008, 2009]. This setting is useful when acquisition costs of getting individual data are expensive, so detailed information is only obtained from a small portion of the entire population. The HRR model relates the regression coefficients from both aggregate and individual data. This analysis has been shown to reduce the ecological bias, but the type of the auxiliary information used in HRR is different from the setting in this article. In our setting, auxiliary individual-level information has no overlapping feature or column with available aggregate-level data. This happens because aggregated features are privacy sensitive and hence cannot be revealed at an individual level for even a small subset of the population. We instead focus on a generative process of such data and derive an inference mechanism to get estimated individual values for the aggregated features. From the generative process, heterogeneity of ecological groups is naturally captured by suitable mixture distributions, resulting in better imputation.

Inference Algorithms in Bayesian Graphical Models. In Bayesian graphical models, such as the model presented in this article, inference can often be challenging. The expectation maximization (EM) algorithm is a popular approach when latent variables are present in the models. However, many sophisticated models, such as Latent Dirich-let Allocation (LDA) [Blei et al. 2003], have intractable posterior distributions for the latent variables. To approximate the posterior distributions, other techniques, such as variational EM algorithm, Gibbs sampling, and collapsed Gibbs sampling, have been proposed. Although their computational complexities and assumptions are slightly dif-ferent, their performances are often comparable [Asuncion et al. 2009]. In this article, we demonstrate an approximated Gibbs sampling approach, which is specialized for our setting. Then we propose a related deterministic algorithm that is not only much faster but also scalable to massive datasets. We denote the set of attributes or features that are available at the individual level by x , where  X  X ndividual X  refers to entities at the finest resolution available. The features that are observed only at an aggregated level are denoted by x u , where u denotes  X  X nobserved X  at the individual level. Thus there is an underlying complete dataset, D The data provider only provides the values of observed variables though. In addition, it  X  for any distinct p , q . These partitions specify the aggregated values provided on s different partitions (and hence levels of aggregation) may apply to different unobserved variables. Though our approach can be readily extended 1 to cover such situations, in this article, we consider a common partitioning to keep the notation and exposition simple.

Suppose we want to find K clusters denoted by { C 1 , C 2 ,..., C K } in the complete titions typically reflect geographical or administrative units, so that the partitions don X  X  match with the intrinsic clusters. To cater to the unobserved data, an assump-the mixing coefficients of the partition p .Let  X  k and  X  k be the sufficient statistics for the individual level, an LDA-like clustering model can be built based on the conditional independence assumption, as in Figure 2(a), where  X  is sampled from a Dirichlet distribution parametrized by  X  .As x u and x o are independent given C k , they can be separated using different nodes. Figure 2(b) shows a modified clustering model that accommodates the aggregated nature of the unobserved variables. In the model, x u is not observed; rather the derived (aggregated) features s are observed.

Even though the model of Figure 2(b) captures the problem characteristics, it is highly inefficient and contains redundant nodes. Fortunately, the complexity of the model can be reduced by removing the unobserved nodes x u  X  X  if N p is large enough. Let  X  and T 2 k be the mean and variance of the distribution, p ( x u | C k ). Using the linearity of mean statistics and the central limit theorem (CLT), s p can be approximated as being generated from a normal distribution as follows. of s p  X  X , since the CLT only requires the mean and variance of the samples. As the actual values of x u  X  X  don X  X  contribute to the likelihood of this process, x u can actually be removed, resulting in the efficient Clustering Using features with DIfferent levels of Aggregation (CUDIA) model, as shown in Figure 3. The full generative process for CUDIA is as follows.
 For s p in D s ,  X  is sampled from a Dirichlet distribution parametrized by  X  , and observed sample mean statistics s is generated from a normal distribution parametrized by a mixture of true means  X   X  X  and a covariance 2 . z  X  X  in each partition are sampled from a multinomial distribution parametrized by  X  , which is specific to the partition, and corresponding x depends on the properties of the variable x o  X  X . For conciseness, the remaining sections of this article will denote x o as x . From the generative process, the likelihood function of the CUDIA model is given by The posterior distribution of the hidden variables,  X   X  X  and z  X  X ,isasfollows. The key inferential problem is how to calculate this posterior distribution. A generic EM algorithm [Dempster et al. 1976] cannot be applied, since the normalization constant of its posterior distribution in Equation (4) is intractable. Collapsed Gibbs sampling [Liu 1994] also cannot be applied because  X  cannot be integrated out due to non-conjugacy between s and  X  in p ( x , s ,  X  , z |  X ,  X ,  X  ). In this case, the model can be learned using either variational methods or Gibbs sampling approaches, and this article follows the latter alternative. Nevertheless, na  X   X ve Gibbs sampling approaches are computation-ally inefficient, thus this article employs an approximated Gibbs sampling approach, which can be applied when the dimension of x is small. The model parameter estima-tion follows the MCEM algorithm [Booth and Hovert 1999] using this approximation technique. In the CUDIA model, the latent variables are  X  and z .Sowehave For each partition p , the Gibbs sampling is performed as follows. However, sampling  X  is problematic, as Equation (6) is not a trivial distribution. Instead of sampling directly from Equation (6), Metropolis-Hastings (MH) algorithm can be used with a proposal density Dirichlet (  X  ). This algorithm is described in Algorithm 1.
Sampling from a Dirchlet distribution might be computationally heavy in some pro-gramming languages, such as Numpy in Python. 2 As an alternative, the prior distri-bution of  X  can be replaced by a logistic normal distribution or a uniform distribution by modifying the CUDIA model, so that we can adopt a different proposal density function according to the modified model. In our empirical evaluation, different prior distributions showed marginal differences in their performances. Even though this MH algorithm inside the Gibbs sampling becomes inefficient when dealing with large datasets, the sampling step of z  X  X  can be avoided given a large enough size of N p for low dimensional x  X  X .

The overall idea of this approximation is as follows: If x is generated from an exponential family distribution, p ( z k | x , X  ) is continuous with respect to x ,sothat are in the ball is large enough, then n ( z  X  k ) in the ball can be approximated as n ( z  X  k )  X  X  B r ( x c ) | E [ z k |  X  p , x c ]  X  x  X  B when N p is large and a low dimensional x is given, even better when x is a discrete vari-able. Assuming partitional balls over D p x , n ( z  X  k ) in the partition p can be approximated described in Algorithm 2.

The last line of the algorithm is derived by using the Partition Theorem of conditional expectation [Grimmett and Stirzaker 2001]. As a result, the actual sampling process occurs only in MH sampling (Algorithm 1). In this article, we used a burning period of ten samples, and N Gibbs  X  50 to 100 [Agarwal and Chen 2009]. Our empirical results show that with this small number of samples, the algorithm converges with reasonable speed.
 The model parameters are  X  ,  X  ,and  X  . Maximization on  X  and  X  can be easily per-formed and won X  X  be discussed in this article.  X   X  and T  X  can be obtained by alternating the maximization steps on  X  and T , respectively. However, if we assume T 2 k =  X  2 k I , the maximization step on  X  can be simplified. To simplify the notation, the following matrices are defined.
 Note that As s is normally distributed in CUDIA, the relationship between S i and H  X  i in the CUDIA model can be described as However, each s p has a different variance, thus the solution of weighted linear regres-sion can be applied to get the optimal H  X   X  i .
 Note that rank (  X  T W  X  ) = rank (  X  ) = K w. p . 1 i f P &gt; K . However, mean values (  X  ) are susceptible to outliers from the Gibbs sampling. To ensure a more stable solution, regularization techniques can be incorporated. For example, if a Ridge penalty is used, then H becomes The entire inference algorithm is described in Algorithm 3.
 The CUDIA model leads to an intuitive deterministic hard clustering algorithm. Start-ing from the log-likelihood of CUDIA, the hard clustering objective function is obtained as follows (see Appendix A for details). statistics. Local minima of this objective function can be found by alternating minimiza-tion steps between z and (  X ,  X  ), as in Algorithm 4. One iteration of this algorithm costs ( KN ). For a fixed number of iterations I , the overall complexity is therefore ( KNI ), which is linear in all relevant factors. The complexity of this algorithm is the same as the k-means algorithm, promising its scalability to massive datasets. Moreover, this algorithm can be used as an initialization step for the probabilistic algorithm (Algorithm 3), which in turn will reduce the total running time.

The squared loss function in the deterministic algorithm is appropriate for an ad-ditive Gaussian model. Our approach can however be generalized to any exponential family distribution (of which the Gaussian is a specific example) by exploiting the bijec-tion property between exponential family and the family of loss functions represented by Bregman divergences [Banerjee et al. 2005]. Given two vectors x and  X  , the Bregman divergence is defined as where  X  (  X  ) is a differentiable convex function and  X   X  (  X  ) represents the gradient vector of  X  evaluated at  X  . Although the Bregman divergence possesses many other interesting properties, this article focuses on its bijective relationship to the exponential family distribution.

This bijective relation can be exploited when clustering data points cannot be ap-propriately modeled using the Gaussian distribution, as in the Bregman Hard/Soft Clustering algorithms [Banerjee et al. 2005]. Table I shows the relationship between specific Bregman divergences and their corresponding exponential family distributions. The results in Banerjee et al. [2005] state that minimizing the negative log-likelihood is the same as minimizing the corresponding expected Bregman divergence. For exam-ple, if clustering data points are generated by a mixture of Gaussian distributions, the maximum likelihood parameters can be obtained by minimizing the squared loss func-tion, which is the corresponding Bregman divergence for Gaussian distributions. Using this bijection and adopting the idea from the Bregman Hard Clustering algorithm, the deterministic algorithm of CUDIA can be extended by modifying the assignment step of Algorithm 4 as follows.  X  Assignment Step  X  can be chosen based on the distribution of x and the update step remains the same. Note that s p follows a Gaussian distribution according to the central limit theorem re-gardless of the underlying distribution of x u  X  X . Thus, the second term in Equation (19) remains the same, but the first term is changed to capture various exponential distri-butions. The update step remains the same as in Algorithm 4, since a unique minimizer of a Bregman divergence is given by its mean (see Proposition 1 [Banerjee et al. 2005]).
This extended algorithm captures various distributions while maintaining the orig-inal complexity. Furthermore, the linkage between Bregman divergences and the ex-ponential family distributions enables probabilistic interpretations on the resultant clustering assignments, as in the Bregman Soft Clustering algorithm. Perhaps the most useful case is when the vectors represent probability distributions, in which case the KL-divergence (another special case of Bregman divergences) is the appropriate loss function to use. After all the parameters of the CUDIA model are learned, the model allows us to impute the unobserved features x u  X  X  at the individual level. Given the observed features and learned parameters, the imputation is as follows. Equation (21) provides the exact imputation formula for any p ( x u | z k ), depending on the form of the cluster-conditional pdf of the unobserved features. For example, if x u | z k is generated from an exponential family distribution with a mean  X  k and a covariance  X 
I , the imputation formula obtained is
This imputation method also can be applied to the deterministic algorithm. The bijective relationship between Bregman divergence and exponential family yields a soft cluster assignment as follows.
 For example, if x o  X  X  are generated from a mixture of Gaussians, which means d  X  ( x ,  X  ) = As another example, if x o  X  X  are generated from a mixture of d -D multinomial distribu-tions, using the bijective relationship, we get d  X  ( x ,  X  ) = d j = 1 x j log x j  X  Then, Equation (23) becomes Thus, the deterministic algorithm provides not only the cluster centroids/assignments but also the basic imputation framework on the unobserved features, which in turn can be used for preliminary tests for the model X  X  applicability.

The imputation formula, Equation (22), calculates an unbiased estimate for x u . More-over, the variance of the imputation is inversely proportional to the size of the data. The detailed properties of this imputation are derived and explained in Appendix B. In this section, we show how CUDIA can be applied to data that have differently aggregated variables, by considering the case when two variables aggregated at two different levels. The extension from two variables/levels to more will be clearer when we fully describe the process.

We presume that these two variables are present at the individual level, namely x a u and x b u , but not observed at the individual level. Thus, the underlying complete dataset variable, we have different partitionings, P a and P b , where P a = P b . Then, aggregated variables, s p a and t p b , are derived from their corresponding partitionings. As in the CUDIA process, a probabilistic generative process can be used to model the observed variables x o , s p a ,and t p b ; however, the resultant generative process would involve necessarily a deeper hierarchy structure, and its extension to non-nested par-titionings would be also problematic.

In lieu of building more complex probabilistic models, we introduce two simple ap-proaches, which are easily extensible to more number of different aggregation levels. The first approach is a brute-force parallel application of the CUDIA imputation. In this approach, each aggregated variable is paired with the individual-level variable(s) x , and then the CUDIA imputation is applied to each pair of variables. However, this approach ignores information between s p a and t p b ,whichmaybeuseful in some cases. To utilize such information, we suggest the second approach, which se-quentially imputes individual-level variables. In the second approach, one aggregated variable is first paired with the individual-level variable, then its individual-level value is estimated using the CUDIA imputation. This imputed variable can be further utilized as another (estimated) individual-level variable when imputing the other aggregated variable. These two approaches can be easily implemented from the original CUDIA algorithm with minor modification, thus we primarily focus on the common partitioning case throughout. Note that the performance of these two approaches and other possible extensions need more thorough analysis, and we leave these for our future work. In this section, we provide two kinds of experimental results. (i) First, imputation quality of the CUDIA model is assessed using a simulated mixture of Gaussians data. (ii) Then, its applicability to predictive modeling 3 is discussed using the data from the Behavioral Risk Factor Surveillance System (BRFSS). We demonstrate CUDIA X  X  properties for imputation using a simulated dataset. The dataset is generated by a mixture of three 2D Gaussians ( K = 3), as shown in Figure 4(a). We generated 2,000 samples, then partitioned into ten groups according to the randomly generated mixture coefficients ( ). Thus, P = 10 and N p = 200  X  p . The mixture coefficients are as follows. From this dataset, we assume the first column (x-axis column) is the auxiliary individual-level information, and the second column (y-axis column) is aggregated within each partition. If the unobserved individual-level second column is imputed by its corresponding aggregated value, that is, everyone in the same partition shares the same feature value, then the resultant dataset is as in Figure 4(b). We can observe that this na  X   X ve imputation scheme does not reflect the underlying heterogeneous distributions. Next, we run our CUDIA model over these individual-and aggregate-level datasets to discover the underlying mixture distributions. Figure 4(c) shows the CUDIA imputation based on Equation (22). The CUDIA imputation captures the hidden underlying mixture distributions, and the imputation follows the mean statistics of each intrinsic cluster. Figure 5 shows the mean squared error (MSE) between the true and the imputed data points. The CUDIA imputation achieves lower MSE as well as lower variance compared to the na  X   X ve imputation. In this section, we provide the experimental results using a real world dataset in various settings.

Dataset Description. We demonstrate the proposed method using the BRFSS 2009 dataset. BRFSS (Behavioral Risk Factor Surveillance System) 4 is the world X  X  largest telephone health survey since 1984, tracking health conditions and risk behaviors in the United States. The data are collected monthly in all 50+ states in the United States. The dataset contains information on a variety of diseases like diabetes, hypertension, cancer, asthma, HIV, etc., and in this article, we mainly focus on diabetes rather than on other diseases. 5 The 2009 dataset contains more than 400,000 records and 405 variables, and the diabetic (positive class) ratio is 12%. Empty and less-informative columns are dropped, and we finally chose six variables to perform our experiments. The selected variables are Age, Body Mass index (BMI), Education level, Income level, Hypertension, and Hyper-cholesterol.

In many cases, revealing personal disease records or medical conditions can be prob-lematic, or even cause traumatic situations (e.g., HIV). Rather than having the raw individual disease records, suppose the data is provided at an aggregate level, such as state-level or county-level summaries. The aggregation level we chose in this article is the U.S. census division, as shown in Figure 6. For each division, the important feature distributions are described in Figure 7. Although the distributions are slightly different across each division, we can observe that they do not reflect the true clusters of the individual-level features. In this section, we focus on diabetes records that are aggregated at the U.S. division come level as the individual-level features and the aggregated diabetes ratio as the aggregate-level feature. This individual-level dataset along with the division-level ag-gregated diabetes records are given as the inputs to the CUDIA model. Although the individual-level features are numeric values, their values are grouped ranging from three to six levels. To prevent the singular variance problem in the EM algorithm, their values are perturbed with a negligible Uniform noise before the learning process. After all the parameters in the CUDIA model are learned, the hidden individual-level dia-betic condition (diabetes or healthy) is imputed based on the underlying distribution. Note that the imputation formula produces a probabilistic estimate of how he/she is likely to have diabetes. Hence the imputation quality can be measured by the receiver operating characteristic (ROC) curve and area under ROC (AUROC) values. Figure 8 shows the ROC curves and the AUROC values from the aggregated diabetes dataset, ranging from K = 3to K = 9. We compare the performance of CUDIA with the base model, which makes everyone in the same division share the corresponding average diabetic rate. We can observe that all CUDIA models outperform the base model in ROC space.

The CUDIA model provides other valuable information about the data, which is the underlying distribution. Table II shows the learned parameters from the model. Noticeably, Cluster 7 exhibits a high risk for diabetes. Their profiles can be described as  X  X igher age X ,  X  X bese X , and  X  X iddle-class X , where this relationship between obesity and diabetes coincides with the medical research literature [Steppan et al. 2011]. On the other hand, Cluster 3 shows a lower risk, and their profiles can be summarized as  X  X lim X ,  X  X igh education X , and  X  X igh level income X . Note that these cluster parameters are learned without accessing the individual diabetes information. In this section, we consider a different setting based on the same dataset, in which the target variable is available at the individual level but other important features are masked due to privacy or legal issues. In this case, we can impute the masked features using the CUDIA model, then propagate its results to various predictive modeling algorithms. Figure 9 describes the main idea of this approach.

In this setting, Age and BMI are the individual features, Hypertension and High-cholesterol are the masked features, and Diabetic condition is the target. The masked features are aggregated using the U.S. census division mapping, and the target is only used in the predictive modeling, that is, the target is not used before the predictive modeling. As the formulated problem is a binary prediction problem, we can use any binary classifier, such as SVM, logistic regression, decision tree, na  X   X veBayes,etc.Ifa regression problem is formulated for this setting, one can use other regression tech-niques, such as Lasso and Ridge regression [Park and Ghosh 2011, 2012]. In this article, we demonstrate this predictive modeling framework using a logistic regression family, decision trees, random forests, and SVM.

Table III shows the estimated parameters when K = 9 from the dataset. The people belonging to Cluster 8 have higher hypertension risk as well as high-cholesterol risk. Their observed individual features are centered at the  X  X igher age X  and  X  X bese X  cen-troid [Carmelli et al. 1994]. On the other hand, the people from Cluster 3 have lower hypertension risk while their ages are comparably high. But interestingly, their BMI X  X  are very low, and this supports the result. For the rest of the predictive modeling tasks, we used K = 9and  X  = 0 . 1, where  X  is in Equation (14).

As in Section 6.3, the aggregated variables are estimated at the individual level using the CUDIA imputation framework. These imputed variables now form individual-level predictors, together with the two individual-level variables, Age and BMI. This newly created dataset, namely the CUDIA dataset, can be plugged into various predictive models for diabetes. On the other hand, without the CUDIA imputation, the best way to utilize the aggregated variables is to view them as the individual-level representatives, and we name this dataset as the baseline dataset. These two datasets along with the complete dataset (the ground truth individual-level variables) will be compared in various predictive models. We expect the CUDIA dataset would result in better prediction than the baseline dataset, as the CUDIA imputed variables will be closer to the true individual-level values than the coarse baseline variables. Figure 10 shows the imputation quality of the CUDIA imputed features as well as the baseline variables. Note that the original hypertension and high-cholesterol variables are binary variables at the individual level, while the CUDIA imputation results in numeric estimates, which are basically weighted averages of cluster centroids in Table III. These numeric estimates are better aligned with the underlying true individual-level values, and we measured AUROC X  X  for different values of K  X  X  as well as the baseline. As can be seen, the aggregated variables (the baseline) across nine census divisions show almost no predictive power on their original individual-level values, resulting in nearly 0.5 AUROC values. On the other hand, the CUDIA imputed variables tend to follow the original individual-level values, even with very small K  X  X . 6.4.1. Logistic Regression with Aggregated Features. In some cases, the relationship be-tween the aggregated features and the target might be of primary research interest. If we have available individual side information (in this case, age and BMI) along with the aggregated features, we can use either the CUDIA imputed values or the aggregate values (baseline approach). The logistic regression equation is given as
Figure 11 shows the logistic regression results from three different kinds of datasets: (i) baseline dataset (direct aggregate variable imputation), (ii) complete dataset (full individual observation), and (iii) CUDIA dataset (CUDIA imputation). In Figure 11(a), we can observe that the coefficients from the CUDIA dataset mimic the coefficients of the complete dataset quite well. Five-fold cross validation is performed, and the average log-likelihood values of the hold-out samples are recorded. Figure 11(b) shows that the CUDIA dataset outperforms the baseline dataset, while it performs slightly worse than the complete dataset. 6.4.2. Logistic Regression with L1 Constraints. The rest of the experiments use a combina-tion of the individual-and the aggregate-level features. The dependent variables are two individual variables (Age and BMI) and two aggregate variables (Hypertension and High-cholesterol). Unfortunately, many features in the BRFSS dataset are interdependent, such as Age and Income level, BMI and Hypertension, etc. This prop-erty becomes even worse when the interdependent numeric values are grouped into a few number of bins, as in the BRFSS dataset. This type of problems can be alleviated if we adopt shrinkage methods, also known as regularizers, such as L 1or L 2 [Hastie et al. 2009]. In this article, we demonstrate two widely used regularizers, L 1and L 2.
The L 1 regularizer is known to generate a sparser solution compared to a normal regression [Cawley et al. 2006], which can be regarded as an automatic feature se-lection technique. Figure 12 shows the results from the L 1 logistic regression. From Figure 12(a), we can observe that the hypertension affects the most in both the com-plete and the CUDIA datasets, but not in the baseline dataset. The coefficients for the aggregate variables from the baseline dataset are actually zeroed out due to the effect of the L 1 regularizer. Furthermore, the average log-likelihood values from five-cv show that the CUDIA imputation is more effective in this predictive task than in the baseline imputation. 6.4.3. Logistic Regression with L2 Constraints. The results obtained on applying an L 2 constraints (ridge regression) are shown in Figure 13. Unlike the L 1 case, all the coefficients have nonzero values in Figure 13(a). Note that the coefficients from the complete and the CUDIA datasets have very similar weights. Again, from Figure 13(b), we observe that the CUDIA imputation is more effective than the baseline dataset. 6.4.4. Decision Tree. Decision trees are recursive rule based classifiers. We demonstrate the impact of CUDIA using two kinds of decision trees based respectively on (i) Gini criterion [Breiman 1984] and (ii) entropy criterion [Quinlan 1993]. We used the decision tree package from KNIME, 6 and the Minimum Description Length (MDL) principle is used for pruning.

Figure 14(a) shows the results from the decision trees. The performance is measured using the area under receiver operating characteristic (AUROC) curve in both cases. Surprisingly, the CUDIA imputation recorded almost the same performance as the complete dataset. Originally, the CUDIA model is designed to model the underlying distribution, then the individual values are imputed utilizing the learned conditional distributions. As the recursive decision tree algorithms focus more on the condi-tional distributions between the target and the features than the individual values themselves, the CUDIA model shows its strength especially in decision tree algorithms. 6.4.5. Support Vector Machine and Random Forests. We provide two more demonstrative examples of the CUDIA imputation framework using support vector machine (SVM) and random forests. 7 Figure 15 shows the results from both SVM and random forests. We used linear kernel for the SVM classifier, and the default setting for the rest of the parameters. As can be seen, the CUDIA imputed dataset provides better classification results than the baseline datasets in both experiments, and it performs even better than the complete dataset in the random forests example. 6.4.6. Performance Analysis. Figure 16 shows the performance comparison between the baseline dataset and the CUDIA imputed dataset across various classifiers. The exper-imental results from SVM, decision trees, and random forests show significant perfor-mance improvement using the CUDIA imputed dataset, contrary to the slight improve-ment in the logistic regression experiments. We contemplate two possible explanations for this kind of result. First, the two CUIDA-imputed features, Hypertension and High cholesterol, turn out to be rather strongly correlated with the individual-level BMI feature, violating the feature independence assumption of linear models. Although the imputed features are correlated with the original features, these imputed features are based on underlying clusters, providing richer information about the dataset. There-fore, classifiers resistant to feature dependencies, such as SVM and decision trees, may be able to produce better predictive performance using this additional information. Second, the Hypertension and High-cholesterol features are originally binary features, but the CUDIA-imputation results in real-valued estimates at individual-level, which can be interpreted as probabilistic estimates of having hypertension and high choles-terol. In decision trees, these numeric features provide larger freedom of splitting rules differing a cutting threshold, while binary features can result in only two-way splits. In a case of datasets with aggregated binary features, the CUDIA imputation may provide a higher degree of freedom in decision rules, as long as the numeric estimates are close to the features. Thus, the CUDIA imputation not only helps to reconstruct the individual values of the aggregated features but also supports the predictive modeling using the imputed features. In this section, we demonstrate a multisource integration example using CUDIA. We use the BRFSS dataset as the individual-level information and the Kaiser Family Foundation dataset as the aggregate-level information. The Kaiser Family Foundation (KFF) is a nonprofit, private foundation, which focuses on the major healthcare issues facing the nation. Statehelthfacts.org is a project of KFF, which provides various health-related statistics for all 50 states in the U.S. From the dataset, we selected two state-level summaries: (i) Average Fruit/Vegetable Consumption and (ii) Average Heart Disease Rate. The state-level summaries are weighted by the BRFSS sample selection bias, then averaged to make the U.S. divisional statistics. Thus, we have Age and BMI as the individual-level data from the BRFSS dataset, and the adjusted Fruit/Vegetable Consumption and Heart Disease Rate as the aggregate-level data from the KFF dataset.

Figure 14(b) shows the results from the decision trees when the individual diabetic condition is set as the target. We compare the performance of the CUDIA imputation with two other datasets: (i) without using the aggregate information, that is, only the BRFSS dataset and (ii) with division-level direct imputation (base model). We used the same decision trees as in the previous experiment. The CUDIA dataset exhibits the best performance among the approaches considered.

The distributions of the imputed variables are shown in Figure 17. Note that we do not have the ground truth individual information for the KFF dataset. Although we cannot measure the imputation accuracy, we can check the quality of the results through the distributions. From Figure 17(a), we can observe that higher risk groups for heart disease contain more people with higher age and higher BMI. Moreover, fruits and vegetables are consumed more by people with lower BMI. In this article, aggregated statistics over certain partitions are utilized to identify clus-ters and impute features that are observed only as aggregated values. The imputed features are further used in predictive modeling, leading to improved performance. The experiments provided in this article are illustrative of the generality of the pro-posed framework and its applicability to several healthcare-related datasets in which individual records are often not available, and different information sources reflect different types and levels of aggregation.

In the CUDIA framework, the aggregate data do not need to be the actual average of the underlying individual-level data. The CLT approximation in CUDIA provides flexibility in this and many other practical settings. For example, in the U.K. census, some aggregate data are calculated using a 10% sample to maintain confidentiality. The observed statistics are not the same as the true sample average, thus the direct application of the model of Figure 2(b) is no longer valid. However, the difference between the subsampled average and the true sample average can be modeled using a normal distribution, which fits the key assumption of the CUDIA approximation. As another example, to maintain confidentiality or privacy, a popular technique is to add noise to the true values. Additive Laplace or Gaussian noise are known to guarantee ( ,  X  )-differential privacy [Dwork 2006] under certain assumptions [Dwork et al. 2006a, 2006b]. Adding a Gaussian noise exactly fits the assumption in the CUDIA model so that the CUDIA model becomes no longer an approximation in this case.

CUDIA is quite scalable, and in particular, the deterministic hard clustering version can be readily applied to massive datasets. Furthermore, the square loss function on x can be generalized to all Bregman divergences, or equivalently, one can cater to any noise function from the exponential family of probability distributions [Banerjee et al. 2005]. One restriction of the current model is that the number of clusters ( K ) cannot be more than the number of partitions ( P ) specified by the data provider. This is why we had to stop at K = 9 for several of the results even though the performances were improving with increasing K . For the aggregate variables from different partitions, the CUDIA framework can be applied in either parallel or sequential way by dividing the problem with having one aggregate variable per each, and we leave the implementation and evaluation of these approaches as our future work.

