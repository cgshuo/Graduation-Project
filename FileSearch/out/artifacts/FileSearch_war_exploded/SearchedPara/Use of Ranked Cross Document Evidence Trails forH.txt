 This paper focuses on detecting how concepts are linked across multiple text documents by generating an evidence trail explaining the connection. A traditional search involv-ing, for example, two or more person names will attempt to find documents mentioning both these individuals. This re-search focuses on a different interpretation of such a query: what is the best evidence trail across documents that ex-plains a connection between these individuals? For exam-ple, all may be good golfers. A generalization of this task involves query terms representing general concepts (e.g. in-dictment, foreign policy). Previous approaches to this prob-lem have focused on graph mining involving hyperlinked documents, and link analysis exploiting named entities. A new robust framework is presented, based on (i) generating concept chain graphs, a hybrid content representation, (ii) performing graph matching to select candidate subgraphs, and (iii) subsequently using graphical models to validate hy-potheses using ranked evidence trails. We adapt the DUC data set for cross-document summarization to evaluate evi-dence trails generated by this approach.
 H.3.1 [ Information storage and retrieval ]: Content anal-ysis and indexing X  Linguistic Processing, Indexing methods Algorithms,Experimentation,Security text mining, cross document summarization, graph mining
Open source document collections reflect diverse sources and authors; they often reveal interesting information other Copyright 2007 ACM 978-1-59593-609-7/07/0008 ... $ 5.00. than what is explicitly stated. The goal of information ana-lysts is to sift through these extensive document collections and find interesting links that connect facts, assertions or hypotheses that may be otherwise missed. What is required is a set of automated tools that will expose such links, or at least generate plausible patterns. We refer to this special case of text mining as unapparent information revelation (UIR).

Currently, analysts perform this task with assistance from social network analysis (SNA) and graph or pattern match-ing tools. Such techniques while powerful are fragile: (i) they rely on an information extraction system to accurately tag key entities and relationships, (ii) they do not take into account more general concepts such as trucking industry, but are limited to named entities, and (iii) they require users to anticipate and predefine specific scenarios of interest; this in turns involves building complex domain models which is a chokepoint. In order for domain models to be effectively used in pattern matching, they should reflect the data model which is derived from processing a corpus. Analysts cannot anticipate all types of event patterns leading to a specific scenario: the system should discover these patterns! On the other hand, analysts are able to specify the set of concepts typically involved in the pattern being sought. The goal of this research is to take as input such broad queries, referred to as concept graph queries , and generate the corpus-specific hypothesis (pattern) that corresponds to it. Each hypoth-esis will typically involve more concepts than the original query, and will be backed up by a textual evidence trail that shows the logical connection between the original input con-cepts. Multiple hypotheses, along with multiple evidence trails may be generated. Such a technique is designed to reduce the burden on analysts to do cumbersome modeling.
Figure 1 illustrates a typical concept graph query. The analyst is looking for patterns involving a religious leader participating in some activity involving both a US and for-eign city. The query is simply a graph representing these three key concepts. Part (ii) of the figure shows the spe-cific matching subgraph in the corpus based on the evidence trail shown in part (iii). The sentences are prefixed by the documents from which they are extracted. In this case, the subgraph in part (ii) is the corpus-specific hypothesis that is generated. An analyst could then modify the subgraph to generalize the pattern, or make it more specific. This could then be matched against the graph representing the data model. The arrows in the latter graph are only used to highlight the connections in the resulting evidence trail; the graph itself is undirected.

Previous work in UIR [14] focused on concept chain queries, a special case of concept graph queries involving only two concepts. Various models were used to derive the best con-cept chains. The goal was to generate the best cross-document chain connecting two concepts with document sets (rather than sentences) as evidence. The work described here ex-tends the previous work by focusing on (i) evidence trail gen-eration, and (ii) concept graph queries. The UIR solution framework consists of several steps ranging from information extraction to graph mining to graphical models for evidence trail generation. Experimentation on the Document Under-standing Conference (DUC) data set validates the approach through quantitative evaluation. The DUC data set is de-signed to evaluate cross-document summarization; evidence trails are a special case of this. The next section describes some differences between the two tasks.

The remainder of this paper is organized as follows: Sec-tion 2 discusses criteria for rank ing evidence trails, Section 3 discusses related work, Section 4 presents an overview of the UIR framework and solution, Section 5 discusses the gener-ation of concept chain graphs, Section 6 discusses the graph mining and hypothesis candidate generation, Section 7 dis-cusses the generation and ranking of evidence trails. Finally Sections 8 and 9 present the experiments conducted on the DUC data set and results.
There are various criteria for ranking concept chains, in-cluding: (i)recency, (ii) most interesting, (iii) most plausible, and (iv) going through certain specified concepts. In this pa-per, we are focused on finding chains that are coherent and informative .

Coherence is the main criteria that needs to be satisfied when generating chains. It is not sufficient for individual links between concepts to make sense: the chain as a whole must make sense also. Chains of length 5 or greater often have problems since at some point, the thread gets lost. In formulating ranking algorithms for concept chains, coher-ence is the primary factor to be considered. Later sections describe how techniques used to evaluate student essays for coherence are adapted to rank evidence trails. In some cases there is not a single thread connecting all concepts; the re-sulting evidence trails reflect this and represents disjoint sets of connections spanning all concepts.

We also require evidence trails to be informative. For ex-ample, if person A eats breakfast and person B also eats breakfast, then although eating breakfast is a valid connec-tion, it is not of interest. On the other hand, if both of them have a liking for exotic spicy food from Southeast Asia, the connection starts to get more interesting. Developing quan-titative measures for judging the informativeness of a chain is part of this effort. Techniques such as distributional sim-ilarity [9] are useful in at least determining the concepts which are important to the domain based on the difference between general and corpus-specific usage.

Finally, it should be noted that while the evidence trails serve as summaries of how concepts are connected across documents, this is not the same as the task of cross docu-ment summarization [12]. In later sections we use data sets and queries designed for evaluating cross document summa-rization; however there is a difference in the intended use of evidence trails. They are intended to weave a thread be-tween query concepts and place heavy emphasis on the or-der in which the concepts appear in the evidence trail. The resulting explanation may focus on less important themes and topics than what a traditional summary would aspire to cover. Nevertheless there is enough overlap between the goals of the two tasks to motivate us to use the same data sets.
There are several research e fforts that are related to the work on concept chain queries described here. The DARPA EELD program has resulted in text mining efforts that use more sophisticated information extraction (IE) output such as named entities, relationships and events [18]. Such sys-tems typically use IE tools to extract salient entities and relationships; these are then input to either visualization or link analysis tools. Since IE systems have yet to achieve a level of recall for relationship and event detection that is sufficient for this type of analysis, such techniques may fail to capture significant links/paths. Wang et al [17] describes text mining on a corpus of govt. documents with the goal of discovering interesting patterns involving groups of enti-ties and topics. While not focused on specific concept chain queries, it does go beyond named entities and establishes correlations across documents between entities and general concepts.

There has been work on discovering connections between concepts across documents using social network graphs, where nodes represent documents, and links represent connections (typically URL links) between documents. However much of the work on social network analysis has focused on dif-ferent types of problems, such as detecting communities [5]. Faloutsos et al [4] is the work which is closest to the research presented here, at least in its goals. The authors model the problem of detecting associations between people as find-ing a connection subgraph and present a solution based on electricity analogues. The most notable difference is their reliance on URL links to establish connections between doc-uments; our approach extracts associations based on con-tent (textual) analysis. Second, the connection subgraph approach presents all paths together, while our approach presents the paths individually. This allows greater user in-put in determining the best paths, including recency, novelty, semantic coherence, etc. Third, the approach presented here attempts to generate an explanation of the chains, whereas the connection subgraph approach does not. Finally, the connection subgraph solution only addresses named entities whereas this approach extends to general concepts.
IR-based approaches have also been employed to solve similar problems. Srinivasan [15] discusses a technique based on constructing semantic profiles of concepts to derive con-nections between concepts in biomedical documents. This reflects an attempt to replicate the results of Swanson X  X  [16] pioneering efforts in text mining through purely automated techniques; the results are impressive and hence this is used as a baseline model for our own experiments described later.
This section describes the UIR framework and an overview of the solution to the hypothesis generation problem. A key part of the solution is the representation framework. What is required is something that supports traditional IR models (such as the vector space model), graph mining and proba-bilistic graphical models. We have formulated a representa-tion referred to as concept chain graphs (CCG).

Formally a CCG is a hypergraph G ( E, V )with E edges and V nodes representing a set of documents D with the following properties:
Nodes can have attributes such as the weight of a con-cept; edge weights indicate the strength of an association. The weighted graph representation enables traditional graph mining algorithms to be applied to the CCG. The CCG can also be viewed as a specialized index consisting of four lay-ers: 1. Document Layer. This layer contains documents and 2. Concepts and Associations Layer. Consists of concepts 3. Instances Layer. Tracks instances of concepts and as-4. Index Terms Layer. Consists of index terms and hits.
Figure 2 illustrates a schematic representation of a small portion of the CCG that has been constructed from the 9/11 document collection. The figure illustrates the explicit rep-resentation of connections between concepts; the concept bin ladin in document D121 is linked to concepts occurring in document D236. Edges labeled Ax represent associations between concepts. The number of hits for a concept in a document is also recorded. A typical forward index view can be obtained by traversing concepts associated with a document. A reverse index view is obtained by examining nodes and links emanating from a given concept (and what documents they are connected to). Ontological links are also illustrated; white house is a type of organization .
The solution to the hypothesis generation problem can then be described as the following steps: CCG construction: Process the corpus through an infor-Graph Matching: Using the query concepts, generate hy-Evidence Trail Generation: Generate evidence trails cor-
There is an observation that should be made with respect to the above process. We have adopted a strategy whereby graph matching techniques are first used to generate can-didate hypothesis matches; the candidates are then ranked based on the quality of the evidence trails that can be as-sociate with them. Ideally, the hypothesis generation pro-cess should be driven by the evidence trail generation phase, not the other way around. It is possible to formulate a hi-erarchical graphical model solution to this problem based on selecting the best sequence of sentences from the corpus that connect all the concepts; this in turn would generate the best matching hypothesis (subgraph). Since the initial focus is on evidence trail generation, we have chosen an ap-proach where that subtask can be independently evaluated and refined.
The process of generating the CCG includes the following steps: (i) domain customization, (ii) concept extraction and filtering, (iii) mapping concepts to target ontology, and (iv) construction of the UIR index. Domain customization con-sists of (i) developing a suitable ontology, and (ii) tailoring the information extraction engine to the domain in question. This includes customization of the named entity tagger. The Semantex 1 engine [13] was used for this effort. It permits customization of the lexicons, as well as defining new entity types which were useful for this effort. We adapted Teknowl-edge X  X  Terrorism Ontology 2 but modified it to suit our task. It contains 21 top-level concepts, a total of 180 concepts, and extends to a maximum depth of 5 although the typical depth is 3. Other top-level ontology nodes can be seen in Table 1 which is discussed further in the next section. In addition
Courtesy of Janya Inc. http://ontology.teknowledge.com to the top-level classes shown, it also includes the standard named entity categories, person, organization, location etc. to handle instances of those types. To facilitate mapping of concepts into the ontology, each node in the ontology was associated with WordNet synsets.
Concept extraction involves running an information ex-traction engine, Semantex on the corpus. Semantex tags named entities, common relationships associated with per-son and organization, as well as providing subject-verb-object (SVO) relationships. We extract as concepts all named en-tities, as well as any noun phrases participating in SVO re-lationships. All named entity instances are retained as in-stances of their respective named entity concept category. Concepts that are not named entities undergo filtering and mapping phases. The extracted set of concepts are filtered on the basis of their global significance in the document cor-pus. Singleton concepts are removed. Some concept merg-ing is performed in an attempt to consolidate aliases across documents, e.g., spelling variations for names. Table 1 illus-trates some of the concepts that are extracted by the sys-tem; all these have survived the filtering processing and have automatically been mapped into the ontology; mapping is discussed in the next section. Concepts in the table such as fbi report, american soil indicate the data-driven manner in which these concepts are extracted.
Semantex generates a modest number of semantic rela-tionships involving person, organization and location enti-ties in particular such as employed by, whence etc. All these are used in our content representation. However, we recog-nize that this does not even begin to capture all the various relationships that are important. To increase the coverage of significant associations, we also exploit key syntactic re-lationships that Semantex generates. In particular, if two concepts are linked through a set of subject-verb and verb-object links, we generate an (unlabeled) association between them. We consider this to be an improvement over simply using sentence-level co-occurence to generate associations.
WordNet is used to map concepts to ontology nodes using a hierarchical traversal technique; details of this are beyond the scope of this paper. As Table 1 illustrates, the technique used above sometimes produces erroneous results. For ex-ample, patriot act is mapped into the ontology node human action rather that into document which includes acts of leg-islation. Concepts that cannot be mapped into categories are placed in a miscellaneous category and can still partic-ipate in concept chains. Finally, the CCG is constructed which also records weights on associations, number of hits, etc. The E4Graph package is employed which enables us to persist the graph.
This section discusses the graph matching techniques used to generate candidates for the hypothesis subgraph. The ob-jective is to take as input a set of query concepts, and find the best matching subgraph that connects all of these. In cases where no direct edges are present between certain con-cepts, the system attempts to find the best chain of concepts connecting them. We first describe this simpler case involv-ing only two query concepts: we refer to this as a concept chain. This is followed by a generalization to concept graph queries involving three or more concepts.
Generating the best concept chain for a given concept chain query is performed using a Markov chain model. A concept can make a transition to another concept through one of the  X  X ecognized relations X . If a concept X is related to another concept Y which has a similar context as that of X , then such a relation can be coherent and meaningful. Each link can be seen as some drift away from the original concept. Keeping this in mind we calculate the transition probabilities of the concepts based on their contexts.
A concept canbeformedbyoneormoreterms. These terms define a term vector for that concept in the n dimen-sional Vector Space Model [1]. A context vector of a concept is given by the union of term vectors of the concept and the term vectors of its related concepts; these related concepts are extracted from a domain-specific ontology. For example, concepts related to concept  X  X md X  would include  X  X hemi-cal weapon, biological weapon, nuclear weapon X , etc.
In the Context Model, for any concept C i and C j ,the transition probability is given as
P ( C i ,C j )=
Where sim context vectors of concept C i and concept C j .Itisimpor-tant to note that even though the similarity measures are symmetric in that sim ( C i , C j )= sim ( C j , C i ), the transition probabilities are not symmetric i.e. P ( C i ,C j ) = P ( C This asymmetricity arises from the fact that each concept has a different neighborhood. It is in a way interesting to have the forward probabilities differ from the backward probabilities in that it gives a possibility to get a different best Markov Sequence from C i to C j than from C j to C i
As an extension to the Concept Chain model described previously, we pursue techniques for generating a graph of concepts rather than a linear chain in response to the user query. The primary motivation behind this approach was to improve upon the dimensionality of both the input and out-put of the hypothesis candidate generation; i.e. whereas the concept chain model was restricted to two query concepts as an input and a linear path as an output, the concept graph model can accept any number of inputs and produces a connected graph of concept nodes and associations as an output. This new approach also better models a real-world scenario where user queries would rarely consist of just two query concepts.
 Unlike the Markov Chain Model used for the Concept Chain generation model, we approached this problem as a pure graph matching problem, using the entire CCG con-cept neighborhood as the parent graph and the query con-cepts as key vertices within the graph. Although superfi-cially this problem resembles the Minimum Spanning Tree (MST) problem, closer analysis reveals that it is in fact more similar to the Steiner Tree problem.

Formally, given a weighted graph G(V,E,w) and a vertices subset S ; we call a Steiner Tree an acyclic sub graph of G that connects all vertices in S . A Minimum Steiner Tree is a minimum edge-weight instance of a Steiner Tree for a given graph. The vertex subset S are also called Steiner Points of the sub-graph.

We use Mehlhorn X  X  Algorithm [11] to build the graph: 1. Compute the entire distance network N d between all 2. Build a Minimum Spanning Tree T d in N d [Kruskals 3. Transform T d into a subnetwork N of T d by replacing 4. Build a Minimum Spanning Tree T for the subnetwork 5. Transform T into a Steiner Tree S k by successively
The bottleneck in this algorithm is the first step of com-puting the entire distance network, especially in the case of larger graphs with larger queries. This is done by calling Dijkstras Algorithm once for every terminal. However, once the complete distance network is known, the magnitude of the problem is significantly reduced.

Two small modifications were made to this algorithm to account for the sparseness of the data set we used in our experiments: (i) While computing the distance network, to enable the algorithm to favor smaller length chains ,weintro-duced a dampening factor of 0.2 for every edge encountered along the path. This ensured that shorter chains were fa-vored over longer ones, but at the same time a long chain with a sufficiently high score would still be chosen over weaker short chains. (ii) We put a cap on the maximum depth of the shortest path algorithm to search for nodes at a maximum distance of 5 away from the source node.
Additionally, we experimented with improving candidate subgraph generation by introducing the notion of concept importance to the algorithm. The motivation for using con-cept importance includes (i) eliminating (or reducing) sub-graphs containing several trivial concepts such as  X  X art X , and (ii) the desire to dampen the effect of sink nodes which ultimately led to a poor summary being generated. Sink nodes (e.g.  X  X SA X ) are characterized by a high number of associations, many of which do not carry much semantic importance. Faloutsos et al [4] refer to this as the pizza delivery man phenomenon: since he delivers pizza to many people in an area, there is a trivial connection between many people through him. As an initial step, we adopted purely graph-based techniques to assign the concept weights; fu-ture plans call for measuring the distributional similarity of a concept in a domain specific language model as compared to a generic language model.
 We used three graph-based metrics used traditionally in Social Network Analysis (SNA) to represent the relative im-portance of concepts; these measures are normalized based on the number of nodes in the graph: 1. Degree Centrality : Measures the number of nodes that 2. Betweenness Centrality : Measures the number of geodesic 3. Closeness Centrality : Measures the total graph-theoretic
We also made a distinction between concepts that were named entity (NE) references (e.g. people, places and or-ganization names) and generic concepts (e.g.  X  X ower X ). In-tuitively, NEs add more information to a summary than generic concepts or events. However generic concepts some-times provide the only link between two other concepts and hence cannot be excluded. Thus NE concept nodes were weighted higher than non-NE nodes.

The following observations were made based on the ex-periments we ran: (i) In general, the size of the graph and the length of each individual chain seems to have increased. (ii) The higher ranked graphs tend to have more Named Entities in them as expected. (iii) The various candidate graphs for a single query seem to have a lot more overlap in them than with unweighted concept nodes.
 Figure 3(a) shows a small subset of the entire CCG graph; Figure 3(b) shows a sample Concept Graph for query con-cepts atta , ksm and hazmi .
This section describes a Hidden Markov model to gen-erate the best evidence trail given a set of concept chains. We suppose all the documents within the corpus belong to one topic. We follow Harabagiu [7] to represent a topic as a structure of themes. A theme is defined as a clus-ter of sentences which convey the same semantic informa-tion. Themes represent events or facts that are repeated throughout the document collection. A content model [3] is built to capture the relationships among these themes. Evidence trails are generated by Viterbi decoding on the content model. Desirable evidence trails need to be both informative and coherent. Informativeness is incorporated in concept chains induced from the Concept Chain Graph. Coherence is obtained by the drift of topic themes captured in the Content Model.
 Figure 3: Sample Concept Graph produced from CCG
A Content model is essentially a Hidden Markov Model in which states correspond to themes and state transitions in-dicate the probabilities changing from one theme to another by examining the theme ordering in the topic. It is built on the assumption that all texts describing a given topic are generated by a single content model. We adapt the Content Model described above to our problem.

Given a set of sentence clusters c 1 ,c 2 , ...c m ,weconstruct a Content Model whose states s 1 ,s 2 , ...s m corresponds to these clusters. Here onwards, we do not differentiate be-tween cluster c i and state s i . Each state is associated with a state-specific language model. For state s i , a state-specific bigram language model is built as: where f c i ( x ) is the frequency with which word sequence x occurs in cluster c i ; V is the vocabulary;  X  1 is a smoothing parameter.

The sentence emission probabilities are determined by the state-specific language model and association overlap. P ( x | c i )=  X  1  X  P lm i ( x )+(1  X   X  1 )  X  where count ( a k ,c i ) is the number of times that association a k is observed in cluster c i ; P lm i ( x ) is the probability of sentence x being generated by the language model associated with cluster c i .

The transition probability measures the likelihood of state s preceding state s j , which is captured by considering sen-tence ordering in the original documents, and the overlap of concepts between them. The transition probability is com-puted as follows: P ( c j | c i )=  X  2  X  D ( c i ,c j )+  X  2 where m is the number of states; D ( c i ,c j )isthenumber of documents in which a sentence from c i immediately pre-cedes a sentence from c j ; count ( cpt k ,c i )isthenumberof times that association cpt k is observed in cluster c i ;  X  smoothing parameter.

The sentence clusters are initialized by complete-link anal-ysis. We follow Barzilay et al [3] who use an EM-like Viterbi re-estimation procedure to build the model: re-cluster sen-tences by placing them in the state which is most likely to have generated it after the Viterbi decoding. The new clus-ters are then used as input to estimate HMM parameters. Repeat this cluster/estimation procedure until the clusters stabilize.
The generation of evidence trails is accomplished by (1) traversing through the hypothesis graph returned in the pre-vious section to enumerate all possilbe chains between the input concepts, (2) treating these chains as input to the con-tent model to decode sequences of themes, (3) organizing sentences from decoded themes to form evidence trails.
This process takes as input a concept chain which can be also viewed as a sequence of associations. Observe that here the emission of the content model changes from sentences into associations: the content model needs to be modified ac-cordingly. The state transition probabilities stay the same. The emission probability is now defined as: where count ( a j ,c i )isthenumberoftimesthatassociation a j is oberserved in cluster c i . This emission probability is not smoothed because we are not interested in cluster c i it doesn X  X  include the current association a j .

Viterbi decoding of the input chain results in a sequence of sentence clusters which has the highest possibility of gen-erating the observed concept chain. Since one association can appear in several sentences within a cluster, there could be several candidate sentences for each association in the chain. Since each cluster is a topic theme conveying similar semantic meaning, it does not impact the informativeness of the evidence trail if different sentences are chosen. It is readability that is of our concern now, which is taken care of by the ranking metric later. So we treat all sentences which contain the corresponding association in the decoded sentence cluster as candidates, and generate all possible ev-idence trails out of them. The evidence trails for concept chains enumerated from one hypothesis graph are combined together with duplicate associations and their corresponding sentences removed to form the final result.
Evidence trails are ranked based on two main criteria, co-herence and informativeness. A key consideration is the lack of a gold standard with which to compare the evidence trail: a standalone technique for ranking is required. There have 1 Argentine British relations post Falkland War 2 Amazon Rainforest Problems 3 New Successful Applications of Robot Technology 4 Tourism in Great Britain 5 Saving Tourists and Tourism 6 Welsh devolution and British Parliament been previous efforts to model coherence [2]. For the current research we are motivated by efforts to measure coherence in essays written by grade school students. The Coh-Metrix tool [6] in particular is relevant to the current work. Coh-Metrix is a computational tool that produces indices of the linguistic and discourse representations of a text. It takes into account many other dimensions of good writing such as readability, concreteness, sophistication of writing etc. Since we are not judging the quality of sentences, only the selec-tion and ordering, many of these factors are not relevant to our work. Currently, the evidence trail ranking module uses latent semantic analysis [8] to compute a score. LSA is an algebraic technique for analyzing relationships between a set of documents and the terms they contain resulting in a reduced concept space that best characterizes the set of documents and terms. Sentences are represented as vectors in this space; similarity between sentences can be computed using cosine distance. We use a sentence to sentence com-parison technique whereby a summary of n sentences results in n -1 cosine comparisons between the sentences. The mean of the individual comparisons is used to judge the goodness of an evidence trail. The semantic space is based on college level general English 3 We are in the process of retraining the LSA model on a comprehensive set of DUC documents and using this reduced space matrix for the summary rank-ing system. While this does not capture all the criteria we desire, it is a good starting point. Experiments have shown that permuting the sentences in a manually judged good evidence trail significantly alters the LSA score.
This section describes the experiments that were con-ducted. The Document Understanding Conference (DUC) 2005 data set was used. The data set is organized into a set of topics where each topic has a set of 25-50 documents rele-vant to it. The task: given a user profile, a DUC topic, and a cluster of documents relevant to the DUC topic, create from the documents a brief, well-organized, fluent summary which answers the need for information expressed in the topic, at the level of granularity specified in the user profile. The summary should include (in some form or other) all the information in the documents that contributes to meeting the information need. Some generalization may be required to fit everything in. The summary can be no longer than 250 words 4 .

The DUC data set was selected since it is one of the few data sets involving a cross document analysis task. It has
The tools available at http://lsa.colorado.edu/ were used in this module. more information can be found at http://www-nlpir.nist.gov/projects/duc/duc2005/tasks.html been explained earlier that the evidence trail, though a form of a cross-document summary is not designed to maximize the criteria being evaluated by the DUC task. However it comes tantalizingly close, and hence we use this data set for formal, quantitative evaluation. Table 2 illustrates the six queries that were chosen for this experiment; only the titles, not the full narrative description of each query is shown. These queries were selected since they call for question-answering techniques, and thus lend themselves well to con-cept graph queries where some chain of thought is required.
The following steps are used to process each query.
The highest ranking evidence trails produced by the sys-tem is evaluated against human generated summaries pro-vided by DUC. Several such summaries are provided; we use the average of these scores. A s corer, which calculates a dis-tance metric based on a sliding window of n -grams is used to judge the system produced summary against the gold stan-dard. The lower the score, the more similar to the desired summary. http://www.cs.ualberta.ca/ lindek/demos.htm
To facilitate quantitative evaluation of our technique, we devised a naiive baseline algorithm to produce a cross docu-ment summary. The algorithm works in two distinct steps: (i) Using a greedy search technique, we determine a max-imally weighted set of associations such that every query term is connected by at least one association. If no direct association exists between any two query concepts, we try to find the best chain of maximum length 2 between the two; if this does not exist, we ignore the concept pair for our summary. (ii) For the best set of associations extracted in the previous step, we select the 1st sentence in which that association occurs in and add it to the summary (after eliminating duplicate sentences). This is a simple technique for generating a cross-document summary based on sentence selection.

Table 3 illustrates a complete example, including the orig-inal query and the top-ranked evidence trail (using query expansion) from the algorithm described above. This was judged the best of 5 hypothesis graph candidates. The three concept chains (enumerated from the hypothesis graph can-didate) are also shown. Each sentence is prefixed with the document number that it emanated from: 6 unique docu-ments are used to generate this trail. It is also interesting to see the quality of evidence trails produced by the different methods on this query. The table shows the output of the Baseline algorithm on this query as well as one of the Ref-erence (gold standard) summaries. The baseline summary contains additional sentences at the end that were omitted for space reasons. Although both summaries cover similar topics, the one based on the Hypothesis Generation and Ev-idence Trail ranking algorithm appears to be more succinct and meanders less. For example, in the baseline summary, sentence D26 is out of place. Finally, both system-generated evidence trails seem to be covering the same themes that are mentioned in the human generated summary. This includes mention of diplomatic visits, the oil and gas industry, ten-sions related to fishing, as well as the specific mention of President Carlos Menem. As expected, our summaries are richer in specifics, such as people and organization names since the sentences are selected from the corpus.
Table 4 illustrates the results obtained for six of the DUC queries. The score is based on the n -gram distance metric discussed in the previous section. Four systems are com-pared: the baseline system with and without query expan-sion, and the Hypothesis Generation and Evidence Trail (HypoGen) solution described above, also with and without query expansion. The score reflects the mean distance be-tween the system-generated summary and each of the human generated summaries. Since the value of concept weighting in hypothesis graph generation is still inconclusive, it was not used in generating these results.

In all cases, the HypoGen technique scored better than the baseline. Furthermore, HypoGen with query expansion con-sistently produced better scores. While the results between systems are comparable in most queries, in a few cases, such as queries 3, 4, and 6, the baseline with query expansion fared much worse. Query expansion often introduces noise and a simple technique such as the baseline can easily be led astray. It is interesting that the HypoGen technique with Narrative HypothGen Reference Table 4: Evaluation Results Based on N -gram Dis-tance Metric Base-Q 0.0483 0.0825 0.2525 0.2557 0.0845 0.2702 HypG 0.0397 0.0535 0.0517 0.0639 0.0590 0.0439
HypG-Q 0.0392 0.0427 0.0453 0.0535 0.0413 0.0419 query expansion still performed the best in this query, and was not adversely affected by query expansion.

It should be noted that the evidence trail generation tech-nique was designed for a slightly different purpose, namely finding the best conne ctions between concepts. If the goal were to produce a better cross-document summary, a post-processing module could be developed which would produce higher scoring summaries. Currently, evidence trails are be-ing cut off to keep the length manageable; these could be extended. Temporal issues are not being taken into account; it is possible to order sentences based on this as well as co-herence. High-scoring DUC summaries tend to choose sen-tences from the initial portions of documents; we are not taking this into account. Finally, the queries themselves could be processed in a more intelligent manner, with the goal of better summaries. Nevertheless, the technique used for hypothesis generation has resulted in a viable method for generating query-drive n cross-document summaries.
This paper has presented a new framework for generating corpus-specific hypotheses graphs. This approach has the promise of reducing the effort on the part of analysts in con-structing domain models that can be matched against data collections to look for scenarios of interest. The highlight is the generation of evidence trails, cross-document summaries that explain how the query concepts are connected. Results from experimentation show that this can also be viewed as a general technique for cross-document summarization. Ongo-ing work includes: (i) fine-tuning graph matching algorithms to account for the importance of concepts, (ii) generating ev-idence trails directly from the hypothesis graph candidates, and (iii) improved techniques for ranking evidence trails. This work is sponsored by NSF grant IIS-0325404 and FAA grant 032-G-009. The authors would like to thank Abhishek Singh for his help in evaluation. [1] R. Baeza-Yates and B. Ribeiro-Neto. Modern [2] R. Barzilay and M. Lapata. Modeling local coherence: [3] R. Barzilay and L. Lee. Catching the drift: [4] C. Faloutsos, K. S. McCurley, and A. Tomkins. Fast [5] D. Gibson, J. Kleinberg, and P. Raghavan. Inferring [6] A. C. Graesser, D. S. McNamara, M. M. Louwerse, [7] S. Harabagiu and F. Lacatusu. Topic themes for [8] T. K. Landauer and S. T. Dumais. A solution to [9] L. Lee. Measures of distributional similarity. In [10] D. Lin. Using syntactic dependency as local context to [11] K. Mehlhorn. A faster approximation algorithm for [12] D. Radev, H. Jing, and M. Budzikowska.
 [13] R. K. Srihari, W. Li, T. Cornell, and C. Niu. [14] R. K. Srihari, L. Xu, and A. Bhasin. A text mining [15] P. Srinivasan. Text mining: Generating hypotheses [16] D. R. Swanson. Migraine and magnesium: Eleven [17] X. Wang, N. Mohanty, and A. McCallum. Group and [18] S. Weiss, N. Indurkhya, Zhang, and D. T. Text
