 It has becoming a common phenomenon that when we need to make decisions 2014 ].
 aspects reviews is called Aspect Rating Prediction.
 information they need from the huge amount of review data.
 need to consider the review-level ratings for better aspect-level rating. In this paper, we address the task of Entity Aspect Rating. A novel hierar-ratings at both review-level and entity-level.
 From the review data, we find that (a) reviews on the same entity tend to have similar scores, (b) aspect ratings on the same entity tend to be similar, (c) aspect ratings in one review tend to be similar.
 Based on such observations, in the proposed model, we assume (a) the aspect pling words from the combination of aspect-word distribution and rating-word distribution within the same aspect.
 Experiment results on review data set from Tripadvisor show that our model vidual reviews. The proposed model can be applied to any collection of review data to find latent topical aspects and predict aspect ratings. Aspect review rating has been studied for a long time. Early work Hu and Liu then summarized the opinions in each review. Recent work starts to develop Lerman et al. [ 2009 ]; Zhuang et al. [ 2002 ].
 Topic models, due to their ability to uncover latent aspects from text, have been popular in predicting aspect ratings from review data recently. address the aspect rating problem. A typical one is MaxEnt-LDA Zhao et al. [ 2010 ] which is a maximum entropy hybrid model and is able to discover both aspects and aspect-specific opinion words from reviews simultaneously. Multi-grain LDA (MG-LDA) Titov and McDonald [ 2008a ] is another LDA based model which models review-specific elements and ratable aspects at the same time. Brody and Elhadad (2008) proposed the Local-LDA which analyzes sentences and discovers ratable aspects in review data.
 To predict review rating, supervised LDA (sLDA) model Blei and McAuliffe with latent regression technique for aspect ratings in the reviews. Other work Perceptron Rank (PRank) algorithm to rate aspects for entities and that of Sauper et al. (2010) which integrated the HMM model to improve both multi-aspect rating prediction and aspect-based sentiment summarization. mation, hence the aspect rating sometimes suffer. 3.1 Problem Formulation For a review date set, let N be the number of entities. Each entity comprised of a collections of reviews, E i = { d ij } j = N d sentence is comprised of a collection of words s = { w t } t = n number of words with sentence s . Assume that there are K with an overall rating R i,k and a K-dimensional aspect rating vector R overall rating r all d and a K-dimensional aspect { r dk } of ing R all i and aspect rating R i,k for each entity in test  X  data r d and aspect rating 3.2 Hierarchical Model is not independent from each other. Specifically, for each entity all score is firstly drawn from a Gaussian distribution with mean value deviation  X  2 , as shown in Equ. (1) Then the entity aspect rating R ik and review overall rating drawn from a Gaussian distribution with mean value R all i The aspect rating for review d is drawn from the normal distribution whose the rating for the entity as follows: where  X  is the parameter controls the influence of r all d in our approach.
 We assume that words of one sentence are all from the same topic Gruber et al. behavior is as follows: to generate an opinionated review vocabularies such  X  X ad X ,  X  X errible X ,  X  X irty X  etc. Since r value, we take the rounding value of r d,k , denoted as [ distribution.
 In the text modeling, we assume each aspect A k is characterized by a multi-aspects  X  is drawn from a Dirichlet distribution Dir the joint probability of observed word contents W, latent aspect assignments { z n =1 and aspect proportion where p ( w n | z s , [ r d,z topic z and aspect z s . p ( w n | z s ,r d,z The dependency between review text content and the latent aspect ratings is bridged by the aspect assignment z . We try to learn the values of parameters {  X ,  X ,  X  2 in train  X  data then use  X  to estimate  X  = { R all { r be defines as follows: aspects, in this paper, we use the full set of keywords generated by the boot-which will be discussed in details in Section 4. 3.3 Inference Since the posterior distribution of latent variables can not be computed effi-to approximate posterior distribution. where the aspect assignment z for each word is specific by a multinomial distribution Mul (  X  s ), aspect proportion  X  dimensional Dirichlet distribution Dir (  X  d ), entity overall score R with a normal distribution with mean value  X  i,k and variation By applying the Jensen X  X  inequality, we obtain a lower bound of the log-likelihood log p ( w ,r all d ,r d,k |  X ,  X ,  X ,  X  2 1 , X  2 where H ( q )=  X  E [log q ] is the entropy of the variational distribution We use variational EM procedure that maximizes a lower bound with respect maximizes the lower bound with respect to the model parameters R , X , X  .
 E-step : Find the optimizing values of the variational parameters {  X ,  X ,  X ,  X ,  X ,  X  } according to  X  X  update  X ,  X  2 , X  k , X  2 k : update:  X  ,  X  their values.
 with respect to the model parameters: Update  X  and  X  : Update  X  As shown at Equ.( 18 ) the derivative with respect to  X  k we therefore must use an iterative method to find the maximum by Newton-Raphson as in Blei et al. (2003)  X  X  work. The Hessian matrix involved can be easily calculated as follows: Update  X ,  X  2 , X  2 1 , X  2 2 , X  2 3 3.4 Prediction In test  X  data , the prediction for  X  is calculated as follows: The meanings for  X  and  X  have been demonstrated in the previous subsection. p ( w |  X ,  X  ) can be calculated by integrating out parameters hard to calculate. Here, we employ an alternative method which firstly assign p ( w |  X ,  X , z ) and can be attained as follows: p ( w  X  E We set the derivative of p ( w  X  E i |  X  i , X ,z ) with respect to element in get the following results: T ( r Since the derivative for one parameter in  X  depends on others, we employ the updated according to Equ.(26)-(30) until convergence.
 4.1 Data Sets and Preprocessing on 7 predefined aspects such as service , cleanliness , value filtering, we have 86,134 reviews from 462 hotels. 4.2 Aspect and Rating Evaluation We compare our hierarchical model HM with existing methods on the quality of identified topical aspects. We compare our model with other three models: LDA , Local-LDA sLDA . Our model is similar to LDA except that LDA only use the word co-occurrence. sLDA extends LDA by adding a regression model to capture the overall response and Local-LDA is a sentence-level LDA which is good at discovering aspects. For fair comparison, we do not use any word prior for any models.
 set of keywords (in total 309 words) generated by the bootstrapping method is used as a prior to train a LDA model on this data set. The learned topics are regarded as the ground-truth aspect descriptions. All the three models are then trained without any keyword supervision. To align the topics detected by three models and ground-truth topics, we use the Kuhn-Munkres algorithm and than others because the aspects they identified are closer to the ground-truth aspects. than those general content words. In our model, we have different aspect-word sentimental word in topic modeling. Local-LDA, as a sentence-level model, is more effective in mining review aspects because usually words in one sentence tends to talk about one aspect.
 Table 1 shows top words in 7 aspects (the topic number is set to 10). We Table 3 shows the top words in aspect rating word distributions with corre-words (i.e. great, bad) in the rating-word distribution, but aspect words are are less than 10%), while sentimental words usually focus on certain specific the uniformly distributed words and show the remaining top words at Table 4 . Due to the space limit, we just show top words with  X  X ooms X  aspect and  X  X alue X  model. 4.3 Quantitative Prediction to align discovered aspects with predefined aspects. For evaluation, we employ measures employed in Wang et al. (2010): (1) Mean Square Error ( predicted ratings compared with the ground-truth ratings; (2) Mean Average Precision ( MAP N ); (3) NDCK N : a commonly used ranking evaluation algo-rithm in IR.
 strap+SVR and LocalLDA+SVR . Results are presented at Table 5 .sLDA and our model (HM) achieves comparative results. Both are better than LDA+ SVR and Bootstrap+SVR. Our model and sLDA can leverage the side infor-mation (e.g., rating scores) and discover latent topic representations so they outperform the decoupled two-step procedure as adopted in unsupervised topic models (Zhu and Xing, 2001).
 SVR and LocalLDA+SVR as baselines. Results are presented at Table 6 .The proposed model outperforms all other methods in all measures except MAP, for which it attains comparative results with sLDA+SVR. For hotel-level overall rating R all i , we use the hotel rating scores provided by Northstar truth ratings. Results are shown at Table 7 . The proposed model outperforms are hard to obtain, we use the average aspect ratings for each review in the models. Each baseline makes review-level aspect predictions and predict hotel-level aspect rating by the average of those predictions. Results are shown at This is because aspect information is much noisier and harder to predict. Also Since reviews within the same hotel tend to have similar ratings, hierarchical structures can help review-level rating predictions. That is why the proposed hierarchical model outperforms others at review level. The hierarchical model proposed in this paper is able to simultaneously pre-dict the review-level and entity-level overall and aspect ratings. Experiments of external keyword input.

