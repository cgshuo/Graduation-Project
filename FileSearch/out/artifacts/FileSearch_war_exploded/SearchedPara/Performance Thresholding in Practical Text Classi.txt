 In practical classification, there is often a mix of learnable and unlearnable classes and only a classifier above a min-imum performance threshold can be deployed. This prob-lem is exacerbated if the training set is created by active learning. The bias of actively learned training sets makes it hard to determine whether a class has been learned. We give evidence that there is no general and efficient method for reducing the bias and correctly identifying classes that have been learned. However, we characterize a number of scenarios where active learning can succeed despite these difficulties.
 I.2.6 [ Artificial Intelligence ]: Learning; H.3.1 [ Information Storage and Retrieval ]: Content Analysis and Indexing Algorithms, Measurement, Performance, Experimentation practical text classification, active learning, accuracy esti-mation, learnability
Text classification is the problem of devising an automatic method for determining membership of a document in a pre-defined text category or class [23]. For example, a spam fil-ter recognizes email messages as being part of the category spam and directs them to a special folder. The behavior of many other document-centric applications depends on cat-egory membership in a similar way. Text classification has therefore become one of the key technologies for processing and managing electronic documents.

Text classifiers are commonly created by estimating the parameters of a statistical classification model on a labeled Copyright 2006 ACM 1-59593-433-2/06/0011 ... $ 5.00. training set. Most academic research on text classification assumes the prior existence of a sufficiently large training set. However, in most practical settings there initially exists no labeled training set. Creating the labeled set that is needed for parameter estimation is therefore an integral part of the text classification problem.

In fact, training set creation is often the main cost in text classification since it has to be done manually. Sometimes labelers are hired and paid for the sole purpose of training set creation. In other cases, one needs to find a subject expert within an organization, typically an employee who has other responsibilities and will be reluctant to spend hours on end with a repetitive labeling task.

Another characteristic of practical text classification is that classifiers must operate at a minimum level of perfor-mance. For most business problems it is unacceptable to deploy a classifier with unknown performance. We there-fore need a way to assess the performance of a classifier in order to make sure that only well performing classifiers are deployed.

We call this setting practical text classification to distin-guish it from work on text classification that assumes the prior existence of a large labeled training set. Practical text classification is characterized by the following conditions:
For small classes, one cannot efficiently create a training set by drawing a random sample. A category with a rela-tive frequency of 0.01 is expected to have just 10 positive examples in a random sample of 1000. In general, one needs many more than 10 positive examples to train a reliable text classifier.

A more efficient method of training set creation is ac-tive learning (AL) [14]. AL samples documents from the subspace that contains  X  X nformative X  or  X  X ncertain X  docu-ments, i.e., those documents that the current classifier is unsure about and that are likely to provide the most infor-mation per labeling decision. An iteration of AL selects one or more such informative documents from the pool of un-labeled documents, labels them, adds them to the training set and retrains the classifier, thus redefining the region of uncertainty in each step. To get AL started, one needs a small seed set of labeled documents, which may come from a keyword search or some other source. AL implicitly as-sumes that a large pool of unlabeled documents is available and that querying this pool is cheap compared to labeling since the latter takes a human expert X  X  time.

It has been shown that AL is effective in creating high performance classifiers with a relatively small number of la-beling decisions [4, 14, 27]. However, given the additional need in practical text classification to only field classifiers that exceed some predetermined threshold, two problems arise: learnability and decidability.

Learnability refers to the fact that some classes are not learnable. We define a class to be learnable if there exists a learning procedure that produces a classifier exceeding a performance threshold  X  with a certain level of confidence, where threshold and confidence level depend on the applica-tion (a  X  of F=70% to 95% and a confidence level of 95% to 99% are typical). A class may not be learnable in principle because of a high Bayes error r ate intrinsic to the learn-ing problem or it may not be learnable because the selected document representation formalism is not powerful enough (e.g., a bag of words model). Note that this concept of learn-ability is different than PAC learnability in machine learning [18, 9]. In PAC learnability, a problem is considered learn-able if, with high probability, the learner finds a hypothesis within a delta of the minimum possible error. In contrast, we want the learner to find a hypothesis within delta of an absolute level of performance (and to detect if that is not possible). So learnability as defined here is with respect to an absolute level of performance whereas it is relative to the optimal possible performance in PAC learnability.
Some corpora used in text classification research only con-tain learnable classes. But in practice classes are selected and defined according to their utility for a business prob-lem. Our experience at several companies is that very often a subset of these classes turn out to be unlearnable. In some cases the reason is that even humans cannot make the classi-fication decision; in other cases the representations typically used in text classification omit important information (e.g., the order of words in the bag-of-words model). In both cases, the Bayes error rate of the classification problem, as defined in the particular application, is too high to predict class membership reliably (i.e., with performance exceeding the chosen threshold). If AL is applied to such a category, the training set will not contain sufficient information to learn the category. Most previous work on AL assumes that all categories are learnable, so the learnability problem was not addressed. In those cases where researchers work with a mix of learnable and unlearnable classes, their goal is to optimize classification performance even if optimal performance is at a low level for a subset of classes [14, 15] or to maximize micro-average [30], which is mostly determined by perfor-mance on large categories. The problem of learnability is ignored in both approaches.

Decidability refers to the meta-problem of deciding whether a category has been learned at a given point in AL or not. We need an automatic decision procedure that tells us whether learning was successful. The problem is that assessing per-formance objectively is hard in the absence of a randomly sampled training set. The actively learned training set is highly biased, so that standard methods (e.g., leave-one-out) do not produce usable estimates as we will show below. Decidability is critical in practical text classification as we define it since a classifier can only be deployed if it meets minimum performance requirements.

The bulk of this paper is concerned with developing a method that addresses learnability and decidability based on an idea due to [12] for estimating the performance of a classifier from unlabeled data. We propose several variants of this method and evaluate them experimentally. Our con-clusion is that there is no general solution to learnability and decidability in practical text classification that is more efficient than random sampling. In particular, we conclude that in reality AL does not reduce the effort required to pro-duce a deployable classifier unless additional assumptions are made. While our study is empirical, we believe that the experimental evidence presented here provides strong sup-port for our conclusions.

In addition to recasting the problem of practical text clas-sification in these terms, our main contribution is to bound the applicability of AL. We characterize a number of sce-narios where AL is an effective approach to practical text classification in spite of the difficulties discussed.
The paper is organized as follows. Two examples illus-trating the problems of learnability and decidability are pre-sented in Section 2. Section 3 describes a decision procedure based on Lewis X  estimator of F. Five methods that attempt to produce unbiased estimates for this procedure are pro-posed and evaluated in Section 4. Section 5 analyzes the results of the experiments. Sections 6 and 7 discuss related work and learnability and decidability in practical text clas-sification in light of our experiments.
For illustration, we present two examples, one synthetic and one from the experiments described below. In the syn-thetic example, we want to learn a binary classification func-tion f defined on the real numbers. Let the hypothesis space be the set of sets of real non-overlapping intervals: for i  X  n c  X  1. A point is in a category iff it is in one of its up with a seed set of points none of which is in an interval [2 i, 2 i +1] ,i &gt; 0, then AL will focus on the interval [ and its surroundings. There simply is no information that we could exploit systematically that would distinguish cases where we learn c 0 from cases where we learn one of the other and thus reach precision close to 100%. But there is no gen-eral procedure for determining whether there are unexplored parts of the space that contain positive examples. We call such unexplored regions missed clusters . Missed clusters can only be found with random sampling, which by definition is not AL. As a result, AL cannot learn the categories c i ,i&gt; 0 for a seed set that does not contain points from the inter-val [2 i, 2 i + 1]. This is the learnability problem: a category c i ,i &gt; 0 will only be learned for certain fortuitous seed sets. If (parts of) a category are not learned, then this affects accuracy estimation, in particular recall estimation. Recall will be overestimated in the case of an unknown missed clus-ter. For that reason a correct decision as to whether a classi-fier has reached a certain level of accuracy cannot be made. This is the decidability problem.
To provide an example from our experiments, we analyze an AL model for the category  X  X ustralia X  in RCV1, a corpus of newswire articles from 1996 and 1997 covering topics like politics, business and sports (see Section 4 for details on our experimental setup). A set of false negatives from unlabeled data was clustered into 10 clusters using k-means. We com-puted the distance of these clusters to the  X  X abeled cluster, X  the cluster consisting of the entire training set labeled in AL. We define the distance of two clusters as the smallest Eu-clidean distance of any pair of members. Cluster 7 (with 53 documents) was the cluster with the largest distance (1.64) to the labeled cluster. Note that the maximum distance of two normalized vectors is 2.0, so the two clusters are at close to maximum distance from each other. Of terms that did not occur in the labeled set, the following 5 occurred in most documents in Cluster 7: cbot (occurred in 48 documents), nymex (48), roundup (48), comex (47), and bushel (46). The cluster turned out to be a cluster of documents about the topic  X  X ustralian commodities roundup X . Except for the single word  X  X ustralian, X  there is nothing else that makes Cluster 7 similar to the concept of relevance that the AL process has learned.  X  X ustralian X  also occurs in many non-relevant documents, so there is no simple rule that would distinguish the false negatives in Cluster 7 from true nega-tives. Cluster 7 corresponds to the intervals [2 i, 2 i +1] in the synthetic data: AL is not able to identify the members of Cluster 7 as informative or uncertain and therefore worthy of being included in the training set  X  Cluster 7 is too far from the decision boundary. And unless we are willing to give up AL for random sampling, there is no search algorithm for locating such clusters far from the decision boundary. As a consequence there is no obvious criterion for stopping AL  X  we never know whether there is an undiscovered cluster remaining or not. 1
These two examples show that missed clusters, if they oc-cur, pose serious problems for the decision problem of inter-est. However, it is possible that missed clusters rarely occur in practical classification problems. In the next section, we test the performance of an accuracy estimation procedure that is expected to perform well in the absence of missed clusters and give evidence that it fails because of the missed cluster effect.
One solution to decidability is to define a level of accept-able performance  X  , say F=80%, and stop AL when this level has been reached. This procedure is motivated by our experience with text classification in practical applications. Usually a minimum quality is required for a classifier to be deployed. An alternative goal would be to achieve optimal performance, but this is insufficient  X  a performance of 5% can be optimal for a particular category. A classifier with such a low performance cannot be deployed.

The key question is: how do we know whether the classi-fier is below or above  X  ? We need an absolute (as opposed to relative) assessment of accuracy of the classifier. We do not have the luxury of a random held-out set in AL  X  if there were sufficiently many labeled examples for such a set, then there would be no need for AL. We demonstrate below that
If the pool is finite, all documents will eventually be added to the training set, but relying on the finiteness of the pool is no better than exhaustive labeling of all available data. leave-one-out estimation on the labeled sample has a large bias.

The decision procedure we evaluate as an alternative is based on Lewis X  estimator of F for an unlabeled random sample. This estimator takes advantage of the fact that we can compute expected error rates for a random sample if our classifier is probabilistic. For example, if the class prob-ability of a document according to the classifier is 80%, then there is a 20% error probability for a discrete assignment. The key advantage of this decision procedure is that we use an unlabeled instead of a labeled random sample to estimate the accuracy of the classifier.

We use the F 1 measure(basedon[28],henceforthF),the harmonic mean of precision (P) and recall (R), for evalua-tion. Actively learned classifiers sometimes fail catastroph-ically with respect to precision or recall. In those cases, F is close to the minimum of the two, which is a good char-acterization of classifiers with either precision or recall close to 0.

F can be defined as a function of true positives (tp), false positives (fp) and false negatives (fn):
F =( 1
We define F to be 0 if there are no true positives. We compute the expectations of tp X  X , fp X  X , and fn X  X  following [12]: where n is the number of documents,  X  p i is the estimated probability of document i being relevant and d i =1for  X  p i &gt; =0 . 5and d i =0for  X  p i &lt; 0 . 5.

Following Lewis, we can then estimate F as follows:  X 
F =
Lewis derives error bounds for this estimator in [12] and shows that, for two hypothetical data sets, it performs well if the classifier estimates the distribution of  X  p i over the data correctly and if this paper, we test the estimator empirically. The impor-tant property of  X  F for our purposes is that it allows us to assess the quality of a classifier without a large labeled train-ing set. All we need is a large unlabeled random sample and unbiased probability estimates. We can compute probabil-ity estimates  X  p i , make classification decisions d i based on these estimates, compute an estimate of F and then make a deployment decision  X  all without actual labels.

However, Lewis X  simulations suggest that this decision procedure for AL will only produce correct results if the probability estimates are unbiased. Biased estimates will in general lead to overly optimistic or pessimistic estimates of F because the classifier over-or underestimates its confidence in making decisions.
There can be a conflict between the two goals of classifica-tion accuracy and  X  X nbiasedness, X  i.e. the goal of producing and c 2 for a class C with prior probability p C =0 . 5. The estimates the probability of relevance for all documents as 0 . 5+ .Then c 1 is an unbiased estimator. Its classification accuracy is 50%. c 2 estimates a probability of relevance of 0.0 (without bias) for non-relevant and 0.9 (with negative bias) for relevant documents. c 2 is a biased estimator be-cause it underestimates its certainty by 0.1 for relevant doc-uments, but it has higher accuracy than c 1 (100 vs. 50). A choice between c 1 and c 2 is a choice between classification ac-curacy and unbiasedness. Ideally, we can find a method that is optimal on both counts; but in practice we may have to trade less bias for worse classification accuracy. This trade-off does in fact occur in the experiments below (methods C vs. LR for estimating precision).

We do not view this tradeoff as a traditional bias-variance tradeoff [7] because the learning problem (in particular, the training set) is variable. We are comparing a learning prob-lem with a randomly selected training set (low classification accuracy, low estimation bias) with a learning problem with a different, actively learned training set (high classification accuracy, high estimation bias). This comparison is the one of interest here since we want to hold the cost of classifica-tion, which corresponds to the number of labeling decisions, constant.

There is a large body of research about methods that pro-duce unbiased probability estimates (e.g., logistic regression) versus those that do not (e.g., Naive Bayes, [5]). However, the problem at hand is complicated by the fact that our trainingsetis1)smalland2)biased.

In the following sections, we look at 5 different classifica-tion methods and their ability to provide unbiased proba-bility estimates in the service of our decision procedure for AL. These methods are also compared with leave-one-out estimation.
The first half of the RCV1 corpus [15] (400,001 docu-ments) was used as the experimental testbed. 2 This first half was randomly split into POOL and EVAL. Seed and query documents are drawn from POOL, EVAL is used for evaluation.

In the first set of experiments linear SVMs [29] were used because they are generally viewed as having optimal or close to optimal performance in text categorization [31].
Each document is represented as a stemmed, tf-idf-weighted word frequency vector, using the formula (1 + log tf)  X  log for tf &gt; 0where N is the number of documents. Document vectors are normalized to unit length. A stop list of common words was used. Using this representation, the document vectors have 276,727 dimensions. Note that in contrast to other AL studies, feature selection was not necessary.
We use uncertainty sampling as proposed by [14], which, in each iteration, selects the most uncertain document for la-beling. The seed set for AL consisted of 5 positive and 5 neg-ative documents that were randomly selected from POOL. 100 iterations of AL were performed consisting of computing
We intend to use the second half as a temporally separated data set for research not discussed here. an SVM model ( M i , 0  X  i  X  99) on the labeled set, labeling the document with score closest to 0, and adding it to the labeled set. One last model, M 100 , was then computed on the labeled set of size 110.

The number of 100 iterations was chosen because the ex-pert X  X  time per category is often limited due to cost con-straints. If an expert (often a highly paid and specialized employee who is needed elsewhere) can label 2 examples per minute, then labeling 110 examples for 43 categories would take roughly one week. When highly uncertain long docu-ments (as opposed to short titles) are judged for extended periods of time, then a speed of greater than 2 per minute is hard to exceed, especially if high labeling accuracy is re-quired(asisthecaseinAL).

One set of 10 categories was selected randomly from each of 4 frequency ranges of n ([10 1 , 10 2 ), [10 2 , 10 3 ), [10 and [10 4 , 10 5 )) where n is the number of positive documents in POOL and EVAL. All categories with n&gt; 10 5 (a total of 3) were included. Categories with n&lt; 10 1 were excluded. The evaluation set therefore contains 43 categories. By con-struction, the set contains a mix of  X  X mall X  and  X  X arge X  cate-gories and is in that respect a good model of many practical text classification tasks. 5 trials of AL were run for each of the 43 categories.

To determine the optimal level of performance, one set of SVM models was trained on the entire pool (which was treated as labeled in this case). Table 1 compares the perfor-mance of this optimal model (column  X  X ptimal X ) with M 100 . Performance of M 100 is about 9% worse on average (56 vs. 65). All performance numbers in Table 1 were computed on EVAL.

To model a realistic AL scenario, we needed a mix of learnable and unlearnable categories. We confirmed post-hoc that our selection procedure achieved this. The median F on EVAL after 100 AL iterations is 65. 12 categories had an optimal F score of less than 50, which qualifies the cat-egory as not having been learned in most practical settings we are familiar with.

The experimental system was implemented in python. It uses svmlight [11] for SVM computations (with default pa-rameters for linear SVMs) and R for (non-regularized) lo-gistic regression.
Leave-one-out (LOO) estimation is commonly used for ac-curacy estimation. 110 SVM models were trained on the dif-ferent subsets of size 109 of the labeled set and then applied to the remaining labeled document. The 110 decisions were recorded and evaluated using F. The average estimation er-ror of LOO was -4, the average absolute estimation error was 19 (see Table 1, column  X  LOO). For each category, the standard deviation of the (non-absolute) error over 5 trials was computed. The average of these 43 standard deviations was 6, indicating moderate variability. Sigmas for all other non-absolute means in Table 1 were similarly computed to show that variability across the 5 runs was moderate in gen-eral (with the exception of  X  S ).

The magnitude of the estimation error (19) makes LOO unusable in practice. To illustrate this point, we estimated the expected error of accepting a classifier with performance  X  F estimated by LOO and true (unknown) performance F
F  X  c 3 6 8 17 3 5 4 6 3 5 4 4
P  X  c 5 6 12 12 4 3 7 7 5 6 7 6
R  X  c 3 6 6 17 2 5 5 8 4 5 3 3 the corresponding method (  X  F i  X  F i ,  X  P i  X  P i ,and
R i ).  X  LOO is the error of LOO for estimating the using threshold  X  on  X  F by where L is the (unit) loss that occurs when the decision is wrong. Assuming that the LOO estimates  X  F are normally distributed (with parameters e stimated from the 5 trials for each of the 43 categories), thresholding at 0.75 has a 35% chance of making a mistake over all the categories. LOO is effective if the labeled set is random and large. But if the labeled set is biased and small, as in our case, LOO does not provide good estimates and therefore does not lend itself to making reliable deployment decisions.
The premise of Lewis X  method is that probability esti-mates are unbiased. We test four different strategies for computing unbiased estimates: a hybrid model that converts scores into probabilities (method S), bagging (models C and CM), a model that is trained on unlabeled data (model SM) and multivariate logistic regression (model LR). Models S, C and LR are standard models commonly used in text clas-sification (see references below). Method SM (and also CM) were designed to use unlabeled data for the specific purpose of producing unbiased probability estimates. Most work in machine learning that exploits unlabeled data is instead fo-cussed on improving classification accuracy.
The first model is a simple hybrid SVM classifier. M 100 is applied to the final labeled set of size 110. A logistic model is then fit on the 110 scores as predictors and the known labels as responses. We call this model  X  X imple X  (S). This type of conversion of scores into probabilities is common in text classification [14, 20].

Classification and estimation results for method S are shown in columns S and  X  S of Table 1. Classification accu-racy is similar to that of M 100 (57 vs. 56). Estimation error is larger than for LOO: average absolute error is 30 (vs. 19 for LOO). The estimation bias ( X  S) is strongly positive: +23.

The reason for this positive bias is the bimodal distribu-tion of SVM scores that the logistic model is trained on: they are either close to -1 or 1. As a result, most estimated probabilities of relevance are close to 0 or 1: 96% of proba-the classifier is too sure of itself due to a training set with few documents in the middle range between clearly non-relevant and clearly relevant. 3 For the bagging [2] classifier (method C), we follow [21]. 5 SVMs were trained on subsets of 109 of the labeled set. The 5 SVMs were applied as a voting committee to EVAL and the probability of class membership was then computed as the proportion of yes votes. In initial experiments we found that uncorrelated committees perform best. To find 5 uncor-related methods we selected from the unlabeled pool a sub-set C consisting of the 1000 documents whose M 100 scores were closest to 0 (500 with positive scores, if available, and the rest with negative scores). The similarity s between two models was then defined as the correlation of their scores on C . With respect to a set of available models A and a subset of selected models U , we define the minimally similar model with the set A = { L i | 1  X  i  X  110 } where L i is the model trained on the set of 110 labeled documents minus docu-ment i . We then selected { D i A ( { L 110 } ) | 0  X  i  X  committee of 5. Unfortunately, model C X  X  classification and estimation results are poor, in fact the average error of +55 was the worst in any of the experiments performed. Bagging does not seem to produce accurate probability estimates for small biased training sets.

The third method, SM, exploits the distribution of un-labeled data for more accurate estimates. The basic idea is to guess the labels of unlabeled documents in the pool based on their distances from the separating hyperplane.We defined the uncertainty margin as all points at a distance of at most d from the separating hyperplane computed by
We ran the same experiment with disjoint training sets for SVM and logistic regression (under a cross-validation regime) on the hypothesis that overtraining might exacer-bate bias in estimation. However, there was no significant increase in the accuracy of estimates of F. M 100 . For the experiment we chose d =0 . 25. An unlabeled document with score s in POOL was  X  X rtificially X  labeled as true for s&gt;d ,asfalsefor s&lt;  X  d or assigned a class membership probability of 0 . 5  X  for  X  d  X  s  X  0and0 . 5+ for 0 &lt;s  X  d .Wechose = 1 6 because the four probabil-ities { n 3 , 0  X  n  X  3 } are the simplest way of distinguishing the four cases: certainly positive, uncertain tending positive, uncertain tending negative, certainly negative.
 A logistic model was then trained on the SVM scores of beled set) and artificial labels as responses. We call this model  X  X imple-mixed X  (SM) since it is trained on a mix of labeled and unlabeled data. The model X  X  performance for F is similar to M 100 , with the average being higher (61 vs 56, column SM). The absolute error in estimating F is worse than LOO (27 vs. 19, column  X  SM).

The last SVM method,  X  X ommittee mixed X  (CM), com-bines information from the 5 models in method C and the unlabeled pool in method SM. The 5 committee models were selected to be as diverse as possible in the hope that the amount of disagreement among models can be converted to an unbiased probability estimate. As an indicator of con-fidence in the prediction of the models, we simply use the average of the 5 scores. The  X  X rtificial X  labeling of the un-labeled pool is performed by method C so that documents are assigned a class membership probability in { 0 . 2  X  n n  X  5 } . A logistic model is trained on the averages of the 5 SVM scores as predictors and the union of true labels (for the labeled set) and method C predictions (for the unlabeled pool) as responses. In classification, the 5 SVM scores are averaged and the logistic model is then applied to this av-erage. Classification accuracy of method CM is better than than for LOO (average absolute error of 34 vs. 19, column  X CM).
SVMs are optimized with respect to optimal discrimina-tion without regard to producing accurate probability esti-mates. The multi-stage procedures that convert scores into probability estimates are not guaranteed to be optimal. We therefore also tested regularized logistic regression (LR) as a method that outputs unbiased probability estimates if cer-tain assumptions hold. We used the BBR package [8] to train a logistic classifier on the final training set of 110 la-beled documents.

The average F of LR was 58 (column LR). The bias of the estimation error for precision (+7, column  X  LR) is smaller than for any of the other reasonably performing methods (method C is an exception due to its poor F). However, the magnitude of the absolute error of F (39) is larger than for the SVM-based methods (again, except for method C).
All AL methods are by construction biased: Training set examples are selected non-randomly in order to create a maximally informative training set for a given number of la-beling decisions. It is however possible that some AL meth-ods construct training sets with a higher bias than others. To investigate a possible dependency of bias on AL method, we ran a final set of experiments with support vector ma-chine active learning (SVMAL [27]; here, LIBSVM [3] was used). SVMAL trains two SVM models for each member of Figure 1: Estimation error of methods CM (solid) and LR (dashed). Example: the true probability of relevance for documents with LR estimates in the in-terval [ . 9 , 1] was .881, the average estimate was .967, so the estimation error was .967-.881=.086. This is represented as the last data point on the dashed line: (.967,.086). the pool in each iteration, thus making a repetition of our large experiment with SVMAL infeasible. We instead ran a smaller experiment with 5000 documents in the pool and 20 classes. The average absolute errors were 36 and 38 for the MaxMin Margin and MaxMin Ratio approximations of SVMAL, respectively, vs. 37 for uncertainty sampling. This indicates that SVMAL-based accuracy estimates of F are too error-prone to be usable in reliable deployment decisions of classifiers.
We use the sign test [26] for significance testing. It is a weak test, but it turned out to be sufficiently sensitive for our purposes. The number of paired samples is 215 (43 cat-egories  X  5 trials). We report the two-tailed significance level p . The main result of the experiments is that Lewis X  estimator does not estimate F better than LOO. The other 5 methods perform worse than LOO, and all but method SM do so significantly ( p&lt; 10  X  5 ). We performed a de-tailed analysis for methods CM and LR. Figure 1 shows that CM fails because it overestimates for positive decisions ( X  p&gt; 0 . 5), thus increasing the precision estimate, and under-estimates for negative decisions ( X  p&lt; 0 . 5), thus increasing the recall estimate. Both overestimating for  X  p&gt; 0 . 5and underestimating for  X  p&lt; 0 . 5 amount to an overestimation of the classifier X  X  certainty that its decision is correct and therefore lead to large positive errors. Logistic regression also has a positive bias for positive decisions (dashed line), albeit a smaller one; but it overestimates probabilities for negative decisions, thus underestimating recall. (The  X  X eg-Figure 2: Distribution of relevant documents with respect to estimated probability. Each bar corre-sponds to the proportion of relevant documents that receive probability estimates in the corresponding range on the x-axis. 27% of relevant documents re-ceive LR probability estimates of less than 0.1, in-dicating the presence of missed clusters. ative X  interval [0.1,0.2] in Figure 1 only contains 17% of all documents, so that the positive bias for the other intervals dominates.) This explains why logistic regression has a large underestimation error (-41) for recall.

For methods S, CM and SM, final probabilities are pro-duced by a univariate logistic regression on a distribution of scores with most values close to -1 or 1 (since the small ac-tively learned training sets can easily be linearly separated in high-dimensional space). As a result, probabilities are for more than 99% of probability estimates for method CM. In contrast, the multivariate logistic regression employed by method LR produces many  X  X ncertain X  estimates: only 71% of estimates are close to 0 or 1. This uncertainty results in less optimistic precision estimates and pessimistic recall es-timates.

The experiments suggest that it is hard, if not impossi-ble, to estimate recall correctly based on ensemble methods (C, CM) or methods exploiting the distribution of unlabeled data (SM, CM). LOO performs best with an absolute er-ror of 18. The other 5 methods perform significantly worse ( p&lt; 10  X  5 ). An error of 18 or worse will make the deci-sion of whether a classifier can be deployed or not subject to high error when relying on LOO estimates. Recall esti-mation is hard because of the missed cluster effect discussed earlier. In the absence of information about whether there are unexplored parts of the space with relevant documents, an accurate assessment of recall is impossible.

In addition to missed clusters there are however other con-ceivable causes for incorrect probability estimates for true LOO  X  S SM C CM Table 2: Sign tests comparing error of precision es-timates for 6 methods. / indicate a level of sig-nificance of p&lt;. 01 .  X  indicates lack of significance. and false negatives. In particular, a difficult decision bound-ary could also give rise to poor recall estimation. In that case, true and false negatives with incorrect estimates would be located close to the decision boundary. Figure 2 shows that instead, a large number of documents are located far from the decision boundary (i.e., they receive LR estimates close to 0). This indicates that the main culprit for poor recall estimation is indeed the missed cluster effect. 4
Despite the difficulty of recall estimation, methods based on Lewis X  estimator should in principle be able to compute unbiased estimates for the densely sampled space around the decision boundaries and hence be able to estimate pre-cision accurately. Indeed, three of the methods (C, CM, and LR) are more accurate than LOO for estimating precision (see Table 2). LR beats all methods but C. However, C is the least accurate classifier: its F at 28 is less than half of optimal. This is an example of the estimation accuracy vs. unbiasedness tradeoff discussed earlier. C X  X  precision estimates are unbiased, but it achieves this by only assign-ing documents to the category that it is absolutely certain about. As a result, it can estimate the precision it achieves better than the other methods.

We conclude from these results that recall cannot be esti-mated accurately, but that probabilistic methods like mul-tivariate logistic regression can, in principle, estimate pre-cision correctly. While 12 is still a large absolute error, we have used the BBR system out-of-the-box without any tun-ing. Additional optimization for accurate estimation of pre-cision should improve this result. [22] present a stopping criterion for AL, but they do not address the issue of accuracy estimation. Having achieved optimal performance is not a satisfactory deployment crite-rion if that optimal performance is below minimum accept-ability. Also, the arguments with respect to recall apply to their proposed stopping criterion as well. No stopping cri-terion can tell us for sure that recall has reached an optimal level. Decidability is therefore not addressed.

There are other algorithms besides SVMAL that perform
For LOO, over-and underestimation of recall almost bal-ance out across classes (bias is -2). The missed cluster ef-fect explains cases of positive bias. Negative bias can oc-cur for highly non-redundant training sets. A document that has no close neighbors is likely to be misclassified in LOO. Rank correlation of [bias] and [number of documents in the training set whose closest neighbor had a cosine sim-ilarity of more that 0.15] (a measure of redundancy) was 0.54 ( p&lt; 0 . 001). This correlation supports the hypothesis that non-redundancy and missed clusters are counteracting effects in LOO, but further research on this issue is neces-sary. somewhat better than uncertainty sampling, but are also computationally more expensive (e.g., [24, 6, 10]). We chose the computationally most efficient AL method, uncertainty sampling, because a fast querying method is important, so that experts can be provided with the next document within a few seconds of their last judgment, even if the pool is large. Longer response times prevent concentration on the task at hand [25]. An efficient implementation of uncertainty sam-pling on a fast multi-processor machine meets this criterion, even for a very large pool as it was used in this paper. The pool must be large for learning small categories.
As in AL, learnability is also a problem for a combina-tion of supervised and unsupervised learning [1, 17, 19]. We would like to extend our results to combination learning strategies.

Bagging performed reasonably well for accuracy estima-tion in [21]. However, the newsgroup categories used in this study are taken from non-overlapping distinct corpora, e.g., politics vs. electronics newsgroups. Reuters categorization tasks are more difficult since categories are overlapping and correspond to more subtle human categorization decisions. The difference between Naive Bayes and SVM may also play a role. The accuracy/unbiasedness tradeoff implies that it is harder to get accurate probability estimates for a classifier with optimal (or close to optimal) accuracy. [16] find that a disagreement-based accuracy estimator performs well compared to LOO in a transductive learning setting for balanced categories. This result is not directly applicable to experiments where training and test set are drawn from the same distribution and categories are  X  X mall X  (or unbalanced) as in this paper.

In this paper, we have invest igated three performance measures: F, P, and R. Another important measure is utility (e.g., [32], where, as in our case, the complicating circum-stance of a biased training set is investigated). We don X  X  see any difference between accuracy estimation for F, P, and R vs. utility in principle: It seems hard to conceive of an un-biased estimator of utility that would not in turn rely on unbiased estimates of relevance. The literature on filtering has focussed on the decision p&gt; X  where p is the probabil-ity of relevance and  X  is the filtering threshold. However, estimates of relevance can be strongly biased (and therefore unsuitable for accuracy estimation) even if they  X  X nswer X  the question  X  p&gt; X  ? X  correctly. This question has not been investigated in filtering as far as we know.
We have argued that learnability and decidability are crit-ical issues in practical text classification and investigated unbiased estimation as a possible solution. The experimen-tal results suggest unbiased estimation is hard. In fact, we believe that there is no general solution to unbiased esti-mation for small and biased sets as they are produced in AL.

This is partly due to the missed cluster problem. If there are important parts of the representational space that have no representation in the small and biased AL sample, then available information is simply not sufficient for estimating a model that would be consistent with the true distribution of relevant documents.

Another possible solution to accuracy estimation would be to correct the bias. [13] quotes Catlett as suggesting stratification as a bias correction strategy. Stratification is effective if we have an independent characterization of the strata, for example their true relative frequencies. However, it is not clear with respect to what we can stratify in the estimation problem at hand. Stratification with respect to the document space is out of the question because of its high dimensionality. An alternative that we have tested ex-perimentally is to stratify wi th respect to the SVM scores or probability estimates that are output by the classifiers for the AL set. However, these attempts did not produce better estimates of true performance than the Lewis X  esti-mator used here. The problem is again the small size of the sample which does not contain enough information to reli-ably estimate the density of relevant documents, especially in the  X  X ow probability X  part of the space (low probability according to the actively learned classifier).

If learnability and decidability are serious problems, why have they not been addressed in previous work on AL? One reason is that most experimental work has been done on a few text collections with categories that have a straightfor-ward correspondence between word distribution and cate-gory membership. For example, most of the categories in the 20 newsgroups corpus are defined by a set of words that occur together, words such as  X  X raphics X ,  X 2D X ,  X 3D X ,  X  X ir-cle X ,  X  X ezier X  for comp.graphics. If significant words co-occur with each other enough, then AL can discover them one after the other. Cases like the commodities cluster for Australia will not occur. Recall is likely to be high after a small number of iterations and those parts of the space that contain relevant documents will be sufficiently densely sampled to compute usable estimates.

The importance of decidability has not been realized be-cause in most AL experiments the optimal performance for a classifier is high and classes are learnable. In this type of sce-nario, the typical number of iterations used in the literature (50 X 100) is sufficient to learn the category with acceptable performance. But this is an unrealistic assumption for prac-tical text classification. In most cases, we don X  X  know what the best possible level of performance is and many practical applications have a mix of learnable and unlearnable cate-gories.

We have shown that none of the methods tested here are estimators that would produce a good estimate of R, and hence F. The reason is that their probability estimates are highly biased. We are not aware of other methods that pro-duce unbiased estimates for small and biased training sets. Our conclusion is that, except for those cases where a large random training set is available in practical text classifica-tion, Lewis X  estimator cannot be used to address the prob-lems of learnability and decidability in AL.
The alternative to Lewis X  estimator is evaluation on a ran-dom sample. Indeed, most publications on text classifica-tion evaluate experiments with respect to a held-out random sample. They do not address the problem that such a held-out random sample is usually not available in practical text classification. In particular, it is not available in AL. AL has therefore no built-in method for evaluating the success of learning.

This means that neither of the two avenues for determin-ing whether a category has been learned is available. Neither random sampling nor unbiased estimation (and then using Lewis X  estimator) is an option that is available as a generic method in practical text classification.

However, if we are willing to make additional assumptions or expend significant additional resources, then the dilemma posed here can be overcome in certain situations. We con-clude by characterizing some of the scenarios in which prac-tical text classification problems can be solved even though there is no large randomly sampled training set available.
We conclude from our experiments that AL is not a gen-eral solution to the problem of practical text classification. The key to practical text classification is to bound the area of applicability of AL, the method of choice for creating the training set in supervised text classification. It is therefore important to characterize scenarios where AL is applicable and the problems of learnability and decidability can be ad-dressed.

Acknowledgments. We are grateful to Reuters for RCV1 and to Byron Dom, Omid Madani, Amir Najmi, Helmut Schmid, Jason Utt, Tong Zhang and the anonymous review-ers for comments. The first author was partially supported by Deutsche Forschungsgemeinschaft under grant SFB 632 Nexus. [1] A. Blum and T. Mitchell. Combining labeled and [2] L. Breiman. Bagging predictors. Mach. Learn. , [3] C.-C. Chang and C.-J. Lin. LIBSVM: a library for [4] D. Cohn, L. Atlas, and R. Ladner. Improving [5] P. Domingos and M. J. Pazzani. On the optimality of [6] Y. Freund, H. S. Seung, E. Shamir, and N. Tishby. [7] J. H. Friedman. On bias, variance, 0/1-loss, and the [8] A. Genkin, D. D. Lewis, and D. Madigan. Large-scale [9] D. Haussler. Probably approximately correct learning. [10] V. S. Iyengar, C. Apte, and T. Zhang. Active learning [11] T. Joachims. Learning to Classify Text using Support [12] D. D. Lewis. Evaluating and optimizing autonomous [13] D. D. Lewis. Training text classifiers by uncertainty [14] D. D. Lewis and W. A. Gale. A sequential algorithm [15] D. D. Lewis, Y. Yang, T. G. Rose, and F. Li. RCV1: [16] O. Madani, D. M. Pennock, and G. W. Flake.
 [17] A. K. McCallum and K. Nigam. Employing EM in [18] T. M. Mitchell. Machine Learning . McGraw-Hill, New [19] I.Muslea,S.Minton,andC.A.Knoblock.Active+ [20] J. Platt. Probabilistic outputs for support vector [21] N. Roy and A. McCallum. Toward optimal active [22] G. Schohn and D. Cohn. Less is more: Active learning [23] F. Sebastiani. Machine learning in automated text [24] H. S. Seung, M. Opper, and H. Sompolinsky. Query [25] S. L. Smith and J. N. Mosier. Guidelines for designing [26] G. W. Snedecor and W. G. Cochran. Statistical [27] S. Tong and D. Koller. Support vector machine active [28] C. J. van Rijsbergen. Information Retrieval . [29] V. N. Vapnik. Estimation of Dependencies Based on [30] Y. Yang. Sampling strategies and learning efficiency in [31] Y. Yang and X. Liu. A reexamination of text [32] Y. Zhang and J. P. Callan. Maximum likelihood
