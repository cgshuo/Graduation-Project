 From a knowledge discovery standpoint, outliers are often more interesting than the common ones since they contain useful information underlying the abnormal much from the rest of the dataset by som e measure. Outlier detection has many pharmaceutical test and weather prediction. Various data mining algorithms [1,2,3,8,10,11,12,13,15,18,17,20,21,23,24,25] for outlier detection were pro-posed. The outlierness of an object typically appears to be more outstanding with respect to its local neighborhood. For example, a network intrusion might has been focused on finding local outliers , which are essentially objects that have significantly lower density 1 than its local neighborhood [2]. As an objec-of its density and the average density of its neighboring objects [2]. To quantify what are p  X  X  neighboring objects, users must specify a value k ,and k a value of 3. In this case, the three neighboring objects of p will have higher definition in [2]. This is obviously correct based on our intuition.
Unfortunately, the same cannot hold in more complex situation. Let us look at the following example.
 Example 1: We consider Figure 2 in which p is in fact part of a sparse cluster C Case I: The densities of the nearest neighboring objects for both p and q are stronger outliern ess measure than q , which is obviously wrong. Case II: Although the density of r is lower than p , the average density of its those of p . Thus, when the proposed measure is computed, p could turn out to have a stronger outlierness measure than r , which again is wrong.
Note that the two cases we described are not only applicable to p but also to the two objects above and below p . In general, any member of C 2 that is lying near the border between the two clusters could have been misclassified as showing stronger outlierness than q and r .
 From these examples, we can see that exis ting outlierness measure is not easily applicable to complex situation in which the dataset contains multiple clusters with very different density distribution. The reason for the above problem lies C 1 instead of C 2 .

To get a better estimation of the neighborhood X  X  density distribution, we pro-pose to take both the nearest neighbors (NNs) and reverse nearest neighbors (RNNs) [14] into account. The RNNs of an object p are essentially objects that have p as one of their k nearest neighbors. By considering the symmetric neigh-borhood relationship of both NN and RNN, the space of an object influenced by other objects is well determined, th e densities of its neighborhood will be reasonably estimated, and thus the outliers found will be more meaningful. As a simple illustration in Figure 3 which depicts the same situation as Figure 2, we show that p has two RNNs: s and t . This distinguishes it from q which has no RNNs, and r which has only an outlier as its RNNs. Later on in this paper, we will show how such an observation can be incorporated to ensure that the r . We now summarize our contributions in this paper: (1) We propose the mining of outliers based on a symmetric neighborhood rela-tionship. The proposed method considers the influenced space considering both neighbors and reverse neighbors of an object when estimating its neighborhood density distribution. To the best of our knowledge, previous work of outlier de-tection has not considered the effect of RNN. Such a symmetric relationship between NNs and RNNs will make the outlierness measurement more robust and semantically correct compar ing to the existing method. (2) We assign each object of da tabase the degree of being INFLuenced Out-lierness(INFLO) . The higher INFLO is, the more likely that this object is an outlier. The lower INFLO is, the more likely that this object is a member of a cluster. Specifically, INFLO  X  1 means the object locates in the core part of a cluster. (3) We present several efficient a lgorithms to mining top-n outliers based on INFLO . To reduce the expensive cost incurred by a large number of KNN and RNN search, a two-way search method is developed by dynamically pruning those objects with value INFLO  X  1 during the search process. Furthermore, we take advantage of the micro-cluster [11] technique to compress dataset for efficient symmetric queries, and use two-phase pruning method to prune out those objects which will never be among the top-n outliers. (4) Last but not the least, we give a comprehensive performance evaluation and analysis on synthetic and real data sets. It shows that our method is not only efficient and scalable in performance, but also effective in ranking meaningful outliers.
 new outlier measurement using symmetric neighborhood relationship and discuss some of its important properties. In s ection 3, we propose efficient methods for mining and ranking outliers in databases. In section 4, a comprehensive performance evaluation is made and the results are analyzed. Related work is discussed in section 5 and section 6 concludes the paper. In this section, we will introduce our ne w measure and related properties. The following notations will be used in the remaining of the paper. Let D be a database of size N ,let p , q and o be some objects in D ,andlet k be a positive integer. We use d ( p, q )todenotethe Euclidean distance between objects p and q .
 Definition 1 ( k -distance and nearest neighborhood of p ). The k -distance ( k { X  X  D \{ p }| d ( p, X )  X  k dist ( p )) } .
 inverse of the k -distance of p , i.e., den ( p )=1 /k dist ( p ) . Although the k -nearest neighbor of p may not be unique, k dist ( p ) is unique. Hence, the density of p is also unique. The nearest neighbor relation is not symmetric. For a given p , the nearest neighbors of p may not have p as one of also be taken into account when the outlierness of p is computed. Therefore, we introduce the concept of reverse ne arest neighbors [14] as follows. Definition 3 (reverse nearest neighborhood of p ). The reverse k -nearest neighborhood RNN is an inverse relation which can be defined as: RNN k ( p )= { q | q  X  D, p  X  NN For any object p  X  D , NN k search always returns at least k results, while the RNN can be empty, or have one or more elements. By combining NN k ( p ) and be used to estimate the density distribution around p .Wecallthisneighborhood space the k -influence space for p , denoted as IS k (p). Example 2: Figure 4 gives a simple description of how to obtain RNN in { p , q 1 , q 2 , q 3 , q 4 , q 5 } when k =3. NN k ( q 1 )= { p , q 2 , q 4 } , NN k ( q 2 ) = { p , q = { p , q ing the search of k -nearest neighbors of p , q 1 , q 2 , q , q 4 and q 5 , RNN k ( p )= { q 1 , q 2 , q 4 } is incre-mentally built. Similarly, RNN k ( q 1 ), RNN k ( q 2 ), RNN k ( q 3 ), RNN k ( q 4 )and RNN k ( q 5 ) are found.
 Note that NN k ( p )= { q 1 , q 2 , q 4 } = RNN k ( p ) (here IS 3 ( p )= { q 1 , q 2 , q 4 } ). If the value of k changes, RNN k ( p )maynotbeequalto NN k ( p ), or totally different.
 Unlike the nearest neighborhood, the influence space for an object p contains influential objects affecting p , more precisely estimating density around p  X  X  neighborhood w.r.t. these objects.
 Definition 4 (influenced outlierness of p ). Theinfluencedoutliernessisde-INFLO is the ratio of the average density of objects in IS k ( p )to p  X  X  local density. p  X  X  INFLO will be very high if its density is much lower than those of is a local outlier if INFLO k ( p ) &gt;t where t 1. On the other hand, objects with density very close to those in their influence space will have INFLO  X  1. Without loss of generality, we assume that for any local outlier object q outlier p cannot belong to RNN k ( q ).
 Lemma 1. Given any object p, q  X  D ,if max p  X  IS k dist ( q ) then den avg ( IS k ( p )) &gt;den avg ( IS k ( q )) . den avg ( IS k ( q )) Lemma 3. For p  X  D ,ifthereexists r  X  RNN k ( p ) such that k dist ( p )  X  k then p is a local outlier.
 k k a local outlier.
 sity value of p .
 Proof. Because the size of any clu ster should be larger than k (usually k = MinPts [2]), the higher the above ratio, the more influence for the local neigh-borhood to the object, and the higher density for this object. Essentially, mining influenced outliers is based on the problem of finding the influence space of objects, which is in KNN and RNN . In this section, we provide several techniques for finding influenced outliers, including the naive index-based method, the two-way search method and the micro-cluster method. 3.1 A Naive Index-Based Method Finding influence outliers requires the operations of KNN and RNN for each in a spatial index like R-tree, the cost of range queries can be greatly reduced by the state-of-the-art pruning technique [19]. Suppose that we have computed between p and the MBR 3 of a node in the R-tree (called MinDist( p ,MBR))is subtree rooted under the node will be among the k -nearest neighbors of p .This optimization can prune entire sub-tree containing points irrelevant to the KNN search for p . Along with the search of KNN ,the RNN of each object can be dynamically maintained in R-tree [14]. After building the index of KNN and RNN , the outlier influence degree can be calculated and ranked. The following algorithm is to mining top-nINFLO by building KNN and RNN index within R-tree. Algorithm 1 Index-based method.
 Input: k , D , n ,the root of R-tree.
 Output: Top-nINFLO of D .
 Method: 1.

FOR each object p  X  D DO 2. MBRList = root ; k dist ( p )=  X  ; heap =0; 3. WHILE (MBRList) != empty DO 4. Delete 1st MBR from MBRList ; 5. IF (1 stM BR is a leaf) THEN 6. FOR each object q in 1 stM BR DO 7. IF ( d ( p, q ) &lt;k dist ( p )) AND ( heap.size &lt; k ) THEN 8. heap.insert( q ); 9. k dist ( p )= d ( p, heap.top ); 10. ELSE 11. Append MBR X  X  children to MBRList; 12. Sort nodeList by MinDist; 13. FOR each MBR in MBRList DO 14. IF ( k dist ( p )  X  MinDist ( p, M BR )) THEN 15. Remove Node from MBRList; 16. FOR each object q in heap DO 17. Add q into NN k ( p ), add p into RNN k ( q ); 18. FOR each object p  X  D DO 19. Ascending sort top-n INFLO from KNN and RNN ;
Here MBRs are stored in ascending order based on MinDist( p , MBR ), as lines 11-12. The algorithm searches KNN p only in those MBRs with MinDist smaller than the temporary k dist ( p ), otherwise these MBRs are pruned (lines reverse nearest neighbor (lines 16-17). Finally, INFLO is calculated based on KNN and RNN index. 3.2 A Two-Way Search Method Two major factors hamper the efficiency of the previous algorithm. First, for any object p , RNN space cannot be determined unless all the other objects have finished nearest neighbor search. Second, large amount of extra storage is required on R-tree, where each object at least stores k pointers of its KNN ,and will be prohibitive. Therefore, we need reduce the computation cost for RNN and corresponding storage cost. By analyzing the characteristics of INFLO ,it is clear that any object as a member of a cluster must have INFLO  X  1even without INFLO calculation. So we can prune off these cluster objects, saving not only the computation cost but also the extra storage space. Theorem 1. For p  X  D , if for each object q  X  NN k ( p ) ,italwaysexists p  X  NN k ( q ) ,then INFLO k ( p )  X  1 .
 are close to each other. They are actually in a mostly mutual-influenced neigh-borhood. Since k is potentially the number of objects forming a cluster, under this circumstance, p resides in core part of a cluster.
 acluster( INFLO k ( p )  X  1), we can prune p immediately without searching cor-responding RNN . Such a early pruning technique will improve the performance significantly. The two-way search algorithm is given as follows: Algorithm 2 ATwo-waysearchmethod.
 Input: k , D , n ,the root of R-tree, a threshold M .
 Output: Top-nINFLO of D Method: 1.

FOR each p  X  D DO 2. count = | RNN k ( p ) | ; 3. IF unvisited ( p ) THEN 4. S = getKNN ( p ); // search k-nearest neighbors 5. unvisited ( p )= FALSE ; 6. ELSE 7. S = KNN ( p ); // get nearest neighbors directly 8. FOR each object q  X  S DO 9. IF unvisited ( q ) THEN 10. T = getKNN ( q ); unvisited ( q )= FALSE ; 11. IF p  X  T THEN 12. Add q into RNN k ( p ); 13. Add p into RNN k ( q ); 14. count ++; 15. IF count  X | S | X  M THEN // M is a threshold 16. Label p pruned mark; 17. FOR each object p  X  D DO // D is unpruned database 18. Ascending sort top-n INFLO from KNN and RNN ; The algorithm aims to search and prune objects that are likely to have low INFLO , thus avoid unnecessary RNN search. The | RNN k ( p ) | is initialized to 0 for p . Search process is taken two directions, that is, from one object to nearest neighbors, their nearest neighbors X  spaces contain p ,ormostofthem and can be pruned (lines 15-16). Finally, top-nINFLOs are calculated (lines 17-18).
 3.3 A Micro-Cluster-Based Method In order to further reduce the cost of distance computation, we introduce micro-micro-cluster can be estimated in influenced space. Under the guidance of the two-way search, those micro-clusters which actually are  X  X ore parts X  of clusters can be pruned and top-n outliers are ranked in the remaining dataset. Definition 5. (MicroCluster) The MicroCluster C for a d -dimensional dataset is r =max n j =1 ( X j  X  CF 3( C )) 2 . of any object.
 have l + n  X  1 micro-clusters. Algorithm 3 Micro-cluster method.
 Input: A set of micro-clusters MC 1 , ..., MC l , M .
 Output: Top-nINFLO of D .
 Method: 1.

FOR each micro-cluster MC 2. FOR each p  X  MC i Do 3. Get Max/Min of k dist ( p ) ;// based on theorem 2 4. IF Min k dist ( p ) &lt; Min k dist ( MC i ) THEN 5. Min k dist ( MC i )= Min k dist ( p ); 6. IF Max k dist ( p ) &gt; Max k dist ( MC i ) THEN 7. Max k dist ( MC i )= Max k dist ( p ); 8.

FOR each micro-cluster MC 9. count = | RNN k ( MC i ) | ; 10. IF unvisited ( MC i ) THEN 11. S = getKNN ( MC i ); // search k-nearest micro-clusters 12. unvisited ( MC i )= FALSE ; 13. ELSE 14. S = KNN ( MC i ); // get nearest micro-clusters directly 15. FOR each micro-cluster q  X  S DO 16. IF unvisited ( q ) THEN 17. T = getKNN ( q ); unvisited ( q )= FALSE ; 18. IF Min k dist ( q )  X  Max k dist ( MC i ) THEN 19. Add q into RNN k ( MC i ); 20. Add MC i into RNN k ( q ); 21. count ++; 22. IF count  X | S | X  M THEN // M is a threshold 23. Label MC i pruned mark; 24.

FOR each object p  X  unpruned micro-clusters MC DO 25. Ascending sort top-n INFLO from KNN and RNN ;
After building micro-clusters, the process of finding outliers is similar to the two-way search method. We simply trea t each micro-cluster as a single object to search KNN . As the number of micro-clusters is much less than that of of the k -nearest micro-cluste rs of a micro-cluster MC contain MC in their k -nearest micro-clusters as well, then MC will be located in the core part of any MC  X  X  neighboring micro-cluster q is bigger than the upper bound of that for MC ,then q belongs to MC  X  X  RNN (lines 18-21). By combining the two-way search and the micro-cluster technique, it achieves a significant improvement in performance. In this section, we will perform a comprehensive experimental evaluation on the efficiency and the effectiveness of our mining algorithm. We will compare our methods with the LOF method in [2] and show that our methods not only achieve a good performance but also identify more meaningful outliers than LOF . We perform tests on both real life data and synthetic data. Our real life dataset is the statistics archive of 2000-2002 National Hockey League (NHL), based on multiple-gaussian distribution, where the cardinality varies from 1,000 on 1.3GHZ AMD processor, with 512M B of main memory, under Windows 2000 advanced-server operating system. All algorithms are implemented by Microsoft Visual C++ 6.0.
 Experiments on Effectiveness. To achieve a com-prehensive understanding on the effectiveness of the INFLO measure, it is necessary to test on a series of datasets with different sizes and dimensions. We gen-erate our dataset with complex density distribution by a mixture of Gaussian distribution. Most outliers detected by our methods are meaningful with good explanations, and some of them cannot be found by LOF . For easily illustrating, we just pick up a por-tion of 2-dimensional dataset containing a low den-sity cluster A and a high density cluster B in Figure 5. The top-6 outliers are listed by INFLO and LOF respectively in Table 1.

Due to the limitation of space, we only show two instances. Table 1 lists the top 6 outliers based on the sample dataset in Figure 5, by both LOF and INFLO measures. The most outstanding ou tliers can be recognized by either measure. In this sample, 50 percentage of the top 6 outliers are the same points by both measures. When n is increased, INFLO will find even more differ-ent top outliers from LOF . By visual comparison, the top 6 outliers found by INFLO is more meaningful. Even for the same objects appeared in top-n lists of both measure, their position could be different and INFLO -based results are obviously more reasonable. In addition, INFLO can detect outliers which can be overlooked by LOF . For instance, the 50 th object and the 4 th object have inversely ranking orders by different measure. LOF only considers nearest neighborhood as a density estimation space, and the NN of both the 1 st and is ranked as a higher outlier than the 1 st with a high density. While INFLO measure considers both NN and RNN , some objects of B will influence the 50 th object, and thus make it being less outlierness than 1 st object. It is clear that using INFLO as outlierness measure preser ves more semantics than using LOF . Another interesting phenomena in experiments is that INFLO measure gives more rational indication for the outlier degree assignment. As an example, LOF value that are assigned to those bordering objects of a cluster has only a dering objects will have significantly larger INFLO values than the core part of cluster. Figure 6 presents such value differences curve by LOF and INFLO ,in which the difference is evaluated by cluster bordering objects and cluster mean center.
In the following experiments, we run our pro-posed algorithms with NHL 2000-2002 playoff data (22180 tuples) to rank top-n exceptional players in NHL. The results are compared with those computed from LOF .Wevaried k from 10 to 50. Projection is done on dataset by ran-domly selecting dimension s, and the outlierness of hockey players is evaluated. For example, we focus on the statistics data in 3-dimensional subspace of Games played, Goals and Shoot-ing percentage. Due to the limitation of space, information can be found in our examination. For example, there are two players Although he only took two games and got one point, his 100% shooting per-centage dominated other two statistics numbers in comparison. As it happens in identified by LOF . For example, Rob Blake ranks 4th in our method but is only few of players can become excellent shoo ter. Comparing to those players who have similar statistics number in Games Played and Goals dimensions, although Blake X  X  shooting percentage is rather lo w, Blake is still not too far away from other player when viewed in term of distance. Thus based on LOF measure, Blake X  X  could not be ranked in the top players. But the reason for him being a most exceptional player by INFLO is that there is no such type of player whose Shooting Percentage is so low while having so many Goals. Actually, Blake is the only defence whose number of goals scored is over 12. He must have shot too many times in the games without getting goals.

Another interesting example is Jaromir Jagr, who scores in the 3 rd position and ranks as the second outlier in LOF , but the 24 th in our measure. The reason is that even though Jagr has a strong goaling capability and a big fame, there are over twenty players who have higher statistics than him in Shooting Percentage and Games. So objectively, he is not ranked as the most exceptional player during 2000-2002 seasons. Note that we treat all the hockey data equal in the analysis not like hockey fans who always weigh goals much higher than other factors .
 Efficiency Issues of Experiments. We evaluate the efficiency of the proposed mining algorithms by varying the data size, dimension number, k and pruning parameter accordingly. Figure 7 shows the performance curves of different meth-ods, along with the runtime (include CPU time and I/O time) corresponding to different size of dataset with 5 dimensions. It shows that the run time of three methods are similar when the number of tuple is less than 100k. When the data way search is better than index-based method. When the size of the load is near to 1000k, swapping operation between R-tree and disk will happen frequently. As such, the performance of index-based method starts to degrade. On the other hand, since the two-way search method does early pruning in the search process, it reduces the total computation cost greatly and saves much time. Micro-cluster method achieves best performance because it not only uses the similar pruning technique as the two-way search, but also reduces the huge number of the near-mance is done by sacrificing some precision in KNN approximation. However, M increases, more objects in the database remain unpruned, but the possibility be removed, and the cost of future computation will be reduced. It is particu-candidates. Figure 10 shows the pruning results under the different radius of micro-cluster. We can see that when th e radius increases, more objects will be inside the micro-clusters, and the difference between lower and upper bound of not be pruned. Figure 8 presents different performance results of the two-way search method when k varies from 10 to 50. If k is less than 30, the scalability is good with the support of R-tree. When k is over 30, the cost for the nearest search is rather expensive, more MBRs w ill be searched to compute the distance between the objects and the query object. Thus the running time would increase drastically with the increased number of distance computation.

We also studied the relationship between performance of our algorithm and the number of dimensions, and Figures 11 and 12 show the runtime of our algorithm with different dimensions and varying database with respect to the microcluster-based method and the two-way search method respectively. From the experiment results, we know that the algorithms on smaller dimensionality hindering the efficiency of the algorithms. an object o being an outlier, if at most p objects are within distance d of o .A presented. The time complexity of this cell-based algorithm is O( N + c k )where based method, but still exponential to the number of dimensions. Ramaswamy k -nearest neighbor to rank the outliers. An efficient algorithm to compute the [20].

Some clustering algorithms like CLARANS [16], DBSCAN [6], BIRCH [26], and CURE [7] consider outliers, but only to the point of ensuring that they do of clustering algorithms, and these algorithms cannot rank the priority of outliers .
 The concept of local outlier, which assigns each data a local outlier factor LOF of being an outlier depending on their neighborhood, was introduced by their outlierness. To compute LOF for all objects in a database, O(n*runtime of a KNN query) is needed. The outlier f actors can be computed efficiently if mining algorithm which uses distance bound of micro-cluster to estimate the density, was presented in [11].

There are several recent studies on loca l outlier detection. In [5], [4], three enhancement schemes over LOF are introduced, namely LOF X  and LOF X  and GridLOF, and [22] introduces a connectivity-based outlier factor (COF) scheme that improves the effectiveness of a n existing local outlier factor LOF scheme when a pattern itself has similar neighborhood density as an outlier. They ex-outliers which are close to some non-outliers with similar densities. While our measure based on the symmetric relationship is not only compatible with their improved measures, but also identifies more meaningful outliers. LOCI [17] ad-dresses the difficulty of choosing values for MinPts in the LOF technique by using statistical values derived from the data itself. In this paper, we discuss the problem with existing local outlier measure and proposed a new measure INFLO which is based on a symmetric neighborhood relationship. We proposed various methods for computing INFLO including the naive-index based method, the two-way pruning method and the micro-cluster based method. Extensive experiments are conducted showing that our proposed methods are efficient and effective on bot h synthetic and real life datasets.
