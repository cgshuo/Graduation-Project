 discriminative analysis frameworks [19]. label MKL that is sublinear in the number of classes.
 running time, making it suitable for multi-label learning w ith a large number of classes. concludes this work. We denote by D = { x y k = ( y k assigned to the k -th class and y k points in D , i.e., K a p optimization problem: P dual problem of (1): where Q can view it as a minimization problem, i.e., min subgradient descent approach in [10] and compute the gradie nt of A ( p ) as where  X  k ( p ) = arg max 2.1 A Minimax Framework for Multi-label MKL kernel matrix K ( p ) that minimizes the worst classification error among m classes, i.e., Eq. (3) differs from Eq. (1) in that it replaces P m of using max approximation.
 dual problem of Eq. (3) as follows (more details can be found i n the supplementary documents) where follows where  X  = { (  X  1 , . . . ,  X  m )  X  R m  X  p a L ( p ,  X  ) =  X  where  X  k = arg max Following the mirror prox descent method [24], we define pote ntial functions  X   X  where Z p optimizing p and  X  , respectively.
 gradient of L ( p ,  X  ) with respect to p a and  X  k , denoted by b g p The computation of b g p Proposition 1. We have where E proportional to 1 / X  smoothing effect, without modifying  X  where  X  &gt; 0 is a small probability mass used for smoothing and SA for short. Algorithm 1 gives the detailed description. 2.2 Convergence Analysis of a solution ( p ,  X  ) We denote by ( p Proposition 2. We have the following properties for  X ( p ,  X  ) supplementary document.
 obtained by Algorithm 1 max Algorithm 1 Multi-label Multiple Kernel Learning: ML-MKL-SA 1: Input 2: Initialization 3: for t = 1 , . . . , T do 4: Sample a classification task j t according to the distribution M ulti (  X  1 6: Compute the estimated gradients b g p 8: end for 9: Compute the final solution b p and b  X  as Corollary 1. With  X  = m 2 3 and  X  iterations, we have E[ X ( b p , b  X  )]  X  O ( nm 1 / 3 p (ln m ) /T ) in terms of m , n and T . algorithm on the order of O ( m 1 / 3 p (ln m ) /T ) , sublinear in the number of classes m . ciency and effectiveness on the visual object recognition t ask. 3.1 Data sets suitable for multi-label learning. Abbreviations SA, GMKL, Sum, Simple, VSKL, AVG stand for ML-MKL-SA, Generalized MKL, ML-MKL-Sum, SimpleMKL, variable sparsity kernel learning and average k ernel, respectively class. 3.2 Kernels 300 visual words are used to form the chi-squared kernel.
 kernels for VOC2007 data set. 3.3 Baseline Methods use LIBSVM implementation of one-versus-all SVM where need ed. 3.4 Experimental Results Figure 3: Classification accuracy (AUC) of the proposed algorithm Ml-MKL-SA on CALTECH-101 using differ-ent values of  X  (for  X  system.
  X  is set to be 0.2. For simplicity we take  X  =  X  number of classes.
 ML-MKL-Sum, GMKL, and VSKL) on CALTECH-101 data set. We obse rve that, overall, ML-MKL-SA shares a stable algorithm than the proposed algorithm ML-MKL-SA. To further compare ML-MKL-SA with ML-MKL-Sum, significantly over a relatively short period of time, making it less desirable method for MKL. general insensitive to the choice of step size (  X  ). by exploring the correlation and structure among the classe s. (R31-2008-000-10008-0).

