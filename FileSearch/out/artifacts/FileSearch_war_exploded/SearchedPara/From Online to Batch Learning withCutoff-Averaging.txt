 Batch learning (also called statistical learning ) and online learning are two different supervised machine-learning frameworks. In both frameworks, a learni ng problem is primarily defined by an learning, we assume that there exists a probability distrib ution over the product space X  X Y , and that we have access to a training set drawn i.i.d. from this di stribution. A batch learning algorithm uses the training set to generate an output hypothesis , which is a function that maps instances in X to labels in Y . We expect a batch learning algorithm to generalize , in the sense that its output hypothesis should accurately predict the labels of previou sly unseen examples, which are sampled from the distribution.
 On the other hand, in the online learning framework, we typic ally make no statistical assumptions regarding the origin of the data. An online learning algorit hm receives a sequence of examples and processes these examples one-by-one. On each online-learn ing round, the algorithm receives an instance and predicts its label using an internal hypothesi s, which it keeps in memory. Then, the algorithm receives the correct label corresponding to the i nstance, and uses the new instance-label pair to update and improve its internal hypothesis. There is no notion of statistical generalization, as the algorithm is only expected to accurately predict the l abels of examples it receives as input. The sequence of internal hypotheses constructed by the onli ne algorithm from round to round plays a central role in this paper, and we refer to this sequence as t he online hypothesis sequence . Online learning algorithms tend to be computationally effic ient and easy to implement. However, many real-world problems fit more naturally in the batch lear ning framework. As a result, we are sometimes tempted to use online learning algorithms as if th ey were batch learning algorithms. A common way to do this is to present training examples one-by-one to the online algorithm, and use the last hypothesis constructed by the algorithm as the o utput hypothesis. We call this tech-nique the last-hypothesis online-to-batch conversion technique. The appeal of this t echnique is that it maintains the computational efficiency of the original on line algorithm. However, this heuris-tic technique generally comes with no theoretical guarante es, and the online algorithm X  X  inherent disregard for out-of-sample performance makes it a risky pr actice. In addition to the last-hypothesis heuristic, various prin cipled techniques for converting online al-gorithms into batch algorithms have been proposed. Each of t hese techniques essentially wraps the online learning algorithm with an additional layer of instr uctions that endow it with the ability to generalize. One approach is to use the online algorithm to cr eate the online hypothesis sequence, and then to choose a single good hypothesis from this sequence. For instance, the longest survivor tech-nique [8] (originally called the pocket algorithm) chooses the hypothesis that survives the longest number of consecutive online rounds before it is replaced. T he validation technique [12] uses a validation set to evaluate each online hypothesis and choos es the hypothesis with the best empirical performance. Improved versions of the validation techniqu e are given in [2, 3], where the wasteful need for a separate validation set is resolved. All of these t echniques follow the single hypothesis approach. We note in passing that a disadvantage of the vario us validation techniques [12, 2, 3] is that their running time scales quadratically with the numbe r of examples. We typically turn to online algorithms for their efficiency, and often a quadratic runni ng time can be problematic. Another common online-to-batch conversion approach, whic h we call the ensemble approach, uses the online algorithm to construct the online hypothesis seq uence, and combines the hypotheses in the sequence by taking a majority [7] or by averaging [2, Sec. 2.A]. When using linear hypotheses, averaging can be done on-the-fly, while the online algorithm is constructing the online hypothesis sequence. This preserves the computational efficiency of th e online algorithm. Taking the majority or the average over a rich set of hypotheses promotes robustn ess and stability. Moreover, since we do not truly know the quality of each online hypothesis, buil ding an ensemble allows us to hedge our bets, rather than committing to a single online hypothes is.
 Sometimes the ensemble approach outperforms the single hyp othesis approach, while other times we see the opposite behavior (see Sec. 4 and [9]). Ideally, we would like a conversion technique that enjoys the best of both worlds: when a single good online hypothesis can be clearly identified, it should be chosen as the output hypothesis, but when a good h ypothesis cannot be identified, we should play it safe and construct an ensemble.
 A first step in this direction was taken in [10, 5], where the co nversion technique selectively chooses which subset of online hypotheses to include in the ensemble . For example, the suffix averaging conversion [5] sets the output hypothesis to be the average o ver a suffix of the online hypothesis sequence, where the suffix length is determined by minimizin g a theoretical upper-bound on the generalization ability of the resulting hypothesis. One ex treme of this approach is to include the entire online hypothesis sequence in the ensemble. The othe r extreme reduces to the last-hypothesis heuristic. By choosing the suffix that gives the best theoret ical guarantee, suffix averaging automat-ically balances the trade-off between these two extremes. R egretfully, this technique suffers from a computational efficiency problem. Specifically, the suffix averaging technique only chooses the suffix length after the entire hypothesis sequence has been c onstructed. Therefore, it must store the entire sequence in memory before it constructs the outpu t hypothesis, and its memory footprint uses no memory aside from the memory used by the online algori thm itself. When the training set is massive, storing the entire online hypothesis sequence i n memory is impossible.
 In this paper, we present and analyze a new conversion techni que called cutoff averaging . Like suffix averaging, it attempts to enjoy the best of the single h ypothesis approach and of the ensemble approach. One extreme of our technique reduces to the simple averaging conversion technique, while the other extreme reduces to the longest-survivor con version technique. Like suffix averaging, we search for the sweet-spot between these two extremes by ex plicitly minimizing a tight theoretical generalization bound. The advantage of our technique is tha t much of it can be performed on-the-fly, as the online algorithm processes the data. The memory requi red by cutoff averaging scales with square-root the number of training examples in the worst case, and is far l ess in the typically case. This paper is organized as follows. In Sec. 2 we formally pres ent the background for our approach. In Sec. 3 we present the cutoff averaging technique and provi de a statistical generalization analysis for it. Finally, we demonstrate the merits of our approach wi th a set of experiments in Sec. 4. Recall that X is an instance domain and that Y is a set of labels, and let H be a hypothesis class, where each h  X  H is a mapping from X to Y . For example, we may be faced with a confidence-rated binary classification problem, where H is the class of linear separators. In this case, X is a subset of the Euclidean space R n , Y is the real line, and each hypothesis in H is a linear function the actual binary label predicted by h , and | h ( x ) | as the degree of confidence in this prediction. to denote the penalty incurred for predicting the label h ( x ) when the correct label is actually y . Returning to the example of linear separators, a common choi ce of loss function is the zero-one loss , which is simply the indicator function of prediction mistak es. Another popular loss function is the hinge loss , defined as As noted above, in batch learning we assume the existence of a probability distribution D over the product space X  X Y . The input of a batch learning algorithm is a training set, sa mpled from D m . The risk of a hypothesis h , denoted by  X  ( h ; D ) , is defined as the expected loss incurred by h over examples sampled from D . Formally, We can talk about the zero-one-risk or the hinge-loss-risk, depending on which loss function we choose to work with. The goal of a batch learning algorithm fo r the hypothesis class H and for the loss function  X  is to find a hypothesis h  X   X  X  whose risk is as close as possible to inf In online learning, the labeled examples take the form of a se quence S = ( x i , y refrain from making any assumptions on the process that gene rates S ; it could very well be a stochas-tic process but it doesn X  X  have to be. The online algorithm ob serves the examples in the sequence one-by-one, and incrementally constructs the sequence of o nline hypotheses ( h h  X  X  . The first hypotheses, h 0 , is a default hypothesis , which is defined in advance. Before round t begins, the algorithm has already constructed the prefix ( h algorithm observes x t and makes the prediction h the algorithm suffers a loss of  X  ( h to construct the next hypothesis h the online learning algorithm. In this paper, we make the sim plifying assumption that the update rule is deterministic, and we note that our derivation can be extended to randomized update rules. Since S is not necessarily generated by any distribution D , we cannot define the risk of an online hypothesis. Instead, the performance of an online algorith m is measured using the game-theoretic notion of regret . The regret of an online algorithm is defined as In words, regret measures how much better the algorithm coul d have done by using the best fixed hypothesis in H on all m rounds. The goal of an online learning algorithm is to minimi ze regret. To make things more concrete, we focus on two online learning algorithms for binary classification. The first is the classic Perceptron algorithm [13] and the sec ond is a finite-horizon margin-based variant of the Perceptron, which closely resembles algorit hms given in [11, 4]. The term finite-horizon indicates that the algorithm knows the total length of the se quence of examples before ob-serving any data. The term margin-based indicates that the algorithm is concerned with minimizing the hinge-loss, unlike the classic Perceptron, which deals directly with the zero-one loss. Pseudo-code for both algorithms is given in Fig. 1. We chose these two particular algorithms because they exhibit two extreme behaviors when converted into batch lea rning algorithms. Specifically, if we were to present the classic Perceptron with an example-sequ ence S drawn i.i.d. from a distribution D , we would typically see large fluctuations in the zero-one-r isk of the various online hypotheses. (see Sec. 4). Due to these fluctuations, the ensemble approac h suits the classic Perceptron very well, input S = ( x i , y set w 0 = (0 , . . . , 0) set w 0 = (0 , . . . , 0) for i = 1 , . . . , m for i = 1 , . . . , m and typically outperforms any single hypothesis approach. On the other hand, if we were to repeat this experiment with the margin-based Perceptron, using hi nge-loss-risk, we would typically see a monotonic decrease in risk from round to round. A possible ex planation for this is the similarity between the margin-based Perceptron and some incremental S VM solvers [14]. The last hypothesis constructed by the margin-based Perceptron is typically be tter than any average. This difference between the classic Perceptron and its margin-based varian t was previously observed in [9]. Ideally, we would like a conversion technique that performs well in bo th cases.
 From a theoretical standpoint, the purpose of an online-to-batch conversion technique is to turn an online learning algorithm with a regret bound into a batch le arning algorithm with a risk bound. We state a regret bound for the margin-based Perceptron, so tha t we can demonstrate this idea in the next section.
 Theorem 1. Let S = ( x i , y and let  X  denote the hinge loss. Let H be the set of linear separators defined by weight vectors in the unit L Perceptron (see Fig. 1) when it processes S . Then, for any  X  h  X  X  , The proof of Thm. 1 is not much different from other regret bou nds for Perceptron-like algorithms; for completeness we give the proof in [1]. We now present the cutoff averaging conversion technique. T his technique can be applied to any conservative online learning algorithm that uses a convex h ypothesis class H . A conservative al-gorithm is one that modifies its online hypotheses only on rou nds where a positive loss is suffered. On rounds where no loss is suffered, the algorithm keeps its c urrent hypothesis, and we say that the hypothesis survived the round. The survival time of each distinct online hypothesis is the num-ber of consecutive rounds it survives before the algorithm s uffers a loss and replaces it with a new hypothesis.
 Like the conversion techniques mentioned in Sec. 1, we start by applying the online learning algo-rithm to an i.i.d. training set, and obtaining the online hyp othesis sequence ( h arbitrary non-negative integer, which we call the cutoff parameter . Ultimately, our technique will set k automatically, but for the time-being, assume k is a predefined constant. Let  X   X  ( h the set of distinct hypotheses whose survival time is greate r than k . The cutoff averaging technique defines the output hypothesis h  X  as a weighted average over the hypotheses in  X  , where the weight of a hypothesis with survival time s is proportional to s  X  k . Intuitively, each hypothesis must qual-ify for the ensemble, by suffering no loss for k consecutive rounds. The cutoff parameter k sets the bar for acceptance into the ensemble. Once a hypothesis is in cluded in the ensemble, its weight is determined by the number of additional rounds it perseveres after qualifying. We present a statistical analysis of the cutoff averaging te chnique. We use capital-letter notation throughout our analysis to emphasize that our input is stoch astic and that we are essentially ana-lyzing random variables. First, we represent the sequence o f examples as a sequence of random variables ( X line hypothesis sequence ( H function H of independently of (( X In words, the risk of the random function H suffered on round i + 1 , conditioned on the random examples 1 through i . This simple observation relates statistical risk with online loss, and is the key to c onverting regret bounds into risk bounds. Define the sequence of binary random variables ( B Now define the output hypothesis Note that we automatically include the default hypothesis H detail makes our analysis more elegant, and is otherwise irr elevant. Also note that setting k = 0 results in B conversion technique. At the other extreme, as k increases, our technique approaches the longest survivor conversion technique.
 The following theorem bounds the risk of H  X  Note that this is indeed the case for the margin-based Percep tron and the hinge loss function. Since the margin-based Perceptron enforces k w i k  X  1 , and assuming that k x i k  X  R , it follows from the Cauchy-Schwartz inequality that  X   X  [0 , R + 1] . If the loss function is not convex, the theorem does not hold, but note that we can still bound the average risk of t he hypotheses in the ensemble. Theorem 2. Let k be a non-negative constant and let  X  be a convex loss function such that constructs the online hypothesis sequence ( H B bility at least 1  X   X  , it holds that To prove the theorem, we require the following tail bound, wh ich is a corollary of Freedman X  X  tail bound for martingales [6], similar to [3, Proposition 2].
 Lemma 1. Let ( L of arbitrary random variables such that L U m  X  4 and for any  X   X  (0 , 1) , with probability at least 1  X   X  , it holds that Due to space constraints, the proof of Lemma 1 is given in [1]. It can also be reverse-engineered from [3, Proposition 2]. Equipped with Lemma 1, we now prove T hm. 2. Proof of Thm. 2. Define U P Now notice that, by definition, Since B expectation above. Using the observation made in Eq. (2), we have U we have shown that Using Jensen X  X  inequality, the left-hand side above is at le ast P m We can now complete the definition of the cutoff averaging tec hnique. Note that by replacing  X  with  X /m in Thm. 2 and by using the union bound, we can ensure that Thm. 2 holds uniformly for all k  X  { 0 , . . . , m  X  1 } with probability at least 1  X   X  . The cutoff averaging technique sets the output hypothesis H  X  to be hypothesis in { H  X  bound. In other words, k is chosen automatically so as to balance the trade-off betwe en the benefits of averaging and those of good empirical performance. If a sm all number of online hypotheses stand out with significantly long survival times, then our te chnique will favor a large k and a sparse ensemble. On the other hand, if most of the online hypotheses have medium/short survival times, then our technique will favor small values of k and a dense ensemble. Even if  X  is not convex, minimizing the bound in Thm. 2 implicitly minimizes the aver age risk of the ensemble hypotheses. given by Thm. 2 can be turned into a data independent risk bound. A detailed derivation of such a bound exceeds the scope of this paper, and we just sketch the p roof in the case of the margin-based Perceptron. It trivially holds that the risk of H  X  is upper-bounded by the bound given in Thm. 2 for k = 0 . When Thm. 2 is applied with k = 0 ,  X  L simply becomes the average loss suffered by the online algorithm over the entire training set and P B by the average loss of any  X  h  X  H on the sequence ( X be the hypothesis with the smallest risk in H , namely,  X  h = arg min bound for sums of independent bounded random variables, suc h as Hoeffding X  X  bound or Bernstein X  X  bound. The result is that, with high probability,  X  ( H  X  ; D )  X  min derivations appear in [2, 3].
 As mentioned in the introduction, our approach is similar to the suffix averaging conversion tech-nique of [5], which also interpolates between an ensemble ap proach and a single hypothesis ap-proach. However, the suffix conversion requires  X ( m ) space, which is problematic when m is large. In contrast, cutoff averaging requires only O (  X  m ) space. Our technique cannot choose the optimal value of k before the entire dataset has been processed, but neverthel ess, it does not need to store the entire hypothesis sequence. Instead, it can group the on line hypotheses based on their survival times, and stores only the average hypothesis in each group a nd the total loss in each group. By the time the entire dataset is processed, most of the work has already been done and calculating the optimal k and the output hypothesis is straightforward. Using simple combinatorics, the maximal number of distinct survival times in a sequence of m hypotheses is O (  X  m ) .
 Finally, note that Lemma 1 is a Kolmogorov-type bound, namel y, it holds uniformly for every prefix of the sequence of random variables. Therefore, Thm. 2 actua lly holds simultaneously for every prefix of the training set. Since our conversion is mostly cal culated on-the-fly, in parallel with the online rounds, we can easily construct intermediate output hypotheses, before the online algorithm has a chance to process the entire dataset. Thanks to the Kolm orogorv-type bound, the risk bounds for all of these hypotheses all hold simultaneously. We can m onitor how the risk bound changes as the number of examples increases, and perhaps even use the bound to define an early stopping criterion for the training algorithm. Specifically, we coul d stop processing examples when the risk bound becomes lower than a predefined threshold. Figure 2: Test error (zero-one-loss) of last-hypothesis and cutoff averaging , each applied to the stan-dard Perceptron, on ten binary classification problems from RCV1. The x-axis represents training set size, and is given in log-scale. Each plot represents the average over 10 random train-test splits. We conducted experiments using Reuters Corpus Vol. 1 (RCV1), a collection of over 800K news articles collected from the Reuters news wire. An average ar ticle in the corpus contains 240 words, and the entire corpus contains over half a million distinct t okens (not including numbers and dates). Each article in the corpus is associated with one or more high-level categories , which are: Cor-porate/Industrial (CCAT), Economics (ECAT), Government/ Social (GCAT), Markets (MCAT), and Other (OTHER). About 20% of the articles in the corpus are associated with more than on e high-level category. After discarding this 20% , we are left with over 600K documents, each with a single high-level label. Each pair of high-level labels defines the binary classification problem of distin-guishing between articles of the two categories, for a total of ten different problems. Each problem has different characteristics, due to the different number of articles and the varying degree of homo-geneity in each category.
 coordinate in the vector represents one of these tokens. If a token appears s times in a given article, the respective coordinate in the feature vector equals log We applied the cutoff averaging technique to the classic Per ceptron and to the margin-based Per-ceptron. We repeated each of our experiments ten times, each time taking a new random split of the data into a training set ( 80% ) and a test set ( 20% ), and randomly ordering the training set. We trained each algorithm on each dataset in an incremental man ner, namely, we started by training the algorithm using a short prefix of the training sequence, and g radually increased the training set size. We paused training at regular intervals, computed the outpu t hypothesis so far, and calculated its test loss. This gives us an idea of what would happen on smaller tra ining sets.
 Fig. 2 shows the test zero-one loss attained when our techniq ue is applied to the classic Perceptron algorithm. It also shows the test zero-one loss of the last-h ypothesis conversion technique. Clearly, cases, adding training data actually deteriorates the perf ormance of the last hypothesis. If we decide to use the last hypothesis technique, our training set size c ould happen to be such that we end up with a bad output hypothesis. On the other hand, the cutoff averag ing hypothesis is accurate, stable and consistent. The performance of the simple averaging conver sion technique is not plotted in Fig. 2, but we note that it was only slightly worse than the performan ce of cutoff averaging. When using the classic Perceptron, any form of averaging is beneficial, and our technique successfully identifies this.
 Fig. 3 shows the test hinge loss of cutoff averaging, last-hy pothesis, and simple averaging, when applied to the margin-based Perceptron. In this case, the la st hypothesis performs remarkably well Figure 3: Test hinge-loss of last-hypothesis , averaging , and cutoff averaging , each applied to the finite-horizon margin-based Perceptron, on ten binary clas sification problems from RCV1. The x-Within 1000 online rounds ( 0 . 1% of the data), the cutoff averaging technique catches up to th e last hypothesis and performs comparably well from then on. Our te chnique X  X  poor performance on the first 0 . 1% of the data is expected, since the tail bounds we rely on are me aningless with so few examples. Once the tail bounds become tight enough, our tech nique essentially identifies that there is no benefit in constructing a diverse ensemble, and assigns all of the weight to a short suffix of the online hypothesis sequence.
 We conclude that there are cases where the single-hypothesi s approach is called for and there are cases where an ensemble approach should be used. If we are for tunate enough to know which case applies, we can simply choose the right approach. However, i f we are after a generic solution that performs well in both cases, we need a conversion technique t hat automatically balances the trade-off between these two extremes. Suffix averaging [5] and cuto ff averaging are two such techniques, with cutoff averaging having a significant computational ad vantage.
 [1] Anonimous. Technical appendix submitted with this manu script, 2008. [2] N. Cesa-Bianchi, A. Conconi, and C. Gentile. On the gener alization ability of online learning [3] N. Cesa-Bianchi and C. Gentile. Improved risk bounds for online algorithms. NIPS 19 , 2006. [4] O. Dekel, S. Shalev-Shwartz, and Y. Singer. The Forgetro n: A kernel-based perceptron on a [5] O. Dekel and Y. Singer. Data-driven online to batch conve rsions. NIPS 18 , 2006. [6] D. A. Freedman. On tail probabilities for martingales. Annals of Prob. , 3(1):100 X 118, 1975. [7] Y. Freund and R. E. Schapire. Large margin classification using the perceptron algorithm. [8] S. I. Gallant. Optimal linear discriminants. Proc. of ICPR 8 , pages 849 X 852. IEEE, 1986. [9] R. Khardon and G. Wachman. Noise tolerant variants of the perceptron algorithm. Journal of [10] Y. Li. Selective voting for perceptron-like learning. Proc. of ICML 17 , pages 559 X 566, 2000. [11] Y. Li, H. Zaragoza, R. He, J. ShaweTaylor, and J. Kandola . The perceptron algorithm with [12] N. Littlestone. From online to batch learning. Proc. of COLT 2 , pages 269 X 284, 1989. [13] F. Rosenblatt. The perceptron: A probabilistic model f or information storage and organization [14] T. Zhang. Solving large scale linear prediction proble ms using stochastic gradient descent
