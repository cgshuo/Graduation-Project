 Information-theoretic clustering aims to exploit information theoretic measures as the clustering criteria. A common practice on this topic is so-called INFO-K-means, which performs K-means clustering with the KL-divergence as the proximity function. While expert efforts on INFO-K-means have shown promising results, a remaining challenge is to deal with high-dimensional sparse data. Indeed, it is possi-ble that the centroids contain many zero-value features for high-dimensional sparse data. This leads to infinite KL-divergence values, which create a dilemma in assigning ob-jects to the centroids during the iteration process of K-means. To meet this dilemma, in this paper, we propose a Summation-based Incremental Learning (SAIL) method for INFO-K-means clustering. Specifically, by using an equiv-alent objective function, SAIL replaces the computation of the KL-divergence by the computation of the Shannon en-tropy. This can avoid the zero-value dilemma caused by the use of the KL-divergence. Our experimental results on var-ious real-world document data sets have shown that, with SAIL as a booster, the clustering performance of K-means can be significantly improved. Also, SAIL leads to quick convergence and a robust clustering performance on high-dimensional sparse data.
 H.2.8 [ Database Management ]: Database Applications X  Data Mining ; I.5.3 [ Pattern Recognition ]: Clustering Algorithms, Experimentation Information-theoretic Clustering, K-means Distance, SAIL
In recent years, we have witnessed an increased interest in information-theoretic clustering [6, 7, 21, 16, 17, 8], since information theory [3] can be naturally used as the guidance for the clustering process. For instance, the clustering anal-ysis can be treated as the iteration process of finding a best partition on data in a way such that the loss of mutual infor-mation due to the partitioning is the least [6]. Indeed, many information theoretic measures, such as the KL divergence and the Shannon entropy, have been widely exploited as the clustering criteria for information-theoretic clustering.
This paper is focused on the problem of K-means cluster-ing with the KL-divergence as the proximity function. To fa-cilitate the discussion, we call the K-means with information-theoretic measures as INFO-K-means. To better understand the theoretic foundation of INFO-K-means, we present an organized study of two different views on the objective func-tions of INFO-K-means. First, we derive the objective func-tion of INFO-K-means from a probabilistic view. In this regard, we know that the probabilistic view takes several assumptions on data distributions, and the goal of INFO-K-means is to maximize the likelihood function on multinomial distributions. In contrast, the information-theoretic view has no prior assumption on data distributions. In this case, the objective function of INFO-K-means is to find a best par-tition on data so that the loss of mutual information is min-imized. The above indicates that the information-theoretic view on INFO-K-means is more appealing, since we do not need to make any assumption on data distributions. As a result, in this paper, we take the information-theoretic view on INFO-K-means.

While INFO-K-means has the sound theoretic foundation, there are some challenging issues with INFO-K-means. For example, people have shown that, for document clustering, the performance of K-means with the KL-divergence is not better than that of the spherical K-means, which has the cosine similarity as the proximity function [21]. Indeed, for high-dimensional sparse document data, K-means with the KL-divergence often has some difficult scenarios. As an ex-ample, the centroids in sparse data usually contain many zero-value features. This creates infinite KL-divergence val-ues, which lead to a challenge in assigning objects to the centroids during the iteration process of K-means.
A traditional way to handle this zero-value dilemma is to smooth the sparse data by adding a very small value to every instance in the data [16]. In this way, there is no in-stance having zero feature values. This smoothing method can avoid the zero-value dilemma for K-means with the KL-divergence, however it can degrade the clustering perfor-mance since the real data values have been changed.
As an alternative to the smoothing method, in this pa-per, we propose a Summation-based Incremental Learning (SAIL) method for INFO-K-means clustering. Specifically, by using an equivalent objective function, SAIL replaces the computation of the KL-divergence by the computation of the Shannon entropy. This can avoid the zero-value dilemma caused by the use of the KL-divergence. Our experimen-tal results on various real-world document data sets have shown that, with SAIL as a booster, the clustering perfor-mance of K-means can be significantly improved. Also, SAIL leads to quick convergence and a robust clustering perfor-mance on high-dimensional sparse data. Finally, the key ideas developed in the SAIL method are not limited to the KL-divergence. These ideas can be extended to some other types of proximity functions that fit K-means clustering in the case that the values of proximity functions cannot be directly computed for data with some special properties.
To better understand the theoretic foundation of INFO-K-means, in this section, we provide an organized study of two different views, the probabilistic view and the information-theoretic view, on the objective functions of INFO-K-means.
K-means [13] is a prototype-based, simple partitional clus-tering technique which attempts to find the user-specified K clusters. These clusters are represented by their centroids (a cluster centroid is typically the mean of the data objects in that cluster). K-means has an objective function: where c i denotes cluster i , m i is the centroid of c i , dist ( is the distance function, and  X  x is the weight of x given P x  X  x = 1. To reach the minimum, K-means employs a heuristic clustering process as follows. First, K initial centroids are selected. Then the two-phase iterations are launched. In the first phase, every point in the data is as-signed to the closest centroid, and each collection of points assigned to a centroid forms a cluster; then in the second phase, the centroid of each cluster is updated based on the points assigned to the cluster. This process is repeated until no point changes clusters.

As we know, different distance functions lead to different types of K-means. Our focus in this paper is on INFO-K-means. Let D ( x y ) denote the KL-divergence between two discrete distributions x and y ,wehave It is easy to observe that, in most cases, D ( x y ) = D ( y x ), so D (  X  ) is not a true metric which holds the symmetric prop-erty, such as the well-known Euclidean distance. If we let dist (  X  )  X  D (  X  ) in Formula (1), we have the objective function of INFO-K-means with the KL-divergence as follows. where each instance x has been normalized before clustering. To further understand INFO-K-means, we take two different views on the objective function (Equation (3)) of INFO-K-means in the following two subsections.
In this subsection, we first derive the objective function of INFO-K-means from a probabilistic view. Specifically, the objective function can be derived by maximizing the  X  X artitioned X  likelihood function of the EM algorithm, i.e., the crisp version of EM [21].

Let us take the document data set as the example. As-sume that we have a document data set D , which consists of K crisp partitions in multinomial distributions with dif-ferent parameters, i.e.,  X  1 ,  X  X  X  , X  K respectively. Let n ( x, y ) denote the number of occurrences of term y in document x , and n ( x )=
Theorem 1. Let L = P ( X |  X ) =  X  x p ( x |  X ) be the like-lihood function, A =  X  where H (  X  ) is the Shannon entropy [3]. Then, we have where p ( x )= n ( x ) /
Proof: By definition, where  X  X  X  reflects the  X  X risp X  property of the modified EM model, and  X  X  X  follows the multinomial distribution, i.e., p ( x |  X  )= X  y p ( y |  X  ) n ( x,y ) .
 Meanwhile, A can be transformed into
If we substitute the transformed A and log L into the left-hand-side of Equation (4), we can easily get the right-hand-side. So, the proof is completed.
 Remark: Let us compare Equation (4) with Equation (3). If we let  X  x  X  p ( x ), x  X  p ( Y | x ), and m k  X  p ( Y have obj  X  min( A  X  log L ) /B  X  max log L .Thisim-plies that, if we take the probabilistic view of the objec-tive function, INFO-K-means aims to maximize the likeli-hood function on multinomial distributions. This proba-bilistic view of INFO-K-means requires a series of assump-tions: p ( x )= n ( x ) / p ( y |  X  k )=( p ( x ) / imental section, we will show these assumptions can degrade the performance of INFO-K-means.
Here, we derive the objective function in Equation (3) from an information-theoretic point of view. We begin our analysis by introducing an important lemma as follows.
Given a set of probabilistic distributions { p 1 ,p 2 ,  X  X  X  and the corresponding weights {  X  1 , X  2 ,  X  X  X  , X  n } ,wehave
Lemma 1 (Cover&amp;Thomas,2006).
Now, given a document data set D , we want to partition D into K clusters without overlapping. Let random variables X , Y and C denote the document, term and cluster respec-tively, and x , y and c be the corresponding instances with p ( x ), p ( y )and p ( c ) as the occurrence probabilities. Further-more, we assume that p ( c )=
Theorem 2. Let I ( X, Y ) be the mutual information be-tween discrete distributions X and Y ,then I ( X, Y )  X  I ( C, Y )= Proof: By definition, If we substitute p ( y | c k )= have (c)=(d). Furthermore, it is easy to show By Lemma 1, we finally have Equation (6).
 Remark: Theorem 2 illustrates the information theoretic view of INFO-K-means; that is, INFO-K-means tries to find a best partition on data so that the loss of mutual informa-tion due to the partitioning is minimized.
 In summary, we can have two different views on INFO-K-means. Both views provide the sound theoretic founda-tion. Furthermore, while Equation (6) is very similar to Equation (4), the information-theoretic view on INFO-K-means seems to be more appealing, since there is no prior assumption for p ( x )and p ( x | c ), which is inevitable if we take the probabilistic view. In fact, we can view the probabilis-tic framework as a special case of the information-theoretic framework for INFO-K-means. In addition to having sound theoretic foundations, INFO-K-means has been widely shown that it has inferior per-formances to the spherical K-means on document cluster-ing [21]. However, in this section, we illustrate an implemen-tation challenge of INFO-K-means. We believe this chal-lenge is one of the key issues that degrade the clustering performance of INFO-K-means.

For example, assume that we use INFO-K-means to clus-ter document data sets. To optimize the objective in Equa-tion (3), we also launch the two-phase iteration process  X  reassigning instances to the  X  X earest X  centroids, and up-dating the centroids according to the newly assigned in-stances subsequently. To this end, we must compute the KL-divergence values between each instance p ( Y | x )andeach centroid p ( Y | c k ). In practice, we usually let where n ( x ) is the sum of all the term frequencies of x , p ( x ) is the weight of x , as in Equations (4) and (6). Therefore, by Equation (2), to compute D ( p ( Y | x ) p ( Y | c k )), we should expect that all the feature values of x are positive real num-bers. Unfortunately, however, this is not the case for high-dimensional document data sets, which are famous for the sparseness in their high dimensions.

To illustrate this, we observe the computation of each di-mension y .Asweknow, To simplify the discussion, we denote p ( y | x )log( p ( y by D y below. Then, the different combinations of p ( y | p ( y | c k ) values can result in four different cases as follows. 2. Case 2: p ( y | x )=0 and p ( y | c k ) &gt; 0 . In this case, 3. Case 3: p ( y | x ) &gt; 0 and p ( y | c k )=0 . In this case, 4. Case 4: p ( y | x )=0 and p ( y | c k )=0 . In this case, we
We summarize the above four cases in Table 1. As can be seen, for case 1 and 4, the computation of D y is reason-able. However, the computation of D y in case 2 has trouble; that is, it can not reveal any difference between p ( Y | p ( Y | c k ) in dimension y , although p ( y | c k )maybemuchlarger than zero. Nevertheless, the most difficult case to handle is case 3. On the one hand, it is hard to do computations with +  X  in practice. On the other hand, it is easy to know, if thereissomedimension y of case 3, the total KL-divergence of p ( Y | x )and p ( Y | c k ) is infinite. This does not work for high dimensional sparse data sets, because the centroids of such data sets may typically contain many zero-value fea-tures. Therefore, we will have big challenges in assigning instances to the centroids, since the  X  X nstance-centroid X  dis-tances measured by the KL-divergence can be infinite. We call this problem as the zero-value dilemma.
 Table 1: Cases in KL-divergence Computations.

One way to solve the above dilemma is to smooth the sparse data sets. For instance, we can add a very small value to the entire data set so as to avoid having any zero feature value. While this smoothing technique indeed changes the sparseness property of the data sets, we will demonstrate in the experimental section that this method actually degrades the clustering performance of INFO-K-means.

In summary, there is a need to develop a new implemen-tation scheme for INFO-K-means which should be able to avoid the zero-value dilemma.
In this section, we propose a new implementation scheme, named SAIL, for INFO-K-means. To illustrate SAIL, we first present the concept of K-means distance, and then use K-means distance to simplify the objective function of INFO-K-means. Finally, we introduce the SAIL method.
Here, we briefly introduce the K-means distance. A de-tailed study of the K-means distance is available in [19].
Definition 1 (K-means Distance). We say that a dis-tance function F is a K-means distance, if there exists some differentiable convex function  X  : R d  X  R such that
In fact, any distance function that fits K-means must have the ability to facilitate the convergence of the two-phase iterations. So we have
Lemma 2. A distance function F ( x, y ) : R d  X  R d  X  R fits K-means 1 ,ifandonlyif  X  C = { x 1 ,x 2 ,  X  X  X  ,x n } X  R d
The K-means distance fits the K-means clustering. In-deed, under certain acceptable assumptions, the K-means distance is the only distance that fits K-means when the centroid type is the mean. That is,
Theorem 3. Assume that F : R d  X  R d  X  R is a non-negative function such that: (1) F ( x, x )=0 ,  X  x  X  R d (2) F and F x are continuous, and (3) F y is continuously differentiable on x .Then F fits K-means if and only if F is a K-means distance.

The details and proofs can be found in [19]. We must point out that a K-means distance is not necessary to be a metric; that is, a K-means distance may not satisfy symmetry and triangle inequality properties. Also, the K-means distance is a family of distance functions with different  X  , including the well-known Bregman divergence induced by strictly convex functions [1]. We list some popular K-means distances in Table 2.

We know that the KL-divergence belongs to the family of K-means distance. Specifically, according to Definition 1, KL-divergence can be rewritten as
Throughout this paper we focus on K-means with the arith-meticmeanofclustermembersasthecentroid.
 In this subsection, we introduce the S ummation-B A sed I ncremental L earning (SAIL) scheme for INFO-K-means. In general, SAIL has two distinct functions: 1) performing summations for each cluster during the clustering process; and 2) maintaining the summations incrementally.

To simply the discussion, we first transform the objective function in Equation (6) as follows.
 Theorem 4. obj :min is equivalent to given p ( c k )= Proof: By Formula (7), we have Since we have
It is easy to observe that (b) is constant for the given data set and the weights for the instances. Thus, the goal of minimizing the original objective function is equivalent to minimize (a).

Next, based on the simplified objective function in (8), we establish the SAIL scheme for INFO-K-means. Gener-ally speaking, SAIL is a greedy scheme which updates the objective function value  X  X nstance by instance X . In other words, SAIL iteratively repeats the same procedure. First, SAIL randomly selects an instance from the data and assigns it to the most suitable cluster; then updates the objective function value and other related variables according to the assignment. This process is repeated until some stopping criterion is satisfied.
 It is clear that SAIL differs from the traditional K-means. Indeed, SAIL is an incremental algorithm while the tradi-tional K-means usually employs the batch learning mode.
Furthermore, SAIL also differs from the traditional incre-mental K-means; that is, to decide the assignment of each selected instance, SAIL does not compute the KL-divergence values between the instance and all the centroids. Instead, it computes the Shannon entropies for the centroids which are assumed to be updated. This computation is supported by the two incrementally maintained summations for each clus-ter c : p ( c k )=
For instance, suppose SAIL randomly selects p ( Y | x )from acluster c k . Then, if we assign p ( Y | x ) to the cluster c the objective function in (8), the new objective value after this assignment will be as follows. where (a)-(b) and (c)-(d) represent the two parts of changes on the objective function value due to the movement of p ( Y | x )fromcluster c k to cluster c k . It is clear that the additivity of computation. Then, among the new objective function val-ues resulted by assigning the instance to all the clusters, we select the smallest one and assign the instance to the corresponding cluster. Finally, we update the two sum-mations for c k and c k accordingly. This is the assigning routine of SAIL for each selected instance. Apparently, the two incrementally updating summations, i.e., P x  X  c p ( x ) p ( Y
In summary, by the equivalent objective function in (8), the computations of the KL-divergences are replaced by the computations of Shannon entropies. As a result, we can avoid the zero-value dilemma. Figure 1 shows the pseudocode of the SAIL algorithm. Some implementation details are as follows.

Lines 1-3 are about data initialization. The preprocessing of
D in line 2 includes the row and column modeling. This routine is also used to smooth the instances and/or assign weights to the features. Then, in line 3, we normalize the for any feature y .

Lines 4-17 show the clustering process. Line 5 is for initial-ization, where objV al denotes the objective function value, label n  X  1 contains the cluster labels of the instances,  X  stores the weight summation of the instances in each clus-ter, and cluSum K  X  d stores the summation of the weighted instances in each cluster. That is,  X  [ k ]= cluSum [ k ]= d and K are the numbers of instances, features and clusters respectively. Two initialization modes have been employed in our implementation. In  X  X andom Assignment X  mode, we randomly assign the labels to the instances and then com-1. Read D and {  X  x | x  X  D } ; 2. D =Preprocess( D ); 3.  X  x  X  D , x = Normalize( x ); 4. for i =1: reps 6. for j =1: maxIters 7. for l =1:n 8. x = RandomSelect( { x | x  X  D } ); 12. end for 13. if label ( i ) is unchanged in iteration l 14. break; 15. end if 16. end for 17. end for pute the required variables. But in  X  X andom Read X  mode, we read the whole instances one by one at random, and use a routine very similar to the clustering one to assign each instance. Lines 8-11 describe the clustering routine for each instance, which has been introduced in details in the previ-ous subsection. Lines 13-14 show one stopping criterion in addition to the maxIter parameter; that is, in one cluster-ing if no instance changes its label after several iterations, we should stop this clustering. Finally, lines 18-19 choose the best clustering result among reps clusterings.
Next, we briefly discuss the convergence issues and the computational complexity of SAIL. First, SAIL is a heuris-tic approach, so it does not guarantee to converge to a global minimum. While the objective function value decreases con-tinuously (may not strictly) after reassigning each instance, and the combinations of the instances are limited, SAIL can still converge to some local minima. That is why we do multiple clusterings in SAIL and choose the best solution. In addition, SAIL also preserves the prominent advantage of the partitional clustering methods  X  low computational cost. Specially, the storage required is O (( n + K ) d )andthetime requirement for SAIL is O ( IKnd ), where I is the number of iterations required for convergence, K is the number of clusters, d is the number of features, and n is the number of objects. Since K is small, I is often not beyond 20, and d can be substantially reduced due to the sparseness of data sets, the SAIL algorithm is extremely fast in our experiments.
In this section, we demonstrate the effectiveness of SAIL on improving the performance of INFO-K-means. Specifi-cally, we show: 1) the traditional implementation of INFO-K-means with the KL-divergence is not effective on sparse data sets; 2) a simple smoothing technique is not a good so-lution for the zero-value dilemma; and 3) SAIL based INFO-K-means can achieve superior performances to the smooth-ing method as well as the spherical K-means. Experimental Tools. In the experiments, we employ four types of clustering tools. The first one is our developed SAIL-based INFO-K-means. The other three are well-known software packages for K-means clustering, including MAT-LAB v7.1 [14], CO-CLUSTER v1.1 [5], and CLUTO v2.1.1 [11].
The MATLAB implementation of K-means is the batch-learning version which must compute the distances between instances and centroids. We extend it to include more dis-tance functions, such as KL-divergence. Therefore, it is a centroid-based implementation of INFO-K-means which has to compute the KL-divergence directly.

CO-CLUSTER is a C++ program which implements the information theoretic co-clustering algorithm [7]. Although it still computes the  X  X nstance-centroid X  KL-divergences, it provides various methods to improve the clustering perfor-mances such as annealing, batch and local search, etc.
CLUTO is a software package for clustering high dimen-sional data sets. Specifically, its K-means implementation with cosine similarity as the proximity shows superior per-formances on clustering document data sets and gene ex-pression data sets [20]. In the experiments, we compare CLUTO with our SAIL-based INFO-K-means on a number of real-world data sets.

Note that the parameters of the four K-means implemen-tations were set to match one another for the purpose of the comparison, and the cluster number K was set to match the class number of each data set.
 Validation Measures. Many recent studies on clustering used the Normalized Mutual Information ( NMI )toeval-uate the clustering performance [21]. For consistency, we also use NMI in our experiments, which can be computed as: NMI = I ( X, Y ) / ables X and Y denote the cluster and class sizes, respec-tively. NMI values are in [0,1], and a larger NMI value indicates a better clustering result.

Moreover, we use Coefficient of Variation ( CV ) [4] to mea-sure the dispersion degree of the cluster sizes. The CV is defined as the ratio of the standard deviation to the mean. Given a set of objects X = { x 1 ,x 2 ,...,x n } ,wehave CV = s/  X  x where  X  x = CV is a dimensionless number that allows comparison of the variation of populations that have significantly different mean values. In general, the larger the CV value is, the greater the variability in the data.
 Experimental Data Sets. For our experiments, we used a number of real-world document data sets. Some character-istics of these data sets are shown in Table 3.

The fbis data set was from the Foreign Broadcast Infor-mation Service data of the TREC-5 collection [18], and the classes correspond to the categorization used in that collec-tion. The sports data set was derived from the San Jose Mercury newspaper articles that were distributed as part of the TREC collection (TIPSTER Vol. 3). It contains doc-uments about baseball, basketball, bicycling, boxing, foot-ball, golfing, and hockey. Data sets tr11 , tr12 , tr23 , tr31 , tr41 and tr45 were derived from TREC-5 [18], TREC-6 [18], and TREC-7 [18] collections. The classes of these data sets correspond to the documents that were judged relevant to particular queries. Data sets la1 , la2 and la12 were ob-tained from articles of Los Angeles Times that was used in TREC-5 [18]. The categories include documents from the entertainment, financial, foreign, metro, national, and sports desks. The ohscal data set was obtained from the OHSUMED collection [10], which contains documents from various biological sub-fields. Data sets k1a , k1b and wap were from the WebACE project [9]; each document corre-sponds to a web page listed in the subject hierarchy of Ya-hoo!. In particular, k1a and k1b contain exactly the same set of documents but the former contains a finer-grain catego-rization. The classic data set was obtained by combining the CACM, CISI, CRANFIELD, and MEDLINE abstracts that were used in the past to evaluate various information retrieval systems. Data set cranmed was attained similarly. Finally, the data set re0 was from Reuters-21578 collection Distribution 1.0 [12]. For all data sets, we used a stop-list to remove common words, and the words were stemmed using Porter X  X  suffix-stripping algorithm [15]. Here, we demonstrate the effect of the zero-value dilemma. Specifically, we show the difficulties of the traditional im-plementation of INFO-K-means with the KL-divergence on sparse data sets. Since MATLAB K-means can handle in-finity (denoted by INF), we selected tr23 and tr45 as the test data sets and applied MATLAB K-means for testing without smoothing. The clustering results are shown in Ta-ble 4, in which CV 0 and CV 1 represent the distributions of the class sizes and cluster sizes, respectively.

As indicated by the close-to-zero NMI values, the cluster-ing performance of MATLAB K-means without smoothing is extremely poor. Also, by comparing the CV 0 and CV 1 values, we found that the distributions of the resultant clus-Table 4: Clustering Results of MATLAB K-means.
 Table 5: Clustering Results of CO-CLUSTER.
 ter sizes are much more skewed than the distributions of the class sizes. In fact, for both test data sets, nearly all the data instances have been assigned to ONE cluster! There-fore, this experiment result confirms our analysis in Section 3 on the zero-value dilemma on sparse document data sets.
Furthermore, we tested CO-CLUSTER on tr12 and tr31 data sets. As mentioned above, CO-CLUSTER also com-putes the KL-divergence values in the clustering process, but it provides different search modes and the annealing technique to avoid poor local minima. Table 5 shows the clustering results, where  X  X earch X  and  X  X nnealing X  indicate the search modes and annealing parameters respectively.
In Table 5, we can observe that the use of annealing tech-nique and different search modes does not improve the clus-tering performance. The near-to-zero NMI values indicate the poor clustering performance. This again confirms that the direct computation of the KL-divergence values is infea-sible for sparse data sets. Another interesting observation is that, the clusters produced by CO-CLUSTER are much more balanced than the clusters produced by MATLAB K-means as indicated by the much smaller CV 1 values.
Here, we first illustrate the effect of the smoothing tech-nique on sparse data sets. In this experiment, we use MAT-LAB K-means and select seven document data sets. Fig-ure 2(a) and 2(b) show the clustering results on data sets tr11 and tr45 , where the added small values gradually in-crease along the horizon axis.

One observation is that data smoothing indeed improves the clustering performance of INFO-K-means. This fur-ther justifies our observation on the zero-value dilemma for INFO-K-means with the KL-divergence as the proximity function. Another interesting observation is that the opti-mal added values (OAV) are different for different data sets. For instance, while OAV  X  0 . 1 for tr11 , OAV  X  0 . 01 for tr45 . This implies one issue with data smoothing in prac-tice; that is, it is difficult to have the optimal smoothing effect. Nevertheless, we should avoid setting extreme values for added values. A tiny added value may not mitigate the zero-value dilemma, but a large value may damage the in-tegrity of the data instances, and thus leads to the degraded clustering performance. Figure 2(b) well illustrates this.
For the purpose of comparison, we also tested SAIL on these data sets. The parameters were set by default as fol-lows. 1)  X  x =1 /n , for any x  X  D ;2)noroworcolumn modeling in step 2 of Figure 1; 3) the initialization mode of the variables in step 5 of Figure 1 is  X  X andom read X . Figure 3 shows the comparison results. As can be seen, SAIL shows consistently superior performances to the smooth-ing technique. For some data sets such as tr11 , tr12 , tr41 and tr45 , SAIL takes the lead with a wide margin. Please note that, for the smoothing method, we tried a series of added numbers, i.e., 10  X  5 ,10  X  4 ,10  X  3 ,10  X  2 ,10  X  selected the best one for comparison.

In summary, while the traditional data smoothing tech-nique can improve the performance of INFO-K-means on sparse data sets, it changes the data integrity and has is-sues on setting the optimal data value added into the data. In contrast, SAIL has no parameter setting issue and can lead to consistent better clustering performances than the smoothing technique. This indicates that SAIL is a better solution for the zero-value dilemma.
In this subsection, we compare the clustering performance between SAIL and the spherical K-means. In the literature, people have shown that spherical K-means usually produces better clustering results than traditional K-means [21]. And the CLUTO version of the spherical K-means [11] even shows superior performances on document data sets, which makes it to be the benchmark method for document clustering. However, we would like to show in the experiment that the performance of SAIL is comparable to or even slightly better than the spherical K-means in CLUTO.
In this experiment, the parameter settings in CLUTO are as follows: clmethod=direct, crfun=i2, sim=cosine, ntri-als=10. Since the column modeling makes great impact on the spherical K-means, we provide clusterings with and with-out column modeling, corresponding to  X  X DF X  (Inverse Doc-ument Frequency) and  X  X one-IDF X  in Table 6. For SAIL, we used the default settings as the previous experiments. Table 6 shows the clustering results.

As can be seen, if matched parameters are used (i.e.,  X  X one-IDF X  for the spherical K-means), SAIL shows consis-tent better clustering quality on all 18 data sets. This can be observed in Figure 4: the NMI curve of the spherical K-means is completely included in the NMI curve of SAIL.
The Spherical K-means with IDF shows more appealing clustering results. As can be seen in Table 6, the cluster-ing performances of 11 out of 18 data sets have been im-proved with IDF. In particular, for data sets ohscal , re0 , tr23 , tr31 and wap , the clustering results are even better than that of SAIL. Nevertheless, we should also notice that the spherical K-means with IDF also degrades the cluster-ing performances of the rest 7 data sets. For data sets la12 , tr11 , tr12 , tr41 and tr45 , the negative impact is quite sub-stantial. This implies that column modeling is an X factor for the performance of the spherical K-means. That is, for data sets without class information, whether to use column modeling in the spherical K-means is a dilemma: it may im-prove or degrade the clustering performance. In contrast, SAIL with default settings shows consistent clustering per-formances and is more robust in practice.
 Table 7: Impact of Parameter Settings on SAIL.

In this subsection, we investigate the robustness and com-putation performance of SAIL.
 First, we observe the impact of data sparseness on SAIL. We compute the  X  X ensity X , i.e., the number of none-zero val-ues to the number of all values, for each document data set in Table 6. Thus we have the correlation between  X  X AIL X  and  X  X ensity X  columns: -0.16. This implies that the performance of SAIL has no strong correlation with data sparseness. In other words, the performance of SAIL is not sensitive to the data sparseness. This means that SAIL is a good solution to the zero-value dilemma.
 Second, we observe the impact of parameter settings on SAIL. Table 7 shows the results, where  X  X efault X  means no row or column modeling and the weights are the same,  X  X olumn Modeling X  employs IDF only, and  X  X ow Weigh-ing X  uses entropy-based and sum-based weighing methods respectively. Note that, for each instance x ,theentropy-based method uses 1 /H ( p ( Y | x )) as the weight, while the summation-based method uses n ( x )astheweight.

As can be seen in the table, column modeling and row weighing improve the clustering performances of SAIL on several data sets, as highlighted by bold font. However, in many cases, SAIL with default settings can produce satis-fying results already. Therefore, SAIL is relatively robust with the parameter settings.

Finally, Figure 5 shows the relationship between the num-ber of data instances and the number of iterations for con-vergence. In the figure, we can observe that the number of iterations for convergence (NIC) of SAIL is no more than 20 except for la2 and ohscal .Also, NIC increases slowly as the rapid increase of the number of objects n .
In the literature, great research efforts have been taken to incorporate information theoretic measures into exist-ing clustering algorithms, such as K-means. However, the zero-denominator dilemma remains a critical challenge. For instance, Dhillon et al. proposed information-theoretic K-means, which used the KL-divergence as the proximity func-tion [6]. While the authors noticed the  X  X nfinity X  values when computing the KL-divergence, they did not provide specific solutions to this dilemma. In addition, Dhillon et al further extended information-theoretic K-means to the so-called information-theoretic co-clustering [7]. This algorithm is the two-dimensional version of information theoretic K-means which monotonically increases the preserved mutual information by interwinding both the row and column clus-terings at all stages. Also, there is no solution provided for handling the zero-value dilemma when computing the KL-divergence. Finally, the information bottleneck (IB) is simi-lar to INFO-K-means in preserving mutual information [17]. In [16], Slonim and Tishby also found that the IB-based word clustering can lead to the zero-value dilemma. They suggested to use the smoothing method by adding 0.5 to each entry of the document data set.

This paper indeed fills this crucial void. Specifically, in our SAIL method, the computation of the KL-divergence is replaced by the computation of the Shannon entropy. This helps to avoid the zero-value dilemma.
This paper studied the problem of exploiting information theoretic measures, such as the KL divergence and the Shan-non Entropy, as the clustering criteria for K-means cluster-ing. In particular, we revealed the dilemma of the KL diver-gence for handling high-dimensional sparse data; that is, the centroids in sparse data usually contain zero-value features, and thus lead to infinite KL divergence values. This makes it difficult to use the KL divergence as a criterion for as-signing objects to the centroids. To that end, we developed a Summation-based Incremental Learning (SAIL) method, which can avoid the zero-value dilemma by using the Shan-non entropy instead of the KL divergence. This replacement is guaranteed by an equivalent mathematical transformation in the K-means objective function. Finally, as demonstrated in our experiments, SAIL can greatly improve the perfor-mance of K-means on high-dimensional sparse data.
This research was partially supported by the National Sci-ence Foundation of China (NSFC) (No. 70621061, 70321010), and the Rutgers Seed Funding for Collaborative Computing Research. Also, this research was supported in part by a Fac-ulty Research Grant from Rutgers Business School-Newark and New Brunswick. [1] A. Banerjee, S. Merugu, I. Dhillon, and J. Ghosh. [2] L. Brand. Advanced Calculus: An Introduction to [3] T. Cover and J. Thomas. Elements of Information [4] M. DeGroot and M. Schervish. Probability and [5] I. Dhillon. Co-clustering software, version 1.1. [6] I. Dhillon, S. Mallela, and R. Kumar. A divisive [7] I. Dhillon, S. Mallela, and D. Modha.
 [8] C. Elkan. Clustering documents with an [9] E.-H. Han, D. Boley, M. Gini, R. Gross, K. Hastings, [10] W. Hersh, C. Buckley, T. Leone, and D. Hickam. [11] G. Karypis. Cluto-version 2.1.1. [12] D. Lewis. Reuters-21578. [13] J. MacQueen. Some methods for classification and [14] MathWorks. K-means clustering in statistics toolbox. [15] M. Porter. An algorithm for suffix stripping. Program , [16] N. Slonim and N. Tishby. The power of word clusters [17] N. Tishby, F. Pereira, and W. Bialek. The information [18] TREC. Text retrieval conference. http://trec.nist.gov. [19] J. Wu, H. Xiong, J. Chen, and W. Zhou. A [20] Y. Zhao and G. Karypis. Criterion functions for [21] S. Zhong and J. Ghosh. Generative model-based
