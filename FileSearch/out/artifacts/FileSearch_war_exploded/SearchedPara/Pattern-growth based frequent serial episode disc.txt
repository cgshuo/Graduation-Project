 1. Introduction
Most of the existing algorithms for frequent episode disc overy concentrate on either serial or parallel episodes. essentially an ordered sequence of event types. An example of a 3-node serial episode can be denoted as ( A many applications. In this paper we will be interested in discovery of serial episodes. growth based algorithms for discovering frequent serial episodes.
 situations, it may be useful to count only non-overlapped occurrences the form of event of one type causing event of another type to occur and so on. frequent serial episodes are to represent underlying causative chains then such constraints are important. depth-first algorithms and the existing apriori-based algorithms. 1.1. Related work the dataset formed by the remaining suffixes of those sequences after matching the pattern. [19 exact and approximate schemes to calculate these probabilities from the uncertain data. handle span or gap constraints. We provide correctness proofs for the algorithms presented here. simulation results in Section 5 and conclusion in Section 6 . 2. Episodes in event streams In episode framework, the data, referred to as an event sequence , is denoted by number of events in the data stream. In each tuple ( E i , t occurrence of the event. The event types E i , take values from a finite alphabet, i =1,2, ... . The following is an example sequence with 10 events: event-type occurs at most once at a time-tick.

De fi nition 1. [1] An N -node episode  X  , is a tuple, ( V  X 
Thus an episode is a (typically small) collection of event-types along with an associated partial order. When  X  is referred to as a serial episode and when  X   X  is empty,
As mentioned already, in this paper, we are interested in discovery of serial episodes. that the total order on the nodes of  X  is given by v 1  X   X  g ( v 1 )= A , g  X  ( v 2 ) = B, and g  X  ( v 3 )= C ,with v 1 i th event-type in  X  . As per the above notation,  X  [ i ]is g (  X  [1]  X   X  [2]  X   X   X   X  [ N ]). Note that under the notation, a 1-node episode with event-type A is denoted as ( A ).
De fi nition 2. [1] Given a data stream,  X  ( E 1 , t 1 ), ... h : V  X   X  {1, ... , n } such that g  X  ( v )= E h ( v )  X  v
In the example event sequence (1), the events ( A ,2), ( B ,3) and ( C ,8) constitute an occurrence of ( A episode need not be contiguous in the data stream.

An episode  X  is said to be a subepisode of  X  (denoted  X   X   X  as that in  X  . For example, ( A  X  C ) is a 2-node subepisode of the episode ( A then it is easy to see that every occurrence of  X  contains an occurrence of
Given an occurrence h of an N -node (serial) episode  X  , t constraint is specified by a threshold, T X , such that occurrences of episodes whose span is greater than T threshold T g . That is, we would only count occurrences h for which t constraints can also improve efficiency of the discovery process by reducing the search space. 2.1. Various frequency de fi nitions criterion. For example, consider the data stream with successive event types being ( A  X  B  X  C ) but only 4 occurrences of each of its 2-node subepisodes, namely, ( A
De fi nition 3. [1] A window on an event sequence, D  X  E 1 ; t 1 integers such that t s  X  t n and t e  X  t 1 . The window width of [ t windows-based frequency of  X  is the number of windows of width T
For example, in the event sequence (1), there are 5 windows of width 5 which contain an occurrence of ( A are: [7,12], [10,15], [11,16], [12,17] and [13,18]. Hence its window-based frequency in (1) is 5 (for T
De fi nition 4. [1] The time-window of an occurrence, h, of  X  is given by t which contains an occurrence of  X  and no proper sub-window of it contains an occurrence of window is called a minimal occurrence. The minimal occurrences-based frequency of of minimal windows of  X  in D .

In the example sequence (1) there are 3 minimal windows of ( A three minimal occurrences in each of the minimal windows.

De fi nition 5. [8] Given a window-width k , the head frequency of occurrence of  X  starting at the left-end of the window and is denoted as f
De fi nition 6. [8] Given a window width k , the total frequency of
For a window-width of 7, the head frequency f h (  X  ,7) of ( A  X  B  X  C ) which contribute to its head frequency. The total frequency of ( B frequency does [8].
De fi nition 7. [9] Two occurrences h 1 and h 2 of  X  are said to be non-overlapped if either t occurrences of  X  in D is maximal if | H |  X  | H  X  |, where H frequency with time constraints like maximum span constraint ( T  X  candidate generation  X  step combines frequent episodes of size constraints reported in literature.
 head frequency [15] for serial episodes. 3. Discovery algorithms under the non-overlapped frequency each  X  -node serial episode  X  branches into F 1 jj number of episode) to the right-end of  X  . For example, if  X  =( A  X  the tree of all serial episode patterns,  X  branches down into 3 children episodes, namely, ( A ( A episode and hence its frequency, using the occurrence windows lists of its subepisode. We now explain our algorithms that use this strategy for frequent episode discovery. 3.1. With maximum span constraints
In(3) , {[1 2 3],[4 6 8],[10 12 14]} constitutesa maximal set of non-overlapped occurrencesofthe episode ( A this is the only such set. The corresponding list of occurrence windows for ( A windows for ( A  X  B  X  C  X  D ), using the lists of ( A  X  B episodes under non-overlapped occurrence based frequency with a span constraint. The algorithm is explained below. windows list of its (  X   X  1)-node prefix subepisode (say  X  minimal windows, we compute a maximal set of non-overlapping windows.
The function Compute-MO-list , whose pseudo-code is given as Algorithm 1, computes the minimal windows of
MO-list (  X  ), given the list of minimal windows, MO-list ( simultaneously and a temporal join of a minimal window of of ( E )at t E is its first occurrence after t e (lines 7 and 8 of Algorithm 1) and (ii) [ t 11 (cf. Lemma 1 ) thatthe function will correctly compute all the minimal windows.Giventhelist of minimal windows of
Algorithm 2. We go down the list of minimal windows, MO-list ( strategy is shown to yield maximal set of non-overlapped occurrences. episodes using the recursive call to NO-Recursive-fn (lines 5 (Algorithm 4), it forms episodes  X   X   X  p  X  E ;  X  E  X  F 1 ,computesthe MO-list ( non-overlapped count, f no (  X  ) by calling Find-NO-Count (lines 2 above the threshold, the lattice is traversed further down by calling NO-Recursive-fn recursively with
Once  X  p is extended with all the frequent one node episodes in F patterns  X  formed from  X  p has the same frequency; if not, we add the key (not shown in the procedures). So all the candidates with same f ( A
We now show the correctness of this procedure. We first show that the set of minimal windows of any episode in Algorithm 1.
 episode  X  inadatastream D . On this set, there is a  X  natural
De fi nition 8. The lexicographic ordering, b  X  ,on H , the set of all occurrences of h and h 2 ,of  X  , h 1 b  X  h 2 if the least i for which t h 1
We say that the occurrence h 1 is earlier than occurrence h prefix subepisode  X  p and the 1 -node suffix subepisode  X 
Proof. First we show that every window generated using this method is a minimal window of and t E be a time of occurrence of  X  [ N ] which are joined by our procedure to generate the occurrence window [ t that [ t s , t E ] is a minimal window. Suppose [ t s , t containing an occurrence of  X  . There are two possibilities here: (i) t occurrence of  X  p which starts after t s and ends before t
This means that [ t s , t e ]isnotthe last minimal window of consider the case t  X  E b t E . That is, there is an occurrence window of type  X  [ N ] occurring at t  X  E .If t  X  E N t e , it means that t joined [ t s , t e ]with t E .If t  X  E b t e , it means that there is an occurrence of
Next, we show that every minimal window [ t s , t e ]of  X  the earliest occurrence (as per Definition. 8 )of  X  p in [ t after t  X  e ; otherwise it contradicts the minimality of [ t procedure would perform a temporal join of t s ; t  X  e  X  and t
In the above algorithm, instead of computing the non-overlapped frequency of an episode for an N -node serial episode  X  under the minimal windows count, its ( N that in our DFS approach, an N -node episode  X  is generated if its ( N discovery based on a maximum gap constraint under the minimal occurrences-based frequency. 3.2. With maximum gap constraints occurrence of episode A  X  B  X  C given as [1 5 8]. With a gap constraint of T
A extensions. This is because, given any list of non-overlapped occurrence windows for an episode any given prefix or suffix subepisode  X  of  X  , in each of the non-overlapped occurrence windows of consider the following event stream.

In the event stream (4), there exists only one minimal window of contains only one occurrence of  X  , namely [1 2 3 9] which violates a gap constraint T of window of an episode satisfying a gap constraint.

De fi nition 9. A window [ t s , t e ] is a minimal window of an episode an occurrence of  X  satisfying G and none of its sub-windows contain an occurrence satisfying G . not true under gap constraint as the earlier example shows.

As mentioned in the previous subsection, [14] presents a method for finding the minimal occurrence windows windows.

De fi nition 10. [14] Let o =[ t s , t e ]beanoccurrenceofanepisode (mpo) of  X  iff  X  t 1 ; t 2  X   X  mo prefix  X   X  X  ; D  X  X  ,if t sequence.
 occurrence that starts strictly after t s and ends at or before t
N -node episode can be generated as a temporal join of some mpo of its ( N of its 1-node suffix subepisode. From the example sequence (4), with a gap constraint T is {[1,3],[1,10]} corresponding to the occurrences {[1 2 3],[1 5 10]} and the set of mpo's of A list of A  X  B  X  C is {[1,3]}.

Now our method for mining episodes under non-overlapped count with maximum gap constraints is as follows. We first the extracted minimal windows. We denote this set by H ng occurrences in H ng satisfy the following property.

Property 1. h 1 ng is the earliest minimal occurrence satisfying gap constraints. For any i, h
N-node episode ) starting after t h ng starts after t h ng We use this to prove the correctness of our algorithm as stated in the theorem below. Theorem 1. H ng is a maximal set of non-overlapped occurrences satisfying gap constraints.
Proof. Consider any other set of non-overlapped occurrences satisfying gap constraints: H h b  X  h  X  i  X  1 . Let m = min { f , l }. To show the maximality of H
This will be shown by induction on i . We first show it for i = 1. Suppose t constraint, this means that we have found a minimal occurrence satisfying gap constraints ending before h the first statement of Property 1. Hence t h ng
Property 1 , h i +1 ng is the first minimal occurrence of t constraints starting after t h ng
This is because if H  X  is such that l N f , then from Eq. (5), h occurrence of  X  satisfying gap constraints in the window of h no minimal occurrence satisfying T g beyond t h ng gap constraints.  X  To illustrate H ng with an example, consider the following event sequence. {[2 4 7],[5 8 10],[9,11,12]}. The maximal set of non-overlapped occurrences H given in [14] . Finally we obtain the maximal non-overlapped windows count using the Algorithm 2. 4. Discovery under total frequency with span constraints is provided in [8]. This uses the following relation. where  X  s is (  X  [2]  X   X  [3]  X   X   X   X  [ N ]), the ( N  X  1)-node suffix subepisode of subepisodes of  X  which include  X  [1] are at least as frequent as described below.
 episode  X  branches into an ( N + 1)-node episode obtained by appending an event-type to the right end of episodes in a tree where the child of an episode  X  is obtained by appending an event-type to the left end of patterns,  X  branches down into 3 children episodes namely ( A ( N transiting (ET) occurrences [13].

De fi nition 11. [13] An occurrence h of a serial episode  X  t
In the example sequence (1) , the ET occurrences of the episode A width of T X is simply the number of ET occurrences whose span is less than T occurrences of any N -node episode  X  can be computed from the corresponding ET windows list of its ( N (say  X  s ) and its 1-node prefix subepisode (say ( E )). 6 episode  X  ,let ET-list (  X  ) denote the list of ET windows of
ET-list (  X  s ), where ( E ) is the 1-node prefix subepisode of given ET window of the ( E ), (corresponding to, say, occurrence h to, say, occurrence h j  X  occurrence of  X  s after h ( E ) i , the combined occurrence h doing this for all the occurrences of ( E ) (line 11, Algorithm 5), we get the complete list ET-list ( windows of a new episode  X  (lines 2  X  3, Algorithm 7). From this it calculates the needed total frequency (lines 4
Tot-Recursive-fn . Once again this is needed to use Eq. (7). 5. Experiments real neuronal data recorded from dissociated cortical cultures [24]. 2 GB RAM, running a Windows 7 OS. 5.1. Synthetic data generation detail the data generation process.
 an episode event-stream for ( A  X  ( BC )) would, e.g., look like generate a noise stream  X  ( X 1 ,  X  1 ), ( X 2 ,  X  2 ), ...  X  streams in a time-ordered fashion). The data generation process has three important user-specified parameters: parameter), p (inter-occurrence parameter) and  X  (noise parameter), whose roles are explained below. between successive events in an occurrence is generated according to a geometric distribution with parameter episodes, the geometric parameter is  X  (0 b  X   X  1) and for all other event-types this parameter is set to instant. We note here that the value of  X  does not indicate any percentage of noise. For example, with events at every time tick. Thus, even small values of  X  can insert substantial levels of noise in the data. of the level of noise.
 and time-ticks of the generated sequence, n is the number of events in the sequence, threshold on frequency. 5.2. Effectiveness model. Towards this, we generated 9 data sets each with two 7-node serial episodes (say generated by varying the parameters  X  ,  X  and p as given in Table 1 . The span parameter inter occurrence parameter p is varied; and in the Z sets, we vary the noise parameter expected inter-occurrence time reduces and hence the extent of overlap between occurrences of the mining task harder (the data denser) because, spurious patterns consisting of a part of lower than the expected span of the occurrences embedded (as on data set X1 with T and TOT is that the number of frequent patterns repor ted under the total frequency is much higher because f 5.3. Apriori vs pattern-growth We now report run-time comparisons between the existing apriori-based and the proposed pattern-growth approaches.
In the existing apriori-based algorithms, the candidate generation step generates an occurrences and (ii) by steadily increasing the noise level.

Towards the first study, we steadily decrease the parameter ( N  X  1)/  X  for an N -nodeepisode(wedenoteitas E sp ). In the experiments to follow, T embedded occurrences, unless otherwise specified. In the second study, we increase the noise level ( L 5.4. Scaling
We now demonstrate how our pattern-growth algorithms scale with the number of embedded patterns and size of the equal to 75% of the expected frequency, the DFS starts outperforming apriori for a N frequency threshold 60% of the expected frequency, DFS starts outperforming apriori for a N 5.5. Mining multi-neuronal spike train data 5.5.1. Gap constrained mining existing apriori-based algorithm [6].
 connections with large strength. The strength of a connection of, say, A firing after a specific delay ( T syn ) in response to a spike from A . We refer to such a strength parameter as P within a delay time, a gap constraint for an episode is a natural choice.
We generate two data streams by embedding a 7-node pattern at different values of the synaptic delay ( T run-times for higher synaptic delays and gap thresholds. 5.5.2. Real neuronal data multi-electrode array setup [24].
 neurons, for a variety of frequency and expiry-time thresholds.
 T 6. Conclusions windows of a N -node episode  X  given the minimal occurrence windows of its ( N occurrences of the one node episode which is the last event type in example, if we are currently computing the frequency of an episode say ( A exploit any information of having counte d the occurrences of subepisodes like ( A ( A
Through extensive empirical studies, we compared the computational times of apriori-based and pattern-growth based pattern-growth based algorithms.

In this paper we considered maximum gap constraints only under non-overlapped frequency. Extending our depth-first algorithms to take care of interval-based gap constraints is another interesting open problem.
References
