 We perform an exp erimen tal comparison of the graph-based multi-relational data mining system, Sub due, and the induc-tive logic programming system, CProgol, on the Mutagene-sis dataset and various arti cially generated Bongard prob-lems. Exp erimen tal results indicate that Sub due can signif-ican tly outp erform CProgol while disco vering structurally large multi-relational concepts. It is also observ ed that CProgol is better at learning seman tically complicated con-cepts and it tends to use bac kground kno wledge more e ec-tively than Sub due. An analysis of the results indicates that the di erences in the performance of the systems are a result of the di erence in the expressiv eness of the logic-based and the graph-based represen tations. The abilit y of graph-based systems to learn structurally large concepts comes from the use of a weak er represen tation whose expressiv eness is inter-mediate between prop ositional and rst-order logic. The use of this weak er represen tation is adv antageous while learn-ing structurally large concepts but it limits the learning of seman tically complicated concepts and the utilization bac k-ground kno wledge. Multi-relational data mining (MRDM)[7] is a sub eld of data mining whic h focuses on kno wledge disco very from rela-tional databases comprising multiple tables. Represen tation is a fundamen tal as well as a critical asp ect in the pro cess of disco very and two forms of represen tation, namely the graph-based represen tation and the logic-based represen ta-tion, have been used for MRDM.
 Logic-based MRDM popularly kno wn as Inductiv e Logic Programming (ILP)[16], is the intersection of Mac hine Learning and Logic Programming. ILP is character-ized by the use of logic for the represen tation of multi-relational data. ILP systems represen t examples, bac k-ground kno wledge, hypotheses and target concepts in Horn clause logic. ILP systems suc h as FOIL [20], CProgol[17], Golem[18], SMAR T+[4], G-Net[1], CHILLIN[26], TILDE[2] and WARMR[6] have been extensiv ely applied to sup ervised learning and to a certain exten t to unsup ervised learning. The core of ILP is the use of logic for represen tation and the searc h for syn tactically legal hypotheses constructed from predicates pro vided by the bac kground kno wledge. The ILP pro cess is basically a searc h wherein the states are hypothe-ses and the goal is the hypothesis that is frequen t or whic h distinguishes positiv e and negativ e examples.
 An ILP system can be characterized by the way the hypoth-esis space is structured and the searc h strategy used to ex-plore the hypothesis space. ILP systems may be classi ed on the basis of four key factors. ILP systems may learn a single concept or multiple concepts. ILP systems may be batc h or incremen tal dep ending on how they accept examples. ILP systems may be interactiv e or non-in teractiv e dep ending on whether they use human advice in the pro cess of learning. Lastly , ILP systems may revise a theory or may learn con-cepts from scratc h, the former being kno wn as theory revi-sion systems. Although the factors are indep enden t, most ILP systems are either non-in teractiv e, single concept batc h learners that build concepts from scratc h or are incremen-tal, interactiv e theory revisers that learn multiple concepts. The former are kno wn as empirical ILP systems while the later are called incremen tal ILP systems.
 Graph-based approac hes are characterized by represen tation of multi-relational data in the form of graphs. Graph-based MRDM systems have been extensiv ely applied to the task of unsup ervised learning, popularly kno wn as frequen t sub-graph mining and to a certain exten t to sup ervised learning. Graph-based approac hes represen t examples, bac kground kno wledge, hypotheses and target concepts as graphs. These approac hes include mathematical graph theory based ap-proac hes like FSG[14] and gSpan[24], greedy searc h based approac hes like Sub due [5] or GBI[15], and kernel function based approac hes[12]. The core of all these approac hes is the use of a graph-based represen tation and the searc h for graph patterns whic h are frequen t or whic h compress the input graphs or whic h distinguish positiv e and negativ e ex-amples.
 Mathematical graph theory based approac hes mine a com-plete set of subgraphs mainly using a supp ort or frequency measure. The initial work in this area was the AGM[11] system whic h uses the Apriori level-wise approac h. FSG tak es a similar approac h and further optimizes the algo-rithm for impro ved running times. gFSG [13] is a vari-ant of FSG whic h enumerates all geometric subgraphs from the database. gSpan uses DFS codes for canonical lab el-ing and is much more memory and computationally ecien t than previous approac hes. Instead of mining all subgraphs, CloseGraph[25] only mines closed subgraphs. A graph G is closed in a dataset if there exists no sup ergraph of G that has the same supp ort as G. Gaston[19] ecien tly mines graph datasets by considering frequen t paths. These fre-quen t paths are rst transformed to trees and these trees are then transformed to graphs. FFSM[9] is a graph mining system whic h uses an algebraic graph framew ork to address the underlying problem of subgraph isomorphism.
 In comparison to mathematical graph theory based ap-proac hes whic h are complete, greedy searc h based ap-proac hes use heuristics to evaluate the solution. The two pioneering works in the eld are Sub due and GBI. Sub due uses MDL-based compression heuristics, and GBI uses an empirical graph size-based heuristic. The empirical graph size de nition dep ends on the size of the extracted patterns and the size of the compressed graph.
 Lastly , the kernel function based approac hes have been used to a certain exten t for mining graph datasets. The kernel function de nes a similarit y between two graphs. When high dimensional data is represen ted in linear space, the function to learn is dicult in that space. We can map the linear data to nonlinear space and the problem of learning in that high dimensional space becomes the learning of scalar pro ducts. Kernel functions mak e computation of suc h scalar pro ducts very ecien t. The key to applying the kernel function based approac h to mining graph data is nding ecien t mapping functions and good feature vectors. The pioneering work that applied kernel functions to graph structures is the dif-fusion kernel [12].
 We perform an exp erimen tal comparison of graph-based and logic-based MRDM. We iden tify three key factors for com-paring graph-based and logic-based multi-relational data mining; namely , the abilit y to disco ver structurally large concepts, the abilit y to disco ver seman tically complicated concepts and the abilit y to e ectiv ely utilize bac kground kno wledge. CProgol is selected as a represen tativ e of logic-based approac hes and Sub due is selected as a represen ta-tive of graph-based approac hes. Exp erimen ts are performed on the Mutagenesis dataset whic h is a benc hmark dataset for MRDM. In most of the exp erimen ts, transformations are applied to the Mutagenesis dataset or distinct types of bac kground kno wledge are pro vided to Sub due and CProgol. The rationale behind doing so is to perform lesion studies and gain insigh t on the speci c abilities of the approac hes. Additional exp erimen ts are performed on arti cially gener-ated Bongard problems to reinforce the ndings from the exp erimen ts on the Mutagenesis dataset. We analyze the exp erimen tal results and presen t the insigh ts and future di-rections dra wn from them.
 Our analysis of the exp erimen tal results indicates that the di erences in the performance of the systems are a result of the di erence in the expressiv eness of the logic-based and the graph-based represen tations. The insigh ts from this compar-ison are pertinen t to various tasks involving learning from multi-relational data like inductiv e databases[10] and link mining[8].
 The rest of the pap er is organized as follo ws. In Section 2 we describ e the exp erimen tal setup, comprising of the logic-based MRDM system CProgol, the graph-based MRDM system Sub due, the Mutagenesis dataset and the Bongard problems used for arti cial exp erimen ts. In Section 3, we iden tify the factors for comparing graph-based and logic-based MRDM. Section 4 describ es the exp erimen ts with the Mutagenesis dataset. Sections 5 describ es the arti cial do-main exp erimen ts with Bongard problems. In Section 6, we analyze the exp erimen tal results. Conclusions and future work are presen ted in Section 7. In this section we describ e the exp erimen tal setup, compris-ing of the logic-based MRDM system CProgol, the graph-based MRDM system Sub due, the Mutagenesis dataset and the Bongard problems used for arti cial exp erimen ts. CProgol[17] is an ILP system, characterized by the use of mo de-directed inverse entailmen t and a hybrid searc h mec h-anism. Inverse entailmen t is a pro cedure whic h generates a single, most speci c clause that, together with the bac k-ground kno wledge, entails the observ ed data. The inverse entailmen t in CProgol is mo de-directed that is, it uses mo de de nitions. A mo de declaration is a constrain t whic h im-poses restrictions on the atoms and their argumen ts app ear-ing in a hypothesis clause by, 1. Determining whic h atoms can occur in the head and 2. Determining whic h argumen ts can be input variables, 3. Determining the num ber of alternativ e solutions for The user-de ned mo de declarations aid the generation of the most speci c clause. CProgol rst computes the most spe-ci c clause whic h covers the seed example and belongs to the hypothesis language. The most speci c clause can be used to bound the searc h from below. The searc h is now bounded between the empt y clause and the most speci c clause. The searc h pro ceeds within the bounded -subsumption lattice in a general-to-sp eci c manner. The searc h is a hybrid searc h, because it is a general-to-sp eci c searc h bounded from be-low with resp ect to the most speci c clause. The searc h strategy is an A* algorithm whic h is guided by a weigh ted compression and accuracy measure. The A* searc h returns a clause whic h covers the most positiv e examples and max-imally compresses the data.
 Any arbitrary Prolog program can serv e as bac kground kno wledge for CProgol. The mo de de nitions and the bac k-ground kno wledge together de ne a hypothesis language. The hypothesis space explored by CProgol consists of ev-ery hypothesis de ned by the hypothesis language. Sub due[5] is a graph-based MRDM system capable of un-sup ervised and sup ervised learning. When operating as a sup ervised learner, Sub due accepts a set of example graphs lab eled as positiv e or negativ e and nds subgraphs distin-guishing the positiv e graphs from the negativ e graphs. The hypothesis space of Sub due consists of all the connected sub-graphs of all the example graphs lab eled positiv e. Sub due performs a beam searc h whic h begins from substruc-tures consisting of all vertices with unique lab els. The sub-structures are extended by one vertex and one edge or one edge in all possible ways, as guided by the input graph, to generate candidate substructures. Sub due main tains the instances of substructures (in order to avoid subgraph iso-morphism) and uses graph isomorphism to determine the instances of the candidate substructure in the input graph. Candidate substructures are evaluated according to classi -cation accuracy or the minim um description length principle Figure 1: Bongard Problems. (a) Example of a Bongard Problem. (b) Concept to be learned in the Bongard prob-lem. (c) Instances of the concept in the Bongard problem. [22]. The length of the searc h beam determines the num ber of candidate substructures retained for further expansion. This pro cedure rep eats until all substructures are considered or the user imp osed computational constrain ts are exceeded. At the end of this pro cedure the positiv e examples covered by the best substructure are remo ved. The pro cess of nd-ing substructures and remo ving positiv e examples con tinues until all the positiv e examples are covered.
 Sub due can be pro vided with prede ned substructures as bac kground kno wledge. Sub due uses this bac kground kno wledge by prepro cessing the examples and compressing eac h of the user de ned substructures whic h form the bac k-ground kno wledge into a single vertex. The Mutagenesis dataset[23] has been collected to iden tify mutagenic activit y in a comp ound based on its molecular structure and is considered to be a benc hmark dataset for MRDM. The Mutagenesis dataset consists of the molecu-lar structure of 230 comp ounds, of whic h 138 are lab eled as mutagenic and 92 as non-m utagenic. The mutagenicit y of the comp ounds has been determined by the Ames Test. The task is to distinguish mutagenic comp ounds from non-mutagenic ones based on their molecular structure. The Mu-tagenesis dataset basically consists of atoms, bonds, atom types, bond types and partial charges on atoms. The dataset also consists of the hydrophobicit y of the comp ound (logP), the energy level of the comp ound's lowest uno ccupied molec-ular orbital (LUMO), a boolean attribute iden tifying com-pounds with 3 or more benzyl rings (I1), and a boolean attribute iden tifying comp ounds whic h are acen thryles (Ia). Ia, I1, logP and LUMO are relev ant prop erties in determin-ing mutagenicit y. Bongard problems[3] were introduced as an arti cial domain in the eld of pattern recognition. A simpli ed form of Bon-gard problems has been used as an arti cial domain in the eld of ILP[21]. We use a similar form of Bongard problems for our arti cial domain exp erimen ts. We use a Bongard problem generator to generate datasets. Eac h dataset con-sists of a set of positiv e and negativ e examples. Eac h exam-ple consists of a num ber of simple geometrical objects placed inside one another. The task is to determine the particular set of objects, their shap es and their placemen t whic h can correctly distinguish the positiv e examples from the nega-tive ones. Figure 1(a) sho ws a Bongard problem, while (b) and (c) sho w the concept to be learned and their instances in the positiv e examples resp ectiv ely. By performing a comparison of the graph-based and logic-based approac hes to MRDM, we intended to analyze the abilit y of the approac hes to ecien tly disco ver complex multi-relational concepts and to e ectiv ely utilize bac k-ground kno wledge. For doing so it is essen tial to estab-lish some intuitiv e notions on the complexit y of a multi-relational concept and to iden tify the types of bac kground kno wledge generally available in the task of MRDM. The complexit y of a multi-relational concept is a direct con-sequence of the num ber of relations in the concept. A multi-relational concept is more complicated to learn than some other multi-relational concept if learning that concept in-volves learning more relations than the other concept. For example learning the concept of arene (six mem ber ring as in benzene) whic h comprises learning six relations, involves the exploration of a larger hypothesis space than learning the concept of hydro xyl (oxygen connected to hydrogen as in methanol), whic h comprises learning one relation. The con-cept of arene is thus more complicated than that of hydro xyl. Although the num ber of relations in the multi-relational con-cept is a key factor in the complexit y of the multi-relational concept, there are also other factors suc h as the num ber of relations in the examples from whic h the concept is to be learned. For example, learning the concept of hydro xyl from a set of large molecules (e.g., phenols, etc.) involves the exploration of a larger hypothesis space than learning the same hydro xyl concept from a set of small molecules (e.g., methanol, etc.). The concept of hydro xyl group is thus more complicated to learn from phenols than it is from a set of alcohols. We iden tify this complexit y as structur al complexity .
 In order to learn a particular concept, it is essen tial that the represen tation used by a multi-relational data mining system is able to express that particular concept. For a rep-resen tation to express a particular concept, it is bene cial to have both the syn tax whic h expresses the concept and the seman tics whic h asso ciates meaning to the syn tax. The concepts whic h cannot be represen ted by the represen tation used by the MRDM system can be explicitly instan tiated in the examples . A relational concept can be said to have a higher complexit y than some other relational concept if represen ting that concept requires a more expressiv e rep-resen tation. For example to learn numerical ranges, it is essen tial to have the syn tax and the seman tics for repre-sen ting notions like `lesser than', `greater than' and `equal to'. We iden tify this complexit y as semantic complexity . A relational learner can be pro vided bac kground kno wledge whic h condenses the hypothesis space. For example if the concept to be learned is `comp ounds with three arene rings' (six mem ber ring as in benzene) and the concept of an arene ring is pro vided as a part of the bac kground kno wledge, then the arene rings in examples could be condensed to a single entity. This would cause a massiv e reduction in the hy-pothesis space required to be explored to learn the concept and the relational learner would perform more ecien tly than without the bac kground kno wledge. We iden tify suc h bac kground kno wledge as backgr ound know ledge intende d to condense the hyp othesis space .
 A relational learner can be pro vided bac kground kno wledge whic h augmen ts the hypothesis space. For example con-sider that the relational learner is pro vided with bac kground kno wledge whic h allo ws it to learn concepts like `lesser than',`greater than' and `equal to'. In this case, the rela-tional learner would explore a hypothesis space larger than what it would explore without the bac kground kno wledge. Thus introducing bac kground kno wledge has augmen ted the hypothesis space and has facilitated the learning of concepts whic h would not be learned without the bac kground kno wl-edge. We iden tify suc h bac kground kno wledge as backgr ound know ledge intende d to augment the hyp othesis space . Using these notions, we can iden tify three key factors for comparing graph-based and logic-based multi-relational data mining, 1. The abilit y to disco ver structurally large concepts. 2. The abilit y to disco ver seman tically complicated con-3. The abilit y to e ectiv ely utilize bac kground kno wledge In this section we presen t our exp erimen ts on the Mutagen-esis Dataset. We compared the performance of Sub due and CProgol on four di eren t represen tations of the Mutagen-esis Dataset. Eac h represen tation was selected in order to analyze a speci c abilit y of the systems.
 In rst case, we compare the abilit y of the approac hes to learn large structural concepts. Both the relational learners were pro vided only with the basic information of the atoms, the elemen ts and the bonds without any other information or bac kground kno wledge. The relational learners are not pro vided with any additional information or any form of bac kground kno wledge, because we intended to compare the abilit y to learn large structural concepts. The partial charge information was not pro vided to either system because this information would con tribute to the accuracy and mak e it dicult to analyze how the approac hes compare while learn-ing structurally large concepts. The atom type and bond type information was also not pro vided to either system. The reasoning behind doing so is that we view the atom type and bond type information as a prop ositional repre-sen tation of relational data. Suc h information allo ws the relational learners to learn prop ositional represen tations of relational concepts instead of the true relational concept. Consider for example the rule found by CProgol on the Mu-tagenesis dataset[23],atom(A,B,c,195,C). This rule denotes that comp ounds with a carb on atom of type 195 are muta-genic. The atom type 195 occurs as the atom shared by 3 fused rings 6 mem ber rings. Therefore all comp ounds with 3 fused 6 mem ber rings are lab eled activ e. It is interest-ing to note that a rule involving 15 relations (3 fused 6 mem ber rings) has been learned by learning a single rela-tion. Learning suc h a rule has allo wed CProgol to learn a prop ositional represen tation of a relational concept rather that the true relational concept. Pro viding atom type and bond type information would allo w both systems to learn prop ositional represen tations of structurally large relational concepts rather than the true relational concepts. We do not consider the learning of suc h concepts equiv alen t to the learning of structurally large relational concepts. We there-fore do not pro vide either system with the atom type and bond type information. This is depicted in Figure 2(a) and (e).
 In the second case we compare the performance of the sys-tems while learning seman tically complicated concepts, we ran Sub due and CProgol on the Mutagenesis dataset. Eac h system was pro vided with bac kground kno wledge so that numerical ranges could be learned. For CProgol this was achiev ed by introducing Prolog based bac kground kno wl-edge. For Sub due this was achiev ed by explicitly instan tiat-ing the bac kground kno wledge, i.e., additional structure was added to the training examples. This is depicted in Figure 2(b) and (f).
 In the third case we pro vide eac h system with the bac k-ground kno wledge indicating the presence of benzyl rings (I1) and iden tifying comp ounds whic h are acen thryles (Ia). This is depicted in Figure 2(c) and (g).
 In the fourth case, eac h system was pro vided with the bac k-ground kno wledge indicating certain generic chemical con-cepts like benzene rings, nitro groups, etc. This is depicted in Figure 2(d) and (h).
 The results of these exp erimen ts are sho wn in Table 1. For the training set, the accuracy for one run on the en-tire dataset and the learning time are sho wn. For 10-fold cross validation (CV), average learning time over 10 folds is sho wn.
 From these exp erimen ts we can observ e the follo wing. 1. Sub due performs signi can tly better than CProgol in 2. CProgol outp erformed Sub due in the second case, i.e. 3. In the third and the fourth cases i.e. while utilizing It must be noted that in the second case i.e while learning seman tically complicated concepts, Sub due has achiev ed an accuracy lower than what was achiev ed without the bac k-ground kno wledge to pro cess ranges. Man ual insp ection of the concepts learned by Sub due and CProgol (not sho wn here) indicate that Sub due did not learn any concept involv-ing ranges as learned by CProgol. We performed additional exp erimen ts (not rep orted here) with di eren t graph-based represen tations for learning ranges. In all these exp erimen ts, Sub due had similar performance. We performed additional exp erimen ts using arti cially gen-erated Bongard problems to reinforce the insigh ts from the exp erimen ts on the Mutagenesis dataset. We now discuss the results of these exp erimen ts.
 We systematically analyzed the performance of Sub due and CProgol on arti cially generated Bongard problems with in-creasing num bers of objects in the concept and increasing num bers of objects in the examples in order to compare the abilities of the systems to learn structurally large con-cepts. Figure 3(a) sho ws a Bongard example, while (b) and (c) sho w the graph-based and logic-based represen tations of the example, resp ectiv ely. In this exp erimen t, the num ber of objects in the Bongard concept was varied from 5 to 35. The num ber of additional objects in eac h example (ob jects whic h are not a part of the concept) were kept constan t at 5. For every concept size from 5 to 35, 10 di eren t concepts were generated. For eac h of the 10 concepts a training set and a test set of 100 positiv e and 100 negativ e examples was generated.
 Figure 4(a) sho ws the average accuracy achiev ed by CPro-gol and Sub due on 10 datasets for every concept size ranging from 5 to 35. In order to further analyze the performance of the systems we reran the same exp erimen t but in this case the systems were iterativ ely given increased resources (this was achiev ed by varying the nodes parameter in CProgol and the limit parameter in Sub due) so that we could determine the num ber of hypotheses eac h system explored before it learned the concept (a cuto accuracy of 80% was decided). Figure 4(b) sho ws the num ber of hypotheses explored by eac h system so as to achiev e an accuracy of 80% (this ex-perimen t was only performed for concept size varying from 5 to 18 as a signi can tly large amoun t of time was required). A snapshot of the exp erimen t (Accuracy vs. Num ber of Ex-plored Hyp otheses) for concept size 10 is sho wn in Figure 4(c). A similar exp erimen t for increased example size was performed where the concept size was kept constan t at 5 and the example size was varied from 10 to 35. Figure 4(d) Figure 3: Represen tation for Bongard Problems. (a) A Bon-gard example. (b) Graph-based represen tation of the Bon-gard problem. (c) Logic-based represen tation of the Bon-gard problem. (d) Graph-based represen tation for the in-troduction of bac kground kno wledge in Bongard problems. (e) Logic-based represen tation for the introduction of bac k-ground kno wledge in Bongard problems. sho ws the average accuracy achiev ed by CProgol and Sub-due on 10 datasets for every example size ranging from 10 to 35. Figure 4(e) sho ws the hypotheses required to be ex-plored to learn the concept (a cuto accuracy of 80% was decided) determined by iterativ ely increasing the resources for eac h system. A snapshot of the exp erimen t (Accuracy vs. Num ber of Explored Hyp otheses) for example size 15 is sho wn in Figure 4(f).
 Exp erimen tal results indicate that, 1. Sub due achiev es increasingly higher accuracy than 2. CProgol has to explore an increasingly larger num ber 3. Sub due achiev es increasing higher accuracy than 4. Sub due achiev es increasingly higher accuracy than 5. CProgol has to explore an increasingly larger num-6. Sub due achiev es increasing higher accuracy than Exp erimen ts were also performed to analyze the abilit y to utilize bac kground kno wledge. Figure 3(d) and (e) sho w the represen tations used for CProgol and Sub due resp ectiv ely. We systematically analyzed the performance of Sub due and CProgol on arti cially generated Bongard problems with in-creasing amoun ts of bac kground kno wledge while learning a large concept (more objects in the concept) and with in-creasing amoun ts of bac kground kno wledge while learning a concept from a large example (more objects in eac h exam-ple). In this exp erimen t, for a concept of size 10 and addi-tional objects in eac h example equal to 5, 10 concepts were generated. For eac h of these concepts a training set and test set of 100 positiv e and 100 negativ e examples were gener-ated. Figure 4(g) sho ws the accuracies achiev ed by Sub due and CProgol (Note that both the systems were given less re-sources than the exp erimen ts whic h analyzed the abilit y to learn structurally large concepts so that the e ect of bac k-ground kno wledge could be analyzed). Figure 4(h) sho ws the hypotheses required by eac h system to learn the con-cept (a cuto accuracy of 80% was decided). A similar exp erimen t for a concept of size 5 and example size of 15 was performed. Figure 4(i) sho ws the accuracies achiev ed by Sub due and CProgol (Note that both the systems were given less resources than the exp erimen ts whic h analyzed the abilit y to learn structurally large concepts so that the e ect of bac kground kno wledge could be analyzed). Figure 4(j) sho ws the hypotheses required by eac h system to learn the concept (a cuto accuracy of 80% was decided). Exp erimen tal results indicate that when increasing amoun ts of bac kground kno wledge are introduced, 1. CProgol achiev es increasingly higher accuracy than 2. Sub due has to explore an increasingly larger num ber of 3. CProgol achiev es increasingly higher accuracy than 4. Sub due has to explore an increasingly larger num ber of The results of the exp erimen ts performed in Sections 4 and 5 indicate that Sub due signi can tly outp erforms CProgol while learning structurally large concepts. It is also observ ed that CProgol performs better than Sub due while learning seman tically complicated concepts and utilizing bac kground kno wledge. Here we rst attempt to explain the empirical results based on the represen tational and algorithmic di er-ences between the two systems and then analyze whether these ndings are applicable, in general to all graph-based and logic-based approac hes.
 An analysis of the represen tations used by Sub due and CProgol indicates that the graph-based represen tation used by Sub due has much less expressiv eness than that of CPro-gol whic h can accept any Prolog program as bac kground kno wledge. The expressiv eness of the graph-based repre-sen tation used by Sub due is intermediate between prop o-sitional and rst-order logic. This di erence can explain the di erences between the performances of the two sys-tems. Learning structurally large concepts involves learning bac kground kno wledge, for large example size. a large num ber of relations explicitly presen t in the exam-ples. In this case, the less expressiv e represen tation used by Sub due leads to an ecien t exploration of the hypothesis space. An example of this is the massiv e num ber of redun-dan t hypothesis whic h are not generated and evaluated by Sub due.
 Learning seman tically complicated concepts involves learn-ing relations whic h are not explicitly presen t in the examples but whic h must be implicitly deriv ed from the examples. In this case CProgol whic h uses a more expressiv e represen ta-tion not only performs more ecien tly but also can learn concepts whic h may not be expressed by Sub due's mec ha-nism of explicit instan tiation. An example of this is Sub-due's explicit instan tiation to learn ranges in Figure 2(b). Explicit instan tiation is cum bersome in most cases and also not a generalized metho dology to learn complicated seman-tic concepts. For example, supp ose a domain exp ert were to suggest that the ratio of the num ber of carb on atoms to the num ber of hydrogen atoms in a molecule has an e ect on the mutagenicit y. CProgol with some added bac kground kno wl-edge could use this information to classify the molecules. Sub due on the other hand would require making changes to the represen tation suc h that the pattern would be found in terms of a graph. CProgol allo ws the exploration of hy-potheses through implicitly de ned bac kground kno wledge rather than explicit instan tiation in the examples. The di erence between the expressiv eness of the represen-tations can also explain the di erence in the abilities of the systems to utilize bac kground kno wledge. In Sub due, bac k-ground kno wledge is introduced as a vertex whic h is con-nected to all the entities whic h comprise the bac kground kno wledge for example,in the Mutagenesis dataset as in Fig-ure 2(d) and Bongard problems as in Figure 3(d). In the case of CProgol, similar bac kground kno wledge is introduced in the form of a predicate, as in Figure 2(h). Sub due's graph-based represen tation does not allo w any speci c mec hanism to express that a set of entities (ob jects and atoms in the case of Bongard problems and Mutagenesis resp ectiv ely) are kno wn to be a part of a group whic h is relev ant in classify-ing the examples. Suc h a group of entities is to be added in a single re nemen t step to generate candidate hypothesis, and bene t from the bac kground kno wledge. Sub due gen-erates candidate hypotheses by extending the sub-graph by an edge and a vertex or just an edge in all possible ways as in the examples. Sub due's candidate hypothesis genera-tion adds the entities kno wn to be a part of the bac kground kno wledge one at a time and this leads to a massiv e increase in the hypothesis space. This may cause the introduction of bac kground kno wledge to deteriorate performance in some cases.
 Sub due does pro vide an alternate way of introducing bac k-ground kno wledge by prepro cessing the examples and com-pressing eac h of the user de ned substructures whic h form the bac kground kno wledge into a single vertex. This tech-nique has the dra wbac k of information loss and Sub due will not be able to learn a concepts that con tain only a partial portion of the bac kground kno wledge substructure. The di erences between the performance of the two systems can thus be explained by the di erence in the expressiv e-ness of the represen tations used by the two systems. Any graph-based and logic-based system will tend to beha ve sim-ilarly due to this underlying di erence in expressiv eness. The use of a less expressiv e represen tation will facilitate an ecien t searc h and lead to a sup erior performance while learning structurally large concepts. The use of a weak er represen tation would also limit the learning of seman tically complicated concepts and the e ectiv e use of bac kground kno wledge. It is imp ortan t to note that these characteristics are the result of the di erence in the expressiv eness of the two represen tations and are not inheren t to graph-based and logic-based represen tations in general. It would be possible to introduce the syn tax and seman tics in graph-based repre-sen tations to express seman tically complicated concepts us-ing ordered graphs, hyper-graphs or graph rewriting rules. In this case, a graph-based system would tend to have a performance similar to a logic-based system. We performed an exp erimen tal comparison of the graph-based multi-relational data mining system, Sub due, and the inductiv e logic programming system, CProgol. From this comparison we conclude that the use of a less expressiv e represen tation, like the presen t graph-based represen tation can achiev e sup erior performance while learning structurally large multi-relational concepts. The use of a less expressiv e represen tation limits the learning of seman tically compli-cated multi-relational concepts and the utilization of bac k-ground kno wledge.
 Dev eloping metho dologies for using the less expressiv e graph-based systems and more expressiv e logic-based sys-tems in com bination may achiev e the learning of struc-turally large concepts and seman tically complicated con-cepts. It may also allo w better utilization of bac kground kno wledge. For example, the structurally complicated hy-potheses learned by a graph-based system could then be used as bac kground kno wledge by a logic-based system. We plan to pursue this as part of our future work. This researc h is sponsored by the Air Force Researc h Lab o-ratory (AFRL) under con tract F30602-01-2-0570. The views and conclusions con tained in this documen t are those of the authors and should not be interpreted as necessarily repre-sen ting the ocial policies, either expressed or implied, of AFRL or the United States Governmen t. [1] C. Anglano, A. Giordana, G. L. Bello, and L. Saitta. An [2] H. Blo ckeel and L. D. Raedt. Top-do wn induction of [3] M. Bongard. Pattern Recognition . Spartan Books, 1970. [4] M. Botta and A. Giordana. Smart+: A multi-strategy [5] D. J. Cook and L. B. Holder. Substructure disco very us-[6] L. Dehasp e and H. Toivonen. Disco very of frequen t dat-[7] S. Dzeroski. Multi-relational data mining: an introduc-[8] L. Geto or. Link mining: a new data mining challenge. [9] J. Huan, W. Wang, and J. Prins. Ecien t mining of [10] T. Imielinski and H. Mannila. A database persp ectiv e [11] A. Inokuc hi, T. Washio, and H. Moto da. An apriori-[12] R. I. Kondor and J. D. La ert y. Di usion kernels on [13] M. Kuramo chi and G. Karypis. Disco vering frequen t [14] M. Kuramo chi and G. Karypis. An ecien t algo-[15] T. Matsuda, T. Horiuc hi, H. Moto da, and T. Washio. [16] S. Muggleton. Inductiv e logic programming. New Gen-[17] S. Muggleton. Inverse entailmen t and progol. New Gen-[18] S. Muggleton and C. Feng. Ecien t induction of logic [19] S. Nijssen and J. N. Kok. A quic kstart in frequen t struc-[20] J. R. Quinlan. Learning logical de nitions from rela-[21] L. D. Raedt and W. V. Laer. Inductiv e constrain t logic. [22] J. Rissanen. Sochastic Complexity in Statistic al Inquiry . [23] A. Sriniv asan, S. Muggleton, M. J. E. Stern berg, and [24] X. Yan and J. Han. gspan: Graph-based substructure [25] X. Yan and J. Han. Closegraph: mining closed frequen t [26] J. M. Zelle, R. J. Mo oney , and J. B. Kon visser. Com-
