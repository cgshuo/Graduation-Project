 A computer graphics artist sits down to use a simple renderer to find appropriate surfaces for a typical reflectance model. It has a series of parameters that must be set to control the simulation:  X  X pecularity X ,  X  X resnel reflectance coefficient X , and other, less-comprehensible ones. The parame-ters interact in ways difficult to discern. The artist knows in his mind X  X  eye what he wants, but he X  X  not a mathematician or a physicist  X  no course he took during his MFA covered Fresnel reflectance models. Even if it had, would it help? He moves the specularity slider and waits for the image to be generated. The surface is too shiny. He moves the slider back a bit and runs the simulation again. Better. The surface is now appropriately dull, but too dark. He moves a slider down. Now it X  X  the right colour, but the specularity doesn X  X  look quite right any more. He repeatedly bumps the specularity back up, rerunning the renderer at each attempt until it looks right. Good. Now, how to make it look metallic...? Problems in simulation, animation, rendering and other areas often take such a form, where the desired end result is identifiable by the user, but parameters must be tuned in a tedious trial-and-error process. This is particularly apparent in psychoperceptual models, where continual tuning is required to make something  X  X ook right X . Using the animation of character walking motion as an example, for decades, animators and scientists have tried to develop objective functions based on kinematics, dynamics and motion capture data [ Cooper et al. , 2007 ] . However, even when expen-sive mocap is available, we simply have to watch an animated film to be convinced of how far we still are from solving the gait animation problem. Unfortunately, it is not at all easy to find a mapping from parameterized animation to psychoperceptual plausibility. The perceptual objective function is simply unknown. Fortunately, however, it is fairly easy to judge the quality of a walk  X  in fact, it is trivial and almost instantaneous. The application of this principle to animation and other psychoper-ceptual tools is motivated by the observation that humans often seem to be forming a mental model of the objective function. This model enables them to exploit feasible regions of the parameter space where the valuation is predicted to be high and to explore regions of high uncertainty. It is our the-Figure 1: An illustrative example of the difference between models learned for regression vesus optimization. sis that the process of tweaking parameters to find a result that looks  X  X ight X  is akin to sampling a perceptual objective function, and that twiddling the parameters to find the best result is, in essence, optimization. Our objective function is the psycho-perceptual process underlying judgement  X  how well a realization fits what the user has in mind. Following the econometrics terminology, we refer to the objective as the valuation . In the case of a human being rating the suitability of a simulation, however, it is not possible to evaluate this function over the entire domain. In fact, it is in gen-eral impossible to even sample the function directly and get a consistent response! While it would theoretically be possible to ask the user to rate realizations with some numerical scale, such meth-ods often have problems with validity and reliability. Patterns of use and other factors can result in a drift effect , where the scale varies over time [ Siegel and Castellan, 1988 ] . However, human beings do excel at comparing options and expressing a preference for one over others [ Kingsley, 2006 ] . This insight allows us to approach the optimization function in another way. By presenting two or more realizations to a user and requiring only that they indicate preference, we can get far more robust results with much less cognitive burden on the user [ Kendall, 1975 ] . While this means we can X  X  get responses for a valuation function directly, we model the valuation as a latent func-tion, inferred from the preferences, which permits an active learning approach [ Cohn et al. , 1996; Tong and Koller, 2000 ] .
 This motivates our second major insight  X  it is not necessary to accurately model the entire ob-jective function . The problem is actually one of optimization, not regression (Figure 1). We can X  X  directly maximize the valuation function, so we propose to use an expected improvement function (EIF) [ Jones et al. , 1998; Sasena, 2002 ] . The EIF produces an estimate of the utility of knowing the valuation at any point in the space. The result is a principled way of trading off exploration (showing the user examples unlike any they have seen) and exploitation (trying to show the user improvements on examples they have indicated preference for). Of course, regression-based learning can produce an accurate model of the entire valuation function, which would also allow us to find the best valua-tion. However, this comes at the cost of asking the user to compare many, many examples that have no practical relation what she is looking for, as we demonstrate experimentally in Sections 3 and 4. Our method tries instead to make the most efficient possible use of the user X  X  time and cognitive effort.
 Our goal is to exploit the strengths of human psychology and perception to develop a novel frame-work of valuation optimization that uses active preference learning to find the point in a parameter space that approximately maximizes valuation with the least effort to the human user. Our goal is to offload the cognitive burden of estimating and exploring different sets of parameters, though we can incorporate  X  X lider twiddling X  into the framework easily. In Section 4, we present a simple, but practical application of our model in a material design gallery that allows artists to find particular appearance rendering effects. Furthermore, the valuation function can be any psychoperceptual pro-cess that lends itself to sliders and preferences: the model can support an animator looking for a particular  X  X artoon physics X  effect, an artist trying to capture a particular mood in the lighting of a scene, or an electronic musician looking for a specific sound or rhythm. Though we use animation and rendering as motivating domains, our work has a broad scope of application in music and other arts, as well as psychology, marketing and econometrics, and human-computer interfaces. 1.1 Previous Work Probability models for learning from discrete choices have a long history in psychology and econo-metrics [ Thurstone, 1927; Mosteller, 1951; Stern, 1990; McFadden, 2001 ] . They have been studied World Chess Federation FIDE to model the probability of one player defeating another. Glickman and Jensen [ 2005 ] use Bayesian optimal design for adaptively finding pairs for tournaments. These methods all differ from our work in that they are intended to predict the probability of a prefer-ence outcome over a finite set of possible pairs, whereas we work with infinite sets and are only incidentally interested in modelling outcomes.
 In Section 4, we introduce a novel  X  X reference gallery X  application for designing simulated materials in graphics and animation to demonstrate the practical utility of our model. In the computer graphics field, the Design Gallery [ Marks et al. , 1997 ] for animation and the gallery navigation interface for Bidirectional Reflectance Distribution Functions (BRDFs) [ Ngan et al. , 2006 ] are artist-assistance tools most like ours. They both uses non-adaptive heuristics to find the set of input parameters to be used in the generation of the display. We depart from this heuristic treatment and instead present a principled probabilistic decision making approach to model the design process.
 Parts of our method are based on [ Chu and Ghahramani, 2005b ] , which presents a prefer-ence learning method using probit models and Gaussian processes. They use a Thurstone-Mosteller model, but with an innovative nonparametric model of the valuation function. [ Chu and Ghahramani, 2005a ] adds active learning to the model, though the method presented there differs from ours in that realizations are selected from a finite pool to maximize informative-ness. More importantly, though, this work, like much other work in the field [ Seo et al. , 2000; Guestrin et al. , 2005 ] , is concerned with learning the entire latent function. As our experiments show in Section 3, this is too expensive an approach for our setting, leading us to develop the new active learning criteria presented here. By querying the user with a paired comparison, one can estimate statistics of the valuation function at the query point, but only at considerable expense. Thus, we wish to make sure that the samples we do draw will generate the maximum possible improvement.
 Our method for achieving this goal iterates the following steps: 2.1 Preference Learning Model Assume we have shown the user M pairs of items. In each case, the user has chosen which item she likes best. The dataset therefore consists of the ranked pairs D = { r k c k ; k = 1 ,...,M } , where the symbol indicates that the user prefers r to c . We use x 1: N = { x 1 , x 2 ,..., x N } , x i  X  X   X  R d , to denote the N elements in the training data. That is, r k and c k correspond to two elements of x 1: N . Our goal is to compute the item x (not necessarily in the training data) with the highest user valuation in as few comparisons as possible. We model the valuation functions u (  X  ) for r and c as follows: where the noise terms are Gaussian: e rk  X  N (0 , X  2 ) and e ck  X  N (0 , X  2 ) . Following [ Chu and Ghahramani, 2005b ] , we assign a nonparametric Gaussian process prior to the unknown mean valua-where f = { f ( x 1 ) ,f ( x 2 ) ,...,f ( x N ) } and the symmetric positive definite covariance K has en-tries (kernels) K ij = k ( x i , x j ) . Initially we learned these parameters via maximum likelihood, but soon realized that this was unsound due to the scarcity of data. To remedy this, we elected to use subjective priors using simple heuristics, such as expected dataset spread. Although we use Gaus-sian processes as a principled method of modelling the valuation, other techniques, such as wavelets could also be adopted.
 Random utility models such as (1) have a long and influential history in psychology and the study of individual choice behaviour in economic markets. Daniel McFadden X  X  Nobel Prize speech [ Mc-Fadden, 2001 ] provides a glimpse of this history. Many more comprehensive treatments appear in classical economics books on discrete choice theory.
 Under our Gaussian utility models, the probability that item r is preferred to item c is given by:
P ( r k c k ) = P ( u ( r k ) &gt; u ( c k )) = P ( e ck  X  e rk &lt; f ( r k )  X  f ( c k )) =  X  where  X  ( d k ) = 1  X  tribution. This model, relating binary observations to a continuous latent function, is known as the Thurstone-Mosteller law of comparative judgement [ Thurstone, 1927; Mosteller, 1951 ] . In statistics it goes by the name of binomial-probit regression. Note that one could also easily adopt a logis-tic (sigmoidal) link function  X  ( d k ) = (1 + exp (  X  d k ))  X  1 . In fact, such choice is known as the Bradley-Terry model [ Stern, 1990 ] . If the user had more than two choices one could adopting a multinomial-probit model. This multi-category extension would, for example, enable the user to state no preference for any of the two items being presented. 2.2 Inference Our goal is to estimate the posterior distribution of the latent utility function given the discrete data. there exist sophisticated variational and Monte Carlo methods for approximating this distribution, we favor a simple strategy: Laplace approximation. Our motivation for doing this is the simplicity and computational efficiency of this technique. Moreover, given the amount of uncertainty in user valuations, we believe the choice of approximating technique plays a small role and hence we expect the simple Laplace approximation to perform reasonably in comparison to other techniques. The application of the Laplace approximation is fairly straightforward, and we refer the reader to [ Chu and Ghahramani, 2005b ] for details.
 Finally, given an arbitrary test pair, the predicted utility f ? and f are jointly Gaussian. Hence, one can obtain the conditional p ( f ? | f ) easily. Moreover, the predictive distribution p ( f ? |D ) follows by of Gaussian processes, the fact that they are slow with large data sets, is not a problem for us, since active learning is designed explicitly to minimize the number of training data. 2.3 The Expected Improvement Function Now that we are armed with an expression for the predictive distribution, we can use it to decide what the next query should be. In loose terms, the predictive distribution will enable us to balance the tradeoff of exploiting and exploring. When exploring, we should choose points where the predicted variance is large. When exploiting, we should choose points where the predicted mean is large (high valuation).
 Let x ? be an arbitrary new instance. Its predictive distribution p ( f ? ( x ? ) |D ) has sufficient statis-predictive distribution thus far. That is,  X  max is the highest valuation for the data provided by the individual. The probability of improvement at a point x ? is simply given by a tail probability: where f ? ( x ? )  X  X  (  X  ( x ? ) ,s 2 ( x ? )) . This statistical measure of improvement has been widely used in the field of experimental design and goes back many decades [ Kushner, 1964 ] . However, it is known to be sensitive to the value of  X  max . To overcome this problem, [ Jones et al. , 1998 ] defined the improvement over the current best point as I ( x ? ) = max { 0 , X  ( x ? )  X   X  max } , which resulted in an expected improvement of To find the point at which to sample, we still need to maximize the constrained objective EI ( x ? ) over x ? . Unlike the original unknown cost function, EI (  X  ) can be cheaply sampled . Furthermore, for the purposes of our application, it is not necessary to guarantee that we find the global maximum, merely that we can quickly locate a point that is likely to be as good as possible. The original EGO work used a branch-and-bound algorithm, but we found it was very difficult to get good bounds over large regions. Instead we use DIRECT [ Jones et al. , 1993 ] , a fast, approximate, derivative-free optimization algorithm, though we conjecture that for larger dimensional spaces, sequential quadratic programming with interior point methods might be a better alternative. The goal of our algorithm is to find a good approximation of the maximum of a latent function using preference queries. In order to measure our method X  X  effectiveness in achieving this goal, we create a function f for which the optimum is known. At each time step, a query is generated in which two points x 1 and x 2 are adaptively selected, and the preference is found, where f ( x 1 ) &gt; f ( x 2 )  X  x 1 x 2 . After each preference, we measure the error, defined as = f max  X  f (argmax x f  X  ( x )) , that is, the difference between the true maximum of f and the value of f at the point predicted to be the maximum. Note that by design, this does not penalize the algorithm for drawing samples from X that are far from argmax x , or for predicting a latent function that differs from the true function. We are not trying to learn the entire valuation function, which would take many more queries  X  we seek only to maximize the valuation, which involves accurate modelling only in the areas of high valuation.
 We measured the performance of our method on three functions  X  2D, 4D and 6D. By way of demon-stration, Figure 2 shows the actual 2D functions and the typical prediction after several queries. The test functions are defined as: all defined over the range [0 , 1] d . We selected these equations because they seem both general and difficult enough that we can safely assume that if our method works well on them, it should work on a large class of real-world problems  X  they have multiple local minima to get trapped in and varying landscapes and dimensionality. Unfortunately, there has been little work in the psychoperception literature to indicate what a good test function would be for our problem, so we have had to rely to an extent on our intuition to develop suitable test cases.
 The results of the experiments are shown in Figure 3. In all cases, we simulate 50 queries using our method (here called max EI ). As a baseline, we compare against 50 queries using the maximum variance of the model ( max s ), which is a common criterion in active learning for regression [ Seo et al. , 2000; Chu and Ghahramani, 2005a ] . We repeated each experiment 20 times and measured the mean and variance of the error evolution. We find that it takes far fewer queries to find a good result using max EI in all cases. In the 2D case, for example, after 20 queries, max EI already has better average performance than max s achieves after 50, and in both the 2D and 4D scenarios, max EI steadily improves until it find the optima, while max s soon reaches a plateau, improving only slightly, if at all, while it tries to improve the global fit to the latent function. In the 6D scenario, neither algorithm succeeds well in finding the optimum, though max EI clearly comes closer. We believe the problem is that in six dimensions, the space is too large to adequately explore with so few queries, and variance remains quite high throughout the space. We feels that requiring more than 50 user queries in a real application would be unacceptable, so we are instead currently investigating extensions that will allow the user to direct the search in higher dimensions. Properly modeling the appearance of a material is a necessary component of realistic image syn-thesis. The appearance of a material is formalized by the notion of the Bidirectional Reflectance Distribution Function (BRDF). In computer graphics, BRDFs are most often specified using vari-ous analytical models observing the physical laws of reciprocity and energy conservation while also exhibiting shadowing, masking and Fresnel reflectance phenomenon. Realistic models are therefore fairly complex with many parameters that need to be adjusted by the designer. Unfortunately these parameters can interact in non-intuitive ways, and small adjustments to certain settings may result in non-uniform changes in appearance. This can make the material design process quite difficult for the end user, who cannot expected to be an expert in the field of appearance modeling.
 Our application is a solution to this problem, using a  X  X reference gallery X  approach, in which users are simply required to view two or more images rendered with different material properties and indicate which ones they prefer. To maximize the valuation, we use an implementation of the model described in Section 2. In practice, the first few examples will be points of high variance, since little of the space is explored (that is, the model of user valuation is very uncertain). Later samples tend to be in regions of high valuation, as a model of the user X  X  interest is learned.
 We use our active preference learning model on an example gallery application for helping users find a desired BRDF. For the purposes of this example, we limit ourselves to isotropic materials and ignore wavelength dependent effects in reflection. The gallery uses the Ashikhmin-Shirley Phong model [ Ashikhmin and Shirley, 2000 ] for the BRDFs which was recently validated to be well suited for representing real materials [ Ngan et al. , 2005 ] . The BRDFs are rendered on a sphere under high frequency natural illumination as this has been shown to be the desired setting for human preception of reflectance [ Fleming et al. , 2001 ] . Our gallery demonstration presents the user with two BRDF images at a time. We start with four predetermined queries to  X  X eed X  the parameter space, and after that use the learned model to select gallery images. The GP model is updated after each preference is indicated. We use parameters of real measured materials from the MERL database [ Ngan et al. , 2005 ] for seeding the parameter space, but can draw arbitrary parameters after that. 4.1 User Study To evaluate the performance of our application, we have run a simple user study in which the gen-erated images are restricted to a subset of 38 materials from the MERL database that we deemed to be representative of the appearance space of the measured materials. The user is given the task of finding a single randomly-selected image from that set by indicating preferences. Figure 4 shows a typical user run, where we ask the user to use the preference gallery to find a provided target image. At each step, the user need only indicate the image they think looks most like the target. This would, of course, be an unrealistic scenario if we were to be evaluating the application from an HCI stance, but here we limit our attention to the model, where we are interested here in demonstrating that with human users maximizing valuation is preferable to learning the entire latent function.
 Using five subjects, we compared 50 trials using the EIF to select the images for the gallery ( max EI ), 50 trials using maximum variance ( max s , the same criterion as in the experiments of Section 3), and 50 trials using samples selected using a randomized Latin hypercube algorithm. In each case, one of the gallery images was the image with the highest predicted valuation and the other was selected by the algorithm. The algorithm type for each trial was randomly selected by the computer and neither the experimenter nor the subjects knew which of the three algorithms was selecting the images. The results are shown in Table 1. n is the number clicks required of the user to find the target image. Clearly max EI dominates, with a mean n less than half that of the competing algorithms. Interest-ingly, selecting images using maximum variance does not perform much better than random. We suspect that this is because max s has a tendency to select images from the corners of the param-eter space, which adds limited information to the other images, whereas Latin hypercubes at least guarantees that the selected images fill the space.
 Active learning is clearly a powerful tool for situations where human input is required for learning. With this paper, we have shown that understanding the task  X  and exploiting the quirks of human cognition  X  is also essential if we are to deploy real-world active learning applications. As peo-ple come to expect their machines to act intelligently and deal with more complex environments, machine learning systems that can collaborate with users and take on the tedious parts of users X  cognitive burden has the potential to dramatically affect many creative fields, from business to the arts to science.
 Figure 4: A shorter-than-average but otherwise typical run of the preference gallery tool. At each (numbered)
