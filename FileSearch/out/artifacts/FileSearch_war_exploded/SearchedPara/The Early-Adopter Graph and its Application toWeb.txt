 In this paper we present a novel graph-based data abstrac-tion for modeling the browsing behavior of web users. The objective is to identify users who discover interesting pages before others. We call these users early adopters .Bytrack-ing the browsing activity of early adopters we can identify new interesting pages early, and recommend these pages to similar users. We focus on news and blog pages, which are more dynamic in nature and more appropriate for recom-mendation.

Our proposed model is called early-adopter graph .Inthis graph, nodes represent users and a directed arc between users u and v expresses the fact that u and v visit simi-lar pages and, in particular, that user u tends to visit those pages before user v . The weight of the edge is the degree to which the temporal rule  X  u visits a page before v  X  X olds.
Based on the early-adopter graph, we build a recommen-dation system for news and blog pages, which outperforms other out-of-the-shelf recommendation systems based on col-laborative filtering.
 H.3.3 [ Information Search and Retrieval ]: [Information filtering]; H.2.8 [ H.2.8 Database applications ]: [Data Mining] Algorithms, Experimentation.
 user-browsing analysis, log mining, early-adopter graph, web-page recommendation.  X 
This work was done while the author was an Intern at Ya-hoo! Research, Barcelona, Spain
The digital revolution witnessed during the last decade has resulted in an explosion of available online content. An increasing number of people is now using the Internet on a daily basis to search for specific information, but also to stay informed by reading the news, or consuming user-generated content such as blogs.

Web-search engines are popular tools that allow people to search for information on the Internet. However, web search is effective only when the users have a clear idea of what they are looking. On the other hand, in many cases people have no specific information need, yet they are interested in discovering interesting and relevant content. Such are cases when people surf the Web to read news, funny stories, interesting blog posts, or check what their friends are posting in social-media platforms. Helping the users to find relevant content when they do not have a concrete information need is a problem of information filtering .

Recommender systems aim at producing relevant sugges-tions to the users of a system, and thus, they are essen-tial tools in addressing the problem of information filtering. However, recommender systems offer effective mechanisms in static and relatively noise-free environments. For exam-ple, typical applications of recommender systems consist of recommending movies based on user-rating data, or recom-mending books based on purchase data, where a purchase is a clear indication of interest. On the other hand, de-signing recommender systems for web content, poses signif-icant challenges due to high dynamicity: new pages appear continuously and old pages become obsolete very fast. Fur-thermore, a recommendation system based on user-browsing data should be able to deal with very high levels of noise, since visiting a web page is not as clear indication of interest as, say, renting a dvd to watch a movie or buying a book.
In this paper, we introduce a novel approach for making personalized web page recommendations, in particular rec-ommending news articles and blog posts. Our idea is simple and intuitive: given a user-browsing log, we identify users whotendtodiscoverinterestingpages before others. We call these users early adopters , a term we borrow from social sciences, economics and marketing research, in which early adopters are people who embrace new technologies before others, buy new products soon after their release, and play an important role in influencing others to adopt innovations.
In contrast to previous approaches, we do not just identify clusters of similar users. Instead, we use the input log to build a directed and weighted graph among users. An edge between two users, u and v , expresses the fact that u and Figure 1: The workflow of our web-page recommen-dation framework. visit similar pages and also that user u tends to visit those pages before user v . Thus, our model encodes the latent temporal patterns underlying user visits to web pages. As with technological innovations we assume that some users are better and faster in discovering interesting pages and other users  X  X ollow X  them. By tracking the browsing activity of early adopters we can discover new interesting pages early on, and recommend these pages to users who  X  X ollow X  the early adopters.

Our model is inspired by information networks , like twit-ter , in which users form a social network by following other users. In such a network, users influence each other, and information propagates by posts and re-posts of short mes-sages. Much of social-network analysis research has been devoted to discovering influential users and quantifying the degree to which users influence each other [4, 5, 7, 10, 11, 13, 16, 17, 19]. Early adopters in our model correspond pre-cisely to influential users in an information network, and link strength expresses the degree to which users are influenced by other users, as in social influence studies. The main dif-ference is that the early-adopter graph is implicit ,thatis, users are unaware of the behavior of other users. However, as our results demonstrate, there is sufficient evidence that users exhibit common behavior and follow patterns of in-fluence, which can be potentially explained by exogenous influences (events in the real world) and latent similarities among user interests.
 Contributions . Our contributions can be summarized as follows: ( i ) we introduce the novel concept of early-adopter graph, a model we build based on user-browsing logs; ( ii show how to use this model for recommending news articles and blog posts to users; ( iii ) we evaluate the recommenda-tions provided by our system on a real dataset and we prove that they outperform state-of-the-art recommender systems.
In this section we provide an overview of the model. The outline of the overall workflow is given in Figure 1.
The input to our framework is a dataset D that records the browsing activity of a set of web users. Abstractly, we represent the dataset D as a set of triples ( u, p, t ), indicat-ing that a user u visitedthewebpage p at time t .Fora user who visited the same page more than once, we keep in consideration only the first visit.

We create a dataset D by collecting browsing data from the Yahoo! toolbar . A toolbar is an application installed on top of a web browser that provides certain search function-alities, such as quick-links and other utilities.
The model we propose in this paper is an attributed , weighted ,and directed graph G ( U, A,  X  , X ,w ), which we call the early-adopter graph , and which can be built from the dataset D . The early-adopter graph is specified as follows:
The idea underlying our approach is that whenever an early adopter u visits a page p for the first time, the informa-tion can be propagated along the edges of the early-adopter graph, and the page p can be recommended to other users v  X  U . Ranking the page recommendations for each user will depend on various factors, such as the early-adopter score  X  ( u ) of the node u , the  X  X nfluence X  score w ( u, v early adopter u to the user v , the topic of the page, and the interests of the users. We will combine all these factors by a ranking score s ( u, p ) for each user u and a page p .
The problem we consider can be seen as a special case of the typical problem in recommender systems, however, there are a number of peculiarities. First, we do not have ratings but only visits to pages, which can be very noisy indications of interest. Second, we are dealing with a cold-start problem : we want to recommend new and interesting pages as soon as possible, even if we do not have sufficient information for those pages. And third, the pages we want to recommend are not given as input to the problem, as in the standard setting of recommender systems, but need to be discovered . We do this kind of discovery by exploiting the capability of early adopters to find interesting web pages before others.
In this section we provide details on how to build the early-adopter graph and how to learn its parameters. As we already explained, we start with a user-browsing log D , consisting of triples ( u, p, t ), where ( i ) u is the anonymous id of the user, ( ii ) p is the url of visited page, and ( timestamp of the visit of u at p . We collect such a dataset by sampling data from the toolbar log of Yahoo! .Werestrict our analysis to five months of data, from January 2011 to May 2011, and we consider only urls of news pages and blog sites. To identify which urls correspond to news or blog sites we use a white list of known such sites. We also restrict our dataset to urls whose popularity (the number of distinct users that visited the page) is greater than 50.

For a pair of users u and v we define the frequency freq( u, v ) of the directed pair ( u, v ) to be the number of dis-tinct pages p  X  P for which we observe in the data that the user u visited p before user v . In other words, freq( u, v | P u ; v | = |{ p  X  P | ( u, p, t u ) , ( v,p,t v )  X  X  and t
In order to focus our analysis only to relevant arcs and reduce noise effects, we adopt a minimum support thresh-old  X   X  1. This means that we consider only arcs ( u, v )such that freq( u, v )  X   X  .Inourexperimentsweuse  X  = 50, which gives a pruned graph with 5 202 nodes and 335 091 arcs.
The average degree of 64.41 reflects that the graph is fairly dense. We also observed that users live on a  X  X mall world X , as demonstrated by the average shortest-path length of 2.57. The number of communities that maximizes modularity is 54. Moreover, the graph has strong community structure at microscopic level and weak community structure at macro-scopic level , as demonstrated by the very high clustering coefficient of 0.57 and the relatively low maximum value of modularity of 0.18, respectively.
For a page p  X  P we use U p  X  U to denote the set of users who visited p . Similarly, for a user u  X  U we denote with P u  X  P the set of web pages visited by u .Givena page p j  X  P we can then organize the visits that p j received by all users in a chronologically sorted access list A ( p ( u 1 ,t 1 j ) , ( u 2 ,t 2 j ) ,..., ( u n ,t nj ) ,where t of the first visit of user i to the page j . Naturally, early adopters tend to appear at the beginning of such a list.
In this paper we experiment with two different definitions of early-adopter score. Both definitions produce a score  X  ( u )  X  [0 , 1] for all users u  X  U . The more a user exhibits an early-adopter behavior, the higher is the score  X  ( u ). The score  X  ( u ) is computed as an average over all pages visited by u ;moreprecisely,wedefine: where r ( u, p ) is a measure of how early the user u appears in the access list A ( p ). We adopt two definitions for r the first one considers only the relative position of the user in the list A ( p ), while the second one considers the relative time distance.
 Relative position: we define the relative-position score to of the list.
 Relative time distance: we define the relative-time dis-time that the user u visited page p , while t 0 ( p )and t denote the time of the first and last visit to p , respectively. Example 1 Consider three pages, as shown in Figure 2, the early-adopter scores for user u 3 , according to the two definitions provided above, are as follows: Relative position:  X  ( u 3 )=1  X  1 3 ( 3 6 +1+ 1 4 )=0 . 42 . Relative time distance:  X  ( u 3 )=1  X  1 3 ( 2 5 +1+0)=0 .
The distributions of the relative-position and relative-time-distance scores for our dataset are shown in Figure 3(a). (a) Distribution of  X  ( u ) (b) Distribution of w ( u, v ) Figure 3: The distributions of the early-adopter scores (left), and edge weights (right).
The strength of ( u, v ), expressed by a weight w ( u, v ) [0 , 1], represents the likelihood that a page visited by user will be visited by user v .

Providing an estimate for the weight w ( u, v ) resembles the problem of learning influence probabilities in social net-works [11], which are used in applications such as influence maximization [13]. As we discussed in the introduction, the main difference between these social-networking studies and our early-adopter model is that in our application the graph is not explicit, instead it is reconstructed from user-browsing actions. Another difference is that we do not assume any un-derlying propagation model. Nevertheless, the problems are fairly similar, and thus, it is meaningful to try to estimate the strength of the arcs by using methods developed in the literature on social influence.

In this paper, we follow the work of Goyal et al. [11] and we adopt two basic ways of estimating the strength of an arc ( u, v ):
The Bernoulli measure interprets each visit of u toapage as a hypothetical attempt of u to influence v in visiting the same page. The Jaccard measure considers also the pages visited by v and not by u : thus, it captures whether v follows mostly the actions of u and not many more.

One drawback of the two above definitions is that they consider only the ordering of the visits to the pages but not the time distance between such visits.

Based on the above consideration, to account for the time difference between page visits, we substitute the numerator | P u ; v | of the definitions Bernoulli and Jaccard by a time-dependent term  X  t u ; v , defined as follows: We then obtain two time-dependent versions for arc weights, in particular: Bernoulli (time): w ( u, v )=  X  Jaccard (time): w ( u, v )=  X  The distributions of edge weights for our dataset are shown in Figure 3(b).
We now discuss how to extend our model by incorporating information regarding the topics of interest of the users.
First, we assume that each page belongs to one and only one topic. Then, we model each user u by a topic distri-bution  X  u ,wherethe z -th coordinate  X  uz =Pr[ Z = z | u denotes the interest of user u in the topic z  X  T , and it is computed using the empirical frequency that a user visits pages of a certain topic.

For the classification, we consider 15 topics from the odp directory, such as entertainment , finance , politics , sports , etc. Given a topic z  X  T we construct the vocabulary V ( z ), which is a set of terms that it is typically associated with the topic. The vocabulary is composed of terms ex-tracted from odp categories and most discriminative words appearing in the web pages of the dataset.

Given a page p  X  P we create its bag-of-word represen-tation B ( p ), which is made out of terms appearing in the url, title, and content of the page. We normalize the terms by removing stop words, removing special characters, and converting to lower case.

In order to find the topic z for which the vocabulary V ( matches best with the bag representation B ( p ), we apply a tf . idf -based measure. In particular, given a term t  X  V ( z ) we compute the tf . idf ( t, p )weightoftheterm t in the page p . The classifier assigns the page p to the topic z such that z ( p ) = arg max z  X  T t  X  V ( z ) tf . idf ( t, p maximum score is zero, p is assigned to the notClassified .
Using the approach described above we are able to assign a topic distribution to almost all the users in our dataset. We note that we have used this simple classification algorithm as a proof-of-concept demonstration, and using a better clas-sifier has the potential to improve our results.
Our recommendation approach leverages the information found in the early-adopter influence graph G . Given an arc ( u, v )  X  A we consider suggesting to user v pages that have been visited by user u . To improve the relevance of our recommendations, we rank recommendations by consider-ing the early-adopter score  X  ( u ) of the user u from whom the recommendation originates, as well as the edge weight w ( u, v ) that reflects the strength of the connection between u and v . Additionally, we use page topics to boost scores of pages whose topics match the interests of the user v .Over-all, the recommendation score s ( v,p | u ) of a page p recom-mended to v , given that the recommendation has originated by the early adopter u is: where z  X  is the topic of the recommended page p ,and  X  vz  X  is the preference of user v for that topic.

When a page is suggested to v by different early adopters, the final score s ( v,p )of p recommended to v is the sum of all the scores. Hence, where N  X  ( v,p ) is the set of early adopters who have an arc to v and have visited the page p . The recommendation al-gorithm computes all these scores and then creates a ranked list of pages to recommend. Our empirical evaluation, de-scribed in the next section shows that our recommendation algorithm predicts user clicks with very good precision.
We evaluate our recommendation algorithm on the dataset described in Section 3. We split our dataset, at the level of pages, in two portions: training and test sets. Referring to our notation in Section 3.2, we form the access lists A ( p ) for all pages p and then we split the set of those lists at a ratio of 80-20 for the training vs. test sets. We also ensure that the two sets have a similar distribution in terms of the popularity of the pages.

The training subset is used to build the early-adopter graph G , learn the early-adopter scores  X  ( u ), the arc weights w ( u, v ), and the topic distributions  X  u .Givenauser v and the set of early-adopters N  X  ( v ), the algorithm recommends pages visited by u  X  N  X  ( v )to v . These pages are ranked by the score defined in Equation (1). The recommendation algorithm is evaluated by using precision-at-k ( p@k )for k =1 , 5 , 10 , 15, which gives an indication of the percentage of recommended pages that are actually visited by v .
We compare our recommendation algorithm against col-laborative-filtering approaches. We assume that a click on apagecorrespondstoaratingequalto1whileanon-click corresponds to a rating equal to 0, and we compute user similarity with Tanimoto and Log-Likelihood coefficients. For brevity we present the results achieved with the Bernoulli definition of the edge weights. 1 As we can see in Figure 4, our approach outperforms significantly algorithms based on collaborative filtering. In particular, the improve-ment is of 10% for p@1 and of 20% for p@15 .

We note that this is a difficult recommendation task, and a certain level of noise is present. Nevertheless, our method-ology is completely automated, and we think that it is ap-propriate for comparing different algorithms.
The results obtained with the other edge-weight definitions are qualitatively the same. Figure 4: Precision results: early-adopter graph (EAG) vs. collaborative-filtering (CF). Web-usage mining. Web logs represent a valuable source of information to study user behavior and to improve user web experience. Bilenko et al. [6] and White et al. [18] ana-lyze web-activity logs to identify web sites frequently visited by users after a query. These X  X opular X  X estinations are then used to suggest authoritative websites for queries. Recommender systems. Recommender systems allow to learn user preferences and make recommendations, based on user past behavior. They are used extensively for rec-ommending products (e.g., books, movies, music, etc.) and helping people finding web content (e.g., news, photos, etc.).
Das et al. [9] propose a scalable content-agnostic approach based on collaborative filtering to recommend news. Resnick et al. [14] present a distributed system for gathering reader ratings of news.
 Social influence and information propagation. Our work is also related to the large body of research on social influence and information pro pagation. The main computa-tional problems in this area are: ( i ) distinguishing genuine social influence from  X  X omophily X  and other factors of cor-relation [3, 4, 8, 10]; ( ii ) measuring the strength of social influence over each social link [11, 16, 17, 19]; ( iii )dis-covering a set of influential users [2, 13]. Besides, many researchers have focused on analyzing data (e.g., twitter data) to better understand the phenomenon of viral propa-gation of information in social networks and micro-blogging platforms [1, 5, 7, 12, 15].
In this paper we introduce a novel approach for recom-mending web pages. We exploit user-browsing behavior data to construct an implicit network of influence. We then iden-tify users who discover interesting pages and we call them early adopters . The general idea of our framework is to mon-itor the activity of early adopters, and recommend pages discovered by them to users who  X  X ollow X  the early adopters.
The early-adopter graph is a general model and its appli-cation to other domains deserves further investigation. As future work we also plan to investigate applying different influence models to learn edge weights, as well as applying a more sophisticated topic classifier. [1] E. Adar and L. A. Adamic. Tracking information [2] N. Agarwal, H. Liu, L. Tang, and P. S. Yu. Identifying [3] A. Anagnostopoulos, R. Kumar, and M. Mahdian. [4] S. Aral, L. Muchnik, and A. Sundararajan.
 [5] E. Bakshy, J. M. Hofman, W. A. Mason, and D. J. [6] M. Bilenko and R. W. White. Mining the search trails [7] M. Cha, H. Haddadi, F. Benevenuto, and K. P.
 [8] D. J. Crandall, D. Cosley, D. P. Huttenlocher, J. M. [9] A. Das, M. Datar, A. Garg, and S. Rajaram. Google [10] T. L. Fond and J. Neville. Randomization tests for [11] A. Goyal, F. Bonchi, and L. V. S. Lakshmanan. [12] D. Gruhl, R. V. Guha, D. Liben-Nowell, and [13] D. Kempe, J. M. Kleinberg, and  X  E. Tardos. [14] P. Resnick, N. Iacovou, M. Suchak, P. Bergstrom, and [15] D. M. Romero, B. Meeder, and J. M. Kleinberg. [16] K. Saito, R. Nakano, and M. Kimura. Prediction of [17] J. Tang, J. Sun, C. Wang, and Z. Yang. Social [18] R. W. White, M. Bilenko, and S. Cucerzan. Studying [19] R. Xiang, J. Neville, and M. Rogati. Modeling
