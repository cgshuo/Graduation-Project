 The forecasting of time series is a well known and significant prediction task. Many methods have been proposed in this area, ranging from the simple to the very sophisticated. An important class of techniques includes methods which learn a model based on optimizing a regularized risk function, such as linear regression, Gaussian processes, neural networks and support vector machines. Common drawbacks in the development of time series forecasting methods are that many of them are applicable to only a specific type of time series, such as medical data or stock market series; they may require certain conditions to be fulfilled; or the improvement in performance is associated with a high increase in complexity.
 time series prediction, since it is simple, intuitive and produces models with a high degree of interpretability. Its performance is also often surprisingly good compared to more complex met hods. Nevertheless, reduction of prediction error is a key concern and it rema ins attractive to consider techniques for improving the behavior of standard linear regression, particularly for challenging circum-stances. Such circumstances include non stationary and noisy time series, where changes in the distribution of the series often occur over time.
 that takes a different perspective. Given a single univariate time series, instead of optimizing the arithmetic mean of the loss over all training samples, we pro-pose to optimize the quadratic mean of the loss over groups of training samples. In particular, the univariate time series is segmented into a number of groups, where each group contains one or more s amples. A linear model is then learned which simultaneously optimizes both the average loss of each group, as well as the variance of the loss across groups. In other words, there are two concur-rent optimization objectives. First, the model which is produced should have low overall error rate -this is achieved by ensuring the average loss within each group is small. Second, the groups should not vary too much with respect to the error rate of each individual group -this ensures that there is no single group which can significantly bias the characteristics of the output model. A primary question is how should the univariate time series be segmented into groups ? Our proposal is to segment the samples according to their distribution charac-teristics. The intuition here is that we would like to learn a model which is not biased towards any single distribution, since we do not know which distribution the future behavior of the time series will most resemble.
 real stock market datasets, 5 non-financial time series datasets and 5 synthetic datasets. We find that our new method (which we call QMReg) can produce linear models which are quite different from those of standard linear regression, and often have significantly less error, with empirical reductions typically in the range of 10%-30%. Our proposed method is an intuitive technique that ensures more evenly distributed, less volatile error and sensitivity to changes in the dis-tribution of the series. Data mining researchers have a range of d ifferent types of classification and re-gression algorithms at their disposal, and many of them have been applied to the time series forecasting problem. Artificial Neural Networks(ANNs) [17], Support Vector Machines [13] and Hidden Markov Models [20] [9] are other methods that have been used with some success for financ ial time series forecasting, and clus-tering has been used as an aid in the forecasting process as well.
 embodied in the Autoregressive Integr ated Moving Average (ARIMA) models, which are descriptive, intuitive and often perform as well as advanced models. Weighted linear regression and AutoRegressive Conditional Heteroskedasticity (ARCH) models are more complex modifications of linear regression, which re-quire some additional statistical expertise.
 the statistical properties of the data samples when learning a model [15], by han-dling the outliers in the data, and assigning appropriate weights (or losses) to reduce their influence [18]. Incorrect labelling of a sample as an outlier can be a concern when using robust regression. Choosing the most appropriate approach often requires statistical expertise by the user.
 mization function, a potentially related area of research is regularized multitask learning [16], where the objective is to learn multiple classification tasks simul-taneously, rather than independently. However, to our knowledge, research in multitask learning has not used a quadratic mean loss function for the simulta-neous optimization of loss across groups, as is done in this paper. We propose an algorithm that has two phases: (1) detection of distribution change points to segment the time series into groups, and (2) training the re-gression model using a quadratic mean to minimize both the individual loss of each group and the variance of the loss across groups. 3.1 Time Series Segmentation -Distribution Change Points Distribution changes in time series variables are from a continuous process, sub-ject to a set of external factors [1] [5]. The task of breaking up the samples of a dataset into segments or groups based on distribution or similarities is not an un-familiar challenge -simple clustering be effective in many cases. When it comes to time series, as we may prefer to preser ve the time element, distribution based methods can also be used. Potential candidate tests for non-parametric change point detection methods include the Wilcoxon rank sum method (WXN) and the kernel change method (KCD) [14]. They can be applied for this task as they are understandable and easy to introduce in the learning process. The Wilcoxon rank sum method (WXN) assess whether two sets of data samples follow the same distribution according to a statistic al measure. It is an easy to implement statistical test, and no a-priori knowled ge is required (other than specification of an appropriate p-value, commonly set to 0.05).
 we appoint after it another sliding window of the same length, [ m +1,2 m ]. We move the second window and compare if the samples in both windows follow the same distribution: if that is the case, we continue moving the second window, until the distribution changes. The change point will be at the last sample of the second window (point 2 m + p ), p&gt; 0, where we detect the group window v for that group of samples; we move the first window just after that point [2 m + p +1, 3 m + p ], the second window comes after the first one [3 m + p +1,4 m + p ]and we repeat the process for the rest of the dataset(Figure 1). The choice of the window size m can vary, and we choose it to be the same size as the testing set. 3.2 Quadratic Mean Based Empirical Loss Function The quadratic mean is defined as the square root of the average of the squares of each element in a set. In the case of just two errors, 1 and 2 ,thevaluesof the quadratic mean QM and the arithmetic mean AM can be written as: This shows that the quadratic mean is lower bounded by the arithmetic mean, and this bound is reached when 1 = 2 . This form of optimization was successfully tested for the scenario of imbalanced relational datasets [7] where there are only two groups (positive and negative classes). The more advanced form we use in our methods is specialised for the case of time series and permits any number of groups.

Many machine learning methods address the learning process as finding the minimum of the regularized risk function. For n training samples ( x i , y i ) ( i =1,..,n), where x i  X  R d is the feature vector of the i  X  X h training sample, d is the number of features, and y i  X  X   X 1, 1 } isthetruelabelforthe i  X  X h training sample (in the case of classification) and y i  X  R in the case of regression, the regularized risk function is: w is the weight vector which also includes the bias term b (which makes x having and additional bias feature x d +1  X  1), and  X  is a positive parameter that balances the two items in Equation 2, and R emp ( w )= 1 n n i =1 l ( x i ,y i , w ). The loss function l ( x i , y i , w )in R emp ( w ) measures the distance between a true label y and the predicted label from the forecasting done using w , and in the case of Linear Regression it has the form of 1 2 ( w T x -y ) 2 .
We investigate the use of the quadratic mean as a risk function which balances k values, instead of just two values. Each value represents the average loss for a group of instances. The grouping of instances can be conducted in a range of ways. A simple way is to segment the time series data into k groups by their distribution, and we use the Wilcoxon method for that purpose. Each group consists of consecutive data samples, and may vary in size depending on the distribution of the underlying data. The effect of using the quadratic mean is to produce a model which optimizes the average loss for each group, as well as the variance of the average loss across the k groups.

The details behind the groups error optimization are as follows: with n sam-The empirical loss function of k group has the form of: where f j is the average error for group j consisting of n j consecutive samples following the same distribution where x ji is the i -th sample of group j ,and y ji is the i -th output of group j . After some manipulation, it can be rewritten as where  X  is the mean error of the k groups 1 k j = k j =1 f j ( w )and  X  is the standard deviation of the error across groups 1 ,...,k . This form clearly shows the overall loss is the sum of two components, the average loss per group and the variance across groups.

An ideal w minimizes the square root of the average of squares of errors per group, while keeping the structural risk mi nimization as well, therefore resulting in the final optimization function This form of calculation of the loss function is the form we use for the proposed QMReg model, and this empirical loss function introduces robustness in the algorithm, making it capable of minimizing the effect of outliers and the error per distribution group. By using linear optimization methods, such as the bundle method [6], we can calculate the subradients of the empirical loss and use them to iteratively update the w vector in a direction that minimizes the quadratic mean y ) 2 , and it X  X  gradient will have the from of l ( x i , y i , w )=( w T x  X  y ) 2 .Usingthis in the calculation of the loss for a group in Equation 4, for the k groups of Algorithm 1. Bundle methods for solving the k-group quadratic mean mini-mization problem samples, the subgradient function will have the form of Equation 7, and the detailed pseudo-code of the entire learning process is presented as Algorithm 1. 3.3  X  X very Sample as a Group X  Strategy We compare our method of k groups with 2 extreme ca ses -when there is only one group, and when every single instance is a group. It can be easily shown that in the case of 1 group, the quadratic mean is equivalent to the standard linear regression model. As a single instance can be represented as a group, we inves-tigate this research direction as well. The resulting model is QMSampleGroup. In this case the quadratic mean will try to minimize the variance of error across the entire set of samples. Evaluation of the performance of the QMReg method was the main target of the experimental work we conducted. To achieve this goal in the experiments both real datasets and synthetic datasets were used. We tested 20 real stock market time series datasets obtained from [11], in the time frame of 2000-2012. We obtained daily stock market closing prices, one of the most often analysed types of data [1]. The sizes of the datasets are between 200 and 600 samples, divided on training set and test set.
 tested (Table 1). The initial set represen ted a visible deterministic trend, after which 2 types of changes were introduced : increasing or decreasing the last sam-ples, and adding different amounts of noise, in order to test the newly proposed algorithm the ability to work with noisy data. We also performed testing on 5 non-financial time series, revealing opportunities for application of quadratic mean based approach in the non-financial time series domain [12].
 4.1 Testing and Results Comparison between 6 methods was conducted in order to evaluate the ef-fect of the QMReg methodology: Standard Least Squares Linear Regression (LS, regressing to past 4 values), Distribution based Quadratic Mean Linear Regression (QMReg, regressing to past 4 values), Quadratic Mean Linear Re-gression with every sample as a group (QMSampleGroup, regressing to past 4 values), ARIMA(3,0,1), Robust Regression (Huber M -estimator) and SVM Re-gression(SVM,  X  = 1, C=1, =0.001,  X  =  X   X  =0.001),). The Root Mean Square Error(RMSE) was chosen as a performance metric, and we also calculate the error reduction (ER) compared to the Least Squares models: From the results presented in Table 2, we can clearly see that the QMReg method performed significantly better than the standard Least Squares linear regression, much better than the ARIMA model, and very similar to the Robust Regression and SVM, and was the method with the most stable convergence towards the optimal weight vector as well, which was not always the case with Least Squares. The performance of the QMReg method was mostly greater for datasets where higher number of groups was detected. The SVM output arguable less easy to interpret -support vector samples in the dataset are not as much of use as simple coefficients for the features. Wit h similar performance as SVM regression, QMReg does show potential for use in cases when one is not familiar with more complex forecasting metho ds, but can interpret the output of a linear regression model. The objective of the QMReg to reduce the loss between groups can be seen from Figure 2 which graphically shows how the errors per group are minimized and made more even when QM grouping in performed.

The Robust regression is easy to interpret too, but a more statistical analysis of the data is required -the non-parametric QMReg can be used as a black-box method, and still deliver similar results. The difference with Robust regression can be seen from Figure 3: the crossed points are outliers, and red points are the test samples. We can notice that both methods are dealing successfully with the outliers, but the subtle change in the distribution is more correctly detected by the QMReg method, while robust regre ssion is considering the end points as possible outliers and still keeps the line towards the overall mean. It is this subtle change detection that further leads to lower error for QMReg (RMSE=0.89) than the Robust regression error (RMSE=2.09). We can see the QMReg is highly competitive when compared to Robust regression, and based on Figure 3 we believe it may have the potential to deal with non-stationary behaviour better. 4.2 Loss Function Analysis By performing grouping among the training set, the QM methods tend to mini-mize not only the overall training error, but the error per group. This results in a lower variance in the loss, as it can be seen from Table 3: the standard deviation in QMReg method was at its best up to 58% less than the one of the LS method. The grouping performed in the case of QMReg was also conducted for LS. The loss per group was calculated, and standard deviation among the loss per group showed that the QM method indeed minimizes the loss amongst the groups: Table 3 shows the standard deviation of the error per group is statistically lower in the case of QMReg when compared to LS. Time series forecasting is a classic predi ction problem, for wh ich linear regression is one of the best known and most widely used methods. In this paper, we have proposed a technique that enhances standard linear regression, by employing an optimization objective which explicitly recognises different groups of samples. Each group corresponds to a segment of the time series whose samples have similar distribution characteristics. Our objective simultaneously minimizes the expected loss of each group, as well as the variance of the loss across the groups. By doing so, we ensure a model that produ ces more stable, less volatile predic-tions, and capable of additionally minimizing the effect of outliers or noisy data. that it could produce linear models differ ent to that of standard linear regression and which also achieved consistent reductions in error in the range 10% to 30% on average, up to 40% error reduction in some cases. As the performance of our proposed method is comparable to more ad vanced forecasting methods(SVM re-gression and Robust Regression), our model has an advantage that it improves the performance of linear regression wh ile avoiding unnecessary complexity and unwanted parameters, so it can be used by more general practitioners on diverse types of time series.
 for cases where the quality of a group wit h respect to the prediction task can be estimated, and introduce the time element in the learning process even further.
