 Estimating the effectiveness of a search performed in re-sponse to a query in the absence of relevance judgments is the goal of query-performance prediction methods. Post-retrieval predictors analyze the result list of the most highly ranked documents. We address the prediction challenge for retrieval approaches wherein the final result list is produced by fusing document lists that were retrieved in response to a query. To that end, we present a novel fundamental predic-tion framework that accounts for this special characteristics of the fusion setting; i.e., the use of intermediate retrieved lists. The framework is based on integrating prediction per-formed upon the final result list with that performed upon the lists that were fused to create it; prediction integration is controlled based on inter-list similarities. We empirically demonstrate the merits of various predictors instantiated from the framework. A case in point, their prediction qual-ity substantially transcends that of applying state-of-the-art predictors upon the final result list.

The effectiveness of retrieval methods often varies across queries [35, 15]. Thus, the ability to automatically infer which queries are more difficult for a retrieval method than others can be very important. These observations motivated a large body of work on predicting query performance [5]; that is, estimating the effecti veness of a search performed in response to a query in the absence of relevance judgments.
Pre-retrieval predictors [18, 16] analyze the query expres-sion and utilize corpus-based statistics. By definition, then, their prediction is not based on the ranking at hand, the ef-fectiveness of which is the goal of prediction. Post-retrieval predictors [9, 5], on the other hand, analyze also the result list of the most highly ranked documents. Hence, they often yield prediction quality that (substantially) transcends that of pre-retrieval predictors [5].

We address the challenge of post-retrieval query perfor-mance prediction for fusion -based retrieval [14, 8]. The goal is predicting the effectiveness of a document list that is pro-duced by fusing (merging) a few lists that were retrieved, from the same corpus, in response to the query.

The motivation for addressing the challenge just men-tioned is twofold. First, fusion methods post effective re-trieval performance on average [8]; e.g., when fusing lists retrieved by using different query and document representa-tions or retrieval methods [8]. Furthermore, although fusion-based retrieval was shown to somewhat improve retrieval-performance robustness across queries [2], we empirically show that commonly used fusion methods still suffer from substantial performance variance. Hence, predicting which queries are more difficult for fusion-based methods than oth-ers is motivated by the same reasons that drove forward work on query-performance prediction in general.

The second factor motivating the pursue of the query-performance prediction challenge for fusion-based retrieval is the unique characteristics of this retrieval paradigm. Specif-ically, we address the following question, which to the best of our knowledge, has not been been tackled before: Can us-ing information induced from the intermediate results of the fusion-based retrieval process (i.e., the document lists that are fused) in addition to that which is the focus of previous work on post-retrieval prediction (i.e., that induced from the final result list) help improve prediction quality?
We present a novel probabilistic query-performance pre-diction framework for fusion-based retrieval that addresses the question stated above. The framework is based on inte-grating prediction performed upon the final result list, the effectiveness of which is the goal of prediction, with that em-ployed over the lists that are fused; prediction integration is controlled by using inter-list similarity measures. The un-derlying idea is that lists that are similar  X  with respect to the documents they contain and their ranking  X  should have similar performance. We ins tantiate various predictors from the framework by varying t he prediction-integration approach, the single-list predictors employed, and the inter-list similarity measure used.
Empirical evaluation performed with several tracks of TREC, and with various fusion methods applied to runs submitted to these tracks, attests to the merits of our prediction ap-proach. For example, the prediction quality of the predictors we derive substantially transcends that of applying state-of-the-art post-retrieval predictors upon the final result list.
Our main contributions can be summarized as follows. 1. Previous work on post-retrieval prediction [5] focuses 2. Empirically demonstrating the substantial merits of
There is a large body of work on query-performance pre-diction [5]; specifically, on devising post-retrieval predictors that analyze properties of the result list of top-retrieved doc-uments. For example, clarity-based predictors [9] measure the divergence of a model induced from the result list from that of the corpus. Other prediction methods measure dif-ferent notions of the robustness of the result list [37, 33, 39, 3, 40]. Some predictors analyze the retrieval scores of documents in the result list [13, 40, 27, 11, 12, 31].
The prediction methods mentioned above analyze the prop-erties of a single retrieved list. In contrast, our prediction framework predicts the effectiveness of the final result list by analyzing both its properties and those of the lists that were fused to create it. We use adaptations of some pre-viously proposed predictors [9, 40, 31] for single-list-based prediction in our framework.

Query-performance predictors were used to improve fusion effectiveness [4, 28]. Specifically, predictors were applied upon the lists that are fused so as to weigh them in the fusion process. In contrast, our goal is to predict the effectiveness of the final document list that is the result of fusion.
The similarity between the final result list and the inter-mediate lists fused to create it was used as a predictor in its own right for the effectiveness of the intermediate lists [32, 3, 13]. In contrast, our methods predict the effectiveness of the final result list; specifically, by using the inter-list simi-larities to control the integration of prediction applied upon the intermediate lists and upon the final result list.
Some recent work [20] showed that several post-retrieval predictors [9, 13, 40, 30, 31] essentially rely on the following principle. Given a result list, the effectiveness of which we want to predict, a pseudo effective and/or ineffective  X  X ef-erence X  document lists are created; then, the similarity be-tween the reference lists and the result list at hand serves for prediction. In contrast, our goal is to predict the effec-tiveness of a result list that was produced by fusing several given result lists. To that end, we propose a novel prob-abilistic prediction framework wherein inter-list similarities serve a specific role.

There is recent work [21] that derives the prediction prin-ciple mentioned above, of using reference lists, in a prob-abilistic manner. In contrast to our approach, prediction applied directly upon the final result list, which we empiri-cally show to be highly important for the fusion setting we address here, is not integrated in the proposed model. In fact, the fundamental predictor we present, which serves for deriving various predictors, can be viewed as a generalized form of the reference-list-based prediction model presented in this work [21].
Throughout this section we assume that the following have been fixed. A query q , a corpus of documents C ; and, m document lists, L 1 ,...,L m , each of which contains n docu-ments that were retrieved from C by some retrieval method in response to q .

Suppose that some fusion method [8] was employed over the retrieved lists to produce a final result list, L res documents. Our goal is to predict the effectiveness of L with respect to the information need expressed by q in the absence of relevance judgments.

The query-performance prediction framework that we present does not assume knowledge of the fusion method, nor knowledge about the retrieval methods used to create the lists L 1 ,...,L m . Hence, the framework is quite general and yields high quality prediction for various fusion methods as we show in Section 4. Yet, in Section 3.5 we discuss the connection between the prediction framework and a specific family of fusion approaches.

The prediction framework is based on utilizing some post-retrieval query-performance predictor P base . The predictor P base operates upon q and a document list L that was re-trieved in response to q ; corpus-based information can also be utilized. The prediction value assigned by P base , denoted P base ( L ; q ), is an estimate for the effectiveness of L with re-spect to q . In Section 3.4 we discuss several adaptations of previously proposed predictors that we use for P base .
We can state the prediction goal as that of estimating, in the absence of relevance judgments, p ( L res | q ), i.e., the prob-ability that L res satisfies the information need expressed by q . The resultant estimate reflects the presumed effectiveness of L res . Following previous work on post-retrieval query-performance prediction [5], we can apply P base directly upon L res ,soastoderiveanestimatefor p ( L res | q ). This is the Direct predictor:
However, we would like to exploit the special character-istics of the fusion-based retrieval setting to potentially im-prove prediction quality; that is, utilize also information in-duced from the lists L 1 ,...,L m that were fused to produce L res . A case in point, if many of these lists are predicted to be highly effective, and L res is not very different  X  in the documents it contains and their ranking  X  from these lists, then the effectiveness predicted for L res should be high.
Accordingly, we use the lists L 1 ,...,L m as proxies for L in estimating its effectiveness. Specifically, we can write Equation 2 is based on the premise that the effectiveness of L res can be predicted (estimated) using a mixture of esti-mates (predictions) used for the effectiveness of the lists ( L that were fused to create it. In what follows we further de-velop Equation 2 and present a variety of predictors that can be derived based on this premise. Later on, specifically in Section 3.5, we discuss the formal connections between some of the derived predictors and a specific family of fusion meth-ods (namely, linear models). Yet, we hasten to point out that in Section 4 we show that the proposed predictors post high prediction quality for a variety of fusion approaches, among which is a highly effective non-linear method.
We estimate p ( L res | q, L i ) in Equation 2 using a linear mixture governed by a free parameter  X  :(1  X   X  ) X  p ( L res p ( x ). Applying some probabilistic algebra, and assuming that  X  p ( L i | q ) is a probability distribution over the lists L (i.e., tion approach P  X  p ( L | q ) is an estimate for the probability that the list L satis-fies the information need expressed by q . Thus, the resultant prediction for the effectiveness of L res isbasedonintegrat-ing a direct estimate for its effectiveness ( X  p ( L res | estimates for the effectiveness of the lists L i from which it was fused (  X  p ( L i | q )); these estimates are weighted by L  X  X ssociation X  with L res ( X  p ( L res | L i )).
To instantiate specific prediction methods from Equation 3, we use the prediction value P base ( L ; q ) for the estimate  X  p ( L | q ). To estimate the association between L res ( X  p ( L res | L i )), we use an inter-list similarity measure, sim ( We discuss two similarity measures in Section 3.2. 2 The resultant AMean predictor instantiated from Equation 3, which is based on an arithmetic mean of (weighted) predic-tion values, is defined as Setting  X  = 0 amounts to the Direct predictor from Equa-tion 1. Higher values of  X  result in increased importance attributed to prediction based on the lists L i .

As an alternative to the arithmetic-mean-based approach employed by AMean, we explore the GMean predictor that
A somewhat conceptually similar estimation technique, and consequent derivation, were used in work on cluster-based retrieval [19].
Normalizing the similarity measure to yield a probability distribution showed no empirical merit in terms of predic-tion quality. We also note that the base predictors used in Section 3.4, as the vast majority of previously proposed pre-dictors [5], do not induce proba bility distributions. Hence, while our framework is probabilistic, it is instantiated using estimates that are not probability distributions. is based on a form of a geometric mean: 3 Consequently, if L res or any of the lists L i is predicted by P base to be of very low effectiveness, the resultant predic-tion value is lower for GMean than for AMean as GMean is more  X  X onservative X . Similar considerations motivated the use of the GMAP effectiveness evaluation metric [36] (i.e., the geometric mean of per-query average precision values) as a conservative alternative to MAP (the arithmetic mean).
There is a wide variety of inter-list similarity measures that can be used in the prediction methods presented above. Here, we study two measures that attribute more impor-tance to documents at high ranks of the lists than to those at low ranks. This is because retrieval effectiveness, the goal of prediction, is often measured, as will be the case in Sec-tion 4, using metrics that are heavily affected by top ranks (e.g., average precision). The first inter-similarity measure that we use, KL , utilizes rank information, is asymmetric, and only considers the most highly ranked documents in the list. The second measure, Cosine , uses retrieval scores, is symmetric, and considers all documents in the list. documents in the lists that are fused. We use p L (  X  )todenote the representation of L as a probability distribution over C init . To induce p L (  X  ), we follow a previous proposal [3]. Specifically, let d r (  X  L ) be the document at rank r of L . ( r = 1 is the highest rank.) Then, p L ( d r ) def = 1 2 rank r of L with r&gt;c ,orthatisnotin L , p L ( d )issetto0. The similarity between lists L i and L j is defined based on the asymmetric KL divergence measure: sim KL ( L i ,L j ) exp(  X  KL TheCosinemeasure. The second inter-list similarity mea-sure is based on a vector-space representation of L ,de-noted L , defined over C init . Specifically, the l  X  X h compo-nent of the vector L is set to the score of document d (  X  X  init )if d l appears in L and to 0 otherwise. The co-sine is used as a symmetric inter-list similarity measure:
To study the merits of differentially weighting the predic-tion values assigned to the lists L i based on their similarity with L res , we devise variants of the AMean and GMean
GMean is the geomtric mean of the prediction value as-signed to L res and the weighted prediction values assigned to the lists L i . We do not use the geometric mean of the lat-ter (i.e., use the inter-list similarity values for the exponent of the prediction values) as doing so turned out to yield worse prediction quality than that attained by GMean as defined here. We also note that GMean cannot be directly derived from Equation 3 as opposed to AMean. methods that use a uniform inter-list similarity measure; i.e., the similarity between L i and L res is not accounted for. Specifically, we study the UniAMean predictor: and, the UniGMean predictor 4 : m is the number of lists.

The methods presented above assign a prediction value that is based on the aggregation of the prediction values for all the lists that are fused. To consider the alternative of using the prediction for only  X  X epresentative X  lists, we upper-bound the prediction value assigned by UniAMean in Equa-tion 4 using the UniMax predictor: P and lower-bound it using the UniMin predictor: P
The last step for instantiating predictors using the meth-ods described above is defining the predictor P base that op-erates on a document list L . However, most previously pro-posed post-retrieval predictors were devised for specific re-trieval methods [5]; in fact, most state-of-the-art predictors were shown to be effective when the retrieval was based on surface-level document-query similarities (e.g., [9, 13, 40, 17, 30, 31]). Here, we have to employ P base upon lists L ,...,L m that were retrieved by some retrieval methods that may not be those for which previous predictors were designed for; and, upon L res which is created using a fusion method. Thus, we use some recently proposed adaptations [31] of three (highly) effective post-retrieval predictors that operate on a list ( L ) retrieved by some method. In what follows we use s ( d ; L ) to denote the (non-negative) score of document d in L . For the lists L i that are fused, we normal-ize the original retrieval scores so they sum to 1; s ( d ; L )is the normalized retrieval score in this case. (See Section 4.1 for further details.) The predictors we present focus on L the k highest ranked documents in L ; k is a free parameter. Clarity. Clarity measures the focus of a retrieved list L with respect to the corpus by computing the divergence between their induced language models [9]. The higher the diver-gence, the more distant the models are; and, L is presumed to be more effective.

The (relevance) language model utilized by Clarity, which is induced from L , is based on a language-model-based re-trieval used to create L . Here, we use Clarity for the general
UniGMean is the geometric mean of (i) the prediction value assigned to L res , and (ii) the geometric mean of the predic-tion values assigned to the lists L i . retrieved list case. Let w ( d ; L ) be a weight assigned to doc-ument d with respect to a list L in which it appears. Then, the relevance model R L [ k ] induced from L [ k ] is defined by: p ( t | R the vocabulary; p ( t | d ) is the probability assigned to t by a language model induced from d . (Language model induction details are provided in Section 4.1.) Clarity is the KL diver-gence between the relevance model and the (unsmoothed) corpus language model ( p (  X |C )):
We use two variants of Clarity. The first, sClarity ,weighs document d by its retrieval score in L : w sClarity ( d ; L ) s ( d ; L ). The rClarity measure, as in previous proposals [10, 13], weighs d using a linearly decreasing function of its rank r (  X  k )in L ; specifically, w rClarity ( d ; L ) def =2( k +1 WIG. The WIG predictor is based on the premise that high retrieval scores at top ranks of the list, specifically, with re-spect to that of the corpus, indicate effective retrieval [40]. WIG was proposed in the markov random field (MRF) re-trieval framework [25]; and, was also shown to be effective for predicting the effectiveness of a language-model-based retrieval [30]. Here, for the case of a list retrieved by some method, we use for prediction the mean of retrieval scores of the highest ranked documents [31] 5 : We note that the corpus retrieval score cannot be used, as opposed to the original implementation of WIG [40], since we do not have knowledge of the retrieval method used to create L ; or, it might be created by fusion as is the case for L res . While the corpus retrieval score originally served to ensure inter-query compatibility of prediction values due to lack of inter-query compatibility of retrieval scores, here retrieval scores in the lists L i are sum-normalized to [0 , 1]. NQC. The NQC predictor measures the standard devia-tion of retrieval scores of top-retrieved documents [31]. It was argued that increased standard deviation amounts to potentially reduced query drift in the result list; and hence, implies improved retrieval effectiveness [31]. While the argu-ment was based on the assumption that retrieval scores re-flect document-query surface-level similarities, we use NQC for a list retrieved by some method [31]. Specifically, let
The original WIG measure also employs query-length nor-malization [40]. This normalization is important for inter-query compatibility of retrieval scores in the MRF and lan-guage modeling frameworks. We found that such normaliza-tion hurts the prediction quality when using lists retrieved by some method (e.g., TREC runs) as is the case here.
Sum-normalization of the fusion scores in L res degraded prediction quality for the different predictors, and hence was not used here and after.  X 
Suppose that the final result list, L res , contains a single document, d . Then, using Equation 3 with  X  =1yields That is, the predicted (estimated) relevance of d to q is de-termined based on d  X  X  association with the lists L i ( X  p ( d wherein the association is weighted by the predicted effec-score (or rank) in L i , then we attain the general form of lin-ear fusion functions [14, 34]. For example, the commonly used CombSUM method [14], which is one of the fusion approaches used for the evaluation presented in Section 4, is based (in spirit) on using d  X  X  normalized score in L i ing the L i  X  X  to be equi-effective.

Thus, we see that the propose d prediction framework can conceptually be viewed as a generalization (including a geometric-mean-based form), and adaptation, of the basic linear fusion principle to the prediction task. Yet, the framework utilizes not only information induced from the lists that are fused, as is the (natural) case in the fusion-based retrieval pro-cess, but also information from the final result list ( L specifically, if  X &lt; 1. (See Equation 3.) This is an impor-tant fundamental difference, among others, that rises due to the inherent difference between the prediction and fusion (retrieval) tasks. For experiments we use the ad hoc tracks of TREC3 and TREC8, the Web tracks of TREC9 and TREC10, and the robust track of TREC12. Thus, a variety of query sets and corpora (newswire and Web) serves for evaluation.

We randomly sample m = 5 runs from all those submit-ted to a track and that contain at least 100 documents as results for each query. The n = 100 highest ranked docu-ments for a query in a run serve as one of the 5 lists L i be fused 8 . We repeat this runs-sampling process 20 times and report the average prediction quality of each predictor over the samples. Statistically significant differences of pre-diction quality are determined using the two-tailed paired t-test computed at a 95% confidence level with respect to the 20 samples.
As is the case for WIG, the original NQC measure normal-izes retrieval scores with respect to that of the corpus [31]. In contrast, here we cannot use corpus-based normalization of retrieval scores, but the scores in the lists L i are sum normalized.
We use 100 rather than 1000 documents as results for each query, as fusion methods are most effective when the number of documents in the lists that are fused is relatively small [34]. We use the 100 documents most highly ranked by the fusion method for the final result list L res .

To measure prediction quality, we follow common practice [5]. That is, we compute Pearson X  X  correlation between the values assigned by a predictor to the queries in a track, and the true average precision (AP at cutoff 100) values for these queries  X  determined using TREC X  X  relevance judgments  X  attained by the fusion method at hand.
 Fusion methods. Fusion methods that utilize retrieval scores require inter-list score compatibility. Therefore, we normal-ize the retrieval score of a document in a list with respect to the sum of all scores in the list. As before, we use s ( d ; L denote the resultant normalized retrieval score of document d in L i ;if d  X  L i ,weset s ( d ; L i ) def = 0. In what follows, F ( d ) denotes the score assigned to d by a fusion method x .
The CombSUM linear fusion method [14] scores d by the sum of its retrieval scores: F CombSUM ( d ) def = CombMNZ [14, 22], which is a non-linear fusion method, further rewards documents that appear in many lists: F The Borda method [38], in contrast to CombSUM and CombMNZ, considers only rank information. Specifically, d is scored by the number of documents not ranked higher than it in the lists. Formally, if d  X  L ,weuse r ( d ; L )todenote d  X  X  rank in L . (The rank of the top most document is 1.) Then, d  X  X  Borda score is: F Borda ( d ) def = L i : r ( d ; L i ) &gt;r ( d ; L i ) } . The RRF fusion method [6], which was shown to be effective, also relies only on ranks: F parisons. We follow previous recommendations with re-gard to effective free-parameter settings for the Clarity pre-dictor [30]. Specifically, for all predictors that use the rClar-ity and sClarity measures, we set k , the cutoff at which the relevance model is computed, to n = 100, the number of doc-uments in the list; the language models of documents from which the relevance model is constructed are not smoothed; and, the number of terms used is 100. For predictors that use the WIG and NQC measures, k ,thenumberofmost highly ranked documents considered by these measures, is set to a value in { 5 , 10 , 25 , 50 , 100 } . 9
The interpolation parameter  X  (see Equation 3) is set to avaluein { 0 , 0 . 1 ,..., 1 } . The cutoff parameter c used for computing inter-list similarity with the KL approach is set to 20 following previous recommendations [3].

For predictors  X  both those proposed here and the refer-ence comparisons  X  that incorporate free parameters, i.e., k and/or  X  , we first set the free parameters to values optimiz-ing prediction quality over the query set for a track. Thus, we start by using all prediction methods with their free pa-rameters set to (generally) effective values. This practice en-ables to study the influence on prediction quality of various aspects (e.g., the inter-list similarity measure used, and the approach employed for aggregating prediction values) when the effect of free-parameter values is ameliorated. Then, in
NQC X  X  prediction of the effectiveness of a TREC run is better when retrieval scores are not normalized, while that of WIG is better when retrieval scores are normalized [31]. However, as some of the fusion methods employed require score normalization, retrieval scores in the runs are always normalized as described above. Section 4.2.5 we perform an in-depth study of the effect of free-parameter values on prediction quality. In Section 4.2.6 we present the prediction quality of our methods, and that of the reference comparisons, when free-parameter values are set using a train set of queries.

The Direct predictors, which manifest the common prac-tice of applying post-retrieval predictors upon the final re-sult list, serve as our reference comparisons. This compari-son sheds light on whether integrating prediction employed upon the lists that are fused with prediction performed upon the final result list  X  the underlying idea of our approach  X  can improve over using only the latter.
 the main factor driving the development of query-performance prediction methods is the significant variability of retrieval performance across queries [35, 15]. We now turn to study this variability for the fusion methods. In comparison, we present the variability for the best and worst (MAP) per-forming runs among the 5 fused. Specifically, we report the MAP, which is the mean of the average precision (AP) val-ues attained for the queries, and, their standard deviation. The numbers are reported in Table 1 and represent averages over the 20 samples of 5 runs.

We see that in most cases, both the MAP and the stan-dard deviation of AP posted by the fusion methods are some-what lower than those posted by the best-performing run. Yet, they are much higher than those posted by the worst-performing run, which can be quite ineffective as suggested by its MAP. Furthermore, the standard deviation of AP for the fusion methods, as well as for the best run, is very high. These results show that fusion methods, as other retrieval approaches, post performance that can substantially vary across queries.

In the evaluation presented below we focus on predicting the query-performance of CombMNZ. CombMNZ is a stan-dard baseline in many reports on fusion approaches [22, 23, 1, 26, 24, 29]. In Section 4.2.4 we study the effectiveness of our predictors for the other fusion methods.
Our first order of business is studying the general effec-tiveness of our prediction framework; specifically, that of the two main prediction methods, AMean and GMean. To that end, and unless otherwise specified, we use the KL inter-list similarity measure, as it will be shown in Section 4.2.2 to outperform the Cosine measure. As noted above, we focus on predicting the query-performance of the CombMNZ fu-sion method. Table 2 presents our main result. The numbers Table 2: Main result table. The prediction quality of the AMean and GMean methods. Prediction qual-ity, here and after, is measured by Pearson corre-lation with the ground-truth retrieval performance. (Refer to Section 4.1 for details.) CombMNZ serves for the fusion method, and the KL inter-list simi-larity measure is used. The best result per track and a base predictor is boldfaced. Underline marks the best result for a track;  X  X  X  marks a statistically significant difference with Direct. in Table 2, and those in all the following tables and graphs, represent prediction quality measured using Pearson X  X  cor-relation. (Refer back to Section 4.1 for details.)
We first observe in Table 2 that when using the base pre-dictors alone upon the final result list (Direct), WIG is the best-performing method. NQC outperforms both variants of Clarity, which can be quite ineffective for the Web tracks (TREC9 and TREC10). Yet, even in these cases, using the Clarity measures in our AMean and GMean methods can yield relatively effective prediction. Recall that, originally, WIG [40] and NQC [31] were devised for retrieval meth-ods utilizing surface-level document-query similarities. Here we see that their adapted variants, which were discussed in Section 3.4, yield relatively high prediction quality when em-ployed upon the final result list (Direct) that is the result of fusion. This finding is novel to this study.

Perhaps the most important observation based on Table 2 is that the AMean and GMean predictors outperform the Direct predictor in a vast majority of the relevant compar-isons (base predictor  X  track); most of the improvements are substantial and statistically significant. This finding attests to the merits of the underlying idea of our prediction frame-work  X  integrating prediction based on the final result list (Direct) with that based on the intermediate lists that are fused to create it. In comparing AMean and GMean we see that the former is more effective than the latter when using Table 3: Prediction quality of using the vari-ous inter-list similarity measures. ( X  X os X  stands for  X  X osine X .) The best result per track in a AMean/GMean block is boldfaced; the best result per track is underlined.  X  X  X  and  X  X  X  mark statistically significant differences with Direct and with using a uniform similarity measure (the Uni predictors), re-spectively. CombMNZ is the fusion method.
 WIG and Clarity for base predictors, while the reverse holds when using NQC.

Using the base predictors in AMean and GMean yields relative prediction-quality patterns similar to those of using these alone upon the final result list (Direct); that is, WIG (almost always) outperforms NQC which outperforms both Clarity variants. Hence, in the evaluation below we mainly focus on using WIG and NQC for base predictors. Indeed, the best prediction quality for a track in Table 2 is always attained by either a WIG-based or a NQC-based predictor. Specifically, using WIG in AMean yields the best prediction quality for 3 out of the 5 tracks. We next compare the prediction quality of using the KL, Cosine, and uniform inter-list similarity measures. Table 3 reports the prediction quality numbers.

We can see in Table 3 that in most relevant comparisons (base predictor  X  track  X  AMean/GMean) the KL measure yields prediction quality that is better than that of using the Cosine and uniform measures. (The main exception is the comparison with UniGMean for NQC.) Furthermore, KL yields more statistically significant improvements over Di-rect. The superiority of KL over the uniform measure, which is in quite a few cases (specifically, for WIG) statistically sig-nificant, attests to the merits of differentially weighting the prediction values for the lists that are fused based on their similarity to the final result list.

We also see in Table 3 that using WIG in AMean, with the KL measure, yields prediction quality that is, in general, highly effective with respect to that of the other predictors; specifically, the prediction quality is the best for 3 out of the 5 tracks as was the case in Table 2.

The Cosine measure is often less effective than the uniform measure; yet, both yield prediction quality that transcends that of Direct in almost all cases; sometimes, the improve-ments are statistically significant. All in all, these findings demonstrate the effectiveness of our framework when applied with different inter-list similarity measures.
 Table 4: Using the minimal and maximal prediction values (UniMin and UniMax) vs. using the mean of all prediction values (UniGMean and UniAMean).
 Boldface: the best result per track in a block of a base predictor. Underline: the best result per track.  X  X  X  marks a statistically significant difference with Direct. CombMNZ serves for the fusion method.
Insofar, we focused on two approaches for aggregating the prediction values assigned to the lists that are fused and to the final result list; i.e., those based on the arithmetic (AMean) and geometric (GMean) mean. We now compare these with the methods that integrate the prediction value assigned to the final result list with the minimal and max-imal prediction values for the lists that are fused (UniMin and UniMax, respectively). As UniMin and UniMax uti-lize the uniform similarity measure, we use UniAMean and UniGMean for the comparison.

We see in Table 4 that UniMin and UniMax outperform the Direct predictor in several cases. However, these pre-dictors are almost always outperformed  X  often, quite sub-stantially  X  by AMean and GMean. This exemplifies the benefit in utilizing the mean prediction value for the lists that are fused with respect to using the minimal and maxi-mal prediction values.
The evaluation presented thus far focused on predicting query performance for the CombMNZ fusion method. In Table 5 we present the prediction quality of our AMean and GMean predictors for the CombSUM, Borda, and RRF fusion methods in addition to CombMNZ. (Refer back to Section 4.1 for details.) WIG and NQC serve as the base predictors and KL serves as the inter-list similarity measure. Table 5 shows that AMean and GMean outperform the Direct predictors in most relevant comparisons (base pre-dictor  X  track  X  fusion method); specifically, across fusion methods. The best prediction quality (boldfaced numbers) attained for Borda and RRF, which use rank information for fusion, is almost always higher than that obtained for CombSUM and CombMNZ, which utilize retrieval scores.
Thus, we see that our prediction approach is effective for different fusion methods whether they are based on retrieval scores (CombSUM and CombMNZ) or ranks (Borda and RRF) and whether they are linear (CombSUM, Borda and RRF) or not (CombMNZ). Furthermore, we see that the conceptual connection between our prediction approach and linear fusion methods, which was discussed in Section 3.5, does not necessarily imply that prediction quality is better for linear fusion methods. For example, the best prediction quality attained when using NQC in AMean is always higher for CombMNZ than for CombSUM.
The AMean and GMean prediction methods incorporate two free parameters: k , the number of highest ranked docu-ments in a list  X  both for the lists used for fusion and for the final result list  X  that are considered by the base predictor employed; and,  X  , which controls the balance between using prediction based on the final result list and that based on the lists that are fused.

Insofar, k and  X  were set to values that yield optimal prediction quality for a predictor for a track X  X  query set. (For the Direct predictors, k was set to optimize prediction quality.) In Figure 1 we present the effect on prediction quality of varying their values when WIG and NQC serve for the base predictors and KL is used for the inter-similarity measure. While CombMNZ serves for the fusion method, we note that the patterns presented are the same for the other fusion methods. When studying the effect of varying the value of k we set  X  to its optimal value and vice versa. The effect of k . WeseeinFigure1thattheprediction quality of using WIG as a base predictor is relatively stable when varying the value of k . 10 Optimal prediction quality is obtained when k is set to a relatively small value. The latter observation is in accordance with findings about using WIG to predict the effectiveness of a single retrieved list [40]. The prediction quality of using NQC monotonically rises with increasing values of k , until optimal prediction quality is attained for k = 100. This finding is in line with those about using NQC to predict the effectiveness of a single retrieved list [31]. Finally, we observe that the effect of varying the value of k is quite consistent across tracks and between using AMean and GMean.
 The effect of  X  . Recall from Equation 3 that  X  = 0 amounts to relying only on the prediction employed upon the final
This also holds when k approaches 100, the number of doc-uments in a list. For k = 100, WIG X  X  prediction value for each of the 5 lists that are fused is 1 100 (see Equation 9). In this case, prediction is based on that performed upon L res and on L res  X  X  similarity with the 5 lists. result list; and, increasing the value of  X  results in more weight put on the predicted effectiveness of the lists that are fused.

When using WIG as the base predictor, whether in AMean or in GMean, the optimal prediction quality is almost always attained for  X  = 1; i.e., not using the prediction performed upon the final result list, and relying only on the predicted effectiveness of the lists fused to create it. This is a striking observation as the goal of prediction is the effectiveness of the final result list. Thus, this finding re-echoes the impor-tance of using prediction performed upon the lists that are fused so as to predict the effectiveness of the final result list  X  a key principle in our framework.

When using NQC as the base predictor, we see that the prediction quality patterns change between AMean and GMean. For AMean the patterns are not consistent across tracks, while for GMean the optimal prediction quality is often attained for  X   X  X  0 . 1 , 0 . 2 } . Thus, automatically set-ting  X   X  X  value for a prediction method and a query (and/or a corpus) is an important future venue to explore.
Finally, we see that the prediction quality for the Web tracks (TREC9 and TREC10) is consistently lower than that for the tracks using newswire corpora (TREC3, TREC8, and TREC12). This finding echoes those in previous reports about the query-performance prediction task for a single re-trieved list being highly challenging for the WT10g corpus and its accompanying queries [17]. This corpus and queries were the ones used in TREC9 and TREC10.
As already noted, the free parameters ( k and/or  X  )ofall predictors (ours and the reference comparisons) were set at the above to values that yield optimal prediction quality per track X  X  query set. This practice enabled to study the impact of various aspects of prediction (e.g., the inter-list similarity measure, the aggregation method for prediction values, the fusion method used for retrieval) while ameliorating effects of free-parameter values. In Section 4.2.5 we explored the effects of the free-parameter values on prediction quality.
Our goal now is studying the prediction quality when free-parameter values are set using a held-out query set. We randomly split the queries for a track into two equal-sized folds, train and test. Then, the values of the free param-Statistically significant differences with Direct are marked with  X  X  X . eters that yield optimized prediction quality over the train fold are used for all queries in the test fold. We repeat this procedure 30 times, to avoid irregularities rising from a sin-gle random split, and report for each predictor its average prediction quality and its standard deviation over the 30 test folds. (Recall that the evaluation of prediction quality for any query set is based on 20 random samples of 5 runs.) The prediction quality numbers are presented in Table 6 when using the CombMNZ fusion method and the KL inter-list similarity measure. Note that the reported numbers are not necessarily comparable to those presented above. A case in point, the evaluation here is based on average prediction quality for 30 samples of half of the queries in the track, while that above was based on all queries per track.
We see in Table 6 that in a vast majority of the relevant comparisons (4 base predictors  X  5 tracks), our AMean and GMean predictors outperform the Direct predictor. Fur-thermore, the prediction quality of AMean and GMean is statistically significantly better than that of Direct in 12 and 13 relevant comparisons, respectively, out of the entire 20. These finding s, again, attest to the importance of inte-grating prediction performed upon the lists that are fused with prediction performed upon the final result list (Direct)  X  the core principle of our prediction framework.
 A specific interesting observation is with regard to using NQC over TREC3, and sClarity and rClarity over TREC10. In these cases, the prediction quality of Direct, which is based only on the final result list, is extremely low. This is in part due to the fact that effective free-parameter values do not necessarily generalize across queries in this case. Yet, the prediction quality of using the base predictors in our AMean and GMean predictors can be quite higher, and to a statistically significant degree.

Another observation that we make based on Table 6 is that the standard deviation of the prediction quality of the AMean predictors is almost always lower than that of Di-rect. We note that while AMean and GMean incorporate two free parameters ( k and  X  ), Direct incorporates only one ( k ). The standard deviation for GMean is in most relevant comparisons higher than that for AMean, yet in quite a few relevant comparisons it is still lower than that of Direct. These findings attest to the improved robustness of the pre-diction quality of our framework, as measured over different query sets, with respect to that of the Direct predictors.
All in all, we see that our prediction framework is effective and quite robust with respect to the Direct predictors when setting free-parameter values using a train query set.
We addressed the query-performance prediction task for retrieval methods that are based on fusion of retrieved lists. Specifically, we presented a novel prediction framework that integrates prediction performed upon the final result list with that performed upon the lists that are fused to cre-ate it; inter-list similarity measures control the integration. The framework is generic in that it does not assume knowl-edge of the methods used to retrieve the lists that are fused, nor of the fusion method employed. Empirical evaluation demonstrated the merits of our approach.

We plan to explore whether the framework can be im-proved by exploiting knowledge of the fusion method em-ployed. Furthermore, we intend to study whether the frame-workcanbeusedforeffective meta fusion ; i.e., fusing result lists produced by different fusion methods based on the lists X  predicted effectiveness.
 Acknowledgments We thank the reviewers for their help-ful comments. This paper is based upon work supported in part by the Israel Science Foundation under grant no. 557/09, by IBM X  X  Ph.D. fellowship and SUR award, and by Google X  X  and Yahoo! X  X  faculty research awards. Any opin-ions, findings and conclusions or recommendations expressed in this material are the authors X  and do not necessarily re-flect those of the sponsors.
