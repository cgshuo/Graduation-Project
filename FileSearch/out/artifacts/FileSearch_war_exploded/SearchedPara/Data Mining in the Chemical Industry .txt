 In this paper we describe th e experience of introducing data mining to a large chemical manufacturing company. The multi-national nature of doing business with multiple business units, presents a unique opportunity for the deployment of data mining. While each business unit has its own objectives and challenges, which may be at odds with those of other units, they also share many common interests and resources . In this environment, data mining can be used to identify potential value-creating opportunities, through large site inte gration of multiple assets and synergies from the use of common assets, such as site-wide manufacturing facilities, and world-wide supply-chain, purchasing and other shared services. However, issues arise, on one hand from overly complex syst ems, and on the other hand, from the danger of reaching sub-optimal solutions, if a big enough picture is not considered when executing projects. The company-wide initiative and use of Six Sigma at all levels of the company provided a fertile ground for making the case for data mining and facilitating its acceptance. The Six Sigma mindset of measuring the performance of processes and analyzing data promotes data-based decision making, therefor e making data mining a natural extension of this methodology. We will describe the approach for launching a data mining capability within this framework, the strategy for securing upper mana gement support, drawing from internal modeling, statistical, and other communities, and from external consultants and unive rsities. Lessons learned from industrial case studies, enterprise -wide tool evaluation and peer benchmarking will be discussed. I.6.5 [ Simulation and Modeling ]: Model Development  X  modeling methodologies. Management, Measurement, Documentation, Performance, Design, Human Factors, Sta ndardization, Verification. Data mining, manufacturing, chemical industry. In the late 1990 X  X  the Dow Chemical Company launched the practice of Six Sigma methodology [4]. By now, almost everyone at Dow has been exposed to or has in some way been involved in Six Sigma. The  X  X easure X ,  X  X nalyze X ,  X  X mprove X , and  X  X ontrol X  (MAIC) phases are clearly delineated. Significant efforts and resources have gone into deve loping and delivering training materials on these topics. However, until recently, the  X  X efine X  phase was largely being practiced inconsistently. As a result, many Six Sigma projects were dela yed or terminated due to the lack of, or poor execution of, defined deliverables for this phase. Furthermore, projects that at first blush might have looked  X  X ood X , often turned out to not be viable. It became apparent that a better  X  X ata-driven X  method was needed to identify projects and generate charters with a greater potential for success.  X  X ata Mining and Modeling X  (DMM), the methodology of  X  X inding relations hips between inputs and outputs (modeling) and converting this e xploratory model into value X , was identified as a viable approach for accomplishing this and a team was formed to bring knowledge about the DMM methodology into the company and make it accessible to the Six Sigma community at large. Fortunately, Six Sigma has played a key role in promoting a mentality of continuous improvement: Its foundation is that in order to be able to fix something you have to be able to measure it and analyze it first. This mindset has provided a fertile ground for making the case of data mining and facilitating its acceptance as a natural extension to these phases of the Six Sigma methodology. Upper management in particular was much more open to give data mining a try and provide adequate resources to launch it at a significant level. Although the company shares some of the same issues and concerns as other companies when it comes to data mining, it is also different in many ways than the companies where  X  X raditional X  data mining has been largely applied of the sort done in insurance, banking, credit card, financial institutions and the large retail or on-line stores, wher e millions of transactions may take place on a daily basis. Fo r example, we only have 40,000 customers and just over 1 million shipments per year. Our  X  X ransaction X  load is no where near that of a large on-line retailer (millions per day). It is probably fair to say that much attention has been devoted by vendors to provide tools and methods to address these types of problems. And although other industries can be the beneficiaries of such developments, still there are unique requirements driving the need for a different sort of data mining approaches: In contrast to the huge terabyte data sets, for example, we generally deal with smaller gigabyte-size sets of greater variety, such as manufacturing process data, research and development data for new mate rial and product development, marketing and business data (order s, purchasing, etc), and supply-chain data of a globally distributed company dealing with multi-national regulatory issues. The company is essentially a collection of businesses, each with different (and sometimes opposing) needs and objectives, yet they share in many of the benefits that come from large scale implementation and integration, especially at the geographic site level, through pooled resources and services. The end re sult of all of this is that any widely deployed data mining methodology and tools must be general enough and flexible enough to accommodate diverse needs, in order to solve  X  X ocal X  problems effectively, while avoiding sub-optimization. Consultants were contracted to assess the existing situation, and recommend approaches for how and where a core capability should be established. Relationshi ps were also established with universities. A special arrangeme nt with Central Michigan University was setup whereby funding was provided to launch the CMURC Business Intelligence/Data Mining resource center in close proximity to company hea dquarters. This arrangement made available to the compan y both hardware and software resources as well as personnel to kick off data mining projects. Training of selected personnel on specialized software was also provided. The intent of the CMURC is to provide an  X  X ncubator X  like environment to  X  X ick the tires X  so to speak before fully investing in a data mining infrastructure. As in any investment, there are hidden costs to be concerned with. As 80% of a data mining effort is in the data preparation phase, the extent to which any given company has invested in data warehouses and data marts will determine the size of this initia l investment. Therefore, for a company like ours where the transactional  X  X oad X  is low as compared to the large retail stores , it is important to demonstrate value with low initial costs, before jumping in and investing heavily in expensive tools. A core group was created with members of diverse backgrounds drawn from different functions representing the main three chunks of the company (manufacturing, commercial, and R&amp;D). The group was chartered to develop a business plan, set standards and best practices, define infrastructure, identify and deploy enterprise-wide tools, execute large scale projects, and manage external relations. The group also launched  X  X vangelical X  type of communications campaign throughout the company (businesses, functions, and departments) and at all levels of management. The first task of the core group was to pull together various people who were already devoting a significant part of their time as data and knowledge workers (e .g., math modelers, Six Sigma  X  X lack belts X , etc.) and elevat e their knowledge of data mining through an intense training progr am. The participants were selected such that they would  X  X eside X  within various businesses, in order to seed the interest by placing knowledgeable people within major functions. After an initial search in the market place, it was decided that the kind of DMM course that woul d address the company X  X  overall needs, one that would addre ss business, manufacturing, and research &amp; development needs, was rather unique. Such a course was simply not available on the market, and the course curriculum had to be developed internally, with the help of an external consultant. The curriculum deve lopment team consisted of Six Sigma  X  X aster Black Belts X  and data mining &amp; modeling domain experts from inside and outside th e company, who in addition to developing and delivering the curri culum also were assigned as mentors to course participants. One constraint in developing the curriculum was that it would have to be built around tools that were already deployed in the company. Essentially, this was considered a pilot and it was important to do at as low a cost as possible. The expectation of successful completion of the two week-long course is that DMM practitioners would be able to perform advanced data analyses and create models that would enable them to make intelligent decisions regarding the viability of  X  X ixing X  a perceived process or product defect, and to know which part should be fixed if needed; in ot her terms, to develop good project charters --the actual  X  X ixing X  w ould still be left to the MAIC process. Still, the course was de signed at the  X 101 X  level, i.e., targeted to the relative newcomer to DMM and additional, more advanced training would need to be sought by those who intend to be practitioners in the field. The team began the curriculum development late in 2001 and delivered two  X  X aves X  of the course in 2002. Thirty five people took the course, seven of whom went on to actively put this knowledge into practice. Although it is still early to assess the full potential of this effort, 34 projects have been identified (13 of which have been initiated) with potential value of over $1 billion. The core group developed a business plan for setting up and acting on a corporate data mining effort. The key elements of the business include: the mission, pr oduct/service offering, identified customer base, need for the business, value proposition, business strategy, resource needs, time tabl e, metrics, communication plan and requirements, constrai nts, and risks. The primary outcome of the busine ss plan is the development of a long-term, sustainable, resour ce model for data mining and modeling in the company by: A very formal approach was put in place to assess projects, to determine the support model, and to assess and track value. One of the main activities of the core group is to identify, define, and promote best practices with re gards to data mining. This has to be in a manner consistent the company X  X  overall approach to establishing well defined work processes, according to what is known as  X  X ost Effective Technology  X , or MET. This calls for detailed processes, roles and responsibilities, resource models, technology, training and documentation. In order to accommodate the diverse business needs for data mining, the group developed th e Data Mining and Modeling (DMM) process (see Figure 1), based on its core set of competencies in mathematical modeling. When compared to other data mining methodologies, like CRISP-DM [22], SEMMA [21], the Virtuous DM Cycle [2], or the KDD process [7], there are both similarities and differences. DMM takes a bit deeper look at the  X  X ystem X  under study using various methods generally found in the  X  X ystems X  literature, to detail the system. Also, there is a more formal separation between pr ocess and methods used to aid certain steps in the process. It is important to note that despite its  X  X lock X  or  X  X inear X  appearance, the DMM process, much like the KDD process [7], is highly iterative and interactive. A high level description of each of the phases of the DMM process follows: The main deliverable of this st ep is a preliminary DMM project charter, which includes modifying an existing project charter that may have been handed to the data miner. Under consideration here are understanding the busine ss objectives, alignment with strategic business goals, an initial assessment of the value of the opportunity, including the costs of doing the project as well as the estimated hard and soft benef its (e.g., revenue generation, cost reduction, etc). The preliminary plan will include scope and boundaries, assumptions, constraints, risks, expected deliverables and initial translation of the business goals to data mining project tasks. The purpose of this step is ensure that the proposed data mining project is consistent with othe r projects and initiatives that the business may already have underway, or is planning to do in the near future, as defined in the business X  X  managing improvement (MI) plans as well as enterprise-wid e initiatives. This is to ensure the relevance of the proposed data mining project and also to avoid sub-optimization that could re sult if the project were to be done in a vacuum. Another outcome of this step is to identify the business success criteria, including various measurements (customer-, process-, and fina ncial-measurements), and any relevant benchmarking studies. The purpose of this step is to id entify the key stakeholders from a long list of potential candidates, including sponsors (people driving the project) and those who will fund the project as well as other decision makers and the pro cess owners. Other stakeholders may include data miners, math modelers, subject matter experts and other technical people, black belts (a Six Sigma term referring to the individuals who will actually implement the solution), and finally the people who will ultimately benefit form the work (the end-users). A desired outcome of this step is to determine hypotheses held by the key stake holders. This is important both as a means of cross-validation of project expectations, but also for formulating the data mining plan. These hypotheses are determined through interview se ssions using brainstorming and mind mapping techniques. Finally, the other objective here is to develop a communication plan --what should be communicated to whom and how often. The objective here is to review both internal documents and external literature to identify other related work on similar chemistries, systems, and work processes. Ideally, prior analysis and modeling work would be iden tified which may be useful for the proposed project in terms of lesson learned, what worked, what didn X  X , etc. The purpose of this step is to doc ument as much as possible about the system under consideration, in order to provide a meaningful context for data mining. The documentation may include system diagrams and process maps, mind maps, relationship maps, information exchange diagrams, fl ow diagrams of material flow and money flow, and envelope analysis. The objective here is to identify a ll sources of data and determine if all of the data needed for analys is is already available or if it is necessary to start a data acquisition campaign. The desired outcome is to identify the specifi c data sets (e.g., named sources and databases), to do a preliminar y assessment of any gaps in the data, identify important missing va riables and ways to remedy such, and determine if not being able to acquire missing data or variables in a timely fashion would be detrimental to the project. In addition, here we collect as much information as possible regarding the data sources, including the collection method (manual, electronic, or instrument). Here, we identify the core group of people that will be involved in the execution of the data mining project. This typically includes data miners, process and system domain experts, data content experts, and data access/ extraction experts. Given what has been found up to this point, including input from stakeholders, prior work, and what is now known about the available data, it is time to develop a specific problem statement. This is done in the form of a  X  X ro ject charter X  and templates have been developed to facilitate capturing important elements. The problem statement is reviewed w ith the process owner and other key stakeholders. This step is aimed at the detailed understanding of the data, including the sampling frequency (and if the data is from a process information data histor ian, the granularity and any filtering or smoothing that may have been done), any intentional biases (and if outliers have been omitted, the selection criteria), update frequencies, alignment crite ria, collection gaps, and any processing algorithms that may have already been applied to the data. Also, here we identify the input-and output-variables, describe the variable types and attributes, and document attribute definitions, the scale (interval, nominal, ordinal), units of measure, standard operating range s (upper and lower limits), and any real physical limits. We also determine if a given variable is measurable, controllable, random, or descriptive. Formatting issues are documented, e.g., file ty pes (flat files, relational files, delimiters, missing data indicators, etc). Finally, sources and magnitude of measurement error are identified. All of this is for building the appropriate context for mining the data and is driven by the principle of  X  X unctional paranoia X  that can best be described via a seri es of questions: What has been done to the data? Is it filtered, aggregated, calculated or measured? What is the update timing and time stamp rules? What is the data taxonomy and how does that map to the business structure? Why was the data collected? How does it fit in with the project goals? Why is it needed? Are there gaps or holes in time? Is the data available at the right frequency and the appropriate level of granularity to solve the problem on hand? What about the information content: are there lots of rows of data but not enough variation in the patterns? The purpose of this step is to asse mble all relevant sources of data and/or start a data collection campaign to secure needed data that is not yet available. This step generally requires working with database analysts who will do the actual data extraction, so it is important to be very specific a bout how the data sets should be created, including prescriptions for the time frame, level/ aggregation, frequency, scope, form at, variable names, file type, delimiters, index variables, sort-merge-stack sequences, harmonization strategy for multiple data sets, etc. The data is prepared and structured so that it may be imported into the analysis platform. Th is may involve harmonizing data from multiple data sets as well as merging and stacking the data and is often done outside of the final analysis platform through the use of desktop databases. The merged dataset is then imported for analysis. This phase essentially follows a tr aditional data analysis approach [14], [20]. Major steps include vi sual exploration and inspection of descriptive statistics for in terval and ordinal variables, assessing information conten t, assessing colinearity in independent variables and elimin ating redundant variables, and assessing variability and time characteristics. Also, we identify missing values and devise a strate gy to handle them, and identify and handle outliers. In this phase, we select the vari ables that will be used as inputs and outputs. It is characterized primarily by four aspects: imputation, feature creation, va riable reduction, and variable partitioning. Imputation techniques are used to fill in missing data, in cases where th ere is still adequate information to consider the variable as an input. Featur e creation techniques are used to create meta variables, i.e., variables not directly found in the original data but that may be derived by combining/transforming other variables that are in the data. This generally requires domain expertise to draw from physicochemical principles. Variable reduction and variable partitioning is done when there are a lot of variables. In some cases, too many dimensions may pose a problem for some of the downstream modeling techniques, so down selection becomes a nece ssity. Clustering, principal components and other multivariate techniques are used for these activities. Standard transformation techni ques (e.g., standardization, normalization, log and other transforms) are used to recode the data. Categorical data is recoded into numerical data as appropriate. The last step of the data preprocessing phase is to document the data finding and transformations and to communicate and validate these with the stakeholders. Pending acceptance, the data sets are finalized to be used for modeling. This phase is the essentially the  X  X eart X  of the DMM process consisting of two main activities: exploratory data analysis and modeling. Here, the decision is made as to what analyses and modeling techniques will be used. This is done on the basis of the type of problem on hand. For supervised type problems (i.e. when there is a response variable), the c hoices depend on whether it is a prediction, classification, estima tion, or optimization problem. For unsupervised type problem s (when there is no response variable), the choice may be clus tering, association, or linkage type models. A methods-selection decision tree in the form of an interactive mind map has been de veloped and is provided to the data miner to facilitate the selection of the appropriate analysis and modeling methods, emphasizi ng the assumptions, strengths, and limitation of each method, and providing a framework for assessing methods unto themselves as well as comparing them to one another. After the analys is/modeling methods have been selected, it may be necessary to re -format or re-structure the data accordingly to accommodate these methods. Also, it may be necessary to re-sample the data, as the data set may be too large for some techniques. The aim here is to look for patterns and themes using visualization and other technique s, such as distributions and histograms, X by Y plots, conti ngency plots, linear regression, clustering, and recursive partiti oning. If necessary, row reduction is done to enhance information c ontent. Feature extraction and dimensionality reduction is done if necessary, using techniques like principal components analysis. Finally, in the cases of highly non-linear systems, we consider the generation of alternative functional forms via genetic progr amming. Using such features can help to linearize the problem and thus make it amenable to standard techniques. The data is partitioned into training, validation, and test sets [10]. Again, depending on whether or not a response variable is present and the type of variables, differe nt techniques may be appropriate [5], but the general approach is to try linear methods first, since these provide ample tests for testing the significance of the models, then move on to non-linear models if necessary. The general suite of methods used includes clustering techniques, principal components analysis, discriminant and factor analysis, linear and logistic regression, d ecision trees, and neural networks [9], [15], [23]. For time series type models, special techniques are used [3], aimed at accounting for different sources of variability in order to identify any periodic patterns or trends in the data. Individual models are assessed for performance according to technique-specific procedures (e.g., F-test, correlation coefficient, lack of fit, root mean square error, predicted vs. actual plots, residual plots, etc), as well as the differences of model fit on training, validation and test sets. Comparison of performance between different types of mode ls is trickier (e.g., linear regression vs. neural nets), sin ce significance tests that apply to one do not always apply to the other. The stability of non-linear models in particular is checked to ensure that convergence has been reached and that the parameter estimation procedure did not get stuck at a local minimum. As is the case for the entire DMM process, this phase in particular is highly iterative; it may be necessary to cycle back to the beginning and re-build models. Finally, the best model or models are chosen and assessed against business criteria in order to identif y and validate relationships that can be explored for further opportunities for improvement. This phase is characterized by three main activities: First the developed model(s) may be used to make an immediate business decision; this in and of itself may be the extent of a particular data mining project. Furthermore, the developed model may be adequate/accurate enough to be implemented as part of an on-line or real-time system. In this case, further development may be needed (e.g. software development or re-coding in a format appropriate for the deployment pl atform). The other outcome of the data mining effort is that opportunities are identified that will need further work to be realized. These are cast as Six Sigma MAIC projects and are turned over to black belts. In this case, it is the job of the data miner to define the project charters, including assessment of the relative value of discovered opportunities, identification of uncertainties, full documentation of all data mining activities and key findings, a description of the model(s), a summary of the results, and any recommendations and identification of potential challenges. As part of the development of most effective technology (MET), we have formally launched an evaluation of various technology solutions. Independent entities lik e Gartner, Forrester Research, Frost and Sullivan, etc. have done the same over the years, but each organization has its own set of requirements, and needs to go through its own learning curve in terms of the technology. Thus, we have established a formal pilot to review the top players in the industry and will then conduct a formal assessment and choose the technology that best suites the company X  X  needs. It is important to note that we do not assume that only one technology will suite all of our needs. In fact, we will adapt a layered approach for technology. Reporting and OLAP tools are at the base of this pyramid, used by thousands of people in the company. At the next level, we ha ve mid range statistical tools. Specifically, JMP [21] is broadly used at this level, with over 3000 users. JMP does in fact ha ve some basic exploratory data analysis/data mining capabilities (P LS, neural networks, decision trees, and linear time series) and was selected as the basis for our DMM 101 training curriculum. In the JMP space there are two tiers of modelers: Those that ha ve been trained fully in basic statistics via our Six Sigma progr am, and those that have taken our entry level DMM course. At the next level, and this is where the evaluation will take place, we expect to install a single enterprise wide tool like SAS/EM, SPSS Clementine, S+ InsightfulMiner, IBM X  X  IntelligentMiner, etc., of which we expect about 50 -75 users with the appropriate training to utilize as their primary tool. Beyond that layer, we will draw from specialty packages like those found in Wolfram X  X  Mathematica or Mathworks X  MATLAB (e.g., symbolic regression, support vector machines, genetic algorithms, etc.). In this tier we only expect a dozen or so of the highest level modelers to be involved. An important part of learning how to structure a data mining effort is via collaborating with peers. The CMURC environment is designed to do just that. On a quarterly basis, companies such as The Dow Chemical Company, Ford, Eli Lilly, Steelcase, Henry Ford Health Care Systems, EDS, Kelly Services, GFS, Kitchen Aid, IBM, SAS, ESRI, Harris Inter active, etc. get together to discuss BI/DM applications, data sources, and technology trends. This allows companies to  X  X et out of the box X  a bit and generate ideas of where and how to apply data mining in their own situations. In order to better understand how to design, support, value, fund, and gain acceptance of the a data mining effort, The Dow Chemical Company, Ford, and Eli Lilly have joined with CMURC to design and launch a BI/DM benchmarking study. This study will give participants a look into various kinds of organizations and industries that are at various stages of implementation concerning BI/DM. Results of the study will be presented at the July 2005 BI Forum at CMU. The first  X  X roject X  that we did in partnership with the CMURC was an effort to link Customer Loyalty to financial impact [12]. Satisfaction/Loyalty type perception data. From 1999 to 2005, some 50+ separate studies ha ve been run across the globe resulting in a Customer Loyalty data repository of over 30,000 observations of which 2/3rds is competitive data. Considering the cost of design, collection, analysis, reporting and action, as the results of this work are used in market planning, setting service standards and feeding Six Sigma projects, Customer Loyalty can cost a company a considerable am ount of time and money. Thus, as in the case of any large comp any X  X  initiatives, the question is asked, does Customer Loyalty ma ke any difference financially? As most people associated with the Customer Satisfaction industry realize, this is in fact the holy grail. In order to establish the value proposition for loyalty, a large data mining effort was established. Data was amassed from sources ranging from perception data that included point in time market orientation assessments of the different business units, global employee satisfaction studies, customer complaint data and the customer loyalty data, to behavior data that included volume and sales trend data, pricing data, pr ofitability data, and attraction and retention data. These data sets ranged in size from dozens of variables and hundreds of observations to hundreds of variables and millions of observations. This data was harmonized with a series of complex SAS programs in order to produce a  X  X odeling X  data set. Data preparation processes included but was not limited to imputation, hostage modeling and removal, outlie r testing and removal, transformations, and smoothing. The fundamental model used was based on a blend of the theo retical framework for Customer Loyalty of Rust [19], Gale [8], Reichheld [16], Oliver [13], Johnson and Gustafson [11] and is primarily linked to the work of Rey and Johnson [17]. This fundamental hierarchical path modeling framework is well suited to the Customer Loyalty problem, but assumes that the rela tionships are all linear. Various authors have shown this not to be the case [1]. Thus, a structured neural network approach was us ed to first model the  X  X ithin study X  framework, and the  X  X cro ss many studies X  framework, and then the full  X  X ustomer loyalty-p rofit chain X  framework. This work was in fact unique in that some authors have claimed that there has never been an  X  X ccount X  level modeling effort that showed the linkage between cu stomer loyalty and financial impact. In the end, data miners would set themselves up for failure if they expect to find loyalty as  X  X he X  key driver of financial impact. In general, the global economy, regional economy, industry economy, market economy, company economy, business economy and customer economy will play a larger role in the financial landscape than loyalty alone. Breaking a financial number like profitability down to its constituent parts, one sees that there are very few aspects of profitability that can actually be affected by loyalty. In the end, margin is a function of revenue and costs. Revenue is based on volume and price. Thus, what can loyalty affect in volume, price and cost? It has been hypothesized by many authors that loyal customers bring more volume, allow higher prices and cost less to serve. In an industrial commodity market with deep and frequent price cycles like that found in the chemical indus try, this often translates into slower rate of change of volume and price for loyal customers, high account share for loyal customers, and lower costs to serve in the long run. Using primarily a cross sectional approach, and applying traditional time-lagged econometri c adjusted models based on previous years financial trends, this data mining effort did in fact show that customer loyalty perceptual measures, market orientation perceptual measur es and employee satisfaction perceptual measures do contribute, at the account level, to the explanation of financial impact in a statistically significant fashion, Rey [18]. The findings that related to perceptual vs. behavioral measures followed mu ch of what Dick and Basau reported [6]. In this paper we described our approach to introducing data mining in a large, global chemical company. Due to the unique nature of the company and the dynamic nature of the needs of its constituents businesses, a customized and targeted methodology had to be developed, borrowing beneficial aspects from published methodologies, while fine-tuning other aspects to better suit special needs. Some lessons learned include that data mining is not for the faint of heart in terms of quantitative methods, and that data preparation is an important skill set (between data extractors and data miners). Along the way, we had to deal with the ubiquitous abuse of the term  X  X ata mining X   X  it means different things to different people; anyone that has ever manipulated a spreadsheet is a self-proclaimed data mining expert, and data extractors are considered by many to be data miners. In a way, we and the vendors and the trade journals carry some of the blame  X  in our zeal to preach the virtues of data mining we often end up overselling and hyping. This often leads to unrealistic expectations. Some of the mi sconceptions at high management levels included that only the  X  X ig iron X  will do, that data mining is only for terabyte type problems, th at it is too esoteric and no one within the company knows modeling. We also confirmed our notion that while it is important to leverage external resources as much as possible, it is equally important to have experienced people internally to jump-start the process, oversee projects and keep the consultants honest. On the plus side, the widespread use of Six Sigma methods and the measuring and analyzing minds et that it promotes, proved to be catalysts for both motivating the use of data mining and facilitating its acceptance. Our thanks to Dorian Pyle of Da ta Miners Inc. for his assistance in developing and deploying the training program. We would also like to thank Jim Mentele, Tim Pletcher, and Carl Lee of CMURC, as well as our Dow colleagues Andy Paquet, Ken Beebe, Dave Rothman, and Mike Costa. [1] Anderson, E. W. and Mittal, V., Strengthening the [2] Berry, M. J. A., Linoff, G. S., Data Mining Techniques: For [3] Box, G., Jenkins, G., and Reinsel, G., Time Series Analysis -[4] Breyfogle, W. III, Implementing Six Sigma  X  Smarter [5] Dhar, V., and Stein, R., Seven Methods for Transforming [6] Dick, A. S. and Basu, K., Customer Loyalty: Toward an [7] Fayyad, U. M., Piatesky-Shapiro , G., and Smyth, P. (eds), [8] Gale, B. T., Managing Customer Value, The Free Press , [9] Hand , D. J., Mannila, H., and Smyth, P., Principles of Data [10] Haykin, S., Neural Networks: A Comprehensive Foundation, [11] Johnson, M. and Gustafsson, A., Improving Customer [12] Lee, C., Mentele, J., Gaver, Rey, T.D., Structured Neural [13] Oliver, R. L., Satisfaction: A Behavioral Perspective on the [14] Pyle , D., Data Preparation for Data Mining , Morgan [15] Pyle , D., Business Modeling and Data Mining , Morgan [16] Reichheld, F. The Loyalty Effect: The Hidden Source Behind [17] Rey, T. D. and Johnson, M., Modeling the Connection [18] Rey, T. D., Tying Customer Lo yalty to Financial Impact. In [19] Rust, Z. and Kenningham, Return on Quality: Measuring the [20] Wang, X. Z . Data Mining and Knowledge Discovery for [21] SAS Institute, http://www.sas. com/technologies/analytics/-[22] Shearer, C., The CRISP-DM Model: The New Blueprint for [23] Witten, I. H., Frank, E., Data Mining: Practical Machine 
