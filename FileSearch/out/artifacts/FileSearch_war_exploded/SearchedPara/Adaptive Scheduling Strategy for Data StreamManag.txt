 Data stream is a sequence data tuples with large volume and the the data arrival is busty without any discipline. The dat a rate fluctuates over time. Recently, due to its distinguishing characteristics and growing demands from a wide range of applications, the continuous query processing system, named Data Stream Management System (DSMS), over dat a streams has recei ved more and more attention. There are a large various representative applications of DSMS, such as sensor network, network traffic analysis, financial ticket[1], electrical train service[2], and so on. These applications have motivated a considerable body of research ranging from algorithms and architectures for stream processing to a full-fledged data stream processing such as Telegraph[3], STREAM[4], Aurora[5], Niagara[6], Tapestry[7], and Tribeca[8].

Due to the special features of data stream, the DSMS differs from a traditional static database in the following two ways. (i) the input data tuples are high volume with intermittent busts. (ii) the system should provide fast response time[2], [9], [10], [11], [12]. So it is not appropriate to apply DBMS to these applications. These unique characteristics compared with traditional database motivate us to develop newer techniques and methods in order to process input data efficiently and effectively. DSMS just is developed to solve these tricky problems, only by scanning these data one time or several times according to their arrival order. At the same time, under the memory constraint, the DSMS reduces the data latency with all its eff orts. The scheduling strategy is a key factor that impacts the system performance evidently. In this paper, we mainly focus on the operator scheduling problem in DSMS.

The scheduling strategy determines w hich operator needs to be scheduled at each time unit in multiple, continuous query processing system. DSMS scheduling strategy has two main objects: memory consumed and data latency, first DSMS must reduce the memory requirement because the input volume is large, and it is impossible to store all these data in computer memory, second, applications re-quest the DSMS must response the query within the time limitation of applica-tions. In fact, the two metrics are trade off. Some scheduling strategies have been proposed in some literatures. However, most of them ignore the variety of input data. Our main contribution is designing two novel adaptive scheduling strategies by combining Chain and FIFO strategies, which adjust process strategy according to input load and system free resources. The outcomes of simulation experiment indicate our strategies indeed surpass o ther existed competing strategies.
The rest of the paper is organized as follows. In section two, we discuss two important strategies (Chain and FIFO) and another competing strategy such as Chain-Flush, and these strategies will be compared with our novel strategies in experiment section. In section three, we construct the model of DSMS. Some assumptions and notions are defined also. In section four, new strategies and some conclusive discussion are shown. In the next section, we design simulation experiments and compare the performance of our strategies with other existed strategies. Conclusion and more discussion is in the last section. In this section we will discuss several previous strategies. First we introduce Chain [11], which is nearly optimized in memory usage. Second we introduce FIFO, which is latency optimized strategy. Both Chain and FIFO strategies of theory merit are referred to by our novel strategies. However, due to some bad characteristics, nearly no real DSMS adopts FIFO or Chain alone. At last we will show another strategy Chain Flush of real system, which will be compared with our novel strategies.
 Chain Strategy: At any time instant, consider all tuples that are currently in the system. Of these, schedule for a single time unit the tuple that lies on the segment with the steepest slope in its lower envelope simulation. If there are multiple such tuples, select the tuple which has the earliest arrival time[11].
The difference between the Chain Strate gy and the strategy which consumes the least memory is only a constant [11]. The problem of the optimization of consumed memory is NP-complete [11]. Therefore, Chain Strategy is our best choice unless P = NP , if the system memory is limited. However, there is nearly no system adopting this strategy alone, due to its severe tuple latency. An example will be shown following to make these metrics clearly.
 FIFO Strategy: Tuples are processed in the order that they arrive. A tuple is passed through both operators in two consecutive units of time, during which time no other tuple is processed.

It is not hard to justify that FIFO is the best strategy in data latency [9]. But, it ignores the different selectivities of different operators on one operator path and just process corresponding to their arrival order. Though the data latency is optimized, there is nearly no system is available to satisfy the high memory requirement.
 Chain Flush Strategy: First, assume th ere is a latency constraint assumed for each data tuple. The Chain Flush Strategy proceeds e xactly like Chain until deadline constraints become tight, forcing a change in strategy. For example, there are n tuples totally in the system, and the i th tuple is on the verge of missing its deadline. At this point, no more processing on the later arriving tuples can be performed until the i th tuple and all earlier tuples have been com-pletely processed. For more details, ple ase refer to literature [11]. Chain Flush Strategy is applied in real systems, designed by Stanford University. But Chain Flush ignores the variation of input load and system free resources. The perfor-mance comparison with novel strategi es will be shown in section five through the obtained data of simulation experiment. In this section, the model of DSMS is constructed and some important assump-tions and definitions are introduced mainly after [11].

Data stream systems are characterized by the presence of multiple continuous queries [13],[14],[15],[16]. Tuple is the minimum unit of input streaming data. Query execution can be conceptualized as a data flow diagram as in [14], which is a directed acyclic graph(DAG) of nodes and edges, where the nodes are pipelined operators that process tuples and edges represent composition of operators. An edge from node A to node B indicates that the output of operator A is an input to node B. The edge (A, B) also represent an input queue that buffers the output of operator A before it is input to the operator B. Input data streams are represented as leaf nodes that have no input edges and query outputs are represent as root nodes that have no output edges[1],[3],[9],[10].

As mentioned earlier, we assume that all operators execute in a streaming or pipelined manner. Operators like select an d project naturally fit in this category. Every tuple that enters the system must pass through a unique path of operators, referred to as an operator path. And multiple continuous queries input can be divided into several independent single stream queries [11]. Therefore, we only consider the characteristics of a singl eoperatorpath.ThereistheFigure1 following to show the structure of DSMS and operator path clearly. There are two operator path: the first is op 1 ,op 3 ,op 4 and the second is op 2 ,op 3 ,op 4 .
The operators that we consider act like filters, which operate on a tuple and produce s tuples, where s is the selectivity of the operator. The selectivity s is less than 1 for select and project operators, but it may be greater than 1 for a join operator. For the rest of the paper, we view the operator as a filter that produces s tuples on processing 1 tuple. Obviously, the selectivity assumption does not hold at the granularity of a single tuple but is merely a convenient abstraction to capture the average behavior of the operator. In terms of this model, the goal of operator scheduling is to minimize the total memory requirement and reduce thetuplelatencyatthesametime.

Selectivity and per-tuple processing time are the most important parameters of operators. Obviously, it is important to consider the different sizes of a tuple as it progresses through its operator path. We assume that the selectivities and per-tuple processing times are known for each operator [14] and capture this using the notation of a progress chart, illustrated in Figure 2.

The horizontal axis of the progress chart represents time and the vertical consisting of n operators, where the i th (1  X  i  X  n )operatortakes t i  X  t i  X  1 units of time to process a tuple of size s i  X  1 at the end of which it produces a tuple of size s . The selectivity of operator i is s i /s i  X  1 [11]. From the progress chart, we can obtain the lower envelope simulation for the progress chart through making the convex curve, which just encloses the lower side of the progress chart. The lower envelope simulation cuts the progress c hart into several segments. Segment will be important element in the following scheduling strategies. In figure 2, progress chart and lower envelope simulation are shown clearly; the lower envelope is represented by a dashed line and it cu ts the path into three segments. In this section, we first present an poten tial example, which gives us some inspi-ration. Then two novel scheduling strategies are introduced. 4.1 A Potential Example As mentioned above, the Chain Strategy is the memory consumed nearly op-timized strategy and the FIFO Strategy can obtain the minimum data tuple latency. But, both Chain and FIFO have their own advantages and disadvan-tages. We should consider Chain and FIFO strategies together to achieve our objection, which guarantees consum ed memory not exceed the upper limit of DSMS and reduce streaming data latency at the same time. Following is an in-teresting example, which will give some inspiration on how to combine the two strategies.

Assume an operator path are composed by there operators ( denotedO 1 ,O 2 ,O 3 ) will not output any data. The time cost for each operator to process one unit data system, the arrival times distribute as are shown in the following Table 1. We pro-pose two strategies to deal with these tuples (suppose S 1 ,S 2 ), S 1 processes the first five tuples by Chain Strategy and the other six tuples by FIFO Strategy; in-versely, S 2 use FIFO Strategy to deal with the first five tuples and the left by Chain Strategy. The following Table 2 and Table 3 show their memory consumed and data tuples latency.

From the above two tables, we can make the conclusion that the second strat-egy S 2 not only reduce the maximum memory requirement, also get much lower tuple latency than strategy S 1 . An intuitive explanation is that: When the input load is light, strategy S 2 adopts FIFO to get a better tuple latency, and while the input load is heavy it uses Chain to reduce the consumed memory. This fact inspires us to proposal two novel strategies based on the input data distribution. 4.2 Novel Strategies Before our new strategies are introduced, some useful notations are defined as follows:  X  M is the upper limit of system memory, the consumed memory should never  X  m represents the currently consumed memory of DSMS.  X  s i represents the selectivity of the ith operator.  X  t i represents the time for the ith operator to process one tuple and there are  X  t,  X  are the parameter of DSMS, whose values are to be determined  X  f is an approximation of the data arrival rate.  X  size is the memory consumed by one tuple.

Now, we introduce the first strategy, Adaptive Switch Chain and FIFO Strat-egy (ASCF). ASCF process input streaming data by FIFO Strategy to get a good latency if the input load is not tight and free source is abundant, else it will execute Chain Strategy to save memor y. The following expression (1) is an approximation of the input load and system sources. If the inequation is satis-fied, that means the indicates input load is not heavy and the system left free sources is abundant, thus FIFO Strategy is executed, or else Chain Strategy will be executed to save memory. The advantage of ASCF is that ASCF is a kind of adaptive strategy, and it will change policy along with fluctuation of the input data.
Then we introduce the second scheduling strategy. It has been proved that if the slope of segments along an operator path is decreased [11], the last sev-eral segments of an operator path will cause severity data latency but have little contribution to reduce consumed memory. Therefore, a good data latency and consumed memory can be obtained through modifying the lower envelope and combining the last several segments into one. The more segments are com-bined, the lower data latency can be obtained but more system memory will be required. Obviously, that how many segments of an operator path should be combined depends on the system free recourse and input load. Before to do the determination, we define some variable: K i = n j = i s j T proximation of the proces s velocity if segments i, i +1 , ...n are combined. k i is the process velocity of the i th segment. The second strategy, Chain with Segments Combined Strategy (CSC), is proposed base on the following two conditions, which approximately computes the system free resources and input data load. We get l and i by solving expressions (2) and (3). Denote w equals to max ( l, i ). At last we determine the last w segments (e.g. operator w, w +1 , ..., n ) combined. Through the above expressions, the more free sources and lighter input load, the more segments will be combined. The performance will be evaluated in simula-tion experiment. To better evaluate our newer strategies, various simulation experiments have been designed to check the metrics and performance of these strategies under the same environment. All the input data are generated randomly. The DSMS separately executes the two new strategies, and other competing strategies under the same environment.

First step, the experiment environment is set up as follow:  X  all the distributions of the input data of different strategies are uniform.  X  Our designed system adopts the Poisson Stochastic, which has a broad ap- X  The number of operator path, operator and the selectivity of our system are  X  The total input data volume is more than 200 tuples, while the memory limit  X  The parameter of the strategies  X  is set to 0 . 6and0 . 8 separately, but the
To evaluate our novel strategies, test the performance and show an impartial comparison with other strategies, three kinds of experiments are proposed: First, compare ASCF, CSC with the important basic strategies Chain and FIFO; the second kind experiment will vary the parameter of each strategy but still under the same environment; At last, the comparison novel strategies with strategy of real systems will be given as a convictive proof that the proposed strategies possess practical worthy. 5.1 Compare with Basic Strategies Due to the importance of Chain Strategy and FIFO Strategy, First, the memory consumed and data latency of novel strat egies and basic strat egies are compared and we lay out the outcome by the following charts, Figure 3 and Figure 4.
Just as we predict above, ASCF and CSC reduce the data latency as possible as they can and not break the memory constraint. Avoid breaking down the system memory, DSMS gets good data latency at the same time. It is more practical and valuable to use them in real DSMS. 5.2 The Influence of Important Parameter Parameter  X  is a key factor in our strategies t hat will affect the metrics signifi-cantly. From the following experiment, it is meaningful to make the conclusion that the smaller parameter  X  is set, the less memory is consumed but the data latency increase at the same time. To better evaluate the affection caused by parameter,  X  is set to 0 . 8and0 . 6 respectively. The per formance comparison is depicted in the following graphs under different parameters with the same strategy and the same environment. It is easy to analysis the difference when the parameter changes. It is available and convenient for designers of DSMS to adjust the strategy on the base of different environment by varying parameters. 5.3 Compare with Chain-Flush Chain-Flush is an excellent scheduling strategy for DSMS which has been in-troduced in section three. For more deta ils please refer to literature [11]. Here we compare the performance of Chain-Flush with ASCF and CSC. According to the following result, ASCF and CSC use less maximum memory usage than Chain-Flush and get much lower data latency. Because at first when data load is not heavy, Chain-Flush adopt strategy to save memory but when the data load is heavy it make effort to reduce latency. Chain-Flush strategy is passive in this situation. This is a good evidence to indicate the merits and benefits of ASCF and CSC strategies. Most of previous research work pays much attention to the architecture of DSMS, query language and so on and ignored the scheduling problems. However, dif-ferent scheduling strategies can affect th e system performance significantly. It is quite important and challenge for researcher to design scheduling strategies for DSMS. Most important two strategies are Chain and FIFO. Chain is nearly memory optimized, and FIFO is tuple lat ency optimized. Through combining Chain and FIFO, we have presented two new adaptive scheduling strategies, ASCF and CSC, which will adjust scheduling strategies according to the distrib-ution of input data. The next step work includes doing simulation experiment to compare performance of different strategies with the same input data and com-putation ability. The comparison of novel strategies and other strategies shows the excellence and merit of our strategies.

Besides the memory consumed problem and data latency problem, there are still many problems worthy of attentions. Some problems are as below, which will be our future work.
 Starvation problem: In many static priority systems, some low priority tuples may stay in system too long without processed. Because the priority will never change. Even if there is only one tuple with higher priority, the lower priority tuples will not be executed. ASCF and CSC successfully avoid this problem due to their adaptation and agility. If it is possible, they will assign free source to reduce the data latency to avoid starvation.
 Scheduling overhead problem: If each new tuples enter the system, DSMS will reschedule the tuple, these scheduling strategies themselves will exhaust CPU resource. ASCF and CSC both are operator-based strategies not tuple-based and in most DSMS the number of operators is not very large, which can still be sustained according to the computation ability of modern CPU. Therefore the scheduling cost will not depress the performance of system severely. Dynamic priority problem: ASCF and CSC belong to the kind of adaptive strate-gies and the priorities of operators are different according to the varying input data and system free source. Therefore ASCF and CSC possess more flexibility and can solve the starvation and long waiting problem.
 This work is supported by the National Science Foundation of China under the grant No.60533020. We would like to thank the anonymous referees for their useful suggestions to improve the presentation of this paper.

