 Online reviews are a cornerstone of consumer decision mak-ing. However, their authenticity and quality has proven hard to control, especially as polluters target these reviews to-ward promoting products or in degrading competitors. In a troubling direction, the widespread growth of crowdsourc-ing platforms like Mechanical Turk has created a large-scale, potentially difficult-to-detect workforce of malicious review writers. Hence, this paper tackles the challenge of uncov-ering crowdsourced manipulation of online reviews through a three-part effort: (i) First, we propose a novel sampling method for identifying products that have been targeted for manipulation and a seed set of deceptive reviewers who have been enlisted through crowdsourcing platforms. (ii) Second, we augment this base set of deceptive reviewers through a reviewer-reviewer graph clustering approach based on a Markov Random Field where we define individual poten-tials (of single reviewers) and pair potentials (between two reviewers). (iii) Finally, we embed the results of this prob-abilistic model into a classification framework for detecting crowd-manipulated reviews. We find that the proposed ap-proach achieves up to 0.96 AUC, outperforming both tradi-tional detection methods and a SimRank-based alternative clustering approach.
 Categories and Subject Descriptors: H.3.5 [Online In-formation Services]: Web-based services Keywords: crowdsourced manipulation; deceptive review; amazon; crowdsourcing site; review site
Reviews are a ubiquitous component of online commerce  X  from hotel and travel booking sites (e.g., Expedia, Trip Advisor and hotels.com) to e-commerce sites (e.g., Ama-zon and eBay) to app stores (e.g., Google Play and Apple X  X  App Store). Online reviews provide a voice for customers c  X  Figure 1: An example deceptive review task posted to a crowdsourcing platform. to praise or criticize a product or service, often in con-junction with a star rating, providing helpful information to future potential customers. Unsurprisingly, research has shown that product review ratings are, indeed, correlated with sales [4, 10].

Naturally, there is an incentive to pollute these reviews to-ward promoting one X  X  products or in degrading one X  X  com-petitors. This pollution has been identified as a growing threat to the trustworthiness of online reviews by major media and by the research literature [7, 20, 22, 25, 26]. Recently, Ott et al. suggested that up to 6% of reviews on sites like Yelp and TripAdvisor may be deceptive [22]. These reviews can have serious consequences: a deceptively promoted weight loss supplement identified as part of this project led one customer to write: . . . these pills made me really sick, palpitations, increased heart rate and troubled breathing. I requested a return, and the response I got from the company selling these horrible pills was very rude. . . .
In a troubling direction, the widespread growth of crowd-sourcing platforms has created a new attack vector for pol-luting online reviews. Crowdsourcing platforms like Ama-zon Mechanical Turk have been hailed for their effectiveness in intelligently organizing large numbers of people [3, 11]. These platforms however also enable a large-scale, poten-tially difficult-to-detect workforce of deceptive review writ-ers [17, 28]. To illustrate, Figure 1 shows an example task that asks each worker to leave a high product rating and a  X  X keptical and realistic but positive X  review. Figure 2 shows a sample of two deceptive reviews written in response to this type of crowdsourced task. Compared to traditional spam bots that typically leave identifiable footprints [15, 16, 18], these human-powered deceptive reviews are inherently dis-tinct, linked only by their common theme and not in com-mon keywords, phrases, or other easily identifiable signals. And since crowdsourced deceptive reviews are generated by humans rather than bots, their ongoing detection is even times to lose some weight, but this is the only thing that really worked for me ! You have to try it, this is money well spend . strong for the first two days but then my body got use to it and I X  X  losing weight Figure 2: Two crowdsourced reviews for a weight-loss product sold by Amazon. more challenging since crowds can actively circumvent de-tection methods.

Unfortunately, there is a significant gap in our under-standing of crowdsourced manipulation of online reviews and effective methods for uncovering such manipulation, due to a number of challenges. One critical challenge is a lack of clear ground truth for analyzing deceptive reviews and in building countermeasures; it is difficult to ascertain which reviews are deceptive and which ones are legitimate. A sec-ond important challenge is that polluters may rely on multi-ple communication channels to coordinate their activities  X  including private methods such as email and instant messen-ger which are difficult to observe  X  and so deceptive intent may be further obscured. To tackle these issues, we propose a three-part effort:  X  First, we propose a novel sampling method for identify- X  Second, we augment the seed set of sampled deceptive  X  Finally, we embed the results of this probabilistic model
In this section, we summarize previous research work re-lated to deceptive reviews and crowdsourced manipulation.
First of all, in the deceptive or spam review research field, researchers have analyzed the growth of deceptive reviews over time and have studied how to detect these reviews. For example, Ott el al. [22] have reported that the number of deceptive reviews has grown across multiple consumer-oriented review sites. Danescu-Niculescu-Mizil et al. [5] suggested an underlying model for measuring review quality. They reported that when controversy of a product was high, some of the reviews whose ratings were different from the average rating of the product often got higher helpfulness scores. Other researchers discovered distinguishing patterns between spam and legitimate reviews from product review ratings [9] and temporal distribution of spam reviews [29].
A related line of research has studied coordinated groups of review spammers. Lu et al. [19] found that the social con-text of reviews was useful to find groups of review spammers. They showed that one can assess the quality of reviews with higher accuracy when one assumed the quality of a reviewer depended on the quality of her peers in the social network. Mukherjee et al. [21] found that analyzing groups of review spammers revealed clearer evidence of spam reviews than analyzing an individual spam review. Ott et al. [23] showed that linguistic features were not reliable for humans to dis-tinguish between deceptive reviews and legitimate reviews. The proposed sampling method, reviewer clustering, and de-ceptive review classifier in this paper complement these ex-isting approaches.

Next, researchers have begun to study the crowdsourced manipulation problem of spreading manipulated contents to target sites such as social networking sites, review sites, and search engines [17, 27, 28]. Wang et al. [28] analyzed the use of two Chinese crowdsourcing platforms, and esti-mated that 90% of all tasks were malicious tasks. Lee et al. [17] analyzed tasks in Western crowdsourcing platforms, and found that the primary targeted systems were online social networks including review sites (56%) and search en-gines (33%). But, the previous research work did not specif-ically focus on a crowdsourced deceptive review problem. Researchers expressed that labeling fake reviews by human is sometimes hard [13]. Our research is the first work to link crowdsourced deceptive review tasks to target products, and analyze these deceptive reviews and behaviors of deceptive reviewers toward building a deceptive review classifier. Our ultimate goal is to identify deceptive online reviews. We formulate this problem as a classification problem, where the goal is to assign a class label of deceptive or legitimate to a candidate review r based on a classifier c :
The first critical challenge to building an effective classi-fier is in identifying valid ground truth training data. Typi-cal methods include: (i) rule-based heuristics  X  like labeling as deceptive all reviews containing a particular keyword or some other intuitive signal  X  however, such methods are brit-tle to changes in the strategies of deceptive reviewers; and (ii) asking human labelers to assess reviews, however these labelers typically do not know the intent of the original re-view writer, and so the labels may be in error. In contrast, we propose instead to sample directly from crowdsourcing platforms that publicly advertise review tasks.
Our sampling strategy is first to collect tasks from crowd-sourcing platforms that require workers to post a review on a target site (e.g., Amazon, Yelp) akin to the example task in Figure 1. By linking these tasks to products in a target site, we can then sample the products targeted, the reviews associated with the product, as well as the reviewers who contributed the reviews. The overall sampling framework is illustrated in Figure 3. Figure 4: Cumulative Distribution Function of re-ward per task targeting Amazon.

In particular, we sampled tasks posted to three crowd-sourcing platforms over a span of several weeks in 2013: RapidWorkers.com, ShortTask.com, and Microworkers.com. Each of these platforms supports a variety of tasks, typi-cally offering on the order of $0.25 per task completion. We sampled all tasks on these platforms that target Amazon. Figure 4 presents a cumulative distribution function (CDF) of reward per task targeting Amazon. The average reward per task was $0.31, and the biggest reward per task was $3.99. There were various tasks targeting Amazon such as manipulating the number of clicks of a product page and the helpfulness of a review, downloading/purchasing a product and writing deceptive reviews. Among these tasks targeting Amazon, the majority tasks were to write deceptive reviews.
To study crowdsourced manipulation of reviews on Ama-zon, we extracted the Amazon URL from each task which links to a product page on Amazon, collected the product page, all reviews associated with the product, and all re-viewers X  information. In total, we identified 1,000 products on Amazon that had been the target of deceptive reviews. We call this the root dataset .

We augmented this base set of products, reviews, and re-viewers via a breadth-first search crawling method. Specif-ically, we collected the previous Amazon reviews of each reviewer in the root dataset and the Amazon product pages those reviews were associated with. We subsequently col-lected all of the new reviews and new reviewer profiles from these product pages. In total, we crawled out three hops from the root dataset to identify a larger candidate set of potential deceptive reviews, reviewers, and targeted prod-ucts. In total, this expanded dataset includes 71.1k reviews, 14.5k reviewers, and 48.9k products.
Given the expanded dataset consisting of reviewers and products, our next goal is to reveal hidden linkages among the reviewers towards uncovering previously unknown de-ceptive reviewers who participate as part of hidden groups of coordinated manipulation. The original sampling method identifies products (in the root set) for which we are cer-tain reviewers have targeted for manipulation. However, many efforts to pollute reviews may rely on private com-mand and control (and so be out of reach of our sampling method) [28]. Can we exploit the linkages among reviewers and products to identify these hidden deceptive reviewers? This section presents a Reviewer-Reviewer graph clustering approach based on a pairwise Markov Random Field (MRF) that defines individual potentials (of single reviewers) and pair potentials (between two reviewers). In addition to pro-viding evidence of coordinated manipulation, the output of this clustering approach can be integrated as an additional feature for building the deceptive review classifier.
Our root dataset indicates that the payoff for a single fa-vorable review is generally less than a US dollar, so users are likely to write multiple reviews (about 5 each, in our dataset). As we assume that some reviewers cooperate with one another in targeting certain products, we build a reviewer-reviewer graph to capture these subtle dependencies, and detect  X  X on-crowdsourcing X  deviant reviewers.

Our expanded dataset can naturally be represented as a Reviewer-Product graph G R,P . The Reviewer-Product graph is a bipartite graph of consumer products connecting reviewers with products, with edges representing reviews. Using this information, we can construct the Reviewer-Reviewer graph G R,R := ( R,E,W ) where the vertex set R denotes the reviewers who wrote reviews on Amazon and the weighted edges (arcs) in E indicate how many pairs of reviewers have reviewed the same item during a short time window. Finally, W is the set of edge weights. Edge weights are calculated according to the weighting function w : E  X  W , as follows. w R,R ( a,b ) := where N R,P ( a ) is the set of products rated favorably by a , t ( a,p ) is the time when a reviewed p , and finally T is a constant time window size which was set to three days in our case. In essence, the numerator captures reviewer similarity by counting the number of products both a and b rated favorably in the same time window.
Following the projection of G R,P onto G R,R we prune edges whose weight denominator is 1.0 and then vertices with degree zero from G R,R . The former is to avoid cre-ating a strong ( w = 1 . 0) collaboration edge between two reviewers due to a single review co-occurrence.
Given this Reviewer-Reviewer graph, we now describe a probabilistic approach for clustering reviewers (nodes) to-ward identifying groups of similar reviewers. We formu-late this approach using a MRF defined over the Reviewer-Reviewer graph G R,R . Each node in the random field corre-sponds to a reviewer and is a discrete random variable whose value is the cluster the corresponding reviewer belongs to.
The proposed approach captures the following intuition:  X  If two reviewers have collaborated heavily, they should  X  Toward creating cohesive clusters, we additionally re-
We model reviewers using two potential functions, captur-ing the intuitions above. The first is a singleton potential , as it is defined per each reviewer  X  j . The other type of function is per pair of reviewers  X  pair , and hence called pair poten-tial . The singleton potentials have higher value when the re-viewer features are close to that of the mean cluster features in the feature space. The pair potentials are higher when two connected reviewers in the graph are assigned to the same cluster. We aim to determine the most likely cluster assignments given the Reviewer-Reviewer graph and these potential functions are used to factorize the probability dis-tribution: where D stands for observations from the data, and is bolded to signify a set of random variables; Z represents cluster as-signments which is not observed (hidden data);  X  represents the parameters of the model;  X  j ( Z j ) is the potential func-tion over the labeling of reviewer j and  X  pair is over pairs of labels. Since the MRF is wholly conditioned on the obser-vation data D , it is an instance of a Conditional Random Field (CRF). Next we will describe the factors (potentials) in our model.
 Singleton Potentials. The singleton potential function is designed to capture how well an individual labeled reviewer corresponds to the cluster in which it belongs. Formally, we Figure 5: The independence assumption of the vari-ables during calculation of singleton potentials  X  . The indicator variables P correspond to each prod-uct and indicates whether that product was re-viewed favorably. We assume the probability of that only depends on which cluster Z the author belongs to and the preference of that cluster  X  . F represents various features of the authors in the cluster. define the singleton potential as:  X  ( Z j ) = Pr( Z j , where the variables F j and P j are the observed data and Z is the hidden variable (cluster of reviewer j ).  X  represents the model parameters. F j s are the features of a reviewer j . P s are the products reviewed by reviewer j . In practice, we can adopt any of a number of features that may be good candidates for distinguishing between deceptive and legiti-mate reviewers. In this paper, we adopt the following six features F j s in the singleton potential function:
We additionally make the assumption that feature val-ues are conditionally independent given the cluster (as il-lustrated in Figure 5, which shows a graphical model of our independence assumption). As a result, the conditional probability of the observed data is: Pr( F j , P j |  X ,Z j ) = Y where the symbol  X  stands for adjacency in the graph.
We can model the binary variables (e.g., Real Name , Help-fulness ) as Bernoulli random variables:
Since we find that the length of reviews has a log-normal distribution, the average log-length of reviews of a reviewer follows a Gaussian distribution which is denoted by L in the following:
Note that we do not require that each cluster of coordi-nated reviewers be equally likely to review all the products. Rather, we assume that each reviewer has concentrated on writing reviews for a subset of the products in the cluster. This is modeled by the second part of the singleton potential function. Given the cluster, there is a probability distribu-tion over products  X  c where product i has the chance  X  c,i being reviewed by a reviewer in a cluster c . This part of the singleton potential tends to push dissimilar set of reviewers into separate clusters.
 Pair Potentials. The second potential function of the MRF likelihood formulation is the one between pairs of re-viewers. The role of this function is to force that reviewers who have collaborated on writing favorable reviews, end up in the same cluster. In addition, if the number of clusters is larger than the actual underlying number of hidden groups, this potential function will regulate the resulting clustering where too many distinct clusters with edges between them are punished. The likelihood of two adjacent reviewers to be in the same cluster in the graph depends on a tunable parameter  X  and on the weight of collaboration between the two reviewers w R,R ( j,k ). By increasing the value of  X  , connected reviewers are more likely to be in the same cluster.
Based on the probabilistic modeling part described in the previous section, we now have the following:
We aim to maximize the likelihood L ( Z ,  X  ; D ). That is, we need to see what sort of cluster parameters  X  and cluster membership Z fit our evidence (i.e., our reviewer-reviewer graph) D the most. A common way to deal with maximiz-ing likelihood functions which depend on hidden data ( Z ) is to use a local optimization method like Expectation Max-imization (EM). EM is a popular method for missing data problems and has been successfully applied to similar MRF models in other domains [2]. EM iterates over two steps to increase the likelihood [6]. However it might get trapped in a local maximum. Therefore, we use a variant of EM, called hard EM with random restarts, which consists of the following steps: 1. Initialize parameters  X  0 to random values 2. While parameters have not converged:
In the following, we present our parameter initialization and EM steps.
 Parameter Initialization. The p c parameters for binary features are sampled from a Dirichlet distribution Dir([  X , X  ]).  X  in this case is a hyper-parameter for the p c parameters. We pick  X  values relatively large so the sampled p c  X  X  don X  X  end up being close to the extremes (0 or 1). Instead they will be (in this case) close to 0.5 with a little perturbation. The ini-tialization distribution for  X  c,i is similarly Dir([  X  1 where P is the number of all products and  X  i s have the same value. For large  X  the resulting  X  c,i is close to uniform distribution with a little perturbation which is what we de-sired. The  X  c s of review lengths are initialized uniformly in the range of [0 , max { log(review length) } ]. The parameter  X  for review lengths is fixed at 1.
 E-Step. In this step we should assign cluster labels so the following log likelihood function is maximized.
We adopt an Integer Program formulation of this prob-lem by Kleinberg and Tardos called Uniform Metric La-beling [14]. For each node j (reviewer in the graph) we define an indicator variable x j,c . If x j,c = 1, it indicates node j is assigned to cluster c . In the pair potentials part of the summation, for each edge ( j,k ) the variable d
P c  X  C | x j,c  X  x k,c | is the binary distance between the as-signed clusters of j and k , where 0 means identical clusters and 1 is different clusters. For each edge of the reviewer-reviewer graph, we then have the potential: log  X  pair ( Z j ,Z k ) = w R,R ( j,k ) ( d jk log(1  X   X  ) + (1  X  d
The Integer Program formulation can be relaxed to a Lin-ear Program: Minimize
X Subject to Once the optimum x jc are calculated, we pick the c with the highest x jc as the cluster assignment for reviewer j . M-Step. In the M-Step we update model parameters  X  with their maximum likelihood estimate (MLE) given the cluster assignments. The MLE estimates can be simply determined using frequency counts.

Pr( Z = c ) = In the denominator, S is the normalizing factor so P i  X  P 1. Similarly, the value of  X  c is updated as follows. Similarly, the values for p c s for various binary features of clusters can be determined with frequency counts.
To summarize, our goal toward identifying deceptive re-views (and reviewers) is based on the following logic. First, we sample known products that have been the target of de-ceptive reviews by crawling several crowdsourcing platforms. We call this the root dataset . We augment this base set of products, reviews, and reviewers via a breadth-first search crawling method to identify the expanded dataset . Given this expanded dataset, we aim to reveal hidden linkages among reviewers via a probabilistic clustering approach. The out-put of this clustering are groups of reviewers who often posted reviews to same products. In addition to providing additional evidence of coordinated manipulation, the output of this clustering approach can be integrated as an additional feature for building the deceptive review classifier. In this section, we evaluate two main sets of experiments. First, we evaluate the reviewer clustering method over: (i) a synthetic dataset to validate the method X  X  capacity to uncover groups of coordinating reviewers; and (ii) the ex-panded dataset to determine if meaningful groups of de-ceptive reviewers can be discovered from a real Amazon dataset. Second, we build a deceptive review classifier by using the output of the clustering algorithm as an addi-tional feature and evaluate its performance versus a base-line without knowledge of the clustering output. We then compare this cluster-aware deceptive review classifier ver-sus one based on an alternate clustering method (SimRank + k-medoids) to further validate the design choices in our reviewer-reviewer clustering method.
Our first goal is to understand how well the proposed clus-tering method performs. This is an important step for con-necting known deceptive reviewers to potentially unknown ones. We first consider a synthetic dataset designed to con-tain natural groupings; does the proposed clustering method recover these underlying groups? Based on these observa-tions, we then apply the clustering method over the expanded dataset based on 71.1k reviews, 14.5k reviewers, and 48.9k products. Do we discover groups engaged in coordinated manipulation?
We begin by considering a synthetic dataset that is gener-ated according to the same process the model in Section 3.2 assumes as detailed in Algorithm 1. The data generation procedure takes as input the number of clusters ( K ) and a total number of reviewers ( N ), and outputs a synthetic collaboration graph of reviewers. The relative sizes of each of K clusters is sampled from a Dirichlet distribution which is commonly used as a prior distribution for a multinomial. The parameter  X  determines how variant/uniform the clus-ters sizes will be. This allows for generating various propor-tions of cluster sizes. Then for each cluster, all its parame-ters are sampled.
 Algorithm 1 Synthetic data generation procedure Input: K (no. clusters), N (no. reviewers) Input: Dirichlet params (  X  1 ,..., X  K ),  X 
Sample cluster sizes  X  Dir(  X ,..., X  ) for each cluster c do VerifiedPurchase }  X  Uniform 0  X  1 end for
Assign reviewers to clusters for each cluster c in K clusters do
VerifiedPurchase  X  Bernoulli( p c, feature ) end for for each reviewer a,b pairs do end for Figure 6: The proposed clustering method identifies four clusters from a synthetic graph consisting of 950 nodes and 2,150 edges. Members of each cluster are colored differently.

Once all cluster parameters are sampled, for each cluster, | C i | nodes are generated where | C i | is the cluster size of cluster i . The node values are sampled from the cluster parameter as illustrated in Figure 5. Finally, the edges are sampled. The average degree is kept constant (30 in our Table 1: Rand Index results of our reviewer cluster-ing approach on synthetic data.
 case). So for each possible edge, if it is between two nodes of the same cluster, it occurs with probability  X  and if the edge is between two dissimilar clusters, it occurs with probability 1  X   X  . An example resulting graph of such a process with 4 clusters is shown in Figure 6. We run our method by over-estimating the number of clusters as 10 and use the same  X  that was used by the generation process. Using higher values for  X  mostly resulted in two of the detected clusters being labeled as the same cluster.

As shown in Figure 6, our clustering method successfully recovered most of the clusters as long as the intra-cluster edges occurred more frequently than inter cluster edges. Most mistakes happened in the central cluster where the density was low. The clustering method recovered 4 clusters which matched the true number of clusters, and 94.3% of the nodes were clustered correctly.

As another evaluation metric, we used Rand Index , a well-known metric for evaluating the quality of clustering when the ground truth is known. Graphs of size 1.2K nodes and 2.6K edges with 4 clusters were generated using the process described earlier. Given different numbers of predetermined clusters to the clustering algorithm, we list the average Rand Index of 10 runs in Table 1. One noticeable point in Table 1 is the improved clustering performance when the number of predetermined clusters over-estimates the actual number. The reason is that the clustering method is based on EM which is a local optimizer of the likelihood. More clusters with random initial parameters spread out in the parameter space mean a better chance of finding a more optimum final likelihood, so we observe better clustering results.
These results suggest that the proposed clustering method works well; but does it uncover meaningful groups over the expanded dataset?
Based on the expanded dataset of products, reviews, and reviewers, we formed the Reviewer-Reviewer graph as de-scribed in Section 3.2.1. In the graph, nodes are reviewers, and edges appear when a pair of reviewers write favorable reviews for the same products in a short time frame. The higher number of same products a pair of reviewers posts reviews to, the larger the edge weight of the reviewers is. This graph is shown in Figure 7. As a cleaning step, we dis-carded small connected components of this graph; we kept reviewers who belonged to a connected component of size 10 or more. We modeled reviewers using the six features described in Singleton Potentials in Section 3.2.2.
Next, we applied the described clustering method on the resulting reviewer graph. The EM algorithm was run with 16 random restarts. The predetermined number of clusters was set to 10. The value of  X  determines how likely connected nodes belong to the same cluster. One of the strengths of this clustering method is that connected nodes with dissim-ilar clusters are punished regardless of the number of pre-determined clusters. Hence, the eventual number of emerged clusters can be less than what is predetermined. For a high value of  X  like 0.99 we ended up with almost one cluster for Figure 7: Detected clusters in the Amazon reviewer-reviewer graph. Nodes and edges are colored based on the cluster to which they were assigned. Two out of the three largest clusters are deceptive reviewer clusters. all the nodes. For lower value of  X  = 0 . 7 we ended up with 6 clusters, 3 of which had higher densities. Since we got a reasonable result returning clusters with high densities when we set  X  = 0 . 7, we used this value for experiments.
Specifically, all listed and detected clusters were dense as shown in Table 2. The three largest clusters had high densi-ties. By inspecting reviewers in the three clusters, we found that the workers, who posted deceptive reviews on products advertised on crowdsourcing websites, belonged to the first two clusters. This indicates this clustering method worked well with the extended dataset. The last cluster contained legitimate reviewers who joined Amazon Vine program [1]. Users, who had posted high quality reviews and had gotten high helpfulness from other Amazon users, were invited by Amazon for the Vine program. We analyzed why these re-viewers in the third cluster had high density, and found that they posted reviews of similar sets of products in a short time window. Interestingly, these reviewers posted many favorable reviews. The biggest difference between the first two clusters (deceptive reviewers) and the third cluster (le-gitimate reviewers) is that the legitimate reviewers in the third cluster posted lengthier reviews so that they can share detailed opinions regarding products that they had used.
Finally, we list a number of products associated with de-ceptive reviews written by the deceptive reviewers in the first two clusters. Table 3 shows the top-15 products favor-ably reviewed by the deceptive reviewers. Popular target products were health and beauty products, and books.
In summary, we evaluated our clustering method over both a synthetic dataset and a the extended (real) dataset. Our experimental result showed that the clustering method successfully found clusters of deceptive reviewers.
Given these encouraging results, we now turn to the chal-lenge of detecting deceptive reviews. We adopt a standard Figure 8: Length distributions of deceptive reviews and legitimate reviews fit log-normal distributions. Support Vector Machine (SVM) with RBF kernel. We com-pare a deceptive review classifier using a standard set of fea-tures versus one that additionally incorporates the output of the proposed clustering method. We then compare the cluster-aware deceptive review classifier versus one based on an alternate clustering method (SimRank + k-medoids). Ground Truth. To get the ground-truth (i.e., which re-view is deceptive or not), we labeled a review as deceptive if (i) the review was associated with a product which was targeted by a crowdsourced malicious task (that is, it was identified in the original root dataset ); and (ii) the review with a high rating was posted within a few days after the task was posted. Otherwise, it is labeled as a legitimate re-view. Note that this labeling choice is conservative, in that there may be deceptive reviews that are labeled as legiti-mate. If a user posted at least one deceptive review, we considered the user as a deceptive reviewer.

Table 4 shows our collected dataset which contains 5.5K deceptive reviews and 65.6K legitimate reviews with a num-ber of corresponding reviewers and products.
 Features. To train and test a classifier, first we converted each review to a set of feature values. In this study, we used seven standard features plus one additional feature based on the output of our reviewer clustering method (see Table 5). The seven standard features are: 1. Verified purchase fea-ture is a binary feature. If a reviewer actually purchased a product that he reviewed, the feature value would be true. 2. Star rating is the number of stars given to the product in the review. 3. Review length is the logarithm of the length of the review in characters. We hypothesize that deceptive reviewers (crowd workers) would not want to spend a long time to write a review. In order to verify this, we compared length of deceptive reviews with length of legitimate reviews. Figure 8 shows length distributions of deceptive reviews and legitimate reviews, which follow log-normal distributions.
The log-normal distributions were also observed in other user generated text [24]. These distributions suggest that length of deceptive reviews are shorter than length of legiti-mate reviews. 4. Helpfulness ratio is the number of helpful votes divided by all votes. On Amazon.com, a review has a star rating (1 through 5), and other users can rate the review as either  X  X elpful X  or  X  X ot helpful X . We measured helpfulness ratio of each review in our dataset as follows: star rating for deceptive reviewers and legitimate reviewers. Figure 9 depicts helpfulness ratios with error bars of decep-tive and legitimate reviews under star ratings. A circle and rectangle indicate a median helpfulness ratio of reviews. In star ratings 1  X  3, deceptive and legitimate reviewers had Figure 9: Helpfulness ratio of deceptive reviews and legitimate reviews under different star ratings. similar helpfulness ratios. But, in 4  X  5, deceptive reviewers X  reviews got lower helpfulness ratios compared with ones of legitimate reviewers. This analysis reveals that even though legitimate users were able to punish some of these decep-tive reviews with lower helpfulness ratings, many deceptive reviews were not so identified. 5. Total votes is the denomi-nator of helpfulness ratio , and captures the number of votes a review has accumulated. 6. Reviewer got more helpfulness for favorable reviews is a binary feature that is true when favorable reviews (i.e., star rating is 4 or 5) of a reviewer are more helpful than his unfavorable (i.e., 1  X  3)reviews. 7. Reviewer has more verified purchases for favorable reviews is another binary feature.

We evaluate the quality of a deceptive review classifier based on these seven features alone versus a classifier based on these seven features plus the output of the proposed re-viewer clustering method (from Section 3.2). This eighth feature is: 8. Cluster assignment: the K + 1-dimensional feature vector of indicator variables for whether the review author belongs to each of the clusters. The extra clus-ter is for when the reviewer is missing from the Reviewer-Reviewer Graph. By considering this feature separately, we can evaluate the importance of uncovering  X  X idden X  connec-tions among reviewers.
 Unbalanced and Balanced Training and Testing Sets: From our dataset presented in Table 4, we created two pairs of training and testing sets: (i) unbalanced training and test-ing sets; and (ii) balanced training and testing sets. In the unbalanced training and testing sets, each set contains the half of the original dataset, following the same class ratio. But, to create balanced training and testing sets, first we performed undersampling of legitimate reviews so that the balanced dataset contains the same number of deceptive and legitimate reviews (i.e., 5.5K deceptive and 5.5K legitimate reviews). Then, we split the balanced dataset to the bal-anced training and testing sets each of which contains the half of the balanced dataset.
 Evaluation Metrics and Setup: To evaluate prediction accuracy of our approach, we use three metrics: Area under the ROC Curve (AUC), precision and recall. Especially, we use AUC as our primary evaluation metric, since a higher AUC means that a model is good at correctly predicting both class instances regardless of class imbalance [8].
We developed two SVM classifiers: (i) a basic SVM clas-sifier based on the first seven features in Table 5; and (ii) an advanced SVM classifier based on all eight features including the output of our clustering method. The basic classifier was used as a baseline. Each classifier (e.g., basic and advanced) was trained by each of training sets.
 Table 5: Features of our deceptive review classifier. Figure 10: Precision and recall curves of review classification results under four cases (2 different datasets and 2 different classification approaches). Results: Figure 10 shows experimental results of two classi-fiers with unbalanced and balanced training and testing sets. In case of using unbalanced training and testing set, while the basic SVM classifier achieved 0.50 AUC, the advanced SVM classifier achieved 0.77 AUC. In case of using balanced training and testing set, AUC has been increased in both basic and advanced SVM classifiers. Specifically, while the basic SVM classifier achieved 0.89 AUC, the advanced SVM classifier achieved 0.96 AUC. Overall, adding the output of our clustering method to feature set significantly improved AUC by 54% and 8% in both unbalanced and balanced data sets. Such performance improvement indicates the effective-ness of incorporating the social context of reviewers (i.e., the output of the clustering method) into deciding whether a review is deceptive or legitimate.
 Versus an Alternative Clustering Method: Finally, we investigate the quality of deceptive reviewer classification if the proposed clustering method is swapped out in favor of an alternate one. Concretely, we consider a clustering method based on SimRank [12] and K-medoids. First, Sim-Rank measures similarity between nodes of a graph. Then, k-medoids clustering algorithm is performed based on the similarities of nodes in the graph. Specifically, we calculated SimRank score in two graphs  X  (i) a bipartite graph con-taining both reviewers and products; and (ii) the Reviewer-Reviewer collaboration graph G R,R . After a similarity ma-trix of each graph is calculated, we use k-medoids to cluster the reviewers for each graph. Finally, a cluster membership of each reviewer is used as a feature for a SVM based de-ceptive review classifier. In this experiment, we used the unbalanced training and testing set.

Since we had two different similarity matrices, we built two classifiers. The first clustering method based on the bipartite graph performed poorly enough to be almost iden-tical to the performance of the seven features based SVM classifier without using any clustering output. The second clustering algorithm based on the Reviewer-Reviewer graph achieved 0.71 AUC which was lower than performance (0.77 AUC) of our proposed approach as shown in Figure 11. Figure 11: Our classification approach with the clus-ter assignment (0.77 AUC) outperformed SimRank and k-medoids based classifier (0.71 AUC) under un-balanced dataset.
 With respect to efficiency, SimRank took a long time O (hours) for a large graph since time complexity of the orig-inal SimRank is O ( n 4 ) where n is the number of the nodes in a graph compared to our pairwise MRF parameterization which has O ( n 2 ) parameters and took O (minutes).
In summary, our proposed clustering based classifier out-performed SimRank and k-medoids based classifier in terms of both effectiveness and efficiency.
In this paper, we presented a novel sampling method for identifying products that have been targeted for manipula-tion and a seed set of deceptive reviewers who have been enlisted through the command-and-control of crowdsourc-ing platforms. Specifically, we have sampled tasks target-ing Amazon via the crowd marketplaces RapidWorkers.com, ShortTask.com, and Microworkers.com. We have augmented this seed set to identify additional deceptive reviewers who participate as part of hidden groups of coordinated ma-nipulation through a reviewer-reviewer graph clustering ap-proach. Finally, we have embedded the results of this prob-abilistic model into a classification framework for detect-ing crowd-manipulated reviews. Our classification approach using the reviewer clustering results as a feature signifi-cantly outperformed a classification approach not using the reviewer clustering results. Specifically, our approach with clustering results have achieved 0.77 AUC in unbalanced testing set and 0.96 AUC in balanced testing set, improv-ing 54% AUC in unbalanced testing set and 8% AUC in balanced testing set respectively compared with the classifi-cation approach without using reviewer clustering results. Additionally, the proposed approach has outperformed a SimRank and K-medoids based approach in terms of effec-tiveness and efficiency.
This work was supported in part by AFOSR Grant FA9550-12-1-0363 and Google Faculty Research Award. Portions of Dr. Squicciarini X  X  work was supported by the Army Re-search Office under grant W911NF-13-1-0271. Any opinions, findings and conclusions or recommendations expressed in this material are the author(s) and do not necessarily reflect those of the sponsor.
