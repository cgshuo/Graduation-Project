 Query Performance prediction aims to evaluate the effective-ness of the results returned by a search system in response to a query without any relevance information. In this paper, we propose a method that considers both magnitude and variance of scores of the ranked list of results to measure the performance of a query. Using six different TREC test sets, we compare our predictor with three of the state-of-the-art techniques. The experimental results show that our method is very competitive. Pairwise comparisons with each of the three other methods show that our predictor performs better in more data sets.
 H.3.3 [ Information Search and Retrieval ]: Information Search and Retrieval Algorithms query difficulty; query performance prediction; score distri-bution
Query difficulty prediction aims to predict whether a query will return a high quality result list ( X  X asy X  queries), or low quality result list ( X  X ard X  queries), when no relevance infor-mation is given by a human operator. Query difficulty pre-diction is also referred to as query performance prediction. It has many potential applications in a variety of IR tasks such as improving retrieval consistency, query refinement, and distributed IR. This is why the problem has received considerable attention in the IR community in recent years.
Accurate performance predictions can help a user decide if the results are acceptable. If more relevant results are needed, then the user may decide to reformulate the query so as to obtain some different results from the same search engine as before or use other search services available.
Query performance prediction can be roughly categorized into two types: pre-retrieval prediction and post-retrieval prediction. Pre-retrieval methods evaluate the query before the search takes place, thus they must rely on the statistics of the query terms in the collection [8]. The advantage of such methods is that they can be computed quickly, using avail-able statistics of the query terms gathered at indexing time. However, a disadvantage of such predictors is that they do not take into account the specific retrieval algorithms, so the predictions may not be as accurate as the post-retrieval prediction methods [3].

Post-retrieval prediction methods are usually more com-plex and expensive as the search results need to be analyzed after retrieval. Post-retrieval prediction algorithms can be further divided into clarity score based methods [10], rank-ing robustness based methods [14], and score analysis based methods [12, 13].

Prior research on score analysis demonstrates that mag-nitude and variance of scores are two factors that are cor-related with query performance. The score-based perfor-mance prediction methods proposed previously consider ei-ther magnitude or variance of scores [13, 2, 9] but not both. In this paper, we propose a method that takes both magni-tude and variance of scores into consideration at the same time. Experiments with 6 groups of TREC data are very promising..

The rest of this paper is organized as follows: related prior work is discussed in section 2. Section 3 describes our method for creating an estimator for query performance in detail. Section 4 presents the experiments we conducted on TREC data. Section 5 concludes the paper.
In this section, we review some prior work on post-retrieval query prediction which category our proposed method falls into.

Cronen-Townsend et al.[10] proposed a method of com-puting the relative entropy between the models of the query and the collection. Afterwards, a few more clarity-based predictors have been proposed by other researchers [5, 6, 7].
Zhou et al.[14] built a novel framework called ranking ro-bustness to predict query performance. Robustness based approaches evaluate how robust the results are to perturba-tions in the query, the result list and the retrieval method. Related research by others may be found, for example, in [1, 4, 11].

A main branch of the post-retrieval prediction methods is score analysis. Zhou et al. proposed a predictor, Weighted Information Gain (WIG) [13], which measures the diver-gence between the mean retrieval score of some top-ranked documents and that of a typical document in the entire cor-pus. Shtok et al. proposed another predictor called Normal-ized Query Commitment (NQC) [2] to estimate query drift in the list of top-ranked and/or bottom-ranked documents. More recently, P  X  erez-Iglesias et al. also focused on the vari-ance aspect of scores. Some experiments are conducted by using standard deviation and some of its variants to capture the differences between  X  X ard X  and  X  X asy X  queries. [9].
In this paper, we investigate the problem of query per-formance prediction by analyzing score distribution. Those methods based on score analysis focused on either the mag-nitude [13] or the variance [2, 9] of the scores, yet none are able to utilize both. Therefore, we propose a query perfor-mance prediction method that takes both factors into con-sideration at the same time. As we can see later, such a combination is not trivial. Experiments with TREC data are conducted to evaluate the effectiveness of our method. Let us begin by setting out the notation. Let q , D ,and M denote a query, a corpus, and a retrieval method, re-spectively. We use L ( q, M )and D [ k ] q to denote the result list returned in response to query q by M over D and the top-k documents ranked highly in the result list L ( q, M respectively. k is a free parameter, set to an arbitrary nat-ural number prior to the search. Our goal is to establish a predictor for evaluating the quality of the ranking list re-turned by M over D for a given query q without relevance judgment information.

Previous work on query performance prediction observes that there is a certain relationship between score distribution and query performance [2, 9, 13]. In particular, two factors, namely the magnitude and deviation of scores may be used to predict query performance.
 WIG [13] uses Equation 1
WIG ( q, M )= 1 to calculate scores for a given query q . 1 Here Score ( d )is the score that document d is awarded by M , Score ( D )is the score that an average document in D would be given by M ,and | q | is the number of terms in q .InFigure1,we can see that WIG mainly considers the magnitude of scores that those retrieved documents obtain. | q | serves as an scale factor to make WIG scores comparable over different queries.
 NQC uses Equation 2 NQC ( q, M )= 1
This is a simplified version of WIG, which only uses score information of the results. According to [12], it is a very effective method. Figure 1: Topic 324  X  X rgentine/British Relations X , average precision is 0.6670; topic 327  X  X odern Slav-ery X , average precision is 0.1773. Data is taken from pircRB 04 t 3 , a run submitted to the Robust Track in 2004. Figure 2: Topic 324  X  X rgentine/British Relations X , average precision is 0.6670; topic 304  X  X ndangered Species (Mammals) X , average precision is 0.1049. Data is taken from pircRB 04 t 3 , a run submitted to the Robust Track in 2004. to calculate scores for a given query q .Here X   X  is the aver-age of the scores of all k results in D [ k ] q .NQC[2]canbe regarded as a variation of standard deviation (referred to as SD), which is investigated in [9].
 Let us consider two examples to illustrate why WIG and NQC work in some situations, but fail in others. All the data is taken from pircRB 04 t 3, which is the best (measured by MAP) among all those submitted to TREC Robust 2004. Figure 1 shows the results for queries 324 and 327. The re-sults for query 324 obtain higher scores than that for query 327, and the variance of both score distributions are simi-lar. MAP(324) &gt; MAP(327). These conditions are ideal for performance prediction methods such as WIG. On the other hand, methods such as SD and NQC may not work well since the curves for both queries have very similar shape. As a matter of fact, we have and so NQC would make a wrong decision about performance comparison of these two queries.
 The second example is to compare 324  X  X rgentine/British Relations X  X nd query 304  X  X ndangered Species (Mammals) X . Figure 3: Topic 561. Data is taken from pircRB 04 t 3 , a run submitted to the Robust Track in 2004.
 MAP(324) &gt; MAP(304). Figure 2 shows the score curves for both results. The results for query 304 obtain higher scores than that for query 327, but the latter has a greater variance than the former. These conditions are ideal for performance prediction methods such as NQC and SD, but they are not good for methods such as WIG.

From the above two examples we can see that considering either score magnitude or deviation may work in some sit-uations but not the others. It would be an advantage if we can consider both of them together. We use the following Equation 3 to calculate scores for a given query q . Here SMV stands for our method, which considers both Score Magnitude and Variance.  X   X  is the average of the scores of all k results in D q . Inside the summation of Equation 3, there are two components. One is Score ( d ) and the other is | ln Score The former is used to represent score magnitude and the latter is used to represent a form of score variance. We combine the two by multiplication.

In order to understand the contribution that results at different ranks can make, let us consider an example, the re-sults for query 561 from thutd 5. At each rank, the value of raw score Score ( d ), the value of w ( d )= | ln Score ( final score (product of w ( d )  X  Score ( d )) are shown in Figure 3. We can see that each result makes some contribution to the final score. Results at the very top and bottom make more contribution than those in the middle. This is reason-able because results at both ends are more informative than those in the middle. Let us recall the two aforementioned examples. WIG works in the first example, but not the sec-ond; NQC works in the second example, but not the first. SMV works in both examples.

In its current form, Equation 3 works with positive scores but not negative scores. If there are negative scores, then Equation 3 can be modified to support that. Let us define low s to be the lowest score from the results for a group of queries. Score ( d ) can be replaced by Score ( d )  X  low s ,thus all negative scores transform to positive scores and Equation 3 can be used without any problems.
Topic 672 is removed because of no relevant results identi-fied.
 Table 2: Pearson s correlation coefficients for corre-lation with actual retrieval performance. Bold cases mean the most accurate prediction per collection.
In this section, we evaluate SMV. Experiments are con-ducted on 6 different TREC collections. Table 1 summarizes the information of these test collections. They are used in different tasks including ad hoc, web, terabyte, and robust. Their sizes vary from 0.5 million (disks 2&amp;3, disks2&amp;4, disks 4&amp;5-CR) to 25 million (GOV2). We compare the prediction quality of SMV with that of the three state-of-the-art pre-dictors: Standard Deviation (SD) [9],Weighted Information Gain (WIG) [13] and NQC [2].

As described in previous section, we need to set a value for the parameter k in all the predictors. As recommended in [12], k is set to 5 for WIG. For the three other methods NQC, SD and SMV, k is set to the same value. It is set to 1000 for the GOV2 collection and 100 for all other collections, as in [2]. Special treatment is given to GOV2 because in the TREC 2004 Terabyte task, 10000 results were retrieved for each query, whereas only 1000 results were retrieved for all other cases. We use the average score of all the retrieved results (1000 or 10000) to estimate Score ( D ).
 For each collection, we select the best run (measured by MAP) that is submitted to TREC to carry out the experi-ment. They are CnQst 2(TREC4), ETHme 1(TREC5), fub 01 be 2 (TREC 2001), thutd 5 (TREC 2002), pircRB 04 t 3 (TREC 2004 Robust), uogT BQEL (TREC 2004 Terabyte).
The prediction quality of a method is evaluated by mea-suring both Pearson s and Kendall s  X   X  correlation be-tween the ranking of queries by their actual performance (measured by MAP) and the ranking of queries by a per-formance predictor. In statistics, Pearson s correlation is a measure of the linear correlation between two variables. Its range is [-1,1], where 1 presents total positive corre-lation, 0 no correlation and -1 total negative correlation. Kendall s  X   X  coefficient is used to measure the association between two measured quantities. Its range is also [-1,1], where 1 denotes that the two rankings are the same, and -1 denotes that one ranking is the reverse of the other. The experimental results are shown in Tables 2 and 3 for Pearson s correlation and Kendall s  X   X  rank coefficient Table 3: Kendall s  X   X  rank coefficients for correlation with actual retrieval performance. Bold cases mean the most accurate prediction per collection.
 Table 4: Average of Pearson s correlation (P) and Kendall s  X   X  rank coefficients (K) for correlation with actual retrieval performance. 6 collections are di-vided into two types: clean (including Disks 2&amp;3, Disks 2&amp;4 and Disks 4&amp;5-CR) and noisy (including WT10g, GOV and GOV2).

Collections SD WIG NQC SMV respectively. Generally speaking, SMV predicts query per-formance better in more collections when either of the two measures is used. More specifically and compared in pair-wise fashion with any of the three other methods, SMV out-performs each of the three methods in 5 out of 6 collections with respect to Pearson s correlation; the figures are 4 (SD), 4 (WIG), and 5 (NQC) out of 6 if considering Kendall s  X   X  rank coefficient. Apart from the best run in each task, we also randomly select and evaluate a few more runs. The experimental results are sim ilar to those reported in the pa-per. Therefore, SMV performs very well compared to other state-of-the-art techniques under the same conditions.
In all 6 collections, WT10G, GOV and GOV2 are collec-tions whose documents are crawled from the web. Unlike the three other collections, these web collections are noisy because there are many duplicates or near-duplicates, spam, documents written in foreign languages, binary data docu-ments, etc. Some researchers (e.g., in [9]) observe that for such collections, query performance prediction is less accu-rate. In our experiment, we divide the 6 collections into 2 types: clean and noisy. Thus 3 web collections are classi-fied as noisy whilst the rest are clean. For these collections, too, SMV performs better on average than each of the other methods. Table 4 gives more detailed information. We can see that on average, all performance prediction methods do better with clean collections than with noisy collections.
We have presented a performance prediction method SMV by considering both score magnitude and variance at the same time. Evaluated with 6 different collections used in TREC, we find that our predictor performs better than any of the three other predictors in more data sets. Thus, we can conclude that the proposed method is very competitive.
In terms of future work, we shall focus on a few specific retrieval systems and models such as Terrier, Indri, BM25, Kullback-Leibler Divergence Language Model to further in-vestigate the performance prediction problem. If we can treat those results from different systems/models in differ-ent ways then more accurate prediction is possible since the distribution of scores may differ from one system to another.
In a different vein, we can take more information such as certain statistics of the collection, query terms, and so on into consideration. Thus the method proposed in this paper can be used together with others for more accurate performance prediction. [1] J. A. Aslam and V. Pavlu. Query hardness estimation [2] O. A. Shtok and D. Carmel. Predicting query [3] C. Hauff, D. Hiemstra and F. de Jong. A survey of [4] E.Yom-Tov,S.Fine,D.CarmelandA.Darlow.
 [5] F. Diaz and R. Jones. Using temporal profiles of [6] G. Amati, C. Carpineto and G. Romano. Query [7] B. He and I. Ounis. Inferring query performance using [8] B. He and I. Ounis. Query performance prediction. [9] J. P  X  erez-Iglesias and L. Araujo. Standard deviation as [10] S. Cronen-Townsend, Y Zhou and W. Bruce Croft. [11] V. Vinay, I. J. Cox, N. Millic-Frayling and K. R. [12] Y. Zhou. Retrieval performance prediction and [13] Y. Zhou and W. Bruce Croft. Query performance [14] Y. Zhou and W. Croft. Ranking robustness: a novel
