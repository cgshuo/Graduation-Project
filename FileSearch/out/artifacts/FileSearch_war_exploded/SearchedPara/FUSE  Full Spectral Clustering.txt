 Multi-scale data which contains structures at different scales of size and density is a big challenge for spectral clustering. Even given a suitable locally scaled affinity matrix, the first k eigenvec-tors of such a matrix still cannot separate clusters well. Thus, in this paper, we exploit the fusion of the cluster-separation informa-tion from all eigenvectors to achieve a better clustering result. Our method FU ll S pectral Clust E ring (FUSE) is based on Power Itera-tion (PI) and Independent Component Analysis (ICA). PI is used to fuse all eigenvectors to one pseudo-eigenvector which inher-its all the cluster-separation information. To conquer the cluster-collision problem, we utilize PI to generate p ( p &gt; k ) pseudo-eigenvectors. Since these pseudo-eigenvectors are redundant and the cluster-separation information is contaminated with noise, ICA is adopted to rotate the pseudo-eigenvectors to make them pair-wise statistically independent. To let ICA overcome local optima and speed up the search process, we develop a self-adaptive and self-learning greedy search method. Finally, we select k rotated pseudo-eigenvectors (independent components) which have more cluster-separation information measured by kurtosis for clustering. Various synthetic and real-world data verifies the effectiveness and efficiency of our FUSE method.
 Spectral clustering; Power iteration; ICA; Givens rotation; Multi-scale data
Clustering is a basic technique in data analysis and mining. Two commonly used methods are k -means and Expectation Maximiza-tion clustering (EM) which pre-assume that data fits a Gaussian model. Such model-based clustering methods perform well if data fits the model. However, in most cases, we do not know the distri-bution of data. It is hard to decide which model to adopt. Spectral clustering, on the other hand, does not pre-assume any model. It only uses local information (point to point similarity) to achieve global clustering. Thus it is very elegant and popular in data ming and machine learning. Spectral clustering transforms the clustering of a set of data points with pairwise similarities into a graph parti-tioning problem, i.e., partitioning a graph such that the intra-group edge weights are high and the inter-group edge weights are low. There are three kinds of similarity graphs, i.e., the  X  -neighborhood graph, the k -nearest neighbor graph and the fully connected graph [20]. Luxburg [20] emphasized that  X  theoretical results on the question how the choice of the similarity graph influences the spec-tral clustering result do not exist  X . However, the parameters (  X  , k ,  X  ) of these similarity graphs highly affect the clustering results, espe-cially in cases where data contains structures at different scales of size and density. One usually used objective function in spectral clustering is normalized cut [16]. As pointed out in [14], the nor-malized cut criterion does not always work even given a proper affinity matrix.

Consider three clusters of different geometry shapes and densi-ties in Figure 1(a). Both Gaussian clusters have 100 data points. The rectangular stripe cluster has 400 data points sampled from a uniform distribution. Conventional spectral clustering algorithms tend to fail on this multi-scale data. Self-tuning spectral cluster-ing (ZP) [23] proposes to use the locally scaled affinity matrix to solve the limitation. Further, ZP rotates the eigenvectors to cre-ate the maximally sparse representation to estimate the number of clusters automatically. However, such proposals still do not work on multi-scale data because of the unsuitability of the normalized cut criterion only using local information. Such an argument can be inferred from Figure 1(c). ZP fails to correctly separate the three clusters. Both cuts are along the stripe. Intuitively, it is not difficult to understand. The normalized cut criterion tries to make clusters  X  X alanced X  as measured by the number of vertices or edge weights. Since each of the two Gaussian clusters only has 100 data points and they are so close to the stripe cluster, cuts between the Gaus-sian clusters and the stripe cluster have a higher penalty than those along the stripe.

Differing from other spectral clustering algorithms, our method combines the cluster-separation information from all eigenvectors to achieve a better clustering result. As can be seen from Fig-ure 1(d), only some controversial data points lying on the bound-aries are clustered incorrectly. The fusion of the cluster-separation information from all eigenvectors is accomplished by exploiting truncated Power Iteration (PI). To yield good clustering, spec-tral clustering uses the first k eigenvectors of the graph Lapla-cian matrix. Similarly, we use PI to generate p ( p &gt; k ) pseudo-eigenvectors. Each pseudo-eigenvector is a linear combination of all original eigenvectors, including the information not only from the  X  X nformative X  eigenvectors but from the  X  X oise X  eigenvectors. Note that the pseudo-eigenvectors are redundant to each other. One main question is how to make the information from the  X  X nforma-tive X  eigenvectors stand out and suppress the information from the  X  X oise X  eigenvectors? In this paper, we use Independent Compo-nent Analysis (ICA) to reduce the redundancy, i.e., to make the pseudo-eigenvectors statistically independent (non-redundant) to each other. After whitening (more details in Section 3.1), ICA ro-tates the pseudo-eigenvectors to find the direction in which the en-tropy is minimized. Subsequently, a kurtosis -based selection strat-egy is exploited. Such a minimum-entropy rotation plus a kurtosis -based selection improve the cluster separation. Our contributions are as follows, Contributions 1) We achieve the eigenvector-fusion by using Power Iteration (PI) . The generated pseudo-eigenvectors include information from all eigenvectors. 2) We improve the cluster separation by applying ICA com-bined with a kurtosis -based selection strategy . Since the gener-ated pseudo-eigenvectors are redundant to each other, which is not beneficial to good clustering, we apply ICA to make them statisti-cally independent. Then, a kurtosis -based selection strategy is ex-ploited to improve the cluster separation. To the best of our knowl-edge, we are the first to apply ICA on spectral clustering. 3) We develop a greedy search method to render searching for statistically independent components more efficient and effec-tive . The greedy search strategy discriminates the search order to let ICA not get easily trapped into local optimal. In addition, dur-ing the search process, the greedy search makes use of self-adaptive and self-learning strategies to balance the efficiency and effective-ness.
In this section, we give some major notations used in this paper and some background techniques our algorithm is based on to make this paper self-contained. We use lower-case Roman letters (e.g. a,b ) to denote scalars. Upper-case Roman letters (e.g. X,Y ) are used for continuous ran-dom variables. We denote vectors (row) by boldface lower case letters (e.g. x ). Matrices are denoted by boldface upper case letters (e.g. X ). We denote entries in a matrix by non-bold lower case let-ters, such as x ij . Row i of matrix X is denoted by the vector x column j by the vector x  X  j . We use [ x 1 ,  X  X  X  ,x n ] to denote a row created by stacking n continuous random variables; similarly, we use X = [ x 1 ;  X  X  X  ; x m ] to denote creating a matrix by stacking x along the rows. A set is denoted by calligraphic capital letters (e.g. S ). A cluster C is a set of data objects, i.e., C = { o 1 An undirected graph is denoted by G = ( V , E ) , where V is a set of graph vertices and E is a set of graph edges. An affinity matrix of the vertices is denoted by A  X  R n  X  n with a ij  X  0 , a ij graph is undirected). The degree matrix D is a diagonal matrix as-sociated with A with d ii = P j a ij . A normalized affinity matrix W is defined as D  X  1 A . Thus, a normalized graph random-walk Laplacian matrix is L = I  X  W = I  X  D  X  1 A according to Meila and Shi [13]. We list common symbols used throughout this paper in Table 1.
 Symbol Description I Identity matrix A Affinity matrix W Normalized affinity matrix D Degree matrix L Graph Laplacian matrix U Eigenvector matrix  X  Eigenvalue matrix V Pseudo-eigenvector matrix E Fused eigenvector matrix M Demixing matrix
I ( X ; Y ) Mutual information between two random variables
Although spectral clustering has gained its popularity and suc-cess in data mining and machine learning fields, its high time com-plexity ( O ( n 3 ) for computing the eigenvectors of the graph Lapla-cian matrix L ) limits its practical use in real-world data. To address the difficulty, Lin and Cohen [11] used truncated power iteration to find a pseudo-eigenvector on the normalized affinity matrix W with time complexity O ( n ) , which is very efficient and attractive. Note that the k largest eigenvectors of W are also the k smallest eigenvectors of L . Power Iteration (PI) is an efficient and popular method to compute the dominant eigenvector of a matrix. PI starts with a random vector v 0 and iteratively updates as follows [11],
Suppose W has eigenvectors U = [ u 1 ; u 2 ;  X  X  X  ; u n ] with eigen-values  X  = [  X  1 , X  2 ,  X  X  X  , X  n ] , where  X  1 = 1 and u 1 We have WU =  X U and in general W t U =  X  t U . When ignor-ing renormalization, Equation 1 can be written as
According to Equation 2, we have
So the convergence rate of PI towards the dominant eigenvector u 1 depends on the significant terms  X  i  X  1 finally converge to the dominant eigenvector u 1 which is of little use in clustering. PIC [11] defines the velocity at t to be the vector  X  = v t  X  v t  X  1 and defines the acceleration at t to be the vector =  X  t  X   X  t  X  1 and stops PI when k t k max is below a threshold  X  . Let s 1 ,  X  X  X  , s m be m one-dimensional independent sources. Each has n i.i.d samples denoted by s i = [ s i 1 ,  X  X  X  ,s m ) . Let S = [ s 1 ;  X  X  X  ; s m ]  X  R m  X  n and we assume S is hidden and only a matrix X of mixed sources can be observed. The task of ICA is to find a demixing matrix M  X  R m  X  m such that S = MX and every two components s i and s j ( 1  X  i,j  X  m,i 6 = j ) are sta-tistically independent. Without loss of generality, we assume data has been whitened, which means (1) the expectation value is zero and the covariance matrix is an identity matrix ( I ), (2) the demixing matrix is square, orthogonal ( M  X  M = I ) and full rank.
For real-world data, a single pseudo-eigenvector is not enough when the number of clusters is large. The reason is we need more eigenvectors to discriminate clusters when the cluster count in-creases. Thus, the cluster-collision problem may happen on one-dimensional pseudo-eigenvector. However, using PI p ( p &gt; k ) times with random generated starting vectors to generate p pseudo-eigenvectors is not sufficient either, which can only alleviate the situation a little because of the redundant information provided by these pseudo-eigenvectors. It is just the first step of the eigen-vector fusion. We also need to reduce the redundancy in these pseudo-eigenvectors. Our goal is twofold: 1) generate p pseudo-eigenvectors, 2) reduce redundancy to make the cluster-separation information stand out and suppress the noise information. The goal can also be rephrased as fusing the cluster-separation information from all original eigenvectors to improve clustering. Why do we need to fuse the information from all eigenvectors? The analysis in [14] shows that  X  when confronted with clusters of different scales, corresponding to a multi-scale landscape potential, standard spec-tral clustering which uses the first k eigenvectors to find k clusters will fail  X . Even given a locally scaled affinity matrix [23], ZP still cannot overcome the limitation if clusters have comparable densi-ties.

For example, Figure 2(a) demonstrates the eigenvector space (consists of the eigenvectors associated with the top three minimum eigenvalues) found by ZP (other spectral clustering algorithms yield similar ones) on the running example (S YN 1) in Section 1. It demonstrates that the three clusters are connected together in the eigenvector space, which is the reason for k -means X  difficulty in separating them. The cluster-separation information is not provided by the first three eigenvectors. However, we can see from Figure 2(b) that the blue and red clusters have fewer overlapped data points with the black cluster. If we fuse the information from the eigen-vectors in Figure 2(b) to those in Figure 2(a), we can achieve a better clustering result. In this paper, we use truncated Power Iter-ation (PI) to fuse eigenvectors. Figure 2 (c) shows the four pseudo-eigenvectors returned by running PI four times with randomly gen-erated starting vectors. The resulting pseudo-eigenvectors are very similar and redundant, e.g., the pseudo-eigenvectors v Thus, the cluster-separation information is not standing out. Figure 2(d) gives the pseudo-eigenvector space returned by our algorithm. In such space, the blue and red clusters have much fewer close data points to the black cluster compared to those in Figure 2(a), which makes k -means easily distinguish them from each other.

Consider that each pseudo-eigenvector generated by PI is a linear combination of all eigenvectors of the normalized affinity matrix W and every pair of distinct pseudo-eigenvectors is re-dundant. One way to reduce redundancy is to make p pseudo-eigenvectors statistically independent, which can be accomplished by ICA. Mathematically speaking, the problem formulation is as follows, Problem: Statistically Independent Pseudo-eigenvectors. Given a pseudo-eigenvector matrix V  X  R p  X  n generated by running PI p times, find a demixing matrix M  X  R p  X  p such that E = MV and the sum of mutual information between pairwise components of E is minimized, where E  X  R p  X  n is a resulting independent pseudo-eigenvector matrix.

The demixing matrix M can be derived by determining the di-rections of minimal entropy. To find such directions, ICA requires to whiten data, i.e., the expectation value of data is zero and the co-variance matrix of data is an identity matrix, by applying PCA. The demixing matrix M is orthonormal in white space. After whiten-ing data, ICA finds those directions of minimal entropy by rotat-ing the whitened data. In this paper, instead of using fastICA [6], we use Jacobi ICA [8, 7] to find statistically independent pseudo-eigenvectors for the reasons that 1) we can choose different kinds of contrast functions, 2) we can make it escape from local optima more easily. Note that Equation 4 can be solved by iteratively opti-mizing every pairwise mutual information. We rewrite Equation 4 as the following objective function: Now it comes to how to select k independent components. Since ICA is interested in searching for non-Gaussian directions, in which negentropy is minimized. Non-Gaussian may be super-Gaussian as well as sub-Gaussian. We are only interested in sub-Gaussian directions, in which clusters are as much separated as possible. Such directions can be best modeled by uniform distribu-tions [2]. In this paper, we use kurtosis to measure the distance of the probability distribution of an independent component to Gaus-sian distribution. The kurtosis is the fourth standardized moment , defined as, where  X  4 is the fourth moment of the mean and  X  is the standard deviation.

The kurtosis of any univariate Gaussian distribution is 3. The kurtosis of any sub-Gaussian distribution is below 3 and the kur-tosis of any sup-Gaussian distribution is above 3. We prefer the independent components associated with the top k minimum kur-tosis values. which the clusters are well separated.
The objective function in Equation 5 is difficult to solve. Inspired by Learned-Miller et. al [8] and Kirshner et. al [7] in which they used Givens rotation to estimate a demixing matrix for indepen-dent component analysis by sequentially rotating every two mixture components. The reason behind Givens rotation is: 2d-pairwise distances are not changing after rotation, thus joint distribution re-mains the same, whereas marginal distributions change after ro-tation. Thus, any metric based on joint distribution and marginal distributions varies when the rotation angle  X  varies. We can find the maximal or minimal values of metrics with respect to  X  . For d-dimensional data, a Givens rotation of angle  X  for dimensions i and j is defined as: where sin  X  is on the j -th row and i -th column and  X  sin  X  is on the i -th row and j -th column of G .
 The demixing matrix M can be estimated as where G ( i,j, X   X  ) is a Givens rotation of the best angle  X  mizing the mutual information of the dimensions i and j of a data matrix.
Learned-Miller et. al [8] and Kirshner et. al [7] optimized Equa-tion 5 by exhaustively search over K = 150 values of  X  in the range 0 ,  X  2 which is time-consuming. Besides, they did not discriminate the order of optimization for different pairwise dimensions, which results in getting easily trapped in local optima. Considering the two limitations, we propose a new optimization method which is very efficient and effective.

To speed up the search process, we do not exhaustively search over K = 150 values of  X  in the range 0 ,  X  2 . In contrast, we use the history search as anchors to guide the search process. From this perspective, the greedy search is a self-learning method based on its learned knowledge. The strategy of adjusting the search resolution (see the following) makes our greedy search self-adaptive. Note that we only need to consider  X  in the interval 0 ,  X  2 because the effectiveness of any 90 degree rotation is equivalent as explained in [8]. In this paper, we adopt kernel generalized variance (KGV) proposed by Bach and Jordan [1] to estimate pairwise mutual infor-mation considering its linear complexity and especially its smooth-ness w.r.t. log function. For a detailed description, please see [1, 17]. Now we give an example to demonstrate our greedy search method. Because in practical use we can choose different contrast functions, here we just give a generalized function f to demonstrate the main idea. The curve in Figure 3 (a) is a function of  X  . The goal is to find the best  X   X  achieving the maximal objective function f in the interval 0 ,  X  2 (the minimal f can be achieved by finding the maximal  X  f ). Our method is described as follows:
Case 1: As depicted in Figure 3 (a), we set the ascending and descending step size to  X  2 K . We choose three different initial  X  (in our example is  X  1 ,  X  2 and  X  3 ) with the same interval (e.g. and compute their objective function ( f (  X  1 ) , f (  X  If f (  X  3 )  X  f (  X  2 )  X  f (  X  1 ) , we assume that the function is con-tinuously decreasing and we multiply the descending step size by  X  = 2 ( the search resolution) and update  X  3 for the following itera-tion. If  X  is two large, the method may skip some important search niche, while if  X  is too small, the efficiency will be decreased. We update  X  1 =  X  2 ,  X  2 =  X  3 , f (  X  1 ) = f (  X  2 ) and f (  X  history reference values for the following search.

Case 2: We compute f (  X  3 ) as depicted in Figure 3 (b). If f (  X  2 )  X  f (  X  3 ) and f (  X  2 )  X  f (  X  1 ) , we assume the function is continuously increasing. We set the descending step size to its ini-tial value and multiply the ascending step size by  X  and update  X  . Also, we update  X  1 =  X  2 ,  X  2 =  X  3 , f (  X  1 ) = f (  X  f (  X  2 ) = f (  X  3 ) as history reference values for the following search. If f (  X  3 )  X  f (  X  2 )  X  f (  X  1 ) , go to case 1.

Case 3: We compute f (  X  3 ) as depicted in Figure 3 (c). If f (  X  1 )  X  f (  X  2 )  X  f (  X  3 ) , we assume the function is continuously increasing. We set the descending step size to its initial value and multiply the ascending step size by  X  and update  X  3 . Also, we up-date  X  1 =  X  2 ,  X  2 =  X  3 , f (  X  1 ) = f (  X  2 ) and f (  X  history reference values for the following search. if not, go to case 4.

Case 4: In this case (Figure 3 (d)), f (  X  1 )  X  f (  X  2 ) and f (  X  f (  X  2 ) , we assume there may be some peaks in the interval. We exhaustively search in the interval [  X  1 , X  3 ] with a step size nally, we update  X  1 =  X  2 ,  X  2 =  X  3 , f (  X  1 ) = f (  X  and set the ascending step size to its initial value. Since now f (  X  2 )  X  f (  X  1 ) , we assume the function is continuously decreas-ing. We multiply the descending step size by  X  and update  X  f (  X  3 )  X  f (  X  2 ) , go to case 1; if f (  X  3 )  X  f (  X 
We repeat the above four cases until  X  3  X   X  2 . Note that, in each case, we update the best objective function value f b and  X 
As said before, in [8] and [7], the authors do not differ between the order of optimizing pairwise dimensions which results in the algorithm X  X  getting easily trapped in local optima. In this paper, we use a greedy selection method, i.e., computing the objective function values for each pairwise dimensions and then optimizing pairs according to their objective function values from the worst to the best, to make our method not easily get trapped in local optima. Our pseudo-code is given in Algorithm 1.

Steps 1  X  2 initialize the demixing matrix M to an identity ma-trix, compute the affinity matrix A and normalize it to W . Steps 3  X  8 generate p = k + 1 pseudo-eigenvectors using power iteration (PI). In step 4, the starting vector is randomly generated following a Gaussian distribution with mean 0 and variance 1. In Steps 5  X  8, each starting vector is iteratively updated by power iteration until the acceleration threshold  X  or the maximum iteration number is reached. Step 10 whitens the pseudo-eigenvector matrix V to let it have zero expectation value and an identity covariance matrix. In step 11, c includes the indices of each pairwise components in E and their mutual information values ( a i  X  { 1 , 2 ,  X  X  X  ,p } ). To let the algorithm escape from local optima, step 14 sorts c in descend-ing order w.r.t. mutual information values. Steps 15  X  19 use the greedy search to find the best  X   X  for each pairwise components of E to make them statistically independent and update components X  pairwise mutual information value stored in c j 3 , and also update E and M . We set the mutual information threshold to 0.1 for a bal-ance between the efficiency and effectiveness. If we set it lower, the efficiency will be decreased but effectiveness will be increased and vice versa. Step 20 returns the selected independent components which will be fed to k -means to find clusters.
We omit the runtime for computing the affinity matrix which is a common step in all spectral clustering methods. The analysis of runtime complexity of FUSE is as follows: In steps 3  X  8, gen-erating one pseudo-eigenvector costs O ( n ) time [12], and thus the runtime complexity to generate p = k + 1 pseudo-eigenvectors by power iteration is O (( k + 1) n ) . In step 10, as a preprocess-ing step, whitening data costs O (( k + 1) 2 n ) time. In this pa-per, we adopt Kernel Generalized Variance (KGV) using incom-plete Cholesky decomposition proposed in [1] to estimate mutual information. The complexity of KGV is O ( m 2 M 2 n ) [1], where m is data dimension and M is the maximal rank considered by the low-rank decomposition algorithms for the kernels. In step 11, the computation time for all pairwise mutual information values is from local optimal, we sort c using quick sort algorithm. The run-time complexity of the ordering process is 3( k + 1)  X O ( l log l ) = of finding independent pseudo-eigenvectors is 3( k + 1)  X  K  X O (2 2 M 2 2 n ) = O ( k 3 M 2 2 n ) . Finally, we use k -means to cluster on the selected independent pseudo-eigenvectors. The total run-time complexity of FUSE is O k 2 ( M 2 1 n + knM 2 2 + k plus the time complexity of k -means, i.e., O ( nk )  X  ( # k -means it-erations) [3].
 Algorithm 1: FUSE Input : Data X  X  R m  X  n
Output : cluster indicator vectors 1 T  X  1000 , levels  X  3 , sweeps  X  p ; / * p = k + 1 * / 2 Initialize M  X  R p  X  p to an identity matrix and compute the normalized affinity matrix W  X  R n  X  n ; 3 for j  X  1 to p do 4 t  X  0 , v 0 j  X  randn (1 ,n ) ; / * v j  X  R 1  X  n * / 5 repeat 8 t  X  t + 1 ; 9 until k  X  t +1 j  X   X  t j k max  X   X  or t  X  T ; 10 V = [ v 1 ; ... ; v p ] ; 11 V  X  whiten ( V ) , E  X  MV ; / * c has l = p 2 tuples * / 12 c  X  (( a 1 ,a 2 ,I 1 = I ( v a 1  X  ; v a 2  X  )) ,..., ( a 13 for level  X  1 to levels do 14 for sweep  X  1 to sweeps do 15 c  X  order_descending_by_I_value ( c ) ; 16 for j  X  1 to l do 17 if c j 3 &gt; 0 . 1 then 18 [  X   X  ,c j 3 ]  X  greedy_search ( c j 1 ,c j 2 , E ) ; 19 E  X  G ( c j 1 ,c j 2 , X   X  ) E ; 20 M  X  G ( c j 1 ,c j 2 , X   X  ) M ; 21 compute kurtosis of each pseudo-eigenvector in E and return the pseudo-eigenvectors associated with the top k minimum values; Competitors : To evaluate the performance of FUSE, we adopt three spectral clustering methods NCut [16], NJW [15] and ZP [23] and power-iteration-based clustering methods PIC [11], PIC-k [10], DPIC [18] and DPIE [5] as competitors. FUSE and all the comparison methods are written in Matlab. All experiments are run on the same machine with an Intel Core Quad i7-3770 with 3.4 GHz and 32 GB RAM. The code of FUSE and all synthetic and real-world data used in this paper are available at the website Parameters : The parameters for spectral clustering, power-iteration-based clustering methods are set according to their origi-nal papers. For FUSE, we set  X  i = i  X d log( k ) e X  1 e  X  5 by DPIE [5]. For text data, we use cosine similarity ( x to compute the affinity matrix. For network data, the element a of the affinity matrix A is simply 1 if blog i has a link to j or vice versa, otherwise a ij = 0 . For all other data, the locally scaled affin-ity matrix is computed as the way proposed in [23] with K The original ZP method automatically chooses the number of clus-ters. For a fair comparison, we give ZP the correct number of clus-ters.

Since all the comparison algorithms use k -means in the last step, in each experiment k -means is run 100 times with random starting points and the most frequent cluster assignment is used [11]. We run each experiment 50 times and report the mean and standard deviation of Adjusted Mutual Information (AMI) [19]. For AMI, higher value means better clustering.
Synthetic dataset S YN 1 is the running example used in Section 1. S YN 1 has three clusters. Each of the two Gaussian clusters has 100 data points and the stripe cluster has 400 data points. We have shown the results before.

Synthetic dataset S YN 2 has three clusters as well depicted in Fig-ure 4(a). Both Gaussian clusters have 100 data points and the rect-angular cluster has 400 data points. Both Gaussian clusters have some very close data points to the rectangular cluster making them hard to be separated correctly. The mean AMI of FUSE is 0.750 and the highest of the comparison algorithms X  is 0.574 achieved by PIC-k . ZP only has a value 0.483. Figure 4(b) X  (d) give us an intuitive demonstration. FUSE just wrongly clustered a few data points in the magenta rectangular cluster to the blue Gaussian clus-ter, while ZP and PIC-k wrongly clustered about a half of the data points in the magenta rectangular cluster to the blue Gaussian clus-ter. Our algorithm is superior to the competing algorithms.
S YN 3 in Figure 5(a) also has three clusters. Two Gaussian clus-ters each has 90 and 92 data points, respectively. The blue ring cluster has 130 data points. S YN 3 is very interesting because the density of the blue ring cluster is lower than those of the Gaussian clusters. And the ring cluster is very close to the Gaussian clusters, which could make the K NN = 7 neighbors of some points in the blue ring cluster be belonging to the Gaussian clusters. S also difficult to cluster correctly. However, FUSE achieved the best compared to all the comparison algorithms. PIC-k even clustered several data points belonging to two Gaussian clusters to the ring cluster, which did not make sense.

S YN 4 in Figure 6(a) contains five clusters, each of the two Gaus-sian clusters has 100 data points, each of the two square clusters has 82 and 100 data points respectively and the ring cluster has 56 data points. The ring cluster is very close to the square clusters and even has some overlap with the two Gaussian clusters. Still, FUSE achieved the best result, only not distinguishing between the over-lapped data points from the Gaussian clusters and the ring cluster. https://github.com/yeweiysh/FUSE ZP wrongly detected a half of the data points in the ring cluster to the green Gaussian cluster. PIC-k wrongly clustered several data points in the ring cluster to the black square cluster although the densities of these two clusters are significantly different.
For all these multi-scale synthetic datasets, our algorithm FUSE outperforms all the competing algorithms. FUSE is even superior to the spectral clustering algorithms ZP, NCut and NJW, which proves that the normalized cut criterion is not alway suitable for clustering. FUSE-E is our algorithm using the exhaustive search strategy over  X  proposed in [8, 7] which does not discriminate the order of optimization of the pseudo-eigenvectors generated by PI. We can see that sometimes it gets trapped in local optimal (the re-sult on S YN 3). Our algorithm FUSE adopting the greedy search strategy achieves quite similar or even better results than using the exhaustive search strategy.
In this experiment, we want to test the runtime against the num-ber of data points using data S YNC 5. Synthetic dataset S generated as follows: Firstly, we generate two 2D clusters sampled from uniform distributions with the number of data points 4,000 and 1,000 respectively. The two clusters are our basis clusters. Then at each step we increase the data points in each cluster by the size of its basis. Finally, we have data points varying from 5,000 to 30,000 by a step size 5,000. We feed each algorithm with the same affinity matrix. Thus, the runtime does not include the computa-tion time for the affinity matrix. The results are demonstrated in Figure 7. Since the runtime of NJW and NCut are similar, here we only show the runtime of NCut for a clearer demonstration. Figure 7 shows that the runtime of FUSE becomes much lower than that of ZP, NCut and DPIC when increasing the number of data points. Compared to PIC, we can see their slope variances are quite simi-lar. The runtime difference between FUSE and PIC is owing to that FUSE needs to determine the directions in which the entropy of pseudo-eigenvectors is minimized. Compared with PIC-k , the run-time of FUSE becomes close to that of PIC-k when the number of data points increases to 30,000. Note that FUSE-E is our algorithm using the exhaustive search strategy. FUSE is faster than FUSE-E as can be seen from the figure. Our algorithm is of more practical use than ZP, NCut, NJW and DPIC.
 Table 2: AMI on Synthetic Data (mean  X  standard deviation)
Now we demonstrate the effectiveness of our FUSE on seven real-world datasets. P ENDIGITS is available from UCI machine learning repository 2 . The original datasets MNIST, 20N GROUPS , R EUTERS 21578, TDT2 and RCV1 are available at this http://archive.ics.uci.edu/ml/ website 3 . 20 NG D from [11] is a subset of 20N EWSGROUPS MNIST0127 from [18] is a subset of MNIST. TDT2_3 CLASSES , R
EUTERS _4 CLASSES and RCV1_4 CLASSES are samples from original R EUTERS 21578, TDT2 and RCV1 corpus using ran-dom indices from the website 3 . A GBLOG is a connected network dataset of 1222 liberal and conservative political blogs mined from blog homepages [11]. For text datasets, we use the preprocessed document-term matrix to compute the TF-IDF matrix. Then each feature vector is normalized to have unity norm. Finally, we use cosine similarity to compute the affinity matrix. The statistics of all datasets are given in Table 3.

From Table 4, we can see that on all datasets, FUSE achieves the best results, even outperforms self-tuning spectral clustering al-gorithm (ZP) and the conventional spectral clustering algorithms (NCut and NJW). Compared to PIC and PIC-k , FUSE improves AMI on each dataset. The most likely reason is the pseudo-eigenvectors found by FUSE are statistically independent (non-redundant), which make every cluster stand out in each pseudo-eigenvector. Compared to DPIC and DPIE which also aim at re-ducing redundancy in pseudo-eigenvectors generated by PI, FUSE http://www.cad.zju.edu.cn/home/dengcai/Data/data.html improves AMI much on most datasets. One reason is that finding directions in which the entropy is minimized is much more benefi-cial to clustering.
 Two most interesting results are on A GBLOG and TDT2_3 CLASSES datasets. All comparison methods except PIC and PIC-k fail on A GBLOG dataset. A GBLOG dataset has two balanced clusters with the number of instances 586 and 636, respectively. We show the most frequent data embeddings of FUSE and ZP (the results of NJW and NCut are very similar) in Figure 8(a), (b) and (c). We can see that most data points (blue ones in Figure 8(b) and (c)) are assigned to one cluster, which makes the results of ZP and PIC-k not appealing. However, our algorithm finds the embedding space in which two clusters are separated evenly.
Figure 9 shows the clustering results on the TDT2_3 CLASSES dataset. Figure 9(b) demonstrates the eigenvector space found by ZP, in which the red square cluster and the dot magenta cluster are connected together. ZP only achieves 0.673 in terms of AMI. PIC-k found two pseudo-eigenvectors. Also in its found embed-ding space, two clusters (blue and magenta) are not well separated. However, in the embedding space detected by our algorithm, three clusters are well separated, which makes the value of AMI much higher than those of the competing methods as can be seen in Ta-ble 4. Thus, the embedding space found by our algorithm is much more attractive and effective.

If we look into Table 5, we can find that we have six datasets on which the runtime of our method is much lower than that of NCUT and NJW. And we also have four datasets on which our method is faster than ZP. Our method is very efficient and promising for prac-tical use. However, on P ENDIGITS dataset, FUSE is slower than the conventional spectral clustering algorithms because the maxi-mal rank M considered by KGV is close to n which costs much time to compute the pairwise mutual information. In this section, we apply our algorithm on image segmentation. Figure 10 shows two examples from the Berkeley Segmentation Dataset and Benchmark 4 . Each pixel is represented as a five di-mensional vector of its pixel coordinates x and y , and the color intensities [3]. We set the number of clusters in Figure 10(a) three and four for Figure 10(e). Since the results returned by ZP, NCut and NJW are very similar, we only show the results of ZP here. For other methods, we show such methods whose results are more interpretable. Figure 10(b) shows that FUSE separates the deer, the grass and the forest very well. ZP correctly segments the grass, but does not distinguish the deer from the forest, while PIC-k recog-nizes the deer but not the grass or the forest. Compared to Figure 10(a), Figure 10(e) demonstrates a more challenging task because the two elephants are very similar in terms of the color and posi-tion. However, our method FUSE successfully distinguishes these two similar elephants and also recognizes the sky. ZP cannot sep-https://www.eecs.berkeley.edu/Research/Projects/CS/vision/bsds/ BSDS300/html/dataset/images.html arate the two elephants, but the sky is well segmented. DPIE can also distinguish the two elephants but the segmentation is worse than FUSE X  X . In addition, DPIE does not segment the sky well.
Spectral clustering . Spectral clustering is very popular in data ming owing to its ability to detect arbitrary-shape clusters in data spectrum space. Spectral clustering can be divided into three cat-egories by the type of Laplacian matrix, i.e., unnormalized spec-tral clustering, normalized spectral clustering proposed by Shi and Malik (NCut) [16] and another normalized spectral clustering pro-posed by Ng, Jordan and Weiss (NJW) [15]. After deciding the type of Laplacian matrix, it computes the first k eigenvectors of the Laplacian matrix and then uses k -means to cluster in the space formed by these eigenvectors. Spectral clustering is very elegant. However, the computation cost is very high for large-scale data. Finding eigenvectors takes O ( n 3 ) in general. Recently, researchers have proposed many fast approximating techniques, such as IRAM and sampling techniques [4, 22]. Spectral clustering assumes that the  X  X nformative X  eigenvectors are those associated with the small-est k eigenvalues, which seems not to be successful on some real-world data with much noise or multi-scale density. And this pro-motes many researchers work on how to select much informative eigenvectors, and how to estimate the local scale of data with vary-ing densities, shapes and levels of noise [23, 9, 21, 3]. ZP [23] is a representative of all these algorithms. ZP takes local scal-ing into consideration and constructs a locally scaled affinity ma-trix which proves beneficial to clustering especially for multi-scale data or data with irregular background clutter. ZP also exploits the structure of eigenvectors to improve clustering. As NCut, NJW and other spectral-based clustering methods, ZP only uses the first k eigenvectors to cluster, which is not appropriate in some cases. However, our algorithm FUSE exploits all  X  X nformative X  eigenvec-tors and fuses all their information to accomplish better clustering.
Power-iteration-based clustering . Power Iteration Clustering (PIC) [11] uses truncated power iteration on a normalized affinity matrix of the data points to find a very low-dimensional data em-bedding which is a linear combination of the major eigenvectors for clustering. It is very elegant and efficient. However, the as-sumptions it bases on are very strict and it returns only one pseudo-eigenvector which prevents its performance on data with large num-ber of clusters, where cluster-collision problem is easy to happen. PIC-k [10] has been proposed to alleviate the situation but actu-ally it still cannot solve the cluster-collision problem due to much similarity exists in the returned pseudo-eigenvectors. Another clus-tering algorithm based on power iteration is Deflation Power Itera-tion Clustering (DPIC)[18] which uses Schur complement deflation to generate multiple orthogonal pseudo-eigenvectors. However, the pseudo-eigenvectors still contain noise together with cluster-separation information. Diverse Power Iteration Clustering (DPIE) [5] normalizes the residue (regression) error which is obtained by subtracting the effects of the already-found DPIEs from the em-beddings returned by PIC. However, DPIE cannot guarantee to find diverse embeddings in every iteration and it bases on the as-sumption that clear eigen-gap exists between every two succes-sive eigenvalues which is also very strict. Our method FUSE does not make any assumptions and finds statistically indepen-dent pseudo-eigenvectors, each of which is a different linear com-bination of the original eigenvectors. Besides, each statistically independent pseudo-eigenvector eliminates noise and only keeps cluster-separation information which makes FUSE much more ad-vanced and effective.
We have proposed FUSE to handle multi-scale data on which the normalized cut criterion tends to fail even given a suitable locally scaled affinity matrix. FUSE exploits PI and ICA to fuse all  X  X nfor-mative X  eigenvectors to yield better clustering. Since the pseudo-eigenvectors fused by PI are redundant and the cluster-separation information does not stand out, ICA is adopted to reduce the redun-dancy. Then, a kurtosis -based selection strategy is used to improve cluster separation. To speed up the search process, we have devel-oped a greedy search method which learns from its history search records and also adaptively adjusts its search resolution. Extensive experiments and evaluations on various synthetic and real-world data show FUSE X  X  promising in dealing with multi-scale data. Fu-ture work would go to explore how to adaptively select the number of pseudo-eigenvectors for different datasets.
 Warm thanks to Hao Huang for his DPIE code and anonymous re-viewers. This work has been supported by the China Scholarship Council (CSC). [1] F. R. Bach and M. I. Jordan. Kernel independent component [2] C. B X hm, C. Faloutsos, and C. Plant. Outlier-robust [3] C. D. Correa and P. Lindstrom. Locally-scaled spectral [4] C. Fowlkes, S. Belongie, F. Chung, and J. Malik. Spectral [5] H. Huang, S. Yoo, D. Yu, and H. Qin. Diverse power [6] A. Hyv X rinen. Fast and robust fixed-point algorithms for [7] S. Kirshner and B. P X czos. Ica and isa using schweizer-wolff [8] E. G. Learned-Miller et al. Ica using spacings estimates of [9] Z. Li, J. Liu, S. Chen, and X. Tang. Noise robust spectral [10] F. Lin. Scalable methods for graph-based unsupervised and [11] F. Lin and W. W. Cohen. Power iteration clustering. In [12] F. Lin and W. W. Cohen. A very fast method for clustering [13] M. Meila and J. Shi. A random walks view of spectral [14] B. Nadler and M. Galun. Fundamental limitations of spectral [15] A. Y. Ng, M. I. Jordan, Y. Weiss, et al. On spectral [16] J. Shi and J. Malik. Normalized cuts and image [17] Z. Szab X , B. P X czos, and A. L  X  orincz. Undercomplete blind [18] N. D. Thang, Y.-K. Lee, S. Lee, et al. Deflation-based power [19] N. X. Vinh, J. Epps, and J. Bailey. Information theoretic [20] U. Von Luxburg. A tutorial on spectral clustering. Statistics [21] T. Xiang and S. Gong. Spectral clustering with eigenvector [22] D. Yan, L. Huang, and M. I. Jordan. Fast approximate [23] L. Zelnik-Manor and P. Perona. Self-tuning spectral
