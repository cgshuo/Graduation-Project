 Modern data mining settings involve a combination of attribute-valued descriptors over entities as well as specified relation-ships between these entities. We present an approach to cluster such non-homogeneous datasets by using the rela-tionships to impose either dependent clustering or disparate clustering constraints. Unlike prior work that views con-straints as boolean criteria, we present a formulation that allows constraints to be satisfied or violated in a smooth manner. This enables us to achieve dependent clustering and disparate clustering using the same optimization frame-work by merely maximizing versus minimizing the objective function. We present results on both synthetic data as well as several real-world datasets.
 Categories and Subject Descriptors: H.2.8 [Database Management]: Database Applications -Data Mining; I.2.6 [Artificial Intelligence]: Learning General Terms: Algorithms, Measurement, Experimenta-tion.
 Keywords: Clustering, relational clustering, contingency tables, multi-criteria optimization.
This paper focuses on algorithms for mining non-homogeneous data involving attribute-valued descriptors over objects from different domains and connected through a relationship. Con-sider, for instance, the schematic in Fig. 1 (top) which re-veals many-many relationships between companies and coun-tries. Each company is characterized by a vector indicating stock values, profit margins, earnings ratios, and other fi-nancial indicators. Similarly, countries are characterized by vectors in a different space, denoting budget deficits, infla-tion ratio, unemployment rate, etc. Each company is also related to the countries that it conducts business in.
Since Fig. 1 (top) has two different vector spaces and one relation, there can be diverse objectives for clustering such a non-homogeneous dataset. We study two broad objectives here. In Fig. 1 (bottom left), we seek to cluster compa-nies (by their financial indicators) and cluster countries (by their economic indicators) such that the relationships be-tween individual entities are preserved at the cluster level. In other words, companies within a cluster tend to do busi-ness exclusively with countries in a corresponding cluster. In Fig. 1 (bottom right), we identify clusters of companies and clusters of countries where the original relationships be-tween companies and countries are actually violated at the cluster level. In other words, clusters in the company space tend to do business with (almost) all clusters in the country space. These two conflicting goals of clustering are meant to reflect two competing hypotheses about companies and their economic performances: 1. Dependent clustering: Fortunes/troubles of indi-2. Disparate clustering: Diversification helps prepare Observe that in either case, the clusters are still local in their respective attribute spaces, i.e., points within a cluster are similar whereas points across clusters are dissimilar.
Without advocating any point of view, we posit that it is important to design clustering algorithms that can sup-port both the above analysis objectives. The need for clus-tering non-homogeneous data with such conflicting crite-ria arises in many more contexts, including bioinformatics (studied here), social networks [15], hypertext modeling, rec-ommender systems, paleontology, and epidemiology.

The chief contributions of this paper are: Figure 1: Clustering non-homogeneous data with two different criteria. Here both domains are clus-tered into three clusters each based on their at-tribute vectors. (left) Dependent clustering. (right) Disparate clustering. Figure 2: Contingency tables for (left) dependent clustering and (right) disparate clustering for the scenarios from Fig. 1. As stated in the introduction, we require our clusters to have two key properties. First, the individual clusters must be local in the respective attribute spaces. Second, when compared across relationships, the clusters must be either highly dependent on each other, or highly independent of each other. We present a uniform framework based on con-tingency tables that works for both dependent and disparate clusterings.

Fig. 2 presents contingency tables for the two clusterings from Fig. 1. The tables are 3  X  3, where the rows denote the clusters from the left domain (here, company clusters) and the columns denote the clusters from the right domain (here, country clusters). The cells indicate the number of entries from the corresponding clusters that are related in the original dataset. For instance, cell (1,1) of Fig. 2 (left) indicates that there are 4 relationships between entities in Cluster 1 of the companies dataset and entities in Cluster 1 of the countries dataset. Observe that the actual sizes of the clusters are not reflected in this matrix, just the number of relationships. Contrast this cell with the corresponding entry of the disparate case, which shows the smaller number of relationships (viz. 2) obtained from a different clustering.
Thus the ideal dependent case is best modeled by a diago-nal or permutation contingency matrix. In practice, we can aim to achieve a diagonally dominant matrix. Similarly, the disparate case is modeled by a uniform (or near uniform) distribution over all the contingency table entries. It is im-portant to note, however, that we do not have direct control over the contingency table entries. These en-tries are computed from the clusters, which are in turn de-fined by the prototype vectors. So the only variables that we can adjust are the prototype vectors but the optimization criteria must be stated in terms of the resulting contingency tables. Mathematically, In reverse order, Here, the free parameters are the prototypes (Prototypes1, Prototypes2) and the objective function Obj is meant to be either minimized (for disparate clustering) or maximized (for dependent clustering).
Let X and Y be two datasets, where X = { x s } ,s = 1 ,...,n x is the set of (real-valued) vectors in dataset X , where each vector is of dimension l x , i.e., x s  X  R l x (Like-wise Y = { y t } ,t = 1 ,...,n y , y t  X  R l y ). The many-to-many relationships between X and Y are represented by a n x  X  n binary matrix B , where B ( s,t ) = 1 if x s is related to y B ( s,t ) = 0. Let C ( x ) and C ( y ) be the cluster indices, i.e., indicator random variables, corresponding to the datasets X and Y and let k x and k y be the corresponding number of clusters. Thus, C ( x ) takes values in { 1 ,...,k x } and C takes values in { 1 ,...,k y } . We present our formalisms in accordance with the structure of Eqn. 1 so that they can then be composed to yield the objective function. Let m i, X be the prototype vector for cluster i in dataset X (similarly m j, Y ). (These are precisely the quantities we wish to estimate/optimize for, but in this section, as-sume they are given). Let v ( x s ) i (likewise v ( y t ) ter membership indicator variables, i.e., the probability that data sample x s is assigned to cluster i in dataset X (resp). means hard assignment is given by: v (Likewise for v ( y t ) j .) Ideally, we would like a continuous function that tracks these hard assignments to a high degree of accuracy. A standard approach is to use a Gaussian kernel to smooth out the cluster assignment probabilities. Here, we present a novel smoothing formulation which provides tunable guarantees on its quality of approximation and for which the Gaussian kernel is a special case. First we define  X  where is the pointset diameter. We now use argmin cluster assignments so the goal is to track min with high accuracy. The approach we take is to use the Kreisselmeier-Steinhauser ( KS ) envelope function [13] given by where  X  0. The KS function is a smooth function that is infinitely differentiable (i.e., its first, second, 3rd, .. derivatives exist). Using this the cluster membership indi-cators are redefined as:
An analogous equation holds for v ( y t ) j . The astute reader would notice that this is really the Gaussian kernel approx-imation with  X /D being the width of the kernel. However, this novel derivation helps tease out how the width must be set in order to achieve a certain quality of approximation. Notice that D is completely determined by the data but  X  is a user-settable parameter, and precisely what we can tune.
Preparing the k x  X  k y contingency table (to capture the relationships between entries in clusters across X and Y ) is now straightforward. We simply iterate over every combina-tion of data entities from X and Y , determine whether they have a relationship, and suitably increment the appropriate entry in the contingency table: We also define where w i. and w .j are the row-wise and column-wise counts of the cells of the contingency table respectively.
We will find it useful to define the row-wise random vari-ables  X  i ,i = 1 ,...,k x and column-wise random variables  X  ,j = 1 ,...,k y with probability distributions as follows The row wise distributions represent the conditional distri-butions of the clusters in dataset in X given the clusters in Y ; the column wise distributions are also interpreted analo-gously.
Now that we have a contingency table, we must evaluate it to see if it reflects a dependent or disparate set of clusterings (as the requirement may be). Ideally, we would like one criterion that when minimized leads to a disparate clustering and when maximized leads to a dependent clustering.
For this purpose, we compare the row-wise and column-wise distributions from the contingency table entries to the uniform distribution. (In the example from Fig. 2, there are three row-wise distributions and three column-wise distri-butions.) For dependent clusters, the row-wise and column-wise distributions must be far from uniform, whereas for disparate clusters, they must be close to uniform. We use KL-divergences to define our unified objective function:
F = 1 where the KL-divergence between distributions p 1 ( x ) and p ( x ) over the sample space X is given by: D
KL [ p 1 || p 2 ] measures the inefficiency of assuming that the distribution is p 2 when the true distribution is actually p
Note that the row-wise distributions take values over the columns 1 ,...,k y and the column-wise distributions take values over the rows 1 ,...,k x . Hence the reference distribu-tion for row-wise variables is over the columns, and vice versa. Also, observe that the row-wise and column-wise KL-divergences are averaged to form F . This is to miti-gate the effect of lopsided contingency tables ( k x k y or k y k x ) wherein it is possible to optimize F by focusing on the  X  X onger X  dimension without really ensuring that the other dimension X  X  projections are close to uniform.
Finally observe that the KL-divergence of any distribution with respect to the uniform distribution is proportional to the negative entropy (  X  H ) of the distribution. Thus we are essentially aiming to minimize or maximize (for dependent or independent clusters) the entropy of the cluster condi-tional distributions between pairs of two datasets.
Now we are ready to formally present our data mining algorithms as optimization over the space of prototypes.
Here the goal is to minimize F , a non-linear function of m i, X and m j, Y . For this purpose, we adopt an augmented Lagrangian formulation with a quasi-Newton trust region algorithm. We require a flexible formulation with equality constraints (i.e., that mean prototypes lie on the unit hy-persphere) and bound constraints (i.e., that the prototypes are bounded by the max and min (componentwise) of the data, otherwise the optimization problem has no solution). The LANCELOT software package [6] provides just such an implementation.

For ease of description, we  X  X ackage X  all the mean proto-type vectors for clusters from both datasets (there are k of them) into a single vector  X  of length t . The problem to solve is then: argmin F (  X  ) subject to h i (  X  ) = 0 , i = 1 ,..., X , where  X  is a t -dimensional vector and F , h i are real-valued functions continuously differentiable in a neighborhood of the box [ L,U ]. Here the h i ensure that the mean proto-types lie on the unit hypersphere (i.e., they are of the form || m 1 , X || X  1, || m 2 , X || X  1,  X  X  X  , || m 1 , Y || X  1, || m The bound constraints are uniformly set to [  X  1 , 1]. The augmented Lagrangian  X  is defined by where the  X  i are Lagrange multipliers and  X  &gt; 0 is a penalty parameter. The augmented Lagrangian method (implemented in LANCELOT) to solve the constrained optimization prob-lem above is given in OptPrototypes .
 Algorithm 1 OptPrototypes 1. Choose initial values  X  (0) (e.g., via a k-means algo-rithm),  X  (0) , set k := 0, and fix  X  &gt; 0. 2. For fixed  X  ( k ) , update  X  ( k ) to  X  ( k +1) by using one step of a quasi-Newton trust region algorithm for minimizing
 X   X , X  ( k ) , X  subject to the constraints on  X  . Call Prob-lemSetup with  X  as needed to obtain F and  X  X  .  X  . 4. If  X  ( k ) , X  ( k ) has converged, stop; else, set k := k + 1 and go to (2). 5. Return  X  .
 In Step 1 of OptPrototypes , we initialize the prototypes using a k-means algorithm (i.e., one which separately finds clusters in each dataset without coordination), package them into the vector  X  , and use this vector as starting points for op-timization. For each iteration of the augmented Lagrangian method, we require access to F and  X  X  which we obtain by invoking Algorithm ProblemSetup .
 Algorithm 2 ProblemSetup 1. Unpackage  X  into values for mean prototype vectors. 2. Use Eq. (2) (and its analog) to compute v ( x s ) i and v 3. Use Eq. (3) to obtain contingency table counts w ij . 4. Use Eqs. (4) and (5) to define r.v.s  X  i and  X  j . 5. Use Eqn. (6) to compute F and  X  X  (see [19].) 6. Return F ,  X  X  .
 This routine goes step-by-step through the framework de-veloped in earlier sections to link the prototypes to the ob-jective function. There are no parameters in these stages except for  X  which controls the accuracy of the KS approxi-mations. It is chosen so that the KS approximation error is commensurate with the optimization convergence tolerance. Gradients (needed by the trust region algorithm) are math-ematically straightforward but tedious, so are not explicitly given here (see [19]).

Modulo the time complexity of k-means (which is used for initializing the prototypes), the per-iteration complexity of the various stages of our algorithm can be given as follows: Step Time Complexity
Assigning vectors to clusters O ( n x l x k x + n y l y k Preparing contingency tables O ( k x k y n x n y ) (na  X   X ve) Evaluating contingency tables O ( k y k x + k x k y ) Optimization O ((  X  + 1) t 2 ) First, observe that this is a continuous, rather than dis-crete, optimization algorithm, and hence the overall time complexity depends on the number of iterations, which is an unknown function of the requested numerical accuracy. The step of assigning vectors to clusters takes place inde-pendently in the two datasets, so the time complexity has two components. For each vector, we compare it to each mean prototype, and an inner loop over the dimensionality Figure 3: Degenerate contingency tables for (left) dependent clusters and (right) disparate clusters. These are bad solutions to be avoided because the clusters in (a) are highly imbalanced and (b) is ob-tained by trivially assigning all points to all clusters. of the vectors gives O ( n x l x k x + n y l y k y ). The straightforward way to prepare contingency tables as suggested by Eqn. 3 gives rise to a costly computation, since for each cell of the contingency table (there are k x k y of them), we will expend O ( n x n y ) computations. In [19] we show how we can reduce this by an order of magnitude using a method of  X  X eplicat-ing X  vectors which helps us treat the relationship matrix B as if it were one-to-one. In this case, the per-cell complexity will be simply be a linear function of the non-zero entries in B , i.e., |B| . Evaluating the contingency tables requires us to calculate KL-divergences which are dependent on the sample space over which the distributions are compared and the number of such comparisons. There are two terms, one for row-wise distributions, and one for column-wise distri-butions. Finally, the time complexity of the optimization is O ((  X  + 1) t 2 ) per iteration, and the space complexity is also O ((  X  + 1) t 2 ), mostly for storage of Hessian matrix ap-proximations of F and h i . Note that t = k x l x + k y l  X  = k x + k y . In practice, to avoid sensitivity to local minima, we perform several random restarts of our approach, with different initializations of the prototypes.

Dependent clustering proceeds exactly as above except the goal now is to min  X  X  (i.e., to maximize F ). Simply replac-ing F with  X  X  in the above algorithm conducts dependent clustering. For ease of description later, we henceforth refer to F as F indep and to  X  X  as F dep .
Degenerate situations can arise as shown in Fig. 3. In the dependent case, we might obtain a diagonal contingency table but with imbalanced cluster sizes. In the independent case, the data points are assigned with equal probability to every cluster, resulting in a trivial solution for ensuring that the contingency table resembles a uniform distribution. See [19] for how to add additional terms in the objective function to alleviate both these issues.

A final issue is the determination of the right number of clusters, which has a direct mapping to the sufficient statis-tics of contingency tables necessary to capture differences between distributions. We have used the minimum dis-crimination information (MDI) principle (discussed later) for model selection. Due to space limitations, we are unable to cover this aspect in detail.
We evaluate our approach using both synthetic and real datasets. The questions we seek to answer through our ex-periments are: 1. Can we realize classical constrained clustering and al-Figure 4: Realizing classical single-dataset cluster-ing scenarios using our framework. (a) Cluster-ing with must-link constraints. (b) Clustering with must-not-link constraints. (c) Clustering with both must-link and must-not-link constraints. (d) Find-ing alternative clusterings. 2. How much does our emphasis on clustering relations 3. How does our approach (of defining an integrated ob-4. As the number of clusters increases, does it become 5. Can we pose integrated dependent and disparate clus-6. In mining non-homogeneous datasets with multiple cri-
In constrained clustering, we are given a single dataset D with instance level constraints such as must-link and must-not-link constraints [7, 20]. We can model such problems in our relational context as shown in Fig. 4 (a), (b), and (c). We create two copies of D into D 1 and D 2 . In the case with only must-link (ML) constraints (Fig. 4 (a)), such as between x 1 and x 3 , we create a relation between the entries: x of D 1 and x 3 of D 2 , and between entries: x 3 of D 1 and x of D 2 . In addition we include relations between the same instances in D 1 and D 2 . Applying the dependent clustering criterion F dep on this dataset will realize the constrained clustering scenario. Conversely, as shown in Fig. 4 (b), for must-not-link (MNL) constraints we would create relations between the entries that should not be brought together, and use F indep as the optimization criterion. In Fig. 4 (a), the relations would force the clusterings to be dependent and as (right) Normalized mutual information. a result, either clustering would respect the ML constraints. In Fig. 4 (b), the F indep objective will force the clusterings to violate the relations (which are really MNL constraints).
Going further, we can combine the above modeling ap-proaches in Fig. 4 (c) which has both ML and MNL con-straints. For this scenario, the optimization criterion is es-sentially a convex combination of both F dep and F indep . As we vary  X  smoothly from 0 to 1, we increase our emphasis from satisfying the ML constraints to satisfying the MNL constraints. Here we set  X  to 0.5 (and explore other settings in future sections). We compare our constrained cluster-ing framework with simple unconstrained k-means and two constrained k-means algorihtms (MPCK-M EANS and PCK-M EANS ) from [4]. In overall, the number of constraint viola-tions from our approach (Fig. 5 (left)) is worse than that of either MPCK-M EANS and PCK-M EANS , except for a small number of clusters. This is to be expected since our method does not take a strict (boolean) view of constraint satisfac-tions. Conversely, the objective function in our approach is the best possible value (Fig. 5 (middle)) when compared with the solutions obtained by the other three algorithms. Finally, as shown in Fig. 5 (right), the normalized mutual information score (between the cluster assignments and the class labels) is best for our approach than for the other three algorithms. This shows that taking a soft view of constraints does not compromise the locality of the mined clusters.
We investigate alternative clustering using the Portait dataset as studied in [11]. This dataset comprises 324 images of three people each in three different poses and 36 illumina-tions. Pre-processing involves dimensionality reduction to a grid of 64  X  49 pixels. The goal of finding two alternative clusterings is to assess whether the natural clustering of the Table 1: Contingency tables in analysis of the Por-trait dataset. (a) After k-means with random ini-tializations. (b) after disparate clustering. images (by person and by pose) can be recovered. We utilize the same 300 features as used in [11] and setup our frame-work as shown in Fig. 4 (d). Two copies of the dataset are created with one-to-one relationships and we aim to cluster the dataset in a disparate manner.

Table 1 shows the two contingency tables in the analysis of the Portrait dataset and table 2 depicts the achieved accura-cies using simple k-means, convolutional-EM [11], decorrelated-kmeans [11] and our framework for disparate clustering. Our algorithm performs better than all other tested algorithms according to both person and pose clusterings.
 Fig. 6 shows how the accuracies of the person and the Figure 6: Monotonic improvement of objective func-tion (finding alternative clusterings for the Portrait dataset). pose clusterings improve over the iterations, as the objec-tive function is being minimized. The quasi-Newton trust region algorithm guarantees the monotonic improvement of the objective function without directly enforcing error met-rics over the feature space. But because the objective func-tion captures the dissimilarity between the two clusterings, indirectly, we notice that the accuracies w.r.t. the two al-ternative clusterings improve with the increase in number of iterations (though, not monotically).
In this section, we consider two synthetic datasets with one (possibly many-many) relationship between them. The parameters we study are: l x ,l y , the dimensions of the vectors (varied from 4 to 20); n x ,n y , the number of vectors (fixed at 100, because as our time complexity analysis shows, they only affect the step of assigning vectors to clusters); k the number of clusters sought (also varied from 4 to 20; and |B| , the number of relationships between the datasets (varied from a one-to-one case to about a density of 50%). The vectors were themselves sampled from (two different) mixture-of-Gaussians models.

Fig. 7 (a) answers the question of whether our approach yields local clusters as the number of relationships increase (and hence each dataset is more influenced by the other). In this figure, we used settings of 4 and 20 clusters and used our framework to find dependent as well as disparate clus-ters, and also compared them with k-means (which doesn X  X  use the relationship). Fig. 7 (a) shows that even though the k-means algorithm is mining two separate datasets indepen-dently, our algorithms achieve very closely comparable re-sults in spite of the co-ordination (dependence or disparate) Figure 8: Our approach helps drive a k-means clus-ter assignment (top) toward either dependent (bot-tom left) or disparate (bottom right) sets of clusters. requirements. Thus, locality of clusters in their respective attribute spaces is not compromised and unvarying with the sparsity of the relationship matrix. At the same time, as Fig. 8 shows (for the case of four clusters), we achieve the specified contingency table criteria.

Fig. 7 ((b),(c)) shows the runtime for our algorithm as a function of attribute vector dimensions (i.e., l x number of clusters (i.e., k x ,k y ). We vary one parameter, keeping the other fixed ( k x ,k y fixed at 8 versus l x , l at 12). In overall these plots track the complexity analy-sis presented earlier except for the higher dimension/cluster settings which show steeper increases in time. This can be attributed to the greater number of iterations necessary for convergence in these cases.

Finally, we explore how our results are influenced by the number of clusters, for both dependent as well as disparate clustering formulations. (see Fig. 7 (d)). As the number of clusters increases, both objective criteria ( F dep and F become difficult to attain, but for different reasons (recall that the intent of both criteria is to be minimized). In the case of dependent clusters, although the problem gets easier as clusters increase (every point can become its own cluster), the objective function scores get lower due to our regular-ization as explained in Section 4. In the case of disparate clusters, as the number of clusters increases, the size of the contingency table increases quadratically with the number of samples staying constant. As a result, it becomes diffi-cult to distribute the samples across the contingency table entries without introducing some level of dependence (i.e., some entries must be zero implying dependence).
In this study, we focus on time series gene expression pro-files collected over heat shock experiments done on organ-isms of varying complexity: H : human cells (4 time points) [17], Y : yeast (8 time points) [10], and C : C. elegans (worm; 7 time points) [16]. We also gathered many-many (top-k) ortholog information between the three species. A typical goal in multi-species modeling is to identify both conserved gene expression programs as well as differentiated gene ex-pression programs. The former is useful for studying core metabolism and stress response processes, whereas the lat-ter is useful for studying species-specific functions (e.g., the yeast is more tolerant to desiccation stress, but the worm is the more complex eukaryote).

First we study a 3-way clustering setup with only two constraints, namely that clusters in H and W must be de-pendent, denoted by H = W , and that clusters in W and Y must be disparate, denoted by W &lt;&gt; Y (See Fig. 9 (top)). As the balance between these criteria is varied from one ex-treme to another (via the convex combination formulation), this curve traces out the objective function values. The top left corner is the point where complete emphasis is placed on achieving the H = W criterion (conversely for the bottom right corner). As we seek to improve the other criteria, note that we might (and will) sacrifice the already achieved crite-rion. The point of maximum curvature on this plot gives a  X  X weet spot X  so that any movement away from the sweet spot would cause a dramatic change in the objective function val-Figure 9: Balancing objectives in multi-criteria clus-tering optimization. Points of maximum curvature on these plots reveals a balancing point between the conflicting criteria. ues. A qualitatively different type of plot is shown in Fig. 9 (middle) (for the case study described in the next section) but here again the point of maximum curvature reveals a balancing threshold of the two criteria. A 3-way clustering setup with three constraints is described in Fig. 10 and its corresponding tradeoff plot is in Fig. 9 (bottom). Here there are likely multiple points of interest depending on which cri-teria are sacrificed in favor of others.
Finally, we present a case study that has a diversity of both organisms and stresses. To capture process-level simi-larities and differences, the data vectors we cluster here cor-respond to Gene Ontology categories rather than individual gene expression profiles. We used three time series datasets: CA X  C.elegans aging (7 time points), DA X  D. melanogaster aging (7 time points) and DR X  D. melanogaster caloric re-striction (9 time pints). Observe that the first two datasets share a similarity of process whereas the latter two share a similarity of organism. In a sense, the D. melanogaster aging dataset is squarely in the  X  X iddle X  of the other two datasets. When subjected to clustering together, the inherent tradeoff is what we seek to capture.

For this evaluation, we studied the enrichment of clusters obtained from our framework vis-a-vis k-means clustering. We set the number of clusters at 7 and evaluated GO terms for an FDR-corrected q -value of 0.05. First, we study the clustering setup so that DA=DR AND CA=DA, for a set-ting of  X  = 0 (so that more emphasis is placed on achieving the dependent clustering DA=DR). Here, we observed 75 GO terms enriched (versus 37 for k-means). Similar im-provements were seen for  X  = 0 . 5 (55 versus 20) and for  X  = 1 (89 versus 35). Observe the greater numbers of terms enriched in general for the extremalities (which is to be ex-pected). In terms of process-level similarities, the GO terms common across the aging datasets but which do not appear when we emphasize organism-level similarities are: Conversely, the organism-level similarities are captured in: These results show that process-level similarities involve higher order functions whereas organism-level similarities involve growth and metabolism processes. The careful interplay be-tween aging and caloric restriction, both at the organismal and at the inter-organismal level, is an interesting conclusion from this study.
MDI: The objective functions defined here have connec-tions to the principle of minimum discrimination informa-tion (MDI), introduced by Kullback for the analysis of con-tingency tables [14] (the minimum Bregman information (MBI) in [3] can be seen as a generalization of this prin-ciple). The MDI principle states that if q is the assumed or true distribution, the estimated distribution p must be chosen such that D KL ( p || q ) is minimized. In our objective functions the estimated distribution p is obtained from the contingency table counts. The true distribution q is always assumed to be the uniform distribution. We maximize or minimze the KL-divergence from this true distribution as required. Space restrictions prevent us from describing the connection to MDI in further detail.
 Co-clustering binary matrices, Associative cluster-ing, and Cross-associations: Identifying clusterings over a relation (i.e., a binary matrix) is the topic of many ef-forts [5, 8]. The former uses information-theoretic criteria to best approximate a joint distribution of two binary variables and the latter uses the MDL (minimum description length) principle to obtain a parameter-less algorithm by automat-ically determining the number of clusters. Our work is fo-cused on not just binary relations but also attribute-valued vectors. The idea of comparing clustering results using con-tingency tables was first done in [12] although our work is the first to unify dependent and disparate clusterings in the same framework.
 Finding disparate clusterings: The idea of finding dis-parate clusterings has been studied in [11]. Here only one dataset is considered and two dissimilar clusterings are sought simultaneously where the definition of dissimilarity is in terms of orthogonality of the two sets of basis vectors. This is an indirect way to capture dissimilarity whereas in our paper we use contingency tables to more directly capture the dissimilarity. Furthermore, our work enables the com-bination of similar clusterings and disparate clusterings in a more expressive way. For instance, given just two datasets X and Y with two relationships R1 and R2 between them, our work can identify clusters in X and Y that are similar from the perspective of R1 but dissimilar from the perspec-tive of R2: it is diffcult to specify such criteria in terms of the basis vectors since they will be the same irrespective of the relationship.
 Clustering over relation graphs: Clustering over rela-tion graphs is a framework by Banerjee et al. [2] that uses the notion of Bregman divergences to unify a variety of loss functions and applies the Bregman information princi-ple (from [3]) to preserve various summary statistics defined over parts of the relational schema. This framework can handle all the types of data and relationships we study here, since the notion of Bregman divergences is very general and can capture both information-theoretic criteria (from our contingency tables) as well as geometric measures (for our locality of clusters). However, our work unifies dependent with disparate clustering, whereas Banerjee et al. focuses on only the dependent case. This entails several key differ-ences. First, Banerjee et al. aim to minimize the dis-tortion as defined through conditional expectations over the original random variables, whereas our work is meant to both minimize and introduce distortion as needed , over different parts of the schema as appropriate. This leads to tradeoffs across the schema which is unlike the tradeoffs experienced in [2, 3] between compression and accuracy of modeling. (see also MIB, discussed below). A second dif-ference is that our work does not directly minimize error metrics over the attribute-value space and uses contingency tables (relationships between clusters) to exclusively drive the optimization. This leads to the third difference, namely that the distortions we seek to minimize/maximize are w.r.t. idealized contingency tables rather than w.r.t. the original relational data. The net result of these variations is that relations (idealized as well as real) are given primary impor-tance in influencing the clusterings.
 Multivariate information bottleneck: Our work is rem-iniscent of the multivariate information bottleneck (MIB) [9] which is a framework for specifying clusterings in terms of two conflicting criteria: compression (of vectors into clus-ters) and preservation of mutual information (of clusters with auxiliary variables that are related to the original vec-tors). We share with MIB the formulation of a multi-criteria objective function derived from a clustering schema but dif-fer in the specifics of both the intent of the objective func-tion and how the clustering is driven based on the objective function. Furthermore, the MIB framework was originally defined for discrete settings whereas we support a mixed modality of datasets.
We have presented a very general and expressive frame-work for clustering non-homogeneous datasets. We have also shown how it subsumes many previously defined formula-tions and that it sheds useful insights into tradeoffs under-lying complex relationships between datasets.

Our directions for future work are two fold. Thus far, we have used distinct relations to enforce disparate and depen-dent clusterings. One of the first directions for future work is to allow both types of clusterings to be captured in the same relation. This would help capture more expressive relation-ships between datasets, such as a banded diagonal structure in the contingency table. Secondly, just as the theory of functional and multi-valued dependencies (FDs and MDs) helps model relations in and between individual tuples, we aim to develop a theory of  X  X lustering dependencies X  that can help model relations in the aggregate, e.g., between clusters. This work is supported in part by US National Science Foundation grants CCF-0937133, CNS-0615181, and the In-stitute for Critical Technology and Applied Science (ICTAS) at Virginia Tech. [1] E. Bae and J. Bailey. COALA: A Novel Approach for [2] A. Banerjee, S. Basu, and S. Merugu. Multi-way [3] A. Banerjee, S. Merugu, I. S. Dhillon, and J. Ghosh. [4] M. Bilenko, S. Basu, and R. J. Mooney. Integrating [5] D. Chakrabarti, S. Papadimitriou, D. S. Modha, and [6] A. R. Conn, N. I. M. Gould, and P. L. Toint.
 [7] I. Davidson and S. S. Ravi. Clustering with [8] I. S. Dhillon, S. Mallela, and D. S. Modha.
 [9] N. Friedman, O. Mosenzon, N. Slonim, and N. Tishby. [10] A. P. Gasch, P.T. Spellman, C.M. Kao, Carmel-Harel, [11] P. Jain, R. Meka, and I. S. Dhillon. Simultaneous [12] S. Kaski, J. Nikkil  X  a, J. Sinkkonen, L. Lahti, J.E.A. [13] G. Kreisselmeier and R. Steinhauser. Systematic [14] S. Kullback and D.V. Gokhale. The Information in [15] B. Long, X. Wu, Z. Zhang, and P. S. Yu.
 [16] S. A. McCarroll, C. T. Murphy, S. Zou, S. D. Pletcher, [17] T. J. Page, D. Sikder, L. Yang, L. Pluta, R. D. [18] Z. Qi and I Davidson. A Principled and Flexible [19] S. Tadepalli. Schemas of Clustering . PhD thesis, [20] K. Wagstaff, C. Cardie, S. Rogers, and S. Schr  X  odl.
