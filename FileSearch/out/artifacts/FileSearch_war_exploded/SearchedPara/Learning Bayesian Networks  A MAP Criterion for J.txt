
For learning Bayesian Network (BN) structures, it has become common practice to use the Bayesian Dirichlet (BD) scoring criterion. In contrast to most other scoring metrics that functionally can be interpreted as regularized maximum likelihood criteria, the BD metric cannot be con-sidered as such. The functional dissimilarity of the BD met-ric compared to other metrics is an obstacle from an an-alytical point of view; this is for instance becomes clear in the context of the Structural EM algorithm for learning BNs from incomplete data. Also, it is not easy to pin-point why exactly and to what extend regularization is taken care of by applying the BD metric. We introduce a Bayesian scoring criterion that is closely related to the BD metric, but solves the obvious disadvantages of the BD metric. We arrive at this result by using the same basic assumptions as for the BD metric, but in contrast to the BD metric, where focus is on learning the model structure only, we aim at learning the most probable BN pair jointly, i.e., model structure and the parameter are selected as a pair. This approach yields a scoring metric that has the functional form of a regular-ized maximum likelihood metric. We perform experiments, and show that this MAP BN metric also yields better results than the BIC and BD metrics on independent test data.
In the last decade, graphical models have become well-known statistical tools for analyzing and reasoning about multivariate data. The BN formalism, a particular kind of graphical model, is now routinely employed in areas such as machine learning, data mining and decision support sys-tems. Since very early on, the induction, or learning of BNs, has been an active area of research. The reason for this is multifold, but primarily driven by the fact that BNs are an attractive and intuitively appealing formalism for gaining insight into the mechanism thought to have generated the multivariate data.

Model selection has a long tradition, and the issues and caveats that are inherent to model selection have been well-studied and described in literature long before BNs were conceived. In a machine learning context, the concept of overfitting is an ever-reoccurring concern that has to be taken explicit care of when doing model selection. To coun-teract this problem, learning methods employ generaliza-tion methods. In the context of learning BNs, a scoring metric is used that dampens the tendency to favor models that are too flexible. There are various scoring metrics, mo-tivated by different arguments and from different schools, such as information theory. In general however, they can all be brought into a functional form we refer to as regu-larized (maximum) likelihood . The similarity means that we may often use the metrics interchangeably depending on preferences etc., but in particular, in many likelihood-based derivations this is an advantage. Unfortunately, one of the most popular scoring metrics does not enjoy this property, namely the BD scoring metric. In this paper we derive a new scoring metric referred to as the MAP BN, which is closely related to the BD scoring metric, but this new metric can be regarded a regularized maximum likelihood criterion.
This paper is organized as follows: The next section states some preliminaries and the notation is introduced. In Section 3 we briefly discuss the existing penalized maximum-likelihood scoring metrics for learning BNs. In Section 4 the BD metric is derived and some formulas are introduced which we require in Section 5, where we derive the new MAP BN scoring metric, and discuss some regu-larization issues. In Section 6 we show that learning BNs from incomplete data using Structural EM remains possible with the new scoring metric. We performed experiments on learning BNs in Section 7, where we compare the BNs learned using the MAP BN metric with some other metrics. Finally, in Section 8 we draw conclusions. We use upper case letters to denote random variables. The corresponding lower case letters denote values of the variables. Bold letters denote vectors.

We are concerned with directed acyclic graphs (DAGs), with vertices X =( X 1 ,...,X p ) that coincide with dis-crete random variables. When X i  X  X j then X i is a parent of X j , and X j is a child of X i . Parents of X i belong to the parent set X pa ( i ) .

We use the term Bayesian Network (BN) to denote the pair B =( m,  X  m ) consisting of the DAG denoted by m with vertices X and the parameter of m denoted by  X  m =(  X  1 ,...,  X  p ) , where  X  i consists of probabilities  X 
The goal is to learn from an independent and identically distributed (iid) data sample d =( d 1 ,..., d c ) , consisting of c records. Each record d j =( x j 1 ,...,x j p ) , contains in-stantiations x j i of variable X i for i =1 ,...,p for the j th case. We reduce the data sample to contingency tables rep-resenting vectors with observational counts distributed ac-cording to a product multinomial distribution parametrized by  X  m =(  X  1 ,...,  X  p ) where  X  i consists of the local pa-rameters  X  X falling into a particular cell associated with vertex X i cell counts are extracted from the tables using the function n (  X  ) , e.g., n ( x i , x pa in the data sample of X i = x i and X pa ( i ) = x pa ( i
The likelihood function of the product multinomial dis-tribution is The last expression explicitly shows that each record j is an independent realization from the BN, B =( m,  X  m ) . For the product multinomial, the individual Maximum Likelihood (ML) parameter estimates are found in closed form via
It is well-known that learning m based on ML generally leads to overfitting. The resulting m will often be too dense (too many arcs), although this will only rarely lead to com-pletely connected DAGs (all vertices connected), overfitting needs to be taken care off. Overfitting is prevented by in-troducing a measure of model complexity. Let r i = |  X  X i and q i = |  X  X defined as This is the number of parameters that needs to be estimated for a given model; for every parent set configuration, all configurations of the child need to be determined (except one, because the probability sums to unity). Hence, for dense DAG models, there are more free parameters. The model selected is then Here f ( m ) is a regularization term and  X   X  m is the ML esti-mate of  X  m .

If f ( m )=  X   X  ( m ) then the AIC criterion [2] is obtained, derived from decision theory. Setting f ( m )=  X   X  ( m ) 2 ( c being the number of records) we get the BIC crite-rion [18], based on the large sample approximation from Bayesian theory, or equivalently, the (negative) MDL crite-rion based on coding theory [13].

The regularized maximum likelihood model selection approaches assumes that the resulting full BN will be  X  B ( X  m,  X   X   X  m ) , i.e., the parametrization will be the ML-estimate. Thus, in reality, we do not only select the most likely DAG structure, but actually select the most likely BN pair, i.e., both model and parameter. If the DAG model is selected using these metrics, any other parametrization than the ML is incorrect.
In this section we introduce the basic assumptions in or-der to derive Bayesian scoring metrics for learning BNs.
A key to a Bayesian analysis is that we now consider the parameter,  X  , as a random variable and encode all un-certainty pertaining to this variable via prior and posterior distributions. Via Bayes X  law these are related as Pr(  X  | m, d )= The denominator in eq. 3 is the normalizing term which we will return to shortly. For computational convenience we use a conjugate prior and assume that each parame-ter distribution conditional on a parent configuration can be updated independently of each other, i.e., that the parame-ters are locally and globally independent [20], Pr(  X  | m )=
The decomposition per vertex and parent set configura-tion of the product multinomial and hence the product like-lihood, coincides with the decomposition of the overall pa-rameter distribution of  X  under global and local parameter independence. The conjugate prior for the multinomial is the Dirichlet distribution. Thus, we define a Dirichlet dis-tribution for each multinomial where  X  &gt; 0 is the vector of prior hyper parameters of the Dirichlet. The normalizing factor is With global and local parameter independence, the prior
Pr(  X  | m ) effectively becomes a product Dirichlet distribu-tion .

By applying Bayes X  rule from eq. 4, it is easy to see that the posterior is again a product Dirichlet Pr(  X  | m, d )= where the (product) normalizing term for the posterior is updated with the sufficient statistics n from d
The above assumptions and derivations form the basis for the BD metric which will be discussed in the next sec-tion, and also for the MAP BN metric derived in Section 5.
Pr( d | m ) is the marginal likelihood which coincides with the normalizing term in eq. 3. We don X  X  actually need to compute this integral, because by slightly rearranging it fol-lows that
The equation reduces to the product of the ratios of the nor-malizing factors of the prior product Dirichlet and the pos-terior product Dirichlet. Using eq. 5 and eq. 6 we write eq. 7 as
Pr( d | m )=
This expression gives the probability of the data under model m . The formula was derived from a somewhat differ-ent point of view in [7, 14], where it was referred to as the
BD metric (Bayesian Dirichlet). It is clear that the BD scor-ing metric is functionally very different from the metrics discussed in Section 3. In particular, the Gamma functions form a large obstacle for further reduction and analysis of the BD metric.

In general the DAG may be subject to a full Bayesian treatment, in which case the DAG is considered as a random variable, M . Also, we may then apply Bayes X  rule again as to obtain Pr( M | d )  X  Pr( d | M )  X  Pr( M ) . The goal is then to find the MAP (maximum aposteriori) model  X  m =argmax
The BD metric is quite different from the penalized max-imum likelihood scoring approach. It takes the entire range of possible parameter assignments into consideration by ex-plicitly  X  X eighing X  the likelihood according to the parame-ter distribution, viz. the integral in the denominator of eq. 3. Although the BD metric seems to be concerned with the
DAG model only, leaving the parameter entirely out of the picture, the metric implicitly does assume a particular point estimate of the parameter. To see this, first off note that in a Bayesian statistical context with iid data and global and local parameter independence, we have that the joint over X conditional on the model structure and data alone (so, no parameter given on the conditional side), obeys the factor-ization where from the 1st integral to the 2nd integral we make use of the iid assumption.
 is the expectation with respect to the Dirichlet distributions. Hence when no parameter is specified on the conditional side of Pr( X | m, d ) , the implicit assumption is that the pa-rameter estimates should be identified as the expectations with respect to the posterior parameter distribution. Also, by adhering to the prequential approach [19, 8], stating that we may  X  X raverse X  the data sample on a per record basis us-ing the chain rule of probability theory, the BD metric in fact can be rewritten as a product of expectations, where n j (  X  ) returns the counts from d extracted from the contingency tables based on the sample ( d 1 ,..., d j ) . 1
In effect this means that the final BN should be parametrized using point estimates corresponding to the ex-pectation of the (product) Dirichlet distributions, given d . The BN pair selected according to the BD metric is there-fore B =(  X  m, E[  X  | d ]) , where  X  m is the MAP estimator of Pr( M | d ) , and the BN parameter is the expectation with re-spect to Pr(  X  |  X  m, d ) . If the BN with  X  m is parametrized with anything else than the expectation, it is incorrect. Recall that for the regularized maximum likelihood metrics some-thing similar was the case, however, there the ML estimate should be used instead.
We will now investigate a Bayesian MAP selection scheme where not only the MAP model is selected, but the MAP of the whole BN pair is selected jointly; that is, we want to learn the most probable BN, given data. Thus, the objective is to select the BN pair consisting of DAG and parameter that maximizes the posterior joint distribution Here Pr(  X  ,M ) can be regarded the prior distribution be-fore having seen data, and Pr(  X  ,M | d ) the posterior ob-tained via Bayes X  law, i.e., after having seen data.
In the MAP context the normalization term Pr( d ) need not be computed, and it suffices to maximize the numera-tor. By filling in Pr( d | M,  X  ) and Pr(  X  | M ) and slightly rewriting, we get The likelihood is easily recognized as that of a product multinomial sample, consisting of the data plus the prior ob-servations (minus 1), i.e., Pr( d ,  X   X  1 | M,  X  ) . It is worth observing that the regularization term is a combination of the prior model distribution Pr( M ) and the normalizing factor of the prior product Dirichlet distribution (a product of eq. 5), which in turn is also dependent on M ; we reduce this to the regularization term f ( M,  X  ) . In the remainder of this paper we leave Pr( M ) out of the picture, and assume a uniform distribution on DAG models (equiprobable).
Given M , we know the closed form solution for the pa-rameter that maximizes the likelihood based on the statistics ( d ,  X   X  1 ) , namely the ML estimator given in eq. 1; this is equivalent to the MAP of the Dirichlet given d . Since the regularization term does not depend on the choice of param-eters, this solution also maximizes Pr( M,  X  | d ) . Hence, in order to obtain  X  B =argmax B log Pr( B| d ) ,wefind  X  m =argmax such that the MAP BN becomes  X  B =( X  m,  X   X   X  m ) , where is the ML estimator based on the statistics ( d ,  X   X  1 ) .
The similarity with the regularized maximum likelihood scoring metrics in Section 3 is obvious. Observe that the role of the prior hyper parameters is twofold: they act as prior counts by adding to d the number of instances ob-served  X  X n the past X  and they determine the degree of reg-ularization. For large samples/small prior counts we have Pr( d ,  X   X  1 | M,  X  )  X  Pr( d | M,  X  ) , because  X  will then have an insignificant influence. Consequently, the main role of  X  is therefore to determine the degree of regularization. More on this in next section.
To gain some insight into the regularization term, we rewrite it under the assumption that for all configurations The term p i =1 r i q i is the number of parameters which is semantically related to the number of free parameters de-fined in eq. 2. However, in contrast to the number of free parameters, the number of parameters is not the same for equivalent DAGs, i.e., for DAGs that are statistically indis-tinguishable. Still, when the complexity of the DAG in-creases, i.e., more arcs are introduced, then this term also increases.

Looking at the term we see that it is independent of the model complexity; it only depends the cardinality of the state space of X i . Notice that the term may differ between vertices.

By investigating at the Gamma function, we can say a few things about when eq. 10 acts as a penalty, i.e., when  X  log  X ( r i  X  i ) &gt; log  X (  X  i ) .
 The Gamma function has its global minimum at  X (1 . 461632 ... )=0 . 8856 ... and is very rapidly monoton-ically decreasing in L =]0; 1 . 461632 ... [ and very rapidly monotonically increasing in R =]1 . 461632 ... ;  X  [ .It is thus easy to verify that when r i  X  i  X  L (thus also  X  i  X  L ) then r i  X ( r i  X  i ) &lt;  X (  X  i ) , corresponding to a penalty. Similarly, when  X  i  X  R (thus also r i  X  i  X  R ) then r i  X ( r i  X  i ) &gt;  X (  X  i ) , corresponding to a reward. How-ever, the general case where we may have  X  i  X  L and r  X  i  X  R , it is more complicated. Figure 1 is a plot of p ( r,  X  )= 1 r  X  log  X ( r X  )  X  log  X (  X  ) for r =2 ,... and 0 &lt; X   X  1 . Values larger than 0 imply a reward and values less than 0 imply a penalty.
Looking at the plot we observe that for small values of  X  , p decreases (towards penalty) as r becomes larger, but that for larger values of  X  , p increases (towards reward) as r becomes larger. Also, for fixed r there is a gradual change from penalty to reward as  X  moves from 0 to 1. 5.1.1 Hyper prior considerations The BNs favoured using the MAP BN and the BD metric are different, given the same hyper prior. To illustrate this, consider the so-called K2 hyper prior  X  = 1 (introduced in [7] in the context of the BD metric). Using this prior, eq. 10 reduces to where we exploit that for integers  X ( s )=( s  X  1)! . Also, we have Pr( d ,  X   X  1 | M,  X  )=Pr( d | M,  X  ) , such that  X  m = arg max For a binary variable X i , log (2  X  1)! = 0 , and there is no penalty nor reward (see figure 1). Hence, when all variables in X are binary, then the ML estimate is ob-tained,  X  m =argmax m log Pr( d | m,  X   X  ) , generally known to be prone to overfit. On the other hand, the K2 prior in the context of the BD metric is known to prevent overfit-ting (albeit, differently for equivalent BN models). Hence, the same hyper prior yields different regularization for the MAP BN and the BD metric, and therefore different BNs are favoured even though the hyper prior for both metrics captures the exact same notion of  X  X nstances observed in the past X  encoded via the Dirichlet prior.

We have established that  X  i determines the regulariza-tion imposed. However, we usually want to specify  X  i such that the regularization is the same no matter the cardinality of the parent set X pa ( i ) , i.e., we want to fix the regulariza-tion pertaining to vertex X i in advance. Looking at eq. 9, observe that each term in the sum can be regarded as a reg-ularization factor for adding parents to X i . The degree of regularization is thus determined by all prior counts rele-r q i numbers that need to be specified. Since we want the regularization, let X  X  say it is governed by  X  i for X i ,tobe the same irrespective of the number of parents, it follows that we should uniformly share  X  i across all r i q i parameter prior counts. Hence, if  X  i =  X  i r  X  is small thereby inflicting a large penalty, and vice versa. In order to guarantee score equivalence, i.e., DAGs that en-code the same set of conditional independences receive the same quality score,  X  i must be the same for all vertices; this corresponds to the BDeu hyper prior introduced in [5, 14].
In this section we illustrate the convenience of the regu-larized maximum likelihood form of the MAP BN metric.
EM [9] is a well-known generic parameter learning method from incomplete data, and can also be employed for BN parameter learning. In [10, 11] it was shown that it is in fact possible to do DAG model selection within EM such that the best model is selected in the limit. This algo-rithm is referred to as Structural Expectation Maximization (SEM).

In [10] SEM was suggested and shown to be fully valid in conjunction with the regularized likelihood model scor-ing metrics, MDL/BIC. In [11] the BD metric was used in-stead, for which SEM was shown to be only approximately correct, reason being the cumbersome functional form of the BD scoring metric. Since the MAP BN selection cri-terion is functionally similar to the regularized maximum likelihood metrics, we can easily show that SEM is also valid in conjunction with the MAP BN metric. In order to do so, we follow the same line of reasoning as given in [10].
In the following, we are given a data sample with in-complete records. Each record contains instantiations of the variables such that for record j we have that d j =( o j , u where o j is the observed part of the record, and u j is the unobserved part.
 Using the same line of reasoning as for the  X  X lassical X  EM derivation, we may restrict attention to the following Q -function, Q ( B =( m,  X  m ) ,  X  B ( t ) )= where the ( t ) in the superscript refers to the learned BN pair at iteration t of SEM. Intuitively this sum amounts to filling in the missing items at iteration ( t +1) in the incomplete data by performing inference on a per-record basis using  X  B t ) with evidence o j (the so-called predictive distribution ).
The log-likelihood of the multinomial sample is linear in the sufficient statistics from the data, and by adding the logarithm of the regularization term, the whole expression remains a linear function of the sufficient statistics. As with EM, this means that the expectation of the entire expres-sion reduces to the expectation of the sufficient statistics, ( o , u ,  X   X  1 ) , plus the regularization term.
Using the same argument as for normal EM, it fol-lows that by letting  X  B ( t +1) be the value of B that maxi-mizes Q ( B ,  X  B ( t ) ) , it is a better estimate than not difficult to find  X  B ( t +1) , because we know that for any given m , log Pr( o , u ,  X   X  1 | m,  X  m )+log f ( m,  X  ) is max-imized by the ML estimate  X   X  m based on the statistics from ( o , u ,  X   X  1 ) . Thus, we select the model  X  m ( t +1) mizes this regularized log-likelihood using  X   X  ( use the pair  X  B ( t +1) =( X  m ( t +1) ,  X   X  ( distribution for the next iteration, and so on. In the limit this iterative process yields the most probable BN pair, i.e.,  X  B ) =argmax B log Pr( B| o )for t  X  X  X  .
In order to demonstrate the performance of the MAP BN metric, we performed some experiments, and compared it to the BD, BIC and the pure ML metric (no regularization imposed). We considered 4 non-trivial and relatively large BNs that model actual domains: Water [15], Alarm [3], In-surance [4] and the Hailfinder [1] networks, with 32, 37, 56 and 48 vertices, respectively. We also considered the artifi-cial and randomly generated A and B networks introduced in [16] (both with very dense model structures) consisting of 54 and 18 vertices, respectively. 5000 complete records were generated by sampling from the aforementioned BNs. For all data sets, 2500 records were put aside, and used as test sets. 500, 1000 and 2500 of the remaining records were used for learning the BNs. For learning the BN model (structure), we apply a greedy hill-climber algorithm, and traverse the space of essential graphs by applying the Re-peated Covered Arc Reversal (RCAR) operator [6]. This operator simulates the search space of essential graphs. For the BD and MAP BN metrics, the BDeu hyper prior with  X  =1 was used.

Table 1 shows the log-likelihood on the independent test sample. We observe, as expected, that the likelihood in-creases with more data used for learning, and that data gen-erating BN has the highest likelihood. We see that for each of the 3 different sample sizes the MAP BN metric has a higher likelihood than the BNs learned with any other met-ric; this is the case for all networks. Also, there is a ten-dency for the MAP BN metric to perform better than the other metrics as the sample size decreases. We conclude that MAP BN approach yields BNs that are better from a predictive point of view. A BN that predicts well (on a test sample not used for learning) necessarily has a model struc-ture that is more similar to the generating BN than a model structure that predicts poor. Notice that the ML metric sur-prisingly outperforms the BD metric in a couple of situa-tions (marked with boldface in the table).
The BD metric is probably the most widely used scoring criterion for learning BNs. In this paper we have derived the MAP BN metric which is quite similar to the BD scoring metric. No extra assumptions are made; the only difference is that we seek the most probable BN, rather than just the most probable BN model structure. This has the following advantages:  X  The metric can be regarded as a regularized maximum  X  The penalty term is easy to identify and the impact it  X  Empirically it seems to select BNs that are better than
In this paper we have concentrated on model/BN selec-tion. This is merely a summary statistic, and strictly speak-ing model/BN selection does not fulfill the requirement of being a true Bayesian approach. There, the entire poste-rior distribution is of interest. Using an MCMC sampling methodology a  X  X ull X  Bayesian approach can be realized. For the BD metric this approach has been taken by several authors [17, 12]. The metric introduced in this paper can easily be adapted and applied in a MCMC sampling scheme too.

