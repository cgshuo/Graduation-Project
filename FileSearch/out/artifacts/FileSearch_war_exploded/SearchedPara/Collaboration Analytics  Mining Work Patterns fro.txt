 People are increasingly using more and more social softwares, generating flooding communications. User analytics may be per-formed to mine a person X  X  activities on different social systems and extract patterns, be it interest patterns, social patterns, or work pat-terns. Such patterns may benefit both the individuals and the orga-nizations the users associated with, as the information is valuable in numerous tasks, including recommendation, evaluation, manage-ment, and so on. In this article, we present an actionable solution of user analytics, namely collaboration analytics , by focusing on mining a person X  X  work patterns from her collaboration activities. Our solution effectively makes use of a user X  X  heterogeneous data collected from various collaboration tools to derive an integrated description of the user X  X  collaborative work. A number of  X  X ork areas X , each of which contains its work topics and people involved, are generated for every user. The challenges we face include the clustering of items with short texts and prioritizing/weighting data items based on importance/relevance. Our solutions to those is-sues will be described in this article. In particular, we mine users X  background information from various types of data and use such information to enrich the semantics of the short texts contained in the activity instances on collaboration tools before clustering those instances into work areas. Finally, we have developed a prototype of our collaboration analytics solution and evaluated it with real-world data and people.
 H.2.8 [ Database Applications ]: Data mining Algorithms, Experimentation
Communication plays a crucial role in people X  X  work and every-day life. Users X  activities on a variety of communication systems contain valuable information about the users themselves. The goal of user analytics is to discover various kinds of information about people from their communication. The information to be discov-ered includes but not limited to interest patterns , social patterns , and work patterns . A user X  X  interest patterns reveal what the user is or was interested in. A user X  X  social patterns contain information on who the user is connected to and how strong those connections are. A user X  X  work patterns are about who the user works with, on what topics, and when (i.e. who, what, and when). Work patterns tell an organization how its employees work together, which is crucial for the organization to evaluate and adjust its management strategy. Work patterns also provide valuable information for services such as expertise finding and team-based analysis.

User analytics is a broad topic. In this paper, we focus on mining work patterns from users X  activities on various collaboration tools used in working environments. In practice, user X  X  work information may be scattered across multiple collaboration tools. A user may adopt different tools for different projects; he/she may also employ different tools for the same project. In general, there is not a central place to collect all the information related to a topic for a user. To discover a user X  X  work patterns, we need to group his/her activities on different tools together based on topics.

Clustering a user X  X  activities across multiple collaboration tools is not an easy task. First, most contents from collaboration ac-tivities are very short and the semantics of words differ from one working environment to another. The latter renders useless outside sources such as Wikipedia or WorkNet for semantic information of the words in the short texts on the collaboration tools. Second, a user X  X  activities on various collaboration tools are not equally important. For example, composing the instructions for a newly released software on a knowledge sharing tool demands more ef-fort and higher level of expertise than joining an interest reading group. Third, the information available on different tools may be heterogenous. Some may provide structured information about a user X  X  work, which includes topics and collaborators; others may provide tags, which are words in a free form and do not contain direct information about collaborators.

We propose a novel collaboration analytic solution to tackle the above challenges. Our solution consists of three steps (see Fig-ure 1): First, for each activity on collaboration tools, we extract in-formation on topics, people, and time from the activity and create an information unit called work item . Work items provide a uni-form format for data retrieved from various information sources. The problem of mining work patterns from heterogenous data is then converted to the problem of clustering work items. Second, we compute the importance of the work item, determine and enrich the semantics of short texts in work items. We mine the semantic in-formation about words appearing in work items from internal data, including both structured and unstructured data. Finally, we use a hybrid approach to cluster the expanded work items into groups called work areas . We take a user X  X  most important background words as features to build clusters around. We then classify work items into those features and combine similar clusters.

We have implemented our solution and evaluated its perfor-mance using data collected from real-world collaboration systems. We have compared the results produced by our solution with those from Latent Dirichlet Allocation (LDA) [5].
D EFINITION 1 (W ORK I TEM ). A work item is represented as a quadruplet  X  s; w; { u i } ; t  X  , where s is the source of the activity, w is a piece of text that describes the activity, { u i } is the set of users involved in the activity, and t is the time-stamp of the activity.
In general, a work item consists of four pieces of essential infor-mation on an instance of users X  social and collaborative activities. Work items can be extracted from a variety of social and collab-oration tools. Example collaboration tools include Project Wikis and Expert Communities . Project Wikis allows users to post and share the information about their past or ongoing projects as well as other professional activities. We create a work item for each of the articles that a user posts on Project Wikis. Expert Communities allows users to create and join groups dedicated to the discussion of certain topics. We create a work item for each of the community which the concerned user is a member of. Due to performance is-sues on data retrieval, we only utilize the titles of users X  wikis and communities in work items creation.

We also retrieve unstructured data from tagging systems for our mining. For example, a people-tagging system allows users to ap-ply tags on each other. Those tags usually describe the user X  X  at-tributes, such as her affiliations, expertise, and the projects she has been involved in. Users X  tags on the social bookmarking system and the people-tagging system may be used as auxiliary informa-tion in the expansion of work items, which will be discussed next.
Most of the texts we retrieved for analysis are short. Analyzing short texts has been shown to be an extremely challenging task be-cause the text may not provide enough semantic information on the words appearing in it. In this section, we develop an novel approach to enrich the short texts with additional information to solve this problem. More specifically, we examine two different approaches, including content expansion and user-profile expansion.
We first present the algorithm to enrich the short text in work items by utilizing the semantic similarity between words. To better determine the meaning of a word, we need to combine the seman-tic information we gather from multiple work items that contain the word. We create a keyword dictionary called domain-specific synonyms in which we associate each word with its closely related words, which in turn can enrich the semantics of the target word. We find closely related words based on co-occurrence.
 word dictionary consists of a set W of words and a function l . For any w i  X  W , we have l ( w i ) = { w i 1 ; : : : ; w w 1 ; : : : ; w i m are the words closely related to w i semantically.
In particular, words that are project names may not have a clear meaning by themselves. For example, a product team may name their new collaboration tool  X  X angaroo X . The name  X  X angaroo X  itself does not tell what the project is about; we have to resort to its related words to get hints on its meaning.

Given a work item I j , we attempt to extend the short text of I by adding semantic information from the keyword dictionary. For every word w in I j  X  X  text, we retrieve w  X  X  related words in the entry and expand the word-base of I j with them. The expanded words may have lower weights than the words appearing in I j  X  X  text. We also explore enriching information from users X  backgrounds. Work items are associated with sets of people; the common back-ground of the people involved in a work item should reflect the context of the work item to some extent.
We mine the background information of a user from her work items and other auxiliary information sources such as the user X  X  tags. Users X  background information is stored in the user-background index .
 user-background index is represented as a set U of users and a function g . For any u i  X  U , we have g ( u i ) = { ( w i 1 ; c i 1 ) ; : : : ; ( w i m ; c i m a word related to u i and c i j is a real number. The larger c more important w i j is with regards to u i .

For a user u i , the weight of her background word w is deter-mined by three factors: the number of u i  X  X  work items that contain w , the weights of those work items, the number of times w is used as tags by u i .
 Weighting work items . All work items are not equally important to a user. The importance of a work item depends on a number of factors. The first factor is time. Intuitively, recent activities better reflect a user X  X  current interests and expertise than old ones. The second factor is the source where the work item is from. For exam-ple, writing a Wiki post on a topic takes more time and demands higher level of expertise than joining a discussion community on the same topic; it is natural to give more weight to the former than the latter. The third factor is the role the user plays in the work item. For example, the first author of a report usually assumes more re-sponsibility on the report than the last author.

Given a work item I , let d ( I ) and s ( I ) be the date and the source of I , respectively. Let r ( u; I ) be the rank of u in I  X  X  people list. The weight of I is computed as: In the above formula, f ( d ) is a function whose value increases as d increases, so more recent items carry more weights. The func-tion g ( s ) returns the weight of the source s . Right now, the weight of a source is manually specified. We plan to develop techniques to learn the source weights automatically in the future. The func-tion h ( r; s ) returns the score of rank r on source s . If the ranking of people is not important for work items from source s , h ( r; s ) always returns a constant (say, 1); otherwise, the value of h ( r; s ) decreases as r increases, so that the work item is less important to a user whose name ranks low in the item. Computing word scores . Given a user u , let W I u be the set of her work items. For every word w in the keyword dictionary, let W I u ( w ) be the set of work items containing w . The weight N wi ( w ) in u  X  X  entry in the user-background index is computed as N work item I j . Intuitively, the more work items containing w and the more important those items are, the higher weight w carries with regards to the user u .

Next, for every word w in the keyword dictionary, we count the total number N tag ( w ) of times w is applied as tags by u in the bookmarking system and as people-tags to u in the people-tagging system. N tag ( w ) is the weight of w based on u  X  X  tags.
Finally, we combine the weight of a word from u  X  X  work items with its weight from u  X  X  tags to get a final weight for u  X  X  entry in the user-background index. We first normalize the scores computed from work items and those from tags so that the top k (say k = 10 ) scores have the same average value. The final score of w for u is computed as c ( w ) = N  X  wi ( w )+(1  X  ) N  X  tag ( w ) , where N and N  X  tag ( w ) are normalized scores, and  X  [0 ; 1] determines the combination weights of the two sources. The higher c ( w ) , the more important w is with regards to user u .
Sometimes, a work item I j may contain only one or two words in its text and those words may have few or no related words in the keyword dictionary. For example, a community may have a project name  X  X angaroo X  as title; and since the project is very new, no related word for  X  X angaroo X  is recorded in the keyword dictio-nary. For such work items, we are unable to expand them using the keyword dictionary. Instead, we may expand such work items using the background information of their associated people. For instance, if most people working on the project  X  X angaroo X  have background on  X  X mail management X , we may guess that  X  X anga-roo X  has something to do with  X  X mail management X  and expand the work item accordingly.

To expand a work item I j , we retrieve the background-word vec-tors of all people in I j from the user-background index. We com-bine the background-word vectors into an overall background-word vector. In the overall background-word vector, the weight of a word w is equivalent to the sum of its weights in the individual vectors (if w does not appear in a vector, its weight is 0 in that vector). We then sort the words in the overall background-word vector and ex-pand I j with the top y words with highest weight. Those top words in the overall vector are likely to be the most common and impor-tant features of the people in I j . Again, the expanded words may have lower weights than the words appearing in I j  X  X  text.
After enriching the short text of work items, the next step is to mine work areas by clustering the expanded work items based on topics. Our clustering algorithm consists of two steps. First, we se-lect the features to build clusters around. Second, we categorize the expanded work items based on the selected features and combine similar clusters.
 Feature Selection . Recall that we have stored users X  background words and their weights in the user-background index. Given a user u , we retrieve the top background words with the largest weights as the selected features. The features selected here are the topics important to u . When we cluster u  X  X  work items based on those topics, we can get clusters that best reflect u  X  X  engagement. Categorization . First, we categorize u  X  X  expanded work items based on the selected features. We create a category for each se-lected feature w . Those words that are related to w in the keyword dictionary are added to the keyword list of the category as well. A work item is placed into a category if the similarity between its keywords and the keyword list of the category is higher than a cer-tain threshold. For each category, we aggregate the keywords and the people, and compute the average time of its members.
Next, we apply a hierarchical clustering algorithm to group those clusters produced previous step. We compute the similarity be-tween two work items/clusters based on their keywords, people, and time. For similarity between clusters, we also consider the similarity between their member work items. In other words, the similarity measure is computed based on multiple dimensions. We skip the details here due to space limit.
We have built a prototype of our approach on mining users X  work patterns from collaboration tools. Our current implementation re-trieves users X  information from four collaboration tools with struc-tured data and two collaborative tagging systems with tags in free form. Work items are created from users X  activities on these four collaboration tools. Our system clusters a user X  X  work items using the approach described in previous sections. We have also evalu-ated our solution in real-world scenarios, with the objectives to find out: (1) Can our solution generate good clusters from users X  work items? (2) How does our solution compare with existing clustering algorithms in terms of efficiency and the quality of results?
We interviewed 10 users. The largest number of work items a user had was 110, while the average number of work items per user was 67. For each user, we clustered his/her work items into work areas, sorted the work areas based on the average weight of the work items in a work area, and presented the sorted list of work areas to the user. Each user interview consisted of two phrases.
First, the user is asked to examine the list of his/her work areas and mark each work area as  X  X orrect X  or  X  X ncorrect X . A  X  X orrect X  work area represents one of the user X  X  ongoing or past projects, while an  X  X ncorrect X  work area does not. The user is also asked to write down the major projects that he/she worked on but is not represented by any work areas in the list. We refer to the projects a user writes down  X  X issing X  work areas. For each user, we compute the precision , recall , and F 1 score of the clustering results.
Second, the user is asked how he/she would like to modify the given work areas to best reflect his/her work patterns in his/her mind. In our interviews, we found that users had three possible answers for each work area: (1)  X  X plit X : the user indicates that a work area should be split into x ( x  X  2) smaller work areas. (2)  X  X erge X : the user specifies that a certain set of y ( y  X  eas should be merged into one. (3)  X  X erfect X : the user indicates that the work area correctly represents his/her work at the right granu-larity; no further modification is needed on the work area. We re-compute the precision, recall, and F 1 score for each user based on his/her modification instructions. For convenience, we refer to the three criteria computed in the second phrase refined precision , re-fined recall , and refined F 1 score , respectively. Let N number of the top k work areas that are marked with both  X  X orrect X  and  X  X erfect X . The refined precision p  X  k of the top k work areas in the list is computed as p  X  k = N k ( c; p ) =k . The refined recall r defined as r  X  k = N k ( c; p ) =N  X  , where N  X  is the number of work ar-eas the user prefers to have. N  X  is computed in such a way: each of the work areas marked with  X  X issing X  or with both  X  X orrect X  and  X  X erfect X  counts 1; a work area that needs to be split into x ( x smaller work ares counts x ; if a group of y work areas should be merged, the y work areas together counts 1. Table 1: Comparison between three solutions. The notations R.Prec, R.Rec, and R. F 1 are short for refined precision, refined recall, and refined F 1 , respectively.
Latent Dirichlet Allocation (LDA) [5] and its variants have been widely used in topic modeling in data mining community. We com-pared the results produced by our solution with those from LDA as a very competitive baseline. For each user, we applied the LDA al-gorithm to mine topics from his/her work items. We went through the topics generated by LDA with the above two phrases during the user interviews. For each cluster from LDA results, it contains the top 10 keywords. As long as there is at least one keyword that the user believes is representative of his/her work area, we mark it as  X  X orrect X . Furthermore, in order to remove the bias that our work items are enriched with additional information, we also run LDA on the same input as our method, that is, apply LDA on the expanded work items rather than the original ones. For convenience, we refer to the test method of applying LDA on expanded work items xLDA . The averaged results from the 10 users are shown in Table 1. First, we compare our solution with LDA. As we can see from Table 1, our solution had higher precision, recall, and F LDA. When we compared the top 5 clusters, the precision, recall, and F 1 score of our solution were 38% , 43% , and 42% higher than those of LDA, respectively. In particular, the precision of our so-lution is close to 1. This suggests that our solution is not only effective in mining users X  work patterns, but also does a good job in prioritizing work areas by introducing the weighting scheme as discussed in Section 3.2.1. When it comes to refined precision, re-fined recall, and refined F 1 score, the advantages of our solution over LDA were 64% , 78% , and 60% , respectively. The bigger im-provement by our method indicates that our solution meets users X  expectations better than those from LDA. This may be due to the fact that, in our approach, clusters are built around a user X  X  impor-tant background features. Similarly, our solution retain the signif-icant advantage over LDA, when evaluated on the top 10 clusters. Note that most of the 10 users we interviewed specified less than 10 work areas. This explains why the precision was relatively low when we consider the top 10 clusters.

Next, we compare with xLDA, which is applied on users X  ex-panded work items. The results show that our method still out-performs xLDA significantly. Comparing the results of LDA with xLDA, we can see that the two methods performed similarly. The fact that both LDA and xLDA lack behind the algorithm we de-signed indicates that the LDA model might not be a good fit for mining work patterns in the scenario we consider. We may need to modify the statistical model for further performance enhancement, which we leave as interesting future work.

Finally, we have compared the running time of our solution with that of LDA. We used the Mallet package [6] for our LDA imple-mentation. Our experiments show that our algorithm is much more efficient than LDA in terms of running time.
Clustering short texts is a challenging problem, because short texts do not provide enough word co-occurrence or shared context for a good similarity measure [4]. Researchers have proposed solu-tions that leverage external information sources to provide greater context for short texts. External information sources that have been considered include search engines [2], Wikipedia, and Word-Net [4]. However, those solutions cannot be directly applied to mining work patterns from collaboration tools. Work environments oftentimes have their specific vocabulary, which makes it impossi-ble to get the correct semantics of all the words, such as internal project names, from external sources. Our solution further distin-guishes from existing work in that it enriches short texts not only with related words from a dictionary, but also with the background of the people associated with the work items where the short texts appear. To our knowledge, enriching short texts with people back-ground information has not been studied in the literature.
Our work is also related to research on harvesting user infor-mation from collaboration systems and social networks. Carmel et al. [3] gathered users X  social relations from multiple social and collaboration systems and exploited such social relations to person-alize search results. They focused on the relationship among users, rather than users X  topic-based backgrounds. They did not study clustering users X  activities on social and collaboration systems.
In this paper, we identified the task of user analytics and pro-vided an actionable solution of  X  X ser analytics X , namely collabora-tion analytics, by focusing on mining a person X  X  work patterns from her collaboration activities. This also opens up a door to many in-teresting follow-up work and further improvement on the system.
As future work, we would like to improve our system in a more analytic and systematic way. For example, the weighting of work items before deriving work areas is based on three crite-ria. One is the co-occurrence of the same word in multiple work items; the other is the time that work item occurs; the third is the roles/activeness of the user participating in that work item. We would like to study more on the measures of user X  X  activeness and use those measures to automatically guide our weight calculation. We believe that identifying and using the most important and rele-vant work items during clustering is crucial to the accuracy of the clustered results. [1] S. Banerjee, K. Ramanathan, and A. Gupta. Clustering short [2] D. Bollegala, Y. Matsuo, and M. Ishizuka. Measuring [3] D. Carmel, N. Zwerdling, I. Guy, S. Ofek-Koifman, N. Har X  X l, [4] X.-H. Phan, L.-M. Nguyen, and S. Horiguchi. Learning to [5] D. M. Blei, A. Y. Ng, and M. I. Jordan. Latent dirichlet [6] A. K. McCallum. Mallet: A machine learning for language
