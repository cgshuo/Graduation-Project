 A web search engine often employs partition-aggregate ar-chitecture, where an aggregator propagates a user query to all index serving nodes (ISNs) and collects the responses from them. An aggregation policy determines how long the aggregators wait for the ISNs before returning aggregated results to users, crucially affecting both query latency and quality. Designing an aggregation policy is, however, chal-lenging: Response latency among queries and among ISNs varies significantly, and aggregators lack of knowledge about when ISNs will respond. In this paper, we propose aggre-gation policies that minimize tail latency of search queries subject to search quality service level agreements (SLAs), combining data-driven offline analysis with online process-ing. Beginning with a single aggregator, we formally prove the optimality of our policy: It achieves the offline optimal result without knowing future responses of ISNs. We extend our policy for commonly-used hierarchical levels of aggrega-tors and prove its optimality when messaging times between aggregators are known. We also present an empirically-effective policy to address unknown messaging time. We use production traces from a commercial search engine, a commercial advertisement engine, and synthetic workloads to evaluate the aggregation policy. The results show that compared to prior work, the policy reduces tail latency by up to 40% while satisfying same quality SLAs.
 H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval X  Search process Aggregation; Scheduling; Tail Latency; Web Search
A web search engine often employs partition-aggregate ar-chitecture, forming a distributed system with aggregators and index serving nodes (ISNs). The web index is large, containing information on billions of web documents, and is typically document-sharded among hundreds of ISNs [7, 12]. Such a system includes aggregators that merge results col-lected from ISNs (e.g., a single aggregator (Figure 1(a))).
An aggregation policy determines how long the aggregator waits for its ISNs before sending the results back. Design-ing a good aggregation policy is crucial: It directly impacts quality and responsiveness, which are among the most im-portant metrics in web search. For example, the aggregator can choose to wait for responses from all ISNs, returning complete results, or it may wait for only a fixed time dura-tion, returning partial results from a subset of ISNs. There is a clear trade-off: The longer the aggregator waits, the more results it collects, improving quality but degrading re-sponsiveness. The objective of the aggregation policy can be expressed as reducing a high latency percentile while meet-ing one or more quality constraints. Reducing a high latency percentile, i.e., tail latency, is more suitable than average la-tency to ensure consistently low latency, which is important to attract and retain users [4, 17, 25, 27, 34].

Designing an effective online aggregation policy is chal-lenging: Aggregators do not know when ISNs will return their results and their response times vary significantly. For example, different web search queries exhibit highly variable demand: the 99-th percentile query execution time is often a factor of 10 larger than the average [13, 22]. Even for the same query, the response times of different ISNs could vary widely [5]: It may take only 10 ms to collect responses from a majority of ISNs but require another 200 ms to receive the remaining few.

Prior work provides aggregation heuristics that reduce the tail latency compared to simply waiting for all responses, but they miss significant potential latency improvement. Instead of using heuristics, we take a different approach: Imagine if the aggregator knows the future, in particular when each ISN will respond. We study this case to derive an offline pol-icy, which decides when to return the results on a per-query basis. We further show that this offline policy is optimal, forming a clear latency target that an online policy strives to achieve.

However, in practice, the aggregator must make online de-cisions without knowing the future information (e.g., when ISNs will respond), which invalidates offline policies and makes the problem challenging. When an online aggregation policy decides to terminate a query and returns its partial re-sults, it makes a bet that waiting for the remaining responses to arrive will take too much time. However, all the remain-Figure 1: Web search engine architecture: (a) single-level aggregation, (b) two-level aggregation. ing responses might arrive immediately afterwards. In this situation, search quality is degraded with only a marginal latency reduction.

Inspired by the offline policy, we develop an online policy that uses two thresholds, one for query quality and the other is a time threshold. Only if a query meets or exceeds the quality threshold at the time threshold, the aggregator ter-minates the query. We employ offline analysis of the query logs to compute the quality and time thresholds.

We prove a surprising result: The aggregation policy is optimal, providing the same results as the offline optimal policy. It minimizes the tail latency of queries while meet-ing their quality service level agreements (SLAs). In con-trast, prior work does not identify the optimal structure of the online policy, or compare to the offline optimal solution. They employ heuristics to reduce tail latency indirectly, for example, by minimizing latency variance [20].

Next, we study multiple levels of aggregators, which is an important practical case yet largely unaddressed in prior work. There is no heuristic specifically designed for multi-ple aggregators. Figure 1(b) shows a cluster with two-level aggregation, containing a single top-level aggregator, called TLA, and several mid-level aggregators, called MLAs. Mi-crosoft Bing search employs the two-level aggregation archi-tecture [1]. This architecture fits naturally with the hierar-chy of data center networks, where MLAs collect responses from ISNs located in the same rack and TLA collects re-sponses from the MLAs, reducing inter-rack communication. Moreover, MLAs can reduce the processing load of the top aggregator, for example by re-ranking results and removing near duplicates [35].

Multilevel aggregation is significantly more challenging than single-level aggregation because the aggregator X  X  de-cisions at the different levels are coupled and cannot be op-timized independently. For example, an aggressive TLA pol-icy (e.g., terminating queries based on a lower time or quality threshold) needs to be combined with a more conservative MLA policy to meet the quality SLAs. Finding a good policy requires exploring various combinations of aggregation de-cisions at different levels and deciding the parameters used at each level jointly, which significantly increases the design space and computation complexity. Further, messaging time between aggregators (including both processing and trans-mission time) may or may not be known online.

We extend our policy to address the challenges for multi-level aggregation and prove its optimality when messaging times between aggregators are known. For unknown mes-saging times, we show that the policy performs very close to the offline optimal using empirical evaluation. To the best of our knowledge, this policy is the first one to solve the multilevel aggregation problem.

We assess our techniques using query logs from a produc-tion cluster of a commercial search engine, a commercial advertisement engine, as well as using synthetic workloads derived from well-known distributions. We evaluate the pol-icy extensively in both single-level and two-level aggregation clusters with different quality and latency targets, varying the number of ISNs, latency distributions. We conclude our policy consistently outperforms the state-of-the-art tech-niques. In particular, the results show, to meet the target quality constraints, on a 44-ISN search cluster with a single aggregator, we reduce the 95-th tail latency by 36% (from 59 ms to 38 ms) compared with the best existing work. The results also show similar savings for two-level aggregation. The key contributions of this work are the following: (1) We characterize the workload to outline how a good aggre-gation policy should behave. (2) We develop an optimal ag-gregation policy, combining both data-driven offline analysis with online processing, to minimize a crucial metric  X  tail latency of search queries  X  subject to quality constraints. (3) We generalize our policy to address multilevel aggre-gation, which has practical importance but has not been addressed in prior research. (4) We conduct extensive eval-uation using production traces as well as various synthetic workloads to assess the benefits of the proposed aggregation policy and show that it outperforms existing solutions.
Aggregation policy assigns an online algorithm for each aggregator to decide when it returns its response.

Latency. Web search requires consistently low latency to attract and retain users [4, 17, 25, 27, 34], for which average latency is not a suitable metric due to potentially very large variation in latency. Thus, service providers often target to reduce high percentile latency of web search queries, which is also called tail latency [13, 19, 31]. We define query latency as the latency measured at top aggregator.

Quality. To quantify search quality, we define the utility of a query as the fraction of ISNs from which the TLA re-ceives results and returns to the user. This definition is also used in prior work [20]. Lower utility indicates the query is more likely to miss important answers, thus resulting in a lower quality. A utility of 1 is the best baseline case: The aggregators receive responses from all ISNs.

We use both average utility and tail utility, which are two important metrics for web search SLAs [2, 19]. The average utility is the mean utility of all queries, and tail utility is high-percentile utility of queries, ensuring consistently high quality results for the search queries.

Optimization Objective. The objective of an aggrega-tion policy is to minimize query tail latency of a large-scale web search engine, while satisfying average and tail utility constraints. This is a common objective in many prior stud-ies [11, 20, 22] as well as in real systems. For example, the optimization criteria of Bing search is to minimize the 95-th tail latency subject to an average utility of 0.99 and 95-th tail utility of 0.95.

Given an aggregation policy A and a set of n queries, we denote the latency and utility of query i by t i and u respectively, for i = 1 , 2 ,  X  X  X  ,n . Note that both latency and utility are affected by the aggregation policy A . The k -th percentile latency for the set of n queries, denoted by t k % , is the d kn e -th largest latency among these n queries. Likewise, the h -th percentile utility, denoted by u h % , is the Figure 2: Progress of 4 sample queries.
 Table 1: Aggregation Policy Comparison.
 Co mplete T erminate Co mplete I nputs M ulti F ast S traggling Lo ng Ti me U tility l evel d hn e -th lowest utility. Thus, we formulate the tail latency minimization problem (called TLM ) as follows: Constraint (2) specifies the average and tail utility.
Production Deployment Environment. When an ag-gregator decides to return partial results, there is an imple-mentation choice to either abort query execution on the ISNs that have not yet responded or to let them complete. The later is more common in practice as observed in both com-mercial search engines, e.g., Bing [20], and the well-known open source search software Apache Lucene [18]. Supporting query abort has a cost and comes with limited benefits.
Supporting abort messages and interrupting running queries increases search engine implementation complexity, making logging, testing, debugging more difficult in addition to the message processing overhead of the abort messages. Also, the computational resource savings from aborting a late query are often negligible. More specifically, as query pro-cessing on ISNs has a timeout limit (e.g.,  X  200 ms in Bing search [19]), no query takes longer processing time than the timeout limit. Moreover, to meet an average utility of 0 . 99, at most 1% of ISNs could terminate processing earlier us-ing an abort message, making the overall computational re-source savings limited. Finally, aborting late messages also has little impact on the waiting time (and thus the latency) of other concurrent queries, because commercial search en-gines are provisioned with additional capacity to ensure that ISNs operate at low to modest loads with a very small query waiting time [21].

Therefore, we present and evaluate our aggregation poli-cies based on systems without abort messages. However, as the performance impact of abort messages is small, these policies are also well applicable to systems with aborts.
We derive the key intuitions of our aggregation policy us-ing query workload characteristics. Then, we discuss what a good policy should do and what existing policies miss.
Workload Characterization. The execution time dis-tribution of web search queries has a high variability: Prior work shows that the 99-th percentile query execution time is a factor of 10x larger than the average, and even 100x larger than the median [13, 22]. In addition, for processing a single query, some ISNs may take much longer than others even if they all use the same hardware due to several well-known factors, such as document partitioning scheme, correlation between query term frequencies and inverted list sizes [5]. Figure 2 shows utility progress for a few examples from Bing. We categorize queries into three types: (1) Fast query for which the responses from all ISNs arrive quickly, e.g., Q1 in Figure 2. (2) Straggling query for which most of the responses arrive quickly but a few ISNs take much longer to respond, e.g., Q2 in Figure 2. (3) Long query for which most responses take a long time, e.g., Q3 and Q4.

Key Intuition. Our policy identifies query type and de-cides which queries among the three types to complete or terminate by considering the tradeoff of latency and utility:
Complete fast queries. Since they do not impact tail la-tency even if we wait for all responses, we complete them with full utility.

Terminate straggling queries. We reduce latency with lit-tle utility loss. For any given utility target, we can only tolerate a limited utility loss. For example, for an average utility of 0.99, only 0.01 utility loss is allowed. We allocate the allowed utility loss to straggling queries to maximize la-tency reduction.

Complete long queries. As their responses from ISNs ar-rive late but altogether at TLA, terminating them early can cause major utility loss. Thus, we let long queries complete without affecting the tail latency, because a latency per-centile target allows for a few slow queries without penalty (e.g., for 95-th latency, 5% slow queries are allowed), which we call tail latency slackness .

The distinction among fast, straggling and long queries is relative, depending on the selection of time and utility thresholds obtained by offline analysis of query workloads.
Existing Policies. Table 1 compares six aggregation policies with respect to their actions on the three types of queries, inputs, and support for multilevel aggregations. Wait-All waits for every ISN before returning the results. The query latency is dominated by the slowest ISN, re-sulting in a high latency. Time-Only terminates a query upon a timeout threshold [22]. It terminates both long and straggling queries without differentiation. Utility-Only ter-minates a query if it already received a given percentage of responses, even including fast queries whose full completion does not affect tail latency. These three policies miss either time or utility to differentiate among query types.
Time-Utility terminates a query when its timeout thresh-old is triggered and the query utility reaches a threshold. It is better than Time-Only, but it still early terminates long queries after they receive the threshold percentage of responses. Kwiken [20] terminates a query if it runs a fixed time interval either after a utility threshold or exceeding a time threshold. It selects these thresholds by minimizing latency variance among all responses. Although minimizing latency variance may indirectly reduce tail latency, it can degrade utility significantly. For example, in Figure 2, Q2 and Q3 have similar variances in terms of latency from their ISNs. However, terminating the straggling query Q2 causes little utility loss, while terminating a long query Q3 results in a large utility loss. Although Time-Utility and Kwiken use both time and utility for aggregation, neither of them directly optimizes for tail latency by exploiting tail latency slackness to obtain high utility for long queries.

Furthermore, none of the existing policies address multi-level aggregation, which is common in practice, and is much more challenging than a single level. In comparison, our policy takes effective actions for Fast, Straggling and Long queries, so we name it FSL. FSL exploits time and utility, and addresses single and multiple aggregation levels.
This section presents the design and analysis of our pol-icy FSL (for fast, straggling and long queries) for a single aggregator. FSL decides when to terminate a query by com-bining both online processing and offline analysis. For online processing, FSL jointly considers the time and utility of a query. In particular, we define a time threshold t utility threshold u  X  . Based on whether a query is completed by t  X  , and its utility at time t  X  , we categorize the running query as fast, straggling or long, and take actions accord-ingly. The offline processing calculates the optimal values of t  X  and u  X  according to the workload information, utility target, and type of tail latency we seek to minimize. As re-sponse time distributions of interactive services vary slowly over time [20, 24], we trigger offline processing periodically to update the parameters for online processing. The la-tency can also be monitored online to trigger offline analysis whenever needed. To adapt to varying system load, we can construct a table that maps each load (e.g., query arrival rate) to its time and utility threshold values through offline analysis of the corresponding query logs. Then, online pro-cessing simply applies the proper parameters for the current load. FSL is not another heuristic: We derive an offline op-timal algorithm with complete future information in Section 4.3 and prove that the online algorithm FSL is optimal, per-forming as well as offline optimal without knowing future responses of queries.
Online processing of FSL takes two runtime properties of a query  X  time and utility  X  as inputs, and it decides when to terminate query under execution. In particular, as described in Algorithm 1, if the aggregator does not receive all query responses after the query has been sent to the ISNs for time t , FSL checks the progress of the query: if the utility of the current query is higher than or equal to u  X  , then terminate the query at time t  X  (for reducing tail latency with a small utility loss); otherwise, let the query run till completion (to avoid significant utility loss).

Intuitively, FSL uses t  X  and u  X  to differentiate the three types of queries and take actions accordingly. (1) If a query completes by t  X  , the query is fast: FSL executes fast queries till completion. (2) If a query has received most of the re-sponses by t  X  , the remaining utility of the query is marginal and the query is straggling: FSL early terminates strag-gling queries. (3) If responses are only received from no or a few ISNs at time t  X  , the query is long: FSL executes long queries till completion, avoiding a significant utility degra-dation. Moreover, to optimize a tail latency target, e.g., 95%, the longest 5% of queries have no effect. Thus, by carefully choosing the values of t  X  and u  X  through offline Algorithm 1 FSL: Online processing 1: for each query do 3: Do nothing 5: Early terminate this query 6: else 7: Run query until completion (or system failure timeout) 8: end if 9: end for analysis (as shown in Section 4.2), FSL allows long queries to complete, avoiding utility degradation without affecting the desired target tail latency.

In practice, system failure timeouts are set at the aggre-gators to prevent them from waiting forever in the case of server/network failures. Upon this timeout, FSL returns the collected results, just like all other aggregation policies. In addition, when there is only a tail utility constraint with-out average utility constraints, FSL can return the results of long queries after reaching the target tail utility rather than query completion.
Offline processing includes a search algorithm to find the optimal time and utility thresholds. Algorithm 2 presents its formal description.

Inputs and outputs. FSL takes the workload data, util-ity constraints and tail latency objective as inputs. The workload data includes receiving time of each ISN responses for every query in the training set, which is the duration from aggregator sending the query to receiving its response. The output of FSL is the optimal time and utility threshold ( t ,q  X  ) that minimizes the k -th percentile latency subject to all utility constraints.

High-level description. The search algorithm iterates over all possible values of time thresholds. For each time threshold t , it calculates the optimal value of utility thresh-old u to maximize query utility (average and/or tail) while producing k -th percentile latency of t . Then among all the t values, we select the smallest one which meets the utility constraints, producing the optimal k -th latency subject to utility constraints. The corresponding ( t,q ) values are the optimal time and utility thresholds ( t  X  ,q  X  ).

Step 1. Given a step size  X  , there are a set of candidate time thresholds {  X , 2  X ,...,m X  } , where m is decided by the maximum latency t max of all responses. The tail latency of FSL can exceed the (theoretically) true optimal by at most  X  . By reducing  X  , our result approaches arbitrarily close to the theoretically optimal, while the search complexity in-creases, trading off algorithm complexity for accuracy. For web search,  X  =1 ms is sufficiently accurate.

Step 2. For any possible time threshold t , we compute the corresponding utility of each query. We sort the queries according to a descending order of the utility values. Fig-ure 3(a) shows an example for a set of queries with sorted utility. The queries at the top of the list are fast queries: they complete before time threshold t and each gets a full utility of 1. At the very bottom are long queries, whose utility is affected significantly by early termination at t . Be-tween fast and long queries are straggling queries, which, if Figure 3: Example of the matrix construction of B and C at time threshold t , where k = 95 and n = 1000. early terminated, incur smaller utility loss than long queries would. Such a list of utility values for a given time threshold t corresponds to column t/ X  of matrix B in Algorithm 2.
A research question here is how we differentiate straggling versus long queries, which also relates to how we compute the utility threshold u . Given a latency objective k , (e.g., k =95 for optimizing 95-th latency), we know that we can disregard the latency of less than (100  X  k ) = 5 percent of queries (i.e., classify these queries as long queries). We choose the long queries to be those with the highest loss in utility if we early terminate them, i.e., the bottom (100  X  k ) percent of queries in the list. The remaining queries are straggling queries to be early terminated, losing less utility in total. Here, the utility threshold u is equal to the utility value of the (100  X  k )-th percent query from the bottom. As shown in Figure 3(a), suppose that there are n = 1000 queries in total, u corresponds to the utility value of the n  X  k/ 100 = 950-th query from the top. The beauty of dif-ferentiating straggling and long queries in this way is that (1) it allows us to obtain the highest utility gain for any given latency target, and (2) during online processing, it does not require future response times of ISNs to differenti-ate straggling versus long queries.

Step 3. We update the resulting utilities of queries ac-cording to online processing: the utility values of fast and straggling queries remain unchanged, while the utility values of the long queries become 1 since these queries will run till completion. An example is shown in Figure 3(b).

In the case where long queries are returned due to system failure timeout, their utility values are set to the utility of the query at the timeout instead of a full utility of 1.
Step 4. We find the smallest termination time threshold t value that meets all quality SLAs.

Time complexity. Let r represent the number of ISNs, m the number of time threshold candidates, and n the num-ber of queries. Step 1 takes time n  X  r to compute t Step 2 takes O ( rnm ) for the matrix construction of A , and takes O ( n log( n ) m ) for the sorting to compute B . Step 3 take O ( n log( n ) m ) for sorting, and other operations have lower asymptotical order. Step 4 performs a search over all entries of the table with cost of O ( nm ). Therefore, the overall time complexity of the offline processing of FSL is O (( rn + n log( n ))( t max / X  ). The offline processing is an ef-ficient polynomial time algorithm for any given value of  X  . For 10,000 queries, 1,000 ISNs, t max = 350 ms, and  X  = 1 ms, we compute this in 43 seconds on a desktop PC.
 Algorithm 2 FSL: Offline Processing
FSL is an online policy without future information: it does not know when the remaining responses will be returned by ISNs. In Theorem 1, we compare FSL with an optimal offline policy, which assumes complete information. We show that FSL performs as well as the optimal offline policy  X  FSL is an optimal online policy achieving the minimum tail latency subject to utility constraints.

We first define an optimal offline policy , which decides termination time of each query by assuming full knowledge of the query as well as the entire query set. Each query may have its own termination time. A simple brute-force algorithm to compute such an optimal solution is as follows. For each query i , its candidate termination time t i is one of the response times of all ISNs, and hence there are r choices (assuming that there are r ISN). For a set of n queries, the search space is r n . The brute-force algorithm finds the set of { t i } , which produces the smallest tail latency while satisfying all utility constraints.

Theorem 1. For a cluster with a single aggregator, FSL achieves an arbitrarily close-to-minimum tail latency while meeting average and tail utility constraints.

Proof. We show the equivalency of FSL and the optimal offline policy.

First, we perform a transformation from the optimal solu-tion to FSL. Suppose that the offline optimal finishes query i at t i . Without loss of generality, we assume that t 1  X  X  X  X  t n among a total of n queries. Then, k -th tail latency of the optimal offline policy is t 0 = t ( k  X  n/ 100) , and we have two observations.  X  For queries whose finish time is smaller than t 0 , finishing them at t 0 does not change the k -th tail latency.  X  For queries whose finish time is bigger than t 0 , waiting for all ISNs does not change the k -th tail latency.
This observation indicates, in the optimal offline policy, each query can have three possible cases: finish before t (i.e., fast query), finish exactly at t 0 (i.e., straggling query) or wait until receiving all ISNs X  results (i.e., long query). Hence, this is the same as our policy. Algorithm 2 of FSL is a constructive proof of finding the minimum t 0 , which is equivalent to the minimum time threshold t  X  (with a devi-ation smaller than the step size  X  ). Thus, when the search time step size  X   X  0, the tail latency produced by FSL can be made arbitrarily close to offline optimal.
This section extends FSL for multiple levels of aggrega-tors, which are common in practice but are much more chal-lenging than a single level. First, the aggregators X  decisions on different levels are coupled and must be coordinated to reduce tail latency subject to quality SLAs. Second, the number of communication messages between different levels of aggregators must be small, e.g., an MLA cannot simply forward the results to TLA whenever it receives an ISN re-sponse, which further magnifies the design challenge.
At runtime, an MLA may or may not know the messag-ing time between itself and its higher-level aggregators in advance. Here the messaging time primarily includes net-work latency plus some lightly-weighted processing such as reranking and removing near duplicates. We consider three different scenarios of the messaging time: (a) known mes-saging time, (b) unknown but varying in a small bounded range, (c) general case of unknown messaging time with po-tentially large variations. In this section, we first focus on two-level aggregation, and generalize our results to arbitrary levels in Section 5.4.
Inspired by FSL and its analysis in Theorem 1, we de-velop an optimal online policy for two (or multiple) levels of aggregators when messaging times between aggregators are known. We call this FSL-K, where  X  X  X  stands for Known messaging time. The key idea of FSL-K is as follows: A TLA returns the results of each query in one of the three ways: (1) return full results before/at time threshold t based on the query utility progress at t  X  , return partial re-sults at a time threshold t  X  , or (3) wait until receiving all responses from MLAs. We show that FSL-K at TLA is op-timal, performing as well as an optimal offline policy. Once the top-level aggregation policy is known, we derive the time threshold for MLA by considering both t  X  and the messaging time between TLA and MLAs.
 Online Processing of FSL-K. Online processing of FSL-K takes as inputs time threshold t  X  and utility threshold q as decided by offline analysis.

TLA policy. For each query, if TLA receives responses from all MLAs and their ISNs prior to the time threshold t , this query is fast and TLA returns complete results. Oth-erwise, TLA has two choices: (1) If the utility of the cur-rent query is no less than q  X  , then return the query results at t  X  ; Or (2) Complete the query and collect the results from all MLAs and their ISNs. In practice, system failure timeouts are set at TLA/MLA to stop waiting in case of server/network failure.
 MLA policy. Suppose the messaging time between an MLA i and its TLA is d i . For each query, MLA i responds to TLA in one of the two cases.  X  If MLA i received all ISN responses before time t  X   X  d it sends back the responses to TLA. This is a common case, where an MLA only sends one message to TLA.  X  Otherwise, the MLA sends its results to TLA twice, because it cannot tell if a query is straggling or long (without Algorithm 3 FSL-K: Offline processing global information from TLA). First, at time t  X   X  d i , it sends back all the responses it received so far to TLA, in order to catch the checkpoint time of TLA at t  X  . Second, the MLA sends the additional results to TLA after it received the responses from all of its ISNs, which is to obtain full results for the long queries that do not finish prior to t  X   X  d i
Alternatively, a TLA can inform its MLAs of not sending the second message when it decides to early return. The benefits of this alternative are arguable since (1) This does not decrease the overall number of messages (i.e., reducing one MLA message for straggling queries by adding a TLA message); (2) MLAs still send two messages for the long queries; (3) Search engines usually do not abort execution of straggling queries at ISNs (Section 2).

FSL-K is efficient at runtime in terms of both computa-tion and communication. Its computation cost at both TLA and MLAs is negligible, requiring a simple counting on the number of responses. For communication cost, each MLA sends at most two messages to TLA. The average number of messages is even smaller: in most of the cases, queries are fast, requiring one message per MLA only; even if queries are straggling or long, only the MLAs with slow ISNs send two messages. For example, MLAs send 2 messages only 5.58% of the time on our evaluation using commercial ad-vertisement search workloads (Section 6.2).

Offline Processing of FSL-K. Algorithm 3 presents of-fline processing of FSL-K, which has a similar flow as FSL with two differences. (1) FSL-K requires messaging times from MLAs to TLA as additional inputs. (2) At Step 2, to compute the utility of a query i at a time threshold t at TLA, we sum over its utilities obtained at the q -th MLA at time threshold t  X  d q . Its time complexity is still O (( rn + n log( n ))( t max / X  )).
 Optimality Proof. Theorem 2 shows the optimality of FSL-K. Its proof is analogous to Theorem 1, which we skip for interest of space.

Theorem 2. For two-level aggregation with known mes-saging time, FSL-K achieves an arbitrarily close-to-minimum tail latency while meeting utility constraints.
We extend FSL-K to FSL-B for the case when messaging times between aggregators are unknown but bounded within a small range, where  X  X  X  of FSL-B stands for Bounded mes-saging time. Suppose that the messaging time from the q -th MLA to TLA for the i -th query is bounded in the range [ Y the (worst-case) upper bound Y + i,q as messaging time, and then uses the same offline and online processing as FSL-K. Thus, FSL-B is guaranteed to satisfy all utility constraints, and the resulting tail latency is at most max i,q ( Y higher than theoretically optimal in the worst case.
Now, we consider a general case where the messaging times between aggregators are unknown with potentially large variations. It models the cases such as with poor net-work condition or with aggregators sharing processing with other workloads. In this case, even if we find the optimal time threshold t  X  at TLA, it is still difficult for MLAs to decide when to respond to TLA to meet the checkpoint t  X  . While it is challenging to provide a performance guarantee, we develop an effective heuristic policy, which shows very good empirical performance (Section 6). We call this FSL-U, where  X  X  X  stands for Unknown messaging time.

FSL-U defines three parameters: besides time and util-ity thresholds t  X  and q  X  , respectively, FSL-U includes the third parameter, i.e., MLA threshold t  X  m , which indicates the time threshold when MLAs shall return their results to TLA. Note that we choose to use the same t  X  m for all MLAs, be-cause without knowing how much it may take each MLA to respond, there is no reason to differentiate MLAs in terms of the time thresholds for returning their responses. For search engine in practice, each MLA is often associated with the same number of ISNs and running on the same hardware.
Online Processing of FSL-U. TLA still takes t  X  and u  X  as inputs, working in the same way as FSL-K. MLA also works similarly as FSL-K, but it applies an additional MLA time threshold of t  X  m , obtained by offline processing. Under FSL-U, each type of queries behaves as follows.  X  Fast queries: TLA receives complete results before t  X  To achieve this, every MLA must have received all its ISNs X  responses before t  X  m , and all messaging times between MLAs and TLA are smaller than t  X   X  t  X  m .  X  Straggling queries: The utility at TLA is not equal to 1, but bigger than or equal to u  X  at t  X  . This can happen if any one of the above conditions for fast queries does not satisfy.  X  Long queries: The utility at TLA is smaller than u  X  at t . In this case, TLA waits for the responses from all MLAs and their associated ISNs (within timeouts).
 Offline Processing of FSL-U. Algorithm 4 uses both ISN latencies and inter-aggregator messaging times to search for the optimal values of the three parameters. Compared with FSL-K, the main difference is that FSL-U needs to search over one more dimension  X  the time threshold t for MLA (Step 2 of Algorithm 4). The time complexity of Algorithm 4 is O (( rn + n log( n ))( t max / X  ) 2 ), which is higher than FSL-K by a multiplicative factor O ( t max / X  ) due to the enlarged search space of t  X  m .

Note that, although offline processing of FSL-U still finds the best aggregation policy parameters ( t  X  ,u  X  ,t  X  m ) based on log data to minimize the tail latency, we cannot claim that FSL-U is optimal when applied online. The offline opti-Algorithm 4 FSL-U: Offline processing 6: Do Steps 3 &amp; 4 in Algorithm 2. 8: v = v + 1 9: end for mal solution may give different MLA time threshold values to each MLA, because it has the offline information on the inter-aggregator messaging times for each query while this information cannot be obtained online (unlike FSL-K where the messaging time is assumed to be known online). How-ever, our empirical results in Section 6 show FSL-U performs close to offline optimal.
We can easily extend FSL-K, FSL-B, and FSL-U to more than two levels of aggregators. Here, we still call the top-level aggregator as TLA, but there may be more than one level of MLAs. For FSL-K (known messaging time), TLA still has one time threshold t  X  and utility threshold q  X  levels of MLAs send their responses in advance such that TLA receives the responses prior to or at t  X  . The optimality of FSL-K still holds, and the time complexity is O (( srn + n log( n ))( t max / X  )) with s levels of aggregators. The exten-sion of FSL-B to multilevel aggregation is also straightfor-ward, following that of FSL-K. The performance bound of FSL-B becomes the summation of the upper bounds on mes-saging times over all levels. To extend FSL-U to multiple levels, we use a different time threshold t  X  m for each level of MLA. The resulting complexity of offline processing in-creases with the increased level of MLAs, due to enlarged search space, the time complexity is O (( rn + n log( n ))( t since each level of MLA has a search complexity of O ( t max
We evaluate FSL for single-level aggregation by comparing it to alternative policies using a production trace of a 44-ISN cluster from a commercial web search engine, and we use several latency objectives and utility constraints. We also study several distributions forming synthetic workloads. For two-level aggregation, we create eight aggregation policies and compare FSL-K and FSL-U to them using a trace from a two-level commercial advertisement engine, in addition to a synthetic workload to model a large system with 1936 ISNs. Figure 4: Tail latency comparison for single aggregator with Bing production traces.
Bing Production traces. We use a query log from a production cluster of Microsoft Bing search engine. The log records the latency of 44 ISNs over 66,922 queries. These queries missed the result cache and were sent to ISNs for pro-cessing. We use a subset of 10,000 queries for training, and the remaining 56,922 queries for evaluation. As Bing search does not abort processing of late responses at ISNs (Section 2), the recorded query execution times of ISNs remain un-affected by aggregation policy and hence we replay in our simulation to compare the alternative aggregation policies. We set the system failure timeout as 500 ms. A response that does not arrive by 500 ms is ignored by all policies.
Evaluation with production traces. Our target is to minimize the 95-th tail latency under the constraint that av-erage utility is greater than or equal to 0.99. We implement all policies in Table 1 and compute the best parameters for each policy using the same training dataset. We show their results in Figure 4 that depicts the 95-th tail latency on the x-axis and the average utility on the y-axis. Wait-All is the baseline with utility = 1 and it has the highest 95-th tail latency. For Utility-Only we use two configurations be-cause when the utility threshold is 0.99, the aggregator must wait for responses from all ISNs because (43 / 44) &lt; 0 . 99. We, therefore, set utility threshold to (43 / 44) = 0 . 977 and to (42 / 44) = 0 . 955, and report both settings in the figure. FSL provides the shortest tail latency compared to all other schemes: (FSL: 38 ms, Time-Utility: 65 ms, and Kwiken: 59 ms). In particular, FSL reduces the tail latency by 53% over Wait-All and by 36% over the best alternative. All policies have similar average latencies (between 15 and 17 ms) be-cause the average response latency is determined mainly by fast queries which constitute majority queries.

To see why FSL achieves lower tail latency, we study the utility of all queries according to their length (measured as the maximum latency of all ISN responses) quantized into bins of 20 ms as depicted in Figure 5. The first point on each curve shows the average utility for the queries with length  X  [0 , 20], and second point for  X  [20 , 40]. Notice that the first two points represent over 92% of queries. We do not show a curve for Utility-Only because it is a horizontal line across all queries regardless of their length. Time-only terminates all queries after a threshold and we see a similar behaviour for Time-Utility: All relatively slow queries are terminated Figure 5: Average utility for quantized query length (single aggregator &amp; Bing production traces). early regardless of being long or straggling. Next, Kwiken terminates queries starting from the second point which im-pacts many fast queries, and again, all relatively slow queries are terminated early. In contrast, FSL exploits the (5%) la-tency slackness to obtain high utility for long queries, which allows it to terminate straggling queries more aggressively, reducing tail latency further while meeting utility targets.
Sensitivity study. Using the production traces, we study the two tail latency optimization targets (95-th and 99-th) under several utility constraints as depicted in Table 2. The values of the first row show the 95-th tail latency correspond-ing to data in Figure 4. The stricter the utility constraint, the fewer the chances to reduce the tail latency (compared with Wait-All) while meeting the constraint. This behaviour applies both for average and tail utility constraints as well as across the two tail latency objectives. The last row shows that we can enforce two utility constraints, one on average and another on the 95-th tail utility. Under all configura-tions FSL reduces the tail latencies the most: It provides the minimum tail latency while meeting the utility constraints, consistently outperforming heuristic policies.

Synthetic datasets. We evaluate the policies using sev-eral synthetic datasets with average utility constraints (AVG  X  0.99) assuming a single aggregator with 44 ISNs. For each dataset, we generate 66,922 queries and use 10,000 as a train-ing set and the remaining 56,922 for evaluation. We use two methods to generate synthetic datasets: (1) In one-phase sampling, query latency at each ISN is sampled indepen-dently from a distribution. (2) In two-phase sampling, we first sample the query mean from a distribution, and then sample its latency at each ISN from a second distribution parametrized by the query mean. While one-phase sampling generates latencies from all ISNs of all queries assuming in-dependence, two-phase sampling models the scenarios where long (short) queries tend to have overall higher (lower) laten-cies from many of its ISN responses. We report the average of Pearson X  X  correlation coefficient (PCC) between ISNs for all pairs of ISNs, as well as the average of coefficient of varia-tion (  X / X  ) for all queries. As a reference, for the production traces: PCC = 0 . 9920 which is a high value showing signifi-cant correlation among ISN latencies for a particular query, and CV = 0 . 1777, a small value as workload is dominated by many fast queries.

Evaluation with synthetic datasets. Table 3 reports the results. The first two rows show one-phase sampling with log-normal and exponential distributions. PCC is small in-dicating little correlation among ISN latencies per query: each query has similar utility at any time point, which does not produce the straggling and long query behaviours. The policies Time-Only, Time-Utility, Kwiken reduce the tail la-tency compared to the baseline, and FSL provides bigger tail latency reduction. We observe similar behaviour under all the distributions we employed (not reported due to space).
Next, we study the two-phase sampling, which has higher correlation and is more realistic for search workloads. We use both Exponential and Bounded Pareto distributions for the first sampling phase. We use log-normal distribution for the second-phase sampling because sampling from normal distribution may result in negative values for latency. FSL clearly outperforms the other policies and the improvement of FSL over other polices increases as PCC increases. (a) Advertisement Engine Figure 6: Policy comparison for cluster with two-level ag-gregators on (1) production traces of advertisement engine and (2) synthetic datasets.

We evaluate two-level aggregation using a production trace of Bing Advertisement Engine as well as a synthetic dataset modelling a large-scale system with a few thousand ISNs.
Advertisement engine production traces. We use a query log from a production cluster with two-level aggre-gation of Bing advertisement engine. The log records the latencies of 1 TLA, 16 MLAs and 4 ISNs per MLA (total ISNs is 64) over 16,311 queries. We use a subset of 10,000 queries for training and the remaining 6,311 for evaluation.
There is no policy that targets two-level aggregation in prior work. So we develop the following eight policies for the MLA&amp;TLA policies. (1) Wait-All &amp; Wait-All (baseline), (2) Wait-All &amp; Utility-Only (0.95), (3) Utility-Only (0.95) &amp; Utility-Only (0.95), (4) Time-Only &amp; Time-Only, (5) Time-Utility &amp; Wait-All, (6) Wait-All &amp; Time-Utility,(7) Kwiken &amp; Wait-All, (8) Wait-All &amp; Kwiken. We make the MLA delay known to FSL-K (which obtains the optimal solution) and unknown to FSL-U (which is heuristic) and study how they compare to each other as well as to other polices.
Figure 6(a) compares the 95-th tail latencies for average utility target of 0.99. FSL-K offers the shortest tail latency (112 ms). FSL-U is close (125 ms), within 12% of the opti-mal, lower by 30% over the baseline and lower by 15% over the best alternative policy. (Wait-All &amp; Utility-Only and Utility-Only &amp; Utility-Only produce same tail latency and average utility as baseline, so are not plotted in the figure.) Evaluation with synthetic datasets. We also evaluate FSL using a synthetic dataset modelling a large-scale system with two levels of aggregation containing 1 TLA, 44 MLA and 44 ISNs per MLA (total ISNs is 1,936). The synthetic dataset has the same mean and variance as Bing queries. In addition, the MLA messaging delay is sampled from an exponential distribution with parameter  X  = 7 . 5 ms, which is the measured average messaging time between aggrega-tors. Figure 6(b) shows that FSL-K provides the shortest tail latency (49 ms), and FSL-U is close (54 ms) and is sub-stantially better than the other polices; the tail latency of FSL-U is within 16% of the optimal, and lower by 51% over the baseline and by 38% over the best alternative. A thor-ough sensitivity study is presented in a technical report [32].
Complementary latency reduction techniques on web search. ISNs of web search often employ dynamic pruning to early terminate the post-list processing of a query and to avoid scoring the postings for the documents that un-likely make the top-k retrieved set [3, 33]. Early termination is an example that web search trades completeness of query response for latency. Prior work [19] quantifies the relation-ship of quality and latency at ISNs, and trades quality for latency under heavy loads. Multicores [15, 22] and graphics processors [14] have also been used to parallelize query pro-cessing and reduce query latency. All the above techniques improve ISN latency, complementary to our study where we take ISN latency as inputs and focus on aggregation policy.
Beyond ISN-level improvement, there are studies on re-ducing the latency for search queries across different system components, e.g., optimizing caching [6, 9, 10, 16, 26] and prefetching [23] to mitigate I/O costs, and improving net-work protocols between ISNs and aggregators [28]. These studies are also complementary to our work. Only queries that are not cached are sent to ISNs for processing. We take transmission time between ISNs and aggregators as inputs, working (orthogonally) for any transmission protocols.
Request reissue is a standard technique to reduce latency tail in distributed systems at the cost of increased resource usage [13, 20, 29, 34]. When web indices are replicated, reissuing a query at another cluster or server may improve latency by using responses that finishes first. Reissuing con-sumes extra resources to replicate indexes and to process a query more than once. Therefore, it is more preferable to have an effective aggregation policy to reduce latency and meet quality SLAs without reissuing many queries.
Aggregation policies. We will not repeat the discus-sions of existing aggregation policies and performance in Sections 3 and 6. Broder and Mitzenmarcher [8] study a related problem on maximizing reward as a function of time and utility, but they assume the responses of ISNs are i.i.d (independent and identically distributed), which Section 6.1 shows that ISN responses are highly correlated in practice. Moreover, they do not consider multilevel aggregations. Fi-nally, we note that aggregation problem also exists in other domains (e.g., aggregate data sources in wireless sensor net-works for energy minimization [30]), which requires domain-specific techniques and differs from our work.
We develop an aggregation policy, combining data-driven offline analysis with online processing, to reduce tail latency of web search queries subject to search quality SLAs. We first focus on a single aggregator and prove the optimality of our policy. We then extend our policy for multilevel aggrega-tion and prove its optimality when messaging times between aggregators are known. We also introduce an empirically-effective policy to address unknown messaging time. We conduct experiments using production logs from a commer-cial web search engine, a commercial advertisement engine and synthetic workloads. Compared with prior work, the proposed policy reduces tail latency by up to 40%.
 This work was partially done while J.-M. Yun was an intern at Microsoft Research. S. Ren was supported in part by the U.S. NSF CNS-1423137 and CNS-1453491.

