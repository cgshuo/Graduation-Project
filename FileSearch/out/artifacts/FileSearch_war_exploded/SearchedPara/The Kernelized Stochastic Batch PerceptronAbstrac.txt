 Andrew Cotter cotter@ttic.edu Shai Shalev-Shwartz shais@cs.huji.ac.il John S. Cohen SL in CS, The Hebrew University of Jerusalem, Israel Nathan Srebro nati@ttic.edu We present a novel algorithm for training kernel Sup-port Vector Machines (SVMs). One may view a SVM as the bi-criterion optimization problem of seeking a predictor with large margin (low norm) on the one hand, and small training error on the other. Our approach is a stochastic gradient method on a non-standard scalarization of this bi-criterion problem. In particular, we use the  X  X lack constrained X  scalar-ized optimization problem introduced by Hazan et al. (2011) where we seek to maximize the classification margin, subject to a constraint on the total amount of  X  X lack X , i.e. sum of the violations of this margin. Our approach is based on an efficient method for comput-ing unbiased gradient estimates on the objective. Our algorithm can be seen as a generalization of the  X  X atch Perceptron X  to the non-separable case (i.e. when errors are allowed), made possible by introducing stochas-ticity, and we therefore refer to it as the  X  X tochastic Batch Perceptron X  (SBP).
 The SBP is fundamentally different from Pegasos (Shalev-Shwartz et al., 2011) and other stochastic gra-dient approaches to the problem of training SVMs, in that calculating each stochastic gradient estimate still requires considering the entire data set. In this re-gard, despite its stochasticity, the SBP is very much a  X  X atch X  rather than  X  X nline X  algorithm. For a lin-ear SVM, each iteration would require runtime linear in the training set size, resulting in an unacceptable overall runtime. However, in the kernel setting, es-sentially all known approaches already require linear runtime per iteration. A more careful analysis reveals the benefits of the SBP over previous kernel SVM op-timization algorithms.
 In order to compare the SBP runtime to the runtime of other SVM optimization algorithms, which typi-cally work on different scalarizations of the bi-criterion problem, we follow Bottou &amp; Bousquet (2008); Shalev-Shwartz &amp; Srebro (2008) and compare the runtimes required to ensure a generalization error of L  X  + , assuming the existence of some unknown predictor u with norm k u k and expected hinge loss L  X  . The main advantage of the SBP is in the regime in which =  X ( L  X  ), i.e. we seek a constant factor approxima-tion to the best achievable error (e.g. we would like an error of 1 . 01 L  X  ). In this regime, the overall SBP run-time is k u k 4 / , compared with k u k 4 / 3 for Pegasos and k u k 4 / 2 for the best known dual decomposition approach. Training a SVM amounts to finding a vector w defin-ing a classifier x 7 X  sign(  X  w,  X  ( x )  X  ), that on the one hand has small norm (corresponding to a large classi-fication margin), and on the other has a small training error, as measured through the average hinge loss on the training sample:  X  L ( w ) = 1 n P n i =1 ` ( y i  X  w,  X  ( x where each ( x i ,y i ) is a labeled example, and ` ( a ) = max (0 , 1  X  a ) is the hinge loss. This is captured by the following bi-criterion optimization problem: We focus on kernelized SVMs, where the feature map  X ( x ) is specified implicitly via a kernel K ( x,x  X   X  ( x ) ,  X  ( x 0 )  X  , and assume that K ( x,x 0 )  X  1. We consider only  X  X lack box X  access to the kernel (i.e. our methods work for any kernel, as long as we can com-pute K ( x,x 0 ) efficiently), and in our runtime analy-sis treat kernel evaluations as requiring O (1) runtime. Since kernel evaluations dominate the runtime of all methods studied (ours as well as previous methods), one can also interpret the runtimes as indicating the number of required kernel evaluations. To simplify our derivation, we often discuss the explicit SVM, using  X ( x ), and refer to the kernel only when needed. A typical approach to the bi-criterion Problem 2.1 is to scalarize it using a parameter  X  controlling the tradeoff between the norm (inverse margin) and the empirical error: Different values of  X  correspond to different Pareto optimal solutions of Problem 2.1, and the entire Pareto front can be explored by varying  X  .
 We instead consider the  X  X lack constrained X  scalar-ization (Hazan et al., 2011), where we maximize the  X  X argin X  subject to a constraint of  X  on the total allowed  X  X lack X , corresponding to the average error. That is, we aim at maximizing the margin by which all points are correctly classified (i.e. the minimal dis-tance between a point and the separating hyperplane), after allowing predictions to be corrected by a total amount specified by the slack constraint: In this scalarization, varying  X  explores different Pareto optimal solutions of Problem 2.1. This is cap-tured by the following Lemma, which also quantifies how suboptimal solutions of the slack-constrained ob-jective correspond to Pareto suboptimal points: Lemma 2.1. (Hazan et al., 2011, Lemma 2.1) For any u 6 = 0 , consider Problem 2.3 with  X  =  X  L ( u ) / k u k . Let  X  w be an  X  -suboptimal solution to this problem with objective value  X  , and consider the rescaled solution w =  X  w/ X  . Then: In this section, we will develop the Stochastic Batch Perceptron. We consider Problem 2.3 as optimization of the variable w with a single constraint k w k  X  1, with the objective being to maximize: f ( w ) = max Notice that we replaced the minimization over train-ing indices i in Problem 2.3 with an equivalent mini-mization over the probability simplex,  X  n = { p 0 : 1
T p = 1 } , and that we consider p and  X  to be a part of the objective, rather than optimization variables. The objective f ( w ) is a concave function of w , and we are maximizing it over a convex constraint k w k  X  1, and so this is a convex optimization problem in w . Our approach will be to perform a stochastic gradi-ent update on w at each iteration: take a step in the direction specified by an unbiased estimator of a (su-per)gradient of f ( w ), and project back to k w k X  1. To this end, we will need to identify the (super)gradients of f ( w ) and understand how to efficiently calculate unbiased estimates of them. 3.1. Warmup: The Separable Case As a warmup, we first consider the separable case, where  X  = 0 and no errors are allowed. The objec-tive is then: This is simply the  X  X argin X  by which all points are correctly classified, i.e.  X  s.t.  X  i y i  X  w,  X ( x i )  X  X  X   X  . We seek a linear predictor w with the largest possible mar-gin. It is easy to see that (super)gradients with re-spect to w are given by y i  X ( x i ) for any index i attain-ing the minimum in Equation 3.2, i.e. by the  X  X ost poorly classified X  point(s). A gradient ascent approach would then be to iteratively find such a point, update w  X  w +  X y i  X ( x i ), and project back to k w k X  1. This is akin to a  X  X atch Perceptron X  update, which at each iteration searches for a violating point and adds it to the predictor.
 In the separable case, we could actually use exact su-pergradients of the objective. As we shall see, it is computationally beneficial in the non-separable case to base our steps on unbiased gradient estimates. We therefore refer to our method as the  X  X tochastic Batch Perceptron X  (SBP), and view it as a generalization of the batch Perceptron which uses stochasticity and is applicable in the non-separable setting. In the same way that the  X  X atch Perceptron X  can be used to max-imize the margin in the separable case, the SBP can be used to obtain any SVM solution along the Pareto front of the bi-criterion Problem 2.1. 3.2. Supergradients of f ( w ) For a fixed w , we define c  X  R n be the vector of  X  X e-sponses X : Supergradients of f ( w ) at w can be characterized explicitly in terms of minimax-optimal pairs p  X  and  X   X  such that p  X  = arg min p  X   X  n p t ( c +  X   X  ) and  X  Lemma 3.1 (Proof in Appendix C) . For any w , let p , X   X  be minimax optimal for Equation 3.1. Then P This suggests a simple method for obtaining unbiased estimates of supergradients of f ( w ): sample a train-ing index i with probability p  X  i , and take the stochas-tic supergradient to be y i  X  ( x i ). The only remaining question is how one finds a minimax optimal p  X  . It is possible to find a minimax optimal p  X  in O ( n ) time. For any  X  , a solution of min p  X   X  n p T ( x +  X  ) must put all of the probability mass on those indices i for which c i +  X  i is minimized. Hence, an optimal  X   X  will maximize the minimal value of c i +  X   X  i . This is illus-trated in Figure 1. The intuition is that the total mass n X  available to  X  is distributed among the indices as if this volume of water were poured into a basin with height c i . The result is that the indices i with the low-est responses have columns of water above them such that the common surface level of the water is  X  . Once the  X  X ater level X   X  has been determined, the op-timal p  X  must be uniform on those indices i for which  X  i &gt; 0, i.e. for which c i &lt;  X  , must be zero on all i s.t. c i &gt;  X  , and could take any intermediate value when c i =  X  (that is, for some q &gt; 0, we must have c i &lt;  X   X  p  X  i = q , c i =  X   X  0  X  p  X  i  X  q , and c &gt;  X   X  p  X  i = 0 X  X ee Figure 1). In particular, the uniform distribution over all indices such that c i  X   X  is minimax optimal. Notice that in the separable case, where no slack is allowed,  X  = min i c i and any distribu-tion supported on the minimizing point(s) is minimax optimal, and y i  X ( x i ) is an exact supergradient for such an i , as discussed in Section 3.1.
 It is straightforward to find the water level  X  in linear time once the responses c i are sorted (as in Figure 1), i.e. with a total runtime of O ( n log n ) due to sorting. It is also possible to find the water level  X  in linear time, without sorting the responses, using a divide-and-conquer algorithm, further of which may be found in Appendix B 1 . 3.3. Kernelized Implementation In a kernelized SVM, w is an element of an implicit space, and cannot be represented explicitly. We there-fore represent w as w = P n i =1  X  i y i  X  ( x i ), and main-tain not w itself, but instead the coefficients  X  i . Our stochastic gradient estimates are always of the form y
 X ( x i ) for an index i . Taking a step in this direction amounts to simply increasing the corresponding  X  i . We could calculate all the responses c i at each iteration require a quadratic number of kernel evaluations per iteration . Instead, as is typically done in kernelized SVM implementations, we keep the responses c i on hand, and after each stochastic gradient step of the form w  X  w +  X y j  X  ( x j ), we update the responses as: This involves only n kernel evaluations per iteration. In order to project w onto the unit ball, we must ei-ther track k w k or calculate it from the responses as k w k = P n i =1  X  i c i . Rescaling w so as to project it back into k w k  X  1 is performed by rescaling all coefficients  X  i and responses c i , again taking time O ( n ) and no additional kernel evaluations. 3.4. Putting it Together We are now ready to summarize the SBP algorithm. Starting from w (0) = 0 (so both  X  (0) and all responses are zero), each iteration proceeds as follows: 1. Find p  X  by finding the  X  X ater level X   X  from the re-2. Sample j  X  p  X  . Updating the responses as in Equation 3.4 requires O ( n ) kernel evaluations (the most computationally ex-pensive part) and all other operations require O ( n ) scalar arithmetic operations.
 Since at each iteration we are just updating using an unbiased estimator of a supergradient, we can rely on the standard analysis of stochastic gradient descent to bound the suboptimality after T iterations: Lemma 3.2 (Proof in Appendix C) . For any T, X  &gt; 0 , after T iterations of the Stochastic Batch Perceptron, with probability at least 1  X   X  , the average iterate  X  w = 1 T P T t =1 w ( t ) (corresponding to  X   X  = 1 T P satisfies: f (  X  w )  X  sup k w k X  1 f ( w )  X  O q 1 T log Since each iteration is dominated by n kernel evalu-ations, and thus takes linear time (we take a kernel evaluation to require O (1) time), the overall runtime to achieve suboptimality for Problem 2.3 is O ( n/ 2 ). 3.5. Learning Runtime The previous section has given us the runtime for ob-taining a certain suboptimality of Problem 2.3. How-ever, since the suboptimality in this objective is not di-rectly comparable to the suboptimality of other scalar-izations, e.g. Problem 2.2, we follow Bottou &amp; Bous-quet (2008); Shalev-Shwartz &amp; Srebro (2008), and an-alyze the runtime required to achieve a desired gen-eralization performance, instead of that to achieve a certain optimization accuracy on the empirical opti-mization problem.
 Recall that our true learning objective is to find a predictor with low generalization error L 0 / 1 ( w ) = Pr ( x,y ) { y  X  w,  X ( x )  X  X  X  0 } with respect to some un-known distribution over x,y based on a training set drawn i.i.d. from this distribution. We assume that there exists some (unknown) predictor u that has norm k u k and low expected hinge loss L  X  = L ( u ) = E [ ` ( y  X  u,  X ( x )  X  )] (otherwise, there is no point in train-ing a SVM), and analyze the runtime to find a predic-tor w with generalization error L 0 / 1 ( w )  X L  X  + . In order to understand the SBP runtime, we must de-termine both the required sample size and optimiza-tion accuracy. Following Hazan et al. (2011), and based on the generalization guarantees of Srebro et al. (2010), using a sample of size: and optimizing the empirical SVM bi-criterion Prob-lem 2.1 such that: suffices to ensure L 0 / 1 ( w )  X  L  X  + with high proba-bility. Referring to Lemma 2.1, Equation 3.6 will be satisfied for  X  w/ X  as long as  X  w optimizes the objective of Problem 2.3 to within:  X  = where the inequality holds with high probability for the sample size of Equation 3.5. Plugging this sample size and the optimization accuracy of Equation 3.7 into the SBP runtime of O ( n/  X  2 ) yields the overall runtime: for the SBP to find  X  w such that its rescaling satisfies L 0 / 1 ( w )  X L ( u ) + with high probability.
 In the realizable case, where L  X  = 0, or more gener-ally when we would like to reach L  X  to within a small constant multiplicative factor, we have =  X ( L  X  ), the first factor in Equation 3.8 is a constant, and the run-time simplifies to  X  O ( k u k 4 / ). As we will see in Section 4, this is a better guarantee than that enjoyed by any other SVM optimization approach. 3.6. Including an Unregularized Bias It is possible to use the SBP to train SVMs with a bias term, i.e. where one seeks a predictor of the form x 7 X  (  X  w,  X ( x )  X  + b ). We then take stochastic gradient steps on: f ( w ) = (3.9) Lemma 3.1 still holds, but we must now find mini-max optimal p  X  ,  X   X  and b  X  . This can be accomplished using a modified  X  X ater filling X  involving two basins, one containing the positively-classified examples, and the other the negatively-classified ones. As in the case without an unregularized bias, this can be ac-complished in O ( n ) time X  X ee Appendix B for details. We discuss the relationship between the SBP and sev-eral other SVM optimization approaches, highlighting similarities and key differences, and comparing their performance guarantees. 4.1. SIMBA Recently, Hazan et al. (2011) presented SIMBA, a method for training linear SVMs based on the same  X  X lack constrained X  scalarization (Problem 2.3) we use here. SIMBA also fully optimizes over the slack vari-ables  X  at each iteration, but differs in that, instead of fully optimizing over the distribution p (as the SBP does), SIMBA updates p using a stochastic mirror de-scent step. The predictor w is then updated, as in the SBP, using a random example drawn according to p . A SBP iteration is thus in a sense more  X  X horough X  then a SIMBA iteration. The SBP theoretical guarantee (Lemma 3.2) is correspondingly better by a logarith-mic factor (compare to Hazan et al. (2011, Theorem 4.3)). All else being equal, we would prefer performing a SBP iteration over a SIMBA iteration.
 For linear SVMs, a SIMBA iteration can be performed in time O ( n + d ). However, fully optimizing p as de-scribed in Section 3.2 requires the responses c i , and calculating or updating all n responses would require time O ( nd ). In this setting, therefore, a SIMBA iter-ation is much more efficient than a SBP iteration. In the kernel setting, calculating even a single response requires O ( n ) kernel evaluation, which is the same cost as updating all responses after a change to a single coordinate  X  i (Section 3.3). This makes the responses essentially  X  X ree X , and gives an advantage to methods such as the SBP (and the dual decomposition methods discussed below) which make use of the responses. Although SIMBA is preferable for linear SVMs, the SBP is preferable for kernelized SVMs. It should also be noted that SIMBA relies heavily on having direct access to features, and that it is therefore not obvious how to apply it directly in the kernel setting. 4.2. Pegasos and SGD on  X  L ( w ) Pegasos (Shalev-Shwartz et al., 2011) is a SGD method optimizing the regularized scalarization of Problem 2.2. Alternatively, one can perform SGD on  X  L ( w ) sub-ject to the constraint that k w k  X  B , yielding similar learning guarantees (e.g. (Zhang, 2004)). At each iter-ation, these algorithms pick an example uniformly at random from the training set. If the margin constraint is violated on the example, w is updated by adding to it a scaled version of y i  X ( x i ). Then, w is scaled and possibly projected back to k w k  X  B . The actual up-date performed at each iteration is thus very similar to that of the SBP. The main difference is that in Pega-sos and related SGD approaches, examples are picked uniformly at random, unlike the SBP which samples from the set of violating examples.
 In a linear SVM, where  X ( x i )  X  R d are given ex-plicitly, each Pegasos (or SGD on  X  L ( w )) iteration is extremely simple and requires runtime which is lin-ear in the dimensionality of  X ( x i ). A SBP update would require calculating and referring to all O ( n ) re-sponses. However, with access only to kernel evalua-tions, even a Pegasos-type update requires either con-sidering all support vectors, or alternatively updating all responses, and might also take O ( n ) time, just like the much  X  X marter X  SBP step.
 To understand the learning runtime of such methods in the kernel setting, recall that SGD converges to an -accurate solution of the optimization problem after at most k u k 2 / 2 iterations. Therefore, the overall run-time is n k u k 2 / 2 . Combining this with Equation 3.5 yields that the runtime requires by SGD to achieve a learning accuracy of is  X  O (( L  X  + ) / ) k u k 4 / 3 When =  X ( L  X  ), this scales as 1 / 3 compared with the 1 / scaling for the SBP (see also Table 1). 4.3. Dual Decomposition Methods Many of the most popular packages for optimiz-ing kernel SVMs, including LIBSVM (Chang &amp; Lin, 2001) and SVM-Light (Joachims, 1998), use dual-decomposition approaches. This family of algorithms works on the dual of the scalarization 2.2, given by: and proceed by iteratively choosing a small working set of dual variables  X  i , and then optimizing over these variables while holding all other dual variables fixed. At an extreme, SMO (Platt, 1998) uses a working set of the smallest possible size (two in problems with an unregularized bias, one in problems without). Most dual decomposition approaches rely on having access to all the responses c i (as in the SBP), and employ some heuristic to select variables  X  i that are likely to enable a significant increase in the dual objective. On an objective without an unregularized bias the structure of SMO is similar to the SBP: the responses c are used to choose a single point j in the training set, then  X  j is updated, and finally the responses are updated accordingly. There are two important differ-ences, though: how the training example to update is chosen, and how the change in  X  j is performed. SMO updates  X  j so as to exactly optimize the dual Problem 4.1, while the SBP takes a step along  X  j so as to improve the primal Problem 2.3. Dual feasibility is not maintained, so the SBP has more freedom to use large coefficients on a few support vectors, potentially resulting in sparser solutions.
 The use of heuristics to choose the training example to update makes SMO very difficult to analyze. Although it is known to converge linearly after some number of iterations (Chen et al., 2006), the number of itera-tions required to reach this phase can be very large (see a detailed discussion in Appendix E). To the best of our knowledge, the most satisfying analysis for a dual decomposition method is the one given in Hush et al. (2006). In terms of learning runtime, this analy-sis yields a runtime of  X  O (( L ( u ) + ) / ) 2 k u k 4 guarantee L 0 / 1 ( w )  X  L ( u ) + . When =  X ( L  X  ), this runtime scales as 1 / 2 , compared with the 1 / guaran-tee for the SBP. 4.4. Stochastic Dual Coordinate Ascent Another variant of the dual decomposition approach is to choose a single  X  i randomly at each iteration and update it so as to optimize Equation 4.1 (Hsieh et al., 2008). The advantage here is that we do not need to use all of the responses at each iteration, so that if it is easy to calculate responses on-demand, as in the case of linear SVMs, each SDCA iteration can be calculated in time O ( d ) (Hsieh et al., 2008). In a sense, SDCA relates to SMO in a similar fashion that Pegasos re-lates to the SBP: SDCA and Pegasos are preferable on linear SVMs since they choose working points at ran-dom; SMO and the SBP choose working points based on more information (namely, the responses), which are unnecessarily expensive to compute in the linear case, but, as discussed earlier, are essentially  X  X ree X  in kernelized implementations. Pegasos and the SBP both work on the primal (though on different scalar-izations), while SMO and SDCA work on the dual and maintain dual feasibility.
 The current best analysis of the runtime of SDCA is not satisfying, and yields the bound n/ X  on the num-ber of iterations, which is a factor of n larger than the bound for Pegasos. Since the cost of each iteration is the same, this yields a significantly worse guarantee. We do not know if a better guarantee can be derived for SDCA. See a detailed discussion in Appendix E. 4.5. The Online Perceptron We have so far considered only the problem of opti-mizing the bi-criterion SVM objective of Problem 2.1. However, because the online Perceptron achieves the same form of learning guarantee (despite not optimiz-ing the bi-criterion objective), it is reasonable to con-sider it, as well.
 The online Perceptron makes a single pass over the training set. At each iteration, if w errs on the point under consideration (i.e. y i  X  w,  X ( x i )  X  X  X  0), then y
 X ( x i ) is added into w . Let M be the number of mistakes made by the Perceptron on the sequence of examples. Support vectors are added only when a mis-take is made, and so each iteration of the Perceptron involves at most M kernel evaluations. The total run-time is therefore Mn .
 While the Perceptron is an online learning algorithm, it can also be used for obtaining guarantees on the generalization error using an online-to-batch conver-sion (e.g. (Cesa-Bianchi et al., 2001)).
 From a bound on the number of mistakes M (e.g. Shalev-Shwartz (2007, Corollary 5)), it is possible to show that the expected number of mistakes the Percep-tron makes is upper bounded by n L ( u )+ k u k p n L ( u )+ k u k 2 . This implies that the total runtime required by the Perceptron to achieve L 0 / 1 ( w )  X  L ( u ) + is O (( L ( u ) + ) / ) 3 k u k 4 / . This is of the same or-der as the bound we have derived for SBP. However, the Perceptron does not converge to a Pareto optimal solution to the bi-criterion Problem 2.1, and there-fore cannot be considered a SVM optimization proce-dure. Furthermore, the online Perceptron generaliza-tion analysis relies on an  X  X nline-to-batch X  conversion technique (e.g. (Cesa-Bianchi et al., 2001)), and is therefore valid only for a single pass over the data. If we attempt to run the Perceptron for multiple passes, then it might begin to overfit uncontrollably. Although the worst-case theoretical guarantee obtained after a single pass is indeed similar to that for an optimum of the SVM objective, in practice an optimum of the em-pirical SVM optimization problem does seem to have significantly better generalization performance. We compared the SBP to other SVM optimization ap-proaches on the datasets in Table 2. We compared to Pegasos (Shalev-Shwartz et al., 2011), SDCA (Hsieh et al., 2008), and SMO (Platt, 1998) with a second order heuristic for working point selection (Fan et al., 2005). These approaches work on the regularized for-mulation of Problem 2.2 or its dual (Problem 4.1). To enable comparison, the parameter  X  for the SBP was derived from  X  as k  X  w  X  k  X  = 1 n P n i =1 ` ( y i  X  w  X  where  X  w  X  is the known (to us) optimum.
 We first compared the methods on a SVM formula-tion without an unregularized bias, since Pegasos and SDCA do not naturally handle one. So that this comparison would be implementation-independent, we measure performance in terms of the number of ker-nel evaluations. As can be seen in Figure 2, the SBP outperforms Pegasos and SDCA, as predicted by the upper bounds. The SMO algorithm has a dramatically different performance profile, in line with the known analysis: it makes relatively little progress, in terms of generalization error, until it reaches a certain criti-cal point, after which it converges rapidly. Unlike the other methods, terminating SMO early in order to ob-tain a cruder solution does not appear to be advisable. We also compared to the online Perceptron algorithm. Although use of the Perceptron is justified for non-separable data only if run for a single pass over the training set, we did continue running for multiple passes. The Perceptron X  X  generalization performance is similar to that of the SBP for the first epoch, but the SBP continues improving over additional passes. As discussed in Section 4.5, the Perceptron is unsafe and might overfit after the first epoch, an effect which is clearly visible on the Adult dataset.
 To give a sense of actual runtime, we compared our im-plementation of the SBP 2 to the SVM package LIB-SVM, running on an Intel E7500 processor. We al-lowed an unregularized bias (since that is what LIB-SVM uses), and used the parameters in Table 2. For these experiments, we replaced the Reuters dataset with the version of the Forest dataset used by Nguyen et al. (2010), using their parameters. LIBSVM con-verged to a solution with 14 . 9% error in 195s on Adult, 0 . 44% in 1980s on MNIST, and 1 . 8% in 35 hours on Forest. In one-quarter of each of these runtimes, SBP obtained 15 . 0% error on Adult, 0 . 46% on MNIST, and 1 . 6% on Forest. These results of course depend heavily on the specific stopping criterion used. The Stochastic Batch Perceptron is a novel approach for training kernelized SVMs. The SBP fares well empirically, and, as summarized in Table 1, our run-time guarantee for the SBP is the best of any existing guarantee for kernelized SVM training. An interesting open question is whether this runtime is optimal, i.e. whether any algorithm relying only on black-box ker-nel accesses must perform  X  (( L  X  + ) / ) 3 k u k 4 / kernel evaluations.
 As with other stochastic gradient methods, deciding when to terminate SBP optimization is an open issue. The most practical approach seems to be to terminate when a holdout error stabilizes. We should note that even for methods where the duality gap can be used (e.g. SMO), this criterion is often too strict, and the use of cruder criteria may improve training time. Acknowledgements: S. Shalev-Shwartz is supported Without unreg. bias With unreg. bias
Data set Training size n Testing size  X  2  X   X   X  2  X   X  Reuters money fx 7770 3229 0 . 5 1 / n 6 . 34  X  10  X  4
MNIST  X 8 X  vs. rest 60000 10000 25 1 / n 2 . 21  X  10  X  4 25 1 /
