
Temporal data, which is a sequence of data tuples mea-sured at successive time instances, is typically very large. Hence instead of mining the entire data, we are interested in dividing the huge data into several smaller intervals of interest which we call temporal neighborhoods. In this paper we propose an approach to generate temporal neighborhoods through unequal depth discretization.
 We describe two novel algorithms (a) S imilarity based Merg ing ( SMerg ) and, (b) St ationary distribution based Merg ing ( StMerg ). These algorithms are based on the robust framework of Markov models and the Markov Sta-tionary distribution respectively. We identify temporal neigh-borhoods with distinct demarcations based on unequal depth discretization of the data. We discuss detailed experimental results in both synthetic and real world data. Specifically we show (i) the efficacy of our approach through precision and recall of labeled bins, (ii) the ground truth validation in real world datasets and, (iii) knowledge discovery in the temporal neighborhoods such as global anomalies. Our results indicate that we are able to identify valuable knowledge based on our ground truth validation from real world traffic data. Keywords Temporal neighborhoods, Discretization, Markov Model, Stationary Distribution Phenomenon in space and time are captured in spatio tem-poral datasets such as (a) sensors monitoring environmental phenomenon, (b) disease spread in a region over a period of time, and (c) vegetation changes captured in satellite imagery over time.
 If we consider the spatial and temporal aspects of data in combination, many interesting patterns may not be discov-ered due to the multi dimensional nature of the data. Pre-vious [11], [15], [20] approaches have successfully shown that considering the cross sections in space and time leads to discovery of interesting and meaningful patterns which would otherwise not be possible. To perform knowledge discovery in such complex datasets, the data has to be broken up into relevant sub groups. In spatial data this has been done by generating meaningful neighborhoods comprising of similarly behaving spatial objects [10], [24]. Our focus in this paper is on achieving this goal of generating meaningful neighborhoods for temporal data. In temporal data this can be done by discretizing the data into relevant intervals cap-turing the similar behavior in various intervals of time [6], [13], [14]. The intervals can be considered as the temporal neighborhoods which are relevant for knowledge discovery such as discovery of anomalous intervals or anomalous points within intervals.
 In this paper we focus on discovering temporal neighbor-hoods through discretization of temporal data by dividing it into unequal depth bins. This is useful in various application domains. For instance, if we want to discover peak periods in traffic monitoring data or discover unusual peak periods on some days, we can do so by discovering the intervals and quantifying the behavior of the objects in the interval using approximations. Secondly, if we want to discover specific local anomalies in the temporal data then we can mine the intervals and compare the distance of the data points in the interval to discover the specific points which are most unusual as compared to the rest. Thus it can be seen that the quality of mining results is highly dependent on the quality of the discretization. In general time series data is measured at successively uniform intervals of time. However our focus is not limited to data measured at uniform intervals therefore we use the term temporal data as our approach can be generalizable to any temporal datasets. Particularly for the temporal analysis data needs to be discretized or divided into bins for the analysis. In general equal width and equal frequency discretization are the most commonly used techniques. However these methods impose unnatural bounds on the data and in several cases are vulnerable to outliers in the data affecting the boundaries and widths of the discrete intervals.
 Let us consider a specific application in the domain of Transportation dealing with traffic congestion to motivate this problem. Here we consider the data being gathered by sensors that monitor the traffic at various locations along highways.
 excess of vehicles on a portion of a roadway at a particular time resulting in slower speeds than normal. Our focus here is the Recurring congestion that occurs at regular times at a site (for example: morning or evening peak hour congestion, or congestion due to regular events such as a street market on a particular day each week). Traffic congestion has become a major problem for many American cities with a measurable impact as outlined in the Texas Transportation Institute X  X  2007 Urban Mobility Report [23]. Specifically Congestion leads to $78 billion annual drain on the U.S. economy comprising of 4.2 billion lost hours and 2.9 billion gallons of wasted fuel. The average peak period traveler ends up spending an extra 38 hours of travel time and consumes an additional 26 gallons of fuel, amounting to a cost of $710 per traveler [23].
 If we analyze the traffic of any one specific spatial location we can see that the behavior, captured in traffic data, is consistent and repetitive (example at peak periods) yet highly variable and unpredictable (example: change in traffic pattern due to frequent crashes). This embodies the behavior of a temporal process. However a problem like congestion has redefined peak periods (historically and across cities, small vs large cities, peak periods vary from peak hour to peak intervals [5]). Thus definition of peak periods is critical in comparing various traffic patterns.
 We can see in this dynamic problem area several challenges emerge:(a) Addressing recurring problems is important since this will help in alleviating non-recurring problems, (b) The correct identification of peak periods is important for any traffic pattern analysis , (c) It is important to identify the right non-recurrent problems especially in peak periods so that the congestion does not worsen, (d) We need to consider the consistent vs. variable nature of the traffic data in such a way that small fluctuations do not impact the discovery of such peak periods at the same time these fluctuations for instance frequent crashes [25] can be captured in the intervals.
 To resolve these challenges a robust framework is re-quired which identifies these clearly demarcated distribu-tions within the temporal data. Such a demarcation of the data should provide an optimal solution and should not be adhoc or heuristic based such as equal width or equal frequency discretization. [17] discusses the symbolic representation of temporal data by using SAX technique and also describes how the clustering works on the symbolic representation with a very high degree of accuracy. How-ever it does not take into account the similarity between successive intervals while creating this approximation. Since the PAA technique used by SAX relies on the mean for approximation, it is highly susceptible to be affected by noise or outliers, thereby losing some interesting subtle patterns. Extended SAX technique [18] was proposed to improve this drawback of PAA which is highly affected by extremely large or small values, by keeping min and max values for each interval but still it does not consider similarities between adjacent intervals while obtaining a temporal approximation. In addition [7] and [12] discuss clustering temporal data. The major issue is that these tech-niques do not consider the similarity of data or distribution while creating bins. Further certain techniques such as equal frequency binning only consider the equal size of data in each of the bins. As a result, data tuples that are very similar to each other in terms of distribution may fall in different bins, even though they are adjacent. Hence local data mining applied to the bins may not be able to extract the expected pattern. Lastly the discretization should go beyond an approximation and be robust in the presence of noise. In this paper we address these issues to propose an approach for temporal neighborhood discovery using a Markov model based discretization technique.

Specifically we make the following contributions:  X  We present a novel approach to generate temporal  X  We model our approach using the robust framework of  X  We also present a stationary distribution of Markov  X  Both the algorithms take care of the transitive closure  X  We present detailed experimental results for (i) the The rest of the paper is organized as follows: In section II we outline our approach. In section III we discuss the experimental results. Finally we conclude in section IV.
In this paper we propose an approach to generate temporal neighborhoods by discretizing temporal data into unequal depth bins. We describe this process in the following distinct steps: (a) Markov Modeling: We first begin with an equal fre-quency binning to divide data into initial equal depth bins. We consider these bins as the states of a Markov model. We then compute the similarity between the bins using a distance measure d . We use various distance measures as described in section II-A1. We then generate a transition matrix based on these similarities for this Markov model and subsequently normalize to obtain a row-stochastic matrix. (b) Unequal depth discretization: In order to form an unequal depth discretization we propose two solutions. We use the intuition that the adjacent bins in the previous step having high degree of probability of transition should be merged, in order to obtain unequal depth bins. Specifically we propose a S imilarity based merg ing (SMerg) and a Markov St ationary distribution based merg ing (StMerg). We next describe our approach in details. Figure 1 presents our approach using both the algorithms.
 A. Markov Modeling
Our data is a series of temporal observations. We first formally define temporal data: Definition 1 (Temporal data): Sequence of data tuples X temporal measured at successive time instances T . A size-N dataset X temporal can be defined as X temporal { X i | i  X  I N } , where T = { t i | i  X  I N } with t i &lt; t i &lt; j and I N = { 1 , 2 ,..,N } , such that X ( t i ) = X X = { X i | t i  X  T ,i  X  I N } .
 Here the temporal data is associated with a set of attributes. For instance in the case of the traffic example attribute values can be speed or count of traffic. Next we divide the data using equal frequency binning defined as follows:
Definition 2 (Equal frequency binning): An n -partition of the temporal dataset X temporal into a set of equal depth temporal bins B temporal = { B 1 ,B 2 ,...B n } , each bin with size m = b N n c , such that B i  X  X temporal  X  with | B i | = m ,  X  i = 1 ...n .
 Given a set of temporal bins B temporal = { B i | i  X  I n mean  X  i and variance  X  2 i for all B i are computed in order to extract a temporal summarization set, which is a size-n set of 2-tuples  X  i = (  X  i , X  2 i ) , defined by,  X  {  X  i | i  X  I n } , where the set of time instants  X  = {  X  with  X  i = t mid ( B i ) = 1 2 (( i.m + 1) + ( i + 1) m ) . By definition of temporal data this summarization set also forms a temporal dataset. This temporal summarization now represents our equal frequency bins.
 Now, we model the temporal summarization dataset as a Markov process. Since there is a temporal dependency between the successive bins, we consider the summarized bins as the states of a Markov model.
 We next define a first order Markov model in the context of our approach which also explains why we can use a Markov model.

Definition 3 (Markov model): Given temporal summa-rization set  X  temporal = {  X  i | i  X  I n } and corresponding times  X  = {  X  i | i  X  I n } , we can model a stochastic process Y (  X  ) ,  X  =  X  1 .. X  n , where Y (  X  ) is a random variable with values Y (  X  i ) = Y i =  X  i = (  X  i , X  2 i ) ,  X  i, i = 1 ...n and as-sume the Markov property P ( Y i = y i | Y i  X  1 = y i  X  1 y Hence, the stochastic process Y (  X  ) becomes a Markov process.
 We define a first order Markov chain in which the condi-tional probability of any state S i given all the previous states S i  X  1 ,...,S 1 is only dependent on the previous state S i  X  1 A Markov chain can be completely defined by the initial distribution and transition probability matrix. However, first we introduce the concept of similarity measured in terms of distance, which will be later used for these definitions. We compute the similarity between the intervals using any given distance measure d . We use different distance measures to test the efficacy of our approach for various distance measures. Specifically we use (a) Bhattacharya [2],(b)Kullback-Leibler or differential entropy [16], (c) Ma-halanobis [19], (d) Hellinger [22]. 1) Distance Measures: The distance measures compute the distance between two probability distributions (of two bins), using the mean and variance of the distributions. For normalized temporal data Gaussian distribution has been assumed in existing approaches [17], we define the distance measures formally (under Gaussian assumption) as follows: Given two normal (Gaussian) distributions  X  0 k (  X  and  X  1 k (  X  1 ,  X  1 ) , with covariance matrices  X  respectively, we formally define the various distance mea-sures between these two distributions (from  X  0 k (  X  0 ,  X   X  k (  X  1 ,  X  1 ) ). Here we discuss a one dimensional special case, however, our approach generalizable to multidimen-sional temporal data.
 Definition 4 (KL Divergence Measure): The Kullback-Leibler divergence (differential entropy) = D for one dimension, for any two bins B i and B j (such that B i  X   X  (  X  i ,  X  2 i ) , B j  X   X  (  X  j ,  X  the KL divergence reduces to the following,
Definition 5 (Mahalanobis Distance Measure): The Ma-halanobis Distance Measure = D Mahalanobis (  X  0 duces to, d ( B i ,B j ) = (  X  i  X   X  j ) 2 2  X  2 Definition 6 (Bhattacharyya Distance Measure): The Bhattacharyya Distance Measure = D Bhattacharya (  X  Definition 7 (Hellinger Distance Measure): Hellinger Distance between two bins B i and B j (such that B i  X   X  (  X  i ,  X  2 i ) , B j  X  r 2) Computing Transition Probabilities: In the previous section we have demarcated equal frequency bins and con-sidered them as the states of the Markov model. To have the complete definition of the Markov model we consider the similarities between the states to generate a transition probability matrix. We first define the initial distribution: Definition 8 (Initial Distribution): Given set of states S = { S 1 ,...,S n } , we assume all the states are equally likely, hence we start with the 1  X  n vector [ 1 n ,..., our initial distribution.

We will define normalized transition probability shortly, but before that let us formally define the concept of temporal window (or lag) first, Definition 9 (Temporal window): Given a set of bins B temporal = { B 1 ,...,B n } , a temporal window of size w for any starting bin B i is defined as a set of bins W ( B i ) = { B k : | k  X  i |  X  b w 2 c} , when i  X  X  w 2 i + b w 2 c X  n ,  X  i .
 The temporal windows, within the transition probability matrix, for the bins are shown in figure 2. In this figure the greyed out areas are non zero transition values. The extreme left and extreme right windows are defined by suitably shifting the windows towards right or left respectively, thereby making its size w . We next define the transition probability between any two states on our Markov model:
Definition 10 (Transition Probability): Given a set of states S = { S 1 ,...,S n } , transition probability P ( S p ij = 0 .
 Since the distance measures have the property that the distance between any two bins B i ,B j , decreases with the increase in similarity between the bins, but the transition probability from one bin to another one has the property that the probability must increase with increase in similarity between the bins. A decreasing function is used to map the computed distance measures to probability space. Various decreasing functions can be used such as 1 d ( i,j ) best result, hence we use this as our decreasing function. The transition probability of each bin to every other bin is thus captured in the Markov transition probability ma-trix T which is an n  X  n matrix. It is easy to see that lim thought of as a probability measure.
 The Markov transition probability matrix must be at least row (or column) stochastic, that is the entries in the row or column must sum to 1. Therefore, we need to row normalize the matrix, since we want to convert the transition matrix to at least a row-stochastic one, if not a doubly stochastic matrix [26].

Definition 11 (normalized Transition probability): Given a set of states S = { S 1 ,...,S n } , normalized transition probability p ij = p ij X
We can see from figure 2 that during transition matrix computation we need to consider only these w windows for each bin, thereby reducing the complexity of computation. We summarize the above process in figure 3 using an example. It shows the Markov modeling for initial number of bins = 4 and w = 4 , where B 1 ...B 4 are the initial (equal-frequency) bins, the dotted lines show the bin de-marcations. The temporal summarization (  X , X  2 ) from each bin is obtained. The Markov model is defined based on the summarized bins. Figure 3 (a) shows the state transition diagram of the Markov model, while figure 3 (b) and (c) show the initial distribution and transition probability matrix for the Markov model, which are computed from the distance measures using formula figure 3 (d).
 We outline this process of generating the Markov model in algorithm 1. In algorithm 1 on line 7 we compute the transition probability between any two states i, j which is then normalized on line 8, in order to obtain a row-stochastic matrix. It is important to note that this normalization how-ever breaks the symmetry of the matrix. In the following merging algorithm we address this aspect.

Here d can be any one of the various distance measures discussed above and d ( i,j ) = d ( S i ,S j ) = d ( X  d ( B i ,B j ) . Finally on line 9 we take the average of p p ji to compute the probability of transition between bins B and B j . Also note on line 3, n min is an optional parameter to be provided by the user to avoid overmerging. The algorithm 1 has time complexity O ( N + nw ) (lines 1-6 with complexity O ( N ) and lines 7  X  8 with complexity O ( nw ) ), where N is data size and n is the number of bins and w is the window size.
 Algorithm 1 InitBin: Setup Markov Model B. S imilarity based merg ing (SMerg) Our aim is to identify unequal depth bins. In this approach we iteratively merge highly similar bins. At every iteration the transition matrix is recomputed since bin population has changed. The steps of this approach are outlined in the algorithm 2. Re-computation of transition probabilities (line 7) is necessary for the in-coming edges to any of the two states (bins) merged, p k i and p k j and also for the out-going edges from any of the two states merged, p i k p j k ,  X  k = 1  X  X  X  n . The algorithm takes as an input the series of temporal observations t , n : Initial number of bins, n min : Minimum number of bins to be output where m defaults to 2 , w : Lag or window size where w defaults to r and k : Threshold factor, so that threshold for bin merging =  X  = k n  X  1 , k defaults to 1 . We iteratively do the following two steps until no (two) mergeable bins are left. On lines 3 and 10, we find a pair of bins having maximum probability of transition between them, which implies the highest degree of similarity and mergeability. The iterative merging from line 4-10 takes care of the transitive closure property. On line 5 based on this we merge these two bins and on line 6 we compute the statistics of this newly merged bin (mean and variance both being algebraic measure), combining the individual bins statistics. | B i | and | B j | represent the sizes of B i and B j respectively. We use threshold as terminating condition of the iterative algorithm. This threshold has to be carefully selected.
 Algorithm 2 SMerg: Generation of Unequal depth Bins using similarity based merging Threshold is computed for each iteration using the formula n  X  1 , where k is a constant, typically 1 , in which case threshold becomes equal to: the probability of transition from a bin to any other bin, if all the transitions are equally likely. In our experimental results we discuss results with different values of threshold, with k starting from 0 . 1 to 2 . 0 and the results are compared.
 The worst-case complexity of algorithm 2 is O ( N + n 2 w ) , when all the bins are very similar in nature. Line 1 has complexity of O ( N + nw ) and line 7 is O ( nw ) , line 3 and line 10 has complexity O ( n ) , all other lines are constant time operations, except the while loop on line 4-11 that is ( n  X  n final ) = O ( n ) in the worst case. The window size used for this algorithm is typically a constant (typically w  X  20 ), hence the complexity is O ( N + n 2 ) , where N is data size and n is the number of bins and w is the window size. C. Markov St ationary distribution based Merg ing (StMerg)
Given the initial equal-depth bins (figure 4 (a)) and the transition matrix T , we compute the stationary distribution vector, using power-iteration method on lines 3-9. This essentially takes a self product of the transition matrix to generate the next level transition matrix. Our aim is to get to a convergence point where every row of the matrix is approximately the same producing a stationary distribution Algorithm 3 StMerg: Generation of Unequal depth Bins using Markov Stationary Distribution (figure 4 (b)) which is formally defined as follows which is adapted from [8], [21] in the context of our approach:
Definition 12 (Stationary Distribution): Given a finite, ir-reducible and aperiodic Markov chain with transition prob-ability matrix T , then T k converges to a rank-one matrix in which each row is called the stationary distribution  X  , that is, lim  X  . This is stated by the Perron-Frobenius theorem. [9] For this convergence we consider the difference in the rows of the matrix such that the error term error defining this difference is minimized. However this can be very expensive and the convergence may not always be achieved in a small number of steps. In this case we consider another option which is to take an average of the rows of the matrix and use this as the stationary distribution vector. This is outlined on lines 10-14 in algorithm 3. Once we have this stationary distribution vector we need to detect the spikes in the vector which indicate the split points such that the data on either side of any split point is very dissimilar and the data within two split points is highly similar. This is possible since the stationary distribution provides a transitive closure such that it takes care of all possible similarities for every bin. Thus the split points indeed capture a truly similar behavior. In order to detect these spikes in the stationary distribution vector we use Discrete Fourier Transform (DFT) and Inverse Discrete Fourier Transform (IDFT) [4] with higher h coef-ficients as High Pass Filter (HPF) (figure 4 (c)). For this the stationary distribution vector  X  is first transformed from time domain to frequency domain, using DFT. Then the spatial domain vector  X  is transformed back to time domain, using IDFT, but this time use only higher h coefficients are taken ( figure 4, (e), (f)). After application of HPF, on lines 17-21 the split points are found -these are the (spike) points (with sharp changes in temporal bin distribution) where there are changes of sign (from + ive to  X  ive or vice-versa). We then merge all successive equal frequency bins between any two split points to form unequal frequency bins (as shown in figure 4 (e)) resulting in the final bins (figure 4 (d)). The worst-case complexity of this algorithm is O ( N + n 3 ) . This is due to the fact that matrix multiplication during power-iteration method is O ( N + n 3 ) (if Strassen X  X  algorithm [21] is used, matrix multiplication complexity can be improved to O ( N + n 2 . 807 ) ). Here N is data size and n is the number of bins. Here typically w = n .
 D. Temporal Neighborhoods
The goal of the SMerg and StMerg algorithm is to generate temporal neighborhoods using unequal depth dis-cretization. Essentially the merged bins discovered through the algorithms correspond to our temporal neighborhoods such that any mining task can be focused to these bins which have high intra neighborhood(or bin) similarity and high inter neighborhood (or bin) dissimilarity. We define our temporal neighborhood based on the merged bins as follows: Definition 13 (Temporal Neighborhood): Given a set of equal depth temporal bins B temporal = { B 1 ,B 2 ,...B n } , Temporal Neighborhood  X  whenever i 6 = j such that inter neighborhood dissimilarity is maximized and intra neighborhood similarity is maximized. We discuss detailed experimental results of our approach. We use synthetic and real world datasets for these results as described below: Synthetic data : we used (a) Uniform : randomly generated by C function rand() with different seeds and , (b) Gaussian: randomly generated using Box-Muller transformation [3]. The data ranged from 1000 to 20,000,000 temporal data points. This data was generated by mixing 3 to 6 different probability distributions leading to distinct bins, specifically Synthetic (1) has 5 distributions, Synthetic (2) has 6 distributions and Synthetic (3) has 3 distributions.
 CATT Lab [1] data : is recorded by traffic sensors measur-ing attributes such as vehicle counts or speed. We considered 500 to 20,000 temporal data points ranging from measure-ments throughout a day, a week, 2 weeks, months and several months. This data is captured at every 5 minute intervals. We specifically analyzed the data from various individual sensors located along highways in Maryland specifically (a) US  X  50 West @ Church Rd West and US  X  50 East @ Church Rd East, (b) I  X  395 near Seminary Rd.
 Metrics for evaluation: For the synthetic data: the results are discussed in terms of the efficacy of discovering known bins in the synthetic data, measured using recall and precision. The known bins are labeled based on the mix of distributions used to generate the synthetic data. The precision and recall of the expected bins is depicted in each of the figures and is discussed below. We define the metrics we use to measure the accuracy of the methods: This measure reflects the numbers of bins accurately merged, hence it is very crucial for the algorithms to have this metric value as close to 1 as possible. This measure additionally takes care of over-splitting, i.e., reflects the numbers of bins expected to be merged but actually not merged (due to threshold etc.). This metric value should also be as close to 1 as possible. For the correctness of an algorithm precision is more important and for the efficiency recall can be used.

For the CATT lab data: we did not have the labels for the expected bins however we validate the bins we found with real world ground truth for which we used visual analysis and heuristics as will be discussed in the subsequent sections.
 A. Results in the CATT Lab data US-50: In figure 5 and 6 we perform various runs of the two algorithms with various parameters. We discuss some key observations here: (1) In figures 5 and 6 (a, b) we can see that the weekend days are clearly different from the weekdays in 5 and 6 (d, e). Further if we consider a very high level granularity such as in figure 5 and 6 (c) using algorithm 3 with the KL distance we can entirely demarcate the weekends as compared to the weekdays. (2) On the weekend days it was observed in figure 5 and 6 (a, b) that the weekend days are further broken down such that there is a high vehicle count on the roads in the afternoons and the early mornings and evenings have relatively less traffic (time stamps shown in the x axis of the figures). This is the reverse of the weekdays traffic. In fact it was interesting to see that the weekends the traffic is very low in the mornings and picks up during the afternoon and is again low in the evenings. So this is an interesting phenomenon that during weekdays afternoons is the lowest intensity of traffic where as on weekdays afternoon is the highest traffic intensity. (3) For the week days traffic let us consider figure 5 and 6 (e). These are demarcated at a higher level granularity combining the entire days traffic into one bin. However in figure 5 and 6 (d, f) we can see that the day is further broken down with the morning being the most traffic intensity. It is interesting to note that this pattern of high intensity is not repeated in the evening. This is because the traffic is directional therefore the traffic intensity on one sensor in the morning may be mirrored in the sensor on the opposite direction in the pm hours. This is validated by looking at the two East and west sensors in figures 5 and 6 (d). Also, we see that even from the stationary distribution obtained in StMerg algorithm (using KL distance measure) we can find the outlier (circled in the figure 5 and 6 (c)), which is week-end pattern here (that is different from weekdays) and the stationary distribution closely resembles the original trend of temporal data. (4) It can be seen from the figures 5 and 6 (c) that the global outliers can also be detected the stationary distribution vector (marked by a circle), which represents an approximation for the temporal data.
 I-395: In figure 7, temporal interval (a) and (b) obtained after merging the intervals shows that there is an anomaly in the data from 05 / 12 / 2009 23 : 03 : 39 to 05 / 13 / 2009 00 03 39 . We indeed find that speed value recorded by the censor is 20, constant all over the time period, and we verified from CATT lab representative that this is due to a possible  X  X ree flow speed condition X  where the sensor may go into a powersave mode if the traffic intensity is very minimal or close to zero. B. Results in the Synthetic data
The distributions of the synthetic data are known. Thus, we can use the experimental results on these synthetic data as a supervised test using precision and recall, because the ideal output bins are known here. Hence, we verified the results of our methods against them and verified the accuracy of our methods. Also, we verified that the stationary distribution obtained in StMerg can very well approximate the temporal data and can act as a compression method for the temporal data. We next discuss accuracy measures of results varying the following: (a) Number of temporal observations, (b) Initial number of bins, (c) Different Similarity measures, finally we also discuss the variation in the threshold value k for SMerg and h for StMerg .

In each of the above we also discuss which distance metric performed the best. A series of experiments were conducted to obtain the results regarding the inter-dependency of any two of the parameters keeping others fixed.
 Number of temporal observations As it can be seen from figures 8 and 9 both SMerg and StMerg are scalable in terms of large data size upto 20 million observations, almost always giving precision and recall values more than 90 percent and close to 100 percent. However it was also seen that algorithm 2 in figure 8 is more scalable, since it works with equal accuracy (100 percent) for all the different similarity measures used. Algorithm 3 works most accurately for KL distance measure as shown in figure 9, as the most scalable and outperforms the other measures with increasing data size, in terms of precision and recall values.
 Initial number of bins From figure 10 and 11 we can see that, the algorithm SMerg is more scalable in terms of initial number of bins. For SMerg and StMerg KL distance measure gives the most accurate result (with precision and recall more than 90 percent most of the times).
 Discussion of results It should be noted that the algorithm performance depends upon the initial number of bins. How-ever this can be mitigated by striking a balance between the initial number of bins and the threshold value. The merging of the bins is controlled by the threshold value. So say we start with an extremely large number of initial bins (say N/ 2 ). Then keeping a low threshold will lead to more merging which will result in a more accurate outcome even though initial number of bins is high. Essentially if initial number of bins is large then the final bins produced will grab more delicate changes in the data with respect to time, if it is small it will grab the less delicate changes. Similarly if threshold is small the final bins produced will grab more delicate changes in the data with respect to time and vice versa. For time series data that has huge change in values in a short period of time, we choose higher initial number of bins and smaller threshold, thereby increasing the complexity of the algorithms. Hence, there is a balance between initial number of bins and threshold value -typically initial number of bins in the range of 100  X  200 (or in general 1-10% of N) and threshold = 1 n  X  1 gives good results for CATT Lab data even of size several thousands. It is interesting to note that the StMerg algorithm can be used to approximate the temporal data, using stationary Markov distribution.

In general it was seen that both SMerg and StMerg perform well with the KL distance metric. However SMerg performs slightly better that StMerg . The performance of StMerg can possibly be improved by using a better HPF for finding the split points. However in general both algorithms have a high recall and precision.
 The techniques described in this paper to divide the temporal data into unequal depth bins to form temporal neighborhoods are not only novel in the sense that they are binning techniques based on similarity, but they also provide useful dimensionality reduction/summarization techniques for the temporal data which are typically very large. We provided both theoretical and experimental validation of the tech-niques proposed, for synthetically generated data as well as real-world data. Future work will extend the methods described to multi-attribute temporal data. We also intend to improve further the performance of StMerg by using a better HPF for finding the split points. We would also like to compare our approach to existing temporal binning and segmentation approaches. Lastly we would like to extend this to spatio-temporal Markov models for spatial and spatio-temporal neighborhood discovery.

