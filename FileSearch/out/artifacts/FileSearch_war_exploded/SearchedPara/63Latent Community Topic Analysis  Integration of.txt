 Topic modeling is a classic text mining task which is to discover the hidden topics that occur in a document collection. Topic models, such as PLSA [Hofmann 1999], LDA [Blei et al. 2003] and their variants [Blei 2011], use a multinomial word distribution to rep-resent a semantic coherent topic and model the generation of the text collection with a mixture of such topics. With more and more text content online, it is difficult for us to read all the documents and digest all the information. Topic modeling provides an effective approach to help understand these huge amounts of information. The discov-ered topics are also useful to organize and search the content.

With the development of social media, a lot of user-generated content is available with user networks. Users communicate and interact with each other in social media sites. Besides the links among users, users generate a lot of text content as well. Along with rich information in networks, user graphs can be extended with text information on nodes. In social networking sites, users maintain profile pages, write comments and share articles. In photo and video sharing sites, users use short text to tag photos and videos. In microblogging sites, users post their status updates. We consider a graph with text on nodes as a text-associated graph.

To discover the community-based latent topics in text-associated graphs, we are interested in the following three tasks. First, we would like to discover the commu-nity structure in the graph, so we can know the relationships among different users. Identified communities not only can provide summarization of network structure and help understand the graphs, but also are important to analyze user behaviors in the setting of social networks. Second, we would like to discover the latent topics in text-associated graphs. In this way we can know the interests of the users in the graph. Third, we would like to learn the relationship between communities and topics, so we can know which communities are interested in a specific topic or which topics a specific community cares about.

In this article we incorporate community discovery into topic analysis and pro-pose a community-based topic analysis framework called LCTA (Latent Community Topic Analysis). With the development of social networks, discovering communities in graphs draws much more attention than before [Parthasarathy et al. 2011]. A commu-nity in a network is considered as a group of nodes with more interactions and com-mon topics among its members than between its members and others, and community discovery is the process to group the nodes into the clusters of close interaction and common interests. To discover communities in graphs, typically an objective function is chosen to capture the intuition of a community as a set of nodes with better internal connectivity than external connectivity based on link structure [Leskovec et al. 2010]. However, if we only use link to discover communities, we cannot capture the coher-ence of common interests inside communities. A good community should be coherent in interaction patterns as well as shared topics. Most of previous studies overlook the connection between interactions and interests, and hence might have difficulties in finding the most appropriate communities. To discover the community-based topics in text-associated graphs, we follow the previous text mining studies [Blei et al. 2003; Hofmann 1999; Liu et al. 2009; McCallum et al. 2005] by using topics to model text corpus. Our work is different from the previous work in our assumption that topic and community are different concepts. Instead of modeling topics by considering pairwise link relationships, we consider topic modeling in the community level. We assume that one community can correspond to multiple topics and multiple communities can share the same topic. For example, in a network one community can be interested in both politics and entertainment topics, while multiple communities can be interested in a politics topic. The analysis of topics and communities could benefit each other. In our model, users are likely to form a link to another user from the same community and users in the same community usually share coherent interests as topics. Topics are generated from communities in our method, so it captures the topical coherence in the community level. As we will see in the experimental results, the interaction of commu-nities and topics provides flexibility in both community discovery and topic modeling process.

The contributions of the article are summarized as follows.  X  We introduce the problem of latent community topic analysis.  X  We propose a model called LCTA to incorporate community discovery into topic modeling.  X  We perform extensive experiments on two real datasets to demonstrate the effectiveness of our LCTA method.

The rest of the article is organized as follows. We introduce the problem of latent community topic analysis in Section 2. We propose our method LCTA in Section 3. We compare different methods and show the performance in Section 4. We summarize the related work in Section 5 and conclude the article in Section 6. In this section, we introduce the problem of latent community topic analysis and define the related concepts. The notations used in this article are listed in Table I. Definition 2.1. A text-associated graph is a graph with text information on nodes. Formally, G ( U , E ) is a graph that contains users and edges, where U is the user set in G and E is the edge set in G . u is a user in U that consists of both text and links. w u is the text part of user u and l u is the link part of user u .

Definition 2.2. A community is a group of users in the graph with more interactions and common interests within the group than between groups. We denote C as the community set and c is a community in C .

The conditional probability of a community given a user represents the participation level of the user in the community. Formally, p ( c | u ) is the probability of community c given user u ,s.t. c  X  C p ( c | u ) = 1. We denote  X  as the community distribution set for community user u is most likely to belong.

Definition 2.3. A topic is a semantically coherent theme, which is represented by a multinomial distribution of words. Formally, each topic z is represented by a word as the word distribution set for Z ,thatis, {  X  z } z  X  Z .

The conditional probability of a topic given a community represents the relationship between the topic and the community. Formally, p ( z | c ) is the probability of topic z given user c ,s.t. z  X  Z p ( z | c ) = 1. We denote  X  as the topic distribution set for C ,that mostly interested in.
 To help understand these definitions, we give two examples below.

Example 2.4. In DBLP 1 (a digital computer science bibliographic graph), authors are considered as users, the paper titles of the authors are the text of users and the coauthorship relationship forms the links between users. In this text-associated graph, communities are the groups of authors that have close collaboration and common research interests with each other, and topics can be different research areas in computer science domains.

Example 2.5. In Twitter 2 (a microblogging site), users can post text of up to 140 characters on their profile pages. The published tweets are the text of users and the follower relationship forms the links of users. In this text-associated graph, commu-nities are the groups of users that have similar follower patterns and common dis-cussed topics with each other, and topics can be the popular themes in the social community.

Given the definitions of text-associated graph, community and topic, we define the problem of latent community topic analysis as follows.

Definition 2.6. Latent community topic analysis is the process to group the nodes in a graph into different communities and discover the topics that are coherent in com-munities. Formally, given a text-associated graph G ( U , E ), the number of communities N and the number of topics K , we would like to know the following results in latent community topic analysis.  X  The community distribution set  X  for user set U ,thatis, {  X  u } u  X  U where  X  u is the community distribution for user u ,thatis, { p ( c | u ) } c  X  C where | C | = N . Based on  X  we can assign users to the most likely communities that they belong to.  X  The word distribution set  X  for topic set Z ,thatis, {  X  z } z  X  Z where | Z | = K and  X  z is the word distribution for topic z ,thatis, { p ( w | z ) } w  X  V . Based on  X  we can know the the discussed topics in the text-associated graph.  X  The topic distribution set  X  for community set C ,thatis, {  X  c } c  X  C where  X  c is the topic distribution for community c ,thatis, { p ( z | c ) } z  X  Z . Based on  X  , we can know the relationship between topics and communities, that is, which topics are related to a specific community. In this section we introduce our framework of latent community topic analysis. First, we propose a model called LCTA. Second, we introduce how to estimate the parameters in the model. Third, we analyze the complexity of our algorithm. In our LCTA (Latent Community Topic Analysis) model, we would like to discover both communities and topics in a text-associated graph. The network structure provides information of how popular a node is and how it is connected to its neighbor nodes. When a group of nodes are closely connected together and share common interests, the group can be considered as a community. Another important information existing in a text-associated graph is topic. If we explore the semantics of the text in the graph, we can find meaningful topics shared by different users.

In LCTA we use the following characteristics in text-associated graphs.  X  Topic and community are different concepts. In practice, a community 2can incor-porate multiple topics, while multiple communities can share the same topic. For example, in DBLP (a digital computer science bibliographic graph), one community can be interested in both  X  X atabase X  and  X  X ata mining X  topics, while multiple com-munities can be interested in  X  X nformation retrieval X  topic.  X  Good community structure is useful for modeling topics. The users in the same community are closely connected to each other and share common topics. Topics are related to community structure instead of individual nodes. Therefore, we can guar-antee the topic coherence in the community level. For example, in DBLP, if several authors often collaborate together and form a community, we can assume that they have some common research interests. The discovered community structure can be used for generating more meaningful topics.  X  Meaningful topics can help discover communities. The users form a community not only because they are linked to each other but also because they share common top-ics. Instead of single terms, topics are used as the latent concepts in the text, which can represent different aspects more comprehensively. Meaningful topics can guide the discovery of community structure. For example, if two authors are both inter-ested in  X  X ata mining X  topic, they are more likely to belong to the same community.
Besides, the link graph may not be complete sometimes, so the topics can provide additional information for community discovery.

Based on these characteristics in text-associated graphs, we would like to integrate community discovery and topic modeling in our LCTA model. It is not difficult to see that the analysis of communities and topics can mutually enhance each other. A good community needs to be coherent in both links and topics. A user is more likely to form a link with another user within the same community and the users in the same commu-nity share the common topics. We believe that integrating both community structure and text topics will lead to a better description of communities and hence more accu-rate analysis. To model topics, we consider the topical coherence in the community level beyond the constraints on the pairwise relationship. In traditional topic model-ing methods topics are from documents and the relationship between terms and docu-ments are predetermined. In our model topics are from communities and we explore the relationship between terms and communities, so communities in our method can be considered as pseudo-documents. The communities in our model are not predefined and they are discovered along with the topic modeling process. Through the mutual enhancement between community discovery and topic modeling, we can discover the communities that are coherent in both link and topical structure and identify the top-ics that are coherent in the community level. The generative process to generate a text-associated graph is as follows.
For each user u in a text-associated graph G : (1) To generate each word for user u : (2) To generate each link for user u :
In order to generate a user u in graph G , we need to generate both the text w u and the links l u of user u . To generate each word in w u , we first sample a community c from multinomial  X  u .  X  c is the topic distribution for community c . Since the users in the same community are likely to have the same topics, we sample a topic z from  X  . Lastly we sample a word w from multinomial  X  z . To generate each link in l u ,we first sample a community c from multinomial  X  u .  X  is the user distribution set for user participation in community c . Since a user is more likely to link to another user from the same community, we sample a user v from multinomial  X  c and form a link between user u and v .
 represents the links of user u , the log-likelihood of the collection is as follows.
We assume that the words in each community are generated from a mixture of a background model and the community-based topic models. The purpose of using a background model is to make the topics concentrated more on discriminative words, which leads to more informative models [Zhai et al. 2004].  X  common words from the topics. In this article the mixing weight  X  B is set as 0.9 following the empirical studies [Mei et al. 2006; Zhai et al. 2004]. p ( w | B ) is the back-ground model, which we set as follows. where n ( u ,w ) is the frequency of word w with regard to user u . In order to estimate parameters  X ,  X ,  X ,  X  in log-likelihood, we use maximum likeli-hood estimation. In particular, we use Expectation Maximization (EM) algorithm to estimate parameters, which iteratively computes a local maximum of likelihood. In the EM algorithm, we introduce the probabilities of the hidden variables, that is, longing to community c and topic z and p ( c | u ,v ) is the probability of linked user v in terms of user u belonging to community c . In the E-step, it computes the expectation of the complete likelihood. In the M-step, it finds the estimation of the parameters that maximizes the expectation of the complete likelihood.

In the E-step , p ( c , z | u ,w )and p ( c | u ,v ) are updated according to Bayes formulas.
In the M-step , we update the parameters as follows, where n ( u ,w ) is the frequency of word w with regard to user u and n ( u ,v ) is the weight of the link from user u to v .
We update topic-related parameters p ( z | c )and p ( w | z ) in Equations (7) and (8) and update community-related parameter p ( v | c ) in Equation (9). In Equation (10), the community distribution of a user p ( c | u ) is updated according to the information from both topics and links. The EM steps can be considered as the mutual enhancement between community discovery and topic modeling. In our model, topics are generated from communities, so a good community grouping can help extract meaningful topics. On the other hand, since the communities are coherent in topics, a good topic modeling can improve community discovery process. We analyze the complexity of parameter estimation process in this section. In the E-where K is the number of topics, N is the number of communities and | W | is the number of words in all the users. To calculate p ( c | u ,v ) in Equation (6) for all ( u ,v ) pairs, it needs O ( N | E | ) where | E | is the number of edges in the graph. In the M-step, it needs O ( KN | W | ) to update p ( z | c ) in Equation (7) for all the communities and to Equation (9) for all the communities. To get updated p ( v | c ) in Equation (9), it needs p ( |
U | is the number of users. In total, the space complexity is O ( KN | W | + N | E | ). EM algorithm can be parallelized with MapReduce [Lin and Dyer 2010], so our algorithm is scalable to large-scaled datasets. In this section, we demonstrate the evaluation results of our method. First, we intro-duce the datasets used in the experiment. Second, we demonstrate the discovered topics and the corresponding communities by our LCTA (Latent Community Topic Analysis) model. Third, we compare our method with other community discovery meth-ods. Fourth, we compare our method with other topic modeling methods. Lastly we study the effect of parameter changes on the results. We evaluate the proposed method on two datasets; DBLP and Twitter.  X  DBLP . Digital Bibliography Project (DBLP) is a computer science bibliography. We collected the authors in four categories including data mining, databases, machine learning and information retrieval according to the labeling in Gao et al. [2009]. In this data set, authors are considered as users, the paper titles of the authors are the text of users and the coauthorship relationship forms the links of users. There are 4236 users, 5577 unique terms and 15272 links.  X  Twitter . Twitter is a microblogging site where users can post text of up to 140 char-acters on their profile pages. We collected the tweets related to  X  X bama X  and  X  X ocial media X  published by the users from the celebrity list in Kwak et al. [2010]. In this data set, the tweets are the text of users and the follower relationship forms the links of users. There are 1023 users, 5361 unique terms and 350929 links. In this section, we demonstrate the discovered topics and the corresponding communi-ties by our LCTA (Latent Community Topic Analysis) model in the datasets. 4.2.1. DBLP. In DBLP dataset, we set the number of communities as 20 and the number of topics as 4. The topics are listed in Table II. From the result in Table II, we can see that our method can discover the topics in four different areas. Topic 1 is about information retrieval and its popular words are information , retrieval , web , search , query , document , etc. Topic 2 focuses on the words like learning , classification and reasoning . We can infer that it is about machine learning. Topic 3 is about data mining with its emphasis on mining , clustering , frequent , patterns , etc. Topic 4 is related to database and its popular words contain database , query , system and xml .
In Table III, we show the selected discovered communities related to four different topics and their user distributions inside the communities. As we can see from the result in Table III, one topic can correspond to multiple communities and the authors in the same community are closely related to each other. Besides, in our model one community can be related to multiple topics. For example, in our experiment one com-munity has the probability 70.21% in data mining topic and 29.79% in database topic, and its users include Wei Wang 0.1159, Jeffrey Xu Yu 0.0966, Hongjun Lu 0.0929, Haixun Wang 0.0676, etc. It is a representative community whose members are inter-ested in both data mining and database. 4.2.2. Twitter. In Twitter dataset, we set the number of communities as 20 and the number of topics as 2. In Table IV, we listed two topics. Topic 1 is about Obama with its focus on health , care , white , house , etc. Topic 2 is about social media. In Twitter, many popular users are entrepreneurs and marketers, so its popular words in Topic 2 contain social , media , marketing and business . Some users are technology lovers and some specialize in development and search engine optimization, so the words like ways and tips are also popular.

We show several selected communities and their user distributions in Table V. In the communities related to topic Obama, Community 1 is about news media. For ex-ample, NPR Politics is political coverage and conversation from NPR News. NewsHour is one of the most trusted news programs on TV. David Shuster is a journalist for NBC News and MSNBC. Karl Rove is the former deputy chief of staff to President George W. Bush and is the author of Courage and Consequence . Community 2 is a community about conservative politics. For example, MichaelPatrick Leahy is the author of Rules for Conservative Radicals . ChadTEverson is a conservative activist. Nansen Malin is a student who does conservative politics. Most of the users in the communities related to social media are entrepreneurs, strategists, authors, speakers, business coaches, etc. For example, Zee M Kane is the editor-in-chief of The Next Web. Robert Clay is an entrepreneur and business mentor to aspiring market leaders. Jonathan Nafarrete is a social media strategist. Our LCTA (Latent Community Topic Analysis) model is closely related to community discovery. LCTA can handle community discovery and topic modeling simultaneously. Specifically, in LCTA topics are generated from different communities. In this way we guarantee topical coherence in the community level. It is interesting to compare the performance of our model with community discovery methods.

We compare the following methods in this section.  X  NormCut [Shi and Malik 2000]: Normalized cut algorithm on a link graph  X  SSNLDA [Zhang et al. 2007a]: An LDA-based hierarchical Bayesian algorithm on a link graph where communities are modeled as latent variables and defined as distributions over user space  X  LCTA: Our Latent Community Topic Analysis model that integrates community discovery with topic modeling
To compare the discovered topics from LCTA with the ones based on NormCut and SSNLDA, we first use NormCut and SSNLDA to cluster the link graph into 20 communities in both datasets and pool the text of the users in the same community together. We consider the text in each community as a document and run topic model-ing method PLSA [Hofmann 1999] on the collection. We set the number of topics as 4. NormCut+PLSA and SSNLDA+PLSA can be considered as the approaches to discover topics based on clustered communities. The topics discovered by NormCut+PLSA and SSNLDA+PLSA in DBLP dataset are listed in Table VI and Table VII. We can see the topics discovered by NormCut+PLSA and SSNLDA+PLSA are not meaningful and the result by LCTA in Table II is much better. The topics discovered by NormCut+PLSA and SSNLDA+PLSA in Twitter dataset are listed in Table VIII. We can see that com-pared with the result by LCTA in Table IV, the topics discovered by these two ap-proaches are not pure enough. In Table VIII, the topic related to Obama contains the terms like social media , while the topic related to social media also contains the term obama . Therefore we can see that our LCTA model considering community discov-ery and topic modeling in a unified framework performs better than those approaches processing community discovery and topic modeling separately.

Besides the qualitative evaluation, we also quantitatively evaluate the topical co-herence. In DBLP dataset, each user is categorized into one domain of data mining, databases, machine learning and information retrieval according to the labeling in Gao et al. [2009]. Therefore, accuracy and normalized mutual information (NMI) [Cai et al. 2008] can be used to measure the clustering performance and topical coherence in DBLP dataset.
 tained from the given methods, accuracy is defined as follows.
 where | U | is the number of all the users and  X  ( x , y ) is the delta function that is one if x = y and is zero otherwise, and map ( r u ) is the permutation mapping function that maps the label r u of user u to the corresponding label in the dataset. The best mapping between the labels can be found by Kuhn-Munkres algorithm [Kuhn 1955].
 dataset and C as the ones obtained from the given methods. The mutual information metric MI ( C , C )isdefinedasfollows.
 where p ( c ) is the probability that a user arbitrarily selected from the dataset has label c ,and p ( c , c ) is the joint probability that the arbitrarily selected document has label c and is assigned with label c . The normalized mutual information NMI is defined as follows.
 where H ( C ) is the entropy of C . Specifically, NMI =1if C and C are identical, and NMI =0if C and C are independent.

We show the result of accuracy and normalized mutual information in DBLP dataset in Table IX. From the table, we can see that LCTA performs the best among all the methods. The labels of the users in DBLP dataset mainly consider the coherence of topics. Since NormCut and SSNLDA do not consider the text information, both of them perform poor in the dataset. When the number of communities is large such as 15 and 20, NormCut performs relatively well in accuracy measure. The reason is that in the result of NormCut there is a small number of very large communities while in the result of other methods the clusters are of similar sizes. There are only four types of labels in DBLP dataset. If the result is dominant by one big cluster, the clus-ter can be mapped to one of the four labels, so the result will have a relatively large accuracy value. Therefore, accuracy may not be a good measure to compare the per-formance especially when the number of communities is large, so we use normalized mutual information as additional evaluation measure. In normalized mutual infor-mation both NormCut and SSNLDA perform poorly. Therefore we can see that our LCTA model performs better than those approaches processing community discovery and topic modeling separately. In this section we compare our LCTA (Latent Community Topic Analysis) model with other topic modeling methods.

We compare the following methods in this section.  X  PLSA [Hofmann 1999]: Probabilistic latent semantic analysis  X  NetPLSA [Mei et al. 2008]: PLSA regularized with a harmonic regularizer based on a link graph structure  X  LinkLDA [Erosheva et al. 2004]: A generative model of both text and links where words and links are generated according to the same latent topic space  X  LCTA: Our Latent Community Topic Analysis model that integrates community discovery with topic modeling
The latent topics discovered by PLSA, NetPLSA and LinkLDA can also be used to calculate the clusters of users. Similarly accuracy and normalized mutual information (NMI) can be used to measure the clustering performance and topical coherence in DBLP dataset for comparing different methods.

The comparison result of both accuracy and normalized mutual information is listed in Table X. From the table we can see that our LCTA model performs the best among all the methods. Compared with PLSA, NetPLSA considers the link graph structure to regularize the topic modeling process, so it has a better performance than PLSA. Our LCTA model separates the concepts of topic and community and it performs better than LinkLDA in which both words and links are generated according to the same latent topic space.

Beside evaluating the topical coherence, we also compare the link structure co-herence for different topic modeling methods. From the clusters of users by PLSA, NetPLSA and LinkLDA, we can calculate the normalized cut measure based on the partition of the graphs.
 cut ( A , B ) is the total weight of the edges that have been removed by disconnecting two parts A and B . assoc ( A , U ) is the total connection from nodes in A to all the nodes in the graph.
 We list the result of DBLP dataset in Table XI and the one of Twitter dataset in Table XII. In both datasets, compared with other methods, LCTA performs better, which means that LCTA considers the link information relatively well and can dis-cover more coherent communities. In this section, we study the effect of parameter changes on the result. We have to set two parameters in our model, that is, the number of communities and the number of topics.

To study the effect of the number of communities, we build a subset of DBLP dataset including all the coauthors of Hector Garcia-Molina, Rakesh Agrawal, Christos Faloutsos, and Jiawei Han, which results in a graph of 494 users. We set the number of communities as 4 and 20 separately and compare the discovered communities. In Table XIII, we can see that if we set the number of communities as 4 there are four communities related to these four researchers. We increase the number of communi-ties from 4 to 20 and show the selected communities related to Christos Faloutsos and Jiawei Han in Table XIV. Community 1 of Christos Faloutsos is about his commu-nity at Carnegie Mellon University and Community 2 is about his community along with Yahoo Research and Cornell University. Community 1 of Jiawei Han is about his community at University of Illinois at Urbana and Champaign including his stu-dents Xifeng Yan, Dong Xin, etc. Community 2 is another collaboration community of Jiawei Han. From this example, we can see that if we set the number of communi-ties to a small value, we can have communities of coarse granularity. If we increase the number of communities, coarse communities will break down into the ones of fine granularity.

To study the effect of the number of topics, we vary the number of topics and com-pare the the results. If we set the number of topics as 4, from Table II, we can get the four topics including information retrieval, machine learning, data mining and database. If we increase the number of topics from 4 to 20, we can have the topics of fine granularity. In Table XV, we list several topics related to database and machine learning when the number of topics is 20. The first topic related to database is about relational database and query optimization, and the second is about spatial temporal database. The first topic related to machine learning is about Bayesian networks and kernel methods, and the second is learning in computer vision. In this section we discuss related work to our study including community discovery and topic modeling.
 Diehl 2005], is to divide the network nodes into densely connected subgroups [Clauset et al. 2004; Leskovec et al. 2010; Newman 2004b; Newman and Girvan 2004; Palla et al. 2005], which is an important task in datasets including social networks [Parthasarathy et al. 2011], Web graphs [Flake et al. 2000], biological networks [Girvan and Newman 2002], coauthorship networks [Newman 2004a], etc. Tang and Liu [2010] provided a good overview of community discovery algorithms using net-work structures. Newman and Girvan [2004] proposed an algorithm to remove edges from the network iteratively to split it into communities. The edges removed being identified using betweenness measures and the measures are recalculated after each removal. Palla et al. [2005] analyzed the statistical features of overlapping communi-ties to uncover the modular structure of complex systems. Ruan and Zhang [2007] introduced an efficient spectral algorithm for modularity optimization to discover community structure. Nowicki and Snijders [2001] proposed a statistical approach to a posteriori blockmodeling to partition the vertices of the graph into several latent classes where the probability distribution of the relation between two vertices depends only on the classes to which they belong. Zhang et al. [2007b] proposed an LDA-based hierarchical Bayesian algorithm called SSN-LDA, where communities are modeled as latent variables in the graphical models and defined as distributions over social actor space. Zhang et al. [2007a] used a Gaussian distribution with inverse-Wishart prior to model the arbitrary weights that are associated with the social interaction occur-rences. Leskovec et al. [2010] studied a range of network community detection meth-ods originating from theoretical computer science, scientific computing, and statistical physics in order to compare them and to understand their relative performance and the systematic biases in the clusters they identify. All these studies focus on the link structure of the networks without considering the text information. Long et al. [2007] proposed a probabilistic model for relational clustering under a large number of expo-nential family distributions and they also did not consider the topical coherence in the clustering process.
 models for uncovering the underlying semantic structure of a document collection based on hierarchical Bayesian analysis of the text collection. Topic models, such as PLSA [Hofmann 1999] and LDA [Blei et al. 2003], use a multinomial word dis-tribution to represent a semantic coherent topic and model the generation of the text collection with a mixture of such topics. Some studies extend topic modeling with networks. Mei et al. [2008] introduced a model called NetPLSA that regularizes a statistical topic model with a harmonic regularizer based on a graph structure in the data. Sun et al. [2009] defined a multivariate Markov Random Field for topic distribution random variables for each document to model the dependency relation-ships among documents over the network structure. In these studies the links in the graph are not modeled in a generative process. Zhou et al. [2006] proposed a genera-tive probabilistic model to discover semantic community in social networks, but they used text information only without considering link structure. There are several stud-ies on generative topic models based on text and links including Author-Topic model [Rosen-Zvi et al. 2004; Steyvers et al. 2004], Author-Recipient-Topic model [McCallum et al. 2005, 2007; Pathak et al. 2008], Group-Topic model [Wang et al. 2005], Link-PLSA-LDA [Nallapati and Cohen 2008], Block-LDA [Balasubramanyan and Cohen 2011], Topics-on-Participations model [Zheng et al. 2010, 2011]. Cohn and Hofmann [2000] proposed a joint probabilistic term-citation model where the generation of each link in a document is a multinomial sampling of the document. Following Cohn and Hofmann [2000], Erosheva et al. [2004] used a mixed membership model for words and references but treated the membership scores as random Dirichlet realizations. In Cohn and Hofmann [2000] and Erosheva et al. [2004] both text and links are from the same topic-specific space, so they cannot capture the topical coherence in the com-munity level as our model does. Liu et al. [2009] proposed a model called Topic-Link LDA where the membership of authors is modeled with a mixture model and whether a link exists between two documents follows a binomial distribution parameterized by the similarity between topic mixtures and community mixtures as well as a ran-dom factor. Wang and Blei [2011] combined the merits of collaborative filtering and probabilistic topic modeling whereas in our model we integrate community discovery with topic modeling. Deng et al. [2011] proposed a topic model with biased propaga-tion algorithm to incorporate heterogeneous information network with topic modeling. However, it is not related to community discovery, and the discovered topics are not community-based either. In our model community and topic are different concepts. One community can correspond to multiple topics and multiple communities can share the same topic. The interaction of communities and topics provides flexibility in the community discovery process. We also compare our method with topic modeling meth-ods in our experiment.
 With the development of social media a lot of user-generated content is available with user networks. The user graphs extended with text information on the nodes form text-associated graphs. In this article we study the problem of latent community topic analysis in text-associated graphs and propose a model called LCTA to incorporate community discovery into topic modeling. We handle topic modeling and community discovery in the same framework to guarantee the topical coherence in the communi-ties. We perform extensive experiments on two real datasets and show that our model outperforms other methods.

Our work opens up several interesting future directions. First, the communities in our LCTA model are of the same level but the communities in real world may have hi-erarchical structure. It is interesting to extend our framework to hierarchical commu-nity discovery scenarios by bottom-up or top-down strategy. Second, the user-generated content in social media sites includes not only text data but also other rich informa-tion such as pictures, time, and spatial information. It is interesting to integrate those heterogeneous information together in the framework.

