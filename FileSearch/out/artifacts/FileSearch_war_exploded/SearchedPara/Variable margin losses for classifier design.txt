 function f  X  C f more recently, shown to outperform most other boosting algo rithms in computer vision problems, constructive. It turns out that many pairs ( C  X  Furthermore, while there is a closed form relationship betw een  X  and ( C  X  by the properties of either C  X  loss. We start by showing that, while many pairs ( C  X  the derivative of C  X  and show that it leads to an equally tight connection between the pair ( C  X  trivial analytical tractability. This enables a detailed a nalytical study of how C  X  consider the case where the inverse of f  X  Since this property can be controlled by a single parameter, the latter becomes a margin-tunning tested. P
X ( x ) if it minimizes the risk R ( f ) = E risk E two functions, p ( x ) = f (  X  ( x )) , where  X  ( x ) = P Classifiers are frequently designed to be optimal with respe ct to the zero-one loss where we omit the dependence on x for notational simplicity. The associated conditional ris k is The risk is minimized if
Logistic Regression log(1 + e  X  v ) log  X  Examples of optimal link functions include f  X  = 2  X   X  1 and f  X  = log  X  conditional (zero-one) risk is A loss which is minimized by the BDR is Bayes consistent. A num ber of Bayes consistent alter-loss of logistic regression, and the hinge loss of SVMs. They have the form L is minimized by the link leading to the minimum conditional risk function C  X  link, and minimum risk of some of the most popular classifier d esign methods. Here, the goal is to find the probability estimator  X   X  that maximizes the expected reward where I reward when y =  X  1 . The functions I maximal when  X   X  =  X  , i.e.
 with equality if and only if  X   X  =  X  . The conditions under which this holds are as follows. 2) (8) holds if and only if Hence, starting from any convex J (  X  ) , it is possible to derive I enables the following connection to risk minimization.
 ties hold then the functions I with Under the conditions of the theorem, I (  X ,  X   X  ) =  X  C design [3]. Rather than specifying a loss  X  and minimizing C optimal link f  X  and derive, from (14) with J (  X  ) =  X  C  X  optimal classifier.The only conditions are that C  X  In general, given J (  X  ) =  X  C  X  relates the symmetry conditions, on J (  X  ) and f  X  Then J  X  (  X  ) is invertible and Hence, under the conditions of Theorem 2, the derivative of J (  X  ) has the same symmetry as f  X  Since this symmetry is the only constraint on f  X  Definition 1. Let J (  X  ) be as defined in (8), and C  X  associated with C  X  the risk C loss given by (14), a canonical loss.
 convex, differentiable, and symmetric J (  X  ) =  X  2 p  X  (1  X   X  ) . Since this has derivative discussed in detail in Section 5.
 While canonical risks can be easily designed by specifying ei ther J (  X  ) or f  X  canonical pair ( f  X  , J ) . The following result solves this problem. Theorem 4. Let C condition for the canonical form. For example, logistic reg ression has [ f  X   X  [ f (  X  , C  X  with most optimal links of Table 1. ants have sigmoidal inverse links [ f  X  degrees of freedom of the sigmoid itself are in its behavior w ithin this region. curvature of the loss at the origin,  X  (2) (0) . Since, from (18),  X  (2) (0) = ([ f  X  can be controlled by varying the slope of [ f  X  minimum risk of boosting. We consider a parametric extensio n of this risk, From (16), the canonical optimal link is and it can be shown that is an IS link, i.e. satisfies (19)-(24). Using (18), the corre sponding canonical loss is link is indeed sigmoidal, and that the margin is determined b y a . Since  X  (2) (0; a ) = a increases with decreasing a .
 consider the parametric extension of the minimum risk of log istic regression From (16), This is again a sigmoidal inverse link and, from (18), inverse link for various a . Since  X  (2) (0; a ) = a suggests that the margin parameter a could be cross-validated to achieve best performance. A number of easily reproducible experiments were conducted to study the effect of variable mar-which minimize the canonical logistic and boosting losses, for various margin parameters. Gra-averaging over its five train/test pairs.
 1 loss has a heavier tail and approaches zero more slowly than t he canonical logistic loss. which presents the parameter value of rank 1 for each of the ten datasets. Improved performance 9 margin-tunned classifiers is better than both Logit and AdaB oost for each dataset. outperforming both canonical losses only happens in 2 and 3 datasets, respectively. the associated minimum conditional risk and optimal link fu nctions. This analysis was shown to or LogitBoost.
