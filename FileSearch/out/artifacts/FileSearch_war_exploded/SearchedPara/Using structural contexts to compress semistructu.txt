 1. Introduction
Compression of large document collections not only reduces the amount of disk space occupied by the data, but it also decreases the overall query processing time in text retrieval systems. Improvements in processing times are achieved thanks to the reduced disk transfers necessary to access the text in compressed form. Since times by processor decompression times has become an attractive choice. Moreover, recent research on  X  X  X irect X  X  compressed text searching, that is, searching a compressed text without decompressing it, has led to a win-win situation where the compressed text takes less space and is searched faster than the plain text ( Witten, Moffat, &amp; Bell, 1999; Ziviani, Moura, Navarro, &amp; Baeza-Yates, 2000 ).

Compressed text databases pose some requirements to the eligible compression methods. The most crucial is the need of random access to the text without decompressing it from the beginning. This rules out adaptive compression methods such as Ziv X  X empel or PPM compression. On the other hand, many semistatic methods yield poor compression. In the case of compressing natural language texts, however, it has been shown that an model coupled with a Huffman coder ( Huffman, 1952 ) gives compression ratios those usually obtained with popular adaptive methods. It is even possible to switch to byte-oriented Huffman coding, where each source symbol is coded as a sequence of bytes instead of bits. Although compression ratios raise to 35% (which is still competitive), we have in exchange much faster decoding and searching, which are tial searching and of compressed inverted indexes over the text ( Witten et al., 1999; Ziviani et al., 2000; Navarro, Moura, Neubert, Ziviani, &amp; Baeza-Yates, 2000; Moura, Navarro, Ziviani, &amp; Baeza-Yates, 2000; Brisaboa, Farin  X  a, Navarro, &amp; Esteller, 2003 ).

Although the area of natural language compressed text databases has gone a long way since the end of the to the widespread acceptance of SGML, HTML and XML as the standards for storing, exchanging and pre-senting documents, semistructured text databases are becoming the standard. Some compression techniques to exploit the text structure have been proposed, such as XMill ( Liefke &amp; Suciu, 2000 )and XMLPPM ( Cheney, 2002 ), permit searching but do not take much advantage of the structure.

Our goal in this paper is to consider the text structure in the context of a compressed text database. We aim
Huffman compression. Our essential idea is to use separate semistatic models to compress the text that lies ments (i.e., XML tags) should follow a similar distribution, different from other texts. We call our general approach SCM (for Structural Contexts Model), and call SCMHuff the specific compressor obtained with a word-based byte-oriented Huffman coder.

The idea of using different models depending on the structural context has been anticipated in most of the alternative structure-aware compressors mentioned above. Yet, those usually distinguish among types of
XML tags. For example, XMLPPM has four separate models, namely for elements, attributes, characters, and others. Yet, our idea of using a different model for each different tag name has not been explored before as far as we know. Actually, XMLPPM does use this information up to some extent. When it starts compress-the fake character is  X  X  X orgotten X  X  after a few characters.
Having to store all those different models may or may not pay off. In our example, coding the dates sep-arately is probably a good idea, but coding the subjects separate from the bodies is probably not worth the extra space of storing two models (e.g., two Huffman trees). Hence we also design a technique to merge the ing seems to be a hard combinatorial problem, we design a heuristic to obtain a reasonably good merging from an initially separate set of models, one per tag.

The SCM approach can be coupled with an adaptive model as well. In this case the only goal is to use the contexts to improve compression, as the technique cannot be used for compressed text databases. We imple-ment this idea by using a different PPM model for each different structural element (XML tag name). We call the resulting compressor SCMPPM .

Our experimental results show that SCMHuff compresses 2 X 4% better than other methods (including plain word-based Huffman without structure, the best alternative semistatic method we know of). Therefore our method is the best among those that permit random access to the text. From a pure compression point of view,
SCMPPM compresses 2 X 5% more than any other compressor we tried, including the base PPM and structure-aware compressors like XMill and XMLPPM , soon after the collection exceeds 5 MB. As explained, those space savings translate into I/O time reductions. 2. Related work 2.1. Standard text compression take into account the structure of the documents they compress. Our aim is not to cover the whole area but just to focus on three families of compressors that are relevant for this paper.

Text compression is usually divided into two kinds. Statistical compression is based on estimating source symbol probabilities and assigning them codes according to the probabilities. Dictionary methods consist in ceptually divided into two tasks. Modeling regards the text as a sequence of source symbols and assigns prob-abilities to them, possibly depending on their surrounding symbols. Zero-order modeling assigns probabilities tion of the k symbols preceding them. Coding assigns to each source symbol a sequence of target symbols (its code ), based on the probabilities given by the model. The output of the compressor is the sequence of target symbols given by the coder. Compression is semistatic when a single model is obtained for the whole text before coding starts, so that all the occurrences of the same source symbol (in the same context) are assigned the same code. Adaptive compression interleaves the modeling and coding tasks, so that the model is built and updated as coding progresses. In adaptive compression, each new symbol is encoded using the current model and therefore different occurrences of the same source symbol may be assigned different codes.
Semistatic compression requires two passes over the text, as well as storing the model together with the compressed file. On the other hand, adaptive compression cannot start decompression at arbitrary file posi-tions, because all the previous text must be processed so as to learn the model that permits decompressing the text that follows. 2.1.1. Lempel X  X iv
Lempel X  X iv compression is a dictionary method based on replacing text substrings by previous occurrences &amp; Lempel, 1978 ). A well-known variant of the latter is called LZW ( Welch, 1984 ).

LZ77 maintains a window of the last N processed characters. In each step, it reads the longest possible resumes the scanning just past string sa .

In principle the use of a longer window improves compression because it makes more likely to find longer strings for replacement. However, the representation of position p requires log grows. In practice the most convenient window size is not very long (for example, 64 kB). This considers not only the achievable compression but also the time and space cost of searching the window for strings.
Decompression of LZ77 compressed files is extremely fast and simple. The compressed text is basically a rent output position, and then output a . Well-known representatives of LZ77 compression are Info-ZIP X  X  zip and GNU X  X  gzip .

Other variants, such as LZ78 and LZW, restrict somehow which previous strings can be referenced. This is done for efficiency reasons of different types, for example to improve compression time or to improve the com-pression ratio. A well-known representative of LZW is Unix X  X  compress .

The Lempel X  X iv family is the most popular to compress text because it combines compression ratios around 35% on plain English text with fast compression and decompression. However, Lempel X  X iv com-pressed text cannot be decompressed at random positions, because one must process all the text from the beginning in order to learn the window that is used to decompress the desired portion. 2.1.2. Huffman
Huffman coding ( Huffman, 1952 ) is designed for statistical compression. It assigns a variable-length code to each source symbol, trying to give shorter codes to more probable symbols. Huffman algorithm guarantees that the code assignment minimizes the length of the compressed file under the probabilities given by the model.

A common usage of Huffman coding is to couple it with semistatic zero-order modeling, taking text characters Being semistatic, Huffman compression permits easy decompression of the text starting at any position.
Huffman compression is not very popular on natural language text because it achieves poor compression ratios compared to other techniques. However, the situation changes drastically when one uses the text words , rather than the characters, as the source symbols ( Moffat, 1989 ). The distribution of words is much more skewed than that of symbols, and this permits obtaining much better compression ratios than character-based
Huffman compressors. On English text, character-based Huffman obtains around 60% compression ratio, while word-based Huffman is around 25% ( Ziviani et al., 2000 ). Actually, similar compression ratios can be obtained by using Lempel X  X iv on words ( Bentley, Sleator, Tarjan, &amp; Wei, 1986; Horspool &amp; Cormack, 1992; Dvorsky  X  , Pokorny  X  , &amp; Sna  X  sel, 1999 ).

However, the text in natural language is not only made up of words. There are also punctuation, separator, and other special characters. The sequence of characters between every pair of consecutive words is called a separator . Separators must also be considered to be symbols of the source alphabet. In Moffat (1989) they use the so-called separate alphabets model , where words and separators are modeled separately. As every word is further information is necessary to decode the stream of codes from the two different alphabets.
Word-based Huffman compression has other advantages. Not only the text can be compressed and decom-public domain relying on word-based Huffman is the MG system ( Witten et al., 1999 ). 2.1.3. Kth order models
These models assign a probability to each source symbol as a function of the context of k source symbols and those based on the Burrows X  X heeler transform (BWT). sion, but more memory and time is necessary to compress and decompress.

More precisely, PPM uses k + 1 models, of order 0 to k , in parallel. It usually compresses using the k th order model, unless the character to compress has never been seen in that model. In this cases it switches to a lower-order model until the character is found.

The BWT ( Burrows &amp; Wheeler, 1994 ) is a reversible permutation of the text that puts together characters to k th order compression (for example, by applying move-to-front followed by Huffman or arithmetic coding).
PPM and BWT usually achieve better compression ratios than other families (around 20% on English text), yet they are much slower to compress and decompress, and cannot decompress arbitrary portions of the text collection. Well known representatives of this family are Seward X  X  bzip2 , based on the BWT, and Shkarin/
Cheney X  X  ppmdi ( Shkarin, 2002 ) and Bloom/Tarhio X  X  ppmz , two PPM-based techniques. 2.2. Structured text compression
There exist a few approaches specifically designed to compress structured text, taking advantage of its structure. 2.2.1. XMill ( Liefke &amp; Suciu, 2000 )
Developed at AT&amp;T Labs, XMill is an XML-specific compressor designed to exchange and store XML documents. Its compression approach is not intended for directly supporting querying or updating the com-pressed documents. XMill is based on the zlib library, which combines Lempel X  X iv compression with a variant
Each component is compressed separately. Another Lempel X  X iv based compressor, cutting the structure at some depth and using plain Lempel X  X iv compression for the subtrees, is commercial XMLZip ( http:// www.xmls.com ). 2.2.2. XMLPPM ( Cheney, 2001 )
This is a PPM-like compressor, where the context is given by the path from the root to the tree node that contains the current text. XMLPPM is an adaptive compressor that does not permit random access to indi-vidual documents. The idea is an evolution over XMill , as different compressors are used for each component, and the XML hierarchy information is used to improve compression. 2.2.3. XCQ ( Levene &amp; Wood, 2002 ) and Exalt ( Toman, 2004 )
These are compression methods based on separating structure from data, and then using grammar-based compression for the structure. In XCQ , the tree shape is compressed using the DTD information, while the text is compressed using a standard Lempel X  X iv software such as gzip .In Exalt , both elements are compressed using grammar-based methods. In particular, zero-order prediction depending on the structural context, plus arithmetic coding, is used for the tags. Other grammar-based techniques can be found in ( Tarhio, 2001 ), as well as in XML-Xpress , a commercial software ( http://www.ictcompress.com ) that compresses well when the DTD is known. 2.2.4. XGrind ( Tolani &amp; Haritsa, 2002 )
This compressor is interesting because it directly supports queries over the compressed files. An XML doc-ument compressed with XGrind retains the structure of the original document, permitting reuse of the stan-dard XML techniques for processing the compressed document. Structure tags are represented in numeric form, while the text is compressed using character-oriented Huffman. A similar idea is explored in XMillau ( Girardot &amp; Sundaresan, 2000 ).
 2.2.5. LZCS ( Adiego, Navarro, &amp; de la Fuente, 2004, in press )
This compressor uses an idea similar to LZ77, restricted to replacing whole subtrees. This permits the com-pressed text being accessed, searched, and navigated without decompressing it. Yet, the technique is oriented to highly structured documents (such as e-commerce exchanges, for example) and it does not perform well on general semistructured data. 3. Structural contexts with a semistatic model
Our first contribution is a structure-aware compressor that permits random access and direct searching on source symbols together with their assigned codes.

An encoder based on the separate alphabets model (see Section 2 ) must use two source symbol dictionaries: semistructured documents, but we can extend the mechanism to do better.

In most cases, natural language texts are structured in a semantically meaningful manner. This means that from that of another tag. In our example of the Introduction, where the tags correspond to the fields of an email archive, we can expect that the From: field contains names and email addresses, the Date: field contains dates, and the Subject: and Body: fields contain free text.

In cases where the words under one tag have little intersection with words under another tag, or their dis-to group tags under a single dictionary. 3.1. Compressing the text
We compress the text with word-based Huffman ( Huffman, 1952; Bentley et al., 1986 ). The text is seen as an alternating sequence of words and separators, where a word is a maximal sequence of alphanumeric characters and a separator is a maximal sequence of non-alphanumeric characters.

Besides, we will take into account a special case of words: tags . A tag is a code embedded in the text which represents the structure, format or style of the data. A tag is recognized from surrounding text by the use of delimiter characters. A common delimiter character for an XML or SGML tag are the symbols  X   X   X  and  X   X   X . tags , which mark the end of a container element,  X   X  / ...  X   X .

Tags will be wholly considered (that is, including their delimiter characters) as words, and will be used to determine when to switch dictionaries at compression and decompression time. 3.2. Model description
The structural contexts model (as the separate alphabets model) uses one dictionary to store all the sepa-wise, it must insert either an empty word or an empty separator. There must be at least one word dictionary, If only the default dictionary exists for words then the model is equivalent to the separate alphabets model.
Fig. 1 shows the general SCM scheme, and Algorithm 1 shows the generic pseudocode for a modeling, cod-ing, or decoding pass over the text. For compression we make a modeling and a coding pass over the text. In arators dictionary. These are based on the statistics of words under each tag, under no tag, and separators, respectively. In the second pass, the texts are encoded according to the model obtained. Decompression reads the model and decodes the text in a single pass.
Algorithm 1 (Dictionary switching). current _ dictionary default _ dictionary while there are more symbols do
At the beginning of the modeling process, words are stored in the default dictionary. When a start-structure tag appears we push the current dictionary in a stack and switch to the appropriate dictionary. When an end-structure tag is found we return to the previous dictionary stored in the stack. Both start-structure and the encoding and decoding processes use the same dictionary switching technique. 4. Merging dictionaries
Up to now we have assumed that each different tag uses its own dictionary. However, this may not be opti-happen to share many terms and to have similar probability distributions, then merging them under a single dictionary is likely to improve the compression ratio.

In this section we develop a general method to obtain a good grouping of tags under dictionaries. A key all text. The exact way to do this is to compute the union of the vocabularies, sum the frequencies of common words, and run Huffman algorithm to obtain the exact length of the text when dictionaries are joined. As this is costly, we estimate the size of the Huffman-compressed text without running Huffman algorithm. For this The following definitions give the conceptual framework for our final algorithm.
 4.1. Entropy estimation
Assume we have a text T of n terms partitioned into N texts T the raw frequency relative to a given dictionary d .

Definition 1 ( Raw frequency ). The raw frequency f d ( i ) of vocabulary term i is given by where occ d ( i ) is the number of occurrences of vocabulary term i in T rence probability of term i .

Definition 2 ( Zero-order entropy estimation ). Let V d be the number of vocabulary terms for text T order entropy H d of text T d is estimated as
We can now define the overall entropy of a text T partitioned into multiple texts. This is a lower bound to the average codeword length obtained by applying any zero-order compressor to the text under each dictionary.

T ={ T 1 , ... , T N } is computed as the weighted average of zero-order entropies H A Huffman coding over text T d will assign  X  d ( i ) bits to symbol i , so that the average code length,
L
H d 6 L d &lt; H d  X  1 (in practice L is much closer to H than to H  X  1).
 If we apply Huffman encoding to each T d , the resulting compressed file length is fore the average codeword length is As L d &lt; H d  X  1 for all d , L &lt; with the partitioned text as well. For this reason, we will use H as an estimation of L .
Definition 4 ( Estimated size contribution of a dictionary ). Let V constitutes dictionary d , and H d its estimated zero-order entropy. Then the estimated size contribution of dictionary d (considering vocabulary and text) is given by
Considering the last definition, we determine to merge dictionaries i and j when the sum of their contribu-tions is larger than the contribution of their union. In other words, when T
To compute T i [ j we have to compute the union of the vocabularies and the entropy of that union. This can be done in time linear in the vocabulary sizes.
 The last definition gives the tool we use for the optimization algorithm.

Definition 5 ( Estimated saving of a merge ). Let A i ; j Then 4.2. Optimization algorithm
Our optimization algorithm works as follows. We start with one separate dictionary per tag, plus the default dictionary (the separators dictionary is not considered in this process). Then, we progressively merge pairs of dictionaries until no further merging promises to be advantageous. Obtaining the optimal division into groups seems to be a hard combinatorial problem, so we use a heuristic which produces good results and is reasonably fast.

We start by computing T i for every dictionary i , as well as T this is positive. If it is, we erase i and j and introduce i [ j in the set, computing savings A the new element k that corresponds to i [ j . This process is repeated until all the A
Algorithm 2 depicts the process at a high level. H is a max-priority queue storing triples  X  i ; j ; A by A i ; j (note that A i ; j is not explicitly stored). Each occurrence of T are V .
 Algorithm 2 (Merging dictionaries).

H ; for 1 6 i 6 N do create dictionary i and compute T i for 1 6 i &lt; j 6 N do H H [ X  i ; j ; T i  X  T j T i [ j  X  k N +1 ( i , j , savings ) extract _ max ( H ) while ( savings P 0) do 4.3. Example
Fig. 2 shows an example XML file, from the personal repository of papers of John Smith. Let us first con-sider compressing all the words using a single model. The text has V total words. The words and their frequencies follow (excluding stopwords and separators).
John(4), Smith(4), Susan(1), Tate(1), Ashton(2), Albers(2), Jacob(1), Ziv(2), Com-pression(4), Algorithms(4), Applications(3), Lempel(1), 2001(1), 2003(1), 2005(2), Journal(3), ACM(2), Communications(1) the dictionary, let us assume that we need 8 bits per different dictionary word, thus V
Eq. (3) , the overall number of bits to represent the text is T 299 bits.
 Let us now separate the tags and compute the raw frequencies of the words within each tag.  X  author  X  : John(4), Smith(4), Susan(1), Tate(1), Ashton(2), Albers(2), Jacob(1),
 X  journal  X  : Journal(3), ACM(2), Algorithms(1), Compression(1), Communications(1) The entropies for each tag are computed using Eqs. (1) and (3) .

Tag Entropy (bits/word) Number of words Different words Total bits (Eq. (3) )
If we consider the tags in separate form we obtain, using Eq. (2) , a total entropy of H  X  2 : 334 bits per is always lower when we split a text. However, we must also consider the cost of maintaining separate vocab-when the tags are not separated.

Finally, let us consider the possibility of merging tags title and journal , as they share some words. The raw frequencies are now as follows:
 X  year  X  : 2001(1), 2003(1), 2005(2)
 X  title [ journal  X  : Compression(4),Algorithms(4),Applications(3),Ziv(1),Lempel(1), 118 bits. Added to the previous sizes obtained for author and year we get 256 bits. This is better than both previous extremes and exemplifies the benefits of merging, even on a small example. 5. Random access and searching
One of the most important characteristics of a semistatic method like word-based Huffman is that it permits ( Ziviani et al., 2000 ). In this section we show how those tasks can be carried out on SCMHuff . 5.1. Local decompression
The main obstacle to start decompressing from a given position in SCMHuff compressed text is that we need knowledge of the content of the stack of dictionaries at that point of the compression process (recall
Algorithm 1 ). Which is equivalent, we need to know the position of the text to access in the structure tree of the document collection, so that the stack of dictionaries corresponds to the root-to-leaf path that leads to the text position we wish to start decompression at.

In some browsing schemes, the system is already aware of the place in the tree that corresponds to the posi-tion where decompression must start, so let us focus in the case where there is no such knowledge.
We propose, essentially, to maintain an explicit structure tree. Each tree node corresponds to a tag or to a maximal space between tags, and it indicates its tag type and the position in the compressed document where the node starts. This way, once we wish to start decompression at some position of the compressed text, we to a space between tags, then we reached a tree leaf and can start decompression, otherwise we push the dic-we reach a leaf. When we start decompression, we can use Algorithm 1 from that point, as we have the correct
Algorithm 3 (Rebuilding the stack). t tree root
S empty stack while t is not a leaf do start decoding with stack S at position pos
An obvious question is how much space do we require for the structure tree. A first choice is not to rep-sonable depending on the application, so let us consider the alternative of explicitly storing the tree. per collection. A tree can be represented using 2 bits per node, as a sequence of opening and closing parentheses. Then the tree can be rebuilt in memory from a linear pass over the sequence. We need further-is reconstructed from the sequence of parentheses). Finally, we need to represent the sequence of lengths of the leaves in the compressed text, so as to reconstruct the initial text position of every tree node. We esti-have an overhead of 15 bits per tree node in our text collections, which translates into incrementing our compression ratios by 0.7 percentual points (that is, we should add 0.7 to the compression ratios achieved by SCMHuff to access it at random). On the other hand, in decompressed traversable form this tree requires 3.4% extra space, so for example we can manage a 1 GB text collection using just 35 MB of main memory for the structure tree.

We remark that having the structure tree might be anyway a requirement of the structured text retrieval system, and this also permits new search and traversal capabilities for such a system. 5.2. Searching
With plain Huffman compression, it is possible to compress the pattern and search the text for it, as the code of the pattern is the same across all the compressed text. This is a bit more complicated in SCMHuff . The reason is that the pattern is coded differently inside each dictionary.

The most general way the search can be carried out is as follows ( Turpin &amp; Moffat, 1997; Moura et al., 2000 ). We first find and mark all the vocabulary words that match the search pattern (it can be just one if the pattern is a simple string). Then we traverse the text, (essentially) scanning the target symbols one by one, and traversing the Huffman tree accordingly. Each time we arrive at a leaf, we check whether the leaf is marked. If so, we report an occurrence of the pattern. In any case, we return to the tree root and resume the scanning.

In our case we must preprocess the Huffman tree of each dictionary. In addition to the pattern, we are inter-ested in marking the Huffman tree leaves corresponding to new start-tags and to the end-tag corresponding to new start-tag, we push the current dictionary in a stack and start using the Huffman tree of the new context. When we find the end-tag, we pop and restore the previous dictionary.

Boyer X  X oore type searching is also possible on Huffman-compressed text ( Moura et al., 2000 ), especially pressed pattern, we enable a multipattern search for the compressed pattern and also the codewords of start-tags and end-tags, so as to switch models when the context changes. Alternatively, we can search for all the ture tree to find out whether the occurrence we have found corresponds to the current context. 6. SCMPPM
If we disregard any attempt of direct access or searching, still the SCM concept is useful in terms of boost-ing compression. To prove that this is the case and to show that SCM is general enough to accommodate other compression methods, we combine it with PPM compression. The essential idea of SCMPPM is that each dif-ferent structural element name will have its own PPM model to compress the text that lies under it.
A PPM coder usually works with characters as symbols, and we use it in this way. Still, we parse the text as a sequence of tokens so as to recognize tags. As PPM is adaptive, it does not need to store the model in the compressed file. As a result, there is no penalty for maintaining multiple models other than more compression time and space. Hence there is no point in merging similar models.

At the begining of the process, the default model is used to predict the symbol probabilities. When a start-structure tag appears, we push the current model in a stack and switch to the appropriate model. When an end-structure tag is found we return to the previous model stored in the stack. Both start-structure and end-structure tags are coded using the current model and then we switch models. The encoding and decoding processes use the same model switching technique. Algorithm 4 shows the model switching used for encoding and decoding.

Algorithm 4 (Model switching). current _ model default _ model while there are more token do 7. Experimental evaluation In this section we empirically evaluate SCMHuff and SCMPPM . For the former we use word-oriented
Huffman coding, compressing the dictionaries using arithmetic character-based adaptive coding. For the latter we use ppmdi coders for each model. Both prototypes are compared against state-of-art compressors. We remind that SCMHuff permits random access to individual documents, while SCMPPM does not. The tests were carried out on the SuSE Linux 9.1 operating system, running on a computer with a Pentium
IV processor at 1.2 GHz and 384 megabytes (MB) of RAM. We used g++ compiler with full optimization. For the experiments we selected different size subcollections (more precisely, prefixes) of WSJ, ZIFF and AP, from
TREC-3 3 ( Harman, 1995 ). Several characteristics of the collections are shown in Table 1 . We concatenated files so as to obtain approximately similar subcollection sizes from the three collections, so the size in MB is approximate.
 cating documents, and inside each document, tags indicating document identifier, date, title, author, source, content, keywords, etc. This structuring is very similar to INEX additional formatting tags. 7.1. Evaluation of the merging algorithm In this section we focus on SCMHuff , and in particular in its algorithm to merge models (Section 4 ).
Let us first consider speed. The average speed to compress all collections is around 265 KB/s (variance is low and not dependent on the collection type). This value includes the time needed to build models, merge dictionaries, and compress. Time for merging dictionaries ranges from 4.37 s for 1 MB to 40.27 s for significant for the largest collection (about 5%). The reason is that merging time is linear in the vocabulary size, which grows sublinearly with the collection size ( Heaps, 1978 ), typically around O  X  ing time also depends on the number of different tags, this number is usually small and does not grow with the collection size but depends on the DTD/schema.

We focus on the effectiveness of the algorithm now. Table 2 shows the number of dictionaries merged. Col-umn  X  X  X nitial X  X  tells how many dictionaries are there in the beginning: The default and separators dictionary umn  X  X  X inal X  X  tells how many different dictionaries are left after the merge.

For example, for small WSJ subsets, the tags  X  DOCNO  X  and  X  DOCID  X  , both of which contain numbers and internal references, were merged. The other group that was merged was formed by the tags  X  HL  X  ,  X  LP  X  and  X 
TEXT  X  , all of which contain the text of the news (headlines, summary for teletypes, and body). On the larger
WSJ subsets, only the last group of three tags was merged. This shows that our intuition that similar-content tags would be merged is correct. Also, the larger the collection, the less the impact of storing more vocabu-laries ( Heaps, 1978 ), and hence the fewer merges will occur.

In Fig. 3 we can see a comparison, for WSJ (in the other collections we have obtained similar results), of the compression performance with and without merging dictionaries. It can be seen that compression ratios improve for larger collections, as the impact of the vocabulary is reduced. Merging plays a sort of smoothing naries more aggressively, obtaining almost 2% of additional compression on texts of 1 MB. As the text collec-tion grows, the cost of storing (one or more) vocabularies becomes less significant, and thus the method merges fewer dictionaries. For large enough texts, merging would not occur and the method would store one different vocabulary per text tag. In column SAM we show the result of the basic separate alphabets model (that is, as if we merged all the dictionaries except that for separators). Note that, although SCM without merging can be worse then SAM for small collections, merging always finds an optimum point between the two extremes. We also included column MG , in principle similar to SAM , to show the differences due to dif-ferent implementations of the same idea.

Another question is how good is our heuristic to choose which dictionaries to merge. On one hand, we use an entropy-based method to predict the size of the merged dictionaries from the vocabulary distributions. This is very accurate: our lower-bound prediction is usually 99 X 99.5% of the final value.

To globally evaluate the heuristic, we have compared it against an exhaustive search for all the merging possibilities. We have taken 1 Mb from WSJ and limited the tags under consideration to reduced subsets of size 7 and 8. We have considered all the 666 and 2284 merging possibilities, respectively, and have compressed the text using each of them. In both cases, our heuristic obtained the result that achieved best compression. Tables 3 and 4 show some detailed results.

We could produce a case where the heuristic failed to find the best combination, over a 10 Kb file with little structure. Yet, the difference with the best combination was just 0.01%.
 7.2. Comparison against classical compressors
We now compare SCMHuff and SCMPPM against several classical compression systems: (1) GNU gzip v.1.3.5, 5 which uses LZ77 plus a variant of Huffman algorithm (we also tried zip with almost identical results but slower processing); (2) UNIX X  X  compress v.4.2.4, which implements LZW algorithm; (3) bzip2 v.1.0.2, 6 , which uses the Burrows X  X heeler block sorting text compression algorithm, plus Huffman coding; (4) ppmdi (extracted from XMLPPM v.0.98.2 7 ), a PPM compressor; and (5) MG System v1.2.1 ten et al., 1999 ). The MG system is a public domain information retrieval system software, versatile and of general purpose, which handles compressed text, indexes and images. For texts it uses a word-based Huffman variant called Huffword and implements the separate alphabets model. We used standard options for all and also maximum compress option whenever possible, except for bzip2 where maximum compression is the default. In this case, we also tried minimum compression option. Compression ratios for each collection are shown in Fig. 4 .

Let us first consider the standard compressors. Compress obtained the worst compression ratios, not com-petitive in this experiment. It is followed by gzip , which has almost no difference between its default and maximum compression options. The next is bzip2 , with a wide difference between best and worst compres-sion performance. Finally, the best standard compressor is ppmdi , which achieves 22 X 23% compression ratios.

Our SCMPPM compresses better than ppmdi as soon as the text collections exceed 1 X 5 MB size (depending on the collection). This shows that ppmdi method is actually boosted by separating the models according to the text structure.

Let us now focus on methods that permit direct access to the text. In our experiment, those are the word-based methods MG and SCMHuff . Both improve as the text grows, because the impact of storing the vocab-ulary decreases. At some point the compression ratios of MG stabilize around 28%. Our SCMHuff compresses uniformly better than MG , reaching a stable improvement of 1.5 X 2 percentual points. After subtracting from this improvement the 0.7 points to have direct access to SCMHuff compressed text (Section 5 ), we still have 2 X 4% better compression ratios compared to MG . 7.3. Comparison against structure-aware methods We now compare SCMHuff and SCMPPM against other compression systems that exploit text structure: XMill v.0.8 9 and XMLPPM v.98.2. 10 XMill ( Liefke &amp; Suciu, 2000 ) is an XML-specific compressor based on
Ziv X  X empel and Huffman, able to handle the document structure. On the other hand, XMLPPM ( Cheney, 2001 ) is also specific of XML and based on adaptive PPM over the structural context.
 XGrind 11 was excluded from this comparison because we could not make it work properly on our dataset. To be sure that this exclusion was not important, we altered our collection until producing 1 MB of text where
XGrind finally worked. The resulting compression ratio was 57.28%, which is not competitive at all in this experiment. XCQ was also excluded because we could not find the code, yet results reported in ( Lam, Wood, show to be not competitive in our experiments either. The same happens with Exalt , according to the results in ( Toman, 2004 ).

Compression ratios are shown in Fig. 5 . We used standard options for all and also maximum compression option whenever possible.

XMill obtains an average compression ratio roughly constant in all cases because it uses zlib as its main compression machinery. The compression ratio obtained, 33 X 35%, is not competitive in this experiment.
XMLPPM , on the other hand, is the most competitive alternative to SCMPPM . Although for 1 MB the compression ratios are similar, soon SCMPPM wins and for 100 MB its margin is around 3.3% in all cases. This shows that the idea of using the structural context to compress pays off.

SCMHuff is the only method permitting navigation and random access. It compresses better than XMill for collections of more than 1 MB, and for longer texts its margin over XMill grows up to 25%. 7.4. Speed and memory usage
Fig. 6 shows the overall average values for compression and decompression speed in relation to the com-pression ratio. We averaged values over all the collections because they did not significantly depend on the collection and variance was always low.

The fastest at decompression is gzip (based on LZ77), followed by compress (based on LZ78) and XMill later come bzip2 and SCMHuff . All the PPM-based compressors are slowest at decompression, as expected from this family. MG does not appear in the decompression plot, because it does not permit decompressing fully optimized.

For compression, the figures are similar except that SCMHuff is the slowest, possibly because of the need to takes the same time of the slow PPM compressors.

Fig. 7 shows the overall average values for memory usage in relation to the compression ratio. Most com-pressors are tuned to use 10 Mb of memory to compress. The PPMDi variants use 20 X 30 Mb to achieve better compression ratio. Our SCMPPM uses more than 200 Mb, as it stores a PPMDi model per tag, yet it achieves improved compression ratio.

This raises the question of whether SCMPPM compresses more than ppmdi simply because it uses more memory. To show that memory can indeed be used better under the SCM approach, we have tuned
SCMPPM to use the same memory given to ppmdi -9 , 22 Mb. Table 5 shows that it is possible to compress better using the same memory under the SCM paradigm.

To achieve these results, however, we have tuned SCMPPM . The current prototype uses the same amount of memory for all the tags, which wastes a lot of memory on those tags that handle less text. A more refined text would never require much memory. Moreover, we could use a two-pass technique which, based on the k th concept, we have manually tuned the amount of memory used by each tag and also merged the tags according to SCMHuff heuristic: HL + LP + TEXT and DOCNO + DOCID for WSJ, HEAD + TEXT + NOTE and FIRST +
SECOND for AP, TITLE + TEXT + ABSTRACT + SUMMARY + DESCRIPT and DOCNO + DOCID for ZIFF. A more sound heuristic, better adapted to PPM modeling, could achieve better results and do it automatically. 8. Conclusions and future work
We have proposed a new model for compressing semistructured documents based on the idea that texts grouping of tags so as to code each group with a separate model. The impact of the model on the retrieval performance should be negligible.

We have shown that the idea actually improves compression ratios. We have compared our prototype against state-of-the-art compression systems, showing that our word-based Huffman prototype obtains 2 X  4% better compression ratios than existing alternatives that permit random access to the compressed text.
On the other hand, combining the SCM general concept with PPMDi does not permit random access any-more, but it yields unbeaten compression compared to any other compressor, by a margin of 2 X 5%.
Reducing space is not that important by itself, especially if we consider the low cost of massive storage devices. The point is that those massive devices are usually orders of magnitude slower than smaller memories.
In recent years, the performance gaps in the memory hierarchy (registers, cache, RAM, disk, network) have only widened, making compression more and more appealing. A space reduction, even if it involves higher work). Even random accesses of small pieces of the file over a disk benefit from compression, as the disk seek times depend on the number of tracks to traverse, and this a linear function of the file size.
Current research in compression usually strives to obtain gains that are below 1%. By taking the structure into account, we have obtained much more significant gains in compression ratios. It is likely that more more in depth the relationship between the type and density of the structuring and the improvements obtained with our method, since its success is based on a semantic assumption and it would be interesting to see how this works on other text collections.
 References
