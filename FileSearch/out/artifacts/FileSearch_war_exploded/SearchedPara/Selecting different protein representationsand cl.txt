 School of Computing and Centre for Biomedical Informatics, University of Kent, Canterbury, UK 1. Introduction
The task of computational prediction of protein function based on the protein X  X  amino acid sequence is an active area of research in the fi eld of proteomics [18,47]. One approach that can be used to infer proteins functions is supervised machine learning  X  more precisely, the classi fi cation task of machine learning or data mining. The goal is to use a set of proteins whose functions are known to build a classi fi cation model that can be used to predict the functions of proteins whose functions are unknown. The use of supervised machine learning (classi fi cation) algorithms is common practice in the fi eld [1,20, 37,43].

There are two major problems in the task of computational protein function prediction with classi fi -cation algorithms, which are the choice of the protein representation and the choice of the classi fi cation algorithm. Those are open problems, even in the conventional scenario of  X  fl at X  classi fi cation (where there are no hierarchical relationships among classes), as there are many choices and it is not clear which representation and classi fi cation algorithm are the best. In the hierarchical classi fi cation scenario addressed in this paper, where protein functional classes are organised into a hierarchy, these problems are aggravated, due to the large number of classes and classi fi cation sub-problems (where different algorithms and different representations might be best for different class levels).

There are severalways of extracting features from a protein, and the choice of the feature representation might be as important as the choice of the classi fi cation algorithm. Apart from a few works, such as [28], the issue of which feature representation to use is often overlooked as the authors are usually more focused on which classi fi cation algorithm to use or related issues. One particular challenge is that not all feature sets are available for every experiment, as some biological databases are highly specialized in one particular organism and the same information might not be available for other organisms.
According to [13] there are two broad types of representations that can be derived for proteins: alignment-independent, which are features computed from the sequence by using some computational method without performing sequence alignment, and alignment-dependent, which are features obtained from biological databases of motifs or domains that were typically discovered by performing sequence alignment on a large-scale, in order to identify conserved regions in the sequences of homologous proteins.

In this paper we address the problems of both protein representation selection and algorithm selection in a synergistic way by using selective hierarchical classi fi cation approaches. More precisely, we dynamically select the best protein representation and the best classi fi cation algorithm for each class in the hierarchy. Both selections are done in a data-driven way.

The remainder of this paper is organized as follows: Section 2 presents a brief introduction to the task of protein function prediction and the 8 different protein representations employed in this work. Section 3 brie fl y introduces the task of hierarchical classi fi cation and the 3 hierarchical selective approaches: (a) selecting the best classi fi er for each class node given a fi xed representation for all class nodes; (b) selecting the best representation for each class node given a fi xed classi fi er for all class nodes; and (c) selecting the best classi fi er and representation for each class node. Section 4 presents the experimental setup for the experiments. The computational results and their discussion are presented in Section 5 and fi nally, in Section 6 we state our conclusions and future research directions. 2. Protein function prediction 2.1. Overview of the protein function prediction problem
Proteins are large molecules that execute the vast majority of the functions performed by a living cell [2]. Proteins are produced from genes, by transforming the genes X  DNA material into amino acids, the building blocks of proteins. Hence, a protein essentially consists of a long sequence of amino acids, which folds into a speci fi c 3D structure, where different protein structures are suitable for performing different functions. In the last few years there has been an enormous progress in genome sequencing technology (giving us the knowledge of the full DNA contents of many organisms, including humans), and as a result the number of proteins with known sequence of amino acids has been growing very fast. Unfortunately, however, the number of proteins with known function is growing at a much lower rate, because it is much more time-consuming and expensive to determine the structure and function of a protein than just determining its sequence of amino acids. Knowledge of protein functions is very important in biomedical sciences, not only for a better understanding of cell biology in general, but also because many diseases are caused by or at least associated with defects in protein functions. Hence, an effective method for the prediction of protein functions can potentially contribute to generate new biological knowledge that can lead to a better treatment and diagnosis of diseases, design of more effective medical drugs, etc.

Therefore, there is a clear motivation to develop data mining methods (speci fi cally classi fi cation methods, based on supervised machine learning) that can predict the function of a protein based on its sequence of amino acids. Although such computational predictions are not so reliable as the results of biological experiments that directly determine a protein X  X  function, computational methods are much cheaper and faster, and so they can give researchers valuable clues for the design of future biological experiments.

In this paper we predict protein function from the protein X  X  amino acid sequence by using classi fi cation methods. Hence, each example (data instance) corresponds to a protein, the values of the predictor attributes for an example represent properties of the corresponding protein and the classes represent function(s) associated with the protein. The problem is challenging for at least two reasons. First, there are many different types of protein properties that could be used as predictor attributes, and it is not clear which type of property(ies) has(ve) greater predictive power. (To cope with this, our system automatically selects the best type of protein representation in a data-driven manner, as will be explained later.) Secondly, there are a large number of protein functions, which are typically organized into a hierarchy of functions, naturally leading to a hierarchical classi fi cation problem, where the classes to be predicted are hierarchically-structured in the form of a tree of class nodes, in the case of our datasets. Hierarchical classi fi cation is, by comparison, a much less investigated research area than standard ( fl at) classi fi cation, and the former tends to be a considerably more dif fi cult type of problem due mainly to the large number of classes to be predicted. The de fi nition of predictor attributes for protein function prediction is discussed in the next Sub-section (2.2), whilst methods of hierarchical classi fi cation in the context of protein function prediction are discussed in Section 3. 2.2. Protein feature types
In this section we describe the protein representations used and evaluated in this work. The protein representations in Sub-sections 2.2.1, 2.2.2, 2.2.3, 2.2.4 are alignment-independent representations. The protein representations in Sub-section 2.2.5 are alignment-dependent. 2.2.1. Sequence length and molecular weight
The sequence length is a numerical value which is simply the count of amino acids of a protein. The molecular weight is the sum of the molecular weights of all amino acids in the protein.

These features have been used (with other attributes) in [1,23,28]. Since these features are believed to be important for protein functional prediction and they are easily available, we always use them in conjunction with the other protein representation studied in this work. 2.2.2. Z-values
The z-values [12,37], also known as Sandberg Descriptors [32,36], are the principal components of 26 different physicochemical measured and calculated properties of amino acids, and essentially represent hydrophobicity/hydrophilic ity (z1), steric/ bulk properties and polarizability (z2), polarity (z3), and electronic effects (z4 and z5) of the amino acids [32].
 In [37] 5 z-values are used to represent each amino acid of the protein sequence. For example, the Alanine (A) amino acid has 5z values: z1 = 0.24, z2 =  X  2.32, z3 = 0.60, z4 =  X  0.14, z5 = 1.30. Therefore a protein sequence of length n would be represented by n *5 features. In [12,37] the authors suggested that the z-values for all amino acids of each protein are averaged so that a protein is represented by just 5 z-values, instead of 5*n. This is needed because most machine learning methods cannot cope with instances (in this case proteins) which have varying number of features (in this case the z-values). It should be noted that they tried more complicated ways of aggregating z-values, but they had better results with this simpler method.

Originally in [37] the authors used the averaged z-values from the whole amino acid sequence. After some experimental research they found out that in order to classify GPCR (G-Protein Coupled Receptor) proteins, it would be better to use 15 z-values [12]. These z-values are then computed as follows: 5-values are computed and averaged over the whole protein sequence. Another 5 z-values are computed from the N-terminus (the fi rst 150 amino acids of the protein sequence) of the protein and further 5 z-values are computed from the C-terminus (the last 150 amino acids of the protein sequence). The number of 150 amino acids was found, in previous experiments, to give the largest improvement in accuracy [12].
In this work we use both 5 z-values and 15 z-values. 2.2.3. Amino acid composition ( AA )
Another feature which is very simple to compute based on the protein sequence is the percentage occurrence of each amino acid within a protein sequence. This will create a feature set of 20 features, each of them with the percentage of how many times a particular amino acid occurs within the protein X  X  amino acid sequence.

This type of feature has been used in [1,22,28,43]. 2.2.4. Local descriptors ( LD )
The local descriptors, also known as global protein sequence descriptors [16], were used in [7,9,11, 44].

There are three types of local descriptors used in the aforementioned works (and also used in our own experiments): Composition, Transition and Distribution, which are computed based on the variation of occurrence of functional groups of amino acids within the primary sequence of the protein. The functional groups used were: hydrophobic (amino acids CVLIMFW), neutral (amino acids GASTPHY), and polar (amino acids RKEDQN).

Composition accounts for the percentage com position (rela tive frequency) of a par ticular functional group within the amino acid seque nce. Therefor e, there are three compos ition features , one for each functional group of amino acids.

Transition features represent the relative frequency in which an amino acid from a particular functional group is followed by an amino acid from another functional group. More precisely, the following transitions are considered: Polar  X  Neutral or Neutral  X  Polar; Polar  X  Hydrophobic or Hydrophobic  X  Polar; and Neutral  X  Hydrophobic or Hydrophobic  X  Neutral.

Distribution features are computed based on the percentage of how many amino acids of a particular functional group are present on the fi rst, 25%, 50%, 75% and 100% of the amino acid sequence.
In total there would be 21 features (3 composition, 6 transition, 12 distribution) if they were computed from the whole amino acid sequence. However, in [11,44] the authors divided the protein sequence into 10 descriptor regions (A-J) as follows: Regions A,B,C and D are obtained by dividing the entire protein sequence into four equal-length regions. Regions E and F are obtained by diving the protein sequence in two equal-length regions. Region G represents the middle with 50% of the sequence. Region H represents the fi rst 75% of the sequence, Region I the fi nal 75% of the sequence and Region J the middle with 75% of the sequence. For each region the 21 lo cal descriptors are extr acted, resu lting in a 210 feature vector. These regions are illustrated in Fig. 1.
 2.2.5. Motif-based features
Instead of computing features directly from the protein sequence, like in the previously described protein representations, it is possible to use features obtained from biological databases. In [5,6,15,20, 23,25,31,35,46]the authors use the absence/presence of a particular type of protein signatures ( X  X otifs X ) as binary features.

In this work we use protein signatures from four different databases as features. The employed signatures are PROSITE patterns [26], which use regular expressions to encode the motifs; Fingerprints from the PRINTS [3] database, which are created by considering several motifs to be present in the same protein; motifs from the PFAM [4] database, which are created by using hidden Markov models; and entries from the InterPro [34,27] database.

The PROSITE patterns are encoded as regular expressions, and the rationale behind its development is that a protein family could be characterized by a single most conserved motif within a multiple alignment of its members sequences, as this would likely encode a key biological feature [21]. However, as pointed out in [21], most protein families are characterized not by one, but by several conserved motifs. This is the rationale behind the development of the fi ngerprints motifs used in the PRINTS database. Another approach to characterize protein fa milies adopts the principle t hat the variable regions between conserved motifs also contain valuable information. In the PFAM database, the pro fi les are encoded using hidden Markov models. Although there is some overlap between these three databases, their content is signi fi cantly different. Also, as pointed out in [21], these motifs have different areas of application, e.g.: PROSITE patterns are unreliable in the identi fi cation of members of highly divergent superfamilies (where HMMs excel); fi ngerprints perform relatively poorly in the characterization of very short motifs (where PROSITE patterns do well); and HMMs are less likely to give speci fi c subfamily diagnoses (where fi ngerprints excel). For these reasons, the curators of these databases (among others) decided to combine efforts in the creation of the INTERPRO database, which combines the information from all these and other databases. 2.2.6. Summary of protein features used in this work
Table 1 presents a summary of the feature types and the respective number of features used in this work. As explained earlier, the top 4 features in Table 1 are alignment-independent features, whilst the bottom 4 features are alignment-dependent. In this table EC and GPCR refer to the Enzyme and GPCR datasets whose creation is explained in detail in Section 4.2. 3. Hierarchical protein function prediction
Protein functions are often speci fi ed in a functional class hierarchy, with more generic functions at higher levels and more speci fi c functions at deeper levels. For instance, Fig. 2 illustrates a small part of the Enzyme Commission hierarchy. On the fi rst level of the hierarchy, there are 6 classes. The meaning of each class is as follows: EC 1 = Oxidoreductases, EC 2 = Transferases, EC 3 = Hydrolases, EC 4 = Lyases, EC 5 = Isomerases, EC 6 = Ligases. The remaining classes shown on Fig. 2 have the following functions: EC 1.1 = Acting on the CH-OH group of donors, EC 1.1.1 = With NAD or NADP as acceptor, EC 1.1.1.1 = alcohol dehydrogenase, EC 1.1.1.2 = alcohol dehydrogenase (NADP + ), EC 1.1.1.3 = homoserine dehydrogenase.

The existing hierarchical classi fi cation methods can be analyzed under different aspects [42,17,41], as follows:  X  The type of hierarchical structure of the classes, which can be either a tree structure of a DAG (Direct  X  How deep the classi fi cation in the hierarchy is performed. I.e., if the output of the classi fi er is always  X  Whether an example (protein) is assigned to exactly one leaf node in the class hierarchy or potentially  X  How the hierarchical class structure is explored by the algorithm. The existing hierarchical classi fi -
In the global X  X odel approach, a single (relatively complex) classi fi cation model is built from the training set, taking into account the class hierarchy as a whole during a single run of the classi fi cation algorithm. When used during the test phase, each test example (unseen during training) is classi fi ed by the induced model, a process that can assign classes at potentially every level of the hierarchy to the test example [17,39,42].

The local X  X odel approach consists of creating a local classi fi er for every parent node (i.e., any non-leaf for each class node (parent or leaf node, except for the root node) [10]. In the former case the classi fi er X  X  goal is to discriminate among the child classes of the classi fi er X  X  corresponding node. In the latter case, each binary classi fi er predicts whether or not an example belongs to its corresponding class. In both cases, these approaches are creating classi fi ers with a local view of the problem.

Despite the differences on creating and training the classi fi ers, these approaches are often used with the same  X  X op-down X  class prediction strategy in the testing phase. The top-down class-prediction approach works in the testing phase as follows. For each level of the hierarchy (except the top level), the decision about which class is predicted at the current level is based on the class predicted at the previous (parent) level. More precisely, once an instance (protein) is assigned a class at a certain level, only the subclasses (child nodes) of that class are considered as candidate classes to be assigned at the next lower level. The main disadvantage of the local approach with the top-down class-prediction approach is that a classi fi cation mistake at a high level of the hierarchy is propagated through all the descendant nodes of the wrongly assigned class. 3.1. Selective classi fi er and representation approaches
In [12,37] the authors hypothesise that it would be possible to improve the predictive accuracy of the local, top-down approach by using different classi fi cation algorithms at different nodes of the class hierarchy. The choice of which classi fi er to use at a given class node is made on a data-driven manner using the training set. More precisely, in order to determine which classi fi er should be used at each node of the class hierarchy, during the training phase, the training set is randomly split into mutually-exclusive sub-training and validation sets. Different classi fi ers are then trained using this sub-training set and are then evaluated on the validation set. The classi fi er chosen for the current class node is the one with the highest classi fi cation accuracy on the validation set. In this approach the protein representation is fi xed, i.e. all classi fi ers are trained with the same feature set. This approach is referred to as the Selective Classi fi er (Sel. C.) approach.

In this work as components of the Sel. C. approach we have employed the k nearest neighbor (k-NN) with k = 3, Naive Bayes (NB) and Support Vector Machines (SVM) classi fi ers. All these classi fi ers were used with the WEKA Data mining Tool [45] with default parameters. The rationale behind the choice of these particular classi fi ers is that they are well-known classi fi ers which have been successfully used in fl at (non-hierarchical) protein function prediction problems and also they have very different inductive biases, meaning that they will construct different classi fi cation models, therefore insuring a diversity of predictions to be exploited by the Sel. C. approach.

Inspired by the selective classi fi er approach, in [40] the authors proposed that instead of selecting the best classi fi er, it might be better to select the best representation (feature set) at each node of the class hierarchy. In this approach the classi fi er is fi xed, i.e. at all class nodes the same type of classi fi er is trained with each of the different types of feature set, and the best type of feature (on the validation set) is chosen at each class node. This approach is referred to as the Selective Representation (Sel. R.) approach.

Another alternative [40] to selecting only the best classi fi er or only the best representation is to try to select the best combination of both for each node of the class hierarchy. In this approach, all the classi fi ers are trained with all the available representations, and the best joint combination of classi fi er and representation is selected. This approach is, therefore, the combination of Sel. C. and Sel. R.. This approach has the advantage of having a greater fl exibility as it considers the interactions between classi fi ers and representations. However, it has the drawback that it is computationally expensive, meaning that in practice, we need to limit the number of classi fi ers and representations to a small number.

It should be noted that the Sel. R. and Sel. C. + Sel. R. approaches proposed in [40] have originally been evaluated in a music genre classi fi cation dataset, while in this work, we perform many more experiments on a very different application domain, namely protein function prediction.

Note that, at a very high level of abstraction, the idea of representation selection seems similar to the well-known idea of feature selection in data mining [33]. However, the motivation for representation selection rather than feature selection in a hierarchical classi fi cation scenario is explained by the following reasons: (a) it is much more ef fi cient (faster) to select a representation at each class node than to perform feature selection at each class node; (b) Representation selection produces results at a coarser grain of information, possibly providing new insights to biologists, that is, it might reveal that some broad type of representations (sets of features of the same type, rather than single features) are particularly more effective to classify protein functions at particular levels. It also differs from feature selection as different representations in a dataset are actually just  X  X andidate representations X , because just one will be chosen, unlike in feature selection where any subset of features could be chosen. 4. Experimental setup 4.1. Evaluation metrics for hierarchical predictive accuracy
Unfortunately, in the task of hierarchical classi fi cation there are no standard measures to evaluate the results. Comprehensive reviews of hierarchical classi fi cation measures can be found in [8,42]. An aspect their hierarchical classi fi cation algorithms. Therefore, the question that naturally arises, since there is no consensus in the literature , is  X  X hat evaluation metric to use? X . In order to evaluate the algorithms we have used the metrics of hierarchical precision (hP), hierarchical recall (hR) and hierarchical f-measure (hF) proposed in [29]. These measures are extended versions of the well known metrics of precision, recall and f-measure but tailored to the hierarchical classi fi cation scenario. They are de fi ned as follows: speci fi c class predicted for test example i and all its ancestor classes and  X  T i is the set consisting of the most speci fi c true class of test example i and all its ancestor classes. The main advantage of using this particular metric is that it can be applied to any hierarchical classi fi cation scenario (i.e. single label, multi-label, tree-structured, dag-structured, ma ndatory-leaf node or non-manda tory leaf node problems). 4.2. Data preparation
The protein datasets used in this work were originally developed by [25]. These datasets were originally created from the information about two types of proteins (Enzymes and GPCRs  X  G-Protein Coupled Receptors) obtained from different protein databases. For both datasets, the classes (protein functions) form a tree where each node represents a class. An excerpt of the class tree associated with the Enzymes dataset was shown in Fig. 2, where classes at different levels are separated by a  X . X  E.g., as shown in that fi gure, there are 6 classes at the fi rst level, each of them sub-divided into sub-classes, and so on, until the fourth class level. Each class essentially refers to the type of chemical reaction catalyzed by an enzyme. In the case of the GPCR dataset, each class essentially denotes the type of ligand that binds to the GPCR (GPCRs essentially transmit signals received from ligands outside the cells to other molecules inside the cell). For further details of the meaning of the functional classes in these two datasets, see [25].
Originally there were 8 datasets (4 for Enzymes and 4 for GPCR) created based on protein data available in the Uniprot database and motif information obtained from the Interpro, Pfam, Prints and Prosite databases. Each of those datasets contained only one type of motif representation. For example, the EC-Interpro dataset had as predictive attributes only the Interpro entries motifs.

It should be noted that proteins obtained from biological databases contain non-standard amino acids and in such cases we have made the following substitutions, as it has been done in [12]: B (either an asparagine or aspartic acid)  X  N (asparagine); Z (either a glutamine or a glutamic acid)  X  Q (glutamine); X (unknown residues)  X  A (alanine); U (selenocysteine)  X  C(cysteine).

One of the objectives of this work is to evaluate the impact of the many different types of representations discussed in Section 2.2. Therefore, we expanded the number of representations used in each of the original eight datasets by extracting the alignment-independent attributes described in Section 2.2. This means that each of these 8 datasets now has 5 representations (5z,15z,AA,LD,one type of motif). These datasets are hereafter referred to as single-motif datasets.

Although these datasets allow us to verify the impact of each of the alignment-independent features against each of the motif representations, they do not allow us to verify if there is any difference in the predictive power of the different motifs representations. For this reason, we have also created two new datasets, which we refer to as  X  X ultiple-motif EC X  and  X  X ultiple-motif GPCR X  which were created from the common proteins that appeared in all four corresponding speci fi c datasets, i.e. the four datasets about EC or the four datasets about GP CR. Therefore, each multiple-motif dataset has 8 representations (5z,15z,AA,LD,Interpro motifs, Pfam motifs, Prints motifs and Prosite motifs).

Table 2 presents a summarized description of the datasets. The last column of Table 2 presents the number of classesat each level of the hierarchy (1st/2nd/3rd/ 4th levels). Note that concerning the number of protein representations, the multiple-motif datasets are more comprehensive than their single-motif counterpart datasets, because the 5 candidate representations used in a single-motif dataset are a proper subset of the 8 candidate representations used in the c orresponding multip le-motif dataset. However, the motivation for performing the experiments on both single-motif and multiple-motif datasets is that the latter datasets have a reduced number of examples (specially in the case of Enzymes), since a protein is included in a multiple-motif dataset only if it appears in all the four single-motif datasets for the protein in question (Enzymes or GPCRs). Hence, the single-motif datasets have considerably more examples, offering a better statistical support to some experiments. The datasets used in the experiments are available at: http://s ites.google.co m/site/carlossillajr/resources. 5. Computational results and discussion
In this section, we will fi rst discuss the impact of the different protein representations (which is less investigated in the literature) on the task of hierarchical protein function prediction and we will also discuss the impact of the different classi fi ers. Also, all the experiments were performed using 10-fold cross-validation. 5.1. Impact of the different protein representations 5.1.1. Results for the single-motif datasets
One of the main contributions of this paper is to asses the impact of the choice of a type of protein representation on the hierarchical protein function prediction problem. Table 3 presents the results obtained by each representation on each single-motif dataset. However, verifying the particular impor-tance of each protein representation is not straightforward, since as seen in Section 2.2.5 different motif representations have very different rationale behind their development. For this reason, in the analysis of the different protein representations based on the single-motif datasets, we break down the analysis by the type of motif. That is, for each of the 4 types of motif, we analyse the result for both EC and GPCR datasets with that motif as a candidate representation to be selected. E.g., taking into account the results on both EC-Interpro and GPCR-Interpro, as they have the same type of motif-based protein representation.

For the Interpro-motif based datasets, considering all the representations (including the selective representation method), the average ranking of the protein representations (computed by the Friedman statistical test, considering the hierarchical f-measure values) is: Sel. R. (1.375), Interpro motifs (2.0), LD (3.0), AA (3.875), 15z (4.875) and 5z (5.875) (the smaller the rank number, the better the method). This ranking provides an overall order of the effectiveness of each protein representation across all datasets without going into the merit of wins/loses in individual datasets [14]. In order to identify on which pairwise comparisons there is a statistical difference between the results, we conduct a post-hoc test. As strongly recommended by [19] we use the Shaffer static procedure for  X  = 0.05. This combination of Friedman statistical test and Shaffer post-hoc test was used to produce all results shown in Figs 3 to 8. Figure 3 shows the result of this test in a graphical way as suggested by [14]. In Fig. 3 the bold horizontal lines connect the representations whose results are not found to be statistically signi fi cantly different. (This graphical representation is also used in Figs 4 through 8.) The analysis of the results in Fig. 3 shows that there is no statistical difference, when comparing the Sel. R., Interpro Motifs, LD and AA. There is a statistical difference when comparing the Sel. R., Interpro Motifs and LDs with 5z and 15z.
 For the Pfam-motif based datasets, the average ranking of the protein representations is: Sel. R. (1.25), LD (2.875), Pfam motifs (3.125), AA (3.5), 15z (4.5) and 5z (5.875). Figure 4 shows the graphical result of the Shaffer static post-doc test. The analysis of the results in Fig. 4 shows that there is no statistical difference, when comparing the Sel. R., LD, Pfam Motifs and AA. There is a statistical difference when comparing the Sel. R., Pfam Motifs and LDs with 5z and 15z.
 For the Prints-motif based datasets, the average ranking of the protein representations is: Sel. R. (1.0), LD (2.75), Prints motifs (2.875), AA (3.75), 15z (4.75) and 5z (5.875). The analysis of the results in Fig. 5 shows that there is no statistical difference, when comparing the Sel. R., LD, Pfam Motifs and AA. There is a statistical difference when comparing the Sel. R., Prints Motifs and LDs with 5z and 15z.
For the Prosite-motif based datasets, the average ranking of the protein representations is: Sel. R. (1.0625), LD (2.8125), AA (3.5), Prosite motifs (3.875), 15z (4.25) and 5z (5.5). The analysis of the results in Fig. 6 shows that there is no statistical difference, when comparing the Sel. R., LD, and AA. There is a statistical difference when comparing the Sel. R. with Prosite Motifs, 5z and 15z. 5.1.2. Results for the Multiple-Motif datasets
Recall that apart from the si ngle-motif datasets, we h ave also created two mu ltiple-motif datasets in order to evaluate the performance of each particular type of motif against the others as well as against the alignment-independent features and the Sel. R. approach. Table 4 presents the predictive accuracy (measured by hier archical pr ecision, r ecall and f-measure v alues) by each represen tation on the multiple-motif datasets. The average ranking of the protein representations (computed by the Friedman statistical test, considering the hierarchical f-measure values) is: Sel. R. (1.5), Interpro motifs (2.75), Prints motifs (3.5), LD (4.625), AA (5.5), Prosite motifs (6.0), Pfam motifs (6.625), 15z (6.5), 5z (8.0). Again, this ranking provides an overall order of the effectiveness of each protein representation across all datasets and classi fi cation algorithms without going into the merits of individual wins/loses. Figure 7 shows the graphical result of the Shaffer static post-doc test. The analysis of the results in Fig. 7 shows that there is no statistical difference, when comparing the Sel. R., Intepro motifs, Prints motifs, LD, and AA. There is a statistical difference when comparing the Sel. R. with Pfam and Prosite Motifs, 5z and 15z. 5.1.3. Discussion of results for different protein representations
The overall analysis of the results shows some interesting points. First, although not statistically signi fi cantly different from some representations, the Sel. R. has ranked 1st in all experiments, meaning that it is an interesting approach to deal with the problem of hierarchical protein function prediction.
Second, the result that 15z is better than 5z (although not statistically signi fi cant) corroborates with the experiments of [11,38] where the authors came to the same conclusion. Note, however, that in their experiments they used only one GPCR dataset, while in this study we have employed 4 GPRC and 4 Enzyme datasets. Our work therefore, validates their initial proposal in a larger number of datasets. According to [37], the z-values representation provides a numerical description of the proteins X  physiochemical properties that potentially results in a higher predictive accuracy than the use of amino acid sequence composition. However, in our experiments we have empirically veri fi ed that this is not the case, since the AA features are always ranked above both 5z and 15z features (although this difference is not statistically signi fi cant).

Third, the best performing alignment-independent feature is the LD. Its results are better than all the other alignment-independent features (although only statistically signi fi cantly different from 5z on some motif-based datasets).

Fourth, the use of the alignment-dependent features (motifs) on the single-motif datasets have ranked 2nd for Intepro motifs, 3rd for Pfam and Prints motifs and 4th for the Prosite motifs. On the multiple-motif datasets the alignment-dependent features (motifs) have ranked 2nd (Intepro), 3rd (Prints), 6th (Prosite) and 7th (Pfam). Considering the rankings it is clear that the use of Interpro motifs lead to higher predictive accuracies than the use of other types of motifs. This is an expected result, since (as previously discussed) Interpro is a joint effort from curators of all its members databases which includes Prosite, Prints and Pfam among others.

Note that the Sel. R. was the best protein representation on both experiments (single-motif datasets and multiple-motif datasets). Hence, it is interesting to analyse which features were selected the most by the selective representation approach at each level of the class hierarchy. Tables 5 and 6 present the percentage of how many times a particular protein representation was selected in each dataset at each level of the class hierarchy for the datasets with 5 and 8 representations, respectively, corresponding to single-motif and multiple-motif datasets, respectively.

The analysis of Table 5 shows that for the single-motif datasets, the motif features are highly predictive for the classes at the fi rst level of the class hierarchy being selected on average in 90.6% of the time. In fact, the only dataset where other type of protein representation is selected at this level is the GPCR-Prosite dataset, where the LD representation is selected 75% of the time. For the other three class levels, it seems that the motifs are often selected for the EC datasets, while a combination of alignment-independent features are selected for GPCRs. An explanation for this was presented in [13] were the authors claim that there are several instances where the application of alignment-free techniques have been proven to be more effective than alignment-based techniques. And the GPCRs are an example of this, because they have a great structural and/or functional homology but a low degree of sequence similarity.

For the multiple-motif datasets presented in Table 6 the same conclusions can be drawn. That is, the motif features are highly discriminative at the 1st level of the class hierarchy, specially the Interpro motifs. There is a signi fi cant difference in the number of times that motif-based and alignment-independent features are selected for the Enzyme and GPCRs datasets. These results con fi rm that the Sel. R. approach effectively determines which protein representation is the best to be used with each classi fi er across different levels in the class hierarchy structure. 5.2. Impact of the different classi fi ers
It is a well-known fact in machine learning that there is  X  X o free-lunch X , i.e. a classi fi er which is the best for all applications do not exist. Recall that in this work we are employing the selective classi fi er approach with three classi fi cation algorithms: k-NN, SVM and NB. To measure the performance of the classi fi ers we consider their average ranking over all datasets and over all representations (computed by the Friedman statistical test, considering the hierarchical f-measure values). The resulting ranking is: Sel. C. (1.5454), Knn (1.560606), SVM (3.3181), NB (3.5757).

Again we employ the Shaffer static post-hoc test and the graphical representation of the result of the test is shown in Fig. 8. The analysis of the results in Fig. 8 shows that the Sel. C. and Knn are both (statistically signi fi cant) better than SVM and NB, but there is no statistically signi fi cant difference between the results of Sel. C. and KNN. Also, there is no st atistically signi fi cant difference between the results of SVM and NB.

Considering we are using the Sel. C. approach and it gives results just slightly better than the Knn method the Knn classi fi er is almost always chosen. Table 7 presents the relative classi fi er importance for each dataset, i.e. the number of times a particular classi fi er is selected at each class level. The analysis of Table 7 reveals that at the fi rst level the Knn classi fi er is selected in about 90% of the experiments. This result corroborates with the experiments reported in [38] where for one GPCR dataset the Knn classi fi er was always selected at the fi rst class level. For the other class levels it seems that, although the Sel. C. approach actually selects different classi fi ers, this does not impact signi fi cantly on the results. Other studies on hierarchical protein function prediction that employed the Sel. C. approach achieved similar conclusions [24,37], i.e. the Sel. C. is better than most classi fi ers but is not statistically signi fi cantly different from a Knn classi fi er, even though the former employs several classi fi ers, which has the disadvantageof considerably increasing the training time of the hierarchical classi fi cation system. Therefore, it seems that the use of the Sel. C. approach does not bring the same bene fi ts as the Sel. R. approach.
 Moreover, the negative impact of using a bad representation even with a good classi fi er (e.g. 5z with Knn on EC-Intepro has an hierarchical f-measure of 42.36%) seems to be greater than the impact of using a bad classi fi er with a good representation (e.g. Motif with NB on EC-Interpro has an hierarchical f-measure of 77.01%). Interestingly, most papers in protein function prediction are more concerned with trying different classi fi cation algorithms than different features and their impact on predictive accuracy. 6. Conclusions
In this work we presented an empirical study analyzing the impact of different protein representations and different types of classi fi cation algorithms for the task of hierarchical protein function prediction. We have employed 8 types of protein representation, 4 of which are alignment-independent representations computed from the protein sequence: 5z-values (5z), 15z-values (15z), Amino Acid Composition (AA) and Local Descriptors (LD); and 4 alignment-dependent protein signatures (motifs) from the biological databases Interpro, Pfam, Prints and Prosite. To perform the classi fi cation we have used 3 classi fi ers: k-NN, SVM and Naive Bayes and 3 selective approaches: one fi xing the classi fi er for all nodes in the class hierarchy and selecting the best representation at each class node, one fi xing the representation and selecting the best classi fi er at each class node, and one that selects the best match of representation and classi fi er at each node of the class hierarchy.

We have carried out the experiments on 10 datasets, being 5 datasets with G-Protein Coupled Receptors (GPCR) proteins and 5 with Enzymes. Our experimental results show that in general, regardless of the type of protein: 15 z-values are better than 5 z-values; AA is a very good descriptor with k-NN since it is simple and provides better results than z-values; LD is the best alignment-independent representation that can be computed directly from the protein sequence.
 Considering the results speci fi cally for GPCRs, the LD provides very good results (except for the GPCR-Interpro, in which its results are similar to the results of the motif representation) considering they can be computed directly from the sequence. Concerning the results speci fi c to the EC datasets the motif representation performs better than the alignment independent features computed from the sequence. The fact that these better results of alignment-independent features was observed for GPCRs but not for enzymes is possibly explained by the fact that GPCRs have a great structural and/or functional homology but a low degree of sequence similarity, which does not seem to be the case for enzymes.
Therefore, our recommendation (based on our experimental results) is that when using alignment-independent features derived from the sequence, we suggest the use of Local Descriptors. When motif features are available, we recommend the use of the Interpro entries as they provide in general better results than the other types of motifs for GPCRs and all motif features are roughly equally effective for Enzyme classi fi cation.

Future research would include performing experiments with other types of protein representations, more classi fi ers and with other types of proteins. Another direction for future research is to perform more controlled experiments to see if the number of features has a signi fi cant in fl uence on the effectiveness of a particular type of feature: e.g. the z-values representation has a very small number of features, what if z-values were computed for the same 10 regions as the LD approach (considerably increasing the number of z-value features)? Acknowledgment
The fi rst author is fi nancially supported by CAPES  X  a Brazilian research-support agency (process number 4871-06-5).
 References
