 Data quality has been addressed in different areas, such as statistics, management science, and computer science [1]. Dirty data is the main reason to cause data quality. Many surveys reveal dirty data exists in most database systems. The consequences of dirty data may be severe. Having uncertain, duplicate or inconsistent dirty data leads example, it is reported [2] that dirty data in retail databases alone costs US consumers $2.5 billion a year. There fore, several techniques have been developed to process dirty data to reduce the harm of dirty data. Existing work on processing dirty data can be divided into two broad categories. inconsistencies from data to improve data quality. However, data cleaning cannot information. Besides this, existing data cleaning techniques are generally time-consuming. Therefore, some researchers propose algorithms in the other category, to perform queries on dirty data directly and obtain query results with clean degree from the dirty data [4-6].

Several models for dirty data management without data cleaning have been relational database model in which one tuple represents an entity. This model can better reflect the real world entities and their relationships. 
In applications, the different representations of the same real-word entities often data sources need to be integrated [10-11] . In the entity-based relational d atabase, for the duplicate data referring to the same real-world entity, we combine these data, and for inconsistent data (or uncertain data), we endow each of them a value (we call it as quality degree) which reflects its quality. Example 1 shows this process. their representations are different. By preforming entity resolution and combining data, which implies that the value of one attribute in a tuple may be uncertain, and it degree in accordance with their proportion, as shown in Table 2. In tuples 1, 3 and 6, other quality degrees can be given. Then we get an entity tuple as shown in Table 2.
As Example1 shows, the entity-based relational database ingeniously processes the this model, query optimization techniques are in demand. As the base of the query optimization, the estimation technique computing the size of the results of an operator query estimation for traditional relational database management systems. Most approaches for query estimation are based on histogram [14], which records data distributions. However, the traditional histograms are not suitable for entity-based relational database, and often lead to overestimation, especially for range queries. One reason is that query processing on the entity-based relational database need to consider the effect of the quality degrees of values, but the traditional histograms are only concerned about the attribute value without the consideration of the quality degree. The other reason is that traditional approaches based on histogram often lead contain multiple values in the entity-based relational database, if all values are overestimation occurs. 
Therefore, the traditional query estimation approaches based on histogram cannot be applied to our problem. Unfortunately, there is no work for the query estimation of the entity-based relational database. New query estimation approaches are in demand. Our Contributions: In this paper, we propose new range query estimation methods suitable for entity-based relational database. As we know, this is the first paper considering such problem. These algorithms are demonstrated in details and the complexity of these algorithms is analyzed. We theoretically prove our algorithms are unbiased. Last, we experimentally validate the e ffectiveness of our algorithms and show that our methods are accurate. 
The rest of this paper is organized as follows. Section 2 introduces entity-based relational database model and some related conceptions. Section 3 presents our range query estimation methods. We show our experimental results in Section 4. We conclude our paper and discuss the future work in Section 5. 2.1 Entity-Based Relational Database Model We firstly define the Uncertain Attribute Value in Definition 1. An uncertain attribute degrees. Then we give the definition of Entity in Definition 2. Entity is the basic unit of storage in the entity-based relational database system, containing a set of uncertain attribute values.
 pair  X  = {  X  X , X  X  | v is possible value of the attribut e and p is the quality degree of the value v }.
 Definition 2 (Entity): An entity is a pair  X  X , X  X  X  X  X  , where  X  is a set of uncertain entity-ID).
 degree dimension, we need to define a new conception to reflect whether a tuple satisfies a query, and we call this conception Similarity . is a constraint, the similarity be tween them is defined as follows: For a selection query with a constraint  X  X  X  ( for the convenience, we use this We use an example to illustrate it. 
With the support of the conceptions, some query operators are defined. 2.2 Operators less interesting than higher similarity answers, we consider those results with a system sets a default value) as unsatisfied for a query. Therefore, the results of queries given by  X  X   X   X  can be defined as an operator as follows: based relational database. In next section, we will give our approaches in details. preliminary query estimation method in Section 3.1, and this method can well estimate unbounded range query (e.g.,  X  X   X   X  or  X  X   X   X  ) result size, but for general range queries (e.g.,  X   X   X  X  X  X  X   X  ), it often leads to underestimation. Then, a more solve the underestimation problem which the former method encounters. 3.1 Preliminary Range Query Estimation Method queries on entity-based relational database management system for two reasons that with satisfying  X  X  X   X   X  X  X   X   X  X  , which means that  X  satisfies the following relationship: If all possible values of an uncertain value are sorted in database system, the where  X   X   X   X   X   X   X   X   X   X   X   X  X  , and return the values satisfying  X   X   X   X   X   X  X  . 
Fig. 1 shows an example of the cumulative distribution functions (CDF) of several tuples on attribute A, whose corresponding values are shown in Table 3. In the figure, each stacked line represents one tuple. The meaning of every stacked line is like the cumulative distribution function of every uncertain value. Fo r examp le, the point P on query Q (  X  X   X   X   X  ), the total number of tuples which satisfy query Q can be estimated  X  X  X   X   X 1 X  X  X , . Histogram Structure Based on the above discussion, we define a basic two-dimensional histogram. The range  X   X 1 X   X   X  X   X   X  X  X 1 X  X  X  X ,  X   X  , where  X   X  and  X   X  are the widths of histogram bucket along x and y axis. Each bucket has a value, which stores the height of this bucket that records the number of tuples whose stacked lines intersect this bucket.
 number of the stacked line inflection points in buckets and do not exceed them. If there is no inflection points in bucket  X   X  , the estimation for queries which are located in  X   X  ensure that the number of the inflection points in each bucket  X   X  is small enough. 
In our approach, the histogram is firstly partitioned into  X  equal-width buckets, and we set the number of the inflection points in each bucket should not exceed  X  (  X / X  X  X  , where  X  is the total number of inflection points, which equals the number of all possible attribute values). When a bucket contains more than  X  inflection points, this bucket is partitioned into  X  equal-width buckets (generally,  X  X  X  , and  X  can be considered as a constant) and we set each new bucket containing  X / X  inflection points. partitioned until that all buckets contain less than  X  inflection points. 
We now present the histogram construction algorithm. To facilitate the description of algorithm, we firstly summarize the main notations that will be used in our paper in Table construction. Note that, we assume all po ssible values of an uncertain value are  X   X   X   X  bucket, and when the size of some bucket exceeds  X  , it is partitioned into  X  equal-width where symbol  X   X  represents the number of inflection points in bucket  X   X  . Theorem 1: The time complexity of Algorithm 1 is  X  X  X  X  X  X  X  X  X  X  X  and the space complexity is  X  X  X  X  X  X  . histogram buckets, where m is the length of the histogram in x dimension. In the worst case, partition occurs per  X / X 1 X  X  X  X  X  tuples, and partition times does not exceed  X 1 X  X  X  X / X  X  (i.e.,  X  X  X  X 1 X / X  X  X  X  X / X  ). Each partition adds  X 1 X  buckets and adjusts  X  buckets along y axis, so  X  X   X   X 1 X   X   X 1 X  X  X  X  X  X  X  X  X 1 X  X  X  X  X / X  X  . Thus the time complexity is  X   X   X  X 1 X  X  X  X  X   X   X  X   X   X 1 X  X  X  X / X  X  X  X   X  and the space complexity is  X  X  X  X 1 X  X  X  X  X  X  X  . Thus the time complexity is  X  X  X  X  X  X  X  X  X  X  X  and the space complexity is  X  X  X  X  X  X  , because  X  can be considered as a constant. Query Estimation Method With the histogram structure, we can easily estimate query result size. Given a query  X  X   X   X   X  , query result size is estimated as the sum of the heights of the buckets where  X  in details. 
However, this algorithm is only applicable for the unbounded range queries in form of  X  X   X   X   X  . For anther unbounded range queries in the form of  X  X   X   X   X  , we need to perform an equivalent transformation to make Algorithm 2 suitable for such form. with a modification of the loop range in line 4 , and the loop range should be modified tends to infinity. Theorem 2: The estimation method in Algorithm 2 is unbiased when  X  tends to infinity.
 Proof: To facilitate the proof, we assume  X 2 X  , for other cases, the proof process is similar. As proved in Theorem 1, partition times does not exceed  X 2 (i.e.,  X  X  X  X / X  X  1 X  ), and each partition adds 1 (i.e.,  X 1 X  ) buckets. Given a query  X  X   X   X   X  , we make the following assumptions. First,  X   X  falls each bucket with equal probability. Second,  X  times partitions occur. Last,  X  buckets contain more than  X  inflection points  X  X  X  . We have known the estimation error does not exceed the number of inflection points in bucket which  X   X  is located in. Hence th e expectation of estimation error is:  X  X  X  X  Therefore, when  X  tends to infinity, the expectation of estimation error tends to 0, and this approach is unbiased. We have discussed the unbounded range queries. Consider the general range query Q (  X   X   X  X  X  X  X   X  ), and that is  X  X  X   X   X   X   X  X  X  X  X   X   X   X  X  . The unbounded range queries can be considered as a special cas e of the general range query. To estimate the general range query result size, with the application of the techniques in this section, a na X ve method is proposed. Firstly, the numbers of tuples that satisfy query Q1 (  X  X   X   X   X  ) and 2 X  respectively. We can use 1 X 2 X  X  as the estimation of query Q (  X   X   X  X  X  X  X   X  ). Clearly, if we do not consider the threshold, this method is correct. However, it often leads to underestimation with the consideration of the effect of the similarity this na X ve method, we propose more accurate query range estimation method in next section. 3.2 Accurate Range Query Estimation solve the underestimation problem discussed in Section 3.1. In order to adapt to general range queries, we add another dimension to the histogram proposed in Section additional dimension ( z axis) represents the beginning of the query. Therefore, given a general range query  X  (  X   X   X  X  X  X  X   X  ), we can estimate the size of query result set by  X 1 X  X  X  and  X  X  X   X  , similar to Fig. 1. That is equivalent to executing a query  X  X  (  X  X   X   X   X  ) on the plane, where  X  X  X   X  . 
We call such new histogram as improved histogram. In this histogram, every plane on z axis is a basic histogram proposed in Section 3.1, corresponding to the constraint to  X  X   X  X  X  X   X  X   X  X  X  X   X / X  ). The detailed algorithms for constructing this improved histogram and estimating the result size of a general range query are respectively presented in Algorithm 3 and Algorithm 4 . Compared with Algorithm 1 , Algorithm 3 only adds another layer of loops on z axis, but this improved histogram structure can give more accurate estimation than the basic histogram of Section 3.1. Theorem 3 gives the time and space complexity of the construction algorithm, and Theorem 4 proves this estimation algorithm using this improved histogram is also unbiased when  X  tends to infinity. Theorem 3: The time complexity of Algorithm 3 is  X  X  X   X   X  X  X  X  X  X  X  X  and the space complexity is  X  X  X   X   X  X  with the assumption:  X   X   X  X  X  X   X  X  X  X   X  X   X  X  X  X   X / X  ) .
 Proof: Compared with Algorithm 1 , this algorithm only adds another dimension, and the length of this dimension is  X  . Therefore, similarly the analysis of the complexity complexity is  X  X  X   X   X  X  . Theorem 4: The estimation method in Algorithm 4 is unbiased when  X  tends to infinity.
 Proof: Compared with the basic histogram of Section 3.1, this improved histogram with more detailed information can get a more accurate estimation for general queries. Therefore, with the conclusion of Theorem 2, the estimation method using this improved histogram is also unbiased when  X  tends to infinity. In this section, we study the performance of our proposed algorithms experimentally. Our experiments are conducted on a 2.93 GHz Inter(R) Core(TM)2 Duo CPU with 2 GB main memory. 4.1 Data Sets parts of synthetic data sets and real-world data sets. Table 5 summarizes some information about these data sets. Synthetic Data Sets: We generate the synthetic data sets and each tuple has a Tuple ID, along with an uncertain value. The number of the possible values of an uncertain value is randomly generated from 0.01 to 1 and these quality degrees sum up to 1 for an uncertain value. To evaluate the robust of our approaches, we consider three distributions and zipfian distribution. Real-World Data Sets: One of the most important applications of the histograms is for those cases in which the distribution of the data is unknown or cannot be simply consider the real-world data sets: eCommerce data. We respectively collect book information about Computers &amp; Technology from eBay (http://www.ebay.com) and Amazon (http://www.amazon.com). After the processes for original data, we get the real data set with 10053 entities. In this da ta set, each tuple represents a book which experiments by building the histograms on attribute price . 4.2 Query Set and Error Metric Without loss of generality, we ran every experiment on a variety of queries. All its domain. We measure the error of estimation made by histograms on the above query set by using the average of the relative error:  X   X   X  our experiments, we randomly generate 100 queries for each query set. 4.3 Experimental Results Our experiments compare the two estimation algorithms proposed in Section 3. We denote them by 1 X  (in Section 3.1) and 2 X  (in Section 3.2). Without explicit explanation , the default value of the similarity threshold is 0.2 for all experiments; the default size of initial granularity  X  of partition and granularity  X  of repartition are 50 and 4; the default size of bucket width on similarity dimension  X   X  is 0.1. 4.3.1 Effect of Data Distribution For query estimation algorithms based on histogram, data distribution is an important factor affecting the accuracy of estimation. Fig. 2 shows the effect of data distribution and algorithm 2 X  is always more accurate than algorithm 1 X  . 4.3.2 Effect of Data Set Size Fig. 3 shows the effect of data set size. In this experiment, we vary the data set size by selecting the desired number of tuples from the synthetic data set T , and the data set sizes are 100K, 200K, 400K, 600K, 800K and 1000K (for eCommerce data, data set error is not sensitive to the dataset size for both two algorithms. 4.3.3 Effect of Threshold For the entity-based relational database, the threshold plays an important role in query processing. Fig. 4 shows the impact of the threshold with different thresholds from 0.1 to 0.9. For algorithm 2 X  , no matter how the data is distributed, the change is not very significant. Therefore, algorithm 2 X  is relatively stable for different thresholds. with the threshold . When the threshold is in  X 0.4,0.6 X  , the relative errors is minimum. Hence the accuracy of algorithm 1 X  is related to threshold  X  as mentioned in Section 3.1, and it can give accurate estimations when  X  X  X 0.4,0.6 X   X  . 4.3.4 Effect of Granularity of Partition relative error both algorithm 1 X  and 2 X  . As initial granularity of partition  X  increases, the relative error decreases. This phenomenon empirically verifies the conclusion that our approaches are unbiased when  X  tends to infinity. 4.3.5 Effect of Width of Query Range In Section 3.1, we mentioned that the accuracy of the estimation using the basic width of query range. Fig. 6 gives the experimental results. We can observe that with an increasing width of query range, the relative error has a decreasing trend for both accurate . As a result, the wider query range is, the more accurate estimation is. Entity-based relational database is a practical method for dirty data management. Intermediate result size estimation is crucial for the query optimization for entity-based relational database. However, traditional estimation methods cannot be applied to this problem directly. In this paper, we study this problem. To solve this problem, we propose two histogram-based methods for different form of queries and requirements. It is proven that they are unbiased. The experimental results validate the effectiveness of our algorithms, and they can indeed give good estimations for range the cost of estimation, especially the estimation of join result size. 
