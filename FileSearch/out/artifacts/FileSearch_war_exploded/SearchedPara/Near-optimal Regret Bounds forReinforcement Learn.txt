 In a Markov decision process (MDP) M with finite state space S and finite action space A , a learner in state s  X  X  needs to choose an action a  X  X  . When executing action a in state s , the learner Reinforcement learning of MDPs is a standard model for learning with delayed feedback. In contrast to important other work on reinforcement learning  X  where the performance of the learned policy is considered (see e.g. [1, 2] and also the discussion and references given in the introduction of [3])  X  we are interested in the performance of the learning algorithm during learning . For that, we compare the rewards collected by the algorithm during learning with the rewards of an optimal policy. In this paper we will consider undiscounted rewards. The accumulated reward of an algorithm A after T steps in an MDP M is defined as average reward can be maximized by an appropriate stationary policy  X  : S X  X  which defines an optimal action for each state [4].
 The difficulty of learning an MDP does not only depend on its size (given by the number of states and actions), but also on its transition structure. In order to measure this transition structure we propose a new parameter, the diameter D of an MDP. The diameter D is the time it takes to move policy  X  is executed on MDP M with initial state s . Then the diameter of M is given by A finite diameter seems necessary for interesting bounds on the regret of any algorithm with respect to an optimal policy. When a learner explores suboptimal actions, this may take him into a  X  X ad part X  of the MDP from which it may take about D steps to reach again a  X  X ood part X  of the MDP. appears in the regret bound.
 For MDPs with finite diameter (which usually are called communicating , see e.g. [4]) the optimal average reward  X   X  does not depend on the initial state (cf. [4], Section 8.3.3), and we set The optimal average reward is the natural benchmark for a learning algorithm A , and we define the total regret of A after T steps as 1 In the following, we present our reinforcement learning algorithm U CRL 2 (a variant of the UCRL algorithm of [5]) which uses upper confidence bounds to choose an optimistic policy. We show  X ( p D |S||A| T ) on the total regret of any learning algorithm is given as well. These results establish the diameter as an important parameter of an MDP. Further, the diameter seems to be more natural than other parameters that have been proposed for various PAC and regret bounds, such as the mixing time [3, 6] or the hitting time of an optimal policy [7] (cf. the discussion below). 1.1 Relation to previous Work We first compare our results to the PAC bounds for the well-known algorithms E 3 of Kearns, Singh [3], and R-Max of Brafman, Tennenholtz [6] (see also Kakade [8]). These algorithms achieve ing time T mix  X  (see below). As the polynomial dependence on  X  is of order 1 / X  3 , the PAC bounds translate into T 2 / 3 regret bounds at the best. Moreover, both algorithms need the  X  -return mixing It is easy to construct MDPs of diameter D with T mix  X   X  D/ X  . This additional dependency on  X  further increases the exponent in the above mentioned regret bounds for E 3 and R-max. Also, the exponents of the parameters |S| and |A| in the PAC bounds of [3] and [6] are substantially larger than in our bound.
 The MBIE algorithm of Strehl and Littman [9, 10]  X  similarly to our approach  X  applies confidence bounds to compute an optimistic policy. However, Strehl and Littman consider only a discounted reward setting, which seems to be less natural when dealing with regret. Their definition of regret measures the difference between the rewards 2 of an optimal policy and the rewards of the learning algorithm along the trajectory taken by the learning algorithm . In contrast, we are interested in the regret of the learning algorithm in respect to the rewards of the optimal policy along the trajectory of the optimal policy .
 Tewari and Bartlett [7] propose a generalization of the index policies of Burnetas and Katehakis [11]. These index policies choose actions optimistically by using confidence bounds only for the estimates in the current state. The regret bounds for the index policies of [11] and the OLP algorithm of [7] are asymptotically logarithmic in T . However, unlike our bounds, these bounds depend on the gap between the  X  X uality X  of the best and the second best action, and these asymptotic bounds also hide an additive term which is exponential in the number of states. Actually, it is possible to prove a corresponding gap-dependent logarithmic bound for our U CRL 2 algorithm as well (cf. Remark 4 below). This bound holds uniformly over time and under weaker assumptions: While [7] and [11] consider only ergodic MDPs in which any policy will reach every state after a sufficient number of steps, we make only the more natural assumption of a finite diameter. We summarize the results achieved for our algorithm U CRL 2 which is described in the next section, and also state a corresponding lower bound. We assume an unknown MDP M to be learned, with S := |S| states, A := |A| actions, and finite diameter D := D ( M ) . Only S and A are known to the learner, and U CRL 2 is run with parameter  X  .
 of U CRL 2 is bounded by for a constant c 1 which is independent of M , T , and  X  .
 It is straightforward to obtain from Theorem 2 the following sample complexity bound. Corollary 3. With probability 1  X   X  the average per-step regret is at most  X  for any steps, where c 2 is a constant independent of M .
 Remark 4. The proof method of Theorem 2 can be modified to give for each initial state s and T &gt; 1 an alternative upper bound on the expected regret, average reward and the second best average reward achievable in M .
 These new bounds are improvements over the bounds that have been achieved in [5] for the original UCRL algorithm in various respects: the exponents of the relevant parameters have been decreased considerably, the parameter D we use here is substantially smaller than the corresponding mixing time in [5], and finally, the ergodicity assumption is replaced by the much weaker and more natural assumption that the MDP has finite diameter.
 The following is an accompanying lower bound on the expected regret.
 Theorem 5. For some c 4 &gt; 0 , any algorithm A , and any natural numbers S, A  X  10 , D  X  20 log A S , and T  X  DSA , there is an MDP 3 M with S states, A actions, and diameter D , such that for any initial state s  X  X  the expected regret of A after T steps is In a different setting, a modification of U CRL 2 can also deal with changing MDPs.
 Remark 6. Assume that the MDP (i.e. its transition probabilities and reward distributions) is al-lowed to change ` times up to step T , such that the diameter is always at most D (we assume an initial change at time t = 1 ). In this model we measure regret as the sum of missed rewards com-pared to the ` policies which are optimal after the changes of the MDP. Restarting U CRL 2 with parameter  X /` 2 at steps d i 3 /` 2 e for i = 1 , 2 , 3 . . . , this regret is upper bounded by with probability 1  X  2  X  .
 MDPs with a different model of changing rewards have already been considered in [12]. There, the transition probabilities are assumed to be fixed and known to the learner, but the rewards are allowed to change in every step. A best possible upper bound of O ( stationary policy, given all the reward changes in advance, is derived. Our algorithm is a variant of the UCRL algorithm in [5]. As its predecessor, U CRL 2 implements the paradigm of  X  X ptimism in the face of uncertainty X . As such, it defines a set M of statistically plausible MDPs given the observations so far, and chooses an optimistic MDP  X  M (with respect to the achievable average reward) among these plausible MDPs. Then it executes a policy  X   X  which is (nearly) optimal for the optimistic MDP  X  M .
 More precisely, U CRL 2 (Figure 1) proceeds in episodes and computes a new policy  X   X  k only at the beginning of each episode k . The lengths of the episodes are not fixed a priori, but depend on transition probabilities and mean rewards from the observations made before episode k . In Step 4, a set M k of plausible MDPs is defined in terms of confidence regions around the estimated mean the true MDP M is in M k . In Step 5, extended value iteration (see below) is used to choose a near-current policy has been chosen in episode k equally often as before episode k . Thus, the total number of occurrences of any state-action pair is at most doubled during an episode. The counts v k ( s, a ) keep track of these occurrences in episode k . 4 3.1 Extended Value Iteration In Step 5 of the U CRL 2 algorithm we need to find a near-optimal policy  X   X  k for an optimistic MDP. While value iteration typically calculates a policy for a fixed MDP, we also need to select an op-timistic MDP  X  M k which gives almost maximal reward among all plausible MDPs. This can be achieved by extending value iteration to search also among the plausible MDPs. Formally, this can be seen as undiscounted value iteration [4] on an MDP with extended action set. We denote the state the set of transition probabilities  X  p  X | s, a satisfying condition (2).
 a linear optimization problem over the convex polytope P ( s, a ) . This implies that only the finite number of vertices of the polytope need to be considered as extended actions, which guarantees convergence of the value iteration. 5 The value iteration is stopped when which means that the change of the state values is almost uniform and actually close to the average reward of the optimal policy. It can be shown that the actions, rewards, and transition probabilities condition (3) of algorithm U CRL 2. In the following we present an outline of the main steps of the proof of Theorem 2. Details and the complete proofs can be found in the full version of the paper [13]. We also make the assumption Considering unknown stochastic rewards adds little to the proof and only lower order terms to the regret bounds. We also assume that the true MDP M satisfies the confidence bounds in Step 4 of algorithm U CRL 2 such that M  X  X  k . This can be shown to hold with sufficiently high probability (using a union bound over all T ).
 We start by considering the regret in a single episode k . Since the optimistic average reward  X   X  k rewards of policy  X   X  k . By the choice of  X   X  k and  X  M k in Step 5 of U CRL 2,  X   X  k  X   X   X   X  1 / regret  X  k during episode k is bounded as The sum over k of the second term on the right hand side is O ( further in this proof sketch. The first term on the right hand side can be rewritten using the known deterministic rewards r ( s, a ) and the occurrences of state action pairs ( s, a ) in episode k , 4.1 Extended Value Iteration revisited To proceed, we reconsider the extended value iteration in Section 3.1. As an important observation for our analysis, we find that for any iteration i the range of the state values is bounded by the diameter of the MDP M , value iteration. The diameter of this extended MDP is at most D as it contains the actions of the true be achieved by the following policy: First follow a policy which moves from s 0 to s 1 most quickly, which takes at most D steps on average. Then follow the optimal i -step policy for s 1 . Since only D For the convergence criterion (5) it can be shown that at the corresponding iteration for all s  X  X  , where  X   X  k is the average reward of the policy  X   X  k chosen in this iteration on the optimistic MDP  X  M k . 7 Expanding u i +1 ( s ) according to (4), we get and hence Defining r k := r k s,  X   X  k ( s ) s as the (column) vector of rewards for policy  X   X  k ,  X  P k :=  X  p k ( s 0 | s,  X   X  k ( s )) s,s 0 as the transition matrix of  X   X  k on as term on the right hand side of (8) is of lower order, and by (7) we have 4.2 Completing the Proof matrix P k of  X   X  k in the true MDP M , we get for this martingale. This yields with high probability, which gives a lower order term in our regret bound. Thus, our regret bound is mainly determined by the first term in (11). Since  X  M k and M are in the set of plausible MDPs M k , this term can be bounded using condition (2) in algorithm U CRL 2: P i&lt;k v i ( s, a ) . By the condition of the while-loop in Step 6 of algorithm U CRL 2, we have that v ( s, a )  X  N k ( s, a ) . Summing (12) over all episodes k we get Here we used for (13) that where X k = max n 1 , P k i =1 x i o and 0  X  x k  X  X k  X  1 , and we used Jensen X  X  inequality for (14). Noting that Theorem 2 holds trivially true for T  X  A gives the bound of the theorem. We first consider an MDP with two states s 0 and s 1 , and A 0 = b ( A  X  1) / 2 c actions. For each while the average reward of any other policy is 1 2 . Thus the regret suffered by a suboptimal action in state s 0 is  X (  X / X  ) . The main observation for the proof of the lower bound is that any algorithm reliably.
 Considering k := b S/ 2 c copies of this MDP where only one of the copies has such a  X  X ood X  times to detect the  X  X ood X  action. Setting  X  = p  X kA 0 /T , suboptimal actions need to be taken  X  kA 0  X / X  2 =  X ( T ) times which gives  X ( T  X / X  ) =  X ( Finally, we need to connect the k copies into a single MDP. This can be done by introducing A 0 + 1 root, A 0 actions to go toward the leaves). The diameter of the resulting MDP is at most 2( D/ 10 + have constructed an MDP with  X  S states,  X  A actions, and diameter  X  D which forces regret  X (  X  Acknowledgments This work was supported in part by the Austrian Science Fund FWF (S9104-N13 SP4). The research leading to these results has received funding from the European Community X  X  Seventh Framework Programme (FP7/2007-2013) under grant agreements n  X  216886 (PASCAL2 Network of Excel-lence), and n  X  216529 (Personal Information Navigator Adapting Through Viewing, PinView). This publication only reflects the authors X  views.
 References
