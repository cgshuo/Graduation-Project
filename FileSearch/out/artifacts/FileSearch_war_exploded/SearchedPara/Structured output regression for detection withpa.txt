 There has been a steady increase in the performance of object category detection as measured by the annual PASCAL VOC challenges [3]. The training data provided for these challenges specifies if an object is truncated  X  when the provided axis aligned bounding box does not cover the full extent of the object. The principal cause of truncation is that the object partially lies outside the image area. Most participants simple disregard the truncated training instances and learn from the non-truncated ones. This is a waste of training material, but more seriously many truncated instances are missed in testing, significantly reducing the recall and hence decreasing overall recognition performance. In this paper we develop a model (Fig. 1) which explicitly accounts for truncation in both train-ing and testing, and demonstrate that this leads to a significant performance boost. The model is specified as a joint kernel and learnt using an extension of the structural SVM with latent variables framework of [13]. We use this approach as it allows us to address a second deficiency of the pro-vided supervision  X  that the annotation is limited to axis aligned bounding boxes, even though the objects may be in plane rotated so that the box is a loose bound. The latent variables allow us to specify a pose transformation for each instances so that we achieve a spatial correspondence be-tween all instances with the same aspect. We show the benefits of this for both the learnt model and testing performance.
 Our model is complementary to that of Felzenszwalb et al. [4] who propose a latent SVM frame-work, where the latent variables specify sub-part locations. The parts give their model some toler-ance to in plane rotation and foreshortening (though an axis aligned rectangle is still used for a first stage as a  X  X oot filter X ) but they do not address the problem of truncation. Like them we base our implementation on the efficient and successful HOG descriptor of Dalal and Triggs [2].
 Previous authors have also considered occlusion (of which truncation is a special case). Williams et al. [11] used pixel wise binary latent variables to specify the occlusion and an Ising prior for spatial coherence. Inference involved marginalizing out the latent variables using a mean field approxima-tion. There was no learning of the model from occluded data. For faces with partial occlusion, the resulting classifier showed an improvement over a classifier which did not model occlusion. Others have explicitly included occlusion at the model learning stage, such as the Constellation model of Fergus et al. [5] and the Layout Consistent Random Field model of Winn et al. [12]. There are nu-merous papers on detecting faces with various degrees of partial occlusion from glasses, or synthetic truncations [6, 7].
 Our contribution is to define an appropriate joint kernel and loss function to be used in the context of structured output prediction. We then learn a structured regressor, mapping an image to a list of objects with their pose (or bounding box), while at the same time handling explicitly truncation and multiple aspects. Our choice of kernel is inspired by the restriction kernel of [1]; however, our kernel performs both restriction and alignment to a template, supports multiple templates to handle different aspects and truncations, and adds a bias term which significantly improves performance. We refine pose beyond translation and scaling with an additional transformation selected from a finite set of possible perturbations covering aspect ratio change and small in plane rotations. Instead of explicitly transforming the image with each element of this set (which would be prohibitively ex-pensive) we introduce a novel approximation based on decomposing the HOG descriptor into small blocks and quickly rearranging those. To handle occlusions we selectively switch between an object descriptor and an occlusion descriptor. To identify which portions of the template are occluded we use a field of binary variables. These could be treated as latent variables; however, since we consider here only occlusions caused by the image boundaries, we can infer them deterministically from the position of the object relative to the image boundaries. Fig. 1 illustrates various detection examples including truncation, multiple instances and aspects, and in-plane rotation.
 In training we improve the ground-truth pose parameters, since the bounding boxes and aspect asso-ciations provided in PASCAL VOC are quite coarse indicators of the object pose. For each instance we add a latent variable which encodes a pose adjustment. Such variables are then treated as part of learning using the technique presented in [13]. However, while there the authors use the CCCP algo-rithm to treat the case of margin rescaling, here we show that a similar algorithm applies to the case of slack rescaling. The resulting optimization alternates between optimizing the model parameters given the latent variables (a convex problem solved by a cutting plane algorithm) and optimizing the latent variable given the model (akin to inference). The overall method is computationally efficient both in training and testing, achieves very good performances, and it is able to learn and recognise truncated objects. Our purpose is to learn a function y = f ( x ) , x  X  X , y  X  Y which, given an image x , returns the poses y of the objects portrayed in the image. We use the structured prediction learning framework of [9, 13], which considers along with the input and output variables x and y , an auxiliary latent variable h  X  X  as well (we use h to specify a refinement to the ground-truth pose parameters). The function f is then defined as f ( x ; w ) =  X  y x ( w ) where and  X ( x , y , h )  X  R d is a joint feature map. This maximization estimates both y and h from the data x and corresponds to performing inference. Given training data ( x 1 , y 1 ) ,..., ( x N , y N ) , the parameters w are learned by minimizing the regularized empirical risk
R ( w ) = Here  X ( y i , y , h )  X  0 is an appropriate loss function that encodes the cost of an incorrect prediction. In this section we develop the model  X ( x , y , h ) , or equivalently the joint kernel function for the case of a single unoccluded instance in an image including scale and perturbing transforma-tions, then generalise this to include truncations and aspects; and finally to multiple instances. An appropriate loss function  X ( y i , y , h ) is subsequently defined which takes into account the pose of the object specified by the latent variables. 2.1 Restriction and alignment kernel Denote by R a rectangular region of the image x , and by x | R the image cropped to that rectangle. appropriate kernel between images. The goal is that the joint kernel should be large when the two regions have similar appearance.
 Our kernel is similar, but captures both the idea of restriction and alignment . Let R 0 be a reference rectangle and denote by R ( p ) = g p R 0 the rectangle obtained from R 0 by a geometric transformation g p : R 2  X  R 2 . We call p the pose of the rectangle R ( p ). Let  X  x be an image defined on the reference rectangle R 0 and let H (  X  x )  X  R d be a descriptor (e.g. SIFT, HOG, GIST [2]) computed from the image appearance. Then a natural definition of the restriction and alignment kernel is where K descr is an appropriate kernel for descriptors, and H ( x ; p ) is the descriptor computed on the transformed patch x as H ( x ; p ) = H ( g  X  1 p x ) .
 Implementation with HOG descriptors. Our choice of the HOG descriptor puts some limits on the space of poses p that can be efficiently explored. To see this, it is necessary to describe how HOG descriptors are computed.
 The computation starts by decomposing the image x into cells of d  X  d pixels (here d = 8 ). The descriptor of a cell is the nine dimensional histogram of the orientation of the image gradient inside the cell (appropriately weighed and normalized as in [2]). We obtain the HOG descriptor of a rectangle of w  X  h cells by stacking the enclosed cell descriptors (this is a 9  X  w  X  h vector). Thus, given the cell histograms, we can immediately obtain the HOG descriptors H ( x,y ) for all the cell-aligned translations ( x,y ) of the dw  X  dh pixels rectangle. To span rectangles of different scales  X  &gt; 0 , and s is a discrete scale parameter. To further refine pose beyond scale and translation, here we consider an additional perturbation g t , indexed by a pose parameter t , selected in a set of transformations g 1 ,...,g T (in the experiments we use 16 transformations, obtained from all combinations of rotations of  X  5 and  X  10 degrees and scaling along x of 95%, 90%, 80% and 70%). Those could be achieved in the same manner as scaling by transforming the image g  X  1 t x for each one, but this would be very expensive (we would need to recompute the cell descriptors every time). Instead, we approximate such transformations by rearranging the cells of the template (Fig. 1 and 2). First, we partition the w  X  h cells of the template into blocks of m  X  m cells (e.g. m = 4 ). Then we transform the center of each block according to g t and we translate the block to the new center (approximated to units of cells). Notice that we could pick m = 1 (i.e. move each cell of the template independently), but we prefer to use blocks as this accelerates inference (see Sect. 4).
 Hence, pose is for us a tuple ( x,y,s,t ) representing translation, scale, and additional perturbation. Since HOG descriptors are designed to be compared with a linear kernel, we define 2.2 Modeling truncations If part of the object is occluded (either by clutter or by the image boundaries), some of the descriptor cells will be either unpredictable or undefined. We explicitly deal with occlusion at the granularity of the HOG cells by adding a field of w  X  h binary indicator variables v  X  { 0 , 1 } wh . Here v j = 1 means that the j -th cell of the HOG descriptor H ( x ,p ) should be considered to be visible, and v = 0 that it is occluded. We thus define a variant of (4) by considering the feature map where 1 d is a d -dimensional vector of all ones,  X  denotes the Kroneker product, and the Hadamard (component wise) product. To understand this expression, recall that H is the stacking of w  X  h 9-dimensional histograms, so for instance ( v  X  1 9 ) H ( x ,p ) preserves the visible cells and nulls the others. Eq. (5) is effectively defining a template for the object and one for the occlusions. Notice that v are in general latent variables and should be estimated as such. However here we note that the most severe and frequent occlusions are caused by the image boundaries (finite field of view). In this case, which we explore in the experiments, we can write v = v ( p ) as a function of the pose p , and remove the explicit dependence on v in  X  . Moreover the truncated HOG cells are undefined and can be assigned a nominal common value. So here we work with a simplified kernel, in which occlusions are represented by a scalar proportional to the number of truncated cells: 2.3 Modeling aspects A template model works well as long as pose captures accurately enough the transformations result-ing from changes in the viewing conditions. In our model, the pose p , combined with the robustness of the HOG descriptor, can absorb a fair amount of viewpoint induced deformation. It cannot, how-ever, capture the 3D structure of a physical object. Therefore, extreme changes of viewpoint require switching between different templates. To this end, we augment pose with an aspect indicator a (so that pose is the tuple p = ( x,y,s,t,a ) ), which indicates which template to use.
 Note that now the concept of pose has been generalized to include a parameter, a , which, differently from the others, does not specify a geometric transformation. Nevertheless, pose specifies how the model should be aligned to the image, i.e. by (i) choosing the template that corresponds to the aspect a , (ii) translating and scaling such template according to ( x,y,s ) , and (iii) applying to it the additional perturbation g t . In testing, all such parameters are estimated as part of inference. In training, they are initialized from the ground truth data annotations (bounding boxes and aspect labels), and are then refined by estimating the latent variables (Sect. 2.4). We assign each aspect to a different  X  X lot X  of the feature vector  X ( x ,p ) . Then we null all but the one of the slots, as indicated by a : where  X  a ( x ; p ) is a feature vector in the form of (6). In this way, we compare different templates for different aspects, as indicated by a .
 The model can be extended to capture symmetries of the aspects (resulting from symmetries of the objects). For instance, a left view of a bicycle can be obtained by mirroring a right view, so that the same template can be used for both aspects by defining where flip is the operator that  X  X lips X  the descriptor (this can be defined in general by computing the descriptor of the mirrored image, but for HOG it reduces to rearranging the descriptor components). The problem remains of assigning aspects to the training data. In the Pascal VOC data, objects are labeled with one of five aspects: front, left, right, back, undefined. However, such assignments may not be optimal for use in a particular algorithm. Fortunately, our method is able to automatically reassign aspects as part of the estimation of the hidden variables (Sect. 2.4 and Fig. 2). 2.4 Latent variables The PASCAL VOC bounding boxes yield only a coarse estimate of the ground truth pose parameters (e.g. they do not contain any information on the object rotation) and the aspect assignments may also be suboptimal (see previous section). Therefore, we introduce latent variables h = (  X p ) that encode an adjustment to the ground-truth pose parameters y = ( p ) . In practice, the adjustment  X p is a small variation of translation x,y , scale s , and perturbation t , and can switch the aspect a all together.
 We modify the feature maps to account for the adjustment in the obvious way. For instance (6) becomes 2.5 Variable number of objects: loss function, bias, training So far, we have defined the feature map  X ( x , y ) =  X ( x ; p ) for the case in which the label y = ( p ) contains exactly one object, but an image may contain no or multiple object instances (denoted respectively y = and y = ( p 1 ,...,p n ) ). We define the loss function between a ground truth label y and the estimated output y as where B is the ground truth bounding box, and B 0 is the prediction (the smallest axis aligned bound-ing box that contains the warped template g p R 0 ). The overlap score between B and B 0 is given by overl( B,B 0 ) = | B  X  B 0 | / | B  X  B 0 | . Note that the ground truth poses are defined so that B ( p l ) matches the PASCAL provided bounding boxes [1] (or the manually extended ones for the trun-cated ones).
 The hypothesis y = (no object) receives score F ( x , ; w ) = 0 by defining  X ( x , ) = 0 as in [1]. In this way, the hypothesis y = ( p ) is preferred to y = only if F ( x ,p ; w ) &gt; F ( x , ; w ) = 0 , which implicitly sets the detection threshold to zero. However, there is no reason to assume that this threshold should be appropriate (in Fig. 2 we show that it is not). To learn an arbitrary threshold, it suffices to append to the feature vector  X ( x ,p ) a large constant  X  bias , so that the score of the is large, the weight w bias remains small and has negligible effect on the SVM regularization term. Finally, an image may contain more than one instance of the object. The model can be extended sive X  force that prevents multiple overlapping detections of the same object. Performing infer-ence with such a model becomes however combinatorial and in general very difficult. Fortu-nately, in training the problem can be avoided entirely. If an image contains multiple instances, the image is added to the training set multiple times, each time  X  X ctivating X  one of the instances, and  X  X eactivating X  the others. Here  X  X eactivating X  an instance simply means removing it from the detector search space. Formally, let p 0 be the pose of the active instance and p 1 ,...,p N the poses of the inactive ones. A pose p is removed from the search space if, and only if, max i overl( B ( p ) ,B ( p i ))  X  max { overl( B ( p ) ,B ( p 0 )) , 0 . 2 } . Minimising the regularised risk R ( w ) as defined by Eq. (2) is difficult as the loss depends on w through  X  y i ( w ) and  X  h i ( w ) (see Eq. (1)). It is however possible to optimise an upper bound (derived below) given by which only the observed part y i is known from the ground truth).
 Alternation optimization. Eq. (11) is not a convex energy function due to the dependency of h  X  i ( w ) on w . Similarly to [13], however, it is possible to find a local minimum by alternating optimizing w and estimating h  X  i . To do this, the CCCP algorithm proposed in [13] for the case of margin rescaling, must be extended to the slack rescaling formulation used here.
 Starting from an estimate w t of the solution, define h  X  it = h i ( w t ) , so that, for any w , and the equality holds for w = w t . Hence the energy (11) is bounded by and the bound is strict for w = w t . Optimising (12) will therefore result in an improvement of the energy (11) as well. The latter can be carried out with the cutting-plane technique of [9]. Derivation of the bound (11) . The derivation involves a sequence of bounds, starting from This bound holds because, by construction, the quantity in the square brackets is not smaller than bound the loss by Substituting this bound into (2) yields (11). Note that  X  y i ( w ) and  X  h i ( w ) are defined as the max-imiser of  X  w,  X ( x i , y , h )  X  alone (see Eq. 1), while the energy maximised in (14) depends on the loss  X ( y i , y , h ) as well. Figure 2: Effect of different model components. The left panel evaluates the effect of differ-ent components of the model on the task of learning a detector for the left-right facing PASCAL VOC 2007 bicycles. In order of increasing AP (see legend): baseline model (see text); bias term (Sect. 2.5); detecting trunctated instances, training on truncated instances, and counting the trun-cated cells as a feature (Sect.: 2.2); with searching over small translation, scaling, rotation, skew (Sect. 2.1). Right panel: (a) Original VOC specified bounding box and aspect; (b) alignment and as-pect after pose inference  X  in addition to translation and scale, our templates are searched over a set of small perturbations. This is implemented efficiently by breaking the template into blocks (dashed boxes) and rearranging those. Note that blocks can partially overlap to capture foreshortening. The ground truth pose parameters are approximate because they are obtained from bounding boxes (a). The algorithm improves their estimate as part of inference of the latent variables h . Notice that not only translation, scale, and small jitters are re-estimated, but also the aspect subclass can be updated. In the example, an instance originally labeled as misc (a) is reassigned to the left aspect (b). Data. As training data we use the PASCAL VOC annotations. Each object instance is labeled with a bounding box and a categorical aspect variable (left, right, front, back, undefined). From the bounding box we estimate translation and scale of the object, and we use aspect to select one of multiple HOG templates. Symmetric aspects (e.g. left and right) are mapped to the same HOG template as suggested in Sect. 2.3.
 While our model is capable of handling correctly truncations, truncated bounding boxes provide a poor estimate of the pose of the object pose which prevents using such objects for training. While we could simply avoid training with truncated boxes (or generate artificially truncated examples whose pose would be known), we prefer exploiting all the available training data. To do this, we manually augment all truncated PASCAL VOC annotations with an additional  X  X hysical X  bounding box. The purpose is to provide a better initial guess for the object pose, which is then refined by optimizing over the latent variables.
 Training and testing speed. Performing inference with the model requires evaluating  X  w,  X ( x ,p )  X  for all possible poses p . This means matching a HOG template O ( WHTA ) times, where W  X  H is the dimension of the image in cells, T the number of perturbations (Sect. 2.1), and A the number of aspects (Sect. 2.3). 1 For a given scale and aspect, matching the template for all locations reduces to convolution. Moreover, by breaking the template into blocks (Fig. 2) and pre-computing the convolution with each of those, we can quickly compute perturbations of the template. All in all, detection requires roughly 30 seconds per image with the full model and four aspects. The cutting plane algorithm used to minimize (12) requires at each iteration solving problems similar to inference. This can be easily parallelized, greatly improving training speed. To detect additional objects at test time we run inference multiple times, but excluding all detections that overlap by more than 20% with any previously detected object. Figure 3: Top row. Examples of detected bicycles. The dashed boxes are bicycles that were detected with or without truncation support, while the solid ones were detectable only when truncations were considered explicitly. Bottom row. Some cases of correct detections despite extreme truncation for the horse class.
 Benefit of various model components. Fig. 2 shows how the model improves by the successive introduction of the various features of the model. The example is carried on the VOC left-right facing bicycle, but similar effects were observed for other categories. The baseline model uses only the HOG template without bias, truncations, nor pose refinement (Sect. 2.1). The two most significant improvements are (a) the ability of detecting truncated instances (+22% AP, Fig. 3) and (b) the addition of the bias (+11% AP). Training with the truncated instances, adding the number of occluded HOG cells as a feature component, and adding jitters beyond translation and scaling all yield an improvement of about +2 X 3% AP.
 Full model. The model was trained to detect the class bicycle in the PASCAL VOC 2007 data, using five templates, initialized from the PASCAL labeling left, right, front/rear, other. Initially, the pose refinimenent h is null and the alternation optimization algorithm is iterated five times to estimate the model w and refinement h . The detector is then tested on all the test data, enabling multiple detections per image, and computing average-precision as specified by [3]. The AP score was 47%. By comparison, the state of the art for this category [8] achieves 56%. The experiment was repeated for the class horse, obtaining a score of 40%. By comparison, the state of the art on this category, our MKL sliding window classifier [10], achieves 51%. Note that the proposed method uses only HOG, while the others use a combination of at least two features. However [4], using only HOG but a flexible part model, also achieves superior results. Further experiments are needed to evaluate the combined benefits of truncation/occlusion handling (proposed here), with multiple features [10] and flexible parts [4].
 Conclusions We have shown how structured output regression with latent variables provides an integrated and ef-fective solution to many problems in object detection: truncations, pose variability, multiple objects, and multiple aspects can all be dealt in a consistent framework.
 While we have shown that truncated examples can be used for training, we had to manually extend the PASCAL VOC annotations for these cases to include rough  X  X hysical X  bounding boxes (as a hint for the initial pose parameters). We plan to further extend the approach to infer pose for truncated examples in a fully automatic fashion (weak supervision).
 Acknowledgments. We are grateful for discussions with Matthew Blaschko. Funding was provided by the EU under ERC grant VisRec no. 228180; the RAEng, Microsoft, and ONR MURI N00014-07-1-0182. [1] M. B. Blaschko and C. H. Lampert. Learning to localize objects with structured output regres-[2] N. Dalal and B. Triggs. Histograms of oriented gradients for human detection. In Proc. CVPR , [3] M. Everingham, L. Van Gool, C. K. I. Williams, J. Winn, and A. Zisserman. The [4] P. F. Felzenszwalb, R. B. Grishick, D. McAllister, and D. Ramanan. Object detection with [5] R. Fergus, P. Perona, and A. Zisserman. Object class recognition by unsupervised scale-[6] K. Hotta. Robust face detection under partial occlusion. In Proceedings of the IEEE Interna-[7] Y. Y. Lin, T. L. Liu, and C. S. Fuh. Fast object detection with occlusions. In Proceedings of [8] P. Schnitzspan, M. Fritz, S. Roth, and B. Schiele. Discriminative structure learning of hierar-[9] I. Tsochantaridis, T. Hofmann, T. Joachims, and Y. Altun. Support vector machine learning for [10] A. Vedaldi, V. Gulshan, M. Varma, and A. Zisserman. Multiple kernels for object detection. [11] O. Williams, A. Blake, and R. Cipolla. The variational ising classifier (VIC) algorithm for [12] J. Winn and J. Shotton. The Layout Consistent Random Field for Recognizing and Segmenting [13] C.-N. J. Yu and T. Joachims. Learning structural SVMs with latent variables. In Proc. ICML ,
