 Modern distributed information retrieval techniques require accurate knowledge of collection size. In non-cooperative environments, where detailed collection statistics are not available, the size of the underlying collections must be esti-mated. While several approaches for the estimation of col-lection size have been proposed, their accuracy has not been thoroughly evaluated. An empirical analysis of past estima-tion approaches across a variety of collections demonstrates that their prediction accuracy is low. Motivated by ecologi-cal techniques for the estimation of animal populations, we propose two new approaches for the estimation of collection size. We show that our approaches are significantly more accurate that previous methods, and are more efficient in use of resources required to perform the estimation. H.3.3 [ Information Storage and Retrieval ]: Selection Process; H.3.4 [ Systems and software ]: Distributed Sys-tems; H.3.7 [ Digital Libraries ]: Collection Algorithms Capture-History, Sample-Resample, Capture-Recapture, Col-lection Size Estimation
Large numbers of independent information collections do not permit their content to be indexed by third parties, and enforce search through their own search interfaces. Dis-tributed Information Retrieval (DIR) systems aim to pro-vide a unified search interface for multiple independent col-lections. In such systems, a central broker receives user queries and sends these in parallel to the underlying collec-tions, and collates the received answers into a single list for Copyright 2006 ACM 1-59593-369-7/06/0008 ... $ 5.00. presentation to the user. The broker selects the collections to use for each query on the basis of information that it has previously accumulated on each collection. This information  X  usually referred to as collection summaries [Ipeirotis and Gravano, 2004] or representation sets [Si and Callan, 2003a]  X  typically includes global statistics about the vocabularies of each collection.

In cooperative DIR environments, collection summaries are published, which brokers use for collection selection. In practice, however, many collections are non-cooperative , and do not publish information that can be used for col-lection selection. For such non-cooperative collections, bro-kers must gather information for themselves, generally by sending probe queries to each collection and analyzing the answers, a process known as query-based sampling (QBS) [Callan and Connell, 2001].

The size of a collection is a key indicator used in the most effective of the collection selection algorithms, such as KL-Divergence [Si et al., 2002] and ReDDE [Si and Callan, 2003b]. In a non-cooperative environment, a broker must es-timate collection size; QBS mechanisms such as the sample-resample method [Si and Callan, 2003b] have been proposed as suitable for this purpose, but their accuracy has not been fully investigated. However, if the estimation is in-accurate, the effectiveness of these methods is significantly undermined.

In this paper, we describe experiments on collections of real data that show that sample-resample is in fact unsuit-able for appraising collection size in non-cooperative envi-ronments, producing widely varying estimates ranging from 50% to 800% of the actual value. To fulfill the need for ac-curate estimates, we present two new methods based on the capture-recapture approach [Sutherland, 1996], a simple sta-tistical technique that uses the observed probability of two samples containing the same document. Our refined meth-ods, which we call multiple capture-recapture and capture-history , make rich use of sampling patterns. Our experi-ments show that they provide a closer estimate of collection size than previous methods, and require less information than the sample-resample technique.
 The remainder of this paper is organised as follows. In Section 2, we review related work. In Section 3 we introduce our approach and provide simple examples. Experimental issues and biases that occurred in implementation are dis-cussedinSection4. Section5containstheexperimental results of different approaches; we conclude in Section 6.
In cooperative environments, brokers have comprehensive information about each collection, including its size [Pow-ell and French, 2003]. However, there is limited previous work on collection size estimation in non-cooperative envi-ronments, where brokers do not have access to information on the collections. Using estimation as a way to identify a collection X  X  size was initially suggested by Liu et al. [2002], who introduce the capture-recapture method. This approach is based on the number of overlapping documents in two random samples taken from a collection: assuming that the actual size of collection is N ,ifwesample a random docu-ments from the collection and then sample (after replacing these documents) b documents, the size of collection can be estimated as  X  N = ab c ,where c is the number of documents common to both samples. However, Liu et al. [2002] do not examine how to implement the proposed approach in prac-tice. This is non-trivial; for example, it is unclear what the sample size should be, as it is not possible to accurately estimate the size of a collection of more than 1 000 000 doc-uments using just two samples of 1000 documents each.
It is also far from obvious how a random sample might be chosen from a non-cooperative collection. Callan and Con-nell [2001] propose that QBS be used for this purpose. Here, an initial query is first selected from a standard list of com-mon terms. For this and each subsequent query, a certain number of returned answer documents are downloaded, and the next query is selected at random from the current set of downloaded documents. QBS stops after downloading a predefined number of documents, usually 300 [Callan and Connell, 2001]. We have found that QBS rarely produces a random sample from the collection, which undermines the initial hypothesis of the capture-recapture method.
Figure 1 shows the distribution of document ids (docids) in two experiments on a subset of the WT10g collection described in Section 4; this collection has 11 341 documents. We extracted one hundred samples from the collection, once by selecting documents at random, and once using QBS. Each sample contained 300 documents. The horizontal axis shows how often the same document was sampled, while the vertical axis shows the number of documents sampled a given number of times. On the left, 1 005 documents did not appear in the samples, while, for example, 330 documents were repeated 6 times. The distribution of documents in our random sampling is similar to the Poisson distribution. In contrast, we observe that, for QBS, many documents appear in multiple samples; indeed, 3 documents appeared in all the samples, while 5 927 documents (52% of the total) did not appear in any of the samples. None of the documents in our random samples appeared in more than 11 samples; for ease of comparison, we omit data points related to documents that appeared in more than 11 QBS samples.

Clearly, the samples produced by QBS are far from ran-dom. Given the biases inherent in effective search engines  X  by design, some documents are preferred over others  X  this result is unsurprising. It does however mean that size estimation methods cannot assume that QBS samples are random. Na  X   X ve capture-recapture would be drastically inac-curate; the degree of inaccuracy in other methods is explored in our experiments.

An alternative to the capture-recapture methods is to use the distribution of terms in the sampled documents, as in the sample-resample (SRS) method [Si and Callan, 2003b].
SRS was introduced as a part of the ReDDE collection selection algorithm. ReDDE applies SRS to estimate the relevant document distribution among multiple collections. It selects suitable collections for each query based on a rank calculated as: where Relq ( i ) is the estimated number of relevant docu-ments in collection i and is given by: Here, P ( rel | d i ) is the probability of relevance of document i in the collection; N C j samp is the size of the sample obtained by query-based sampling for collection C j ;and  X  N is the esti-mated size of that collection calculated by the SRS method. As in the KL-Divergence method  X  which we do not discuss here because of our space limitation  X  the estimated size of the collection has direct impact on its rank for collection selection, and inaccurate estimations will produce inferior results. ReDDE has proven effective at matching queries to suitable collections without training [Hawking and Thomas, 2005; Si and Callan, 2003a;b]. UMM [Si and Callan, 2004] has been reported to produce slightly higher precision than ReDDE for almost all kinds of queries. However, it needs prior training using a set of queries. Similarly, while meth-ods based on anchor text [Hawking and Thomas, 2005] pro-duce better results for homepage and named-page finding tasks, they are not as good for general queries; moreover, they require a huge crawled dataset that needs to be gen-erated before collection selection starts. The performance of the KL-Divergence and ReDDE algorithms have been re-ported to be similar, with the latter outperforming the for-mer marginally in most cases [Hawking and Thomas, 2005; Si and Callan, 2003a].

Assuming that QBS [Callan and Connell, 2001] produces good random samples, the distribution of terms in the sam-ples should be similar to that in the original collection. For example, if the document frequency of a particular term t in a sample of 300 documents is d t , and the document fre-quency of the term in the collection is D t , the collection size can be estimated by SRS as  X  N = d t D t 300 .

This method involves analyzing the terms in the samples and then using these terms as queries to the collection. The approach relies on the assumption that the document fre-quency of the query terms will be accurately reported by each search engine. Even when collections do provide the document frequency, these statistics are not always reliable [Anagnostopoulos et al., 2005]. In addition, as we have seen, the assumption of randomness in QBS is questionable. Compared to capture-recapture, the additional costs of SRS are significant: the documents must be fetched and a great many queries must be issued. In practice, SRS uses the documents that are downloaded as collection representation sets. In the absence of such documents, SRS cannot esti-mate collection size unless it downloads new samples. For capture-recapture, only the document identifiers or docids are required for estimating the size.

Si and Callan [2003b] investigated the accuracy of SRS on relatively small collections extracted from the TREC newswire data. However, they did not investigate how well SRS estimates the size of larger collections. We test our methods on collections extracted from the TREC WT10g [Bai-ley et al., 2003] and GOV [Craswell and Hawking, 2002] data sets that are more similar to current Web collections.
SRS has been used in other papers for estimating the size of collections. Karnatapu et al. [2004] extend the work of Si and Callan [2003b] to find independent terms from the sam-ples, focusing on improving sample quality rather than col-lection size estimation. The work has the same limitations as that of Si and Callan [2003b]. In other papers [Gravano et al., 2003; Ipeirotis and Gravano, 2002; Ipeirotis et al., 2001], researchers have used QBS and SRS for classifying the hidden-web collections. Hawking and Thomas [2005] also used SRS to estimate the size of collections in their pa-per. In all these papers, the size of collection has been used as an important parameter in collection selection.
In previous work on using estimated collection size, the accuracy of the estimate has not been investigated in detail. We propose use of empirical evidence  X  using collections of various sizes and characteristics  X  to evaluate how well approaches estimates the collection size. However, accurate estimation of collection size remains an open research ques-tion. The challenge is to develop methods for estimating collection size that are robust in the presence of bias, and that do not rely on estimates of term frequencies returned by collections. We present and evaluate two alternative ap-proaches for estimating the size of collections. These are inspired by the mark-recapture techniques used in ecology to estimate the population of a particular species of animal in a region [Sutherland, 1996].

We adapt the mark-recapture approach to collection size estimation, using the number of duplicate documents ob-served within different samples to estimate the size of the collection.
In the standard capture-recapture technique, a given num-ber of animals is captured, marked, and released. After a suitable time has elapsed, a second set is captured; by in-specting the intersection of the two sets, the population size can be estimated. Assume we have a collection of some un-known size, N . For a numerically-valued discrete random variable X , with sample space  X  and distribution function m ( x ), the expected value E ( X )is: where x is a possible value for X in the sample space. We first collect a sample, A ,with k documents from the collec-tion. We then collect a second sample, B , of the same size. If the samples are random, the likelihood that any document in B was previously in A is k N . The likelihood of observing i duplicate documents between two random samples of size k is: The possible values for the number of duplicate documents are 0 , 1 , 2 ,...,k ,thus:
E ( X )=
This method can be extended to a larger number of sam-ples to give multiple capture-recapture (MCR). Using T samples, the total number of pairwise duplicate documents D should be:
This result is similar to the traditional capture-recapture method for two samples of size k . By gathering T random samples from the collection and counting duplicates within each sample pair, the expected size of collection is:
Capture-recapture is one of the oldest methods used in ecology for estimating population size. An alternative, in-troduced by Schumacher and Eschmeyer [1943], uses T con-secutive random samples with replacement, and considers the capture history . We call this the CH method. Here, Table 1: Using the capture-history method to esti-mate the size of a collection with 5 samples of 10 documents each.
 where K i is the total number of documents in sample i , R i is the number of documents in the sample i that were already marked, and M i is the number of marked documents gathered so far, prior to the most recent sample.
For example, assume that five consecutive random sam-ples, each containing ten documents, are gathered with re-placement from a collection of unknown size. The values for parameters in Equation 8 are calculated in each step as shown in Table 1. The estimated size of the collection after observing the fifth sample would be 25350 101  X  251 documents.
Figure 2 illustrates the accuracy of these algorithms for estimating the size of a collection of 301 681 documents, us-ing 160 samples of 100 documents. Documents are selected randomly.

We observe that both methods produce close estimates after about 80 samples. In practice, it is not possible to select random documents from non-cooperative collections. These are accessible only via queries; we have no access to the index, and it is meaningless to request documents with particular docids.

In the following section, we show how can we apply these algorithms to estimation of the size of collections in non-cooperative distributed environments, and propose a correc-tion to the CH method to compensate for the bias inherent in sampling via QBS.
The CH and MCR methods both require random samples of the collection to produce correct results. In practice, how-ever, generating random samples by random queries is sub-ject to biases. For example, some documents are more likely to be retrieved for a wide range of queries, and some might never appear in the results [Garcia et al., 2004]. Moreover, long documents are more likely to be retrieved, and there could be other biases in the collection ranking functions.
We tested the algorithms on collections with different rank-ing functions and found similar estimations; longer docu-ments are more likely to be returned, with a similar skew for both the Okapi BM25 and cosine [Baeza-Yates and Ribeiro-Neto, 1999] ranking functions. Figure 3 illustrates the per-formance of the CH algorithm for estimating the size of the same collection discussed in Figure 2. As can be seen, the size of the collection is significantly underestimated across a range of ranking parameters.

To partially overcome the bias, we could keep only the first document returned for each query; however, as previ-Figure 3: Effect of search engine type on estimates of collection size using the CH method.
 Table 2: Properties of data collections used for train-ing and testing.
 ously noted by Agichtein et al. [2003], this would require thousands of queries, and is therefore impractical.
We develop our approach and evaluate competing tech-niques using distinct training and test sets. Each set com-prises different collections of varying sizes as detailed in Table 2. The first three training collections each repre-sent a subset of TREC WT10g [Bailey et al., 2003]. GOV-7 is a two-gigabyte collection extracted from the TREC crawl of the .gov domain. DATELINE 509 is a subset of TREC newswire data created from Associated Press arti-cles [D X  X ouza et al., 2004]. Other training collections are different subsets of TREC WT10g, with different sizes. In the test set, GOV-123456 is a 12 GB subset of the TREC .GOV collection, and LATimes consists of news articles ex-tracted from TREC Disk 5 [Voorhees and Harman, 2000]. GOV-4 was also extracted from the TREC .GOV collec-tion. The rest are different subsets of the TREC WT10g collection. The largest collections contain more than eight hundred thousand documents. Considering that the largest crawled server in TREC .GOV2 collection has fewer than 720 000 documents, this upper limit seems reasonable for our experiments.
We propose that the capture methods be modified to com-pensate for the biases discussed earlier. To calculate the amount of bias, we compare the estimation values and ac-tual collection sizes using the training set. We create random selected at random. samples by passing 5 000 single query terms to each collec-tion and collecting the top N answers for each query. The choice of 5 000 is dictated by the daily limit of the Yahoo search engine developer kit ( http://developer.yahoo.net ) and seems a practical number of queries for sampling com-mon collections. We selected 10 as a suitable value for N . The query terms themselves should be chosen with care. Since it is hard to choose specific queries for each collection, query terms should be general enough to return answers for almost all types of collections. The queries should ideally be independent of each other. To make the system more efficient, we should also avoid query terms that are unlikely to return any answers.

We experimented with selecting query terms at random from the Excite search engine query logs [Jansen et al., 2000], and from the index of 290 175 web pages. We found that using the query log led to poor results; we conjecture that this is due to the limited breadth of popular query top-ics. Therefore, the random terms from query logs are less likely to be independent. In preliminary experiments, we found that index terms with low document frequency failed to match any documents in some collections. To avoid this, we eliminate terms occurring in fewer than 20 documents. Our initial results suggested that, in all experiments, the CH and MCR algorithms underestimate the actual collec-tion sizes at roughly predictable rates, due to the biases discussed earlier. To achieve a better estimation, we ap-proximated the correlation between the estimated and ac-tual collection size by regression equations as below; we refer to these as MCR-Reg and CH-Reg respectively: log(  X  N MCR )=0 . 5911  X  log( N )+1 . 5767 R 2 =0 . 8226 (9) log(  X  N CH )=0 . 6429  X  log( N )+1 . 4208 R 2 =0 . 9428 (10) where  X  N is the estimated size obtained by the methods, and N is the actual collection size. The R 2 values indicate how well the regression fits the data.

We used 25 single-word resample queries for SRS to esti-mate the size of collections: where | S | isthesamplesize,and Di and d i are the document frequencies of the query term i in the collection and the sample respectively.
We probed the test collections using varying numbers of the single-term queries. The answers for each query are a small sample picked from the collection. Using the number of duplicate documents within these samples, we estimate the size of each collection. We tested the performance of MCR-Reg and CH-Reg algorithms for different numbers of queries  X  from 100 to 5 000  X  on selected collections from the test set. Increasing the number of queries generally im-proves the estimation slightly. Although the performance might continue to increase beyond this range, as discussed previously, 5 000 appears to be a practical upper limit. We omit the results here for brevity.

The bars in Figure 4 depict the performance of the three algorithms on various collections for 5 000 queries. The CH method produces accurate approximations, while MCR usu-ally underestimates the collection size. The SRS method produces good approximations for some collections, but con-siderably overestimates the size for other collections.
To evaluate the estimation of different methods, we define the estimation error as:
A negative estimation error indicates underestimation of the real size. The lower the absolute estimation error, the more accurate the estimation algorithm. Table 3 shows the estimation error of each algorithm for the collections of Fig-ure 4. The CH-Reg method has the lowest error rate; it gives Table 3: Estimation errors for size estimation meth-ods. All values are percentages.
 the best approximation for all collections except WT10g-1 and LATimes. SRS has a small advantage over CH-Reg for only the LATimes collection. The CH-Reg method pro-duces accurate approximations for collections of different sizes. There is no significant difference between MCR-Reg and SRS; the former generally produces better results and is more robust. Since MCR-Reg produces poorer results than CH-Reg, we do not discuss it further in this paper.
The SRS method requires lexicon statistics (document fre-quencies) to estimate the size of a collection. Therefore, while the MCR and CH methods only require the document identifiers (or, in practice, the URL of the answer docu-ment), SRS requires that many documents be downloaded. In collection selection algorithms, these downloaded docu-ments can also be used as collection representation sets. Ta-ble 4 shows the number of docids returned from each collec-tion for 5000 queries.

Although answering 5 000 queries seems feasible for many current search engines, downloading the answer documents might be impractical. Thus, we also compare the methods using a smaller number of queries (that is, smaller sample size for SRS). A typical QBS sample size has been reported to be 300 documents [Callan and Connell, 2001]. The same number was suggested by Si and Callan [2003b] for SRS. Therefore, we compare the performance of the CH method with that of SRS using 300 document samples. On average, Table 4: The total number of docids returned for 5000 queries in each experiment.
 60 queries were required to gather 300 documents by query-based sampling from our test collections (each query returns at most 10 documents) 1 . Assuming that the cost of viewing a result page and downloading an answer document are the same, and considering that we used 25 resample queries, this involves 385 interactions: 60 sampling queries, 300 results pages, and 25 resample queries. This is exactly equal to what has been reported by Si and Callan [2003b]. The CH method downloads only the result page for each query. Since indexing costs are negligible for small numbers of queries and documents, the cost of a typical SRS method for our test collections is at least as high as the cost of a CH method that uses 385 single-term queries. Table 5 compares the performance of the alternative schemes. For 385 queries (300 document samples for SRS), the CH method outperforms SRS in 5 of 7 instances.

SRS produces more accurate estimations for the GOV col-lections. For the smallest collection, they both produce sim-ilar poor estimates. Interestingly, for 5 of 7 collections the CH method remains accurate with fewer queries, producing better results with 140 queries than the SRS method based on 100 documents. On average, 15 probe queries were suf-ficient to extract 100 documents from the test collections. Taking the costs of 25 resampling queries and downloading 100 documents into account, the total cost is equivalent to
We used the Lemur toolkit ( www.lemurproject.org ) for all experiments reported in this paper. Table 6: The impact of size estimation algorithms on the precision at different recall points for the Non-relevant testbed. TREC topics 51-100 used as queries. One collection is selected per query.
 a CH method using 140 queries. Generally, the CH method with 140 queries produces more accurate estimates than SRS with 385 queries. The average of absolute estimation errors on the test collections for 385 queries is 57 . 42% for SRS and 44 . 85% for the CH method. For 140 queries, these values are 66 . 14% and 41 . 28% respectively. Moreover, the SRS method is limited to collections that have a search inter-face providing the document frequency of query terms, and is dependent on the accuracy of this information. The CH methods do not rely on values provided by the collection search interface, and produce better approximations using about one-third as many queries.
 analysed the impact of size estimation on collection selection algorithms on five well-known DIR testbeds; trec123-100col-bysource, relevant, nonrelevant and representative. The de-tails and characteristics of these testbeds are discussed else-where [Si and Callan, 2003b; 2004].

We used ReDDE [Si and Callan, 2003b] as the collection selection algorithm in our experiments. We only selected one collection for each query. Therefore, the type of merg-ing algorithm does not affect the final results. ReDDE ap-plies the estimated collection size values to approximate the number of relevant documents in each collection. For our experiments, we estimated the size of collections by both SRS and CH-Reg methods. We also compared the results with a capture-history method that does not use Eq. (10) for correction (CH).

Except for the relevant testbed, our evaluation results showed that using the CH and CH-Reg methods led to higher precision values than SRS. For brevity, we only show the results for the nonrelevant testbed, in Table 6. We use the t-test to measure the statistical differences between the methods. The numbers specified by  X  are significantly bet-ter than the corresponding SRS values at the 0.90 confidence intervals. None were significant at the 0.95 level.
These results show that using a more accurate size es-timation algorithm such as capture-history X  X ven without correction X  X an improve the effectiveness of collection selec-tion and thus of final ranking.
We have described new methods for estimating the size of collections in non-cooperative distributed environments. These methods are an application of capture-recapture and capture-history methods to the problem of size estimation; by introducing an explicit (though ad hoc) compensation for the consistent biases in sampling from search engines, we have considerably improved the estimates that these meth-ods provide.

We have presented evaluation results across several col-lections; to our knowledge these are the first measurements of such size estimation methods. These experiments show that the capture-history method significantly outperforms the other algorithms in most cases and is robust to varia-tions in collection size. In contrast to the sample-resample methods used in previous work, our proposed algorithm does not rely on the document frequency of query terms being returned by search engines, and does not need to download and index the returned results of probe queries, significantly reducing resource and bandwidth consumption. We also showed that the capture-history method is less expensive and more accurate than sample-resample, producing more accurate estimates while using fewer probe queries.
In addition, we evaluated the impact of using a better size estimator metric on collection selection. The results show that the effectiveness of collection selection algorithms can be improved by use of more accurate estimations of collec-tion size.

Several interesting research problems remain. In partic-ular, the choice of ranking measure produces biases that could be factored into the estimation process. Our experi-ments used the Okapi BM25 and cosine similarity measures; similarity functions used in commercial search tools incorpo-rate measures such as PageRank [Brin and Page, 1998] and weighting for anchor-text [Craswell et al., 2001], as well as making use of other forms of evidence such as click-through data. Therefore, our techniques are not in their present form suitable for estimating the size of the vast collections held by web search engines. We plan to explore robustness across more diverse data and search engine types. The results we have presented in this work indicate that our methods rep-resent progress towards this goal.
 Agichtein, E., Ipeirotis, P. G., and Gravano, L. (2003). Mod-eling query-based access to text databases. In Interna-tional Workshop on Web and Databases , pages 87 X 92, San Diego, California.
 Anagnostopoulos, A., Broder, A. Z., and Carmel, D. (2005).
Sampling search-engine results. In Proceedings of 14th
International Conference on the World Wide Web , pages 245 X 256, Chiba, Japan.
 Baeza-Yates, R. A. and Ribeiro-Neto, B. (1999). Modern
Information Retrieval . Addison-Wesley Longman Pub-lishing Co., Inc., Boston, MA.
 Bailey, P., Craswell, N., and Hawking, D. (2003). Engi-neering a multi-purpose test collection for Web retrieval experiments. Information Processing and Management , 39(6):853 X 871.
 Brin, S. and Page, L. (1998). The anatomy of a large-scale hypertextual Web search engine. In Proceedings of 7th
International Conference on the World Wide Web , pages 107 X 117, Brisbane, Australia.
 Callan, J. and Connell, M. (2001). Query-based sampling of text databases. ACM Transactions on Information Sys-tems , 19(2):97 X 130.
 Craswell, N. and Hawking, D. (2002). Overview of the TREC-2002 Web Track. In Proceedings of TREC-2002 , Gaithersburg, Maryland.
 Craswell, N., Hawking, D., and Robertson, S. (2001). Ef-fective site finding using link anchor information. In Pro-ceedings of 24th Annual International ACM SIGIR Con-ference on Research and Development in Information Re-trieval , pages 250 X 257, New Orleans, Louisiana. D X  X ouza, D., Thom, J., and Zobel, J. (2004). Collection se-lection for managed distributed document databases. In-formation Processing and Management , 40(3):527 X 546. Garcia, S., Williams, H. E., and Cannane, A. (2004). Access-ordered indexes. In Proceedings of 27th Australasian Com-puter Science Conference , pages 7 X 14, Darlinghurst, Aus-tralia.
 Gravano, L., Ipeirotis, P. G., and Sahami, M. (2003). Qprober: A system for automatic classification of Hidden-
Web databases. ACM Transactions on Information Sys-tems , 21(1):1 X 41.
 Hawking, D. and Thomas, P. (2005). Server selection meth-ods in hybrid portal search. In Proceedings of 28th An-nual International ACM SIGIR Conference on Research and Development in Information Retrieval , pages 75 X 82, Salvador, Brazil.
 Ipeirotis, P. G. and Gravano, L. (2002). Distributed search over the hidden Web: Hierarchical database sampling and selection. In Proceedings of 28th International Conference on Very Large Data Bases , pages 394 X 405, Hong Kong, China.
 Ipeirotis, P. G. and Gravano, L. (2004). When one sam-ple is not enough: improving text database selection us-ing shrinkage. In Proceedings of ACM SIGMOD Inter-national Conference on Management of Data , pages 767 X  778, Paris, France.
 Ipeirotis, P. G., Gravano, L., and Sahami, M. (2001). Probe, count, and classify: categorizing Hidden Web databases. ACM SIGMOD Record , 30(2):67 X 78.
 Jansen, B. J., Spink, A., and Saracevic, T. (2000). Real life, real users, and real needs: a study and analysis of user queries on the Web. Information Processing and Man-agement , 36(2):207 X 227.
 Karnatapu, S., Ramachandran, K., Wu, Z., Shah, B.,
Raghavan, V., and Benton, R. (2004). Estimating size of search engines in an uncooperative environment. In Workshop on Web-based Support Systems , pages 81 X 87, Beijing, China.
 Liu, K., Yu, C., and Meng, W. (2002). Discovering the representative of a search engine. In Proceedings of 11th
ACM CIKM International Conference on Information and Knowledge Management , pages 652 X 654, McLean, Virginia.
 Powell, A. L. and French, J. (2003). Comparing the perfor-mance of collection selection algorithms. ACM Transac-tions on Information Systems , 21(4):412 X 456.
 Schumacher, F. X. and Eschmeyer, R. W. (1943). The esti-mation of fish populations in lakes and ponds. Journal of the Tennesse Academy of Science , 18:228 X 249.
 Si, L. and Callan, J. (2003a). The effect of database size distribution on resource selection algorithms. In Proeed-ings of SIGIR 2003 Workshop on Distributed Information Retrieval , pages 31 X 42, Toronto, Canada.
 Si, L. and Callan, J. (2003b). Relevant document distri-bution estimation method for resource selection. In Pro-ceedings of 26th Annual International ACM SIGIR Con-ference on Research and Development in Information Re-trieval , pages 298 X 305, Toronto, Canada.
 Si, L. and Callan, J. (2004). Unified utility maximization framework for resource selection. In Proceedings of 13th ACM CIKM Conference on Information and Knowledge Management , pages 32 X 41, Washington, D.C.
 Si, L., Jin, R., Callan, J., and Ogilvie, P. (2002). A lan-guage modeling framework for resource selection and re-sults merging. In Proceedings of 11th ACM CIKM Inter-national Conference on Information and Knowledge Man-agement , pages 391 X 397, New York, NY.
 Sutherland, W. J. (1996). Ecological Census Techniques . Cambridge University Press.
 Voorhees, E. M. and Harman, D. (2000). Overview of the sixth Text REtrieval Conference (TREC-6). Information
Processing and Management , 36(1):3 X 35.
