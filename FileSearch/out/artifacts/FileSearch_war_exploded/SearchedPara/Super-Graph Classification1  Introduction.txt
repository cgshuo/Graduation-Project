 Many applications, such as social networks and citation networks, commonly use graph structure to represent data entries ( i.e. nodes) and their structural relationships ( i.e. edges). When using graphs to represent objects, all existing frameworks rely on two approaches to describe node content (1) node as a single attribute: each node has only one attribute (single-attribute node). A clear drawback of this representation is that a single attribute cannot precisely describe the node content [6]. This repr esentation is commonly referred to as a single-attribute graph (Fig.1 (A)). (2) node as a set of attributes: use a set of independent attributes to describe the node content (Fig.1 (B)). This representation is commonly referred to as an attributed graph [2,3,8].
Indeed, in many applications, the attributes/properties used to describe the node content may be subject to dependency structures. For example, in a citation network each node represents one paper and edges denote citation relationships. information of a paper. Instead, we can r epresent the content of each paper as a graph with nodes denoting keywords and edges representing contextual corre-lations between keywords ( e.g. co-occurrence of keywords in different sentences or paragraphs). As a result, each pape r and all references cited in this paper can form a super-graph with each edge between papers denoting their citation relationships. In this paper, we refer to this type of graph, where the content of the node can be represented as a graph, as a  X  super-graph  X . Likewise, we refer to the node whose content is represented as a graph, as a  X  super-node  X .
To build learning models for super-graphs, the mainly challenge is to properly calculate the distance between two super-graphs.  X 
Similarity between two super-nodes: Because each super-node is a graph, the overlapped/intersected graph struc ture between two super-nodes reveals the similarity between two super-nodes, as well as the relationship between two super-graphs. Traditional hard-node-matching mechanism is unsuitable for super-graphs which require soft-node-matching.  X 
Similarity between two super-graphs: The complex structure of super-graph requires that the similarity measure considers not only the structure simi-larity, but also the super-node similarity between two super-graphs. This cannot be achieved without combining node matching and graph matching as a whole to assess similarity between super-graphs.

The above challenges motivate the proposed Weighted Random Walk Kernel ( WRWK ) for super-graphs. In our paper, we generate a new product graph from two super-graphs and then use weighted random walks on the product graph to calculate similarity between super-graphs. A weighted random walk denotes a walk starting from a random w eighted node and following succeeding weighted nodes and edges in a random manner. The weight of the node in the product graph denotes the similarity of two super-nodes. Given a set of labeled super-graphs, we can use an weighted product graph to establish walk-based relationship between two super-graphs and c alculate their similarities. After that, we can obtain the kernel matrix for super-graph classification. Definition 1. (Single-attribute Graph) A single-attribute graph is repre-E  X  V  X  V denotes a finite set of edges, and f : V  X  Att is an injective function from the vertex set V to the attribute set Att = { a 1 ,a 2 ,  X  X  X  ,a m } . Definition 2. (Super-graph and Super-node) A super-graph is represented nodes. E X  X  X V denotes a finite set of edges, and F : E X  X  is an injective graphs. A node in the super-graph, which is represented by a single-attribute graph, is called a Super-node.

Formally, a single-attribute graph g =( V,E,Att,f ) can be uniquely described by its attribute and adjacency matrices. The attribute matrix  X  is defined by  X   X   X  ij = 0. However, because of the complic ated structure of super-graph, we cannot use an unique matrix to describe its super-node information. To calculate conveniently as this article shows later, an super attribute matrix  X   X  R N  X  S is  X  adjacency matrices for a si ngle-attribute graph and a super-graph are shown in the top right corner of Fig. 2.
 Definition 3. (Super-graph Classification) Given a set of labeled super-a discriminative model from D L to predict some previously unseen super-graphs D Defining a kernel on a space X paves the way for using kernel methods [7] for classification, regression, and clustering. To define a kernel function for super-graphs, we employ a random walk kernel principle [4].
 Our Weighted Random Walk Kernel ( WRWK ) is based on a simple idea: Given a pair of super-graphs ( G 1 and G 2 ), we can use them to build a product graph, where each node of the product graph contains attributes shared by super-nodes in G 1 and G 2 . For each node in the product graph, we can assign a weight value (which is based on the similarity of the two super-nodes generating the current node). Because a weighted product node means the common weight of thenodeappearedinboth G 1 and G 2 , we can perform random walks through these nodes to measure the similarity by counting the number of matching walks (the walks through nodes containing intersected attribute sets) and combining weights of the nodes. The larger the number, the more similar the two super-graphs are. Adding weight value to the random walk is meaningful. It provides a solution to take graph matching of super-nodes into consideration to calculate the similarity between super-graphs. We consider the similarity between any two super-nodes as the weight value instead of just using 1/0 hard-matching. The node similarities between different super-nodes represent the relationship between two graphs in a more precise way, which, in turn, helps improve the classification accuracy. Acco rdingly, we divide our kernel design into two parts based on the similarity of super-nodes and super-graphs. 3.1 Kernel on Single-Attribute Graphs We firstly introduce the weighted random walk kernel on single-attribute graphs. Definition 4. (Single-Attribute Product Graph) Given two single-attribute product graph is denoted by g 1  X  2 =( V  X  ,E  X  ,Att  X  ,f  X  ) ( g  X  for short), where  X  V  X  = { v | v = &lt;v 1 ,v 2 &gt;,v 1  X  V 1 ,v 2  X  V 2 } ;  X  E  X  = { e | e =( u ,v ) ,u  X  V  X  ,v  X  V  X  ,f  X  ( u ) =  X ,f  X  ( v ) =  X ,  X  Att  X  = Att 1  X  Att 2 ;
In other words, g  X  is a single-attribut egraphwhereavertex v is the inter-section between a pair of nodes in g 1 and g 2 . There is an edge between a pair of vertices in g  X  , if and only if an edge exists in corresponding vertices in g 1 and g , respectively. An example is shown in Fig. 2 ( g 1  X  2 ). In the following, we show that an inherent property of the product graph is that performing a weighted random walk on the product graph is equivalent to performing simultaneous random walks on g 1 and g 2 , respectively. So the single-attribute product graph provides an effective way to count the number of walks combining weight values on nodes between graphs without expensive graph matching.

To generate g  X   X  X  adjacency matrix  X   X  from g 1 and g 2 by using matrix opera-tions, we define the Attributed Product as follow: Definition 5. (Attributed Product) Given matrices B  X  R n  X  n , C  X  R m  X  m and H  X  R n  X  m , the attributed product B C  X  R nm  X  nm and the column-stacking operator vec ( H )  X  R n m are defined as where H  X  i and H j  X  denote i th column and j th row of H , respectively.
Based on Def. 5, the adjacency matrix  X   X  of the single-attribute product graph g  X  can be directly derived from g 1 (  X  1 , X  1 )and g 2 (  X  2 , X  2 ) as follow: where  X   X   X  is a conjunction operation ( a  X  b =1 iff a =1and b =1).

To better assess the similarity between two single-attribute graphs, we assign each node a weight value, with weight indicating the importance of a node in the graph. So a weighted random walk can be calculated by multiply all the weight values of the nodes along the walk. Then we can calculate the weighted random walk counting by using matrix operation. More specifically, for the adjacency ma- X  w x =[ w 1 ,w 2 ,...w n ] . Each element [(  X  w x  X  w x weight value of all random walks with length z from v i to v j in g x ,where X  )  X  denotes element-wise multiplication (the same as  X .* X  operator in Matlab). For simplicity, we set an uniform distributions for all the statistical weights of nodes erated from g 1 and g 2 with the node sizes of n 1 and n 2 , respectively, where
According to Eq. (1), performing a weighted random walk on the single-attribute product graph g  X  is equivalent to performing random walks on graphs g 1 and g 2 simultaneously. After g  X  is generated, Weighted Random Walk Kernel (WRWK), which computes the similarity between g 1 and g 2 , can be defined with a sequence of weights  X  =  X  0 , X  1 ,  X  X  X  (  X  i  X  R and  X  i  X  0 for all i  X  N ): where n 1 and n 2 are the node sizes of g 1 and g 2 , respectively.  X  is the control parameter used to make the kernel function convergence. As a result, kernel values are upper bounded (the proof is given in Section 5). For simplicity, we set  X  = 1 n single-attribute product graph.

To compute the WRWK for single-attribute graph, as defined in Eq. (2), a diagonalization decomposition method [4] can be used. Because  X   X  is a sym-metric matrix, the diagonalization decomposition of  X   X  exists:  X   X  = THT  X  1 , where the columns of T are its eigenvectors, and H is a diagonal matrix of cor-responding eigenvalues. The kernel defined in Eq. (2) can then be rewritten as:
By setting  X  z =  X  z /z ! in Eq. (3), and use e x =  X  z =0 x z /z !, we have,
The diagonalization decomposition can greatly expedite WRWK kernel com-putation. An example of WRWK kernel is shown in Fig. 2. 3.2 Kernel on Super-Graphs The WRWK of single-attribute graph helps calculate the similarity between two graphs, so it can be used to calculate the similarity between two super-nodes. Given a pair of super-graphs G 1 and G 2 , assume we can generate a new product graph G  X  whose nodes are generated by super-nodes in G 1 and G 2 ,andweight value of each node is the similarity betw een the super-nodes which generate this node, then the same process shown in Section 3.1 can be used to calculate the WRWK for super-graphs G 1 and G 2 to denote their similarity.
 Definition 6. (Super Product Graph) Given two super-graphs G 1 =( V 1 , E 1 , G , F 1 ) and G 2 =( V 2 , E 2 , G 2 , F 2 ) , their Super Product Graph is denoted by G  X  2 =( V  X  , E  X  , G  X  , F  X  ) ( G  X  for short), where  X  V  X  = { V | V = &lt;V 1 ,V 2 &gt;,V 1  X  X  1 ,V 2  X  X  2 } ;  X  E  X  = { e | e =( V,V ) ,V  X  X   X  ,V  X  X   X  , F  X  ( V ) =  X , F  X  ( V ) =  X ,
An example of super product graph is shown in Fig. 2 ( G  X  ). Similar to Eq. (1), the adjacency matrix  X   X  of the super product graph G  X  can be directly derived from G 1 (  X  1 , X  1 )and G 2 (  X  2 , X  2 ) as follows:
Because we use the similarity between two super-nodes as the weight value of the node in the super product graph, the kernel value may increase infinitely with the increasing size of the super-graph. So for super-graph kernel, we add a control variable  X  to limit the range of the super-graph kernel value. Then the Weighted Random Walk Kernel, which computes the similarity between G 1 and G 2 , can be defined with a sequence of weights  X  =  X  0 , X  1 , for all i  X  N ): where N 1 and N 2 are the node sizes of G 1 and G 2 , respectively, and
Similar to WRWK of single-attribute graphs in Eq. (4), the WRWK on super-graphs can be calculated by setting  X  z =  X  z /z !, we have, where ww )  X   X  = T H T  X  1 . To ensure kernel function convergence, we set where  X  is the parameter of WRWK on super-graphs, and  X  is the parameter of WRWK on single-attribute graphs which is given in previous sub-section. The WRWK of super-graphs is also upper bounded with proof showing in Section 5. The WRWK provides an effective way to measure the similarity between super-graphs. Given a number of labeled super-graphs, we can use their pair-wise sim-ilarity to form an kernel matrix. Then generic classifiers, such as Support Vector Machine (SVM), Decision Tree (DT), Naive Bayes (NB) and Nearest Neighbour (NN), can be applied to the kernel matrix for super-graph classification.
Algorithm 1 shows the framework of using WRWK to train a classifier. Theorem 1. The Weighted Random Walk Kernel function is positive definite. Algorithm 1. WRWK Classifier Generation Proof. As the random walk-based kernel is closed under products [4] and the WRWK can be written as the limit of a polynomial series with positive coeffi-cients (as Eqs. (4) and (7)), the WRWK function is positive definite. Theorem 2. Given any two single-attribute graphs g 1 and g 2 , the weighted ran-is the parameter of the weighted random walk kernel.
 0. Then we only need to show that the upper bound of k ( g 1 ,g 2 )is e  X  .
Based on the definition of WRWK, assume the node sizes of two single-attribute graphs g 1 and g 2 are n 1 and n 2 , respectively, the number of random walks on g 1  X  g 2 must be not greater than that of the complete connect graph g c which has n 1  X  n 2 nodes. We assume that g c = g 1  X  g 2 ,where g 1 and g 2 are with n k ( g 1 ,g 2 )=  X 
Because  X  = 1 n graph, where the diagonal elements are equal to 0 and other element in the adjacency matrix are equal to 1.

So
Then
Because  X  z =  X  z z ! ,wehave
Because e x =  X  z =0 x z /z !, we have Theorem 3. The weighted random walk kernel between super-graphs G 1 and G 2 walk kernel parameters for single-attribute graph and super-graph, respectively.
Similar to Theorem 2, Th eorem 3 can be derived. 6.1 Benchmark Data DBLP Dataset: DBLP dataset consists of bibliography data in computer sci-tion with a number of attributes such as abstract, authors, year, venue, title, and references. To build super-graphs , we select papers published in Artificial Intelligence (AI: IJCAI, AAAI, NIPS, UAI, COLT, ACL, KR, ICML, ECML and IJCNN) and Computer Vision (CV: ICCV, CVPR, ECCV, ICPR, ICIP, ACM Multimedia and ICME) fields to form a classification task. The goal is to predict which field (AI or CV) a paper ( i.e. a super-graph) belongs to by using the abstract of each paper ( i.e. a super-node) and abstracts of references ( i.e. other super-nodes), as shown in Fig. 3. An edge (undirected) between two super-nodes indicates a citation relationship between two papers. For each pa-per, we use fuzzy cognitive map (E-FCM ) [5] to convert paper abstract into a graph (which represents relations between keywords with weights over a thresh-old ( i.e. edge-cutting threshold)). This graph representation has shown better performance than simple bag-of-words representation [1]. We select 1000 papers (500 in each class), each of which contains 1 to 10 references, to form 1000 super-graphs.
 Beer Review Dataset: The online beer review datase t consists of review data some attributes such as appearance score, aroma score, palate score, and taste score (rating of the product varies from 1 to 5), and detailed review texts. Our reviews. The graph representation for reviews is similar to the sub-graphs in DBLP dataset. Each review is represen ted as a super-node. The edge between super-nodes are built using following method: Because a review has four rating scores in appearance, aroma, palate, and taste, we use these four scores as a feature vector for each review. If two rev iews X  X  distance in the feature space (Euclidean distance) is less than 2, an edge is used to link two reviews ( i.e super-nodes). We choose 1000 beer products, half from Ale and the rest is from Large and Hybrid Style, to form 1000 super-graphs for classification. 6.2 Experimental Settings Baseline Methods: Because no existing method can handle super-graph classi-fication, for comparison purposes, we use two approaches to generate traditional graph representations from each super-graph: (1) randomly selecting one at-tribute ( i.e one word) in each super-node (and ignoring all other words) to form a single-attribute graph, as shown in Fig. 4 (C). We repeat random selection for five times with each time generating a set of single-attribute graphs from super-graphs. Each single-attribute graph set is used to train a classifier and their majority voted accuracy on test super-graphs is reported in the experiments. (2) We use the attribute set of the single-attribute graph in each super-node as multi-attributes of the super-node to generate an attributed graph (which is equal to removing all the edges from the super-graph as shown in Fig. 4 (B)). For all the two graph representations, we use the traditional random walk kernel ( RWK ) to measure the similarity between any two graphs [4].

We use 10 times 10-fold cross-validation classification accuracy to measure and compare the algorithm performance. To train classifiers from graph data, we use Naive Bayes (NB), Decision Tree (DT), Support Vector Machines (SVM) and Nearest Neighbor algorithm (NN). Majority examples are based on the kernel parameter settings:  X  = 100 and  X  = 100. 6.3 Results and Analysis Performance on Standard Benchmarks: In Fig. 5, we report the classi-fication accuracy on the benchmark data sets. The experimental results show that WRWK constantly outperforms traditional RWK method, regardless of the type of graph representations used by RWK . This is mainly because in traditional graph representations each node only has one attribute or a set of independent attributes, whereas single -attribute nodes (or multiple independent attributes) cannot precisely describe the node content. For super-graphs, the graph associated to each super-node pr ovides an effective way to describe the node content. In WRWK method, we consider the similarity between any two super-nodes as the weight value instead of just using 1/0 hard matching to rep-resent whether there is an intersection of attribute sets between two nodes. The soft matching node similarities between super-nodes captures the relationship between two graphs in a more precise way. This, in turn, helps improve the classification accuracy.
 Performance under different Super-Graph Structures: To demonstrate the performance of our WRWK method on super-graphs with different char-acteristics, we construct super-graphs on Beer review dataset by using different super-node sizes and different structures of single-attribute graph (the struc-ture of the single-attribute node is controlled by the edge-cutting threshold) as shown in Fig. 6 (a)-(d). The result shows that our WRWK method is stable on super-graphs with different structures.
 Performance w.r.t. Changes in Super-Nodes and Walks: The proposed weighted random walk kernel relies on the similarity between super-nodes and the common walks between two super-graphs to calculate the graph similarity. This raises a concern on whether super-node similarity or walk similarity (or both) plays a more important role in assessing the super-graph similarity.
In order to resolve this concern, we design the following experiments. In the first set of experiments (Fig. 7 (a) and (b)), we fix super-graph edges, and change edges inside super-nodes, which will impact on the super-node similarities. If this results in significant changes, it means that super-node similarity plays a more important role. In the second set of experiments (Fig. 7 (c)), we fix super-nodes but vary the edges in super-graphs (by randomly removing edges), which will impact on the common walks between super-graphs. If this results in significant changes, it means that walk plays a more important role than super-nodes.
Fig. 7 (a) and (b) report the algorithm performance with respect to the edge-cutting threshold. The accuracy decrease s dramatically with the increase of the edge-cutting threshold for both datasets. When the edge-cutting threshold is set to 0 . 0001 on DBLP and 0 . 00001 on Beer Review, the single-attribute graph in each super-node is almost a complete graph and four methods achieve the highest classification accuracy. As the threshold is set to 0 . 01 on DBLP and 0 . 1 on Beer review, the single-attribute graph in each super-node is very small and contains very few edges. As a result, the accuracies are just around 65%. This demonstrates that WRWK heavily relies on the structure information of each super-node to assess the sup er-graph similarities.

Fig. 7 (c) reports the algorithm performance by fixing the super-nodes but gradually removing edges in the super-graph of Beer review dataset (as Fig. 5 (b)). Similar to the result in Fig. 7 (a) and (b), the classification accuracy de-creases when edges are continuously rem oved from the super-graph (even if the super-nodes are fixed). From Figs. 7, we find that graph structure of super-graph is as important as that of super-nodes. This is mainly attributed to the fact that our weighted random walk kernel relies on both super-node similarity and walks in the super-graph to calculate graph similarities. In this paper, we formulated a new super-graph classification problem. Due to the inherent complex structure representation, all existing graph classification methods cannot be applied for super-graph classification. In the paper, we pro-posed a weighted random walk kernel which calculates the similarity between two super-graphs by assessing (a) the simila rity between super-nodes of the super-graphs, and (b) the common walks of the super-graphs. Our key contribution is twofold: (1) a weighted random walk kernel considering node and structure similarities between graphs; and (2) an eff ective kernel-based s uper-graph clas-sification method with sound theoretical basis.
