 Visualization as an important tool for exploratory data analysis has attracted much research effort in recent years. A multitude of visualization approaches, especially the nonlinear dimensionality reduction techniques such as Isomap [9], Laplacian Eigenmaps [1], Stochastic Neighbor Embedding (SNE) [6], manifold sculpting [5], and kernel maps with a reference point [8], have been proposed. Although they are reported with good performance on tasks such as unfolding an artificial manifold, they are often not successful at visualizing real-world data with high dimensionalities. A common problem of the above methods is that most mapped data points are crowded together in the center without distinguished gaps that isolate data clusters. It was recently pointed out by van der Maaten and Hinton [10] that the  X  X rowding problem X  can be alleviated by using a heavy-tailed distribution in the low-dimensional space. Their method, called t-Distributed Stochastic Neighbor Embedding (t-SNE), is adapted from SNE with two major changes: (1) it uses a symmetrized cost function; and (2) it employs a Student t-distribution with a single degree of freedom ( T 1 ). In this dimensional data.
 The t-SNE development procedure in [10] is restricted to the T 1 distribution as its embedding sim-ilarity. However, different data sets or other purposes of dimensionality reduction may require gen-eralizing t-SNE to other heavy-tailed functions. The original t-SNE derivation provides little infor-mation for users on how to select the best embedding similarity among all heavy-tailed functions. Furthermore, the original t-SNE optimization algorithm is not convenient when the symmetric SNE is generalized to use various heavy-tailed embedding similarity functions since it builds on the gra-manually specified. The performance of the t-SNE algorithm depends on laborious selection of the optimization parameters. For instance, a large learning step size might cause the algorithm to di-verge, while a conservative one might lead to slow convergence or poor annealed results. Although comprehensive strategies have been used to improve the optimization performance, they might be still problematic when extended to other applications or embedding similarity functions. In this paper we generalize t-SNE to accommodate various heavy-tailed functions with two major contributions: (1) we propose to characterize heavy-tailed embedding similarities in symmetric SNE by their negative score functions. This further leads to a parameterized subset facilitating the choice of the best tail-heaviness; and (2) we present a general algorithm for optimizing the symmetric SNE objective with any heavy-tailed embedding similarities.
 The paper is organized as follows. First we briefly review the related work of SSNE and t-SNE in Section 2. In Section 3, we present the generalization of t-SNE to our Heavy-tailed Symmetric SNE (HSSNE) method. Next, a fixed-point optimization algorithm for HSSNE is provided and its con-vergence is discussed in Section 4. In Section 5, we relate the EM-like behavior of the fixed-point algorithm to a pairwise local mixture model for an in-depth analysis of HSSNE. Section 6 presents two sets of experiments, one for unsupervised and the other for semi-supervised visualization. Fi-nally, conclusions are drawn in Section 7. Suppose the pairwise similarities of a set of m -dimensional data points X = { x i } n i =1 are encoded in a symmetric matrix P  X  R n  X  n + , where P ii = 0 and Embedding (SSNE) [4, 10] seeks r -dimensional ( r  X  m ) representations of X , denoted by Y = { y is minimized, where Q ij = q ij / bedding and The optimization of SSNE uses the gradient descent method with A momentum term is added to the gradient in order to speed up the optimization: rate; and  X  ( t ) is the momentum amount at iteration t . Compared with an earlier method Stochastic Neighbor Embedding (SNE) [6], SSNE uses a symmetrized cost function with simpler gradients. Most mapped points in the SSNE visualizations are often compressed near the center of the visualiz-ing map without clear gaps that separate clusters of the data. The t-Distributed Stochastic Neighbor Embedding (t-SNE) [10] addresses this crowding problem by using the Student t-distribution with a single degree of freedom as the embedding similarity distribution, which has a heavier tail than the Gaussian used in SNE and SSNE. For brevity we denote such distribution by T 1 . Using this distribution yields the gradient of t-SNE: In addition, t-SNE employs a number of strategies to overcome the difficulties in the optimization based on gradient descent. As the gradient derivation in [10] is restricted to the T 1 distribution, we derive the gradient with a the direct chain rule used in [10] may cause notational clutter and conceal the working components in the gradients. We instead employ the Lagrangian technique to simplify the derivation. Our approach can provide more insights of the working factor brought by the heavy-tailed functions. Minimizing J ( Y ) in Equation (1) with respect to Y is equivalent to the optimization problem: where the embedding similarity function H (  X  )  X  0 can be any function that is monotonically de-creasing with respect to  X  for  X  &gt; 0 . Note that H is not required to be defined as a probability function because the symmetric SNE objective already involves normalization over all data pairs. The extended objective using the Lagrangian technique is given by Setting  X   X  L ( q, Y ) / X  X  ij = 0 yields  X  ij = 1 / pliers to the gradient with respect to y i , we have  X  J ( Y ) where h (  X  ) = dH (  X  ) /d X  and We propose to characterize the tail heaviness of the similarity function H , relative to the one that leads to the Gaussian, by its negative score function S , also called tail-heaviness function in this H (  X  ) = (1 +  X  )  X  1 and thus S ( H ) = H .
 The above observation inspires us to further parameterize a family of tail-heaviness functions by the power of H : S ( H,  X  ) = H  X  for  X   X  0 , where a larger  X  value corresponds to a heavier-tailed differential equation  X  d log H (  X  ) /d X  = [ H (  X  )]  X  , which gives with c a constant. Here we set c = 1 for a consistent generalization of SNE and t-SNE. Thus the Gaussian embedding similarity function, i.e. H (  X  ) = exp(  X   X  ) , is achieved when  X   X  0 . Figure 1 shows a number of functions in the power family. Unlike many other dimensionality reduction approaches that can be solved by eigendecomposition in a single step, SNE and its variants require iterative optimization methods. Substantial efforts have been devoted to improve the efficiency and robustness of t-SNE optimization. However it remains unknown whether such a comprehensive implementation also works for other types of embedding similarity functions. Manually adjusting the involved parameters such as the learning rate and the momentum for every function is rather time-consuming and infeasible in practice.
 Here we propose to optimize symmetric SNE by a fixed-point algorithm. After rearranging the terms in  X  J / X  X  i = 0 (see Equation (11)), we obtain the following update rule: for HSSNE simply involves the iterative application of Equation (14). Compared with the original t-SNE optimization algorithm, our method requires no user-provided parameters such as the learn-ing step size and momentum, which is more convenient for applications. The fixed-point algorithm usually converges, with the result satisfying the stationary condition  X  J / X  X  = 0 . However, it is known that the update rule (14) can diverge in some cases, for example, when Y ki are large. There-fore, a proof without extra conditions cannot be constructed. Here we provide two approximative theoretical justifications for the algorithm.
 Denote  X  = Y  X  Y ( t ) and  X  the gradient of J with respect to Y . Let us first approximate the Then we can construct an upper bound of J lin ( Y ) : (14). Iteratively applying the update rule (14) thus results in a monotonically decreasing sequence Even if the second-order terms in the Taylor expansion of J ( Y ) are also considered, the update rule With the approximated Hessian H ijkl =  X  kl equation by directly inverting the huge tensor H is however infeasible in practice and thus usually implemented by iterative methods such as and only employ the first iteration of each inner loop. Then one can find that such an approximated imation technique has also been used in the Mean Shift algorithm as a generalized Expectation-Maximization solution [2]. Further rearranging the update rule can give us more insights of the properties of SSNE solutions: classical Gaussian mixture model (e.g. [7]), or more particularly, the Mean Shift method [3, 2]. This resemblance inspires us to find an alternative interpretation of the SNE behavior in terms of a particular mixture model.
 bound of sample has its own mixing coefficients because of locality sensitivity.
 logarithm as Maximizing this quantity clearly explains the ingredients of symmetric SNE: (1) P ij reflects that symmetric SNE favors close pairs in the input space, which is also adopted by most other locality preserving methods. (2) As discussed in Section 3, S ij characterizes the tail heaviness of the em-bedding similarity function. For the baseline Gaussian similarity, this reduces to one and thus has no effect. For heavy-tailed similarities, S ij can compensate for mismatched dimensionalities between the input space and its embedding. (3) The first factor in the exponential emphasizes the distance graph matching, which underlies the success of SNE and its variants for capturing the global data structure compared with many other approaches that rely on only variance constraints [10]. A pair of Q ij that approximates P ij well can increase the exponential, while a pair with a poor mismatch yields little contribution to the mixture. (4) Finally, as credited in many other continuity preserv-ing methods, the second factor in the exponential forces that close pairs in the input space are also situated nearby in the embedding space. 6.1 t-SNE for unsupervised visualization In this section we present experiments of unsupervised visualization with T 1 distribution, where our Fixed-Point t-SNE is compared with the original Gradient t-SNE optimization method as well as another dimensionality reduction approach, Laplacian Eigenmap [1]. Due to space limitation, we We followed the instructions in [10] for calculating P ij and choosing the learning rate  X  and momen-tum amount  X  ( t ) for Gradient t-SNE. Alternatively, we excluded two tricks,  X  X arly compression X  and  X  X arly exaggeration X , that are described in [10] from the comparison of long-run optimization because they apparently belong to the initialization stage. Here both Fixed-Point and Gradient t-SNEs execute with the same initialization which uses the  X  X arly compression X  trick and pre-runs the Gradient t-SNE for 50 iterations as suggested in [10].
 The visualization quality can be quantified using the ground truth class information. We adopt the measurement of the homogeneity of nearest neighbors: where  X  is the number of mapped points belonging to the same class with their nearest neighbor and n again is the total number of points. A larger homogeneity generally indicates better separability of the classes.
 The experimental results are shown in Figure 2. Even though having a globally optimal solution, the Laplacian Eigenmap yields poor visualizations, since none of the classes can be isolated. By con-trast, both t-SNE methods achieve much higher homogeneities and most clusters are well separated in the visualization plots. Comparing the two t-SNE implementations, one can see that our sim-ple fixed-point algorithm converges even slightly faster than the comprehensive and carefully tuned Gradient t-SNE. Besides efficiency, our approach performs as good as Gradient t-SNE in terms of both t-SNE objectives and homogeneities of nearest neighbors for these data sets. 6.2 Semi-supervised visualization Unsupervised symmetric SNE or t-SNE may perform poorly for some data sets in terms of iden-tifying classes. In such cases it is better to include some supervised information and apply semi-supervised learning to enhance the visualization.
 Let us consider another data set vehicle from the LIBSVM repository 3 . The top-left plot in Figure 3 demonstrates a poor visualization using unsupervised Gradient t-SNE. Next, suppose 10% of the intra-class relationships are known. We can construct a supervised matrix u where u ij = 1 if x i and x j are known to belong to the same class and 0 otherwise. After normalizing U ij = u ij /  X  is set to 0.5 in our experiments. All SNE learning algorithms remain unchanged except that P is replaced with  X  P . Figure 2: Unsupervised visualization on three data sets. Column 1 to 3 are results of iris , wine and segmentation , respectively. The first row comprises the learning times of Gradient and Fixed-Point t-SNEs. The second to fourth rows are visualizations using Laplacian Eigenmap, Gradient t-SNE, and Fixed-Point t-SNE, respectively. Figure 3: Semi-supervised visualization for the vehicle data set. The plots titled with  X  values are produced using the fixed-point algorithm of the power family of HSSNE.
 The top-middle plot in Figure 3 shows that inclusion of some supervised information improves the homogeneity (0.92) and visualization, where Class 3 and 4 are identifiable, but the classes are still very close to each other, especially Class 1 and 2 heavily mixed. We then tried the power family of HSSNE with  X  ranging from 0 to 1 . 5 , using our fixed-point algorithm. It can be seen that with  X  increased, the cyan and magenta clusters become more separate and Class 1 and 2 can also be identified. With  X  = 1 and  X  = 2 , the HSSNEs implemented by our fixed-point algorithm achieve even higher homogeneities (0.94 and 0.96, respectively) than the Gradient t-SNE. On the other hand, too large  X  may increase the number of outliers and the Kullback-Leibler divergence. The working mechanism of Heavy-tailed Symmetric Stochastic Neighbor Embedding (HSSNE) has been investigated rigorously. The several findings are: (1) we propose to use a negative score func-tion to characterize and parameterize the heavy-tailed embedding similarity functions; (2) this find-ing has provided us with a power family of functions that convert distances to embedding similari-ties; and (3) we have developed a fixed-point algorithm for optimizing SSNE, which greatly saves the effort in tuning program parameters and facilitates the extensions and applications of heavy-tailed SSNE. We have compared HSSNE against t-SNE and Laplacian Eigenmap using UCI and LIBSVM repositories. Two sets of experimental results from unsupervised and semi-supervised visualization indicate that our method is efficient, accurate, and versatile over the other two approaches. Our future work might include further empirical studies on the learning speed and robustness of HSSNE by using more extensive, especially large-scale, experiments. It also remains important to investigate acceleration techniques in both initialization and long-run stages of the learning. The authors appreciate the reviewers for their extensive and informative comments for the improve-ment of this paper. This work is supported by a grant from the Research Grants Council of the Hong Kong Special Administrative Region, China (Project No. CUHK 4128/08E).
 References [1] M. Belkin and P. Niyogi. Laplacian eigenmaps and spectral techniques for embedding and [2] M. A. Carreira-Perpi  X  n  X  an. Gaussian mean-shift is an em algorithm. IEEE Transactions On [3] D. Comaniciu and M. Peter. Mean Shift: A robust approach toward feature space analysis. [4] J. A. Cook, I. Sutskever, A. Mnih, and G. E. Hinton. Visualizing similarity data with a mixture [5] M. Gashler, D. Ventura, and T. Martinez. Iterative non-linear dimensionality reduction with [6] G. Hinton and S. Roweis. Stochastic neighbor embedding. Advances in Neural Information [7] G. J. McLachlan and D. Peel. Finite Mixture Models . Wiley, 2000. [8] J. A. K. Suykens. Data visualization and dimensionality reduction using kernel maps with a [9] J. B. Tenenbaum, V. Silva, and J. C. Langford. A global geometric framework for nonlinear [10] L. van der Maaten and G. Hinton. Visualizing data using t-SNE. Journal of Machine Learning
