 1. Introduction
In this information age, search engine acts as an efficient tool for seeking information from a vast heap of net has increased by more than 130-folds (from 1.3 to 171 millions quickly and also provide accurately what the users want ( Salton, 1998 ). Making use of phrases in searching and indexing seems to be a viable direction in improving the effectiveness of searching due to the following phrases have a smaller degree of ambiguity than their constituent words. That is, while two words are both uous interpretation of the other. Lastly, by using phrases as index terms, a document that contains a phrase would be ranked higher than a document that just contains its constituent words in unrelated contexts.
To enhance the accuracy, phrase is considered to be an important concept in information retrieval. Previ-more effective way than simply listing the results ( Paynter, Witten, Cunningham, &amp; Buchanan, 2000 ). traditional way of document indexing capable of providing fast searching through an enormous amount of document identification numbers, within-document frequencies and offsets at which the term appears. To set for a phrase query. However, for evaluating phrases containing common words, the process of merging tics do not retain the complete set of result documents.

Nextword index provides a fast alternative for resolving phrase queries, phrase browsing and phrase com-consumption which is around 60% of the size of the indexed data. With careful optimization techniques
An auxiliary nextword index proposed by Bahle, Williams, and Zobel (2002) further reduces the space over-head to only 10% of the size of the inverted index file. It reduces the disk access requirement in order to improve the efficiency.

In this paper, we propose a new indexing structure which we call common phrase index . Building on the each root-to-leaf path represents a phrase starting from a common word and ending in a terminal word (to be uating phrases efficiently from a large text database. Moffat and Zobel (1996) proposed that a compressed performance is possible by introducing additional locations at which decoding can commence into the com-pressed inverted list. In other words, the points enable a certain degree of random access for a compressed postings list and thus has some similarities to our index. However, our index applied the concept of phrase ment in efficiency. Last, our index is not required to determine the number of synchronization points in a group.

In our experiments, we implemented a prototype system to compare an inverted index, auxiliary nextword index and common phrase index against a set of benchmark documents and query logs. Our results show that common phrase index speeds up the overall efficiency and large-sized query evaluation by about 11% and 62% ger. The storage usage is just increased by about 19% of that of the auxiliary nextword index. If the common phrase index is implemented with dynamic feature, further improvement can be achieved.
The rest of this paper is organized as follows. Section 2 and 3 describe the inverted index, nextword index and auxiliary nextword index. Section 4 introduces our common phrase index and dynamic nature of common tion 6 is our conclusions. In the appendices, we present the implementation details of our prototype system. 2. Inverted indexes
Inverted index , or inverted file structure, is the most commonly used index structure for database manage-ment and information retrieval systems ( Kowalski, 1997 ). An inverted index is a two-level structure. The
Following the notation of Zobel and Moffat (1998) , each posting is a triple of the form: where d is the identifier of a document containing term t , f vocabulary search. For instance, suppose we have the following four documents: Document 1 { computer science } Document 2 { computer engineering } Document 3 { search engine } Document 4 { computer science search engine }
Then, the postings lists of  X  X  X omputer X  X  and  X  X  X cience X  X  are: &lt; computer , 3 ,&lt; 1 , 1 ,[ 1 ]&gt;, &lt; 2 , 1 ,[ 1 ]&gt;, &lt; 4 , 1 ,[ 1 ]&gt;&gt; &lt; science , 2 ,&lt; 1 , 1 ,[ 2 ]&gt;, &lt; 4 , 1 ,[ 2 ]&gt;&gt;
It indicates that document 1, 2 and 4 have one occurrence of  X  X  X omputer X  X  each at the position 1 of the doc-uments. The term  X  X  X cience X  X  exists at position 2 of both document 1 and 4. To resolve a phrase query  X  X  X om-one by one starting from the postings list of the rarest term to that of the most common term. The interme-query  X  X  X omputer science X  X  are document 1 and 4.

To support ranking, the postings in the list attached to each term can first be sorted according to the weighting. A method of calculating the weighting is term frequency ( tf ) and inverse document frequency ( idf ). When a search is performed, the temporary structure and the tuples are merged as in the previous isfy the query. 3. Nextword indexes and auxiliary nextword indexes
Inverted index is not efficient for evaluating query containing common words since the three most common to construct an index by recording additional index for supporting fast evaluation of phrase queries. is a postings list of the positions at which that firstword X  X extword pair occur. Using the same example we employed in Section 2 , the postings lists of all firstword X  X extword pairs of the nextword index are shown below: &lt; computer _ science , 2 ,&lt; 1 , 1 ,[ 1 ]&gt;, &lt; 4 , 1 ,[ 1 ]&gt;&gt; &lt; computer _ engineering , 1 ,&lt; 2 , 1 ,[ 1 ]&gt;&gt; &lt; science _ search , 1 ,&lt; 4 , 1 ,[ 2 ]&gt;&gt; &lt; search _ engine , 2 ,&lt; 3 , 1 ,[ 1 ]&gt;, &lt; 4 , 1 ,[ 3 ]&gt;&gt; liams et al. (1999) . However, the size of index is large as mentioned before.

Bahle et al. (2002) observe the weakness of resolving phrase query by using an inverted index and the enor-mous size overhead of a nextword index and hence proposed auxiliary nextword index. The main idea of the auxiliary nextword index is that only the top-frequency words are to be indexed with nextwords. For example, using the same sample documents as Section 2 and assuming that  X  X  X omputer X  X  is the only high-frequency or common word, all firstword X  X extword pairs of the auxiliary nextword index are: &lt; computer _ science , 2 ,&lt; 1 , 1 ,[ 1 ]&gt;, &lt; 4 , 1 ,[ 1 ]&gt;&gt; &lt; computer _ engineering , 1 ,&lt; 2 , 1 ,[ 1 ]&gt;&gt;
To resolve a query, the steps are similar to an inverted index. However, the auxiliary nextword index first contends with the common words of query terms, and then the rest. Experimental result in Bahle et al. (2002) shows that having the three most common terms as firstwords consumes just 10% of space of the inverted index. Therefore, a huge amount of space is saved compared with nextword index. An auxiliary nextword index works best with phrase query of size two because only one fetching is required. However, in the query log of Excite dating 1997 and 1999, queries of size two occupied only 35.28%. For queries of size one, which accounts for 25.36%, no index can be more efficient than an inverted index. For the remain-ing 39.36% of queries of size 3 or more, a nextword index has to perform at least two fetchings and one intersection. We will show that a common phrase index can further improve the efficiency for query size larger than two. 4. Common phrase indexes
The main difference between a common phrase index and an auxiliary nextword index is that the additional index terms are not fix-sized firstword X  X extword pairs but variable-sized common phrases . For each common phrase, there is a postings list containing the positions at which that common phrase occurs. 4.1. Common phrases
We define common phrase as a sequence of two or more contiguous words that starts with a common word and ends in a terminal word . We now give the definitions of common words and terminal words.
We define common words as those words having the highest frequencies in a set of queries. Thus our notion of common word is sensitive to the query workload. For concreteness, we take the Excite query log dating 1997 and 1999 as reference. We count the frequency of each distinct word of the query logs. Note that a word words in all single-word queries are not counted. The highest-frequency words are known as common words and the others are called rare words .

Terminal words are words which are likely to be the end of a phrase. We observe that in an auxiliary next-words to Merriam-Webster OnLine and retrieving the corresponding functions, see Table 1 . If a word has Table 1 includes misspellings and proper nouns.
 pronouns occupy only 0.408% of all last words of the queries. That is, a query often contains a phrase with last word having other kinds of function. Thus, we define the terminal word of our common phrase as any terminal words.

The structure of common phrase index is illustrated in Fig. 1 , where the C  X  X  are common words, R  X  X  are rare words, T  X  X  are terminal words and W  X  X  are the others. Note that both the auxiliary nextword index and common phrase index are additional index structure to an inverted index. Thus, for each common word in and common phrase index. For instance, suppose we have the following documents: Document 1 { students of the same year } Document 2 { computer and applications } Document 3 { usage of the search engine } word index for the documents contains: &lt; and _ applications , 1 ,&lt; 2 , 1 ,[ 2 ]&gt;&gt; &lt; computer _ and , 1 ,&lt; 2 , 1 ,[ 1 ]&gt;&gt; &lt; of _ the , 2 ,&lt; 1 , 1 ,[ 2 ]&gt;, &lt; 3 , 1 ,[ 2 ]&gt;&gt; &lt; the _ same , 1 ,&lt; 1 , 1 ,[ 3 ]&gt;&gt; &lt; the _ search , 1 ,&lt; 3 , 1 ,[ 3 ]&gt;&gt; while the common phrase index for the documents contains: &lt; and _ applications , 1 ,&lt; 2 , 1 ,[ 2 ]&gt;&gt; &lt; computer _ and _ applications , 1 ,&lt; 2 , 1 ,[ 1 ]&gt;&gt; &lt; of _ the _ same , 1 ,&lt; 1 , 1 ,[ 2 ]&gt;&gt; &lt; of _ the _ search , 1 ,&lt; 3 , 1 ,[ 2 ]&gt;&gt; &lt; the _ same , 1 ,&lt; 1 , 1 ,[ 3 ]&gt;&gt; &lt; the _ search , 1 ,&lt; 3 , 1 ,[ 3 ]&gt;&gt;
The number of contiguous words in an auxiliary nextword index is capped at two while that in a common down into  X  X  X f_the_same X  X  and  X  X  X f_the_search X  X  in the common phrase index. By issuing a query  X  X  X omputer mon phrase index has to perform just a single fetching.

Note that by using common phrase index, high efficiency in phrase evaluation can be sustained even if the query size is large. Our experiments in Section 5 show that improvement in query time of common phrase index enlarges when the query size increases. The further breakdown of the postings lists in a common phrase index supports high efficiency with low additional storage overhead compared with auxiliary nextword index. 4.2. Query evaluation
To evaluate a query with a common phrase index, we perform the following steps: (1) Identify the first common word of the query. (2) Expand the common word to a common phrase by adding the succeeding words of it until a terminal (3) Find the next common word from the query terms that are not yet covered . (4) Repeat Step 2 and intersect the fetched postings list with the temporary structure until all common 4.3. Dynamic nature of common and rare words
In our experiments, our common phrase index uses a set of common words that is static for a certain set of that occurred more than 1110 times in the 531416 unique queries. The top 8 terms with more than 5000 times the top 75 terms. However, some terms especially those having less than 2000 times each in queries ranked in the top 75 are not that stable. They are popular for querying at a certain period of time because of some in 1997 because many people are in memory of Diana, Princess of Wales. The high-frequency word  X  X  X iana X  X  eration. Similarly, the term  X  X  X rincess X  X  is ranked from 45th in 1997 and dropped to 362nd in both 1997 and 1999.

This observation suggests that a common phrase index should be dynamically changing according to the into the set of common words and also a set of disqualified common words. For those disqualified words, we just have to delete the common phrases and their corresponding postings lists. For those newly qualified words, we have to build a postings list for each of them. The details of the dynamic feature implementation original index at the search engine can then be replaced by the updated one.
 set and a dynamic set of common words. The result shows that using a dynamic set of common words can 5. Experimental results
In our experiment, we implemented a prototype system to evaluate phrase queries for comparing the effi-ciency and total size of inverted index, auxiliary nextword index and common phrase index. All experiments were run on an Intel 3 GHz Pentium 4-based server with 2 Gbytes of memory, running Linux operating system under light load. 5.1. Document and query sets
We used a set of benchmark documents and query logs in our experiments. For test documents, we used the .Gov web research collection from TREC. This collection is especially for information retrieval systems in a
Web context and large-scale information retrieval systems design and evaluation. It was collected in early 2002 and consists of 1247753 .Gov web pages with a total size of about 18.1 Gbytes.

Excite query log. Queries with sizes larger than 19 contribute only a small percentage ( 6 0.02%) of queries submit about 40% of queries with size larger than 2 in which our common phrase index can show improve-ment in efficiency.
 We did the following preprocessing to each document. First of all, any formatting information such as
HTML or XML tags are removed from the document. Special characters are replaced by blanks and all upper-case letters are changed to lower-case. After these, a document becomes a sequence of words separated by blanks. We then constructed the postings lists for each kind of indexing. 5.2. Comparison in efficiency and storage consumption
In our experiments, we first focus on comparison in efficiency between auxiliary nextword index and com-mon phrase index. We tried designating different number of words as common words in the vocabulary for both indexes, see Tables 3 X 5 .

For each size of the set of common words, we show the overall efficiency and also break down the results according to the query size in order to pinpoint where improvements are made. Note that, we group the que-for details.

For common word sets 10, 20 and 255, there are 6.35%, 8.92% and 11.25% improvements respectively in the sect operations, and hence, the query evaluation time.

For queries of size one and two, the common phrase index does not have any improvement as we have the improvement increases when the size of query increases. Common phrase index can even achieve an improvement of 61.74% when the query size is larger than or equal to six and common word size is 255.
We also compare the efficiency between inverted index, auxiliary nextword index and common phrase index, see Table 6 . Again, we observe that common phrase index has a better improvement (the rightmost two col-umns) than auxiliary nextword index.
 mon phrase index is just larger than that of the auxiliary nextword index by 8.41%, 12.11% and 18.73% for evaluation.

Note that the ANI with 254 common words in Bahle et al. (2002) shows about 1.3 Gbytes index size for a
Table 7 . For example, the size of ANI(255) is the sum of 2.77 Gbytes from inverted index and 1.34 Gbytes from the additional index size of the ANI(255). The additional index size includes the sizes of a vocabulary file, compressed inverted index and a Huffman code tree for decoding. 5.3. Comparison between dynamic and static CPI
The above experiments for the indexes are employing a static set of common words. To investigate the effect of using a dynamic set of common words in common phrase index, we simulate the query evaluation process as 10 query logs with 212716 queries each except the last one which has 212718 queries. Note that we preserve ries, the second part of query log contains the 212717th to 425432th queries and so on. We submit the 10 query logs to our prototype system one by one. This division of the original query log provides us with nine intervals to perform dynamic update. In order to compare the efficiency of dynamic CPI with that of static
CPI in the previous section, we build common phrase index using the static set of common words at the begin-ning of our experiment. After the system finished the first part of evaluation, we reindex the common phrase query logs to the system and repeat the above steps for the rest of the query logs.

The experimental results are presented in Table 8 . It shows that the implementation of dynamic feature in common phrase index can achieve an improvement of 2.84% and 6.8% for overall queries and large queries respectively compared with a common phrase index using a static set of common words. Comparing to an auxiliary nextword index, it shows that the dynamic common phrase index can improve 11.5% and 59.6% for overall and large queries respectively. Compared with an inverted index, the result shows an improvement locality of common words of our dynamic common phrase index, which adapts to the changing set of common words, performs better than the static common phrase index. Note that, we here only investigate the common phrase index with 20 common words and dynamic feature. However, we can still expect similar improvement for different sizes of common word set as we explored in Tables 3 X 5 .

We also show the total number of queries involving common words for different number of intervals (or updates) we apply to the Excite query log, see Table 9 . The results indicate that if we update the common phrase index more frequently, we will have more queries containing common words, and thus smaller average query time. 6. Conclusions
We have proposed a novel extension of auxiliary nextword index. Phrase queries on large text databases can be supported by using common phrase index . In this approach, all words in the text document are indexed the same as inverted index; in addition, the most common words are indexed via common phrase. Unlike the index and two words for an auxiliary nextword index), common phrase index has variable-sized index term.
These variable-sized index terms further break down the postings lists and support the fastest phrase query evaluation among inverted index and auxiliary nextword index. Having efficiency improvement especially index by about 19% for using 255 common words.

Our experimental results show that we can implement common phrase index for evaluating phrase queries without significant storage overhead. The only additional requirement is having a dictionary for checking of after we built it once.

We have also implemented and experimented a version of our index that can adjust its structure according to the dynamic nature of common words. Since the performance of a common phrase index is highly related to results show that further improvement in efficiency with the dynamic update can be achieved. Appendix. Implementation details
We implement our prototype system by using compressed inverted files for each kind of indexes. In this paper, we do not follow the original implementation in the preliminary version paper ( Chang &amp; Poon, 2006 ) because we want to get rid of all the possible effects introduced by the database management system. In our implementation, we employ the Huffman code algorithm to compress the files.

For each query evaluation, special characters of the query are replaced by blanks and all upper-case letters are changed to lower-case. After these, the query becomes a sequence of words separated by blanks. We then time + I/O time), number of bytes retrieved and number of intersect operations for each query evaluation for comparing the efficiency of different kinds of index. In Appendix A , we describe our implementation of the vocabulary and inverted files. In Appendix B , we present the algorithms for dynamic update. Appendix A. Vocabulary and inverted files of index to be scalable even if it has a large vocabulary like inverted index.

The examples of an inverted index, an auxiliary nextword index and a common phrase index for the .Gov corpus are shown in Figs. A.1 X  X .3 respectively.
 Inverted index is the simplest index compared with auxiliary nextword index and common phrase index. vocabulary accompanied by the corresponding file pointers pointing to the inverted file. Another file is the set of document tuples. A document tuple consists of its document identification number, the within-document 1999 ) to improve the inverted file compression.
 pointer pointing to the following postings list:
It means that the term  X  X  X f X  X  occurs in 1056681 documents. The first document has an identification number 1 and the term occurs nine times in the document at positions 5, 22, ... and 133. The second document has an identification number 2 and the term occurs one time in the document with location 3 and so on.
Auxiliary nextword index is an additional index on inverted index, see Fig. A.2 . Logically, for each com-nextword lists and the postings lists. As we can see, the structures of a common word and a nextword are the same. Hence, we put the common terms and the nextword lists in the same file in our implementation. The file pointer of a common term points to a list of nextwords which resides in the same file.
Unlike inverted index and auxiliary nextword index, common phrase index has variable-sized common vocabulary and an auxiliary nextword index has a two-level vocabulary. Although a common phrase index file. The file pointer of a leaf node points to a postings list in the inverted file.
 Appendix B. Dynamic feature
Dynamic feature can further improve the efficiency of phrase query evaluation as shown in Section 5 .To the last update. The update procedure is shown in Algorithm 1, 2 and 3. The algorithm has a high level pro-be created).
 Algorithm 1. Dynamic Update Function
Function reindex (old, new) 1: for all o 2 old do 2: if o 6 2 new then 3: deleteCommonPhrase ( o ) 4: for all n 2 new do 5: if n 6 2 old then 6: createCommonPhrase ( n ) Algorithm 2. Delete Common Phrase Function
Function deleteCommonPhrase ( commonterm ) 1: get tid by commonterm 2: for all node id with tid as root node term id do 3: delete all postings indexed by node id 4: delete the node indexed by node id Algorithm 3. Create Common Phrase Function
Function createCommonPhrase ( commonterm ) 1: set IncompeletedPhrases as empty 2: for all document 2 AllDocuments do 3: for all term 2 document do 4: for all icp 2 IncompeletedPhrases do 5: concatenate term at the rear of icp 6: if term is a terminal word then 7: for all icp 2 IncompeletedPhrases do 8: add icp to database 9: set IncompeletedPhrases as empty 10: if term is a common word then 11: add term to IncompeletedPhrases References
