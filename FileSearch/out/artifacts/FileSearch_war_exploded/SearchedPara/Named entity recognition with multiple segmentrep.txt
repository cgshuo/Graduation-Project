 1. Introduction
Named Entity Recognition (NER) aims to identify meaningful segments in input text and categorize them into pre-defined semantic classes such as the names of people, locations and organizations. This is an important task because its performance directly affects the quality of many succeeding natural language processing (NLP) applications such as information extrac-tion, machine translation and question answering. NER has been mostly formalized as a sequence labeling problem that per-forms the recognition of segments and the classification of their semantic classes simultaneously by assigning a label to each token of an input text.

While many researchers have focused on developing features that capture textual cues of named entities, there are only a (SRs) such as the IOB 2 and the IOBES notations. This issue has been extensively discussed for a different NLP task, word seg-mentation (WS). In this task, complex SRs consisting of four to six segment labels have been proposed based on linguistic  X  than the simple BI SR. 1 However, complex SRs are not always beneficial, especially when the size of training data is small, be-cause they can result in undesirably sparse feature space. In NER, the data-sparseness problem is an important issue because only a small portion of training data is named entities. Therefore, the use of a complex SR, which may better explain the char-acteristics of target segments than a simple SR, may not be much effective or even can bring performance degradation.
In this paper, we present a feature generation method that creates an expanded feature space with multiple SRs. The expanded feature space allows a model to exploit highly discriminative features of complex SRs while alleviating the data-sparseness problem by incorporating features of simple SRs. Furthermore, our method incorporates different SRs as feature functions of Conditional Random Fields (CRFs), so we can use the well-established procedure for training.
We also show that the tagging speed of a proposed model using multiple SRs can be boosted up as fast as that of the model using only the most complex SR of the proposed model. The proposed method is evaluated on the two NER tasks: the BioCreative 2 gene mention recognition task ( Smith, 2008 ) and the CoNLL 2003 NER shared task ( Tjong Kim Sang &amp; De Meulder, 2003 ). The experimental results demonstrate that the proposed method contributes to the improvement of NER performance.

The next section investigates several SRs developed for various NLP tasks, and explains a hierarchical relation among them that is the key concept to our proposed method. In Section 3 , we shows the effect of different SRs on NER and analyze the results in two ways. This analysis motivates the necessity of using multiple SRs for NER. Section 4 describes the proposed feature generation method that creates an expanded feature space with multiple SRs. We also show how to speed up the tagging speed of a model using the proposed method. In Section 5 , we present the experimental results and the detailed anal-ysis. Finally, Section 6 summarizes the contribution of our research and future work. 2. Segment representations
SRs are necessary for sequence labeling tasks that involve segmentation as a sub-task. This section introduces SRs used in various NLP tasks and presents a hierarchical relation among these SRs that will become the basis of our proposed method. 2.1. Segment representations in various NLP tasks Several SRs have been developed for and adopted to various NLP tasks such as NER ( Ratinov &amp; Roth, 2009 ), WS ( Xue, 2003; the definition of some of these SRs. Each SR in the SR type column consists of segment labels in the Segment Labels column.
The Examples column presents a few example label sequences of named entities, chunks and words with respect to the target tasks. We would like to note that the O label of the SRs in the NER and the SP tasks denotes a token that does not belong to any target segments. In WS, however, the O label is not necessary because every character of an input sentence is a part of a word.

In NER, the IOB 2 and the IOBES SRs have been used most frequently. The IOB 2 SR distinguishes tokens at the B eginning, the I nside and the O utside of named entities. On the other hand, the IOBES SR identifies tokens at the B eginning, the I nside and the E nd of multi-token named entities, tokens of S ingle token named entities and tokens of the O utside of named entities. In SP, the IOB 2 and the IOBES SRs work in the same manner as in NER. The IOE 2 SR uses the E label to differentiate the end tokens of chunks instead of the B label of the IOB 2 SR. The IOB 1 and the IOE 1 SRs are basically equivalent to the IO
SR that uses the I label to denote tokens of chunks and the O label to indicate tokens outside chunks. However, the IO SR cannot distinguish the boundary of two consecutive chunks of a same type. To overcome this problem, the IOB 1 SR assigns
B label to the token at the beginning of the second chunk, whereas the IOE 1 SR gives the E label to the token at the end of the first chunk. Lastly, in WS, the BI SR identifies the beginning and the inside of words, the BIS SR deals with single character words separately by assigning the S label to these words and the BIES SR uses the E label for the end characters of words. In addition, the BB 2 IES assigns the B 2 label to the second characters of words consisting of more than two char-acters, whereas the BB 2 B 3 IES gives the B 2 and the B 3 than three characters.
 Table 2 shows a sample text annotated with the seven SRs which will be used in this work. In addition to the IOB 2 and the
IOBES SRs that have been commonly used in NER, we also use the IOE 2 SR to investigate whether it is better to distinguish the beginning or the end of named entities. The IO SR is adopted as the simplest SR that actually does not perform any segmen-tation. Because two named entities are not likely to appear consecutively, we can recognize named entities as a sequence of tokens that have a same label. The BI , the IE and the BIES SRs, to the best of our knowledge, were proposed for WS and have not been used for NER. We apply these SR to NER by regarding the O label as a semantic class and augmenting it with the remaining segment labels. This application is based on the observation that tokens appearing around named entities are not random words. In this example, for instance, the left round bracket appears between the full name of a gene and its abbre-viation and the right round bracket occurs after the abbreviated gene name. Therefore, it is worth differentiating these to-kens from the others by assigning separate labels. 2.2. Relation among segment representations
Conceptually, only two segment labels are necessary (e.g. B -gene and I -gene for gene names) to distinguish segment boundaries unambiguously. However, many words tend to appear at specific positions not at random places. For example, the names of location often end with the words such as  X  X  X treet X  X ,  X  X  X oad X  X  and  X  X  X venue X  X  and the names of companies are fre-quently followed by the phrases such as  X  X  X orporation X  X  and  X  X  X o., Ltd. X  X  Therefore, complex SRs that can capture these char-acteristics of target segments are able to create a more informative feature space than simple SRs. Xue (2003) articulated that choosing a suitable SR is a task-specific problem that depends on the characteristics of segments and the size of available training data.

Segment labels of a complex SR often denote more specific positions than those of a simple SR. While every pair of any SRs can be inter-convertible if enough context information (segment labels of neighboring tokens) is provided, some of them are deterministically mappable by looking at only current labels. For example, to convert the IOBES SR to the IOB 2 SR, we can sim-hierarchical relation among the seven SRs used in the previous example in Table 2 . In this figure, a complex SR can be deter-ministically mapped to a simple SR if they are connected by directed arrow(s). Table 3 shows how to map the segment labels of the BIES SR to those of simpler six SRs.

The existing sequence labeling framework using the Viterbi algorithm assumes the Markov property for computational tractability. Therefore, it is impossible to use arbitrary context information for mapping segment labels of one SR to those of another SR. However, we can avoid this problem by considering only a subset of SRs that can be deterministically mapped from one SR to another SR as shown in Fig. 1 . For example, when we use the IOBES SR, we can utilize the features created from not only this SR but also the other SRs which can be deterministically mapped from it (e.g. IOB 2, IOE 2 and IO ). 3. The effects of different segment representations on NER
To investigate the effects of different SRs on NER, we performed a preliminary experiment on the BioCreative 2 gene men-tion recognition (BC2GMR) task ( Smith, 2008 ). For the experiment, we trained seven models with seven different SRs ( IO ,
IOB 2, IOE 2, BI , IE , IOBES and BIES ), but with the same textual cues. inally designed for the WS task and do not use the O label. We assumed a sequence of continuous O labeled tokens as a kind of special named entities, namely O -class named entity, and gave them separate O labels to apply these SRs to the NER tasks. For example, the BI SR uses the B -O and I -O labels instead of the O label.

For machine learning, we implemented a linear-chain CRFs with the L-BFGS algorithm. (2001) . defines a linear chain CRFs as a distribution: where x = h x 1 , x 2 , ... , x T i is an input token sequence, y = h y tion factor over all label sequences, T is the length of the input and output sequences, K is the number of features, f ture and k k is a feature weight for the f k .

In a linear-chain CRFs, f k is either a transition feature or a state feature. For example, a transition feature resents the transition from the B -gene label to the E -gene label of the IOBES SR, can be defined as defined as
Training a linear chain CRFs model is equivalent to find a set of feature weights which maximize a model log-likelihood for a given training data. However, it is often necessary to use regularization to avoid overfitting. We use the following model log-likelihood formula ( Sutton &amp; McCallum, 2007 ). The last term is for regularization.
The parameter C determines the strength of regularization and it can be chosen by using development data. A smaller C value liminary experiment, we reserved the last 10% of the original training data as the development data for tuning the C value. We examined ten C values 6 for each model and used the best performing C value for evaluation on the test data.
We used features generated from input tokens, lemmas, POS-tags, chunk-tags and gazetteer matching results. The de-tailed explanation of the feature set is in Section 5 .
 3.1. Evaluation based on standard performance measures
The seven models are evaluated in standard performance measures: precision, recall and F1-score. As shown in Table 4 , precision tends to improve as the number of labels increases. On the other hand, recall does not exhibit such a clear tendency where the IOE 2 and IOBES models achieve the higher recall than other models. If we follow the conventional approach, the
BIES SR, which has not been used for NER, will be most suitable for this corpus. 3.2. Evaluation based on the difference of tagging results
Although the evaluation in standard performance measures demonstrated that the BIES SR is most suitable for this corpus, we found that the tagging results of these seven models are quite varied. Table 5 shows how the tagging results change when negative (TN) and false positive (FP). Since the BIES model clearly outperforms the IO model, we anticipate that the BIES mod-el will produce more correct tagging results. The BIES model actually corrects 372 false negatives and 381 false positives of the IO model. However, surprisingly, it introduces new 254 false negatives and 235 false positives which are non-negligible amount of errors.

This analysis suggests that different SRs can produce feature spaces which are complementary to each other; and using multiple SRs is highly likely to improve NER performance. In the following section, we explain how to integrate multiple SRs into a CRF-based NER model. 4. The proposed method This section presents a feature generation method which incorporates multiple SRs into a single CRF-based NER model.
An expanded feature space created with the proposed method allows a model to exploit both high discriminative power of complex SRs and robustness of simple SRs against the data sparseness problem.

In Section 4.1 , we explain the mapping relation of the SRs, and design four groups of SRs for the proposed method. Sec-tion 4.2 describes a modified linear chain CRFs model which can automatically generate and evaluate features of multiple
SRs. In Section 4.3 , we show that a simple model computation after training makes the tagging speed of a proposed model using multiple SRs as fast as the conventional model using the most complex SR of the proposed model. 4.1. The mapping relation of segment representations
In Section 2.2 , we presented a hierarchical relation among seven SRs that can be deterministically mappable and ex-plained how to exploit multiple SRs without violating the Markov property. We call the most complex SR among all SRs used for a model as a main SR , and the other SRs as additional SRs . A conventional NER model can be interpreted as a model using only a main SR. For the experiment, we selected two most popular SRs, IOB 2 and IOBES , and the most complex one, BIES ,as the main SRs. As additional SRs, we basically use all deterministically mappable SRs to show the maximum effect of the pro-posed method. Three groups of SRs are shown in Table 6 and their names are marked with  X + X  symbol. In addition, we trained a model using only the BIES and the IO SRs, which are the most complex and the simplest SRs. This will minimize the increase of the total number of features, while allowing the model exploit complementary feature information of SRs in very different granularities. 4.2. A modified linear chain CRFs model for multiple segment representations
In Section 3 , we briefly introduced a linear chain CRFs. To enable a model to use features generated from multiple SRs, we define a set of feature sets, C ={ F l }, where F l is a set of features generated from the l SR. Then, we re-define a model as where f is a feature of a feature set F l of the SR l , and k features generated from multiple SRs.

However, we need to remind that a label sequence y belongs to the main SR. Therefore, it cannot directly evaluate the features of additional SRs. For example, a model, which uses the IOBES as its main SR and the IOB 2 as its additional SR, to which a label belongs.)
This feature cannot be directly evaluated because the input argument labels ( y the feature is of an additional SR ( IOB 2).

To solve this problem, we define a label conversion function, g the SR l . Then the transition feature above can be re-defined as
For g l ( y ), we use a deterministic conversion function that works as explained in Section 4.1 . This mapping function allows us to use well-established algorithms for training a model. 4.3. Boosting up tagging speed
A models using the proposed method generates more features and it inevitably slows down training speed. However, we can speed up the tagging speed of this model as fast as the model using only the main SR. The proposed method uses a deter-ministic label mapping function. It means that we know what kinds of features of additional SRs will be triggered for every feature of the main SR. By calculating the sum of feature weights that always appear together in advance and using it as the new weights for the main SR, the model can work as if it uses only the main SR. The model size and tagging speed will be identical to the model actually trained with the main SR only.
 5. Experiments
The proposed method is evaluated on two NER tasks in different domains: the BioCreative 2 gene mention recognition (BC2GMR) task ( Smith, 2008 ) and the CoNLL 2003 NER shared task ( Tjong Kim Sang &amp; De Meulder, 2003 ).
We added a necessary functionality 7 into our implementation of a linear-chan CRFs so that it produces features with a given iance of the model likelihood of the latest twenty models is smaller than 0.0001 or if it reaches the maximum number of iter-ations, 2000. 5.1. NER in the biomedical domain
To prepare the experiment, we performed the following pre-processing. First, the corpus is tokenized based on the same tokenization method in the previous work ( Leaman &amp; Gonzalez, 2008 ). Although this tokenization method produces more tokens than the Penn Treebank tokenization, 8 the output is very consistent: that is, no named entities begin or end in the POS-tagging and shallow parsing information. Lastly, we applied two gazetteers compiled from the EntrezGene ( Maglott, Ostell, Pruitt, &amp; Tatusova, 2005 ) and the Meta-thesaurus of the Unified Medical Language Systems (UMLS) ( Bodenreider, 2004 ).
Features are extracted from tokens, lemmas, POS-tags, chunk-tags and gazetteer matching results. The feature set for our biomedical NER system is listed in Table 7 and the symbols used for the features are explained in Table 8 . Most of these fea-tures are common for biomedical NER tasks ( Leaman &amp; Gonzalez, 2008; Lee, Hwang, Kim, &amp; Rim, 2004; Nadeau &amp; Sekine, 2007 ), while chunk features and several orthographic features are newly added. The L2-regularization parameter (C) is optimized by using the first 90% of the original training data as the training data and the rest 10% as the development data. Ten C values 9 are tested on the development data and the best-performing one is chosen for each model.

The BC2GMR task provides two types of annotations: the main and the alternative annotations. A gene name in the main annotation may have alternative names that are semantically equivalent but have different textual spans. Therefore, one can say that the official evaluation using both of them is based on a relaxed-match criterion. Table 9 summarizes the experimen-tal results of seven models using a single SR (the conventional models) and four models using multiple SRs (the proposed models) based on the strict-match and the relaxed-match (in a pair of parentheses). We use the strict-match results for com-paring the models because the detection of correct entity boundaries is also an important sub-task of NER and the relaxed-match results can underestimate it.

Conventional models tend to improve precision as they use more complex SRs than the baseline models baseline model (best BM) records the highest precision that is notably higher than that of the BM. However, recall does not ex-
Proposed models improve both precision and recall when they use complex SRs. In addition, every proposed model out-performs the conventional models that employ one of the SRs used by the proposed model. The best proposed model (best
PM) achieves higher recall (1.22%) and comparable precision ( 0.09%) to the best BM. The improvement of recall is an impor-tant merit of the proposed method because NER models frequently suffer from low recall due to an asymmetric label distri-bution where the O labels dominate the other labels ( Kambhatla, 2006 ) in training data. Considering that the only difference of the proposed models from the conventional ones is a set of SRs for feature generation, we can conclude that the proposed method effectively remedies the data sparseness problem of using complex SR while takes advantage of its high discrimina-tive power. This conclusion is also supported by the relation between the average number of feature instances per feature (AFI) and the number of features (#feat). For example, the best PM has about 20% higher AFI (15.60) than the best BM (13.44), whereas it has almost four times more features than the best BM.

To verify whether these improvements are meaningful, we performed the statistical significance test using the bootstrap re-sampling method ( Smith, 2008 ), which is commonly used for NER. Table 10 presents the estimated p values for the pro-posed models (the top row) against the conventional models (the leftmost column). In most cases, the proposed models have the p values lower than 0.05. Comparing a proposed model and its counterpart model, which uses the main SR of the pro-posed model, the p value decreases as the proposed model integrates more SRs of different granularity. As a result, the BIES + the null hypothesis against the best BM given the threshold p value 0.05. Considering that both the BIES &amp; IO and the IOB 2+ models use only two SRs, integrating SRs of very different granularities is more effective than that of similar granularity.
We also show how the tagging results change when the proposed method is applied. For the sake of analysis, we use two conventional models, BIES and IO , and the proposed model, BIES &amp; IO , that utilizes the SRs of the IO and BIES models. In
Table 11 , the tagging results of the two conventional models are divided into two groups depending on whether they make with  X  X  X greed X  X  shows the tagging results of the BIES &amp; IO model when the IO and BIES models make the same predictions. In most cases, the BIES &amp; IO model makes the same predictions with the conventional models ( P 96%). In the lower table titled with  X  X  X isagreed X  X , the two conventional models make different predictions and only one of them is correct. We can see that
BIES &amp; IO model makes less predictions same to the BIES model when it makes wrong predictions (from about 90% to 80%), even though the BIES model clearly outperforms the IO model by 2.73 points in F1-score.
 We present several gene names that are correctly recognized obviously by the help of the proposed method. For example,
BIES &amp; IO model correctly recognized a gene name mouse and human HPRT genes , whereas the BIES model recognized only a part of it, human HPRT genes . Both words, mouse and human , mostly appear at the beginning of a gene name (94 vs. 25 times human because it occurs almost four times more than mouse in the training data. On the other hand, the IO model, which correctly recognized this gene name, does not experience this problem because it can give the same I label to these words. the IO SR. There are similar cases where the BIES &amp; IO and IO models correctly recognized gene names such as serum insulin and type I and II collagen , while the BIES model recognized only the last word, insulin and collagen . These last words often appear as gene names by themselves (33 among 44 times for insulin and 8 among 16 times for collagen ). Therefore, the BIES model is likely to give the S label for these words.

However, incorporating the features of the IO model can cause difficulties in finding correct entity boundaries. For exam-ognized incorrect textual spans as upstream Oshox1 binding sites , phP1 mutation and Pms .

Next, we examined the effect of the proposed method based on the size of available training data. Models are trained on the first 10%, 20%, 40% and 100% of the original training data that is 15,000 sentences in total. Regularization parameters are tuned by using the last 10% of the original training data as the development data. For the models using 100% of the original training data, they are first trained on the first 90% portion for parameter tuning and the final models are trained on the full training data.
 Fig. 2 shows the precision of the three proposed models ( IOB 2+, IOBES + and BIES +) and their counterpart model ( IOB 2,
IOBES and BIES ). The precision of a proposed model is almost identical to that of its counterpart model at each point. In addi-tion, the models using more complex SRs achieve higher precision than the models using simpler ones regardless of appli-cation of the proposed method. This result shows that precision is mostly determined by the granularity (the number of segment labels) of the most complex SR employed by a model.

However, complex SRs can cause negative impact on recall. For example, in Fig. 3 , the BIES model records the lowest recall when the size of training data is 10% and 20% of the original training data. The low recall of the BIES model at beginning is due to the insufficient training data considering that it achieves similar or higher recall than other two conventional models as the size of training data reaches 40%. A proposed model, BIES +, on the contrary, achieves almost highest recall from the beginning and outperforms all other models as the size of training data increases. Therefore, by using the proposed method, we cannot only take advantage of high discriminative power of complex SRs but also boost recall by incorporating simple SRs.

In Table 12 , we compare the best proposed model (best PM) to the systems participated in the BC3GMR competition. The comparison is just for reference since BC2 systems exploit various techniques and external resources such as model ensem-ble, post-processing, abbreviation detection and resolution, semi-supervised learning, gazetteers and unlabeled data. This information is summarized in the last column of Table 12 . The best PM is also compared with BANNER 2009 ). It is placed between the 1st and 2nd ranked BioCreative 2 systems. The overview paper of BioCreative 2 competition tem rivals to the top performing system in the BioCreative 2 competition. Two recently proposed state-of-the-art systems ( Li et al., 2009 ; Hsu et al., 2008 ) achieve higher performance than the best PM. They obtain such a high performance by combining the results of multiple NER models. The best component NER model in each state-of-the-art system achieves 86.20 and 87.12 in F1-score respectively. Therefore, we can say that the best PM achieves the state-of-the-art performance as a single NER model.
In addition, there is a possibility that even better performance can be obtained by integrating the best PM into these systems.
While the proposed method produces a more desirable feature space for a model and improves its performance, the in-crease of the number of features inevitably slows down training speed. The last column in Table 9 shows the number of fea-tures for each model that is proportional to the training speed. The most complex model, BIES +, uses more than 60 million features; and the training speed is almost ten times slower than the IOB 2 baseline model. As a simple speed up technique, the
BIES &amp; IO model is trained with only two SRs, BIES and IO . Surprisingly, this model achieves comparable performance to to the conventional models when the training speed is important. 5.2. NER in the general domain The proposed method is also evaluated on the CoNLL 2003 NER shared task data which is a general domain NER corpus.
Features used in the study ( Kazama &amp; Torisawa, 2007 ) are adopted in this experiment. We used the POS and the chunking information originally provided in the CoNLL training data. However, gazetteers are not employed to observe the effects of our proposed method in isolation.

Table 13 shows the experimental results. The IE model achieves the best F1-score in this task. However, the difference compared to other models is not so significant, except the IO model. In addition, as a SR becomes more complex, the overall performance begins to decrease as shown with the IOB 2, IOBES and BIES models. The size of the training data could be a rea-son because the number of named entities is quite small. For example, named entities of the miscellaneous class only appear 3,438 times, whereas the training data of the BioCreative 2 corpus has almost 18,000 named entities of the single class, gene . SR increases as shown in the fifth column.

When the proposed method is applied, the performance of the proposed models ( IOB 2+, IOBES +, BIES + and BIES &amp; IO ) con-sistently improves. Especially, the BIES + model achieves the best performance for the test data while its corresponding base-line model BIES records the worst. Since the results are very similar to that of the previous experiment, we omit the detailed analysis on this task. 6. Conclusion &amp; future work
In this paper, we presented a feature generation method for incorporating multiple SRs into a single CRFs model. Our method creates a more desirable feature space; therefore, a model can exploit both features of complex SRs which provide high discriminative power and features of simple SRs which alleviate the problems that can be caused by the data-sparse-ness. Furthermore, we explained how a model computation after training can make the tagging speed of a model using the proposed method as fast as a model using a single SR.

The proposed method is evaluated on two NER tasks of biomedical and general domain corpora. The results demonstrated that our motivation of using multiple SRs is beneficial to better NER performance. In addition, we provided the results of the statistical significance test to show that the improvement is not by chance, and the detailed performance analysis to explain the effects of using multiple SRs for NER. Lastly, the evaluation on CoNLL NER corpus is also provided to show the domain independence of our proposed method.

Although many researches say that statistical NER systems have reached the plateau of performance, we think that still there is a room for meaningful improvement. Our method suggested one of such ways that use multiple perspectives for a problem. In addition, the proposed method is applicable to any segmentation tasks such as shallow parsing and word seg-mentation. We expect that the proposed method is also beneficial to these tasks too because the proposed model using mul-tiple SRs exhibited better performance than the best conventional model.
 References
