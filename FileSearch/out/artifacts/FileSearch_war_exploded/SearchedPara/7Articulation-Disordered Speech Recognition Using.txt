 CHUNG-HSIEN WU, HUNG-YU SU, and HAN-PING SHEN , Speech articulation-disorders are characterized by mispronouncing phonetic units by deleting, distorting, substituting, or inserting sounds which make speech difficult to understand. Articulation-disordered people may have only a couple of sounds in error, but will consistently make these e rrors on the same sounds in words. They usually suffer from difficulties in communicating with others in daily life due to dys-phasia or hearing impairments [Bernthal and Bankson 2004]. In this study we worked with people who have mild-to-moderate hearing impairment which induces articula-tion disorders. The invited subjects were hearing-impaired college students who have disorders with approximately 65 dB hearing loss. Augmentative and alternative com-munication (AAC) technologies and strategies can provide written or spoken assis-tance to people with temporary or permanent communication impairments. Recently developed speech processing technologies, such as automatic speech recognition (ASR) and Text-to-Speech (TTS), have improved the communication abilities of articulation-disordered (AD) people. Although there are as many as 2.5 million individuals in Taiwan with articulation disorders that a re due to many different causes, only around 700 speech therapists are qualified to provide articulation training and rectification to such people. Moreover, dealing with articulation disorders requires speech thera-pists to expend substantial time and effort in assessing pronunciation and selecting appropriate training courses. Accordingly, developing an assistive system for assess-ing articulation-disordered speech with ASR technologies can provide articulation-disordered individuals with a preliminary self-testing tool and therapists with a quick overview of an articulation disorder. Additionally, the assessment results from the sys-tem can also be used to predict possible articulations of the words while decoding the recognized phonetic sequence into a word sequence.

The scope for articulation-disordered speech recognition has recently been extended from isolated word recognition to domain-specific and even large vocabulary continu-ous speech recognition. Although ASR researchers generally focus on non-noisy, read speech recognition, which can obtain satisfactory recognition performance, a recent trend in this research is to address the practical problems of recognizing spontaneous speech instead of read speech. In general, spontaneous speech is likely to contain pro-nunciation variations or mispronunciations. Most proposed methods of recognizing spontaneous speech employ a multiple-pronunciation dictionary to deal with pronun-ciation variations. The pronunciation variants are added to expand the pronunciation dictionary. Information about pronunciation variants can be incorporated into the dic-tionary implicitly or explicitly [Tsai et al. 2007]. The explicit approach lists the surface forms of the words (which are the actual word pronunciations) observed from the cor-pus as pronunciation variations [Cremelie and Martens 1999; Torre et al. 1997]. The implicit approach, however, exploits several methodologies, such as phonological rules, decision trees, artificial neural networks, or confusion matrices to generate or infer the pronunciation variations from the canonical forms of the words (which represent the standard words) [Aubert and Dugast 1995; Nock and Young 1998]. One of the chal-lenges is that the performance may degrade as a result of the increasing pronuncia-tion entries. To address this, the pruning mechanism was used to get rid of undesired pronunciation entries according to counts, entropies, and acoustic likelihood. Nowadays, speech disorder assessment is most frequently conducted using the Frenchay test [Enderby 1988], which is used to analyze some important parameters of the motor speech system, guide treatment, and assist with neurological diagnosis. It has good reliability and validity between and within clinicians without extensive training. A system combining neural network and rule-based methods was proposed by Carmichael et al. to diagnose dysarthria [Carmichael et al. 2008]. Middag et al. [2009] utilized phonological features, automatic speech alignment, context-dependent speaker feature extraction, and intelligibility prediction based on a small model trained on pathological speech samples for automated intelligibility assessment of pathological speech. Applications of ASR can be found in the dialogue systems for avi-ation, travel, telecommunication, healthcare, and particular communication aids for hearing/speech-impaired people. Current ASR-related applications for articulation X  disordered people can be classified into assessment, therapeutic, and AAC. The typical assessment procedure is for the client to tr anscribe phonetic symbols exactly as they are pronounced, and the speech therapist then identifies the specific articulation disor-ders. For example, the computer-aided speech and language analysis method proposed by Serry et al. [1997] automatically extracts pr onunciation rules to improve the initial pronunciation models. However, the need to manually transcribe the speech samples and analyze the results is overly time-consuming. Although using ASR for automatic transcription is a practical approach, recognizing articulation-disordered speech is a major challenge in general-purpose ASR. In therapeutic applications, an ASR system recognizes the speech and provides feedback to the articulation-disordered people to reflect the accuracy or consistency of articul ation. The articulation-disordered peo-ple can thus try to refine their speech to fit the feedback [Rodriguez et al. 2008]. An AAC device is an assistive system to help p eople overcome communication barriers [Morris 1997]. Stevens and Bernstein [1985] compared the comprehension of disor-dered speech by a human listener and an ASR system and found that three out of five impaired speakers were better understood by the latter. Coleman and Meyers [1991] proposed an ASR system for translating difficult-to-understand speech into better re-alized transcriptions that can be represented by texts or synthesized voices. More re-cent studies have focused on recognizing dysarthric or disordered speech [Blaney and Wilson 2000; Doyle et al. 1997; Duffy 1995; Hawley et al. 2006; Kent 2003; Morales and Cox 2009; Mosbah 2006; Selouani et al. 2009] or providing a Human-Computer Interface for disabled people to control certain systems using only their voices [Green et al. 2003; Matsumasa et al. 2007].

In this study, we develop an effective and efficient approach to construct a speaker-adaptive automatic speech recognition (ASR) system for recognizing the speech of an articulation-disordered speaker with limited adaptation data. Figure 1 illustrates the framework of the proposed method for constructing a personalized ASR system from a universal ASR system trained from TCC-300. The TCC-300 is a Mandarin speech corpus containing 8,953 utterances. The proposed procedure is based on an unsu-pervised adaption mechanism that evaluates and chooses the confident recognition results from an ASR system, then iterative ly updates acoustic models to gradually fit the specific speaker. Automatic evaluat ion is performed according to a universal disordered-speech matrix and a posterior probability obtained from the ASR system. The universal disordered-speech matrix is constructed from clinical diagnoses that contain statistical data regarding possibl e disordered patterns in Mandarin. The diag-noses are carried out by speech therapists c ase by case, in which each of the single word pronunciations are recorded. In disordered articulations, the surface form of an utter-ance departs from the canonical form in di fferent ways. In the universal disordered-speech matrix, element (i,j) represents the frequency of the i-th consonant which is mispronounced as the j-th consonant. Therefore, an unsupervised adaptation process exploiting automatic transcription from ASR and knowledge-based measurement is used to choose the syllables paired with the corresponding recognized syllables with high confidence, which are termed  X  X igh confidence pairs, X  for acoustic model adapta-tion.  X  X igh confidence pair (C, S) X  means canonical form C is often mispronounced as S. The process is performed until the hig h confidence pairs no longer appear at the next iteration. The adapted ASR system obtains precise recognition results and the resulting confusion matrix can be used to discover the personalized articulation pat-terns. The recognized and manually transcribed syllables are further characterized and associated with each other using the articulatory features to expand the poten-tial articulation patterns that may not appear in the confusion matrix. The expanded articulation patterns are used in an explicit way to be included in a pronunciation dic-tionary, thus improving the accuracy of the personalized ASR system for articulation-disordered speech recognition. As this study is focused on the acoustic modeling for a specific articulation-disordered speaker for disordered speech assessment or recog-nition, insertion and deletion problem is likely to occur. This study adopts an empty unit into the universal disordered-speech matrix to deal with this problem. That is, a deletion can be regarded by a consonant being recognized as an empty unit, while an insertion is regarded as an empty unit being recognized into a consonant.

The rest of this article is organized as follows: Section 2 describes the proposed unsupervised adaptation and the application of automatic assessment in speech ther-apy. Section 3 then introduces the expansion of a pronunciation expansion dictionary based on phonological analysis. Next, Section 4 presents the evaluation and experi-mental results. Conclusions are drawn in Section 5, along with recommendations for future research. ASR is an attractive method to improve the efficiency of assessment in speech ther-apy performed on a one-to-one and case-by-case manner. In this study, a personalized ASR system is adapted from the universal ASR system and used for disordered speech recognition. Figure 2 shows a block diagram of the proposed unsupervised adaptation method. In this figure, the acoustic models of the universal ASR system are adapted based on the speech from a certain speaker with articulation disorder. In order to automatically adapt the acoustic models, the speech is labeled by the recognition re-sults, and a set of sub-syllable segments with high confidence is used for adaptation. In adaptation, acoustic models are iteratively used for recognition and then adapted by the high confidence sub-syllable pairs until no more high confidence pairs appear compared to the previous iteration. Phonological analysis is an important procedure for articulation assessment in speech therapy, and the identified errors can indicate the specific articulation behaviors of an articulation-disordered subject. Table I X  X II list the standard articulation places and manners which are represented by articulatory features for Mandarin phonetics. The most commonly identified articulation disorders based on phonological analysis are fronting, backing, aspirating, un-aspirating, voicing, un-voicing, stopping, affricat-ing, consonant omission, and vowel simplification. In these articulation disorders, several errors conflict with each other, su ch as fronting vs. backing. In addition, a unidirectional phenomenon (i.e., one phoneme can be easily mispronounced as the other, but the reverse cannot occur) can be o bserved in clinical diagnoses conducted by the Otolaryngological Department of National Cheng Kung University Hospi-tal. Accordingly, the pronunciations from disordered speakers may contain invalid consonant-vowel combinations or distorted pronunciations compared to standard Man-darin speech; therefore, conventional adaptation methods are not directly applicable to personalized acoustic model adaptation. Speaker adaptation is generally used to improve speech recognition accuracy while recognizing the speech from a specific speaker. In general speech recognition, adapta-tion can be directly applied with transcriptions of canonical forms (which is classified as supervised adaptation). However, articulation-disordered people have significantly distorted speech, and thus the canonical forms are quite different from the surface forms produced by such speakers. In this article, we propose an unsupervised, incre-mental adaptation approach to obtain a set of specific acoustic models to construct a personalized ASR system.

Selecting high-confidence pairs from the recognition results in the unsupervised adaptation process requires accurate recogn ition results and a universal disordered-speech matrix obtained from clinical diagnoses. In this article, 453 clinical diag-noses from speech therapy were provided by the Otolaryngological Department. An interesting phenomenon revealed by these diagnoses is that disordered articulations involved consonants much more than vowels. In general, the intensity of a consonant is much lower than that of a vowel, and thus consonants are much more likely to be disorderly articulated than vowels. Therefore, the proposed approach to the construc-tion of speaker-adaptive acoustic models and the discovery of personalized articulation patterns are only conducted on the 21 Mandarin consonants, while the vowel parts still use the universal acoustic vowel models.

For high confidence pair detection, the probability of the canonical form C c of a consonant pronounced as a surface form S c is defined as follows.
 where X denotes the possible pronunciation variations of the input speech of a canoni-cal form C c produced by the articulation-disordered speakers who conducted the clini-cal diagnoses. The P(S c | X) is the probability of X pronounced as Sc and P(X | C c )isthe probability of C c pronounced as X, which can be obtained from a universal disordered-speech matrix. The universal disordered-speech matrix is constructed based on the confusion statistic between every two consonants (including the empty unit) obtained over the data from clinical diagnoses. Each element in the universal disordered-speech matrix represents the frequency of mispronunciations from one consonant to another. Higher mispronunciation frequency means a consonant is more likely to be substituted by another consonant. The summation term of consonant C c being pronounced as S c obtained from the universal disordered-speech matrix, which is regarded as a reference for the effect of articulation disorders. The basic assumption for high confidence pair detection in this work is that if the likeli-hood of C c being recognized as S using the ASR system is greater than the probability of C c pronounced as S c obtained from the universal disordered-speech matrix, then C c is likely to be pronounced as S c . Therefore, the consonant pair (C c ,S c )isregardedas a high confidence pair, and so P(S c | C c ) in Equation (1) is adopted to detect the high confidence pair. That is, if the following inequality is satisfied, consonant pair (C c ,S c ) is regarded as a high confidence pair and selected for acoustic model adaptation. where  X  P ( S c | C c ) indicates the probability of C c being recognized as S c using the ASR system and can be estimated from a speaker-specific consonant confusion matrix con-structed from the speaker-specific data. The speaker-specific confusion matrix is con-structed using a standard ASR system to recognize the speaker-specific speech data, and the mis-recognition frequency from one consonant to another is used to construct the confusion matrix. Figure 3 shows the confusion matrix where element (C c ,S c )de-notes the misrecognition frequency of consonant C c with regard to consonant S c .
For acoustic model adaptation, the Maximum Likelihood Linear Regression (MLLR) [Leggetter and Woodland 1995] algorithm is used to adapt the universal acoustic mod-els using the collected speech data of high-confidence pairs for a specific speaker. The MLLR adapts the mean values of the Gaussian mixture components in the models, as follows: where  X   X  is the vector of the adapted mean values of the mixture components in an HMM state. The A is an n  X  (m + 1) transformation matrix for the state, n is the number of mixture components in a state and m is the number of states. The  X  is an extended mean vector defined as: where  X  is the original mean vector of the mixture components, and  X  is an offset term for distinguishing the training data from the adaptation data. The transformation matrix  X  A is determined by maximizing the equation with adaptation data  X  :
Using Equation (3) provides more precise and specific acoustic models for articulation-disordered speech recognition. In dealing with insertion and deletion problems, if the probability of insertion or deletion characterized by the empty unit for a specific subject is greater than a predefined threshold, the insertion or deletion will be regarded as a variation of disordered articulation and will be appended to the personalized pronunciation dictionary directly. As the adaptation process converges, several elements contained in the confusion ma-trix are too small to be reliable. Two filtering processes are thus employed to improve the precision of the confusion matrix to diagnose the articulation disorders. The first filter removes the elements with low frequencies and the second filter screens out the noisy elements in the confusion matrix. A confusion matrix element that appears in less than 5% of all tested sub-syllables is considered as a noisy element. The puri-fied matrix can show the articulation-disordered behaviors of a speaker, and these are usually unidirectional for an articulation-disordered individual. For example, if a speaker has a problem pronouncing consonants without aspiration, the speaker is likely to pronounce /b/, /d/, and /g / as /p/, /t/, and /k/, respectively, while the reverse articulation behaviors are unlikely to appear and should be disregarded. Since vowels are less likely to be mispronounced (according to the results from clinical diagnoses) than consonants, we focused on the 21 Mandarin consonants for the construction of speaker-adaptive acoustic models and the discovery of personalized articulation pat-terns in this work. Figure 3 shows an exampl e of the purification process applied to the confusion matrix for an articulation-disordered speaker. In the figure, the elements in the confusion matrix after purification can be enhanced and regarded as highly confi-dent disordered patterns of the speaker. Although the previous section has presented an approach to detecting disordered ar-ticulations at the sub-syllable level, articulation disorders also affect multi-syllable words. This section will deal with the phonetic-level transcriptions. Figure 4 shows a block diagram explaining how to discover and prune the potential articulation pat-terns. In articulation pattern discovery, syllables of canonica l forms and the corre-sponding recognized surface forms are first transformed into articulatory features based on their articulation manners and places and then combined to form  X  X rans-actions X . The Apriori algorithm [Agrawal et al. 1993] is then conducted to extract association rules from these transactions, and each extracted rule is converted into a set of pronunciation pairs that show the potential articulation patterns of the speaker. However, as the size of the discovered potential articulation patterns grows, a pruning mechanism is employed to remove the unsuitable patterns. Finally, these articulation patterns are used to expand the pronunciation dictionary to recognize the articulation-disordered speech of a specific speaker. The Apriori algorithm is a data mining algorithm that is widely used to obtain associ-ation rules from a series of transactions. A transaction is a collection of items. A set of items is termed as an itemset. The Apriori algorithm is an influential algorithm for mining frequent items as the association rules using a  X  X ottom-up X  method in which the itemset is extended one item at a time (a step known as  X  X enerator X ). When no further successful extensions are found, the algorithm terminates. Finally, the algo-rithm can find all frequent items and generate strong association rules. By adopting the Apriori algorithm, Mandarin syllables are considered as transactions, and the ar-ticulation patterns are considered as associ ation rules. Therefore, the key issue is to represent Mandarin syllables (transactions) by items. Each Mandarin syllable can be decomposed into three parts: consonant, glide, and vowel. Figure 5 shows the tree representation of a syllable structure. Table IV shows the coding table used to encode each syllable according to the articulatory features listed in Table I to Table III.
In this study, each Mandarin syllable can be characterized into a distinct composi-tion of items based on the coding table. Accordingly, the syllable-based codes of the canonical form and the surface form can be combined into the transaction used to discover the association rules using the Apriori algorithm. For example, a syllable /j-i-ang/ is pronounced as /k-i-ang/. The coded results of these two syllables are merged by the syllable pair as a transaction listed in Table V. The left-hand side of  X - X  indicates the type of articulatory features, and the right-hand side is the change of the fea-ture type. Table VI shows the Apriori algori thm, which extracts the association rules by iteratively joining the items with association rules from a previous iteration. The pruning process in Table VI is then used to prune the rules which have an infrequent sub-pattern according to the value of suppo rt, which is the number of transactions in which the association rule appears. The first step in articulation pattern discovery is to remove transactions that con-tain no variations for each item. For example, the pattern  X  X H-FF, GV-HH, GR-NN, VH-FF, VV-LL, VR-NN, XH-XX, XN-GG X  has n o articulation errors, so it cannot provide information for discovering potential articulation patterns and should be re-moved. Association rules with seven or more items are retained to discover the pos-sible varied pronunciation pairs. Since the discovered pairs are extracted from the articulatory features, the fewer features involved, the greater the number of varied pronunciation pairs that can be extracted. For example, the association rule  X  X P-12 X  can produce 96 articulation patterns, whic h are the number of syllables with /b/, /p/ and /m/ replaced with /f/. However, these results are too general to find a specific be-havior of articulation disorder. In this example, many conditions that have /b/, /p/, and /m/ can be replaced with /f/. Further, specific articulation patterns are undetectable. Hence, the more items that are included, the more constraints that are obtained for recovering the articulation p atterns from the association rules. Each of the remaining association rules contains at least one item that has a variation, such as GH-FL, XN-NG, CP-15, and so on. In the second step, the transactions should be converted into syllable pairs. Table VII demonstrates an example of converting the articulation pat-terns from a transaction. This transaction i s separated into a sequence of articulatory features and a sequence of varied articulatory features. According to the sequence of articulatory features, several syllables can be matched with the canonical syllables. In this example, three canonical forms: /c-a /, /c-ai/, and /c-ao/, are matched with the articulatory features in the extracted transa ction and can therefore be changed to /t-a/, /t-ai/, and /t-ao/, respectively, based on the varied articulatory features. The disordered articulation variants are then applied to the matched syllables to produce the potential disordered articulations. This procedure can be used to construct a confusion matrix to expand the pronunciation dictionary in ASR to improve the recognition results from the disordered syllable sequence for a specific speaker. In ASR, the optimal word sequence  X  W given a speech signal H can be obtained by the following equation: where P(H | W) denotes the acoustic likelihood of the speech signal H for a word se-quence W, and P(W) represents the sentence likelihood based on the language model. For the ASR proposed in this article, P(H | W) is estimated as where S indicates one of the possible surface-form syllable sequences, P(H | S) is the recognition likelihood given a possible surface-form syllable sequence S, and word se-quence W can be approximated by a canonical-form syllable sequence C of the input speech H while recognizing the articulation-disordered speech. The P(S | C) models the possible articulations produced by a specific speaker.

Given a small speaker-specific corpus, syllable articulations in an input speech se-quence are assumed to be independent. Based on the structure of a Mandarin syllable shown in Figure 5, each syllable can be separated into three parts: consonant, glide, and vowel. For a sequence with n syllables, P(S | C) can be estimated as: the sub-scripts c, g, and v indicate the consonant, glide, and vowel, respectively. The variation of disordered articulation for a syllable is verified and constructed by the three parts of a Mandarin syllable independently. Equation (4) shows the likelihood of a recognized surface form Si for a canonical form Ci of the input speech, and P(Si | Ci) is estimated by the number of Ci and Si pairs, which are the discovered potential articulation patterns: If the value of P(Si | Ci) is greater than a predefined t hreshold, Si will be regarded as a variation of disordered articulation for syllable Ci. The surface form Si will be appended to the personalized pronunciation dictionary for speaker-specific disordered speech recognition. Besides, the detected insertion or deletion variation for a specific subject will be appended to the personalized pronunciation dictionary too. In this study, a Mandarin ASR system [Huang and Wu 2007] was adopted to evaluate the performance of the proposed unsupervised adaptation method as well as the poten-tial articulation patterns discovered for disordered speech recognition. The Mandarin ASR system contains 152 acoustic sub-syllable models based on Mandarin phonology. Thus, vowels are classified into 38 acoustic models (including one for null), each of which has four HMM states. Consonants are classified into 114 models (including one for silence and one for null), each of which has three HMM states. One-state HMM is used for both silence and null models. Each state contains one GMM with sixteen Gaussian mixtures. The acoustic features are composed of 12 MFCC, 12 MFCC, 12 wasusedtotraintheacousticmodels. Th e TCC-300 is a microphone-recorded Man-darin speech corpus, which contains 8,953 utterances recorded by 300 native speakers (150 males and 150 females). In this study, we focus on robust acoustic model construc-tion for a specific articulation-disordered speaker for disordered speech assessment or recognition at the acoustic level. In order to avoid the effect from the language model and focus on the contribution of the proposed approach at the acoustic level, the lan-guage model is not used in any of the following experiments.

Five hearing-impaired college students who have mild-to-moderate disorders with approximately 65 dB hearing loss were invited to record two sets of utterances for adaptation and evaluation. A balanced corpus with 112 sentences designed to have balanced appearances for each Mandarin syllable was recorded for adaptation. Table VIII shows the statistics of the balanced corpus, and most of the syllables ap-pear more than twice. A test corpus with 50 sentences was randomly selected from the balanced corpus for evaluation. Each of the five subjects was required to record these two corpora (162 utterances). In evaluation, two transcriptions of phonetic sequences for these utterances were annotated; one was the canonical form generated automati-cally from the text-to-phoneme conversion system, and the other was the surface form which was manually annotated by two speech therapists. A preliminary evaluation was performed to test the capability of the standard ASR sys-tem in recognizing disordered speech. The evaluation included 500 normal utterances randomly selected from TCC-300 and 162 disordered utterances (112 for adaption and 50 for testing) were used for evaluation. Table IX lists the recognition results. Error rate is adopted for performance evaluation and computed as
The term  X  X estdata X  denotes the recognition unit. In the experiments, sub-syllable and syllable are chosen as the recognition unit. The recognized transcriptions indicate that the utterances of the subjects were more consistent in surface form than those in canonical form. The recognition results show that most of the vowel parts can be cor-rectly recognized, even for the vowels with disordered articulation. This is why vowels are not considered for acoustic model adaptation and articulation pattern discovery. The performance of the proposed unsupervised adaptation was evaluated by compar-ing with the acoustic models adapted with canonical forms. Table X shows the recog-nition results compared to the manually annotated ground truth.

The comparison results show that the proposed unsupervised adaptation achieved an ER reduction of 6.1% compared to 3.8% fro m the acoustic models adapted using the canonical forms. Table XI lists the recognition error rates for vowels and consonants. Adaptation using manually annotated surface forms only achieved a 58.4% syllable error rate and 45% sub-syllable error rate (average among five subjects). This experiment also examined the precision/recall rates of the articulation patterns discovered from the association rules with different itemset sizes, and the results were compared to an articulation pattern set obtained by human assessment. In human assessment, speech therapists examined each subject for articulation disorders at the sub-syllable level. According to the incorrectly pronounced consonants and vowels enu-merated by the therapists, the set of articulation patterns generated for a subject was considered as the ground truth for that subject. A set of potential articulation patterns extracted from the proposed approach was compared to the ground truth to estimate the recall and precision rates for pattern discovery. Figures 6 X 8 show the average re-call rate, precision rate, and F1 measure of p attern extraction for each subject. The F1 measure was computed from the F -measure with  X  = 1 as follows.

While the matched number of articulation patterns correlated with the number of the extracted association rules, the increas e in the matched articulation patterns fall behind the increase in the extracted articulation patterns. Figures 9 X 11 show the relations of the numbers of the extracted association rules and the discovered articu-lation patterns for different sizes of the itemset. For practical evaluation, the Department of Special Education at National Tainan Uni-versity provided 500 sentences commonly used in daily life. The sentences were used to develop a specialized ASR system to enhance the daily communication abilities of the deaf. The developed ASR system contained 3,191 frequently used Chinese words. Each subject in the experiment was asked to read the 500 sentences. Figure 12 shows the recognition results (word-level error rate) of the ASR system with the expanded pronunciation dictionary and the articulation patterns from the association rules of different sizes. The x -axis in the figure indicates the size of association rules, and the y -axis indicates the recognition error rate of the ASR system using the expanded pronunciation dictionary of different sizes. The figure shows that the overgrown articulation patterns caused by decreased itemset size resulted in ambiguous word construction from the recognized syllables. Table XII lists the syllable error rates for each subject using the subject-specific pronunciation dictionary under the condi-tion of association rules with a length of 10, which obtained the best recognition rate indicated in Figure 12. In Table XII, the use of the dictionaries for other subjects achieved results comparable to those obtained using the universal dictionary with-out pronunciation dictionary expansion, but lower than those obtained using subject-specific dictionaries. Accordingly, the articulation pattern sets substantially differed, possibly because the differences in articulation disorders were due to subject-specific behaviors. However, appropriate expansion of the pronunciation dictionary can boost the performance of ASR for disordered speech recognition. This article proposed an effective and efficient approach to develop a personalized ASR system for automatically assessing the speech of articulation-disordered speakers. The proposed approach, which is based on the traditional Mandarin ASR architecture, is used to adapt the acoustic models of canonical forms to precisely recognize the sur-face form of the produced utterances for a sp ecific articulation-disordered speaker. The recognized and manually transcribed syllables were used to discover the possible syllable set for expanding the pronunciation dictionary to be used in personalized ASR. The experimental results indicate that the proposed unsupervised adaptation method is robust in recognizing the surface forms of the input utterances and the incremen-tal adaptation process boosts the convergence of adaptation. In addition, the Apriori algorithm is adopted to extract possible articulation patterns through the recognized surface forms and its corresponding canoni cal forms. Using the discovered articula-tion patterns, the personalized ASR system can obtain promising improvements in recognizing the articulation-disordered utterances using the personalized pronuncia-tion dictionary expanded based on a speaker-specific small corpus (112 utterances). In general, a person with an articulation disorder has difficulty in producing cer-tain speech sounds. Types of errors include deletions, insertions, and substitutions. In this study, we focus on robust acoustic model construction for a specific articulation-disordered speaker for disordered speech assessment or recognition at the acoustic level. Accordingly, we dealt w ith the recognition errors by simply constructing robust speaker-adaptive acoustic models and discovering articulation patterns for pronunci-ation dictionary expansion to improve the recognition performance of the speech by a specific speaker. Besides constructing acoustic models and discovering articulation patterns, an appropriate language model will allow the ASR system to deliver a rea-sonable WER. In the future, language models can be employed to further improve the recognition performance. In addition, more p honetic knowledge about articulation can be integrated into ASR applications for further analysis.

