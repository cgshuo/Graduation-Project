 Institute for Theoretical Computer Science and chooses an appropriate action a of the action, i.e., the reward r prohibitively expensive.
 actions.
 immediate reward functions require that much larger sets of actions are being employed. state-action value function Q ( s , a ) and uses the greedy operator max weighted regression. The greedy operation max Regression (LAWER).
 policies. time is given in the form H = { &lt; s regression are generated by The regression problem for finding the function Q and the regression procedure Regress nonparametric function approximators.
 a Gaussian distribution for the belief of the optimal action . We sample n distribution. Then, the best e parameters of this distribution. The whole process is repea ted for k uniformly distributed set of sample actions.
 3.1 Weighted regression for value estimation V ( s ) = R of this integral. Consider the quadratic error function for probability density functions to derive an upper bound o f Equation (4) function Error (  X  V ) .
 Proof. To see this, we compute the square and replace the term R In difference to the original error function, the upper boun d Error wardly by samples { ( s  X   X  the factors  X  defines a weighted regression problem which is given by the da taset D weighted regression procedure WeightedRegress D Algorithm 1 FQI with Advantage Weighted Regression
Input: H = { &lt; s
Initialize  X  V for k = 0 to L  X  1 do end for the expensive integration over the action space for each sta te s at a finite set of state-action pairs. 3.2 Soft-greedy policy improvement policy  X  the knowledge of the mean m policy  X   X  with N ( A ( s , a ) | m tion. Thus, the denominator of  X   X  mated value function  X  V ( s ) is used to replace the greedy operator V ( s  X  rithm 1. As we can see, the Q-function Q Furthermore only already seen state action pairs ( s costs instead of the immediate one. to kernel based methods, our algorithm needs to be able to cal culate the similarity w a state s all s a similarity measure w a kernel w a Using the state similarity w function for each state s 4.1 Approximating the value functions is therefore given by: where  X  s diag ( w Q u where  X  s = [1 , s T ] T , S = [  X  s matrix. We bound the estimate of  X  V examples in this area are removed from the dataset. 4.2 Approximating the policy mean  X  ( s ) and the variance  X  2 ( s ) are given by where A = [ a weight of the initial variance in comparision to the varianc e comming from the data,  X  for all experiments. n 3-dimensional control variables. e was set to the number of state and action variables and n of position x for c Learned torque trajectories for c 5.1 Pendulum swing-up task The time step was set to 0 . 05 s . Two experiments with different torque punishments c c = 0 . 025 were performed.
 We used L = 150 iterations. The matrices D and D bottom position and 5 episodes starting from a random position. A comparison of the LAWER algorithm to CE-based algorithms fo r c 1(a) and for c the CE-Tree and the CE-LWR algorithm. In Figure 1(c) the traje ctories are shown for c and in Figure 1(d) for c 5.2 Acrobot swing-up task 100 steps. The matrices D and D 5.3 Dynamic puddle-world on the acrobot swing-up task (c) Setting of the 2-dimensiona l dynamic puddle-world. LAWER algorithm. (d) Torque trajectories learned with the CE -Tree algorithm. and D to initial position and 20 episodes starting from a random position. found by the LAWER algorithm look very smooth and goal directe d. iteration based algorithms. The computational complexity of the max operator max allows us to reduce the policy improvement step V ( s ) = max FQI algorithm based on LWR.
 lating max The simulations were run on a P4 Xeon with 3.2 gigahertz.
 on combining the AWR approach with the regression trees prese nted in [8]. for the academic internship which made this work possible.

