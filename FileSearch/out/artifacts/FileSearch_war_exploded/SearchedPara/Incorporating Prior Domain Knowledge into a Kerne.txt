 In the machine learning community, there is a popular belief that an increasing amount of features will enhance the performance of learning machineries, where the feature selection always reduces the information contained by the resultant models. Some re-search shows that irrelevant features do not increase the information, but introduce additional noise that eventually harms the performance of the resultant models. Peter Cheeseman suggests that all effects that have not been modelled add to the noise term [2]. Irrelevant features introduce noise into the learning process, also degrading the performance. Too many features cause the curse of dimensionality, which is always a negative result in machine learning. Simultaneously, the loss of strong relevant features degrades the performance of the resultant model too.

Many of existing feature selection algorithms emphasizes the discovery of the relevant features but ignore the elimination of redundant features. They suffer from quadratic, or even higher complexity about N , such that it is difficult to scale up high dimensionality. This paper proposes an appr oach to construct an optimal subset of features for a given machine learning algor ithm. The optimal subset of features con-tains the majority of relevant information with less redundancy. Mark Hall defined feature selection as  X  X uccessful if the dimensionality of the data is reduced and the accuracy of a learning algorithm improves or remains the same X  [3]. Daphne Koller el at formally defined the purpose of feature selection: let  X  and  X  be two distri-butions over some probability space  X  . The cross-entropy of  X  to  X  is defined as timal subset is a feature set G for which  X  G = f Pr ( f )  X  G ( f ) is reasonably small [4]. It is quite difficult to measure the difference,  X  G , especially in case of continuous data. Thus in practice some alternative ways to measure the difference  X  G are required to define the optimal subset.

The second section introduces some basic c oncepts of feature relevance and feature redundancy. The third section introduces the method of mutual information measure-ments and ML and MP constraints. The fourth section presents the algorithm and the fifth section follows by experiments. The sixth section concludes the whole paper. Before discussing the proposed method, it is necessary to define the related concepts. Considering supervised learning, the task of the induction algorithm is to induce a struc-ture (a decision tree or SVM) such that, g iven a new instance, it is possible to accu-rately predict the target Y . George John et al defined two concepts about relevance: Strong Relevance : A feature X i is relevant if f there exists some x i , y and s i for which p ( Y = y | X i = x i ,S i = s i ) = p ( Y = y | S i = s i ) . Weak Relevance : A feature X i is weakly relevant if f it is not strongly relevant, and there exists a subset of features S i of S i for which there exists some x i , y , s i with p ( Y = y the feature can sometimes contribute to prediction accuracy, but the strong relevance indicates that the feature is crucial and not replaceable with respect to the given task. It is obvious that an optimal subset of features must include strong relevant features. But in term of weak relevant features, so far there is no principle indicating which weak relevant features are included.

Thus in order to extract the optimal subset of features, it is necessary to introduce two other concepts: Markov blanket and redundant features. Given a feature F i  X  F , let M  X  F be a set of features that does not contain F i . We say that a set of features M is a Markov blanket for F i if and only if F i is conditionally independent of a subset of features that does not contain the M and feature F i , F  X  M  X  F i ,given M , P ( F  X  M  X  F i | F i ,M i )= P ( F  X  M  X  F i | M i ) [4]. If M is a Markov Blanket of F i , denoted as MB ( F i )= M , then it is also the case that class C is conditionally independent of the feature F i given M: p ( Y = y | M = m, F i = f i )= p ( Y = y | M = m ) ,thatis  X 
M =  X  M + F i . Redundant feature :Let G be the current set of features, a feature is redundant and hence should be removed from Giff it is weakly relevant and has a Markov Blanket M i within G [5].

It is worthwhile highlighting that the optimal set of features is approximately equiv-where the two structures seem very similar only from the data set (see Figure 1). In the figure 1a, two strong relevant features directly impact the target with the proba-features are weak relevance and can replace each other. It is clear that the feature f 1 impacts the target T through the feature f 2 . But if mutual information between the lationship between two features, it is impossible to distinguish these two graphs, so that where the information loss occurs.

Classical backwards feature selection, su ch as Recursive Feature Elimination (RFE) proposed by Guyon et al, implicitly remove s the redundant features, and might not uncover the optimal set [6]. For example, a backward feature selection with a high threshold, say greater than 0 . 12 , does not construct the optimal subset of features by excluding the feature f 1 in the first case (Figure 1a). But in the second case, it functions well. Contrary to the classical backwards feat ure selection, if the feature selection is based on these two graphs, the resultant optimal subsets are explicitly correct. At the same time, the backwards feature selection is v ery time-consuming. Some researchers suggests that for computational reasons it is more efficient to remove several features at a time at the expense of possible classification performance degradation [6]. Therefore another issue rises: how does one partition features and reduce the negative impacts on the classification performance. In order to eliminate redundant features from the candidates, it is important to measure the mutual information between a pair of features. In classical statistics and information theory, the correlation coefficient and cross e ntropy are two often-used measurements of the influence between features. However, in s ome practical application, especially the continuous data set, these two methods require discretisation as a pre-process. This is due to the fact that the cross entropy only works in case of discrete data set. The quality of the discretisation relies heavily on a users X  setting as during the process, and the important information might be lost. At the same time, regular correlation analysis only measures linear correlation coefficients. Therefore both of them are unable to measure the nonlinear correlation coefficients within the continuous data sets. The core idea of the proposed method is to convert a nonlinear problem into a linear problem via kernel mapping. Within the resultant feat ure space, the regular canonical correlation is employed to measure the impact between mapped features. As a result, canonical correlation is kernalized and extended into a nonlinear problem.

The kernalization of canonical correlation is not a completely new idea, and some unsupervised kernel methods already implement it. Canonical correlation analysis is a multivariate extension of correlation analysis, and is employed by the Independent Component Analysis (ICA) as a contrast function to measure the independence between resultant latent variables. Beyond the linear Canonical Correlation Analysis (CCA), the kernelized version of CCA works in a feature space. It utilizes extra information from a higher order element of moment function than the second order in the linear canonical correlation. The contrast function used in the kernel ICA developed by Bach and Jordan is employed as an approximation of mutual information between two fea-tures (or variables) [7]. The mutual information between two features can be written as: method is employed in the Kernel-based ICA algorithms. 3.1 ML and MP Constraints It is important to highlight that the Kernel CCA (KCCA) is an approximation to the real mutual information bet ween features. The accuracy of the approximation relies on the users X  tuning, as with other kernel methods. To some degree, domain knowledge such as known relevant features, usefully helps to tune and reduce the approximation error. In this part, known related features in the formats of constraints are introduced into the training process of the Kernel CCA, such as a grid search, to get optimal values of parameters. In some domains, a significant amount of relevant features are known. For example, in the research of audit quality, a linear formula is available and provides sets of features, clearly demonstrating their influences to the target.

Ian Davidson proposed must-link as a constraint for clustering. The Must-Link (ML) requires two instances to be part of the same cluster [8]. In this feature se-lection problem, the must-link constraints are slightly different from the concept de-fined by Davidson. The new ML requires that the results from the KCCA must show the higher correlation between known feature and the target than a given number: ML :  X  F i  X  S known ,Corr ( F i ,T ) &gt; X  . For example, the below equation has been known and verified by the current research of audit quality: LnAF =  X  1 +  X  2 LnT A +  X 
LnSub +  X  4 DE +  X  5 Quick +  X  6 Foreign +  X  7 CATA +  X  8 ROI +  X  9 Loss +  X  10 Opinion +  X  11 YE +  X  12 Intangible +  X  13 InverseMillsRatios + e [1], and then the ML can be defined as  X  F i  X  X  LnTA,LnSub,...,InverseMillsRatios } , ( Corr ( F i ,LnAF ) &gt; X  ) .

Another constraint represents the order of influence with each known relevant fea-ture to the target. This is the Must-Precede (MP): { F i ,F j } X  S known , ( Corr ( F i ,T )  X  tween F i and the target is larger than the correlation between F j and the target. Consid-ering the previous example, if the correlation between the feature LnT A and the target LnAF is bigger than the correlation between the feature LnSub and the target LnAF : { LnT A, LnSub } X  S known , ( Corr ( LnT A, LnAF )  X  Corr ( LnSub, LnAF ))  X  MP : { LnT A, LnSub } .
 In this proposed feature selection method, the domain knowledge in the format of ML and MP constraints guides the tuning process of the KCCA to measure the nonlin-ear correlation coefficient between features . By overcoming the difficulty of measuring the nonlinear correlation coefficients, one gains more insight into the interrelationship between features. The proposed algorithm assembles the previous discussion and consists of three major steps. The first step is a grid search which r epeats the KCCA between feature candidates until the results are consistent with the ML and MP constraints. The second step consists of a feature ranking by using a Recursive Feature Elimination (RFE) SVM proposed by Guyon et al [6]. The RFE is a kernel-based backwards feature selection method. With every iteration, it eliminates one or multiple features, testing the impact of elimination on the model coefficients learned by the SVM. If the impact of one removal feature is minimal among the candidates, it has the least influence in the resultant model. The output of the RFE is a list of ordered features. The order is determined by the influence of the features. For example, in the descending list, the most influent feature is the first one in the list. The third step follows Lei Yu and Huan Liu X  X  Fast Correlation-Based Filter (FCBF) to remove the redundant featur es based on the results from the previous two steps [9]. In the case of a descending list, a search starts from the beginning of the list. Suppose a feature F j precedes another feature F i ; if the correlation coefficient SU i,j between F j and F i is greater than the correlation coefficient R i,c between F i and the target T , the feature F i is removed from the list (see Figure 2). The search carries on until the end of the list, the result being an optimal subset of features. The pseudo code of the algorithm is outlined below: Algorithm 1. Feature Selection Algorithm 1: procedure F EATURE S ELECTION ( S ( F 1 ,F 2 , ..., F N ,T ) ,ML,MP, X  ) 2: S list =FeatureRanking(S) 3: while (  X  SU i  X  SU ) SU i { ML,MP } do 4: SU= GridSearch(KCCA(S, S known )) 5: end while 6: F j = getFirstElement( S list ) 7: while F j = NULL do 8: F i = getNextElement( S list , F j ) 9: while F i = NULL do 10: if SU i,j &gt;R i,c then 11: remove F i from S list 12: end if 13: F i =getNextElement( S list , F i ) 14: end while 15: F j =getNextElement( S list , F j ) 16: end while 19: end procedure
In this algorithm, prior knowledge in the format of known relevant features is rep-between the target T and each of known relevant features F i  X  S known , and 2) a set of must-precede constraints MP ( F i ,F j ) between known relevant features, where Corr ( F i ,T )  X  Corr ( F j ,T ) and F i ,F j  X  S known . These constraints play a crucial role in directly determining the accuracy of the measurement of KCCA. For example, if the RBF is employed as the kernel functions, the grid search aims to detect the opti-determines mapping between the input space and the feature space. First, the author modifies a data generation provided by Leo Breiman [10] to produce vations. The modification is: within ten features, assign the 0 . 2 time of the value of fifth the thirty features of this artificial data set, there are six weak relevant features, three strong relevant features and twenty-one irrelevant features. The index of weak relevant features is 4, 14, 24, 5, 15, 25 and the index of strong relevant features is 6, 16, 26. The Recursive Feature Elimination (RFE) SV M produces a list of ranked features and the values of R-square while eliminating one feature every iteration (See Figure 3). The curve, the weak relevant features and strong relevant features mix with a few irrelevant features. It is difficult for the backward seque ntial feature selection to discover the re-dundant features from the weak redundant features and then uncover the optimal subset of features. By taking account of the mutual information between feature candidates, the proposed feature selection discovers the strong correlation between feature candi-dates. At the same time, the correlation between the 5th feature and the target is larger than the correlation between the 4th feature and the target. Therefore the optimal subset of feature produced by the proposed feature selection algorithm includes the features { 6, 28, 27, 26, 5, 16, 25, 15 } . The mean R-square of 10 times cross-validation of a SVR with the same hyper-parameter as the previous SVRs is 0.689527. That is close to the best R-square value of the previous backwards sequential feature selection.
Secondly, a real-world data set is employed to test the proposed feature selection algorithm. The data set is collected from the auditing and accounting reports from listed companies in the Australian Stock Exchange (ASX) in 2003. To ensure an appropriate data set for the experiments, we first need to exam whether the data set contains both relevant and redundant features. The RFE SVM produces an ascending list of 39 ranked features (more important features are close to the end): { 35, 30, 29, 34, 33, 32, 17, 1, 18, 4, 8, 31, 14, 36, 37, 10, 13, 15, 3, 25, 12, 28, 24, 26, 22, 27, 16, 20, 21,11, 2, 23, 38, 7, 9, 6, 19, 5, 39 } . The same as previous experiment, Figure 3 demonstrates the test result of nonlinear Radial Basis Function Support Vector Regression (RBF SVR), with each iteration removing one feature. The ord er of feature removal follows the order of feature-ranking list. The flat middle par t of the curve indicates that the eliminated features do not have strong impacts on the test accuracy of the resultant model, and this result indicates the existence of redundant features.
According to the proposed algorithm, the Kernel CCA is employed to measure the correlation coefficients between features. If t he correlation coefficient between features is greater than 0 . 05 ( KCCA &gt; 0 . 05 ), the coefficient is retained in the final KCCA table. Otherwise, this experiment sets two of the corresponding features as irrelevant. In the final KCCA table, the index of the features without strong correlations to the known relevant features is (10, 14, 25, 29-32, 36-37). Based on these results, the pro-posed algorithm generates an optimal subset of features consisting of 16 features: { Loss Indicator (1), YE (8), OFOA (31), Current Liabilities (14), 12 Months (36), Currency (37), Current Assets (10), DE (3), QUICK (2), AF/EBIT (38), CATA (7), FOREIGN (9), LnSUB(6), Partner Code (19), LnTA (5), AF/TA (39) } . Using the SVR with 100 random sampling 80% of the original data set as the training data, the average test result (R-square) is 0 . 8033 with the standard deviation 0 . 0398 . This result is slightly higher than 0 . 774 , the best test result (R-square) using the SVR with the same hype-parameters but the subset of features produced by the backwards feature sequential selection.
 This research expands Lei Yu and Huan Liu X  X  Fast Correlation-Based Filter (FCBF) [9]. It does so primarily in that it implements the redundancy measurement in feature selection for the non-linear regression. As domain knowledge, known relevant features are included in the process to guide th e process of the KCCA, under the condition that a sufficient amount of known relevant features is available. Considering the human involvement, it is worth ensuring whether the KCCA produces an appropriate approx-imation to the real mutual information. In this research, however, domain knowledge from experts is utilized to guide tuning of the parameters of selected kernels.
The results of these experiments show that the optimal set of features increases the accuracy to a relatively high level with relati vely small optimal subset of features. Sim-ilar idea can be found in Carlos Soares X  X  meta-learning methods to select kernel width in SVR [11]. Their meta-learning methodology exploits information about past exper-iments to set the width of the Gaussian kernel. In the feature selection experiment, domain knowledge collected from domain experts X  past experiments is included to set the width of the kernel. At this stage, the result still relies heavily on the given domain knowledge with the assumption that the given domain knowledge is perfect. The most pertinent area for further investigation is the negative impacts of given domain knowl-edge on the resultant subset of features. It is still not clear that known related features will become redundancy when other features are included. The desired optimal set of features may contain all or a subset of known features. It is still an open question.
