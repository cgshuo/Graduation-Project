 YINGGONG ZHAO, SHUJIAN HUANG, XIN-YU DAI, and JIAJUN CHEN , For statistical machine translation (SMT) systems, the language model (LM) plays a crucial role in ensuring the fluency of translation outputs. Traditional n -gram LMs model over discrete representations of words and thus face severe data sparseness. To tackle this issue, neural network LMs are proposed [Bengio et al. 2003], which use low-dimensional dense rather than one-hot representation for words. These models are shown to be effective in various MT tasks [Vaswani et al. 2013; Auli et al. 2013]. However, current neural LMs used in MT systems usually predict the probability of the current word based on previous words within the sentence. Modeling merely on local context misses some important global information, such as topic distribution of sentences or documents. Without such global information, it is hard for MT systems to choose appropriate candidates when translating a sentence from a specific domain.
One practical solution to the local context issue is to take topic information into consideration. There are two issues that need to be tackled under this approach: the first is how to add the topic distribution to a neural network LM so that the LM could still be efficiently incorporated into MT systems. The second is how to predict target side topics before translation starts, considering that target translations are generated from scratch during decoding, which makes the estimation of the target topic distribution from source words impractical.

Although topics have been shown to improve MT performance when incorporated into back-off n -gram LMs, these approaches either utilize topics only at the word level as adaptation [Tam et al. 2007; Ruiz and Federico 2011] or require costly linear mixture on topicwise LMs [Yu et al. 2013]. Such problems can be avoided in a certain way when topics are incorporated into a recurrent neural network LM (RNNLM) [Mikolov and Zweig 2012]. However, considering that RNNLM requires all previous words as context to compute LM probability, it can be applied only to a phrase-based decoder [Koehn et al. 2003] and expels more powerful CKY paradigm [Chiang 2007; Marcu et al. 2006; Liu et al. 2006; Shen et al. 2008] decoders. In this article, we follow this direction, but in contrast to Mikolov and Zweig [2012], we add topics to a feedforward LM [Bengio et al. 2003] for the MT task. The advantage of feedforward LM is that it accepts fixed context and can be used by all MT decoders for direct decoding. Moreover, feedforward LM can outperform RNNLM on an MT reranking task, although RNNLM usually achieves lower perplexity [Mikolov 2012; Zhao et al. 2014].

To predict target topics, we exploit a multinomial logistic regression (MLR) model. In contrast to previous-topic concurrence direct mapping [Xiao et al. 2012; Yu et al. 2013; Zhang et al. 2014] and the maximum entropy (ME) model [Xiong and Zhang 2013], our MLR model can be directly optimized based on the loss of topic distribution and is flexible to both binary and real features.
 For the features of the prediction task, previous work [Xiao et al. 2012; Yu et al. 2013; Zhang et al. 2014; Xiong and Zhang 2013] uses only source topics to predict target topics and achieves a good performance. However, topic is only part of the information that the source text contains;, a lot of source information will be lost if using merely source topics for prediction. Inspired by Le and Mikolov [2014], we propose a modified neural network model to learn joint representations of the source input text, which captures information on different levels and can boost the topic-prediction model.
In summary, in this work we incorporate topics into a feedforward LM and propose an MLR model with joint source representations for topic prediction. Experiments demonstrated that not only topic-enhanced feedforward LM reduces the perplexity significantly, but also both topic-prediction models and source side joint representa-tions contribute to learning better target topic distributions. On a large-scale Chinese X  English MT experiment, our topic-enhanced feedforward LM with predicted topics significantly improves the translation performance by 1.0B LEU when adopting only feedforward LM. The improvement still reaches 0.3 to 0.4B LEU under a strong setting that uses both n -gram and feedforward LMs.

The rest of this article is organized as follows: in Section 2, we briefly review the feedforward neural network LM, which is the basis of our work. We describe how to incorporate topics to feedforward LM in Section 3. The MLR model for target topic prediction is presented in Section 4. In Section 5, we describe a neural network model for learning joint representations. Comprehensive experimental results are reported in Section 6. Section 7 describes related work. We discuss our conclusions and future plans in Section 8. Typically, a feedforward LM [Bengio et al. 2003] models the probability of the current word w given context u with a fixed size. Given LM order n , context u is composed of previous ( n  X  1) words, and w ranges over words in the vocabulary V .

Following the architecture proposed by Vaswani et al. [2013] (Figure 1), the neural network computes the probability P ( w | u ) for given word w as follows: (1) The embeddings of the words in u are concatenated at the input layer. Then the (2) h 1 is then mapped to the second hidden layer. The corresponding output h 2 is: (3) Finally, at the output layer, the probability of the output word w is computed as:
However, according to Equation (3), it is hard to scale up feedforward LM on a large training corpus as an expensive softmax summarization over whole vocabulary V is always required during both training and testing steps. So far, three strategies are proposed to overcome this barrier: class-based factorization [Goodman 2001], tree-based factorization [Mnih and Hinton 2009], and noise contrastive estimation (NCE) [Gutmann and Hyv  X  arinen 2010]. Baltescu and Blunsom [2015] compared these meth-ods in details, showing that the NCE approach always performs the best in both performance and efficiency. Besides, a key advantage of the NCE approach is that the normalization term can be ignored during MT decoding [Vaswani et al. 2013; Zhao et al. 2014], which is 10 times faster than the other two methods [Baltescu and Blunsom 2015]. Therefore, in this work, we adopt NCE for feedforward LM training. The motivation of introducing topics to feedforward LM is to incorporate global infor-mation beyond the local context. In a typical feedforward LM with two hidden layers, there are different positions to add the topics. Here, we take a simple strategy that we append the topic vector to the input layer (Figure 2), so that the topics could exert in-fluence on the whole neural network. Under this scenario, the input to the first hidden layer h 1 turns to be: where T is a weight matrix between the input and the projection layer for topic distri-bution t , while the rest of the symbols remain the same as in Equation (1).
We should note that computation on the rest of the layers is the same as conventional feedforward network. In addition, adding topics to a neural network brings little bur-den, as only an extra parameter size O ( | T | X | h 1 | ) is required, where | T | is the number of topics and | h 1 | is the size of the first hidden layer h 1 .

During NCE training [Gutmann and Hyv  X  arinen 2010], negative instances are sam-pled according to word probability distribution estimated from the training text. When topic distribution is considered, word probability might be estimated conditioning on topics, which makes the learning much heavier on both memory and efficiency aspects. Therefore, we still use the conventional word probability-based sampling and leave the topic-dependent sampling as future work. As the topic-enhanced LM adopted in the SMT system is built on a target-side monolin-gual corpus, the topic distribution of the target side should be given when translating source-side text. However, such information is not available in advance and it is imprac-tical to dynamically infer the topic distribution of partial translations during decoding considering severe accuracy and efficiency obstacles.

Instead, we attempt to infer target topic distribution indirectly by prediction. Ac-cording to the definition of parallel corpus, one sentence pair should share consistent meanings (including topics). Therefore, we could predict the corresponding target-side topics with source-side information (including topics). This avoids the heavy computa-tion of target-side topic posteriors on the fly. Here, we could not simply use source-side topics as target-side because topics on the source and target sides are not identical, as the corpus for building LMs is usually monolingual and the topics are learned independently.

We adopt a multinomial logistic regression (MLR) model to fulfill the prediction task. Given a collection of N parallel sentences { s i , t i } , where s i denotes the available features of the i th source sentence and t i is the topic distribution vector of the i th target sentence, which is learned jointly with the monolingual LM corpus, the MLR model tries to minimize: where  X  is a softmax function and W is a mapping matrix, which is estimated by stochastic gradient descent (SGD) training.

Then, before translating one sentence, we can infer its target topics from source features s i by: In fact, the MLR model can also be viewed as a simplified feedforward neural network model, in which the input layer and the output layer are connected directly. In addition, the MLR model can accept different types of source features, including both dense and sparse. We adopt the following features in this work:
Source-topic features . Topics on words, sentences, and documents are used for pre-diction. Here, word features denote the averaged topics of each word in the sentence. Topic features are usually sparse and binary.

Source-representation features . These features are real values and include word, sentence, and document levels. Details will be described in Section 5.
 In summary, there are two groups of source features, that is, topic and representation, each of which has three different levels, that is, word, sentence, and document. Previous work [Xiao et al. 2012; Zhang et al. 2014] used source-side topics to pre-dict topics on the target side. However, topics contain partial information only for a sentence or document. If we could use more information from the source sentence or document, we may get better topic predictions. In recent years, many deep learn-ing techniques [Le and Mikolov 2014; Kalchbrenner et al. 2014; Socher et al. 2013] were proposed to learn low-dimensional representations, which even outperform tradi-tional features ( n -gram on word, POS and so on) in natural language processing (NLP) tasks. Considering this, we apply representation as features for our topic prediction. However, most state-of-the-art neural models focus on sentence-level representation learning, while representations on the document level are also important for the topic prediction task.

Inspired by the paragraph vector model [Le and Mikolov 2014], we modify the feed-forward LM (Figure 3) to capture sentence-and document-level representations. In contrast to conventional feedforward LM, which tries to model P ( w | u ), the joint net-work focuses on P ( w | u , s , d ). Here, s and d denote the representation of the sentence and document that the n -gram u w belongs to, respectively. Therefore, the input to the first hidden layer h 1 turns to be: where D and S are the matrix of sentence and document embeddings, respectively, and the rest of the symbols are also the same as described in Equation (1).

As only the input layer of the feedforward neural network is modified, the computa-tion of the rest of the layers also remains the same. In fact, the neural joint model can be viewed as a feedforward LM with order ( n + 2). Meanwhile, the output vocabulary remains the same, but the input vocabulary contains both the original truncated vocab-ulary and the collection of sentence and document IDs. Moreover, only an extra space ( | S |+| D | )  X  X  h 1 | is needed for the connection between the input and the first hidden layer. |
S | and | D | are the number of sentences and documents in training data, respectively, and | I S | and | I D | are correspondent embedding dimensions.

One benefit of the proposed model is that it requires little extra effort for training, except the update of representations of sentences and documents. Besides, the NCE method can still be adopted for model training. When the training procedure finishes, three levels of representations are obtained simultaneously and can be fed to the MLR model as inputs 1 . In our experiments, we attempted to answer the following questions: (1) Does topic-enhanced feedforward LM achieve better performance than conven-(2) Does the MLR model predict more precise target-topic distribution with source-side (3) Can an SMT system produce better translations with proposed topic-enhanced We conducted three experiments to answer these questions. Before describing all ex-periments, in Section 6.1, we give a detailed description of the basic settings used in all experiments. In Section 6.2, we compare LM performance of topic-enhanced feed-forward LM with conventional n -gram, feedforward, and RNNLMs. In Section 6.3, we validate the effects of our MLR model and the joint source representations on target-topic prediction. In Section 6.4, we evaluate the MT performance by incorporating feedforward LM with predicted topics to a state-of-the-art MT system. The statistics of corpora used in our experiments are illustrated in Table I. For all experiments, we built a feedforward LM with order 5 on the Xinhua portion of Giga-word 2 . We extended the feedforward LM toolkit nplm 3 to support the training with topic distribution. We also adopted the text preprocessing method and neural network settings in Vaswani et al. [2013]. All digits were converted to 0, then the vocabulary table was sorted based on the frequency in descending order. Only the top 100K words were kept; the rest of the words were replaced as &lt; unk &gt; . Meanwhile, feedforward LM used a dimension size of 150 for both input and output embeddings, and two hidden layers with 750 and 150 units, separately. Furthermore, feedforward LM was optimized by 10 epochs of stochastic gradient descent (SGD) with mini-batch size 1000 and an initial learning rate of 1. The number of noise samples was set at 100 per training example.

We also used RNNLM 4 to train a recurrent neural network LM on Gigaword for comparison. Following the settings of feedforward LM, the vocabulary size was 100 k and the dimension of word embeddings was 150. The class number was 500 and di-rect connection was 2 million under order 4. During training, the independent mode was turned on and a model after 10 epochs was used in our experiments for a fair comparison.
 Finally, we trained two conventional 5g LMs on Gigaword under modified Kneser-Ney (KN5) smoothing [Chen and Goodman 1998] under different vocabulary sizes separately, one with a full vocabulary and the other with a 100 k vocabulary.
For topic modeling, we used OpenHTMM 5 to train topics. In order to address the influence of topic numbers, we tried three different topics (20, 40, and 100) in all experiments. In this section, we validate the effect of topics on feedforward LMs. The performance was evaluated in perplexity [Brown et al. 1992]. The validation set includes target references of all MT evaluation sets (MT03 to MT08 in Table I). We trained one con-ventional and three topic-enhanced feedforward LMs on the Gigaword corpus. n -gram and RNN LMs were also constructed on Gigaword for comparison.
 First, we evaluate the perplexity results for all conventional LMs. Illustrated in Table II, although RNNLM still performs the best, LM performance of three types of LMs under the same vocabulary size is quite close. This observation is different from Wang et al. [2015], who find that, when evaluated on two relatively small-scale datasets, RNNLM outperforms feedforward LM, while feedforward LM outperforms n -gram LM, both with a moderate gain. We think it is due to the size of the Gigaword corpus, which is 46 times bigger than the FBIS corpus and 433 times bigger than the Penn Treebank corpus used in Wang et al. [2015]; the power of different LMs turns out to be similar.

When equipped with topics (Table III), the perplexity of feedforward LM greatly falls by 14.72 points under 20 topics. When the topic number increases, the perplexity further decreases by 19.47 (40 topics) and 24.46 (100 topics) points separately. Therefore we see that topics do improve the performance of the feedforward LM and larger topic number brings a better result. 6.3.1. Settings. The parallel corpus for training topic prediction is FBIS 6 ,whichis a collection of document-level news reports. The source side of the FBIS corpus is used for learning representation. Meanwhile, the validation set is MT03 7 . As there are four references for each source sentence in MT03, we simply combine four references and used their averaged target topic distribution for evaluation. We evaluated the prediction performance under L1 and Hellinger loss [Xiao et al. 2012] and validated the predicted topic in perplexity.

We reimplemented the matrix mapping method proposed by Xiao et al. [2012] (Xiao X  X  method). Since sentence-level topic distribution is also provided by OpenHTMM, we ran sentence-level matrix mapping (Sentence Map) as another baseline. We evaluated the effect of our MLR model for two aspects: first, we used traditional source topics only; second, we validated the effect of joint source representations induced by the neural model proposed in Section 5. As the source-side information comes from three levels, that is, word, sentence, and document, we compared different MLR models trained under combinations of source information.
We modified nplm [Vaswani et al. 2013] to support the learning of the joint rep-resentations. The settings of the neural network model are the same as that of the feedforward LMs except that the vocabulary is truncated to 10K considering the cor-pus size. We also set the dimension of both document and sentence representations as 150. 6.3.2. Results. Without loss of generality, we reported prediction loss results with 100 topics only in Table IV. We found that, when equipped with the same source-side topic feature, the MLR model achieves comparable performance against matrix mapping on asentence( ST vs. Sentence Map ) level, while there is a 200-point reduction on the document level ( DT vs. Xiao method ) on both metrics. Meanwhile, when conducted on the same single level of the source feature, the MLR model with representation ( DR , SR , WR ) feature works as well as the corresponding topic ( DT , ST , WT ) feature. Combining three levels of source representations features ( DR+SR+WR ) obtains much lower prediction loss than with topic ( DT+ST+WT ) features. Finally, combining all topic and representation features ( ALL ) brings the lowest prediction loss, which is 10% lower than Xiao X  X  result under L1 loss and about 20% lower under Hellinger loss. Such reductions are also observed under the other two topics.

We also evaluated the perplexity of validation set under feedforward LMs with pre-dicted target topics (Tables IV and V). From Table IV, we see that, when using single type of source feature (row 3 X 7, 9 X 11), topic-enhanced feedforward LM achieves only comparable result than the baseline feedforward LM (row 2), since the prediction loss is relatively high. However, the perplexity slowly drops when more features are used and turns out to be very close to the oracle score when all source features ( ALL ) are included. When joint representations are used for topic prediction ( DR+SR+WR ), the perplexity is only slightly worse than the best prediction, which indicates that joint representations work well for the topic prediction task. Finally, similar results (see Table V) can be observed under the other two topics. 6.4.1. Settings. We conducted MT experiments on NIST2012 Chinese X  X nglish evalu-ation task. The parallel training corpus contains about 9 million sentences (Table I). Symmetric word alignments of training corpus were learned by running GIZA++ [Och and Ney 2003] in both directions and refined under the  X  X row-diag-final-and X  method.
An in-house hierarchical, phrase-based SMT system [Chiang 2007] was used to report experimental results; both phrases and SCFG rules were extracted from all training data. In addition to the n -gram LM trained on Gigaword, we also trained a 5-gram conventional LM on the target side of training data with full vocabulary.
We ran MERT on the NIST 2003 test data (MT03) and tested MT performance on datasets from the NIST 2004 to 2008 (referred to as MT04, MT05, MT06, and MT08). The results were evaluated under case-insensitive NIST B LEU . In order to stabilize the results, we ran MERT three times [Clark et al. 2011] and reported the average score. Finally, we conducted bootstrap sampling [Koehn 2004] with 1000 rounds to test the B LEU significance.

As direct decoding has been proved to be more effective than n -best re-ranking [Vaswani et al. 2013], we conducted direct decoding only for feedforward LMs in MT experiments. Considering the effect of n -gram LMs on the MT system, we compared MT quality on decoding with topic-enhanced feedforward LM against conventional feedforward LM in two ways: Under both settings, we also ran an MT system with averaged reference topics, which could be viewed as oracle results. We used the best projected target topics (setting ALL in Table V) in MT experiments. 6.4.2. Results. Table VI displays the results under the neural setting. We see that decoding with a topic-enhanced feedforward LM ( FF-ALL ) achieves a significant im-provement against with conventional feedforward LM ( FF ) on 9 of total 12 groups; the averaged improvement reaches up to 0.7 to 1.1 B LEU . Our topic prediction method performs quite well since the B LEU score under the best ALL setting is only slightly worse than the system FF-Oracle . Such a result is also consistent with the perplexity performance in Table IV. In addition, we see that a larger topic number always leads to higher MT performance under this setting.

We compare the effect of different types of LMs on MT system performance. Seen from Table VII, decoding with n -gram LMs with 100K vocabulary ( N-gram (100k) ) leads to a 0.64B LEU drop comparing with full vocabulary ( N-gram ). On the other hand, compared to system N-gram , RNNLM reranking ( N-gram+RNN ) achieves an average improvement of 0.3B LEU while direct decoding with feedforward LM ( N-gram+FF ) performs even better, with an improvement up to 0.69B LEU . These results also confirm the observations in Zhao et al. [2014], that is, direct decoding with feedforward LM outperforms reranking with RNNLM, and n -gram LM with full vocabulary outperforms with truncated vocabulary. No improvement is obtained under two feedforward LMs ( N-gram+2FFs ). Based on these observations, it is reasonable to use N-gram+FF as the baseline system under the full setting.

Table VIII shows the results under the full setting. We observe that decoding with topic-enhanced feedforward LM ( N-gram-FF-ALL ) achieves higher performance than with conventional feedforward LM. An improvement around 0.3 to 0.6B LEU can be ob-tained on three of all test datasets (MT05, MT06, and MT08). Although little change appears on MT04, the averaged improvement on four test sets still reaches 0.3 to 0.45B LEU under various topics. In addition, 5 of 12 results are significant compared to system N-gram-FF . Considering that decoding with feedforward LMs ( N-gram-FF ) already surpasses system N-gram by 0.69B LEU , the cumulative 0.97 to 1.15B LEU im-provement against the initial baseline N-gram is very meaningful.

In summary, we demonstrate that the MLR model achieves good performance on topic prediction task, especially with joint source representations. More important, topic information does help feedforward LMs for SMT.
 There are three lines of work that are related to our research: language model adapta-tion with topic information, topic prediction, and representation learning.
Language model adaptation is a research topic with a long history. Previous work focused on incorporating topic information into traditional back-off n -gram LMs. Yu et al. [2013] built a topic-enhanced n -gram LM that computes the probability under linear mixture on LMs of each topic. Although they reported progress on both LM evaluation and MT tasks, the biggest obstacle to their method is the low efficiency caused by a large number of LM components that have to be built and loaded for MT decoding. In contrast, this problem does not exist in neural network LMs, as the topic information can be encoded into the network seamlessly. Mikolov and Zweig [2012] enhanced the recurrent neural network LM with the context of the sentence being modeled. By performing LDA using a block of preceding text, they obtained a topic-conditioned RNNLM, which reduced the perplexity on small-scale datasets. Our work is inspired by Mikolov and Zweig [2012], but we added sentence-level topics to a feedforward LM and achieved good perplexity performance on a large-scale corpus. More importantl, to the best of our knowledge, we validated the effect of topic-enhanced feedforward LM on a more challenging MT task for the first time.

As only source text is available during translation, we need to predict topics of the target side for computing probability under LM with topics. Previously, Xiao et al. [2012] built a mapping matrix based on the occurrence of aligned source and tar-get words with their topic distribution. This method was also adopted in some later work [Zhang et al. 2014; Yu et al. 2013]. However, the key limitation of their work is that they used only source topics, which is shown to be inefficient in our work. Later, the maximum entropy (ME) model proposed by Xiong and Zhang [2013] could utilize previous topics and traditional topic features induced by OpenHTMM. Theoretically, it is reasonable to use topic information of previous sentences for prediction. But it does not obtain consistent improvement in their work [Xiong and Zhang 2013]. In our work, we used document-level topics and representations as features, which also could be viewed as global features. In addition, only binary features can be used by the ME model, and it can only predict topic-label rather than topic-vector distribution. In con-trast, we treated topic prediction as a general prediction task and adopted an MLR model, which exploits different types of source information, including topics and joint representations, to predict topic distribution directly.

Recently, representation learning has been popular in the NLP community. Such representations can be induced via different neural network models, including feedforward [Le and Mikolov 2014], recurrent [Mikolov et al. 2010], recursive [Goller and Kuchler 1996] and convolutional [Kalchbrenner et al. 2014]. In this work, we adopted joint representation on the source side for the topic prediction task and achieved good performance. Although our feedforward neural network model for in-ducing the representations is inspired by Le and Mikolov [2014], our model is different in two aspects: first, our model originates from the feedforward neural model but theirs comes from more simple CBOW and skip-gram models [Mikolov et al. 2013]; second, our model learns both sentence and document representations simultaneously, while theirs learns representations only on a single level. In this article, we explored the adaptation of feedforward LM for SMT with topic in-formation, which is estimated from a collection of documents and encoded into a feed-forward LM to obtain a better estimation across various domains. We further adopted a MRL model to learn the prediction of target topics based on source information. In order to get effective information other than source-topic distribution, we used a variant of a feedforward neural network inspired by Le and Mikolov [2014] to learn representations on word, sentence, and document levels jointly. Throughout this work, we applied a feedforward neural model to solve three different tasks. Our experimental results showed that the topic-enhanced feedforward LM not only significantly reduces the perplexity, but also achieves a moderate progress against the strong baseline on a large-scale Chinese X  X nglish MT task.

In the future, we are going to extend our work in the following directions: as topics have been proved to be effective in language modeling in the MT task, we plan to incorporate topics to the neural translation model [Devlin et al. 2014; Le et al. 2012]; in addition to the topic model used in this work, we plan to investigate more powerful topic models for neural network LMs, for example, polylingual tree-based topics [Hu et al. 2014], so that topics can show a stronger impact in decoding; finally, as it has been shown that neural MT approaches [Sutskever et al. 2014; Luong et al. 2014; G  X  ulc  X ehre et al. 2015] can catch up with or even surpass traditional MT systems, it would be worthwhile to add topics or other types of domain information to neural MT systems for adaptation.

