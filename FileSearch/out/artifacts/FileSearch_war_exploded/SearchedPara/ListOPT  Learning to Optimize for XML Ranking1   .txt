 Search engines have become an indispensable part of life and one of the key issues on search engine is ranking. Given a query, the ranking modules can sort the retrieval doc-uments for maximally satisfying the user X  X  needs. Traditional ranking methods aim to compute the relevance of a document to a que ry, according to the factors, term frequen-cies and links for example. The search resu lt is a ranked list in which the documents are sequenced by their relevance score in descending order. These kinds of methods include the content based functions such as TF*IDF [1] and BM25 [2], and link based functions such as PageRank [3] and HITS [4].

Recently, machine learning technologies have been successfully applied to informa-tion retrieval, known and named as  X  X earning-to-rank X . The main procedure of  X  X earning-to-rank X  is as follow: In learning module, a set of queries is given, and each of the queries is associated with a ground-truth ranking list of documents. The process targets at creating a ranking model that can precisely predict the order of documents in the ground-truth list. Many learning-to-rank approaches have been proposed and based on the di erences of their learning samples, these methods can be classified into three cate-gories [5]: pointwise, pairwise and listwise. Taking single document as learning object, the pointwise based methods intent to com pute the relevance score of each document with respect to their closeness to the ground-truth. On the other side, pairwise based approaches take the document pair as learning sample, and rephrase the learning prob-lem as classification problem. Lisewise bas ed approaches take a ranked list as learning sample, and measure the di erences between the current result list and the ground-truth list via using a loss function. The learning purpose of listwise methods is to minimize the loss. The experimental results in [5] [11] [12] show that the listwise based methods perform the best among these three kinds of methods.

It is worth noting that, from the perspective of ranking, the aforementioned learning-to-rank methods belong to the learning based ranking technologies. Here the search results are directly obtained from the learning module, without considering the tradi-tional content based or link based ranking functions. However, there is no evidence to confirm that the learning based methods perform better than all the other classic content based or link based methods. Accordingly, to substitute the other two kinds of ranking technologies with the learning based methods might not be appropriate.

We hence consider a learning-to-optimize method ListOPT that can combine and utilize the benefits of learning-to-rank met hods and traditional content based meth-ods. Here the ranking method is the extension to the widely known ranking function BM25. Due to previous studies, experiments a re conducted on selecting the parameters of BM25 with the best performance, typically after thousands of runs. However, this simple but exhaustive procedure is only applicable to the functions with few free pa-rameters. Besides, whether the best parameter values are in the testing set is also under suspect. To attack this defect, a listwise learning method to optimize the free parameters is introduced.

Same as learning-to-rank methods, the key issue of learning-to-optimize method is the definition of loss function. In this paper, we discuss the e ect of three distinct defini-tion of loss in the learning process and the experiments show that all three loss functions converge. The experiments also reveal that the ranking function using tuned parameter set indeed performs better.

The primary contributions of this paper include: (1) proposed a learning-to-optimize method which combine and utilize the traditional ranking function BM25 and listwise learning-to-rank method, (2) i ntroduced the definition of thr ee query-level loss func-tions on the basis of cosine similarity, Euclidean distance and cross entropy, confirmed to converge by experiments, (3) the verified the e ectiveness of the learning-to-optimize approach on a large XML dataset Wikipedia English[6].

The paper is organized as follows. In section 2, we introduce the related work. Sec-tion 3 gives the general description on learning-to-optimize approach ListOPT. The definition of the three loss functions are discussed in section 4. Section 5 reports our experimental results. Section 6 is the conclusion and future work. 2.1 Learning-to-Rank In recent years, many machine learning methods were applied to the problem of ranking for information retrieval. The existing learning-to-rank methods fall into three categories, pointwise, pairwise and listwise. The pointwise approaches [7] are firstly proposed, transforming the ranking problem into regression or classification on single candidate documents. On the other side, pairwise approaches, published later, regard the ranking process as a classification of document pairs. For example, given a query Q and an arbitrary document pair P ( d 1 d 2 ) in the data collection, where d i means the i -th candidate document, if d 1 shows higher relevance than d 2 , then object pair P is set as ( p ) 0, otherwise P is marked as ( p ) 0. The advantage of pointwise and pair-wise approaches is that the existing classifi cation or regression theories can be directly applied. For instance, borrowing support vector machine, boosting and neural network as the classification model leads to the methods of Ranking SVM [8], RankBoost [9] and RankNet [10].

However, the objective of pointwise and pairwise learning methods is to minimize errors in classification of single documen t or document pairs rather than to minimize errors in ranking of documents. To overcome this drawback of the aforementioned two approaches, listwise methods, such as ListNet [5], RankCosine [11] and ListMLE [12], are proposed. In lisewise approaches, the learning object is the result list and various kinds of loss functions are defined to measure the similarity of the predict result list and the ground-truth result list. ListNet, the first listwise approach proposed by Cao et al., uses the cross entropy as loss function. Qin et al. discussed about another listwise method called RankCosine, where the cosine similarity is defined as loss function. Xia et al. introduced likelihood loss as loss function in the listwise learning-to-rank method ListMLE. 2.2 Ranking Function BM25 In information retrieval, BM25 is a highly cited ranking function used by search en-gines to rank matching docum ents according to their relevance to a given search query. It is based on the probabilistic retrieval framework developed in the 1970s and 1980s. Though BM25 is proposed to rank the HTML format documents originally, it was in-troduced to the area of XML documents ranking in recent years. In the last three years of INEX 1 [6] Ad Hoc track 2 [17] [18] [19], all the search engines that perform the best use BM25 as basic ranking function. To improve the performance of BM25, Taylor et al. introduced the pairwise learning-to-rank method RankNet to tune the parameters in BM25, named as RankNet Tuning method [13] in this paper. However, as mentioned in 2.1, the inherent disadvantages of pairwise methods had a pernicious influence on the approach. Experiments in section 5 will compare the e ectiveness of RankNet Tuning with the other methods proposed in this paper. In this section, we describe the details in the learning-to-optimize approach. Firstly, we give the formal definition of the ranking function BM25 used in XML retrieval and analyze the parameters in the formula. Then, the training process of the listwise learning-to-optimize approach ListOPT is proposed in 3.2. 3.1 BM25 in XML Retrieval Unlike the HTML retrieval, the searching retrieval results are elements in XML re-trieval, the definition of BM25 is thus di erent from the traditional BM25 formula used in HTML ranking. The formal definition is as follow: In the formula, tf ( t e ) is the frequency of keyword t appeared in element e ; Nd is the number of files in the collection; n ( t ) is the number of files that contains keyword t ; len ( e ) is the length of element e ; avel is average length of elements in the collection; Q is a set of keywords; ps ( e Q ) is the predict relevance score of element e corresponding to query Q ; b and k are two free parameters.

As observed, the parameters in BM25 fall into three categories: constant parameters , fixed parameters and free parameters . For example, parameters describing the features of data collection like avel and Nd are defined as constant parameters . Given a certain query and a candidate element, tf ( t e )and len ( e ) in the formula are fixed values. This kind of parameters is called fixed parameters . Moreover, free parameters ,suchas k and b in the function, are set to make the formula more adaptable to various kinds of data collections. Therefore, the ultimate objective of learning-to-optimize approach is to learn the optimal set of free parameters . 3.2 Training Process In training, there is a set of query Q q 1 q 2 q m . Each query q i is associated with a element to query q i and n ( i ) is the size of E i . The candidate elements are defined as the elements that contain at least one occurrence of each keyword in the query. Moreover, indicating the relevance score of each elements in E i . Given that the data collection we used only contains information of whether or not the passages in a document are relevant, we apply the Fmeasure cite14 to evaluate the ground truth score. Given a query q i , the ground-truth score of the j -th candidate element is defined as follow: In the formula, relevant is the length of relevant contents highlighted by user in e , while irrelevant stands for the length of irrelevant parts. REL indicates the total length of relevant contents in the data coll ection. The general bias parameter is set as 0.1, denoting that the weight of precision is ten times as much as recall.

Furthermore, for each query q i , we use the ranking function BM25 mentioned in 3.1 Then each ground-truth score list G i and predicted score list R i form a  X  X nstance X . The loss function is defined as the  X  X istance X  between standard results lists D i and search results lists R i . In each training epoch, the ranking functio n BM25 was used to compute the predicted score R i . Then the learning module replaced th e current free parameters with the new parameters tuned according to the loss between G i and R i . Finally the process stops either while reaching the limit cycle index or when the parameters do not change. In this section, three query level loss functions and the corresponding tuning formulas are discussed. Here the three definitions of loss are based on cosine similarity, Euclidean distance and cross entropy respectively. After computing the loss between the ground-truth G i and the predicted R i , the two free parameters k and b in BM25 are tuned as formula (4). Especially, and are set to control the learning speed. 4.1 Cosine Similarity Widely used in text mining and information retrieval, cosine similarity is a measure of similarity between two vectors by finding the cosine of the angle between them. The definition of the query level loss function based on cosine similarity is: Note that in large data collection, given a query, the amount of relevant documents is regularly much less than the number of irrelevant documents. So that a penalty function j is set to avoid the learning bias on irrelevant documents. Formula (6) is the weight of relevant documents in learning procedure, while formula (7) is the weight of irrelevant document. The formal definition is as follow: Where NR i is the number of relevant elements according to query q i and NIR i is the number of irrelevant ones. After measuring the loss between the ground-truth results and the predicted results, the adjustment range parameters k and b are determined according to the derivatives of k and b :
With respect to k : In which: b analogously: In which: 4.2 Euclidean Distance The Euclidean distance is also used in the definition of loss function. The circumscrip-tion of penalty parameter i j is the same as in formula (6) and (7). Hence, the loss function based on Euclidean dist ance is defined in formula (12). The same as cosine similarity loss, we derive the derivatives of the loss function based on Euclidean distance with respect to k and b . The definition of r as in formula (9) and formula (11) respectively.
 With respect to k: b analogously: 4.3 Cross Entropy When considering cross entropy as metric, the loss function turns to formula (15). Moreover, the penalty parameter i j in the formula is the same as in formula (6) and (7) and the detailed tuning deflection of k and b is defined in formula (16) and formula (17) respectively. Additionally, the definition of r (9) and formula (11). With respect to k : b analogously: In this section, the XML data set used in comparison experiments is first introduced. Then in section 5.2 we compare the e ectiveness of the optimized ranking function BM25 under two evaluation criterions: MAP [15] and NDCG [16]. Additionally in section 5.3, we focus on testing the association between the number of training queries and the optimizing performance under the criterion of MAP. 5.1 Data Collection The data collection used in the experiments consists of 2,666,190 English XML files from Wikipedia, used by INEX Ad Hoc Track. The total size of these files is 50.7GB. The query set consists of 68 di erent topics from competition topics of Ad Hoc track, in which 40 queries are considered as traini ng queries and others are test queries. Each query in the evaluation system is bound to a standard set of highlighted  X  X elevant con-tent X , which is recognized manually by the participants of INEX. In the experiments, the training regards these highlighted  X  X elevant content X  as ground truth results. 5.2 E ect of BM25 Tuning To explore the e ect of the learning-to-optimize method ListOPT, we evaluate the ef-fectiveness of di erent parameter sets. In the comparison experiments, Traditional Set stands for a highly used traditional set: k 2 b 0 75; RankNet Tuning stands for the tuning method proposed in [10]; cosine similarity, Euclidean distance and cross en-tropy are the learning-to-optimize methods using cosine similarity, Euclidean distance and cross entropy as loss function respectively.

Evaluation System of Ad Hoc is the standard experiment platform in e ectiveness comparison here. We evaluate the searching e ectiveness of the aforementioned five methods in two criterions: MAP and NDCG. In MAP evaluation, we choose interpo-lated precision at 1% recall (iP[0.01]), precision at 10% recall (iP[0.10]) and MAiP as the evaluation criterions. While in NDCG evaluation, we test the retrieval e ect on NDCG@1 to NDCG@ 10.

Figure 1 illustrates the comparison resu lts under MAP measure. As is shown, the three learning-to-optimize methods proposed in this paper perform the best. It might looks confusing since that INEX 2009 Ad Hoc Focused track used to have search engine reaching ip[0.01] 0.63, compared to 0.34 the highest in our plot. However, this e ectiveness is regularly obtained by combining various ranking technologies to-gether, such as two-layer strategy, re-ranking strategy and title tag bias, but not only do such a combination. In this condition, the searching e ectiveness scores (ip[0.01]) INEX did.

The result presented in figure 2 show that the learning-to-optimize methods are in-deed more robust in ranking tasks. The performance of the ranking methods becomes better when more results are returned. From the perspective of users, this phenomenon could be explained by the fact that INEX queries are all information query , meaning that the user X  X  purpose is to find more relevant information. On contrary, if the query is a navigation query , the user only needs the exact webpage. The first result is hence of highest importance and the evaluatio n score might decreases accordingly. 5.3 Number of Training Queries Figure 3 shows the relationship between the tuning e ectiveness and the quantity of training queries. In this experiment, the num ber of training queries changes from 1 to 40. As illustrated, the MAiP score lines all share an obvious ascent during the first several query numbers. After that, the pulsation of the performance keeps in a low level. This situation corresponds with the learning theory: when there are few queries in the training set, the learning is overfitting. With the increasing of query samples, the performance of learning procedure gets better and better till finally the most proper parameters for the data collection are found.
 In this paper, we proposed a learning-to-optimize method ListOPT. ListOPT combines and utilizes the benefits of the listwise learn-to-rank technology and the traditional rank-ing function BM25. In the process of learning, three query level loss functions based on cosine similarity, Euclidean distance and cro ss entropy respectively are introduced. The experiments on a XML data set Wikipedia English confirm that the learning-to-optimize method indeed leads to a better parameter set.

As future work, we will firstly try to tune W t in the formula. Then we would like to extend the learning-to-optimize method ListOPT approach to other tuning fields, like tuning the parameters in other ranking functions or ranking methods in the future. In a further, the comparison of ListOPT and some other learning and ranking methods, such as ListNet, XRank, XReal and so on, will be done on benchmark data sets.
 This work was supported by the National High-Tech Research and Development Plan of China under Grant No.2009AA01Z136.

