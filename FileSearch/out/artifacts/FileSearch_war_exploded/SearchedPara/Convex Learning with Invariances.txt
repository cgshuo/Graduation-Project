 [5] and others [6].
 from Lie groups). Another mathematically appealing formulation of the problem of learning with icantly when the observed inputs are degraded in some way. Robust estimation has been applied to specific types of data corruption.
 advantage of existing optimization infrastructures. In this paper we propose a method which has what we believe are many appealing properties: incorporates invariance transformations on the input objects. We assume that we are given input patterns x  X  X from from some space X and that we want to estimate outputs y  X  Y . For instance Y = { X  1 } corresponds to binary classification; Y = A n corresponds to sequence prediction over function f : X  X  Y  X  R , i.e.  X  y ( x ) := argmax by solving a nontrivial discrete optimization problem, e.g. by means of dynamic programming. In kernel machines, due to the availability of scalable optimizers for that class of estimators. 2.1 Invariance Transformations and Invariance Sensitive Cost y or substitutions like Viagra  X  V1agra , V!agra ) should not affect our decision function. Of substitution and an insertion can change Viagra  X  diagram . Furthermore, certain invariances a slight abuse of notation since s may depend on y but this should always be clear in context.) the input has undergone very extreme transformations. In a image labeling problem, for example, sampled images since they contain very little information. 2.2 Max Margin Invariant Loss the maximal cost compared with the true target: upper bound on this worst case cost which incorporates a notion of (scaled) margin: between margin violations and the prediction cost  X  .
 and convergent algorithms as proposed above.
 Lemma 1 The loss l ( x, y, f ) is convex in f for any choice of  X  ,  X  and S . f , hence (weakly) convex. Taking the supremum over a set of convex functions yields a convex function.
 feature map with associated kernel or required by [8], or the ellipsoidal shape described by [2].
 Lemma 2 The loss l ( x, y, f ) provides an upper bound on C ( x, y, f ) = sup tion f ( s  X  ( x ) , y  X  )  X  f ( s  X  ( x ) , y ) . Plugging this inequality into Eq. (2) yields methods for dealing with invariances can be viewed as special cases of Eq. (2). cost function  X  may assign quite a small cost to a transformation s which takes x very far away applicable to any type of structured estimation. that we are given a training set of input patterns X = { x { y 1 , . . . , y m } regularized empirical risk functional of the form A direct extension of the derivation of [16] yields that the dual of (5) is given by Here the entries of the kernel matrix K are given by the dual coefficients  X  for dealing with invariances can be viewed as heuristics for approximately minimizing an approx-valuable before introducing specific applications of the invariance loss.
 Whenever the are an unlimited combination of valid transformations and targets (i.e. the domain S  X  Y is infinite), the optimization above is a semi-infinite program, hence exact minimization of on a batch scenario, inspired by SVMStruct [16], and one based on an online setting, inspired by BMRM/Pegasos [15, 12]. 3.1 A Variant of SVMStruct The work of [16, 10] on SVMStruct-like optimization methods can be used directly to solve regu-larized risk minimization problems. The basic idea is to compute gradients of l ( x the dual space. While bundle methods work directly with gradients, solvers of the SVMStruct type are commonly formulated in terms of column generation on individual observations. We give an in combination with a corresponding label which violates the margin most.
 O (  X  1 ) . We omit technical details here. Algorithm 1 SVMStruct for Invariances 1: Input: data X , labels Y , sample size m , tolerance 2: Initialize S i =  X  for all i , and w = 0 . 3: repeat 4: for i = 1 to m do 5: f ( x 0 , y 0 ) = P 6: ( s  X  , y  X  ) = argmax 9: Increase constraint set S i  X  S i  X  X  ( s  X  , y  X  ) } 10: Optimize (6) using only  X  iz where z  X  S i . 11: end if 12: end for 13: until S has not changed in this iteration 3.2 An Application of Pegasos lems of type Eq. (5). Algorithm 2 is an adaptation of their method to learning with our convex weight vector back to a feasible region k f k X  q 2 R [0] Algorithm 2 Pegasos for Invariances 1: Input: data X , labels Y , sample size m , iterations T , 2: Initialize f 1 = 0 3: for t = 1 to T do 5: Compute constraint violator 9: end if 10: end for to Eq. (3). We assume that the latter is given by R . We can apply [12, Lemma 1] immediately: Theorem 3 Denote by R t . In this case Algorithm 2 satisfies the following bound: In particular, if T is a multiple of m we obtain bounds for the regularized risk R [ f ] . Virtual Support Vectors (VSVs): The most straightforward approach to incorporate prior knowl-the optimization.
 Second Order Cone Programming for Missing and Uncertain Data: In [2], the authors consider to correspond to a second order cone program (SOCP). Instead of solving SOCP, we can solve an equivalent but unconstrained convex problem.
 Semidefinite Programming for Invariances: Graepel and Herbrich [8] introduce a method for optimization scheme we suggest will perform considerably faster than standard SDP solvers. Robust Estimation: Globerson and Roweis [7] address the case where invariances correspond to of the invariance in [7] and show how it can be optimized efficiently. MNIST data, and spam filtering on the ECML06 dataset. Both examples are standard multiclass 5.1 Handwritten Digits Recognition Humans can recognize handwritten digits even when they are altered in various ways. To test our and original samples were used to train a multiclass SVM (VIR-SVM). Finally, we also trained a multiclass SVM that did not use any invariance information (STD-SVM). All of the aforementioned SVMs were trained using RBF kernel with well-chosen hyperparameters. For evaluation we used the standard MNIST test set.
 Results for the three methods are shown in Figure 1. It can be seen that Invar-SVM and VIR-SVM, which use invariances, significantly improve the recognition accuracy compared to STD-SVM. This comes at a certain cost of using more support vectors, but for Invar-SVM the number of support vectors is roughly half of that in the VIR-SVM. 5.2 SPAM Filtering spam authors will change their style. One common mechanism of style alteration is the insertion of common words, and avoiding using specific keywords consistently over time. If documents are Figure 1: Results for the MNIST handwritten digits recognition task, comparing SVM trained on original samples (STD-SVM), SVM trained on original and virtual samples (VIR-SVM), and our of the number of original samples per digit used in training. Right figure shows the number of support vectors corresponding to the optimum of each method. represented using a bag-of-words, these two strategies correspond to incrementing the counts for some words, or setting it to zero [7].
 Here we consider a somewhat more general invariance class (FSCALE) where word counts may be we specialize it to the feature deletion case (FDROP) in [7].
 The invariances we consider are thus defined by O ( d log d ) time.
 ( ecml06a-tune ). ecml06a-eval has 4000 / 7500 training/testing emails with dimensionality 206908 , and ecml06a-tune has 4000 / 2500 training/testing emails with dimensionality 169620 . We selected the best parameters for each methods on ecml06a-tune and used them for the train-ing on ecml06a-eval . Results and parameter sets are shown in Table 1. We also performed McNemar X  X  Tests and rejected the null hypothesis that there is no difference between hinge and FSCALE/FDROP with p -value &lt; 10  X  32 .
 Algorithm 3 FSCALE loss 1: Input: datum x , label y , weight vector w  X  R d , invariance-loss parameters ( K, l, u ) 2: Initialize i := 1 , j := d 3: B := y  X  w  X  x 4: I := IndexSort(B), such that B ( I ) is in ascending order 5: for k = 1 to K do 6: if B [ I [ i ]]  X  (1  X  u ) &gt; B [ I [ j ]]  X  (1  X  l ) then 7: x [ I [ i ]] := x [ I [ i ]]  X  u and i := i + 1 8: else 9: x [ I [ j ]] := x [ I [ j ]]  X  l and j := j  X  1 10: end if 11: end for Table 1: SPAM filtering results on ecml06a-eval averaged over 3 testing subsets.  X  is regu-special case FDROP statistically significantly outperform the standard hinge loss (Hinge). We have presented a general approach for learning using knowledge about invariances. Our cost the worst case invariance for a given data point and model. This approach can allow us to solve invariance problems which previously required solving very large optimization problems (e.g. a invariances used and efficiency of optimization.
 Acknowledgements: We thank Carlos Guestin and Bob Williamson for fruitful discussions. Part of the work was done when CHT was visiting NEC Labs America. NICTA is funded through the Network of Excellence, IST-2002-506778.

