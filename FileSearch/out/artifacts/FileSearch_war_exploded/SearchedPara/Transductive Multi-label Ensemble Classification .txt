 Advances in biotechnology have made available multitudes of heterogeneous proteomic and genomic data. Integrating these heterogeneous data sources, to automatically infer the function of proteins, is a fundamental challenge in computa-tional biology. Several approaches represent each data source with a kernel (similarity) function. The resulting kernels are then integrated to determine a composite kernel, which is used for developing a function prediction model. Proteins are also found to have multiple roles and functions. As such, several approaches cast the protein function prediction prob-lem within a multi-label learning framework. In our work we develop an approach that takes advantage of several unla-beled proteins, along with multiple data sources and multiple functions of proteins. We develop a graph-based transductive multi-label classifier (TMC) that is evaluated on a composite kernel, and also propose a method for data integration us-ing the ensemble framework, called transductive multi-label ensemble classifier (TMEC). The TMEC approach trains a graph-based multi-label classifier for each individual kernel, and then combines the predictions of the individual mod-els. Our contribution is the use of a bi-relational directed graph that captures relationships between pairs of proteins, between pairs of functions, and between proteins and func-tions. We evaluate the ability of TMC and TMEC to predict the functions of proteins by using two yeast datasets. We show that our approach performs better than recently pro-posed protein function prediction methods on composite and multiple kernels.
 I.5.2 [ Pattern Recognition ]: Design Methodology-Clas-sifier Design and Evaluation; J.3 [ Life and Medical Sci-ences ]: Biology and Genetics Algorithms, Performance, Experimentation Multi-label Ensemble Classifier, Directed Bi-relation Graph, Protein Function Prediction
Rapid advances in biotechnology have resulted in a variety of high throughput experimentally obtained genomic and proteomic datasets. Examples include protein-protein in-teraction networks, microarrays, genome sequences, protein structures, and genetic interaction networks, each of which provide a complementary view of the underlying mechanisms within a living cell. Manually annotating a protein, i.e. de-termining the  X  X rotein function X , using the vast volumes of data available from heterogeneous sources is challenging and low throughput. As such, various computational meth-ods have been developed for predicting protein functions by integrating the available biological data [17, 18].
Kernel based approaches [12, 14] have been very popular for developing several bioinformatics tools. The data is rep-resented by means of a kernel function, K , that computes pairwise similarities between proteins (or genes). Kernel functions capture the underlying biological complexity asso-ciated with the data. For each data source, a unique kernel function is defined, and each kernel function captures a differ-ent notion of similarity. For example, for protein sequences a string kernel [13] can be defined, and for protein-protein interaction (PPI) data a random walk kernel [23] function can be used. Both the sequence and PPI datasets are now defined by different kernel functions, each of which captures similarities between protein pairs within different feature spaces (or embeddings). Several methods [17, 24] have been developed to take advantage of the complementary embed-dings induced by different data sources. Many approaches [12, 15, 27] use a weighted combination (optimal or ad hoc) of multiple kernels derived from the different sources. This kind of approaches can be coined as data integration .In addition, supervised ensemble approaches [20, 21] have been developed to integrate the multiple data sources.
Several supervised [20] and semi-supervised [24, 27, 29] approaches have been introduced for predicting the func-tion of proteins. Transductive or semi-supervised approaches are able to take advantage of the large number of available unannotated (unlabeled) proteins to improve the accuracy of function prediction algorithms. Further, proteins are gener-ally involved in more than one biological process, and as such are annotated with multiple functions. Several approaches [9, 10, 19, 30] formulate the protein function prediction problem within a multi-label framework. By treating the dependencies between the different function classes, multi-label approaches achieve superior classification performance in comparison to single-labeled prediction methods.

In this paper we develop a protein function prediction method called Transductive Multi-label Classifier (TMC) that treats the relationship between proteins and functions as a directed bi-relation graph. To integrate the heterogeneous sources of protein data, we also develop an ensemble based classifier called Transductive Multi-label Ensemble Classifier (TMEC). We can summarize our key contributions as follows: 1. We analyze the use of an undirected bi-relation graph 2. We design a TMC on the directed bi-relation graph, 3. We compare classifier integration against data integra-
We performed comprehensive experiments evaluating the performance of TMC and TMEC on two yeast protein func-tion prediction benchmark sets. One of the datasets contains 5 kernels, whereas the other one contains 44 kernels. Our re-sults show that the use of a directed bi-relation graph achieves higher accuracy than the undirected one. Our approaches outperform state-of-the-art protein function prediction ap-proaches; namely, two transductive multi-label classification approaches [9, 30] and a transductive classifier [15].
The rest of the paper is organized as follows. In Section 2, we review related work on protein function prediction using multi-label learning and multiple data sources. In Section 3, we introduce the directed bi-relation graph and the corresponding training procedure. We also describe our ensemble approach to make use of multiple data sources. Section 4 details the experimental protocol, and Section 5 discusses the experimental results. In Section 6, we provide conclusions along with directions for future work.
Numerous computational approaches have been developed for protein function prediction. They vary in terms of method-ology, input data, and even problem definition. We refer the reader to a comprehensive review on this topic [18], and discuss here only the work most relevant to the scope of the paper.
Traditional function prediction methods represent proteins in a feature space and train binary classification models, treating each functional class independently [20]. Given the fact that proteins are involved in multiple functions, and structured relationships are prevalent within protein function annotation databases (e.g., GO 1 is a directed acyclic graph), researchers have formulated the protein function prediction as a multi-label classification problem [26]. RankSVM [7] incorporates a ranking loss function within the minimiza-tion function. The work of Chen et. al. [4] improves the RankSVM approach by explicitly including label correlations within the objective function using a hypergraph of inter-related proteins. Specifically, a hypergraph is defined where proteins are connected to each other if they share the same function. Since, the protein function databases like Gene Ontology [5] and FunCat [22] represent proteins within a hierarchy (or a directed acyclic graph), several approaches incorporate the parent-children relationship within the al-gorithm. The work of Barutcuoglu et. al. [1], first trains independent binary SVM classification models for each of the GO function classes, and then integrates the prediction by incorporating GO X  X  hierarchical structure within a Bayes formulation. Pandey et. al. [19] defined a pairwise similarity term between the different GO labels and incorporated it within a weighted multi-label k NN classifier.

Using protein-protein interaction data, several semi-supervised multi-label learning algorithms have been de-veloped. MCSL [10] uses an objective function that is sim-ilar to the local and global consistency method [31]. The approach in [10] incorporates a pairwise function correla-tion term within the regularization penalty resulting in a Kronecker matrix. To address the computational complex-ity associated with storing a Kronecker matrix in memory, PfunBG [9] uses an undirected bi-relational graph [28] to represent the relationships between proteins and functions. The bi-relational graph captures three types of relationships: (i) protein-protein similarities, (ii) function-function simi-larities and (iii) protein-function associations. The GRF method [30] uses Jaccard coefficients to measure pairwise function-function similarities, which are incorporated within a manifold regularization framework [2]. All the methods described, MCSL, PfunBG and GRF, utilize pairwise label correlations within a semi-supervised learning framework, but are developed for protein function prediction using a single data source only.
Several protein function prediction approaches that cap-ture the complementary information associated with multiple heterogeneous data sources have been developed [17]. Within the kernel optimization framework, Lanckriet et. al. [12] represent each data source as an individual kernel and deter-mine the optimal weighted combination of kernels by solving a semi-definite problem (SDP). This optimization does not scale with large number of proteins, and the work of Tsuda et. http://www.geneontology.org/ al. [27] use a dual problem and gradient descent to determine the optimal combination of kernels. Shin et. al. [24] seek to determine weights using the EM algorithm [6], by iterating over reducing the prediction error and combining weights. Mostafavi et. al. [16] proposed a heuristic ridge-regression algorithm to combine multiple kernels. We refer the reader to a recent survey of multiple kernel learning methods [8].
The composite kernel determined as a linear combination of individual kernels obtained from the methods described above can be used within SVM-based or graph-based classifiers for protein function prediction. These methods determine the set of weights per function class, which results in increased time complexity. On the other hand, Mostafavi et. al. [15] proposed a method that simultaneously learns a set of weights for a group of correlated functions. This approach was called X  X imultaneous Weighting (SW) X . SW optimizes a set of weights for a group of functions, constructs a composite kernel for the functional groups, and then uses a graph-based semi-supervised classification scheme. Tsuda et. al. [27], Lewis [14] and G  X  onen et. al. [8] observe that a composite kernel combined with optimized weights has similar performance to a composite kernel combined with equal weights, i.e. without optimization.

In this paper we develop approaches that differ from tra-ditional kernel integration methods [12, 15, 27]. Traditional methods first combine the kernels, and then annotate proteins using the composite kernels. In contrast, our approach first trains a transductive multi-label classifier (TMC) on each of the kernels representing a data source, and then integrates the predictions using an ensemble classification framework (TMEC). Our experimental evaluation demonstrates that, for the protein function prediction problem, TMEC outper-forms methods that train classifiers on composite kernels. In addition, we observe that TMEC outperforms the TMC trained on an individual kernel. This result confirms that the ensemble approach is effective, and therefore the trans-ductive multi-label classifiers trained on individual kernels are complementary to each other.
We consider R different kinds of features that describe the same set of N proteins with C functions. Different kinds of features provide different representations for proteins ( e.g. as vectors, trees, or networks). We assume that the first l proteins have known functions, and the remaining u proteins are not annotated ( l + u = N ). The R different sources of proteins are transformed into R kernels [ K r ] R r =1 R
N  X  N ), one kernel per source. Our objective is to first train a TMC on a directed bi-relation graph adapted from the kernel K r , and then combine these TMCs into an ensemble classifier, thus giving TMEC. Finally, we use TMEC to annotate the u proteins. In this section, we first review the bi-relation graph and analyze its drawbacks when used for label propagation. Next, we propose a directed bi-relation graph and train TMC on this graph. We then define our TMEC on multiple kernels.
Most graph-based multi-label learning methods explic-itly incorporate a label correlation term into the general framework of graph-based semi-supervised learning to han-dle multi-labels [10, 30]. Unlike these methods, Wang et al. [28] design an undirected bi-relation graph for image classification and apply a random walk on it [25]. This graph includes both images and labels as nodes. For consistency, hereinafter, we use proteins instead of images and functions instead of labels. A bi-relation graph is composed of three kinds of edges: between proteins, between functions, and between proteins and functions. For the latter, if protein i has function c , an edge is set between them.

A random walk on a graph is often described by a transition probability matrix. For a random walk on a bi-relation graph, the transition probability matrix W is defined as: where W PP and W FF are the transition probability matrices of the intra-subgraphs of proteins and functions, respectively. W
PF and W FP are the inter-subgraph transition probability matrices between proteins and functions, and  X  controls the relative importance of the intra-and the inter-subgraphs. W PP and W FF are computed as: where S PP  X  R N  X  N is the similarity matrix of proteins, and S FF is the correlation matrix between functions. D PP and D FF are the diagonal matrices of the row sums of S PP and S FF , respectively. The correlation between functions can be defined in various ways [9, 19, 30]. Here we define the correlation between functions m and n using the cosine similarity as follows: where f m  X  R N (1  X  m  X  C )isthe m -th function vector on all proteins: if protein i has function m , then f m ( i )=1, otherwise f m ( i )=0. W PF and W FP are calculated as: where S PF is the relation matrix between proteins and func-tions and S FP is the transpose of S PF . D PF is the diagonal matrix of the column sums of S PF and D FP is the diagonal matrix of the row sums of S PF . We observe that if protein i has function c then S PF ( i, c ) = 1 ; otherwise S PF ( i, c )=0.
Similarly to hypergraph-based multi-label learning [4], also in the bi-relation graph the c -th function node and the pro-teins annotated with this function are considered as a group: where v F c is the c -th function node and v P i is the i -th pro-tein node of the bi-relation graph. In the bi-relation graph, instead of computing the node-to-node relevance between a function node and an unannotated protein node, the rele-vance between a protein and a group G c is considered. For the c -th function, the distribution vector  X  Y c is: where  X  Y P c ( i )= 1 N controls the probability for the random walker to jump from a protein subgraph to a function subgraph.
Based on these preliminaries, an iterative objective func-tion, also applied in our TMC, is defined on this bi-relation graph as follows: where F ( t ) ( j ) is the predicted likelihood score vector of the j -th protein in the t -th iteration,  X  is a scalar value to balance the tradeoff between the initial function set and the predicted function set. From Eq. (7), we can see that the functions of a protein are predicted by the functions of its connected nodes. This makes Eq. (7) a direct protein function prediction method [23].

However, the application of Eq. (7) for protein function prediction has a major drawback. Suppose i is a protein vertex annotated with a function vertex j . The function j may be overwritten by the functions of the proteins connected to i , thus causing the loss of reliable information. As such, the functions of initially annotated proteins may be changed in the iterative label propagation. This phenomenon is similar to the one occurring in the local and global consistency method [31], and should be avoided.
 A formal analysis of the above phenomenon is as follows. For simplification, the parameter  X  in Eq. (1) is not included in the following. Eq. (7) can be rewritten as follows: From Eqs. (8-9), we can see that W PF propagates function information from function to protein nodes, and W FP prop-agates function information from proteins to function nodes. In a bi-relation graph, we expect that information propagates from function to protein nodes, but not vice versa. Thus, we change the undirected bi-relation graph into a directed one. The resulting directed bi-relation graph is shown in Figure 1. In this graph, information can be propagated in the intra-subgraphs W PP and W FF , and in the inter-subgraph W
PF , but not in the inter-subgraph W FP . Therefore, we define a new transition probability matrix W d on the directed bi-relation graph as follows: where 0  X  R C  X  N . In the case where proteins have incorrect annotations (i.e., noisy labels), it will be advantageous to use undirected relationships between proteins and functions. Based on Eq. (7), setting F (1) =  X  Y ,wehave: Since 0 &lt; X &lt; 1 and 0  X  (1  X   X  ) W d &lt; 1, the first term in Eq. (11) is bound to 0, and the second term (excluding  X  is a geometric series with the following limit: where I  X  R ( N + C )  X  ( N + C ) is the identity matrix. Thus the equilibrium solution F of Eq. (7) is: The predicted F ( j ) is a real value vector with size C , where each entry reflects the likelihood that protein j has the corresponding function. Thus, we also refer to F ( j )asthe predicted likelihood score vector of protein j . From Eq. (13), we can see that F is determined by W d and a well-structured bi-relation graph can produce a competent F .
TMC avoids the risk of overwriting the information given by function nodes. However, because of the noisy edges and isolated proteins present in a single bi-relation graph, it is still limited in providing a confident likelihood score vector F ( j ) from a single data source. To avoid this limitation, we can leverage the various graphs (or kernels) associated with thesamesetofproteins( e.g. , PPI network, gene interaction network, and co-participation network in a protein complex) [17, 24]. These graphs are, to some extent, independent to one another, and also carry partially complementary information. Some researchers have advocated integrating such multiple kernels into a composite kernel [15, 16, 17]. The experimental results showed the annotator trained on the composite kernel has a superior likelihood score vector than the annotator trained on the single kernel. But the classifiers used on the composite kernel are binary and cannot make use of the correlation between the functions.

Here we predict protein functions using multiple kernels derived from multiple sources by performing classifiers integration . More specifically, we first transform each kernel into a directed bi-relation graph. We then train a TMC on each of the graphs. Finally, we combine these TMCs into a transductive multi-label ensemble classifier, TMEC. TMEC is described in Algorithm 1 . In Eq. (14), we combine the F r values using a weighted majority vote. The motivation is that different kernels have different quali-ties and have different levels of confidence on the predicted functions of a protein. For example, if kernel K 1 is more confident on annotating protein i with function m ,and K 2 is more confident on annotating protein i with function n , then K 1 will have more influence on determining the m -th function of protein i ,and K 2 will have more influence on determining the n -th function of protein i .
 Algorithm 1 TMEC: Transductive Multi-label Ensemble Classification Input: Output: 1: Specify Y using Eq.(6) 2: for r =1to R do 3: Set W PP = K r and construct a directed bi-relation 4: Get the r th annotator F r using Eq.(13) 5: end for 6: Ensemble the R annotators { F r } R r =1 as:
Due to the different structures across different kernels, the base classifiers F r in Eq. (14) are diverse. In addition, because of the complementary information between different kernels, the predicted likelihood score vectors F r ( j )arealso complementary to each other. In contrast, the annotator trained on the composite graph cannot make use of these predicted likelihood score vectors. In ensemble learning, diversity between base classifiers is paramount to gain a consensus classifier with a good generalization ability [11]. For these reasons, TMEC can annotate proteins with higher confidence than the annotator trained on the composite kernel. Our experimental results in Section 5 confirm this advantage. An additional merit of TMEC is that it does not require to have all the data sources available beforehand, and each TMC can be trained individually or sequentially. In addition, new data sources can be appended into TMEC without repeating the entire training process.
We evaluate the performance of our algorithm on two previously defined protein function prediction benchmarks. Both these datasets provide heterogeneous protein informa-tion for the yeast ( Saccharomyces cerevisiae ) organism. The first dataset obtained from Tsuda et. al. [27] 2 consists of 3588 proteins, annotated with 13 highest-level functional classes from the MIPS Comprehensive Yeast Genome Data (MIPS CYGD) 3 . This dataset, called yeast1 here, represents pairwise protein similarities as five different kernel functions. These kernels are obtained from the following sources: Pfam domain structure ( K 1 ), co-participation within protein com-plexes ( K 2 ), PPI network ( K 3 ), genetic interaction network http://www.cbrc.jp/ tsuda/code/eccb05.html http://mips.helmholtz-muenchen.de/genre/proj/yeast/ ( K 4 ), and cell cycle and gene expression measurements ( K 5 We refer the reader to the original paper [27] for more details. The second dataset was obtained from the study by Mostafavi et. al. [15] 4 . This dataset includes 3904 pro-teins annotated with 1188 protein functions, according to the biological process terms of the Gene Ontology [5]. The dataset provides 44 different kernel functions, most of which are protein-protein interactions obtained from different ex-periments [15]. As done in the previous study [15], we filtered the dataset to include only those GO functions that had at least 100 proteins and at most 300 proteins. This resulted in a dataset containing 1809 proteins annotated with 57 different functions. We refer to this dataset as yeast2 .
We evaluate the protein function prediction problem as a multi-label classification problem. Given C different protein functions, our approach results in a predicted likelihood vec-tor that assigns a protein j to a function c with probability F ( j, c ). To convert the probabilistic assignment into a hard function assignment, we choose the top k most probable func-tion assignments. For each protein, the k largest likelihood scores are chosen as relevant functions [3, 30].

We compute the Macro F 1 score (MacroF1) across all the protein functions: where p c and r c are the precision and recall of the c -th function.

We also compute the Ranking loss. Ranking loss evaluates the average fraction of function label pairs that are not correctly ordered: where y i is the function set of protein i ,and  X y i is the com-plementary set of y i . The performance is perfect when RankingLoss = 0, and the smaller the value, the better the performance. To keep consistency with MacroF 1, we use 1  X  RankingLoss instead of RankingLoss .
In this section, we evaluate our TMC and TMEC by com-paring them against PfunBG [9], GRF [30] and SW [15]. PfunBG and GRF are recently proposed semi-supervised multi-label classifiers based on PPI networks for protein function prediction. SW is a recently introduced efficient protein function prediction method based on composite ker-nels. TMC and PfunBG have a similar objective function. The main difference between them is that TMC is trained on the directed bi-relation graph and PfunBG is trained on the undirected bi-relation graph. Here, we view function and protein nodes as equal, and set the parameters  X  and  X  in the bi-relation graph equal to 0.5. Similarly to the local and global consistency method [31] and PfunBG [9],  X  in Eq. (7) is set to 0.01. http://morrislab.med.utoronto.ca/ sara/SW/
Yeast1 has 5482 annotations on 3588 proteins, thus on average each protein is annotated with 1.52 functions. Among the 3588 proteins, 3489 (97%) proteins have  X  3 functions, so we evaluate all the methods on yeast1 with k  X  X  1, 2, 3 Yeast2 has 7874 functions on 1809 proteins, thus on average each protein has 4.35 functions. Among the 1809 proteins, 1333 proteins (73.69%) have  X  5 functions and 1656 proteins (91.54%) have  X  10 functions. Therefore, we evaluate all methods on yeast2 with k from 1 to 10.

As the authors reported in [14] and [27], the composite kernel combined with different weights has similar perfor-mance to the composite kernel combined with equal weights. Therefore, for simplicity, TMC, PfunBG and GRF are all trained on the composite kernel combined with equal weights. In addition, we take the recently proposed SW, which gives different weights to different kernels, as a baseline method. In Subsection 5.2, we investigate the performance of TMC on each single kernel. In the following experiments, unless otherwise specified, all the results are the average of 100 runs. In each run, 80% of the proteins are randomly selected for training, and the remaining 20% are used for testing.
To investigate the difference between directed and undi-rected bi-relation graphs, we compare our TMC against PfunBG and GRF on the composite kernel, and TMEC against ensemble PfunBG (PfunBG-MK) and ensemble GRF (GRF-MK) on yeast1 and yeast2. The base classifiers of PfunBG-MK and GRF-MK on these 5 kernels are combined inthesamewayasTMECinEq.(14).Table1andTable2 list the corresponding macro F1 scores and RankingLoss on the composite kernel and multiple kernels of yeast1. Table 3 reports the RankingLoss on the composite kernel of yeast2, and Table 4 records the RankingLoss on the multiple kernels of yeast2. Figure 2 plots macro F1 scores on the composite kernel and multiple kernels of yeast2. In these tables, results reported in boldface are significantly better, with significance level 95% in 100 runs using paired t -test. Standard deviations are also reported. The same t -testisalsousedtoanalyze the results in Figure 2. In these figures, the highest bar in each index indicates a significantly better result.
From these tables and figure, we can observe that, in most cases, TMC trained on the directed bi-relation graph is capable of achieving a higher performance than PfunBG trained on the undirected bi-relation graph. These results show the advantage of using a directed bi-relation graph. SW takes advantage of regression to seek the optimal combining weights, and binary classification to predict protein functions; it achieves a higher RankingLoss on yeast1, but it loses to PfunBG and TMC on the macro F1 score. As to yeast2, TMC significantly outperforms the other three methods on MacroF1 and RankingLoss. The ensemble classifiers are capable of achieving a better performance than the corre-sponding classifiers trained on the composite kernel. This result supports the use of multi-label ensemble classification for protein function prediction.
In this section, we conduct additional experiments to in-vestigate the difference between the TMC on the composite kernel, the TMC on a single kernel from one data source, and the TMEC on multiple kernels. Since the average number of functions of a protein is 1.52 in yeast1 and 4.35 in yeast2, we set k = 2 on yeast1 and k = 5 on yeast2. The results are plotted in Figures 3 and 4. The first two bars in each figure represent TMEC and TMC on the composite kernel; the remaining bars describe the results of TMC trained on a single kernel. The highest bar in each index indicates that the corresponding result is significantly better than the others bars for that index.

From Figures 3 and 4, we can observe that TMC trained on the composite kernel always outperforms TMC trained on a single kernel. This can be attributed to the complementary information across different kernels. TMEC performs better than TMC trained on the single kernel, and it outperforms TMC trained on the composite kernel (except in Figure 3(b)). The explanation is that TMEC not only takes advantage of the complementary information between different kernels, but also makes use of the structural difference among differ-ent kernels and the complementary likelihood score vectors. These results support the benefits of classifier integration over data integration .
TMC and TMEC have three parameters:  X  ,  X  and  X  .  X  is a parameter common to label propagation methods and often is given a small value. In this section, we analyze the sensitivity of our methods with respect to  X  and  X  .Wevary  X  and  X  between 0 . 1 and 0 . 9 with step size 0.1, and record the macro F1 score and Ranking loss. The experimental results are given in Figures 5 and 6.

From these figures, we can observe that TMEC is less sensitive than TMC to parameter selection. TMEC has wider ranges of effective parameter values than TMC. We also computed the maximum, minimum, average, and median values of MacroF1 and RankingLoss for the different values of  X  and  X  . Due to space limit, we do not report them here. With respect to these four statistics, the values of MacroF1 and RankingLoss of TMEC are always better than that of TMC. This result again supports the advantage of classifier integration over data integration .
In this paper, we analyze the drawback of using undi-rected bi-relation graphs for protein function prediction. To avoid this limitation, we propose to use a directed bi-relation graph, and define a TMC on it. We further improve the performance by combining various TMCs trained on multiple data sources (TMEC). Different from traditional methods that make use of multiple data sources by data integration, TMEC takes advantage of multiple data sources by classifier integration. TMEC does not require to collect all the data sources beforehand. Our experimental results show that classifier integration is a valuable methodology to leverage multiple biological data sources.

The experimental results show that different kernels have different levels of quality. We will investigate a scheme that gives optimal weights to multi-label classifiers trained on different kernels, and then combine such classifiers using the weights for improved protein function prediction.
This paper is partially supported by grants from NSF IIS 0905117, Natural Science Foundation of China (Project Nos. 1 . 40 46 . 70  X  1 . 30 80 . 93  X  0 . 99 1 . 27 47 . 40  X  1 . 25 80 . 50  X  0 . 87 1 . 19 47 . 11  X  1 . 01 80 . 79  X  1 . 01  X  1 . 28 80 . 44  X  0 . 88 denotes TMC on the single kernels of yeast1 denotes TMC on the single kernels of yeast2. (c) TMC (RankingLoss) (c) TMC(RankingLoss) 60973083, 61070090, 61003174 and 61170080), grants from the NSFC-Guangdong Joint Fund (Project No. U1035004 and U1135004) and China Scholarship Council (CSC). [1] Z. Barutcuoglu, R. Schapire, and O. Troyanskaya. [2] M. Belkin, P. Niyogi, and V. Sindhwani. Manifold [3] P. Bogdanov and A. Singh. Molecular function [4] G. Chen, J. Zhang, F. Wang, C. Zhang, and Y. Gao. [5] G. O. Consortium et al. Gene ontology: tool for the [6] A. Dempster, N. Laird, and D. Rubin. Maximum [7] A. Elisseeff and J. Weston. A kernel method for [8] M. G  X  onen and E. Alpaydin. Multiple kernel learning [9] J. Jiang. Learning protein functions from bi-relational [10] J. Jiang and L. McQuay. Predicting protein function by [11] L. Kuncheva and C. Whitaker. Measures of diversity in [12] G. Lanckriet, T. De Bie, N. Cristianini, M. Jordan, and [13] C. Leslie, E. Eskin, A. Cohen, J. Weston, and [14] D. Lewis. Combining kernels for classification .PhD [15] S. Mostafavi and Q. Morris. Fast integration of [16] S. Mostafavi, D. Ray, D. Warde-Farley, C. Grouios, and [17] W. Noble and A. Ben-Hur. Integrating information for [18] G. Pandey, V. Kumar, and M. Steinbach.
 [19] G. Pandey, C. Myers, and V. Kumar. Incorporating [20] P.Pavlidis,J.Weston,J.Cai,andW.Noble.Learning [21] M. Re and G. Valentini. Ensemble based data fusion [22] A. Ruepp, A. Zollner, D. Maier, K. Albermann, [23] R. Sharan, I. Ulitsky, and R. Shamir. Network-based [24] H. Shin, K. Tsuda, and B. Sch  X  olkopf. Protein functional [25] H. Tong, C. Faloutsos, and J. Pan. Random walk with [26] G. Tsoumakas, I. Katakis, and I. Vlahavas. Mining [27] K. Tsuda, H. Shin, and B. Sch  X  olkopf. Fast protein [28] H. Wang, H. Huang, and C. Ding. Image annotation [29] J. Weston, C. Leslie, E. Ie, D. Zhou, A. Elisseeff, and [30] X. Zhang and D. Dai. A framework for incorporating [31] D. Zhou, O. Bousquet, T. Lal, J. Weston, and
