 Link analy sis algorithms have b een extens ively us ed in W eb inform ation retrieval. However, current link analy sis algorithms generally work on a flat link graph, ignoring the hierarchal structure of the Web graph. They often suffer from two problems: the sparsity of link graph and biased ranking of newly -emerging pages. In this paper, we pr opose a novel ranking algorithm called Hierarchical Rank as a s olution to thes e two problem s, which cons iders both the hierarchical s tructure and the link structure of the W eb. In this algorithm , W eb pages are firs t aggregated bas ed on their hierarchical structure at directory , host or domain level and link analy sis is perform ed on the aggregated graph. Then, the im portance of each node on the aggregated graph is distributed to individual pages belong to the node based on the hierarchical structure. This algorithm allows the importance of linked Web pages to be dis tributed in the W eb page s pace even when the space is spars e and contains new pages . Experim ental res ults on the .GOV collection of TREC 2003 and 2004 show that hierarchical ranking algorithm c onsistently outperforms other well-known ranking algorithms, including the PageRank, BlockRank and Lay erRank. In a ddition, experimental results show that link aggregation at the host level is m uch better than link aggregation at either the domain or directory levels. H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval; H.3.3 [ Information Interfaces an d Pres entation ]: Hy pertext/Hy perm edia; Experimentation. Keyw ords: Link Analy sis, Hierarchical W eb graph, Hierarchical Random Walk Model. Link analy sis algorithm s play key roles in W eb s earch s ystem s. They exploit the fact that the Web link structure convey s the relative importance of Web pages. For example, Google X  X  PageRank [24] is a widely a pplied algorithm, which can be described as the stationary probability distribution of a certain random walk on the Web link graph -the graph whose nodes are the individual Web pages and dir ected edges are the hy perlink between pages. However, the existing link analy sis algorithms often suffer from two problems: sparsity of link-graph and biased ranking of newly -emerging pages [10] [14] . The first problem is caused by the fact that the link distribution of the Web graph generally satisfies the power law [15] and the sparse link m atrix makes most of the pages unable to obtain any importance ranking at all [11] [16] . Additionally , such link distribution makes the distribution of PageRank scores follow a power law [28] . The second problem implies that a ne wly emerging Web page has too few in-links to obtain a reas onable importance scores [10] . We suggest in this paper using the inherent hierarchical structure of the Web, which is embedded in URLs, to solve the problem s. For example, in URL http://www. cs. berkeley .edu/Research/ Projects/, we might expect to find some project-related information about research in the computer science department of UC Berkeley . In [26] , S imon argued that all the s ystem is likely to be organized as a hierarchical s tructure. The World Wide Web is a good example of the hierarchical organization [5] . From a local view of the W eb, a W eb s ite is organized as a hierarchical structure where the hierarchical inform ation is repres ented by the directory structure. From a globa l view of the WWW, the whole Web is als o organized as a hierarchical s tructure, in which the first level provides the top-level domains (such as berkeley .edu). Subsequently , the following levels contain the virtual hosts, the virtual folders and the Web pages. Furthermore, as discussed in [13] [23] , the link structure of Web is closely related to such hierarchical s tructure too. In this paper, we take the link structure of the W eb as well as the hierarchical s tructure of the W eb into cons ideration in page im portance calculation. Different from the traditional flat link graph, we construct a new Web-li nk graph which consists of two lay ers, i.e. the upper-lay er graph and lower-lay er graph, as shown in Figure 1. A novel random wa lk model is defined, which assumes that a user s eeks for inform ation by starting from the upper-lay er and either random jump to another upper-lay er node or follows the hierarchical links down to the lower-lay er. Bas ed on this random walk model, we propose a new link analy sis algorithm called Hierarchical Rank to calculate the im portance of supernode that is computed at the upper-lay er graph to be propagated down to the Web pages in the lower-lay er graph. We show that the link distribution in the upper-lay er graph is much denser than that in general link graph. By exploiting this fact, the propagation step can solve the new page issue more satisfactorily . In this paper, we conduct expe riments on the .GOV collection of TREC 2003 and 2004. The experiment results show that our hierarchical rank consistently outperforms existing ranking algorithms at the flat link graph. Furthermore, we demonstrate that the aggregation at host level works m uch better than other two aggregation level, i.e. domai n level and directory level. review the recent works on the link analy sis and W eb graph analy sis. Then, we present th e characteristics about the Web graph in Section 3. In Section 4, we propose our hierarchical ranking algorithm to take the link structure and hierarchical presented in Section 5. Finally , we summarize our main contributions and discuss the fu ture works in Section 6. There are two types of works in recent res earch area, one is working on  X  X ink analy sis X , and the other is working on  X  X eb structure analy sis X . The link analy sis technology has been widely used to analy ze the Web pages X  importance, such as HITS [20] and PageRank [7] [24] . PageRank is a core algorithm of Google which measures the importance of Web pages. It models the users X  browsing behaviors as a random surfing mode l, which assumes that a user either follows a link from the current page or jumps to a random page in the graph. The PageRank of a page p i is then com puted by the following equation: where  X  is a dampening factor, which is usually set between 0.1 and 0.2; n is the number of nodes in G ; and out-degree( p number of the edges leaving page p j , i.e., the number of hy perlinks in page p j . The PageRank could be computed by an iterative algorithm and corresponds to the primary eigenvector of a m atrix derived from adjacency m atrix of the available portion of the Web. Some extended link an aly sis algorithms to PageRank and HITS are proposed recently , such as [4] [8][9] [17] . But m ost of these works only consider the  X  X lat X  Web link structure and assign the same weight to the hy perlink. In [18] , the author divides the Web graph into different blocks. According to the hy perlink that links between th e different blocks or among the same block, they assign the differe nt weight to the hy perlinks to com pute the P ageRank and evaluate the perform ance for s earch. Recently , there are als o several wo rks to cons ider the aggregated graph analy sis. A. Z. Broder et al. [6] proposed an efficient PageRank approximation on the aggr egated graph. Y. As ano [1] proposes to find the directory -based site and then to efficiently find the Web com munity . Recently , W u et al. [27] proposed a two-lay er Lay ered Markov Model for decentralized Web ranking. In our experiments presented late r in the paper, W u et. al X  X  algorithm is com pared as the nam e  X  X ayerRank X . Since the algorithm pay s much attention on the intra-link and does not consider the inter-link when com puting the im portance of the page lay er, its performance is not better than the PageRank. As we discussed, the above algorith ms only consider the individual Web pages as the elem ents while the inherent hierarchical structure of the Web are not taken into account. In this paper, we propose a combination of the host structure and the link analy sis and show that this integration could improve the performance of ranking results. There are many res earch works on exploiting the hierarchical structure of the Web. Nadav et al. [12] proposed that hy perlinks tend to exhibit a  X  X ocality  X  that is correlated to the hierarchical structure of URLs, and that many features of the organization of information in the Web are predictable from the knowledge of the hierarchical structure. Kam var et al. [19] proposed to utilize the hierarchical s tructure of the W eb graph to accelerate the computation of PageRank. There als o exis ts some work that m odel the W eb by cons idering the hierarchical structure of the W eb. A hierarchical m odel of the Web was previously suggested by Laura et. al [21] . In their model, every page that enters the graph is assigned a constant value for the abstract  X  X egion X  the page belongs to; the page is allowed to link only to other pages in the sa me region. Nadav et al. [13] described a Web-graph model to integrate the link s tructure and the explicit hierarchical s tructure together, which reflects a social division by the organization. We first list the key term inologies used in the following http://cs.stanford.edu/research/ inde x.htm , as shown in Table 1. Term Example: cs.stan ford .edu/res earch /in dex.h tm Domain Host Directory 
Page As mentioned in Section 1, the We b is organized as a hierarchical structure. In this paper, we pr opose a two-lay er model of the Web. The upper-lay er graph is an aggreg ated link graph which consists with supernodes and superedges, in which each supernode (such as domain, host and directory ) aggregates the pages from the same supernode and superedge s among supernodes are also aggregated from the underly ing links of pages. The lower-lay er graph is the hierarchical tree stru cture, in which each node is the individual Web page in the supernode, and the edges are the hierarchical links between the pages . (S ee F igure 1). Within the hierarchical s tructure, the W eb contains the W eb pages , the directories , the hos ts as well as the dom ains . Thus , the whole Web graph could be abs tracted according to a hierarchical structure, such as a page level, a directory level, a host level and a domain level. According to the diffe rent levels, the hy perlinks at each level can be divided into two types : intra-links and inter-links . For exam ple, when we cons truct the abs tract W eb graph at the directory level, the Web pages in the same directory are organized as a super node and the hy perlinks that link two Web pages in the directory are called intra-links. Likewise, the hy perlinks that link two Web pa ges in different super nodes are called the inter-links. Furtherm ore, the hy perlink of a page could outlink that links from the page. A ccording to the analy sis in [18] [22] , the intra-link play s less value than the inter-link when computing the PageRank at differe nt level. However, the work still does not exploit the hierarchical structure inherent in the Web graph. gather the following statistics. W e take all the hy perlinks in the .GOV collection, and count how many of the links are  X  X ntra-link X  and  X  X nter-link X  at different abstract level. Tab le 2. Lin k Dis trib ution over .GOV Collection at Differen t As shown in Table 2, the percen tage of the intra-link and the inter-link at four different levels is different. Take the result of the host level as an example, there are about 86% of all links were intra-links to a host, which tend to show a high degree of locality [13] . Another observation is interesting. We have also found that when links are inter-links between two different hosts, they tend to link have also shown up in other way s [19] , allowing very high levels of compression for the link graph and enabling a block-oriented approach to accelerate the convergence of PageRank. Based on the analy sis, we construc t a two-lay er m odel of the W eb, which is described in detail as follows. We us e the G =( V , E ) to repres ent the directed graph of the W eb, where the V and E refer to the vertex and edge set respectively . If a page p is represented by a graph vertex, we will use p to refer to the page as well as to the vertex. definition, this is also a partition of all the pages in the W eb ). We define the following ty pes of directed graphs. Upper-layer graph . An upper-lay er graph contains m vertices called supernodes, one for each elem ent of the partition. Supernodes are linked to each othe r using directed edges called superedges. Superedges are creat ed based on the following rule: there is a directed s uperedge j i E , from S i to S one page in S i that links to som e page in S upper-lay er graph is a weighted graph where the weight of the edges from S i to S j is the number of the links from pages in S pages S j .
 Lower-layer graph . In order to capture the es sence of the structure within a supernode, we represent all the individual pages { p , p 1 , ..., p n } in a supernode S with a hierarchical tree s tructure by considering the URL properties. Thus, all the pages in one supernode are organized by the URL relationship. For example, a supernode begins with a unique root node, which is entry page of supernode. Then the pages in the first level provide the children nodes of the root page, and so on. The structure in Figure 2 is generated by this method at the level of the host. According to the inherent hierarchical s tructure of the W eb, we first consider a surfing model that considers both the link structure and the hierarchical s tructure of the W eb. The hierarchical random walk model is as follows: 1. At the beginning of each browsing session, a user random ly selects the supernode. 2. After the user finished reading a page in a supernode, he may select one of the following three actions with a certain probability : (a). Going to another page w ithin current supernode, following the hierarchical link structure of the supernode. (b). Jumping to another super node that is linked by current supernode. (c). Ending the browsing. According to the hierarchical ra ndom walk model, we can apply a two-s tage com putation of the hierarchical rank: firs t s tage calculates the im portance of supernodes according to the surfing behaviors among the supernodes and second stage calculates the im portance of pages according to the s urfing behaviors ins ide a supernode. In the first stage, a user selects a supernode randomly , and jumps to another supernode random ly according to the superedges. The supernode surfing behavior in our model is exactly the same as the inter-page surfing behavior in PageRank. Thus, we can obtain the importance of supernodes by performing the PageRank algorithm on the supernode-level weighted graph. In the second stage, we deal with page-level surfing inside a supernode. Since the interest of any page can be traced back to the root of the supernode, and the interest will dissipate when propagating among pages inside a supernode, it is analogously like a Dissipative Heat Conductance (DHC) model [29] to des cribe the behaviors of users . In the DHC m odel, we place a single heat s ource with tem perature SI i at the root of each supernode S i , where SI i is the importance of the supernode S heat dissipates when it propagates alone with the tree structure inside the supernode P j . If we keep tem perature of the entry point constant, the tem perature will converge after propagation for a tim e. The final tem perature of each page gives the im portance of that page. W e give details in the following sections. We represent the upper-lay er gra ph as a matrix. Suppose that the Web contains m supernodes, the m  X  m adjacency m atrix is denoted by A and the entries A [ i , j ] stands for the weighted link from supernode i to supernode j . The adjacency m atrix A is used to com pute the im portance of each supernode S i , denoted as SI im portance of all the hos ts that point to host i : However, in practice, many supe rnodes have no in-links (or the weight of them is 0), and the ei genvector of the above equation is mostly zero. Therefore, the bas ic equation (3) is m odified to obtain a  X  random walk mode l X  to deal with this issue. W hen browsing a supernode, with the probability 1- X  , a user randomly chooses one of the links on the current supernode and jumps to the supernode it links to. With a probability  X  , the us er  X  resets  X  by jumping to another supernode uniformly from the collection of supernodes. Therefore, the s upernode importance formula is modified as Equation (4): Or in a m atrix form : where e r is the vector of all 1 X  X , and  X  (0&lt;  X  &lt;1) is a parameter. In our experiment, we also set  X  to 0.15. After obtaining the im portance of a supernode, the intuitive method for assigning the importance to a page is the method proposed in [14] , in which the importance of the pages in the supernode is equal to the importa nce of the supernode they belong to. Here we propose that the page s X  importance scores should be calculated according to the im portan ce of the supernode as well as the hierarchical structure in the supernode. In this section, we introduce a Dissipative Heat Conductance (DHC) m odel to com pute the im portance of the pages in a hierarchical s tructure. Several factors could affect the calculation of the page X  X  im portance, s uch as whether a page is an index page or a content page, as well as the number of ex ternal links from the outside of the supernodes. We formulate a hierarchical tree structure of the supernode as a directed weighted tree structure as shown in Figure node to its child nodes and also the edge is weighted by the properties of the Web pages. Here a function is given to calculate the weight of page related to his parent page. Given a page p j in the supernode S  X  j to its parent node is calculated as: Index is used to assign different weights to the index pages and other pages. Here we also use the hierarchical rule to judge ended with  X / X , the page is the index page. The parameter  X  is a factor between 0 and 1. Its importance is explained later. Link is defined to calculate the percentages of the inlinks that the page p j has. Form ally , the functi on is defined as follows: where  X  is the factor to give the inter-link and the intra-link with the different weight. IIL ( p j ) is the number of the intra-hy perlink to Bas ed on the hierarchical weighted structure, a page X  X  im portance in a supernode can be calculated recursively from the root page down to the bottom pages by using DHC algorithm . Actually , in this paper, we use the equations 8, which is sim plifications of DHC , to calculate the page X  X  im portance bas ed on the hierarchical weight structure. Each page p j gets a value w ij important the page p j is in the supernode S i . where the param eter  X  is a heat dissipative factor. Thus, we get a m atrix W m  X  n that each entry w ij is value of page p importance in supernode S i . Obviously , if page p j does not belong set to 1. Finally , the importance of a page p j on the whole Web graph , denoted as PI j , is calculated as follows: Or in a m atrix form : where the page p j belongs to supernode S i , SI i is the im portance of importance of supernode. An ex ample is shown in Figure 4. We designed the experiment on the TREC . GOV dataset. The queries of the topic distillation in TREC 2003 and TREC 2004 are used in our experiments. BM2500 MAP 0.1285 0.1517 The collection of TREC .GOV c onsists of 1,247,753 Web pages from a fresh crawl of the Web pages made in early 2002. Among them, 1,053,372 are text/html, whic h are used in our experiment. There are totally 7,569,353 hy perlinks in the collection. In the experiment, we used BM2500 [25] as the relevance weighting function. The mean average precision on TREC 2003 is 0.1285 and the P@10 is 0.112. Compared with the best result of TREC 2003 anticipants (with MAP of 0.1543 and P@10 of 0.1280), this baseline is reasonable. While the performance of BM2500 on TREC 2004 is that the mean av erage precision and the P@10 are 0.1517 and 0.2053, respectively . Details are shown in Table 3. In order to m easure the retrieva l performance of different ranking algorithm s, we take P @10 and M eans Average P recis ion (M AP ) as the evaluation metrics which is widely used in TREC and the details could be found in [3] . As we discussed in Section 1, the sparseness problem and new pages problem are two major problems in current link analy sis. We als o com pare the effectivenes s of different ranking algorithm s on solving the two problems by KDist [19] distance. Consider two partially ordered lists of web pages  X  1 and  X  2 , KDist( randomly selected pair of distinct nodes. In this work, we just com pared the lis ts containing the s ame sets of Web pages, so that KDist is identical to Kendall X  X   X  dis tance. We describe four existing ranki ng methods to compare with our proposed hierarchical ranking algorithm. We im plem ented the PageRa nk algorithm based on the link matrix deduced from traditional page level link analy sis. In [18] , the authors divide the whole graph into blocks. Then the hy perlinks between the different blocks and the hy perlinks among the same blocks are assigned di fferent weights when computing the important of the Web pages. Their experiments verified that the host level could achieve the higher performance than the domain level and the directory leve l. In this paper, we also com pare with the host level partition. As we reviewed in Section 2, a two-lay ered ranking algorithm [27] was proposed. One lay er is host-lay er PageRank and the other lay er is document-lay er PageRank inside a host. Then, a weighted product between them is applied to obtain the final global ranking for all the Web pages. We also conduct an experiment to compare with the block-level PageRank [12] , which divide the whole Web page into different semantic blocks. Then the Pa geRank algorithm is performed on the block-level. Hierarchical ranking corresponds to our proposed algorithm, which interpolates the link s tructure and the hierarchical s tructure together. Three level abstracti ons are proposed to partition the Web s pace into a dom ain level, a hos t level and a directory level. Accordingly , we define corresponding ranking as Domai nRank , HostRank and DirectoryRank , res pectively . In the following test, we chose the top 2000 results according to the BM2500 score. Then we combine the relevance with the im portance as follows: The function f can be score-based or the order-based of the page p in the results. The score-based function is the relevance score or the im portance score, while the order-based function is the page X  X  position in the list which is sorted by relevance score or by importance score. For each ranking m ethod, we both evaluate the two functions on the TREC 2003 and take the higher performance function as the combining method. Several parameters for our experi ments are fixed in the following experiments, i.e.  X  =0.6,  X  =0.6,  X  =0.4 and  X  =0.8. W e us ed the ranking  X  Through tuning on TREC 2003 for the best p@10 precision, we set the combining parameter  X  to 0.85, 0.6 and 0.73 for DomainRank, HostRank and Dir ectory Rank, res pectively . For the methods of PageRank, Lay erRank, WeightRank and BlockRank, we take the parame ters that achieve the best performance on these ranking. The partitioned Web graph could be aggregated into different abstraction levels, the domain, hos t, and directory levels, which correspond to three ranking algorithms: DomainRank, HostRank and Directory Rank, res pectivel y. We make the comparison on TREC 2003 and TREC 2004 to show which level gives a higher perform ance for each algorithm s. F igure 5 s hows that the hos t-level abs traction is the bes t choice for the hierarchical link analy sis. The perform ance decreas ed at the directory -level link analy sis since the navigation links within the host do affect the perform ance of link analy sis. A dditionally , the domain-level link analy sis als o can not achieve the high perform ance. By observation on the link data, some useful inter-links between the hosts which are recommendation hy perlink, cannot take into account when computing the DomainRank. So the rest experiments are all conducted at the host level. That is, we compare other algorithms with the HostRank. As can be seen in Figure 6, all ranking algorithms achieve higher performance than the baseline of relevance function, which confirm s that the link analy sis on the TREC data could work well. In the topic distillation task of TREC 2003, our proposed HostRank algorithm significantly outperforms BM2500, BlockRank, PageRank, Lay erR ank and WeightRank on average precision by 57.4%, 25.5%, 36.2%, 46.0%, and 33.1%, respectively . Also, on the measure of P@10, HostRank significantly outperforms BM2500, BlockRank, PageRank, Lay erRank and WeightRank by 46.4%, 12.3%, 22.4%, 43.9%, and 15.5%, respectively . In the topic distillation task of TREC 2004, our proposed HostRank algorithm significantly outperforms BM2500, BlockRank, PageRank, Lay erR ank and WeightRank on average precision by 14.3%, 5.9%, 4.6%, 11.8%, and 5.7%, respectively . Meanwhile, on the measure of P@10, HostRank significantly outperforms BM2500, BlockRank, PageRank, Lay erRank and WeightRank by 21.4%, 14.0%, 16.9%, 20.6%, and 16.1%, res pectively . From results on the queries of topic distillation task of TREC 2003 and 2004. Our proposed hierarchical ranking algorithm consistently achieves highest performance than the well-known link analy sis algorithm s. Integrating the hierarchical structure and the link s tructure of the W eb exactly im prove the perform ance of retrieval. Lay erRank algorithm , which perform s two-lay er PageRank calculation, can not achieve better perform ance than PageRank and Weight Rank. Since the algorithm pay s much attention on the intra-links of the host while these hy perlinks contain less information. Weight Rank algorithm can also acquire higher performance than the PageRank and Lay erRank algorithms. By assigning different weight to th e intra-and inter-links of the Web graph, the effect by the na vigation link can be leveraged to some extent. BlockRank algorithm can achieve higher perform ance than P ageRank, W eightRank and Lay erRank for the method provide more semantic link graph than on the page level. To understand whether these improvements are statistically significant, we performed t-tests and the results are shown in Table 4. Compared to the baseline BM2500, all the t-test results show that HostRank significantly im proves the s earch res ults. The density of a link graph can have a significant impact on the perform ance of link analy sis. To show the performance of our ranking function, we conducted an experiment to simulate the phenomenon of the sparseness of link graph and compare the performance about four rank algor ithms: PageRank, WeightRank, Lay erRank and HostRank. In Figure 7 we empirically analy ze how KDist between ranking orders on the sparse link graph and the whole link graph evolves when the density of link graph becomes from slight to strong. In this experiment, we randomly select 20%, 40%, 60%, 80% and 100% of the whole hy perlinks to represent different degree of how tightly the link graph is. The results show that the degree of how tightly the link graph is has different impact on the final ranking of the different link analy sis algorithms. When the link graph becomes denser, the ranking orders of all link analy sis algorithms tend to the final orders . As seen from the F igure 7, the KDist of HostRank algorithm is belo w that of the other algorithm, which m eans that the s pars enes s has the leas t im pact on the ranking order of the HostRank algorithm. We also conduct the experim ents by utilizing the different ranking to show the performance on retrieva l. As seen in F igure 8, the HostRank both achieve the highest performance and least impact on the sparse link graph. P@10 of HostRank is decreased about 1% when the hy perlink number changed from 100% to 20% on TREC 2003, while P@10 of other three algorithms is decreased about 2%. As we mentioned in Section 1, our methods can alleviate the new pages biased problem [10] . To show the performance of our ranking function, we conducted an experiment to simulate the phenomenon of the new pages and compare the performance about four rank algorithms: Page Rank, WeightRank, Lay erRank and HostRank. First, we randomly select 10,000 pa ges with different rank values as the test pages. Then, we remove 90% of the hy perlinks that linked to the 10,000 pages. In th is way , we constructed a new Web graph with the remaining hy perlinks. We performed four algorithms on the modified Web graph and then m eas ured the KDist to show the effect on the new pages. The results are shown in Table 5. The average distance KDist is 0.0159 for the HostRank in the .GOV dataset. Notice that it is lower. This means that the ordering induced by HostRank on the partial graph is very close to HostRank on the full graph and th e new pages could also get its more approximate ranking by the HostRank algorithm. In this section, the experiment s are conducted on the 50 queries of topic distillation at TREC 2003 to show how the param eters affect the performance of our proposed hi erarchical ranking algorithms. We conduct several experiment s on the queries of topic distillation at TREC 2003 by host level ranking algorithm to show the performance on different parame ters. Each param eter is tuned independently . In Equation 6, the parameter  X  is us ed to s how a trade-off between the index page and the non-index page. As shown in Figure 9-1, when we set  X  to 0.6, the hierarchical rank could achieve the highes t perform ance. Meanwhile, we als o perform the experiment to show the different weight on the performance between intra-inlink and inter-inlink. As shown in Figure 9-2, experime ntal res ult als o verifies that inter-inlink should give higher weight than intra-inlink and the ranking could get the highest performance when  X  in equation 7 is set to 0.4. In Equation 5, we linearly combine the link function and index function by the parameter  X  . As shown in Figure 9-3, the com bination param eter is set to 0.6, which im plies that the index function is more important than the link function which is also confirmed by the experime nt in Section 5.5.2. The last experiment is on tuning the parameter  X  in the Equation 8. As shown in Figure 9-4, the hierarchical rank could achieve the highes t perform ance when the heat dissipative factor is set to 0.8. Lower or higher value could not achieve the higher performance. 
Figure 9. Performance of Di fferent Propagation Parameters As described in Equation 5, 6, a nd 7, there are three factors that affect the calculation of a pages X  importance: the depth of the pages in the tree structure (Level), the in-link distribution of the pages (InLink) and whether the page is an index page (Index). In this section, we conduct experi ments on the queries of topic distillation track at TREC 2003 to show how the param eters affect the performance of hierarchical ranking algorithm individually . The baseline method directly sets the importance of the host to the pages in the host. We denote its method as  X  X one X . As shown in Figure 10, these features have impacts on the perform ance of retrieval. The factor of whether a page is an entry page is the m ost im portant in the topic-distillation task. Then the level of the page also shows great improvement on the perform ance. The in-link feature shows little im provem ent, which also testify that the intra-links ar e less valuable for link analy sis. Considering both the hierarchical structure and the link structure of the Web, we proposed a Hierarchical Random Walk Model, which approximates the behaviors of the users X  surfing the Web. Based on the model, we presented a hierarchical ranking algorithm to calculate the importance of Web pages. The ranking algorithm can significantly improve the performance of Web search, efficiently alleviate the sparse link problem and assign the reasonable rank to the newly-emerging Web pages. In this work, we only perform the experiments on the TREC .GOV collection, which might be different in the other domain. In future work, we will conduct the experiments on the large scale Web collection to evaluate our algorithms. [1] Y. Asano, Applying the Site In formation to the Information [2] R. Baeza-Yates, F. Saint-Jean, and C. Castillo. Web [3] R. Baeza-Yates and B. Ribeiro-Neto. Modern Information [4] K. Bharat and M. R. Henzinger. Improved Algorithms for [5] K. Bharat, B. W. Chang, M. R. Henzinger, and M. Ruhl. [6] A. Broder, and R. Lemp el. Efficient PageRank [7] S. Brin and L. Page. The Anatomy of a Large-Scale [8] S. Chakrabarti, B. Dom, D. Gibson, J. Kleinberg, [9] S. Chakrabarti, M. Joshi, a nd V. Tawde. Enhanced Topic [10] J. Cho and S. Roy. Impact Of Search Engines On Page [11] B. D. Davison. Recognizing Ne potistic Links on the Web. In [12] D. Cai, X. F. He, J. R. We n and W.Y. Ma. Block-level Link [13] N. Eiron and K. S. McCurley. Locality, Hierarchy, and [14] N. Eiron, K. S. McCurley, and J. A. Tomlin. Ranking the [15] M. Faloutsos, P.Faloutsos, a nd C.Faloutsos. On Powerlaw [16] C. Gurrin and A. F. Smeaton. Replicating Web Structure in [17] T. H. Haveliwala. Topic-Sensitiv e PageRank. In Proc. of the [18] X. M. Jiang, G. R. Xue, H. J. Zeng, Z. Chen, W.-G. Song, [19] S. D. Kamvar, T. H. Haveliwala C. D. Manning and G. H. [20] J. Kleinberg, Authoritative Sources in a Hyperlinked [21] L. Laura, S. Leonardi, G. Cald arelli, and P. D. L. Rios. A [22] C. Monz, J. Kamps, and M. de Rijke. The University of [23] E. Ravasz and A.L.Barabasi. Hierarchical Organization in [24] L. Page, S. Brin, R. Motwani, and T. Winograd. The [25] S. E. Robertson. Overview of the Okapi Projects. Journal of [26] H. A. Simon. The Sciences of the Artificial. MIT Press, [27] J. Wu, K. Aberer. Using a Layered Markov Model for [28] G. Pandurangan, P. Raghava n, and E. Upfal. Using [29] D. Zhou, J., Weston, A. Gretton, O. Bousquet, and B. 
