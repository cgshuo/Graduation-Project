 This paper investigates the properties of large n -best lists in the context of statistical machine trans-lation (SMT). We present a method that allows for fast extraction of very large n -best lists based on the k shortest paths algorithm by (Eppstein, 1998). We will argue that, despite being able to generate a much larger amount of hypotheses than previously reported in the literature, there is no significant gain of such a method in terms of translation quality.
In recent years, phrase-based approaches evolved as the dominating method for feasible machine translation systems. Many research groups use a de-coder based on a log-linear approach incorporating phrases as main paradigm (Koehn et al., 2003). As a by-product of the decoding process, one can extract n -best translations from a word graph and use these fully generated hypotheses for additional reranking.
In the past, several groups report on using n -best lists with n ranging from 1 000 to 10 000. The ad-vantage of n -best reranking is clear: we can apply complex reranking techniques, based e.g. on syntac-tic analyses of the candidates or using huge addi-tional language models, since the whole sentence is already generated. During the generation process, these models would either need hard-to-implement algorithms or large memory requirements. 1.1 Related work The idea of n -best list extraction from a word graph for SMT was presented in (Ueffing et al., 2002). In (Zens and Ney, 2005), an improved method is re-ported that overcomes some shortcomings, such as duplicate removal by determinization of the word graph (represented as a weighted finite state automa-ton) and efficient rest-cost estimation with linear time complexity.

There are several research groups that use a two-pass approach in their MT systems. First, they gen-erate n -best translation hypotheses with the decoder. Second, they apply additional models to the out-put and rerank the candidates (see e.g. (Chen et al., 2006)).

Syntactic features were investigated in (Och et al., 2004) with moderate success. Although complex models, such as features based on shallow parsing or treebank-based syntactic analyses, were applied to the n -best candidates, the  X  X impler X  ones were more promising (e.g. IBM model 1 on sentence-level).
In the following section 2, we describe our SMT system and explain how an improved n -best extrac-tion method is capable of generating a very large number of distinct candidates from the word graph. In section 3, we show our experiments related to n -best list reranking with various sizes and the cor-responding performance in terms of MT evaluation measures. Finally, we discuss the results in section 4 and give some conclusive remarks. We use a phrase-based SMT system (Mauser et al., 2006) and enhance the n -best list extraction with Eppstein X  X  k shortest path algorithm which allows for generating a very large number of translation candidates in an efficient way. 2.1 Baseline SMT system The baseline system uses phrases automatically ex-tracted from a word-aligned corpus (trained with GIZA ++ ) and generates the best translations using weighted log-linear model combination with several features, such as word lexicon, phrase translation and language models. This direct approach is cur-rently used by most state-of-the-art decoders. The model scaling factors are trained discriminatively on some evaluation measure, e.g. BLEU or WER, using the simplex method. 2.2 N-best list extraction We incorporated an efficient extraction of n best translations using the k shortest path algorithm (Eppstein, 1998) into a state-of-the-art SMT system. The implementation is partly based on code that is
Starting point for the extraction is a word graph, generated separately by the decoder for each sen-tence. Since these word graphs are directed and acyclic, it is possible to construct a shortest path tree spanning from the sentence begin node to the end node. The efficiency of finding the k shortest paths in this tree lies in the book-keeping of edges through a binary heap that allows for an implicit representa-tion of paths. The overall performance of the algo-rithm is efficient even for large k . Thus, it is feasi-ble to use in situations where we want to generate a large number of paths, i.e. translation hypotheses in this context.
 There is another issue that has to be addressed. In phrase-based SMT, we have to deal with differ-ent phrase segmentations for each sentence. Due to the large number of phrases, it is possible that we have paths through the word graph representing the same sentence but internally having different phrase boundaries. In n -best list generation, we want to get rid of these duplicates. Due to the efficiency of the k shortest paths algorithm, we allow for generating a very large number of hypotheses (e.g. 100  X  n ) and then filter the output via a prefix tree (also called trie) until we get n distinct translations.

With this method, it is feasible to generate 100 000-best lists without much hassle. In gen-eral, the file input/output operations are more time-consuming than the actual n -best list extraction. The average generation time of n -best candidates for each of the sentences of the development list is approximately 30 seconds on a 2.2GHz Opteron machine, whereas 7.4 million hypotheses are com-puted per sentence on average. The overall extrac-tion time including filtering and writing to hard-disk takes around 100 seconds per sentence. Note that this value could be optimized drastically if checking for how many duplicates are generated on average beforehand and adjusting the initial number of hy-potheses before applying the filtering. We only use the k = 100  X  n as a proof of concept. 2.3 Rescoring models After having generated the 100 000-best lists, we have to apply additional rescoring models to all hy-potheses. We select the models that have shown to improve overall translation performance as used for recent NIST MT evaluations. In addition to the main decoder score (which is already a combination of several models and constitutes a strong baseline), these include several large language models trained on up to 2.5 billion running words, a sentence-level IBM model 1 score, m -gram posterior probabilities and an additional sentence length model. The experiments in this section are carried out on n -best lists with n going up to 100 000. We will show that, although we are capable of generating this large amount of hypotheses, the overall performance does not seem to improve significantly beyond a certain threshold. Or to put it simple: although we generate lots of hypotheses, most of them are not very useful.
As experimental background, we choose the large data track of the Chinese-to-English NIST task, since the length of the sentences and the large vo-cabulary of the task allow for large n -best lists. For smaller tasks, e.g. the IWSLT campaign, the domain is rather limited such that it does not make sense to generate lists reaching beyond several thousand hypotheses. As development data, we use the 2002 eval set, whereas for test, the 2005 eval set is chosen. The corpus statistics are shown in Table 1. Table 1: Corpus statistics for the Chinese-English NIST MT task. 3.1 Oracle-best hypotheses In the first experiment, we examined the oracle-best hypotheses in the n -best lists for several list sizes. For an efficient calculation of the true BLEU oracle (the hypothesis which has a maximum BLEU score when compared to the reference translations), we use approximations based on WER/PER-oracles, i.e. we extract the hypotheses that have the lowest edit distance (WER, word error rate) to the references. The same is applied by disregarding the word or-der (leading to PER, position-independent word er-ror rate).

As can be seen in Table 2, the improvements are steadily decreasing, i.e. with increasing number of generated hypotheses, there are less and less use-ful candidates among them. For the first 10 000 candidates, we therefore have the possibility to find hypotheses that could increase the BLEU score by at least 8.3% absolute if our models discriminated them properly. For the next 90 000 hypotheses, there is only a small potential to improve the whole sys-tem by around 1%. This means that most of the generated hypotheses are not very useful in terms of oracle-WER and likely distracting the  X  X earch X  for the needle(s) in the haystack. It has been shown in (Och et al., 2004) that true BLEU oracle scores on lists with much smaller n  X  4096 are more or less linear in log( n ) . Our results support this claim since the oracle-WER/PER is a lower bound of the real BLEU oracle. For the PER criterion, the behavior of the oracle-best hypotheses is similar. Here we can notice that after 10,000 hypotheses, the BLEU score of the oracle-PER hypotheses stays the same.
These observations already impair the alleged usefulness of a large amount of translation hypothe-ses by showing that the overall possible gain with in-creasing n gets disproportionately small if one puts it in relation to the exponential growth of the n .
N BLEU abs. imp. BLEU abs. imp. 1 36.1 36.1 10 38.8 +2.7 38.0 +1.9 100 41.3 +2.5 39.8 +1.8 1000 43.3 +2.0 41.0 +1.2 10000 44.4 +1.1 42.0 +1.0 100000 45.3 +0.9 42.0 +0.0 Table 2: Dev BLEU scores of oracle-best hypothe-ses based on minimum WER/PER. 3.2 Rescoring performance As a next step, we show the performance of tuning the model scaling factors towards best translation performance. In our experiments, we use the BLEU score as objective function of the simplex method.
Figure 1 shows the graphs for the development (on the left) and test set (on the right). The up-per graphs depict the oracle-WER BLEU scores (cf. also Table 2) for comparison. As was already stated, these are a lower bound since the real oracle-BLEU hypotheses might have even higher scores. Still, it is an indicator of what could be achieved if the models discriminated good from bad hypotheses properly.
The lower two graphs show the behavior when (a) optimizing and extracting hypotheses on a sub-set (the first n ) of the 100k-best hypotheses and (b) optimizing on a subset but extracting from the full 100k set. As can be seen, extracting from the full set does not even help for the development data on which the scaling factors were tuned. Experiments on the test list show similar results. We can also observe that the improvement declines rapidly with higher n . Note that an optimization on the full 100k list was not possible due to huge memory require-ments. The highest n that fit into the 16GB machine was 60 000. Thus, this setting was used for extrac-tion on the full 100k set.

The results so far indicate that it is not very use-ful to go beyond n = 10 000 . For the development set, the baseline of 36.1% BLEU can be improved by 1.6% absolute to 37.7% for the first 10k entries, whereas for the 60k setting, the absolute improve-ment is only increased by a marginal 0.1%. For the chosen setting, whose focus was on various list sizes for optimization and extraction, the improvements on the development lists do not carry over to the test list. From the baseline of 31.5%, we only get a mod-erate improvement of approximately 0.5% BLEU.
One possible explanation for this lies in the poor performance of the rescoring models. A short test was carried out in which we added the reference translations to the n -best list and determined the cor-responding scores of the additional models, such as the large LM and the IBM model 1. Interestingly, only less than 1/4 of the references was ranked as the best hypothesis. Thus, most reference transla-tions would never have been selected as final candi-dates. This strongly indicates that we have to come up with better models in order to make significant improvements from large n -best lists. Furthermore, it seems that the exponential growth of n -best hy-potheses for maintaining a quasilinear improvement in oracle BLEU score has a strong impact on the overall system performance. This is in contrast to a word graph, where a linear increment of its density yields disproportionately high improvements in ora-cle BLEU for lower densities (Zens and Ney, 2005). We described an efficient n -best list extraction method that is based on the k shortest paths algo-rithm. Experiments with large 100 000-best lists in-dicate that the models do not have the discriminating power to separate the good from the bad candidates. The oracle-best BLEU scores stay linear in log( n ) , whereas the reranked system performance seems to saturate at around 10k best translations given the ac-tual models. Using more hypotheses currently does not help to significantly improve translation quality.
Given the current results, one should balance the advantages of n -best lists, e.g. easily testing com-plex rescoring models, and word graphs, e.g. repre-sentation of a much larger hypotheses space. How-ever, as long as the models are not able to correctly fire on good candidates, both approaches will stay beneath their capabilities.

