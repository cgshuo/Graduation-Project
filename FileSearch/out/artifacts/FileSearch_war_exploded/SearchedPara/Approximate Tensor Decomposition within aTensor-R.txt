 In this paper, we first introduce a tensor-based relational data model and define algebraic operations on this model. We note that, while in traditional relational algebraic sys-tems the join operation tends to be the costliest opera-tion of all, in the tensor-relational framework presented here, tensor decomposition becomes the computationally costliest operation. Therefore, we consider optimization of tensor decomposition operations within a relational al-gebraic framework. This leads to a highly efficient, ef-fective, and easy-to-parallelize join-by-decomposition ap-proach and a corresponding KL-divergence based optimiza-tion strategy. Experimental results provide evidence that minimizing KL-divergence within the proposed join-by-decomposition helps approximate the conventional join-then-decompose scheme well, without the associated time and space costs.
 H.2.4 [ Database Management ]: Systems X  Query pro-cessing, Relational databases ; H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval X  Infor-mation filtering Algorithms, Experimentation, Measurement Tensor-based Relational Model, Tensor Relational Algebra, Approximate Tensor Decomposition  X 
This work is supported by an NSF Grant #1043583 - X  X iNC: NSDL Middleware for Network-and Context-aware Recommendations X 
Relational data have various representations. The rela-tional model [7] represents the data as sets of tuples, where each tuple is an instance in the attribute domain; the model also encodes the functional dependencies between the at-tributes. The vector model [12] maps each attribute to a di-mension in an n-dimensional space and represents each tuple as a point in this space (a natural representation when at-tributes are totally ordered). The tensor model , on the other hand, maps each attribute to a mode in an n-dimensional array where each possible tuple is a cell.

Thanks to their simplicity in modeling high-dimensional data and the availability of various mathematical tools (such as tensor decompositions) that support multi-aspect analysis of such data, tensors are widely used in many application do-mains including scientific data [2, 5, 8, 14] and sensor data [13]. Tensor-based data representation and tensor analy-sis are also increasingly popular in emerging fields, such as social network analysis [4, 6]. CANDECOMP/PARAFAC (CP) [5, 8] and Tucker [14] models are the two most widely used tensor decompositions generalized as high-order linear tensor factorizations. Since tensor decomposition is a costly process, to address the high computational cost of this very useful operation, many recent works focused on scalable im-plementation of tensor decompositions [13, 10].

Note that these decompositions can be interpreted proba-bilistically, if additional constraints (nonnegativity and sum-mation to 1) are imposed. In the case of the CP decompo-sition, for example, each nonzero element in the core can be thought of as a cluster and the values of entries of the factor matrices can be interpreted as the conditional prob-abilities of the entries given clusters. Recently [6] proposed a nonnegative tensor factorization model using probabilistic interpretation incorporated by users X  prior knowledge.
Spectral analysis of tensor data is often preceded by a ten-sor decomposition operation, which involves partitioning a large tensor into a smaller core tensor (i.e., spectral coef-ficients) and factor matrices (i.e., basis matrices) that rep-resent different facets of the data for multi-aspect analysis: each factor matrix describes one specific aspect of the data, whereas the core tensor describes the strength (e.g., amount of correlation) of the relationships between these distinct data dimensions. Because of these properties, tensors have emerged as useful representations for relational data.
Given a tensor based representation of the data, we can manipulate it in several ways, including tensor algebra [9] and relational algebra [7]. Tensor algebra includes operators, such as addition, multiplication of a mode with a vector (a) join-then-decompose (b) join-by-decomposition Figure 1: (a) Join-then-decompose: Rank-R CP de-or a matrix, inner product of tensors, and the norm of a tensor. Relational algebra, on the other hand, manipulates relational data using operators such as projection, selection, Cartesian-product, and set operators.
Common tensor operations (such as scalar addi-tion/multiplication and tensor addition/multiplication) are well understood. However there is little prior research on efficient implementation of complex and semantically-rich data operations, such as joins , in conjunction with tensor analysis operations, such as decompositions .

In this paper, we first introduce a tensor-based relational data model and define relational algebraic operations on this model. We then note that, while in traditional rela-tional algebraic systems the join operation tends to be the costliest operation of all, in the tensor-relational framework presented here, tensor decomposition becomes the compu-tationally costliest operation. Therefore, we next consider optimization of tensor decomposition operations within a relational algebraic framework.

Example 1.1. Consider two relations described as ten-sors 1 : a 3-mode relational tensor of (user, movie, rat-ing) and 2-mode relational tensor of (movie, genre) .
Let us assume that we have an application that requires us to first combine these two relations based on the movie attribute and then obtain the decomposition of the integrated tensor: Figure 1(a) illustrates how we would first com-bine these two relational tensors on the movie attribute into a 4-mode multi-relational tensor (user, rating, movie, genre) and then perform a tensor decomposition. In the rest of the paper, we refer to this scheme as the join-then-decompose scheme. Note that this combined tensor is higher-dimensional than both input tensors, therefore its decompo-sition is likely to be much more expensive than the decom-positions of the two original input tensors. Therefore, an alternative processing scheme would involve first decompos-ing the input tensors into their spectral components and then combining these into the decomposition of the joined tensor. Figure 1(b) illustrates this join-by-decomposition scheme.
Since we use a tensor model to describe relational data, the corresponding terms in the tensor and the relational model (e.g., a relation and a tensor, an attribute and a mode, etc) can be used interchangeably throughout the paper.
In the above example, since the costliest operation is the decomposition and the join operation increases the num-ber of modes of the tensor, we argue that the join-by-decomposition scheme will be more efficient than the con-ventional join-then-decompose scheme.

However, even if our argument is true, a number of chal-lenges remain. First of all, tensor decomposition can be seen as discovering the eigen-basis of the given tensor and a map-ping of the input data onto these eigen-bases. While this eigen-bases representation is very useful when a fixed basis for analysis is not available, it also poses challenges when integrating decompositions of multiple tensors: since each tensor has its own eigen-bases, combining different decom-positions to obtain the decomposition of the joined tensor is not straightforward. Secondly, unlike the decomposition of the joined tensor, which captures the relationships between all four modes ( user , movie , rating ,and genre ) simultane-ously in the above example, the individual decompositions of the input tensors capture the relationships of the partial subsets of these four modes. As a result, it is important to select a join-by-decomposition strategy that will best ap-proximate the conventional join-then-decompose strategy.
Since in nonnegative CP tensor decomposition, we propose to obtain rank-R decomposition of the joined tensor by combing two rank decompositions whose ranks are the factors of R such that R 1  X  R 2 = R .

This is intuitive in that if the clusters of the first data set are highly correlated with the clusters of the second data set, then when combined these will lead to less than R 1  X  R 2 (= R ) clusters that the user seeks. Relying on this observation, we seek R 1 and R 2 decompositions of the in-put tensors that lead to clusters that are independent rel-ative to the join attributes. In particular, we propose a KL-divergence based measure to quantify the difference be-tween the conditional probability distributions implied by the factor matrices and pick the R 1 and R 2 decompositions which minimize this measure based on the conditional prob-abilities of the entries of the join mode. This leads to a highly efficient, effective, and easy-to-parallelize algorithm for join-by-decomposition strategy.
Aswementionedearlier,tensorshavebeenusedforrepre-senting and manipulating relational data. For example, [4] presents a multi-way clustering framework which operates on relational data represented in the form of multi-mode ten-sor. Note that, like [4], most existing works assume that the available data has been pre-integrated into a single multi-mode tensor, which can then be manipulated using tensor operations. In practice, however, data rarely exists in a pre-integrated form and its lifecycle (from collection to analysis) involves various integration and other manipulation steps. In this section, we present a relational model for data rep-resented as tensors. Figure 2: (a) A sample relation, (b) an occurrence ten-
Let A 1 ,..., A n be a set of attributes in the schema of a relation, R, and D 1 ,..., D n be the attribute domains. Let the relation instance R be a finite multi-set of tuples, where each tuple t  X  D 1  X  ...  X  D n .
 Occurrence Tensor. We define an occurrence tensor R o corresponding to the relation instance R as an n -mode ten-sor, where each attribute A 1 ,..., A n is represented by a mode. For the i th mode, which corresponds to A i ,let D i  X  D i be the (finite) subset of the elements such that and let idx ( v ) denote the rank of v among the values in D i relative to an (arbitrary) total order, &lt; i , defined over the elements of the domain, D i . The cells of the occurrence tensor R o are such that
R o [ u 1 ,...,u n ]=1  X  X  X  t  X  X  s.t.  X  1  X  j  X  n idx ( t. A and 0 otherwise. Intuitively, each cell indicates whether the corresponding tuple exists in the multi-set corresponding to the relation or not (Figures 2(a) and (b)).
 Counting Tensor. Note that the relation instance R is a finite multi-set of tuples; i.e., there can be two tuples, t and t b ,in R such that  X  1  X  j  X  n t a . A j = t b . A j corresponding n -mode counting tensor , R c , such that Intuitively, each cell counts the number of corresponding tuples in the multi-set corresponding to the relation. Value Tensor. Let again A = { A 1 ,..., A n } be the set of attributes in the schema of the relation, R. In the relational model, a candidate key of the relation R is defined as a sub-set, K, of the attributes that uniquely determines the tuple. More formally,  X  t Given a relation R with an attribute set A = { A 1 ,..., A and a candidate key K = { A K(1) ,..., A K( m ) } X  A, let X = { A X(1) ,..., A X( n  X  m ) } denote the set of remaining at-tributes; i.e., (X  X  K=A)  X  (X  X  K=  X  ). Then, for this relation, we define the corresponding value tensor as an m -mode tensor, R v , such that R [ u 1 ,...,u m ]= v 1 ,...,v n  X  m s.t. If K = A, then the value tensor is not defined. Intuitively, in this case, each mode corresponds to an attribute in the can-didate key of the relation and each cell represents the values of the attributes determined by the corresponding instance of the candidate key. Figure 2(c) presents an example. Tensor Conversion. Note that a counting or value tensor can be converted into an occurrence tensor by adding an additional mode, which represents the count in the counting tensor or the value in the value tensor, respectively. Let be a counting or value tensor; the mapping occ ( P )givesthe corresponding occurrence tensor. Similarly, given a candi-date key K, an occurrence tensor, P , can be converted into a value tensor, val ( P , K).
Next, we introduce tensor relational algebra operations to manipulate relations represented as tensors. Let P and Q be two tensors, representing relation instances P and Q , with attribute sets, A P = { A P 1 ,..., A P n } and A Q { A 1 ,..., A Q m } , respectively. In the rest of this section, we denote the index of each cell of P as ( i 1 ,i 2 , ..., i the index of each cell of Q is denoted as ( j 1 ,j 2 , ..., j cell indexed as ( i 1 ,...,i n )of P is denoted by P [ i 1 and the cell indexed as ( j 1 ,...,j m )of Q is denoted by Q [ j 1 ,...,j m ].
 Selection (  X  ). In relational algebra, the selection operation is an operation which takes as input a single relation and a condition,  X  , and returns all the tuples in the relation satisfying the given condition: Given an occurrence or counting tensor P and a selec-tion condition,  X  , we define the condition tensor , C  X  , as a tensor of the same dimensions as P , such that for i ,i 2 , ..., i n ,if  X  ( i 1 ,i 2 , ..., i n ), then C  X  [ i C [ i 1 ,i 2 , ..., i n ] = 0, otherwise. Given the condition tensor, C , the tensor selection operation for the given occurrence or counting tensor P is defined as where  X  is the cell-wise product of P and C  X  and comp () is the compaction operator which eliminates all-0 slices from the resulting tensor to ensure that the elements along all modes correspond to attribute values that occur in at least one tuple satisfying the selection condition.

If P is a value tensor with candidate key K, on the other hand, the selection operation is defined as Projection (  X  ). In relational algebra, the projection op-eration takes as input a single relation R with an at-tribute set, A P = { A P 1 ,..., A P n } , and an attribute set A= { A a 1 ,..., A a k } X  A P and maps the relation into a new relation  X  A ( R ) with the attribute set A such that The corresponding tensor projection operator eliminates all the modes that do not belong to the target attribute set A; i.e., given P with an attribute set, A P ,andtheprojection attribute set A  X  A P , the result is a new tensor  X  A ( P the attribute set A. More specifically, if P is an occurrence tensor, then  X  A ( P )[ i a 1 , ..., i a k ]=1  X  X  X  P [ ...,i a 1 ,...,i On the other hand, if P is a counting tensor, then  X 
A ( P )[ i a 1 , ..., i a k ]= As before, if P is a value tensor, we can define the projection by first converting it into an occurrence tensor: Cartesian product (  X  ). Given two relations P and Q , with attribute sets, A P = { A P 1 ,..., A P n } A
Q = { A Q 1 ,..., A Q m } ,the cartesian product operator re-turns a new relation, P X Q ,withanattributeset { A 1 ,..., A P n , A Q 1 ,..., A Q m } : If we consider two occurrence or counting tensors, P and Q , as inputs, we can define the tensor relational algebraic cartesian product simply in terms of the outer product of the two input tensors: As before, for value tensors, we can define the cartesian product by first converting the tensors into occurrence ten-sors: Join ( ). In relational algebra, given two relations P and Q , and a condition  X  ,the join operation is defined as a carte-sian product of the input relations followed by the selection operation. Therefore, given two relational tensors P and Q and a condition  X  , we can define their join as
Given two relations P and Q , with attribute sets, A P = { A 1 ,..., A P n } and A Q = { A Q 1 ,..., A Q m } , and a set of at-tributes A  X  A P and A  X  A Q ,the equi-join operation, = , A , is defined as the join operation, with the condition that matching attributes in the two relations will have the same values, followed by a projection operation that elimi-nates one instance of A from the resulting relation. There-fore, the equi-join operation can also be defined in terms of the basic tensor relational operators described earlier.
As in the case of relational algebra, a query (or data ma-nipulation) plan can be visualized as a tree, where the leaves of the tree are the input tensors and each node of the tree is a tensor relational operation, selecting, projecting, or joining its inputs.

However the tensor-relational operations are part of a larger framework that involves other tensor operations, such as tensor decompositions. In fact, in most cases, the tensor relational operations precede a tensor decomposition opera-tion to manipulate the data into a form ready for the context of the analysis.

This situation is especially aggravated when the decom-position operation is preceded by a join operation which increases the number of modes of the tensor to be decom-posed. Since tensor decomposition cost is exponential in the number of modes of the input tensor, data integration through joins tends to increase the cost of the whole plan significantly and even renders the whole query infeasible if sufficient resources and time are not available.
In order to obtain approximate decompositions of joined tensors, join-by-decomposition scheme works as follows: as illustrated in Figure 1(b), to construct a rank-R decom-position of the joined tensor, we consider two integers, R and R 2 , such that R 1  X  R 2 = R and we find rank-R 1 and rank-R 2 decompositions of the two input tensors. We then combine these two decompositions along the given factor matrix which corresponds to the join attribute in the equi-join operation (the process is trivially extended to the case where there are multiple equi-join attributes in the query). Intuitively, we treat each diagonal element in the core tensor as a cluster and the factor matrices as the conditional prob-abilities of the attribute values along the modes belonging to the given clusters.

Let us consider two 3-mode relational tensors, P and Q , with u  X  l  X  m and u  X  b  X  s dimensions, respectively, and an equi-join operation on the first mode of these tensors (note that for simplicity, we assume that both modes have u slices along the join attribute, representing the common values for the two relations along the equi-join attribute. The rank-R and rank-R q CP decompositions of P and Q are as follows: P Note that (if decompositions are nonnegative and tensors are properly normalized) the equation for P can be interpreted probabilistically as Here C p  X  are the clusters of P ; analogously, the equation for Q can also be interpreted probabilistically.

Let us denote the equi-join tensor P = , U Q as X . Simi-larly to the input tensors P and Q , we can also probabilisti-cally interpret the rank-R decomposition of X : where C x  X  are the clusters of the joined tensor. Note that if the R p and R q clusters of the input tensors are independent from each other and R p  X  R q = R ,wecanrewritethisin terms of the clusters and membership probabilities of the input tensors as
This gives us a way to reconstruct the decomposition of the join tensor directly from the decompositions of the input tensors, which are much cheaper to obtain. However, this reconstruction makes sense only if the clusters of the input tensors are independent from each other:
Otherwise, there will be a nonzero difference between X and  X  X . Next we describe how to minimize the approxima-tion error, X  X   X  X . The natural approach to minimize the approximation error involves searching for input clus-ters (i.e., decompositions of the input tensors) that are the most independent relative to the join attribute.

This presents two challenges: first of all, we need to enu-merate pairs of ranks that multiply to R andthenobtainthe corresponding decompositions of the input tensors P and Q As we will see in the experiment section (Section 4), while this involves enumeration of multiple (low-modal) decompo-sitions, the overall cost of the process is often much less than the cost of decomposing the (high-modal) joined tensor. Sec-ondly, given a pair of rank-R p and rank-R q decompositions of
P and Q , this requires a measure to quantify the inde-pendence of the clusters relative to the join attribute. The problem is that the term P ( U | C p r p  X  C q r q )requirescounting joins falling within a cluster given by the decomposition of the joined tensor; but this is not known.

Intuitively, independence of the two input clusterings rel-ative to the join attribute implies that given a join element U j that connects clusters C p a and C q b of the inputs, another join element U l is neither more likely or less likely to connect these two clusters. More specifically, given the join elements in C p a , we would expect to see the distribution of the cor-responding C q  X  to be uniformly distributed. Similarly, given the join elements in C q b , the distribution of the corresponding C  X  should be uniform. Therefore, we can quantify the inde-pendence of the input clusters relative to the join attribute by using the KL-divergence of the conditional probability distributions: Therefore, given the set R of all possible combinations of rank decompositions that lead to a rank-R decomposition of the joined tensor, we find the pair which leads to the smallest  X  value.
In this section, we present experimental results as-sessing the efficiency and effectiveness of the proposed join-by-decomposition scheme relative to the join-then-decompose approach.
We use two data sets -movie rating data and Enron email data sets. The movie rating data set is obtained from [1] and consists of 100,000 movie ratings of 983 users for the 1682 movies. From this data set, we created one pair of re-lations, (user, movie, rating) and (movie, genre) ,and their joined tensor (user, rating, movie, genre) ,and the other pair, (movie, user, rating) and (user, occu-pation) , and their joined tensor (movie, rating, user, occupation) . We experimented with 20 subsets of these relations of different sizes. We obtained the Enron email data set from [11]. This subset consists of email exchanges among 184 email addresses during 44 months, which be-comes one relation (month, from_email, to_email) .We created a second relation consisting of the employee informa-tion associated with the email addresses (to_email, posi-tion) . These two relations join into (month, from_email, to_email, position) ofthesize44  X  184  X  184  X  8. We experimented with rank-9 and rank-12 decompositions.
We use MATLAB Tensor Toolbox [3] to manipulate a rela-tional tensor as a sparse tensor. For nonnegative CP decom-position, we employ the N-way Toolbox for MATLAB [2]. Our experiments ran on Pentium(R) Dual-Core CPU E5800 @3.20GHz with 4GB of RAM.

Our evaluation criteria include running time ratio which is the ratio of the running times of join-by-decomposition and join-then-decompose ,and fit ratio defined as where X is the joined tensor,  X  X is the tensor obtained by re-composing the join-then-decompose tensor, and  X   X  X is the tensor obtained by re-composing the join-by-decomposition tensor. Efficiency. Figure 3 shows that the proposed join-by-decomposition scheme becomes much cheaper relative to the traditional join-then-decompose scheme as the tensor size and the number of nonzero entries (i.e., relation size) in-crease. Thisisbecause,whilethe join-by-decomposition scheme needs to perform multiple nonnegative CP decom-positions to select the best, each one is done much faster than the decomposition of the joined tensor.

Here it is important to note that one key disadvantage of the traditional join-then-decompose scheme is the amount of resources it requires to complete its operation. In our experiments, the join-then-decompose scheme was infeasi-ble (with our experimental setup) for the joined tensors with the number of nonzero entries which was greater than 60,000 Figure 3: Relative running times of join-by-since the main memory required for nonnegative CP decom-position for these relational tensors is beyond the capability of our hardware/software setting. The proposed join-by-decomposition scheme, however, did not have this short-coming since it decomposes much smaller input tensors. Effectiveness. Now we examine the effect of the factor of the relation size (i.e., number of nonzero entries) to the fit ratio of the joined tensor (see Figure 4). The fit ratio starts somewhat low between 0.4 and 0.7 when the num-ber of nonzero entries are less than 10,000 however the fit ratio increases as the number of nonzero entries increases. More notably, the fit ratio of the relation (movie, rating, user, occupation) shows a steep increase and reaches to a value higher than 0.9 when the number of nonzero en-tries is about 15,000. On the other hand, the fit ratio of the relation (user, rating, movie, genre) reaches to 0.8 when the number of nonzero entries is around 60,000. In the two movie relations we have experimented, the clusters of the pair (movie, rating, user) and (movie, genre) were less independent than the clusters of the pair (movie, user, rating) and (user,occupation) because, generally, a movie rating of a user can be more affected by the movie genre than the user occupation; i.e., the latter one is likely to show higher inde-pendence among the clusters of the input relations. Indeed, the average KL-divergence measure for the selected combinations for the (user, rating, movie, genre) relation was 601.5; for the (movie, rating, user, occupation) relation, on the other hand, the average KL-divergence for the selected combinations was only 5.55.
We also observed that the average correlation, which indi-cates how the fit ratio changes with the KL-divergence mea-sure, was as expected, strongly negative: -0.7 . This shows that the (inverse of the) KL-based measure captures the like-lihood of a given combination to produce high degrees of fit and the proposed KL-measure is effective in optimizing the quality of join-by-decomposition plans. Figure 4: #nonzero entries of joined relations vs. fit
Lifecycle of most data includes various operations, such as capture, integration, projection, decomposition, and data analysis. In this paper, we first presented a tensor-based relational data model and introduced relational algebraic operations for this model. Since in the proposed tensor-relational framework, tensor decomposition is often costlier than other operations on the data, including joins, we pro-posed a highly efficient, effective, and easily parallelizable join-by-decomposition strategy for approximately evalu-ating decompositions within a relational plan, which is op-timized based on KL-divergence. Experimental results con-firmed that the efficiency and effectiveness of the proposed join-by-decomposition scheme compared to the join-then-decompose approach.
