 The primary target of recommender systems is to propose one or several items to users in which they might be interested. The books , articles or music provided by the rec-ommender system s are items [1][2] . The performance of a recommendation is based on the response of the user. The p erformance is widely measure d by Click -Though Rate (CTR) [3] , which is the average payoff when an item is recommended. Then the goal of recommendations is to maximize the CTR over all users.

Although recommender systems have been deployed g reatly durin g the past decade, there remain several challenging problems. The f irst important challenge is known as cold start . The c old start problem appears when the system has no priori information about the items and users in practice [4][5] . Secondly, recommender systems need to focus on items that raise users X  interest and explore new items to improv e users X  satis-faction at the same time . That creates an exploration -exploitation dilemma , which is the core point of Multi -Armed Bandit (MAB) problems [6] . The last important challenge is scalability, which is faced by recommender systems with a large number of items and contexts [7] .
Contextual bandit appr oaches are already studied in many field s of recommender system s [8] . When the number of contexts is small and finite, we can use recommenders as many as contexts, one for each context, to improve the accuracy of recommenda-tions. However, in large -scale recommender systems, there are large or infinite number of contexts . T he increasing recommenders fail t o ensure effective and efficient recom-mendations. There are several context -aware bandit approaches to solve this problem [7][17] . However, these approaches highly depend on priori information. Besides, con-text -aware bandit approaches always have higher im plementation complexity than con-text -free bandit approach es .

In fact, there exist some recommender systems that the priori information is un-known. Under this situation, recommendation has to be inferred from user feedbacks. distinction, a simple context -aware case is also discussed in this paper . Each item cor-responds to an action (refer red to as the arm in a bandit framework) in our work. 
We propose both context -free and context -aware bandit approaches which try to ad-dress all of the above mentioned challenges in large -scale recommender system s . For two context -free bandit approach es , we do not depend on any priori information about users or items. The recommendation is made only base d on the payoff estimations . And our context -free approaches are proved to converge to optimal item recommendations mation and payoff estimations are considered. The priori information about users and items is referred as contexts.
 O ur approaches are able to achieve the relatively high CTRs. Moreover, a synchronous context -free bandit approach is able to achieve competitive performance with several context -aware bandit approach es , without suffering very high complexity.
The key contri butions of th is paper are: (i) Without depend ing on priori information, our synchronous context -free approach (ii) We reduce the implementation complexity by asynchronous updating, while still (iii) Our context -free bandit approach can obtain similar CTR s with some context -The rest of the paper is organi zed as follows. Section 2 presents some related work s . In section 3, we introduce our approaches, discuss the influence of key parameter and prove the convergence. Section 4 presents experiment al evaluation . Conclusion is made in s ection 5 . Tw o main categories of recommendation algorithms are filtering -based and reinforce-ment learning methods [4] . In this paper, we focus on reinforcement learning methods.
Reinforcement learning methods, such as MAB and Markov Decision Processes (MDPs) [11] , are widely used to recommender system s . MDP -based approaches model the last k choice s of a user as the state and the available items as the action set to max-imize the long -run payoff. But the state set will grow too fast when the number of items increas es. Th ereby , MDP -based approaches suffer very slow convergence rates in large -scale recommender systems [1 8 ] . 
MAB -based approaches make recommendation by balanci ng between exploration and exploitation , such as  X  -greedy [9], softmax [12] , EXP3 [13] and UCB1 [6] . E xplo-ration means learning new items X  payoff for a particular user by recommending new items. E xploitation means recommending the best items based on the payoffs observed so far. Among these context -free approaches,  X  -greedy is the simplest approach , but the able to be applied to various fields successfully . Softmax makes recommendations ac-cording to a probability distribution based on user feedbacks. As a complicated variant probability. UCB1 always recommends the item with the highes t upper confidence in-dex, which relies on the entire sequence of payoffs obtained so far. However, UCB1 needs to sweep all items during the initial period, which may be inappropriate for large -scale recommender systems.

Contexts are considered in recommend er systems, aiming at improving the effec-tiveness of recommendations further. Generally, contexts represent the situations of the user when a recommendation is made [14] [15] , such as time, gender, identity and searc h query . T he LinUCB algorithm is proposed to solve news article recommendation prob-lems [ 19 ]. The Naive III and Linear Bayes approach es define a user -group by a set of features that individual users may have in common [1 7 ]. A context -awar e MAB based clustering approach constructs an item -cluster tree to cluster items adaptively for rec-ommender systems [7] . In [16], t he context space can be modeled as a bounded metric space , however, th is approach can cause the scalability issue in practice since it does not take in to account the large item space . I n this section, we present three recommendation approaches. The first approach is a context -free bandit approach, which is based on the Chosen Number of Action with Minimal Estimat ion , namely CNAME in short. Then we introduce an asynchronou s CNAM E approach , namely Asy -CNAME . The third approach is a contex t ual CNAME approach , namely Con -CNAME . 3.1 CNAME Approach The key idea of CNAME is how to use user feedback s sufficiently . S pecifically , both of estimated payoff and quantity that an action has been chosen are utilize d to update exploration probability and then choose an item. T he exploration probability p is ad-just ed according to the chosen number of action with minimal estimated payoff, defined by m . During exploitation, the CNAME approach chooses the action with maximal estimation. During exploration, an action is chosen randomly. The CNAME algorit hm is presented in Algorithm 1. 
Algorithm 1 :CNAM E 10: R eceive a reward
The CNAME algorithm starts by setting the parameter w (Line 1) . The p arameter w affect s the speed at which the exploration probability is reduced . Af ter initializing the estimation  X   X  Qk and the chosen number  X   X  Nk of each action k , i t iteratively chooses an action to play (referred to recommend a n item in recommender system s ) based on the exploration probability (Line 5 -9), and receives a reward ( L ine 10). Finally, the CNAME algorithm updates the number of chosen and estimated expected payoff at time step t (Line 11 -12 ).

The specific analysis of the CNAME algorithm is as follows: (iii) During continuous exploration , the CNAM E algorithm explores randomly to en-3.2 Asynchronous CNAM E Approac h Aiming at scaling to accommodate a large number of actions easily, the CNAME ap-proach should update the exploration probab ility in an asynchronous manner. The Asy -CNAME algorithm is briefly presented in Algorithm 2. Different from the CNAME algorithm, the Asy -CNAME algorithm clusters a sequence of N samples of the action into a single batch (Line 5 -12 ) and update s the exploration probability after each batch CNAME approach can reduce the implementation complexity. We will discuss the per-formance of the CNAME approach and Asy -CNAME approach on random ly generated dataset in s ection 4.

Algorithm 2 : Asy -CNAME 1 0 : R eceive a reward 1 4 : 1 5 : Using the learning rate  X  , update p as  X   X  1 p p p'  X  X  X   X   X   X  3.3 Convergence of Context -Free Approaches Based on the above description of context -free approach es , we prove that proposed context -free approaches are able to converge to the optimum in the long run. 
A K -armed bandit proble m is defined by random variables X i , n f or 1 iK  X  X  X  and of actions and n refers to the number of trials . Successive trials of action i yield rewards X i ,1 , X i ,2 ...which are independent and identically distributed according to an unknown law with unknown expectation  X  X   X  as an optimal action. In what follows, we write *
Ni , where i is the optimal action. Here
The CNAME and Asy -CNAME are algorithm s that choose the next action based on the sequence of past trials and obtained payoffs. Let () i has been chosen by the CNAME and Asy -CNAME during the first n trials . Of course, we always have Let
T he probability that action i is chosen at trial n is and n trials . Then we have By using the Chernoff -Hoedffing bound, we get I n the last line we dropped the conditioning because each action is chosen at random independently of the previous choices of the algorithm. Since by the Bernstein X  X  inequality we get Finally it remains to lower bound Thus, using ( 1 ) -( 4 ) and the above lower bound on that the CNAME and Asy -CNAME algorithm s choose a suboptimal action i is at most For n  X  X  and K large enough the above bound is 0. It mea ns the CNAME and Asy -CNAME algorithm s are able to converge to the optimal acti on in large -scale MAB problems. This concludes the proof. 3.4 Con text -Aware CNAME Approac h Although this paper mainly focus es on context -free bandit approach es , we still propose a relatively simple Con -CNAME approach , to contrast with the CNAME approach . Because it is remarkable that context -free approaches are competitive with context -aware approaches sometimes.
 Particularly, we compute the priori probability of each con text for visitor features. During exploitation, the Con -CNAME approach combines the priori probabilities with payoffs as new variable s . Then we choose the action which owns the larg est variable . During exploration, an item is recommended randomly to surpri se users in some degree. The contextualization can address the cold start issue . We will discuss the Con -CNAME approach further through the experiment al evaluation in s ection 4. In this section, we discuss the influence of key parameter w and different update man-ners in our context -free bandit approaches . We provide the reference range of parameter w through simulation on a randomly generated dataset. Then we compare the perfor-mance of our approaches with other context -free bandit approaches and context -aware bandit approaches on Yahoo! F ront P age T oday M odule u ser c lick l og d ataset . 4.1 Randomly Generated Dataset The goal of this simulation is to minimize the regret [10], which is the loss between the optimal expected total payo ff and the expected total payoff gained through our ap-proaches. If all information about actions and rewards is known, the optimal action (referred to optimal recommendation in recommender systems) can be chosen easily. The key issue is the information we know is incomplete, that is why we have to learn the optimal action by balancing between exploration and exploitation.
 Experimental Evaluation about Key Parameter . T he subject of this part is a set of tasks with 100 randomly generated K -arm ed bandit proble ms, where the number of actions =1000 K , learning rate =0.8  X  , batch =10 N and terminal time step 2000 T  X  . The actual value of each task and a variance of 1. The reward for each action i is subject to a Gaussian distribution with a mean of order to overcom e the uncertainty of the tasks and environment .

Under the above experimental conditions, we tak e different value s of the parameter w to record the average regret s of 100 random tasks . The average regrets are obtained by CNAME and Asy -CNAME respectively. The performance of algorithms increases with the decreasing of average regret . When parameter w i s in the interval [0 .01 , 0. 1] , as the reference range of w .
 Experimental Evaluation about D ifferent U pdate M anners . The purpose of this part is t o compare influence of different update manners in large -scale MAB problems . We compare the CNAME and Asy -CNAME on a set of 100 randomly generated K -armed bandit problems with 100 1000, 5000, 10000 K  X   X  respectively, where =0.8  X  , 0.1 w  X  , =10 N and 1000 T  X  . Similar ly , the actual values and rewards are subject to a Gaussian distribution . The experiment results are presented in Fig. 2.

Fig . 2 shows that the average regrets obtained by Asy -CNAME are generally lower than CNAME with increasing values of K . Moreover, the Asy -CNAME costs less time since it updates in an asynchronous manner. Hence, the Asy -CNAME obtains even better performance than CNAME with h igh efficiency in large -scale MAB problems . 4.2 Yahoo! Front Page Today Module User Click Log Dataset This dataset contains a fraction of user click log for news articles displayed in the Fea-tured Tab of the Today Module on Yahoo! Front Page 1 . This data set include s 15 days associated with a binary feature vector of 136 dimension s (including a constant feature with ID 1) that contains information about the user like age, gender, be havior targeting features, etc.

In this part, we make recommendations for large -scale recommender systems, through our MAB -based approaches. We compare t he performance of our two ap-proaches with other context -free and context -aware bandit approaches. The perfor-mances of the algorithms are evaluated through CTR. The comparison results are show n in Table 1 and Table 2. In Table 1, the best algori thm s of cont ext -free and con-text -aware are highlighted respectively in boldface.

As shown in Table 1, because of contexts, context -aware bandit approaches obtain higher CTRs than context -free bandit approaches in general, at the expense of relatively longer calculation time. 
We can see that the CNAME significantly outperforms other context -free bandit ap-proaches, even context -aware bandit approaches, such as Naive III approach . The ex-periment al results show that the CNAME approach achieves a 6 % -109 % performance gain over other context -free approaches in terms of CTR . The CTR s of CNAME are even higher than context -aware Naive III approach and Linear Bayes approach. 
The CNAME approach just need s a few seconds to obtain CTR over the first 200 , 000 rows on Yahoo! d ataset , w hile context -aware bandit approaches consume about one or two minutes to get CTR over the same rows. It is a significant improvement that our context -free approach consume s shorter time than some context -aware approaches with better performance . Hence, the CNAME approach is cost -eff ective for large -scale rec-ommender systems.

On the other hand, t he context -aware algorithms seem to make more accurate esti-mations of the expected CTRs of items by employing the contextual information. Table 2 shows that the Con -CNAME approach obtains 4% per formance gain over the CNAME approach in terms of CTR . Moreover, our Con -CNAME approach outper-forms other context -aware bandit approaches in Table 2 . In this paper, we study recommender system s based on large -scale MAB problem s . Two types of large -scale bandit approaches are proposed to address the cold start and scalability issues, which are caused by the emerging new items or users and the large number of items and contexts.

The CNAME approach makes recommendation s without dependen ce on priori in-formation. By cluster ing samples of the action into a single batch, the Asy -CNAME approach reduce s the implementation complexity , while still ensur ing good recommen-dations. That addresses the scalability of large -scale reco mmender systems in some de-gree. Besides, for the CNAME and Asy -CNAME approaches, the cold start problem is addressed by continuous exploration. T o improve recommendation accuracy further, the Con -CNAME approach is proposed by incorporating contexts to the CNAME ap-proach.

Theoretical result shows that the CNAME and Asy -CNAME approach es are able to converge to the optimal recommendation s in the long run. Our simulation about the CNAME and Asy -CNAME approach es , on a number of MAB problems with an in-creasing nu mber of actions , shows that the Asy -CNAME can reduce calculation with better performance. The reference range of key parameter is given through simulation. Besides, t he performance of our approaches and other recommendation approaches is compared on Yahoo! F ront P age T oday M odule u ser c lick l og d ataset. Experiment al results show that our approaches outperform other algor ithms in terms of CTR . Com-pared with some context -aware bandit approaches, the CNAME approach is cost -eff ec-tive . The CNAME approach is able to achieve similar performance with the Con -CNAME approach, while not suffering very high complexity.

Although the Con -CNAME approach achieves significant result, there is a possible improvement that can be made by using contexts mor e rationally and future work may focus on it. Besides, for the Asy -CNAME algorithm, the relationship between some parameters and the environment need to be further studied .
