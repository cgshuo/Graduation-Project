 Michael Gro X hans 1 grosshan@cs.uni-potsdam.de Christoph Sawade 1 sawade@cs.uni-potsdam.de Michael Br  X uckner 2 michael@soundcloud.com Tobias Scheffer 1 scheffer@cs.uni-potsdam.de SoundCloud Ltd., Rosenthalerstra X e 13, 10119 Berlin, Germany Proof of Lemma 1 The partial derivative of Equation 4 for an instance i is given by  X   X   X  x i Due to the convexity of  X   X  d we can compute the min-imum by equating it to zero and solving for  X  x i using the Sherman-Morrison formula. This results in The claim follows, since the optimal transformations are independent from each other.
 Proof of Lemma 2 The partial derivate of Equation 3 is given by = = 2 Setting this gradient equal to zero and solving it for w yields w  X  [  X  ] = I m + definite for any adversary X  X  strategy  X  ( c d ). Thus, all eigenvalues  X  i  X  0 are non-negative. The cor-responding eigenvectors are additionally eigenvectors 1 +  X  i &gt; 0. Thus, the inverse matrix exists indepen-dently of the choice of  X  . This proves the claim. Proof of Lemma 3 We define the action spaces of G 0 as the closure of the convex hull of optimal responses
W 0 = cl ( { w  X  X  | w =  X  w  X  [  X  1 ] + (1  X   X  ) w  X  [  X  2 where cl ( M ) denotes the closure of set M . Since a Bayesian equilibrium is a pair of optimal responses (see Definition 1), each equilibrium in the original game G is a member of the restricted space W 0  X   X  0 . Since both games have identical loss functions and costs distribu-tions each equilibrium point in G 0 is an equilibrium point G and vice versa.
 It remains to be shown that the spaces  X  0 and W 0 of optimal responses are bounded. We start by show-ing that any optimal response of the data generator is bounded. Using the triangle inequality, the spectral norm of an optimal response to a model parameter w (see Lemma 1) can be upper-bounded by k  X   X  [ w ]( c d ) k 2  X  The third summand of the right-hand side in Inequal-ity 14 can be upper bounded as follows. Equation 15 follows by the sub-multiplicativity of the spectral norm. The spectral norm of a symmetric positive definite matrix is given by its largest eigen-value, that is, the maximal diagonal entry of the pos-itive diagonal matrix (see Equation 16). Equation 17 holds, since c d &gt; 0 .
 Analogously, the second summand (see Inequality 14) can be bounded using the sub-multiplicativity of the spectral norm (Inequality 18) and evaluating the largest eigenvalue of the diagonal matrix, which we split up into two factors (see Equation 19). In In-equality 20 we can bound each dominator by one of the positive summands. Finally, using Bound 17 and 21 the function space  X  is bounded by since R max i n + P n i R c d,i d q ( c d ) &lt;  X  exist.
 We now show that if the data generator X  X  action space is convex and compact the space of optimal responses W 0 of the learner is compact and convex as well. Let  X  ( c d ) be a data generator X  X  strategy given costs c d . Then, the learner X  X  optimal response given by Lemma 2 can be bounded using the sub-multiplicativity of the l 2 -norm k w  X  [  X  ] k 2 For any arbitrary symmetric, positive definite matrix the squared spectral norm equals its largest eigenvalue. definite, all eigenvalues  X  i  X  0 are non-negative. The corresponding eigenvectors are additionally eigenvec-tors of I m + R  X  ( c d ) T diag ( c l )  X  ( c d )d q ( c values 1 +  X  i  X  1. Thus, the inverse matrix exists and its norm can be upper bounded by one (see In-equality 22). Then, the claim follows since the data generator X  X  action space is bounded.
 Proof of Theorem 3 Let a = ( a 1 ,...,a n ) T be an arbitrary vector. Then, following Taylor X  X  Theorem the data generator X  X  opti-mal response can be written as with Cauchy remainder
R t ; a ( c d ) = ( t + 1)diag ( c d  X   X  ) for some  X  = (  X  1 ,..., X  n ) T with  X  i  X  [ c d,i ,a i ] or  X  [ a ,c d,i ], respectively. It remains to show that a can be chosen, such that R t ; a ( c d ) converges to the null ma-trix, or equivalently, that lim The limit of the Cauchy remainder can be expressed as = lim  X  lim = lim In Equation 23 we dismiss the two terms (  X  1) t +1 and ( Xw  X  z ) w T from C t +1 (  X  ) (see Equation 10). This leads to a diagonal matrix whose spectral norm is given by the maximal absolute diagonal entry of the matrix (see Equation 24). If k w k = 0 the claim holds. So let k w k &gt; 0. Then, Equation 24 can be factorized as A sufficient condition for the geometric sequence and thus for Equation 25 to tend to zero is that holds for all i and a fixed k . In the following we show by case differentiation according to c d,i that Condi-tion 26 is fulfilled for a i = 1 2 sup {k c d k  X  | q ( c where k c k  X  = max i ( | c i | ) is the maximum norm. Let a i  X  c d,i . Since  X  i  X  [ a i ,c d,i ] it follows that a  X   X  c d,i  X  2 a i and, thus, the quotient can be upper-bounded by Since k w k 2 &gt; 0, it holds that k &lt; 1. Now assume bounded from below by zero, it follows that 0  X  c d,i  X   X   X  a i . Hence, the quotient can be upper-bounded by This proves the claim.
 In Section 6 we studied the behavior of the Bayesian regression model in the context of email spam fil-tering. We compared the Bayesian game regression model (denoted Bayes ) to the Nash game regression model (denoted Nash ), the robust ridge regression (de-noted Minimax ), and a regular ridge regression (de-noted Ridge ).
 Depiction of Theorem 2 In Theorem 2 we derived sufficient conditions for unique equilibrium points. The uniqueness depends on both players X  costs c d and c l . Figure 6 (left) de-picts the optimal responses of the learner (horizontal axis) and the data generator (vertical axis) for dif-ferent costs c l = c d in a one-dimensional regression game where X = 1 ,y = 1 , and z = 1 without uncertainty. Their intersections constitute the equi-librium of the corresponding game. If the costs be-come too large, such that Condition 5 is violated for any ( w, X  )  X  [0 , 1]  X  [0 , 1], the game G is no longer locally convex (indicated by the dashed black line). Playing against a Bayesian adversary In a first experiment, we evaluated how the methods perform against an adversary that chooses a strategy according to a Bayesian equilibrium. We choose q ( c d,i as a gamma distribution and varied its mean  X  and the variance  X  2 . We set the Nash model X  X  conjecture for all values of c d,i to the mean  X  ; this is optimal if the data generator X  X  costs are drawn from a single-point distribution q ( c d,i ) =  X  ( c d,i =  X  ). Figure 3 shows the root mean squared error (RMSE) for Bayes , Nash , and Ridge as a function of  X  and  X  2 (left). Fig-ure 3 (right) shows a sectional view along  X  (top) and along  X  2 (bottom). We observe that all methods co-incide if the data generator X  X  costs vanish. The ad-vantage of Bayes over Nash grows with the variance of q .
 Playing against actual adversaries In a second experiment, we evaluate all methods over time into the future. Here, the models play against actual spammers. Additionally, in order to artificially create a mismatch to our modeling assumptions, we also evaluate the models on test data from the past. The training sample of 200 instances are drawn from month k . To evaluate into the future, the regulariza-tion parameters of all learners (for Bayes , we use a single cost parameter for all instances) are tuned on 1,000 instances from month k + 1; for evaluation into the past, tuning data are drawn from month k  X  1. Test data are then drawn from months k + 2 to k + 1 and from months k  X  2 to k  X  6, respectively. This process is repeated and RMSE measurements are av-eraged over ten resampling iterations of the training set and, in an outer loop, over four training months k (March to June 2008). The data generator X  X  costs pa-rameters are set to  X  = 0 . 01 and  X  2 = 0 . 01 for Bayes and to  X  = 0 . 01 for Nash .
 Figure 4 (left) shows the RMSE over time for fixed mean  X  = 0 . 01 and variance  X  2 = 0 . 01. Figure 4 (cen-ter) depicts the RMSE for a fixed point in the past over a range of different values for  X  . Figure 4 (right) shows the training times of the Bayesian equilibrium model and reference models for varying number of attributes. Additionally we study the shift in spam mails in re-ality and compare it with the computed equilibrium point. For depiction we choose the two most discrim-inant principal components with respect to spam and non-spam. We train separate Nash models on March 2007 (see section 6), April 2007, May 2007 and June 2007. Again we use 200 instances and the data gen-erator X  X  costs are set to  X  = 0 . 01. The learner X  X  costs are tuned on the subsequent month. Given the Nash model we are able to extract the optimal transformed data from training month according to Lemma 1. Fig-ure 5 depicts a training samples (blue), test samples from the six subsequent months (green/yellow) and the computed equilibrium data (red) for three differ-ent training months (April -June 2007); a fourth is shown in Figure 1. If changes of spam mails are con-tinuous they can be well predicted (see e.g. upper right corner). If instead the changes are more volatile (see e.g. lower right corner) the Nash model is not able to predict the appearance of new spam mails.
 Approximation of adversary X  X  responses Finally, we study the impact of the degree t of the Tay-lor approximation on the accuracy and execution time of Bayes . Figure 6 (center) shows the RMSE evaluated over time for t = 1 , 2 , 3 (see Equation 11). Figure 6 (right, top) shows the execution time depending on the number of training emails for a fixed number of at-tributes ( m = 10). Figure 6 (right, bottom) shows the execution time depending on the number of attributes for a fixed number of training mails ( n = 200).
 Michael Gro X hans 1 grosshan@cs.uni-potsdam.de Christoph Sawade 1 sawade@cs.uni-potsdam.de Michael Br  X uckner 2 michael@soundcloud.com Tobias Scheffer 1 scheffer@cs.uni-potsdam.de SoundCloud Ltd., Rosenthalerstra X e 13, 10119 Berlin, Germany Standard regression algorithms are based on the iid as-sumption that data processed at training and applica-tion time are governed by identical distributions. In a variety of applications, the input distribution at appli-cation time may be influenced by an adversary whose interests are in conflict with those of the learner. For instance, in insurance risk assessment, defrauders con-tinuously tweak specific attributes of their insurance applications to make the risk appear lower. For such applications, the iid assumption amounts to the naive assumption that the adversary is entirely passive. When an adversary can exercise some control over the distribution of the data at application time, the out-come for the learner, as well as the outcome for the adversary, depends on both the predictive model that the learner chooses and the changes that the adversary imposes on the input distribution. Such interleaved optimization problems in which learner and adversary do not exchange information about their intended ac-tions constitute non-cooperative games .
 In the zero-sum case, the goals of learner and adversary are directly antagonistic. This amounts to the assump-tion that the adversary intends to inflict the greatest possible harm on the learner. In this case, the learner is best off by choosing a minimax strategy which is the minimizing argument over the parameter space of the learner, of the maximum over the action space of the adversary, of the cost function. For classification, minimax solutions were derived under several assump-tions. Globerson &amp; Roweis (2006) study the case of features that are deleted at test time; El Ghaoui et al. (2003) study features that are changed within an in-terval. The minimax probability machine (Lanckriet et al., 2002) minimizes the maximal probability of mis-classifying new instances for a given mean and covari-ance matrix of each class. For regression problems, Sayed &amp; Chen (2002) derive a minimax model that handle bounded uncertainty in the feature matrix and labels. The SVM with invariances (Teo et al., 2007) solves a convex upper bound of a minimax optimiza-tion problem for arbitrary feature transformations. If both players have conflicting but not perfectly an-tagonistic goals, then the minimax strategy is overly pessimistic and does not necessarily lead to an optimal outcome. A Nash equilibrium point of a game is a pair of strategies that has the property that unilaterally deviating from it increases the costs for either player. If a game has a unique equilibrium and one assumes that the opponent will also act according to that Nash equilibrium, then acting according to this equilibrium point is the optimal strategy. Identifying an equilib-rium point requires complete information about the opponent X  X  cost function. For classification games with complete information, Br  X uckner et al. (2012) show that a unique Nash equilibrium point exists if the players X  cost functions meet certain conditions.
 In security applications, however, the assumption of complete information may still be too strong because the learner may not be fully informed about, for in-stance, the illicit profit that the adversary can make by passing a computer virus unnoticed through a detec-tion mechanism. A further step of relaxation of the as-sumptions on the adversary leads to a non-cooperative game with incomplete information (Harsanyi, 1968). In this model, complete knowledge of the adversary X  X  cost function is replaced by uncertainty that is ex-pressed in terms of a Bayesian prior over parameters of the cost function.
 Finding Equilibrium points in adversarial learning problems has been studied for regret minimization problems where it leads to near-optimal solutions (Fre-und &amp; Schapire, 1996). If players are uncertain about some parameters of their adversaries X  cost function and this uncertainty is expressed in terms of a prior distri-bution over these parameters, then an equilibrium of the expected costs can be identified using counterfac-tual regret (Zinkevich et al., 2008). However, both results rely on finite action spaces and finite ranges of the variables that players are uncertain about. In contrast to regret minimization problems, classi-fication and regression games usually have continu-ous action spaces. For classification games, the iid assumption X  X he assumption of an entirely passive adversary X  X as only been relaxed as far as to the point of non-cooperative non-zero-sum games with complete information; for regression, to non-cooperative zero-sum games with complete information. Here, we will extend this sequence of relaxations to non-cooperative non-zero-sum regression games with incomplete infor-mation about the adversary X  X  cost function. We will not focus on generalization error bounds, but will in-stead study equilibrium points of the game defined by the regularized empirical cost functions.
 The rest of this paper is organized as follows. Section 2 introduces the players and cost functions for adversar-ial regression problems formally. Section 3 introduces the game with incomplete information, and the con-cept of optimal responses and equilibrium points. In Section 4, we show sufficient conditions under which a unique Bayesian equilibrium point exists for regression games. Section 5 derives an algorithm that identifies the unique Bayesian equilibrium point. Section 6 re-ports on experiments, Section 7 concludes. We study prediction games between a learner of a re-gression model and a data generator , which have con-flicting, but not necessarily antagonistic goals. At training time , the data generator produces a train-ing matrix X  X  R n  X  m and a vector y  X  R n of values of the target variable. The matrix rows and corre-sponding values of the target variable are governed by an unknown distribution p ( x ,y ). By contrast, at ap-plication time the data generator produces instances and values of the target variable according to a dis-tribution  X  p ( x ,y ) which may differ from p ( x ,y ); these instances are not yet available at training time. The action of the learner is to select parameters w  X  W of a linear model f w ( x ) = x T w . Here, W is called the learner X  X  action space. We study the action space of all possible parameter vectors, W = R m . The learner X  X  theoretical costs at application time are given by the expected weighted squared loss where c l ( x ,y )  X  R + reflects instance-specific costs. The data generator X  X  action is to manipulate the data generation process. By changing features of individual instances, the data generator transforms the distribu-tion p ( x ,y ) at training time into a distribution  X  p ( x ,y ) at application time. The adversary can change fea-tures, but cannot change the target value y . Intu-itively, a spam sender can make a message look legit-imate by adding random text, but cannot change the true nature of the message. This transformation pro-cess incurs costs which are quantified by  X  d ( p,  X  p ). This term acts as a regularizer on the transformation and implicitly constrains the possible discrepancy between the distributions at training and application time. The data generator may incur costs when the learner classifies an instance x as y . We model these with a squared-loss term ( f w ( x )  X  z ( x ,y )) 2 weighted by instance-specific factors c d ( x ,y )  X  R + , where z ( x ,y ) is the target value that renders the costs for the data generator at zero. For instance, f w may assess the risk of financial transactions and the data generator may generate a mixture of legitimate and fraudulent transaction requests. The target value would then be z ( x ,y ) = 0 because both legitimate and fraudu-lent users want their transactions to be found risk-free and executed. For fraudulent transactions, the instance-specific costs c d ( x ,y ) are proportional to the gain that a defrauder loses when the transaction x is declined. For legitimate transactions, the instance-specific costs may reflect the inconvenience experi-enced by customers whose transactions are denied. The theoretical costs of the data generator add the expected prediction costs to the transformation costs:  X  d ( w ,  X  p,c d ) = The theoretical costs of both players depend on the unknown distributions p and  X  p . We therefore focus on the regularized empirical counterpart of the the-oretical costs based on the training sample ( X , y , z ), where z = ( z 1 ,...,z n ) T and y = ( y 1 ,...,y n ) T are the empirical quantities of z ( x ,y ) and y , respectively. The empirical counterpart of the data generator X  X  regular-izer  X  d ( p,  X  p ) penalizes the divergence between training matrix X and a perturbed sample  X  X that would be the outcome of applying the transformation that trans-lates p into  X  p to matrix X . The transformed training matrix  X  X must not be mistaken for test data which are not assumed to be available at training time. The transformed training data  X  X acts as training data from the test distribution. At application time X  X fter both players have committed to their strategies X  X ew in-stances are drawn according to the distribution  X  p . In the following, we use the vectors c v = ( c v, 1 ,...,c v,n where v  X  X  d,l } , to denote the players X  empirical costs. Then, the empirical costs of predictive model f w and transformation from p to  X  p are:  X   X  ( w ,  X  X , c l ) =  X   X  ( w ,  X  X , c d ) = Our analysis will focus on the standard choice of the l 2 regularizer  X  l ( f w ) = k w k 2 2 for the learner and on the squared Frobenius norm of the difference matrix  X  ( X ,  X  X ) = X  X   X  X 2 that we do not need additional regularization param-eters that control the trade-off between loss functions and regularization terms for the players because this parameter is implicitly included in the scale of the cost vectors c v . Both players X  cost functions defined in Equation 1 and 2 depend on both the parameters w and the trans-formation manifested in  X  X . In general, no single value of w minimizes the learner X  X  costs independently of the data generator X  X  strategy X  X hich is the characteristic property of a game .
 In a game with full information , one assumes that both players disclose their cost functions to their opponent. We relax this assumption and model the data gen-erator X  X  costs as a parameter that the learner is un-certain about. We relax the full-information assump-tion asymmetrically: while the adversary maintains full information about the learner X  X  cost function, the learner X  X  full information is relaxed into uncertainty about the adversary X  X  instance-specific costs c d which is reflected in a Bayesian prior q ( c d ). This asymmetry reflects our adoption of the learner X  X  perspective: in modeling the learner X  X  lack of information about the adversary, we intend to make the learner more robust against new adversaries. This setting is referred to as a game with incomplete information between Bayesian players or, for short, a Bayesian game .
 game; W ,  X   X  l ,  X   X  d and c l are defined in Section 2. From the learner X  X  perspective, the costs c d are a random variable for which a value is drawn accord-ing to prior q ( c d ) at application time. Therefore, to the learner it appears that at training time the data generator commits only to a parametric strat-egy  X  : R n  X  R n  X  m that maps a value of c d  X  X hich is only assigned at application time X  X o a transforma-tion that manifests in matrix  X  X . The data generator X  X  action space  X  therefore contains functions  X  that map To the learner, the data generator X  X  strategy  X  is un-known. However, if  X  were given, then the optimal response to that strategy that minimizes the expected costs over q ( c d ) would be In analogy, if w was known to the data generator, the optimal response for costs c d would be A pair of parameter vector w and the data generator X  X  strategy  X  is called a Bayesian equilibrium (Harsanyi, 1967) if it is a fixed point with respect to the optimal response.
 Definition 1 (Bayesian Equilibrium) . A pair of w and  X  is called a Bayesian equilibrium if it satisfies Equations 3 and 4, respectively.
 If q is a single-point distribution, then a Bayesian equilibrium is a Nash equilibrium. Deviating unilater-ally from the Bayesian equilibrium increases the costs for either player X  X n case of the learner, the expected costs. Therefore, if one player assumes that the op-ponent plays a Bayesian equilibrium, it is optimal for this player to play the Bayesian equilibrium as well. However, it may be the case that more than one equi-librium exists for a game. If the players choose their actions according to distinct equilibria, then the out-come may be arbitrarily bad for either player. It is therefore crucial to study under which circumstances a regression game has a unique equilibrium.
 We now characterize the optimal responses in a way that allows to infer them. In the following, the term diag ( v ) denotes a diagonal matrix with ele-ments (diag ( v )) ii = v i for any arbitrary vector v and I k denotes the identity matrix of size k . Lemma 1 (Optimal Response of the Data Genera-tor) . Let X be a matrix of training data, z the vector of target labels, and c d a cost vector. Then, the opti-mal response to a model w as defined in Equation 4 is uniquely determined by  X   X  [ w ]( c d ) = Lemma 2 (Optimal Response of the Learner) . Let y be the vector of labels and c l a cost vector. Then, the optimal response to any data generator X  X  strategy  X  ex-ists and is uniquely determined by w  X  [  X  ] = I m + The proofs are included in the online appendix. We will now identify sufficient conditions under which a game G = ( W ,  X  ,  X   X  l ,  X   X  d , c l ,q ) has a unique Bayesian equilibrium. First we will show that for each game G with W = R m and  X  = {  X  : R n  X  R n  X  m } a game G 0 with compact and convex action spaces W 0 and  X  0 can be constructed that has identical equilibrium points. Then, by showing that G 0 has at least one Bayesian equilibrium we prove that this is also the case for G . Lemma 3 (Compactness of Action Spaces) . Let G = all functions that map from R n to R n  X  m . Let the ex-pected values R c d,i d q ( c d ) &lt;  X  exist. Then, there is a pact, and convex action spaces  X  0  X   X  and W 0  X  W , such that each Bayesian equilibrium in G is a Bayesian equilibrium in G 0 and vice versa.
 The proof is included in the online appendix. Lemma 3 leads to the the following existence result.
 Theorem 1 (Existence of an Equilibrium) . Let the expected value R c d,i d q ( c d ) &lt;  X  exist. Then, the Bayesian regression game G = ( W ,  X  ,  X   X  l ,  X   X  d , c at least one Bayesian equilibrium.
 Proof. Following Lemma 3 it is sufficient to show that game G 0 has at least one Bayesian equilibrium. Since the action spaces W 0 and  X  0 are bounded, w  X  [  X  ] is continuous in  X  (see proof of Lemma 3), and  X   X  [ w ] is continuous in w , Brouwers theorem implies that there exists at least one fixed point of (  X , w ) 7 X  (  X   X  [ w ] , w  X  [  X  ]) in  X  0  X W 0 .
 We will now derive sufficient conditions for the unique-ness of a Bayesian equilibrium. The equilibrium is unique if c l and c d are sufficiently small in relation to the regularizers; the exact condition is detailed in Equation 5 and can be validated given c l and q . Theorem 2 (Uniqueness of Equilibria) . Let G 0 = and  X  0 are nonempty, convex and compact sets. Fur-thermore, let the expected values R c d,i d q ( c d ) &lt;  X  ex-ist. Then, G 0 has a unique Bayesian equilibrium if for all distinct points ( w , X  ) , (  X  w ,  X   X  )  X  X  0  X   X  0 Theorem 2 generalizes Theorem 8 of Br  X uckner et al. (2012) on the uniqueness of equilibria for games with complete information. If the players X  costs c d and c l are too large, Equation 5 is violated, the game G is no longer locally convex and multiple equilibria can exist. An experiment on the link between uniqueness and cost parameters can be found in the online appendix. Proof. The existence of a Bayesian equilibrium follows from Theorem 1. We now reformulate the two-player game G 0 into an ( n +1)-player game G 00 , where each in-stance x i is transformed by an individual strategy  X  i : R  X  R m and any Bayesian equilibrium in G 0 corre-sponds to a distinct Nash equilibrium in G 00 . Let G 00 game without uncertainty, where W 00 = W 0 and  X  00 i is the strategy space  X  0 restricted to the data generator of instance i ; the functions are the corresponding loss functions for i = 1 ,...,n . Following Theorem 1 of Harsanyi (1968), we now show that if ( w , (  X  1 ,..., X  n )) is a Bayesian equilibrium in G then ( w , X  1 ,..., X  n ) is a Nash equilibrium in G 00 . Sup-pose that ( w , X  1 ,..., X  n ) is not a Nash equilibrium point in G 00 . Then, there exists a better strategy for at least one player. If w is not an optimal choice, the point ( w , (  X  1 ,..., X  n )) is not a Bayesian equilib-rium point in G 0 since the learner X  X  loss functions are equal for fixed data transformation strategies. How-ever, if any  X  i is not optimal, then the expected loss of the i -th data generator (see Equation 6) can be re-duced; there exists some point c d , where the i -th data generator benefits from changing her strategy unilat-erally. Hence, the i -th summand in the loss function  X   X  (see Equation 2) decreases while the rest remain un-changed when the other strategies are kept fixed. Con-sequently, ( w , (  X  1 ,..., X  n )) is not a Bayesian equilib-rium point in G 0 . Hence, it is sufficient to show that there exists at most one Nash equilibrium point in G 00 . The data generators X  action spaces  X  00 i constitute Hilbert spaces of square differentiable functions  X  i R  X  R m on a Lebesgue measurable set with mea-sure q ( c d ). The directional G X ateaux derivative of  X  in the direction d  X   X  00 i is given by
D  X  where  X  X  ,  X  X  denotes the standard Euclidean inner prod-uct in R m . Since W 00  X  R m , the learner X  X  directional derivate for any d  X  X  00 is given by
D Let ( w , X  1 ,..., X  n ) , (  X  w ,  X   X  1 ,...,  X   X  n )  X  X   X  n be two distinct points. Then, by Theorem 2.5 of Carlson (2001), a unique equilibrium in G 00 exists if 0 &lt; D  X  w  X   X  00 l ( w , X  1 ,..., X  n ) , w  X   X  w E where the instance-specific terms k i are given by k i ( w ,  X  w , x ,  X  x ) =  X  x  X   X  x , x  X   X  x  X  + Inserting Equation 8 in 7 and rewriting it in matrix form yields Inequality 5. It is a sufficient condition for the uniqueness of a Nash equilibrium in the proposed Hilbert spaces. In the input space, the data generators can play multiple strategies to reach this unique equi-librium; they differ for costs q ( c d ) = 0 of probability measure zero. However, exactly one of these strategies corresponds to a Bayesian equilibrium in G 0 since the data generator has to play optimal for all c d regardless of the probability distribution q . Hence, there exists at most one Bayesian equilibrium in G 0 . We have derived sufficient conditions for the existence of a unique Bayesian equilibrium in the linear regres-sion game. Since the optimal responses are uniquely defined, the equilibrium in a two-player regression game is already uniquely determined by one single ac-tion. Hence, the number of parameters to be esti-mated in order to find a Bayesian equilibrium can be reduced from m ( n + 1) to m . Therefore, we now define a surrogate function w : w 7 X  w  X  [  X   X  [ w ]]; every fixed point w ( w ) = w with dimension m corresponds to a Bayesian equilibrium ( w , X  [ w ]) and vice versa. Definition 2 (Surrogate Function) . For any weight vector w and a given cost vector c l , the function returns the optimal response for a data transforma-tion  X   X  [ w ] which was itself an optimal response to the vector w .
 Following Lemma 2, function w can be expressed as w ( w ; c l ) = The fixed point of w can be found by fixed-point algo-rithms, which evaluate the optimal responses and pos-sibly their gradients iteratively. Unfortunately, Equa-tion 9 depends on the matrices R  X   X  [ w ]( c d ) T d q ( c and R  X   X  [ w ]( c d ) T diag ( c l )  X   X  [ w ]( c d )d q ( c no closed-form solutions for arbitrary prior distribu-tions q . To tackle this problem, we approximate the data generator X  X  optimal response  X   X  [ w ]( c d ) by the t -th-order Taylor expansion  X  t ; a [ w ]( c d ) at point a :  X  for all 0 &lt; i  X  t . Theorem 3 states that the Approxi-mation 10 becomes more accurate with increasing t if the costs c d are bounded.
 Theorem 3 (Convergence criterion) . Let k c d k be bounded from above. Then, there exists a point a , such that for all points c d with non-zero density q ( c d ) &gt; 0 : The proof is included in the online appendix. If the costs c d are unbounded , there is typically some degree t that approximates  X   X  [ w ]( c d ) better than larger values. The Taylor expansion requires that the first 2 t mo-ments of q exist; let  X  i ( a ) = R diag ( c d  X  a ) i d q ( c the i -th central moment around a . Equation 11 defines a surrogate function according to Equation 9, where we made use of the Taylor approximation  X  t ; a [ w ]( c d of the response  X   X  [ w ]( c d ) of the data generator: The existence of the inverse matrices in Equations 9 and 11 follows from Lemma 2.
 The choice of the pivotal point a influences the radius of convergence (see proof of Theorem 3); for small val-ues of t , it should be located in a high-density region of q . We will now derive an algorithm that infers a Bayesian equilibrium based on the Taylor expansion at a = E [ c d ]. A fixed point of w t ; E [ c found by simplex algorithms (see, e.g., Van der Laan &amp; Talman, 1982), which, unfortunately, have exponen-tial worst-case execution time (Hirsch et al., 1989). On the other hand, standard gradient descend approaches guarantee only local convergence and are not robust against poor starting points. We use a graduated op-timization algorithm to find the fixed point.
 Algorithm 1 uses a sequence of increasing costs, termi-nating at the original costs of the learner. For function FixedPoint , we use a Newton-like method that ap-proximates the Jacobi matrix by difference quotients. The k -th fixed point is a Taylor approximation to the Bayesian equilibrium (Line 5).
 Algorithm 1 Bayesian Equilibrium by Graduated Optimization Input: Sequence of costs c l, 1 ,..., c l,k , where k c l,i Output: Bayesian equilibrium ( w  X  , X   X  ) 2: for i = 1 ,...,k do 4: end for 5: return ( w k , X   X  [ w k ]) In this section we study the behavior of the Bayesian regression model in the context of email spam filtering. Our motivating application is to predict the fraction of users who perceive an email as unwanted. We collected about 190,000 emails from an email service provider between September 2007 and December 2008.
 We approximate the actual fraction of users who per-ceive the message as spam by the log-likelihood for the class spam of a classifier; we train classifiers using ten-fold cross validation on all data and label the held-out data in each iteration with the log-likelihood inferred by the classifier. Emails are represented by the first ten principal components ( m = 10) of the binary bag-of-words features (around 226,000 words). The target score z = 0 reflects that all senders desire their emails to be perceived as non-spam by all recipients. We measure the root mean squared error (RMSE).
 For the Bayesian regression model, we use the first-order Taylor approximation, t = 1 (except when we study different values of t ). For t = 1, the Bayesian re-gression model only depends on mean value  X  and vari-ance  X  2 of the prior q ; the distributional assumption about q ( c d,i ) has no additional influence on the equi-librium. For higher values of t , we use a gamma dis-tribution. We compare the Bayesian regression model (denoted Bayes ) to three reference methods: the Nash regression model that emerges as a special case for one-point distributions q (denoted Nash ), robust ridge re-gression (denoted Minimax ; Sayed &amp; Chen 2002), and a regular ridge regression (denoted Ridge ). We set the Nash model X  X  conjecture for all values of c d,i to the mean value  X  . The perturbation parameter for Min-imax is chosen as minimal value such that the space of possible transformations of the input matrix still includes the solutions of Bayes and Nash .
 In the first experiment, we study how the methods perform against an adversary that chooses a strat-egy according to a Bayesian equilibrium for varying parameters of q ( c d ). In each repetition, we compute two Bayesian equilibrium points on separate, disjoint sets drawn at random from September 2007. We ex-tract the learner X  X  model from the first, and trans-formed data points from the second equilibrium point after drawing actual costs from q ( c d,i ) and playing ac-cording to Lemma 1. The regularization parameters of all methods are set to match the learner X  X  costs of c l,i = 0 . 1. Figure 1 shows the RMSE for varying expected values (left, with fixed variances  X  2 = 1) and variances (center, with fixed  X  = 1) of the data gener-ator X  X  costs, averaged over ten training samples of size 200. We observe that Bayes outperforms the iid base-lines consistently. The advantage of Bayes over Nash grows with the variance of q . This is plausible be-cause the Bayesian game accounts for uncertainty on the data generator X  X  costs whereas the Nash model as-sumes that all values are  X  . More details are docu-mented in the online appendix.
 In a second experiment, we evaluate all methods into the future. Here, the models play against actual spam-mers. The training sample of 200 instances is drawn from month k . The regularization parameters of all learners (for Nash and Bayes , we use a single cost pa-rameter for all instances) are tuned on 1,000 instances from month k + 1. Test data are drawn from months k + 2 to k + 6. Additionally, in order to artificially cre-ate a mismatch to the assumed adversity of the data generator, we also evaluate the models on test data drawn in reverse chronological order; for evaluation into the past, tuning data are drawn from month k  X  1 and test data are drawn from months k  X  2 to k  X  6. This process is repeated and RMSE measurements are averaged over ten resampling iterations of the training set and, in an outer loop, over four training months k (March to June 2008). The data generator X  X  costs pa-rameters are set to  X  = 0 . 01 and  X  2 = 0 . 01 for Bayes and to  X  = 0 . 01 for Nash .
 Figure 2 (left) shows that Bayes and Nash are more robust over time; they outperform the minimax and ridge regression models for emails received at least two months after training. Additionally, Bayes signifi-cantly outperforms Nash . When test data are drawn in reverse chronological order, the relative performance of Bayes , Nash and Ridge is reversed. This corresponds to the mismatch between the assumed adversity of the test data and the actual tendency of historic email spam to be less innovative and difficult. Barely regu-larized ridge regression excels in this setting. Figure 2 (center) shows experiments in which we mea-sure the RMSE for a fixed point in the future over a range of values for  X  . The curve shows that Bayes is robust with respect to the parameter  X  of prior q due to the dominating variance. The online appendix includes more details.
 Figure 1 (right) visualizes the actual chronological shift of spam emails and compares it to the equilibrium point. The axes are the two most discriminative prin-cipal components of the input space. We train a Nash model on 200 instances from March 2008; the green to yellow dots visualize the actual chronological shift of class spam. The red dots visualize the training data, transformed according to the equilibrium point given by Lemma 1. Generally, the equilibrium anticipates the principal trend of the data shift. In the lower right-hand corner, an entirely new cluster emerges in the test data that is not present in the equilibrium. Similar ex-periments for April to June 2008 are included in the online appendix.
 All game-theoretical models are computationally more intense. Figure 2 (right) shows the execution time over the number of training emails. The execution time as a function of the number of attributes can be found in the online appendix. Bayes computes multi-ple fixed points using the gradient of the optimal re-sponses. The optimal responses in a Nash game ignore the variance of the data generator X  X  costs leading to a simplified optimization problem. Minimax has to solve a costly inner optimization problem to determine the worst perturbation.
 Finally, we study the impact of the degree t of the Taylor approximation on the accuracy and execution time of Bayes . Figure 6 in the online appendix shows that its influence on the RMSE is minimal; t = 3 gives marginally lower RMSE values than 2 and 1. The execution time for t = 3 is by a constant factor of approximately 3 higher than for t = 1. Previous work on adversarial classification has re-laxed the iid assumption X  X he assumption of an en-tirely passive adversary X  X s far as to the point of non-cooperative non-zero-sum games with complete infor-mation (Br  X uckner et al., 2012); for adversarial regres-sion, to non-cooperative zero-sum games with complete information (Sayed &amp; Chen, 2002); and for finite ac-tion spaces, to non-cooperative zero-sum games with incomplete information (Zinkevich et al., 2008). This paper extends this to non-cooperative non-zero-sum regression games with incomplete information about the cost function of the adversary. We have shown that regression games have at least one Bayesian equilib-rium, and that the equilibrium is unique when the cost functions are sufficiently strongly regularized. We de-rived an algorithm that identifies the unique Bayesian equilibrium. From our experiments, we conclude that the Bayesian model achieves a smaller RMSE than the Nash model, the minimax model and ridge regression when playing against a Bayesian adversary for email data. When evaluating against actual future emails, the Bayesian models predict the log-likelihood of the class spam for future emails more accurately than the Nash, minimax, and ridge regression models.
 This work was supported by the German Science Foun-dation DFG under grant SCHE 540/12-1.
 Br  X uckner, M., Kanzow, C., and Scheffer, T. Static prediction games for adversarial learning problems.
Journal of Machine Learning Research , 13:2589 X  2626, 2012.
 Carlson, D. A. The existence and uniqueness of equi-libria in convex games with strategies in Hilbert spaces. Advances in Dynamic Games and Appli-cations , 6:79 X 97, 2001.
 El Ghaoui, L., Lanckriet, G., and Natsoulis, G. Ro-bust classification with interval data. Technical Re-port UCB/CSD-03-1279, Computer Science Divi-sion (EECS), University of California, 2003.
 Freund, Y. and Schapire, R. Game theory, on-line pre-diction and boosting. In Proceedings of the 9th An-nual Conference on Computational Learning The-ory , 1996.
 Globerson, A. and Roweis, S. Nightmare at test time:
Robust learning by feature deletion. In Proceedings of the 23rd International Conference on Machine Learning , 2006.
 Harsanyi, J. C. Games with incomplete information played by Bayesian players, i-iii. part i. the basic model. Management Science , 14(3):159 X 182, 1967. Harsanyi, J. C. Games with incomplete information played by Bayesian players, i-iii. part ii. Bayesian equilibrium points. Management Science , 14(5): 320 X 334, 1968.
 Hirsch, M. D., Papadimitriou, C. H., and Vavasis, S. A.
Exponential lower bounds for finding Brouwer fix points. Journal of Complexity , 5(4):379 X 416, 1989. Lanckriet, G., El Ghaoui, L., Bhattacharyya, C., and
Jordan, M. A robust minimax approach to classi-fication. Journal of Machine Learning Research , 3: 552 X 582, 2002.
 Sayed, A. H. and Chen, H. A uniqueness result con-cerning a robust regularized least-squares solution. Systems and Control Letters , 46(5):361 X 369, 2002. Teo, C. H., Globerson, A., Roweis, S., and Smola, A.
Convex learning with invariances. In Proceedings of the 20th Annual Conference on Neural Information Processing Systems , 2007.
 Van der Laan, G. and Talman, A. J. J. On the com-putation of fixed points in the product space of unit simplices and an application to noncooperative n person games. Mathematics of Operations Research , 7(1):1 X 13, 1982.
 Zinkevich, M., Johanson, M., Bowling, M., and Pic-cione, C. Regret minimization in games with incom-plete information. In Proceedings of the 21st Annual
Conference on Neural Information Processing Sys-
