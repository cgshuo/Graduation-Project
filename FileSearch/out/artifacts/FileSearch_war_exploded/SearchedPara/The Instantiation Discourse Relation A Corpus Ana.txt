 In an INSTANTIATION relation, one text span ex-plains in further detail the events, reasons, behaviors and attitudes mentioned in the other (Miltsakaki et al., 2008), as illustrated by the segments below: Sentence [a] mentions  X  X ther reforms X  and a threat to them, but leaves unspecified what are the reforms or how they are threatened. Sentence [b] provides sufficient detail for the reader to infer more con-cretely what has happened.

The INSTANTIATION relation has some special properties. A study of discourse relations as in-dicators for content selection in single document summarization revealed that the first sentences from INSTANTIATION pairs are included in human sum-maries significantly more often than other sentences (Louis et al., 2010) and that being a first sentence in an INSTANTIATION relation is the most powerful indicator for content selection related to discourse relation sense. The sentences between which the relation holds also contain more sentiment expres-sions than other sentences (Trnavac and Taboada, 2013), making it a special target for sentiment anal-ysis applications. Moreover, INSTANTIATION rela-tions appear to play a special role in local coherence (Louis and Nenkova, 2010), as the flow between IN -STANTIATION sentences is not explained by the ma-jor coherence theories (Kehler, 2004; Grosz et al., 1995). Many of the sentences in INSTANTIATION relation contain entity instantiations (complex ex-amples of set-instance anaphora), such as  X  X everal EU countries X  X  X  X he UK X ,  X  X ootballers X  X  X  X ayne Rooney X  and  X  X ost cosmetic purchase X  X  X  X ipstick X  (McKinlay and Markert, 2011), raising further ques-tions about the relationship between INSTANTIA -TIONS and key discourse phenomena.
 Detecting an INSTANTIATION , however, is hard. In the Penn Discourse Treebank (PDTB) (Prasad et al., 2008), I NSTANTIATION is one of the few re-lations that are more often implicit , i.e., expressed without a discourse marker such as  X  X or exam-ple X . Identifying implicit discourse relation is an ac-knowledged difficult task (Braud and Denis, 2015; Ji and Eisenstein, 2015; Rutherford and Xue, 2014; Biran and McKeown, 2013; Park and Cardie, 2012; Lin et al., 2009; Pitler et al., 2009), but the chal-lenge is exacerbated due to the lack of explicit IN -STANTIATION s: explicit relations are shown to im-prove their implicit counterparts using data source expansion (Rutherford and Xue, 2015). Moreover, detecting INSTANTIATION also involves the skewed class distribution problem (Li and Nenkova, 2014a) because although it is one of the largest class of im-plicit relations, it constitutes less than 10% of all the implicit relations annotated in the PDTB.

In this work, we identify a rich set of factors that sets apart each sentence in an implicit INSTAN -TIATION and the pair as a whole. We show that these factors improve the identification of implicit INSTANTIATION by at least 5% in F-measure and 8% in balanced accuracy compared to prior systems. We use the Penn Discourse Tree Bank (PDTB) (Prasad et al., 2008) for the analysis and experiments presented in this paper. There are a total of 1,747 IN -STANTIATION relations in the PDTB, of which 83% are implicit. I NSTANTIATION s make up 8.7% of all implicit relations and is the 5th largest among the 16 second-level relations in the PDTB. We identify significant factors 1 that characterize: (i) s 1 and s 2 : the first and second sentence in an IN -STANTIATION pair vs. all other sentences; (ii) s 1 vs. s : adjacent sentence pairs in INSTANTIATION rela-tion vs. all other adjacent sentence pairs.

Our analysis is conducted on the PDTB except section 23, which is reserved for testing as in prior work (Lin et al., 2014; Biran and McKeown, 2015). In total, there are 1,337 INSTANTIATION sentence pairs and 43,934 non-INSTANTIATION sentences for the corpus analysis. Sentence length. Intuitively, longer sentences are more likely to involve details. Table 1 demonstrates that there is an average of 8.4-word difference in length between the two sentences in an INSTAN -TIATION relation; moreover, s 1 s are significantly shorter (more than 5 words on average) than other sentences, and s 2 s are significantly longer. Rare words. For each sentence, we compute the percentage of words that are not present in the 400K vocabulary of the Glove vector representa-tions (Pennington et al., 2014). Table 2 shows that s 1 of INSTANTIATION s contain significantly fewer out-of-vocabulary words compared to either s 2 and non-INSTANTIATION s. We also compare word pairs across sentence pairs, i.e., ( w i , w j ) , w s , w j  X  s 2 . Compared to non-INSTANTIATION , words across INSTANTIATION arguments show sig-nificantly larger average unigram log probability dif-ference (1.24 vs. 1.22). These numbers show that the first sentences of INSTANTIATION do not involve many unfamiliar words  X  an indication of higher readability (Pitler and Nenkova, 2008).
 Gradable adjectives. The use of gradable ad-jectives (Frazier et al., 2008; de Marneffe et al., 2010) X  popular, high, likely  X  may require further explanation to justify the appropriateness of their use. Here we compute the average percentage of gradable adjectives in a sentence. The list of ad-jectives is from Hatzivassiloglou and Wiebe (2000) and the respective percentages are shown in Table 3. Compared to other sentences, s 1 of INSTANTIATION involves significantly more gradable adjectives. Parts of speech. We study word categories that are heavily or rarely used with INSTANTIATION by inspecting the percentage of part-of-speech tags found in each sentence. In Table 4, we show POS tags whose presence is significantly differ-ent across arguments in INSTANTIATION but not so across non-INSTANTIATION , with significance in non-INSTANTIATION in the reverse direction de-noted in  X  . Four cases of POS occurrences are in-spected:  X  more often in s 1 compared to s 2 ,  X  more often in s 2 compared to s 1 ,  X  more (+) or less (-) in s 1 compared to non- X  more (+) or less (-) in s 2 compared to non-We see that s 1 of INSTANTIATION contains more characteristic POS usage than s 2 . There are more comparative adjectives and adverbs as well as fewer nouns in s 1 compared to s 2 in INSTANTIATION pairs. The usage of verbs is also different between two arguments and s 1 contains more conjunctions and existential there. On the other hand, s 2 con-tains more nouns, numbers, determiners, personal pronouns and proper nouns, intuitively associated with the presence of detailed information.
 Wordnet relations. Here we consider word-level relationships across arguments using Wordnet (Fell-baum, 1998). For each noun, verb, adjective and adverb content word pairs across arguments, we cal-culate the percentage of sentences with each type of Wordnet relation. Shown in Table 5, among IN -STANTIATION sentence pairs there are significantly more noun-noun pairs with hypernym or meronym relationships and verbs with indirect hypernym rela-tionship. We also observe significantly more seman-tically similar verbs ( group (v) ).
 Lexical similarity. Finally, we inspect the similar-ity between sentences in each pair as well as be-tween each sentence in a pair and their immediate prior context; specifically:  X  Between s 1 and s 2 ;  X  Between s 1 and C and between s 2 and C , We compute the Jaccard similarity between sen-tences using their nouns, verbs, adjectives and ad-verbs. I NSTANTIATION arguments are significantly less similar than other adjacent sentence pairs (0.335 vs. 0.505), indicating higher difference in content. Shown in Table 6, both arguments of INSTANTI -ATION are less similar to the immediate context. While other sentence pairs follow the pattern that s 2 is much less similar to s 1  X  X  immediate context, this phenomenon is not significant for INSTANTIATION . In this section, we demonstrate the benefit of ex-ploiting INSTANTIATION characteristics in the iden-tification of the relation.
 Settings. Following prior work that identifies the more detailed (second-level) relations in the PDTB (Biran and McKeown, 2015; Lin et al., 2014), we use sections 2-21 as training, section 23 as test-ing. The rest of the corpus is used for development. The task is to predict if an implicit INSTANTIATION relation holds between pairs of adjacent sentences in the same paragraph. Sentence pairs with INSTANTI -ATION relation constitute the positive class; all other We use Logistic Regression with class weights in-versely proportional to the size of each class. Features. The factors discussed in  X  3 are adopted as the only features in the classifier. We use the av-erage values of s 1 and s 2 and their difference for: the number of words, difference in number of words compared to the sentence before s 1 , the percentage of OOVs, gradable adjectives, POS tags and Jaccard similarity to immediate context. We use the min-imum, maximum and average differences in word-pair unigram log probability, and average Jaccard similarity across sentence pairs. For Wordnet rela-tions, we use binary features indicating the presence of a relation. Results. To compare with our INSTANTIATION -specific classifier ( Inst. specific ), we show results from two state-of-the-art PDTB discourse parsers that identify second-level relations: Biran and McK-eown (2015) ( B&amp;M ) and Lin et al. (2014). We also compare the results with the classifier from our prior work (Li and Nenkova, 2014b) ( L&amp;N ). In that work we introduce syntactic production-stick fea-tures, which minimize the occurrence of zero val-ues in instance representation. Furthermore, we re-implemented Brown-cluster features (concatenation of clusters in each sentence) that have been shown to perform well in identifying INSTANTIATION  X  X  par-
Table 7 shows the precision, recall, F-measure and balanced accuracy (average of the accuracies for the positive and negative class respectively) for each system. We show balanced accuracy rather than overall accuracy due to the highly skewed class distribution. For Inst. specific , we use a threshold with L&amp;N for a soft voting classifier where the la-bel is assigned to the class with larger weighted pos-classifiers achieved at least 5% improvement of F-measure and 8%-10% improvement of balanced ac-curacy compared to other systems. These improve-ments mostly come from a dramatic improvement in recall. The improvement achieved by the vot-ing classifier also indicate that Inst. specific pro-vide complementary signals to syntactic production rules. Note that compared to Lin et al., Inst. spe-cific behaves very differently in precision and recall, indicating potential for further system combination.
Finally, we analyze the confusion matrix induced by false positives and false negatives across Lin et al., B&amp;M, Inst. specific and soft vote 7 . In Ta-ble 8, we list relations contributing at least 10% to false positives for at least one system. Remark-ably, INSTANTIATION is consistently confused with a constant set of relations: RESTATEMENT , CAUSE and EntRel. Different from other systems, the Inst. specific classifier demonstrates more confu-sion towards CAUSE than RESTATEMENT . On the false negative side, all 62 mistakes are not anno-tated (i.e., NoRel/EntRel) in Lin et al. For the 58 false negatives in B&amp;M, we observe above 10%: NoRel/EntRel (56.9%), CAUSE (19.0%), RESTATE -MENT (13.8%), consistent with relations relations involved in false positives. We have characterized the implicit INSTANTIATION relation by studying significant factors that discrim-inate individual arguments and the sentence pairs connected by the relation. We show distinctive pat-terns in sentence length, word usage, semantic rela-tionships between words as well as cross-argument and contextual similarity associated with INSTANTI -ATION . Using these factors as features we demon-strate significant improvement on the detection of
