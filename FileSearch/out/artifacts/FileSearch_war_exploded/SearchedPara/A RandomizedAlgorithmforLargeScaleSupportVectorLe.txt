 Consider a training dataset D = f ( x points and y [1] C-SVM-1: At optimality w is given by w = Consider the set S = f x number of examples, n , and is often much less than n .
 lem (AOP) [2, 3, 4]. An AOP is defined as follo ws: that, for a given F G H , either reports F = min F 0 &lt; F .
 Man y SVM learning problems are AOP problems; algorithms developed for AOP problems can be dimension of the problem [2].
 applied to non-linear kernels. sho ws the randomized algorithm from SVM learning. 2.1 Linearly separable data a few results from random projections which will be used in this section. For a linearly separable dataset D = f ( x formulation is the same as C-SVM-1 with jj w jj , the problem can be reformulated as follo ws: C-SVM-2a: where ^ w = w that is, it is the distance between the 2 supporting hyperplanes, h 1 : y h 2 : y i ( w x i + b ) = 1 .
 vectors, the mar gin is preserv ed with a very high probability . Let w 0 and x 0 space with the dataset being D 0 = f ( x 0 as follo ws: C-SVM-2b: the follo wing lemma.
 Theor em 1. Let L = max jj x a random d k matrix as given in Lemma 2(Appendix A) . Let e w = R T w p on the optimal mar gin l Proof. From Cor ollary 1 of Lemma 2 ( Appendix A ), we have y = 1 . Then the follo wing holds with probability at least 1 2 e 2 k 8 1 ( Appendix A ), we have (1 ) jj w jj jj e w jj (1 + ) jj w jj , with probability at least 1 2 e 2 k 8 . Since jj w jj = 1 , we have p 1 jj e w jj p 1 + . Hence we have which y exists at least one hyperplane ( e w can only be better than this. So the value of k is given by: probability . We start with the follo wing definition.
 Definition An orthogonal extension of a k 1 -dimensional flat( a k 1 dimensional flat is a k 1 -dimensional affine space) h ^ R . Let ^ x classifier with mar gin l to be all 0 X  X  in the last d k coor dinates and identical to w separ ator with mar gin l with mar gin l if, y i ( w p x 0 i + b ) l; i = 1 ; : : : ; n then y i ( e w ^ x i + b ) l; i = 1 ; : : : ; n tions, that is, x 0 T Theor em 2. Given k 8 points x Proof. Let w ; b denote the normal to a separating hyperplane with mar gin l , that is, y space and let w 0 ; z Theor em 1 , y as required. This sho ws the first part of the claim.
 the problem [3, 4]. 2.2 Almost separable data call a dataset almost separ able if by remo ving a fraction = O ( log n becomes linearly separable.
 C-SVM-1 . This problem can be reformulated as follo ws: Theor em 3. Given k 8 lower bound on l as in the Gener alized Optimal Hyperplane formulation and = O ( log n exists a subset of k 0 training points x conditions: w = into to 2 disjoint sets, SV 0 and No w, consider remo ving the points in SV the proof for the first 2 conditions.
 When all the points in SV than this would violate the constraints of the problem. This pro ves condition 3. be upper bounded by k + 1 = a point x also. 2.3 A Randomized Algorithm Algorithm 1 RandSVM-1(D,k,r) Requir e: D -The dataset.
 Requir e: k -The estimate of the number of support vectors.
 Requir e: r -Sample size = ck; c &gt; 0 . 1: S = randomsubset( D; r ); // Pic k a random subset, S , of size r from the dataset D 2: SV = svmlearn( ; S ); // SV -set of support vector s obtained by solving the problem S 4: while j V j &gt; 0 and j SV j &lt; k do 5: R = randomsubset( V , r j SV j ); // Pic k a random subset from the set of violator s 8: end while 9: return SV is near optimal with a very high probability .
 No w consider the case where j SV j &lt; k and j V j &gt; 0 . Let x point such that y obtained is optimal.
 nu -SVM. This can be handled only be solving for k as a function of where is the maximum al-lowed distortion in the L to length 1 , that is, L = 1 , then Equation 1 becomes = (1 + 1+ L 2 result from Theor em 2 , the value of k can be determined in terms of as follo ws: for non-linear kernels, and SVMPerf and SVMLin for linear kernels. Synthetic datasets ( a; a; : : : ; a ) , and the other class has mean ( a; a; : : : ; a ) , where a = 2 = check erboard check erboard used with RandSVM was SVMLight; otherwise it was LibSVM. a point is labelled negative, else the point is labelled positi ve. have a subscript of 2 , for example, ringnorm Real world dataset was split into a training set of 7,00,000 documents and a test set of 104,414 documents. linear kernel, C , for CCA T and C11 were obtained from pre vious work done[12 ]. set to (32 log(4 n= )) = 2 .
 time does not increase rapidly with the increase in the dataset size. that Theor em 2 holds in practice. problems. the L Lemma 1. Let R = ( r E [ jj u 0 jj 2 ] = jj u jj 2 and the following bound holds: and the dot products when the y are projected onto a lower dimensional space [7]. Lemma 2. Let u; v 2 R d . Let u 0 = R T u p &gt; 0 , the following bounds hold tion(This is a slight modification of the corollary given in [7]). Cor ollary 1. Let u; v be vector s in R d s.t. k u k L Then for any &gt; 0 , the following holds with probability at least 1 4 e 2 k 8 [1] V. Vapnik. The Natur e of Statistical Learning Theory . Springer , Ne w York, 1995. [8] Kenneth L. Clarkson. Las vegas algorithms for linear and inte ger programming when the [11] Maria-Florina Balcan, Avrim Blum, and Santosh Vempala. On kernels, mar gins and low-
