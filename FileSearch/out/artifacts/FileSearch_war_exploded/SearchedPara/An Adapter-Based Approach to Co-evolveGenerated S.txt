 The changing nature of DB schemas has been a constant concern since the inception of DBs. Software consuming data is dependent upon the structures keeping this data, i.e. the DB schema. Such software might refer to relational views [5], data mappings (i.e. describin g how data instances of one schema corre-spond to data instances of another) [15] or application code [4]. But, what if this software is not directly coded but generated? The popularity of Model-Driven Forward Engineering (FE) is making cod e generation go mainstream. Key gears of this infrastructure are Model-to-Text (M2T) transformations as supported by Acceleo 12 . M2T transformations (hereafter referred to as just  X  X ransforma-tions X ) obtain  X  X he text X  (i.e. the software code akin to a target platform) from a model, i.e. an abstract representation of the solution (a.k.a. Platform Inde-pendent Model). As any other application, the generated  X  X ext X  is fragile upon changes on the underlying platform, i.e.  X  X he text X  should co-evolve with the plat-form. Traditionally, this is achieved at  X  X he text X  level [4]. FE opens a different way: co-evolving not the generated code but the tra nsformation that generates this code. This is the research question we tackle, i.e. how changes in the DB schema can be propagated to the transformation.

We turn the issue of keeping the application and the DB schema in sync, into one of maintaining the consistency between the code generators (i.e. the transformation) and the DB schema. Despite the increasing importance of FE, this issue has been mostly overlooked. This might be due to understanding that traditional co-evolution techniques can be re-used for transformations as well. After all, transformations are just another kind of applications. However, existing strategies for application co-evolutio n assume that either the generated SQL script is static or the trace for DB cha nges is known (and hence, can later be replicated in the application) [4]. However, these premises might not hold in our scenario. Rationales follow:  X  transformations do not specify but construct SQL scripts. The SQL script  X  the trace for DB changes might be unknown. The DB schema and the trans-We had to face the aforementioned scen arios ourselves when implementing Wiki-Whirl [10] , a Domain-Specific Language (DSL) built on top of MediaWiki . Wiki-Whirl is interpreted, i.e. a WikiWhirl model (an expression described along the WikiWhirl syntax) delivers an SQL script that is enacted. The matter is that this SQL script goes along the MediaWiki DB schema. If this schema changes, the script might break apart. Since MediaWiki is part of the Wikimedia Foun-dation , we do not have control upon what and when MediaWiki releases are delivered. And release frequency can be l arge (the perpetual-beta effect) which introduces a heavy maintenance burden upon WikiWhirl . This paper describes how we faced this issue by applying the adapter pattern to transformations. That is, data manipulation requests (i.e. inser t, delete, update, select) are re-directed to the adapter during the transformation. The adapter outputs the code accord-ing to the latest schema release. In this way, the main source of instability (i.e. schema upgrades) is isolated in the adapter. The paper describes the adapter architecture, a case study and a cost analysis to characterize the settings that can benefit from this approach. We start by framing our work within the litera-ture in DB schema evolution. This section frames our work within the abundant literature in DB schema evo-lution. The aim is not to provide an exhaustive list but just some representative examples that serve to set the design space (for a recent review refer to [5]). For our purposes, papers on schema co-evolution can be classified along two dimen-sions: the target artifact to be co-evolved, and the approach selected to do so. As for the former dimension, the target artifact includes data and applications. As for data , upgrading the schema certainly forces to ripple those changes to the corresponding data (e.g. tuples). The strategy used in [7] is for designers to express schema mapping s in terms of queries, and then use these queries to evolve the data between the DB schemas. On the other hand, applications con-suming data are dependent upon the structures keeping this data. Applications might include DB views [5], data mappings [15] or application code [4]. Our work focuses on a specific kind of applications: transformations.

The second dimension tackles the mechanisms used to approach the aforemen-tioned co-evolution scenarios. First, Application Programming Interfaces (APIs). This technique allows programs to interface with the DB through an external conceptual view (i.e. the API) instead of a logical view (i.e. the SQL data-manipulation language over the DB schema). This technique is investi-gated in [3] where they introduce an API which aims at providing application programs with a conceptual view of the r elational DB. The second technique is the use of rewriting methods . Here, the approach is to replace sub-terms of a formula with other terms. In our setting, this formula can stand for a DB view or data mapping expression (a.k.a. D isjunctive Embedded Dependencies). A rewriting algorithm reformulates a query/mapping upon an old schema into an equivalent query/mapping on the new schema. An example is described in [5]. Next, DB views are also used to support co-evolution. Views ensure logical data independence whereby the addition or removal of new entities, attributes, or relationships to the conceptual schema should be possible without having to rewrite existing application programs. Unfortunately, solutions based on views cannot support the full range of schema changes [11]. Finally, wrappers enclose artifacts to keep the whole ecosystem functioning. Upon upgrades on the DB schema, the artifact to be wrapped can be either the schema itself or the legacy applications. The first approach is illustrated by [4] where the new schema is encapsulated through an API. The wrap per converts all DB requests issued by the legacy application into requests compliant with the new schema. Alterna-tively, wrappers can be used to encapsulate the legacy applications. In this case, wrappers act as mediators between the application requests and the DB system. Wrappers for reusing legacy software components are discussed in [12] and [13]. handled by the transformation. Figure 1 depicts the main artifacts involved: DBS relational is the original DB schema; T M2T is the original legacy application (i.e., the version of the transformation that co-existed with the original DB); DBS X  relational is the new DB schema; T X  M2T is the new target artifact; U  X  relation is the update applied to the original source 3 ;Y  X  M2T is the target update re-sulting from the coevolution.

U  X  relation can be obtained by recording the changes upon DBS relational while the user edits it [4]. However, in our setting this is not always possible since the schema belongs to a third party. Alternatively, an update can be computed using a homogeneous artifact comparison operator, which takes an original ver-sion of an artifact and its new version, and returns an update connecting the two. This is our approach. Now, we could obtain transformation update (i.e. Y  X  M2T )outof U  X  relation . We propose transformations to be engineered for evolution. Therefore, a transformatio n is conceived as a pair (S,N) where S de-notes the stable part, and N stands for the no-stable part. The no-stable part is supported through an adapter. Therefore, Y  X  M2T denotes the update to be con-ducted in the adapter. Therefore, this wo rk presents an architecture to compute adapter updates (i.e. Y  X  M2T ) out of differences between DB schema models (i.e. U  X  relation ). Our vision is for the adapter to be domain-agnostic, and hence, reusable in other domains. We envisage transformation adapters to play a role similar to DBMS drivers. A DBMS driver shelters applications from the hetero-geneity of DBMS. The driver translates the application X  X  data queries into com-mands that the DBMS understands. Likewis e, a transformation adapter seeks to isolate the transformation from changes in the DB schema. As long as these changes are domain agnostic (e.g. the way to face attribute rename is domain independent) then, the adapter can be re-used by the community. Our solution is available in www.onekin.org/downloads/public/Batch_MofscriptAdaptation.rar This section outlines the WikiWhirl project [9] and the challenges posed by its ex-ternal dependency with the MediaWiki DB schema. Wikis are main exponents of collaborative editing where users join forces towards a common goal (e.g. writing an article about a topic). It comes as no surprise that wikis promote an article-based view as opposed to a holistic view o f the wiki content. As a result, APIs and GUIs of wiki engines favor operations upon single articles (e.g. editing and dis-cussing the article X  X  content) while overlooking operations on the wiki as a whole (e.g. rendering the wiki X  X  structure or acting upon this structure by splitting, merg-ing or re-categorizing articles). To amend this, WikiWhirl abstracts wiki structure in terms of mindmaps, where refactoring operations ( WikiWhirl expressions) are expressed as reshapes of mindmaps. Since wikis end up being supported as DBs, WikiWhirl expressions are transformed into SQL scripts. Figure 2 shows a snippet of a WikiWhirl transformation: a sequence of SQL statements with interspersed dynamic parts that query the input model (i.e. the WikiWhirl expression). These statements built upon the DB schema of MediaWiki , and in so doing, create an external dependency of WikiWhirl w.r.t. MediaWiki . In a 4 X  year period, the Me-diaWiki DB had 171 schema versions [6]. According to [6], the number of tables has increased from 17 to 34 (100% increase), and the number of columns from 100 to 242 (142%). This begs the question of how to make WikiWhirl coevolve with the MediaWiki upgrades. Table 1 compares four options in terms of the involvement (i.e. time and focus) and the required technical skills. Option 1: Manually Changing the Generated Code. The designer detects that the new release impacts the generat ed code, and manually updates this code. This approach is discouraged in Forward Engineering outside the protected areas, since subsequent regenerations can override the manual changes. On the upside, this approach acts directly on the generated code, so only SQL skills are required to accomplish the change.
 Option 2: Manually Changing the Transformation. For sporadic and small transformations, this might be the most realistic option. However, frequent platform releases and/or SQL-intensive transformations make this manual ap-proach too cumbersome and error-prone to be conducted in a regular basis. The user needs to know both the platform language (e.g. SQL) and the transforma-tion language (e.g. MOFScript ). No additional infrastructure is introduced. Option 3: Automatically Changing the Transformation. Theideaistoin-ject the transformation into a model and next, use a Higher-Order Transformation (HOT) [14] to upgrade it. HOTs are used to cater for transformation variability in Software Product Lines [8]. Variability is sought to generate code for different plat-forms, different QoS requirements, language variations, or target frameworks. The approach is to define  X  X spects X  (i.e. variability realizations) as higher-order trans-formations, i.e. a transformation whose subject matter is another transformation. Using aspect terminology, the pointcuts denote places in a transformation that may be modified by the high-order transformation (HOT). Advices modify base transformations through additional transformation code to be inserted before, af-ter, or to replace existing code. Likewise, we could rephrase schema co-evolution as a  X  X ariability-in-time X  issue, and use HOTs to isolate schema upgrades. Unfortu-nately, the use of HOTs requires of additional infrastructure: (1) a metamodel for the transformation language at hand, and (2) the appropriate injector/extractor to map from the MOFScript code to the MOFScript model, and vice versa. The availability of these tools is not always guarantee. For instance, MOFScript has both an injector and an e xtractor. However, Acceleo lacks the extractor. Another drawback is generality. It could be possible to develop a HOT for the specific case of WikiWhirl . But we aim at the solution to be domain-agnostic, i.e. applicable to no matter the domain meta-model. Unfortunately, HOTs find difficulties in resolving references to the base transformation X  X  input model where its metamodel is un-known at compile time (i.e. where the HOTs is enacted). Moreover, this approach requires additional infrastructure: (1) SQL schema injectors that obtain a model out of the textual description of the DB s chema and (2), a model differentiation tool that spots the differences among two schema models. This infrastructure is domain-independent.
 Option 4: Automatically Adapting the Transformation. The transforma-tion is engineered for change, i.e. varia ble concerns (i.e SQL scripts) are moved outside the transformation into the adapter. The user does not need to look at the transformation (which is kept unchanged) but at the generated code. SQL skills are sufficient. At runtime, the transformation invokes the adapter in-stead of directly generating the DB code (i.e. the SQL script). This brings two important advantages. First, and unlike the HOT option, the issue of resolving references to the base transformation X  X  input model, does not exist, as references are resolved at runtime by the transformation itself. Second, this solution does not require the model representation of th e transformation (i.e. injectors for the transformation are not needed). On the other side, this approach requires, as option 3, SQL schema injectors and a model differentiation tool.
 Figure 3 outlines the different steps of our proposal. First, DB schemas (i.e. New schema , Old Schema ) are injected as Ecore artifacts (step 1); next, the schema difference is computed (i.e. Difference model ) ( step 2 ) ; finally, this schema difference feeds the adapter used by the transformation (i.e. MOFScript program ). MOFScript is a template-based cod e generator that uses print state-ments to generate code and language instructions to retrieve model elements. Our approach mainly consists of replacing the print statements with invocations to the adapter (e.g. printSQL ). On the invocation, the adapter checks whether the &lt;SQL statement&gt; acts upon a table that is being subject to change. If so, the adapter returns a piece of SQL code compliant with the new DB schema. Upgrades on the MediaWiki X  X  DB schema are well documented 4 . Developers can directly access this documentation to sp ot the changes. Gearing towards autom-atization, these changes can also be ascertained by installing the new release, and comparing the old DB schema and the new DB schema (see Figure 4). The process starts by a notification of a new MediaWiki release (e.g. version 1.19). The developer obtains the model for the new schema ( wikidb119 )aswellasthe model of the schema used in the current release of WikiWhirl ( wikidb116 )using some schema injector (e.g. Schemol). Next, schema differences are computed as model differences (e.g. using EMFCompare). The output is the Difference model.
The Difference model is described as a seque nce of DB operators. Curino et al. proved that a set of eleven Schema Modification Operators (SMO) can completely describe a complex schema evolution scenario. Table 2 indicates the frequency of these change for the MediaWiki case, elaborated from [6]. Fortu-nately, most frequent changes (e.g.  X  X reate table X ,  X  X dd column X ,  X  X rop column X  or  X  X ename column  X ) can be identified from schema differences. Complex changes (e.g.  X  X istribute table X  or  X  X erge table  X ) cannot be automatically detected and therefore are not included in the table. This kind of changes tend to be scarce. For MediaWiki ,  X  X istribute table X  never occurred while  X  X erge table X  accounts for 1,5% of the total changes.
 Schema changes need to be propagated to the generated code through the trans-formation. The transformation delegates to the adapter how the SQL command ends up being supported in the current DB schema. That is, MOFScript X  X  print is turned into the adapter X  X  printSQL (e.g.  X  X rintSQL (&lt;SQL statement&gt;) X  ). On invocation, the adapter checks wheth er the SQL statement acts upon a table that is subject to change (i.e. appears in the Difference model). If so, the adapter proposes an adaptation action to restore the consistency. This adaptation action depends on the kind of change. Similar to other co-transformation approaches (e.g. [2]), changes are classified as (i) Non Breaking Changes (NBC) , i.e., changes that do not affect the transformation; Breaking and Resolvable Changes (BRC), i.e., changes after which the transformations can be automatically co-evolved; and Breaking and Unresolvable Changes (BUC), i.e., changes that require hu-man intervention to co-evolve the transformation. Based on this classification, different contingency actions are undertaken: no action for NBC, automatic co-evolution for BRC, and assisting the user for BUC. Table 2 describes this typol-ogy for DB changes, the usage percentage of each change for MediaWiki [6], and the adaptation counterpart.
Implementation wise, the adapter has two inputs: the Difference model and the model for the new schema (to obtain the full description of new attributes, if applicable). The ZQL open-source SQL parser was used 5 to parse SQL state-ments to Java structures. This parser was extended to account for adaptation functions to modify the statements (e.g. removeColumn ) and support functions (e.g. getTableName ). Figure 5 provides a glimpse of the adapter for the case  X  X emove column  X . It starts by iterating over the changes reported in the Dif-ference model (line 5). Next, it checks (line 6) that the deleted column X  X  table corresponds with the table name of the sta tement (retrieved in lines 3-4). Then, all, the statement, the table name and the removed column are added to a list of parameters (lines 7-10). Finally, the adapter outputs an SQL statement without the removed column, using a function wi th the list of parameters that modifies the expression (lines 12-13). The adaptation process is enacted for each SQLprint statement, regardless of whether the v ery same statement has been previously processed or not. Though this penalizes efficiency, the frequency and the time at which this process is conducted make efficiency a minor concern.

Back to our sample case, the SQL script in Figure 6 is the result of enacting the generation process with wikidiff_v16v19 as the Difference model. Once ref-erences to the variables and the model ele ments have been resolved, MOFScript X  X  printSQL statements invoke the adapter. The adapter checks whether either the tables or the attributes of the printSQL statement are affect ed by the upgrade (as reflected in the Difference model) and applies the appropriate adaptation (see table Table 2). Specifically, the Difference model wikidiff_v16v19 reports: 1. the introduction of three new attributes in the categorylinks table, namely, 2. the deletion of tables math and trackback. This causes the affected printSQL 3. the deletion of attribute user_options in the user table. Consequently, the manual vs the adapter-based approach. To this end, we conducted an assessment on the cost of migrating a WikiWhirl  X  X  MOFScript transformation from version 1.16 to version 1.17 of the MediaWiki DB schema. Table 3 provides some figures of the schema changes and their impact. The experiment was conducted by 8 PhD students who were familiarized with SQL, MOFScript and Ant 6 (4 for the manual and 4 for the assisted).
 Manual Propagation Subjects conducted two tasks: (1) identifying changes between the two versions from the documentation available in the MediaWiki web pages, and (2), adapting manually the transformation. The following equation resumes the main costs: being D : the time estimated for detecting whether the new MediaWiki release impacts the transformation, P : the time needed to P ropagate a single change to the MOFScript code, and #Impacts: the number of instructions in the transfor-mation I mpacted by the upgrade.
 The Experiment. D very much depends on the documentation available. For MediaWiki , designers should check the website 7 , navigate through the hyper-links, and collect those changes that might impact the code. The experiment outputted an average of 38 X  for D MediaWiki , which is not very high due to the subjects being already familiarized with the MediaWiki schema. Next, the de-signer peers at the code, updates it, and checks the generated code. Subjects were asked to provide a default value for the newly introduced columns. On av-erage, this accounts for 4 X  for a single update (i.e. P BRC = 4 X ). Since the 1.17 upgrade impacted 3 MOFScript instructions, this leads to a total cost of 50 X  (i.e. 38 + 4*3). The execution time is considered negligible in both the manual and the assisted options since it is in the order of seconds.
 Assisted Propagation Subjects conducted two tasks: (1) configuration of the batch that launches the assisted adaptation, and (2), verification of the generated SQL script. The batch refers to a macro that installs the new DB release, injects the old schema and new schema, obtains the difference model, and finally, executes the adapter-aware MOFScript code. This macro is coded in Ant and some shell script commands. Running this macro outputs a MOFScript snippet along the lines of the new DB schema. Designers should look at this upgraded code since some manual intervention might still be needed. For instance, the introduction of new columns might also involve the assignment of a value that might not coincide by the one assigned by the adapter. Likewise, column deletion, though not impacting the transformation as such, might spot some need for the data in the removed column to be moved somewhere else. Worth noticing, the designer no longer consults the documentation but relies on the macro to spot the changes in the MOFScript . We assume, the comments generated by th e adapter are expressive enough for the designer to understand the change (Figure 6). On this basis, the designer has to verify the proposed adaptation is correct, and amend it, if appropriate. The following equation resumes the main costs: being C : the time needed to C onfigure the batch; V : the time needed to V erify that a single automatically adapted instruction is correct and to alter it, if applicable.
 The Experiment. It took an average of 5 X  for the subjects to configure the macro (mainly, file paths) to the new DBMS release. As for V, it took an average of 6 X  for users to check the MOFScript code. Therefore, the assisted cost goes up to 11 X .

Similar studies where conducted for other MediaWiki versions. Figure 7 de-picts the accumulative costs of keeping WikiWhirl and MediaWiki in sync. Actu-ally, till version 1.16 all upgrades were handled manually. Ever since, we resort to the macro to spot the changes directly on the generated MOFScript code. Does the effort payoff? Figure 7 shows the break even. It should be noted that subjects were already familiarized with the supporting technologies. This might not be the case in other settings. Skill wise, the manual approach is more demanding on MOFScript expertise while it does not require Ant knowledge. Alternatively, the assisted approach requires some knowledge about Ant but users limit themselves to peer at rather than to program MOFScript transformations.

The cost reduction rests on the existence of an infrastructure, namely, the adapter and the macro. The adapter is domain-agnostic, and hence, can be reused in other domains. On these grounds, we do not consider the adapter as part of the development effort of the WikiWhirl projectinthesamewaythat DBMS drivers are not included as part of the cost of application development. However, there is a cost of familiarizing with the tool, that includes the con-figuration of the batch macro (e.g. DB settings, file paths and the like). We estimated this accounts for 120 X  (reflected as the upfront investment for the as-sisted approach in Figure 7). On these grounds, the breakeven is reached after the third release. The original contribution of this paper is to address, for a specific case study, the issue of transformation co-evolution upon DB schema upgrades. The suitability of the approach boils down to two main factors: the DB schema stability and the transformation coupling (i.e. the number of SQL instructions in the MOFScript code). If the DB schema stability is low (i.e. large number of releases) and the transformation coupling is high, the cost of keeping the transformation in sync, increases sharply. In this scenario, we advocate for a preventive approach where the transformation is engineered for schema stability: MOFScript X  X   X  X rint X  is substituted by the adapter X  X   X  X rintSQL X  . The adapter, using general recovery strategies, turns SQL statements based on the old schema into SQL statements basedonthe new schema.

That said, this approach presupposes that the impact of DB schema changes are confined to the SQL statements without affecting the logic of the transfor-mation itself. In our experience, this tends to be the case for medium-size up-grades. However, substantial changes in the DB schema might require changing the transformation logic. This is certainly outside the scope of the adapter. Next follow-on includes to generalize this approach to other settings where schema evolution is also an issue such as ontologies or XML schemas. Acknowledgements. This work is co-supported by the Spanish Ministry of Education, and the European Social Fund under contract TIN2011-23839 (Scrip-tongue) . Jokin enjoyed a pre-doctoral grant from the Basque Government under the  X  X esearchers Training Program X .

