 Recommender system, as a powerful tool for information filtering, has attracted a lot of interest during the past two decades. And a line of remarkable success have been made in recent years [2]. However, the era of Big Data presents some new challenges.
 Recommendation Based on Implicit User Feedback. Traditional recom-mender systems used to be built based on explicit user feedback, e.g. ratings. However, implicit user feedback exists more widely, e.g. clicks, purchases, and various kinds of user-item interactions. Compared with ratings, these feedback can be collected more easily and inexpensively. There is a big short coming in us-ing the standard recommendation formulation in this situation: the feedback we can observe either is positive or is missi ng. To tackle this problem, researchers have made great efforts to utilize this type of information and gained great achievements [7, 11]. Nevertheless, r ecommendation based on implicit feedback is still a new area which is valuable to be explore.
 Real-Time Recommendation. Timeliness becomes more and more important in this fast changing world. Recommending items in time can greatly improve satisfaction and effectiveness. Although existing dynamic models [4,8] are able to generate high quality recommendations by incorporating time factor. However, coping with fast changing trends in the presence of large scale data might be a challenge, since retraining such batch-training models is costly [6]. Moreover, the batch-training models can not utilize the latest information (e.g. the user feedback in the last few hours), which are valuable for reflecting user X  X  current need. One alternative is to learn the recommendation model online, updating the parameters for each new observation. This calls for an online learning solution.
To address the two aforementioned challenges, this paper focuses on how to efficiently capture user dynamic preference from streaming implicit feedback over time. The main contributions of this work can be concluded as follows:  X  We propose the implicit feedback recommendation model, which flexibly  X  To increase training efficiency and pro vide real-time recommendation results,  X  Comprehensive experiments are conducted to validate the performance of the Recommendation Based on Implicit Feedback. Regarding the observed interactions as positive samples is the common view in the implicit feedback sce-narios. However, the missing ones which contain the interactions in the future can not be considered as negative samples directly. This problem was defined as One Class Collaborative Filtering ( OCCF ) in [10] and Unbalanced Class Prob-lem in [2]. An intuitive idea is to introduce negative samples from missing data. Existing approaches can be broadly classified into two categories: those that randomly sample negative samples from the missing data [3,10], and those that treat all the missing data as negative samples with adding weights on them [10]. However, anyway it is inevitable to introduce noise (cast unobserved positive samples as negative samples) while taking advantage of the  X  X nrated X  infor-mation. 0-1MF [9] and Collaborative Ranking [3,11] are two leading models to leverage  X  X nrated X  information. 0-1MF defines the positive sample R ij =1if user i hasinteractedwiththeitem j , otherwise the negative sample R ij =0. Then the problem is converted to predi ct the unobserved entries based on the observed  X  X atings X . Collaborative Ranking is the pairwise model which aims to find the correct ranking order by conv erting the rank task to maximize the probability that positive samples rank before randomly sampled  X  X egative X  ones [11].
 Real-Time Recommendation. Extended from traditional matrix factoriza-tion model, dynamic recommendation models like timeSVD++ [8] and DMF [4] can improve recommendation accuracy by incorporating time factor. However, these dynamic models have limitations on both effectiveness and training effi-ciency since they are trained off-line in batch mode. Online Collaborative Fil-tering (OCF) [1] techniques have been explored in recent years. By applying the simple online gradient descent (OGD) algorithms, OCF avoids the highly expensive re-training cost of traditional batch matrix factorization algorithms. Based on pairwise ranking methodology, RMFX [6] follows a selective sampling strategy to perform online model updates based on active learning principles. In this section, we first present IFRM, which is tailor-made for implicit feedback recommendation scenarios. And then, we extend IFRM to an online version. 3.1 Implicit Feedback Recommendation Model Following the common assumption in Collaborative Filtering, We also believe that the degree that how a user prefer an item can be measured. In the im-plicit feedback recomme ndation scenarios, we de note this  X  X egree X  as Adoption Tendency Score (ATS) which characterizes the tendency for a user to adopt (in-teract with) an items. However, such score itself is meaningless, since the users comparatively select items in pract ice. For example, for a given user i and item j , the tendency score that i adopts j , which is denoted A i,j ,is5.Itishard to say i would adopt j or not. For user i ,thescoretoitem j is useful only by comparing with the scores of othe r items. Generally speaking, if A i,j is obviously larger than most ATS with respect to user i ,user i may adopt item j with a high probability. Following this idea, for a common user, we assume that the behavior that he or she interacts with a target item is determined by Relative Adoption Tendency .
 Definition 1. The Relative Adoption Tendency  X  ij = A i,j tendency for user i to adopt the item j ,where A i,j is the ATS for user i to adopt item j ,and A i is the average ATS with respect to user i , which also serves as  X  X egative prototype X . The higher  X  ij is, the higher probability for user i to adopt item j is.

A i,j can be regarded as a function with respect to user i and item j .Itcanbe designed differently according to different assumptions or application scenarios. Before develop it carefully, we will first present the overview of IFRM.
Inspired by [11], to transform  X  ij to a probability, we normalize it to [0 , 1] interval by utilizing sigmiod function  X  ( x )= x 1+ x . The probability that user i adopts item j is: Note that if  X  ij =1, P ij =0 . 5 which indicates that user i shows the same adop-tion tendency with respect to item j compared with the most  X  X ormal X  items. This property is useful to update model online (see the following subsection).
We denote O = { &lt;i,j&gt; | user i adopted item j } which is the observed adoption set. By combining all the observed adoptions, we derive the likelihood of observed O as: where  X  is the parameters in the model.

The Bayesian formulation of finding appropriate ATS is to maximize the fol-lowing posterior probability: where P (  X  ) is a normal distribution with zer o mean and variance-covariance matrix  X   X  .

The log of the total probability over the observations is given as follows: lnP (  X  | O )= ln To maximize the log-posterior over the observations is equivalent to minimize the proposed objective function in Eq.4 with hybrid quadratic regularization terms. where  X   X  are model specific regularization pa rameters. A local minimum of the objective function given by Eq.(4) can be found by performing gradient descent in model parameters  X  , which are iteratively updated.

In the rest of this subsection, we will introduce how to design Adoption Ten-dency Score A i,j in common and present a unified model which flexibly integrate various kinds of usual information. We intuitively design A i,j from three parts, including 1) the intrinsic preference wh ich is determined by both user X  X  latent features U i and item X  X  latent features V j , 2) the influence of context which can be estimated by user X  X  personalized sensitivity S i and item X  X  features F j ,and 3) the influence of social environment which can be modelled by the personal degree of user X  X  conformity C i and the popularity of item Pop ( j ) in a specific social environment. Thus, A i,j can be formally defined as: For good generalization capability, we consider Pop ( j ) in a global environment rather than a local social environment. Because the latter needs extract knowl-edge (e.g social relationship or social interactions) which is not always available. In this paper, we simply use Temporal Popularity of item j , which is denoted as TP ( j ), to take the place of Pop ( j ). TP ( j ) can be estimated by counting the adoptions with respect to item j in a sliding time window.
 Finally, we formally present the op timization objective of IFRM:
From Eq.6 we can see that advioding sampling negatives, IFRM naturally address One-class problem. This property make it much more suitable for online learning, since each observation can be learnt separately. 3.2 Online Implicit Feedback Recommendation Model Inspired by [1], we further extend IFRM to an online version by performing On-line Gradient Descent(OGD) on loss function L with respect to model param-eters U , V ,and C . Different with standard Stochastic Gradient Descent (SGD), OGD processes samples one pass. Thus, the key issue of performing OGD is how to determine the learning step T for every sample, since OGD cant go back and learn again. State-of-the-art methods [1,6] adopt a fixed T for all samples. The drawbacks are obvious: 1)The parameter T needs to be set and tuned in practice. 2)There is not a reasonable setting of T for all feedback, since the value of user feedback is different. 3)The online model can be easily affected by noise since we equally treat all the feedback including noisy ones. Unfortunately, noise is inevitable in this setting: On one hand, implicit feedback data like clicks can be more noisy for those hostile attacks launched by a robot or crawler. On the other hand, it is hard to perform data cleaning on data stream online. To overcome the three drawbacks, we design an online learning mechanism which dynamically and automatically adjusts the learning rate for each feedback. By assuming the value of user feedback is different, oIFRM reinforces to learn the new trend while weakens to learn the habitual feedback and noise. By comparing user adoption probability and user behavior confidence, oIFRM can distinguish between valuable feedback(new trend), habitual feedback(well-learnt knowledge) and noisy feedback. To be specific, oIFRM reinforces to learn those feedback with lower occurrence probability and higher user confidence, weakens to learn those feedback with higher occurrence probability or with lower confidence. Note that the probabilistic framework of IFRM has provided the  X  X ser adoption probabil-ity X . We develop a new component  X  i to describe the confidence of an observed behavior with respect to a user i .

The online learning algorithm of oIFRM is described as in Algorithm.1. At the very beginning, we finish the initialization work and perform a burn-in training process. The burn-in training process is designed to fulfill Temporal Popularity component and to make the online model available to give a prediction. In our experiment, the parameter W is set to 10 5 . We utilize a simple method to adjust  X  . For each user i , we maintain a pool to remember all the P ij (Eq.1) in the past. by averaging all the P i  X  in the pool, we can obtain the current  X  i .Note that the average ATS should be computed based on updated parameters. In practice, we can cache a few temporary variables(including average V , average F , and average popularity) for acceleration.
 Algorithm 1. online Implicit Feedback Recommendation Model In this section, we conduct experiments to evaluate the performance of the pro-posed models by comparing with the state-of-the-art method. 4.1 Datasets We base our experiments on two real-world datasets LastFM and Douban. The basic statistics of the datasets are summarized in Table 1.

Last.fm is a popular music website. Music listening is one of the most classic forms of implicit user feedback. Furthermore, the preference of users are drifting more casually and the new songs are emerging at all times, which calls for real-time recommendation eagerly. As a result, it is very suitable for evaluating the performance of different online recommendation methods based on implicit feed-back in real-time recommendation scenarios. This dataset represents the whole listening habits of 132 users from Feb. 14th to Aug. 1st in 2005. Over three hun-dreds of thousands &lt; user, artist, song, timestamp &gt; tuples are collected from Last.fm API(http://www.last.fm/api). We consider the songs as items and the artistsasfeatures.

Douban is one of the largest Chinese socia l review sites where users can review movies and provide ratings to them. Douban also allows the user to add a movie into watched list or wish list without giving an explicit rating score. We consider information, we further crawl the webpages of the relative movies from Douban website and extract 14496 features in total, including Actors(14233), Genres(37), Language(130) and Location(96). The trivial process of feature extraction is omitted for limited space. 4.2 Experimental Protocol We design an online evaluation architecture to evaluate the performance of dif-ferent models in an online fashion. For each user feedback in chronological order, following the conventional widely-used evaluation strategy in most works [5, 6] for implicit feedback recommendation, we randomly sample 1000 items as ir-relevant ones and construct a candidate list which contains these 1000 items together with the one really interact with at the moment. We randomly shuffle the constructed candidate list as the input of various recommendation models. The goal is to rank the item which users really interact with in front of the randomly sampled ones. Finally, we evaluate the recommendation quality based on the recommendation lists which are also the output of different models.
We use three popular evaluation metrics Rank , Hit Ratio and Coverage to measure and compare the performance of various recommendation models. Rank measures the percentile position of the relevant item in the whole recommen-dation list. For example, if the relevant item is ranked at the top of the rec-ommendation list, then Rank =0%.And Rank = 100% if the relevant item is the last one in the recommendation list. Hit Ratio is a precision metric. If the relevant item is in the Top-N ,thenwehavea hit ,otherwisewehavea miss . To estimate the Coverage , we calculate the proportion of recommended items among the whole item set. We average the Rank , Hit Ratio ,and Coverage scores of every 5000 sequential recommendation lists to obtain a statistical evaluation result.
 We implement following models as comparative methods. batch Implicit Feedback Recommendation Model (bIFRM) is the off-line version of our proposed model. In our experiment, for every 50000 passed samples, we add these 50000 new samples into training set and retrain IFRM. And the model will be retain to make the next 50000 times recommendations. Temporal Popularity (TP) is the simplest but effective method for online recommendation. By sliding a window, TP always recommends the mos t popular items in recent time period. Note that it is also a component in our model. Stream Ranking Matrix Factorization (RMFX) [6] is the state-of-the-art method to make real-time recommendation based on implicit user feedback. 4.3 Results Accuracy and Diversity. We enforce all the comparative methods to make real-time Top-N recommendation before each user-item interaction occurred. Note that the Hit Ratio and Coverage metrics evaluate the performance of Top-N recommendation. We respectively set N =5 , 10 , 20 , 30 and conduct a series of experiments. Since the overall performan ces of various models are consistent for all the different N , for limited space, we only report the results when N =30in the following section.
 To show the dynamic performance of d ifferent methods over time, we plot Fig.1. The observations can be summarized into the following points: (1) Online models generally achieve higher recommendation accuracy with lower Rank and higher Hit Ratio . In the scenarios like music-listening where user X  X  interest is drifting fast and items update continually, the online recommendation models show significant superiority for capturing the temporal dynamics immediately. In the scenarios like movie-watching where user X  X  taste is relatively stable and items update occasionally, the performance gain of implementing recommenda-tion models online is not much obvious. On both datasets, the online version oIFRM always performs better than the batch version bIFRM. Even at the very first few evaluations, their performances are significantly different. Note that af-ter the burn-in process, bIFRM and oIFRM are similar. However, oIFRM learns fast from the latest user feedback and utilizes more feedback information while bIFRM is static. Consequently, it is much valuable to design an online learn-ing method for higher recommendation accuracy. (2) Our models make more diverse recommendations meanwhile retain relatively high a ccuracy. Personal-ization plays an important role. oIFRM adopts the common idea of modelling users X  personal preference. Furthermore, it models users X  personal sensitivity and conformity, which introduce more personality. Even though the performance of TP over accuracy is comparable to our models, the advantage of our models over diversity is obvious, since TP always generates non-personalized recommenda-tions. RMFX is basically a pairwise ranking approach, which makes it tend to recommend popular items (the latent f eatures of those items are generally bigger). Thus, the Coverage RMFX achieves is relatively low. A reasonable rec-ommendation model always tends to recommend popular items, because they have high chances to be chosen by users. However, always predicting popular items will narrow users X  horizon and hide the value of the Long Tail. Interpretability and Robustness. Interpretability is another requirement of recommender system. We will show some pr operties of oIFRM from this perspec-tive and verify the utility of introduced components, namely users X  personalized conformity C and Confidence  X  .
 Fig.2 depicts the changing trends of user conformity over time. We plot the Maximum, the Average, and the Minimum value of C at each time-stamp. From these three basic statistics, we can draw s ome interesting conclusions. Generally speaking, the conformity of the users on Last.fm is much less than that on Douban, since the average conformity value of Last.fm is around 0 . 7 (and still decreasing) while that on Douban is around stable 1 . 0. The reason is that most people tend to watch popular movies but few people spend time on enjoining the minority (From Fig.2 (b) we observe that the average line is very close to the maximum line). On the contrary, there are still considerable part of people who tend to follow their own tastes to choose distinctive songs to listen. The difference between the Maximum line and the Minimum line increases over time show that our online recommendation model can gradually capture the degree of different users X  personalized conformity.

Another advantage of our model is the robustness to hostile attack. As we mentioned before, since it is hard to process data cleaning online, the capacity to tolerate noises (especially attacks) is very important to an online recommenda-tion model, especially the one constructe d from implicit user feedback (a crawler can easily  X  X lick X  a mass of items within a small time period). One common type of attack is random visit by a robot (or crawler). Thus, we conduct experiments by simulating the attacks in practice. Firstly, we randomly choose two time point (around the 80000th and the 160000th sample). At each time point, we simulate the random visit behavior of one chosen user. Specifically, we insert the  X  X andom visit noises X  (10000 randomly sampled interactions) into sample stream. Finally, our model and RMFX are trained(updated) by the noisy sample stream. We plot the statistics of confidence  X  in Fig.3 (a) and the difference over Hit Ratio in Fig.3 (b). From (a), we can observe that the confidences of the two chosen users indeed decrease at the inserted time point respectively. This verifies the effectiveness of the introduced confidence parameters  X  . In practice, we can detect the anomaly users by tracking varying  X  . Fig.3 (b) shows that oIFRM can tolerate the inserted noises. On the contrary, the performance of RMFX can be significantly affected under attacks. The reason is that adopting a fixed learning step, RMFX considers every observed f eedback as positive sample and fails to  X  X ilter out X  those low-quality samples.
 Throughput. Another important indicators of online system is throughput. If the update process cant catch up with data stream, the online system would fail. As a result, we examine the average time cost for one update operation. The experiments are conducted on the same computer with double Core i3 CPU and 2G RAM. All the comparative algorithms are implemented in Python 2.7. The statics are concluded in Table 2. The batch models are very time-consuming compared with the online models. It is meaningless to compare these two clus ters of models directly. Consequently, we compare the three online methods TP, oIFRM and RMFX. Since the di-mension of latent features can affect the running time, we conduct experiments repeatedly on different dimensions D =5 , 50 , 200. We run the program 10 times and obtain the average statical results. Surprisingly, the results show that the time cost of our model is insensitive to D . The reason is that higher dimension of latent features leads to more personality. oIFRM dynamically adjusts confidence  X  which is also the controller of iteration times. Thus, more personalized predic-tions at the beginning will decrease  X  and then reduce average iteration times. We implement TP by using a queue and an index, which makes TP the most efficient online model (time complexity O (1)). oIFRM achieves lower time cost compared with RMFX. Because RMFX spend some time to sample negative items from reservoir to co nstruct a pair before updating. oIFRM can directly calculate gradient without sampling. The update process can be more efficient if we cache a few temporary variables.

In our configuration, the throughput of our recommender system is around 429 updates per second. It can be certa inly improved by implementing in C, sampling from data stream, or parallel computing [12].
 This paper addresses two problems in recommender systems, namely 1) recom-mendation based on implicit user feedback and 2) real-time recommendation in a stream setting. To overcome One-class problem, we first propose IFRM which is tailor-made for implicit feedback recommendation. Moreover, IFRM is flexible to incorporate side information such as item features or item temporal popularity. To increase training efficiency and capture user dynamic preference in time, we extend IFRM to an online version, oIFRM. By assuming the value of user feedback is different, oIFRM automatically adopts different learning step for each feedback. By comparing user adoption probability and user behavior confidence, oIFRM reinforces to learn the new trend while weakens to learn the habitual feedback and noise. Finally, comprehensive experiments conducted on two real-world datasets have showed that oIFRM has an obvious superior per-formance over the state-of-the-art approach from various perspectives, including accuracy, efficiency, diversity, interpretability, and robustness.

