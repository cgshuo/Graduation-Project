 rgangadh@andrew.cmu.edu In EBMT, the source sentence to be translated is matched against the source language sentences present in a corpus of source-target sentence pairs. When a partial match is found, the corresponding target translations are obtained through subsenten-tial alignment. These partial matches are put to-gether to obtain the final translation by optimizing translation and alignment scores and using a statisti-cal target language model in the decoding process. Prior work has shown that EBMT requires large amounts of data (in the order of two to three mil-lion words) (Brown, 2000) of pre-translated text, to function reasonably well. Thus, some modification of the basic EBMT method is required to make it ef-fective when less data is available. In order to use tion 4 lists results obtained in a full evaluation of the algorithm. Section 5 concludes and discusses direc-tions for future work. Using a bilingual dictionary, usually created using statistical methods such as those of (Brown et. al., 1990) or (Brown, 1997), and the parallel text, a rough mapping between source and target words can be created. This word pair is then treated as an in-divisible token for future processing. For each such word pair we then accumulate counts for each to-ken in the surrounding context of its occurrences (N words, currently 3, immediately prior to and N words immediately following). The counts are weighted with respect to distance from occurrence, with a linear decay (from 1 to 1/N) to give great-est importance to the words immediately adjacent to the word pair being examined. These counts form a pseudo-document for each pair, which are then con-verted into term vectors for clustering.

In this paper, we compare our algorithm against the incremental GAC algorithm(Brown, 2000). This method examines each word pair in turn, comput-ing a similarity measure to every existing cluster. If the best similarity measure is above a predeter-mined threshold, the new word is placed in the cor-responding cluster, otherwise a new cluster is cre-ated if the maximum number of clusters has not yet been reached. Spectral clustering is a general term used to de-scribe a group of algorithms that cluster points using the eigenvalues of  X  X istance matrices X  obtained from data. In our case, the algorithm described in (Ng. et. al., 2001) was performed with certain variations that were proposed by (Zelnik-Manor and Perona, 2004) to compute the scaling factors automatically and for the k -Means orthogonal treatment (Verma and Meila, 2003) during the initialization. These scaling factors help in self-tuning distances between points according to the local statistics of the neigh-borhoods of the points. The algorithm is briefly de-scribed below. 1. Let S = s (non-convex in the term vector domain) could be useful in a real translation experiment. Some ex-ample classes are shown in Table 1. The first class in an intuitive sense corresponds to measure-ment units. We see that in the &lt; units &gt; case, GAC misses some of the members which are ac-tually distributed among many different classes and hence these are not well generalized. In the second class &lt; months &gt; , spectral clustering has primarily the months in a single class whereas GAC adds a number of seemingly unrelated words to the clus-ter. The classes were all obtained by finding 80 clusters in a 20,000-sentence pair subset of the IBM Hansard Corpus (Linguistic Data Consortium, 1997) for spectral clustering. 80 was chosen as the number of clusters since it gave the highest BLEU score in the evaluation. For GAC, 300 clusters were used as this gave the best performance.

To show the effectiveness of the clustering meth-ods in an actual evaluation, we set up the following experiment for an English to French translation task on the Hansard corpus. The training data consists of three sets of size 10,000 (set1), 20,000 (set2) and 30,000 (set3) sentence pairs chosen from the first six files of the Hansard Corpus. Only sentences of length 5 to 21 words were taken. Only words with frequency of occurrence greater than 9 were chosen for clustering because more contextual information would be available when the word occurs frequently and this would help in obtaining better clusters. The test data was chosen to be a set of 500 sentences ob-tained from files 20, 40, 60 and 80 of the Hansard corpus with 125 sentences from each file. Each of the methods was run with different number of clus-ters and results are reported only for the optimal number of clusters in each case.

The results in Table 2 show that spectral clus-tering requires moderate amounts of data to get a large improvement. For small amounts of data it is slightly worse than GAC, but neither gives much im-provement over the baseline. For larger amounts of data, again both methods are very similar, though spectral clustering is better. Finally, for moderate amounts of data, when generalization is the most useful, spectral clustering gives a significant im-provement over the baseline as well as over GAC. By looking at the clusters obtained with varying amounts of data, it can be concluded that high pu-10k 3.33 50 1.37 20 20k 22.47 300 29.08 80 30k 2.88 300 3.88 200 rity clusters can be obtained with even just moderate amounts of data. From the experimental results we see that spectral clustering leads to relatively purer and more intu-itive clusters. These clusters result in an improved BLEU score in comparison with the clusters ob-tained through GAC. GAC can only collect clusters in convex regions in the term vector space, while spectral clustering is not limited in this regard. The ability of spectral clustering to represent non-convex shapes arises due to the projection onto the eigen-vectors as described in (Ng. et. al., 2001).
As future work, we would like to analyze the variation in performance as the amount of data in-creases. It is widely known that increasing the amount of training data in a generalized EBMT sys-tem eventually leads to saturation of performance, where all clustering methods perform about as well as baseline. Thus, all methods have an operating re-gion where they are the most useful. We would like to locate and extend this region for spectral cluster-ing.

Also, it would be interesting to compare the clus-ters obtained with spectral clustering and the Part of Speech tags of the words in the same cluster, espe-cially for languages such as English where good tag-gers are available.

Finally, an important direction of research is in automatically selecting the number of clusters for the clustering algorithm. To do this, we could use information from the eigenvalues or the distribution of points in the clusters.
 This work was funded by National Business Center award NBCHC050082.

