 Myroslava O. Dzikovska 1  X  Rodney D. Nielsen 2  X  Claudia Leacock 3 Abstract We present the results of the joint student response analysis (SRA) and 8th recognizing textual entailment challenge. The goal of this challenge was to bring together researchers from the educational natural language processing and com-putational semantics communities. The goal of the SRA task is to assess student responses to questions in the science domain, focusing on correctness and com-pleteness of the response content. Nine teams took part in the challenge, submitting a total of 18 runs using methods and features adapted from previous research on automated short answer grading, recognizing textual entailment and semantic tex-tual similarity. We provide an extended analysis of the results focusing on the impact of evaluation metrics, application scenarios and the methods and features used by the participants. We conclude that additional research is required to be able to leverage syntactic dependency features and external semantic resources for this task, possibly due to limited coverage of scientific domains in existing resources. However, each of three approaches to using features and models adjusted to application scenarios achieved better system performance, meriting further inves-tigation by the research community. Keywords Student response analysis Short answer scoring Recognizing textual entailment Semantic textual similarity Accurate assessment and feedback is a critical requirement of any effective educational program. Teachers spend large amounts of time providing feedback on homework and grading student responses to assessment questions. In recent years, the number of online educational applications has been growing steadily X  X ncluding intelligent tutoring systems, e-learning environments, distance education and massive open online courses (MOOCs). To operate effectively, such applications require automated feedback and grading support, which requires processing natural language input.
The student X  X  natural language input can be evaluated on multiple levels, differentiating, in particular between style and content, and between summative assessment and formative feedback (see Sect. 2 ). Student response analysis (henceforth SRA) is the task of evaluating the content of natural language student responses, and labeling them with categories that could help an educational application generate either formative or summative feedback.

In a typical setting, instructional designers create a repertoire of questions that the system can ask a student, together with reference answers (see Table 1 for examples). For each student response, the system needs to decide on the appropriate tutorial feedback, either confirming that the student was correct, or providing additional help to indicate how their response is flawed and help the student improve. This task requires semantic inference, for example, to detect when the students are explaining the same content but in different words, and is linked to recognizing textual entailment (RTE) and semantics textual similarity (STS) tasks. Thus, Dzikovska et al. ( 2012b ) proposed SRA as a new shared task for the NLP community that would allow computational semantics methods to be tested in the context of a novel practical task.
 This paper reports on the results of the shared student response analysis task at SemEval-2013 (SemEval Task 7), aiming to bring together the educational NLP and the semantic inference communities, in the context of the SRA task. 1 We describe the task, datasets and results, and provide an analysis of the results focusing on different application scenarios and features used by the participants. This work builds upon an initial analysis of the evaluation data (Dzikovska et al. 2013b ), which showed that it is not possible to determine a single  X  X  X est X  X  system, as different systems performed best on different test sets and metrics. We extend this initial analysis by examining the impact of evaluation metrics, application scenarios and the methods used by the participants. This detailed analysis can provide a basis for future evaluations of similar educational NLP tasks.

We begin by describing previous shared tasks in automated writing evaluation (Sect. 2 ) and discussing the novel features of the SRA task compared to previous shared tasks. We also briefly discuss the relationship between SRA and other semantic inference tasks. We describe our evaluation corpus, metrics and baselines (Sect. 3 ). We then analyze evaluation results along three dimensions (Sect. 4 ). The original SRA task proposed three different candidate evaluation metrics with different theoretical properties (Dzikovska et al. 2012b ). We show that in practice the system rankings were correlated across metrics (Sect. 4.2 ) and across task variants using different numbers of targeted feedback classes (Sect. 4.3 ). Next, we compare system performance across different application task settings (fully in-domain data versus transfer to new questions and domains, Sect. 4.4 ), and across different implementation features used by participants (syntactic features, semantic features and adaptation to the application setting, Sect. 4.5 ). We discuss the implications of our results for future NLP research on SRA in Sect. 5 . 2.1 Shared tasks in automated writing evaluation The goals of automated writing evaluation (AWE) vary by application: automated grammatical error detection and correction (GEC), automated essay scoring (AES), and automated short answer grading (ASAG).

While GEC applications have been developed with the goal of helping writers, from the Unix Writer X  X  Workbench (MacDonald et al. 1982 ) to the present, GEC is also used to evaluate an essay writer X  X  grasp of grammar, usage, mechanics and other writing conventions (Burstein et al. 2013 ). In recent years, GEC X  X  primary focus has shifted from errors made by native speakers to those most in need of it: English language learners (Leacock et al. 2014 ) which has led to four shared tasks in the NLP community that focus on grammatical error detection for English language learners (Dale and Kilgarriff 2011 ; Dale et al. 2012 ; Ng et al. 2013 , 2014 ). As a sideline to GEC for language learners, a new shared task for Native Language Identification (NLI) was introduced (Tetreault et al. 2013 ) X  X ince knowing the native language of a writer can help predict the kinds of errors likely to be made. For example, if the native language contains no articles ( a, an, the ), then the writer is likely to make article errors in their written English.

In the AES task, the goal is typically to evaluate the quality of writing, in particular essay structure and coherence, and on writing proficiency represented by correct spelling and grammar. The earliest program for scoring essays was developed by Ellis Page in 1996 (Page 1996 ). By now automated essay scoring systems are being used by most, if not all, major educational assessment companies (Shermis and Burstein 2013 ).
 To aid the effective comparison of AES methods, the William and Flora Hewlett Foundation announced a series of shared tasks for automated scoring in 2012 X  offering $100,000 in prizes for each task. The competition was called ASAP (Automated Student Assessment Prize) that was administered by Kaggle ( https://www.kaggle.com/c/asap-aes ). The first phase was for scoring long-form constructed responses (essays). The initial essay data set consisted of six extended-essay prompts: three for  X  X  X raditional X  X  genres (persuasive, expository and narrative essays) and three for source-based essays, where the essay has to be developed based on several reading passages (sources) X  X sually written in different styles and often presenting conflicting opinions. These competitions opened up AES to the world at large, including many machine-learning experts with little or no previous experience with NLP, providing shared data and annotations necessary to facilitate progress in the field. However, especially for the  X  X  X raditional X  X  genres, the essay prompts did not have an expected  X  X  X orrect X  X  answer, and thus the factual correctness and completeness of the specific content were of lower importance for the AES ASAP task.

In contrast, it has long been understood that, especially in STEM (science, technology, engineering and mathematics) subjects, deeper semantic processing is required in order to score student short-text responses (Leacock and Chodorow 2003 ; Pulman and Sukkarieh 2005 ; Nielsen et al. 2009 ; Mohler et al. 2011 ). In applications such as assessment scoring and intelligent tutoring systems, each question posed to the student comes with one or more expected correct answers provided by the content experts ( X  X  X eference answer X  X ). The student is required to cover the relevant points in the reference answer set completely and correctly for their response to receive full credit. Therefore, a system that analyzes student responses to such questions needs to assess the semantic content of the response in some way, in order to establish entailment or similarity between the content of the student response and the content of the reference answer.

This task is called automatic short answer grading task (ASAG) and involves assessing factual correctness and completeness of student answers. A detailed review of ASAG research is presented in Burrows et al. ( 2015 ). The paper identifies three key dimensions of ASAG tasks that differentiates them from AES tasks: they assess recall of facts, focus on content and not on style, and expect the answer to be one phrase to one paragraph long. The SRA task presented in this paper is a typical example of an ASAG task. In the rest of the section, we discuss the ASAG task administered as part of the ASAP competition and on the contribution that the SRA task makes to the field.

The second phase of the ASAP competition focused on scoring short-form constructed responses (short answers). The data set consisted of ten short-text prompts covering general science, biology, and English. Sample sizes ranged from about 2100 to 3000 student responses per item. With such large amounts of data, statistically-based machine learning methods can be used to score short answers relying on the large set of training samples to cover the possible range of student responses.

The SRA task attempts to address two important issues that were not covered by the previous shared tasks, motivated by the needs of educational applications such as homework assessment or intelligent tutoring. The numeric scores in The Hewlett Foundation competitions represent summative assessment needs X  X hat of grading large-scale assessments.

In formative assessments, such as homework grading or intelligent tutoring, detailed information that goes beyond numeric scores is required for providing effective feedback (Nielsen et al. 2008b ; Dzikovska et al. 2012b ). Thus, the SRA task focuses on associating student responses with categorical labels that can be used in providing feedback, labeling student responses as correct, partially correct but incomplete, contradictory to the expected answer, in-domain but not providing content relevant to the answer, and out-of-domain and garbled responses (e.g., swear words, random characters typed). These categories are described in Sect. 3 . They provide a basis for giving students more specific directive feedback, for example, that their response is substantially correct but needs to be extended, or there are factual errors that need to be addressed (Dzikovska et al. 2012b , 2013a ). 2
Secondly, the SRA task is set up to evaluate the application performance in scenarios where limited data are available. Having a shared and annotated data set with many student responses to the same question, as in the short answer grading task, is obviously of great value to the community, allowing for comparison and improvement of existing techniques. However, outside the summative exam assessment community, much of the work on providing detailed feedback is motivated by the needs of intelligent tutoring systems and e-learning systems (Graesser et al. 1999 ; Glass 2000 ; Pon-Barry et al. 2004 ; Jordan et al. 2006a ; VanLehn et al. 2007 ; Dzikovska et al. 2010 ). If one wanted to use SRA techniques in courses authored and conducted in typical classroom settings, large quantities of sample responses as seen in the Hewlett challenge are difficult or impossible to obtain. Moreover, instructors may want to author new questions or change existing ones as a course develops.

Thus, in addition to a common assessment scenario where multiple possible student responses are collected for each questions, there is need for systems that can operate on newly added questions in the same domain for which graded student responses are not yet available. An ideal assessment system would also be able to function in new domains, for example, by using textual entailment or semantic similarity between the student answer and the reference answer, without the need to collect large amounts of question-specific data.

The SRA challenge at SemEval 2013 was set up to evaluate the feasibility of building systems that could operate more or less unchanged across a range of domains and question-types, requiring only a question text and a reference answer for each question. It therefore provides test sets that evaluate three different application scenarios. Each system X  X  performance is evaluated on an in-domain task where labeled data are provided for each question, and on transfer tasks where we look at system performance on previously unseen ( X  X  X ewly authored X  X ) questions and domains. The corresponding test sets are discussed in more detail in Sect. 3.4 . 2.2 Relation to other computational semantics tasks When the SRA task was first proposed, we noted that it has a strong relationship to the RTE task. The RTE community has a tradition of evaluating the RTE tasks within application contexts. Initially, information retrieval, question answering, machine translation and information extraction tasks were used (Dagan et al. 2006 ; Giampiccolo et al. 2008 ), followed by summarization and knowledge base population (Bentivogli et al. 2009 , 2010 , 2011 ).

In a typical response analysis scenario, we expect that a correct student response would entail the reference answer, while an incorrect response would not. More precisely, since students often skip details that are mentioned in the question text, the combination of the question text and student response text should entail the reference answer. An initial feasibility study concluded that labels assigned by human annotators in educational settings align sufficiently well with entailment judgments of annotators previously involved in RTE tasks (Dzikovska et al. 2013b ).
We therefore decided to formally engage the RTE community by providing an additional set of subtasks which used simplified 2-and 3-class labeling schemes similar to 2-and 3-way entailment judgments used in the previous RTE challenges. These labeling subtasks are discussed in more detail in Sect. 3 . The challenge for the textual entailment community was to address the answer analysis task at varying levels of granularity, using textual entailment techniques, and explore how well these techniques can help in this real-world educational setting.
 The task participants were obviously not restricted to only using RTE methods. Another clearly related task is the semantic textual similarity (STS) task (Agirre et al. 2012 , 2013 ). Clearly, any correct student response is expected to be semantically similar to the reference answer, or, more precisely, the combination of question and student response should be similar to the reference answer. However, the challenge for STS systems is to map the numeric judgments of textual similarity into categorical judgments of correctness. This may not always be obvious X  X or example, for a simple similarity metric based on the LSA approach, two contradictory sentences may appear to be very similar if they share most of the content words.
In practice, most of the systems entered into the task used some measure of semantic similarity as one of the features, and several systems designed for STS participated in this task as well. We discuss the range of NLP techniques used by participants in Sect. 4.1 , and analyze the impact of commonly used features in Sect. 4.5 . 3.1 Student response analysis corpus We used the student response analysis corpus (henceforth SRA corpus) (Dzikovska et al. 2012b ) as the basis for our data set creation. The corpus contains manually labeled student responses to explanation and definition questions typically seen in practice exercises, tests, or tutorial dialogue.

Specifically, given a question, a known correct  X  X eference answer X  and a 1-or 2-sentence  X  X tudent response X , each student response in the corpus is labeled with one of the following judgments:  X   X  X orrect X , if the student response is a complete and correct paraphrase of the  X   X  X artially_correct_incomplete X , if it is a partially correct response containing  X   X  X ontradictory X , if the student response explicitly contradicts the reference  X   X  X rrelevant X  if the student response is talking about domain content but not  X   X  X on_domain X  if the student utterance does not include domain content, e.g.,  X  X  X  The examples for all five classes are shown in Table 1 .

The SRA corpus consists of two distinct subsets: B EETLE data, based on transcripts of students interacting with B EETLE II tutorial dialogue system (Dzikovska et al. 2010 ), and S CI E NTS B ANK data, based on the corpus of student responses to assessment questions collected by Nielsen et al. ( 2008b ).
The B EETLE corpus consists of 56 questions in the basic electricity and electronics domain requiring 1-or 2-sentence answers, and approximately 3000 student responses to those questions. The S CI E NTS B ANK corpus contains approximately 10,000 responses to 197 assessment questions in 15 different science domains (after filtering, see Sect. 3.2 ).

Student utterances in the B EETLE corpus were manually labeled by trained human annotators using a scheme that straightforwardly mapped into SRA annotations. The annotations in the S CI E NTS B ANK corpus were converted into SRA labels from a substantially more fine-grained scheme by first automatically labeling them using a set of question-specific heuristics and then manually revising them according to the class definitions (Dzikovska et al. 2012b ). We further filtered and transformed the corpus to produce training and test data sets as discussed in the rest of this section. 3.2 Data preparation and training data In preparation for the task, four of the organizers examined all questions in the SRA corpus, and decided to remove some of the questions to make the dataset more uniform. We observed two main issues. First, a number of questions relied on external material, e.g., charts and graphs. In some cases, the information in the reference answer was sufficient to make a reasonable assessment of student response correctness, but in other cases the information contained in the questions was deemed insufficient and the questions were removed.

Second, some questions in the S CI E NTS B ANK dataset could have multiple possible correct answers, e.g., a question asking for any example out of two or more unrelated possibilities. For example, consider the following question: Q: Sue, Linda, and Arletta, each got matching plant pots and the same amount of the same kind potting soil. Each girl planted 8 sunflower seeds in the soil in her pot. Then each girl took her pot home to watch the growth of the plants. After 10 days they brought their plant pots back to school and compared the results. The picture to the left shows their results. Describe 2 ways the results are different. Its reference answer is The number of plants is different. The plant heights are different. The number of leaves is different. To answer correctly, the student is expected to name only two out of three components of the reference answer; thus, the student response and the question will neither entail nor be semantically similar to all parts of the reference answer. Such questions present additional challenges for RTE and STS methods, beyond those outlined in Sect. 2.2 . They were therefore removed from the test data for our task, though they should be considered in the future SRA work.
Finally, parts of the data were re-checked for reliability. In B EETLE data, a second manual annotation pass was carried out on a subset of questions to check for consistency. In S CI E NTS B ANK , we manually re-checked the test data. The automatic conversion from the original S CI E NTS B ANK annotations into SRA labels was not perfectly accurate (Dzikovska et al. 2012b ). We did not have the resources to check the entire data set. However, four of the organizers jointly hand-checked approximately 100 examples to establish consensus, and then one organizer hand-checked all of the test data set. 3.3 Simplified labeling subtasks The 5-label annotation scheme used in the SRA corpus is based on previous work on annotating student correctness in tutorial dialogue (Campbell et al. 2009 ; Nielsen et al. 2008b ) and attempting to create a shared set of labels that can align with common feedback decisions. However, less complicated labeling schemes that highlight relationships to other computational semantics tasks are also of interest to the computational linguistics community.

We therefore created two subtasks based on the same set of data that are strongly similar to previous RTE challenges, a 3-way and a 2-way labeling subtask. The data for those tasks were obtained by automatically collapsing the 5-way labels. In the 3-way task, the systems were required to classify the student responses as either (1) correct ; (2) contradictory ; or (3) incorrect (combining the categories partially correct but incomplete, irrelevant and not in the domain from the 5-way classification).

In the two-way task, the systems were required to classify the student responses as either correct or incorrect (combining the categories contradictory and incorrect from the 3-way classification).

We discuss these subtasks briefly in Sect. 4.3 . The results there show that system rankings and performance was consistent across the subtasks. Therefore, we focus on the 5-way labeling for most of our data analyses. 3.4 Test data and evaluation tasks We followed the evaluation methodology of Nielsen et al. ( 2008a ) for creating the test data. Since our goal is to support systems that generalize across problems and domains, we created three distinct test sets: 1. Unseen Answers (UA) a held-out set to assess system performance on the 2. Unseen Questions (UQ) a test set to assess system performance on responses to 3. Unseen Domains (UD) a domain-independent test set of responses to topics not These test sets correspond to the common educational NLP tasks discussed in Sect. 2.1 . The UA test sets represent application settings where question-specific training data can be collected and annotated. The UQ and UD test sets represent transfer performance to new questions and new domains respectively, in situations, for example, where a course instructor authors a new question during course development, but does not have the resources to collect a labeled data set and re-train the classifier. We will refer to these different situations as  X  X  X cenarios X  X . Some of the participating systems attempted to adapt their models to different scenarios, and we evaluate the effectiveness of such adaptation in Sect. 4.5 .

The statistics for questions and student responses in the different test sets are shown in Tables 2 and 3 . The final label distribution for train and test data is shown in Table 4 . It shows that the training and test data have very similar label distributions. Approximately the same percentage was also seen across the UA, UQ and UD test sets. 3.5 Evaluation metrics and baselines We used two baselines: the majority (most frequent) class baseline and a word overlap baseline described in detail in Dzikovska et al. ( 2012b ). The performance of the baselines is presented jointly with system scores in the results tables.
For each evaluation data set (test set), we computed the per-class precision, recall and F 1 score. We also computed three main summary metrics: accuracy, macro-average F 1 and weighted-average F 1 .
 Accuracy is computed as the percent of all test set responses classified correctly.
Macro-average , for a given metric, is computed as a simple average of the per-class values for that metric. where k is the number of classes 3 and metric c is the value for class c of the metric being averaged ( P , R ,or F 1 ). A macro-averaged metric favors systems that perform comparatively well across all classes, as each class contributes equally to the final value of the metric. For example, the word overlap baseline significantly outperforms the majority class baseline on this metric, because the latter scores zero on each class other than the most frequent class.

Weighted Average , for a given metric, is computed by multiplying each per-class value for that metric by the proportion of examples having that label in the gold standard.
 where n c is the number of test set examples labeled as class c in the gold standard and N is the total number of examples in the test set. Relative to the macro-average F 1 score, the weighted-average metric favors systems that perform better on the most frequent class, as its weight dominates all others.

Seeing the substantial differences in scores and rankings assigned by the different evaluation metrics to the two baselines that we noted above, one could also expect to see differences in system rankings between the metrics. This motivated using all three metrics in the original evaluation. However, in practice the rankings assigned to different systems were quite similar across the different metrics. We analyze the rankings assigned by different metrics in Sect. 4.2 . 4.1 Participants The participants were invited to submit up to three runs for any combination of the tasks. Nine teams participated in the task, most choosing to attempt all subtasks (5-way, 3-way and 2-way labeling), with one team entering only the 5-way and one team entering only the 2-way subtask. Several teams submitted multiple runs, aiming to better understand the contributions of different features in the overall system performance.

Four of the teams submitted systems custom built for educational applications, either from scratch (CU, EHU-ALM), or by adapting an existing system (ETS, CoMeT). Four teams submitted systems relying on existing RTE (CELI, UKP-BIU) or STS (CNGL, SoftCardinality) systems. One team built a system using a paraphrase identification algorithm (LIMSIILES) that was later adapted to question answering. In addition, some of the features used by the ETS system were also used by an ETS entry in the STS shared task (Heilman and Madnani 2013b ) as well. Thus, a wide range of techniques, with roots in different strands of computational semantics research, has been used for this challenge.

In order to better understand the types of features used by the systems, and their potential impact on performance, we classified the systems according to three dimensions:  X  Use of syntactic dependencies Does the system use a bag-of-words approach, or  X  Use of semantic resources Does the system attempt to use semantic similarity  X  Scenario-specific features Does the system use different features or algorithms
Table 5 displays the summary of all submitted runs, and classifies them according to the above criteria. In the remainder of this section, we briefly review the participating systems, focusing on how they align with our classification criteria. For each team, we summarize how the runs differ and what kind of syntactic, semantic or scenario-specific features are used. Most teams also used other features, such as word overlap, which we do not discuss due to space limitations. CELI (CELI S.R.L.) The CELI team submitted two runs (Kouylekov et al. 2013 ). Both use features based on edit distance to compute word overlap between the student response and the expected answer. A corpus-based distributional approach based on the Wikipedia corpus is used to provide a similarity metric for word matching. The difference between runs is in the details of edit distance calculation.
CNGL (Centre for Next Generation Localisation) The CNGL team submitted two runs (Bicici and van Genabith 2013 ) using machine translation features. The features are computed over n-grams or over head-modifier dependencies extracted using an unsupervised syntactic parser. An n-gram language model trained on a Europarl corpus is used as an additional feature. 4 The difference between the two runs is the classifier used in training.

CoMeT (Universitat Tubingen) The CoMeT team submitted a single system using a combination of syntactic dependencies, corpus based similarity features and WordNet similarity features (Ott et al. 2013 ). In addition, the system performs scenario adaptation by choosing (based on a development set created by the authors from the training data) which feature subset to use, depending on whether the system is presented with an UA, UQ or UD question. In particular, bag-of-word features are not used for UQ and UD questions, since they appeared to have a negative impact on results on the development set.

CU (University of Colorado) The CU team submitted two runs (Okoye et al. 2013 ). Both runs use only bag of words, lemmas and parts of speech features, but do not attempt to use either syntactic dependencies or semantic similarity classes. The difference between the two runs is in the cost parameter of the LibSVM (Chang and Lin 2011 ) binary classifier. This team only entered the 2-way subtask. EHU-ALM (University of Basque Country and University of Edinburgh) The EHU-ALM team submitted three runs, using features based on syntactic dependencies, WordNet similarities and corpus-based similarity measures, as well as scenario adaptation (Aldabe et al. 2013 ). All the runs use syntactic and semantic features, but EHU-ALM 1 trained to Unseen Answers versus other scenarios; EHU-ALM 2 provides separate classifiers for UA, UQ and UD scenarios; EHU-ALM 3 adds adaptation to question type ( X  X  X hat X  X ,  X  X  X hy X  X ,  X  X  X ow X  X  etc.).

ETS (Educational Testing Service) The ETS team submitted three runs, using different combinations of n-gram, text similarity and domain adaptation features (Heilman and Madnani 2013a ). All three runs accomplish scenario tailoring via domain adaptation, having general and specific copies of features (Daume 2007 ). Two of the runs (ETS 1 and ETS 3 ) also use a textual similarity feature (PERP) that relies on WordNet as part of the similarity calculation (Heilman and Madnani 2012 ), while ETS 2 uses only features computed from the training data without external resources. No syntactic dependency features are used by any of the submitted runs.
 LIMSIILES (Basic English Substitution for Student Answer Assessment) The LIMSIILES team submitted a single run (Gleize and Grau 2013 ). The system uses a paraphrase model based on syntactic features, acquired from Simple Wiktionary as a source of semantic equivalence. In addition, features based on Stanford Parser dependencies and WordNet similarity scores are used. No scenario tailoring is performed.

SoftCardinality The SoftCardinality team submitted a single run, using  X  X  X oft cardinality X  X  features based on character and word overlap, without additional syntactic or semantic processing, and without any scenario-specific features (Jimenez et al. 2013 ).
 UKP-BIU (Ubiquitous Knowledge Processing Lab and Bar-Ilan University) The UKP-BIU team submitted three runs with features based on word overlap, WordNet similarity and syntactic dependency-based entailment features from the BIUTEE system (Levy et al. 2013 ). UKP-BIU 1 uses WordNet features only. UKP-BIU 2 uses both WordNet and dependency features. UKP-BIU 3 uses dependency features only. No scenario-specific features are used.

Overall, there were eight runs from five teams using syntactic dependency information, 11 runs from six teams using external semantic resources, and seven runs from three teams using scenario-specific features. In Sect. 4.5 , we use this classification to compare how the different features and resources that were brought to bear upon the task correlate to system performance. 4.2 Evaluation metrics and system rankings AsshowninSect. 3.5 , we calculated three differen t evaluation metrics for the system performance: accuracy, macro-average F 1 and weighted-average F 1 . The evaluation results for all metric s and all participant runs are provided online. 5 To better understand how different metrics serve in terms of ranking system performance, we computed and compared rankings for each system accordingtoeachmetriconthe5-waylabeling.

However, the rankings assigned by different metrics to the same system on the same test set are highly correlated. When comparing weighted-average F 1 and macro-average F 1 , the average Kendall X  X  s is 0.84. Similarly, comparing macro-average F 1 and accuracy, Kendall X  X  s has a mean value of 0.65. In other words, all three metrics result in similar system rankings for our data set, presumably because participants did not tune their system to a single metric.
 Given this similarity, we decided to focus on a single metric, weighted-average F , as the evaluation metric in the rest of this paper. We chose F 1 over accuracy, due to the imbalance in the label distribution, and chose the weighted-average in preference to macro-average because the imbalance was so severe in the S
CI E NTS B ANK training data (just 23 of 4969 training examples were labeled  X  X on_domain X ) that it made learning extremely challenging, and so severe in the test data (there were only single digit numbers in some test sets) that it resulted in chance and variance playing a significant role in system performance on that class. The system ranking according to the weighted-average F 1 is shown in Table 6 . While rankings did not vary significantly depending on the metric, individual system rankings can differ substantially for different domains and scenarios, as can be seen in the table. For example, ETS 2 is ranked first for B EETLE UA and UQ test sets, second for the S CI E NTS B ANK UA test set, but only 13th for the S CI E NTS B ANK UQ test set, and sixth for the S CI E NTS B ANK UD test set. Therefore, we consider the system performance for different scenarios separately in our analyses. 4.3 Comparing results across labeling subtasks Tables 7 , 8 , and 9 present the F 1 scores for the participating systems. Results are shown separately for each test set (TS). In addition, we report mean UA performance (obtained by averaging B EETLE Unseen Answers and S CI E NTS B ANK Unseen Answers sets), mean UQ performance (averaging B EETLE and S CI E NTS B ANK Unseen Questions sets), and the overall mean on all five test sets.
 For reasons of clarity and consistency with the feature-based data analysis in Sect. 4.5 , we report results from eleven test runs. Four teams with multiple runs (CELI, CNGL, EHU-ALM, and ETS) submitted systems using the same type of features, but a different learning algorithm or calculation approach. For those teams, we only included their top performing learning algorithm in the analysis, thus excluding five runs (CELI 2 , CNGL 1 , EHU-ALM 2 , EHU-ALM 3 , and ETS 3 ). The excluded runs were not performing significantly better than the baselines and did not affect the overall pattern of the reported results.

The top performing system and systems with performance that was not statistically different from the best results for a given TS are shown in bold (significance was not calculated for the mean columns). Systems with performance statistically better than the lexical baseline are displayed in italics. Statistical significance tests were conducted using an approximate randomization test (Yeh 2000 ) with 10,000 iterations; p 0 : 05 was considered statistically significant.
Not surprisingly, evaluation scores on 2-way labeling were the highest and on 5-way labeling the lowest, reflecting the additional difficulty of differentiating between a larger number of small classes.

In all tasks, all the systems performed significantly better than the majority class baseline. However, the lexical baseline was somewhat harder to beat on simpler tasks. On the 5-way task, six out of eleven systems outperformed the lexical baseline on the overall mean score. On the 3-way labeling subtask, five of the ten systems outperformed the lexical baseline on the mean TS results, and on the 2-way labeling, four out of eleven systems outperformed the lexical baseline on the mean. This may indicate that the more sophisticated features introduced by different systems are helping the most in differentiating the small additional classes introduced by the 5-way SRA labeling.

The participating systems performed consistently across the tasks, even though there was not a single best performer across different test sets. For the B EETLE UA and S CI E NTS B ANK UA tests sets, CoMeT 1 and ETS 2 were consistently the two teams outperforming the lexical baseline, with ETS 2 the top performer in all cases. For B
EETLE UQ, ETS 2 was the top performer on all subsets; ETS 1 and SoftCardinality 1 were consistently the top performers on S CI E NTS B ANK UQ, and SoftCardinality 1 on S
CI E NTS B ANK UD. In general, the rankings for other systems were consistent across the tasks as well.

Thus, for the rest of this paper we will concentrate on the systems X  performance on the SRA 5-way labeling task only. 4.4 Comparing performance between scenarios Recall, from Sect. 3.4 , that the UA test sets represent questions for which there were student responses included in the training data, while the UQ and UD test sets represent transfer performance to new questions and new domains, respectively.
As can be seen from Table 7 , the average performance on the UA task was consistently better than on UQ and UD tasks. The two UA test sets had more systems that performed statistically better than the lexical baseline, seven systems, than did the UQ test sets, where only two or three systems performed statistically better than the lexical baseline. Twice as many systems outperformed the lexical baseline on UD as on the UQ test sets.

The top performers on the UA test set were CoMeT 1 and ETS 2 . Their performance was not significantly different on either B EETLE or S CI E NTS B ANK UA test sets. However, there was not a single best performer on the transfer tasks. ETS 1 performed best on UQ on average, while SoftCardinality 1 performed statistically better than all other systems on S CI E NTS B ANK UD. The best performers on the two UQ test sets differed as well, with ETS 2 performing statistically better than all other systems on B EETLE UQ, but performing statistically worse than the lexical baseline on S CI E NTS B ANK UQ, resulting in no overlap in the top performing systems on the B EETLE and S CI E NTS B ANK UQ test sets. SoftCardinality 1 was the top performer on S
CI E NTS B ANK UQ as well as UD, but was not among the best performers on the other three test sets.

The difference in performance on the different scenarios is not entirely surprising, as the transfer scenarios would be expected to be more difficult and require different analysis to perform well. As mentioned in Sect. 4.1 , several systems attempted to use external semantic resources or scenario-specific features, which should mitigate the coverage issues to a certain extent. We analyze the effects of such features in the next section. 4.5 Comparing impact of system features As discussed in Sect. 4.1 , the task participants used a wide range of methods and features in the task. We analyzed the data to see if any patterns emerged with respect to using different features in different scenarios. We are interested in which type of features were the most helpful in achieving the best performance: syntactic dependency features, inclusion of external semantic resources, or scenario-specific features.

To answer this question, we split the participant systems into groups depending on whether a given feature type is used, and compare performance between groups.
We made two simplifications to identify patterns in the data more easily. First, as discussed in Sect. 4.3 , for systems that used the same type of features but a different learning algorithm, we used only the top performing run in our data analysis. Second, we focus on the mean Unseen Answers (UA) and Unseen Questions (UQ) figures as defined in Sect. 4.3 , and the Unseen Domains (UD) performance on S
This analysis is not definitive because the number of systems in the competition is relatively small, and we cannot account for feature interactions. However, it provides a pattern for future data analysis and research. 4.5.1 Syntactic dependency features Six out of 11 runs included in our analysis used syntactic features, which are defined as features based on parsing student responses with a dependency parser. Five runs did not use syntactic dependencies. 7 The performance of these two system groups is summarized in Table 10 .
There was not a single best system in either group: different systems performed best for UA, UQ and UD test sets. However, in all cases the top systems in the  X  X  X ithout syntactic dependencies X  X  group performed better than the top systems in the  X  X  X ith syntactic dependencies X  X  group, and the median group performance was higher as well.
 The three top systems in the  X  X  X ithout syntax X  X  group were ETS 1 , ETS 2 and SoftCardinality 1 . Only ETS 1 used external semantic resources, but both ETS systems used scenario-specific features.

This indicates that even though theoretically there are cases where syntactic structure can make a difference in SRA, the participating systems were not able to leverage such features effectively. 4.5.2 External semantic resources Recall from Sect. 4.1 that we are interested in the effectiveness of using semantic features based on external semantic resources, including hand-coded dictionaries such as WordNet and word classes or other features based on word cooccurrence in corpora external to the task. Eight of the runs we analyzed were from system variants using features based on external semantic resources, while three were from system variants not utilizing semantic resources. Group performance for these groups is summarized in Table 11 .

Again, there was not a single best performer in either of the groups. For the  X  X  X ith external semantic resources X  X  group, CoMeT 1 performed best on the UA set, while ETS 1 was the best performer for UQ and UD sets. In the group  X  X  X ithout semantic resources X  X , ETS 2 performed best on the UA and UQ sets, while SoftCardinality 1 was best for UD.

However,similar tothe syntacticdependenciescase,there wasnoobviousadvantage for the systems that used external semantic resources. Median group performance was higher in the  X  X  X ithout semantics X  X  group compared to the  X  X  X ith semantics X  X  group. The bestsystemsinthe  X  X  X ithout semantics X  X  group alsooutperformedthe bestsystemsinthe  X  X  X ith semantics X  X  group on two of the three test sets (UA and UD). Thus, the systems participating in the challenge have not yet found an effective way to leverage the external computational semantics resources for this task.
 4.5.3 Scenario specific features Four of the runs in our analysis set come from systems attempting to use different features or parameters depending on the scenario (test set). Seven runs are from configurations where no scenario-specific configuration was used. The performance of these two system groups is summarized in Table 12 .

The ETS 1 and ETS 2 systems were the best performers in the  X  X  X cenario specific X  X  group, while SoftCardinality 1 was the best performer in the  X  X  X ot scenario-specific X  X  group. For the UQ and UA test sets, ETS 2 and ETS 1 outperformed SoftCardinality 1 . Whereas, for the UD test set, SoftCardinality 1 performed best.

However, the median group performance was better for systems performing the adaptation on all test sets, unlike the case for semantic features, where median performance was better for the  X  X  X ithout external resources X  X  group. Overall, this indicates that using scenario-specific features can be helpful in improving system performance. We originally proposed SRA as a shared task to the computational linguistics community as a way to test out semantic inference algorithms in an interesting and relevant application setting. The systems entered in the challenge brought in a wide variety of computational linguistic approaches, including methods adapted from previous work on textual entailment, STS, and educational NLP. There was wide variability in terms of the features used. Most systems used easy to compute features based on word overlap, and many used additional resources, such as dependency parsers and semantic similarity features derived either from WordNet or from additional textual corpora.

For simplicity, we used a single evaluation metric, weighted-average F 1 , in our analysis. We originally proposed three possible metrics (see Sect. 3.5 ) with different theoretical properties. In principle, the metrics could differ substantially because of the treatment of minority classes, but in practice the relative rankings they assigned to the systems were reasonably consistent. We chose to use weighted-average F 1 in our analysis, due to the nature of the data imbalance as described in Sect. 4.2 . Ideally, the evaluation metric performance should be correlated with practical application indicators. For example, where the system is used to support learning rather than just assessment, the ultimate best metric is the one that results in feedback most beneficial for learning. Dzikovska et al. ( 2012a , 2014 ) carried out an initial investigation into the relationship between B EETLE II system performance and learning. More such studies in multiple domains are necessary to settle the question of which metric is best to use in different applications.

While all of the systems consistently outperformed the most frequent class baseline, beating the word overlap baseline proved to be more challenging. It was achieved by just over half of the results with about half of those being statistically significant improvements. This underscores the fact that there is still a considerable opportunity to improve SRA systems, and in determining which features and resources are useful.

The systems were evaluated on three types of scenarios: a fully in-domain analysis task, with training data specific to the questions ( X  X  X nseen Answers X  X ); transfer to new questions in the same subject area ( X  X  X nseen Questions X  X ), which would be expected to share the domain vocabulary and concepts; and transfer to new questions outside the domain, with previously unseen vocabulary and domain structure ( X  X  X nseen Domains X  X ). As discussed in Sect. 2.1 , these test sets reflect different deployment scenarios, because the difficulty of acquiring question-specific data varies depending on the intended application. In addition, the issues of domain dependence are important to the computational linguistics community as a whole, as many NLP tools such as parsers depend heavily on in-domain supervised training data, and are difficult and expensive to re-train for new domains and applications.
In theory, the use of syntactic dependencies should benefit systems in all scenarios, by allowing them to better handle long-distance dependencies that may be important for precise answer analysis. The use of external semantic resources should be beneficial in all cases, but particularly helpful for transfer scenarios where it is not possible to train lexicalized features adequately. The use of scenario-specific features should result in systems that perform better on a given scenario than the systems not adapted to it. In our data set, however, two of those three hypotheses were not supported.

In our analysis, using syntactic dependency features or external semantic resources did not provide obvious benefits regardless of the scenario. Given the small number of data points, this cannot be taken to mean that such features are not useful for this task in general. However, additional work is clearly required to successfully incorporate them in the SRA task. One issue is that none of the parsers used in the task were adapted to the domain. A pilot study in the B EETLE II domain several years ago indicated that dependency parsers trained on newspaper text were unable to provide reasonable parses for a large number of student utterances, primarily because they did not do a good job of handling the fragments and incorrect punctuation common in (spoken or written) dialogue (McConville and Dzikovska 2008 ). In the future, it would be helpful to investigate how well the parsers used in this task actually performed on the task data, and whether improved performance would make syntactic features more beneficial.

Similarly, semantic resources used by several other systems (WordNet, Europarl) are likely to have coverage issues in all scientific domains. Wikipedia is another source of semantic information that is likely to have good coverage for STEM topics. However, the systems using it in the task (CELI, LIMSIILES) performed below median in our tests. Thus, clearly more work is needed before out-of-domain semantic resources can be most effectively used for this task. Additional research is also required to determine whether the systems would perform better if in-domain semantic resources were available.

For the Unseen Answers and Unseen Question test sets, systems that used scenario-specific features outperformed systems that did not when looking both at top performers and at median group performance. Thus, we can tentatively conclude that scenario-specific features are beneficial. However, on the Unseen Domains test set, the best performing system was SoftCardinality, which is a system that does not use any syntactic or semantic features, but relies instead on a novel character and word overlap metric which is considerably more sophisticated that the word overlap features used in other systems.

As noted above, external semantic resources should be particularly useful, in theory, for transfer tasks such as  X  X  X nseen Domains X  X  by providing missing semantic information that is otherwise (implicitly or explicitly) learned during training. The fact that a word-overlap system significantly outperformed all other systems on the Unseen Domains scenario underscores the limitations of the state of the art computational semantics resources. In the future, it could be particularly interesting to explore whether scenario-specific features would be more effective for the  X  X  X nseen Domains X  X  task if combined with domain-specific semantic resources. While training data in the form of student answers can be difficult to obtain for new domains, other resources, such as course textbooks, are often more readily available and this could allow for building semantic classes or distributional semantics spaces useful in such applications.

It is also important to consider how consistent the system groups are based on the different feature types used. Our current conclusions are limited by the relatively small number of systems in the analysis. As a result, some of the systems in each category might not be ideally comparable. For example, in the syntactic dependency category, most systems used features based on supervised dependency parsers, but one system (CNGL) used features based on the output of an unsupervised dependency parser. Most importantly, the systems in the scenario-specific features group show the most differences. These systems attempt to learn the appropriate features to use in very different ways. The ETS systems start with a base set of features, and add domain-specific and domain-independent copies of each, learning different weights based on the feature X  X  relevance across domains or strictly within a single domain. The CoMeT system uses question and domain IDs as features, attempting to learn which features generalize across the three different scenarios. Finally, the EHU-ALM system trains three different classifiers, using identical feature sets but different training data sets, and then applies a different classifier based on the scenario.

Our analysis currently does not take such differences into account and simply compares system groups with and without the specified characteristics. The tentative conclusions about the utility of different feature types need to be confirmed in future research. This analysis provides a launching point for further exploration of the interactions between system features and performance, serving as a basis for SRA systems developers to classify the features they use and to plan feature ablation experiments. A similar but finer-grained feature classification was proposed in Burrows et al. ( 2015 ), which could be used for future analyses when a greater number of participating systems are available in the more detailed categories. We described the data set, participant systems and evaluation results from the joint SRA and 8th RTE challenge. This has proven to be a useful, interdisciplinary task using a realistic dataset from the educational domain. In almost all cases the best systems significantly outperformed the word overlap baseline, sometimes by a large margin, showing that computational linguistics approaches can contribute to educational tasks. However, there is still significant room for improvement in the absolute scores, reflecting the interesting challenges that educational data present to computational linguistics.

The resulting data set and code are available at http://www.cs.york.ac.uk/ semeval-2013/task7/ and can be used for computational linguistics purposes, advancing further research into computational semantics and educational NLP. References
