 Translations, regardless of the method they were produced with, are different from their source texts and from originally authored comparable texts in the target language. This has been confirmed by many linguistic studies on translation properties commonly called translationese (Gellerstam, 1986). These studies show that translations tend to share a set of lexical, syntactic and/or textual features distin-guishing them from non-translated texts. As most of these features can be measured quantitatively, we are able to automatically distinguish translations from originals (Baroni and Bernardini, 2006; Ozdowska and Way, 2009; Kurokawa et al., 2009). This is useful for Statistical Machine Translation (SMT), as language and translation models can be improved if the translation direction and status of the data (trans-lation or original) is known (Lembersky, 2013).
Research on translationese has recently focused on exploring features capturing aspects of transla-tionese such as simplification, explicitation, conver-gence, normalisation and shining-through (Volan-sky, 2012; Ilisei, 2012). Here we extend this work as follows: (i) we investigate the impact of information density and surprisal features, (ii) we explore the use of features used in machine translation quality estimation (Blatz et al., 2003; Specia et al., 2010), (iii) we explore classification between originally au-thored text and trainee and professional translation, as well as between professional and trainee transla-tion. In order to avoid biasing classification by topic content, throughout our experiments we use fully delexicalised features, resulting in dense vector rep-resentations (rather than sparse vectors, where the size of the vectors can be up to and in fact exceed the size of the vocabulary). We show that informa-tion theory as well as translation quality estimation inspired features achieve state-of-the-art results in mixed-domain original vs. human translation clas-sification.

Languages provide speakers with a large number of possibilities of how they may encode messages. These include the choice of phonemes, words, syn-tactic structures, as well as arranging sentences in discourse. Speakers X  decisions regarding these choices are influenced by diverse factors: cognitive processing limitations can impact variation in lin-guistic encoding across all linguistic levels. Text production conditions, including monolingual vs. multilingual settings, can influence this variation: in translation, choices can be shaped by aspects of both the source and the target language.

Contrastive studies have shown that information density is distributed differently in English and German (Doherty, 2006; Fabricius-Hansen, 1996). These contrasts may impact translation, and in case pect to observe differences between translations and comparable originals in terms of information den-sity. Additionally, translations are often more spe-cialised and more conventionalised than originals (excluding translation of fictional texts). In this pa-per we investigate whether and to what extent infor-mation density based features are useful in human translation classification.

Quality estimation (QE) (Blatz et al., 2004; Ueff-ing and Ney, 2005) is the attempt to learn models that predict machine translation quality without ac-cess to a reference translation at prediction time. Translation, manual or automatic, is always a pro-cess of transforming a source into a target text. This process is prone to error. In this paper we explore whether and to what extent the extensive research on QE can be brought to bear on the problem of human translation vs. originals classification, and in partic-ular the discrimination between novice and profes-sional translation output.

Below we explore the ability of our features to distinguish between 1) non-translated texts and translations by professionals, 2) non-translated texts and translations by translator trainees, and 3) the two translation varieties that diverge in the degree of translation experience. We report results in terms of accuracy and f-score, and provide a feature analy-sis in order to understand the role of the information density and QE inspired features in the task.
The paper is organised as follows: related work is presented in Section 2. The experimental setup is detailed in Section 3, followed by the results and analysis in Section 4. A discussion about our results compared to previous work is given in Section 5. Finally, conclusion and future work are provided in Section 6. We briefly review previous work on translationese, information density, machine translation quality es-timation and studies on human translation expertise. 2.1 Translationese A number of corpus-based studies on translation have shown that it is possible to automatically pre-dict whether a text is an original or a translation (Ba-roni and Bernardini, 2006; Koppel and Ordan, 2011). These approaches are based on the concept of translationese  X  a term coined to capture the spe-cific language of translations by Gellerstam (1986). The idea is that translations exhibit properties which distinguish them from original texts, both the source texts of the translation and comparable texts origi-nally authored in the target language. Baker (1993; 1995) claimed these properties to be universal, i.e. (source) language-independent, emphasising gen-eral effects of the process of translation.

However, translationese includes features involv-ing both source and target language. Most linguis-tic studies distinguish explicitation  X  a tendency to spell things out rather than leave them implicit and implicitation (the opposite effect), simplification  X  a tendency to simplify the language used in transla-tion, normalisation  X  a tendency to exaggerate fea-tures of the target language and to conform to its typ-ical patterns, levelling out or convergence  X  a rela-tively higher level of homogeneity of translated texts compared to non-translated ones, and interference or shining through (e.g. Teich (2003)). While sim-ple lexicalised features including word tokens and character n -grams can produce near perfect clas-sification results for in-domain data (Avner et al., 2014), a significant amount of work has gone into devising features that can capture presumed linguis-tic aspects of translationese (Volansky, 2012). Rabi-novich et al. (2015) explore unsupervised discrimi-nation of translations based on principal components analysis for dimensionality reduction followed by a clustering step. The method is robust to unbalanced and heterogeneous datasets, which may be useful to handle mixed domain, genre and source of data, a common situation when training language and trans-lation models.

Automatic classification of original vs. translated texts has applications in machine translation, espe-cially in studies showing the impact of the nature (original vs. translation) of the text in translation and language models used in SMT. Kurokawa et al. (2009) show that taking directionality into ac-count when training an English-to-French phrase-based SMT system leads to improved translation performance. Ozdowska &amp; Way (2009) analyse the same language pair and demonstrate that the nature of the original source language has an impact on the quality of SMT output. Lembersky et al. (2012) show that BLEU scores can be improved by lan-guage models compiled from translated texts and not from comparable originally authored ones. 2.2 Information Density In a natural communication situation, speakers tend to exploit variations in their linguistic encoding  X  modulating the order, density and specificity of their expressions to avoid informational peaks and troughs that may result in inefficient communica-tion. This is often referred to as the uniform infor-mation density hypothesis (Frank and Jaeger, 2008). The information conveyed by an expression can be quantified by its surprisal , a measure of how pre-dictable an expression is given its context. Sim-plification and explicitation may impact the aver-age information density measured on translated texts compared to comparable originally authored ones in the same language. Source language interference should result in peaks of measured surprisal val-ues in translated texts, while the information density may remain uniform in originals.

According to Hale (2001), a surprisal model al-lows the estimation of the probability of a parse tree given a sentence prefix. Levy (2008) showed that a lexical-based surprisal measure can be ob-tained by computing the negative log probabil-ity of a word given its preceding context: S =  X  log P ( w k +1 | w 1 ...w k ) . Following Demberg et al. (2013), we estimate surprisal in three ways, at the word, part-of-speech and syntax levels, based on n -gram language models and language models trained on unlexicalised part-of-speech sequences and flat-tened syntactic trees. Note that all resulting feature vectors do not represent lexical information but in-formation theoretic surprisal measures. 2.3 Quality Estimation Machine translation QE is the process of estimat-ing how accurate an automatic translation is through characteristic features of the source and target texts, and (possibly) also the translation engine, with a su-pervised machine learning setting to estimate quality scores. QE can be applied at the word, sentence and document level (Gandrabur and Foster, 2003; Ueff-ing et al., 2003; Blatz et al., 2003; Scarton and Spe-cia, 2014).

Many different delexicalised dense features have been explored in previous work on QE, including language and topic models, n -best lists, etc. (Quirk, 2004; Ueffing and Ney, 2004; Specia and Gimenez, 2010; Rubino et al., 2013a). It has been shown that the performance of a supervised classifier to distin-guish between originals and automatic translations is correlated with the quality of the machine trans-lated texts (Aharoni et al., 2014): low quality trans-lation, containing grammatical and syntactic errors, as well as incorrect lexical choices, are robust indi-cators of automatic translations. In the case of hu-man translation, to the best of our knowledge, there are no empirical studies on the level of professional expertise in the translation process and its correla-tion with the performance of a translationese classi-fier. 2.4 Translator Experience J  X  a  X  askel  X  ainen (1997) describes translational be-haviour of professionals and non-professionals who perform translation from English into Finnish. Carl and Buch-Kromann (2010) apply psycholinguistic methods in their analysis. They present a study of translation phases and processes for student and pro-fessional translators, relating translators X  eye move-ments and keystrokes to the quality of the transla-tions produced. They show that the translation be-haviour of novice and professional translators dif-fers with respect to how they use the translation phases. Englund Dimitrova (2005) develops a com-bined process and product analysis and compares translators with different levels of translation experi-ence, but concentrates only on cohesive explicitness.
Most of these works are rather process-oriented than product-oriented, which means that features of translated texts are rarely taken into account. How-ever, some of the findings are valuable for the anal-ysis of translated texts. For instance, G  X  opferich &amp; J  X  a  X  askel  X  ainen (2009) find that with increasing trans-lation competence, translators focus on larger trans-lation units, which can impact the choice of linguis-tic encoding translators use. Our experiments are designed to investigate under-explored topics focusing on (i) information theoretic and (ii) machine translation QE features in transla-tion classification. We use dense vector represen-tations with fully delexicalised features and investi-gate three hypotheses: 1. originals &amp; professional translations should be 2. originals &amp; student translations should be dis-3. professional &amp; student translations should both 3.1 Supervised Classification In order to train a classifier and predict binary la-bels on unseen data, we use a dense vector sentence-level representation associated with a class ( x i ,y i ) , i = 1 ,...,l ( l is the number of training instances) with x i  X  R n ( n is the size of a dense vector) and y  X  { X  1 , 1 } l . We train classification models with a support vector machine SVM (the C -SVC imple-mentation in LIB SVM (Chang and Lin, 2011)) as a quadratic optimization problem: subject to y i (  X  T  X  ( x i ) + b )  X  1  X   X  i ,  X  i  X  0 .  X  is a kernel function and allows the projection of training data to a higher dimensional space. We use the radial basis function (RBF) kernel, as it pro-duced the best empirical results compared to linear and polynomial kernels. We predict the class for un-seen instances x as follows: Table 1: Details of the corpora used to train lan-guage and n -gram frequency models for originally authored texts and translations.
 Two hyper-parameters have to be set for C -SVC with the RBF kernel: the regularisation parameter (or penalty) C and the kernel parameter  X  . We use grid-search to find optimal values, performing a 5 -fold cross-validation on the training data. To avoid over-fitting, we use a held-out development set to evaluate the models obtained. 3.2 Datasets The datasets used in our experiments are separated into two subsets: corpora used to extract features and corpora used to train, tune and test our classi-fiers. The former are taken from the publicly avail-able bilingual English-German parallel corpora con-sisting of parliamentary proceedings, literary works and political commentary, compiled by (Rabinovich et al., 2015). These corpora are used individu-ally to train language models and compute n -gram frequency distributions. Basic corpus statistics are presented in Table 1. The latter ones are com-posed of German texts, taken from the VARTRA corpora (Lapshinova-Koltunski, 2013), which were either originally written in German (originals) or translated from English (translations).

Originals and translations belong to the same gen-res and registers and can be considered compara-ble. They include a mixture of literary, tourism and popular-scientific texts, instruction manuals, com-mercial letters and political essays and speeches. The VARTRA translations are split in two sets: one produced by professional translators, and one pro-duced by translator trainees. Details are presented in Table 2. We extract balanced subsets of training, tuning and testing data containing three, one and two thousands sentences, respectively, of each type. Table 2: Details of the comparable corpora used as training, development and test sets for the originals versus translation classification experiments. 3.3 Feature Sets For classification, input text is represented as a set of feature vectors. The features capture aspects of information density and translation QE. Throughout we use unlexicalised lower-dimensional dense vectors rather than high-dimensional lexicalised sparse vectors to minimize the input of specific content on classification results. We extract a total of 778 features 2 and separate them into four subsets corresponding to broad but distinct characteristics of original and translated sentences: surface and distortion features are related to QE, surprisal and complexity features are inspired by information theory.
 Surface Features -13 surface features based on meta representations of sentences X  lexical form. Features include sentence and average word length, the number of word tokens and number of punc-tuation marks. Three case-based features capture the number of upper-cased letters and words, and a binary feature indicates whether a sentence starts with an upper-case character. Another binary value encodes whether the sentence ends with a period. Two features are obtained from the ratio between the number of upper-cased and lower-cased letters, the number of punctuation marks and the length of the sentence. Finally two features capture the number of periods merged with words and words with mixed-case characters.
 Surprisal Features -225 features based on the surprisal measure presented in Section 2.2 are extracted using language models trained on words, delexicalised part-of-speech and flattened syntactic trees. The language models are trained extract n -gram ( n  X  [1; 5] ) log-probabilities and perplexities, with and without the tags indicating the beginning and ending of sentences, using the SRILM toolkit (Stolcke et al., 2011).
 Complexity Features -315 features based on n -gram frequencies, indicating how frequent the lexical choices, part-of-speech and flattened syn-tactic sequences present in the text to be classified are. As for the surprisal features, we use the same originally authored and translated texts individually to extract n -grams frequency quartiles. We extract the percentage of n -grams ( n  X  [1; 5] ) occurring in each quartile. Frequency percentages are averaged at the sentence level, leading to 4 features per sentence (one per quartile) given a value of n , for each corpus used to define the frequency quartiles. This approach allows us to avoid encoding raw n -gram features and keep a dense vector represen-tation (Blatz et al., 2003).
 Distortion Features -225 features based on the possible distortion in lexical, part-of-speech and syntactic structures observed between originals and translations, as well as between different levels of translation experience. These features are extracted the same way as the suprisal features, but based on language models trained on sentence-level reversed text. The backward language model features are popular in translation quality estimation studies and show interesting results (Duchateau et al., 2002; Rubino et al., 2013b). 3.4 Preprocessing and Tools All data used in our experiments are sentence-split, lower-cased and tokenised using the C ORE NLP toolkit (Manning et al., 2014). The part-of-speech tags and syntactic trees required to extract some fea-tures are obtained with the same set of tools. For parsing, we use the probabilistic context-free gram-mar model trained on the Negra corpus (Brants et al., 2003) and described in (Rafferty and Manning, 2008), before flattening the trees as illustrated in Figure 1. Both part-of-speech and flattened syntac-Figure 1: Flattening and delexicalising a syntactic tree. tic trees are then delexicalised in order to remove all surface forms from the representations. Below we provide details on discriminating between originally authored texts and translations, followed by the prediction of translation experience compar-ing professional translators and students. Finally, we evaluate feature importance with individual and en-semble feature selection techniques. 4.1 Original vs Translated Texts Two sets of experiments are conducted to discrimi-nate between originals and professional translations (Table 3) and originals and student translations (Ta-ble 4). For each classification task, we evaluate fea-ture groups on the test set containing 4 , 000 unseen sentences balanced over two classes, reporting over-all accuracy, and also precision, recall and f-score. Finally, a classification model is trained and evalu-ated combining all features.

Originals vs. professional translations reaches a maximum accuracy of 70 . 0 % using the distortion feature set with surprisal a close second at 69 . 2% . The difference is not statistically significant (boot-strap resampling at p &lt; 0 . 05 ). They outperform the other types of features, as well as the combination of all feature types. Per class evaluation shows a simi-lar trend with the best performing feature sets. The results show that originals and professional transla-tions exhibit differences in terms of sequences of words, part-of-speech and syntactic tags which are captured by language model based features.
 Table 3: Accuracy, precision, recall and F-measure obtained on the originals versus professional trans-lations classification task. Best results in bold and statistically significant winner marked with ? ( p &lt; 0 . 05 ).
 Table 4: Accuracy, precision, recall and F-measure obtained on the originals versus student translations classification task. Best results in bold and statisti-cally significant winner marked with ? ( p &lt; 0 . 05 ).
The classification of originals and student trans-lations shows that the combination of the four fea-ture types leads to the most accurate classifier, fol-lowed by the distortion and the surprisal sets (with equivalent accuracy results at p &lt; 0 . 05 ). The two latter feature sets are the best performing ones over-all based on the two classification tasks. Comparing the two tasks, originally authored texts are closer to professional translations and more distant to student translations, which validates two of our hypotheses listed in Section 3. 4.2 Translation Expertise In order to investigate whether our third assumption is correct, we perform binary classification between professional and student translations (Table 5). The results, barely above the 50% baseline, show the proximity of the two types of translations according to our feature sets, which supports our third assump-tion. The combination of four feature types reaches the highest accuracy, followed by the distortion and complexity sets. However, the surprisal features do not seem to be helpful in differentiating between the professional and the student translations, compared to the two previous binary classification tasks.
This result indicates that the surprisal measure is a reliable source of information to determine whether a sentence is originally authored or a translation, but it is not reliable to separate two translations pro-duced by translators with different levels of exper-tise. The features inspired by translation quality es-timation do not reach high accuracy results: it seems that the difference between professional and student translations cannot be tied to properties of the sur-face level or lexical choices of the human translators as indirectly captured by our features.
 Table 5: Accuracy, precision, recall and F-measure obtained on the professional versus student trans-lations classification task. Best results in bold and statistically significant winner marked with ? ( p &lt; 0 . 05 ). 4.3 3-way Classification Table 6 shows the confusion matrix obtained with the classifier trained on the combination of the four feature sets. This classifier reaches third position overall in terms of accuracy, behind the distortion and surprisal sets with first and second positions, re-spectively. This ranking of classifiers trained on dif-ferent feature sets follows the trend observed in the originals versus professional translation binary clas-sification task.
 Table 6: Confusion matrix obtained using a classifier trained on the four feature sets for the multi-class task, separating originals, professional and student translations.

The training method chosen for the multi-class problem is the one against one , where individual classifiers are first trained on each binary classifi-cation task before being combined to form the final multi-class classifier (Hsu and Lin, 2002). The re-sults indicate that our feature sets distinguish orig-inally authored texts from professional and student translations (first line of the matrix), while the pro-fessional translations are more difficult to separate from the two other types of text. Also, student trans-lations have characteristics differing from originals and professional translations, which can be captured with our feature sets (last line of the matrix). How-ever, the columns of the confusion matrix show that originals are not necessarily closer to professional translations, as indicated by the first column where a larger amount of gold originals are incorrectly clas-sified as student translations. The same trend is ob-servable in the last column. These results go against the hypothesis that originals and student translations are easier to separate, a phenomenon which does not appear for the binary classification task (originals vs. student translations). 4.4 Feature Performance Evaluating the performance of our feature sets is done by calculating the discriminative power of each feature individually which allows us to rank features according to their correlation with a class given a classification task. We follow the  X  X -score X  measure (1) as proposed by Chen (2006): with training vectors x k and k = 1 ,...,m , bi-nary classes n + and n  X  for positive and negative in-stances,  X  x i ,  X  x (+) of the whole, positive and negative instances, and  X  x negative instance. The measure indicates how dis-criminative a feature is given a binary classification task. A drawback of the f-score is that it does not take into account possible feature complementarity.
We report the distribution of the top 25 features amongst the three levels of analysis: lexical, POS and syntax (Figure 2a), as well as amongst the four (left bars) and ensemble of randomised trees (right hatched bars). feature types: surface, surprisal, complexity and dis-tortion (Figure 2b). The results show that POS fea-tures are not ranked as the most discriminant ones when evaluated individually, while syntactic fea-tures are the most important ones for the originals vs. professional translation task and lexical features have the highest discriminative power for the two other tasks. When looking at the feature types, we see that complexity features, based on n -gram fre-quencies, are the most discriminant for the three tasks, followed by the surprisal features, while the distortion and surface features do not have a strong discriminative power. Most of the top n -gram based features rely on sequences between 1 and 3 words, indicating that higher order n -grams are not im-portant features when considered individually. Sur-prisal, distortion and complexity features are based on external resources (detailed in Table 1) and the corpus of political texts translated into German is the most useful one when used to extract the complex-ity and surprisal features, which can be explained by the presence of political speeches and essays in the
The results obtained on individual feature dis-criminative power do not reflect the ones obtained using features grouped by types. Individually, fea-tures indicating complexity based on n -gram fre-quencies are ranked highest. However, only a few of the distortion features appear in the discrimina-tive ranking while this feature type reaches the high-est accuracy scores on the three binary classification tasks. These results indicate that features are highly complementary within a group of a particular type, but also between different types. To capture pos-sible relationships between features, we conduct a non-linear feature selection using the forest of ran-domised trees approach (Geurts et al., 2006) and present the results for the top 25 features in Figure 2 (right hatched bars).

The tree-based feature ranking method shows the complementarity of words and POS features, while the syntactic ones appear in the top 25 for the orig-inal vs. translation tasks for both levels of exper-tise. When looking at the feature types, the originals vs. professional task relies mainly on a mixture of distortion and complexity features, and surprisal in-dicators are totally absent from the top 25 for the professional vs. student task. For both tasks involv-ing student translation, the complexity features are the most important ones, and simple surface features are useful, such as the average words occurrence per sentence or the ratio between the number of punctu-ation marks and the sentence length. The most use-ful external resource used to extract n -gram based features is again the political corpus, indicating once more the domain proximity of our datasets.

Individually, syntactic features appear to be highly discriminant when classifying between orig-inals and translations (regardless expertise), which may indicate two translationese phenomena: simpli-fication, translators use less complex constructions, and interference (shining through), source syntax shines through in translated texts. The ensemble ranking shows that surprisal and distortion, although not as important as complexity and distortion, are important indicators of translationese as they appear in both tasks where originals are classified against translations. These feature types are not present in the top 25 if only translated texts are classified. Previous research (Baroni and Bernardini, 2006; Volansky, 2012) has shown that high classification accuracy ( &gt; 80% ) can be achieved using lexi-calised token n-gram sparse feature vectors. As a sanity check, we conduct a set of experiments for each of our classification tasks using token unigram frequency as features, normalised by the segment length. The vocabulary defining the feature vector dimensionality is taken from the training sets, us-ing the data presented in Table 2 only, leading to 25 , 561 features. The same classification setup as presented in Section 3 is used and we observe ac-curacy results reaching 78 . 0% , 83 . 3% and 65 . 2% for original vs. professional, originals vs. student and professional vs. student classifications respec-tively. For the three-way task, an accuracy score of 62 . 7% is reached. These results are substantially lower than the ones reported by Volansky (2012), mostly because of the text chunks size, which has a strong impact on performance as shown by Rabi-novich and Wintner (2015). In our work, we clas-sify each sentence individually as they appear natu-rally in the corpus, while most previous studies are based on artificial chunks of approximately 2 , 000 tokens. An other explanation of the low perform-ing unigram-based features is related to our mixed-domain setting, as it was shown that classifiers X  per-formance drop drastically when trained on this type of features and tested on out-of-domain data (Rabi-novich and Wintner, 2015). This paper presented a first step in using informa-tion density, and especially surprisal and complexity inspired features, as well as features used in trans-lation quality estimation, as indicators of transla-tionese for originally authored and manually trans-lated text classification. We focused on separating originals and translations produced by humans with different levels of expertise and showed that trans-lationese features based on information density and quality estimation are useful indicators of whether a text was manually translated or originally produced. We conducted experiments in a mixed-domain set-ting, including literary, tourism and scientific texts, as well as instruction manuals, commercial letters and political essays and speeches.

Our experiments on feature type evaluation show that the best performing one is a set of quality esti-mation inspired distortion indicators, extracted from backward language models trained on originally au-thored and translated texts. When features are eval-uated individually according to the  X  X -score X  mea-sure (Chen and Lin, 2006), the most discriminative ones are from the complexity subset, extracted from n -gram frequency quartiles, followed by surprisal features, both extracted at the lexical and syntac-tic levels. The features ensemble evaluation based on randomised trees reveals feature complementar-ity and shows that extracting complexity and distor-tion indicators at the lexical and POS levels leads to the highest performing sets.

The features used in our experiments are extracted at the word-level. As future work, we plan to ex-tend our feature sets to information theoretic as-pects of character-level indicators, such as charac-ter n -grams frequencies and language models, en-coding complexity and surprisal respectively. This approach would allow to capture sub-word informa-tion density indicators, such as morphological infor-mation (Avner et al., 2014).
 This research is funded by the German Research Foundation (Deutsche Forschungsgemeinschaft) un-der grant SFB 1102: Information Density and Lin-
We would like to thank the anonymous reviewers for their insightful comments.
