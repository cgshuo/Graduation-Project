 1. Introduction for developing more user-friendly interfaces.
 acquire parsers for mapping long sentences to logical forms such as the Robocup corpus. corpus of NL and logical forms, to transform NL sentences to logical forms. and semantic information using a semantic augmented parsing tree (SAPT). representation (MR). The system was extended to work with the formal language in deal with logical variables.
 unlabeled data.
 with word-cluster models. The contributions of our proposed method are described as follows.  X  generate a cluster model to enrich the feature space of the learning model.  X  mapping sentences to logical forms.
 experimental results, and Section 6 discusses the advantage of our method and describes future work. 2. Background and base techniques used in the paper. We describe them in the following subsections. 2.1. Meaning representation sequence of terminals. 2.2. Machine learning models for semantic parsing 2.2.1. Support vector machine [16,27] , and so on. vectors only appear in the form of their dot products. In addition, a kernel function K ( x and x j in the dual form. Here are some kinds of kernel functions which are normally used:  X  linear: K ( x i , x j )= x i  X  x j .  X  polynomial: K ( x i , x j )=(  X  x i  X  x j + r ) d ,  X  N 0.  X  radial basic function (RBF): K ( x i , x j )= exp (  X   X   X   X  sigmoid: K ( x i , x j )= tanh (  X  x i  X  x j + r ).  X  String kernel between two strings is defined as the number of common subsequences between them [25] .
Here,  X  , r , and d are kernel parameters. 2.2.2. Maximum entropy model of a class c given a vector of features x according to the MEM formulation as follows: is the total number of features, and  X  i is a weight for a given feature function. The weight optimization technique such as the L-BFGs algorithm [24] . 2.3. Semantic parsing for unambiguous supervision: KRISP In this section we briefly review work on semantic parsing for ambiguous supervision. Fig. 2 shows examples of NL sentences, in which each sentence is paired with the respective correct MR. substrings covered by its children. Let P  X  ( u ) denote the probability that a production as: probability P  X  ( s [ i .. j ]) for each production  X  . For each production positive examples can be obtained from the production  X  if the rule examples. After collecting positive and negative examples, a support vector machine model re-parse all training examples to collect positive and negative examples corresponding to each production
Fig. 3 shows an example of the derivation for parsing the sentence  X 
TRAVERSE  X  Traverse [1 ... 4] 0.91  X  means that this rule covers the substring probability 0.91. 3. Semantic parsing for ambiguous supervision with ambiguous supervision [17] . 3.1. Alignment models sentence to an MR logical form. There are two components for the generative model  X   X  model for this step to compute P ( w | e ). The details of how to compute P ( w | e ) are described in [26] . after performing the alignment. 3.2. Learning with ambiguous supervision tool  X  Weights for data instances  X  available in the LibSVM package
Algorithm 1. The KRISPER semantic parsing [17] 3.3. Semantic parsing with ambiguous supervision using MEM  X   X 
Part of speech of each word with the substring is considered as a feature.  X 
Chunking information of each word with the substring is considered as a feature.  X  Named Entity categories of words in the substring did not improve. 4. Semi-supervised semantic parsing with ambiguous supervision have a change to find a correct answer. (See Fig. 4 .)
For convenience, we briefly present a summary of the Brown algorithm as follows. 4.1. The Brown algorithm agglomerative word clustering algorithm (HAC) [5] , in which the input is a large sequence of words w from raw texts. The output is a hierarchical clustering of words which is represented as a binary tree properties of w (or w 's context). The vector for w i can be considered as counts, for each word w corpus:
C ( w minimal loss of average mutual information. More detail about the algorithm is described in [22] . root of the hierarchical clustering,  X  0  X  is appended to the binary string if we go up, and shows the cluster encoding of words which is used for semantic parsing. 4.2. Word-cluster based features for MEM  X  speech (pos), chunking(chk), and named entities information (ner).  X   X  words. 4.3. Word cluster for SVM string kernel 4.3.1. String kernel  X  state that are next to  X  , and t =  X  the state next to  X  then K ( s , t )=7. with the string kernel for semantic parsing problem.
 4.3.2. Word-cluster for SVM string kernel kernel function is the linear combination between the two kernels, and it can be computed as follows. 4.4. Semi-supervised algorithm for ambiguous semantic parsing focus on the use of Maximum Entropy and Support Vector Machine with string kernel. Algorithm 2. The semi-supervised learning algorithm for semantic parsing with ambiguous supervision 5. Experimental results A models are conducted simultaneously).
 statistics on the Robocup corpus used in the experiment.
 F-measure as in the formula below.
 1,000 word clusters. We called the cluster obtained from Brown as WC-Brown. best result.
 the proposed method compares favorably to the state of the art system [17] . A A performance of semantic parsing. We also evaluated the proposed system in various noise levels reflects that word-cluster features are useful for semantic parsing with ambiguous supervision. obtained by performing the Brown Clustering on the WIKI dataset is called WC-WIKI. model will be improved.
 as [40,36] . 5.1. Error analysis semantic representation wrongly, as follows.  X   X  badPass  X  meanwhile the correct one is  X  turnover  X  .  X 
The system predicts correct predicates but wrong arguments. Sentence (Id 3) shows this case.  X  semantic representation. 6. Conclusions semi-supervised learning methods for the semantic parsing problem. Acknowledgments Research.

References
