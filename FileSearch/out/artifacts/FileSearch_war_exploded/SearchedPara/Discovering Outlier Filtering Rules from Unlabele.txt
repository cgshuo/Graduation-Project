
This paper is concerned with the problem of detecting out-liers from unlabeled data. In prior work we have developed 
SmartSifter, which is an on-line outlier detection algorithm based on unsupervised learning from data. On the basis of 
SmartSifter this paper yields a new framework for outlier filtering using both supervised and unsupervised learning techniques iteratively in order to make the detection pro-cess more effective and more understandable. The outline of the framework is as follows: In the first round, for an initial dataset, we run SmartSifter to give each data a score, with a high score indicating a high possibility of being an outlier. Next, giving positive labels to a number of higher scored data and negative labels to a number of lower scored data, we create labeled examples. Then we construct an outlier filtering rule by supervised learning from them. Here the rule is generated based on the principle of minimizing extended stochastic complexity. In the second round, for a new dataset, we filter the data using the constructed rule, then among the filtered data, we run SmartSifter again to evaluate the data in order to update the filtering rule. Ap-plying of our framework to the network intrusion detection, we demonstrate that 1) it can significantly improve the curacy of SmartSifter, and 2) outlier filtering rules can help the user to discover a general pattern of an outlier group. 
This paper is concerned with the outlier/anomaly detec-tion problem. This is closely related to fraud detection, net-work intrusion detection, etc., since criminal or suspicious activities may often induce outliers. It is also related to unexpected pattern discovery or rare event discovery. 
In prior work [21] we have developed SmartSifter, which is an on-line outlier detection algorithm based on unsuper-vised learning from data. It takes a data sequence as in-put in an on-line way and gives each datum a score, with a high score indicating a high possibility of being an outlier. SmartSifter uses a Gaussian mixture model as a statistical are not made or distributed for profit or commercial advantage and requires prior specific permission and/or a fee. K_DD 01 San Francisco CA USA 
Copyright ACM 2001 1-58113-391-x/01/08...$5.00 representation of normal behaviors. The key idea of Smart-
Sifter is to learn the model with on-line discounting learn-ing algorithms and to calculate a score for a datum on the basis of the change of the model before and after learning from it. The novel features of SmartSifter are: a) a score has a clear statistical/information-theoretic meaning, b) it is adaptive to changes of normal behaviors of new patterns, c) it is computationally inexpensive, d) it attains high de-tection accuracy. In fact, it was demonstrated using KDD 
Cup 1999 [6] that network intrusion data were detected with high accuracy among the data that SmartSifter gave higher scores. The drawback of SmartSifter is, however, that it cannot explain why the identified outliers are exceptional. 
The contribution of this paper is to provide a new frame-work for detecting and explaining outliers by combining Smart-
Sifter with a supervised learning technique. Based on the scores that SmartSifter outputs, we create labeled data by giving positive labels to higher scored data and negative la-bels to lower scored data. By supervised learning from both the positive and negative data, we build an outlier filter-ing rule that discriminates positive data from negative data. 
This filtering rule is used as a preprocessing of SmartSifter for a new dataset. In this framework individual outliers are identified in the unsupervised learning process, while a gen-eral pattern of the identified outliers is discovered in the supervised learning process. 
The purpose of building this framework is two-folds: 1) To improve the power of SmartSifter by combining supervised learning with an unsupervised one. 2) To clearly explain the meaning of the outliers through the rule acquired in the supervised learning process. This leads to outlier pattern discovery. We require that 1) and 2) be realized at once. decision list as a representation of a classification rule and employ a learning algorithm DL-ESC(DL-SC). The key idea of DL-ESC is to select rules on the basis of the principle of minimizing extended stochastic complexity (ESC [20]) (or stochastic complexity (SC [14]), see Section 4). It is theoret-ically [20] and empirically [11] demonstrated in the scenario of information theory or statistical decision theory that this principle leads to a strategy of learning classification rules. 
A version of DL-ESC(DL-SC) which can handle discrete at-tributes only was introduced in the scenario of text classi-fication [11]. This paper introduces a complete form which can handle both continuous and discrete attributes. tion problem, we demonstrate that 1) this framework can significantly increase the accuracy of SmartSifter, and 2) it can help the user to discover a pattern of some specific groups of intrusions. 
In most of existing techniques for outlier/fraud detection, one first learns the mechanism of generating data and then evaluates a given datum relative to the learned mechanism. 
We may classify the approaches of outlier detection into two types: the supervised-learning based approach and the unsupervised-learning based one. The former requires that in the learning process, any example be labeled with re-gard to whether it is exceptional/fraudulent or not (see, e.g., [2],[4],[5],[10],[17],[12]), while the latter does not. The latter is more important in practice since labeled examples are not necessarily available in real situations. 
Unsupervised-learning based methods have been explored on the basis of the theory of statistical hypothesis test-ing (see e.g., [1]). Most of them are univariate in nature and suffers from computational difficulty in dealing with multi-variate data. Burge and Shawe-Taylor [3] developed an algorithm for on-line unsupervised outlier detection. It utilizes current user profile and user profile history to calcu-late a score for each datum in an on-llne process (see [13] [9] for similar profiling methods), similarly with SmartSifter. It cannot, however, handle categorical variables while Smart-
Sifter can. It was also shown in [18] that for the network intrusion detection problem using the dataset KDD Cup 1999 SmartSifter outperformed Burge and Shawe-Taylors' algorithm both in terms of detection accuracy and compu-tational efficiency. 
Knorr and Ng [7] developed novel unsupervised algorithms for finding distance-based outliers, which are defined in terms of a geometric distance without any statistical meaning. We consider statistical outliers, which are defined in terms of deviation from an underlying statistical model. 
It is important to give a clear explanation to the identi-fied outliers, because it can help the user to understand why they are interesting and how valid they are. There are, how-ever, only a few work on identifying and explaining outliers at once. Knott and Ng [8] developed algorithms for finding intensional knowledge of distance-based outliers. This paper provides a different approach from theirs: we explain sta-tistical outliers rather than distance-based ones. Although the previous work [8] can only indicate that each identified outlier is deviated from the pattern that most of the other data have, our approach helps the user to discover a pattern that outliers in a specific group may commonly have. 
To the best of our knowledge, in the area of outlier/fraud detection, there is no previous work which combines a super-vised learning technique with an unsupervised one, as with our framework. Below we describe the framework for outlier filtering. 
We prepare a filtering rule T~ that determines any datum to be "positive" or "negative" where a positive datum is thought to be an outlier with high probability. The filtering rule is initially set to be a default rule "negative." For a given dataset S, we filter S through the rule to extract the dataset P that are determined to be positive by the rule. Then we delete such data from S to get a filtered dataset 
S -P. We then run SmartSifter for S -P. It calculates a score for each datum in S -P and outputs a dataset Q consisting of data with higher scores. Based on the scores, we give positive labels to the top c~% data of highest scores in S-P and let a set of the positive labeled examples be P'. On the other hand, we make a random sampling of/3% data in S -P from the pool of remaining data S -P -P' and give negative labels to them. We let a set of the negative labeled examples be N'. Here a and/3 are pre-determined positive numbers. Next we run a supervised learning algorithm DL-ESC/DL-SC, which takes P~ and N' and outputs a classifier L that discriminates the positive examples from negative ones. Among local rules that appeared in the acquired classifier L, we se-lect a number of rules that would be useful for outlier filter-ing. Criteria for selecting rules are here: 1) their accuracy of identifying positive examples is as high as possible, 2) the number of examples covered by the rule is not too large, and 3) the rule itself is intuitively meaningful. We add the selected rules to 7E to update the filtering rule. This process is repeated. The flow is described in Figure 1. 
The novel features of this framework are as follows: 1) The filtering rule helps the user to understand outliers. The filtering rule uses "if-then-else" form to represent a general feature of a specific group of outliers. Hence it is useful for understanding and explaining why the identified outliers are exceptional and how interesting they are. Note that the attributes in data used for filtering are not neces-sarily the same as those for SmartSifter. Hence the filtering rule is able to characterize outliers in terms of attributes that are not used for SmartSifter. 2) Combining a filtering rule with SmartSifter improves the power of SmartSifter. It is expected that using a filtering rule as a preprocessing for SmartSifter greatly improves its efficiency. This is actually demonstrated in the network in-trusion detection scenario in Section 5. Further, our frame-work is expected to be effective in the discovery of a new pattern of outliers. Once the data is filtered by the rule and SmartSifter is applied to the remaining data in the next stage, we are able to discover patterns of outliers that didn't appear in the earlier stages. 
We briefly overview SmartSifter according to [21]. The approach of SmartSifter is given as follows: 
I) SmartSifter uses a probabilistic model as a representa-tion of an underlying mechanism of data-generation. The model takes the following hierarchical structure: Let (x, y) denote a datum where as denotes a vector of categorical vari-ables and y denotes a vector of continuous variables. We write the joint distribution of (x, y) as p(x, y) = p(x)p(y)x). 
We represent p(x) by using a histogram density with a finite number of disjoint cells, and for each cell, for all xs that fall into it, we represent p(ylx) by using a Gaussian mixture model. Hence we prepare as many Gaussian mixture models as cells in the histogram density. 
II) Every time a datum is input, SmartSifter employs an on-line discounting learning algorithm to update the model. 
Consider the situation where a sequence of data is given: (xl,yl),(x2,y2)'.. in an on-line process. Given the tth input datum (xt,yt), identify the cell that xt falls into and update the histogram density using the SDLE (Sequen-p(t)(as). Then, for that cell, update the Gaussian mixture tion and Maximizing) algorithm to obtain p(t)(y[x). For other cells, set p(t)(y[x) = p(t-1)(y[x). The most impor-tant feature of SDLE and SDEM algorithms is that they gradually discount the effect of past examples in the on-line process. This makes the outlier detector adaptive to non-stationary sources, of which the mechanism for generating data may change over time. The SDLE algorithm was devel-oped in [21] as an on-line discounting variant of the Laplace law based estimation algorithm, while the SDEM algorithm was developed in [21] as an on-line discounting variant of the incremental EM algorithm (see [15]). 
III) SmartSifter gives a score to each datum on the ba-sis of the learned model. Here we calculate the HeUinger score defined as follows: Let p(t)(x,y) be the probability distribution learned after obtaining the tth datum. Then the Hellinger score at the tth datum is given by 
Intuitively, this score measures how large the distribution p(0 has moved from p(t-1) after learning from (ast, a highly scored data indicates a high possibility that the datum is an outlier in the sense that it greatly contributes to changing a statistical model. 
The computation time for SmartSifter is O(dam) where d is the data dimension and m is sample size. 
For a given dataset, we may run SmartSifter to sequen-tially score the data then sort them according to their scores. 
Then the sorted dataset is a final output of SmartSifter. 
As for the rule generation, we use a stochastic decision list as a representation of a classifier and employ the principle of minimizing extended stochastic complexity or stochastic complexity as a criterion for rule selection. 
Let E be a multi-dimensional space called a domain and let .A4 = {0, 1} where "1" means a positive datum while "0" means a negative one. We denote a random variable over E as ~ and that over 
A4 as #. Each component of ~ is called an attribute. For a given positive integer k, let 7-be a set of k-terms on E where a k-term is a conjunction of at most k attribute conditions, (~ denotes the j-th attribute. 
Here for any given input ~, L assigns/~ = vi with probability pl where the i is the least index that ~ makes ti true. Hence it has the following meaning: 
Stochastic decision lists have been developed in a scenario of learning stochastic rules [19l as a probabilistic variant of Rivest's decision lists [16]. We employ an algorithm DL-ESC [11] for learning a stochastic decision list from given positive and negative examples. DL-ESC takes as input a data sequence of pairs of ~ and/~: (~1,/~1)"" (~m,/~,n) and outputs a stochastic decision list. The key of DL-ESC is to employ the principle of minimizing extended stochastic complexity(ESC) [20] in rule selection. Here the ESC is a kind of extension of Rissanen's stochastic complexity (SC [140, which is the measure of information quantity included in a data sequence. SC measures the information in terms of the logarithmic loss, equivalently the codelength required for encoding the sequence, while ESC measures it in terms of a general loss function, specifically the 0-1 loss in this case. We may also use an algorithm DL-SC, which is obtained by just replacing ESC with SC in DL-ESC. The details of DL-ESC/DL-SC are given in Appendix. 
The computation time for DL-ESC/DL-SC is O(dkm) where d is the data dimension, k is a positive integer, usually set less than 4, and m is sample size. 
We used the dataset KDD Cup 1999 [6] prepared for net-work intrusion detection. The purpose of our experiment was to detect as many intrusions as possible without mak-ing use of the labels concerning intrusions. Although in KDD Cup 1999 the data labels were used in training for supervised intrusion detection, we used them only for the evaluation of algorithms. 
Each datum in KDD Cup 1999 is specified by 41 attributes (34 continuous and 7 categorical) and a label describing at-tack type (22 kinds: normal, back, buffer_overflow, ftp_write, warezmaster, etc.) where all labels except "normal" indi-cate an attack. In the original attributes, there are many attributes that are obtained by combining several attributes. We deleted such attributes from the original 41 attributes to get 13 attributes: duration, src_byte, dst_byte, srv_error, wrong frag, numofurgent, numoffail, service, protocol, land root login, guest, flag. We used the 13 attributes for DL-ESC. Further we selected four attributes: service, duration, src_bytes, dst_bytes for the use of SmartSifter. This is because these four were thought of as the most basic attributes. We were interested in seeing how well Smart-Sifter worked using these attributes only. We were also interested in generating rules using more attributes than SmartSifter. Only ~service' is categorical. Since the contin-uous attribute values were concentrated around 0, we trans-formed each value into a value far from 0, by y --log(x+0.1) 39l where the base of logarithm is e. 
The original dataset contains 4,898,431 data, including 3,925,651 attacks (80.1%). This rate of attacks is too large for statistical outlier detection. Therefore, we removed the data whose attribute logged in is negative. The resultant dataset, which we named SF, consists of 976,157 data, in-cluding 3,377 attacks (0.35%). Attacks that successfully logged_in are called intrusions. 
We further produced from SF five datasets S0,S1,$2,$3,$4 by random sampling, where each of them consists of about 10% of SF. Table 1 shows the data size, the number of in-trusions and the details of intrusions for each dataset. 
Let us illustrate how our framework works. First S1 was fed to SmartSifter where the first 8,000 data in S1 were not scored but used only for training because the model would not be well-trained in the early stages. After processing all of the data in S1, we sorted them according to their scores that SmartSifter gave. 
We create labeled examples by giving positive labels to the top 1% data of highest scores and negative labels to the 3% data in S1 randomly sampled from the remaining data. (Note that "positive" does not necessarily mean "intrusion," but highly scored data.) Then we input them to DL-ESC, which output the following decision list: 
Here (539/639) in the first rule means that the number of the data satisfying the condition "duration &gt; 0.742 &amp; protocol =tcp" is 639 while 539 of them were positive. We ignored the first rule because its accuracy (539/639) is not high enough. We picked up only the last rule in the list and use it as a filtering rule: 
Next we input a new dataset $2 into the rule, and let P be the set of data determined to be positive by the rule. The number of data included in P was 311 and 199 of them were intrusions of type back (63.98% accuracy, 63.17% coverage). 
No other type of intrusions was included. (Note: This in-formation about intrusions were not used for learning but only for the evaluation of algorithms.) This implies that the filtering rule successfully generalized a pattern of back and led to discovery of a specific group of outliers, which turned out to be a group of back. For the filtered dataset S2-P, we run SmartSifter again. 
Then we observed that 25 intrusions were included in the top 689(=1000-311) dataset Q of highest scores. Hence we could detect 224(=199+25) intrusions in the 1000 data included in $2. On the other hand, when $2 was fed to 
SmartSifter only, 110 intrusions were included in the top 1000 data of highest scores. This implies that the coverage of intrusion detection in 1000 data was significantly increased from 34.92% (=110/315) to 71.11%(=224/315) by filtering with the rule plus SmartSifter. the filtered dataset was again filtered by the above rule. We observed that 35 intrusions were included in the extracted positive dataset and 70% of them were of type warezclient. 
This led to discovery of a specific group of outliers, which includes a group of warezclient. parison with SmartSifter. Throughout this section, we de-note our framework as R&amp;S (=Rule and SmartSifter) and 
SmartSifter as SS. For example, for the case where SO was used as a training set to construct a filtering rule, each of S1, $2, $3, and $4 was used for test, i.e., it was first filtered by the rule and SmartSifter was applied to the filtered dataset. 
Here the coverage of SS is the ratio of the number of intru-sions in the top n data (n =1,000, 2,000, 3,000) of highest scores produced by SS, to the total number of intrusions in the dataset. The coverage of R&amp;S is the ratio of the num-ber of intrusions in the output of R&amp;S, to that. The output here is the set which consists of P (fiitered data by rules) and Q (highest scored data by SS) in Figure 1. For example, consider the case in which S1 was input as test data and the number of outputs was 1,000 (see the column whose first line is SO in Table 2). Let the number of intrusions included in the positive dataset extracted by the filtering rule be N1. 
Let the number of intrusions included in the top 1,000 -IPI data of highest scores in the filtered dataset S1-P be N2. 
Then the number of intrusions detected by R&amp;S in the 1,000 of S1 is calculated as N1 + N2. This calculation was done for the cases in which the number of outputs were 1,000, 2,000, and 3,000. Each value in the column whose first line is SO is calculated as the average of coverages over all test datasets: S1,$2,...,$4. The second column shows the cover-age of SS averaged over all training sets: S0,..,$4, while the third column shows those of R&amp;S. is less than 2000 data (=2%), R&amp;S could detect significantly more intrusions than SS. Remarkably, R&amp;S attained 68.5% coverage in 1000 data on average while SS attained 43.1% for the same sample size. evaluated sample size increases. The horizontal axis shows the sample rate-the ratio of the size of evaluated sample to the total data size, while the vertical axis shows the cov-erage. We observe that the coverage of R&amp;S grows more rapidly than that of SS for 0-1% sample rates. When the sample rate is larger than 3%, they are not different so much. 
We are able to say that the outlier filtering contributes to improving the accuracy of SS for low sample rates. 
SS for small sample rates. Here the coverages of R&amp;S are plotted for all of different training sets. The coverages were calculated only for positive datasets extracted by the rule. For example, R&amp;S(S0) indicates the coverage of R&amp;S when 
SO was used as a training dataset to construct a filtering rule. All of the coverages of R&amp;S are concentrated around 64%-70%, which is 20%-55% higher than those of SS. 392 #back 70.8 I 69.1 64.3 69.4 68.9 73.1 I 73.0 69.5 71.6 71.6 75.9 74.6 71.6 74.7 74.5 
Figure 3: SS vs RSzS, Different Training Sets 
We have proposed a new framework for outlier detection by combining an unsupervised outlier detector: SmartSifter with a supervised learning algorithm: DL-ESC. An outlier filtering rule is constructed with supervised learning by gen-erating labeled data on the basis of the scores that Smart-Sifter calculates for unlabeled data. By using the rule as a preprocessing of SmartSifter, we are able to significantly improve the power of SmartSifter. Furthermore it helps the user to discover a general pattern that a specific group of outlier may have. We demonstrated through the network intrusion detection that our framework was effective both in identification and explanation of outliers. [1] V. Barnett and T. Lewis, Outliers in Statistical Data, John [2] F. Bonchi, F. Giannotti, G. Mainetto, and D. Pedeschi, A [3] P. Burge and J. Shawe-Taylor, Detecting cellular fraud [4] T. Fawcett and F. Provost, Adaptive fraud detection, Data [5] http://www.hnc.com [6] http://kdd.ics.uci.edu/databases/kddcup99/kddcup99.html [7] E. M. Knorr and R. T. Ng, Algorithms for mining [8] E. M. Knorr and R. T. Ng, Finding intensional knowledge [9] T. Lane and C.E. Brodley, Temporal sequence learning and [10] W. Lee, S. J. Stolfo, and K. W. Mok, Mining audit data to [11] H. Li and K. Yamanishi, Text classification using [12] Y. Moreau and J. Vandewalle, Detection of mobile phone [13] U. Murad and G. Pinkas, Unsupervised profiling for 393 [14] J. Rissanen, Fisher information and stochastic complexity, [15] R. M. Neal and G. E. Hinton, A view of the EM algorithm [16] R.L. Rivest, Learning decision lists, Machine Learning, 2, [17] S. Rosset, U. Murad, E. Neumann, Y. Idan, and G. Pinkas, [18] J.Takeuchi and K.Yamanishi, Empirical evaluation of an [19] K.Yamanishi, A learning criterion for stochastic rules, [20] K. Yamanishi, A decision-theoretic extension of stochastic [21] K. Yamanishi, J.Takeuchi, G.Williams, and P.Milne, APPENDIX: Learning Stochastic Decision Lists 
Below we give a brief sketch of DL-SC(DL-ESC)-the algo-rithm for learning stochastic decision lists. A data sequence is denoted as D TM = (~1,#1)"'" (t~,~,/J,~) where ~t is an in-put, tt~ is a corresponding output, and m is sample size. 
First we give a strategy for discretizing a continuous vari-able. Here the discretizing means determining a threshold r for each variable ~i then change ~ into a binary variable 
D m, I(r[D "~) is minimized with respect to ~': where D~ is a subsequence of D "~ such that ~i &gt; r while 
D_ ~ is a subsequence of D "~ such that ~i &lt; ~. Here I(D r~) is stochastic complexity [14] of D m calculated as follows: where the base of the logarithm is 2, rn~ is the number of examples in D "~ such that # = 1, and H(z) = -zlogz 
For a set of attributes: V = {~1,'." , (~}, for a given input data sequence D m, for each ~, we choose v/= arg rain, ~i = 0 if (~i &lt; vi). V = Dis(V,D "~) = {~1,'" ,~}. Here a discrete variable remains the same. 
The algorithm DL-SC for learning stochastic decision lists consists of two processes: Growing and pruning. In the growing process a rule minimizing the splitting complexity is sequentially added to the list. Below we show how to calculate the splitting complexity. For t E T, for a given data sequence D m, we let D~ be a subsequence of D" such that ~ makes t true, and let D~t be a subsequence of D m such that ~ makes t false. We define the splitting complexity of t w.r.t. D "~ by where I(D m) follows (1). In the process of growing, we choose t so that I(tlD "~) is minimized w.r.t.t. Given: 
Initialization: 
Growing 
Pruning where rnt is the number of examples in D '~ such that/~ = 1, and A is a positive real number. The quantity (3) is called the extended stochastic complexity (ESG) [20] for the 0-1 loss. bottom in order, so that the total complexity is minimized. 
Below we show how to calculate the total complexity. For a decision list L, for a conjunction t appearing in the bottom rule in L, we let 1)t be a dataset such that ~ makes t true. 
We calculate the total complexity of D m w.r.t. L by where the sum is taken over all ts appearing in L, and A' is a positive real number, which may be different from A in (3). 
Here ~(t) = log IT[ for the total number IT I of conjunctions with at most k attributes over 17. A list L for which (4) is minimized is finally output. 
