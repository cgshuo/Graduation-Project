 Spelling correction is a research branch of Natural Language Processing (NLP), which has been widely used in many applications. For example, in English spelling correction, researchers achieve high precision and recall based on statistical machine-learning methods. According to the literature, many methods are based on predefined confusion sets [Golding 1995; Golding and Schabes 1996; Golding and Roth 1999; Li et al. 2006; Zhang et al. 1999]. A confusion set is composed of words with which a word is confused. The confusion set of an English word can be built by using either similar or identical pronunciation between words or edit distance [Fossati and Di Eugenio 2007; Jurafsky et al. 2000; Mays et al. 1991]. Our experience indicates that Chinese confusion sets are also fundamental resources for the Chinese spelling correction [Liu et al. 2013]. Unfortunately, there little work has been done on the generation of Chinese character confusion sets, although similarity in pronunciations and shapes of Chinese characters 1 are used to define confusion sets.

If confusion sets are too large, then it not only reduces the efficiency of automatic spelling correction but also suffers a high false-positive rate and thus reduces the proofreading accuracy. On the contrary, if confusion sets are too small, then many errors cannot be found. So, it is important to generate confusion sets with reasonable coverage.

Unlike English, Chinese characters are hieroglyphic and are typed with special methods, for example, the pinyin input method, which is based on their pronunciation, or the five-stroke input method, which is based on their strokes. It should be noted that different input methods may lead the user to make different misspellings.
Example 1:  X  (charge, chong) 2  X  X nd X  (rush, chong) X  are always confused, because  X   X  X nd X   X  have the same pinyin  X  X hong X ; people often incorrectly input  X  (rush calls, chong hua fei) X  instead of the correct one,  X  (pay calls, chong hua fei). X 
Example 2:  X  (dial, bo) X  and  X  (pull, ba) X  are always confused. They are different in pinyin but their shapes are very similar. So  X  (dial phone, bo dian hua) X  often is input as  X  (pull phone, ba dian hua). X 
Example 3:  X  (not, bu) X  is sometimes confused with  X  (than, bi), X  because the end part  X  X  X  in the pinyin  X  X u X  and the end part  X  X  X  in the pinyin  X  X i X  are adjacent in the computer keyboard.

Why do people get confused with some Chinese characters? On the one hand, people do not know how to use some Chinese characters, such as ignoring the difference among homophone characters. On the other hand, it is associated with input methods. The pinyin input leads to similar pinyin errors, and the shape input (such as Five-Stroke Input) generates similar shape errors. In addition, quick and careless choices of characters often cause errors.

Conventional methods of obtaining confusing words are directly based on the simi-larity of pinyin and shapes of characters, but these methods would cause more unrea-sonable confusing words. According to the frequency of Chinese characters, the high-frequency Chinese characters are seldom misused to the low-frequency ones, even if they have the similar pinyin or similar shape. For example, the Chinese character  X  (charge, chong) X  and  X  (motherwort, chong) X  have the same pinyin  X  X hong. X  When we want write  X  , X  we often write it as  X  , X  while we are unlikely to write  X   X  X s X  . X  The main reason is that  X   X  is a high-frequency word, and the  X   X  is an unusual word. The intuition behind this phenomenon is that we are unlikely to write a common word as an unusual word. Thus, it is unreasonable to generate a confusion set by similar pinyin and shape. But it is a huge effort for people to generate confusion sets manually, and people would often omit many confusing words.

In this article, we introduce a method of generating confusion sets of Chinese char-acters. Our method is composed of two major phases. In the first phase, we build a list of seed confusion sets, one for each Chinese character, based on similarity in character pinyin and character shape. In this phase, all confusion sets are constructed manually, and the confusion sets are organized into a graph, called a seed confusion graph (SCG), in which vertices represent characters and edges are pairs of characters in the form (confused character, confusing character).

In the second phase, we extend the SCG by acquiring more pairs of (confused char-acter, confusing character) from a large corpus. For this, we use several word patterns (or patterns) to generate new confusion pairs and then verify the pairs before adding them into the SCG. Comprehensive experiments show that our method of extending confusion sets is effective. Also, we shall use the confusion sets in Chinese misspelling correction to show the utility of our method.

The rest of the article is structured as follows. In Section 2, we introduce the related works. We introduce the seed confusion set graph and self-extension method in Sec-tion 3. In Section 4 we introduce word patterns and statistics to discover new confusion pairs. Experimental results are presented in Section 5. The last section concludes our work and outlines a few future research problems. In the literature, there are several ways to obtain the confusion set of English words. In 1991, Mays found confusing words in the dictionary that are one typo away from each other [Mays et al. 1991]. Other researchers found confusing words that have the same or similar pronunciations. Some methods identified confusing words based on the distance between words. The main distance function is the Levensthein distance, also known as minimum edit distance [Jurafsky et al. 2000], which is the minimum number of edit operations necessary to transform one word into another. An edit operation is a character insertion, deletion, or substitution, and it sounds reasonable to include in the confusion set of a word all the words with Levensthein distance less than or equal to 2 from the original word.

Unfortunately, we cannot find Chinese confusion characters like in English. There are many reasons that make Chinese characters confusing. One typical reason is the similarity of pinyins of two characters. A Chinese character has one or more pinyins. Chinese characters with a similar pinyin are often confused by people. Another reason is the shape similarity of two characters. Chinese characters have a graphic structure that is composed of strokes and radicals. The more similar the shapes of the two characters, the more confusing are the two characters.

The last reason is the similarity in character meaning. If two Chinese characters have similar meaning, then they are often confused. The confusion set of a Chinese character is often used in the spelling correction [Zhang et al. 2000; Ren et al. 2001]. A confusion set is often built based on a Chinese input method [Zhang et al. 2000; Ren et al. 2001]. Chinese characters can be converted to their pinyin. So we can compute pinyin edit distance to get the confusion set [CAO et al. 2009; Diao et al. 2010]. Cao et al. improved the pinyin edit distance [CAO et al. 2009]. However, the pinyin edit-distance method does not consider the pinyin similarity, such as the similarity in initial consonants and vowels, and does not consider the tone. In such a method, if the initial consonants or vowels of pinyin are similar, then the edit distance is smaller than 1. And if the initial consonants and vowels of two characters are differ, then they give a penalty when the edit distance is computed. The edit-distance method also uses the weight of tone in computing the distance.

For two given characters, there are several methods to compute their shape similar-ity. Diao et al. used the edit distance on the Five-Stroke Input codes of characters to measure their shape similarity [Diao et al. 2010]. Feng et al. used Chinese character lattice data or vector data [Feng and Cao 2006]. The shape similarity of Chinese char-acters is computed by using their components and structure information [Lin and Song 2010; Liu and Lin 2008; Song et al. 1964].

According to Liu et al. [2009], the most common types of error are characters with similar shapes and characters with similar pronunciation. In their work, the confusion sets deal with characters with similar pronunciation and shapes. Zhang et al. built confusion sets based on Shape-Input (Five-Stroke Input) [Zhang et al. 2000]. They used the Five-Stroke Input Codes of Chinese characters, then compiled the confusion sets based on the similar codes because the Five-Stroke and Input Codes reflect the shape of Chinese characters. Chen et al. compiled all of the characters that have the same pronunciation from a dictionary and made them the elements of a confusion set [Chen et al. 2010]. To reduce the size of a confusion set, they treat characters with different tones as belonging to different sets. They used a simple rule to compile characters with similar shapes. They used the key component of a character and its radical as the basic shape of the character to find the characters with the same radicals. After that, they compiled 214 confusion sets with a total of 9,752 different characters.

The existing research results show that their methods to compile confusion sets are only based on pinyin similarity and shape similarity. They do not consider other important factors, such as frequency of Chinese characters. In the following section, we will propose methods based on seed confusion graph and pattern matching to compile new confusion sets. In this section we present the construction process of seed confusion sets and seed confusion graph. First, we present some useful notions.
Definition 1( The Confusion Set of a Chinese Character ). Given a Chinese character c, the confusion set of c is composed of the confusing characters of c. Formally, the For example, CSet ( (a)) = { (a)}, CSet ( (a)) = { (a)}, CSet ( (a)) = { (a), (a)}, CSet ( (a)) = { (a), (a)} are four confusion sets.
 Definition 2( Confusion Graph of Confusion Sets ). Given a list of confusion sets ordered pair Typo_CG = ( , E), where directed arc, where V i represents a confused character and V j is a confusing character of V i .
 V &gt; occur in Typo_CG, then we call the Chinese characters V confusion characters (that is, V i and V j are confused with each other); otherwise, if e = &lt; confusion characters. In this subsection, we first discuss how to generate seed confusion sets, and then we build the confusion graph from seed confusion sets.

In Chinese characters, some characters are similar in pronunciation. For example, the pinyins  X  X an X  and  X  X hang X  have similar initial consonants; that is,  X  X  X  and  X  X h X  are similar. Note that  X  X an X  and  X  X hang X  also have similar vowels; that is,  X  X n X  and  X  X ng X  are also similar in pronunciation.

Based on the discussion above, we compute the pinyin similarity ( PSim ( c 1 ,c 2 )) and to the methods proposed by Feng et al. Feng and Cao [2006] and Wang et al. [2012a, 2012b].
 c . The definition of PSim ( c i ,c j ) [Feng and Cao 2006] is as follows:
Feng et al. introduced the types of pinyins and gave the similarity between the and Cao 2006]: If ic i = ic j and v i = v j , then CSim([ic i , v i ] , [ic j , v j ]) = 1.
If ic i = ic j or v i = v j , then the Chinese characters are divided into about 400 classes. And they give the similarity between all classes based on Phonetics, such as CSim([b , ai] , [b , ei]) = 0 . 8, CSim([ch , i] , [c , i]) = 0 . 92. The definition of SSim(c i , c j ) is as follows [Wang et al. 2012b]: given in Wang et al. [2012b]. According to Wang et al. [2012b], they parse Chinese characters to structure trees that are composed of strokes. Chinese characters are hieroglyphics that have structure. Some characters have left-right structure.  X  (hai, of c i and c j , such that the structural similarity  X  (hai, sea) X  and  X  (he, river) X  is 1 based on the parse trees of the two characters. They define the similarity between strokes.

In addition, we have the following heuristics in which we obtained a comprehensive character analysis: Heuristic 1: The probability of a high-frequency Chinese character being mistaken as a low-frequency one is much smaller than the contrary.
 Heuristic 2: The probability of a simple Chinese character being mistaken as a complex one is much smaller than the contrary. Here, a character is simple if it has a relatively small number of strokes.

Based on the two heuristics, we introduce similarity measures to compute the close-ness of two characters: Definition 4( Cognitive Similarity of Chinese Characters ).
 where, f(c) is the frequency function of character c that is obtained from a large Webpage corpus in the experiment.
 According to the above formula, we build seed confusion sets using Algorithm 1.
To use algorithm 1 to generate the Chinese character confusion sets, we selected 11,934 Chinese characters. The selection is mainly based on their frequency on Chinese Webpages, and an eyeballing of the frequency table made us pick them up.
Manually checking the generated confusion sets is mainly through the following ways. First, if two characters have similar initial consonants and the vowels are ad-jacent key in the keyboard, then the two characters are new confusion pairs. We add the new pair into the confusion sets. Second, if one generated confusion pair are not similar in pinyin, and have less shape similarity, we remove the confusion pair from the generated confusion sets. Third, we use a word that includes the confused character, replace the confusing character, and put it in the search engine (such Baidu or Google) to validate the confusion pair. Through the generation steps, we obtained a total num-ber of 48,865 confusion pairs. 3 On average, each Chinese character has approximately 4.9 confusing Chinese characters.
 Part of the experiment results are presented in Table I.

By definition 2, we form a confusion graph based on the built confusion sets, called the seed confusion graph, or seed graph. To end this section, we introduce two notions of confusion graph.

Definition 5( Confusion Outdegree and Indegree ). Given a Typo_CG and a vertex V , the outdegree of V , denoted as Outdegree ( V ), is the total number of arcs where V represents the confused character. Likewise, the indegree of V , denoted as Indegree(V) , is the total number of arcs where V represents a confusing character. Figure 1 is part of a confusion graph of some confusion sets. In the figure, Indegree ( ( tai )) = 5, and Outdegree ( ( tai )) = 1. The generated seed confusion graph may not be reasonable in two aspects: Some confu-sion pairs may be missing, and some pairs may not be reasonable since the generation steps are purely computed by using heuristic formulae and hypotheses.

To modify the seed confusion graph, we introduce two refinement rules: a self-extension rule and a removal rule. The self-extension rule aims to detect new con-fusion pairs that are reasonable but are missing in the seed graph, and the removal rule attempts to remove unreasonable confusion pairs. It is worth noticing that this part makes use of the graph itself to extend, we call it self-extension, as contract to the extension based on Webpages, which will be presented in the next section. 3.3.1. The Modification Rules. First, we introduce two useful notions.
 Definition 6( Frequency-Weight of Chinese Character ). The frequency weight of a Chinese character is defined as: where n is the total number of Chinese characters (i.e., 11,934), freq(c) is the frequency of Chinese character c which we can get from our experimental Webpages.

Definition 7( Degree of Commonality of a Chinese Character ). The degree of common-ality of a Chinese character is defined as If f c &gt; X  (  X  is the threshold, in our experiment we set  X &gt; 0 . 1), then we call the Chinese character c as a common word; otherwise, it is an uncommon one.

First, we introduce the common sense: If two characters are bidirectional confusion characters, then the other characters that are confused with one of them are always confused with the other one. Now, we introduce the self-extension rule.
 Self-Extension Rule: Suppose that three Chinese characters, c i ,c j ,andc k , are bidi-rectional confusion characters. If there exists another Chinese character c, then the with c; that is, it is reasonable to add the edge &lt; c k ,c &gt; to the graph. Removal Rule: If a Chinese character c i is a common word and c j is an uncommon word, then, generally, c i cannot be confused with c j . Thus, if the graph has these edges, then we remove them from the graph.

In the next section, we use these rules to expand the seed confusion graph and remove the unreasonable confusion pairs. 3.3.2. The Algorithms. As discussed above, similarity in pinyin is one of the features that contributes to the confusion of Chinese characters. In the seed confusion sets, there are some similar pinyin confusion pairs, and we use these seeds and the extension rule to extend the confusion sets, as described in Algorithm 2.

Note that during the self-extension steps, we extend all possible confusion character pairs for all characters in the graph. But some of the extended pairs may not be real confusion pairs. So we must verify the resulting pairs based on the Removal Rule. There are many misused words on the Web pages, and thus we want to retrieve more confusion sets or pairs from the Web pages. In English, one always computes the similarity of two words using the edit-distance approach, the n-gram method, and so on. But, in Chinese, we cannot compute the similarity in the same way. For instance, using the edit-distance method, the similarity of the Chinese words  X  (receive, jieshou) X  and  X  (accept, jieshou) X  is 0.5, and the similarity of  X  (receive, jie shou) X  and  X  (approach, jie jin) X  is also 0.5. But the similarity of  X  (receive, jie shou) X  and  X  (accept, jie shou) X  is greater than the similarity of  X  (receive, jie shou) X  and  X  (approach, jie jin) X  because the  X  (receive, shou) X  and  X  (accept, shou) X  have the same pinyin, but the pinyin of  X  (receive, shou) X  is far away from the one of  X  (approach, jin). X 
In Section 3, we defined the similarity of Chinese characters based on the pinyin sim-ilarity and the shape similarity. We define the new similarity of two Chinese characters (c ,d i ) as follows:
The weight parameters  X  and  X  are normalized, that is,  X  +  X  = 1. Weight parameters  X  and  X  reflect the relative of the pinyin similarity and the shape similarity that can be set according to the concrete applications. For example, in the Chinese Optical Character Recognition (OCR), the confusion of Chinese words is mainly contributed by the shape similarity, so we can set  X  = 0and  X  = 1.

With the similarity of Chinese characters, we define the similarity distance function
If m = n:
Other m = n:
Equation (10) is recursive definition. Therefore, the similarity of Chinese strings W 1 and W 2 is defined as follows:
In this article, our main objective is to acquire the confusion set of Chinese characters, so we just consider the similarity between words that have the same length. In a Chinese sentence, there are no natural spaces to separate the words, so, first, we need to segment the Chinese sentence. We do not need to solve the disambiguation of Chinese word segmentation, so we use the reverse maximum matching algorithm for the word segmentation task. If a character of a Chinese word is replaced by another character, then the word will be split to a loose substring.

For example, in  X  (for no reason at all, wu yuan wu gu), X   X  (reason, gu) X  is often mistaken as  X  (mind, gu), X  partly because the pinyin of  X   X  X nd X   X  are identical. After segmentation,  X   X  has three parts:  X  (to have no opportunity, wuyuan), X   X  (nothing, wu), X  and  X  (mind, gu). X 
There are two types of Chinese words: single-character words and multi-character words (the number of Chinese characters in the words is greater than 1). In the fol-lowing, we only consider pattern matching for tri-character words and quad-character words. One reason is that these words cover all Chinese characters in the dictionary, and the other reason is that if a word has more Chinese characters, it will more con-textual information, and the extension of the confusion graph is more effective.
Through a comprehensive analysis of segmentation results of many tri-character words and quad-character words in which one or more characters are replaced by other characters, we summarize the following patterns: If one or more characters are replaced by other characters in the tri-character word W, then we use W = c 1 c 2 c 3 to represent various possibilities. After segmentation, it has the following possible patterns. In pattern P 1 ,c 1 ,c 2 ,c 3 are all uni-character words after the segmentation because some characters in the W are misused. For example, the  X  (tube, tong) X  in  X  (kaleidoscope, wanhuatong) X  is misused as  X  (same, tong). X  Then, after segmentation, there are three uni-character words that are  X  (ten thousands, wan), X   X  (flower, hua), X  and  X  (same, tong). X  If one or more characters are misused in a tri-character word, then it is possibly segmented to a bi-character word and a uni-character word. In the pattern P 2 ,c 1 c 2 is a bi-character word and c 3 is a uni-character word. For example, the  X  (box, he) X  in  X  there occurs  X  (music, yin yue) X  and  X  (close, he). X  In pattern P 3 ,c 1 is a uni-character word and c 2 c 3 is a bi-character word after W are segmented. For example, the  X  (flower, hua) X  in  X  (kaleidoscope, wan huatong) X  is misused as  X  (word, hua). X  After segmentation, there are  X  (ten thousand, wan) X  and  X  (microphone, hua tong). X 
If one or more characters are replaced by other characters in the quad-character segmentation, it has the following patterns. In pattern P 4 ,W is segmented to four uni-character words. For example, the  X  (black, wu) X  in  X  (love me love my dog, ai wu ji wu) X  is misused as  X  (bird, niao). X  Then after segmentation, there occur  X  (love, ai), X   X  (house, wu), X   X  (and, ji), X  and  X  (bird, niao). X  In pattern P 5 ,W is segmented to a bi-character word and two uni-character words. In P ,c 1 c 2 is a bi-character word, c 3 and c 4 are uni-character words. For example, the  X  (reason, gu) X  in  X  (for no reason at all, wu yuan wu gu) X  is misused as  X  (take care of, gu). X  Then after segmentation, there occur  X  (to have no opportunity, wu yuan), X   X  (no, wu), X  and  X  (take care of, gu). X  and a uni-character word (c 4 ). For example, the  X  (stop, zhi) X  in  X  (one X  X  mind settles as still water, xin ru zhi shui) X  is misused as  X  (this, ci). X  Then, after segmentation, there occur  X  (heart, xin), X   X  (such, ru ci), X  and  X  (water, shui). X  In this pattern, W is segmented to two uni-character words and a bi-character word. the  X  (reason, yuan) X  in  X  (for no reason at all, wu yuan wu gu) X  is misused as  X  (original, yuan). X  Then, after segmentation, there are  X  (no, wu), X   X  (original, yuan), X  and  X  (for no reason, wu gu) X . In this pattern, W is segmented to a tri-character word (c 1 c 2 c 3 ) and a uni-character word (c 4 ). For example, the  X  (disease, zheng) X  in  X  (diabetic syndrome, tangniaobingzheng) X  is misused as  X  (syndrome, zheng). X  Then, after segmentation, there occur  X  (diabetes, tang niaobing) X  and  X  (syndrome, zheng). X  In this pattern, W is segmented to a uni-character word (c 1 ) and a tri-character word (c 2 c 3 c 4 ). For example, the  X  (electric, dian) X  in  X  (electric kettle, dianre shui hu) X  is misused as  X  (point, dian). X  Then, after segmentation, there occur  X  (point, dian) X  and  X  (kettle, re shui hu). X 
Generally, the occurrence frequency of a word being right in the corpus is much greater than that of the word being misused. So, we assume for the loose substring W = c i ... c j , after pattern matching the candidate set is CandidateList(W) = {W k |W k CandidateList(W) . 4.3.1. Verification by Bi-gram. In a Chinese sentence after segmentation, there exist many reasonable loose substrings, because the Chinese words are composed of one or more Chinese characters. But they can match Chinese words in the dictionary, too. So, we exclude the reasonable loose substrings first. For example, for the sentence  X   X  (he, ta) (of, de) (diabetes, tang niao bing) (just, zheng) (be in, chuyu) (convalescence, kang fu qi). X   X  (he, ta), X   X  (of, de), X  and  X  (just, zheng) X  are uni-character words that make up loose substring with the surrounding words. By using the pattern P 8 , the loose string  X  (diabetes, tang niao bing) (just, zheng) X  is matched to the word  X  (diabetes, tang niao bing zheng), X  but in this sentence  X  (he, ta) (of, de), X   X  (diabetes, tang niao bing) (just, zheng) X  are the reasonable loose substrings. So we must exclude this situation. In this sentence, we find that  X  (be in, zheng chu yu) X  is the reasonable bigram. And after a statistics analysis,  X  (diabetes, tang niao bing) (just, zheng) X  and  X  tang niaobing) (just, zheng) X  is not the misusing of the word  X  (diabetes, tang niao bing zheng). X 
We calculate word bi-grams in our large experimental corpus and use the word bi-grams to judge the loose substring which have probably misused words. In order to judge the beginning and ending word of a loose substring, we get the context with the loose substring to judge whether it is wrong. We have the following hypothesis: If the loose substring is reasonable, then the co-occurrence of all bi-grams of the loose strings are greater than the threshold. Based on the hypothesis, we can remove the reasonable loose substrings such as  X  (his, ta de). X 
Formally, we use the pointwise mutual information to measure the dependency of two words as follows: and then we can build the following model: is the probability estimated by the method of maximum likelihood estimation based on our experimental Webpage corpus.

We get the left context word and right context word of the loose substrings to be judged by the above model. If the F values of all bi-grams are 1, then the loose substring is reasonable; otherwise, we think the loose substring probably is misused words and we match it to patterns to get its right words in the dictionary. We will verify them in the next section. 4.3.2. Verification by Confusion Probability. After pattern matching, we get a candidate set of a loose substring. In this section, we discuss how to find the right word of the loose substring.

If a Chinese word is misused, then the misused word and its right word have the same context. The occurrence frequency of the right word is greater than that of the misused one in the same context. We use confusion probability [Essen and Steinbiss 1992] to find the right word.

Formally, the confusion probability P c estimates the possibility of one word W 1 can be replaced by another word W 2 . Our main objective is to find the most possibly right word in the candidate set CList(W) of the loose string W . The problem can be formally defined as follows:
Chinese words are contextually relevant; that is, a Chinese word is misused only in the specific contexts. So we compute the confusion probability like that in Li et al. [2006]: where W belongs to the set of words that occur both in a context of W 1 and in a context of W 2 .

We compute all confusion probabilities of W 1 and each word in the candidate set and select the word with the max confusion probability as the right word.
 To show the effectiveness of our extension method, we conduct two experiments using the graph-based method and pattern-matching method, respectively. For each of the experiments, the performance is evaluated by using the following metrics:
Accuracy: The number of correct confusion pairs generated by the method divided by the total number of pairs generated by method.

Addition rate: The number of correct confusion pairs generated by the system divided by the total number of confusion pairs.

Furthermore, we show the reasonability of the extended confusing sets in Section 5.3. In the experiment, we create 48,110 seed confusion character pairs through the algo-rithm introduced in Section 3.2 and then extend the confusion sets based on the graph of the seed confusion sets. After the experiment, we discover 20,210 new confusion pairs. Through manual verification, more than 17,532 pairs are reasonable. So the addition rate is 36.44%. Table II shows part of results before the extension and after the extension. In our experiment, we use a web corpus of 8GB size to discover confusion pairs. We scrawl the Chinese News web from internet by a web spider. After cleaning up the Html code, the size of the corpus is 8G. Then use a Chinese word segmentation tool called ICTCLAS 4 to segment the corpus. After pattern matching, we obtain 15,133 confusion pairs.

After proofreading manually, the number of correct confusion pairs is 13,218, and 7,872 pairs are really new confusion pairs. The accuracy is 87.35%, and the addition rate is 16.3%. Table III shows part of the results. In order to demonstrate the reasonability of the generated confusion sets and the ef-fective of our methods, we use both the confusion sets of before extending and after extending to solve the task of Chinese spelling correction. In this article, we only eval-uate the reasonability of the confusion sets, so we just use a simple method: dictionary look-up. In order to test the reasonability of the generated confusion sets, we must use the real corpus. We use QA Log Corpus because the QA logs are informal texts which have abundant misused characters. We extract 20,000 sentences from a QA Log randomly. First, we manually labeled the misused characters in these sentences, as shown below:
In the above example, the Chinese character  X   X  is misused as  X  , X  and we label it as this:
We label 20,000 sentences manually, and we find and replace using the Chinese word dictionary and confusion sets that we generate in the experiment. There are 3,813 misused characters in the 20,000 sentences. We inspect the right character, which we replace to see whether it is in the confusion set, and then calculate the recall rate and precision rate. In the experiment, we just demonstrate the reasonability of the confusion sets, so we do not consider problems such as ambiguity. If the replaced result occurs in the test corpus, then we think the confusion pair is reasonable.
In the above example, we get the Chinese word  X   X  and the confusion set of  X   X :CSet( ) = { , ... }, and replace the  X   X  with the each word in the CSet( ), obtaining  X   X ,  X  , X  and so on. Then we find that in the corpus the replaced  X   X  occurs, implying that it is reasonable to have  X   X  X nCSet( ).
 The performance is evaluated by the following metrics:
The Error-Checking Precision:
The Error-Checking Recall:
The test result using the confusion sets before extending show that the recall rate is 75.2% and the precision rate is 89%. The test results using the confusion sets after extending show that the recall rate is 97.8% and the precision rate is 95.66%. Then we get the conclusion that the confusion sets generated by our proposed methods are useful to Chinese spelling correction. In this article, we introduce the method for generating confusion sets for Chinese characters. First, we create a seed of confusion set for each character and build a graph of confusion sets. Based on the graph and modification rules, we propose a self-extension method to extend and modify the graph. Experimental results show that the self-extension method is effective, leading to an increase of confusing pairs by 36.4%. Also, we extend the confusion sets based on pattern matching and the large corpus. We propose a number of patterns and a matching algorithm. After pattern matching, we verify the results using bi-gram and context probability, and a precision rate of 87.35% and the addition rate of 16.3% are achieved in the extension of confusion pairs. Finally, we evaluate the extended confusion sets by using them in the Chinese spelling correction, which shows that the confusion sets are very reasonable.

There are several problems left in the current work, and we will continue to address the following in the future: (1) Although useful, the heuristic rules proposed in the graph of confusion sets are not (2) The confusion sets should be categorized based on more applications, such as speech (3) Using the pattern-matching method to extend confusion sets, we just use tri-(4) A fully pledged application of confusion sets in Chinese spelling correction is to be
