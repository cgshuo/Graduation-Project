 Liang Du 1 , 2 , Yi-Dong Shen 1 , Zhiyong Shen 3 , Jianying Wang 4 , and Zhiwu Xu 1 , 2 Clustering is a common technique for data analysis used in many AI fields, such as machine learning, data mining, pattern recognition. In the last decade, many approaches have been developed to solve the problem of clustering ensemble. Clustering ensemble (See in Figure 1(a)), a.k.a. clustering aggregation [1] or of a data set, coming from multiple base clusterings, into a single consolidated clustering result (  X  )usinga consensus function (  X  ). To construct a consensus function, the key challenges include: 1) characterizing the relationships between the base clusterings; 2) conducting a single consensus clustering result.
In the light of these challenges, we pro pose a novel consensus framework for clustering ensemble, called Self-Supervised Framework for Clustering Ensemble (SSCE). The key idea of the proposed framework (See in Figure 1(b)) is to treat X , a matrix induced from base clusterings (  X  1 ,  X  2 ,...,  X  m )asa pseudo data matrix. On the other hand, each individual base cluster assignments could be viewed as pseudo class labels. We call the task of learning a set of linear supervised learning, since there is actually no supervised information. Viewing from bayesian perspective, we assume the parameters of each classifier share the same prior distribution (with parameter  X  ). We then call the linear classifier, who take expectation (  X  ) of the prior distribution as parameters, the consen-sus classifier . Based on this consensus classifier, we are able to classify adata instance to a consensus cluster assignment (  X  ).

There is a significant feature in current clustering ensemble approaches, i.e., the consensus clustering is directly learned from the base clusterings without accessing the original data ( X ), which is widely recognized as an advantage of clustering ensemble methods. It is unknown, however, whether we could boost the performance of co nsensus clustering with accessing the original data. Suppose there are incoming data instances, thi sisanothercaseinwhichwemayalso consider the original feature of data instances. It is hard to assign them to the base clusters without accessing the original feature. Therefore, as to our best knowledge, there is no clustering ensemble approach which is capable of handling incoming data instances. This raises ano ther question whether we could assign the incoming data instances to the consensus clusters directly based on their original features.

The proposed SSCE framework could be extended to address the above issues by simply replacing the pseudo data matrix with the original data matrix (See in Figure 1(c)). By this means, SSCE incorporates the original data via char-acterizing the relationship between the original data and base clusterings. The consensus classifier learned from X and  X  1 ,  X  2 ,...,  X  m could be applied to an incoming data instance to obtain its consensus cluster assignment directly.
The main contributions of this paper include: 1. We propose a novel framework to characterize the linear relationship between 2. We extend the proposed framework to incorporate the original data for the 3. As to our best knowledge, this work is the first to handle the incoming data 4. We conduct extensive empirical evaluations with real life data sets to demon-In this section, we introduce some existing works in the fields of clustering en-semble and multi-task learning, which a re closely related to the problem studied in this paper.

Clustering Ensemble aims to generate a stable and robust consensus clus-tering by combining multiple base clustering of a dataset. In general, previous works in this area can be grouped into three categories. The first category is graph based approaches. For instance, [3] proposed three graph-based methods. The cluster-based similarity partitioning algorithm (CSPA) uses METIS to parti-tion the induced similarity graph (vertex = objects, edge weight = cluster-based similarity). The hyper graph partitioning algorithm (HGPA) uses HMETIS to partition the hypergraph (vertex = objects, hyperedge = cluster). The meta clus-tering algorithm (MCLA) collapses related hyperedges and assigns each object to the collapsed hyperedge in which it participates most strongly. In addition, [4] proposed the hybrid bipartite graph partition algorithm, which partitions the bipartite graph (vertex = objects and cluster) by spectral graph partition. [5] proposed an approach to partition weighted similarity graph.

In the second category the algorithms take advantage of probabilistic graphical models. [6] represents objects as a set of a ttributes from multiple clusterings, and offers a probabilistic model of consensus using a finite mixture of multinomial distribution in a space of clusterings. [7] proposed bayesian cluster ensemble (BCE), which is a generative probabilistic model for learning cluster ensemble.
The third category is matrix factorization based methods. It has been shown [8] that consensus clustering can be formulated within the framework of nonneg-ative matrix factorization(NMF). [9] proposed weighted consensus clustering, where each input clustering is weighted in such a way that the final consensus clustering provides a better quality solution. [10] proposed weighted graph regu-larized NMF method which incorporates bo th the feature based representation and multiple binary relationships based representation.

Multi-task Learning [11, 12] aims to perform multiple learning tasks to-gether to improve individual performance. Rather differently, in clustering en-semble, we aim to produce a single high quality consensus clustering. Moreover, multiple tasks are often performed on different data sets. While, clustering en-semble operated on an identical data set to reach a consensus. In this section, we propose a novel S elf-S upervised learning framework for C lustering E nsemble (SSCE) to produce high quality consensus clusterings. 3.1 Notations and Preliminaries Suppose we are given a data set with n samples X = { x 1 , x 2 , ..., x n } and m base denote the cluster of x j given by the i -th base clustering. We use k to denote the cluster numbers.

In this paper we investigate the usefulness of the features of the data set for clustering ensemble. The data set can be represented by original features, if it is available, or pseudo features. The pseudo data matrix based on base clusterings can be constructed as follows: For each base clustering  X  i  X  X  n ,weconstruct the binary indicator matrix  X  i , with a column for each cluster. Entries of this matrix  X  i j,k = 1 if the j object is assigned to cluster k . Then we concatenate all the block matrix  X  =(  X  i ,..., X  m ). The pseudo matrix is actually cluster based representation, which is also used in [3] to construct the graph. 3.2 Consistent Labeling To employ supervised learning approach for clustering ensemble we should align the inconsistency input cluster labels from different base clusterings. In order to achieve the most consistent labeling o f clusters between two base clusterings  X  i and maximum weight bipartite matching problem and can be formulated as follows: where { S ij } are the cardinality of inter section of objects labeled i by  X  i and objects labeled j by  X  j ,and { I ij } are indicators which det ermine the correspon-dence between the clusters in the two partitions. An optimal solution of the problem (1) can be found by Hungarian algorithm [13].

A consistent re-labeling of all the base clusterings can be obtained by using a ence  X  r , however, it is unavailable for clustering ensemble. In practice, any base clustering can be choose as a reference. Then all the remaining base clusterings can be relabeled by solving the problem i n Eq. (1) for every pari of partitions  X  , X  i ,i =1 , ..., l, i = r . Once all the base clusterings are relabeled and aligned, they can be seen as a set of self supervised labels of the data set. 3.3 Probabilistic Framework Givenanobject x j with original or pseudo features and self supervised label L i j under i base clustering. we want to find a consensus mapping function f : X X   X  . In this paper we use logistic regression as discriminative model. For 2-class problem the classification model for x j under i base clustering can be written in the form For multi-class problem, the discriminative model for x j under i base clustering takes the form where w ik denote the model parameters of class k under i base clustering. These models { w i } m i =1 can be seen as instances which are generated from a consensus model  X  . We use the assumption that the base clusterings are i.i.d. given. The joint distribution of data and model parameters reads where P ( w i |  X  ) is a gaussian prior on each base clustering independently. To model the relationships among these base clusterings we add matrix normal captures the relationships between features and the covariance matrix  X  models the relationships among different base clusterings. Then the MAP estimation of W and MLE estimation of  X  = {  X  , X  } can be obtained by minimizing the following objective function which is the negative log likelihood of posterior of W . The first three terms in Eq. (5) is convex with respect to both W and  X  if we take convex loss function. The last term is concave which make the problem difficult to optimize. The last term ln |  X  | is used to penalize the complexity of  X  . To simplify the optimize procedure, we replace the last term with a constraint trace(  X  )  X  1 to control the complexity, which has been adopted in [11] [12], the above problem becomes Though the optimization problem in Eq. (6) is convex w.r.t. all the variables jointly, it is not easy to optimize the problem w.r.t. all the variables simultane-ously. We solve problem Eq. (6) by alternatively minimizing the Eq. (6) with respect to each variable by fixing the others. This procedure is repeated until it converges. 4.1 Optimize W by Fixing  X  and  X  We keep u and  X  fixed and minimize over W , that is we solve the problem One straightforward way to learn W is to set the gradient w.r.t. W to0andsolve the corresponding linear system. Because the above problem is convex w.r.t. W , it is also convex w.r.t. w i with all other variables fixed. In this paper we adopt an alternative strategy to perform optimize on W , which is to optimize one column of w i at a time with the other column fixed. this alternative strategy will be guaranteed to converge to the optimal solution. For 2-class problem, the negative log likelihood of Eq. (2) is Hence, the gradient of the above problem with respect to w i is For multi-class problem, the negative log likelihood of Eq. (3) is: The gradient of w.r.t. w i is 4.2 Optimize  X  by Fixing W and  X  By setting the gradient of 6 w.r.t.  X  be 0, we get the close form solution of  X  4.3 Optimize  X  by Fixing W and  X  Fixing W and  X  ,  X  is determined by following problem: The close form solution of above problem is given by where the proof can be found in [11] and [12]. The overall approach, called SSCE, is summarized in Algorithm 1. In this section, we empirically evaluate the proposed SSCE framework over mul-tiple benchmark data sets. We begin with a description of these data sets with the evaluation metrics, and then provide the evaluation results of the consensus clustering as well as the generalization capability. 5.1 Dataset Description We carry out our experiments on totally 13 data sets from UCI machine learning repository. These data sets have been widely used in literatures of clustering ensemble, including [7] and [8]. An overview of these data sets, including numbers of instances, features and classes i n each data set, is given in Table 1. Algorithm 1: SSCE algorithm 5.2 Evaluation Measure We evaluate the clustering ensemble resul ts by comparing the co nsensus cluster-ing produced by clustering ensemble algor ithms with the provided class labels. Specifically, Accuracy of Clustering (AC) is adopted to measure the performance, which discovers the one-to-one relations hip between clusters and classes. Given apoint x i ,let p i and q i be the clustering result and the ground truth label, respectively. The ACC is defined as follows: where n is the total number of samples and  X  ( x, y ) is the delta function that equals 1 if x = y and equals 0 otherwise, and map (  X  ) is the permutation mapping function that maps each cluster index to a true class label. The best mapping can be found by using the Kuhn-Munkres algorithm [15]. The greater clustering accuracy means the better c lustering performance. 5.3 Compared Methods To demonstrate how the performance of cl ustering ensemble can be improved by our method, we compare the proposed approach with the results of running k-means on the original data set or the base clusterings (KC). These are often used as baselines to verify the clustering ensemble approaches often produce more ac-curate and robust results against single clustering methods. We also compare our method with the bayesian cluster ensemble (BCE) in [7], the NMF-based consensus clustering (NMFC) in [8], the cluster-based similarity partitioning algorithm (CSPA), the hyper graph partitioning algorithm (HGPA), and the meta-clustering algorithm (MCLA) described in [3]. For the last three methods, we use the authors X  matlab implementation ClusterPack 1 . We test our algorithm on data sets with original features (SSCE). we also report the results by experi-ments on pseudo data matrix (SSCE-P), which is always available for clustering ensemble. 5.4 Experiments on Consensus Clustering We use similar settings with BCE [7] to show the effectiveness of our proposed algorithm. For all reported results, there are two steps leading to the final con-sensus clustering. First, given n objects, we run k -means 2000 times with random initialization and obtain 2000 base clustering results, which are further divided into 100 subsets, with an n  X  20 base clustering matrix each. Then we run clus-tering ensemble algorithm 100 times on these subsets. All the data sets have been preprocessed such that each feature has zero mean and unit standard de-viation. To simplify our model, we set  X  1 =  X  2 in all the experiments, and the best parameter is obtained by search on the grid of { 0 . 01 , 0 . 1 , 1 , 10 , 100 } .
Table 2 shows the clustering accuracy on the data sets. It is observed that the advantage of the proposed algorithm is much more clear for clustering ensem-ble. For example, the average improvement of SSCE on original features over BCE, the second best algorithm, achieves 5 . 7% on all the data sets, the average improvement of our approach on pseudo data matrix (SSCE-P) over BCE is 3 . 4%. Besides, the proposed approach perform statistically significantly better than the compared methods on 10 / 13 data sets at 95% significance level. Its success can be explained by the fact that the features (Original or Pseudo) are complementary to the base clusterings. We notice that SSCE failed to achieve comparable performance on iris data set with original features. The reason may lies in the iris data set only have 3 features, which make it less discriminative in a supervised manner. When iris is trained on pseudo data matrix, which has 3  X  20 features, our algorithm produce compa rable results. We also observed that our algorithm perform similar on multi class data sets, and SSCE on original features perform better than its counterpart SSCE-P for 2-class problem. 5.5 Experiments on Generalization Capability One of the advantages of SSCE over other clustering ensemble methods is that it has an explicit mapping function over original features. Since SSCE has explicit mapping function, we can choose part of the data to learn a mapping function and use this mapping function to map the rest of data points to the clusters. To evaluate the generalization capability of SSCE, we design the following experi-ments. For each data set, we firstly randomly split the data set into two parts (60% and 40%), with the 60% used to train the model and the 40% used as the hold-out test set. Then we run kmeans 100 times on train set to generate the base clusterings. We then run clustering ensemble algorithms 5 times, each run with 20 kmeans as input base clusterings, we predict the cluster label of the test set. This whole procedure is repeated 10 t imes and the average accuracy, that is over 5  X  10 results, are reported.

Most of the clustering ensemble methods can not directly predict the label of unseen test data. To do this, we assign the cluster label of unseen data with the label of its nearest cluster center, which is computed from train set and consensus clustering result. [7] is a generative g raphic model, which can be used to infer the posterior distribution of the clusters. To do this, we firstly run k -means on test set to construct cluster-based representation, then we use the learned BCE model to infer the cluster assignment of the test data (BCE-Infer). we also report the results of prediction of BCE by nearest cluster center strategy (BCE-NC).
Table 3 summarizes results of the second series of experiments. Similar to results in the first experiment, our proposed approach SSCE usually outperforms the other approaches. For example, the average improvement of SSCE on original features over BCE, the second best algorithm, achieves 4 . 2% on all the data sets. In this paper, we design a novel consensus function for clustering ensemble. We treat the base clusterings as pseudo class labels and learn base classifiers for each of them. By adding priors to the parameters of these classifiers, we cap-ture the relationships between different base clusterings and meanwhile obtain a single consolidated clustering result. We provide the algorithms to estimate the parameters of the base classifiers as well as the prior parameters, from which we induce the consensus classifier. With empirical evaluations over multiple bench-mark data sets, we show that the proposed consensus function outperforms the traditional ones. We also demonstrate we may improve the performance of clus-tering ensemble via incorporating the original data features. Moreover, we exam the generalization capability of the proposed framework and show its advantage in handling incoming data instances. One area of future work is to investigate optimizing the label correspondence toge ther with the parameter estimation. We may directly handel inconsistent labeling problem from multiple base clustering with different number of clusters.
 Acknowledgments. We would like to thank all anonymous reviewers for their helpful comments. This work is supported in part by NSFC grant 60970045 and China National 973 project 2013CB329305.

