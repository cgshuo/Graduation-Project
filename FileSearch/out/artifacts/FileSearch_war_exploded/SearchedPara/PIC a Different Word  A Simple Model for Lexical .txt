 Lexical substitution (McCarthy and Navigli, 2009) is a task in which word meaning in context is de-scribed not through dictionary senses but through substitutes (paraphrases) chosen by annotators. For example, consider the following usage of the adjec-tive bright :  X  X he bright girl was reading a book. X  Valid lexical substitutions for bright include adjec-tives like smart and intelligent , but not words like luminous or colorful .

Originally introduced as a SemEval task in 2007, lexical substitution has often been used to evaluate the ability of distributional models to handle pol-ysemy (Erk and Pad  X  o, 2008; Thater et al., 2010; Dinu and Lapata, 2010; Van de Cruys et al., 2011; Melamud et al., 2015b; Melamud et al., 2015a; Kawakami and Dyer, 2015). Recent models include a simple but high-performing method by Melamud et al. (2015b), which uses the Skip-gram model of Mikolov et al. (Mikolov et al., 2013) to compute the probability of a substitute given a sentence context, and integrates it with the probability of the substi-tute given the target. The current state of the art is held by another model of Melamud (Melamud et al., 2015a), which uses a more complex architecture. In this paper we build on the simple model of Melamud et al. (2015b), as simpler methods are eas-We explore a weak form of supervision that recently has proved beneficial on many NLP tasks: using a language modeling task on unannotated data. We find a strong improvement over Melamud X  X  simple measure, particularly on the all-words ranking task. Interestingly, analysis of PIC shows it improves over baselines by incorporating frequency biases into pre-dictions. In the lexical substitution task, an annotator is given a target word in context and generates one or more substitutes. As multiple annotators label a target, the result is a weighted list of substitutes, where weights indicate how many annotators chose a par-ticular substitute (McCarthy and Navigli, 2009).
There have been numerous approaches on the lex-ical substitution task of varying complexity and us-ing various lexical resources (McCarthy and Nav-igli, 2007). Some approaches focus on explic-itly modeling an in-context vector (Erk and Pad  X  o, 2008; Dinu and Lapata, 2010; Thater et al., 2010; Van de Cruys et al., 2011; Kremer et al., 2014; Kawakami and Dyer, 2015), while others approach it using more sophisticated pipelines, in both super-vised (Szarvas et al., 2013) and unsupervised (Mela-mud et al., 2015a) settings. The latter is the current state-of-art system, and is based around generating and pruning second-order word representations us-ing language models.

In this work, we limit our comparisons to the model of Melamud et al. (2015b), a method which performs nearly state-of-art, is extremely easy to implement, and is a good testbed for focused hy-potheses. They propose a novel measure which uses dependency-based word and context embed-dings derived from Skip-gram Negative Sampling algorithm (SGNS) (Mikolov et al., 2013; Levy and Goldberg, 2014a). Their measure addCos for es-timating the appropriateness of a substitute s as a substitute for t in the context C = { c 1 ,c 2 ,... } is They also propose a similar measure balAddCos , which controls for the context size: balAddCos ( s | t,C ) = | C | cos ( s,t ) + We propose a new measure, called Probability-in-Context (PIC), based on SGNS context vectors to estimate the appropriateness of a lexical substitute. Similar to balAddCos , the measure has two equally-weighted, independent components measuring the appropriateness of the substitute for both the target
The values Z t and Z C are normalizing constants to make sure each distribution sums to one. This measure has two free parameters, W and b , which act as a linear transformation over the context vec-tors. These parameters are estimated from the orig-inal corpus , and are trained to maximize the pre-diction of a target from only its syntactic contexts (c.f. Section 4.4). Given this formulation, a natural question is why not train the embeddings to opti-mize the softmax directly? We choose to parameter-ize the measure rather than the embeddings because (i) SGNS embeddings are already popular and read-ily available and (ii) it ensures the quality of embed-dings remains constant across experimental settings.
To measure the importance of parameteriza-tion, we also compare to a non-parameterized PIC ( nPIC ), which only uses a softmax over the dot prod-uct: We compare our proposed measures to three base-lines: OOC, the Out-of-Context cosine similarity between the word and target (cos ( s,t ) ), and the addCos and balAddCos measures. It is important to note that existing papers on Lexical Substitution all contain subtle differences in experimental setup (vocabulary coverage, candidate pooling, etc.). We compare to our own re-implementation of the base-lines, so our numbers differ slightly from those in the literature. 4.1 Data sets We evaluate on three lexical substitution data sets.
SE07 : The data set used in the original SemEval 2007 shared task (McCarthy and Navigli, 2007) con-sists of 201 words manually chosen to exhibit poly-semy, with 10 sentences per target. For a given target in a particular context, five annotators were asked to propose up to 3 substitutes. As all our experiments are unsupervised, we always evaluate over the entire data set, rather than the original held-out test set.
Coinco : The Concepts-in-Context data set (Kre-mer et al., 2014) is a large lexical substitution cor-pus with proposed substitutes for nearly all content words in roughly 2,500 sentences from a mixture of genres (newswire, emails, and fiction). Crowdsourc-ing was used to obtain a minimum of 6 contextually-appropriate substitutes for over 15k tokens.
TSWI2 : The Turk bootstrap Word Sense Inven-tory 2.0 (Biemann, 2012) is a crowdsourced lexical substitution corpus focused on about 1,000 common English nouns. The data set contains nearly 25,000 contextual uses of these nouns. Though the data set was originally constructed to induce a word-sense lexicon based on common substitution patterns, here we only use it as a lexical substitution data set. 4.2 Task Evaluation We compare models on two variations of the lexical substitution task: candidate ranking and all-words ranking. In the candidate ranking task, the model is given a list of candidates and must select which are most appropriate for the given target. We follow prior work in pooling candidates from all substitu-tions for a given lemma and POS over all contexts, and measure performance using Generalized Aver-age Precision (GAP). GAP is similar to Mean Aver-age Precision, but weighted by the number of times a substitute was given by annotators. See Thater et al. (2010) for full details of the candidate ranking task.

The second task is the much more difficult task of all-words ranking . In this task, the model is not provided any gold list of candidates, but must se-We measure performance by (micro) mean Preci-sion@1 and P@3: that is, of a system X  X  top one/three guesses, the percentage also given by human annota-tors. These evaluation metrics are similar to the best and oot metrics reported in the literature, but we find P@1 and P@3 easier to interpret and analyze. 4.3 Word and Context Vectors We use the word and context vectors released by shown to perform strongly in lexical substitution tasks. These embeddings were computed from a cor-pus of (word, relation, context) tuples extracted from ukWaC and processed using the dependency-based word2vec model of Levy and Goldberg (2014a). These embeddings contain 600d vectors for 173k words and about 1M syntactic contexts. 4.4 Training Procedure To train the W and b parameters, we extract to-kens with syntactic contexts using the same corpus (ukWaC), parser (Chen and Manning, 2014), and ex-traction procedure used to generate the embeddings. See (Melamud et al., 2015b) for complete details. After extracting every token with its contexts, we randomly sample 10% of the data to reduce compu-tation time, leaving us with 190M tokens for training W and b . We use sampled softmax to reduce train-ing time (Jean et al., 2015), sampling 15 negative candidates uniformly from the vocabulary, optimiz-ing cross-entropy over just these 16 words per sam-ple. We optimize W and b in one epoch of stochastic gradient descent (SGD) with a learning rate of 0.01, momentum of 0.98, and a batch size of 2048. We found all of these hyperparameters worked well ini-tially, and did not tune them. Table 1 contains results for all measures across all experimental settings.

The first observation we make is that the PIC mea-sure performs best in all evaluations on all data tion, all measures perform substantially better than the OOC baseline, and the nPIC measure performs comparably to balAddCos . We note that context-sensitive measures give the most improvement in SE07, reflecting its greater emphasis on polysemy.
As we turn to the all-words ranking evaluations, we observe that the absolute numbers are much lower, reflecting the increased difficulty of the task. We also see the that nPIC and PIC both improve greatly over all baselines: The nPIC measure is a relative 30% improvement over balAddCos in SE07 and Coinco, and the PIC measure is a relative 50% improvement over balAddCos in 5 evaluations.
Since both measures have a clear improvement over the baselines, especially in the more difficult all-words task, we next strive to understand why. 5.1 Analysis We first an few cherry and lemon-picked examples to give intuitions about why our model performs bet-ter. Table 2 contains the cherry example, where our model performs better than prior work. While OOC and balAddCos both suggest replacements with rea-sonable semantics, but are all misspelled. nPIC and PIC only pick words with the correct spellings, with the exception of  X  X ealy. X 
Table 3 shows the lemon example, where our model performs worse. We notice that the unusual  X  X ea-change X  item is prominent in the OOC and balAddCos models, but has dropped from the rank-ings in our models. From these and other examples, we hypothesize the model is simply guessing more frequent terms.

We consider a few experiments with this hypoth-esis that the measures do better because they cap-ture better unigram statistics than the baselines. Re-cent literature found that the vector norm of SGNS embeddings correlates strongly with word frequency (Wilson and Schakel, 2015). We verified this for ourselves, computing the Spearman X  X  rank correla-tion between the corpus unigram frequency and the vector length and found rho = 0 . 90 , indicating the two correlate very strongly. Since the dot prod-uct is also the unnormalized cosine, it follows that nPIC and PIC should depend on unigram frequency.
To verify that the nPIC and PIC measures are indeed preferring more frequent substitutes, we compare the single best predictions (P@1) of the balAddCos and nPIC systems on all-words predic-tion on Coinco. Roughly 42% of the predictions made by the systems are identical, but of the remain-ing items, 74% of predictions made by nPIC have a higher corpus frequency than balAddCos (where chance is 50%). We find balAddCos and PIC make the same prediction 37% of the time, and PIC pre-dicts a more frequent word in 83% of remaining items. The results for SE07 and TWSI2 are similar.
This indicates that the unigram bias is even higher for PIC than nPIC . To gain more insight, we manu-ally inspect the learned parameters W and b . We find that the W matrix is nearly diagonal, with the val-ues along the diagonal normally distributed around  X  = 1 . 11 (  X  = 0 . 02 ) and the rest of the ma-trix normally distributed roughly around 0 (  X  =2e-5,  X  =0.02). This is to say, the PIC model is approxi-mately learning to exaggerate the magnitude of the dot product, s T c . This suggests one could even re-place our parameter W with a single scaling param-eter, though we leave this for future work.

To inspect the bias b , we compute the inner prod-uct of the b vector with the word embedding matrix, to find each word X  X  a priori bias, and correlate it with word frequencies. We find rho = 0 . 25 , indicating that b is also capturing unigram statistics.
Is it helpful in lexical substitution to prefer more frequent substitutes? To test this, we pool all anno-tator responses for all contexts in Coinco, and find the number of times a substitute is given correlates strongly with frequency ( rho = 0 . 54 ).

These results emphasize the importance of incor-porating unigram frequencies when attempting the lexical substitution task (as with many other tasks in NLP). Compared to cosine, the dot product in nPIC stresses unigram frequency, and the parame-ters W and b strengthen this tendency. We have presented PIC , a simple new measure for assessing the appropriateness of a substitute in a particular context for the Lexical Substitution task. The measure assesses the fit of the substitute both to the target word and the sentence context. It signifi-cantly outperforms comparable baselines from prior work, and does not require any additional lexical re-sources. An analysis indicates its performance im-provements derive primarily from a tendency to lean more strongly on unigram statistics than baselines. In future work, our measure could be simplified by implementing the bias as a single scaling parameter. We would like to thank Karl Pichotta, Martin Reidl, and the anonymous reviewers for their helpful com-ments and suggestions. This research was supported by the NSF grant IIS 1523637 We acknowledge the Texas Advanced Computing Center for providing grid resources that contributed to these results.
