 Knowledge bases (KBs), such as WordNet (Fell-baum, 1998), YAGO (Suchanek et al., 2007), Free-base (Bollacker et al., 2008) and DBpedia (Lehmann et al., 2015), represent relationships between en-tities as triples (head entity , relation , tail entity) . Even very large knowledge bases are still far from complete (Socher et al., 2013; West et al., 2014). Link prediction or knowledge base completion sys-tems (Nickel et al., 2015) predict which triples not in a knowledge base are likely to be true (Taskar et al., 2004; Bordes et al., 2011). A variety of differ-ent kinds of information is potentially useful here, including information extracted from external cor-pora (Riedel et al., 2013; Wang et al., 2014a) and the other relationships that hold between the enti-ties (Angeli and Manning, 2013; Zhao et al., 2015). For example, Toutanova et al. (2015) used informa-tion from the external ClueWeb-12 corpus to signif-icantly enhance performance.

While integrating a wide variety of information sources can produce excellent results, there are sev-eral reasons for studying simpler models that di-rectly optimize a score function for the triples in a knowledge base, such as the one presented here. First, additional information sources might not be available, e.g., for knowledge bases for specialized domains. Second, models that don X  X  exploit external resources are simpler and thus typically much faster to train than the more complex models using addi-tional information. Third, the more complex mod-els that exploit external information are typically extensions of these simpler models, and are often initialized with parameters estimated by such sim-pler models, so improvements to the simpler mod-els should yield corresponding improvements to the more complex models as well.

Embedding models for KB completion associate entities and/or relations with dense feature vectors or matrices. Such models obtain state-of-the-art per-formance (Nickel et al., 2011; Bordes et al., 2011; Bordes et al., 2012; Bordes et al., 2013; Socher et al., 2013; Wang et al., 2014b; Guu et al., 2015) and generalize to large KBs (Krompa et al., 2015). Ta-ble 1 summarizes a number of prominent embedding models for KB completion.

Let ( h,r,t ) represent a triple. In all of the models discussed here, the head entity h and the tail entity t are represented by vectors h and t  X  R k respec-tively. The Unstructured model (Bordes et al., 2012) assumes that h  X  t . As the Unstructured model does not take the relationship r into account, it can-not distinguish different relation types. The Struc-tured Embedding (SE) model (Bordes et al., 2011) extends the unstructured model by assuming that h and t are similar only in a relation-dependent sub-space. It represents each relation r with two matri-that W r, 1 h  X  W r, 2 t . The TransE model (Bordes et al., 2013) is inspired by models such as Word2Vec (Mikolov et al., 2013) where relationships between words often correspond to translations in latent fea-ture space. The TransE model represents each rela-tion r by a translation vector r  X  R k , which is cho-sen so that h + r  X  t .

The primary contribution of this paper is that two very simple relation-prediction models, SE and TransE, can be combined into a single model, which we call STransE . Specifically, we use relation-specific matrices W r, 1 and W r, 2 as in the SE model to identify the relation-dependent aspects of both h and t , and use a vector r as in the TransE model to describe the relationship between h and t in this subspace. Specifically, our new KB completion model STransE chooses W r, 1 , W r, 2 and r so that W r, 1 h + r  X  W r, 2 t . That is, a TransE-style rela-tionship holds in some relation-dependent subspace, and crucially, this subspace may involve very dif-ferent projections of the head h and tail t . So W r, 1 and W r, 2 can highlight, suppress, or even change the sign of, relation-specific attributes of h and t . For example, for the  X  X urchases X  relationship, certain attributes of individuals h (e.g., age, gender, mari-tal status) are presumably strongly correlated with very different attributes of objects t (e.g., sports car, washing machine and the like).

As we show below, STransE performs better than the SE and TransE models and other state-of-the-art link prediction models on two standard link predic-tion datasets WN18 and FB15k, so it can serve as a new baseline for KB completion. We expect that the STransE will also be able to serve as the basis for extended models that exploit a wider variety of information sources, just as TransE does. Let E denote the set of entities and R the set of re-lation types. For each triple ( h,r,t ) , where h,t  X  X  and r  X  X  , the STransE model defines a score func-tion f r ( h,t ) of its implausibility. Our goal is to choose f such that the score f r ( h,t ) of a plausi-ble triple ( h,r,t ) is smaller than the score f r 0 ( h 0 ,t 0 ) of an implausible triple ( h 0 ,r 0 ,t 0 ) . We define the STransE score function f as follows: using either the ` 1 or the ` 2 -norm (the choice is made using validation data; in our experiments we found that the ` 1 norm gave slightly better results). To learn the vectors and matrices we minimize the fol-lowing margin-based objective function: L = where [ x ] + = max(0 ,x ) ,  X  is the margin hyper-parameter, G is the training set consisting of correct G} X  X  ( h,r,t 0 ) | t 0  X  E , ( h,r,t 0 ) /  X  G} is the set of incorrect triples generated by corrupting a correct triple ( h,r,t )  X  X  .

We use Stochastic Gradient Descent (SGD) to minimize L , and impose the following constraints during training: k h k 2 6 1 , k r k 2 6 1 , k t k 2 6 1 , k W r, 1 h k 2 6 1 and k W r, 2 t k 2 6 1 . Table 1 summarizes related embedding models for link prediction and KB completion. The models differ in the score functions f r ( h,t ) and the algo-rithms used to optimize the margin-based objective function, e.g., SGD, AdaGrad (Duchi et al., 2011), AdaDelta (Zeiler, 2012) and L-BFGS (Liu and No-cedal, 1989).
 DISTMULT (Yang et al., 2015) is based on a Bilinear model (Nickel et al., 2011; Bordes et al., 2012; Jenatton et al., 2012) where each relation is represented by a diagonal rather than a full matrix. The neural tensor network (NTN) model (Socher et al., 2013) uses a bilinear tensor operator to repre-sent each relation. Similar quadratic forms are used to model entities and relations in KG2E (He et al., 2015) and TATEC (Garcia-Duran et al., 2015b).
The TransH model (Wang et al., 2014b) asso-ciates each relation with a relation-specific hyper-plane and uses a projection vector to project en-tity vectors onto that hyperplane. TransD (Ji et al., 2015) and TransR/CTransR (Lin et al., 2015b) ex-tend the TransH model using two projection vec-tors and a matrix to project entity vectors into a relation-specific space, respectively. TransD learns a relation-role specific mapping just as STransE, but represents this mapping by projection vectors rather than full matrices, as in STransE. Thus STransE can be viewed as an extension of the TransR model, where head and tail entities are associated with their own project matrices, rather than using the same ma-trix for both, as in TransR and CTransR.

Recently, Lao et al. (2011), Neelakantan et al. (2015), Gardner and Mitchell (2015), Luo et al. (2015), Lin et al. (2015a), Garcia-Duran et al. (2015a) and Guu et al. (2015) showed that rela-tion paths between entities in KBs provide richer in-formation and improve the relationship prediction. Nickel et al. (2015) reviews other approaches for learning from KBs and multi-relational data. For link prediction evaluation, we conduct experi-ments and compare the performance of our STransE model with published results on the benchmark WN18 and FB15k datasets (Bordes et al., 2013). In-formation about these datasets is given in Table 2. 4.1 Task and evaluation protocol The link prediction task (Bordes et al., 2011; Bordes et al., 2012; Bordes et al., 2013) predicts the head or tail entity given the relation type and the other entity, i.e. predicting h given (? ,r,t ) or predicting t given ( h,r, ?) where ? denotes the missing element. The results are evaluated using the ranking induced by the score function f r ( h,t ) on test triples.
For each test triple ( h,r,t ) , we corrupted it by replacing either h or t by each of the possible en-tities in turn, and then rank these candidates in as-cending order of their implausibility value computed by the score function. Following the protocol de-scribed in Bordes et al. (2013), we remove any cor-rupted triples that appear in the knowledge base, to avoid cases where a correct corrupted triple might be ranked higher than the test triple. We report the mean rank and the Hits@10 (i.e., the proportion of test triples in which the target entity was ranked in the top 10 predictions) for each model. Lower mean rank or higher Hits@10 indicates better link predic-tion performance. Following TransR/CTransR (Lin et al., 2015b), TransD (Ji et al., 2015), TATEC (Garcia-Duran et al., 2015b), R TransE (Garcia-Duran et al., 2015a) and PTransE (Lin et al., 2015a), we used the en-tity and relation vectors produced by TransE (Bor-des et al., 2013) to initialize the entity and relation vectors in STransE, and we initialized the relation matrices with identity matrices. Following Wang et al. (2014b), Lin et al. (2015b), He et al. (2015), Ji et al. (2015) and Lin et al. (2015a), we applied the  X  Bernoulli  X  trick for generating head or tail entities when sampling incorrect triples. We ran SGD for 2,000 epochs to estimate the model parameters. Fol-lowing Bordes et al. (2013) we used a grid search on validation set to choose either the l 1 or l 2 norm in the score function f , as well as to set the SGD learning margin hyper-parameter  X   X  { 1 , 3 , 5 } and the num-ber of vector dimensions k  X  X  50 , 100 } . The lowest mean rank on the validation set was obtained when using the l 1 norm in f on both WN18 and FB15k, and when  X  = 0 . 0005 , X  = 5 , and k = 50 for WN18, and  X  = 0 . 0001 , X  = 1 , and k = 100 for FB15k. 4.2 Main results Table 3 compares the link prediction results of our STransE model with results reported in prior work, using the same experimental setup. The first twelve rows report the performance of mod-els that do not exploit information about alterna-tive paths between head and tail entities. The next two rows report results of the R TransE and PTransE models, which are extensions of the TransE model that exploit information about relation paths. The last row presents results for the log-linear model Node+LinkFeat (Toutanova and Chen, 2015) which makes use of textual mentions derived from the large external ClueWeb-12 corpus.

It is clear that Node+LinkFeat with the additional external corpus information obtained best results. In future work we plan to extend the STransE model to incorporate such additional information. Table 3 also shows that models R TransE and PTransE em-ploying path information achieve better results than models that do not use such information. In terms of models not exploiting path information or external information, the STransE model scores better than the other models on WN18 and produces the highest Hits@10 score on FB15k. Compared to the closely related models SE, TransE, TransR, CTransR and TransD, STransE does better than these models on both WN18 and FB15k.
 Following Bordes et al. (2013), Table 4 analyzes Hits@10 results on FB15k with respect to the re-lation categories defined as follows: for each rela-tion type r , we computed the averaged number a h of heads h for a pair ( r,t ) and the averaged number a t of tails t for a pair ( h,r ) . If a h &lt; 1 . 5 and a t then r is labeled 1-1 . If a h  X  1 . 5 and a t &lt; 1 . 5 , then r is labeled M-1 . If a h &lt; 1 . 5 and a t  X  1 . 5 , then r is labeled as 1-M . If a h  X  1 . 5 and a t  X  1 . 5 , then r is labeled as M-M . 1.4%, 8.9%, 14.6% and 75.1% of the test triples belong to a relation type classified as 1-1 , 1-M , M-1 and M-M , respectively.

Table 4 shows that in comparison to prior models not using path information, STransE obtains high-est Hits@10 result for M-M relation category at (80 . 1%+83 . 1%) / 2 = 81 . 6% . In addition, STransE also performs better than TransD for 1-M and M-1 relation categories. We believe the improved per-formance of the STransE model is due to its use of full matrices, rather than just projection vectors as in TransD. This permits STransE to model diverse and complex relation categories (such as 1-M , M-1 and especially M-M ) better than TransD and other similiar models. However, STransE is not as good as TransD for the 1-1 relations. Perhaps the ex-tra parameters in STransE hurt performance in this case (note that 1-1 relations are relatively rare, so STransE does better overall). This paper presented a new embedding model for link prediction and KB completion. Our STransE combines insights from several simpler embed-ding models, specifically the Structured Embedding model (Bordes et al., 2011) and the TransE model (Bordes et al., 2013), by using a low-dimensional vector and two projection matrices to represent each relation. STransE, while being conceptually sim-ple, produces highly competitive results on standard link prediction evaluations, and scores better than the embedding-based models it builds on. Thus it is a suitable candidate for serving as future baseline for more complex models in the link prediction task.
In future work we plan to extend STransE to ex-ploit relation path information in knowledge bases, in a manner similar to Lin et al. (2015a), Garcia-Duran et al. (2015a) or Guu et al. (2015).
 This research was supported by a Google award through the Natural Language Understanding Fo-cused Program, and under the Australian Re-search Council X  X  Discovery Projects funding scheme (project number DP160102156).

NICTA is funded by the Australian Government through the Department of Communications and the Australian Research Council through the ICT Centre of Excellence Program. The first author is supported by an International Postgraduate Research Scholar-ship and a NICTA NRPA Top-Up Scholarship.

