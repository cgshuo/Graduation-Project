 In this Section, we detail the sampling equations for ( z , ) in the twitter application for concreteness. A.1 Sampling Topic Proportions Since topic proportions for di  X  erent regions are linked through the cascading process defined in Equation (5), we use an auxiliary variable method similar to (Teh et al., 2006) that we detail below. We sample  X  r based on three parts: 1) actual counts n r associated with node r , 2) pseudo counts  X  n r , propagated from all chil-dren nodes of r and 3) topic proportion  X   X  ( r ) from the parent node of r . Thus, topic proportions for node r are influenced by its children nodes and its parent node, enforcing topic proportion cascading on the tree. To sample  X  n r , we start from all children node of r . Let  X  s p,k be the number of counts that node p 2 C ( r )will propagate to its parent node r and n p,k is the actual number of times topic k appears at node p . We sample  X  s p,k by the following procedure. We firstly set it to 0, then for j =1 ,  X  X  X  ,n p,k +  X  n p,k , flip a coin with bias j 1+  X  r,k , and increment  X  s p,k if the coin turns head. The final value of  X  s p,k is a sample from the Antoniak distribution. Thus, for node r ,  X  n r,k = This sampling procedure is done from the bottom to the top. Note that  X  s p,k has the meaning as the number root  X  Dir(0 . 1 /V,  X  X  X  , 0 . 1 /V ). For all other nodes, we sample r from: where m r is the count vector for node r ,  X  m r is a smoothed count vector for node r and ! is a parameter. Here, m ( r,v ) is the number of times term v appearing in node r . For  X  m r , it is a smoothed vector of counts from sub-trees of node r . It can be sampled through a draw from the corresponding Antoniak distribution, similar to Section (8). However, since the element in r is much larger than topic proportions, it is not ef-ficient. Here, we adopt two approximations (Cowans, 2006; Wallach, 2008): 1. Minimal Paths : In this case each node p 2 C ( r ) 2. Maximal Paths : Each node r propagate its full The sum of the values propagated from all p 2 C ( r )to r defines  X  m r . Although the sampling process defined here is reasonable in theory, it might be extremely in-e cient to store values for all nodes. Considering a modest vocabulary of 100k distinct terms, it is di -cult to keep a vector for each region. To address this we use the sparsity of regional language models and adopt a space e cient way to store these vectors. A.4 Tree Structure Kalman Filter For all latent regions, we sample their mean vectors as a block using the multi-scale Kalman filter algo-rithm (Chou et al., 1994). The algorithm proceeds in two stages: upward filtering phase and downward-smoothing phase over the tree. Once the smoothed posterior probability of each node is computed, we sample its mean from this posterior.
 We define the following two quantities, n to be the prior covariance of node n , i.e. the sum of the covari-ances along the path form the root to node n , and F n = level ( n ) 1 [ level ( n ) ] 1 , which are used to ease the computations below.
 We first begin the upward filtering phase by computing the conditional posterior for a given node n based on each of its children m 2 C ( n ). Recall that each child 0 of every node specify the set of documents sampled directly from this node. Thus we have two di  X  erent update equations as follows: B.1 User Location modeling We demonstrate the e cacy of our model on two datasets obtained from Twitter streams. Each tweet contains a real-valued latitude and longitude vector. We remove all non-English tweets and randomly sam-ple 10 , 000 Twitter users from a larger dataset, with their full set of tweets between January 2011 and May 2011, resulting 573 , 203 distinct tweets. The size of dataset is significantly larger than the ones used in some similar studies (e.g, (Eisenstein et al., 2010; Yin et al., 2011)). We denote this dataset as DS1 . For this dataset, we split the users (with all her tweets) into disjoint training and test subsets such that users in the training set do not appear in the test set. In other words, users in the test set are like new users. This is the most adversarial setting. In order to compare with other location prediction methods, we also apply our model a dataset avail-able at http://www.ark.cs.cmu.edu/GeoText ,de-noted as DS2 , using the same split as in (Eisenstein et al., 2010). The priors over topics and topics mixing vectors were set to .1 and ! , to .1 favouring sparser representation at lower levels. The remaining hyper-for each user (note that they use the location of the first tweet as a reference, which may not be ideal), our goal is to infer the location of each new tweet, based on its content and the author X  X  other tweets. Based on our statistics, only 1%  X  2% of tweets have either geographical locations (including Twitter Places) explicitly attached, meaning that we cannot easily locate a majority of tweets. However, geograph-ical locations can be used to predict users X  behaviors and uncover users X  interests (Cho et al., 2011; Cheng et al., 2011) and therefore it is potentially invaluable for many perspectives, such as behavioral targeting and online advertisements. For each new tweet (from a new user not seen during training), we predict its location as  X  l d . We calculate the Euclidean distance between predicted value and the true location and av-erage them over the whole test set 1 N l ( a, b ) is the distance and N is the total number of tweets in the test set. The average error is calculated in kilometres. We use three inference algorithms for our model here: 1) exact algorithm denoted as Exact , 2) M-H sampling, denoted as MH and 3) the approxi-mation algorithm as Approx. .
 For DS1 we compare our model with the following ap-proaches: Yin 2011 (Yin et al., 2011) Their method is es-like structure and therefore more regions are needed to represent the fine scale of locations. In addition, we observe that Exact indeed performs better than Approx. and MH .
 For the comparison on the DS2 dataset, we compare with: (Eisenstein et al., 2010) The model is to learn a (Eisenstein et al., 2011) The original SAGE paper. (Wing &amp; Baldridge, 2011) Their method is essen-(Hong et al., 2012) This was the previous state of For (Eisenstein et al., 2010; Wing &amp; Baldridge, 2011; Eisenstein et al., 2011), the authors do not report op-timal regions. For (Hong et al., 2012), the optimal region is reported from the paper. The best reported results are used in the experiments. For our method, the regions are calculated as the same fashion as above. The results are shown in the second part of Figure 6. It is obvious that our full model performs the best on this public dataset. Indeed, we have approximately 40% improvement over the best known algorithm (Hong et al., 2012) (note that area accuracy is quadratic in the distance). Recall that all prior methods used a flat clustering approach to locations. Thus, it is possible that the hierarchical structure learned from the data helps the model to perform better on the prediction task.
 In Section 8, we discussed how regional language mod-els can be sampled. Here, we compare the two ap-proximation methods and directly sampling from An-toniak distributions based on Approx. , shown in Table 7. We can see that all three methods achieve compa-rable results although sampling Antoniak distributions can have slightly better predictive results. However, it takes substantially more time to draw from the An-toniak distribution, compared to Minimal Paths and Maximal Paths. In Table 6, we only report the results by using Minimal Paths.
 B.3 Ablation Study In this section, we investigate the e  X  ectiveness of dif-ferent components of the model and reveal which parts B.5 Error Analysis In order to understand how our model performs in terms of prediction we conduct a qualitative error anal-ysis on our model as well on the the state-of-the-art model (Hong et al., 2012) on all users in the USA on DS1 . The results are given in Figure 8. Each circle in the map represents 1000 tweets. The magnitude of the circle represents the magnitude of average error made for these 1000 tweets. Note that the circles are re-scaled such as to be visible on the map (i.e. radii do not correspond to absolute location error).
 We observe that in the industrialized coastal regions both models perform significantly better than in the Midwest. This is because that we have more users in those areas and therefore we can, in general, learn better distributions over those regions. At the same time, users in those areas might have much more dis-criminative mobility patterns relative to users in the Midwest. The second observation is our method con-sistently outperforms (Hong et al., 2012). This is par-
