 Domain specific information retrieval has become in demand. Not only domain experts, but also average non-expert users are interested in searching domain specific (e.g., medical and health) information from online resources. However, a typ-ical problem to average users is that the search results are always a mixture of documents with different levels of read-ability. Non-expert users may want to see documents with higher readability on the top of the list. Consequently the search results need to be re-ranked in a descending order of readability. It is often not practical for domain experts to manually label the readability of documents for large databases. Computational models of readability needs to be investigated. However, traditional readability formulas are designed for general purpose text and insufficient to deal with technical materials for domain specific information re-trieval. More advanced algorithms such as textual coherence model are computationally expensive for re-ranking a large number of retrieved documents. In this paper, we propose an effective and computationally tractable concept-based model of text readability. In addition to textual genres of a document, our model also takes into account domain spe-cific knowledge, i.e., how the domain-specific concepts con-tained in the document affect the document X  X  readability. Three major readability formulas are proposed and applied to health and medical information retrieval. Experimen-tal results show that our proposed readability formulas lead to remarkable improvements in terms of correlation with users X  readability ratings over four traditional readability measures.
 Copyright 2006 ACM 1-59593-433-2/06/0011 ... $ 5.00. H.3.3 [ Information Search and Retrieval ]: Information filtering, Retrieval models, Search process, Selection pro-cess.; H.3.7 [ Digital libraries ]: Algorithms Document Ranking, Document Readability, Document Scope and Cohesion
Domain specific information retrieval (IR), particularly in the health and medical area, has become more and more in demand. Not only domain experts, but also average (i.e., non-expert) users are interested in searching domain specific information from online resources. According to the Pew In-ternet &amp; American Life Project [4], for example, 52 million American adults have used the Web to get health or medical information, and 83% percent of them thought that online materials affected their decisions about treatment and care for relatives and themselves. In addition, 91% health infor-mation seekers have searched for information about physical illness.

Unlike general-purpose IR systems, domain specific infor-mation retrieval deals with retrieval of domain specific doc-uments, often a mixture of documents with different levels of readability. A recent investigation [5] reveals that, for 70% of 80 internet health web sites, users will need at least 2 years of college level education to comprehend the privacy statements. Another study [11] shows that much of medi-cal information for the public is written at a  X 10th grade, 2nd month reading level X , higher than the average reading level of the population of patients. Therefore, to average users with little domain knowledge and low education level or the sicks and elders under physical, psychological, and emotional stress, there is a need for an IR system to find documents not only relevant to a query but also with higher degrees of readability.

Domain specific search imposes two major requirements to the readability computation. First, a large number of technical terms are involved. Second, in online information retrieval, readability measurements are expected to be effi-cient enough to deal with a large number of documents in real time. There are several existing techniques for mea-suring readability of textual documents. However, none of them meets the above requirements. Most commonly used readability formulas were not developed for technical mate-rials [9]. On the other hand, some more complex readability measures are computationally expensive.

To address this problem, our work focuses on building an effective and computational tractable model of document readability by taking advantages of both traditional read-ability formulas and domain knowledge. The latter may be defined in a domain-specific controlled vocabulary or taxon-omy. We propose a novel concept-based readability model which takes into account the role of domain-specific con-cepts contained in a document for determining the read-ability of the document. Two basic features of documents, cohesion and scope [17, 16], are adopted together with a tra-ditional readability formula to build a computational model of word-level document readability for domain specific ma-terials. We propose three readability formulas based on this model and test them in health and medical information re-trieval. Through a correlation analysis between users X  read-ability judgements and the results computed by different readability formulas, our proposed measures have demon-strated outstanding performances in comparison with tradi-tional readability formulas.

The rest of this paper is organized as follows: Section 2 introduces different measures of readability including tradi-tional readability formulas and an advanced coherence model. The major problems of applying readability measures on domain specific information retrieval are also discussed. In Section 3, a concept-based readability model is proposed. Section 4 reports experimental setup and results. Section 5 gives conclusions and highlights future research directions.
Readability measures aim at matching the level of dif-ficulty in understanding text against the reading levels of readers. Throughout decades, many readability metrics were proposed to help writers and authors compose texts which can be understood easily by the targeted readers. According to the theory of discourse [8], these metrics can be catego-rized into three types: Surface Code Level Metrics, Text-base and the Situation Model based measures.
Given a piece of text, its surface code level is measured by wording and syntax of sentences. Documents full of  X  X if-ficult X  words are apparently more difficult for readers to understand than documents with simple words. Moreover, a large number of long sentences with complex syntax will surely make a document more difficult to read. The word difficulty and sentence difficulty can be computed in various straightforward ways. Most of them such as Flesch Read-ing Ease Score, Flesh-Kincaid Grade Level, SMOG Index, Gunning-Fog Index, Automated Readability Index (ARI), Coleman-Liau and Dale-Chall will generate a numeric read-ability score. Such a score often corresponds to an educa-tional grade level. Therefore, these surface code level metrics are also called  X  X rade-level formulas X .
McCallum et al [12] summarized that six features reflect-ing word difficulty can be used in readability formulas. The most commonly used features include length of words, num-ber of syllables in words, and popularity of words. For ex-ample, the Flesch Reading Ease Score and Flesh-Kincaid Grade Level measure the average syllables per word. The SMOG Index and Gunning-Fog Index consider the number of words in a document that contain no less than 3 syllables. The Automated Readability Index (ARI) and Coleman-Liau take the number of characters per word into account. Fi-nally, a common word list is used in Dale-Chall X  X  Readability Index formula. Words which are not in the list are regarded as difficult words. The readability of document is counted mainly by the percentage of difficult words in the document.
Sentence difficulty is computed by measuring syntactic features of sentences. The most popular feature is the length of sentences, i.e. counting of words in a sentence. This method has been used in most popular surface code level metrics.
According to Kintsch X  X  theory [8], there are at least three levels of cognitive representation for users to understand text. They are the source code, text-base and situation model. The source code is the literal words and the way they are organized as sentences. The text-base consists of surface meanings of clauses presented by the source code. The situation model is a user X  X  mental model built on the text-base with user X  X  background knowledge for the purpose of understanding what the text is about. It consists elements underlying the surface meaning of the text (i.e. text-base) such as time, place, event and causality. The construction of situation model needs inferences based on reader X  X  knowl-edge and understanding of text-base. Current theory of dis-course believes that in general the more coherent the sit-uation model is, the more text comprehension the reader can achieve. Being fully aware of the difficulty of detect-ing reader X  X  knowledge level, researches are mainly focused on cohesion of text-base by using the statistical or natu-ral language processing methods. For example, the Latent Semantic Analysis (LSA) [10] has been used to represent semantic content and measure the text-base cohesion [3, 14].
Applying readability measures to information retrieval is interesting and challenging. To the best of our knowledge, there is no research work performed so far to address this problem. By incorporating readability analysis, we expect automated domain-specific information retrieval to be more beneficial to average non-expert searchers. However, can existing readability measures cope with the requirement of domain specific information retrieval? In this subsection, we discuss this problem in detail.
Three problems arise when existing readability measures are applied to domain-specific materials. Table 1: MeSH Levels and Word Difficulty Features
Problem 1: Technical word difficulty cannot be simply measured by number of syllables or charac-ters in word.

Not only common words but also technical or professional terms can appear in technical materials. We discover that the word difficulties of technical or professional terms cannot be effectively measured by the traditional syllable counting or word length counting methods. It is not necessarily true that the more syllables or more characters a word contains, the more difficulty readers may encounter in reading and understanding the word.
 We verify this by an analysis performed on the Medical Subject Headings 1 (MeSH), the U.S. National Library of Medicine X  X  controlled vocabulary thesaurus. It is used to index health and medical materials in the MEDLINE, an online database containing more than 11 million citations and abstracts from health and medical journals and other sources. The MeSH consists of descriptors (concepts) which are organized to form a concept hierarchy by broader and narrower relationships. Figure 1 shows a fragment of the MeSH structure.

In this example, the descriptors  X  X vian Leukosis X  and  X  X nfluenza, Avian X  are two types of  X  X ird Diseases X . It is obvious that the deeper a descriptor is in MeSH hierarchy, the more technical it is, and in turn the more difficult it is for a non-expert reader to read and understand. According to this feature of MeSH, our study aims to find out whether the two traditional word difficulty measures still hold, i.e., whether or not the average number of characters and the average number of syllables per descriptor on a MeSH level will increase when the level goes deeper. Table 1 shows the relationship among MeSH level, the average number of syl-lables and the average number of characters per descriptor on that level.

From Table 1, it is evident that the numbers of syllables http://www.nlm.nih.gov/pubs/factsheets/mesh.html and characters per descriptor do not have a correlation with MeSH level (from one to eleven). Therefore, these two vari-ables are insufficient to measure word difficulty for domain specific materials.

Problem 2: Grade level metrics is unsuitable for technical materials.

As we have previously described, readability measurement is a mapping between the reading difficulties of textual ma-terials and the grade levels of readers. However, it is unsuit-able to use grade level metrics for technical materials. Cur-rent grade level metrics were originally designed to measure children X  X  school books rather than domain specific materi-als [13]. Moreover, some readers of technical materials may have high grade levels but weak abilities to read domain specific materials because of illness, physical, psychological, and emotional stress. Therefore, a main idea of this paper is to compute relative readability scores rather than absolute grade levels of technical materials.

Problem 3:  X  X ommon X  words are not always com-mon.

The classical Dale-Chall X  X  Readability Index measures word difficulty by counting the percentage of words which are out of the Dale-Chall Word List 2 , a list of 3,000 words which were known to be familiar to most of the fourth-grade read-ers. However,  X  X ommon words X  are not always common. Redish et al [13] questioned that in some technical or pro-fessional materials, common words will become technical terms. For example, the word  X  X hock X  is a common word frequently used in everyday life. It is regarded as a com-mon word in Dale-Chall Word List. However, in medical and health materials, the meaning of  X  X hock X  could be  X  X  pathological condition that can suddenly affect the hemody-namic equilibrium, usually manifested by failure to perfuse or oxygenate vital organs X  3 .

Nevertheless, the Dale-Chall X  X  Readability Index was not designed to measure the readability of technical materials. On the one hand, the case like  X  X hock X  is true but not quite usual. On the other hand, to the best of our knowledge, no user study has been conducted to investigate the suitability of Dale-Chall X  X  Readability Index formula. In this paper, we will evaluate the effectiveness of Dale-Chall X  X  Readability Index formula in the context of technical materials.
In summary, the above identified problems challenge the existing word difficulty measures and reveal the need of more sensible and practical methods to measure the readability of technical materials.
Document ranking is a fundamental feature for almost all information retrieval systems, in order to tell user how im-portant a document is to a query. Due to the large amount of search results and the limitation of user X  X  time and patience, it is impractical for user to review all the retrieved docu-ments and judge their relevance. Study on users X  searching behavior [6] in Google reveals that user X  X  attention to search result drops down promptly while browsing the retrieved http://www.corelearn.com/PDFS/Articles/Dale-Chall%20Word%20list.pdf http://www.nlm.nih.gov/pubs/factsheets/mesh.html documents. Therefore in what order to present relatively highly readable documents is a major problem in order to benefit average non-expert users.

The research on searching behavior [6] also reveals that searchers usually spend little time, around 5 seconds only, in selecting documents to read from a ranked list of search results. Only 200 to 300 milliseconds of eyes fixation on average are spent to acquire information from a piece of description of a document in order to judge its relevance. It consequently reveals that searchers will not read the ab-stract of a retrieved document sentence-by-sentence. The major reading difficulty an Internet searcher is encountered exist at word level. Therefore, efficient word-based docu-ment readability measurement is in greater demand than sentence-based ones. In this paper, we propose a concept-based readability model which focuses on the estimation of word-level readability of domain specific search results (doc-uments) for the purpose of document retrieval.
We investigate the problem of readability measurement for domain specific information retrieval from the following four perspectives.
 Relative Readability The first is to consider the mea-Word-level Readability The second is to consider doc-Concept-based Readability The third is to consider the General Word-level Readability The fourth is to con-
The hypothesis underlying our proposed concept-based readability are given as follows:
In following subsections, we will present our approach to the computation of concept-based word level readability in the context of medical and health information retrieval. Here we regard the document terms (including compounds) which have a match in a conceptual hierarchy as domain concepts. The terms which cannot be found in the concep-tual hierarchy are referred to non-domain terms.
Document scope is a major characteristic of concept-based readability. For example, consider the following two defini-tionsofSARS.Definition1comesfromABOUT 4 ,aweb information service for daily life. Definition 2 is an official definition from the Department of Health in Hong Kong 5 . 1. ( d 1 ) A viral respiratory illness that was recognized as 2. ( d 2 ) A viral respiratory infection caused by a coron-
We may identify three domain concepts:  X  X espiratory in-fection X ,  X  X oronavirus X  and  X  X ARS-CoV X  in Definition 2. However, Definition 1 (which is more public-oriented) does not contain any domain concept. Instead,  X  X espiratory ill-ness X  is used to broadly describe SARS rather than a more specific concept  X  X espiratory infection X . It is obvious that the scope of the general definition (i.e. Definition 1) is larger than the scope of the professional definition (i.e. Definition 2).

We use a function to sum up the tree depths of all the individual concepts in the document to calculate its scope. Operationally, the tree depth of a domain concept is mea-sured by the distance between that concept and the root in a concept hierarchy. The tree depth of a non-domain term is set to be zero. Therefore the document scope function is a monotonic decreasing function of the average tree depth of all concepts in document.

It is often the case that a document contains large pro-portion of non-domain terms but just small number of do-main concepts. Consequently the scope values of this kind of documents may be very skewed. To make the scope com-putation more sensitive to different average tree depths of domain concepts in different documents, we employ an ex-ponential function. http://about.com http://www.info.gov.hk
In Equation 1, the function depth ( c i ) gets the depth of concept c i in the concept hierarchy. The maximum value of document scope is 1 when a document contains only non-domain terms. The time complexity of scope-based read-ability measurement of m documents is O ( m  X  n ), where n is the maximum number of terms (i.e. both domain concepts and non-domain terms) per document.
Document cohesion is another feature of concept-based readability. It measures the relatedness of concepts in a document. The intuition of our approach is that the more cohesive the domain concepts of a document are, the more readable the document is.

Consider the following two sentences. The first is the title of a journal paper from AIDS, an official journal of the inter-national AIDS society 6 , and the second is from the MeSH. 1. ( d 1 ) AIDS Events Among Individuals Initiating HAART : 2. ( d 2 ) HIV is a non-taxonomic and historical term refer-
In d 2 , three domain concepts can be identified:  X  X IV X ,  X  X IV-1 X  and  X  X IV-2 X . In d 1 ,  X  X IDS X ,  X  X AART X  and  X  X a-tients X  are three identified domain concepts. They both contain the same number of domain concepts, thereby their readability cannot be distinguished in terms of their doc-ument scope. However, there is a stronger cohesion in d 2 than in d 1 . In other words, concepts in d 2 are more strongly associated to each other than those in d 1 . Specifically,  X  X IV-1 X  and  X  X IV-2 X  are two types of  X  X IV X , i.e., there are di-rect links between them in MeSH. People who even does not know HIV-1 and HIV-2 can easily understand this sen-tence. However, in d 1 , X  X IDS X  X sakindofsymptombut  X  X AART X  is a therapy for  X  X IDS X . There is no direct link between  X  X IDS X  and  X  X AART X  in MeSH. People with low domain knowledge can hardly understand what HAART is about and what the relationship is between HAART and AIDS.

The cohesion of a document is computed as associations (semantic relatedness) between domain concepts in docu-ments. The more closely the concepts are associated, the higher degree of cohesion the document would have. Cohesion ( d i )= http://www.medscape.com
In Equation 2, n is the total number of domain con-ceptsinadocument d i . Sim ( c i ,c j ) is a function com-puting the Leacock-Chodorow semantic similarity of con-cepts c i and c j . len ( c i ,c j ) is the function to calculate the shortest path between c i and c j in the MeSH hierarchy. N umberof Associations is the total number of mutual asso-ciations among domain concepts, which is defined in Equa-tion 4.

In Equation 3, D is the maximum tree depth in the con-cept hierarchy. In our experiments, D for the Mesh tree is 11. The scope of Equation 2 is thus [0 ,  X  log ( 1 22 )]. For a document with less than one domain concept, its document cohesion is 0. For a documents with strongest associations among all the concepts within the document, its cohesion is  X  log ( 1 cohesion-based readability measurement on m retrieved doc-uments is O ( m  X  n 2 ), n is the maximum number of domain concepts in documents.
We propose the following Equation 5 to calculate over-all concept-based readability score ( CRS ), which is propor-tional to document scope, document cohesion and the recip-rocal of a Simplified Dale-Chall X  X  Readability Index ( DaCw ).
CRS ( d i )= Scope ( d i )+ Cohesion ( d i )+ DaCw ( d i ) DaC ( d i )=(0 . 0496  X  AvgSL )+(0 . 1579  X  PDW )+3 . 6365 (6)
The original Dale-Chall X  X  Readability Index ( DaC )isgiven in Equation 6, where AvgSL is the average length of sen-tence in document d i and PWD is the percentage of difficult words in d i . Difficult words are all the words not contained in the Dale-Chall Word List 7 . Since we are focusing on the word level readability, the sentence level complexity is removed from Equation 6 leading to a simplified formula DaCw (Equation 7).

The Dale-Chall X  X  Readability Index is a popular readabil-ity formula. It takes the commonness of words into consid-eration rather than the syllables and length of words that were, as mentioned previously, insufficient to act as features of word level readability. The simplified Dale-Chall X  X  Read-ability Index can be a complement of the concept-based scope and cohesion calculation. The larger a document X  X  Dale-Chall Readability Index is, the less readable the docu-ment is. We therefore combine the reciprocal of the simpli-fied Dale-Chall Readability Index with our document scope and cohesion scores forming the Equation 5.
In order to evaluate our proposed concept-based readabil-ity measures, user oriented experiments and evaluations are performed. Human judgements are treated as ground truth in our experiments. The proposed concept-based readability measures with other four widely used classical measurements of word level readability are then compared with the ground truth. http://www.corelearn.com/PDFS/Articles/Dale-Chall%20Word%20list.pdf
In our experiments, we evaluate the following three ma-jor scenarios of readability computation, where document scope, cohesion and simplified Dall-Chall Readability Index are combined in a reasonable manner. They are listed as followings.
 DSChall DCChall DSDCChall
In order to study the effects of document scope and doc-ument cohesion, three additional scenarios are evaluated. They are: DS DC DSDC
There are four classical readability formulas which are widely used in different areas. They are: Automated Read-ability Index (ARI) [15], Flesch-Kincaid Grade Level(FKG) [2], Gunning-Fog Index (GFI) [7] and Dale-Chall Readability In-dex (DaC) [1]. They are simplified by removing the part of sentence level readability calculation in order to test their effectiveness of word level readability computations. All the four simplified formulas are listed as follows.
 ARI Flesch Gunning Gunningw ( d i )= percentageT hreeSyllables ( d i )  X  1 (13) Chall
The test corpus is generated from the PubMed Central (PMC) 8 , which is a free digital archive generated by U.S. National Institutes of Health (NIH). The PMC contains several-hundred thousands free full text articles on biomed-ical and life sciences.

The test corpus is related to the frequently requested health topics in MedlinePlus 9 , which is a site providing  X  X ev-eral search mechanisms for searching for health topics of in-terest to the general public in PubMed X  10 . These topics are: Alzheimer X  X  Disease, Back Pain, Breast Cancer, Cholesterol, COPD (Chronic Obstructive Pulmonary Disease), Depres-sion, Diabetes, Fibromyalgia, Gastroesophageal Reflux/Hiatal Hernia, High Blood Pressure, Lupus, Parkinson X  X  Disease, Pregnancy, Prostate Cancer and Sexually Transmitted Dis-eases.

To simulate queries that average users are likely to use for searching information about the above topics, for each topic, we first submit TOPIC NAME +  X  X AQ X  to Google in order to retrieve a set of frequent answered questions for that topic. Eighteen frequent answered questions are selected as the primitive test queries in our experiments.

We then take advantage of the PubMed X  X  feature of search-ing with MeSH database 11 in order to formulate the test queries. The main search topics in our 18 primitive queries are replaced by the corresponding domain concepts. All the stop words are removed according to the SMART 571 stop word list. For example,  X  X hat is depression? X  is formalized as  X  X epression[MAJR:NoExp]  X . MAJR means  X  X estrict search to major topic headings only X  and NoExp means  X  X o not explode this term (i.e., do not include MeSH terms found below this term in MeSH tree) X . Other keywords which cannot be found in MeSH are appended to the query by a boolean operator  X  X ND X .

Then in PubMed, the top 10 documents are retrieved for each query. Note that some queries may have less than 10 documents returned. Thus we form a data set containing http://www.pubmedcentral.nih.gov/ http://medlineplus.gov/ http://www.nlm.nih.gov/bsd/special queries.html http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=mesh titles and abstracts of a total number of 156 distinct docu-ments retrieved by the 18 queries.
Fourteen users participated in our user experiments. They all have had higher education qualifications with no less than ten years of English language training. All of them are re-quested to voluntarily complete a questionnaire about all the retrieved documents for each of 18 queries. All users have only basic knowledge about health and medical topics. There is no time limit for users to complete the question-naire.

In the questionnaire, the queries are listed in an order of their query numbers from 1 to 18. The retrieved documents under each query are listed (see Figure 2) in an original order of PubMed X  X  original similarity based ranking. Only title and abstract of document are presented to users for their judgements.

In this questionnaire, users will score for the opinion de-noted by O1 for every retrieved document under each of the 18 queries. The scores are in the range of one to five. One for  X  X trongly disagree X , two for  X  X isagree X , three for  X  X ave no idea X , four for  X  X gree X , and five for  X  X trongly agree X . The opinion which is related to readability study is: 1. O1: The words used in this document make it easy-
Users X  ratings of this opinion reflect their judgements of word level based readability of the retrieved documents.
The completed questionnaires may contain some users X  hasty and careless inputs. In order to identify the outliers, a statistical filtering process is applied based on the statistics of the data, such as standard deviation, mean and coefficient of variation. Coefficient of variation is a basic statistical method to measure the relative scatter in user X  X  ratings. It is calculated by dividing the standard deviation of all the 14 users X  ratings (of an opinion for one of the 156 retrieved documents) by their mean value. The range of co-efficient of variation is from 0% to 100%. High co-efficient of variation value means that users cannot have a fairly consistent rat-ings about an opinion for a document retrieved. For each of the 156 retrieved documents, all the 14 users X  ratings of the corresponding opinion are processed by using those statis-tical methods. The ratings from individual users which are two times or more than the standard deviation are dropped.
Table 2 shows the improvement of average co-efficient of variations of users X  ratings of 156 retrieved documents after the statistical filtering process.
Our proposed three cases of concept-based readability mea-sures together with other four simplified classical readability formulas are run on our test corpus. The output rankings for each query are then compared with the 14 users X  rankings. The correlation between the human readability judgements and computer readability measures is used in our study as the performance indicator. The higher the correlation is, the better the readability measure is. Spearman rank-order correlation coefficient is used for correlation analysis since it is a distribution free test considering the rank of a data item instead of its value.

Moreover, four levels of the significance (p) for a two-tailed test, namely critical values, are used to measure the confidence of the Spearman correlation coefficient. The four levels are: 0.10, 0.05, 0.02 and 0.01. The critical value of a Spearman correlation coefficient is fairly significant when it is less than 0.02, highly significant when it less than 0.01. It can be determined by using the critical values table when both a correlation value and the corresponding degree of freedom are given. The degree of freedom is the total num-ber of data points in the correlation analysis minus 2. The greater both the degree of freedom and the correlation coef-ficient are, the higher the critical value can be achieved.
There are totally 18 queries, 156 retrieved documents and 14 users involved in this experiment. In Table 3, the Spear-man rank-order correlation coefficients are calculated be-tween the selected cases of readability measures and users X  ratings of word level based readability. Query# is the query number from 1 to 18. n  X  2 is the degree of freedom for a two-tailed test. n is the total number of documents which are retrieved by a particular query. Avg is the mean func-tion. As to a specific readability measure, the mean value of the degree of freedom and the mean value of the corre-lation coefficient in all the 18 queries indicate the average performance of that measure.

The intuitive diagram of the correlations between users and selected formulas is presented in Figure 3. The X co-ordinator represents different readability measures. The Y coordinator represents the Spearman correlation coefficient.
In Table 3 it is evident that our proposed three read-ability measures DSChall, DCChall and DSDCChall have strong correlations with human X  X  readability judgements in almost all the 18 queries. The average degree of freedom is 7. With this value, we found that the critical values of DSChall and DCChall are all below 0.01, and the critical value of DSDCChall is below 0.02. It shows that all those strong correlation values are highly confident. In contrast, the cor-relation between each of the four test readability measures (i.e. Automated Readability Index, Flesch-Kincaid Grade Level(FKG), Gunning-Fog Index and Dale-Chall X  X  Readabil-ity Index) and human judgements is quite weak.

Figure 3 clearly shows that our proposed readability mea-sures, DSChall, DCChall and DSDCChall, generate remark-ably performance improvement over all four classical word-level based readability measures. We performed a t-test (Paired Two Sample for Means) which compares the paired Spearman correlation coefficients between any of DSChall, DCChall, DSDCChall and any of the four word level based readability measures. With all the p  X  value less than 0.01, it turns out that the improvements made by DSChall, DC-Chall and DSDCChall are all statistically significant.
It is noticeable that most of the four selected readabil-ity measures (i.e. Automated Readability Index, Flesch-Kincaid Grade Level(FKG) and Gunning-Fog Index) are us-ing the number of syllables or the length of words as the ma-jor features of word level readability. Thereby it verifies our observation and argument that word difficulties of technical or professional terms cannot be effectively measured by the commonly used methods such as syllables and word length counting.
 It is also noticeable that Dale-Chall X  X  measure (i.e. Chall), DS, DC and DSDC all have a poor performance when they are used alone. However, when Dale-Chall X  X  measure is com-bined with DS or DC (i.e. DSChall, DCChall and DSD-CChall), the performance is largely boosted. This result demonstrates the advantage of our proposed approach, in which domain specific knowledgebase together with Dale-Chall X  X  word list are used as a solution to determine the word level based readability in the context of technical ma-terials. Moreover, the results also support our argument against Redish X  X  et al [13] opinion that the common words in the Dale-Chall X  X  Word List becoming special jargons in technical materials makes it useless in readability measure-ment of technical materials. In our special case of medical and health materials, it shows that a small portion of jargons in the Dale-Chall Word List will not affect its performance very much. Another t-test shows the Dale-Chall X  X  Readabil-ity Index significantly outperforms the other four classical readability measures, i.e., the Automated Readability Index, the Flesch-Kincaid Grade Level(FKG) and the Gunning-Fog Index, with an acceptable performance.

However, DC is not as good as DS to describe the word level based readability. It results that DSChall outperforms DCChall and DSDCChall. The reason may be that it simply considers semantic relationship between domain concepts only. Since there are a large proportion of non-domain terms in the documents, it is necessary to consider statistical rela-tionships between domain concepts and non-domain terms. A possible solution is to use the frequency of co-occurrence to measure the relatedness of concepts (i.e. both domain and non-domain terms). The more often two concepts co-occur, the stronger their association is.
In this paper, we studied the readability measure prob-lem in domain specific information retrieval, in which not only domain experts, but also average users are interested in searching domain specific information such as online health information. A typical problem to average users is that the search results are always a mixture of documents with dif-ferent readability levels. It is difficult for average users to quickly sort out documents with relatively high readability. It is not applicable for domain experts to manually label readability for large database. Traditional readability for-mulas are oversimplified to deal with technical materials. More advanced algorithms such as textual coherence model are effective but too computational expensive to be applied to re-rank a large number of retrieved documents.
We have proposed a novel computational model of domain specific document readability by taking advantages of both traditional readability formula (i.e. Dale-Chall X  X  Readabil-ity Index) and the knowledgebase. We have introduced an important notion namely concept-based readability, that is, the reading difficulty of domain specific concepts (i.e. tech-nical terms) in terms of a given knowledgebase. Three major readability formulas are proposed and applied in the context of health and medical information retrieval. The readability predictions of our proposed formulas together with 4 tra-ditional readability formulas are compared with 14 users X  readability judgements on 156 retrieved health and medical article abstracts. The results show that our proposed read-ability formulas can make remarkable improvements against the traditional readability measures.

In real applications, the proposed concept based readabil-ity algorithm can be used as a complement of state-of-the-art document ranking algorithms. It is more preferable for users to get documents which are both similar to the queries and easy to understand.

A major advantage of our proposed approach is its domain specific character. The concept based document readability could be computed when a well developed bio-medical con-ceptual hierarchy is available together with the corpus. This could also be regarded as a restriction to the applicability of our algorithms. Without such an ontology database, the document ranking would not be possible.

Furthermore, our study also shows that document cohe-sion is not quite a significant feature of word-level based readability. We plan to study other factors of word-level relatedness. So far we have considered quantifying only the semantic closeness amongst domain concepts in order to calculate the document cohesion. In our further study we will consider the statistical relationships between non-domain terms and domain specific concepts.

Finally, this paper partially addresses the general techni-cal readability problem in the context of online searching. Technical document readability is still an interesting topic when user aims at understanding a specific document rather than a quick browse of the titles and abstracts of a list of domain specific searching results. Without the limitation of browsing time, sentence level readability is still an impor-tant factor for readability computation. In our further study it will be considered to address a more general readability problem for technical documents.
The work reported in this paper has been funded in part by the Co-operative Centre for Enterprise Distributed Sys-tems Technology (DSTC) through the Australian Federal Governments CRC Programme (Department of Education, Science and Training). The work was partially carried out while the first author was visiting the Knowledge Media Institute of the Open University, United Kingdom, during March-May 2006. [1] J.S.ChallandE.Dale. Readability Revisited: The [2] R. Flesch. A new readability yardstick. Journal of [3] P. Foltz, W. Kintsch, and T. Landauer. The [4] S. Fox, L. Rainie, J. Horrigan, A. Lenhart, T. Spooner, [5] M. A. GRABER, D. M. D X  X LESSANDRO, and [6] L. Granka, T. Joachims, and G. Gay. Eye-tracking [7] R. Gunning. The technique of clear writing .
 [8] W. Kintsch. Comprehension: A paradigm of cognition . [9] G.R.Klare.Theformativeyears.In Readability: its [10] T. K. Landauer, P. W. Foltz, and D. Laham.
 [11] G. MA, R. CM, and K. B. Readability levels of [12] D. R. McCallum and J. L. Peterson. Computer-based [13] J. Redish. Readability formulas have even more [14] M. E. Schreiner, B. Rehder, T. K. Landauer, and [15] E. A. Smith and R. J. Senter. Automated readability [16] X. Yan, X. Li, and D. Song. Document re-ranking by [17] X. Yan, X. Li, and D. Song. Document generality: its
