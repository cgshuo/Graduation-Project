 Evaluation measures act as objective functions to be op-timized by information retrieval systems. Such objective functions must accurately reflect user requirements, partic-ularly when tuning IR systems and learning ranking func-tions. Ambiguity in queries and redundancy in retrieved documents are poorly reflected by current evaluation mea-sures. In this paper, we present a framework for evalua-tion that systematically rewards novelty and diversity. We develop this framework into a specific evaluation measure, based on cumulative gain. We demonstrate the feasibility of our approach using a test collection based on the TREC question answering track.
 H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval X  Retrieval models Experimentation, Human Factors, Measurement
For a given query, an information retrieval system should respond with a ranked list that respects both the breadth of available information and any ambiguity inherent in the query. The query  X  X aguar X  represents a standard example of an ambiguous query. In responding to this query, an IR system might best return a mixture of documents discussing the cars, the cats, and the classic Fender guitar. Taken together, these documents should provide a complete picture of all interpretations.

Ideally, the document ordering for this query would prop-erly account for the interests of the overall user population. If cars were more popular than cats, it might be appropriate to devote the first few documents to them, before switch-ing topics. The earlier documents might cover key aspects of each topic. Later documents would supplement this ba-sic information, rather than redundantly repeating the same thing over and over again.

The creation of an IR system that systematically accounts for redundancy and ambiguity presents many challenges. Not the least of these challenges is the lack of a clear and meaningful objective function defining what the optimal re-sponse for a given query should be under such circumstances. The evaluation measures in widespread use  X  such as MAP, bpref [8] and nDCG [20]  X  assume that the relevance of each document can be judged in isolation, independently of other documents. Tuning IR systems to optimize these evaluation measures may produce unsatisfactory results when redun-dancy and ambiguity are considered.

The presence of duplicates and near-duplicates in a doc-ument collection represents an extreme version of the prob-lem. Bernstein and Zobel [5] examined the impact of near-duplicates on the TREC GOV2 collection. This test collec-tion comprises roughly a half-terabyte of Web pages taken from the gov domain, along with topics and corresponding judgments. More than 17% of documents within this collec-tion are essentially duplicates of other documents. Return-ing identical versions of a relevant document may produce a high score on a standard evaluation measure, but would certainly be viewed unfavorably by a user. The expedient solution of deleting these duplicates merely sweeps the prob-lem under the rug. Instead, the evaluation measure itself should directly accommodate th e possibility of duplicates.
The problem is exacerbated by the application of machine learning techniques to IR systems [1, 9, 30]. These systems may learn their ranking functions from masses of relevance judgments and implicit user feedback. To be applicable in these environments, an evaluation measure must reflect gen-uine user requirements. While many machine learning tech-niques do not directly optimize an evaluation measure, it still must be possible to compute the measure rapidly and mechanically, without the need for additional judging, even when previously unseen documents are surfaced.

This paper builds on a thread of related ideas stretching back more than four decades [6, 11, 13, 17, 32]. Many of the central ideas presented in this paper have been expressed in various forms by this earlier work. Our aim is to codify these ideas into a coherent foundation that properly accounts for redundancy and ambiguity. The resulting framework allows us to make a precise distinction between novelty  X  the need to avoid redundancy  X  and diversity  X  the need to resolve ambiguity.

A second aim is to demonstrate the practical application of this framework to the construction of test collections and evaluation measures. Since graded relevance arises natu-rally from the framework, we base our proposed evaluation measure on the Normalized Discounted Cumulative Gain (nDCG) measure developed by J  X  arvelin and Kek  X  al  X  ainen [20], whichassumesgradedrelevance.

In addition, we describe a test collection exploring our proposal based on the TREC 2005/2006 question answer-ing collections. We use this collection to examine the im-pact of pseudo-relevance feedback on novelty. Under tradi-tional evaluation measures, pseudo-relevance feedback gen-erally provides a significant performance gain. The opposite is true under our proposed measure.
In 1964, Goffman [17] recognized that the relevance of a document must be determined with respect to the docu-ments appearing before it. This recognition was echoed by Boyce [6] in 1982, who wrote,  X  X he most relevant document should be topical, novel... The change it makes in the knowl-edge state must then be reflected in the choice of document for the second position. X 
Several recent papers directly inspired our work. Car-bonell and Goldstein [11] describe the maximal marginal rel-evance method, which attempts to maximize relevance while minimizing similarity to higher ranked documents. Zhai and colleagues [31, 32] develop and validate subtopic retrieval methods based on a risk minimization framework and in-troduce corresponding measures for subtopic recall and pre-cision. Chen and Karger [13] describe a retrieval method incorporating negative feedback in which documents are as-sumed to be not relevant once they are included in the result list, with the goal of maximizing diversity.

Sp  X  arck Jones et al. [25] call for the creation of evaluation methodologies and test collections addressing the problem of query ambiguity. They stress that a response from an IR system must accommodate multiple user needs. A user study by Xu and Yin [29] suggests that  X  X ovelty seeking X  is not equivalent to  X  X iversity seeking X , and that the novelty preferences of individual users are directed towards finding more information on specific subtopics of interest, rather than an undirected quest for any new information.
As motivation, and to provide running examples, we present retrieval results for two queries: one taken from a Web search context and the other from the TREC question answering track.
Table 1 shows five of the top ten results for the Web query  X  X PS X , as returned by a leadi ng commercial search engine in late 2007. We have retained the ordering specified by the search engine, but have removed a few results to keep the example concise.

The ambiguity is obvious. A user entering this query might be tracking a package sent via the United Parcel Ser-vice, planning the purchase of an uninterruptible power sup-ply, or searching for the home page of the University of Puget Sound. The correct expansion of the acronym depends on the intent of the user. This intent may correspond to any of the standard Web query types [7]: navigational (seeking a home page), informational (seeking information on power supplies), or transactional (seeking a form to track a pack-age).

It is difficult to argue that any one of these five pages is more relevant than any other. For all of them, there is a group of users for which it is the best result. Under the topic development methodology used by TREC, and by similar experimental efforts, relevance judgments would depend on the details of the topic narrative, details which are hidden from the IR system. Depending on these hidden details, any of these documents might be judged relevant or non-relevant. Naturally, because of the glaring ambiguity, a topic based on this query would not be accepted for inclusion at TREC, allowing the problem to be avoided. Unfortunately, the problem cannot be avoided in practice.

One possible guide for ranking these pages is the relative sizes of the groups for which they would be relevant. At a guess, the group of users intending the United Parcel Service is substantially larger than the group intending the Univer-sity of Puget Sound, even within Washington State. The number of users interested in uninterruptible power supplies may fall somewhere in between.

The ordering in Table 1 is consistent with this guess. But note that a page related to uninterruptible power supplies (#3) lies between two pages related to the United Parcel Service. This arrangement may be justified by assuming that users interested in uninterruptible power supplies form a plurality of the users still scanning the result list at that point. By the fifth result, users interested in the university may form a plurality. Thus, diversity in the results proceeds directly from the needs of the user population.

Assuming that Table 1 gives the best possible ranking (which it may not) it can be justified informally and intu-itively. It should be possible for our evaluation measure to reflect this intuition, assigning the highest score to precisely this ranking. Our second example is based on a topic taken from the TREC 2005 question answering task [28]. In this task, ques-tions were grouped into series, with a single target associ-ated with each of these series. Figure 1 gives the target and questions for topic 85:  X  X orwegian Cruise Lines (NCL) X . The goal of a participating QA system was to provide exact
Figure 1: TREC 2005 question answering topic 85 answers to these questions, when given both the target and the question.

We view this topic in a different light, treating the target as a query, and the questions as representatives or examples of the information a user may be seeking. Table 2 presents the results of executing the target as a query using a typi-cal implementation of the BM25 scoring formula [26]. The corpus is the same AQUAINT collection of newspaper arti-cles used at TREC. The titles of the top ten documents are shown. For each article, the table indicates the questions answered by that article, according to the official TREC judgments. For the purpose of this example, we consider a document to answer question 85.1 if it lists the name of any NCL ship. The last column gives the total number of questions answered.

While these questions certainly do not cover all aspects of the topic, we might view them as reasonable representa-tives. From this viewpoint, we might base overall document relevance on these questions, treating the total number an-swered as a graded relevance value. Therefore, if we consider only the number of questions answered, one  X  X deal X  ordering for the documents would be a-e-b-c-f-g-h-d-i-j, with those documents answering two questions placed before those an-swering one.

If we consider novelty, our ideal ordering would place doc-ument g third, ahead of other documents answering one question, since only document g answers question 85.3. More-over, the ordering a-e-g covers all the questions, with the exception of question 85.5, which is not answered by any document. The other documents might then be considered non-relevant, since they add nothing new.

However, since these other documents likely contain as-pects not covered by the questions, we should not just stop at the third document. In addition, the judgments may contain errors, or the document may not fully answer an in-dicated question. Given the information available, we might complete our ranking by considering the number of times each question is answered. Document b (answering 85.2) might be ranked after document g, followed by document f (answering 85.1), and then by documents c and h (answering these questions for a third time). The final ordering would be a-e-g-b-f-c-h-i-j.
The probability ranking principle (PRP) forms the bedrock of information retrieval research [22, 24]. We state the prin-ciple as follows: The PRP is often interpreted as a nascent retrieval algo-rithm: Estimate the probability of relevance for each doc-ument and sort. We take a different view, interpreting the PRP as the starting point for the definition of an objective function to be optimized by the IR system.

Let q be a query. This query is implicit and fixed through-out our discussion. Let u be the information need occasion-ingausertoformulate q ,andlet d be a document that may or may not be relevant to u .Let R be a binary random variable representing relevance. To apply the PRP, we must estimate
It has become common in the summarization and question answering communities to refer to information nuggets ,and to assess summaries on the basis of the nuggets they con-tain [15]. Following this lead, we model our user X  X  informa-tion need as a set of nuggets u  X  X  ,where N = { n 1 , ...n is the space of possible nuggets. Similarly, the information present in a document is modeled as a set of nuggets d  X  X 
We interpret the notion of a nugget broadly, extending its usual meaning to encompass any binary property of a document. As is typical in summarization and question an-swering, a nugget may represent a fact or similar piece of information. In our QA example, a nugget might represent an answer to a question. However, a nugget may also rep-resent other binary properties, such as topicality. We may also use a nugget to indicate that a page is part of particular Web site or is the home page of a particular organization. In our Web search example, a nugget might represent a specific fact about uninterruptible power supplies, a form for track-ing packages, or the university X  X  home page. Thus, nuggets may be used to model navigational needs, as well as infor-mational needs.

Following the practice at TREC and other evaluation fo-rums [18], we consider a document relevant if it contains any relevant information. In other words, a particular doc-ument is relevant if it contains at least one nugget that is also contained in the user X  X  information need.
 For a particular nugget n i , P ( n i  X  u ) denotes the prob-ability that the user X  X  information need contains n i P ( n i  X  d ) denotes the probability that the document con-tains n i . These probabilities may be estimated for user infor-mation needs separate from documents, and for documents separate from user information needs. The only connection is the set of nuggets associated with each.

Traditionally, the probabilities are estimated to be 0 or 1 forparticularexamplesof u and d ;thatis P ( n i  X  u )=1in-dicates that n i is known to satisfy u ; P ( n i  X  u ) = 0 indicates that n i is known not to satisfy u . Similarly, P ( n i  X  indicates that n i is found in d , and vice versa. This tradi-tional model overstates the certainty with which either of these quantities may be assessed. Taking a more relaxed view better models the true situation. Human assessors are known to be inconsistent in their judgments [27]. Relevance judgments inferred from implicit user feedback may not al-ways be accurate [1, 2, 16, 21]. If a classifier is applied to augment manual judgments, we may take advantage of a probability supplied by the classifier itself [10].
To formulate an objective function over needs and docu-ments, we assume the independence of n i  X  u and n j = i  X  by each document are indicated. also of n i  X  d and n j = i  X  d . Under this assumption, Equa-tion 1 may be rewritten
P ( R =1 | u, d )=1  X 
Next, we turn our attention to the problem of estimat-ing P ( n i  X  u )and P ( n i  X  d ). With respect to the user, we are making the strong assumption that a user X  X  interest in one nugget is independent of other nuggets. We discuss the ramifications of this assumption further in Section 4.2. We then consider ranked lists, where the relevance of each subsequent element is conditioned on the preceding ones.
To estimate P ( n i  X  d ) we adopt a simple model inspired by the manual judgments typical of TREC tasks. We as-sume that a human assessor reads d and reaches a binary decision regarding each nugget: Is the nugget contained in the document or not?
Let J ( d, i ) = 1 if the assessor has judged that d contains nugget n i ,and J ( d, i ) = 0 if not. A possible estimate for P ( n i  X  d ) is then: The value  X  is a constant with 0 &lt; X   X  1, which reflects the possibility of assessor error. This definition assumes that positive judgments may be erroneous, but that nega-tive judgments are always correct. This definition is a crude approximation of reality, but is still a step beyond the tra-ditional assumption of perfect accuracy. More sophisticated estimates are possible, but are left for future work. If we assume Equation 3, then Equation 2 becomes:
In arguing for evaluation methodologies that address query ambiguity, Sp  X  arck Jones et al. [25] emphasize that queries  X  X re linguistically ambiguous, not just in the classic sense of words with multiple senses present in a dictionary, but also ambiguous across place names, person names, acronyms, etc. X  A number of researchers have investigated the rela-tionship between query ambiguity and query difficulty [4, 14, 19]. Cronen-Townsend et al. [14] view ambiguity as a property that is inherent to a query  X  X ith respect to the col-lection being searched X . They develop and validate a clarity score , based on the K-L divergence between a query language model and the collection language model. Their query lan-guage model is constructed from the top-ranked documents returned by the query. The clarity score is intended to re-flect the coherence of these documents: Are they about a single topic or a mixture of topics?
In part, ambiguity may be associated with dependencies between the nuggets, which are ignored by Equation 2. While a user interested in the Norwegian Cruise Lines may find any fact regarding the company useful, the same cannot be said for our Web search example. A user interested in the parcel service will be less interested in nuggets related to power supplies.

An evaluation measure intending to reward diversity must take these dependencies into account when estimating P ( n u ). In the case of our Web search example, we identified three possible interpretations of the query. Assuming our intuition regarding the user population is correct, nuggets related to the parcel service must be assigned substantially higher probabilities than nuggets related to the university. Problems can occur if, for example, the number of nuggets representing a more obscure interpretation is substantially larger than the number of nuggets representing a more pop-ular interpretation. Under these circumstances, a document containing many nuggets related to the more obscure inter-pretation may receive an inappropriately high probability of relevance. We leave for future work the question of whether these dependencies represent a problem in practice
Beyond these dependencies, our notion of ambiguity in-cludes other forms of underspecified queries. A user typing  X  X PS X  may be tracking a package more often than locating the UPS Store in Redmond, Washington. Navigational in-terpretations of a query, for www.ups.com and www.ups.edu , may be accommodated by assigning high probabilities to nuggets associated with these home pages.

Assigning meaningful probabilities requires knowledge of user preferences, which might be determined explicitly or implicitly from user behavior and feedback. In the absence of this knowledge, we might assume that nuggets are inde-pendent and equally likely to be relevant. Assuming P ( n u )=  X  for all i ,where  X  is a constant, and substituting into Equation 4, gives
To this point, we have worked with a single document only. Applying Equation 5 to each document allows us to determine the one to be ranked first. For the second and subsequent documents, we must view relevance in the light of the documents that rank higher.

Assume we have a relevance estimate for the first k  X  1 documents in a ranked list ( d 1 , ..., d k  X  1 ) and are now con-sidering the relevance of d k , the document at rank k .Let the random variable associated with relevance at each rank be R 1 ,..., R k . Our goal is to estimate P ( R k =1 | u, d
We assume that if a specific nugget appears in these first k  X  1 documents, then a repetition in d k will provide no additional benefit  X  that redundancy is to be avoided in favor of novelty. Thus, the probability that the user is still interested in the nugget depends on the contents of these documents We now define the number of documents ranked up to position k  X  1that have been judged to contain nugget n i . For convenience, we define r i, 0 =0. Thus, and in the place of Equation 5 we have,
We now apply the results of the previous section to com-pute gain vectors for use with the Normalized Discounted Cumulative Gain measure [20]. Over the past few years, nDCG has established itself as the standard evaluation mea-sure when graded relevance values are available [1,3,9]. Since graded relevance values arise naturally from the framework in the previous section, application to nDCG seems appro-priate.

The first step in the computation of nDCG is the creation of a gain vector . While we could calculate a gain vector directly from Equation 6, it is convenient to simplify the equation further, as follows: P ( R k =1 | u, d 1 , ... )= 1  X  Dropping the constant  X  X  , which has no impact on relative values, we define the k th element of the gain vector G as For our QA example, if we set  X  =1 / 2, the document or-dering listed in Table 2 would give Note that, if we set  X  = 0 and use a single nugget indicating topicality, the gain vector in Equation 7 represents standard binary relevance.

The second step in the computation of nDCG is the cal-culation of the cumulative gain vector For our QA example, Before computing the cumulative gain vector, a discount may be applied at each rank to penalize documents lower in the ranking, reflecting the additional user effort required to reach them. A typical discount is log 2 (1 + k ), although other discount functions are possible and may better reflect user effort [20]. We define discounted cumulative gain as For our QA example, The final step normalizes the discounted cumulative gain vector against an  X  X deal X  gain vector. However, CG and DCG may also be used directly as evaluation measures. In a study based on Web search results, Al-Maskari et al. [3] provide evidence that CG and DCG correlate better with user satisfaction than nDCG. Nonetheless, we include the normalization step in the results reported by this paper, leaving the exploration of the unnormalized measures for future work.
The ideal ordering is the ordering that maximizes cumu-lative gain at all levels. In Section 3.2 we presented the intu-ition behind the ideal ordering for the documents in Table 2. For these documents, the ideal ordering is a-e-g-b-f-c-h-i-j. The associated ideal gain vector is The ideal cumulative gain vector is and the ideal discounted cumulative gain vector is
In theory, the computation of the ideal gain vector is NP-complete. Given the definition of gain in Equation 7, mini-mal vertex covering may be reduced to computing an ideal gain vector. To transform vertex covering, we map each vertex into a document. Each edge corresponds to a nugget, with each nugget occurring in exactly two documents. Com-puting the ideal gain vector with  X  = 1 provides the minimal vertex covering.
In practice, we have found it sufficient to compute (an approximation to) the ideal gain vector using a greedy ap-proach [13, 31]. At each step, we select the document with the highest gain value, breaking ties arbitrarily. If we never encounter ties, this approach will compute the ideal gain vector. If ties occur, the gain vector may not be optimal. In the unusual event that a retrieval system outperforms this approximation, it would be credited with an ideal result.
As final step in the computation of nDCG we normalize discounted cumulative gain by the ideal discounted cumula-tive gain vector For our QA example, As is typical for IR evaluation measures, nDCG is computed over a set of queries by taking the arithmetic mean of the nDCG values for the individual queries. nDCG is typically reported at various retrieval de pths, similar to precision and recall.

Our version of nDCG rewards novelty through the gain value defined in Equation 7. Otherwise it adheres to a standard definition of nDCG. To distinguish our version of nDCG, we refer to it as  X  -nDCG, emphasizing the role of the parameter  X  in computing the gain vector. When  X  =0,the  X  -nDCG measure corresponds to standard nDCG with the number of matching nuggets used as the graded relevance value.
The theory in the previous sections assumes that together the nuggets provide complete coverage of all information re-lated to all interpretations of the query, potentially thou-sands or millions of nuggets. In practice, we may have to limit ourselves to a much smaller number, particularly if the topic creation and judging process is largely manual.
In this section we explore the creation of a test collec-tion based on the preceding theory. We take as our starting point test collections from the TREC 2005 and 2006 ques-tion answering tracks. While these collections were built for an entirely different purpose, they do provide the basic structure of our desired collection. In this paper, we report only results using the TREC 2006 test collection. We used the TREC 2005 QA test collection for exploratory work; we do not report results using that collection, but they are consistent with the results from the 2006 collection.
The TREC 2006 collection comprises 75 question series, each based around a single target, similar to the example from 2005 given in Figure 1. The target from each series was treated as a query. The questions formed the basis for nuggets, with one or more nuggets associated with most questions. When creating nuggets, we omitted the last ques-tion in each series, which is a catch-all  X  X THER X  question asking for any other information the system could provide. Apart from the list questions, such as 85.1, a single nugget was associated with each of the remaining questions. For list questions, a nugget was associated with each possible answer to the question (unlike the example in Table 2). This proce-dure resulted in a query set with an average of 17.12 nuggets per query. The maximum number of nuggets per query is 56; the minimum is 7.

Official TREC judgments are available for the question se-ries [15]. We processed these official judgments into a total of 3,243 tuples, where each tuple specifies a query, a doc-ument and a nugget contained within it. Unfortunately, it is well known that official TREC judgments for QA tasks are incomplete [23] and our preliminary exploration of the 2005 collection confirmed this view. To complete the judg-ing, we relied on a set of patterns distributed as part of the QA test collections. These patterns are designed to identify potential answers in unjudged documents, which then may be manually confirmed for QA evaluation purposes [23].
When run against the documents with official judgments, these patterns give a recall of 99% and a precision of 36%. While it is not reasonable to expect that an arbitrary doc-ument matching a pattern has a 36% chance of containing the corresponding nugget, it may be reasonable to assume that a document surfaced through a retrieval process does have this chance of containing the nugget. Consistent with the preceding theory, we might then set  X  =0 . 36.
In the following experiments, we base our judging on a combination of the official judgments and the patterns. We expect that any re-usable test collection focused on novelty and diversity will include an automatic judging component. Under many traditional IR evaluation measures, such as MAP and bpref, an  X  X deal X  retrieval result has all the rele-vant documents ranked ahead of all non-relevant documents. Unlike these measures, which consider only binary relevance,  X  -nDCG rewards both diversity and novelty (when  X &gt; 0). Results that would score a perfect 1 under traditional evalu-ation measures may score considerably lower under  X  -nDCG.
To explore the extremes, we consider the effects of re-ordering relevant documents. In Section 5.1 we discussed the computation of an ideal gain vector. It is also possi-ble to compute what we call a reversed ideal gain vector . This vector is constructed using a similar greedy algorithm, but attempts to minimize the  X  -nDCG score of the relevant documents  X  those that are judged to contain one or more nuggets.

Figure 2 plots  X  -nDCG values for the reversed ideal gain vector, for various values of  X  . In this graph, the ideal gain a) Standard Okapi feedback: b) K-L divergence feedback: Figure 3: Impact of pseudo-relevance feedback on BM25 runs. vector corresponds to a horizontal line at 1. As  X  increases, the gap between the two vectors also increases. From the standpoint of classic binary relevance, both the ideal gain vector and this reversed ideal gain vector are equivalent, with all relevant documents ranked first.
The work of Chen and Karger [13] and Amati et al. [4] suggested to us that pseudo-relevance feedback may have a negative impact on novelty. Our collection provides an opportunity to test this hypothesis.

We executed the queries over the AQUAINT corpus, gen-erating three runs. One run was generated by a version of the standard BM25 scoring formula. The other two runs rep-resent variants of pseudo-relevance feedback: the first gener-ated through standard Okapi-style feedback and the second generated through the K-L divergence feedback method de-scribed by Carpineto at al. [12].

Figure 3 shows the impact of pseudo-relevance feedback over the baseline BM25 run for both forms of feedback. When  X  =0the  X  -nDCG measure is equivalent to stan-dard nDCG with the number of matching nuggets used as the graded relevance value. Under this measure, which does not reward novelty, both variants of pseudo-relevance feed-back produce typical performance improvements over the baseline BM25 run. At most ranks, these improvements are significant at the 95% level using a two-sided paired t-test. As  X  increases, rewarding novelty, the situation changes. At  X  =0 . 5, there is no measured improvement over the base-line. At higher values, the curves for the pseudo-relevance feedback lie below the baseline, although this decrease is not significant.
Our goal is to define a workable evaluation framework for information retrieval that accounts for novelty and diversity in a sound fashion. In our framework, documents are linked to relevance through informational nuggets, which represent properties of documents at one end and components of an information need at the other. The relationship between relevance and documents is captured by Equation 2, which lies at the core of our work. Its subsequent development and application to nDCG represents only one possible path. Other paths remain open.

Serious criticism could be applied to many links in our chain of assumptions. In particular, our assumption of in-dependence between nuggets is invalid when the query has multiple unrelated interpretations. Moreover, we take a nar-row view of relevance: that a document is relevant if and only if it contains a previously unreported nugget useful to the user. In some cases, repetition of information may also be useful, perhaps by increasing the user X  X  confidence in its correctness. While the value of repetition is tacitly recog-nized in Equation 3, by giving credit to repeated nuggets, it may be beneficial to explicitly include it in the model.
The relatively small number nuggets used to operational-ize our framework might be the subject of further concern, since many aspects will be unrepresented by these nuggets. This concern might be partially addressed by recognizing that the presence of a nugget in a document suggests the presence of information related to that nugget but not di-rectly represented by it. For example, if the name of NCL X  X  own private island appears in a document, answering ques-tion 85.3 in Figure 1, other information about the island may also appear. If the nugget appears again in later doc-uments, different information may accompany it. Again, Equation 3 tacitly recognizes this possibility, but does not explicitly model it.

Despite these concerns, we believe we have made substan-tial progress towards our goal. Unusual features of our ap-proach include recognition of judging error and the ability to incorporate a user model. While our experiments are limited in scope, they do demonstrate the feasibility of con-structing evaluation measures and test collections under the framework. [1] E. Agichtein, E. Brill, and S. Dumais. Improving web [2] E. Agichtein, E. Brill, S. Dumais, and R. Ragno. [3] A. Al-Maskari, M. Sanderson, and P. Clough. The [4] G. Amati, C. Carpineto, and G. Romano. Query [5] Y. Bernstein and J. Zobel. Redundant documents and [6] B. Boyce. Beyond topicality: A two stage view of [7] A. Broder. A taxonomy of Web search. SIGIR Forum , [8] C. Buckley and E. M. Voorhees. Retrieval evaluation [9] C. J. C. Burges, T. Shaked, E. Renshaw, A. Lazier, [10] S. B  X  uttcher, C. L. A. Clarke, P. C. K. Yeung, and [11] J. Carbonell and J. Goldstein. The use of MMR, [12] C. Carpineto, R. de Mori, G. Romano, and B. Bigi. [13] H. Chen and D. R. Karger. Less is more: Probabilistic [14] S. Cronen-Townsend, Y. Zhou, and W. B. Croft. [15] H. T. Dang, J. Lin, and D. Kelly. Overview of the [16] G. Dupret, V. Murdock, and B. Piwowarski. Web [17] W. Goffman. A searching procedure for information [18] D. K. Harman. The TREC test collections. In Ellen M. [19] B. He and I. Ounis. Query performance prediction. [20] K. J  X  arvelin and J. Kek  X  al  X  ainen. Cumulated gain-based [21] T. Joachims, L. Granka, B. Pan, H. Hembrooke, and [22] J. Lafferty and C. Zhai. Probabilistic relevance models [23] J. Lin and B. Katz. Building a reusable test collection [24] S. Robertson. The probability ranking principle in IR. [25] K. Sp  X  arck Jones, S. E. Robertson, and M. Sanderson. [26] K. Sp  X  arck Jones, S. Walker, and S. E. Robertson. A [27] E. M. Voorhees. Variations in relevance judgments and [28] E. M. Voorhees and H. T. Dang. Overview of the [29] Y. Xu and Hainan Yin. Novelty and topicality in [30] Y. Yue, T. Finley, F. Radlinski, and T. Joachims. A [31] C. Zhai, W. W. Cohen, and J. Lafferty. Beyond [32] C. Zhai and J. Lafferty. A risk minimization
