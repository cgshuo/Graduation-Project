 Web queries can be considered as implicit ques-tions or commands, in that they are performed ei-ther to find information on the web or to initiate interaction with web services. Web users, how-ever, rarely express their intent in full language. For example, to find out  X  X hat are the movies of 2010 in which johnny depp stars X , a user may sim-ply query  X  X ohnny depp movies 2010 X . Today X  X  search engines, generally speaking, are based on matching such keywords against web documents and ranking relevant results using sophisticated features and algorithms.

As search engine technologies evolve, it is in-creasingly believed that search will be shifting away from  X  X en blue links X  toward understanding intent and serving objects. This trend has been largely driven by an increasing amount of struc-tured and semi-structured data made available to search engines, such as relational databases and semantically annotated web documents. Search-ing over such data sources, in many cases, can offer more relevant and essential results com-pared with merely returning web pages that con-tain query keywords. Table 1 shows a simplified view of a structured data source, where each row represents a movie object. Consider the query  X  X ohnny depp movies 2010 X . It is possible to re-trieve a set of movie objects from Table 1 that satisfy the constraints Year = 2010 and Cast 3 Johnny Depp . This would deliver direct answers to the query rather than having the user sort through list of keyword results.

In no small part, the success of such an ap-proach relies on robust understanding of query in-tent. Most previous works in this area focus on query intent classification (Shen et al., 2006; Li et al., 2008b; Arguello et al., 2009). Indeed, the intent class information is crucial in determining if a query can be answered by any structured data sources and, if so, by which one. In this work, we go one step further and study the semantic struc-ture of a query, i.e. , individual constituents of a query and their semantic roles. In particular, we focus on noun phrase queries. A key contribution of this work is that we formally define query se-mantic structure as comprised of intent heads (IH) and intent modifiers (IM), e.g. , [ It is determined that  X  X ast X  is an IH of the above query, representing the essential information the user intends to obtain. Furthermore, there are two IMs,  X  X lice in wonderland X  and  X 2010 X , serving as filters of the information the user receives.
Identifying the semantic structure of queries can be beneficial to information retrieval. Knowing the semantic role of each query constituent, we can reformulate the query into a structured form or reweight different query constituents for struc-tured data retrieval (Robertson et al., 2004; Kim et al., 2009; Paparizos et al., 2009). Alternatively, the knowledge of IHs, IMs and semantic labels of IMs may be used as additional evidence in a learn-ing to rank framework (Burges et al., 2005).
A second contribution of this work is to present methods that automatically extract the semantic structure of noun phrase queries, i.e. , IHs, IMs and the semantic labels of IMs. In particular, we investigate the use of transition, lexical, semantic and syntactic features. The semantic features can be constructed from structured data sources or by mining query logs, while the syntactic features can be obtained by readily-available syntactic analy-sis tools. We compare the roles of these features in two discriminative models, Markov and semi-Markov conditional random fields. The second model is especially interesting to us since in our task it is beneficial to use features that measure segment-level characteristics. Finally, we evaluate our proposed models and features on manually-annotated query sets from three domains, while our techniques are general enough to be applied to many other domains. 2.1 Query intent understanding As mentioned in the introduction, previous works on query intent understanding have largely fo-cused on classification, i.e. , automatically map-ping queries into semantic classes (Shen et al., 2006; Li et al., 2008b; Arguello et al., 2009). There are relatively few published works on un-derstanding the semantic structure of web queries. The most relevant ones are on the problem of query tagging, i.e. , assigning semantic labels to query terms (Li et al., 2009; Manshadi and Li, 2009). For example, in  X  X anon powershot sd850 camera silver X , the word  X  X anon X  should be tagged as Brand . In particular, Li et al. leveraged click-through data and a database to automatically de-rive training data for learning a CRF-based tagger. Manshadi and Li developed a hybrid, generative grammar model for a similar task. Both works are closely related to one aspect of our work, which is to assign semantic labels to IMs. A key differ-ence is that they do not conceptually distinguish between IHs and IMs.

On the other hand, there have been a series of research studies related to IH identification (Pasca and Durme, 2007; Pasca and Durme, 2008). Their methods aim at extracting attribute names, such as cost and side effect for the concept Drug , from documents and query logs in a weakly-supervised learning framework. When used in the context of web queries, attribute names usually serve as IHs. In fact, one immediate application of their research is to understand web queries that request factual information of some concepts, e.g.  X  X siprin cost X  and  X  X spirin side effect X . Their framework, however, does not consider the identification and categorization of IMs (attribute values). 2.2 Question answering Query intent understanding is analogous to ques-tion understanding for question answering (QA) systems. Many web queries can be viewed as the keyword-based counterparts of natural language questions. For example, the query  X  X alifornia na-tional X  and  X  X ational parks califorina X  both imply the question  X  X hat are the national parks in Cali-fornia? X . In particular, a number of works investi-gated the importance of head noun extraction in understanding what-type questions (Metzler and Croft, 2005; Li et al., 2008a). To extract head nouns, they applied syntax-based rules using the information obtained from part-of-speech (POS) tagging and deep parsing. As questions posed in natural language tend to have strong syntactic structures, such an approach was demonstrated to be accurate in identifying head nouns.

In identifying IHs in noun phrase queries, how-ever, direct syntactic analysis is unlikely to be as effective. This is because syntactic structures are in general less pronounced in web queries. In this work, we propose to use POS tagging and parsing outputs as features, in addition to other features, in extracting the semantic structure of web queries. 2.3 Information extraction Finally, there exist large bodies of work on infor-mation extraction using models based on Markov and semi-Markov CRFs (Lafferty et al., 2001; Sarawagi and Cohen, 2004), and in particular for the task of named entity recognition (McCallum and Li, 2003).

The problem studied in this work is concerned with identifying more generic  X  X emantic roles X  of the constituents in noun phrase queries. While some IM categories belong to named entities such as IM: Director for the intent class Movie , there can be semantic labels that are not named entities such as IH and IM: Genre (again for Movie ). Unlike database query languages such as SQL, web queries are usually formulated as sequences of words without explicit structures. This makes web queries difficult to interpret by computers. For example, should the query  X  X spirin side effect X  be interpreted as  X  X he side effect of aspirin X  or  X  X he aspirin of side effect X ? Before trying to build mod-els that can automatically makes such decisions, we first need to understand what constitute the se-mantic structure of a noun phrase query. 3.1 Definition We let C denote a set of query intent classes that represent semantic concepts such as Movie , Prod-uct and Drug . The query constituents introduced below are all defined w.r.t. the intent class of a query, c  X  C , which is assumed to be known. Intent head An intent head (IH) is a query segment that cor-responds to an attribute name of an intent class. For example, the IH of the query  X  X lice in won-derland 2010 cast X  is  X  X ast X , which is an attribute name of Movie . By issuing the query, the user in-tends to find out the values of the IH ( i.e. , cast). A query can have multiple IHs, e.g. ,  X  X ovie avatar director and cast X . More importantly, there can be queries without an explicit IH. For example,  X  X ovie avatar X  does not contain any segment that corresponds to an attribute name of Movie . Such a query, however, does have an implicit intent which is to obtain general information about the movie. Intent modifier In contrast, an intent modifier (IM) is a query seg-ment that corresponds to an attribute value (of some attribute name). The role of IMs is to impos-ing constraints on the attributes of an intent class. For example, there are two constraints implied in the query  X  X lice in wonderland 2010 cast X : (1) the Title of the movie is  X  X lice in wonderland X ; and (2) the Year of the movie is  X 2010 X . Interestingly, the user does not explicitly specify the attribute names, i.e. , Title and Year , in this query. Such information, however, can be inferred given do-main knowledge. In fact, one important goal of this work is to identify the semantic labels of IMs, i.e. , the attribute names they implicitly refer to. We use A the intent class c .
 Other Additionally, there can be query segments that do not play any semantic roles, which we refer to as Other. 3.2 Syntactic analysis The notion of IHs and IMs in this work is closely related to that of linguistic head nouns and modi-fiers for noun phrases. In many cases, the IHs of noun phrase queries are exactly the head nouns in the linguistic sense. Exceptions mostly occur in queries without explicit IHs, e.g. ,  X  X ovie avatar X  in which the head noun  X  X vatar X  serves as an IM instead. Due to the strong resemblance, it is inter-esting to see if IHs can be identified by extracting linguistic head nouns from queries based on syn-tactic analysis. To this end, we apply the follow-ing heuristics for head noun extraction. We first run a POS-tagger and a chunker jointly on each query, where the POS-tagger/chunker is based on an HMM system trained on English Penn Tree-bank (Gao et al., 2001). We then mark the right most NP chunk before any prepositional phrase or adjective clause, and apply the NP head rules (Collins, 1999) to the marked NP chunk.

The main problem with this approach, however, is that a readily-available POS tagger or chunker is usually trained on natural language sentences and thus is unlikely to produce accurate results on web queries. As shown in (Barr et al., 2008), the lexi-cal category distribution of web queries is dramat-ically different from that of natural languages. For example, prepositions and subordinating conjunc-tions, which are strong indicators of the syntactic structure in natural languages, are often missing in web queries. Moreover, unlike most natural lan-guages that follow the linear-order principle, web queries can have relatively free word orders (al-though some orders may occur more often than others statistically). These factors make it diffi-cult to produce reliable syntactic analysis outputs. Consequently, the head nouns and hence the IHs extracted therefrom are likely to be error-prone, as will be shown by our experiments in Section 6.3.
Although a POS tagger and a chunker may not work well on queries, their output can be used as features for learning statistical models for seman-tic structure extraction, which we introduce next. This section presents two statistical models for se-mantic understanding of noun phrase queries. As-suming that the intent class c  X  C of a query is known, we cast the problem of extracting the se-mantic structure of the query into a joint segmen-tation/classification problem. At a high level, we would like to identify query segments that corre-spond to IHs, IMs and Others. Furthermore, for each IM segment, we would like to assign a se-mantic label, denoted by IM: a , a  X  A which attribute name it refers to. In other words, our label set consists of Y = { IH, { IM: a } Other } .

Formally, we let x = ( x an input query of length M . To avoid confusion, we use i to represent the index of a word token and j to represent the index of a segment in the following text. Our goal is to obtain where s = ( s mentation as well as a classification of all seg-ments. Each segment s ple ( u the starting and ending word tokens respectively; y j  X  Y s . We further augment the segment sequence with two special segments: Start and End , represented by s ity, we assume that the intent class is given and mind that the label space and hence the parameter space is class-dependent. Now we introduce two methods of modeling p ( s | x ) . 4.1 CRFs One natural approach to extracting the semantic structure of queries is to use linear-chain CRFs (Lafferty et al., 2001). They model the con-ditional probability of a label sequence given the input, where the labels, denoted as y = ( y 1 , y 2 , . . . , y M ) respondence with the word tokens in the input.
Using linear-chain CRFs, we aim to find the la-bel sequence that maximizes p ( y | x ) = The partition function Z factor.  X  is a weight vector and f ( y a vector of feature functions referred to as a fea-ture vector. The features used in CRFs will be de-scribed in Section 5.

Given manually-labeled queries, we estimate  X  that maximizes the conditional likelihood of train-ing data while regularizing model parameters. The learned model is then used to predict the label se-quence y for future input sequences x . To obtain s in Equation (1), we simply concatenate the maxi-mum number of consecutive word tokens that have the same label and treat the resulting sequence as a segment. By doing this, we implicitly assume that there are no two adjacent segments with the same label in the true segment sequence. Although this assumption is not always correct in practice, we consider it a reasonable approximation given what we empirically observed in our training data. 4.2 Semi-Markov CRFs In contrast to standard CRFs, semi-Markov CRFs directly model the segmentation of an input se-quence as well as a classification of the segments (Sarawagi and Cohen, 2004), i.e. , p ( s | x ) = In this case, the features f ( s fined on segments instead of on word tokens. More precisely, they are of the function form imposing a constraint u reduced to standard linear-chain CRFs. Semi-Markov CRFs make Markov assumptions at the segment level, thereby naturally offering means to incorporate segment-level features, as will be pre-sented in Section 5. In this work, we explore the use of transition, lexi-cal, semantic and syntactic features in Markov and semi-Markov CRFs. The mathematical expression of these features are summarized in Table 2 with details described as follows. 5.1 Transition features Transition features, i.e. , A1 and B1 in Table 2, capture state transition patterns between adjacent word tokens in CRFs, and between adjacent seg-ments in semi-Markov CRFs. We only use first-order transition features in this work. 5.2 Lexical features In CRFs, a lexical feature (A2) is implemented as a binary function that indicates whether a specific word co-occurs with a state label. The set of words to be considered in this work are those observed in the training data. We can also generalize this type of features from words to n -grams. In other words, instead of inspecting the word identity at the current position, we inspect the n -gram iden-tity by applying a window of length n centered at the current position.

Since feature functions are defined on segments in semi-Markov CRFs, we create B2 that indicates whether the phrase in a hypothesized query seg-ment co-occurs with a state label. Here the set of phrase identities are extracted from the query seg-ments in the training data. Furthermore, we create another type of lexical feature, B3, which is acti-vated when a specific word occurs in a hypothe-sized query segment. The use of B3 would favor unseen words being included in adjacent segments rather than to be isolated as separate segments. 5.3 Semantic features Models relying on lexical features may require very large amounts of training data to produce accurate prediction performance, as the feature space is in general large and sparse. To make our model generalize better, we create semantic fea-tures based on what we call lexicons . A lexicon, denoted as L , is a cluster of semantically-related words/phrases. For example, a cluster of movie titles or director names can be such a lexicon. Be-fore describing how such lexicons are generated for our task, we first introduce the forms of the semantic features assuming the availability of the lexicons.
 We let L denote a lexicon, and W set of n -grams extracted from L . For CRFs, we create a binary function that indicates whether any n -gram in W n = 1 , 2 for A3, A4 respectively. For both A3 and A4, the number of such semantic features is equal to the number of lexicons multiplied by the number of state labels.

The same source of semantic knowledge can be conveniently incorporated in semi-Markov CRFs. One set of semantic features (B4) inspect whether the phrase of a hypothesized query segment matches any element in a given lexicon. A sec-ond set of semantic features (B5) relax the exact match constraints made by B4, and take as the fea-ture value the maximum  X  X imilarity X  between the query segment and all lexicon elements. The fol-lowing similarity function is used in this work , where Lev represents the Levenshtein distance. Notice that we normalize the Levenshtein distance by the length of the lexicon element, as we em-pirically found it performing better compared with normalizing by the length of the segment. In com-puting the maximum similarity, we first retrieve a set of lexicon elements with a positive tf-idf co-sine distance with the segment; we then evaluate Equation (4) for each retrieved element and find the one with the maximum similarity score.
 Lexicon generation To create the semantic features described above, we generate two types of lexicons leveraging databases and query logs for each intent class.
The first type of lexicon is an IH lexicon com-prised of a list of attribute names for the intent class, e.g. ,  X  X ox office X  and  X  X eview X  for the intent class Movie . One easy way of composing such a list is by aggregating the column names in the cor-responding database such as Table 1. However, this approach may result in low coverage on IHs for some domains. Moreover, many database col-umn names, such as Title , are unlikely to appear as IHs in queries. Inspired by Pasca and Van Durme (2007), we apply a bootstrapping algorithm that automatically learns attribute names for an intent class from query logs. The key difference from their work is that we create templates that consist of semantic labels at the segment level from train-ing data. For example,  X  X lice in wonderland 2010 cast X  is labeled as  X  X M: Title IM: Year IH X , and thus  X  X M: Title + IM: Year + # X  is used as a template. We select the most frequent templates (top 2 in this work) from training data and use them to discover new IH phrases from the query log.

Secondly, we have a set IM lexicons, each com-prised of a list of attribute values of an attribute name in A erate such lexicons. For example, the lexicon for IM: Title (in Movie ) is a list of movie titles gener-ated by aggregating the values in the Title column of a movie database. Similarly, the lexicon for IM: Employee (in Job ) is a list of employee names extracted from a job listing database. Note that a substantial amount of research effort has been dedicated to automatic lexicon acquisition from the Web (Pantel and Pennacchiotti, 2006; Pennac-chiotti and Pantel, 2009). These techniques can be used in expanding the semantic lexicons for IMs when database resources are not available. But we do not use such techniques in our work since the lexicons extracted from databases in general have good precision and coverage. 5.4 Syntactic features As mentioned in Section 3.2, web queries often lack syntactic cues and do not necessarily follow the linear order principle. Consequently, applying syntactic analysis such as POS tagging or chunk-ing using models trained on natural language cor-pora is unlikely to give accurate results on web queries, as supported by our experimental evi-dence in Section 6.3. It may be beneficial, how-ever, to use syntactic analysis results as additional evidence in learning.

To this end, we generate a sequence of POS tags for a given query, and use the co-occurrence of POS tag identities and state labels as syntactic fea-tures (A5) for CRFs.

For semi-Markov CRFs, we instead examine the POS tag sequence of the corresponding phrase in a query segment. Again their identities are com-bined with state labels to create syntactic features B6. Furthermore, since it is natural to incorporate segment-level features in semi-Markov CRFs, we can directly use the output of a syntactic chunker. To be precise, if a query segment is determined by the chunker to be a chunk, we use the indicator of the phrase type of the chunk ( e.g. , NP, PP) com-bined with a state label as the feature, denoted by B7 in the Table. Such features are not activated if a query segment is determined not to be a chunk. 6.1 Data To evaluate our proposed models and features, we collected queries from three domains, Movie , Job and National Park , and had them manually anno-tated. The annotation was given on both segmen-tation of the queries and classification of the seg-ments according to the label sets defined in Ta-ble 3. There are 1000/496 samples in the train-ing/test set for the Movie domain, 600/366 for the Job domain and 491/185 for the National Park do-main. In evaluation, we report the test-set perfor-mance in each domain as well as the average per-formance (weighted by their respectively test-set size) over all domains. National Park . 6.2 Metrics There are two evaluation metrics used in our work: segment F1 and sentence accuracy (Acc). The first metric is computed based on precision and re-call at the segment level. Specifically, let us as-sume that the true segment sequence of a query is s = ( s 1 , s 2 , . . . , s N ) , and the decoded segment sequence is s 0 = ( s 0 s and recall, then, are measured as the total num-ber of true positives divided by the total num-ber of decoded and true segments respectively. We report the F1-measure which is computed as 2  X  prec  X  recall / ( prec + recall ) .

Secondly, a sentence is correct if all decoded segments are true positives. Sentence accuracy is measured by the total number of correct sentences divided by the total number of sentences. 6.3 Results We start with models that incorporate first-order transition features which are standard for both Markov and semi-Markov CRFs. We then exper-iment with lexical features, semantic features and syntactic features for both models. Table 4 and Table 5 give a summarization of all experimental results.
 Lexical features The first experiment we did is to evaluate the per-formance of lexical features (combined with tran-sition features). This involves the use of A2 in Ta-ble 2 for CRFs, and B2 and B3 for semi-Markov CRFs. Note that adding B3, i.e. , indicators of whether a query segment contains a word iden-tity, gave an absolute 7.0%/3.2% gain in sentence accuracy and segment F1 on average, as shown in the row B1-B3 in Table 5. For both A2 and B3, we also tried extending the features based on word IDs to those based on n -gram IDs, where n = 1 , 2 , 3 . This greatly increased the number of lexical features but did not improve learning per-formance, most likely due to the limited amounts of training data coupled with the sparsity of such features. In general, lexical features do not gener-alize well to the test data, which accounts for the relatively poor performance of both models. Semantic features We created IM lexicons from three in-house databases on Movie , Job and National Parks . Some lexicons, e.g. , IM: State , are shared across domains. Regarding IH lexicons, we applied the bootstrapping algorithm described in Section 5.3 to a 1-month query log of Bing . We selected the most frequent 57 and 131 phrases to form the IH lexicons for Movie and National Park respectively. We do not have an IH lexicon for Job as the at-tribute names in that domain are much fewer and are well covered by training set examples.

We implemented A3 and A4 for CRFs, which are based on the n -gram sets created from lex-icons; and B4 and B5 for semi-Markov CRFs, which are based on exact and fuzzy match with lexicon items. As shown in Table 4 and 5, drastic increases in sentence accuracies and F1-measures were observed for both models.
 Syntactic features As shown in the row A1-A5 in Table 4, combined with all other features, the syntactic features (A5) built upon POS tags boosted the CRF model per-formance. Table 6 listed the most dominant pos-itive and negative features based on POS tags for Movie (features for the other two domains are not reported due to space limit). We can see that many of these features make intuitive sense. For example, IN (preposition or subordinating con-junction) is a strong indicator of Other, while TO and IM: Date usually do not co-occur. Some fea-tures, however, may appear less  X  X orrect X . This is largely due to the inaccurate output of the POS tagger. For example, a large number of actor names were mis-tagged as RB, resulting in a high positive weight of the feature (RB, IM: Cast ). Table 6: Syntactic features with the largest posi-tive/negative weights in the CRF model for Movie
Similarly, we added segment-level POS tag fea-tures (B6) to semi-Markov CRFs, which lead to the best overall results as shown by the highlighted numbers in Table 5. Again many of the dominant features are consistent with our intuition. For ex-ample, the most positive feature for Movie is (CD JJS, IM: Rating ) ( e.g. 100 best). When syntactic features based on chunking results (B7) are used instead of B6, the performance is not as good. Transition features In addition, it is interesting to see the importance of transition features in both models. Since web queries do not generally follow the linear order principle, is it helpful to incorporate transition fea-tures in learning? To answer this question, we dropped the transition features from the best sys-tems, corresponding to the last rows in Table 4 and 5. This resulted in substantial degradations in performance. One intuitive explanation is that although web queries are relatively  X  X rder-free X , statistically speaking, some orders are much more likely to occur than others. This makes it benefi-cial to use transition features.
 Comparison to syntactic analysis Finally, we conduct a simple experiment by using the heuristics described in Section 3.2 in extract-ing IHs from queries. The precision and recall of IHs averaged over all 3 domains are 50.4% and 32.8% respectively. The precision and recall num-bers from our best model-based system, i.e. , B1-B6 in Table 5, are 89.9% and 84.6% respectively, which are significantly better than those based on pure syntactic analysis. In this work, we make the first attempt to define the semantic structure of noun phrase queries. We propose statistical methods to automatically ex-tract IHs, IMs and the semantic labels of IMs us-ing a variety of features. Experiments show the ef-fectiveness of semantic features and syntactic fea-tures in both Markov and semi-Markov CRF mod-els. In the future, it would be useful to explore other approaches to automatic lexicon discovery to improve the quality or to increase the coverage of both IH and IM lexicons, and to systematically evaluate their impact on query understanding per-formance.

The author would like to thank Hisami Suzuki and Jianfeng Gao for useful discussions.
