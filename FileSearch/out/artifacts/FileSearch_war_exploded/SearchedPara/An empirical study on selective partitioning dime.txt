 1. Introduction data sets.
 formulated as where r and s are represented as [ r 1 , r 2 , ... , r d ] and [ s applicable to similarity join queries in high dimensional spaces.
With increasing dimensions, the domain cardinality of a data space grows exponentially (O( c tributions for efficient partitioning of a data space.
 paper is presented. 2. Background resent the resulted cells.
 other set. 3. Determination of partitioning dimensions increasing additional costs during the join step.

With increasing dimensions, the domain cardinality of a data space grows exponentially (O( c off similarity value.
 two input data sets, R and S , and the number of partitioning dimensions, d points, is formulated as follows: lated as follows: for the total number of disk blocks, j R j block for R and j S j
According to the Eqs. (2) and (3) , as the partitioning dimension, d desirable to lessen the CPU cost without enlarging the IO cost. 3.1. The number of partitioning dimensions other way for given the number of partitioning dimension, d
From the equation, N p  X  N 0 p , we can obtain the number of partitioning dimensions, d 3.2. Selection of partitioning dimensions sions. Note that the number of partitioning dimensions, d sions. The detail of the algorithm, PerDimSelect , is represented in Algorithm 1 [14] .
Algorithm 1 . PerDimSelect : Perpendicular dimension selection algorithm 1 set number of partitions, np d 1/ e e ; 2 initialize sampled data sets R s , S s ; 3 initialize partition arrays, P R [1, ... , d ][ 1 , ... , np ], P 4 initialize number of distance computations, JoinCost [1, ... , d ] 0; 5 6 // compute the number of entities for each partition 7 for each entity &lt; e 1 , e 2 , ... , e d &gt;in R s do 8 for each dimension i in [1, ... , d ] do 9 P R [ i ][ d np  X  e i e ]++; 10 end 11 end 12 for each entity &lt; e 1 , e 2 , ... , e d &gt;in S s do 13 for each dimension i in [1, ... , d ] do 14 P S [ i ][ d np  X  e i e ]++; 15 end 16 end 17 18 for each dimension i in [1, ... , d ] do 19 for each partition number p in [1, ... , np ] do 20 if ( i &gt;1) JoinCost [ i ] JoinCost [ i ]+ P R [ i ][ p ]  X  P 21 JoinCost [ i ] JoinCost [ i ]+ P R [ i ][ p ] * P S [ i ][ p ]; 22 if ( i &lt; np ) JoinCost [ i ] JoinCost [ i ]+ P R [ i ][ p ]  X  P 23 end 24 end 25 26 return the dimensions d for which JoinCost [ d ] is smaller. one overlapping with it ( Fig. 2 ).
 mation of the join costs between cells.
 sets may be sampled. 4. Partitioning with diagonal dimensions data sets can be distributed more evenly.

The data space of a diagonal dimension combined from k perpendicular dimensions is partitioning dimension across k perpendicular dimensions, d while the IO cost remains same as Eq. (3) .
 a diagonal dimension. Formally, for given two k -dimensional points, X =( x and their projected points on the diagonal dimension, X 0 inequality holds: selecting diagonal dimensions [16] .

Algorithm 2 . DiaDimSelect : Diagonal dimension selection algorithm 1 set number of partitions, np d 2 initialize sampled data sets R s , S s ; 3 initialize partition arrays, P R [1, ... , d ][ 1 , ... , np ], P 4 initialize number of distance computations, JoinCost [1, ... , d ] 0; 5 initialize sorted list of efficient dimensions resulting from 6 PerDimSelect , PerDim [1, ... , d ]; 7 8 compute the number of entities for each partition 9 for each entity &lt; e 1 , e 2 , ... , e d &gt;in R s do 10 for each dimension k in [1, ... , d ] do 11 P R [ k ][ d np 1 ffiffi 12 end 13 end 14 15 for each entity h e 1 , e 2 , ... , e d i in S s do 16 for each dimension k in [1, ... , d ] do 17 P S [ k ][ np 1 ffiffi 18 end 19 end 20 21 for each dimension i in[1, ... , d ] do 22 for each partition number p in [1, ... , np ] do 23 if ( i &gt;1) JoinCost [ i ] JoinCost [ i ]+ P R [ i ][ p ]  X  P 24 JoinCost [ i ] JoinCost [ i ]+ P R [ i ][ p ] * P S [ i ][ p ]; 25 if ( i &lt; np ) JoinCost [ i ] JoinCost [ i ]+ P R [ i ][ p ]  X  P 26 end 27 if ( JoinCost[i + 1] &gt; JoinCost[i] ) break; 28 end 29 30 return the i th diagonal dimension. 5. Experimental results from the Gaussian distribution.
 the optimal dimension was reduced into 3% as many as that of most of non-efficient dimensions. are distributed very sparsely, which in turn increases the IO cost. tioning results by utilizing more dimensions than PerDimSelect . the data distribution can be sparse, which may lead to the IO cost increase at the join step. 6. Related work lem of similarity join in main memory for low-and high-dimensional data. sets. 7. Conclusion Acknowledgement
The author thanks Hyunkyung Kim for her help in implementation of the algorithms.
References
