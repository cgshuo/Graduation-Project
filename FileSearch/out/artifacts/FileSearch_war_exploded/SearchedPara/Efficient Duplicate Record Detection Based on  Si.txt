 heterogeneous and autonomous data sources, multiple records representing the same entity are often obtained. Such records are ca lled duplicate records. In an information integration system, duplicate records not only cause data redundancy, resulting in the waste of network bandwidth and storage space, but also provide users too many useless detection, which is to partition the record set into clusters each representing the same entity, is the first step of duplicate record elimination. with duplicate records detection. [1] is a su rvey of the early work of duplicate records detection. Some recent work [5, 6, 7, 8] adopt similarity functions of textual similarity, catching most kinds of typographic errors. The major drawback of textual similarity is that it may be misleading in the conditions of heterogeneous expressions (e.g. J. Smith is short for both John Smith and Jane Smith). Some methods based on segmentation components. Segmentation-based methods can solve the problem of heterogeneous Transformation-based and grammar-based methods are proposed in [13] and [14], respectively. However all these methods does not consider the prob lem that the records may have various schema or attribute order and not practical for duplicate detection on records from heterogeneous data sources. 
To represent the similarity of records with heterogeneous schemas, optimal bipartite graph matching is adopted (na X ve Optimal Bipartite Matching Based, OBMAB) [15]. The graph. Such definition can describe the similarity of any records with different schema. As the comparing records are not necessarily having the same attribute number and order, the method is more suitable for the detection of the duplicate records from heterogeneous data sources effectively. However, each record needs to compare with a large number of data set. Moreover, a record will not be classified into a cluster even if only one record and the current record do not meet the requirements of similarity. This makes the method all the duplicate records. Take the following case as an example. Smith, England, 18)} all refer to the student named John Smith. While processing R 4 (John Smith, England), with OBMAB, the similarity of R 1 and R 4 is 0.5. However, if constraint to 0.4, the records as (Jane Smith, 18) can be classified into this cluster.  X  To make OBMAB practical, we proposed Similarity Estimating Duplicate Record Detection (SEDRD). For a record R and a cluster C, the upper and lower bounds of the similarity between R and any record in C can be estimated in O(1) time. These bound processing of R , the computation of the similarity between R and the records in C is restriction of each cluster is relaxed and the recall is improved. 
The contributions of this paper are as follows.  X  The optimal-matching-based methods for dup licate record detection are studied in  X  An estimation method of the range of optimal-matching-based similarity is  X  With the estimated ranges of optimal-matching-based similarity, an optimized  X  Extensive experiments are performed to show that our method outperforms existing 
The rest of this paper is organized as follows: Section 2 introduces some background knowledge. Section 3 presents the duplicat e record detection method which based on Section 5 concludes the whole paper. In this section, we define the problem of detecting duplicate records and introduce the na X ve duplicate records detection method based on optimal bipartite graph matching. 2.1 Duplicate Records Detection duplicate record detection is defined as follows. Definition 1 (Detection of du plicate records on data set R ). Given a original record set R = {R 1 ,R 2 ,...,R m } , detection of duplicate records on is defined as follows. each S i meet the following 3 properties. 2.2 The Na X ve Duplicate Records Detection Method The na X ve OBMAB method involves three key technologies. as follows, where m represents the maximal value of len(A 1 ) and len(A 2 ) . The Similarity of Records. Taking the difference of attributes X  positions, the loss of bipartite graph as follows. A Sim(R 1 , R 2 ) denotes similarity of R 1 and R 2 . m and n represents the number of attributes of R 1 and R 2 , respectively. 
Kuhn  X  Manures algorithm [3,4] (KM algorithm) is a good choice to find the optimal match, so it can be used to co mpute the similarity of records [15]. Detection of Duplicate Records. The main idea is for each R k  X  R , search in the stack to find which cluster R k belongs to. If R k does not belong to any existing cluster, build a new cluster for R k . 
The method uses a stack S to store the records ( signature record ) which represents the existing clusters. Each signature record has a bucket to store th e records belonging to the cluster. Before a record R k is classified to a cluster, it should compare with the signature records (denoted by R s ) in S, from the top to the bottom. If the similarity of R k and R s is no less than required similarity  X  , then R k is compared with the records in the the clusters form a division satisfying Definition 1. Fig.1 shows the method X  X  framework. 
The method has two problems. The one is that it needs to compare a large number of records pairwise. The other is that a strict duplicate records judgment condition results in a low rate of recall. An optimized method is to be proposed in section 3. similarity estimation. At first, the basic idea of our method is presented (Section 3.1). Then, we introduce the method X  X  theoretical basis and underlying assumption (Section estimation (Section 3.3). At last, detailed discussions are given (section 3.4). 3.1 The Main Idea and Framework In order to facilitate the following discussion, the symbols to be referred are listed. l_con , called required bounds and considered as the required upper and lower bounds R respectively, then R k enter the bucket. 3.2 The Method X  X  Theoretical Basis and Underlying Assumption method. The estimating method in this paper is based on the following property. Assumption 1 is another useful property for similarity estimation. R and R 3 are ( A 11 , A 31 ) ( A 12 , A 32 ) ( A 13 , A 33 ). 
Even though Assumption 1 cannot be proven theoretically, it coincides with the 3.3 The Method of Similarity Estimation component, we introduce the method suitable for large data set. The Simple Estimation Method. In this subsection, the simple method to estimate the attributes or records. Sim(A 2 ,A 3 )  X  [1-(d 1 + d 3 ) / m 1-(d 1 -d 3 ) / m ]. For the introduction of the record estimation method, Perfectly Matched Records Pair (PMRP) is defined. Defition 2 (Perfectly Matched Records Pair, PMRP). R 1 and R 2 are two records. If A (R matching A 1i , and for any R 2  X  X  attribute A 2j , there is an A 1i matching A 2j . method discussed above. A (2), Sim(R 2 , R 3 ).ub and Sim(R 2 , R 3 ).lb can be estimated. 
The method works well in the records set containing only PMRPs. But in the original records set, record s are heterogeneous. To deal with this problem, some preparations are required to make sure that each records pair ( R i , R j ) satisfy the PMRP. During the determination of whether a record R k belongs to the cluster C, R k should be R * enter the bucket of R
Rule 1: For each attribute A si of R s , The attributes of R k matching no attribute of A si are ignored.  X  It is easy to see that ( R k * , R s ) is PMRP. In R s .bucket , the records are generated using Rule 1 for computation convenience. They the following descriptions. A ( Sim(A kl ,Match(A kl )).ub and Sim(A kl ,Match(A kl )).lb ) first. Estimating the Similarity Bounds of Attributes. The estimation is based on the following Theorem. is A kl . Then, A .min_len be the maximum and the minimum string length of A kl and all the attributes in Match(A kl ).  X  Sim(A kl ,Match(A si )).lb = Sim(A kl ,A sij ).lb .  X  maintained. The steps of computation are illustrated with an example. A Based on these values, Note that the upper bound may greater than 1, and the lower bound may less than 0. When these happen, the upper bound is set to 1, and the lower bound is set to 0. obtained, where n is the number of attributes in a record. 3.4 Detailed Discussion The Parameters. It is very important to choose the parameters  X  , u_con, l_con when detecting duplication. The chosen of parameters depends on the requirement of precision, efficiency and recall. Parameters with high value will result in high precision but low efficiency and recall. Some techniques of machine learning [16] can be used to choose them. Also, experiences are helpful when choosing them. Other discussion. SEDRD can compensate the two shortcomings of the na X ve method estimates the bounds of the algorithm in O(1) time. A record only needs to compare with the signature records in stack rather th an all the records in the buckets. Second, SEDRD improves the recall. When the required bounds are relaxed, it is not as strict as in the na X ve method for a record to enter a bucket. The recall is improved. However, in general cases, the operation cost is much less, because a record only needs to compare with the signature records in the stack. Especially, for another extreme case with all the records in R belonging to the same cluster, the time complexity of the na X ve method is O(|R| 2 ) while that of our method is O(|R|), because there is always only one record in the stack. 4.1 Experimental Setup Experiments were run on AMD Sempron 3000+ with 512M memory. We do the experiment both on real and virtual data set. 
For real data set, the reco rds comes from the website of ACM and DBLP are used to test the efficiency and effectiveness. 
For the virtual data set, we use the data generator and noises tool to get duplicate difference, 2) the loss of some attributes, and 3) typographic errors. Data generator is used to generate some benchmark record s randomly, and for each benchmark record, some modify operations are used to modify the benchmarks to get duplicate records. 
When generating duplicate records, there are two main parameters: the number of the benchmark records (denoted by N R ), and the number of duplicate records for each benchmark record (denoted by N D ). In order to ensure the accuracy of the experiments, contains 100 benchmark records, and then modify (no more than 50%) the records to the benchmark records is no more than 50%, we set  X  =0.6, which is a little higher than the lowest similarity. 4.2 Experiments on Virtual Data Change the Required Upper and Lower Bounds. In this section, the required upper and lower bounds (u_con and l_con) are changed to test their influence of the algorithm. The record set has 100 benchmark records, for each benchmark record, we generate 100 duplicate records. The result is illustrated in Fig.2 and Fig.3. 
Fig.2 shows the influence caused by the changing of u_con. In this experiment, l_con is set to 0, so it can be ignored. Fig.3 shows the influence caused by the changing of l_con. In this experiment, u_con is set to 0. Since high required bounds need more compare times, it can be seen that the cost of time is increasing with the increase of the required bounds. In Fig.2, the time cost change gently at first, but when u_con is 0.9, the time needed increases sharply. That is because 0.9 is too high that when a record being processing, it is hard to find the suitable bucket. Change the Size of Or iginal Records Set. By changing the size of original records set, we compare the na X ve method and the improved method both on time and recall. The best values of the required bounds are cl osely related with the original record set R. Since there are 100 benchmark records, we can infer that the accurate result is 100 required bounds. As the result of our test in 4.2, we set the required upper bound and lower bound to be 0.7 and 0.1, respectively. We test different size of original records set by changing N D . The results are shown in Fig.4 and Fig.5. 
The time cost of the two methods is shown in Fig.4, we can see that the time cost of the two methods is both increasing when the original records set are getting larger. But the improved method is increasing much slower than the na X ve method. (subsets) to represent the recall rate. The number which is closer to 100 is better. The numbers of clusters are both increasing with th e increase of the size of original records set. But the improved method is increasing much slower than the na X ve one. When the while the na X ve method get 1107 clusters. The recall rate is improved. 4.3 Experiments on Real Data Original data sets with different size are used to test efficiency, adn  X  =0.5, u_con=0.6, l_con=0.5. The result is shown in Fig.6. We put the size of R on X-axis and time cost on Y-axis. 
To test the effectiveness, we use a data set with 50 records, and set  X  =0.7, u_con=0.8, l_con=0.3. Our algorithm gets 45 clusters. That is quite similar with the divide result of human beings, which is 48 clusters. comparisons, but also improves the recall. Theoretical analysis and experimental results show that the method proposed in this paper is correct and effective. 
Our future work includes more accurate estimation of bounds and indices structure for the acceleration of the search in stack. 
