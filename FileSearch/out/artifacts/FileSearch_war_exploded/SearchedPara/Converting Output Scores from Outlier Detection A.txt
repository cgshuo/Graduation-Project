
Current outlier detection schemes typically output a nu-meric score representing the degree to which a given obser-vation is an outlier. We argue that converting the scores into well-calibrated probability estimates is more favorable for several reasons. First, the probability estimates allow us to select the appropriate threshold for declaring outliers using a Bayesian risk model. Second, the probability estimates obtained from individual models can be aggregated to build an ensemble outlier detection framework. In this paper, we present two methods for transforming outlier scores into probabilities. The first approach assumes that the posterior probabilities follow a logistic sigmoid function and learns the parameters of the function from the distribution of out-lier scores. The second approach models the score distri-butions as a mixture of exponential and Gaussian probabil-ity functions and calculates the posterior probabilites via the Bayes X  rule. We evaluated the efficacy of both methods in the context of threshold selection and ensemble outlier detection. We also show that the calibration accuracy im-proves with the aid of some labeled examples.
Outlier detection has been extensively studied for many years, resulting in the development of numerous algorithms [7, 3, 6]. These algorithms often produce a numeric-valued output score to represent the degree to which a given ob-servation is unusual. In this paper, we argue that it is in-sufficient to obtain only the magnitude or rank of outlier score for any given observation. There are many advan-tages to transforming the output scores into well-calibrated probability estimates. First, the probability estimates pro-vide a more systematic approach for selecting the appro-priate threshold for declaring outliers. Instead of requiring the user to choose the threshold in an ad-hoc manner, the Bayesian risk model can be employed, which takes into ac-count the relative cost of misclassifying normal examples as outliers, and vice-versa. Second, the probability estimates also provide a more robust approach for developing an en-semble outlier detection framework than methods based on aggregating the relative rankings of outlier scores [9]. Fi-nally, the probability estimates are useful to determine the uncertainties in outlier prediction.

Obtaining calibrated probability estimates from super-vised classifiers such as support vector machine (SVM), Na  X   X ve Bayes, and decision trees has been the subject of ex-tensive research in recent years [12, 11, 1, 13]. The calibra-tion methods generally fall under two categories: paramet-ric and non-parametric. Parametric methods assume that the probabilities follow certain well-known distributions, whose parameters are to be estimated from the training data. The typical methods used for calibrating classifier X  X  out-puts include logistic regression, asymmetric Laplace distri-bution, and piecewise logistic regression. Non-parametric methods, on the other hand, employ smoothing, binning, and bagging methods to infer probability estimates from the classifier X  X  output scores.

Each of the preceding methods require labeled examples to learn the appropriate calibration function. They are in-applicable to calibrating outlier scores because outlier de-tection is an unsupervised learning task. Therefore a key challenge in this work is handling the missing label prob-lem. Our solution is to treat the missing labels as hidden variables and apply the Expectation-Maximization (EM) al-gorithm [4] to maximize the expected likelihood of the data. We consider two approaches for modeling the data. The first approach models the posterior probability for outlier scores using a sigmoid function while the second approach models the likelihoods for the normal and outlier classes separately.
Our previous work on semi-supervised outlier detection [5] suggests that adding a small number of labeled examples helps to improve the detection rate and false alarm rate of an outlier detection algorithm. In this paper, we further in-vestigate the benefits of semi-supervised outlier detection in the context of improving the probability estimation of out-lier scores by modifying our proposed methods to maximize the joint likelihoods of the labeled and unlabeled data.
In short, our main contributions are summarized below: 1. We develop two calibration methods for transforming 2. We devise a semi-supervised method to further im-3. We illustrate the benefits of converting outlier scores
The rest of the paper is organized as follows. Section 2 describes our calibration method using sigmoid function while Section 3 presents an alternative method using a mix-ture of exponential and Gaussian distributions. Section 4 shows how to extend these methods to incorporate labeled examples. In Section 5, we describe the advantages of us-ing calibrated probabilities for threshold selection and en-semble outlier detection. Experimental results are given in Section 6 while the conclusions are presented in Section 7.
Logistic regression is a widely used method for trans-forming classification outputs into probability estimates. Converting outlier scores, on the other hand, is more chal-lenging because there are no labeled examples available. This section describes our proposed method for learning the labels and model parameters simultaneously using an EM-based algorithm.
Let X = { x 1 ,x 2 ,  X  X  X  ,x N } denote a set of N observa-tions drawn from a d -dimensional space, R d . Suppose the data is generated from two classes: the outlier class O and the normal class M . Let F = { f 1 ,f 2 ,  X  X  X  ,f N } be the corre-sponding outlier scores assigned to each observation in X . Without loss of generality, we assume that the higher f i the more likely x i is an outlier.

Our objective is to estimate the probability that x i is an outlier given its outlier score f i , i.e., p i = P ( O | probability that x i is normal can be computed accordingly by P ( M | f i )=1  X  p i . According to Bayes X  theorem: where
As shown in [2], a i can be considered as a discriminant function that classifies x i into one of the two classes. For a Gaussian distribution with equal covariance matrices, a can be simplified to a linear function: Replacing Equation 3 into 1 yields:
Our task is to learn the parameters of the calibration function, A and B . Let t i be a binary variable whose value is 1 if x i belongs to the outlier class and 0 if it is normal. The probability of observing t i is which corresponds to a Bernoulli distribution. Let T =[ t denote an N -dimensional vector, whose components repre-sent the class labels assigned to the N observations. As-suming that the observations are drawn independently, the likelihood for observing T is then given by Maximizing the likelihood, however, is equivalent to mini-mizing the following negative log likelihood function: LL ( T | F )=  X  Substituting Equation 4 into 7, we obtain: LL ( T | F )=
In supervised classification, since labeled examples { x and learn the parameters of the sigmoid function directly by minimizing the objective function in Equation 8 (see [11] and [13]). Unfortunately, because outlier detection is an unsupervised learning task, we do not know the actual val-ues for t i . To overcome this problem, we propose to treat the t i  X  X  as hidden variables and employ the EM algorithm to simultaneously estimate the missing labels and parame-ters of the calibration function. The learning algorithm is presented in the next section.
The EM algorithm is a widely used method for finding maximum likelihood estimates in the presence of missing data. It utilizes an iterative procedure to produce a sequence of estimated parameter values: {  X  s | s =1 , 2 , ... } . The pro-cedure is divided into two steps. First, the missing label t replaced by its expected value under the current parameter estimate,  X  s . A new parameter estimate is then computed by minimizing the objective function given the current values of T s =[ t s i ] . Table 1 shows a pseudocode of the algorithm. EM algorithm: Input: The set of outlier scores, F = { f 1 ,  X  X  X  ,f N } Output: Model parameters,  X  =( A, B )
Method: 1. s  X  0 . 2. Initialize the parameters to  X  0 3. Loop until algorithm converges
LL ( T | F ) is the negative log likelihood function to be minimized. During the E-step, the model parameters are fixed while LL ( T | F ) is minimized with respect to t i LL ( T | F ) is a linear function of t i , it is minimized by set-ting: During the M step, since T s =[ t s i ] is fixed, minimizing LL ( T | F ) with respect to A and B is a two-parameter op-timization problem, which can be solved using the model-trust algorithm described in [11].
Although fitting posterior probabilities into a sigmoid function is simple, the method makes a strong assumption that the outliers and normal examples have similar forms of outlier score distributions. In this section, we present an alternative method for modeling the outlier scores using a mixture of exponential and Gaussian distributions.
Consider a data set containing outliers that are uniformly distributed and normal observations drawn from a Gaus-sian distribution. We are interested in modeling the dis-tribution of their outlier scores. Suppose the outlier score is computed based on the distance between a data point to its k -th nearest neighbor. Figure 1 shows the typical out-lier score distributions for the outlier and normal classes Observe that the outlier scores for the normal class tend to have an exponential distribution whereas that of the outlier class seems to follow a Gaussian distribution. This result suggests that a mixture model consisting of an exponential and a Gaussian component may fit well to the outlier score distributions.

At first glance, it may seem quite surprising to observe that the scores for the outlier class follow a Gaussian dis-tribution. In the following, we give a theoretical justifica-tion as to why the outliers may indeed have a Gaussian dis-tributed outlier scores.
 Theorem 1 Suppose X and Y are 1-dimensional random variables. If X  X  U (  X   X ,  X  ) and Y  X  N (0 , 1) , then the distances between examples drawn from X and Y follow a Gaussian distribution.
 Proof. Let Z be the random variable for the distance be-tween X and Y , i.e., Z = | X  X  Y | . Then we need to prove that Z follows a Gaussian distribution. We begin with the cumulative distribution function (c.d.f.) for Z : where R is the region defined by | X  X  Y | X  z .If X and Y are independent, their joint probability density function in R is: and zero elsewhere. Therefore The c.d.f. for standard normal is often denoted as  X ( x ) , where So The probability density function for Z is obtained by tak-ing the derivative of its c.d.f with respect to z . Because the derivative of  X ( x ) is a Gaussian distribution and a linear combination of two Gaussian distributions is also a Gaus-sian, therefore, Z must follow a Gaussian distribution.
If the outliers are uniformly distributed and the propor-tion of outliers is considerably smaller than the proportion of normal observations, it is reasonable to assume that the k -th nearest neighbor of an outlier corresponds to a normal observation. Theorem 1 suggests that the distance between a randomly chosen point from a uniform distribution to an-other randomly chosen point from a Gaussian distribution should follow a Gaussian distribution. While such analysis does not conclusively show that the outlier scores must fol-low a Gaussian distribution, it does suggest that modeling the scores of outliers using a Gaussian distribution may be quite a reasonable assumption. Furthermore, our empirical results also seem to support this assumption.

Therefore we will use a mixture of Gaussian and expo-nential distributions for modeling the outlier scores: where  X  ,  X  , and  X  are the parameters of the Gaussian and exponential distributions. The probability of observing t can be written as: where  X  is the prior probability of the outlier component Using Bayes X  rule: The model parameters  X  =(  X ,  X ,  X ,  X  ) are estimated by minimizing the negative log likelihood function: LL ( T | F )=  X 
Given the estimate of the model parameters, the posterior probability of x i being an outlier can be computed using Bayes X  rule:
Similar to the previous method, we use the EM algorithm to minimize the negative log likelihood function given in Equation 14. During the E-step, the expected value for t i where the posterior is calculated from Equation 15.
During the M step, the model parameters are re-computed by solving the following partial derivatives: After some manipulation, this leads to the following update equations for the model parameters:
This section presents a framework for incorporating la-beled examples to improve the calibration accuracy of prob-ability estimates. Let F u and F l be the corresponding sets of outlier scores assigned to the unlabeled data X u and la-beled data X l , respectively. Furthermore, suppose L = l denote the set of labels associated with the labeled data, where l i =1 if x i is an outlier, and 0 otherwise.
In Sections 2 and 3, we have shown that the model pa-rameters are calculated by minimizing the negative log like-lihood. For semi-supervised learning[8], we may decom-pose the negative log likelihood into two parts, each corre-sponding to the labeled and unlabeled data:
Although the EM algorithm is still applicable to learn the model parameters under this framework, some modi-fications are needed. During the E-step, we only need to estimate the values of t i for the unlabeled data using Equa-tion 9. The t i values for the labeled data are fixed, i.e., t = l i (  X  x i  X  X l ) .

During the M-step, it can be shown that the parameters are updated using a combination of the parameter estimates obtained from the unlabeled data and labeled data:  X  where
This section presents two potential applications that may benefit from using probability estimates for outlier scores: threshold selection and ensemble outlier detection.
Most outlier detection algorithms require the user to specify a threshold so that any observation whose outlier score exceeds the threshold will be declared as outliers. A standard approach for determining the threshold is to plot the sorted values of outlier scores and then choose the knee point of the curve as the threshold. Such an ad-hoc method can be imprecise because the location of the knee point is subject to user interpretation.

This section illustrates a more principled approach for threshold selection using the Bayesian risk model, which minimizes the overall risk associated with some cost func-tion. For a two-class problem, the Bayes decision rule for a given observation x is to decide w 1 if: where w 1 and w 2 are the two classes while  X  ij is the cost of misclassifying w j as w i . Since p ( w 2 | x )=1  X  p ( w preceding inequality suggests that the appropriate outlier threshold is automatically determined once the cost func-tions are known. For example, in the case of a zero-one loss function, where: we declare any observation whose estimated posterior prob-ability P ( O | f ) exceeds 0.5 as an outlier.
Recently, Lazarevic et al. [9] have proposed a feature bagging approach for combining outputs from multiple out-lier detection algorithms. There were two approaches inves-tigated in this study: breadth first and cumulative sum .In both of these approaches, the outputs from each detector in the ensemble are sorted and ranked according to decreasing order of their magnitudes. For each observation, the ranks of its outlier scores in the ensemble are aggregated to obtain the final outlier score.

Instead of merging the ranks, we propose to combine their probability estimates. We employ techniques from re-liability theory to perform the probability aggregation. Each outlier detector in the ensemble is considered as a compo-nent in a complex system. The components can be arranged in series, in parallel, or in any combination of series-parallel configurations. Detection of outliers by one of the detectors in the ensemble is analogous to having one of the compo-nents in the system fails. Using this framework, the overall probability that a given observation is an outlier is equiva-lent to the probability that the overall system fails.
Let P ( O | x ) be the posterior probability that x is an out-lier according to the ensemble and P i ( O | x ) be its posterior probability estimated by detector i . For the series configu-ration, at least one of the components must fail in order to make the entire system fails. Therefore:
For the parallel configuration, the system fails only if all the components break down. Analogously, the probability that x is an outlier is:
We have conducted our experiments on several real and synthetic data sets to evaluate the performances of the cal-ibration methods. We have also demonstrated the effec-tiveness of using the probability estimates in the context of threshold selection and ensemble outlier detection.
The data sets used for our experiments are summarized in Table 2. A description of each data set is given below:
Letter: This corresponds to the letter recognition data obtained from the UCI machine learning repository. We choose examples from two classes and designate one of the classes as normal and the other as outlier.

Optical: This is the optical handwritten data set from the UCI machine learning repository. Again, we randomly choose two classes and select data from one class as the normal examples and a small portion of the other class as outliers.

SVMguide1: This data set is used to test the libsvm soft-ware. The data originally comes from an astroparticle ap-plication. One of the two classes is chosen as the outlier class while the other is the normal class.

Cancer: This data set, which is obtained from UCI machine learning repository, records the measurements for breast cancer cases. There are two classes, benign, which is considered as normal and malignant, which is the outlier class.

Lpr: This is one of UNM X  X  benchmark data sets. For each trace generated by a user, an ordered list of the fre-quency counts together with their class label showing  X  X n-trusive X  or  X  X ormal X  is recorded.

Shuttle: We use a subset of the shuttle data set from Stat-log Project Database. We choose two of the large classes as normal and select examples from one of the remaining smaller classes as outliers.

Our calibration methods utilize the outlier scores pro-duced by a distance based outlier detection algorithm. The outlier score is computed based on the distance of each ob-servation to its k -th nearest neighbor [6]. The value of k is set to be 3 to 5 times the number of true outliers depending on the data sets. We choose this method as our underly-ing algorithm because it is easy to implement and is widely used by many authors for comparison purposes.
We have conducted several experiments to demonstrate that our proposed methods help to generate meaningful probability outputs as well as improving the effectiveness of threshold selection and ensemble outlier detection. 6.2.1 Probability Estimation The posterior probabilities not only help us to determine the outlierness of an observation, they also provide estimates of confidence in outlier prediction. This experiment aims to demonstrate that the probability estimates are pushed closer towards 0.5, which indicates a low confidence in prediction,
Figure 2. Plots of probability estimation for synthetic data set with variance = 40 when the outliers are difficult to be distinguished from nor-mal observations.

For this experiment, we use two synthetic data sets gen-erated from a mixture of two distributions. The normal ob-servations are generated from a Gaussian distribution while the outliers are assumed to be uniformly distributed. The Gaussian distributions for both synthetic data sets have the same mean but different variance (40 versus 150). Because of its higher variance, it is harder to distinguish the outliers from normal observations in the second data set compared to the first data set.

Figures 2 and 3 show the calibrated probability estimates for the two data sets. For the first data set, because outliers are well-separated from the normal examples, most of the calibrated values are close to either 0 or 1. For the second data set, because it is harder to distinguish outliers from nor-mal observations, there are more observations with proba-bility estimates around 0.5. Without converting the outlier scores into probabilities, we may not be able to obtain an es-timate of the confidence in outlier detection. The calibration
Figure 3. Plots of probability estimation for synthetic data set with variance = 150 plots for both sigmoid and mixture models look quite simi-lar. However, note that the mixture model approach does not always yield a sigmoid-like curve. The shape of the curve depends on the parameters of the exponential and Gaussian distributions. 6.2.2 Threshold Selection The purpose of this experiment is to compare the effective-ness of using probability estimates for threshold selection. The baseline method for threshold selection is obtained in the following way. For each data set, we first plot the out-lier scores in increasing order of their magnitudes. Figure 4 shows an example of such plot for the SVMguide1 data set. The knee of the curve, which is located somewhere between 0.05 and 0.1, is then chosen as the threshold for declaring outliers. We refer to this method as  X  X nee X  in the remainder of this section.

Tables 3 to 8 show the results of applying different meth-ods for threshold selection.  X  X igmoid X  and  X  X ix X  corre-R 0.9937 0.6582 0.7759 0.7165 0.7848 P 0.4642 1.0000 0.9903 1.0000 0.9810 F 0.6328 0.7939 0.8701 0.8348 0.8720 FA 0.0007 0.0340 0.0226 0.0284 0.0217 R 0.4912 0.7018 0.9123 0.7193 0.9474 P 0.9655 0.9756 0.9123 0.9535 0.9000 F 0.6512 0.8163 0.9123 0.8200 0.9231 FA 0.0483 0.0289 0.0087 0.0273 0.0053 R 0.4903 0.7419 0.7548 0.7419 0.6968 P 0.8636 0.7516 0.7500 0.7516 0.7941 F 0.6255 0.7468 0.7524 0.7468 0.7423
FA 0.0250 0.0129 0.0123 0.0129 0.0151 sponds to the sigmoid and mixture model approaches de-scribed in Section 2.1 and Section 3, respectively.  X  X sig-moid X  and  X  X mix X  are the semi-supervised versions of these algorithms, which utilize some labeled examples to aid the calibration. The labeled examples count for 10% in the data set. We conduct our experiments using the 0-1 loss func-tion, which means that the probability threshold for identi-fying outliers is 0.5.

The following evaluation metrics are used to compare the effectiveness of the threshold selection methods: Pre-cision(P), Recall(R), F-measure(F) and False Alarm rate (FA). All of these metrics are computed from the confusion matrix shown in Table 9. The formula for calculating these metrics are listed in Equation 30.
 R 0.4667 0.9778 0.8222 0.9778 0.8222 P 0.9130 0.6667 0.8222 0.6769 0.8222 F 0.6176 0.7928 0.8222 0.8000 0.8222 FA 0.0515 0.0024 0.0180 0.0024 0.0180 R 0.3564 1.0000 0.4554 1.0000 0.6139 P 1.0000 0.5372 1.0000 0.5372 1.0000 F 0.5255 0.6990 0.6259 0.6990 0.7607 FA 0.0314 0 0.0267 0 0.0191 R 0.4242 0.5152 0.6894 0.5455 0.7045 P 0.7568 0.7010 0.6894 0.7129 0.6889 F 0.5437 0.5939 0.6894 0.6180 0.6966 FA 0.0187 0.0159 0.0103 0.0149 0.0098
A good threshold must balance both precision and re-call, therefore a higher F-measure value is favored. Our results clearly show that the F-measure for all the data sets improved after probability calibration. Thus, convert-ing the outlier scores into probabilities improves thresh-old selection in terms of balancing its precision and recall. The results also show that the addition of labeled examples tends to produce better calibration. More specifically, both  X  X mix X  and  X  X sigmoid X  approaches outperform their unsu-pervised counterparts in five out of six data sets in terms of their F-measure. 6.2.3 Outlier Detection Ensemble This experiment compares the effectiveness of using proba-bility estimates for combining outputs from multiple outlier detectors as opposed to the rank aggregation methods (de-noted as breath and sum) proposed by Lazarevic and Ku-mar [9]. The number of ensembles used in experiments is 10. The receiver-operating characteristic (ROC) curve is often used to show the tradeoff between detection rate and false alarm rate. Alternatively, we may use the area un-der ROC curve (AUC) as our evaluation metric, where the better scheme will have an AUC value closer to 1. From Table 10, it can be observed that combining probability es-timates from multiple outlier detectors yields higher AUC values than combining the ranks of their outlier scores. The semi-supervised approaches ( X  X sigmoid X  and  X  X mix X ) also tend to produce higher AUC values that unsupervised ap-proaches in most of the data sets.

Another interesting observation can be made when com-paring Tables 3 to 8 against Table 10. For threshold se-lection, the  X  X igmoid X  approach tends to outperform the  X  X ix X  approach, whereas for ensemble outlier detection, the  X  X ix X  approach tends to produce higher AUC. One possi-ble explanation is that both applications (threshold selec-tion and ensemble outlier detection) have different require-ments concerning the calibration accuracy. Threshold selec-tion is more concerned with determining whether the poste-rior probability is greater than or less than 0.5 and places less emphasis on how far the estimated probabilities de-viate from their true probabilities. In contrast, the differ-ence between the estimated probability and true probability may affect the effectiveness of the ensemble outlier detec-tion framework. Since the  X  X ix X  approach models the out-lier score distributions separately for the normal and outlier classes, we expect its probability estimates to be more ac-curate. We plan to investigate this issue further as part of our future work. 6.2.4 Reliability Diagram Reliability diagram [10] is a method for visualizing the cali-bration accuracy of a classifier. It can also be used in outlier detection to show the correspondence between outlier score values and their empirical probabilities. For the raw outlier scores, we normalize them to fall within the range of [0,1]. We then discretize the scores into equal with bins(0.1) and compute the fraction of examples in each bin that are true outliers. The latter corresponds to the empirical probability estimate for the corresponding bin. Figure 5(a) shows the reliability diagram for the normalized raw outlier scores. If the outlier scores are in agreement with the estimated proba-bilities, the points should be close to diagonal (dashed) line. This plot shows that the normalized scores obtained directly from distance-based outlier detection algorithms have little to do with posterior probabilities.

After calibration, Figure 5(b) shows that the predicted probabilities are closer to the empirical ones. However the difference is still quite large since the calibration is done without using any labeled information. Finally, Figure 5(c) shows the reliability diagram for semi-supervised calibra-tion. Note that most of the points have moved closer to the diagonal line, which means that the labeled examples help to improve the probability calibration.
In this paper, we study the problem of transforming out-lier scores into probability estimates. Unlike existing cal-ibration algorithms, our proposed methods do not require any labeled examples. In the first approach, we treat the la-bels as hidden variables and fit outlier scores into a sigmoid function. An efficient EM-based algorithm is developed to learn the function parameters. To obtain a more accurate calibration, we propose an alternative approach that mod-els the score distributions using a mixture of Gaussian and exponential distributions. The posterior probability is then calculated using Bayes X  rule. We show that the probabil-ity estimates from outlier scores have many potential appli-cations. We discuss about the use of the probability esti-mates in selecting a more appropriate outlier threshold and in improving the performance of an outlier detection en-semble. Our experimental results suggest that our proposed methods can produce accurate calibration and can be used effectively for threshold selection and outlier detection en-semble. We further demonstrate that the calibration perfor-mance improves with the aid of some labeled examples. [1] P. N. Bennett. Using asymmetric distributions to im-[2] C. M. Bishop. Neural Networks for Pattern Recogni-[3] M. M. Breunig, H.-P. Kriegel, R. T. Ng, and J. Sander. [4] A. P. Dempster, N. M. Laird, and D. B. Rubin. Maxi-[5] J. Gao, H. Cheng, and P.-N. Tan. A novel framework [6] W. Jin, A. K. H. Tung, and J. Han. Mining top-n local [7] E. M. Knorr, R. T. Ng, and V. Tucakov. Distance-[8] T. Lange, M. H. Law, A. K. Jain, and J. Buhmann. [9] A. Lazarevic and V. Kumar. Feature bagging for out-[10] M.H.Degroot and S.E.Fienberg. The comparison and [11] J. C. Platt. Probabilistic outputs for support vector ma-[12] B. Zadrozny and C. Elkan. Obtaining calibrated [13] J. Zhang and Y. Yang. Probabilistic score estimation
