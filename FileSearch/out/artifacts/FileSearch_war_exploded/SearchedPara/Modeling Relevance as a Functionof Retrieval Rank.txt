 A comprehensive set of judged documents derived from human relevance assess-ments is a key component in the successful evaluation of IR systems. However, growing collection sizes make it prohibitively expensive to judge all of the docu-ments that are potentially relevant, and sampling methods such as pooling [ 15 ] are now commonly used to select a subset of documents to be judged. Partial judgments present an interesting challenge in carrying out reliable evaluation, and can result in subtle problems when comparing the quality of two or more systems.
 uments during evaluation. One simple rule  X  and the one often used in practice  X  is to assume that all unjudged documents are non-relevant. Although an evalua-tion score can be obtained using this assumption, any conclusions drawn may be a biased view of a system X  X  relative performance. Two approaches to handling score adjustment [ 7 , 10 , 16 ]. Metric-based solutions can be further categorized as those that ignore the unjudged documents, and work only with the known documents; and those that attempt to infer the total relevance gain achieved by the system, or, at least, to quantify the extent of the uncertainty in the mea-sured scores. Score adjustment approaches require a different type of collection pooling process, which can greatly impact the reusability of the test collection. They also seek to minimize the bias between the pooled and unpooled systems, which is different than the pooling depth bias. Pooling depth bias can occur in contributing systems as well as new systems since using a pooling depth less than the evaluation depth can result in unjudged documents occurring in any system ranking.
 Here we consider traditionally pooled collections, and consider the problem from a fresh angle: does the rank position of a previously unseen document influ-ence the likelihood of it being relevant, and if so, can that relationship be exploited to allow more accurate system scores to be computed? Our estimations of gain based on rank fit well with weighted-precision metrics, and allow both types of bias to be incorporated when performing evaluations. In particular, we measure the aptness of several possible models that build on existing judgments, from which we obtain an observed likelihood of relevance at different ranks. The ben-efit of assessing relevance as a function of rank is that the model can be applied both within the original pooling depth and also beyond it. A further advantage of the proposed approach is that in making the model topic-specific, it automat-ically adapts to differing numbers of relevant documents and to query difficulty, both of which can vary greatly across topics.
 As a specific example of how our techniques might be employed, we consider the rank-biased precision ( RBP )metric[ 9 ], which computes a residual as a quan-tification of the net metric weight associated with the unjudged documents in a ranking. Using an estimator, a value within that identified residual range can also be computed, and given as a proposed  X  X est guess X  score. To demonstrate the validity of our proposal, empirical studies are conducted on two representa-tive TREC datasets: those associated with the 2004 Robust Track; and with the 2006 Terabyte Track. The first collection is believed to be relatively complete [ 13 ], while the second is understood to be less comprehensive [ 8 , 12 ]. The pro-posed models are fitted using topics in the two datasets and compared using a standard goodness-of-fit criterion at different nominal pooling depths. We then explore the predictive power of those models, by comparing extrapolated sys-tem scores generated from shallow-depth pools with the corresponding scores computed using deeper pools. Batch IR evaluations require a set of judgments for each included topic. Pooling [ 15 ] is often used to generate those judgments, but has limitations, since there is no guarantee that all relevant documents for a topic are identified. The usual way of handling that problem during evaluations is to assume that unjudged documents are not relevant. Incomplete judgments have been shown to have lit-tle effect in the NewsWire collections [ 19 ], but the evaluation results in larger web collections can be biased [ 2 ]. As a result, several strategies for dealing with unknown documents have been developed [ 1 , 3 , 5 , 7 , 9  X  11 , 16  X  18 ]. Broadly speak-ing, these strategies can be categorized into two types  X  metrics that deal in some way with the missing judgments, and methods for adjusting the bias. Figure 1 provides a taxonomy of approaches, which we now explore.
 Metrics for Incomplete Judgments. Widely used metrics such as NDCG [ 6 ] were developed on the assumption that the judgments were complete. When they are used with incomplete judgments, unjudged documents are typi-cally assumed to be non-relevant during the calculation process, an assumption that can result in underestimating the effectiveness of a system if it returns many unjudged documents, or overestimating the effectiveness of all systems if there are many undetected relevant documents. Alternative approaches have been proposed that use only the documents which are judged, including con-densed scoring [ 11 , 17 ], and BPref [ 3 ]. Sakai [ 11 ] compared different condensed metrics with BPref and concluded that condensed Q-measure and well in practice, and have a higher discriminative power than metrics using inference [ 17 , 18 ] have also been proposed. For example, estimates the precision at ranks where relevant documents occur, and assumes that relevant documents are distributed uniformly between identified ranks. A drawback is that inferred metrics depend on pools being constructed using a predefined sampling method. A recent study by Voorhees [ 14 ] concluded that a two-strata sampling is a suitable method for constructing collections for inferred metrics.
 by deploying importance sampling when judgment pools are created in order to minimize the likelihood of missing relevant documents. StatAP cision based on a joint distribution derived from the relevance probability of a pair of ranks. The total number of relevant documents is estimated via a uniform sampling process over a depth 100 pool. Combining both estimates produces the final StatAP score. Both InfAP and StatAP have been shown to be highly corre-lated with AP when the judgments are incomplete, using a range of collections [ 17 , 18 ]. However, inferred metrics and StatAP are reliant on specific sampling strategies being followed when pool construction occurs, meaning that applying these methods on unpooled systems may not be appropriate. The final metric-based approach is to provide both the minimum and the maximum effectiveness score for a system, using the notion of a residual that was introduced alongside Rank-Biased Precision ( RBP )[ 9 ]. Instead of generating a point effectiveness score, RBP provides a lower and an upper bound, with the gap between them representing the extent of score uncertainty associated with the unjudged documents. RBP supports traditional score-based system comparisons, and also provides quantitative evidence of the potential impact the unjudged documents may have on that comparison.
 Score Adjustments Based on Estimated Relevance. The alternative is to try and adjust for the bias. The first option is to compensate for system bias  X  the difference between pooled and unpooled systems when using a fixed pool depth  X  using either a metric-based approach [ 7 , 10 , 16 ] or a metric-independent approach [ 5 ]. Based on RBP @10, Webber and Park [ 16 ] propose adjustment methods to deal with the inference from systems and from topics. In separate work, Ravana and Moffat [ 10 ] propose estimation schemes for picking a point within the residual range: a background method; an interpolation method; and a smoothing method that blends the first two. Although Ravana and Moffat primarily focus on system bias, their results also indicate that the same approaches could be applied to adjust the bias resulting from a limited pooling budget.
 Recent work by Lipani et al. [ 7 ] views the problem from another perspective, proposing an  X  X nti-precision X  measure in order to determine when to correct the pooling bias. By using a Monte Carlo method to estimate the adjustment score to be added to a run, Lipani et al. empirically obtain better results than previous work. Lastly, B  X  uttcher et al. [ 5 ] consider the problem independent of the evaluation metric. By transforming bias adjustment into a document classi-fication problem, the relevance of a document can be predicted to minimize rank variance when a leave-one-out experiment is applied.
 Most of this prior work has focused on adjusting the bias between pooled and unpooled systems. When the pooling budget is limited, condensed runs and may be vulnerable to relatively high score variance. Residual-enabled metrics such as RBP at least allow this variance to be quantified, but do not necessarily provide any way of drawing useful conclusions. Sampling methods and inferred metrics may be of some benefit in this regard, but give rise to different issues when systems not contributing to the original pool are to be scored. It is this set of trade-offs that motivates us to revisit the question of system comparisons in the face of a limited pooling budget. We now describe methods for modeling relevance as a function of ranking depth. Gain Models. Consider a weighted-precision metric such as puted as  X  i =1 W ( i )  X  r i , where W ( i ) is the ranking-independent weight attached to the item at rank i according to the metric definition, and r ated with that i th item in the ranking generated for the topic in question. When the judgments are incomplete, and the value r j is not known for one or more ranks j , we propose that an estimated gain  X  r j be used, where  X  r a model of relevance in which topic and retrieval rank j are the inputs. that have contributed to a pooled evaluation to a maximum run length (or evalua-tion depth) of k = d , so that r i,s is the gain attributed to system s by the document it placed at rank i .The empirical gain vector g = g 1 ,g A gain model is a function G ( g ,k ) that generates a value  X  g for g k , the empirical gain at rank k . For example, one simple gain model is to assert that if a document is unjudged its predicted gain is minimal, that is, G 0 ( g ,k )= mingain , where mingain is the lower limit to the gain range and is usually zero. This is the pessimal approach to dealing with unjudged documents that was discussed in Sect. 2 . Similarly, the residuals associated with RBP combine G maxgain is the upper limit to the gain range, and is often (but not necessarily always) one.
 Increasingly Flexible Models. We are interested in gain models that lie between the extremes of G 0 () and G 1 (), and consider five different interpola-tion functions in our evaluation, embodying different assumptions as to how gain varies according to rank. Table 1 lists the five options. The first model listed, G s (), assumes that the gain is static and both topic and rank invariant. For early ranks this is perhaps more realistic than using G itively implausible for large ranks, since the goal of any retrieval system is to bring the relevant documents to the top of the ranking.
 on the assumption that all relevant documents appear in a random manner at the early ranks of each run, and that beyond some cutoff rank m , no further relevance gain occurs. This model is rank-sensitive in a binary sense, and because m is a parameter that is selected in the context of a particular topic, it is also topic-sensitive. That is, the constant model G c adds a level of flexibility to the static G s (), and while it may also be implausible to assert that average gain is a two-valued phenomena determined by rank for any individual topic, in aggregate over a set of topics, each with a fitted value of m , the desired overall behavior might emerge.
 The third step in this evolution is the model G . The constant model G allows an abrupt change in predicted gain as a function of rank, at the topic-dependent cutoff value m . If we add further flexibility and suppose that average relevance gain decreases linearly as ranks increase, rather than abruptly, we get G . This model also has cutoff rank m beyond which the expected gain from an unjudged document is presumed to be zero, given by m = c/ X  option is to allow a tapered decrease, and this is what G Zipfian distribution, in which H n,c is a normalizing constant determined by the controlling parameter c and the ranking length n . The expected gain rate decreases at deeper pooling depths but remains non-zero throughout, due to the long-thin tailed property of the Zipfian distribution.
 Another possibility is that the gain may initially increase or be constant, and then decrease in the longer term. To achieve this option, the monotonicity expectation is relaxed, a possibility captured by the discrete Weibull distribution, model G w . Note that this function allows the possibility of an initial increase, but does not make that mandatory. In particular, when c = 1, the underlying distribution becomes a simple decreasing geometric distribution. Since this model is derived from a discrete Weibull distribution, the gain rate decreases faster than G when the distribution of relevance by rank is similar.
 Given a model G that has been determined in response to a empirical gain vector g , we take  X  r j = G ( g ,j ) for unjudged documents when r and then compute a weighted-precision metric such as RBP in exactly the same manner as before. That is, the estimated gain for that topic is used whenever the actual gain is unknown.
 Measuring Model Fit. With a choice of ways in which relevance might be modeled, an obvious question is how to compare them and identify which ones provide the most accurate matches to actual ranking data. To measure goodness-of-fit we use root-mean-squared-error, or RMSE. That is, given a model G fitted to an empirical gain vector g = g j by choosing values for the controlling parameters (Table 1 ), we compute as an indicator of how well that model and those parameters fit the underlying distribution. Small values of this measure  X  ideally, close to zero  X  will indicate that the corresponding model is a good estimator of the underlying observed behavior.
 Measuring Model Predictive Power. A second important attribute of any model is its ability to be predictive over unseen data, that is, its ability to be used as a basis for extrapolation. In particular, we wish to know if a model fitted to an empirical gain vector computed using judgments to some depth d (training data) can then be used to predict system scores in an evaluation to some greater depth d&gt;d . Figure 2 illustrates this notion. Suppose that pooled relevance judgments to depth d = 10 are available. If a weighted-precision metric such as at an evaluation depth k = 10, all required judgments are available, but even so, there is a still a non-zero score range, or residual. That d = 10 score range is illustrated in Fig. 2 by the solid lines, plotted as a function of k , the evaluation depth. Note that as the evaluation depth k is increased beyond 10 there is still some convergence in the metric, because documents beyond depth d =10in this system X  X  run might have appeared in the top-10 for some other system, and thus have judgments. The endpoints of those lines, at an evaluation depth of k = 100, are marked LB and UB. The dotted lines in the figure show the bounds on the score range that would arise if evaluation to k was supported by pooling to d = 100. The final d = 100 LB-UB range  X  a subset of the wider d =10 LB-UB range  X  is still non-empty, because the residual at depth k accounts for all documents beyond depth k , even if full or partial judgments beyond that depth are available.
 from a pooling process to depth d . If the model has strong predictive power, then the extended-evaluation using the predicted  X  r j values should give rise to a metric score that falls close to  X  or even within  X  the dotted-line LB-UB range that would have been computed using the deeper d = 100 judgment pool. That is, a metric score based on a predictive extrapolation will give rise to one of the three situations shown within the dotted circle: it will either overshoot the d = 100 range by an amount A ; or it will undershoot the d = 100 range by an amount C ; or it will fall within that range, as suggested by the point labeled B. In the latter case, we take B =0.
 The overall process followed is that for each topic we use the set of system runs for that topic, together with the depth-d pooled judgments, and compute the parameters for an estimated gain function. We then use that gain function to extrapolate the depth-d metric scores for that topic for each system, using  X  r values generated by the model in place of r j values whenever the corresponding document does not appear within the depth-d pool. So, for each combination of topic and system an difference is computed relative to the score range generated by a pooled-to-d evaluation. Test Collections. We employ two different test collections, the 2004 Robust task (Rob04, topics 651 X 700) and the Terabyte06 task (TB06, topics 801 X 850), considering only the runs that contributed to the judgment pool. The first dataset has a pooling depth of d = 100 and a set of 42 contributing runs [ 13 ]; the second a pooling depth of d = 50, and 39 contributing runs [ 4 ]. Figure 3 provides a breakdown of document relevance in the two collections. Although the TB06 dataset uses shallower pooling, on average it contains more relevant documents per topic than Rob04 (left pane); and the percentage of relevant documents decreases more slowly as a function of pool depth (right pane). For example, approximately 8% of the TB06 documents that first enter the pool as it is extended from d =40to d = 50 are found to be relevant.
 Goodness-of-Fit Evaluation. Regression was used to compute the two or three parameters for each model (Table 1 ), fitting them on a per-topic-basis, and using a range of nominal pooling depths d . In the static model G predicted gain was set to 0 . 5 at all ranks; and in the constant model G cutoff parameter m was capped at the pooling depth. All of the judgments to the specified test depth d were used, in order to gauge the suitability of the various models. Note that the large volume of input data used per topic and the small number of parameters being determined means that there is only modest risk of over-fitting, even when d is small. Predictive experiments that bypass even this low risk are described shortly.
 pooling depth d . The two columns labeled G H () are discussed shortly. Two-tail paired t -tests over topics were used to compare the RMSE values associated with the five models. When all available judged documents are used, G the smallest RMSE on both datasets compared to the other four models, at a significance level p  X  0 . 05 in all cases, and is a demonstrably better fit to the observed data than are the other four approaches.
 RMSE over the available fitting data for the five primary approaches on a topic-by-topic basis. Two-tail paired t -tests were also conducted between model G and each of the others, and in Table 2 superscript daggers indicate the RMSE measurements that were not found to be significantly inferior to the hybrid approach, again using p  X  0 . 05. The Weibull model is a very close match to the hybrid approach, and of the per-topic selections embedded in the hybrid, the Weibull was preferred around 85% of the time.
 harder to fit a curve to, with overall higher RMSE values for each corresponding depth and model compared to the TB06 judgments. It is also apparent that little separates the Zipfian G z () and linear G () approaches, and that either could be used as a second-choice to the Weibull mechanism. Finally in connection with Table 2 , the consistency of values down each column as data points are added confirms the earlier claim that there is only a modest risk of over-fitting affecting the results of this experiment.
 imation of the empirical gain, which is shown in the graphs as a sequence of black dots. One topic from each of the two datasets is plotted, with two differ-ent pooling depths  X  one graph in each vertical pair using all of the available judgments ( d = 100 for Rob04, and d = 50 for TB06, in the top row), and one graph showing the models that were fitted when pooling was reduced to a nom-inal d = 10 (bottom row). One observation is immediately apparent, and that is that empirical gain does indeed decrease with rank; moreover, in the case of TB06 Topic 819, it does so surprisingly smoothly. Also worth noting is that the empirical gain for the Rob04 topic decreases more quickly than it does for the TB06 topic as the evaluation depth k increases, which both fits with the overall data plotted in the right pane of Fig. 3 , and helps explain the better TB06 scores for the static model in Table 2 . Comparing the top two graphs with the lower two, it is clear that the more volatile nature of the empirical gain in the Rob04 topic has meant that when only d = 10 judgments are available, the models all diverge markedly from the actual g k values when they are extrapolated beyond the fitted range. The smoother nature of the TB06 empirical gain function means that the extrapolated models based on d = 10 continue to provide reasonable projections.
 Predictive Strength Evaluation. The most important test of the various models is whether they can be used to generate reliable estimates of metric scores when extrapolated beyond the pooling depth, the process that was illus-trated in Fig. 2 . Table 3 lists the results of such an experiment, using throughout, a relatively deep metric (at an evaluation depth of 50, the inherent RBP 0.95 tail-residual is 0 . 07, and at an evaluation depth of 100, it is 0 . 006), and with G s () omitted for brevity. To generate each of the table X  X  entries, a pool to depth d is constructed, and the corresponding model fitted to the empirical gain values associated with that pool. Each run is then evaluated to depth k = 100 (Rob04) or k = 50 (TB06) using pooled-to-d judgments, if they are available, or using estimated gain values  X  r j generated by the model for that topic. The RBP score estimate that results is then compared to the score and residual range generated using the full pool, d = 100 for Rob04 and d = 50 for TB06. If the extrapolated RBP score falls within that pooled-to-d range, an of zero is regis-tered for that system-topic combination; if it falls outside the range, a non-zero is registered, as described in Sect. 3 . Each value in the table is then the average over systems of the root-mean-square of that system X  X  topic  X  X ; with the paren-thesized number beside it recording the percentage of the values that are zero, corresponding to predictions that fell within the final RBP measured the  X  X nterpolative X  method of estimating a final described Ravana and Moffat [ 10 ], denoted as  X  X M X  in the table. It predicts scores assuming that the residual can be assigned a gain at the same weighted rate as is indicated by the judged documents for that run.
 sufficient initial observations are available that it is appropriate to extrapolate. Also interesting in Table 3 is that the linear model, G (), provides score predic-tions that are as reliable as those of the Weibull model. As a broad guidance, based on Table 3 , we would suggest that if an evaluation is to be carried out to depth k , then pooled judgments to depth d  X  k/ 2 are desirable, and that application of either the Weibull model G w () or the simpler linear model G to infer any missing gain values between d and k will lead to reliable final score outcomes. Both outperformed the previous RM approach [ 10 ].
 That then leaves the choice of k , the evaluation depth to be used; as noted by Moffat and Zobel [ 9 ], k is in part determined by the properties of the user model that is embedded in the metric. In the RBP model used in Table 3 ,the persistence parameter p =0 . 95 indicates a deep evaluation. When p is smaller and the user is considered to be less patient, the fact that the tail residual is given by p k means that smaller values of k can be adopted to yield that same level of tail residual. Note that it is not possible to analyze hence our reliance on RBP in these experiments. We have investigated a range of options for modeling the relationships between relevance and retrieval rank, calculating the probability of a document being relevant conditioned on a set of systems and the evaluation depths. Our exper-iments show that it is possible to use the models to estimate final scores in weighted-precision metrics with a reasonable degree of accuracy, and hence that pooling costs might be usefully reduced for this type of metric. To date the pre-dictive score models have not been conditioned on the document itself, and the fact that it might be unjudged in multiple runs at different depths. We plan to extend this work to incorporate the latter, hoping to develop more refined esti-mation techniques. We also plan to explore the implications of stratified pooling, whereby only a subset of documents within the pool depth are judged.
