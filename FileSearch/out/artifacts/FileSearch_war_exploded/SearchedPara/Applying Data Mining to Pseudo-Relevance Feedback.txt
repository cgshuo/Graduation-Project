
In this paper, we investigate the use of data mining, in particular the text classification and co-training techniques, to identify more relevant passages based on a small set of labeled passages obtained from the blind feedback of a re-trieval system. The data mining results are used to expand query terms and to re-estimate some of the parameters used in a probabilistic weighting function. We evaluate the data mining based feedback method on the TREC HARD data set. The results show that data mining can be successfully applied to improve the text retrieval performance. We report our experimental findings in detail.
Text classification is the problem of automatically as-signing text documents to pre-defined categories. Typically, text classification systems learn models of categories using a large training corpus of labeled data in order to classify new examples. It is well-known, both theoretically [30] and practically [9, 19], that the more labeled training data we have, the more accurate a classification system we get. However, labeled documents are difficult and expensive to obtain, whilst unlabeled documents are plentiful and easy to obtain. How to reduce the demands for labeling data and how to leverage unlabeled data were identified as one of near-term challenges for IR in a recent workshop on Chal-lenges in IR [2].

Co-training is an effective method for utilizing the unla-beled data for classification. In this method, the input data is used to create two predictors that are complementary to each other. Each predictor is then used to classify unlabeled data which is used to train new corresponding complemen-tary predictors. Typically, the complementarity is achieved either through two redundant views of the same data [5] or through different supervised learning algorithms [12].
Pseudo-relevance feedback (PRF), also known as blind feedback, is a technique commonly used to improve re-trieval performance [21, 22, 32]. Its basic idea is to extract expansion terms from the top-ranked documents to formu-late a new query for a second round retrieval. Through a query expansion, some relevant documents missed in the initial round can then be retrieved to improve the over-all performance. The effect of pseudo-relevance feedback strongly relies on the quality of selected expansion terms from the top-ranked documents. If the words added to the original query are related to the topic, the quality of the re-trieval is likely to be improved.

The main focus of this paper is on the use of the data mining technique to boost the performance of pseudo-relevance feedback. In particular, we incorporate a co-training algorithm or a single text classification algorithm without co-training into a retrieval system at the feedback stage. After the initial search, we create a training data set by choosing the top k passages 1 from the ranked list as pos-itive examples and another k passages from the bottom of the list as negative examples. The remaining passages are treated as the test set. Then, we classify the examples in the test set and select the most confident examples to add into the labeled training data. If a co-training algorithm is used, this process will be done for a few iterations. The positive examples in the resulting labeled set are used as relevant passages to feed back into the retrieval system.
The remainder of the paper is organized as follows. Sec-tion 2 outlines the related work. In section 3, we present our search system and show how the data mining technique is integrated into the search system. Section 4 presents our data mining based feedback method in detail. We describe our experiments and results in sections 5, 6 and 7. Finally, we conclude the paper in section 8.
Many methods have been proposed to improve the per-formance of relevance feedback. Mitra et al [22] investi-gated several ways to improve query expansion by refining the set of documents used in feedback. Fan [11] combined ranking function tuning and blind feedback to improve re-trieval performance. Yu [34] proposed a vision-based page segmentation algorithm to assist the selection of query ex-pansion terms in pseudo relevance feedback. Iwayama [18] clustered the initial retrieval result and chose the top N doc-uments from each cluster to feed back into the system. Su [29] presented a new relevance feedback algorithm based on Bayesian classifiers in image retrieval. Yan [33] augmented the retrieval performance by incorporating classification al-gorithms into PRF and using negative pseudo feedback in video retrieval. He [13] proposed iterative probabilistic one-class SVMs to re-rank the retrieved images in Web image retrieval.

Unlabeled data have been shown to be useful for re-ducing error rates in text classification [5, 12, 19, 23, 35]. Co-training is used mostly in text classification. It was originally proposed by Blum and Mitchell [5] and used to boost the performance of a learning algorithm when there is only a small set of labeled examples available. Blum and Mitchell [5] show that under the assumption that there are two views of an example that are redundant but not com-pletely correlated, unlabeled data can be used to augment labeled data. Craven uses co-training to extract knowledge from the World Wide Web [10]. The result shows that it can reduce the classification error by a factor of two using only unlabeled data. Kiritchnko and Matwin applied co-training in email classification [20]. In their study, they show the performance of co-training depends on the learning method it uses.

There are a number of other studies that explore the po-tentials of co-training in recent years. Some modified ver-sions of the co-training algorithm have been proposed since the original one. In [12], Goldman and Zhou use two dif-ferent supervised learning algorithms to label data for each other. Raskuttii [26] presents a new co-training strategy which uses two feature sets. One classifier is trained using data from original feature space, while the other is trained with new features that are derived by clustering both the la-beled and unlabeled data.

All the above works show that the co-training algorithm can be used to boost the performance of a learning algo-rithm when there is only a relatively small set of labeled examples given. However, this idea is based on the assump-tion that there are two natural features in the dataset, but a document may not contain two different feature sets. Chan, Koprinska and Poon [8] suggest to randomly split one sin-gle feature set into two feature sets for the co-training al-gorithm. Their experiment results show that a random split of the feature set leads to comparable, and sometimes, even better results than using the natural feature sets. In this paper, we investigate the effect of integrating Chan et al  X  X  version of co-training vs. single classifica-tion algorithms without co-training into our search system. The co-training or classification algorithm is used after we obtain the first round result to expand query terms and re-estimate some parameters in the retrieval function.
In this section, we briefly introduce our search system and show how data mining techniques are incorporated into the system architecture. Figure 1 depicts the overall struc-ture of our search system. We use Okapi BSS [3] as the un-derlying retrieval system, based on which we develop our own index models, query processing functions and feed-back modules [17].

The system takes a query topic in the format provided by TREC [1]. Each topic is associated with some meta-data, which include the following fields: Genre, Geography, Granularity, Familiarity, Subject and Related Text 2 . The system also takes two sets of clarification forms (CF) as feedback from users. The topic processing module loads these topic files, extracts relevant fields and converts them into the format acceptable by the term processing module. The term processing module extracts terms from the pro-cessed topic in full text documents and conducts the follow-ing two tasks. The first task is to extract statistical informa-tion of each query term both within the topic and in the full database via Okapi BSS, and then select important terms to query Okapi BSS. The second task is to handle the full text documents provided by the data mining feedback module, expand query terms and update statistics for the existing and new terms.

We build both document-level and passage-level indexes on the database. After receiving a query from the term-processing module, Okapi BSS returns two lists of results. One is passage-based and the other is document-based. The result processing module takes both lists, re-weight the pas-sages based on the two lists and generates a new ranking list of passages 3 . The first k passages are then passed as blind feedback to the data mining feedback module. After the second round of retrieval, the result processing module returns the final result to the user.

The data mining feedback model takes the blind feed-back passages from the result processing module, applies classification algorithms with or without co-training to se-lect more relevant passages that are not in the blind feed-back, and passes the new expanded set of feedback passages back to the term processing module. The term processing module then expands query terms, updates statistics for the terms, and issues a new query to Okapi BSS.
 Next, we describe the weighting formula used in Okapi BSS. Some of the parameters in the weighting formula will be re-estimated according to the data mining feedback. We then explain how we update the passage-level ranking list according to both document and passage-level retrieval re-sults.
Our underlying search engine, Okapi, is based on the probability model of Robertson and Sparck Jones [27]. The retrieved documents are ranked in the order of their proba-bilities of relevance to the query. A query term is assigned a weight based on its within-document term frequency and within-query frequency. The weighting function used in Okapi is BM25, shown as follows: where w is the weight of a query term, N is the number of indexed documents in the collection, n is the number of documents containing the term, R is the number of doc-uments known to be relevant to a specific topic, r is the number of relevant documents containing the term, tf is within-document term frequency, qtf is within-query term frequency, dl is the length of the document, avdl is the aver-age document length, nq is the number of query terms, the k s are tuning constants (which depend on the database and possibly on the nature of the queries and are empirically de-termined), K equals to k 1  X  ((1  X  b )+ b  X  dl/avdl ) , and dicates that its following component is added only once per document, rather than for each term. In our experiments, the values of k 1 , k 2 , k 3 and b in the BM25 function are set to be 1.2, 0, 8 and 0.75 respectively. This parameter set-ting has been tested on many data sets in different domains, which results in good performance [3, 14, 15, 16, 17, 36].
For each query term, R, the number of documents known to be relevant for a specific topic, and r, the number of rel-evant documents containing the term, are initially set to 0, since the relevant document information is unknown when a query is initially submitted. After the feedback is per-formed, the value for R and r can be updated accordingly. Thus, in the next round of retrieval, BM25 can use the newly updated values of R and r to weight terms, which may re-sult in a better list of retrieved documents.
In order to obtain more accurate retrieval for passage level inquires, both document level and passage level in-dexes are built. For each query, we search against both the document level index and the passage level index, and use a merge function to update the weights for passages by com-bining the results from both indexes. If the passages found in a document are not adjacent, we use the following merge function to assign a new weight to each of these passages: where W pnew is the new weight of the passage, W p is the weight of the passage we obtain from the passage level in-dex, W d is the weight of the document containing the pas-sage, obtained from the document level index, | P | is the total number of passages retrieved from this document, and h 1 is a coefficient, which is set to be 3 in our experiments
If there are several adjacent passages found in a docu-ment, we merge these passages into one and use the fol-lowing function to assign the weight to the newly merged passage: where W pnew is the weight of the newly merged passage, W p 1 is the weight of the first of these adjacent passages from the passage level index, W d is the weight of the doc-ument we get from the document level index, | P | is the to-tal number of passages retrieved from this document. W p k is the weight of the kth of these n adjacent passages, and h 1 is a coefficient, which is set to be 3 in our experiments. In [14], we show that the dual index strategy can achieve a very good performance for retrieval at both passage and document levels.
Using the search system described above, we incorpo-rate the data mining technique into the feedback stage of the search. The feedback function provided by Okapi BSS is blind feedback. In blind feedback, the top k documents returned from the initial search are assumed to be relevant. Based on these k  X  X elevant X  documents, new query terms may be identified and the weight for each query term can be adjusted by updating the values of R and r used in Equation (2). Ideally, if these k documents are truly relevant, a bet-ter set of query terms can be formulated and more accurate estimations of R and r can be made, which leads to more accurate weighting of query terms. Thus, a better search result can be generated. Therefore, the choice of the k doc-uments is crucial to the feedback process.

The objective of using data mining in the feedback stage is to improve the quality and quantity of the returned rele-vant documents so that query terms can be better expanded and R and r can be better estimated. The steps of using data mining are as follows. First, we assume the top k (such as k =5) documents in the initial search result are relevant. These k documents are put into the initial training set for the data mining module and labeled as relevant documents, i.e., positive examples. We then choose another k docu-ments from the end of the initial search result, put them into the initial training data and label them as non-relevant doc-uments, i.e., negative examples. In this way, we obtain a small set of labeled training data. Then, we apply a data mining algorithm (such as a single classification algorithm or a co-training algorithm) to extend the labeled data by classifying some of the unlabeled data. When this process stops, an extended set of relevant documents is generated and used as feedback documents to expand the query and to re-estimate parameters R and r . The number of relevant documents in the extended training data is the new value for R and the number of such documents containing a query term is the new value for r for that term.

In order to apply classification and co-training algo-rithms, we need to represent a document using features. Next, we present the feature selection method that we use and our the co-training algorithm in detail.
Extracting features to represent a document is a chal-lenging problem. The most common method for represent-ing a document is to select a  X  X ag of words X  as attributes and use the occurrence frequencies of the words as attribute values to represent the document. We use a bag-of-words method to represent documents. However, instead of using occurrence frequencies as attribute values, we exploit a for-mula proposed in [4] to calculate a weighted frequency for each word and use it as the attribute value for the word. In addition, we propose a word selection method that selects the bag of words based on the query terms and the initial retrieval result from Okapi. 4.1.1 Word Selection The bag of words are selected from two sets. One, denoted by W d , is the set of the words that appear in the documents retrieved by Okapi in the first round. The words in this set are ranked by their occurrence counts in the whole docu-ment set. The other set, denoted as W q , is the set of words in the query. A word in W q is associated with a weight calculated by multiplying its within-query frequency by its BM25 weight returned by Okapi in the initial search. We choose a maximum of M axN words from these two sets to represent a document, and use a bias, b ( 0  X  b  X  1 ), to control the proportion of words chosen from W q . Let | W represent the number of words in W q . The algorithm for selecting the bag of words is described in Figure 2. The unique features of this word selection method include that the initial search result is utilized and that query terms and their weights returned by the search engine are considered because they are important in distinguishing between rele-vant and non-relevant documents. In our experiments, b is set to 1, meaning that all the words in W q are selected. The effect of choosing a value for M axN will be discussed in the experiment section. 4.1.2 Word Representation The selected words are used as attributes to represent a doc-ument. The following formula adopted from [4] is used to compute an attribute value to represent a word i in a docu-ment j : where v i,j is the value for word i and document j ; c i,j the number of times i occurs in document j ; n j is the total number of words present in document j ; i is a normalized entropy of word i in the whole document set.

Let t i = j c i,j , the total number of times word i occurs in the whole document set, and N be the total number of the documents, i is expressed as : By definition, 0  X  i  X  1, where the equalities hold if and only if c i,j = t i and c i,j = t i / N, respectively. A value of close to 1 indicates the word is distributed across many doc-uments throughout the corpus, while a value close to zero means that it only shows in a few documents. Therefore, 1 -i is a measure of selectivity of the word i .

The effect of 1 -i in Equation 5 is to reflect the fact that two words appearing with the same count in document j do not necessarily convey the same information about the document. It considers the distribution of the word in the whole document set.
Having represented documents using features, we are now in the position to describe the co-training algorithm. The algorithm takes a set L of labeled documents. Half of the documents in L are the top ranking documents in the initial search result and are labeled as relevant. The other half of the documents in L are taken from the end of the ranked list and are labeled as non-relevant documents. The remaining documents in the ranked list are considered as a set U of unlabeled documents. The co-training algorithm iteratively labels some of the unlabeled documents from U . The algorithm is described in Figure3.

The two classifiers in the co-training algorithm learn from L based on different sets of features using to a learn-ing algorithm. Each classifier supplies new training exam-ples to the other by classifying examples in the unlabeled set. The intuition for this collaborative training to work bet-ter than using a single classifier is that if one classifier can find an unlabeled example that is very similar to some of the labeled ones, then it can confidently predict the class of this example and provide one more training example for the other classifier [20]. Because the second classifier uses a different set of features, it may not yield the same labels to the new training examples as done by the first classifier. Therefore, the new example can provide useful information to improve the second classifier. In [5], it is proved that if the two feature sets are conditionally independent given the class and the target classification function is learnable, then any classifier can be boosted to arbitrarily high accu-racy using unlabeled examples. Here, we choose the two feature sets by randomly splitting a single feature set. This strategy is chosen based on the findings by Chan et al [8]. They found that a random split of a single feature set leads to comparable and, sometimes, even better results than us-ing two natural independent feature sets. In the experiment conducted by Blum and Mitchell [5] for classifying univer-sities X  Web pages, p and n are set 1 and 3. That is, in each iteration, each classifier is allowed to add 1 new positive and 3 new negative examples to L . In our experiments, we use similar settings for p and n and change the values for other parameters, such as the number of iterations and the learn-ing algorithm used. We will discuss the effect of parameter settings in the experiment section.

In our studies, we choose three different learning algo-rithms to test co-training, namely Naive Bayes, decision trees and support vector machines. The results of these three algorithms will be compared. 4.3.1 Naive Bayesian Classifier The Naive Bayesian classifier is based on Bayes theorem. Suppose that there are m classes, C 1 , C 2 ,, C m . The classi-fier predicts an unseen example X as belonging to the class having the highest posterior probability conditioned on X . In other words, X is assigned to class C i if and only if P ( C i | X ) &gt;P ( C j | X ) for 1  X  j  X  m and j = i .By Bayes theorem, we have As P ( X ) is constant for all classes, only P ( X | C i ) needs to be maximized. Given a set of training data, P ( C can be estimated by counting how often each class occurs in the training data. To reduce the computational expense in estimating P ( X | C i ) for all possible X s, the classifier makes a naive assumption that the attributes used in describ-ing X are conditionally independent of each other given the class of X . Thus, given the attribute values ( x 1 ,x 2 , ..., x that describe X ,wehave The probabilities P ( x 1 | C i ) , P ( x 2 | C i ) , ..., P estimated from the training data. 4.3.2 Decision Tree A decision tree is a tree structured prediction model where each internal node denotes a test on an attribute, each out-going branch represents an outcome of the test, and each leaf node is labeled with a class or class distribution. A typical decision tree learning algorithm adopts a top-down recursive divide-and-conquer strategy to construct a deci-sion tree. Starting from a root node representing the whole training data, the data is split into two or more subsets based on the values of an attribute chosen according to a splitting criterion. For each subset a child node is created and the subset is associated with the child. The process is then sep-arately repeated on the data in each of the child nodes, and so on, until a termination criterion is satisfied.
The most crucial step in decision tree construction for classification is the selection of the attribute on which to branch. An attribute evaluation function can be used for selecting the best attribute to place at a particular node. The evaluation function typically measures the reduction in class impurity if the attribute were selected for branching. The attribute that produces the lowest impurity partition is selected for branching.

Many decision tree learning algorithms exist. They dif-fer mainly in attribute-selection criteria, termination criteria and post-pruning strategies 5 . Representative decision algo-rithms include CART [6] and C4.5 [25]. In our study, we use the implementation of the C4.5 decision tree learning algorithm [25] provided in Weka [31]. 4.3.3 Support Vector Machine The support vector machine (SVM) is widely used in text classification in recent years. Its underlying principle is structure risk minimization. Its objective is to determine a classifier or regression function which minimizes the em-pirical risk (that is, the training set error) and the confidence interval (which corresponds to the generalization or test set error). Given a training data, an SVM determines a hyper-plane in the space of possible inputs. This hyperplane will attempt to separate the positives from the negatives by max-imizing the distance between the nearest positive examples and and negative examples.

In a two-class classification problem, assume that there is a collection of n training instances Tr = { x i ,y i } x i  X  X  N and y i  X  X  X  1 , 1 } for i =1 ,...,n . Suppose that we can find some hyperplane which linearly separates the positive from negative examples in a feature space. The points x belonging to the hyperplane must satisfy where w is normal to the hyperplane and b is the intercept. To achieve this, given a kernel function K , a linear SVM searches for Lagrange multiplier  X  i ( i =1 , ..., n ) in La-grangian such that the margin between two classes, denoted with || , is maximized in the feature space [7]. In addition, in the  X  i optimizing process, Karush Kuhn Tucker (KKT) conditions which require must be satisfied. To predict the class label for a new case x , we need to compute the sign of If the sign function is greater than zero, x belongs to the positive class, and the negative otherwise.
 In the case of non-separable data, 1-norm soft-margin SVMs minimize the Lagrangian where  X  i , i =1 ,...,n are positive slack variables, C is a parameter selected by the user with a larger C indicating a higher penalty to errors, and  X  i are Lagrange multipliers introduced to enforce  X  i being positive. Similarly, corre-sponding KKT conditions have to be met for the purpose of optimization. There are several ways to train SVMs. One particularly simple and fast method is Sequential Minimal Optimization (SMO) [24], and we adopt this approach in our study.
We conducted a series of experiments on applying data mining to pseudo-relevance feedback. First, we evaluated the use of single classification algorithms (including naive Bayes, decision tree learning and support vector machines) without co-training. Implementations of those algorithms came from the standard Weka package [31]. In particu-lar, we adopted J48, which accomplished the C4.5 algo-rithm for decision tree mining [25], in our experiments. We utilized the approach of SMO to train SVMs, and em-ployed Gaussian RBF kernels of the form K ( x i ,x j )= exp(  X   X  | x best values of  X  and C , a 10-fold cross validation is con-ducted on each training dataset. Second, we evaluated the co-training method with different settings of parameters. Third, we compared our data mining based method with two well-known feedback methods reported in [22, 28, 32]. Before presenting the experimental results, we first describe the data set and the performance measure used in the exper-iments. The test collection we use is from the HARD track in TREC 2004 [1]. HARD (High Accuracy Retrieval from Documents) is an information retrieval project, the goal of which is to retrieve highly accurate answers to queries by leveraging additional information about the searcher and/or the search context. This goal is achieved by us-ing techniques like passage retrieval and targeted interac-tion. The 2004 HARD corpus is around 1.6G. It contains one year (2003) of newswire data from eight sources: AFE (Agence France Presse -English), APE (Associated Press Newswire), CNE (Central News Agency Taiwan), LAT (Los Angeles Times), NYT (New York Times), SLN (Sa-lon.com), UME (Ummah Press -English)), and XIE (Xin-hua News Agency -English). In total, 635,650 documents and 9,279,957 passages have been indexed in our experi-ments from the test corpus. 50 evaluation topics were cre-ated for the 2004 HARD track. 25 of them are passage based topics and the other are document based topics. In the experiments, we evaluate our method only on the pas-sage retrieval topics since they require higher accuracy and are more sensitive to the relevance feedback. We use 23 of the 25 passage based topics because there are no relevant documents in the database for the other 2 topics according to the official TREC evaluation results. Three character-based measures are used in the 2004 HARD track: Bpref (Binary PREFerence relations) at X characters retrieved, precision at X characters retrieved, and character-based R-precision. Scores for three different X values (6000, 12000, and 24000) are reported by LDC 6 . The assumption made by the HARD track in TREC 2004 [1] was that 12000 characters is roughly equivalent to a 100-word passage, so these values are roughly equivalent to evaluating after 5, 10, or 20 (100-word) passages.
The Bpref score for the topic is the average of the scores of its relevant documents. The score for each relevant doc-ument is the percentage of non-relevant documents (among the top R non-relevant) that it ranks better than. Bpref tracks closely to MAP (Mean Average Precision) with complete judgments, but degrades much more gracefully than MAP as judgments become more incomplete.

In this paper, we only report the performance accuracy in Bpref, which is also recommended by the 2004 HARD track as the primary passage retrieval measure [1]. All the relevance judgments are provided by LDC. Statistical eval-uation on the passage level was done by the script released from the University of Massachusetts and the script from the TREC 2003 evaluation program.
In [22], Mitra et al. proposed different ways to refine the set of documents used in feedback. One of the automatic ways is using term correlation information. The equation used to compute a new score for each retrieved document is as follows: where Sim new D is the new similarity score, idf ( t j ) is the inverse document frequency of term t i if it occurs in doc-ument D , and is 0 otherwise. P ( t i | t j ) is estimated based on word occurrences in a large set S of documents initially retrieved for a query and is given by
Based on the new similarity scores, the retrieved doc-uments are re-ranked and used in the feedback process to expand the query. Several experiments were conducted in [22]. The results showed that this approach is competitive with the best manual approach. This approach was also compared with Xu X  X  local context analysis [32], and it per-formed better. We will compare Mitra X  X  results with our data mining based results in Section 6.
Rocchio proposed a relevance feedback algorithm back in 1971 [28]. It is considered as the standard relevance feed-back, and one of the most popular algorithms. The basic idea is that users are asked to evaluate the resulting docu-ments as relevant or non-relevant, and give feedback to the system. Based on the feedback, the system formulates a new query. The equation for computing a weight vector of a new query is illustrated as follows: where Q 0 and Q 1 represent the initial and first iteration query vectors, D i represents document weight vectors, | D is the corresponding Euclidian vector length, and  X  ,  X  ,  X  are tuning constants. Equation 14 states that for the first round retrieval, the refined query term weights are modified by adding relevant documents term weights and subtract non-relevant documents term weights. In our experiments, we use Equation 14 in a blind feedback process to update query term weights and compare the results with the ones from our data mining based feedback methods.
Three classification algorithms Naive Bayes (NB), Deci-sion Trees (DT) and Support Vector Machines (SVM), are used to evaluate the proposed technique.

In terms of the Bpref accuracy, our official submission to the 2004 HARD track, in which only blind feedback is used, is 0.3576 [14]. This result was the best passage level result among all the official submissions to the 2004 HARD track. The best, medium and worst results for the 2004 HARD track are given in Table 1. This result will be used as the base result in our experiments, which will be compared to all the results generated by applying data mining to pseudo-relevance feedback.

We conducted a series of experiments under different settings using a single classifier (NB, DT or SVM) with-out co-training. For each run, we choose the top 5 docu-ments from the initial retrieval result as positive examples and bottom 5 documents as the negative examples. These 10 documents constitute the training set and the remaining documents constitute the test set. 7 For each classification algorithm, a classifier is built based on the training data and is used to classify the documents in the test set. We then select k documents in the test set which are considered by the classifier to be most probably relevant. These k docu-ments plus the top 5 documents returned from the first round retrieval, which makes 11 documents in total matching the number of feedback documents in the base run, are fed back into the retrieval system for query expansion and parameter estimation. Table 2 shows the result from the second round of retrieval. For each classification algorithm, we have a few runs with different k and M axN values. Table 2 shows the average results for each algorithm in terms of Bpref value at 12,000 characters on the basis of 14 experiments with the same experimental settings for co-training. The value in the parentheses is the relative rate of improvement over the base run. The last row shows the standard deviation of the runs for each algorithm.
 Table 2. Single classifiers without co-training
From Table 2, we can see that using a single classifier to re-rank the documents improves the retrieval performance compared to the base run whose result is 0.3576. Among three algorithms, DT achieves the best average result. SVM is a close second and is most stable.

To show how the values for k and M axN affect the re-trieval result, Figure 4 depicts the performance of the three classifiers with different M axN values given k =6, and Fig-ure 5 depicts the performance with different k values given M axN =300. From the figures, we can observe that DT and SVM are less sensitive to M axN than NB. For NB, a larger number of features generally leads to better per-formance than a smaller number of features. In terms of k , the value of 2 leads to the best performance for all the classifiers. When k increases, the performance decreases. Especially, the performance decrease is significant for NB and DT.
We now use co-training with each of the three classifica-tion algorithms (NB, DT and SVM). To make it practically feasible, the number of positive and negative examples used for co-training data is usually small. In this experiment, to formulate the first round co-training set, we select five top 5 documents (as positive examples) and bottom five docu-ments (as negative examples) from the initial retrieval re-sult. For each classification algorithm, a series of 14 experi-ments are conducted using co-training with different values for iteration number K and M axN . Table 3 shows the per-formance for each classifier in terms of the Bpref measure and relative rate of improvement over the base run. Com-paring Table 3 with Table 2, we can find that co-training improves the results for all the three classifiers NB, DT and SVM. Among all the results in Tables 2 and 3, using co-training with the decision tree learning method achieves the best result. We can also see that co-training leads to more stable results for all the three classification algorithms com-pared with using the single classifier method without co-training.

To illustrate the results graphically, we re-plot these data in Figure 6, in which the x-axis represents 14 experiments plus the base run and the y-axis shows the retrieval perfor-mance in terms of Bpref at 12,000 characters. An experi-ment is represented by WiKj , which means K is set to j and M axN is set to i  X  100 . For example, W 3 K 2 stands for the experiment in which M axN is set to 300 and K is set to 2. From Figure 6, we can see that all the results are better than the base result, and DT performs the best among the three classifiers in most of the cases. The highest ac-curacy in terms of Bpref@12K is achieved by NB when M axN = 1000 and K =2 , even though NB X  X  perfor-mance is not as stable as the performance of DT and SVM.
To show how the values of M axN influences the re-trieval result, Figure 7 depicts the retrieval performance re-sulting from using co-training feedback at K = 3 with dif-ferent M axN values. We can observe that the decision tree and support vector machine classifiers are not sensitive to the change of the M axN value. But Naive Bayes classifier is sensitive to the MaxN value.

To show how the number of co-training iterations ( K ) influences the result, Figure 8 depicts the retrieval perfor-mance resulting from using co-training feedback at M axN = 300 with different K values. With a fixed M axN value, the retrieval performance resulting from using co-training feedback depends on its K value. The best retrieval result can be achieved by setting K to 3 for Naive Bayes and deci-sion tree classifiers 8 . The retrieval performance from using support vector machine is pretty stable and not sensitive to the change of the K value.
Using Mitra X  X  equation in Section 5.3, we re-rank the ini-tial retrieved documents. The newly ranked documents are used in the blind feedback process for query expansion. Ta-ble 4 shows the results for the Mitra X  X  method. In the exper-iments, we use both  X  X itle X  and  X  X escription X  fields. Top N indicates the number of re-ranked documents for query expansion. From the experimental results, we can see that Mitra X  X  equation does not perform as well as our feedback methods that use single classifiers or co-training.
Using Rocchio X  X  equation in Section 5.4, we use the blind feedback to formulate a new query. We assume that the top K documents are relevant and the bottom K docu-ments are not relevant, where K is set to be 11 in our ex-periments. The results are illustrated in Table 5. In this table, passage level evaluation in terms of Bpref@12K is used. Rocchio X  X  Feedback 1 is the blind feedback which only considers the relevant documents and use these docu-ments to expand the query list. Rocchio X  X  Feedback 2 is the blind feedback which uses both relevant and non-relevant documents to expand the query list. We set  X  ,  X  , and  X  to 1 respectively. From Table 5, we can see that the per-formance of Rocchio X  X  feedback method is better than the base run, but not as good as our methods. We also notice that its performance degrades by considering non-relevant documents. Our conjecture is that terms contained in rele-vant documents are more important than those provided in non-relevant documents. The non-relevant documents may provide some noise information in the term re-weighting process.
The above results are the average results over all the 23 topics. To see whether the improvement of the retrieval performance is due to the better labeling of documents by data mining (or whether the decrease in retrieval perfor-mance is due to the worse labeling of documents by data mining), we analyze the co-training results for three topics. For two of the three topics co-training makes a big retrieval improvement, and for the third topic co-training degrades the retrieval performance. For a run with co-training, we collect the classification accuracy of the co-training algo-rithm, which is the percentage of relevant documents that are identified by co-training. If we compare this classifica-tion accuracy with the percentage of relevant documents in the base run X  X  feedback list without the top 5 documents, we can know whether the co-training feedback provides more relevant documents to the search engine than the blind feed-back. Table 6 compares the co-training runs with the base run in terms of both Bpref@12K and the classification accu-racy (denoted as  X  X cc. X ). For the base run,  X  X cc X  means the percentage of relevant documents in the base run X  X  feedback list without the top 5 documents. For a run with co-training such as DT,  X  X cc X  means the percentage of relevant docu-ments that are identified by co-training.

We can see that the retrieval performance for these three topics changes dramatically by using co-training. For the topics 407 and 445, their retrieval performance increases compared to the base one. But for topic 415, their retrieval performance decreases. For topic 407, both DT and SVM provide more relevant documents to the feedback list, which leads to the increase in retrieval performance. For the same topic, NB does not provide more relevant documents to the feedback list, but its retrieval performance is still im-proved significantly. Similar situations happen to topic 445, in which SVM identified more relevant documents for the feedback list and results in a better retrieval result, while DT and NB do not provide more relevant documents to the feed-back list but still improve retrieval performance. One expla-nation to this phenomenon is that even though co-training sometimes cannot supply more relevant documents to the feedback list, it may provide documents that contain better search terms so that when the query is expanded with these terms, the retrieval result can be improved. For topic 415, its performance decreases compared to the base one. This is because co-training algorithms return more non-relevant documents than the base one.

Regarding the efficiency of co-training, it is obvious that the use of co-training takes more time than pseudo-relevance feedback. However, since the size of the training set and the number of iterations used in co-training are both
Table 6. Retrieval performance vs. classification accuracy small, the increase in running time is not much.
We have presented a new feedback method that com-bines data mining and pseudo-relevance feedback to en-hance the retrieval performance. The proposed method is evaluated on the TREC 2004 HARD dataset. Our results demonstrate that data mining can be successfully applied to information retrieval, especially when there is no labeled example or labeled examples are very few.

The contributions of this paper are as follows. First, we show in detail how to apply the data mining technique to the feedback process of information retrieval. Second, we demonstrate that the data mining based feedback method is effective in improving retrieval performance. Third, we show that for both single classifier based and co-training based methods, DT and SVM perform the best and are the least sensitive to parameter settings. Fourth, we show that co-training can achieve better performance than using sin-gle classifiers. In addition, for all the three classification algorithms (DT, SVM and NB), co-training leads to more stable results. Fifth, we compare our data mining based method with the popular automatic feedback method re-ported by Mitra et al. and the standard feedback method reported by Rocchio. We find that our method performs the best. Furthermore, by looking into the results for in-dividual topics, we find that if co-training provides a feed-back list with more relevant documents, the retrieval per-formance can be improved greatly. However, sometimes, even if it does not identify more relevant documents, re-trieval performance can still be improved. Our conjecture is that the documents identified by co-training through text classification can supply better terms for query expansion even though they are not  X  X elevant X . We plan to investi-gate this issue further. Our future work also include inves-tigating how to automatically choose parameter k for single classifiers or parameter K for co-training. The parameter determines the number of documents used in feedback. We conjecture that the value of k or K should depend on the topic. We also consider designing a hybrid classifier for the co-training algorithm, which may further improve the retrieval performance.

This study was supported by a research grant from the Natural Sciences and Engineering Research Council (NSERC) of Canada. We thank three anonymous review-ers for their useful comments on the paper.

