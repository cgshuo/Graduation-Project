 Given a (large) set of objects and a query, similarity search aims to find all objects similar to the query. A frequent approach is to define a set of base similarity measures for the different aspects of the objects, and to build light-weight similarity indexes on these measures. To determine the over-all similarity of two objects, the results of these base mea-sures are composed, e. g., using simple aggregates or more involved machine learning techniques. We propose the first solution to this search problem that does not place any re-strictions on the similarity measures, the composition tech-nique, or the data set size.

We define the query plan optimization problem to deter-mine the best query plan using the similarity indexes. A query plan must choose which individual indexes to access and which thresholds to apply. The plan result should be as complete as possible within some cost threshold. We pro-pose the approximative top neighborhood algorithm , which determines a near-optimal plan while significantly reduc-ing the amount of candidate plans to be considered. An exact version of the algorithm determines the optimal so-lution. Evaluation on real-world data indicates that both versions clearly outperform a complete search of the query plan space.
 H.3 [ Information Storage and Retrieval ]: Information Search and Retrieval Algorithms
Many commercial applications maintain a person/customer data set that typically contains information such as the name, the date of birth, and the address of individuals that are somehow related to the company. Often, queries against this data set have to be answered extremely fast, e. g., to process online orders or to support call centers. In many cases queries may contain information that differs from the information stored in the data set. For example, there may be typos, outdated values, or sloppy data or query entries. A good search application needs to handle these errors ef-fectively while returning results as fast as possible.
For these and similar problem settings, a common ap-proach is to define several similarity measures for different aspects of the problem. These customized similarity mea-sures focus on calculating the similarity of an aspect as ef-fectively as possible. For example, in the person data use case, we have different similarity measures for the name of a person, for the address, and for the birth date. In the case of image similarity search, one could have different similar-ity measures for the color, the structure, and the textures of images [11].

To combine such similarity measures, there exist differ-ent approaches, such as the weighted sum or the minu-mum/maximum. Recent approaches employ machine learn-ing techniques to learn an optimal combination of similarity measures [2, 18]. In our person data use case, we compared SVMs, decision trees, and logistic regression as combination techniques and decided for logistic regression after thorough evaluation.

Our goal is now to perform efficient search based on a set of similarity measures and an arbitrary composition tech-nique. We assume that we can create a set of similarity indexes that provide efficient access to the set of records based on the defined similarity measures above a parame-terizable threshold (e. g., sim FirstName  X   X  for an arbitrary, but fixed  X   X  [0 , 1]). There are many approaches to cre-ate indexes for specific similarity measures, such as different string similarity measures [15, 16, 21]. For the general case, Christen et al. propose a blocking approach to precompute similarities [6]. (We later sketch how our query plan opti-mization approach can be exploited to optimize the required blocking criterion.)
Apart from the similarity indexes, we do not place any restrictions on the similarity measures or the composition technique. Thus, we cannot apply any of the excellent tech-niques for similarity search in specific data spaces, such as metric space [20] or vector space [3]. Instead, we consider our similarity measures as black boxes and want to find an efficient method for searching with them.

In this paper, we focus on range queries: Given an overall similarity measure sim Overall , a fixed similarity threshold  X 
Overall (the range), a query q , and a record set R , the task is to find all records r  X  R with sim Overall ( q,r )  X   X 
To perform efficient similarity search with an arbitrarily composed similarity measure, we apply a filter-and-refine ap-proach. Given a query, we first filter the entire set of records to derive a set of probably relevant records. For this step, we need one similarity index for each base similarity mea-sure in the filter. We apply filter criteria on these indexes (e. g., all records with sim FirstName  X  0 . 9 as well as all records with sim LastName  X  0 . 8) and then combine the filtered sets (e. g., the intersection of records with similar FirstName and LastName ).

In the refine phase, we calculate the exact similarity (with the combined similarity measure) of the query to each of the records that survived the filtering. The result then contains the set of records above the predetermined threshold for overall similarity (  X  Overall ).

The filter criterion (which operates on the base similarity measures) is an approximation of the overall similarity mea-sure (which composes the base similarity measures). The key to success is to optimize the filter criterion  X  it should be as concise, but as complete as possible. For as many cases as possible, the filtered list of records should contain the cor-rect similar records; the number of missing similar records should be as low as possible. Additionally, this list should be as short as possible, i. e., the number of incorrect matches that are unnecessarily compared to the query record should be as low as possible. In this paper, we define metrics to evaluate filter criteria and algorithms to derive a good fil-ter criterion in a general setting. In the following, we refer to the filter criterion as query plan (to access the similarity indexes) in analogy to query plans in database systems (to access tables and indexes).

In Fig. 1, we illustrate the analogy between our approach and query processing in DBMS. In contrast to database sys-tems, we only support one kind of query: similarity range queries with a fixed range. Thus, we only need to determine a single optimal plan for these queries. Similar to database systems, we first gather statistics about the data and gen-erate a set of possible query plans. We optimize each query plan thresholds and need to handle the trade-off between costs and completeness (while a DBMS is only interested in the cheapest plan as every computed plan yields complete results). We select the overall best query plan based on these criteria and then create the appropriate index structures for efficient access.
The contributions of this paper are: Figure 1: Comparison of our approach with a DBMS architecture The remainder of this paper is structured as follows: In Sec. 2 we describe related work in the area of similarity search. A description of the problem setting can be found in Sec. 3. We propose evaluation metrics for query plans in Sec. 4. Section 5 contains a description of our proposed query plan optimization algorithms. We apply an optimized query plan to a large real-world data set and present results in Sec. 6. The paper concludes in Sec. 7 with an outline of future work.
In the field of similarity search there is a variety of ap-proaches for specific cases of similarity measures and data. If the data can be transformed into a vector space (feature space), the search of similar objects can be reduced to the search of close vectors. A popular similarity measure is the Euclidean distance. For an overview on approaches to this setting, we refer the reader to the excellent survey [3]. For another well-explored space, the metric space, the similarity measure needs to fulfill the metric requirements, while the data can have arbitrary form. In this setting, the triangular inequality can be exploited to efficiently reduce the search space [5, 20].

Another relevant similarity search approach has been sug-gested by Fagin [9]. The presented algorithm retrieves the top k elements by accessing the list of elements in order of their similarity to the query object regarding different as-pects (e. g., order all pictures by their similarity to blue and round ). The approach only works for a very limited set of basic composition techniques for the overall similarity mea-sure; thus, Fagin X  X  algorithm cannot be applied in our setting with arbitrarily composed similarity measures.
Deshpande et al. suggest an index structure for non-metric similarity measures that exploits inverted indexes ( similarity lists ) for all values [7]. As stated by the authors, their index structure AL-Tree is only suitable for attributes with very small numbers of distinct values. In our setting with possibly millions of distinct values per attribute, this approach is infeasible. As none of the aforementioned approaches are applicable to our problem, we cannot empirically compare with them.

An area related to similarity search is duplicate detec-determine all duplicate entries, i. e., all sets of records that refer to the same real-world entity. For similarity search, too, the problem is to find similar entries, but only for one query object. Duplicate detection is often run in a batch processing job, while similarity search usually requires an answer very fast for a satisfying user experience.

Christen et al. [6] propose to determine similar records be-fore inserting a new record into a database, thus preventing the insertion of duplicate records beforehand. Similar to our approach, they exploit a set of similarity indexes. To deter-mine the overall similarity, the authors propose to calculate the sum of the base similarity values, while our approach allows to use any combination technique as the composite similarity measure. Our approach tackles the main question that remains unanswered: Which similarity indexes should be created and queried with which thresholds?
A common approach to duplicate detection is blocking, i. e., similar records are grouped into blocks, and then all records within each block are compared to each other [17]. The problem of finding the best blocking criterion is simi-lar to that of finding the best query plan for similarity in-dexes. In our setting, the blocking predicates are similar to the attribute predicates for which we optimize the thresh-olds. Michelson and Knoblock suggest a machine learning approach to learn blocking schemes, i. e., selected attributes for blocking as well as similarity measures [13]. Bilenko et al. determine an optimal blocking criterion by modeling the problem as red-blue set cover problem [1]. Both approaches can only decide whether the predefined blocking attribute candidates are contained in the optimal blocking criterion or not. In contrast to these approaches, our proposed algorithm supports the optimization of thresholds involved in similar-ity predicates (not only boolean contained/not-contained de-cisions). By exploiting the monotonicity property of these thresholds and defining neighborhoods of query plans, we can quite efficiently traverse the threshold space. Chaud-huri et al. determine duplicates by calculating the union of similarity joins [4]. They reduce the problem of finding the best similarity join predicate to the maximum rectan-gle problem. In a second step, they union the optimal join predicates. We handle both aspects in a unified approach of finding the best disjunction of conjunctions of similarity predicates. Their approach requires the specification of neg-ative points (in their case: non-duplicates) as training data, which is not helpful in the similarity search setting (where for a given query almost all records are irrelevant and thus negative examples). As we cannot rely on these negative examples in our setting, we need a more sophisticated cost estimation. With a more expensive evaluation function for query plans, we also need a suitable algorithm for efficiently traversing the solution space (and saving the evaluation of unpromising query plans).
In this section, we define basic notations for composed similarity measures and the problem to optimize a query plan for efficient search with them (during the filter phase).
We follow the common and proven notion of defining in-dividual similarity measures for different attributes and at-tribute types; for instance, dates are compared differently than names or addresses. These individual similarities are subsequently combined to define the global similarity of two records.

We first split the problem of measuring the similarity of two records into smaller subproblems. We define a set of base similarity measures sim p ( r 1 ,r 2 ), each responsible for calculating the similarity of a specific attribute p of the compared records r 1 and r 2 from a set R of records that is a subset of a universe U of possible records. In our use case, sim City , and sim Zip . All base similarity measures can be cho-sen independently. For example, we could use Jaro-Winkler distance for sim FirstName [19], the relative distance between dates for sim BirthDate , and numeric difference for sim Zip assume the domain of the similarity measures to be between 0 and 1, with 1 representing identity and 0 dissimilarity of the compared record parts:
A composed similarity measure uses the base similar-ity measure to derive an overall similarity judgement. For example, a weighted sum of the base similarity measures is one composition technique. Other techniques involve ma-chine learning approaches, such as logistic regression, deci-sion trees, and support vector machines. Learnable similar-ity measures have been addressed by several researchers [2, 18]. We make no assumptions whatsoever on the composed similarity measure other that it is composed of base similar-ity measures.
A base similarity measure predicate sim p ( q,r )  X   X  p cov-ers all records r  X  R for which the similarity to the query record q  X  U calculated with the base measure sim p is at least  X  p . In the following, we abbreviate this predicate to p  X   X  p and refer to base similarity measure predicates as attribute predicates (due to our running example, in which each base similarity measure covers one attribute).
A query plan template is a combination of attribute predicates (with yet unassigned threshold variables) with the logical operators conjunction and disjunction. We re-quire query plan templates to be in disjunctive normal form (DNF), since this form is popular [1, 4] and easy to un-derstand and modify. Note that all logical combinations of attributes can be expressed in DNF. A query plan template combines N attribute predicates and has the form: with 1  X  i  X  N,  X  i : 0  X   X  p i  X  1.

Once all threshold variables in a query plan template are assigned a value, we call this a query plan . An example for a query plan that covers all records with similar FirstName and Zip or LastName and BirthDate is the following:
To evaluate query plans, we define different performance metrics. Given a set of positive training examples for query/ result record pairs, the completeness comp ( qp ) of a query plan qp is the expected proportion of correct query/result record pairs that are covered by qp . The costs costs ( qp ) of a query plan qp is the expected average number of records in R that are covered by qp . Completeness and costs are described and analyzed in detail in Sec. 4.

Given a set R of records and a cost threshold C , the query plan optimization problem is to find the query plan qp that maximizes comp ( qp ) with costs ( qp ) &lt; C . We analyze problem properties and describe our approach to solve the problem in Sec. 5. In this section, we discuss how we evaluate a query plan. We evaluate based on two dimensions: (1) Completeness: How many matches can be found with this query plan? (2) Cost: How large is the cardinality of an average query result with this query plan? In general, we want to maxi-mize completeness within an upper bound for cost. How to find such best plans is topic of the following Sec. 5.
For a given query plan, we want to estimate how complete the results will be. We express completeness of a query plan as the proportion of query/result record pairs from a set of positive training record pairs that are covered by the plan (i. e., for which the similarity of query and result records are above the query plan thresholds). Completeness is thus the probability that a correct result to a query will be found with the query plan. In the following, we describe how to gather training data and estimate completeness with it. To estimate the completeness a query plan, we need a set T  X  U  X  U of positive training examples for query/result record pairs. There are two main options to gather training data: (1) We have a (preferably large) set of queries with correct answers. (2) We use virtual training data. (1) Real training data : In our use case, we have a set of 2m queries, most of them with results manually labeled as correct. This is the ideal situation, where we can rely on a manually labeled set of query/result record pairs. (2) Virtual training data: The first case relies on man-ually determining sets of query/result record pairs. Since this is often a costly task, one can also create virtual training data. For one training instance, our only concern is whether the overall similarity measure judges that the instance is rel-evant based on its base similarity values. Thus, we can make up base similarity values (without creating a real query/re-sult pair). We can determine whether these base similarity values would lead to a correct match (if the composed sim-ilarity measure computes an overall similarity value above  X 
Overall ). If yes, we have created a positive training instance without the need for real training data. We leave for future work the definition of an algorithm that efficiently traverses the space of possible base similarity values. Inspiring work comes from the domain of learning logical expressions in DNF with membership-query algorithms [12].
With positive training instances T  X  U  X  U at hand, we can estimate completeness of a query plan qp . The function covers qp ( q,r ) evaluates to true iff the pair ( q,r )  X  U  X  U is covered by qp (i. e., if the query plan predicates are fulfilled).
Our basic algorithm to calculate comp ( qp ) for given qp and T is very simple: We iterate over the list T and count all query/result record pairs that are covered by qp . Since there is a potentially large amount of base similarity value combinations that are checked multiple times, we speed up the basic algorithm with a more efficient data structure: For each distinct base similarity value combination, we save its count, thus eliminating all  X  X uplicate X  combinations. In ad-dition, we further reduce the number of value combinations by rounding to two decimal places. Distinct combinations with the same rounded combinations are accumulated. In our use case, we can reduce the amount of value combina-tions to be checked from 2.0m to 4.4k (a reduction rate of 99.8 %).
The second evaluation criterion for query plans is the in-volved costs. We need to estimate how many result records we can expect on average when applying a query plan to the entire data set.

For cost estimation we do not require training data, be-cause we only exploit information from the distribution of attribute values in the complete record set R . A naive ap-proach to estimate query plan costs would be to sample a set S  X  R of records, exactly determine the costs for this sample by comparing each record in S to each record in R , and then average the determined costs. In our use case, exactly determining costs for one record would take several hours. As we need to analyze a large set of query plans during the query plan optimization phase, we need a more efficient procedure. Thus, we precalculate attribute similar-ity histograms with which we can quickly estimate costs of complete query plans.

In the following, we first describe how we derive similar-ity histograms for determining the amount of similar records regarding one attribute. After that, we explain how to com-bine attribute costs to estimate costs for query plans involv-ing multiple attributes.
We want to estimate the cardinality of the predicate p  X   X  i. e., how many records in our database have at least a simi-larity of  X  p regarding the attribute p with respect to a ran-dom query? To derive a generic query plan, this estimation needs to be independent of specific values. We rather want a general estimation for each attribute.

To achieve this, we first create a similarity histogram for several values of the attribute. For each analyzed value, we calculate the similarity to all other values of the at-tribute. Figure 2 shows a distance histogram for the last name  X  X ange X . A reading example is: For LastName  X  0 . 9, there are 172,000 records with a last name with minimum similarity 0.9 to  X  X ange X . The solid line is the cumulated number of similar values up to a total of 66m records with a similarity of at least zero. Figure 2: Similarity histogram for attribute LastName for value  X  X ange X 
We then calculate the average of the created similarity his-tograms. For each possible similarity, we average the mea-sured numbers of records with similar values. The resulting similarity histograms for our attributes are shown in Fig. 3. For the attributes FirstName , LastName , and City , we can see rather smooth curves. The curves for BirthDate and Zip are stepped, because all values have an equal length, and only few errors can occur. These curves allow estimations such as  X  X or FirstName  X  0 . 7, we can expect 500k records with similar values, on average X . Figure 3: Average similarity histograms for all at-tributes
An open question is how to choose appropriate sample val-ues to create these histograms. In general, we recommend using a set of randomly chosen values. For a worst case es-timation of the costs, it may make sense to include more frequent values in the sample set. More frequent values will themselves lead to higher costs and also have more similar values (as we empirically validated). Regarding the sample size, we empirically determined that a set of 100 randomly chosen elements is sufficient in our use case (for a popula-tion of 66m elements, a confidence interval of  X  10%, and a probability of 95%, 96 elements are necessary).
Using the similarity histograms, we are able to estimate the cardinality of a complete query plan. In a nutshell, we estimate the costs of a query plan qp for a random query q  X  U by estimating the probability that a randomly chosen element r  X  R is covered by qp . We multiply the probability with the cardinality of the complete record set to determine expected average costs: By calculating costs using probabilities, we are able to easily handle conjunctions and disjunctions in the query plans with probability theory.

In the following, we abbreviate the probability that a ran-domly chosen element r  X  R is covered by an attribute predicate p  X   X  p in P ( covers p  X   X  p ( q,r ) | q  X  U,r  X  R ) as P ( p  X   X  p ) and any conjunctions and disjunctions accord-ingly.

We first estimate the probability that an element is in the set of records determined with a conjunction , such as LastName  X  0 . 9  X  City  X  0 . 95. For two attributes a and b with thresholds  X  a and  X  b we want to estimate the proba-bility P ( a  X   X  a  X  b  X   X  b ). To resolve the joint probability, we distinguish between attributes that are statistically de-pendent or independent, for ease of calculation. (Although we would theoretically be able to calculate all joint distri-butions, this would consume a significant amount of time and space.) In our case, we observe that City and Zip are dependent and that each attribute is dependent on itself (this is relevant if a conjunction contains several predicates on the same attribute); all other attributes are independent from each other. If dependent attributes are not known in advance, these can be determined with statistical indepen-dence tests such as the  X  2 -test. To estimate joint proba-bilities of dependent attributes, we assume the worst case: the two predicates completely overlap, i. e., the records cov-ered by one predicate are completely covered by the second predicate. Thus, we estimate the probability of the pred-icates X  conjunction as the minimum of the probabilities of two predicates (and accordingly for three or more overlap-ping predicates). For statistically independent attributes, we simply calculate the product of the predicate probabili-ties. Thus, for two predicates we have: P ( a  X   X  a  X  b  X   X  b ) =
For more than two attributes, we accordingly resolve joint probabilities of statistically dependent attributes and then calculate the product of the remaining predicate probabili-ties of independent attributes.

With the similarity histograms, we can estimate the indi-vidual predicate probabilities as: Note that the randomness regarding queries is already cov-ered by the attribute costs estimations. As described above, for each attribute we selected several values from R as ex-ample queries and averaged their real costs.

In a final step, we estimate the probability of a disjunction of conjunctions. For the union of two sets, we can calculate the cardinality as the sum of the cardinalities of the two sets less the intersection of them. The same applies for probabil-ities. For example, we want to estimate P ( a  X   X  a  X  b  X   X  and have:
The remaining probabilities contain only conjunctions and can be estimated as described above. For the general case of n conjunctions c i in the disjunction W n i =1 c i , the principle of inclusion and exclusion gives us:
With these estimations, we can accurately and efficiently estimate the costs of any query plan in DNF.
To analyze the quality of our cost estimation model, we compare estimated and observed costs for a set of query plans templates. We generated all query plan templates with one or two disjunctions of conjunctions that involve two or three attributes each. We required the conjunctions to contain different sets of attribute, while overlapping of attributes in the conjunctions was allowed. Overall, we an-alyzed 210 query plan templates.

We randomly selected 100 queries as test objects for which we can quickly count the exact number of records with the values given in the query using an inverted index. With this method, we can count exact matches (i. e., all similarity thresholds in the query plans are set to 1.00). This allows the evaluation of the quality of the probability model. (For the quality of the similarity histograms, we rely on statis-tical guarantees that we have enough training examples for calculating average frequencies.)
In Fig. 4, we show estimated and average observed costs for the analyzed query plans. Optimal estimations would re-sult in points arranged at a diagonal line with y = x (shown as model line in the graph). We observe that our estimations are quite close to the observed frequencies for all considered plans. Our cost model, albeit not perfectly accurate, seems to be a good estimator for average costs of different query plans.
Based on the performance metrics defined in the previous section, we want to find the best query plan, i. e., the plan with highest completeness below a cost threshold. We it-erate over the set of all possible query templates and then optimize thresholds for each query plan template. We thus have one optimal query plan per query plan template. From these query plans, we can simply select the overall best query plan.

In the remainder of this section, we discuss the problem of optimizing the thresholds of one query plan template. For a given query template containing n attribute predicates, each attribute predicate X  X  threshold can be set to one of v values. Thus, the complexity of a complete search is O ( v exponential in terms of attribute predicates. For example, for 6 attribute predicates, each with 101 possible threshold values (0.00 to 1.00), there are 101 6  X  1 trillion possible query plans. As we need to calculate both completeness and costs for each plan to be analyzed, this large set of possible Figure 4: Comparison of observed and estimated frequencies for the 210 generated query plans. Ex-ample: For the query plan BirthDate  X  1  X  City  X  1 , our estimation is 22 records, and the observed average frequency is 25 records. plans is infeasible to be analyzed completely; thus, more efficient algorithms are required.

We begin this section with observations regarding the dis-tributions of completeness and costs and then discuss algo-rithms for exact and approximative optimization of query plans.
To better understand the problem space, we first analyze the distribution of completeness and costs. For the exem-plary query plan template we evaluated all possible query plans, i. e., all possible thresh-old values for  X  FirstName and  X  LastName . We show the resulting comparison distribution in Fig. 5 and the cost distribution in Fig. 6.

In the cost distribution diagram, we can see that query plans with lower thresholds have higher costs. This is not surprising, as lower thresholds result in at least as many or more covered records as a higher threshold. For decreasing thresholds, we observe that costs grow exponentially. We can especially see that only few query plans in the upper right corner have acceptable costs; as we will see later, more complex plans with disjunctions of conjunctions in the query plan templates achieve better results (i. e., higher complete-ness with lower costs).

The distribution of completeness shows that lower thresh-olds result in higher completeness. While this insight is also not surprising, the growth of completeness is quite different from the cost distribution. Already with the highest thresh-olds (i. e., only exact matches are found), we can cover the vast majority of the 2m training query/result record pairs. The increase in found matches is high for higher thresholds, but for lower thresholds, the increase dwindles. This con-firms our intuition: The last percentages of completeness (recall) are the most difficult to resolve.

The monotonicity observations can be generalized for any combination of similarity measures. Any query plan with thresholds (  X  x , X  y ) has at least the costs and the complete-ness of any query plan with thresholds (  X  x 0 , X  y 0 ) with  X  Figure 5: Distribution of costs for query plan tem-Figure 6: Distribution of completeness for query plan template FirstName  X   X  FirstName  X  LastName  X   X   X  x or  X  y 0 &lt;  X  y . Note that Chaudhuri et al. make similar observations and also exploit the monotonicity property in their algorithm [4]. In our case we do not know complete-ness and cost of the query plans in advance, so we solve a different problem with an entirely different approach.
From the cardinality distribution diagram, we can derive the space of all valid query plans, i. e., all plans with costs at most as high as the predefined cost threshold C . For the two-dimensional case, we can think of a line that separates all valid query plans from the invalid ones. From the mono-tonicity observations, we can infer that such a line always exists and that we can ignore any combinations below this line. Thus, we consider only the combinations above this line without discarding valid combinations. In our following algorithms, reaching this line will be regarded as stopping criterion.
An described above, an algorithm that analyzes all possi-ble query plans for a query plan template is infeasible. Keep in mind that we do not know completeness and cost for any query plan in advance. We need an algorithm that efficiently navigates through the solution space: The algorithm should only evaluate as few query plans as possible and determine an overall good solution.

The general idea of the top neighborhood algorithm is to start with the plan with highest thresholds (in our case, all thresholds are set to 1, which corresponds to the top-right corner of Figs. 5 and 6), then follow promising plans in its neighborhood (the top plans), until we reach the valid query plan separation line. The result is then the plan with highest completeness (and lowest cost, respectively) found so far. In the following, we describe the algorithm in greater detail.

The start plan must be the one with highest thresholds, since any other plan for starting could make it impossible to find the best solution due to our downwards search approach.
We define the neighborhood of a plan to help us navigate the threshold space. A query plan qp has a neighborhood N ( qp ) that contains all query plans that can be constructed by lowering one thresholds of qp by one step (e. g., by 0.01). For example, the neighborhood of a plan with two thresholds  X  a and  X  b has two elements: The neighborhood thus defines all possible directions to tra-verse the solution space given one query plan. We define the neighborhood only for lower thresholds and thus higher completeness (and also higher costs), since we traverse the threshold space from higher to lower thresholds.
 In each iteration of our algorithm, we have a window W = { q 1 ...q n } of n query plans that are currently regarded. We extend the neighborhood concept to windows by defin-ing the set of all neighborhood plans for the plans in W as N ( W ) = S qp  X  W N ( qp ). We then select a subset of these plans: the t plans with highest completeness (and lowest costs, if there are several plans with equal completeness; if there are more than t eligible plans, a random selection is made). We call this set the top t neighborhood T t ( W ). Only plans with costs below the cost threshold C are con-tained in this set.

The cost threshold C also determines the stopping cri-terion of our algorithm. If the top t neighborhood contains only plans with costs above C , then the algorithm terminates and returns the best plan found so far. Otherwise, the algo-rithm continues with a new iteration by setting W := T t ( W ) ( t is thus also the maximum window size).

Note that the algorithm is optimal regarding the number of times a query plan is evaluated. In each iteration, the window contains only plans with the same amount of ap-plied threshold lowering steps (compared to the initial plan). Since the union of the generated plans is calculated before evaluating them, no query plans are evaluated more than once.

In Fig. 7, we illustrate one iteration of the algorithm by means of an exemplary query plan template with two thresh-olds. Note that the illustration is similar to the measured completeness distribution in Fig. 6. The diagram shows the search space of all valid plans, i. e., all plans with accept-able costs (which we do not know in advance). The current window consists of two plans. The neighborhood of the win-dow contains three plans, two of which (with comp=38 ) are selected as the top 2 neighborhood and thus form the new window for the next iteration.
 Exact algorithm. The parameter t allows us to turn the approximative top neighborhood algorithm into an exact  X  0 1 1 Figure 7: Application of top neighborhood algo-rithm to exemplary completeness estimations (in percent) for two thresholds  X  X , X  Y in a query plan template algorithm for finding the optimal query plan. By setting t =  X  , we do not limit the window size and thus evaluate all valid query plans. Still, we avoid the evaluation of a number of query plans: those with costs above C . Regard-ing the search space coverage, this exact algorithm is thus a better approach than a brute force search over all possible query plans. A disadvantage is the higher memory require-ment, since all evaluated query plans of an iteration need to be kept in memory.
 gorithm depends on the number of necessary iterations as well as the number of generated query plans per iteration. A worst case estimation for the number of necessary iterations assumes that all thresholds need to be lowered to their low-est possible value. The number of iterations is thus limited by the number of all possible thresholds of all predicates of the query plan template. The number of generated plans per iteration is determined by the top neighborhood size (and thus maximum window size) t and the number of predicates in the query plan template (for which one threshold can be lowered in an iteration). Thus, the complexity of the top neighborhood algorithm is linear in the following parame-ters:
To evaluate the top neighborhood algorithm, we analyzed the behavior of the algorithm on our data set with 2m (cor-rect) query/result record pairs. We ran the algorithm for three randomly chosen query plan templates with different numbers of predicates and varied the parameter t . Figure 8 shows the results. For comparison, the first line of each ta-ble shows the results for exact matching (i. e., all thresholds are set to 1) as baseline, and the last line contains the result of the exact version of our algorithm (with t =  X  ). We re-fer to the result of the exact algorithm as the optimal plan, since no better plan can be found with the given query plan template.

For all analyzed templates, a small value for t is sufficient to find a plan with near-optimal completeness (only a few of the 2m records are missing). For all analyzed values of t , we can see a significant improvement over the baseline results for exact matching. In general, a larger value for t results in larger completeness. The numbers of evaluated query plans also confirm the linearity of the algorithm regarding t and the query template complexity (i. e., the number of predicates as well as the number of thresholds per predicate; this cannot be distinguished in this experiment).

With growing query plan templates (i. e., more predicates), the results become more complete, since the template is more expressive. In general, the thresholds of a more com-plex predicate are higher in order to satisfy the cost con-straint.

The more complex the query plan, the larger the number of saved query plan evaluations: For the template with six predicates, we consider only less than a percent of the valid query plans. For all templates, we only evaluate a fraction of all possible plans. Even the exact version of our algorithm with unlimited t considers only 16% of the possible plans for the small template and 0.6% for the large template. For the large template and t = 200, still only less than a ten thousandth of all possible plans have been evaluated.
To summarize, the results show that the top neighborhood algorithm determines a near-optimal plan while evaluating only a fraction of the possible query plans. We have mea-sured similar results for other query plan templates.
In addition to the experiments for individual aspects of our approach in earlier sections, we also ran an experiment on the complete data set with the overall optimal query plan.
To determine the overall optimal query plan for our data set, we used the set of query plan templates from the experi-ment in Sec. 4.3. This set contains all query plan templates with one or two disjunctions of conjunctions that involve two or three attributes each. We required the conjunctions to contain different sets of attributes, while overlapping of attributes in the conjunctions was allowed. For each of the 210 query plan templates, we ran our top neighborhood algo-rithm with t = 100 (a window size that results in acceptable runtime of our algorithm) and C = 1000 (a cost threshold that is acceptable for our use case). We selected the overall best query plan for this experiment.

As test queries, we randomly selected 50,000 queries from our test data set. We ran these queries with different search settings and show the results (completeness and cost as de-fined in Sec. 4) in Table 1.

The first line shows a basic result when searching for only those records where all attribute values of the query exactly match the values in the record. This shows that 87 % of the queries are  X  X asy X  to answer, since the corresponding correct matches in the record set do not contain any differences to the query. The costs for queries with this search settings are quite low, since there is usually at most one such exactly matching record.

We then tried to answer the same set of queries with the query plan that turned out to be optimal regarding com-pleteness (according to our top neighborhood algorithm). We first did an exact search with this plan (line 2 in Ta-Table 1: Search performance of different search set-tings ble 1). In comparison to the first search setting where all attribute values were required to exactly match, this plan only requires that one of two conjunctions (of two attributes each) match. The result shows that the majority of the re-maining records can be found with this setting, but that the average costs are also higher for this setting (again compared to search setting (1)).

By applying the top neighborhood algorithm, we opti-mized the thresholds of the query plan. Line 3 in Table 1 shows that we achieve even better results than for search setting (2). The completeness is near 100 %. As already pointed out, the last couple of records are the most difficult to find. This query plan thus has higher costs, but they are still below the cost threshold of C = 1000.

For comparison, we also show that comparing the query to each record in our data set (line 4 in Table 1) would result in perfect completeness, but also in costs of 66m, which is clearly infeasible. Our optimized query plan exploits the cost limit best and is thus an appropriate choice for efficient similarity search in the given data set.
We introduced a novel approach to efficient similarity search for composed similarity measures. We showed how to effi-ciently compute completeness and costs, two important per-formance metrics for query plans based on a set of similarity indexes. The resulting trade-off between completeness and costs has been solved with the approximative top neighbor-hood algorithm. We showed that the algorithm efficiently determines near-optimal query plans for real-world data. Further applications. Our proposed algorithm for op-timizing query plans can also be adapted to determine a good blocking criterion. Blocking is a simple approach to improve efficiency of duplicate detection [17]. Another ap-plication scenario is the creation of a similarity index for an attribute predicate sim p  X   X  p : We create blocks of similar values of the attribute p and then compare only the values in the blocks with each other. For each attribute value v we store all values v 2 with sim p ( v 1 ,v 2 )  X   X  p in a sorted list for efficient access. To optimize this process, a blocking cri-terion needs to be defined with which as many similar values and as few dissimilar values as possible are within each block and are thus compared. The completeness of the blocking criterion should be high, and the costs should be low, similar to the formulation of the query plan optimization problem.
Our algorithm can be applied to this setting as follows: In-stead of query plan templates, our algorithm now optimizes blocking criterion templates that may also include boolean blocking predicates. An example for a blocking criterion is: This blocking criterion means that in a first blocking run, all values with a common prefix of length 3 are compared, and in a second blocking run, all values with a at least two com-mon n-grams are compared (this approach is called multi-pass blocking [10]). Blocking criteria may also contain con-junctions, similar to query plans.

Costs of a blocking criterion are calculated as the average number of similar values for a given  X  X uery X  value (to create a complete similarity index, all values will be  X  X ueried X  for similar values). Similar to the description for query plans in Sec. 4.2, we can sample some values and calculate the average number of values covered by the blocking criteria for an appropriate range of blocking predicate thresholds. To create training data for completeness estimation, we can sample some attribute values and then compare these to all other values. In contrast to the similarity search problem (where there is usually only a very small set of objects similar to a query), there is usually a large set of values in the same block. The top neighborhood algorithm can work with these estimations; it starts with a very rigorous blocking criterion (with high thresholds and thus small blocks) and then lowers the thresholds until a predefined cost threshold is reached. Future work. We leave for future work the following re-search directions: We thank Schufa Holding AG for supporting this work. We especially thank Stephan Springob, Boris Zerban, and Mi-chael Stolz for their valuable input.
