 CHUNG-HSIEN WU, ZE-JING CHUANG, AND YU-CHUNG LIN National Cheng Kung University, Tainan, Taiwan ________________________________________________________________________ ________________________________________________________________________ 1. INTRODUCTION Recent research into human-machine co mmunication places greater emphasis on recognition of nonverbal information, especially of emotional reactions. An extensive emotional states [Picard et al. 2001]. The recognition of emotion is an important component of  X  X ffective computing X  and has been implemented in many kinds of media. For example, speech and image are the most common ways to recognize emotion. Picard [1997] proposed the concept of affective computing and deduced theories of emotion, recognition, and generation. Cowie et al. [2001] initiated research on recognizing emotion via all kinds of signals, including em otional keywords, speech signals, and facial expressions. Some researchers proposed a multimodal system for emotion recognition: For example, Cohn and Katz [1998] developed an automated method for recognizing hidden Markov model (HMM) to recognize emotions from both video and audio signals. Among the above approaches, recognizing emotion from speech is the most popular. In speech-based recognition, paralinguistic information such as pitch and energy are the features include zero-crossing-rate (ZCR) [K won et al. 2003]; Teager energy operator-based parameters (TEO-based parameters) [Rahurkar and Hansen 2003]; and frequency-centroid, formant, and pitch vibration. different communication modalities, the variety and complexity of language makes it difficult for researchers to recognize emotional states from pure textual data. Recognizing emotion is extremely important for some text-based communication tools, e.g., the dialog system is a kind of human machine communication system that uses only text input and response and answer types [Lee et al. 2002]. Text is still the main communication tool on control the dialog strategy. In net conferencing, the ability to identify the users X  the conferencing program [Boucouvalas 2002]. textual data is the most appropriate medium for network transmissions. Second, the opinions, and emotions using text only. speaker X  X  emotional state. Using emotional keywords is the most direct way to recognize a user X  X  emotions from text input, and several methods were proposed that used selected emotional keywords. Yanaru [1995] took footage of real speakers while they were talking keywords. Subasic and Huettner [2001] classified a group of emotional words by manually scoring the emotion level for each word. Boucouvalas and Zhe [2002] applied a probability between the emotional keywords and the emotional states. Tao and Tan [2004] used emotional function words instead of emotional keywords to evaluate emotional states. However, all keyword-based systems have the following problems: (1) ambiguity in defining all emotional keywords; (2) recognizing emotion from sentences with no emotional keywords; and most importantly (3) lack of semantic and syntactic information for emotion recognition. Besides keyword-based approaches, some researchers utilized other textual information clues such as pragmatic intent, text content plausibility, and paragraph structure [Dijkstra et al. 1994]. Litman and Forbes [2003] and the sequence of speakers, in order to reco gnize the emotional stat e in a dialog system [Forbes-Riley and Litman 2004]. Schuller et al [2004] integrated both acoustic and linguistic information in emotion recognition. These methods do not take the semantic article with a complete chapter structure. Wi th further analysis, some researchers believe that textual data is rich with emotion at the semantic level; that is, that emotional content sentences and constructed a symbolic netw ork to reduce language model perplexity. Woods [1970] used a transition network to analyze natural language. A semantic network-based emotion recognition mechanism [Chuang and Wu 2002] was proposed using emotional keywords, semantic/syntactic information, and emotional history in order to recognize the emotional state of a speaker. a diagram of this approach is shown in Figure 1. There are two main phases in the generation rules (EGRs) and emotional corpus to generate emotion association rules (EARs). The separable mixture models (SMMs) are applied to classify the most appropriate emotional state from the input text. In both phases a universal lexical ontology is employed. An exhaustive description of the procedures is shown in the following sections. recognition procedure is described in the next section. The details of each component are introduced in Sections 3 to 6. Section 3 describes the derivation of the emotion priori algorithm and the process for generating EARs. The separable mixture model is respectively. 2. OVERVIEW OF EMOTIO N RECOGNITION PROCEDURES emotion generation rules (EGRs) are deduced manually to describe the conditions for generating emotion. All of the sentences in the collected emotional corpus are then annotated with the emotional state and EGR to which they belong. With further analysis, each EGR is seen to consist of domain-inde pendent and domain-dependent components. For example, one EGR for the emotional state  X  X APPY X  is domain-dependent component is the semantic item  X  X omething that is beneficial. X  A sentence with this EGR annotation implies that we can extract these two components from the sentence. To obtain the domain-independent component, we define semantic labels (SLs) by analyzing the hierarchical hy pernym structure of  X  X btains, X  which is already defined in a universal lexical ontology. Every word that contains predefined hypernyms can be converted into the corresponding SL. In the opposite direction, the domain-dependent components cannot be extract ed as directly because they have no general definition. With the same example, the definition of  X  X hat is beneficial X  is different in different domains and for different persons. Our approach to extracting these domain-dependent components is to train emotion association rules (EARs) using the a priori algorithm [Han and Kamber 2001]. Befo re the training process, a preprocessor converts all sentences in the training corpus into a sequen ce of SLs, ATTributes (ATTs) emotional state. The format of the converted result is Then, by applying the a priori algorithm, the so-called EARs consisting of SLs, the ATTs and the inferential emotional state are ex tracted. Because the main idea of emotion recognition is to evaluate the similarity be tween an input sentence and the EARs of each emotional state, the separable mixture model (SMM) [Hofmann and Puzicha 1998] is probability of the emotional states given by th e SLs and ATTs. In the test phase, the test sentence is first transferred into the SL and ATT sequences by applying the same universal lexical ontology. The SMM is then adopted to estimate the maximal probability of the emotional state. providing prior knowledge of the input sentences. In the ontology, each concept is relationship between the concepts . The ontology used in our system was developed by Chen et al. [2003] based on the Chinese knowledge base, HowNet [Dong and Dong 1988], to represent the lexical knowledge. To expand the depth and width of the concept relationships, WordNet [Fellbaum 1998] was integrated into the ontology using a bilingual dictionary. Every concept in the universal lexical ontology will finally relate to several attributes (ATTs) via the hierarchical links. 3. EMOTION GENERATION RULE Further textual data analysis of emotion recognition shows that not only keywords annoying job. X  The two senten ces express the same emotional state, but the critical emotional keyword  X  glad  X  is not spoken in the first sentence. To solve this problem, the mechanism for generating emotional states must first be investigated. research on emotion psychology [Lazarus and Lazarus 1996]. These conditions are assumed to be generic even though different individual backgrounds and environments are involved. For example, we may feel happy when we have a good reputation, reach a These kinds of conditions or environmental situations are based on emotion psychology and are manually derived as emotion generation rules (EGRs) in this article. there still exist some ambiguities inside EGRs. Take the previous EGR as an example; it is clear that  X  One may be HAPPY if he ob tains something beneficial , X  but the emotional state of  X  X omeone lost something beneficial X  may be ANGRY or SAD, according to deduce EGRs with only two opposite emotional states: HAPPY and UNHAPPY. dependent components. Generally speaking, domain-independent components are  X  X erbs X  in EGRs, while domain-dependent components are  X  X ouns X  in EGRs. As in the above example,  X  obtains  X  and  X  something beneficial  X  are respectively the domain-independent and domain-dependent components of the EGR  X  One may be HAPPY if he corresponding emotional state, domain-independent, and domain-dependent components. 4. SEMANTIC LABEL DEFINITION AND EXTRACTION In the above EGR, the definition and extracti on of SLs are critical to the entire process. The semantic label, as the literal meaning, is defined as a word or a phrase that indicates some specific semantic information. Due to their domain-independent characteristics, it is reasonable to define them with the universal lexical ontology. This section introduces the Figure 2. 4.1 Semantic Label Definition In Figure 2, three kinds of SLs are defined: active SLs, negative SLs, and transitive SLs. With the help of hypernyms defined in the universal lexical ontology, we first define the active SLs which form the main EGR inten tion, such as [REACH], [OBTAIN], or them by simply defining their hypernyms. For example, the concept  X  X eceive X  is the hypernym of concepts  X  X et, X   X  X cquire, X   X  X btain, X  and  X  X arn. X  With the definition  X  concepts that have a hypernym  X  X eceive X  can be the SL [OBTAIN], X  we can convert the four words  X  X et, X   X  X cquire, X   X  X btain, X  an d  X  X arn X  in the input sentence to the SL [OBTAIN]. Some definition of active SLs can al so be hierarchical; that is, the definition of an SL may be the hypernym of a concept X  X  hypernym. The active SL definition job can only be made manually, and was actually done by three researchers in about three months. As the result, 147 hypernyms in total were selected from 803 hypernyms for the definitions of 15 active SLs. negative SLs indicate the words with negative semantic meanings. Only one SL is contained in this kind of SL: [NEGATIVE]. The transitive SLs indicate the transitive semantic meaning and contain two SLs: [TRANS_EXT] and [TRANS_EXT]. The transitive-extractive SL [T RANS_EXT] imply that the ensuing sentence is more important than the preceding sentence, so th e SLs before [TRANS_EXT] will be ignored. Conversely, the SLs following the transitiv e-subtractive SL [TRANS_SUB] will be ignored. Because the concepts belonging to these two SLs are enumerable, it is not necessary for these two SLs to define hypern yms hierarchically in the ontology. For the negative SLs, 47 words are directly defined as [NEGATIVE] SLs, such as  X  X annot, X  directly defined as [TRANS_EXT] and [TRA NS_SUB], respectively. The corresponding words of SL [TRANS_EXT] include  X  X inally, X   X  but, X   X  X ortunately, X  and so on. And the corresponding words of SL [TRANS_SUB] in clude  X  X r, X   X  X ven though, X   X  X lthough, X  X nd so on.. 4.2 Automatic SL Extraction The reason for automatic SLs extraction is to improve system portability. Since we assumed that the emotion reaction to the same sentences depends on personal status, the emotion classification model must be retrained for a new personal status. The emotional corpus must be re-collected and re-annot ated. Due to the subj ective presumption, annotating all of the appropriate SLs for the corpus is time-consuming and always imprecise. sentence is input, the negative SLs and transitive SLs are first converted by simply comparing the predefin ed words. Then, the hierarchic al hypernym structure of the remaining words in the sentence is checked to find appropriate active SLs. When a word process. 5. GENERATING RULES FOR EMOTION ASSOCIATION So far we have defined the SLs that represent the domain-independent components of EGRs which can be extracted automatically. To determine the domain-dependent components in EGRs, the emotion association rule (EAR) training process is described in converted into their corresponding ATTributes (ATTs), which are defined in the lexical ontology for concept representation. The rule -generation algorithm is then applied to mine the association between the SL and ATT sets with high co-occurrence. The uncorrelated ATTs are discarded. The a priori algorithm is adopted as the rule-generation algorithm, which is generally used to mine association rules from raw data. The following sections introduce the a priori algorithm and the EARs  X  generation process. 5.1 The A Priori Algorithm from transaction data [Han and Kamber 2001]. The transaction data is a list of item-sets. Two important parameters are introduced firs t: support and confidence. Calculating these two parameters is described in Eqs.( 1) and (2). number of transactions. The union of A and B , which is also an item-set, is called  X  X arge item-set X  if both Sup ( A,B ) and Conf ( A  X  B ) are larger than the predefined thresholds. resulting association rules. The pruning procedure is described in the following example. appears, then the item i 3 will appear , X  respectively. It is obvious that the first rule can be pruned because it is a subset of the second rule. quality of the association rules. However, in our case, an additional problem occurs when a word is converted into a set of ATTs: the ATT sets will appear in all sentences, and the value of Sup and Conf of these ATT sets are always larg er than the predefined threshold for  X  X arge item-set. X  That means the ATT set for a word will always be an EAR. Because they are extracted from one word, to extract th ese kinds of rules is meaningless. To solve this problem, a new restriction is added to th e original pruning procedure: all large item-sets containing items that can be converted fr om the same word will also be pruned. This new restriction is tested at each step to ensure that there are no redundant large item-sets. 5.2 Emotion Association Rule Generator The emotion association Rrules (EARs) used to represent the association between SLs and ATTs for a specific emotional state are derived from the training text corpus via the a priori algorithm. The process for deriving the EARs follows: Step 1. Sentence preprocessing: Step 2. SL extraction: Step 3. SL and ATT set generation: Step 4. EARs derivation: The following example describes the steps in the extraction process: Input sentence Step 1. Sentence preprocessing Step 2. SLs extraction Step 3. Generation of SL and ATT sets Step 4. EARs derivation (some examples) In this example the input senten ce is preprocessed first. In st ep 2, the appropriate SLs are extracted, including active SLs, negative SLs, and transaction SLs. When deciding the appropriate active SLs for the input sentence, we first determine the POS of the phrases. For each possible word, the corr esponding ATT is compared with the SLs. If a word matches more than one SL, all the SLs will be kept for further processing. In this definitions of the SLs [OBTAIN] and [REACH ], respectively. Therefore, two sentences emphasize the sentence after it, the two ac tive SLs [OBTAIN] and [REACH] are then filtered out. In this step all of the phrases are co nverted into their corresponding SLs. In step 3, the SL and ATT sets are constructed by converting the remaining words into their ATTs. Furthermore, the phrase  X  100 dollars  X  is tagged as [NUM] instead of the original ATTs. The final result is shown in step 3, and is used for further processing in step 4. The EARs are then deduced using the a priori algorithm. 6. THE SEPARABLE MIXTURE MODEL input sentence, that is, to identify the simila rity between the input sentence and the EARs of a specific emotional state represented by SLs and ATTs. Because each input sentence may match more than one EAR for different emotional states, the separable mixture model (SMM) [Hofmann and Puzicha 1998] is appropriate for the cl assification process and for emotion recognition. Given the input sentence s , the corresponding emotional state is decided by maximizing the conditional probability of each emotional state e k . According to the assumption that the emoti onal state of an input sentence can be determined using the EAR with respect to the input sentence, this conditional probability can be expanded as previous sections, the probability P ( EAR r | s ) is defined as 1 if EAR r can be derived from the input sentence s. Otherwise, P ( EAR r | s ) is set to 0. and ATTs in this EAR. The conditional probability can be further expanded according to the Bayes X  rule: an emotional state given by an obvious EAR tends to be smaller than the actual separable mixture model is adopted. EAR is made up of several SLs and ATTs, the appearance of EAR r means the By assuming that the appearance of the SLs and ATTs is conditionally independent, SMM can be adopted to fit our problem. Therefore, the distribution of EAR r is the summation of P ( EAR r | e k ). The parameters of this distribution can be estimated using the EM algorithm [McLachlan and Krishnan 1997]. During th e E-step, the expectation of P ( EAR r ) is where  X  r is the number of EAR r in the corpus. A new variable R rk is then introduced as a Boolean variable; { } 0,1 state e k . By involving this parameter, the expectation in Eq. (4) is simplified to With a similar assumption of SMM in Hofmann and Puzicha [1998], the representation and normalization constraint of the introduced parameter R rc is shown in Eqs. (5) and (6), respectively: Due to this constraint, the Lagrange multiplier is used herein to obtain the maximum partial differentiation. In the following formul as, we take the partial differentiation of  X  x as an example. In this equation, the term  X  X   X  X  X   X  X  X   X  X  X   X  X   X  X  X   X  X  X  X  X  X   X  X  X   X  X  X   X  X  X   X  X  X  The previous two equations represent the result for the kx = and kx  X  cases. The result is differentiation result is obtained. the following three simultaneous linear equations. The EAR distribution can be iteratively estima ted and the appropriat e emotional state for a given input sentence e* can be determined using Eq. (3). In practice, an additional value  X  is experimentally defined, and all se ntences with the SMM likelihood less than  X  will be rejected. 7. EXPERIMENTAL RESULTS Because ATTs are defined as domain-depend ent, we consider a limited application domain for performance evaluation. A dialog system focusing on the students X  daily expressions was constructed, and only three emotional states, HAPPY, UNHAPPY, and NEUTRAL, were considered. To evaluate the proposed approach, four experiments were conducted. Experiment 1 was to determine the best parameter combination for the entire system. In experiment 2 the parameter set was to test the accuracy of emotion recognition. In experiment 3 we ported the system into another domain and evaluated recognition accuracy. Comparisons to other approaches were investigated in experiment 4. of 26 volunteer college students. To counterbalance day-to-day variations, we continually question sets; each question focused on one topi c in the student X  X  daily life. According to the previous training for emotional representation, the students were further divided into two groups. In Group 1 with 6 students, each student recorded 40 dialogs for each emotion. For each of the remaining 20 stude nts, 10 dialogs were recorded for each emotion. Corpus B contains the dialogs from a broadcast drama. In both corpora, we annotated happy, unhappy, and neutral emoti onal states for each sentence. All sentences of different performers were extracted and the emotions of the performers were manually tagged with one of the above three emotional states. Only the protagonists X  dialogs were preserved in the corpus. The sentences with th ree words or less were also discarded. The content of these two corpora ar e shown in Tables III and IV. 7.1 Determining Parameter Combinations In the first experiment, corpus A-1 was used to estimate the best parameter combination. From the description in previous sections, the emotion recognition process includes SL extraction, EAR generation, and SMM construction. The SL extraction procedure is based on predefined knowledge and has no parameter for tuning. The sentences that cannot complete the transformation are marked as neutral emotion. Table V shows the results of SL and ATT set transformations and the recall rates of neutral emotions. In the EAR generation procedure, the parameters include Sup (support) and Conf (confidence). In the SMM construction procedure, we conducted the experiment using an additional parameter,  X  . The parameter  X  is a threshold for SMM in which all sentences with SMM directly affect the performance of the recognition system. Hence we cannot independently modify any one of them. and 2, respectively. From these figures, we fi nd that the best results are obtained when  X  = 0.1, Sup = 1, and Conf = 5. 7.2 The Accuracy of Emotion Recognition Following the previous parameter combination and recognition process, corpus A is used the precision, recall, and rejection rates for the three groups of emotions in Table VII. of 75.90%. The rejection rate is 5% on av erage, except for neutral emotions. The reason for the high rejection rate for the neutral emotio ns is that we tend to extract as many SLs ATTs. However, in the next step, with no SLs necessary, many SL and ATT sets were rejected using the threshold. 65.67%, which shows that the proposed approach is satisfactory for a specific domain. 7.3 Portability Evaluation To evaluate the portability of the system, corp us B was used in this experiment. Because corpus B was collected from a broadcast dr ama performed by professional actors, the emotion reaction in this corpus was more realis tic than that of corpus A. Before porting the original system into this new corpus, the only work was annotating the emotion reaction in the sentences. Our approach assu med that only the primary emotional states, such as happy or unhappy, were necessary for annotation. corpus B could be converted into the SL and ATT sets. The final precision rate was 61.18% under a recall rate of 45.16%. Because the broadcast drama was not a domain-specific corpus, the proposed approach could not be applied directly very well. However, in another simple experiment, when we fo cused only on one actor, the precision rate reached 70%. 7.4 Comparison to Other Systems Several emotion recognition systems were introduced in the previous sections. In this experiment, two different approaches, semantic network and emotional keywords, are compared. In approach 1, the details for co nstructing the semantic network are described considering its syntactic structure and the relationships between lexicons. In approach 2, the emotional keywords are selected manually from the universal lexical ontology. The relationships between the keywords and emotional groups are then calculated statistically. Using both corpora A and B, Table XII shows the average accu racies for the two systems. In the inside test, using a small and specific domain corpus, the keyword-based system is better than the other two. However, when ported into other domains, the other two systems achieved better performance. Generally speaking, the result from the proposed psychology and the separable mixture model. 8. CONCLUSION An automatic emotion recognition approach from text was presented in this article. By applying a universal lexical ontology and research on the psychology of emotion, we defined the EGRs and SLs to automatically extract semantic information from input sentences; the SLs and ATTs represent impl icit semantic information in EGRs. The sentences were converted into th e SL and ATT sequences, and the a priori algorithm was applied to determine the EARs. To identify the relationships between new sentences and EARs, a separable mixture model was adopted to estimate the probability of the emotional state given in the input sentence. The experimental results show that the proposed approach achieves better performance at emotion recognition in a dialog than the other systems we measured. With the combination of semantic information and adopted only happy, unhappy, and neutral emotional states for evaluation. Our future the universal lexical ontology will be a great help in extracting semantic information. REFERENCES 
