 Jun Sakuma jun@fe.dis.titech.ac.jp Shigenobu Kobayashi kobayasi@dis.titech.ac.jp Rebecca N. Wright rebecca.wright@rutgers.edu With the rapid growth of computer networks and net-worked computing, a large amount of information is being sensed and gathered by distributed agents phys-ically or virtually. Distributed reinforcement learning (DRL) has been studied as an approach to learn a con-trol policy thorough interactions between distributed agents and environments X  X or example, sensor net-works and mobile robots. DRL algorithms, such as the distributed value function approach (Schneider et al., 1999) and the policy gradient approach (Moallemi &amp; Roy, 2004), typically seek to satisfy two types of physi-cal constraints. One is constraints on communication, such as an unstable network environment or limited communication channels. The other is memory con-straints to manage the huge state/action space. There-fore, the main emphasis of DRL has been to learn good, but sub-optimal, policies with minimal or lim-ited sharing of agents X  perceptions.
 In this paper, we consider the privacy of agents X  per-ceptions in DRL. Specifically, we provide solutions for privacy-preserving reinfo rcement learning (PPRL), in which agents X  perceptions, such as states, rewards, and actions, are not only distributed but are desired to be kept private. Consider two example scenarios: Optimized Marketing (Abe et al., 2004): Consider the modeling of the customer X  X  purchase behavior as a Markov Decision Process (MDP). The goal is to ob-tain the optimal catalog mailing strategy which max-imizes the long-term profit. Timestamped histories of customer status and mailing records are used as state variables. Their purchase patterns are used as actions. Value functions are learned from these records to learn the optimal policy. If these histories are managed sep-arately by two or more enterprises, they may not want to share their histories for privacy reasons (for exam-ple, in keeping with privacy promises made to their customers), but might still like to learn a value func-tion from their joint data in order that they can all maximize their profits.
 Load Balancing (Cogill et al., 2006): Consider a load balancing among competing factories. Each factory wants to accept customer jobs, but in order to max-imize its own profit, may need to redirect jobs when heavily loaded. Each factory can observe its own back-log, but factories do not want to share their backlog information with each other for business reasons, but they would still like to make optimal decisions. Privacy constraints prevent the data from being com-bined in a single location wh ere centralized reinforce-ment algorithms (CRL) could be applied. Although DRL algorithms work in a distributed setting, they are designed to limit the total amount of data sent be-tween agents, but do not necessarily do so in a way that guarantees privacy preservation. Additionally, DRL often sacrifices optima lity in order to learn with low communication. In contrast, we propose solutions that employ cryptographic techniques to achieve op-timal policies (as would be learned if all the informa-tion were combined into a centralized reinforcement learning (CRL) problem) while also explicitly protect-ing the agents X  private information. We describe solu-tions both for data that is  X  X artitioned-by-time X  (as in the optimized marketing example) and  X  X artitioned-by-observation X  (as in the load balancing example). Related Work. Private distributed protocols have been considered extensivel y for data mining, pioneered by Lindell and Pinkas (Lindell &amp; Pinkas, 2002), who presented a privacy-preserving data-mining algorithm for ID3 decision-tree learning. Private distributed pro-tocols have also been proposed for other data min-ing and machine learning problems, including k -means clustering (Jagannathan &amp; Wright, 2005; Sakuma &amp; Kobayashi, 2008), support vector machines (Yu et al., 2006), boosting (Gambs et al., 2007), and belief prop-agation (Kearns et al., 2007).
 Agent privacy in reinforcement learning has been pre-viously considered by Zhang and Makedon (Zhang &amp; Makedon, 2005). Their solution uses a form of average reward reinforcement learning that does not necessar-ily guarantee an optimal solution; further, their solu-tion only applies partitioning by time. In contrast, our solutions guarantee optimality under appropriate con-ditions and we provide solutions both when the data is partitioned by time and by observation.
 In principle, private distributed computations such as these can be carried out using secure function evalu-ation (SFE) (Yao, 1986; Goldreich, 2004), which is a general and well studied methodology for evaluating any function privately. However, although asymptot-ically polynomially bounded, these computations can be too inefficient for practical use, particular when the input size is large. For the reinforcement learning al-gorithms we address, we make use of existing SFE so-lutions for small portions of our computation in order as part of a more efficient overall solution. Our Contribution. We introduce the concepts of partitioning by time and partitioning by observation in distributed reinforcemen t learning (Section 2). We show privacy-preserving solutions for SARSA learn-ing algorithms with random action selection for both kinds of partitioning (Section 4). Additionally, these algorithms are expanded to Q -learning with greedy or -greedy action selection ( Section 5). We provide ex-perimental results in Section 6.
 Table 1 provides a qualitative comparison of vari-ants of reinforcement learn ing in terms of efficiency, learning accuracy, and privacy loss. We compare five approaches: CRL, DRL, independent distributed re-inforcement learning (IDRL, explained below), SFE, and our privacy-preserving reinforcement learning so-lutions (PPRL). In CRL, all the agents send their per-ceptions to a designated age nt, and then a centralized reinforcement is applied. In this case, the optimal con-vergence of value functions is theoretically guaranteed when the dynamics of environments follow a discrete MDP; however, privacy is not provided, as all the data must be shared.
 On the opposite end of the spectrum, in IDRL (inde-pendent DRL), each agent independently applies CRL only using its own local information; no information is shared. In this case, privac y is completely preserved, but the learning results will be different and indepen-dent. In particular, accu racy will be unacceptable if the agents have incomplete but important perceptions about the environment. DRL can be viewed as an in-termediate approach betw een CRL and IDRL, in that the parties share only some information and accord-ingly reap only some gains in accuracy.
 The table also includes the direct use of general SFE and our approach of PPRL. Both PPRL and SFE ob-tain good privacy and good accuracy. Although our solution incurs a significant cost (as compared to CRL, IDRL, and DRL) in computation and communication to obtain this, it does so with significantly improved computational efficiency over SFE. We provide a more detailed comparison of the privacy, accuracy, and effi-ciency of our approach and other possible approaches along with our experimental results in Section 6. 2.1. Reinforcement Learning and MDP Let S be a finite state set and A be a finite action set . A policy  X  is a mapping from state/action pair ( s, a ) to the probability  X  ( s, a ) with which action a is taken at state s .Attimestep t ,wedenoteby s t , a t ,and r t , the state, action, and reward at time t , respectively. A Q -function is the expected return Q  X  ( s, a )= E count factor (0  X   X &lt; 1). The goal is to learn the op-timal policy  X  maximizing the Q -function: Q  X  ( s, a )= max  X  Q ( s, a ) for all ( s, a ). In SARSA learning, Q -values are updated at each step as:  X  Q ( s t ,a t )  X   X  ( r t +  X Q ( s t +1 ,a t +1 )  X  Q ( s where  X  is the learning rate. Q -learning is obtained by replacing the update of  X  Q by:  X  Q ( s t ,a t )  X   X  ( r t +  X  max Iterating these updates under appropriate conditions, optimal convergence of Q -valuesisguaranteedwith probability 1 in discrete MDPs (Sutton &amp; Barto, 1998; Watkins, 1989); the resulting optimal policy can be readily obtained. 2.2. Modeling Private Information in DRL Let h t =( s t ,a t ,r t ,s t +1 ,a t +1 ), let H = { h t pose there are m agents. We consider two kinds of partitioning of H (see Fig. 1).
 Partitioned-by-Time. This model assumes that only one agents interacts with the environment at any time step t .Let T i be the set of time steps at which only i th agent has interactions with the envi-ronment. Then T i  X  T j =  X  , ( i = j )andtheset H i = { h t | t  X  T i } is considered the private infor-mation of the i th agent.
 Partitioned-by-Observation. This model assumes that states and actions are represented as a collection of state and action variables. The state space and the action space are S = i S i and A = i A i where S i and A i are the space of the i th agent X  X  state and action variables, respectively. Without loss of gener-ality (and for notational simplicity), we consider each agent X  X  local state and actio n spaces to consist of a sin-gle variable. If s t  X  S is the joint state of the agents at time t ,wedenoteby s i t the state that i th agent per-ceives and by a i t the action of i th agent. Let r i t be the local reward of i th agent obtained at time t . We define the global reward (or reward for short) as r t = i r i t in this model. Our Q -functions are evaluated based on this global reword. The perception of the i th agent at time t is denoted as h i t = { s i t ,a i t ,r i t ,s i t +1 private information of the i th agent is H i = { h i t } . We note that partitioning by observation is more gen-eral than partitioning by time, in that one can always represent a sequence that is partitioned by time by one that is partitioned by observation. However, we provide more efficient solutions in simpler case of par-titioning by time.
 Let  X  c be a policy learned by CRL. Then, informally, the objective of PPRL is stated as follows: Statement 1. The i th agent takes H i as inputs. Af-ter the execution of PPRL, all agents learn a policy  X  which is equivalent to  X  c . Furthermore, no agent can learn anything that cannot be inferred from  X  and its own private input.
 This problem statement can be formalized as in SFE (Goldreich, 2004). This is a strong privacy re-quirement which precludes consideration of solutions that reveal intermediate Q -values, actions taken, or states visited. We assume our agents behave semi-honestly , a common assumption in SFE X  X his assumes agents follows their specified protocol properly, but might also use their records of intermediate computa-tions in order to attempt to learn other parties X  private information. Our solutions make use of several existing crypto-graphic tools. Specifically, in our protocol, Q -values are encrypted by an additive homomorphic cryptosys-tem, which allows the addition of encrypted values without requiring their decryption, as described in Sec-tion 3.1. Using the homomorphic properties, this al-lows encrypted Q -values are updated in the regular RL manner, while unencrypted Q -values are not known to agents. For computations which cannot be treated by the homomorphic property, we use SFE as a primitive, as we describe in Section 3.2. 3.1. Homomorphic Public Key Cryptosystems In a public key cryptosystem, encryption uses a public key that can be known to everyone, while decryption requires knowledge of the corresponding private key . Given a corresponding pair of ( sk , pk )ofprivateand public keys and a message m ,then c = e pk ( m ; )de-notes a (random) encryption of m ,and m = d sk ( c ) denotes decryption. The encrypted value c uniformly distributes over Z N if is taken from Z N randomly. An additive homomorphic cryptosystem allows addi-tion computations on encrypted values without knowl-edge of the secret key. Specifically, there is some op-eration  X  (not requiring knowledge of sk ) such that for any plaintexts m 1 and m 2 , e pk ( m 1 + m 2 ; )= e pk ( m 1 ; 1 )  X  e pk ( m 2 ; 2 ) , where is uniformly random provided that at least one of 1 and 2 is. Based on this property, it also follows that given a constant k and the encryption e pk ( m 1 ; ), we can compute multiplications by k via repeated application of  X  . This also enables a re-randomization property, which allows the computa-tion of a new random encryption c = e pk ( m ; )of m from an existing encryption c = e pk ( m ; )of m , again without knowledge of the private key or of m ,asfol-lows: e pk ( m ; )=Enc pk ( m ; 1 )  X  Enc pk (0; 2 ). In the rest of the paper, we omit the random number from our encryptions for simplicity.
 In an ( m, t ) -threshold cryptosystem , m agents share a common public key pk while the agents hold differ-ent private keys sk 1 , ..., sk n . Each agent can encrypt any message with the common public key. Decryption cannot be performed by fewer than t agents, and can be performed by any group of at least t agents us-ing a recovery algorithm based on the public key and a cryptosystem that provides semantic security (un-der appropriate computational hardness assumptions), re-randomization, the additive homomorphic property, and threshold decryption, such as the generalized Pail-lier cryptosystem (D  X  amgard &amp; Jurik, 2001). 3.2. Private Comparison and Division As mentioned, secure function evaluation (SFE) is a cryptographic primitive which allows two or more par-ties to evaluate a specified function of their inputs without revealing (anything else about) their inputs to each other (Goldreich, 2004; Yao, 1986). Although our overall solution is more efficient than using SFE, we do make use of SFE for two kinds of computations. One is the problem of private comparison of random shares .Let x =( x 1 , ..., x d )  X  Z d N . For our purposes, A and B have random shares of x if A has x A = ( x 1 , ..., x x i and x x =( x A i + x B i )mod N for all i .If A holds x A and B holds x B ,where x A and x B are random shares of x , then private comparison of random shares computes the index i  X  such that i  X  =argmax i ( x A i + x B i )insuch awaythat A learns only i  X  and B learns nothing. The other is a problem of private division of random shares . The input of A and B are random shares of x , x A  X  Z N and x B  X  Z N , respectively. Let K be an integer known to both parties. Then, private division of random shares computes random shares Q A and Q
B of quotient Q  X  Z N such that x =( QK + R ) mod N ,where R  X  Z N (0  X  R&lt;K ) ,Q =( Q A + Q B ) mod N . After the protocol, A and B learn Q A and Q B , respectively, and nothing else.
 We use private division of random shares in several places in our protocols to achieve private division of encrypted values . Suppose agent A has a key pair ( pk , sk ) and agent B knows pk and e pk ( x ). The follow-ing protocol allows B to learn the encrypted quotient e pk ( Q )from e pk ( x )and K : In this section, we describe privacy-preserving SARSA update of Q -values under random action selection is described for our two partitioning models. We ex-tend this to ( -)greedy action sel ection in Section 5. We assume that reward r ,learningrate  X  , and dis-count rate  X  are non-negative rational numbers and that  X  t =1 (  X  t Lr max ) &lt;N ,where r max is the largest reward that agents can obtain and L  X  Z N is a param-eter defined in Section 4.1. I n this paper, we describe protocols for two agents; these can be extended to m -agent case ( m  X  3) straightforwardly, as will be shown in an extended version of the paper. 4.1. Partitioned-by-Time Model We first restrict our attention to the case where agent A has perceptions during T A = { 1 , ..., t  X  1 } and B has perceptions during T B = { t } . In this setting, A first computes can l earn intermediate Q -values during the time period T A , because they can be locally com-puted only from A  X  X  perception. At time t , the new Q -values must be computed based on the intermediate Q -values known to A and B  X  X  observation at time t . In brief, we do this by carrying out the update on en-crypted Q -values using the homomorphic property to carry this out privately. However, the update includes the multiplication of rational numbers, such as  X  or  X  , so the computation is not closed in Z N . Hence, we first scale these rational numbers by multiplying with large enough integers so that all computations are closed in Z
N . We use private division of encrypted values to remove the scaling.
 We now describe our protocol for private update, shown in Fig. 2, in more detail. Let pk A be A X  X  pub-lic key. At step 1, A computes c ( s, a )= e pk A ( Q ( s, a )) for all ( s, a ) and sends them to B . B takes action a t gets r t ,s t +1 (step 2), and chooses a t +1 randomly (step 3). A and B must now update the encrypted Q -value c ( s, a ). By encrypting both sides of SARSA update (eq. 1), we obtain: where  X  c ( s t ,a t )= e pk A ( X  Q ( s t ,a t )). If  X  c ( s computed by B from what B observes, B can update c ( s t ,a t ) by eq. 2 locally. Therefore, step 4 is devoted to the computation of  X  c ( s t ,a t ).
 As mentioned, large integers K and L are used to treat the multiplication of rationals  X  and  X  ,where  X  X K  X  Z
N and Lr t sides of eq. 1 and multiplying L to r t ,weobtain K  X  Q ( s t ,a t )  X  K X  ( Lr t +  X Q ( s t +1 ,a t +1 )  X  Q ( s in which the computation is closed in Z N . Encrypting both sides by A  X  X  public key, we obtain Since K, L,  X ,  X  are public and B has r t , c ( s, a ), B can compute e pk A ( K  X  Q ( s t ,a t )) by eq. 3 (step 4(a)). B needs to divide e pk A ( K  X  Q ( s t ,a t )) by K , however, division is again not allowable. Instead, a quotient  X  Q ( s t ,a t ) satisfying  X  Q ( s t ,a t )= K  X  Q ( s t R (0  X  R&lt;K ) is computed by private encrypted division and B obtains e pk A ( X  Q ( s t ,a t )) (step 4(b)). Then, B finally computes It follows that eq. 4 is equivalent to eq. 2 except for the truncation error included by the private encrypted division step (step 4(c)). This truncation is negligibly small if L is sufficiently large.
 Lemma 1. If A and B behave semi-honestly, then af-ter the private update of Q -values for SARSA and ran-dom action selection in partitioned-by-time model, B correctly updates encrypted Q -values but learn nothing else. A learns nothing.
 The proof of this lemma (omitted for space) follows the standardized proof methodology of secure multi-party computation (Goldreich, 2004), showing that one can create the required a lgorithms, called simulators ,for A and B . Intuitively, Step 4(b) is secure because it is implemented by SFE. Everything else that B receives except for messages received at steps for step 4(b) are encrypted by A  X  X  public key, so do not reveal anything. A does not receive anything except messages that are part of the SFE in step 4(b), so does not learn any-thing. Thus, the protocol is secure overall.
 For the general setting of T A and T B ,aftertime t ,if B interacts with the environment at time t + 1 again, the protocol can be started from step 2. When interaction switches back to A , an SFE step is used to change the encryption of the Q -values from A  X  X  private key to B  X  X  private key via an SFE step, and then the roles of A and B are switched. 4.2. Partitioned-by-Observation Model In this model, we use a (2 , 2)-threshold cr yptosystem. Both parties share a common public key pk : encryp-tion of m by pk is denoted by e ( m ) in this section. A and B hold different secret keys sk A and sk B for de-cryption shares, respectively. A and B cannot decrypt without both cooperating.
 In this partitioning model, we write a t =( a A t ,a B t s t =( s A t ,s B t ), and r t = r A t + r B t . A receives only ( s update of Q -values in this model is shown in Fig. 3. In this model, eq. 3 is rewritten as X
A and X B can be computed by A and B .Toobtain tables where h  X  S A ,i  X  S B ,j  X  A A ,k  X  A B .At step 4(a), A sends X A and tables { c ik } , { c ik } with re-randomization such that c ik = c ( s A t ,i,a A t ,k ) c ik = c ( s to B . B determines c ( s t ,a t )= c s B c 4(b)). Then computes e ( X  Q ( s t ,a t )) by private divi-sion (step 4(c)). For all ( hijk ), B sets and sends {  X  c hijk } to A (step 4(d)). Finally, for all ( ik ), Q -values are updated as by A .Withthisupdate, e ( X  Q ( s t ,a t )) is added e (0) is added. Note that A cannot tell which ele-ment is e ( X  Q ( s t ,a t )) in {  X  c hijk } because of the re-randomization. Thus, eq. 9 is the desired update. Lemma 2. If A and B behave semi-honestly, then after the private update of Q -values for SARSA and random action selection in partitioned-by-observation model, A updates encrypted Q -values correctly but learns nothing. B learns nothing.
 By iterating private updates, encrypted Q -values trained by SARSA learning are obtained. Private distributed algorit hms for greedy action selec-tion to compute a  X  =argmax a Q ( s, a ) from encrypted Q -values in both partitioning models are described. These are used for: (1) ( -)greedy action selection, (2) max operation in updates of Q -learning, and (3) ex-tracting learned policies from final Q -values. In the partitioned-by-time model, this is readily solved by us-ing private comparison, so is omitted. 5.1. Private Greedy Action Selection in When A and B observe s A t and s B t , respec-tively, private greedy actio n selection requires that (1) A obtains a A  X  and nothing else, (2) B ob-tains a B  X  and nothing else, where ( a A  X  ,a B  X  )= The protocol is described in Fig. 4. Threshold de-cryption is used here, too. First, A sends encrypted Q -values c ( s A t ,i,j,k ) with re-randomization for all ( i, j, k ). For all ( i, k ), B generates and sends a table { c where  X  : S B  X  S B is a random permutation and Q B i X  ( k )  X  r Z N . At the third step, A recov-dom shares of Q ( s A t ,i,s B t , X  ( k )), the values ( i arg max ( i,k ) ( Q A ik + Q B ik )areobtainedby A using private comparison. Finally, B learns a B  X  =  X   X  1 ( k  X  ), where  X   X  1 is the inverse of  X  .
 Lemma 3. If A and B behaves semi-honestly, then, after the execution of private greedy action selection, A learns a A  X  and nothing else. B learns a B  X  and nothing else.
 Note that a B  X  is not learned by A because index k is obscured by the random permutation generated by B . 5.2. Security of PPRL Privacy-preserving SARSA l earning is constructed by alternate iterations of private update and random ac-tion selection. The policy  X  can be extracted by computing arg max a Q ( s, a ) for all ( s, a ) using private greedy action selection. Th e security follows from the earlier lemmas: Theorem 1. SARSA learning with private update of Q -values and random action selection is secure in the sense of Statement 1.
 Privacy-preserving SARSA learning and Q -learning with ( -)greedy action selectio n can be constructed by combining private update and private greedy random action selection. However, these PPRLs do not follow Statement 1 because it does not allow agents to know greedy actions obtained in the middle of the learning. Therefore, the problem defini tion is relaxed as follows: Statement 2. The i th agent takes H i as inputs. Af-ter the execution of PPRL, all agents learn a series of greedy actions during learning steps and a policy  X  which is equivalent to  X  c . Furthermore, no agent learns anything else.
 Theorem 2. SARSA and Q -learning with private up-date of Q -values and private greedy/ -greedy action se-lectionissecureinthesenseofStatement2. We performed experiments to examine the efficiency of PPRL. Programs were written in Java 1.5.0. As the cryptosystem, (D  X  amgard &amp; Jurik, 2001) with 1024-bit keys was used. For SFE, Fairplay (Malkhi et al., 2004) was used. Experiments were carried out under Linux with 1.2 GHz CPU and 2GB RAM. 6.1. Random Walk Task This random walk task is partitioned by time. The state space is S = { s 1 , ..., s n } ( n = 40) and the action space is A = { a 1 ,a 2 } . The initial and goal states are s and s n , respectively. When a 1 is taken at s p ( p = n ), the agent moves to s p +1 .When a 2 is taken at s p ( p = 1), the agent moves to s p  X  1 , but the agent does not move when p = 1. A reward r = 1 is given only when the agent takes a 1 at s n  X  1 ;else, r =0. Theepisodeis terminated at s n or after 1 , 000 steps.
 A learns 15 , 000 steps and then B learns 15 , 000 steps. CRL, IDRL, PPRL, and SFE were compared. SARSA learning with random or -greedy action selection was used for all settings. Table 2 shows the comparison re-sults of computational cost, learning accuracy (number of steps to reach the goal state, averaged over 30 trials, and number of trials that successfully reach the goal state), and privacy preservation.
 Learning accuracy of PPRL and SFE are the same as CRL because the policy learned by PPRL and SFE are guaranteed to be equal to the one learned by CRL. In contrast, the optimal policy is not obtained suc-cessfully by IDRL because learning steps for IDRL agents correspond to the half of others. Because most of the computation time is spent for private division and comparison, computation time with random se-lection is much smaller than with -greedy selection. These experiments demonstrate that PPRL obtains good learning accuracy, while IDRL does not, though computation time is larger than DRL and IDRL. 6.2. Load Balancing Task In these experiments, we consider a load balancing problem (Cogill et al., 2006) in the partitioned-by-observation model with two factories A and B .Each factory can observe its own backlog s A ,s B  X  X  0 , ..., 5 At each time step, each factory decides whether or not to pass a job to other factories; the action vari-able is a A ,a B  X  X  0 , 1 } . Jobs arrive and are pro-cessed independently at each time step with probabil-ity 0 . 4and0 . 48, respectively. Agent A receives reward r
A =50  X  ( s A ) 2 .If A passes the job to B ,then A  X  X  reward is reduced by 2 as a cost for redirection. If an overflow happens, the job is lost and r A =0is given. Similarly, r B is computed as well. Perceptions ( s
A ,a A ,r A )and( s B ,a B ,r B ) are to be kept private. (In this task, actions cannot be kept private because the parties learn them from whether the job was passed or not.) Distributed reward DRL (RDRL) (Schneider et al., 1999) is tested in addition to the four RLs tested earlier. RDRL is a variant of DRL, which is the same with IDRL except that global rewards are shared among distributed agents (Schneider et al., 1999). SARSA/ -greedy action selectio n was used in all set-tings. Fig 5 shows the changes of sum of global rewards per episode. For avoiding overflows, cooperation be-tween agents is essential in this task. The performance of IDRL agents is inferior to others because selfish be-havior is learned. In contrast, CRL, PPRL and SFE agents successfully obtain cooperative behavior. The performance of RDRL is int ermediate because percep-tions of RDRL agents are limited. Efficiency is shown in Table 3. Since -greedy action selection was used, the privacy of IDRL, PPRL and SFE follow Statement 2. The privacy preservation of RDRL is between CRL and PPRL. As discussed in Section 1, PPRL achieves both the guarantee of privacy preservation and the op-timality which is equivalent to that of CRL; SFE does the same, but at a much higher computational time. This work was started at Tokyo Institute of Technol-ogy and carried out partly while the first author was a visitor of the DIMACS Center. The third author is partially supported by the National Science Founda-tion under grant number CNS-0822269.

