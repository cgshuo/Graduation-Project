 Controlled experiments are widely regarded as the most sci-entific way to establish a true causal relationship between product changes and their impact on business metrics. Many technology companies rely on such experiments as their main data-driven decision-making tool. The sensitivity of a con-trolled experiment refers to its ability to detect differences in business metrics due to product changes. At Netflix, with tens of millions of users, increasing the sensitivity of con-trolled experiments is critical as failure to detect a small effect, either positive or negative, can have a substantial rev-enue impact. This paper focuses on methods to increase sen-sitivity by reducing the sampling variance of business met-rics. We define Netflix business metrics and share context around the critical need for improved sensitivity. We re-view popular variance reduction techniques that are broadly applicable to any type of controlled experiment and met-ric. We describe an innovative implementation of strat-ified sampling at Netflix where users are assigned to ex-periments in real time and discuss some surprising chal-lenges with the implementation. We conduct case studies to compare these variance reduction techniques on a few Netflix datasets. Based on the empirical results, we recom-mend to use post-assignment variance reduction techniques such as post stratification [7] and CUPED [3] instead of at-assignment variance reduction techniques such as stratified sampling [2] in large-scale controlled experiments. Controlled experiment; Variance reduction; A/B testing; Randomized experiment; Sensitivity
Controlled experiments are key for data-driven decisions in many technology companies. Running controlled experi-ments that are not sensitive enough to differences in business metrics caused by product changes can lead to suboptimal decisions with large revenue impact for companies like Net-flix.
 There are three ways to improve the sensitivity of controlled experiments: increasing sample sizes in the experiments, de-signing product changes that lead to large differences in busi-ness metrics, and reducing the sampling variance of business metrics.
 The simplest way to increase sensitivity is to increase sam-ple sizes. While the Netflix user base is very large, this option is not always practical. Many experimental product features affect only a small proportion of the user base, e.g., testing a new kids search experience on Android tablets has a relatively small audience, which limits the sample sizes. Moreover, while Netflix runs over a thousand experiments per year, there is always a desire to increase the pace of innovation by scaling up the number of experiments. With some experiments colliding with one another, available users for each experiment can become scarce. For these reasons, increasing the number of users assigned to experiments is often not feasible.
 Two other avenues to improve the sensitivity of controlled experiments are explored in parallel at Netflix. Product managers lead cross-team efforts to focus on bold product changes that can lead to large positive differences in business metrics, while the experimentation team constantly seeks new experimentation methodologies to reduce the sampling variance of our business metrics.
 This paper compares a few variance reduction techniques both theoretically and empirically based on a few Netflix datasets and provides guidance to experimenters on the choice of variance reduction techniques. Our primary contribu-tions are threefold. First, we review the theory of three variance reduction techniques: stratified sampling [2], post stratification [7], and CUPED [3] and establish theoretical connections between them. Second, we describe an inno-vative implementation of stratified sampling at Netflix that addresses the challenges posed by assigning users to experi-ments in real time. Third, we conduct an empirical evalua-tion of these variance reduction techniques on a few Netflix datasets and compare their amount of variance reduction relative to simple random sampling.
Netflix has a data-driven decision-making culture. We have learned through years of experimentation that using subjective intuition, even in a collective way, to make prod-uct decisions often yields the wrong answer. One way to make product decisions is to hear what users have to say. B ut what users ask for and what actually works are very dif-ferent. Running controlled experiments and making product decisions based on business metrics is the best way to bridge this gap. Business metrics should be chosen such that im-proving them is highly related to increasing the value users get from the Netflix service. See [6] for some examples where actual experiment results do not agree with subjective intu-ition in the movie/TV recommendation algorithm area and a more complete description of how controlled experiments are used to improve our movie/TV recommendation algo-rithms.
 At Netflix, controlled experiments are leveraged in many different product areas such as movie/TV recommendation algorithms, user interface design, and messaging. Different product experiences in experiments are referred to as cells . In each experiment, we are typically interested in comparing various new experience(s), referred to as test cell(s) , with the current production experience, referred to as the control cell . For example, in a controlled experiment in the movie/TV recommendation algorithm area, the control cell maps to the current production algorithm and the test cell(s) map to new algorithm(s) we want to compare with the produc-tion one.
At Netflix, controlled experiments are run on both new and existing users [6]. New users are assigned to experi-mental conditions at the time of signup, while existing users can be assigned anytime after their free trial ends. While product decisions rely on results from both cohorts, they are more heavily based on results from new users since they have not been exposed to the Netflix experience before. For exist-ing users, it is difficult to tease apart whether a movement in business metrics is simply due to a change in experience (change effect) [6] or whether it is caused by the new ex-perience itself. One way to remove such change effect is to run experiments longer and observe if the difference in busi-ness metrics persists after a long time. But this slows down our pace of innovation. The other reason to favor results from new users is because they are more sensitive to prod-uct changes since they start with a free trial during which they are in an evaluation mode. Note that testing on new users is not a common practice in the industry but is very important for Netflix to make product decisions for the rea-sons just explained.
Netflix X  X  monthly subscription business model suggests a framework to define business metrics for controlled exper-iments. Our revenue comes solely from the monthly sub-scription fee that current users pay, and current users can cancel the subscription at anytime. Thus we believe max-imizing revenue through product changes is closely related with maximizing the value we provide to our users. Rev-enue is proportional to the number of users that is affected by three processes: the acquisition rate of new users, cur-rent user cancellation rate, and the rate at which former users rejoin. The focus of this paper is on product changes that directly impact only current users. Hence the primary business metric of interest is current user cancellation rate or retention rate. However, there are some challenges with just looking at retention rate in product experiments. First of all, as much as we hope that better product or user experience can increase user retention rate, it can be af-fected by many other factors that are not directly related to our product changes. Secondly, since our subscription is month by month, users typically choose to cancel their subscription by their next payment period. For new users, it typically takes a whole month to observe retention since they are assigned to the experiments at the beginning of their first payment period. For existing users, the wait time to observe retention varies depending on the number of days it takes from the start of the experiments to the users X  next payment period.
 Fortunately, we have observed that user engagement met-rics are highly correlated with retention but are more sen-sitive. Moreover, we get to observe such engagement met-rics from the start of an experiment. One good example of user engagement metrics is streaming hours. However, the relationship between streaming hours and retention is not linear. What we have learnt from historical data is that get-ting users that stream few hours per month to stream more has a much larger positive impact on retention than getting users that already stream a lot to stream a bit more. This is because those users with low streaming hours are more likely to be one of those on the fence of cancellation and are more sensitive to product changes.
 So we summarize the distribution of streaming hours using I streaming thresholds T i , i = 1 , ..., I . For a given user, T i is a binary metric indicating whether the user streamed more than H i hours in a given time period. H i  X  X  are cho-sen to minimize the loss of information from summarizing the distribution of streaming hours using these thresholds. Details are not covered in this paper. From a business per-spective, these streaming thresholds allow decision makers to gain more insight on which part of the distribution of streaming hours is changed. While we have tried more so-phisticated versions of streaming measurement in the past, these thresholds work well because they are easy to under-stand without much loss of information.
We define all the users that can potentially be impacted by an experiment as the population for the experiment. Sup-pose there is one or more variable(s) that are correlated with the business metrics. These variables are measurable prior to an experiment and independent of the different experi-ences in the cells of the experiment. As an example, the signup country of a user is correlated with how likely the user is to retain but does not depend on the experiences tested in the experiment. We refer to these variables as co-variates and denote them as X . The two sampling schemes considered in this paper are simple random sampling and stratified sampling. In stratified sampling, covariates are used to divide the population into K subpopulations called strata. For example, since Netflix is available in 190 coun-tries, we can divide the population for an experiment in 190 strata based on the signup country covariate.
 Now we introduce some terminology and notations used throughout the paper. Note that the following notations are used for both simple random sampling and stratified s ampling. Next we define two estimates of the population mean. The first is the standard simple sample average denoted as  X  is defined as The second is a weighted average denoted as  X  Y strat . It is defined as where p k is defined above and  X  Y k = 1 n erage of the business metric for users from the k th stratum. Note that, under stratified sampling, the two estimates in (1) and (2) are the same. More details around why this is true are in Section 3.3. However, these two estimates are not the same under simple random sampling. This is the reason why post stratification leads to variance reduction. See Section 3.4 for the details. The subscript strat is used in the weighted average estimate (2) because it comes from stratified sampling. Throughout the paper, we use E srs and E strat to denote the expectation of an estimate under sim-ple random sampling and stratified sampling, respectively. Similarly, we use var srs and var strat to denote the variance of an estimate under simple random sampling and stratified sampling, respectively.
Variance reduction is a procedure to increase the precision of the sample estimate of some parameter such as the pop-ulation mean. The sample estimate is typically based on a random sample of the population. While a well-known pro-cedure in statistics, Monte Carlo simulation [8], and some other areas, its application in controlled experiments is rela-tively new. Next we review a few popular variance reduction techniques that can be easily applied to controlled experi-ments.
 As a starting point, we briefly review the statistical infer-ence in controlled experiments. Suppose we are interested in comparing a test cell and the control cell in an experiment. Denote the business metric in the test cell and the control cell as Y ( t ) and Y ( c ) , respectively. We start with a pair of hypotheses. The null hypothesis is that Y ( t ) and Y ( c ) the same mean and the alternative is that they do not. The sample size in the experiments at Netflix is at least in thou-sands. Regular two-sample t-test is thus applied to test the hypotheses. The t-test statistic is defined as follows. where  X  Y ( t ) is an unbiased estimate for the population mean in the test cell and  X  Y ( c ) is an unbiased estimate for the pop-ulation mean in the control cell. Thus  X  Y ( t )  X   X  Y unbiased estimate for the effect size. In controlled experi-ments, variance reduction is about reducing var (  X  Y ( t ) The sampling in our controlled experiments is without re-placement because a user can not be assigned to two cells at the same time and the population is finite. Thus, strictly speaking, the samples in control and test are not indepen-dent from each other. But the users assigned to a single experiment are typically a small proportion of the Netflix user base. Hence the dependence is negligible and we have Equation (4) shows the equivalence between reducing the variance of the mean estimate in a single cell and reducing the variance of the effect size estimate. Therefore we fo-cus the discussion that follows on variance reduction in a single cell. Fundamentally, variance reduction in a single cell of controlled experiments can be achieved by leverag-ing covariates that are measurable prior to the experiments and are correlated with the business metrics. Covariates can be used at different stages of an experiment. When used at-assignment, the covariates are leveraged during the process of assigning users to the cells, e.g., stratified sam-pling. When used post-assignment, the covariates are lever-aged after the user assignment, e.g., post stratification and CUPED.
Stratified sampling [2] is probably the most well-known at-assignment variance reduction technique. The basic idea of stratified sampling is to divide the population into strata, sample from each stratum independently, and then combine samples across each stratum to give an overall estimate. In stratified sampling, the sample size from the k th stratum n k is fixed for given total sample size n and they have the following relationship where p k i s defined in 3.1 and k = 1 , ..., K . In stratified sampling, the weighted average in (2) is typically used to estimate the population mean  X  . As mentioned in Section 3.1, under stratified sampling, the two estimates in (1) and (2) are the same shown as follows.
 The first equation in (6) follows from the definition of  X  The second equation is true because of (5). Now we derive some statistical properties of the estimate in (2) under strat-ified sampling. We first show the estimate in (2) is unbiased under stratified sampling.
 Secondly, the variance of the estimate in (2) under stratified sampling is The first equation in (8) holds because sampling from the K strata is done independently from each other.  X  2 k and n are defined in Section 3.1.
 In simple random sampling, the standard simple sample av-erage in (1) is used to estimate the population mean. Under simple random sampling, the estimate in (1) is unbiased shown as follows.
 The variance of (1) under simple random sampling is derived as follows. Note that var srs ( Y kj ) =  X  2 because Y kj are all random sam-ples under simple random sampling from the distribution of Y . Next we make a connection between (8) and (10). First let Z denote the stratum number of a random observation from the distribution of Y under simple random sampling. Note that Z is a multinomial random variable that takes values 1 , ..., K and P ( Z = k ) = p k . Then we have var srs ( Y ) = E srs ( var srs ( Y | Z )) + var srs ( E srs where I ( Z = k ) is an indicator variable with value 1 if Z = k and 0 otherwise. Combing (10) and (11), we have To summarize the comparison between stratified sampling and simple random sampling, estimates in both sampling techniques are unbiased. But the variance of the estimate in stratified sampling is smaller than that in simple random sampling by 1 n P K k = 1 p k (  X  k  X   X  ) 2 . The intuition for variance reduction based on stratified sampling is that the variance of the estimate based on simple random sampling can be decomposed into within-strata variance and between-strata variance. Stratified sampling achieves variance reduction by removing the between-strata variance. Fundamentally, this is because the mean of the business metric is different across strata. From a sampling point of view, stratified sampling removes the variation of sample size from each stratum for a given total sample size n and thus reduces the variance of the estimate.
Post stratification is a popular post-assignment variance reduction technique. It assumes simple random sampling but uses the estimate in (2) instead of (1). Note that, when simple random sampling is used, the estimates in (2) and ( 1) are different. This is because the sample size n k from the k th stratum is not necessarily equal to np k under simple random sampling. Here n k , n , and p k are defined in Section 3.1. In fact, n 1 , ..., n K are all random under simple random sampling. The intuition behind post stratification is very simple. The weighted average (2) gives more weights to ob-servations from the strata that are under-represented in the sample. Thus if a sample is badly balanced for some covari-ate such as signup country, the weighted average estimate automatically corrects for it. We now sketch the derivation of the variance of the estimate in (2) under simple random sampling. What is remaining to calculate the variance of the esti-mate in (2) under simple random sampling is to calculate E srs ( 1 n k ) , where k = 1 , ..., K . Note that, n k is a Bernoullian random variable with expected value np k for given sample size n . It can be shown that E srs ( 1 n [ 9], where o ( 1 n 2 ) is a residual term that converges to 0 faster than 1 n 2 a s n  X  X  X  . The proof in [9] is based on some com-plicated factorial expansions because it is for more general cases than the reciprocal of a Bernoullian random variable. In this paper, we provide a simpler proof based on Taylor expansion as follows.
 where the first equation is simply a Taylor expansion of 1 is Bernoullian variable with mean np k and variance np k (1  X  p ). Thus, we have Since p k  X  X ,  X  k  X  X , K , and  X  are finite values, we can always find a large enough n such that the following is true. When equation (16) is true, we have var srs (  X  Y strat )  X  var This shows that post stratification leads to variance reduc-tion for large enough sample size. Hence for large enough n , the comparison of variance of the estimates based on simple random sampling, stratified sampling, and post stratifica-tion can be summarized as follows. Thus, although it is true that the variance of the estimate based on stratified sampling is the smallest, when n is large, the variance difference between post stratification and strat-ified sampling will be much smaller than that between sim-ple random sampling and stratified sampling. This means that post stratification achieves similar variance reduction as stratified sampling when the sample size n is large. It is worth pointing out the derivation of the variance in post stratification requires a regularity condition that none of the n  X  X  is zero [7]. Although the derivation of variance is only of theoretical interest, in practice, we need a mechanism to estimate the mean for those strata that do not have any ob-servation in a cell. One way is to pool or collapse similar strata [7]. This is potentially an issue for post stratification in practice.
Another variance reduction technique is based on control variates. It has been used in Monte Carlo simulation [5]. One can think of the control variates here as covariates de-fined in Section 3.1. The control variates technique was ap-plied to controlled experiments as a variance reduction tech-nique in [3]. The authors name the technique CUPED (con-trolled experiments utilizing pre-experiment data) in their paper because the control variates in their paper are based on pre-experiment data. CUPED is also a post-assignment variance reduction technique because it is based on sim-ple random sampling. Next we briefly review how CUPED works. Suppose the pre-experiment data X is a one-dimensional control variate. In CUPED, instead of looking at the busi-ness metric Y , we look at a new metric defined as where  X  is some parameter that needs to be defined. Next we discuss how to choose  X  to complete the definition of the new metric. For the variance of Y CUP ED under simple random sampling, we have where cov srs ( X, Y ) is the covariance between X and Y un-der simple random sampling. Using simple calculus, we can show that var srs ( Y CUP ED ) is minimized by choosing  X  equal to cov srs ( X, Y ) /var srs ( X ), where the minimal value is where  X  = corr srs ( X, Y ) is the Pearson correlation between X and Y under simple random sampling. The intuition be-hind variance reduction using control variates is that the t otal variance of the business metric Y can be decomposed into two parts: the part that is caused by the variance of the control variate X , and the part that is explained by other unknown variables. By looking at the corrected met-ric Y CUP ED , we have removed the variance caused by X and thus the variance is reduced. So far, the discussion is all in the context of a single cell. It is clear that E srs ( Y CUP ED is different from E srs ( Y ). In controlled experiments, we are typically interested in the difference between the means of the business metric in a test cell and the control cell. Hence the authors suggest using the same  X  for different cells so that the difference between the means of the new metric Y CUP ED is the same as that of the original business metric Y . In practice,  X  can be estimated based on pre-experiment data once we know X . Based on (20), X should be chosen to maximize the magnitude of corr srs ( X, Y ). In practice, the authors suggest using the same metric Y in the pre-experiment period because the same metric over different time periods typically correlate well. In our analysis, we take the authors X  two suggestions: using the same  X  across cells, and using the same business metric prior to experi-ment as the control variate.
 There is also an interesting connection between CUPED and stratified sampling. When X is categorical, it can be math-ematically shown that CUPED and stratified sampling ( X is used to define strata in stratified sampling) is equivalent. For the detailed proof, please see [3].
We have learned through years of research that many fac-tors not related to the product correlate with our business metrics. For example, the signup country of users correlates with retention. The most impactful factors are leveraged as covariates in stratified sampling to help reduce the sampling variance of business metrics. More covariates are leveraged for existing members since we know more about them. Re-sults in Section 5 show that this extra information for exist-ing users leads to significantly more variance reduction. Prior to running an experiment, a target sample size is de-termined, and triggers for assigning users to the experiment are defined. For new users, the trigger is signing up for Net-flix. For existing users, an example of trigger for a product change on the Netflix kids webpage could be a user visit to that page. The triggering rule and target sample size to-gether decide the length of the recruitment period, which can last weeks. In this context, assigning users to cells hap-pens in real-time when the trigger condition is satisfied. Implementing stratified sampling in a real-time assignment scenario is rarely discussed and poses the challenge of having equal representation of the covariates in the test and control cells throughout the whole recruitment period. To address this issue, we rely on a queue system composed of one queue per experiment e and stratum s . Each queue consists of 100-slot segments. Prior to user assignment, the sampling rate for each cell in the experiment is specified. Sampling rate for a cell is defined as the share of users in the experiment that receive the experience in the cell. The cell sampling rate ranges between 0% and 100% in an increment of 1%. For each segment of 100 slots, the slots are mapped to cells such that the share of slots assigned to a cell exactly matches the sampling rate for that cell. Note that the increment cannot be more granular than 1% due to the 100-slot segment de-sign.
 Here is a simple example to illustrate the assignment in a single segment. Suppose we want to run an experiment with two cells and allocate 50% of the users to each of the two cells. We first get a sequence of integers between 1 and 100 as seen in Figure 1 (a) and then reshuffle this sequence as in Figure 1 (b). Finally we map integers 1-50 to Cell 1 and 51-100 to Cell 2 as in Figure 1 (c). As mentioned above, a queue consists of many 100-slot segments. The cell assign-ment in each 100-slot segment is done independently from each other within a single queue, and independently across queues. When a new user eligible for the experiment signs up, we first decide the strata that he falls into based on his covariate information and then assign him to the corre-sponding queue for his strata. He will then take the next available slot in the queue and gets assigned to the cell for the slot. A simple example of new user assignment with two strata and two cells is shown in Figure 2.
 The implementation of stratified sampling based on our queue system does not always achieve perfect balance of strata across cells. This can diminish the amount of variance re-duction based on stratified sampling. There are two factors that contribute to the imbalance.
 Firstly, we only guarantee perfect balance within each seg-ment of 100 slots. Thus the total sample size of a stratum across cells needs to be a multiple of 100 to achieve perfect balance. For example, if there are 100,090 users in a stratum prior to cell assignment, then the queue system guarantees balance for the first 100k users but not the last 90 users. For the last 90 users, the actual sampling rate in each cell may not exactly match the intended sampling rate. And thus, af-ter cell assignment, the percent of users from each stratum may be different across cells. The rationale for having a seg-ment size of 100 is mainly for the convenience of specifying sampling rate per cell (increment of 1%). Potentially we can decrease the segment size to achieve better balance but it also makes the sampling rate specification less granular, e.g., if we change the segment size to 50, then the sampling rate needs to be in increment of 2%. The impact of this imbalance depends on the sample size in each stratum. The second factor preventing us from achieving perfect bal-ance is that we usually have to use many machines to con-duct the sampling because of both high volume of sampling requests and occasional failures of the machines. With M machines, there will be M queues for experiment e and stra-tum s . When a user eligible for an experiment signs up, he is first randomly assigned to a machine, and then assigned to a queue on the machine based on his covariate information, and finally he takes the next available slot in the queue and gets assigned to the cell for the slot. It is intuitive that with multiple machines, it is more difficult to achieve the strata balance across cells. For example, if the sample size of a stratum for the whole experiment is 100k, it would achieve perfect balance with a single machine but not necessarily with multiple machines because the number of users from the stratum on each single machine is not necessarily a mul-tiple of 100. The likelihood to achieve perfect balance de-creases as the number of machines increases. The impact of the number of machines on the variance reduction amount based on stratified sampling is quantified in the next section.
In this section, we compare the amount of variance reduc-tion achieved by stratified sampling, post stratification and CUPED on a few datasets from Netflix. The business met-rics considered are customer retention and seven out of the I streaming thresholds defined in Section 2.2. Simple random sampling is used as the baseline to estimate the amount of variance reduction achieved by each technique. The com-parison is done on both new and existing users. For each user type, we collect covariates and business metrics for a cohort of users. We define these users as the population and repeatedly simulate A/A experiments, which are controlled experiments with two cells and zero effect size. In the case of stratified sampling, an A/A experiment is simulated by splitting users into two cells based on the implementation described in Section 4. For post stratification and CUPED, an A/A experiment is simulated by splitting users into two cells based on simple random sampling. After the user as-signment to two cells in a single A/A experiment is deter-mined, for each business metric, we compute an estimate of the effect size. For the simple random sampling baseline, this estimate is the difference of the simple sample aver-ages in (1). For stratified sampling and post stratification, this estimate is the difference of the weighted averages in (2). For CUPED, this estimate is the difference of the av-erages for the corrected metric in (18). For each business metric and variance reduction technique, 100k A/A experi-ments are simulated independently from each other on the same cohort of users, yielding 100k estimates of the effect size. The sample variance of these estimates is then com-pared with the theoretical variance based on simple random sampling to quantify the amount of variance reduction for each metric and technique combination. For stratified sam-pling, we also estimated the variance reduction percentage pretending there is only one machine to get a sense of the additional variance introduced by the use of multiple ma-chines. For new users, we do not have pre-experiment data for streaming and retention that can be used for CUPED. Thus, eight regression models (one per metric) were built on a different set of users from those used to simulate our A/A experiments. The predictors in the regression mod-els are the same set of covariates used to define strata in stratified sampling. This ensures fair comparison between CUPED and stratified sampling. The predicted mean val-ues of the metrics from the models are then used as pre-experiment data. For existing users, the same metric is used as the pre-experiment data for the streaming thresholds. We did not apply CUPED on retention for existing users since the amount of variance reduction for retention is very small based on the other techniques and we do not expect CUPED to make a significant difference. We report on the variance reduction point estimates along with error bars based on Bootstrap [4] as a measure of the uncertainty of the results based on a finite (although 100k is already pretty large) number of simulations.
The resulting variance reduction estimates are presented in Figures 3 and 4, separately for new and existing users for each of the eight business metrics (retention and seven streaming thresholds). The results show that identifying co-variates that are highly correlated with the business metrics is key for the success of any of these variance reduction tech-niques. The empirical results also align well with the theory in Section 3. Indeed, ignoring challenges posed by practical implementation, stratified sampling, post stratification and Figure 3: New users. Variance reduction results of s tratified sampling, CUPED, and post stratification compared to simple random sampling Figure 4: Existing users. Variance reduction results o f stratified sampling, CUPED, and post stratifica-tion compared to simple random sampling CUPED achieve similar variance reduction amount when leveraging the same covariates. However, in practice, the variance reduction achieved by stratified sampling can be severely impacted by the 100-slot design and the need to use multiple machines as described in Section 4. This is not the case for post stratification and CUPED which are post-assignment techniques. See the following subsections for more detailed findings.
For new users, the amount of variance reduction achieved is very low regardless of the metric or the variance reduction technique used. This is due to the lack of covariates highly correlated with the business metrics for these users at the time of cell assignment. Indeed, the Pearson correlation be-tween the covariates and business metrics ranges from 0.2 to 0.4 for new users.
 For streaming thresholds, the variance reduction for existing users can be up to 40% because we included pre-experiment streaming activity as a stratification dimension or in the post-assignment correction. Note that for existing users, the lowest streaming threshold T 1 prior to the experiment is the only streaming threshold used as covariate in stratified sam-pling and post stratification. Thus it is expected that the amount of variance reduction becomes lesser as the stream-ing threshold moves further away from T 1 and the correla-tion between this covariate and these streaming thresholds becomes weaker.
 For retention, while the amount of variance reduction is small for both new and existing users, it is higher for new users. The reason is because the covariates used to define strata for new users are more correlated with retention than for existing users. Also new users get a free trial in the first month and there is more user-level variation of reten-tion than for existing users who have already passed the free trial period and whose retention metric becomes less sensi-tive.
For all the metrics and both user types, post stratification is comparable to stratified sampling with one machine. This is consistent with the theoretical understanding that post stratification achieves similar variance reduction as stratified sampling when the sample size is large. The sample size in the dataset used for evaluation is at the scale of hundreds of thousands.
As discussed in Section 3.3, from a sampling point of view, stratified sampling achieves variance reduction compared to simple random sampling because it removes the variation of the sample size from each stratum once the total sample size n is given. In Section 4, we described how the practi-cal implementation of stratified sampling cannot completely remove this variation because of the use of a 100-slot queue system and the need to conduct sampling on multiple ma-chines. The empirical evaluation shows that for existing users, variance reduction based on stratified sampling with multiple machines is less than half of that with one machine. This impact is not as clear for new users for whom the num-ber of machines used for sampling is only one fifth of that for existing users. Also the increase in the number of ma-chines tend to have a larger impact on existing users that have smaller strata sizes.
 To provide some intuition around the impact of multiple machines, we run an evaluation procedure similar to the one described in Section 5.1 on a cohort of existing users. In this evaluation, we show the impact of number of machines on the sampling variance of the sample sizes of each stratum. Since there is more than one stratum, we define a weighted average of standard deviation metric as follows where p k is the proportion of users from stratum k in the population and  X  k is the standard deviation of the sam-ple size from stratum k in a random sample for fixed total sample size n . The number of machines used for stratified sampling is varied from one to two hundred. For a given number of machines m , we simulate 100k A/A experiments to estimate  X  k . Each A/A experiment first randomly assigns users to machines and then for each machine splits users into two cells based on stratified sampling as described in Section 5.1. The estimates of  X  k are then plugged into (21) to get the estimate of the weighted average of standard deviation metric. The results are shown in Figure 5. Note that the variation of the weighted standard deviation metric mono-tonically increases as the number of machines increases. The error bars are calculated using the Bootstrap technique [4]. There is no error bar for simple random sampling because it is the theoretical value. So, as the number of machines increases, the variation of the sample size from each stratum increases, which translates to higher variance of the final es-Figure 5: Impact of the number of machines used in s tratified sampling on strata sample size variation timate based on stratified sampling and diminished variance reduction based on stratified sampling.
CUPED performs slightly worse than stratified sampling and post stratification for new users because the pre-experiment data in CUPED is essentially a one-dimensional summary of the covariates used to define strata and there is some loss of information in this summary.
 For existing users, we have the flexibility of correcting stream-ing thresholds with the same metric prior to the experi-ment. Hence the amount of variance reduction is consis-tently around 40% for all the streaming thresholds. Finally, it is worth emphasizing that the difference in variance re-duction between CUPED and the other techniques for high streaming thresholds on existing users in Figure 4 is due to the difference of covariates used in the techniques, not the techniques themselves. The comparison settings were set as such because the initial objective of this case study was to compare CUPED with what was used in production at Net-flix. We also did a fair comparison between CUPED and post stratification by using exactly the same covariates for each of the streaming threshold metrics on existing users. The results in Figure 6 clearly show that these two tech-niques perform comparably when using the same covariates. This is expected due to the following two reasons. Comparisons with stratified sampling based on the same co-variates are omitted since we expect stratified sampling to perform the same as post stratification given the large sam-ple size in the data.
For companies that use controlled experiments to make product decisions, it is critical to run highly sensitive con-trolled experiments in order to not miss product changes that can have a substantial impact on user experience and revenue. In this paper, we focused on improving the sensi-tivity of controlled experiments by reducing sampling vari-ance of the business metrics. We compared a few variance Figure 6: Existing users. Variance reduction of C UPED and post stratification using the same co-variates compared to simple random sampling reduction techniques, both at-assignment (stratified sam-pling) and post-assignment (post stratification &amp; CUPED). We showed that theoretically, these techniques achieve sim-ilar variance reduction when the sample size is large, which is typically the case in online controlled experiments. We applied them to a few Netflix datasets and our empirical re-sults aligned with theory when the same set of covariates is used for all the techniques. However, in practice, stratified sampling performs worse than post-assignment techniques such as post stratification and CUPED because real-time experimental assignment requires a queue system and the use of multiple machines. Moreover, post-assignment tech-niques are cheaper to implement, and very flexible in choos-ing the covariates for post-assignment correction. It is thus recommended to apply post-assignment variance reduction techniques when running large-scale controlled experiments. Our results on new users emphasize that identifying covari-ates that are highly correlated with the business metrics is key for the success of variance reduction techniques. In cases where such covariates cannot be easily found, other methods to improve the sensitivity of controlled experiments should be explored. At Netflix we continuously research new user engagement metrics that are a better tradeoff between sensi-tivity and correlation with retention to help make better pro-duction decisions. In the context of TV/movie recommenda-tion and search algorithms, we leverage offline experiments as a pre-selection mechanism to reduce the number of test cells in a single experiment run online. With fewer test cells , larger sample sizes can be used for each cell and sensitivity is thus improved. See [6] for a more complete description of the offline experiments method as well as its challenges. An-other example is leveraging interleaving-based experiments [1] to remove the between-user variance associated with our traditional controlled experiment design. Finally, more so-phisticated experimental designs such as fractional factorial designs [10] can be used to reduce the number of test cells when multiple changes are tested in one single experiment.
We are grateful to Bryan Gumm and Carlos Gomez-Uribe for designing the queue system for stratified sampling at Net-flix. We would like to give many thanks to Carlos Gomez-Uribe and Harald Steck for their thoughtful comments on earlier drafts of this paper. [ 1] O. Chapelle, T. Joachims, F. Radlinski, and Y. Yue. [2] W. G. Cochran. Sampling Techniques . Wiley, 1977. [3] A. Deng, Y. Xu, R. Kohavi, and T. Walker. Improving [4] B. Efron and R. Tibshirani. An Introduction to the [5] P. Glasserman. Monte Carlo Methods in Financial [6] C. A. Gomez-Uribe and N. Hunt. The Netflix [7] D. Holt and T. Smith. Post stratification. J.R. Statist. [8] C. P. Robert and G. Casella. Monte Carlo Statistical [9] F. F. Stephan. The expected value and variance of the [10] C. J. Wu and M. S. Hamada. Experiments: Planning,
