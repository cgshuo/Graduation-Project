 ORIGINAL PAPER Robert Jay Milewski  X  Venu Govindaraju  X  Anurag Bhardwaj Abstract A new paradigm, which models the relationships between handwriting and topic categories, in the context of medical forms, is presented. The ultimate goals are: (1) a robust method which categorizes medical forms into speci-fied categories, and (2) the use of such information for practical applications such as an improved recognition of medical handwriting or retrieval of medical forms as in a search engine. Medical forms have diverse, complex and large lexicons consisting of English, Medical and Pharma-cology corpus. Our technique shows that a few recognized characters, returned by handwriting recognition, can be used to construct a linguistic model capable of representing a med-ical topic category. This allows (1) a reduced lexicon to be constructed, thereby improving handwriting recognition per-formance, and (2) PCR (Pre-Hospital Care Report) forms to be tagged with a topic category and subsequently searched by information retrieval systems. We present an improve-ment of over 7% in raw recognition rate and a mean average precision of 0 . 28 over a set of 1,175 queries on a data set of unconstrained handwritten medical forms filled in emer-gency environments.
 Keywords Handwriting analysis  X  Language models  X  Pattern matching  X  Retrieval models  X  Search process 1 Introduction This paper describes the first automatic recognition system for handwritten medical forms. In the United States, any pre-hospital emergency medical care provided must be doc-umented. Departments of Health for each state provide a standard medical form to document all patient care from the beginning of the rescue effort until the patient is transported to the hospital. State laws require that emergency personnel fill out one form for each patient. Automatic recognition and retrieval of these forms is quite challenging for several rea-sons: (1) handwritten data in the form is unconstrained in terms of writing styles, variability in font type or size and choice of text due to emergency situations, (2) form images are noisy since they are obtained from a carbon copies of the original forms, (3) dictionary of medical words is huge with over 40 , 000 words which leads to poor recognition results.
Figure 1 shows an example Pre-Hospital Care Report (PCR) [ 67 ] form which contains 16 information regions (see Table 1 ). Handwriting, from PCR regions 8, 9, 11, 13 and 14 are used for recognition and retrieval analysis. There are two phases to our research: (1) the recognition of handwriting on the medical form, and (2) a medical form query retrieval engine. Handwriting recognition is used to tag medical forms with a topic category to subsequently improve recognition performance. The medical forms reflect large lexicons con-taining Medical, Pharmacology and English corpus. While current state of the art recognizers report recognition per-formance between  X  58 X 78%, on comparable lexicon sizes in the postal application [ 36 , 68 , 69 ], our experiments show  X  25% raw match recognition performance on the medical forms. This underscores the extremely complicated nature of medical handwriting (Fig. 1 ). We have developed a method of automatically determining the topic category of a PCR form using machine learning and computational linguistics techniques. We demonstrate the strategy for improving the raw word recognition rate by about 7% for a lexicon size of over 5,000 words. 2 Background Though the task of efficient retrieval of text documents has been addressed by information retrieval community for sev-eral years [ 70 ], robust document search and retrieval has received some considerable attention lately [ 16 ]. The exist-ing methods for document retrieval can be broadly classified into two categories: (1) OCR based methods [ 28 , 58 , 65 ], and (2) Word image matching based methods [ 2 , 54  X  56 , 64 ]. On one hand word image matching based methods rely heavily on the proper selection of image features [ 53 ] and similarity methods [ 2 , 55 ], the OCR based methods depend on the word recognition accuracy. It has been shown that higher word rec-ognition error rate adversely affects the document retrieval performance [ 14 , 40 ]. Therefore, an improved word recog-nition algorithm forms a basis for an efficient document retrieval system.

The basis for reducing the lexicon to improve recognition is a well researched strategy in handwriting recognition [ 26 , 68 ]. Although handwriting recognition and lexicon prun-ing/reduction [ 43 ] have been researched substantially over the years, many challenges still persist in the offline domain. Word recognition applications range from automated check recognition [ 35 ], postal recognition [ 20 ], historical docu-ments recognition [ 18 , 21 , 25 ] and now emergency medical documents [ 45  X  47 ]. Strategic recognition techniques for handwriting algorithms such as Hidden Markov Models (HMM) [ 11 , 17 , 18 , 31 , 37 , 44 , 48 ], Artificial Neural Networks (ANN) [ 6 , 12 , 13 , 22 , 50 ], Boosted Decision Trees [ 30 ] and Support Vector Machines (SVM) [ 1 , 7 ] have been developed. Lexicon reduction has been shown to be critical to improve-ment of performance primarily because of the minimization of possible choices [ 26 ]. Even the systems dealing with a large vocabulary corpus have been successful [ 37 , 38 , 72 ].
Lexicon reduction schemes in general, rely upon finding a specific topic of the document and then using a fixed smaller vocabulary of the chosen category as the reduced lexicon. This is usually achieved by performing categorization of the OCRed document text which is noisy. Bayer et al. [ 3 ] in their work learn the noise model of the OCR using word substrings extracted with an iterative procedure. Taghva et al. [ 63 ] study the performance of a naive bayes classifier applied to 400 rec-ognized documents with an OCR error rate of nearly 14%. In this experiment, 6 categories out of 52 are analyzed and the highest rate of correct classification achieved is 83 . 3%. How-ever, both of these strategies have been applied to machine print OCRed text where the noise level is not as high as the handwritten documents. In the context of medical forms, where the word recognition rate is very low (  X  25%) and only few characters are recognized with high confidence scores, such methods are not applicable. Vinciarelli et al. [ 66 ] study noisy text categorization over synthetic handwrit-ten data. In this research, noisy data is obtained by changing a certain percentage of characters obtained from the OCR. However this method only handles the case when the char-acter is changed to another list of known characters, whereas in the text obtained from medical forms, there are slots for potentially unknown or human unreadable characters.
Additionally, some lexicon reduction strategies have used the extraction of character information for lexicon reduc-tion, such as that by Guillevic et al. [ 27 ]. However, such strategies reduce the lexicon for a single homogeneous cat-egory, namely cities within the country of Finland. In addi-tion, usage of word length estimates for a smaller lexicon are available [ 27 ]. Caesar et al. [ 8 ] also state that prior reduc-tion techniques [ 51 , 60 , 61 ] are unsuitable since they can only operate on very small lexicons due to enormous computa-tional burdens [ 8 ]. Caesar [ 8 ] further indicates that Suen X  X  [ 62 ] approach of n-gram combinatorics is sensitive to seg-mentation issues, a common problem with medical form handwriting [ 8 ]. However, Caesar X  X  method [ 8 ] and those which are dependent on using the character information, and/or the character information of only one word to directly reduce the lexicon, suffer if one of the characters is selected incorrectly [ 8 ]. This is observable in the cursive or mixed-cursive handwriting types.

Many existing schemes, such as that of Zimmermann [ 71 ], assume that some characters can be extracted. However, in the medical handwriting domain this task is error prone. Therefore, operating a reduction scheme which can be robust to incorrectly chosen characters is necessary. We use seq-uences of characters to determine the medical topic category which has a lexicon of its own, thereby reducing the issues of using the character information directly. Similar to the study by Zimmermann et al. [ 71 ], the length of words are used with phrases.

Kaufmannet al. [ 34 ] present another HMM strategywhich isprimarilyadistance-basedmethodandusesmodelassump-tions which are not applicable in the medical environment. For example, Kaufmann [ 34 ] assumes that  X ...people gener-ally write more cooperatively at the beginning of the word, while the variability increases in the middle of the word. X  In the medical environment, variability is apparent when multi-ple health care professionals enter data on the same form. The medical environment also has exaggerated and/or extremely compressed word lengths due to erratic movement in a vehi-cle and limited paper space. Kaufmann [ 34 ] only provides a reduction of 25% of the lexicon size with little to no improve-ment in error rate, and the experiments are run only on a small sample of words. 3 Lexicon reduction Thisresearchproposesthefollowinghypothesiswhichisver-ified experimentally: a sequence of confidently recognized characters, extracted from an image of handwritten medical text, can be used to represent a topic category. A reduced lexicon can then be constructed specifically for a medical form based on its classified categories to improve recognition performance on a second feedback loop.

A medical form training and test set have been created from actual PCR data. A software data entry system has been developed which allows human truthers to segment all PCR form regions and words, and provide a human interpretation for the word, denoted as the truth. Truthing is done in two phases: (1) the digital transcription of medical form text and (2)theclassificationofformsintotopiccategories.Thedistri-bution of PCR forms under each category is approximately equal in both the training and test set. The task has been supervised and performed by a health care professional with several years of field emergency medical services (EMS) experience. This emergency medical data set is the first of its kind.
 A PCR can be tagged with multiple categories from Table 2 . In our data set, no form had more than five category tags. The subjectivity involved in determining the categories makes the construction of a hierarchical chart representing all patient scenarios with respective prioritized anatomical regions a difficult task and exceeds the scope of this research. The following are some examples for classifying medical form text into categories (see Table 2 ): Example 1 A patient treated for an emergency related to her pregnancy would be included in the Reproductive System category (see Table 2 ).
 Example 2 A conscious and breathing patient treated for gun shot wounds to the abdominal region would fall into the Circulatory/Cardiovascular System due to potential loss of blood, as well as being categorized for Abdominal, Back, and Pelvic categories (see Table 2 ).

We take characters with the highest recognition as an input and produce higher level topic categories. A knowl-edge base is constructed during the training phase from a set of PCR forms. The knowledge base contains the relation-ships between terms and categories and essentially describes the features for topic categorization. The testing phase takes an unknown form, and reduces the lexicon using the knowl-edge base. This phase is evaluated using a separate testing set. Finally, after all content of the PCR form has been rec-ognized, a search can take place by entering in a query. This phase is tested by querying the system with a set of phrase inputs. The forms are then ranked accordingly and returned to the user. The complete architecture of the proposed algo-rithm is also shown in Fig. 2 .

In the training phase, a mechanism for relating uni-grams and bi-grams (henceforth uni/bi-grams) as well as categories from a PCR training set are constructed. The testing phase then evaluates the algorithm X  X  ability to determine the cate-gories from a test form by using a lexicon driven word rec-ognizer (LDWR) [ 36 ] to extract the top-choice uni/bi-gram characters from all words. Since the recognizer output is very noisy given the unconstrained handwriting, very few charac-ters are correctly recognized per word image. In our setup, we consider a maximum of 2 characters per word since LDWR [ 36 ] successfully extracts a bi-gram with spatial encoding information 40% of the time. If  X  3 characters are selected, then LDWR [ 36 ] successfully extracts a character  X  1% of the time owing to the noisy recognition output. Hence the maximum value of n in the n-grams is taken to be 2 (see Fig. 4 ). A list of about 400 stopwords provided by PubMed are omitted from text analysis [ 29 , 49 ]. 3.1 Training The training stage involves a series of steps to construct a matrix that represents relationships between terms and cate-gories. The training phase is divided into the following steps: (1) cohesive phrase generation, (2) spatial encoding strategy, (3) normalization, (4) term discrimination ability, and (5) reduced singular value decomposition [ 9 , 10 ]. The process begins with the extraction of cohesive phrases from the med-ical forms. These phrases are then converted to ESI encoding terms (ESI denotes  X  X xact Spatial Information X  used as the encoding procedure for the uni/bi-gram terms; see definitions later in this section). A matrix is then constructed utilizing the ESI terms for the rows and the categories in the columns. The matrix is then normalized, weighted, and prepared in Singular Value Decomposition format.
 Step 1 (cohesive phrase generation) . A cohesive phrase is defined as the frequency of two words co-occurring versus occurring independently (see Eq. 1 ). Figure 3 shows a high cohesive phrase extraction example from a PCR. A passage Pisthesetofallwords w for a PCR form under a category C treated as a single string. For each C, every pair of pas-sages, denoted P 1 and P 2 , is compared. A phrase is defined as a sequence of cohesive non-stopwords [ 19 ]. Here we denote w x as a word located at position x within a passage P. Let a and a , b denote the index of words in an ordered passage P and P 2 respectively ( w a  X  P 1 ,w a  X  P 2 ,w b  X  P 1 ,w such that b &gt; a and b &gt; a ) then a potential phrase con-sisting of exactly two words is constructed. The cohesion of phrases under each C is then computed. If the cohesion is above a threshold, then that phrase represents that category C. Thus a category C is represented by a sequence of high cohesion phrases using only those PCR passages manually categorized under C. An additional list of about 50 words (e.g. male, female, etc.) found in most PCR X  X , which have little bearing on the category are omitted from the cohesion analysis but retained in the final lexicon. cohesion (w a ,w b ) = z  X  The cohesion between any two words w a and w b is computed by the frequency that w a and w b occur together versus exist-ing independently. The top 40 cohesive phrases are retained for each category (see Eq. 1 ). In the given equation, z is a constant weight which can be used if external information on the relationship of w a and w b is available ( z = 2inthis research). The idea here is to analyze relationships between two words based on their correlations. If the two words are related to a category in some way, a higher correlation mea-sure would reflect it accordingly.
 Consider the following two unfiltered strings of words S 1 S 2 under the category legs :
S 1 :  X  X ight femur fracture X 
S 2 :  X  X roken right tibia and femur X  The candidate phrases CP 1 and CP 2 after the filtering step are: CP 1 :  X  X ight femur X  ...  X  X ight fracture X  ...  X  X emur fracture X  CP 2 :  X  X roken right X  ...  X  X ight femur X  ...
 The phrase  X  X ight femur X  is computed from CP 1 and CP 2 , given that w a and w a =  X  X ight X , w b and w b =  X  X emur X , and the conditions b &gt; a and b &gt; a have been met. If the cohe-sion for  X  X ight femur X  is above the threshold across all PCR forms under the legs category, then this phrase is retained as a representative of the category legs .

Tables 3 and 4 illustrate some top choice cohesive phrases generated. Digestive system and pelvic region are anatom-ically close . However, different information is reported in thesetwocasesresultinginmostlydifferentcohesivephrases. Those which are the same, such as CHEST PAIN have dif-ferent cohesion values. This implies that it is likely that the term frequencies will also be different and therefore com-monly occurring terms need to be weighted appropriately to their categories (this will be discussed in more detail later). Phrases sometimes may not make sense by themselves, how-ever, this is the result of using a cohesive phrase formula in which words may not be adjacent.
 Step 2 (spatial encoding strategy) . Select one of three possi-ble term representation strategies: NSI, ESI and ASI. These terms will later be modeled to an anatomical category and used as the essential criterion for lexicon reduction. The nota-tion c denotes that a single character (uni-gram) is extracted from a word whereas c 1 and c 2 denote two ordered characters (bi-gram) are extracted from a word. No Spatial Information (NSI) . An asterisk (*) indicates that zero or more characters are between or outside of c , c 1 c . NSI encodings are the most simple form of encoding (see Fig. 4 examples).
 UNI-GRAM ENCODING:  X  c  X  BI-GRAM ENCODING:  X  c 1  X  c 2  X  BI-GRAM ENCODING EXAMPLE: BLOOD  X  *L*D* Exact Spatial Information (ESI) : The integers (x, y, z) rep-resent the precise number of characters between or outside of c , c 1 and c 2 . ESI encodings are an extension of the NSI encodings with the inclusion of precise spatial information. In other words, the number of characters before, after and between the highest confidence c 1 and c 2 characters are part of the encoding. These encodings are the most successful in ourexperimentssincetherearefewertermcollisionsinvolved. Hence the ESI encodings are preferred.
 UNI-GRAM ENCODING: xcy BI-GRAM ENCODING: xc 1 yc 2 z BI-GRAM ENCODING EXAMPLE: BLOOD  X  1L2D0 Approximate Spatial Information (ASI) : The integers ( y , z of characters between or outside of c , c 1 and c 2 . A  X 0 X  indi-cates no characters, a  X 1 X  indicates between one and two char-acters, and a  X 2 X  represents greater than 2 characters. The ASI encodings are an approximation of ESI encodings designed to handle cases when the precise number of characters is not known with high confidence.
 UNI-GRAM ENCODING: x a cy a BI-GRAM ENCODING: x a c 1 y a c 2 z a BI-GRAM ENCODING EXAMPLE: BLOOD  X  1L1D0 Combinatorial analysis The quantity of all possible NSI, ESI and ASI uni-gram and bi-gram combinations, for a given word of character length n, such that n  X  1, is represented by Eq. 2 . Regardless of the encoding, the same quantity of combinations exist since there is only one encoding slot between or outside of the selected characters c , c 1 and c 2 . This is helpful in measuring the computational complexity of the encoding.
 F ( n ) = However, the function F only considers the combinations of an individual word. The combination inflation of a uni/ bi-gram phrase is shown by Eq. 3 . The equation parameters a and b represent the string lengths of the words considered in a phrase. The total number of possible uni/bi-gram com-binations resulting from a phrase P containing two words of length a and b is the product of the possible combinations of each word denoted as F ( a ) and F ( b ) respectively. P ( a , b ) = F ( a )  X  F ( b ) (3) For example Each of these encodings has its advantages and disadvan-tages. The choice is ultimately based on the quality of the handwriting recognizer X  X  (LDWR) ability to extract charac-ters.Ifthehandwritingrecognizercannotsuccessfullyextract positional information, then NSI is the best approach. If extraction of positional information is reliable, then the ESI is the best approach. However, NSI and ASI create more pos-sibilities for recognizer confusion since distances are either approximated or omitted. ESI is more restrictive on the possibilities as the precise spacing is used leading to lesser confusion among terms.

Using the ESI protocol, all possible uni/bi-gram terms are synthetically extracted from each cohesive phrase under each category. For example, BLOOD can be encoded to the uni-gram 0B4 (zero characters before  X  X  X  and four characters after  X  X  X ) and the bi-gram 0B3D0 (zero characters before  X  X  X , three characters between  X  X  X  and  X  X  X  and zero charac-ters following  X  X  X ). All possible synthetic positional enco-dings are generated for each phrase and chained together (a  X $ X  is used to denote a chained phrase). For example, CHEST PAIN encodes to: 0C4$0P0A2 ... 0C4$1A2 ... 0C0H3$0P1I1 ... 0C0H3$0P2N0, etc. To improve readabil-ity, the notation ( W 1 , W 2 ) is used to represent an ESI encod-ing of a two-word phrase (e.g. Myocardial Infarction: (my, in), (my, if), (my, ia), etc ...). Therefore, each category now has a list of encoded phrases consisting of positional encoded uni/bi-grams. These terms are the most primitive representa-tivelinkstothecategoryusedthroughoutthetrainingprocess. In the training phase, the synthetic information can be ext-racted since the text is known. However, in the testing phase, a recognizer will be used to automatically produce an ESI encoding since the test text is not known.

Amatrix A ,ofsize | T | by | C | , is constructed such that the rows of the matrix represent the set of terms T , and the col-umns of the matrix represent the set of category C as shown in Fig. 5 a. The value at matrix coordinate ( t , c ) is the frequency that each term is associated with the category. The term fre-quency corresponds to the phrasal frequency from which it was derived. It is the same value as the numerator in the cohesion formula (refer to Eq. 1 ): f (w a ,w b ) . For example, if the frequency of CHEST PAIN is 50, then all term enco-dings generated from CHEST PAIN, such as (ch, pa), will also receive a frequency of 50 in the matrix. An example of term frequency construction is shown in Fig. 5 b. Step 3 (normalization) . Compute the normalized matrix B from A using Eq. 4 [ 9 , 10 ], where normalization for a term is done over all possible categories.
 B Matrix A is the input matrix containing raw frequencies, Matrix B is the output matrix with normalized frequencies, and ( t , c ) isa(term,category)coordinatewithinamatrix.The normalization equation is used to normalize the frequency count of a term in a given category by the frequency of the same term in all possible categories, which reflects how rep-resentative the term is with respect to the given category. Step 4 (term discrimination ability) . The Term Frequency times Inverse Document Frequency (TF  X  IDF) are used to favor those terms which occur frequently with a small num-ber of categories as opposed to their existence in all catego-ries [ 41 , 59 ]. While Luhn [ 41 ] asserts that medium frequency terms would best resolve a document, it precludes classifi-cation of rare medical words. Salton X  X  [ 59 ] theory, stating that terms with the most discriminatory power are associ-ated with fewer documents, allows a rare word to resolve the document.
 Step 4A . Compute the weighted matrix X from B using Eq. 5 [ 9 , 10 ][ 29 ]. IDF gives the inverse-document-frequency on term t , where c ( t ) is the number of categories containing term t .
 IDF ( t ) = log 2 Step 4B . Weight the normalized matrix by IDF values using Eq. 6 [ 9 , 10 , 29 , 32 ]. Matrix B is the normalized matrix from Step 3, IDF is the computational step defined in Step 4A, and Matrix X is a normalized and weighted matrix.
 X , c = IDF ( t )  X  B t , c (6) Step 5 (reduced singular value decomposition) . The normal-ized and weighted term-category matrix can now be used as the knowledge base for subsequent classification. A singular value decomposition variant, which incorporates a dimen-sionality reduction step allows a large term-category matrix to represent the PCR training set (see Eq. 7 ). This facilitates a category query from an unknown PCR using the LDWR [ 36 ] determined terms [ 9 , 10 , 15 ].
 X = U  X  S  X  V T (7) Matrix X is decomposed into three matrices: U is a ( T  X  k matrix representing term vectors, S is a ( k  X  k ) matrix, and V is a ( k  X  C ) matrix representing the category vectors. The value k represents the number of dimensions to be finally retained. If k equals the targeted number of categories to model, then SVD is performed without the reduction step. Therefore, in order to reduce the dimensionality, the condi-tion k &lt; | C | is necessary to reduce noise [ 15 ]. 3.2 Testing Given an unknown PCR form, the task is to determine the form categories, and construct a reduced lexicon from those classified categories to drive the word recognizer, LDWR [ 36 ]. In addition, the categories determined can be used to tag the form which can be subsequently used for informa-tion retrieval. The testing phase is divided into the following steps: (1) term extraction, (2) pseudo-category generation, (3) candidate category selection, and (4) reduced lexicon rec-ognition [ 9 , 10 ].
 Step 1 (term extraction) . Given a new PCR image, all image words are extracted from the form, and LDWR [ 36 ]isusedto produce a list of confidently recognized characters for each word. These are used to encode the positional uni/bi-grams consistent with the format during training. All combinations of uni/bi-phrases in the PCR form are constructed. Each word has exactly one uni-gram and exactly one bi-gram. A phrase consists of exactly two unknown words. Therefore it is rep-resented by precisely four uni/bi-phrases (BI-BI, BI-UNI, UNI-BI and UNI-UNI).
 Step 2 (pseudo-category generation) .A ( m  X  1 ) query vector Q is derived, which is then populated with the term frequen-cies for the generated sequences from the term extraction step. If a term is not encountered in the training set, then it is not considered. Positional bi-grams are generated to yield the trained terms 37% of the time, and similarly positional uni-grams 57% of the time. The experiments here illustrate this to be a sufficient number of terms. A scaled vector rep-resentation of Q is then produced by multiplying Q T and U .
Oncethepseudo-categoryisderived,R-SVDisappliedfor the following reasons: (1) it converts the query into a vector space compatible input and (2) the dimensional reduction can help reduce noise [ 15 ]. Since the relationship between terms and categories is scaled by variance, the reduction allows parametric removal of less significant term-category rela-tionships.
 Step 3 (candidate category selection) . The task is now to compare the pseudo-category vector Q with each vector in V  X  S The cosine rule is used for matching the query [ 9 , 10 ]. Both x and y are dimensional vectors used to compute the cosine in Eq. 8 . Vectors x and y in the equations represent the compar-ison of the vectors: pseudo-category Q with every column vector in V r  X  S r . z = cos ( x , y ) = x Each cosine score is mapped onto a sigmoid function using the least square fitting method, thereby producing a more accurate confidence score [ 9 , 10 ]. The least squares regres-sion line used to satisfy the equation f ( x ) = ax + b are shown in Eqs. 9 and 10 [ 39 ]: a = n b = 1 The fitted sigmoid confidence is produced using the cosine score and the regression line, using Eq. 9 : confidence ( a , b , z ) = The confidence scores are then used to rank the categories. If a category is above an empirically chosen threshold, then that category is retained for the PCR. Multiple categories may be thus retained.
 Step 4 (reduced lexicon recognition) . All words correspond-ing to the selected categories are then used to construct a new reduced lexicon which is submitted to the LDWR recog-nizer [ 36 ] on its second round (i.e. bootstrapping/feedback loop). Note that the first LDWR execution round occurred during Step 1 Term Extraction . Given a test PCR form, and thereducedlexicon,theLDWR[ 36 ]convertsthehandwritten medical words to ASCII. Each word which is recognized is compared with the truth. However, a simple string compari-son is insufficient due to spelling mistakes and root variations of word forms which are semantically identical. This occurs 20% of the time within the test set words. Therefore, Porter stemming [ 33 , 52 , 57 ] and Levenshtein String Edit Distance [ 4 ] of 1 allowable penalty are performed on both the truth and the recognizer result before they are compared. Levenshtein is only applied to a word that is believed to be  X  4 characters in length. For example, PAIN and PAINS are identical. How-ever,thisalsoresultsinanimpropercomparisoninabout11% of the corrections. These corrections would only affect the performance measurements of match rate and do not affect the systems recognition ability.
 4 Recognition experiments Our training data consists of 750 PCR forms and the test data consists of a separate blind set of 62 PCR forms. In all experiments it is assumed that the word segmentation and extraction has been performed by a person. Also, forms in which 50% of the content is indecipherable by a human being are omitted. This occurs 15% of the time. A description of the training and test sets can be found in Table 5 . 4.1 Performance measures Table 6 contains seven columns corresponding to perfor-mance measure in recognition performance. These fields are EXP, ACCEPT, ERROR, RAW, LEX, ABSENT and ILLEG which are explained as follows: EXP: the experiment that the performance values refer to.
ACCEPT (accept recognition rate): number of words the wordrecognizeracceptsaboveanempiricallydecidedthresh-old.

ERROR (error recognition rate): number of words incor-rectly recognized among the accepted words.

RAW (raw recognition rate): top choice word recognition rate without use of thresholds.

LEX (lexicon size): the lexicon size for the experiment after any reductions.

ABSENT (truther word not present in the lexicon): per-centage of words (for a specific experiment) not in the lexi-con as a result of incorrectly chosen categories or due to the absence of that word in the training set.

ILLEG (word is illegible to a human being): percentage of the ABSENT set in which even human beings could not reliably decipher all or some of the characters in the word (given the context).

Table 7 contains conclusions in raw recognition and error rate based on the experiments in Table 6 . These fields are RAW and ERROR which are explained as follows:
RAW: shows the improvement (denoted by an upward arrow in Table 7 ) in raw recognition performance between experiments.

ERROR: shows the reduction (denoted by a downward arrow in Table 7 ) in the incorrect accept rate between exper-iments. 4.2 Experiments This section describes several kinds of experiments which correspond to Table 6 . The purpose of these experiments is tocompareandcontrastthetheoreticalmaximumrecognition performance with the actual recognition performance. There are 4 major types of experiments: (C)omplete, (A)ssumed, (R)educed, and (S)ynthetic. The complete experiment means the recognizer was executed with the full lexicon. The ass-umed experiment means that a theoretically reduced lexicon is constructed under the assumption that the medical form categories are supplied by an oracle. The reduced experiment means that the actual latent semantic analysis in this paper is used to extract a reduced lexicon from recognized medi-cal form categories. The synthetic experiment means that the uni/bi-grams were theoretically known (i.e. the handwriting recognizer always extracted 2 characters with 100% accu-racy). However, since all words in a test set may not have been seen in a training set, the 4 experiments are executed in two modes: (1) with just words from the training set lexicon (L) and (2) words merged from both the training lexicon and testing sets (LT). These two modes allow us to compare the performance in situations of known versus unseen words in a form. To indicate in the charts the different of each of 4 experiments in 2 modes, we use acronyms: CL and CLT for complete lexicon analysis in mode 1 and 2 respectively, and similarly AL versus ALT, SL versus SLT, and finally RL ver-sus RLT. The experimental results can be found in Tables 6 and 7 with discussion that follows. 4.3 Discussion In reference to Table 7 which is computed from the most relevant changes of Table 6 : The theoretical RLT (i.e. com-paring RLT to CLT) improves the RAW match rate by 7.48% and drops the error rate 10.78% with a degree of reduction  X  = 61 . 59%. The practical RL (i.e. comparing RL to CL) improves the RAW match rate by 7.42% and drops the error rate by 10.88%. The RLT and RL numbers are close due to the difference in the initial lexicon sizes: CLT/RLT starts with 6,561 words (i.e. training set and testing set lexicons) whereas the CL/RL starts with 5,029 words (i.e. training set lexicon only). The RLT lexicon is more complete, but the lexicon is larger. The RL lexicon is less complete, but the lexicon is smaller. Thus, RLT gives the advantage that the recognizer has a greater chance of the word being a possible selection and RL gives the advantage of the lexicon being smaller. The ALT shows the theoretical upper bound for the paradigm: (1) the categories are correctly determined 100% and (2) the lexicon is complete. The ALT (i.e. compar-ing ALT to CLT) improves the RAW match rate by 17.58% and drops the error rate 24.53% with a degree of reduc-tion  X  = 83 . 01%. The synthetic experiments (SL and SLT) also do not offer much improvement which shows perfect character extraction does not guarantee recognition improve-ment. This is due to two reasons: (1) a form is a representation of many characters and so some incorrectly recognized char-acters are tolerated and (2) the remaining words on the form to be recognized are difficult to determine even when the lex-icon is constructed with only the words of known uni/bi-gram terms.

Table 8 provides insight into the effectiveness of the lexi-con reduction from the complete lexicon (CL) to the reduced lexicon (RL) experiments. The performance measures for lexicon reduction as described by Madhvanath [ 42 ] and Govindaraju et al. [ 26 ] are used with alteration to the defi-nition of reduction efficacy. The Accuracy of Reduction  X  = E ( A ) , such that  X   X  X  0 , 1 ] [ 42 ], and A is a random vari-able [ 5 ], indicates the existence of the truth in the lexicon. The function E computes the expectation [ 5 ]. The Degree of Reduction  X  = E ( R ) , such that  X   X  X  0 , 1 ] [ 42 ], represents the mean size of the reduced lexicon. The Reduction Efficacy  X  = a measure of the effectiveness of a lexicon with respect to a lexicon driven recognizer. This formula is defined differently in this research to weigh the importance of accuracy over the reduction and include the reductions effect on the recognizer. The larger the efficacy value is, the better is the effectiveness of the reduction for one recognizer versus another. The larger the Lexicon Density LDWR ( L ) = ( X  LDWR ( L ))( f LDWR (  X  d to denote a distance metric between two supplied words) the more similar or close the lexicon words are [ 26 ]. A supple-mental distance measure denoted by the N-Gram Lexicon Distance Metric d LDWR ( X  i , X  j ) =  X ( X  i , X  j )/ ( X  i introduced in this research and substituted into the lexicon density equation , provides a measure of uni/bi-grams exist-ing within the lexicon. The value  X  represents the number of uni/bi-gram terms that are not common between  X  i and  X  j denotes the total number of uni/bi-gram term combina-tions between  X  i and  X  j . In order to distinguish between the lexicon density distance metric and the n-gram lexicon distance metric equations,thevalues and willberespec-tively used. The lexicon density distance metric shows less confusion among lexicon words considering all the charac-ters are equally important. This implies that the reduced lex-icon will be less confusing to the recognizer. The n-gram lexicon distance metric shows an increase in the quantity of words with common NSI encodings. This implies the rec-ognizer has a greater chance of selecting a word using the confidently selected characters. 5 Search experiments The ability to query a set of PCR medical forms which match a user supplied input phrase is important for Health Surveil-lance applications. Searching text in digital format is easily accomplished but this is much harder to do for scanned hand-written documents. While searching handwriting has only been demonstrated in certain areas [ 56 ]. The experiments in this section illustrate search effectiveness even when words areincorrectlyrecognized.BoththeoriginalLDWR(CL)and the reduced lexicon LDWR (RL) PCR medical form data sets are compared.

In order to have a query set of sufficient size, the test set is constructed using a leave-1-out strategy. There are eight rounds of recognition such that each round of the 800 PCR X  X  are divided into two different groups of 100 and 700. During each of the rounds, the content of the 100 PCR X  X  is recog-nized using the 700 PCR X  X  as the training data. This allows the full set to be evaluated with no bias. Finally, a set of 1175 phrases, constructed from adjacent non-stopwords, are extracted from a blind set of 200 PCR forms (i.e. these 200 forms are not a subset of the 800 set) such that each phrase is found in at least one form in the 800 set. Each of the query phrase in the query set consists of exactly two words. Differ-ent experiments are conducted which search the PCR forms for at least one of the words or both the words from the input query phrase. d ( a i , b j ) = w ij  X  A query is performed by scanning the forms in the 800 test set for recognized words that match a two-word input query phrase. Any LDWR recognized form which contains the occurrence of both query words independently in the doc-ument are considered matched results. Relevancy is deter-mined if the input query words, for example CHEST and PA I N , are actually found on that form according to the human truth. A two-step ranking algorithm is then performed on all matching documents. First documents are ranked according to the frequencies of the occurring words. Second, those doc-uments with the same word frequency are ranked using the distance measurement in Eq. 12 .Let d ( a i , b j ) be a function which computes the distance between two matched words, a and b j such that i and j respectively represent the word position in the document. w ij here is a weight based on the frequency of occurrences of words a and b in the document. This is especially necessary in situations where word a exists and b does not, and vice versa. Documents with closer prox-imity words are given a higher rank. Discussion on proxim-ity based metrics can be found here [ 23 ]. Finally, the search methods are evaluated using the standard trec_eval system. To account for cases, where the system improperly returns no documents for a given query, -c option of trec_eval is used to include the relevance count of these queries in the final calculation. 5.1 Performance measures MAP (mean average precision) is the mean of the average precision of all individual queries in the set. Average preci-sion of a single query is defined as the mean of the precision after every relevant document retrieved. This performance measure emphasizes on retrieving relevant documents ear-lier.
 R-prec (R-precision) is the precision at R , where R denotes the total number of relevant documents for the given query. This measure emphasizes on retrieving more relevant docu-ments. 5.2 Experiments AND CL . Given a query phrase of two words, both words are found in a PCR form during the search process using a complete training lexicon.

AND RL . Given a query phrase of two words, both words are found in a PCR form during the search process using a reduced training lexicon.

OR CL . Given a query phrase of two words, at least one of the words is found in a PCR form during the search process using a complete training lexicon.

OR RL . Given a query phrase of two words, at least one of the words is found in a PCR form during the search process using a reduced training lexicon.

ESI . An additional query expansion experiment was also performed in which a document was matched if at least one ESI encoding sequence was found in the document (i.e. the requirement for matching words was removed). For example, consider input query phrase CHEST PAIN where CHEST is decomposed into CH, CE, CS, CT, HE, HS, HT, ES, ET, C, H, E, S, and T., and PA I N is decomposed into PA, PI, PN, AI, AN, IN, P, A, I, and N. Since the input phrase is known, and hence the spatial encodings between characters are also known, the ESI encodings for the terms are known. The ESI encodings for CHEST are decomposed into: 0C0H3, 0C1E2, 0C2S1, 0C3T0, 1HE2, 1H1S1, 1H2T0, 2E0S1, 2E1T0, 0C4, 1H3, 2E2, 3S1, and 4T0. The ESI encodings for PA I N are decomposed into: 0P0A2, 0P1I1, 0P2N0, 1A0I1, 1A1N0, 2I0N0, 0P3, 1A2, 2I1, and 3N0. Finally, all possible ESI sequences from the input words are generated: 0C0H3 $0P0A2, 0C0H3$0P1I1, 0C0H3$0P2N0, 0C0H3$1A0I1, etc. 5.3 Discussion The experimental results for each algorithm in terms of MAP and R-precision are shown in Fig. 6 . As shown, retrieval based on reduced lexicon (RL) outperform retrieval based on complete lexicon (CL). This behavior is observed irre-spective if the search is performed using both words from the query phrase (AND) or at least one of the words from the query phrase (OR). An interpolated 11 -point precision curve shown in Fig. 7 also supports this observation. As shown in the figure, after a recall level of 0 . 2, OR-RL method retrieves relevant documents earlier in the order as compared to OR-CL method. In the case of AND logic, RL based method performs better than CL based methods at all recall levels. The improvement in the search performance due to lexicon reduction algorithm used highlights the effectiveness of the proposed method. For the query expansion experiment (ESI) as intuitively expected, the uni/bi-grams match more terms in the test set due to the loss in word information. The precision chart in Fig. 7 illustrates this drop in retrieval effectiveness and shows that searches are more effective at the word level rather than raw encoding level. Similar drop in performance isobservedforthequeryexpansiontechniqueinFigs. 6 and 8 .
To study the effect of different methods on the total num-ber of relevant documents retrieved, we also compute the value of recalland precision levels independently for the top k documents retrieved as shown in Fig. 8 . The results from Fig. 8 suggest that reduced lexicon (RL)-based methods not only retrieve relevant documents earlier, but also retrieve more relevant documents overall as compared to their coun-terpart complete lexicon (CL)-based methods. The contribu-tion of this research is that the lexicon reduction strategy (i.e. the RL experiment) improves both handwriting recognition and search effectiveness. 6 Conclusions This paper defines a new paradigm for lexicon reduction and information retrieval in the complex situation of handwriting recognition of medical forms. An improvement in raw rec-ognition rate from about 25% of the words on a PCR form to approximately about 33% has been shown with a reduction in false accepts by about 7%, a reduction in error rate by about 10 X 25%, and a lexicon reduction from 32 X 85%. The addition of a category driven query facilitates a mean aver-age precision of 0 . 28 and R-prec of 0 . 35 for 1175 queries in a search engine experiment with medical forms. Addi-tionally, using a reduced lexicon for searching medical form also enables retrieving more relevant number of documents overall, as compared to complete lexicon search.

Interestingly,certaincomputationalelementsofbootstrap-ping, described in our work, are consistent with the human interpretation of ambiguous handwriting using contextual cues. Our methodology accomplishes this by modeling char-acter terms as a higher level semantic concept which mimics the human ability to recognize a word within context, when some characters are unknown.
 References
