 A similarity query considers an element as the query center and searches a dataset to find either the elements far up to a bounding radius or the k nearest ones from the query center. Several algorithms have been developed to efficiently execute similarity queries. However, there are queries that require more than one center, which we call Aggregate Similarity Queries. Such queries appear when the user gives multi-ple desirable examples, and requests data elements that are similar to all of the examples, as in the case of applying rel-evance feedback. Here we give the first algorithms that can handle aggregate similarity queries on Metric Access Meth-ods (MAM) such as the M-tree and Slim-tree. Our method, which we call Metric Aggregate Similarity Search (MASS)  X 
This material is based upon work supported by FAPESP (S  X ao Paulo State Research Foundation), CAPES (Brazil-ian Coordination for Improvement of Higher Level Person-nel), CNPq (Brazilian National Council for Supporting Re-search), the National Science Foundation under Grants No. IIS-0705359 CNS-0721736 and under the auspices of the U.S. Department of Energy by University of California Lawrence Livermore National Laboratory under contract DE-AC52-07NA27344 (LLNL-CONF-404625). This work is also par-tially supported by an IBM Faculty Award, a Yahoo Re-search Alliance Gift, a SPRINT gift, with additional fund-ing from Intel, NTT and Hewlett-Packard. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the author(s) and do not necessar-ily reflect the views of the National Science Foundation, or other funding parties.
 has the following properties: (a) it requires only the triangle inequality property; (b) it guarantees no false-dismissals, as we prove that it lower-bounds the aggregate distance scores; (c) it can work with any MAM; (d) it can handle any num-ber of query centers, which are either scattered all over the space or concentrated on a restricted region. Experiments on both real and synthetic data show that our method scales on both the number of elements and, if the dataset is in a spatial domain, also on its dimensionality. Moreover, it achieves better results than previous related methods. H.3.3 [ Information Systems ]: Information Search and Re-trieval X  Search process Algorithms, performance Similarity queries, aggregate dissimilarity, access methods
Nowadays, the massive use of complex data, such as mul-timedia, time series, genetic sequences, among others, in-creases the challenges on database management systems to efficiently organize and retrieve them. Since the main queries asked over this kind of data takes into account their similarity, metric spaces are well suited to represent com-plex data, where only the elements and their pairwise dis-tance matters [15]. A metric space is defined as a pair &lt;
S , X  () &gt; , where S is the data domain and  X  () is a distance function  X  : S  X  S  X  R + that complies with the following 0 &lt;  X  ( s 1 ,s 2 ) &lt;  X  ,  X  s 1 6 = s 2 ; identity:  X  ( s and triangle inequality:  X  ( s 1 ,s 2 )  X   X  ( s 1 ,s 3 ) +  X  ( s  X  s 1 ,s 2 ,s 3  X  S .
The most common similarity queries are the k -nearest neighbor and the range queries. However, they retrieve ele-ments that are similar to only one reference element, called the query center. In this paper, we present a novel and very efficient technique to execute Aggregate Similarity Queries called MASS, the Metric Aggregate Similarity Search tech-nique. An aggregate similarity query retrieves elements based on their composite similarity regarding multiple query centers. As the MASS technique demands to know only the data elements and the distances among them, it is able to work in metric spaces. We assume that aggregate similarity queries can be limited either by an aggregate radius  X  (Ag-gregate Range Query  X  ARq) or by an amount k of elements (Aggregate k -Nearest Neighbor Query  X  k ANNq).

Figure 1 presents examples of the aggregate 1-nearest neighbor query over the set of American cities, where the set of query centers is Q = { Sacramento (California), Atlanta (Georgia), Columbus (Ohio) } using the distances computed by the spherical law of cosines. The similarity is aggregated in three distinct ways. As shown in Figure 1-a, the mini-mization of the maximum distance from each query center to the answer retrieves Hartman (Colorado). In Figure 1-b the minimization of the mean square distance retrieves Har-veyville (Kansas), and in Figure 1-c the minimization of the sum of the distances retrieves Elizabethtown (Kentucky).
In order to perform similarity queries, each element of a dataset is represented either as a vector in a multidimen-sional space (e.g. color features extracted from images), as a function (e.g. the time series or sensor recordings), as graphs (e.g. fingerprints, each of which may be represented by a structure of deltas and endings) or by a string of sym-bols (e.g. DNA sequences). For each type of data there must exist a distance function  X  () that follows the metric space properties. It is worth to note that the Minkowski metrics, broadly employed to compare multidimensional data, are special cases of distance functions. In fact, provided with adequate distance functions, the metric space includes the multidimensional space. In this paper we aim at general-izing the concept of aggregate similarity queries. Thus, we focus on the metric space model, as it is less restrictive and embodies all the mentioned types of datasets.

It is common that a similarity query posed by a human analyst has just one center: users usually want the k images in a database that are the most similar to the one they have, or the DNA sequences that do not differ more than a given number of nucleotides from the one they just sequenced. However, automated processes can also require posing sim-ilarity queries, and in those cases, it is common to have a set of query centers. For example, the following applica-tions can take advantage of the ability to perform aggregate similarity queries.
Despite those applications, there are very few works re-lated to the concepts of aggregate similarity queries, as de-scribed in Section 2. In this work we define them as a new query type and discuss interesting metric spaces properties that allowed the development of the optimization strategies. Based on these properties, we present new algorithms that can be executed over existing metric access methods. Hence, no special structures must be developed to take advantage of them.

Let us remember a few fundamental concepts of the rela-tional model to provide a proper definition of aggregate sim-ilarity queries regarding the database theory and to allow it to be used in the intended applications (which probably have the data stored in relational databases). Such concepts must remain valid when retrieving elements by the similarity of their contents.

Suppose a relation R with n attributes described by a relational schema R = ( S 1 ,...,S n ) is composed of a set of tuples t i . Therefore, if the relation stores m tuples, then R = { t 1 ,...,t m } . Each attribute S j , 1  X  j  X  n , indicates a role for domain S j , that is S j  X  S j . Therefore, when the complex domain from a metric space, each attribute S j stores complex values. Each tuple of the relation stores one value for each attribute S j , where each value s i , 1  X  i  X  m , assigned to S j is an element taken from domain S j and the dataset S j is composed of the set of elements s i that are assigned to the attribute S j in at least one tuple of the stored relation. Notice that more than one attribute S S k from R can share the same domain, that is, it is pos-sible to have S j = S k . Regarding complex domains from a metric space, the elements must be compared by similar-ity, using a distance function  X  () defined over the respective domain. Elements can be compared using the properties of the domain, regardless of the attributes that store the ele-ments. Therefore, every pair of elements from one attribute and from distinct attributes sharing the same domain can be compared.

This paper is organized as follows. Section 2 summarizes existing related work. Section 3 presents the fundamen-tal concepts regarding aggregate similarity queries. Sec-tion 4 describes the MASS optimization strategy and the algorithms developed to execute aggregate similarity queries in metric spaces. Section 5 discusses the experiments per-formed, showing that the proposed algorithms outperform sequential scan and the available related approaches by or-ders of magnitude when executing typical aggregate simi-larity queries. Section 6 brings the conclusions and Table 1 lists the symbols used in this paper.

Metric Access Methods (MAM) are indexing structures well-suited to organize complex data that must be searched by similarity. Many of them were developed to improve single-center similarity search operations (such as k -nearest neighbor and range queries), using the triangle inequality property to prune branches of the trees. Examples of such methods are the M-tree [2] and the Slim-tree [13], which are dynamic, height-balanced, disk-based structures with fixed size disk pages and bottom-up construction. They have two types of nodes: indexes and leaves. An index node is composed of a set of entries, where each entry consists of a pointer to a subtree, a routing element s t (also called the node representative) and the covering radius r t centered at s t , covers all the elements stored in the sub-tree. Leaf nodes store all data elements. A single-center range query regarding a query center s q and radius  X  allows pruning a subtree search when the following condition holds:  X  ( s t ,s q ) &gt; r t +  X  .

Many approaches have been proposed to improve the performance of k -NN queries as, for instance, branch-and-bound [9, 2, 10], incremental [4, 5] and multi-step algorithms [6, 11]. Other approaches are to estimate a final limiting range for the query and to perform a sequence of  X  X tart small and grow X  steps [12]. All of these works refer to algorithms dealing with just one query center.

The aggregate range query was first proposed in [14] and was used as a relevance feedback mechanism for content-based image retrieval in a system named Falcon. The search algorithm proposed there consists of the union of single-center range queries executed at each query element, fol-lowed by a filtering step that evaluates if each element in the union set meets the aggregate similarity condition. However, it depends on an threshold given by the user to bound each range query, whose semantics depends on the user X  X  notion of similarity on the dataset. Moreover no procedure was developed for the evaluation of .

Regarding aggregate k -nearest neighbor queries, the work presented in [7] proposes the Minimum Bounding Method (MBM). This method is based on the use of an aggregate function to rank low-dimensional spatial data and solve ag-gregate k -nearest neighbors. As it was intended to be used on spatial data, it uses geometric properties to perform the pruning on R-trees, and it cannot be generalized to metric data. In [8], the generalization of the two most common similarity queries was proposed as the aggregate similarity queries, aimed at retrieving dataset elements based on their composite similarity regarding multiple centers. It uses ag-gregate similarity queries as a relevance feedback mecha-nism, but no algorithm was given to execute the queries.
In this paper we derive properties that allow pruning sub-trees of MAM, such as the M-tree and the Slim-trees, when executing both range and k -nearest neighbor aggregate sim-ilarity queries, and present the first algorithms that allow a fast computation of both kinds of aggregate similarity queries. These properties are at the kernel of MASS, and they can be applied to any metric space, which include, but it is not limited to, spatial data of any dimensional-ity. The techniques proposed herein do not depend on any external threshold or other parameter besides those in the query predicate.
Aggregate similarity queries correspond to relational se-the form  X  S j  X  Q  X  , and expresses similarity comparison be-tween the set of values S j  X  S j of an attribute S set of constant values Q  X  S j , called the reference or query centers set, taken from the domain S j and given as part of the query predicate. In order to perform the comparison, they require the definition of a similarity aggregation func-tion d g (), which evaluates the aggregate similarity of each element s i  X  S j regarding its similarity measured by the metric  X  () to every element s q  X  Q .

We represent an aggregate similarity selection over at-tribute S j of relation R as where  X  denotes a similarity operator and ` is the limiting condition that elements returned by the aggregation func-tion d g () should meet in order to be selected. There are two basic ways to limit the number of returned elements. The first is based on a given similarity threshold  X  , and the sec-ond is based on a number k of elements. The answer to an aggregate similarity selection is a subset of tuples from R , whose elements s i  X  S j meets the query predicate.
It is important to note that if a proper aggregation func-tion d g () is employed, the similarity predicate of the well-known similarity range and k -nearest neighbor queries turn into special cases of the aggregate similarity predicates, where the set of query centers has only one element Q = { s and the limit ` is either the range radius or the number of neighbors, respectively. The similarity selections exhibit properties distinct from those of the traditional selections (for example, they do not possess the commutative prop-erty), so we use the ( X   X  ) symbol instead of the traditional  X  . We consider aggregate similarity queries defined as follows.
Definition 1. Aggregate Range Query (ARq) : given a maximum aggregate query distance  X  , a similarity aggre-gation function d g () and a set of query centers Q , the query ARq retrieves every tuple t i from R whose value s i from attribute S j satisfies d g ( s i ,Q )  X   X  . An aggregate range se-lection can be expressed as  X   X 
Definition 2. Aggregate k -Nearest Neighbor Query (kANNq) : given an integer value k  X  1, the query kANNq retrieves the k tuples t i from R whose values s i from attribute S j minimize the similarity aggregation function d g () regarding the query centers Q . An ag-gregate k -nearest neighbor selection can be expressed as  X   X 
Following the definition of metric spaces, the dissimilarity function  X  () can be any function comparing pairs of elements s ,s j  X  S j that satisfies the properties of a metric distance function. However, as the set of query centers Q may have more than one element, the distances  X  ( s i ,s q ) from each query center s q  X  Q to the element s i  X  S j must be ag-gregated. Therefore, we define the similarity aggregation function as follows.

Definition 3. Similarity Aggregation Function ( d g () ) : given a set of query centers Q and an attribute S j sharing the same domain S j , that is Q,S j  X  S j , the similarity aggregation function d g : P ( S j )  X  S j  X  aggregates the similarity of each query center s q  X  Q to the element s i  X  S j .

The similarity predicate uses the value of the similarity ag-gregation function to rank the elements in S j with respect to Q . There are several ways to define the similarity aggre-gation function d g (). In this paper we consider the class of functions generated by where  X  () is a distance function over S j , Q is the set of query centers, s i is a dataset element, and the power g  X  is a non-zero real value that we call the grip factor of the similarity aggregation function.

Considering Equation 1, the aggregate range and the ag-gregate k -nearest neighbor queries applied over a unitary set Q correspond to the traditional range and k -NN queries re-spectively. Figure 2 presents the effect of the grip factor g in a 2-dimensional Euclidean space, considering Q composed of the four query centers shown. Each curve represents a geo-metric place where Equation 1 has the same value. There-fore, each curve is an isoline representing a different covering radius, thus defining both range and k -limited queries. As it can be noted, for g &lt; 1 they may generate disjunctive regions. Figure 2: The effect of the grip factor g in an Euclidean 2-dimensional space, considering Q = { q Equation 1 provides interesting minimization functions. For example, g = 2 defines the minimization of the mean square distance (shown in the example in Figure 1-b) and g = 1 defines the minimization of the sum of the distances (shown in Figure 1-c). The minimization of the maximum distance ( g =  X  , shown in Figure 1-a) and the minimiza-tion of the minimum distance ( g =  X  X  X  ) are respectively presented in Equations 2 and 3, as follows:
The computational complexity of the sequential scan to solve the aggregate range and aggregate k-NN regarding the number of distance calculations is O ( n  X  c ), where n is the number of elements and c is the number of query centers. As shown in the experiments, this is a major parameter af-fecting the time to answer the query, so it is important to provide techniques to accelerate its calculation.
In this section we analyze how to solve queries using the similarity aggregation function shown in Equation 1, considering the most meaningful values of the grip factor without loss of generality, let us consider Figure 3 as an example of an aggregate range query in a 2-dimensional Eu-clidean space and grip factor g = 1. Let us consider a query centered at { q 1 ,q 2 } and a subtree branch centered at s covering radius r t . Let h 1 be an unknown hypothetical el-ement that could be stored in the subtree that minimizes d () with respect to q 1 and q 2 . In the discussion following, we use the distances a,b,c,d,w and x as defined in Figure 3, which are independent of the real distance function  X  () and of the grip factor g employed. The challenge here is to compute the lower bound aggregate similarity from centers q 1 and q 2 to h 1 in order to decide if the aggregate range query overlaps the region covered by the subtree branch. Figure 3: Aggregate range query in an Euclidean 2-dimensional space, g = 1 , { q 1 ,q 2 } are the query centers, s t is a subtree node representative and r t is the subtree covering radius.

As seen in Figure 3, r t is known beforehand and { a,c } can be calculated, but { b,d } cannot. To assure that a subtree centered at s t with covering radius r t can be pruned, we need to determine if d g ( Q,h 1 ) = g the query region, i.e., if the two regions overlap. If they do not overlap, the subtree centered at s t can be pruned. From the definition of distance functions on a metric space, the following triangle inequality property always holds:
Thus, for g = 1, d g = b + d :
For g = 2, d g = 2
Generalizing, it can be stated that, for any g 6 = 0:
The following examples show other special cases of those properties for meaningful values of g . For g = 1 / 2, d (  X 
For negative values of g , the relation also holds. For g =
Other examples include g =  X  , d g = and g =  X  X  X  , d g = lim g  X  X  X  X  ( g
Notice that the aggregate radius  X  is a parameter of the query. The combination of the above restrictions for any grip factor g 6 = 0 are useful to define a general algorithm to compute the overlap between the region covered by an aggregate range query whose query references are given as Q and the region covered by a subtree centered at s t with covering radius r t . Those restrictions are the kernel of the MASS technique, and are employed in the proposed aggre-gate similarity query algorithms to decide whether overlaps occur.
The proposed aggregate range query algorithm employs the traditional straightforward depth-first traversal ap-proach, as it is used for the range query algorithm defined for metric access methods such as the M-tree and the Slim-tree. It starts from the root node, and recursively navigates through the hierarchy checking whether a subtree can be pruned. Algorithm 2 presents the aggregate range query. It receives the set of query centers Q , the grip factor g and the aggregate radius  X  , and returns the similarity-ordered list result , containing the elements s i  X  S stored in a MAM Algorithm 1 MASS  X  Verify if a range query region over-laps a subtree-covered region.
 3: return true 4: end if 5: end for 6: if  X   X  7: return true 8: else 9: return false 10: end if that meets the query parameters. The list is ordered by the aggregate dissimilarity between each element and the set of query references. Notice that any hierarchical metric struc-ture that organizes the dataset as subtrees centered at an element and having a covering radius, such as the M-tree and the Slim-tree, can be used. In step 3, Algorithm 2 calls Algorithm 3, the Aggregate Range Query Traversal, navi-gating the structure recursively. The algorithm returns a distance-ordered list of elements.
 Algorithm 2 Aggregate Range Query over a given metric index 1: result = newElementList () 2: node = GetRootNode () 3: AggregateRangeQueryTraversal ( 4: return result
The Step 2 of Algorithm 3 performs a depth first traver-whether or not to prune each subtree. Step 4 performs the recursion in case it is needed to analyze the subtree pointed by node.entry i . When a leaf node is reached (step 8), the qualifying elements are added to the answer set. In step 9, Equation 1 is used to compute the aggregate distance from an element to the set of query centers Q .
 Algorithm 3 Aggregate Range Query Traversal 1: if node .Type is index then 2: for each representative element s i in node do 4: AggregateRangeQueryTraversal ( node.entry i , 5: end if 6: end for 7: else if node .Type is leaf then 8: for each element s i in node do 10: if d  X   X  then 12: end if 13: end for 14: end if
Algorithm 4 shows the aggregate k -nearest neighbor pro-posed. It is based on the best-first approach employed for nearest-neighbor algorithms, extended to handle multiple query centers. The idea is to use the elements retrieved from the index structure during the traversal to progressively re-duce a dynamic query radius, initially set to infinity. The radius is reduced as better elements are found, computing their aggregate distance to the set of query centers as a dy-namic range threshold. After at least k elements are found, the value of range is set to the last aggregate distance in the result list, thus converging to the aggregate distance of the last element in the result set. The priority queue used in the algorithm is a binary red-black search tree that stores data elements ordered by their aggregate distance computed from the query centers Q . Notice that, in step 8, MASS is employed to decide whether to add a subtree to the priority queue.
 Algorithm 4 Aggregate Nearest Neighbor Query over a given metric index 1: result = newElementList () 2: queue = newPriorityQueue () 3: node = GetRootNode () 4: range =  X  5: while not queue .Empty() do 6: if node .Type is index then 7: for each representative element s i in node do 10: end if 11: end for 12: else if node .Type is leaf then 13: for each element s i in node do 15: if d  X  range then 17: if result .Count  X  k then 18: result .Cut( k ) 19: range = result .GetMaxDistance() 20: end if 21: end if 22: end for 23: end if 24: node = queue .GetNextNode() 25: end while 26: return result
The experiments were performed comparing the sequen-tial scan, the related methods available in the literature and the proposed algorithms implemented over the Slim-tree metric access method. For comparison purposes, the sequential scan was implemented as a flat file composed of fixed-size pages with the same size in bytes of the R*-trees and Slim-trees pages. All access methods were implemented in the same framework to allow a fair comparison, for ex-ample, guaranteeing that the same data types are used in the structure that represents a dataset element for all meth-ods. The graphics present the results for the execution of 100 similarity aggregation queries using distinct sets of ran-domly selected query centers. All distances were computed using the Euclidean ( L 2 ) distance function. All experiments were executed in an Intel Pentium D 3.4 GHz CPU, 1 GB of RAM and 200 GB of disk space. The graphics show the be-havior of the algorithms for three main parameters regarding the analysis of the actual cost of similarity queries: the av-erage number of distance calculations, the average number of disk accesses and the total time to execute the queries.
This set of experiments aims at evaluating the behavior of the aggregate range query as the aggregate radius  X  grows. It compares the MASS approach proposed herein with the Falcon approach [14] and with sequential scan. Figure 4 presents the results of aggregate range queries performed over the Corel Image Features dataset, composed of 68,040 color histograms (32 dimensions) extracted from the Corel image collection available at [1], considering g = 1, | Q | = 10 query centers, and varying the aggregate radius  X  from 0.5 up to 3.5. It is important to note that the Falcon algorithm is based on the union of range queries with threshold radius estimated by the user. Though, it only guarantees exact results when the user does not underestimate the value of . In this experiment, we set as the average single query radius value found computing the sequential scan aggregate range query. Figure 5: Average number of elements retrieved for the experiments in Figure 4.

In addition to showing the average number of distance cal-culations (Figure 4-a), the average number of disk accesses (Figure 4-b), and the total time in log scale to execute 100 queries (Figure 4-c), we also present the number of elements returned for each aggregate radius (Figure 5). As it can be seen, the number of elements retrieved increases expo-nentially as the aggregate radius increases. The number of distance calculations, number of disk accesses and execu-tion time follow the same behavior. It is interesting to note that when the aggregate radius  X  = 3 . 5, about 4.4% of the dataset is retrieved (3,000 out of a total of 68,040 elements). Even at this high amount of data retrieved, MASS is yet 3 times faster than sequential scan. Since the Falcon al-gorithm is based on the union of range queries (that takes quadratic comparisons and time to compute), it results in higher number of disk accesses and time spent to run for high values of the aggregate radius. Moreover, by using the estimated as mentioned above, it resulted in 83% exact query answers.
This set of experiments aims at evaluating the behavior of the aggregate k -nearest neighbor queries as the number k of elements grows. It compares the MASS approach proposed herein with the R*-tree MBM approach [7] and the sequen-tial scan. The values of k varied from 50 to 500 elements for the datasets evaluated. In addition to the Corel Image Features dataset (32 dimensions) employed in the first set of experiments, we employed a dataset with a higher dimen-sionality, the ALOI-OV dataset. This dataset is composed of grey-scale histograms (256 dimensions) extracted from the ALOI Object Viewpoint [3], an image collection composed of 72,000 images.

Figure 6 presents the results of the Corel Image Features dataset, considering g = 2 and | Q | = 10 query centers. As expected for a dataset with dimensionality equals to 32, the performance of the R*-tree MBM was compared to the performance of the Slim-tree MASS algorithm for all three graphs. Both algorithms resulted in very similar distance calculations, but MASS resulted in fewer disk accesses and lesser total time.

Figure 7 presents the results of the ALOI-OV dataset, g = 0 . 25 and | Q | = 15 query centers. For a higher dimen-sionality, the performance of the R*-tree MBM was degen-erated due to the nature of this access method. Still, the MBM approach resulted in fewer distance calculations and smaller time to execute the queries than sequential scan, but accessed a higher number of disk pages, even for small values of k . However, the MASS approach kept its performance.
As it can be seen, whereas the curves for the sequential of disk accesses, (c) total execution time. execution time. scan were always constant, the values obtained by MASS are proportional to k . Considering the retrieval of k = 500 (0.7% of the dataset), which is a rather high amount of data for a similarity search, MASS required 6.3 times fewer dis-tance calculations and 4.0 times fewer disk accesses, being 6.2 times faster than sequential scan to process the dataset. It is interesting to note that we performed this experiment aimed at showing that the MASS method is scalable even for queries returning a large amount of data. In general, a similarity query asks for much fewer elements (usually a user does not want to evaluate many images), and in this case the values obtained by MASS are even better.

Figure 8 presents the results of k -nearest neighbor queries for k varying from 10 to 100 over the ALOI-OV dataset while varying the number of query centers | Q | from 2 to 10 only for MASS algorithm. An interesting effect depicted in this figure is that, whereas the distance calculations and the total time required to execute k -nearest neighbor queries in-creases linearly to | Q | (see Figure 8-a and 8-c), the average number of disk accesses increment decreases logarithmically with regard to the increment of | Q | (Figure 8-b). This oc-curs because, as | Q | increases, the region in space where the result resides becomes spherical. Therefore, the pruning ability of the MASS method is higher, reducing the number of nodes candidate to contain answers. The total time ac-companying the number of distance calculations rather than the number of disk accesses indicates that the number of dis-tance calculations is more time consuming for this dataset than the number of disk accesses.
This set of experiments aims at evaluating the scalability of MASS when applied over dimensional data. The datasets employed in these experiments are based on the ALOI Ob-ject Viewpoint [3]. We generated 6 datasets by extracting color histograms quantized in 8, 16, 32, 64, 128 and 256 col-ors for each one of the 72,000 images. As the size of the elements from each dataset with different color resolution is distinct from the other datasets, we indexed each dataset using the page size of the Slim-tree proportional to the data element size. Table 2 summarizes the indexes created. Table 2: Indexes created for the dimensionality scal-ability experiment.

The experiments measured the average number of distance calculations and disk accesses and the total time to execute 100 aggregate 20-nearest neighbor queries with random sets of | Q | = 10 and g = 0 . 5. The same sets of Q were used to perform the queries over the datasets of 8, 16, 32, 64, 128 and 256 dimensions. Figure 9 shows the results.

As it can be seen, the number of distance calculations and disk accesses for MASS presented a sub-linear behavior regarding the dimensionality. The number of disk accesses for the sequential scan is reduced because larger page sizes allow packing more elements in each page (the ratios can be seen in the line Elements/page in Table 2). The total time required to run the queries follows a super-linear growth for both MASS and sequential scan. However, MASS increases at a much smaller pace, as can be seen in Figure 9-c, allowing handling datasets of a much larger dimensionality than it is possible for sequential scan.
This set of experiments aims at evaluating the scalability of MASS regarding the number of elements of the dataset for Aggregate 20-Nearest Neighbors. The datasets used in these experiments are based on vector data with Gaussian distri-bution as in [2]. We generated 32-dimensions datasets with 100,000 to 500,000 elements. All 5 datasets were indexed using Slim-trees and R*-trees with page size of 4 KBytes. Figure 10 shows the behavior of the algorithms for the av-erage number of distance calculations, the average number of disk accesses and the total time to execute the queries. It shows the linear behavior of MASS running on Slim-tree and of MBM algorithm running on R*-tree.

Table 3 summarizes the indexes created for these exper-iments. It shows the linear behavior of the access methods for these datasets, both on the number of disk pages and on the height of the generated hierarchy.
In this paper we presented MASS, the first Metric Aggre-gate Similarity Search method to efficiently execute aggre-gate similarity queries over a dataset in a metric space. Ag-gregate similarity queries retrieve the dataset elements that minimize a given similarity aggregation function. Thus, they retrieve elements considering the aggregate dissimilarity of a dataset element to all elements of a set with any num-Table 3: Indexes created regarding the scalability on the number of elements.
 ber of query centers, according to the similarity aggregation function.

MASS takes advantage of the triangle inequality to speed up the query processing over metric access methods, allied to the proposed new properties that lower-bound the similarity aggregation function, proving that those properties guaran-tee no false-dismissals. Using those properties, MASS can work with any metric or spatial access method, and it can handle any number of query centers.

We presented new algorithms to execute aggregate sim-ilarity queries limited either by a radius threshold, which correspond to aggregate range queries, or limited by the number k of elements in the answer, which corresponds to aggregate k -nearest neighbor queries. In fact, the similar-ity aggregation function employed to develop the properties that enable optimizing a query leads to the common case of the range and k -nearest neighbor queries when the number of query centers | Q | = 1. The results from the experiments performed on real and synthetic datasets show that MASS is scalable on the number of elements in the dataset and, if the dataset is in a spatial domain, also on its dimensional-ity. Moreover, MASS scales better than related techniques as Falcon and R*-tree MBM for typical aggregate similarity queries. Therefore, we can claim that MASS is a powerful method to accelerate the execution of aggregate similarity queries, which can effectively improve the processing of rel-evance feedback methods, to perform essential operations of data mining, such as clustering, as well as the chaining of sub-queries. [1] A. Asuncion and D. J. Newman. UCI machine time. [2] P. Ciaccia, M. Patella, and P. Zezula. M-tree: An [3] J.-M. Geusebroek, G. J. Burghouts, and A. W. M. [4] G. R. Hjaltason and H. Samet. Ranking in spatial [5] G. R. Hjaltason and H. Samet. Distance browsing in [6] F. Korn, N. Sidiropoulos, C. Faloutsos, E. Siegel, and [7] D. Papadias, Y. Tao, K. Mouratidis, and C. K. Hui. [8] H. L. Razente, M. C. N. Barioni, A. J. M. Traina, and [9] N. Roussopoulos, S. Kelley, and F. Vincent. Nearest [10] H. Samet. Depth-first k-nearest neighbor finding using [11] T. Seidl and H.-P. Kriegel. Optimal multi-step [12] M. Tasan and Z. M. Ozsoyoglu. Improvements in [13] C. Traina-Jr., A. J. M. Traina, C. Faloutsos, and [14] L. Wu, C. Faloutsos, K. Sycara, and T. R. Payne. [15] P. Zezula, G. Amato, V. Dohnal, and M. Batko.
