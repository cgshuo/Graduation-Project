 Online data is available in two avors: unstructur ed data that resides as free text in HTML pages, and structur ed data that resides in databases and kno wledge bases. Unstruc-tured data is easily accessed as human-readable text on a bro wser, while structured data is hidden behind web query interfaces (web forms), web services, and custom database APIs. Access to this data, popularly referred to as the hid-den web , entails submitting correctly completed web forms or writing code to access web services using proto cols suc h as SOAP .
 The emergence of powerful searc h engines has greatly im-pro ved our abilit y to searc h for data on the web; however, suc h access is still primarily restricted to unstructured data. We can searc h for and access information available as HTML, but are not yet able to gain easy access to the hidden web. It is not easy to get to the correct web form, and even harder to nd a suitable web service. When we do nd the correct web form or web service, there is an additional step of under-standing its schema, and reform ulating the user's query to t that schema. While humans do this regularly , one form at a time , it is dicult to automate the pro cess of query reform ulation, and therefore we cannot leverage the wealth of information residing behind web forms and services. We observ e that one reason for the success of today's searc h engines is their abilit y to apply statistics computed from large collections of HTML documen ts to rank their searc h results. For example, word-o ccurrence probabilities in the body and anc hor text of HTML pages are used to iden tify words most relev ant to individual pages. Suc h analysis of a corpus of documen ts has also been used successfully for other tasks suc h as classi cation and clustering of text documen ts. Giv en that we now have a vast collection of web forms and services, the natural question that arises is whether we can leverage a corpus of these in order to automate the pro cess of query reform ulation.
 The techniques for exploiting corp ora of documen ts do not apply directly to searc hing structured data. The main rea-son is that searc hing structured data requires understanding the underlying seman tics of the data sources. This struc-ture is mostly (but not completely) speci ed by the schema. However, in specifying these seman tics, the actual words used and the information organization dep end more on the dev elop er's whim, and little variations may accoun t for very di eren t seman tics.
 We are pursuing a pro ject whose goal is to sho w that large corp ora of structures (i.e., web forms and services, database schemata) can be used to address the fundamen tal dicult y of bridging seman tic heterogeneit y. This pap er brie y re-views two recen t dev elopmen ts in this pro ject, and compares between them. Speci cally , we describ e the follo wing: While both of these works exploit a corpus of structures, the problem settings are suc h that di eren t techniques are required. Section 2 describ es the Woogle web service searc h engine, and Section 3 describ es corpus-based schema matc h-ing. Section 4 addresses the similarities and di erences be-tween the two approac hes and describ es related work and future directions in this pro ject. Web services are loosely coupled soft ware comp onen ts, pub-lished, located, and invoked across the web. A web service comprises of sev eral operations (see examples in Figure 1). Eac h operation tak es a SOAP pac kage con taining a list of in-put parameters, ful lls a certain task, and returns the result in an output SOAP pac kage. Eac h web service has an asso-ciated WSDL le describing its functionalit y and interface. A web service is typically (though not necessarily) published by registering its WSDL le and a brief description in UDDI business registries.
 The gro wing num ber of web services available within an or-ganization and on the Web raises a new and challenging searc h problem: locating desired web services. In fact, to address this problem, sev eral simple searc h engines have re-cen tly sprung up (e.g., [1]). Curren tly, these engines pro vide only simple keyw ord searc h: return services that con tain the words in the web service descriptions (obtained from the WSDL le). However, the keyw ord searc h paradigm is 1 http://www.cs.w ashington.edu/w oogle Figure 1: Sev eral example web services (not including their tex-insucien t for two reasons. First, keyw ords do not cap-ture the underlying seman tics of web services. For example, when searc hing zipcode , the web services whose descriptions con tain term zip or postal code but not zipcode will not be returned. Further, keyw ord searc h on web services does not suce for accurately specifying users' information needs: users are typically looking for a speci c operation with some speci c input or output parameters.
 To address the challenges involved in searc hing for web ser-vices, we built Woogle, a web-service searc h engine. In addi-tion to simple keyw ord searc hes, Woogle supp orts similarit y searc h for web services. Starting with a keyw ord searc h, a user can drill down to a particular web service operation. When not satis ed, a user can query for web-service op-erations similar to a given one, those that tak e similar in-puts (or outputs), and those that comp ose with a given one. These searc h primitiv es greatly reduce the tedium involved in curren t web service searc h where a user migh t have to conduct multiple searc h sessions rep eatedly mo difying the searc h keyw ords until nding the most suitable web service operation. For example, the operation GetW eather in W 2 is similar to GetT emp erature in W 1 , and thus may serv e as an alternativ e; while they pro vide weather information, Lo-calTimeByZipCo de in W 3 pro vides other information about locations, and thereb y may be of interest to the user; nally , comp osing CityStateT oZipCo de in W 4 with GetW eather in W 1 o ers a solution for getting the weather when the zip-code is not kno wn.
 Overview of our approac h : Similarit y searc h for web services is challenging because neither the textual descrip-tions of web services and their operations nor the names of the input and output parameters completely con vey the underlying seman tics of the operation. Nev ertheless, kno wl-edge of the seman tics is imp ortan t in determining similarit y between operations. Broadly speaking, our algorithm com-bines multiple sources of evidence to determine similarit y. In particular, we consider similarit y between the textual de-scriptions of the operations and of the entire web services, and similarit y between the parameter names of the opera-tions. The key ingredien t of the algorithm is a novel tech-nique that clusters parameter names found in the collection of web services into semantic ally meaningful concepts. By comparing the concepts to whic h input or output parame-ters belong, we are able to obtain much better searc h results. We thus demonstrate the abilit y to use a corpus to better searc h for web services on the Web.
 In the rest of this section we outline our similarit y searc h algorithm with particular emphasis on the clustering of pa-rameter names. We also presen t results that demonstrate the e ectiv eness of our approac h. For more details on this work, the reader is referred to [9]. To e ectiv ely compare the inputs and outputs of web-service operations, it is crucial to get at their underlying seman-tics. However, this is hard for two reasons. First, parame-ter naming is dep enden t on the dev elop ers' whim. Param-eter names tend to be highly varied given the use of syn-onyms, hypern yms, and di eren t naming rules. They migh t even not be comp osed of prop er English words{there may be missp ellings, abbreviations, etc. Therefore, lexical ref-erences, suc h as Wor dnet [2], are hard to apply . Second, inputs/outputs typically have few parameters, and the as-sociated WSDL les rarely pro vide rich descriptions for pa-rameters. Traditional IR techniques, suc h as TF/IDF [19] and LSI [5], rely on word frequencies to capture the under-lying seman tics and thus do not apply well.
 A parameter name is typically a sequence of concatenated words, with the rst letter of every word capitalized (e.g. LocalTimeByZipCo deResult ). Suc h words are referred to as terms . We exploit the co-o ccurrence of terms in web service inputs and outputs to cluster terms into meaningful con-cepts. As we shall see later, using these concepts, besides the original terms, greatly impro ves our abilit y to iden tify similar inputs/outputs and hence nd similar web service operations.
 Applying an o -the-shelf text clustering algorithm directly to our con text does not perform well because the web ser-vice inputs/outputs are sparse. For example, whereas syn-onyms tend to occur in the same documen t in an IR applica-tion, they seldom occur in the same operation input/output; therefore, they will not get clustered. Our clustering algo-rithm is a re nemen t of agglomer ative clustering [12]. We brie y outline the clustering algorithm below.
 Clustering by Co-o ccurence : We base our clustering on the follo wing heuristic: parameters tend to expr ess the same conc ept if they occur together often . This heuristic is vali-dated by our exp erimen tal results. We use this intuition to cluster parameters by exploiting their conditional probabil-ities of co-o ccurrence in inputs and outputs of web-service operations. We say that a term t 1 is closely asso ciate d with term t 2 , if the asso ciation rule t 1 ! t 2 has a supp ort and a con dence over the corresp onding thresholds. Here the rules are a special case of the more general asso ciation rules [3]. As is typical, we are interested in clusters that have high cohesion and low correlation . The cohesion H I of a cluster I is the percen tage of closely asso ciated term pairs over all term pairs in I . The correlation R IJ between two clusters I and J is the percen tage of closely asso ciated cross-cluster term pairs. In order to balance the cohesion and correlation, we try to maximize the aver age cohesion-c orrelation ratio of the set of clusters C .
 Agglomerativ e Clustering : Our clustering algorithm is a series of re nemen ts over the classical agglomerativ e clus-tering. We start with eac h term being in a cluster of its own. The algorithm pro ceeds in a greedy fashion. It sorts the asso ciation rules in descending order rst by the con -dence and then by the supp ort. Infrequen t rules with less than a minim um supp ort t s are discarded. At every step, the algorithm chooses the highest rank ed rule that has not been considered previously . If the two terms in the rule be-long to di eren t clusters, the algorithm merges the clusters. We now motiv ate and outline two re nemen ts to this basic algorithm.
 Increasing cluster cohesion The basic agglomerativ e al-gorithm tak es the single link age metho d; that is, it links two clusters together when any two terms in the two clusters are closely asso ciated. This merge condition is very loose and can easily result in low cohesion of clusters. To illustrate, supp ose there is a concept for weather, con taining temp era-ture as a term, and a concept for address, con taining zip as a term. If, when operations rep ort temp erature, they often rep ort the area zipcode as well, then the con dence of rule temp erature ! zip is high. As a result, the basic algorithm will inappropriately com bine the weather concept and the address concept.
 The cohesion of a cluster is decided by the asso ciation of eac h pair of terms in the cluster. To ensure that we obtain clusters with high cohesion, we merge two clusters only if they satisfy a stricter condition: given a cluster C , a term is called a kernel term if it is closely asso ciated with at least half 2 of the remaining terms in C ; two clusters are merged only if all the terms in the mer ged cluster are kernel terms . Splitting and Merging : A greedy algorithm pursues local optimal solutions at eac h step, but usually cannot obtain the global optimal solution. In parameter clustering, an inap-propriate clustering decision at an early stage may prev ent later appropriate clustering. Consider the case where there is a cluster for zipcode f zip, code g , formed because of the frequen t occurrences of parameter ZipCo de . Later we need to decide whether to merge this cluster with another clus-ter for address f state, city, street g . The term zip is closely asso ciated with state, city and street , but code is not closely asso ciated with them because it also occurs often in other parameters suc h as TeamCo de and Pro xyCo de , whic h typi-cally do not co-o ccur with state, city or street . Consequen tly, the two clusters cannot merge; the clustering result con trasts with the ideal one: f state, city, street, zip g and f code g The solution to this problem is to split already-formed clus-ters so as to obtain a better set of clusters with a higher cohesion/correlation score. When analyzing a pair of candi-date clusters, our algorithm considers two options: (a) split eac h cluster into a ready-to-mer ge subset that con tains terms closely asso ciated with terms in the union of the two clus-ters and a left-alone subset of the rest terms, and then merge the two ready-for-merge subsets; (b) leave the two clusters as they are. The option with the best cohesion-correlation ratio is selected.
 Clustering Results : The term-lev el clustering algorithm outlined above still has two problems. First, the cohesion condition is too strict for large clusters, so may prev ent closely asso ciated large clusters to merge. Second, early in-2 We tried di eren t values for this fraction and found 1 yielded the best results. appropriate merging may prev ent later appropriate merging. Although we do splitting, the terms tak en o from the orig-inal clusters may have already missed the chances to merge with other closely asso ciated terms. We solv e the problems by running the clustering algorithm iterativ ely. After eac h pass, we replace eac h term with its corresp onding concept, re-collect asso ciation rules, and then re-run the clustering algorithm. This pro cess con tinues when no more clusters can be merged.
 We now brie y outline the results of our clustering algo-rithm. Our dataset, whic h we will describ e in detail in Sec-tion 2.3, con tains 431 web services and 3148 inputs/outputs. There are a total of 1599 terms. The clustering algorithm con verges after the sev enth run. It clusters 943 terms into 182 concepts. The rest 656 terms, including 387 infrequen t terms (eac h occurs in at most 3 inputs/outputs) and 54 fre-quen t terms (eac h occurs in at least 30 of the inputs/outputs) are left unclustered. There are 59 dense clusters, eac h with at least 5 terms. Some of them corresp ond roughly to the concepts of address, con tact, geology , maps, weather, -nance, commerce, statistics, and baseball, etc. The overall cohesion is 0.96, correlation is 0.003, and average cohesion for the dense clusters is 0.76. This result clearly indicates a high cohesion within concepts and a low correlation between concepts. In this section we describ e how we compute the similarit y of web-service operations. We use the intuition that the simi-larit y between two operations is related to the similarit y of their descriptions, that of their input and output parame-ters, and that of their host web services. The similarit y of a pair of inputs (or outputs) is related to the similarit y of the parameter names, that of the concepts represen ted by the parameter names, and that of the operations to whic h they belong. Here the parameter name similarit y compares inputs/outputs on a ne-grained level; while concept simi-larit y compares inputs/outputs on a coarse-grained level. Parameter name similarit y : We consider the terms in all parameter names in the input or output of an operation as a bag of words and use the TF/IDF measure [19] to compute the similarit y of two suc h bags.
 Input/output concept similarit y : To compare the sim-ilarit y of the concepts represen ted by the inputs/outputs, we replace eac h term in the bag of words describ ed above with its corresp onding concept, and then use the TF/IDF measure.
 Input/output similarit y : We compute the similarit y of the inputs of two operations as a linear com bination of the parameter name similarit y, concept similarit y and the op-eration similarit y. As we see this similarit y is recursiv ely related to the similarit y of the operations. We note that careful choice of the weigh ts in the linear com bination will ensure a closed-form solution for both the input/output sim-ilarit y and the operation similarit y.
 Web-service operation similarit y: We compute the sim-ilarit y of two operations as a linear com bination of the simi-larit y of web-service descriptions, the similarit y of the oper-ation descriptions, the similarit y of the inputs and the sim-ilarit y of the outputs. The description similarities are com-puted again with TF/IDF with some simple pre-pro cessing of the web-service description (obtained from the WSDL and the UDDI registry) and the operation description (obtained from the WSDL). We implemen ted a web-service searc h engine, called Woogle, that has access to 790 web services from the main author-itativ e UDDI rep ositories. We ran our exp erimen ts on the subset of web services whose asso ciated WSDL les are ac-cessible from the web; this set con tains 431 web services and 1574 operations in total. Figure 2 plots the Recall-Precision curv es for web-service operation matc hing and input/output matc hing. It clearly sho ws that considering parameter clus-ters can impro ve the performance for both matc hing tasks. Schema matc hing is the problem of determining a set of cor-respondenc es (a.k.a. matches ) that iden tify similar elemen ts in di eren t schemas. Schema matc hing is inheren tly a dif-cult task to automate mostly because the exact seman tics of the data are only completely understo od by the designers of the schema, and not fully captured by the schema itself. In part, this is due to the limited expressiv e-p ower of the data mo del, and often is further hindered by poor database design and documen tation. As a result, the pro cess of pro-ducing seman tic mappings requires a human in the loop and is typically lab or-in tensiv e, causing a signi can t bottlenec k in building and main taining data sharing applications. Schema matc hing has receiv ed steady atten tion in the database and AI comm unities over the years (see [17] for a recen t sur-vey and [16; 4; 6; 8; 10; 11; 15; 23; 21] for work since). A key conclusion from this body of researc h is that an e ectiv e schema matc hing tool requires a principled com bination of sev eral base techniques , suc h as linguistic matc hing of names of schema elemen ts, detecting overlap in the choice of data types and represen tation of data values, considering pat-terns in relationships between elemen ts, and using domain kno wledge.
 However, curren t solutions are often very brittle. In part, this is because they only exploit evidence that is presen t in the two schemas being matc hed. These schemas often lack sucien t evidence to be able to disco ver matc hes. For example, consider table de nitions T1 and T2 in Figure 3(a). While both of these tables describ e the availabilit y of items, it is almost imp ossible to nd a matc h by considering them in isolation.
 This section describ es corpus-b ased matching , an approac h that leverages a corpus of schemas and mappings in a par-ticular domain to impro ve the robustness of schema matc h-Figure 3: Kno wledge of schemas T3 and T4 can be used to better matc h schemas T1 and T2 . ing algorithms. A corpus o ers a storehouse of alternative representations of concepts in the domain. We sho w how suc h alternate represen tations can be used to increase the evidence available in the matc hed schemas and thereb y im-pro ve the abilit y to disco ver dicult matc hes. For further details, the reader is referred to [14].
 Overview of our approac h : To illustrate the intuition be-hind our techniques for exploiting a corpus, supp ose that for any elemen t e (e.g., table or attribute name) in a schema S , we are able to iden tify the set of elemen ts, C e , in the corpus that are similar to the elemen t e . We can use C e to augment the kno wledge that we have about e . In our example, the ta-ble T1.Bo okAvailabilit y is very similar to the table T3.Bo okSto re in the corpus (their columns are also similar to eac h other). Similarly , T2.Sto ck is similar to T4.Pro ductAvailabilit y . It is easy to see that com bining the evidence in T3 with T1 and T4 with T2 better enables us to matc h T1 with T2 : rst, there is increased evidence for particular matc hing techniques, e.g. , alternativ e names for an elemen t; and second, there is now evidence for matc hing techniques that lack evidences earlier, e.g. considering T3 with T1 includes data instances (tuples) where there were none initially . This is the intuition of aug-ment that we will describ e in Section 3.1.
 Another metho d of exploiting the corpus is to estimate statis-tics about schemas and their elemen ts. For example, given a corpus of inventory schemas, we can learn that Availabilit y ta-bles alw ays have columns similar to Pro ductID and are likely to have a foreign key to a table about Warehouse s. These statistics can be used to learn domain constr aints about schemas ( e.g. , a table is less likely to matc h Availabilit y if it does not have a column that can matc h Pro ductID ). We sho w how to learn suc h constrain ts, and how to use them to further impro ve schema matc hing. In particular these constrain ts are used in an A* searc h that selects matc hing elemen t pairs from the elemen t similarit y values computed by augment . We note that previous work has sho wn that exploiting domain constrain ts is crucial to achieving high matc hing accuracy , but suc h constrain ts have alw ays been speci ed man ually . We outline the pro cess of learning and application of constrain ts in Section 3.2. We now describ e the augment metho d in detail. As noted earlier, the corpus is a collection of schemas and elemen ts within the schema. In order to nd elemen ts in the cor-pus that are similar to a given schema elemen t s , we com-pute an interpr etation vector , I s , for s . I s is a vector, lar s is to elemen t e in the corpus. We use mac hine learning to estimate these similarities. Speci cally , for eac h elemen t of the corpus, e , we learn a mo del; given an elemen t s , the mo del of e predicts how similar e is to s .
 Mo dels for corpus elemen ts : The mo del for eac h elemen t is created via an ensem ble of base learners , eac h of whic h exploits di eren t evidences about the elemen t. Some of the base learners that we use are as follo ws: a name learner that determines the word roots that are most characteristic of the name of an elemen t (as compared to the names of other el-emen ts); a data instanc e learner that determines the words and special sym bols (if any) that are most characteristic in instances of an elemen t; and a context learner that deter-mines the characteristics of elemen ts that are related to an elemen t. The predictions of the base learners are com bined via a meta-le arner .
 Training eac h of these learners requires learner-sp eci c pos-itiv e and negativ e examples for the elemen t on whic h it is being trained. For any elemen t s 2 S , the elemen t is a pos-itiv e example of itself, and all other elemen ts in the schema are negativ e examples. If, in some mapping in the corpus, s is deemed similar to an elemen t t in T , then the train-ing examples for t can be added to the training examples for s . Mappings enable us to obtain more training data for elemen ts and hence learn more general mo dels. The meta-learner uses a linear com bination to com bine the predictions made by the base learners in the ensem ble and is trained us-ing a technique called stacking [20].
 Note that all the base learners in the ensem ble need not be classi ers. For example, we can also have a simple name comparator that uses string edit distance to estimate simi-larit y of two names.
 Augmen ting and matc hing elemen ts : The goal of the augment metho d is to enric h the mo dels that we build for eac h elemen t of the schemas being matc hed, thereb y impro v-ing our abilit y to predict matc hes. Supp ose we are deciding the matc h between an elemen t s in schema S , and an ele-men t t in schema T . Giv en the interpretation vector for s , we pick C s , a set of close elemen ts from the corpus, using a simple criteria: pick the N elemen ts that are most sim-ilar to e suc h that p e;s . The augmen ted mo dels are constructed in a way similar to building mo dels for eac h el-emen t in the corpus. Having determined C s , an ensem ble mo del is learned for s by putting together the training data for all the elemen ts in C s . In addition to C s , other elemen ts that are kno wn to map to elemen ts in C s also con tribute to the examples for s .
 We note that since there are multiple schemas in the corpus, it is easier to nd elemen ts in the corpus that are similar to s than directly trying to matc h s with some elemen t in T . Even if a few elemen ts are incorrectly added to the aug-men ted mo del, there are bene ts as long as there are few er of them than correctly added elemen ts.
 We use the learned augmen ted mo dels for elemen ts in schema S and T to compute the similarit y between eac h s 2 S and t 2 T . The similarities are computed as in [8], sim ( s; t ) is average of the probabilities obtained by applying the aug-men ted mo del for s on t and vice-v ersa.
 In [13] we prop osed an alternate metho d, pivot , for comput-ing these similarities by simply computing the cosine mea-sure between the interpretation vector. In practice, we found that augment performed better that pivot in all our domains. The result of both the augment and the pivot metho ds is a similarit y matrix: for eac h pair of elemen ts s 2 S and t 2 T , we have an estimate for their similarit y (in the [0 ; 1] range). Corresp ondence or matc hes can be selected using this matrix in a num ber of ways, as we describ e next. The task of generating matc hes consists of picking the elemen t-to-elemen t corresp ondences between the two schemas being matc hed. As observ ed in previous work [15; 7], relying only on the similarit y values does not suce for two reasons. First, certain matc hing heuristics cannot be captured by similarit y values (e.g., when two elemen ts are related in a schema, then their close elemen ts in the corpus should also be related). Second, kno wledge of constrain ts plays a key role in pruning candidate matc hes.
 Constrain ts can either be generic or dep enden t on a partic-ular domain. As examples of the latter, if a table matc hes Books , then it must have a column similar to ISBN . If a column matc hes DiscountPrice , then there is likely another column that matc hes ListPrice . Most prior work has used only generic constrain ts, and when domain constrain ts have been used, they have been pro vided man ually and only in the con text where there is a single mediated schema for the domain [7].
 In this section we brie y describ e how domain constrain ts can be learned from a corpus of schemas, and also how we can estimate the relev ance of the di erence constrain ts to matc h generation. The latter is imp ortan t because man y of the constrain ts we emplo y are soft constrain ts; i.e. , they can be violated in some schemas.
 Corpus Statistics : In order to learn constrain ts from the corpus, we must rst estimate various statistics of its con-ten ts. For example, given a collection of Book schemas, we migh t nd that all of them have a column for ISBN , 50% of them have author information in separate tables, and 75% have list and discoun t price information in the same table. In order to estimate meaningful statistics about an elemen t, we must have a set of examples for that elemen t, e.g., we can mak e statemen ts about tables of Book s only when we have seen a few examples of similar tables. We hence group together elemen ts in our corpus into clusters that intuitiv ely corresp ond to conc epts . As in the last section, we use ag-glomerativ e clustering to build clusters; however, here we are clustering elemen ts in di eren t schemas. We use a di er-ent set of re nemen ts to the basic clustering algorithms; for example, we do not let two elemen ts from the same schema ever be part of the same cluster. Giv en the clustering of cor-pus elemen ts into concepts, here are some of the statistics that we estimate.
 Tables and Columns : For relational schemas, compute for eac h table concept t i and eac h column concept c j , the con-ditional probabilit y P ( t i j c j ). This helps us iden tify the con-texts in whic h columns occur. For example, the ISBN column most likely occurs in a Books table or an Availabilit y table (as foreign key), but nev er in a Warehouse table.
 Neighb orho od : We compute for eac h concept the most likely other concepts they are related to. Brie y , we construct itemsets from the relationship neigh borho ods of eac h ele-men t, and learn asso ciation rules from these. For example, we learn that AvailableQuantit y ! WarehouseID , i.e., the at-tribute availabilit y is typically speci ed w.r.t. a particular warehouse.
 Ordering : If the elemen ts in a schema have a natural order-ing ( e.g. the input elds in a web form, or the sub-elemen ts of an XML elemen t), then we can determine the likeliho od of one concept preceding another.
 Cost-based Matc h Generation : Giv en two schemas S and T , our goal is to select for eac h elemen t in S the best corresp onding elemen t in T (and vice-v ersa). Speci cally , for eac h elemen t e in S we will assign either an elemen t f in schema T , or no match ( ). Let M represen t suc h a matc h. Thus, M = [ i f e i f i g , where e i 2 S and f i 2 T [f g . We assign a cost to any suc h matc h M that is dep enden t on our estimated similarit y values and constrain ts: where sim [ e i ; f i ] is the estimated similarit y of elemen ts e and f i , eac h K j ( 0) is some penalt y on the mapping M for violating the j th constrain t, and w j is a weigh t that indicates the con tribution of the j th constrain t. The rst sum is an estimate of the total log likeliho od of the mapping (if the similarities were interpreted as probabilities). The second sum is the penalt y for violating various constrain ts. The task of generating the mapping from S to T is now reduced to the task of picking the mapping M with the minimal cost. We use A searc h [18] to pick the best mapping M , whic h guaran tees nding the matc h with the lowest cost. Our constrain ts are enco ded as functions, K j , that pro duce a value in the interv al [0 ; 1]. We do not pro vide the details for eac h K j , but note that the weigh t-learning algorithm describ ed in the next section adapts w j to the values K evaluates to.
 Some of the constrain ts we use are the follo wing. Among generic constrain ts, uniqueness states that eac h elemen t must matc h with a distinct elemen t in the target schema, and mu-tual states that e can matc h f only if e is one of the most similar elemen ts of f and mutually f is one of the most sim-ilar elemen ts of e . As domain constrain ts obtained from the corpus, we have the follo wing: (1) same-c onc ept : if two el-emen ts are to be matc hed, then they have to be similar to the same concept(s) in the corpus; (2) likely-table : if column e matc hes f , then e 's table must be a likely table for f to be in; (3) neighb ors : if elemen t f is to be assigned to e , then elemen ts that are likely related to f must be related to e ; and (4) ordering : if elemen t f is to be assigned to e , then the ordering corpus statistics should not be violated. Learning Constrain t Weigh ts : Since man y of the con-strain ts we use in our matc hing algorithm are soft; i.e. , en-code preferences rather than strict conditions, the choice of weigh t for eac h constrain t is crucial. Prior work has alw ays hard-co ded these constrain t weigh ts. We now describ e how these weigh ts can actually be learned from kno wn mappings. Consider a matc hing task between source schema S and tar-get schema T . Consider a mapping M in whic h the correct matc hes are kno wn for all elemen ts in S except e . If f were the correct matc h for elemen t e , then in order that e is also correctly matc hed with f , given the exact matc hes of other elemen ts, the follo wing condition must hold. This can be re-written for eac h elemen t f i as below. Elemen t e is incorrectly matc hed if the above condition is violated for some f i . In [14] we describ e how we learn the values of the w j s (using hill-climbing searc h) by minimizing the num ber of incorrect violations of this above condition in a kno wn set of mappings. We now presen t exp erimen tal results that demonstrate the performance of corpus-based matc hing. We sho w that corpus-based matc hing works well in a num ber of domains, and in general has better results than matc hing schemas directly . Furthermore, we sho w our techniques are esp ecially e ectiv e on schema pairs that are harder to matc h directly . Datasets : We used a variet y of domains and Table 1 sum-marizes some of their basic characteristics. We note that the web form schemas (set of visible input elds) were extracted by a rather primitiv e text extractor and hence had a num ber of errors that made the matc hing dicult. The relational schemas were created indep enden tly by undergraduate stu-den ts as part of a class pro ject in their database class. These schemas were varied in their choice of tables (3-12), num ber of columns, and data types.
 In eac h domain, we man ually created mappings between ran-domly chosen schema pairs. The matc hes were one-many , i.e., an elemen t can matc h any num ber of elemen ts in the other schema. These man ually-created mappings are used as training data and as a gold standar d to compare the map-ping performance of the di eren t metho ds.
 Exp erimen tal Metho dology : We compared three meth-ods: augment , direct , and pivot : augment is our complete corpus-based solution. direct uses the same base learners describ ed in Section 3.1, but the training data for these learners is extracted only from the schemas being matc hed. direct is similar to the Glue system [8] and can be considered a fair represen tativ e of direct-matc hing metho ds. pivot , as describ ed in Section 3.1, is the metho d that computes co-sine distance of the interpretation vectors of two elemen ts directly .
 The result of eac h of our metho ds is a directional matc h: for eac h elemen t in a schema, an elemen t from the other schema is chosen suc h that the cost of entire mapping is minimized. If the gold standard has a matc h in whic h s matc hes a set of elemen ts E , then a matc her is said to have predicted it correctly if s is predicted to matc h any one elemen t in E , and every elemen t in E is predicted to matc h s . As a result, any 1 : m mapping is considered as m + 1 separate matc hes. We rep ort matc hing performance in terms of F-Me asur e . For a given schema pair, let c be the num ber of elemen ts in the two schemas for whic h a matc h is predicted and the predicted matc h is correct. If a matc h was predicted for n elemen ts, and a matc h exists in the gold standard for m ele-men ts, the f-measure is the harmonic mean of the precision and recall.
 Optimizing for f-measure tries to balance the inverse relation between precision and recall.
 Corpus impro ves F-Measure : Figure 4 compares the results of direct , augment , and pivot in eac h of the four do-mains. There are 22 ( auto ), 16( real estate ), 19( invsmall ), 16( in-vento ry ) schemas resp ectiv ely and 6 mappings in the corpus. augment achiev es a better f-measure than direct and pivot in all domains. There is a 0 : 03 0 : 11 increase in f-measure as compared to direct , and a 0 : 04 0 : 06 increase as compared to pivot . We note that augment has a better recall as com-pared to direct in all four domains, and better precision in three of the four domains (the precision is lower in the in the invento ry domain due to the presence of man y ambigu-ous matc hing columns, but there is a noticeable increase in recall). These results sho w that augmen ting the evidence about schemas leads to the disco very of new matc hes, and in most cases few er incorrect matc h predictions are made. The results in this section consider only the single best matc h for eac h elemen t. In an interactiv e schema-matc hing system, we typically o er the user the top few matc hes when there is a doubt. When we consider the top-3 can-didate matc hes, then augment is able to iden tify the correct matc hes for 97 : 2% of the elemen ts, as opp osed to 91 : 1% by direct and 96 : 7% by pivot .
 Dicult versus Easy matc hing tasks : Our cen tral claim is that corpus-based matc hing o ers bene ts when there is insucien t direct evidence in the schemas. To validate this claim, we divided the man ual mappings in eac h domain into two sets -easy and dicult : all the schema pairs in the test set were matc hed by direct and then sorted by direct 's matc hing performance. The top 50% were iden ti ed as the easy pairs and the bottom 50% as the dicult pairs.
 Figure 4b compares the average f-measure over the dicult matc hing tasks, sho wing that augment outp erforms direct in these tasks. More imp ortan tly, the impro vemen t in f-measure over direct is much more signi can t (0 : 04 0 : 16) than in Figure 4a, e.g., there is an impro vemen t of 0 : 16 in the invsmall domain, and 0 : 12 in the real estate domain as compared to the 0 : 11 and 0 : 07 increases when all tasks are considered.
 Figure 4c sho ws the same comparison over the easy tasks. The performance of augment for these tasks, while still very good, is in fact sligh tly worse than direct in one domain and the impro vemen ts are much less in the other domains. This is quite intuitiv e in retrosp ect. When two schemas are rather similar (easy to matc h directly), including additional evidence can lead the matc her astra y. We describ ed two related pro jects that exploit corp ora of structures. They corresp ond to the two steps in searc hing and accessing the hidden web: (1) locating the desired data source; (2) reform ulating a query onto the schema of the source. While both steps rely on a good understanding of the structure information, they are di eren t in the matc h-ing gran ularit y: data-source location is essen tially nding a similar schema, while schema matc hing looks for similar elemen ts within two given schemas that are assumed to be related. In the former task, the exact details of the vari-ous elemen ts and their relationships are less imp ortan t: it suces to kno w that a similar elemen t exists, rather than kno wing whic h elemen t it is.
 As we dev elop corpus-based techniques to searc hing struc-tures with rich seman tics, there is another dimension to keep in mind, namely , the cohesion of the underlying structure. In particular, a database schema includes a set of tigh tly-coupled tables and attributes that, together, are mean t to mo del a set of objects. This raises the complexit y of the matc hing, but mean while pro vides rich information. For ex-ample, the schema de nition languages for databases are strong in de ning constrain ts, suc h as type constrain ts, key and foreign key constrain ts, and thus presen t more opp or-tunities for exploring constrain ts in schema matc hing. In con trast, the operations in web services are only loosely cou-pled, and eac h one in isolation has much less information. However, the descriptions in the WSDLs and the UDDI en-tries allo w for applying the information retriev al techniques in matc hing. In addition, the input and output parame-ters in an operation are organized in a tree hierarc hy rather than forming a at vector and so imply the relationship be-tween parameters. Web forms can be view ed as very simple database schemata, and are similar to web services in that they also tak e certain inputs. Although here the inputs do not have particular type information, other asp ects of infor-mation can be explored to understand the inputs, suc h as text descriptions on the webpage, the layout of the input comp onen ts on the page, and the values of the drop-do wn boxes. Furthermore, a eld in a web form may already cor-resp ond to a selection or aggregation query over the under-lying schema (e.g., price range or maxim um price), rather than to the schema elemen t itself. The use of previous schema and mapping kno wledge has been prop osed in the past, but in two very restricted set-tings. They either use previous mappings to map multi-ple data sources to a single kno wn mediated schema [7], or comp ose kno wn mappings to a common schema [6]. In our approac h, we sho w that a corpus of schemas and map-pings can be leveraged in man y di eren t ways to disco ver matc hes between two as yet unse en schemas. In [10], the authors construct a single mediated schema for a domain of web forms. They estimate the single most likely mediated schema that could generate all the web forms in a given col-lection. In [22] the authors collectiv ely matc h a num ber of related web forms by clustering their elds. We use suc h clustering as a step in learning constrain ts. There are sev eral exciting future directions we are pursu-ing. First, we believ e that corpus-based techniques should be helpful in authoring database schemata and querying un-familiar schemas. Second, we plan to apply our techniques to soft ware comp onen t matc hing, where the goal is to nd metho ds that have the same signatures and beha vior [24]. While some of our techniques for analyzing web service de-scriptions may apply , in the con text of soft ware comp onen ts signi can t for dicult tasks (b) and more mo dest for easy tasks (c). we may also leverage analysis of pre-and post-conditions. [1] Binding Poin t. http://www.bindingp oint.com. [2] WordNet. http://www.cogsci.princeton.edu/ wn/. [3] R. Agarw al, T. Imielinski, and A. Swami. Mining As-[4] J. Berlin and A. Motro. Database Schema Matc hing [5] S. C. Deerw ester, S. T. Dumais, T. K. Landauer, G. W. [6] H.-H. Do and E. Rahm. COMA -A System for Flex-[7] A. Doan, P. Domingos, and A. Y. Halevy . Reconciling [8] A. Doan, J. Madha van, P. Domingos, and A. Y. Halevy . [9] X. Dong, A. Halevy , J. Madha van, E. Nemes, and [10] B. He and K. C.-C. Chang. Statistical Schema Matc hing [11] J. Kang and J. Naugh ton. On schema matc hing with [12] L. Kaufman and P. J. Rousseeu w. Finding Groups in [13] J. Madha van, P. Bernstein, K. Chen, A. Halevy , and [14] J. Madha van, P. A. Bernstein, A. Doan, and A. Halevy . [15] S. Melnik, H. Garcia-Molina, and E. Rahm. Similarit y [16] N. F. Noy and M. A. Musen. PR OMPT: Algorithm and [17] E. Rahm and P. A. Bernstein. A surv ey of approac hes [18] S. Russell and P. Norvig. Arti cial Intel ligenc e: A Mod-[19] G. Salton, editor. The SMAR T Retrieval System| [20] K. M. Ting and I. H. Witten. Issues in Stac ked Gen-[21] J. Wang, J.-R. Wen, F. Lochovsky , and W.-Y. Ma. [22] W. Wu, C. Yu, A. Doan, and W. Meng. An Interac-[23] L. Xu and D. Em bley . Disco vering Direct and Indirect [24] A. M. Zaremski and J. M. Wing. Speci cation matc hing
