 Naive Bayes (NB) classifiers, due to their simplicity and modest computational and training data requirements, are among the most widely used classifiers on many classification tasks, including text classification tasks [1] and macromolec-ular sequence classification tasks that arise in bio-informatics applications [2]. NB classifiers belong to the family of generative models (a model for generat-ing data given a class) for classification. Instances of a class are assumed to be generated by a random process which is modeled by a generative model. The parameters of the generative model are e stimated (in the case of NB) assuming independence among the attributes given the class. New instances to be classified are assigned to the class that is the most probable for the instance.
NB classifier relies on the assumption that the instances in each class can be described by a single generative model (i.e., probability distribution). According to Langley [3], this assumption can be restrictive in many real world classifica-tion tasks. One way to overcome this limitation while maintaining some of the computational advantages of NB classifiers is to construct a tree of NB classi-fiers. Each node in the tree (a NB classifier) corresponds to one set of generative models (one generative model per class) , with different nodes in the tree corre-sponding to different generative models for a given class. Langley described a recursive NB classifier (RBC) for classifyin g instances that are represented by or-dered tuples of nominal attribute values. RBC works analogous to a decision tree learner [4], recursively partitioning the training set at each node in the tree until the NB classifier of the node simply cannot partition the corresponding data set. Unlike in the case of the standard decision tree, the branches out of each node correspond to the most likely class lebe ls assigned by the NB classifier at that node. In cases where each class cannot be accurately modeled by a single Naive Bayes generative model, the subset of in stances routed to one or more branches belong to more than one class. RBC models the distribution of instances in a class at each node using a Naive Bayes gen erative model. However, according to Langley X  X  reports of experiments on most of the UC-Irvine benchmark data sets, the recursive NB classifier did not yield significant improvements over standard NB classifier [3].

In this paper, we revisit the idea of r ecursive NB classifier in the context of sequence classification tasks. We describe RNBL-MN, an algorithm for con-structing a tree of Naive Bayes classifie rs for sequence classification. Each NB classifier in the tree is based on a multinomial event model [1] (one for each class at each node in the tree). Our choice of the multinomial event model is influenced by its reported advantages over the multivariate event model of sequences [1] in text classification tasks. RNBL-MN works in a manner similar to Langley X  X  RBC, recursively partitioning the training se t of labeled sequences at each node in the tree until a stopping criterion is satisfied. The branches out of each node cor-respond to the most likely class assigned by the NB classifier at that node. As for the stopping criterion, RNBL-MN uses a conditional minimum description length (CMDL) score for the classifier [5], specifically adapted to the case of RNBL-MN based on the CMDL score for the NB classifier using the multino-mial event model for sequences [6]. Previous reports by Langley [3] in the case of a recursive NB classifier (RBC) for data set s whose the instances are represented as tuples of nominal attribute values (such as the UC-Irvine benchmark data), suggested that the tree of NB classifiers offered little improvement in accuracy over the standard NB classifier. In our experiments on protein sequence and text classification tasks, we observe that RNBL-MN substantially outperforms NB classifier. Furthermore, our experiments show that RNBL-MN outperforms C4.5 decision tree learner (using tests on sequence compositio n statistics as the splitting criterion) and yields accuracies that are comparable to those of SVM using similar information.

The rest of the paper is organized as fo llows: Section 2 briefly introduces the multinomial event model for sequences; S ection 3 presents RNBL-MN (recursive Naive Bayes learner based on the multino mial event model for sequences); Sec-tion 4 presents our experimental results; Section 5 concludes with summary and discussion.
 Consider sequences defined over a finite alphabet  X  = { w 1  X  X  X  w d } where d = |  X  | . For example, in the case of protein sequences,  X  can be the 20-letter amino acid alphabet (  X  = { A 1 ,A 2 ,...,A 20 } ). In the case of text,  X  corresponds to the finite vocabulary of words. Typically, a sequence S j  X   X  is mapped into a finite dimensional feature space D through a mapping  X  :  X   X  D .

In a multinomial event model, a sequence S j is represented by a bag of elements from  X  .Thatis, S j is represented by a vector D j of frequencies of occurrences Z  X  denotes the number of occurrences of w  X  ) in the sequence S j . Thus, we can model the sequence S j as a sequence of random draws from a multinomial distribution over the alphabet  X  .Ifwe denote the probability of picking an element w i given the class c j by P ( w i | c j ), the probability of sequence S j given its class c j under the multinomial event model is defined as follows: (Note: To be fully correct, we would need to multiply the right hand side of the above equation by P ( N | c j ), the probability of drawing a sequence of a specific length N =( d i f ij ) given the class c j , but this is hard to do in practice.)
Given a training set of sequences, it is straightforward to estimate the prob-where Count ij is the number of occurrences of w i in sequences belonging to class c and Count j is the total number of words in training set sequences belonging to class c j . 3.1 RNBL-MN Algorithm As noted above, RNBL-MN, analogous to the decision tree learner, recursively partitions the training data set using Naive Bayes classifiers at each node of the tree. The root of the tree is a Naive Bayes c lassifier constructed from the entire data set. The outgoing branches correspond to the different class labels, assigned by the Naive Bayes classifier.

For a given input training data set D 0 (= D current ), we create a Naive Bayes classifier n 0 . We compute the CMDL score Score current for the classifier n 0 (See section 3.2 for details of the calculation of CMDL score for recursive Naive Bayes classifier based on the multinomial event model). The classifier n 0 partitions the data set D 0 into | C | subsets based on the class labe ls assigned to the sequences by the classifier n 0 . Each such subset is in turn used to train additional Naive Bayes classifiers. At each step, the CMDL score for the resulting tree of Naive Bayes classifiers is computed and compared with the CMDL score of the classifier from the previous step. This recursive process terminates when additional refinements of the classifier yield no significant improvement in CMDL score. Fig. 1 shows the pseudo-code of RNBL-MN algorithm.

Analogous to a decision tree, the resulting classifier predicts a class label for a new sequence as follows: starting at the root of the tree, the sequence is routed along the outgoing branches of successive Naive Bayes classifiers, at each node following the branch corresponding to the most likely class label for the sequence, until a leaf node is reached. The sequence is assigned the label corresponding to the leaf node. 3.2 Conditional Minimum Descr iption Length (CMDL) Score for RNBL-MN employs the conditional minimum description length (CMDL) score [5], specifically adapted to the case of RNBL-MN, based on the CMDL score for NB classifier using the multinomial event model for sequences [6] as the stopping criterion.

Recall the definition of a conditional minimum description length (CMDL) score of a classifier h given a data set D [5]: where size ( h ) is the size of the hypothesis h (the complexity of the model), which corresponds to the number of entries in the conditional probability tables (CPTs) of h . CLL ( h | D ) is the conditional log likelihood of the hypothesis h given the data D , where each instance of the data has a class label c  X  C .
When h is a Naive Bayes classifier based on a multinomial event model, the conditional log likelihood of the classifier h given data D can be estimated as follows [6]: where d = |  X  | is the cardinality of the vocabulary  X  , | D | is the number of sequences in the data set D , c j  X  C is the class label associated with the instance S the estimated probability of the element w i occurring in an instance belonging to class c j .

The size ( h ) for the multinomial event model is given by size ( h )= | C | + |
C | d ,where | C | is the number of class labels, and d is the cardinality of the vocabulary  X  . 3.3 CMDL for a Recursive Naive Bayes Classifier We observe that in the case of a recursive Naive Bayes classifier, CLL ( h | D ) can be decomposed in terms of the CLL scores of the individual Naive Bayes classifiers at the leaves of the tree of cl assifiers. Consequently, the CMDL score for the composite tree-structured cl assifier can be written as follows: where size ( h )=( | C | + | C | d ) | h | ,denoting | h | the number of nodes in h .
For example, Fig. 2 shows a Recursive Naive Bayes classifier consisting of 5 individual Naive Bayes classifiers.  X  c + and  X  c  X  are the predicted outputs of each hypothesis.
 In the figure, and
Using the CMDL score, we can choose the hypothesis h that effectively trades off the complexity, measured by the number of parameters, against the accuracy of classification. As is described in Fig. 1, the algorithm terminates when none of the refinements of the classifier (splits of the tree nodes) yields statistically significant improvement in the overall CMDL score. To evaluate RNBL-MN, recursive Naive Bayes learner of multinomial event model, we conducted experiments using two classification tasks: (a) assigning Reuters newswire articles to categories, (b) and classifying protein sequences in terms of their cellular localization. The results of the experiments described in this section show that the classifiers generated by RNBL-MN are typically more accurate than Naive Bayes classifiers using the multinomial model, and that RNBL-MN yields more accurate classifiers than C4.5 decision tree learner (using tests on sequence composition statistics as the splitting criterion). RNBL-MN yields accuracies that are compara ble to those of linear kernel based SVM trained with the SMO algorithm [7] on a bag of letters (words) representation of sequences (text). 4.1 Reuters 21587 Text Categorization Test Collection Reuters 21587 distribution 1.0 data set 1 consists of 12902 newswire articles in 135 overlapping topic categories. We followed the ModApte split [8] in which 9603 stories are used to train the classifier and 3299 stories to test the accuracy of the resulting classifier. We eliminated the stories that do not have any topic associated with them (i.e., no class label). As a result, 7775 stories were used for training and 3019 stories for testing the classifier.

Because each story has multiple topics (class labels), we built binary classifiers for the top ten most populous categories following the setup used in previous studies by other authors [9, 1]. In our experiments, stop words were not elimi-nated, and title words were not distinguished from body words. Following the widely used procedure for text classification tasks with large vocabularies, we selected top 300 features based on mutual information with class labels.
For evaluation of the classifiers, following the standard practice in text clas-sification literature, we report the break-even points, which is the average of precision and recall when the difference between the two is minimum.

Table 1 shows the break-even points of precision and recall as a performance measure for the ten most frequent categories. The results in the table show that, RNBL-MN outperforms the other algorithms, except SVM, in terms of classification accuracy for Reuters 21587 text data set.
 4.2 Protein Subcellular Localization Prediction We applied RNBL-MN to two protein sequence data sets, where the goal is to predict the subcellular localization of the proteins [10, 2].
 The first data set consists of 997 prokaryotic protein sequences derived from SWISS-PROT database (release 33.0) [11]. This data set includes proteins from three different subcellular locations: cytoplasmic (688 proteins), periplasmic (202 proteins), and extracellular (107 proteins). The second data set contains 2427 eukaryotic protein sequences derived from SWISS-PROT database (release 33.0) [11]. This data set includes proteins from the following four different subcellular locations: nuclear (1097 proteins), cy-toplasmic (684 proteins), mitochondrial (321 proteins), extracellular (325 pro-teins).

The accuracy, sensitivity, and specificit y of the classifiers (estimated using 10-fold cross-validation) on the two data sets 2 are shown in Table 2. The results show that RNBL-MN generally outperforms C4.5, and compares favorably with SVM. Specificity of SVM for  X  X itochondrial X  is  X  X /A X , because the SVM clas-sifier always outputs negative when most of the instances in the data set have negative class label (imbalanced), which leads its specificity to be undefined. 5.1 Related Work As noted earlier, Langley [3] investigated recursive Bayesian classifiers for the instances described by tuples of nominal attribute values. RNBL-MN reported in this paper works with a multinomial event model for sequence classification. Kohavi [12] introduced NBTree algorithm, a hybrid of a decision tree and Naive Bayesclassifiersforinstancesrepresentedusingtuplesofnominalattributes.NBTree evaluates theattributes availableateachnodeto decidewhether to continuebuilding a decision tree or to terminate with a Naiv e Bayes classifier. In contrast, RNBL-MN algorithm, like Langley X  X  RBC, builds a decision tree, whose nodes are all Naive Bayes Classifiers.

Gama and Brazdil [13] proposed an algorithm that generates a cascade of classifiers. Their algorithm combines Na ive Bayes, C4.5 decision tree and linear discriminants, and introduces a new attribute at each stage of the cascade. They performed experiments on several UCI data sets [14] for classifying instances represented as tuples of nominal attribute values. In contrast, RNBL-MN recur-sively applies the Naive Bayes classifie r based on the multinomial event model for sequences. 5.2 Summary and Conclusion RNBL-MN algorithm described in this paper relaxes the single generative model per class assumption of NB classifiers, while maintaining some of their compu-tational advantages. RNBL-MN constructs a tree of Naive Bayes classifiers for sequence classification. It works in a ma nner similar to Langley X  X  RBC [3], recur-sively partitioning the training set of labeled sequences at each node in the tree until a stopping criterion is satisfied. RNBL-MN employs the conditional mini-mum description length (CMDL) score for the classifier [5], specifically adapted to the case of RNBL-MN classifier based on the CMDL score for the Naive Bayes classifier using the multinomial event model [6] as the stopping criterion. Previous reports by Langley [3] in the case of a recursive NB classifier (RBC) on data sets whose instances were represented by tuples of nominal attribute values (such as the UC-Irvine benchmark data) had suggested that the tree of NB classifiers offered little improvement in accuracy over the standard NB clas-sifier. In contrast, we observe that on protein sequence and text classification tasks, RNBL-MN substantially outperforms the NB classifier. Furthermore, our experiments show that RNBL-MN outper forms C4.5 decision tree learner (us-ing tests on sequence composition statistics as the splitting criterion) and yields accuracies that are comparable to those of SVM using similar information.
Given the relatively modest computational requirements of RNBL-MN rela-tive to SVM, RNBL-MN is an attractive alternative to SVM in training classifiers on extremely large data sets of sequen ces or documents. Our results raise the possibility that Langley X  X  RBC might outperform NB on more complex data sets in which the one generative model per class assumption is violated, especially if RBC is modified to use an appropriate CMDL criterion.
