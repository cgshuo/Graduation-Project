 1. Introduction
The advent and rapid growth of a variety of Web 2.0 applications, enabling and fostering the establishment of online com-munities and social networks, have contributed to the creation and dissemination of a massive amount of social media con-tent. Social media refers to content created and disseminated via social interactions, and is thus typically associated with user generated content . In general, social media is increasingly becoming a significant fraction of the content searched for and retrieved daily by Web users. Take YouTube, 1 the currently most popular social video sharing application, as an example.
With reportedly 24 h of videos uploaded per minute and 2 billion video views a day, applications in volume of traffic over the Internet. 3
Social media typically includes a main object, stored in one of various media types (e.g., text, image, audio or video), as
Content features are sources of information which can be extracted from the object itself, such as the color histogram of an  X  image. Textual features , on the other hand, are textual content often created and associated with the object by the users themselves. Examples are the object title, description, tags and comments posted by users. Social features , in turn, reflect the social context into which the object is inserted, that is, the user who created it, the users who accessed it, and the user interactions established through it.

Textual features are of particular interest because of their potential as supporting data for a variety of Information Retrie-val (IR) services such as search, recommendation and online advertising. In fact, most IR services available in current Web 2.0 ( Rui, Huang, Mehrotra, &amp; Ortega, 1997; Smeulders, Worring, Santini, Gupta, &amp; Jain, 2000 ). lack of quality as supporting data for effective IR services.

For instance, one particular type of textual feature, namely tags , has been the focus of a large body of recent work. Pre-vious studies include the investigation of its use for supporting search, recommendation, classification and clustering ( Cle-Huberman, 2006; Sigurbjornsson &amp; van Zwol, 2008; Paul Heymann, Andreas Paepcke, &amp; Hector Garcia-Molina, 2010; Santos-
Neto, Figueiredo, Mowbray, Gon X alves, &amp; Ripeanu, 2010 ). However, no consensus has been reached as to the quality of tags only on tags, neglecting the potential use of other textual features, such as title and description.

In this context, this paper presents what, to the best of our knowledge, is the currently most comprehensive study of the textual features commonly found in various popular Web 2.0 applications .

Towards our goal, we crawled object samples from four applications which target different media types as primary means for content dissemination. In addition to YouTube, we also consider Yahoo! Video social video sharing application, LastFM 6 (or simply LastFM), an online radio and music community website, scholarly reference management and discovery service. Our collected samples contain the contents of four textual features,
We characterized the collected data in order to extract evidence of quality of each feature with respect to usage, amount and semantics of content, descriptive and discriminative power, as well as evidence of content and information diversity across features associated with the same object. Our main findings are: (1) all four features but used TITLE and TAGS features exhibit, in general, higher descriptive and discriminative powers, followed by
MENTS , although, on both CiteULike and LastFM, DESCRIPTION estimated based only on the top-5 most descriptive terms; and (5) there is a non-negligible amount of content and informa-tion diversity across features associated with the same object, implying that different features contribute with different pieces of information about it.

We also assessed the relative quality of the features with respect to two specific IR tasks, namely, object classification and tag recommendation. These tasks were selected due to their common applicability to a plethora of services (e.g., automatic construction of Web directories, search, browsing, etc.), and because they allow for automatic assessment of feature quality. Classification experiments were performed varying the source of data for the classifier and the content weighting scheme. combinations of all four features. Our main results, which are supported by our characterization findings, are: (1) weighting schemes that explore discriminative power have better effectiveness either in isolation or in combination with other met-and large amount of content, two quality-related aspects that have important roles for classification effectiveness; (3) combining content from multiple features may improve classification results due to the presence of distinct and somewhat complementary content and information; (4) a simpler feature combination strategy based on bag of words may be as effec-tive or at most slightly worse than concatenating features as different feature spaces; and (5) in spite of its good discrimi-native and descriptive power and its larger object coverage, since its effectiveness is very affected by the small amount of content available.

Motivated by our findings regarding the large amount of content and information diversity across textual features of the same object, we also investigated the potential of exploiting the contents of quality of the TAGS feature through the recommendation of candidate terms. Our results show that, unlike observed for clas-purposes, since, as observed in our characterization, this feature has the best descriptive power in all four analyzed applications.

This paper X  X  main contribution lies in a more solid understanding about the quality of textual features in social media. In comparison with our previous studies ( Figueiredo et al., 2009; Almeida, Gon X alves, Figueiredo, Bel X m, &amp; Pinto, 2010 ), we here present a much more comprehensive analysis. In particular, we extend our prior work to (1) characterize the semantic contents of different features for tag recommendation. The knowledge produced here may be of use to Web 2.0 application designers, who can address issues such as usefulness and attractiveness of each feature to their users. It may also be of rel-mation. In sum, we here offer a more comprehensive exploration of the problem than previous work, covering different features in different applications and their impact on two different IR services, thus providing more sound conclusions.
The remaining of this paper is organized as follows. Related work is discussed in Section 2 . Section 3 introduces the four textual features analyzed, whereas Section 4 summarizes our data collection methodology. The main findings from our tex-most relevant results, whereas our assessment of the potential benefits of exploiting feature contents for tag recommenda-tion is presented in Section 7 . Finally, Section 8 concludes our paper, pointing out possible directions for future work. 2. Related work
Previous efforts towards analyzing and exploiting textual features on the Web 2.0 focused mainly on tagging systems, which have been proposed as a more flexible alternative to describe and organize objects. Indeed, many authors have pro-posed the use of tags to enhance services such as searching, recommendation, clustering and indexing ( Hotho, Jaschke,
Zwol, 2008; Clements et al., 2010; Guy et al., 2010 ). Others have addressed the characterization of tagging systems ( Paul et al., 2010; Golder &amp; Huberman, 2006; Santos-Neto et al., 2010 ). Nevertheless, there still lie many open questions with respect to the quality of tags.

On one hand, Suchanek, Vojnovic, and Gunawardena (2008) analyzed the amount of tags in Delicious according to two semantic databases. They found that only approximately half of the used tags have entries in these dictionar-ies, and that nearly half of these known tags have 10 or more meanings (measured by distinct occurrences in the dictionary). In other words, they concluded that tags contain a significant amount of unknown terms and suffer from polysemy, two issues that of searching in Delicious and LastFM. By checking the overlap between tags and the object X  X  textual content (i.e., bookmarked authors found that tags tend to be accurate with respect to the object X  X  content and related to search queries.
In a recent study, Venetis, Koutrika, and Garcia-Molina (2011) addressed the problem of selecting tags for summarizing a set of results generated by a query. They proposed several metrics to capture the structural properties of tag clouds created for query results, and used these metrics to evaluate their quality. They also evaluated several tag selection algorithms with respect to the quality of their results. In different directions, Paul et al. (2010) analyzed the consistency, quality and com-pleteness of tags posted by users in social cataloging sites, whereas Santos-Neto et al. (2010) proposed a framework to assess the value of user contributions in tagging systems.

In the context of classification and clustering, Ramage et al. (2009) contrasted the usage of tags assigned by users in Deli-cious with the usage of the full textual content of the bookmarked webpages. The authors showed that combining both web-page contents and tags improves the effectiveness of two different clustering algorithms. In addition, Chen et al. (2009) developed a classification model based on both content features and tagging data of musical tracks from LastFM. The authors showed that the use of both types of features improves classification results when compared to using only content features.
There is also a large body of work on tag and content recommendation methods. Tag recommendation aims at supporting tags. Tag co-occurrence patterns are also exploited by Menezes et al. (2010) , although the authors use a lazy associative tag recommendation method in order to efficiently uncover more sophisticated patterns, which ultimately leads to superior re-sults in comparison with the best method in ( Sigurbjornsson &amp; van Zwol, 2008 ). In Bel X m et al. (2011) we extended tradi-tional tag co-occurrence based methods to include not only tags that had been previously assigned to the objects but also terms extracted from other textual features, applying several heuristic metrics to capture the relevance of each candidate term as a recommendation for the target object. Following different approaches, Clements et al. (2010) exploited random walks on a social annotation graph combining content, tags and users, whereas Rendle and Schmidt-Thie (2010) proposed a personalized tag recommendation method that exploits the factorization of tag assignment events into matrices modeling the interactions among users, objects and tags. The relationships among people, tags and objects were also investigated by
Guy et al. (2010) : the authors proposed to recommend objects that are strongly related to people in a user X  X  social network as well as objects related to the user X  X  tags. We note that many existing tag recommendation methods exploit the contents of to our knowledge, no previous study has analyzed the relative quality of different features as sources of data for such task. This is one of the contributions of this paper.

In addition to tag analyses, there is also some work on assessing the quality of other textual features and user generated content. Mishne (2007) characterized the use, popularity, temporal distribution of relevant posts, and amount of information in blog comments, showing that the information in comments can be used to increase the amount of documents retrieved by search queries, and that the temporal distribution can be used to re-rank query results according to blog entries X  post dates.
Other efforts tackled the problem of quality on question/answering (QA) websites. In a QA website, a user posts a question related to a certain topic and receives answers from other users. The relevance of answers to a given question is indicated by the users of the site. Agichtein, Castillo, Donato, Gionis, and Mishne (2008), Bian, Liu, Agichtein, and Zha (2008) and Jeon, Croft, Lee, and Park (2006) made use of textual and social features to identify and/or classify answers with higher quality.
Agichtein et al. (2008) , for instance, manually classified a data set of questions and answers, indicating whether the answer is of low, medium or high quality. They used a series of textual and social characteristics in order to separate high quality content from the rest. Textual quality was analyzed with respect to capitalization, dictionary and grammatical errors, whereas the considered social characteristics were derived from different relationship networks established among users of the system. In ( Shah &amp; Pomerantz, 2010 ), the authors manually evaluated the quality of answers in the Yahoo! Answers website according to various criteria, finding that their evaluation faithfully matched the askers X  perceptions. They also ex-tracted features from questions, answers and user profiles to train a number of classifiers to automatically select the best answer. More broadly, information quality is a matter of relevance to any collaboratively created content. Wikipedia, &amp; Calado, 2009; Giles, 2005; Lih, 2004 ).

All of the aforementioned studies provide evidence of quality of textual content on the Web 2.0, but they focus either on a single feature or on a small sample of objects. To the best of our knowledge, few are the attempts at comparing multiple tex-tual features on Web 2.0 applications. In particular, we cite an analysis of Flickr, which the authors manually compared the contents of three textual features ( and DESCRIPTION better capture the context within the picture, thus offering better descriptions, whereas for organizational purposes. With a different goal, Lipczak and Milios (2010) analyzed the relations between the titles of the personal profile of tags. In comparison with these previous studies, our present analysis is much broader and fully automatic, focusing on much larger object collections from different Web 2.0 applications, and addressing a larger set of quality-related by providing a much more comprehensive analysis, addressing new quality-related aspects such as the semantic properties ison with our previous work, we here perform a much more extensive analysis of the quality of the features for supporting ob-ject classification, including the evaluation of two different widely used classification algorithms, and reach more sound conclusions. We also extend our prior studies by assessing the potential of exploiting different textual features for improving a second type of IR service, namely, tag recommendation.

Information quality has also been studied in other contexts. Language models ( Zhou &amp; Croft, 2005 ) as well as block impor-to analyze quality in traditional (textual) Web documents. In particular, de Moura et al. (2010) proposed several metrics as part of an Information Retrieval Model to assess the importance of different information blocks (e.g., menu, title, etc.) of structured Web documents. We here make use of two of these metrics, adapting them to the specific context of textual fea-tures associated with multimedia Web 2.0 objects. Aiming at a completely different goal from the original work, we here apply these metrics as heuristics to assess the descriptive and the discriminative powers of the features (see Section 5.3 ).
In the context of organizational databases, Strong, Lee, and Wang (1997) introduced four categories of data quality aspects, tation of the data. Accessibility aspects express the concern with easiness of access to the data. Contextual aspects relate to how well the data matches task contexts (e.g., their relevance), whereas representational aspects address the easiness of understanding and interpreting the data as well as the consistency and conciseness of data representation. The authors define a data quality problem as any difficulty encountered along one or more of these dimensions that compromises data usefulness.
Although we do not deal with information at the database level, our study explores evidence of quality with respect to three of the four aforementioned aspect categories. We characterize feature quality with respect to usage and amount of content, which, according to Strong et al. (1997) , fall into the contextual aspect category. We also analyze the semantic properties problems in each analyzed textual feature, providing valuable knowledge to support the design of future Web 2.0 services.
Complementary to our research agenda, some efforts towards applying multimedia IR techniques on Web 2.0 applications are also worth mentioning. Examples include the use of content and textual features to group similar pictures on Flickr techniques and high level semantic concept modeling for effective annotation of music documents ( Shen, Meng, Yan, Pang, &amp; Hua, 2010 ). These studies provided evidence that multimedia IR techniques and social media are converging on some level.
However, textual features still remain as promising data sources for IR tasks on the Web 2.0. 3. Textual features on the Web 2.0
A Web 2.0 object is an instance of a media (such as text, audio, video and image) in a given Web 2.0 application. There are this study, comprise the self-contained textual blocks that are associated with an object, usually with a well defined topic or functionality ( de Moura et al., 2010 ). We here select four textual features for analysis, namely,
MENTS , which are found in many Web 2.0 applications, although some of them are referred to by different names in some appli-cations. CiteULike, for instance, a social bookmarking website for scientific publications, refers to the ABSTRACT , as it usually contains the publication abstract, and name user
Textual features may be categorized according to the level of user collaboration allowed by the application. In particular, the textual features analyzed can be either collaborative or restrictive . Collaborative features may be altered or appended by any user, whereas restrictive ones may only be altered by one user, typically the one who uploaded the object into the sys-tion ( Marlow, Naaman, Boyd, &amp; Davis, 2006 ).
 Table 1 shows the annotation rights of each considered textual feature in each analyzed application.
 rative feature in CiteULike, LastFM and YahooVideo, as any user can add tags to an existing object in these applications.
In YouTube, in contrast, only the video owner (i.e., the user who uploaded the video) can add tags to it. Moreover, while restrictive in both YahooVideo and YouTube, DESCRIPTION has a collaborative nature in both LastFM and CiteULike. In the for-mer, users can edit the information on an artist or music in a wiki-like manner. In the latter, users can provide different ab-stracts to the same publication, although we found that to be very rare in our collected dataset. In all four applications, restrictive and COMMENTS is collaborative.
 We note that some of the applications may automatically fill some of these features when the object is uploaded. You-
Tube, for instance, adds the name of the object as its TITLE allow users to remove automatically added TAGS ,ifa TITLE all features, according to their annotation rights. The exception is LastFM, in which the ically inserted via a media player software based on music files metadata. 4. Data collection methodology and datasets characteristics To perform our study, we built crawlers to sample objects and associated textual features from each application. For You-
Tube, YahooVideo and LastFM, our crawlers follow a snowball sampling strategy ( Goodman, 1961 ). Each crawler starts with a number of objects as seeds (at least 800). For each object under consideration, it follows links provided by the applications and retrieves all related objects, storing them for future consideration. In YouTube and YahooVideo, the seeds were randomly selected from the list of all-time most popular videos provided by the applications. In LastFM, they were selected among the artists associated with the most popular tags. That is, our LastFM dataset consists of artist pages including music tracks. For
CiteULike, which does not provide explicit related object links but makes daily snapshots download, we collected a random sample of one snapshot.
The YouTube, YahooVideo and LastFM crawlers ran for approximately 2 weeks in July, September, and October 2008, respectively, whereas the sampled CiteULike snapshot is from September 2008. In total, we collected:
Both YouTube and YahooVideo allow users to assign one of a set of pre-determined categories to their videos. In order to the application, external data sources can be used to retrieve the musical genre of artists, which, in turn, can be used as ob-ject classes. In particular, as performed by Chen et al. (2009) , we sampled musical genres from the AllMusic website, egorized sample than previous work ( Chen et al., 2009 ). Similarly to LastFM, CiteULike does not provide any article category Thus, we do not include CiteULike in our current classification experiments, leaving its analysis for future work.
In total, there are 20 and 15 categories in YahooVideo and YouTube, respectively, whereas 9 musical genres were used in the categorization of artists in LastFM. These categories, referred to as object classes, are listed in Table 2 . 5. Textual feature characterization
In this section, we analyze the use of the four selected textual features in the four studied applications. Our characteriza-of F .
 Our analyses cover five aspects, providing answers and insights to the following questions: What is the fraction of objects with empty feature instances?
Feature usage is of key importance to IR, as under-explored features, that is, features that have empty content in a non-negligible fraction of the objects, may not be a reliable source of information. We thus characterize feature usage, investi-gating which features are more explored by users in each application.
 What is the amount of content present in non-empty feature instances?
If present (i.e., non-empty), a feature instance should also provide enough content about the object it is associated with to be an effective source of information for IR. We characterize the number of unique terms available in non-empty feature in-stances. We also correlate the amount of content with object popularity to assess whether textual features of more popular objects tend to carry more content.
 What are the relative descriptive and discriminative powers of the features?
Effective sources of data for IR tasks should offer a reasonably accurate description of the object X  X  content and/or discrim-inate objects into different pre-defined categories (for services such as directory organization, and content recommendation) and into levels of relevance or pertinence (for searching and advertising). Thus, we here adopt two heuristic metrics to esti-mate the descriptive and discriminative powers of the features.
 What are the semantic properties of content in each feature?
The semantic properties of the terms in each feature are also important to IR tasks as some grammatical classes (e.g., nouns) are better content descriptors than others (e.g., adverbs). Moreover, the degree of polysemy and noise (i.e., non-exist-semantics of terms in each feature according to two widely used semantic databases.
 How diverse are the content and the information provided by different features of the same object?
We also quantify the amount of content and information diversity across feature instances associated with the same ob-of any feature in isolation.

Our characterization is driven by our belief that these five aspects are key to determine the quality of a feature (and of multiple features) as data source for effective IR services, although we are not claiming any sufficiency property. Moreover, we argue that these aspects are not equally important to all services ( Almeida et al., 2010 ), which motivates us to analyze each of them separately. Take automatic object classification as an example. If a feature contains a few highly discriminative terms, the total amount of content available might not be as important for service effectiveness. However, classification algo-rithms typically combine the discriminative power of multiple terms, using different weights, to make decisions. Thus, more content might indeed be beneficial, as it provides more evidence to support classification decisions. However, if many terms are used indiscriminately across most classes, more content might, in turn, be detrimental. In contrast, for services that ex-ploit content-based filtering (e.g., content recommendation), good descriptive power might be more important. In either case, high descriptive or discriminative powers might be of little use if the feature is empty (i.e., with no content) in most objects.

Our characterization results are presented and discussed in Sections 5.1, 5.2, 5.3, 5.4, 5.5 . Our main findings are summa-rized in Section 5.6 . 5.1. Feature usage
Table 3 shows, for each feature and application, the percentage of empty feature instances, i.e., objects with no terms in the given feature. The annotation rights of each feature are shown in parenthesis: C for collaborative and R for restrictive. The fraction of empty instances is much larger for collaborative features. In fact, some of these features, such as in all applications but YouTube, and DESCRIPTION in LastFM and CiteULike, are vastly under-explored. Even YouTube, with mil-ne, 2007; Marshall, 2009 ). The TAGS feature, focus of most previous efforts to enhance Web 2.0 IR services, is also absent in 16% and almost 19% of the objects collected from YahooVideo and LastFM, respectively. Only tions, is present in practically all objects.
 These results may be partially explained by restrictions imposed by the application, such as the case of YouTube, which may be automatically filled by the system. However, none of the applications enforces the usage of other two applications. In fact, in YahooVideo, it has a much larger presence than cation. This is surprising, as we expected that users would be more willing to provide a few words as
An interesting comparison is that of TAGS in CiteULike and LastFM, where the feature is collaborative. Even though both applications motivate the organization of personal libraries, CiteULike does explicitly ask users to associate articles, whereas no such explicit incentive exists in LastFM. Since tems ( Marlow et al., 2006 ).
 Thus, considering solely object coverage in terms of feature presence, the restrictive
In particular, using collaborative TAGS , DESCRIPTION or COMMENTS due to the lack of content for a significant fraction of objects. 5.2. Amount of content
In this section, we analyze the amount of content available in each textual feature. Since this analysis is language-depen-dent, we focus on the English language, applying a simple filter that disregards objects containing fewer than three English stop-words in the whole content of all textual features, 16
MENTS , before counting stop-words in order to maximize the amount of objects selected for analysis, since it is somewhat ex-pected for combined features to have at least three stop-words, if they are in fact in English. We also focus our analysis on non-empty feature instances.

After removing non-English objects and empty feature instances, we were left with a varying number of objects for the analysis of each feature in each application. This number is larger than 150,000 for but LastFM, on which it exceeds 86,500 objects. Our cleaned datasets also include 152,717, 76,627, 6037 and 76 objects with
COMMENTS in YouTube, LastFM, YahooVideo and CiteULike, respectively. We removed terms containing only non-alphanumeric characters as well as stop-words, and applied the Porter stemming algorithm
We characterize the number of distinct terms in each feature instance, here referred to as its vocabulary size , which repre-sents the amount of content available for use by IR services. These results are summarized in Table 4 , which presents the mean l , coefficient of variation CV (ratio of standard deviation to the mean), and maximum values. the features X  annotation rights.
 In general, TITLE has instances with the smallest vocabulary, followed by the inherent degree of verbosity expected from each feature: in general, DESCRIPTION , which in turn, is expected to be smaller than collaborative features tend to have vocabularies with larger average sizes and high variability (CV). For instance, the table shows that, considering all four features in the same application, of
DESCRIPTION tend to have larger vocabularies in CiteULike and LastFM, where the feature is collaborative, than in the video applications, where it is restrictive. The same holds if we compare Another important factor seems to be the popularity of each feature among users of the application: to carry much less content in YahooVideo and CiteUlike, where the feature is greatly under-explored (Section 5.1 ), than in the other two applications. These results are consistent with the ones previously observed for Flickr ( Marshall, 2009 ).

In CiteULike, specifically, the TAGS feature, despite collaborative, tends to have instances with smaller vocabularies, thus tles. It may also be due to the stabilization of tagging vocabulary with time, previously reported by Golder and Huberman article abstracts, with more content than the poorly used
In LastFM, TITLE has a much stronger bias towards smaller vocabularies than in the other applications. This is expected since the feature usually contains artist names. In contrast, the age of 27 terms per object and reaching as many as 269 terms per object.
 content, on average, although this is biased towards very popular artists (see discussion below).
 a text, in a wiki-like manner, describing the artists.
 YahooVideo and YouTube show similar trends, although the vocabularies of instances of both to be much larger in YouTube, possibly due to its larger audience. In contrast, the instances of ularies in YahooVideo. Whereas the differences in TAGS may be due to the higher degree of collaboration in YahooVideo, the differences in TITLE may reflect differences in usage patterns.

A relevant issue for IR services is whether the amount of content in each feature tends to be larger in more popular ob-each feature instance and the object popularity. Object popularity is estimated by the number of views in YahooVideo and
YouTube, by the number of listeners in LastFM, and by the number of posters in CiteULike. In general, we found non-negli-gible positive correlations only for collaborative features that are largely adopted by users, such as ( q = 0.5) and YouTube ( q = 0.24), TAGS in CiteULike ( q = 0.23) and LastFM ( q = 0.41), and tives to YahooVideo users for tagging objects.

In summary, our results show that, if present, collaborative features, in general, tend to carry more content than restrictive features tend to bring more content in more popular objects, the measured correlations, although non-negligible in several cases, as discussed above, are of at most intermediate degree ( q as well as the social networks developed within the application might also play important roles on how features are explored by users. Thus, all such factors should also be considered in the design of future Web 2.0 applications and services. 5.3. Descriptive and discriminative powers
Multiple strategies can be adopted to estimate the descriptive and discriminative powers of a feature. One possibility is to rely on manual assessment by volunteers. However, such user experiments are typically quite costly and have an unavoid-depend on the aspect under evaluation (e.g., relevance of an object to a query or pertinence to a category or topic), thus requiring assessment in the context of a specific service. An alternative strategy is to use heuristics to capture (to some ex-tent) these powers. As an advantage, heuristics can be applied to much larger object samples at lower cost. However, heu-ristics can still provide valuable insights into the quality of each feature.
 That said, we here choose to apply heuristic metrics to assess both descriptive and discriminative powers of the features.
A number of different heuristics, each one with its own characteristics and limitations, are available in the literature. For example, one could use the distribution of term frequencies, information gain, term entropy ( Mitchell, 1997 ), and concen-sophisticated metrics, such as cohesiveness, coverage and extent, have also been proposed to evaluate the quality of tag clouds ( Venetis et al., 2011 ).

We here choose to make use of two heuristic metrics previously proposed as part of an Information Retrieval model to assert the importance of different blocks in traditional structured webpages (i.e., webpages with specific information blocks, ent context of textual features of Web 2.0 (multimedia) objects. One can see different textual features as defining different content blocks. For instance, on YouTube, the comments posted by users about a video are placed near to each other, in a common ( COMMENTS ) block. Thus, we here consider each feature instance associated with an object as an information block.
The two metrics, here referred to as Average Feature Spread and Average Inverse Feature Frequency , are used to estimate the descriptive power and discriminative power, respectively, of each analyzed feature. 5.3.1. Average Feature Spread
Our estimation of the descriptive power of each feature is based on a heuristic metric called Average Feature Spread , which term which appears in at least one feature instance f associated with o , and T the famous singer.
 spread across all terms in f . That is, given j T f j the number of distinct terms in instance f , FIS ( f , o ) is defined as: with the largest TS , could be applied to compute FIS .

The Feature Instance Spread heuristic assesses how the terms of a given feature instance f are related to the content of instances of other features associated with the same object o . It is thus a heuristic to estimate how the feature instance f is related to o  X  X  content, being here used as an estimate of the average descriptive power of feature instance f for the given object o .

Following this reasoning, the descriptive power of a feature F in the object collection O can be captured by averaging the collection O and the instance of F associated with it, f , the AFS ( F ) is computed as: 5.3.2. Average Inverse Feature Frequency
The Average Inverse Feature Frequency (AIFF) metric, here used to estimate the discriminative power of a feature, builds on a instances of a feature F as a separate  X  X  X ocument collection X  X . In other words, given a feature F with j F j non-empty feature instances, 19 and a term t that occurs in at least one instance of F , the IFF ( t , F ) of term t in F is defined as: tering criteria, such as disregarding very unpopular terms that might undesirably inflate the IFF value (see discussion below).
The IFF metric assesses how much information carries the occurrence of a given term in a given feature . The assumption is that terms occurring in many instances of the feature are bad content discriminators. For example, whereas the occurrence of rence of  X  X  X ting X  X  may be more useful to discriminate it from other objects.
 instances of F . It is thus a heuristic to estimate the discriminative power of the feature F in the object collection. Given T complete term vocabulary of feature F (i.e., considering all instances of F in the object collection), and j T is computed as: 5.3.3. Results
We computed AIFF and AFS values for each feature in all four applications using our stemmed datasets and considering only objects with non-empty instances of all features. Given the negligible fractions of non-empty and CiteULike, we disregarded this feature in both applications. Thus, AFS and AIFF are computed only for objects with all four features in LastFM and YouTube, and with TITLE , TAGS along with corresponding 90% confidence intervals, are shown in Tables 5 and 6 , respectively. Each table shows two blocks of results: one computed using all available terms (upper block) and the other computed after filtering some of the terms at the 90% confidence level when using the Mann X  X hitney X  X  U and Student X  X  T tests ( Jain, 1991 )) are shown in bold.
As shown in Table 5 (upper block), AFS results provide a consistent ranking of the features in all four applications. Accord-ing to this heuristic, TITLE is the most descriptive feature, followed by argue that, as heuristics, FIS , and ultimately AFS , have biases towards larger values for smaller feature instances. In other words, larger instances might have lower FIS values simply because there is a higher chance that most of their terms are not included in the other (smaller) features. In order to reduce the impact of this possible bias on AFS results, we recomputed the FIS and AFS values considering only the k terms with largest TS values. Table 5 (lower block) shows the results for k =5. Note that, despite the smaller differences among AFS values of different features of the same application, the most descriptive feature according to this heuristic. Indeed, the same relative order of the features holds for both video larger AFS (computed over the top-5 TS terms) than TAGS , implying that, on those two applications, few very descriptive terms (according to the AFS heuristic). On CiteULike, at least, this result is not completely surprising given that DESCRIPTION instances typically contain the abstracts of the articles, which are expected to carry some terms that are strongly related to the article X  X  content. On LastFM, in turn, the wiki-like manner in which users collaboratively edit ering the results computed over all available terms, whereas the lower AFS value for of the metric, it might also imply that, in spite of the existence of a few very descriptive terms, larger feature containing full sentences, might also carry many poorly related (or unrelated) terms, which ultimately reduces its overall descriptive power.

In contrast, Table 6 (upper block) shows less distinction in the AIFF values across features in any application, with no con-sistent ranking. In order to understand why the AIFF metric is not able to clearly distinguish one feature from the other, we plotted the term popularity distribution of each feature, considering all instances of the feature. In other words, the popu-tributions are heavy-tailed in all applications, thus containing a large number of terms with very low popularity. These terms have very large IFF values, and end up boosting the AIFF of all features somewhat similarly.

Thus, we recomputed the AIFF values considering only terms that appeared in more than 50 feature instances. sults, also presented in Table 6 (lower block), show a more clear distinction between the features (reported values are statis-tically different at 90% confidence level). Overall, we observe that, according to the AIFF heuristic, discriminative feature, followed by TAGS , DESCRIPTION and, if considered,
One exception is CiteULike, where the TAGS feature has the best discriminative power (according to the recomputed AIFF ). The many objects, which lowers their IFF values.

It should be noticed that, although statistically different, the relative difference in values between some of the results are may depend more on other factors such as the amount of content and diversity in content and information (as we shall dis-cuss on Section 5.5.1 ). Such factors, for instance may have a larger impact in classification results (see Section 6 ). On the other hand, some differences are in fact very large (e.g. YouTube AFS values between nificant implications on the effectiveness of some services such as recommendation, as we shall see in Section 7 . 5.4. Semantic properties
We now turn our attention to the semantic properties of each feature X  X  vocabulary, focusing, as in Section 5.2 , on the Eng-lish language. Golder and Huberman (2006) reported that tagging systems may be very affected by synonymy (multiple words with the same meaning) and polysemy (one word with multiple meanings). These ambiguous terms may have a degenerative impact on the effectiveness of IR tasks. For example, the results produced for a query may not contain a rele-vant object if it is described by terms that are synonyms of the query terms.
 2007 ) semantic databases to analyze the semantic properties of each feature X  X  vocabulary. Unlike in that study, we here ana-lyze distinct terms in their original form, applying only plural stemming. Wordnet is a lexical database of the English lan-guage in which words (nouns, verbs, adjectives, etc.) are grouped into sets of synonyms, called synsets . These sets are connected based on lexical and semantic relations. Yago is a similar database but describes and relates entities. For example,  X  X  X arilyn Monroe X  X , an American Actress, is related to other American Actresses, such as  X  X  X ane Fonda X  X . We use Yago in order to determine if a term is a proper name. Collectively, we refer to both databases as the dictionary .

Table 7 presents the percentage of terms with at least one entry (i.e., a certain grammatical class for a given term) in the dictionary. The percentage of terms with no entry in the dictionary is here taken as an estimate of the amount of noise in each feature. Table 8 presents the distribution of terms that are present in the dictionary across grammatical classes. Also note that, each line in Table 8 does not add up to 100% as the same term may be categorized into multiple grammatical clas-ses (e.g.,  X  X  X ance X  X , which is counted as both verb and noun).

In consistency with the results for TAGS reported in ( Suchanek et al., 2008 ), we find that a reasonable fraction (15 X 39%) of the terms in all features and applications are not found in the dictionary, and are here considered as noise . The fraction is typically larger for COMMENTS , being 29% in YahooVideo and 35 X 37% in the other applications. It is also worth noting the large fraction of terms not found in the dictionary in LastFM TITLES content of this feature, are not found in the dictionary. Moreover, out of the terms with known meaning, most of them (55 X 80%) are nouns, typically good content descriptors. There is also a non-negligible fraction of (known) proper names, which also tend to offer good descriptions, in all applications and particularly in LastFM. This is expected since our LastFM dataset consists of artist pages.

We also measure the percentage of terms in each feature with k or more meanings, for values of k greater than 1. Table 9 shows the results for k equal to 5 and 10. Note that the numbers lie between 5% and 16% for k equal to 10, indicating that some degree of polysemy affects all features, in all applications, which might ultimately impact the effectiveness of IR tasks.
In case these tasks employ any textual context analysis, this impact might be reduced in features which typically contain complete sentences (e.g., DESCRIPTION ) as opposed to isolated words (e.g.,
We note that semantic databases, such as the ones used in this analysis, are not expected to cover the meanings of every single term used in social media applications. For example, Internet slang will most likely not be covered by neither data-base. Another possible shortcoming is that analyzing terms independently of their context will increase the amount of ambiguous terms, due to polysemy. Moreover, even terms not found in a dictionary may still be useful for different IR tasks.
For instance, the co-occurrence of terms with known and unknown/rare meanings may help classification tasks. Such terms may also help users find very specific content in query-based search. Nevertheless, we can still conclude that: (1) a reason-able fraction of terms in all features and applications have no meanings according to the considered databases; (2) most terms with at least one known meaning can be classified as nouns and/or proper names, which are good descriptors of con-tent, and (3) a non-negligible degree of textual ambiguity affects all features and applications, and its impact might be sig-nificant when content analysis is performed by taking terms independently. 5.5. Content and information diversity
So far we have characterized different aspects of each feature separately. We now investigate whether different features associated with the same object contribute with different pieces of content (i.e., terms) and with different pieces of informa-tion (i.e., semantic meaning) about the object. Towards that goal, we characterize the amount of different terms across on the English language, as in Section 5.2 . Content diversity is analyzed over the stemmed data sets, whereas information diversity is characterized over the original (non-stemmed) collections.
 5.5.1. Content diversity We assess the diversity of content across features of the same object by quantifying the similarity between their contents. Content similarity between two feature instances is estimated in terms of term co-occurrence using the Jaccard coefficient. Given two sets of items T 1 and T 2 , the Jaccard coefficient is computed as:
We compute the content similarity between two features associated with the same object using as input sets the N most highly ranked terms from each feature instance based on the product of the TS and IFF metrics.
Table 10 shows the average similarity, along with corresponding 90% confidence intervals, between all pairs of features in all four applications for N = 5 and when all terms of each feature instance are considered. disregard COMMENTS in CiteULike and YahooVideo. According to the table, there seems to be more similarity between restrictive features (e.g., TITLE and TAGS in YouTube), as the same user tends to use common words in them. The exception is Video, which, despite collaborative, shares great similarity with the restrictive 2010 ) which show a significant overlap between TITLE and that the average Jaccard coefficients are all under 0.52. Thus, although there is some co-occurrence of highly ranked terms across features, each feature may still bring new (possibly relevant) content about the object. 5.5.2. Information diversity
The Jaccard coefficient used in the previous section does not capture the fact that different words may be used to describe the same underlying meaning, even when using stemmed datasets. This is because it only analyzes co-occurrence of terms, and thus does not consider synonyms. It also does not capture variations in the degree of specificity of the terms: while some thus extend our analysis to quantify the semantic similarity between (non-stemmed) words occurring in different features associated with the same object.

In order to measure semantic similarity between sets of words, we again make use of the Wordnet dictionary, and more specifically of its hyponym semantic relation: a word w is a hyponym of another word a hyponym of furniture . Using the hyponym relation, we can infer a taxonomy, that is, a hierarchical structure describing the relations between word meanings. The inferred taxonomy can then be used to measure semantic similarity. Fig. 2 shows a small example which illustrates a portion of the taxonomy inferred from the Wordnet hyponym relations. The closer two words are in the taxonomy tree, the more similar they are. In the example, settee is more similar to divan than to yoke .
Words can and frequently do have multiple senses, which may or may not be related. The word chair , for instance, means either a seat or a person in charge of a meeting, organization or conference. Thus, even though professorship is not very re-lated to wheelchair , they are both hyponyms of chair . Therefore, the taxonomy is built with relations between word senses , and not simply between the words themselves. That is, the same word may appear multiple times in the taxonomy, each occurrence representing one of its senses . This strategy avoids relations between unrelated senses .

Given the taxonomy, we make use of the Leacock X  X hodorow ( LC ) metric ( Fellbaum &amp; NetLibrary, 1998 ) in order to mea-sure semantic similarity. Given two word senses s 1 and s logarithm of the length of the shortest path connecting them in the taxonomy, normalized by the maximum depth of that taxonomy, that is: where taxonomy _ dist is the length (in nodes) of the shortest path between s the taxonomy. Since path length is measured in nodes (as opposed to edges), the metric is defined even when the compar-ison is between a word sense and itself: the path between a node and itself includes one node. Note that, for a given taxon-omy, the closer two word senses, the larger their LC values and thus the stronger their semantic similarity (as captured by the LC heuristic).

To illustrate the computation of the LC metric, let X  X  use the taxonomy shown in Fig. 2 to estimate the semantic similarity between word senses wheelchair and yoke . 23 The maximum depth of the taxonomy is 5, whereas the shortest path connecting
We can then estimate the semantic similarity between two words w and row similarity between each sense s 1 of w and every sense s where S u is the set of senses of word u .

Given this definition, we can compare the sets of words that appear in different features associated with the same object by measuring the average similarity of all pairs of words in those features. Thus, given two feature instances f their corresponding sets of distinct words, W f 1 and W f follows:
Note that words that appear in both feature instances are excluded from the IS computation as their similarities are cap-tured by the Jaccard Coefficient. Such words would, otherwise, artificially inflate the similarity between feature instances.
The greater the value of IS between feature instances f 1
Average values of information similarity between different feature instances, along with corresponding 90% confidence intervals, are shown in Table 11 . These numbers are computed considering only nouns that have at least one word sense since the taxonomy only contains nouns. 24 Once again, we disregard
TITLES in LastFM as they contain mostly names of artists or bands (i.e., proper names) which are not captured in the Wordnet hyponym relations.

In general, the semantic similarity between words in different features of the same object can be considered low, with average IS values under 1.6. In order to better understand these results, we further investigated the values of the Lea-cock X  X hodorow coefficient measured in our data sets. We found that, on average, pairs of words appearing in different fea-tures of the same object have a taxonomy _ dist (given their different senses) of approximately 9, which can be considered large. This is also the mode of the distribution of measured taxonomy _ dist values. Some examples of pairs of words that  X  X  X ideo X  X , which clearly have very weak (if any) semantic similarity. We also found pairs of words, such as  X  X  X hing X  X  and  X  X  X aw X  X , which, according to Wordnet, are at a distance equal to 3 in the taxonomy, but whose semantic relation seems very weak to us. In fact, we observed that most pairs of words are connected to each other via other words of very general nature, such as  X  X  X hing X  X ,  X  X  X ntity X  X , and  X  X  X erson X  X .

More broadly, we also interpret the IS values reported in Table 11 , taking as baseline, for each application, a uniform ran-dom model consisting of 1000 objects. These randomized collections are built as follows: considering each feature sepa-rately, we assign words to each instance of the feature, selecting them with equal probability from the feature X  X  word vocabulary (extracted from the corresponding original data set), while keeping the distribution of the number of words per feature instance the same as in the original collection. Note that, by independently selecting words to different features, we are indeed breaking any semantic dependency there might exist among feature instances of an object. Average IS values and 90% confidence intervals for the randomized collections are also shown in Table 11 . In general, they are only slightly smaller than corresponding measures for the original data sets. In other words, the semantic similarity between words across features of the same object is indeed low, only slightly higher than the similarity that arises, by chance, in the ran-domized collections. 25 These results provide evidence that textual features associated with the same object do carry not only different content but also different information. 5.6. Summary of our findings Our characterization results may be summarized into five main findings. First, all four features but ligible fraction of empty instances in at least two of the analyzed applications, and thus might not be effective as single source of data for IR services. More broadly, restrictive features tend to be more often explored by users than collaborative ones, even within the same feature category. Second, in contrast, considering only non-empty feature instances, the amount of content tends to be larger in collaborative features. Third, the typically smaller and more often used exhibit, in general, higher descriptive and discriminative powers, followed by CiteULike and LastFM, DESCRIPTION has a higher descriptive power than the top-5 most descriptive terms, implying that DESCRIPTION cations. Fourth, all textual features have a large amount of nouns and proper names, according to our dictionary, which can be considered good content descriptors. However all of them also suffer from semantic problems such as presence of noise and polysemy. Finally, through the use of two different metrics, namely Jaccard and Information-Similarity coefficients, we found evidence that there is a significant amount of content and information diversity across features associated with the same object.
In the following sections, we assess the relative quality of the textual features when applied to two specific IR tasks, namely, object classification and tag recommendation. Results for both tasks are discussed in light of our characterization findings, since each of the analyzed quality aspects may impact the effectiveness of each task differently. We also experiment with strategies based on the combination of multiple features, motivated by the content and information diversities observed across object features in our datasets. 6. Object classification This section presents our object classification experiments. These experiments use the collected categories, introduced in Section 4 , as object labels. As discussed in that section, we focus our experiments on LastFM, YouTube and YahooVideo. In
Section 6.1 , we present the model adopted for object representation, and define the various classification strategies consid-ered. Our experimental setup is described in Section 6.2 , whereas the main results are discussed in Section 6.3 . 6.1. Object representation model and classification strategies
In order to perform object classification, we first need to define a model to represent the objects. We adopt the vector space model (VSM), representing each object as a vector in a real-valued space whose dimensionality j V j is the size of the vocabulary of the object feature(s) being considered as data source. Each term in the feature(s) is represented by a real-val-ued weight in the vector that represents the degree of relationship between the term and the object it is assigned to.
There are two important issues here: (1) how to determine the weight of each term in order to capture the semantics of the object, and (2) how to model the objects in the VSM using their distinct textual features. Regarding the former, we con-sider the following term weighting schemes: TF IFF : The weight is given by the product TF ( t , f ) IFF ( t , F ).
 The last two schemes allow us to compare TS and the more commonly used Term Frequency (TF) metrics.
 We also examine the following six strategies to model an object o as a vector V , which is normalized so that k V k =1:
Bag-of-Words (Bagow.) : All four features are taken as a single document, as done by traditional IR algorithms, which do
Concatenation (Conc.) : All textual features are concatenated in a single vector, so that the same term in different fea-
In each strategy, vector V is defined as h w f 1 , w f 2 , ... , w the last two strategies are motivated by the results in Section 5.5 , and allow us to investigate how textual features that are effective when used in isolation compare with the combination of multiple features. 6.2. Experimental setup Our classification experiments are performed using two well-known and widely used classification algorithms: a Support and a k-Nearest Neighbors Classifier (kNN) ( Baeza-Yates &amp; Ribeiro-Neto, 2011 ) implemented by ourselves, which uses a co-sine measure to estimate distances between vectors. The first algorithm was selected because it is an effective state-of-art interpret their results. Moreover, this algorithm was originally proposed for binary classification problems. Thus, it might have potential scalability problems when applied to tasks with more than two classes, as is the case of our Web 2.0 appli-bole, Sarawagi, &amp; Chakrabarti, 2002 )). We here adopt the one-against-the-rest strategy, as it is the standard approach implemented in the Liblinear tool.

The kNN algorithm was chosen as an alternative method which has been shown to have a competitive effectiveness ( Yang &amp; Liu, 1999 ) and potentially superior performance to SVM in problems with many classes, besides allowing for more inter-pretable results. We note, however, that kNN might suffer from scalability issues on very high dimensional vector spaces, as it needs to compute vector distances. We also note that we chose to use the cosine metric to compute such distances as it is directly impacted by the adopted term weighting scheme, thus allowing us to have a better assessment of the relative impact of using each of the four proposed schemes on classification effectiveness.

As before, we use our stemmed datasets, considering only labeled objects (i.e., objects with an associated class) with non-empty instances of all features, except in YahooVideo, for which the shows, some object classes are highly underpopulated in our datasets. 2.2% of the objects, removing 8 and 4 classes from YahooVideo and YouTube, respectively.

Our experiments consist of 10 runs, each with a distinct sample of 5000 objects from each application, using 10-fold cross-validation within each sample. Best SVM parameters (e.g., kernel parameter and cost C ) were searched for within each training sample, using cross-validation, and the default values (linear kernel and C = 1) were found to be the best ones, which is consistent with the literature for textual classification ( Joachims et al., 1998 ). The number of neighbors used by the kNN classifier to make a decision (i.e., parameter k ) was set to 30 in all experiments, as this value produced the best results in various preliminary experiments using the training set (also with cross-validation).

We assess classification effectiveness using a commonly used metric, namely F1, which captures both precision and recall out of those that should have been classified to the class. Both are commonly combined into another metric, called F1, which consists of their harmonic mean, i.e.: Precision, recall and F1 are applicable to each class. A general result for all classes can be obtained by considering either
Macro F1 , defined as the average F1 across all classes, or Micro F1 , defined by first computing Micro precision and recall considering all classes and then computing the harmonic mean of these two measures. Micro F1 tends to be dominated by the classifier X  X  performance on more popular categories while Macro F1 gives equal importance to all categories. As our collections are very unbalanced, Macro F1 results are of more importance. Nevertheless, we point out that Micro F1 re-sults led to similar overall conclusions. Thus, for the sake of improving the clarity of result presentation and discussion, we chose to omit Micro F1 results, focusing rather on discussing the results in terms of Macro F1. 6.3. Most relevant results
We perform classification experiments combining each term weighting scheme with each textual feature (and feature combination) object model. Tables 12 and 13 show the results for each configuration considering the three applications, five weighting schemes, six object representation models, and two classification algorithms. The tables show average results, computed over 100 samples (i.e., 10 runs, with 10-fold cross-validation within each run), along with corresponding 90% con-fidence intervals. Note that, according to such intervals, the results deviate from the reported means by less than 10%. For at the 90% confidence level) are indicated in bold.
 pects (e.g., amount of content, discriminative power, descriptive power) are not equally important to all IR tasks. Indeed, even their relative impact on the effectiveness of a given task (e.g., classification) may also vary depending on the inherent biases and on the robustness of the specific algorithm adopted to perform it. As we shall see, in the particular case of the algorithms analyzed here, discriminative power and amount of content play important roles on classification effectiveness.
Other aspects, such as how the classes are defined, the level of semantic overlap among multiple classes, and biases in class distribution, might also impact the results.

We start our discussion by analyzing the impact of the different term weighting schemes on the effectiveness of each clas-sifier. When comparing all weighting schemes against each other, we can see that the IFF weighting scheme, which exploits a
Indeed, IFF is the best weighting scheme, or very close to the best one, for many object representation models. This is perhaps more clearly seen across the results produced by the kNN classifier ( Table 12 ), where the differences among the weighting schemes are somewhat more distinct, although some differences also arise with SVM (check, for instance, the results for
YouTube and BAG-OF-WORDS in Table 13 ). 27 Moreover, we can also see that the results produced with the TF and TS weighting schemes are, in many cases, improved (to some extent) when these schemes are combined with IFF . Once again, such improve-ments are more significant with kNN, reaching as much as 80% (see are very marginal (mostly under 1%). The following discussion considers the best results produced across all weighting schemes.
Turning our attention to the relative effectiveness of both classifiers, our results reveal that, in LastFM, the simpler kNN is very competitive with SVM, with a relative difference falling under 5%. tween the two classifiers are greater, with a clear advantage for SVM.

We now discuss the classification effectiveness as a function of the various object representation models proposed. Con-sidering the results obtained when each feature is used in isolation as object representation, single feature in all applications for both classifiers. This is consistent with our characterization results which show that have: (1) good descriptive and discriminative powers, according to our heuristics, with AFS and AIFF values close to those of , and (2) at least twice more terms than TITLE , on average, in the three applications. Thus, the larger amount of content clearly favors TAGS as source of data for the classifier, particularly for SVM, which is known to work better in the presence of larger content (term) spaces ( Joachims et al., 1998 ). This issue of amount of content may also explain the poor perfor-ones. For example, instances of TITLE in LastFM typically contain artist names, which are usually very short. These may be very objects may be too specific and may not occur in the test, therefore not generalizing.
 Regarding the other two features, COMMENTS is usually a bit better than In spite of the smaller AFS (descriptive power) and a somewhat similar AIFF (discriminative power), on average, more than eight times more terms than DESCRIPTION classification effectiveness (see further discussion below). On the other hand, we find that
LastFM, in spite of the somewhat larger amount of content of the latter and comparable AFS and AIFF values. Despite not being completely supported by our characterization, this result may reflect the wiki-like collaborative nature of the feature in LastFM, which brings a tendency for a higher quality semantic content, a phenomenon also observed in Wiki-being subject of future work.

Recall that the Jaccard and Leacock X  X hodorow Coefficients, analyzed in Section 5.5 , suggest that there exist distinct content and information in each feature, which can be leveraged for classification purposes. This is confirmed by the results of both feature combination strategies, which, except in LastFM, do bring some improvements over using features in isolation, partic-ularly considering SVM. These results, along with the natural expansion in the amount of content, provide evidence supporting the notion that content and information diversity across features can improve the efficacy of IR tasks. We note, however, that such improvements are not very large. Indeed, the largest improvement across all analyzed configurations is obtained with the kNN classifier in YahooVideo. In such case, using BAG-OF-WORDS Macro F1. Considering the results produced by SVM, the largest improvement of using
YouTube). These differences in classification effectiveness can be considered small given that the sizes of the vocabularies of the much larger amount of content does not lead to a corresponding increase in performance, as other aspects (e.g., discrim-inative power of the available content) also impact classification results. Moreover, we should note that such larger vocabu-laries might have strong implications for the time complexity of the classification process, particularly during training. Comparing the results obtained using both feature combination strategies, we find that, in most cases, the results for CATENATION are only slightly better than those obtained with the simpler duced dimensionality of the vector space. This finding is not in consonance with recent results by Ramage et al. (2009) , which demonstrate a clear advantage for the CONCATENATION work, the authors concatenated TAGS with the whole content of the Delicious webpage, a very different approach from the one adopted here.

Comparing the classification effectiveness in the three applications, our results show that, for any given feature (or feature combination), the results in LastFM are much higher than in the other applications (except for cussed). These results are further illustrated in Fig. 4 , which shows per class average F1 values for both results are more evenly distributed across classes. In contrast, in YouTube and YahooVideo, average F1 values can be as low as 0.27 and 0.33, respectively, being also more unevenly distributed.

The higher average F1 values in LastFM may be explained by the fact that LastFM object classes are defined by experts from the music industry, guaranteeing a higher level of consistency in the class assignments when compared to the other applica-is exacerbated if we consider that YouTube and YahooVideo have a larger number of classes, and that some of them may have some degree of semantic overlap, making it even harder for users to determine to which class an object belongs. For example, a
In sum, considering the automatic classification of objects in the analyzed applications, we conclude that: (1) weighting schemes that explore discriminative power have good effectiveness either in isolation or in combination with other metrics; (2) kNN may be competitive with SVM in some applications (notably, LastFM); (3) isolation for classification purposes due to a combination of good discriminative power and large amount of content (besides good descriptive power), leading to results that are often very close to the two analyzed feature combination strategies, while producing a much smaller feature space; (4) feature combination may bring some benefits due to the presence of dis-tinct and somewhat complementary content; (5) a simpler feature combination strategy based on bag of words may be as effective or at most only slightly worse than concatenating features as different feature spaces, which is a much more expen-sive approach; (6) a combination of fewer classes, with possibly less ambiguity, and more qualified human labelers, made automatic classification more effective in LastFM than in the two video applications. 7. Tag recommendation
We now turn our attention to the relative quality of the textual features for supporting a different IR task, namely, tag recommendation. The goal of this task is to support users during the process of tagging Web 2.0 objects, by recommending 2010 ). We here focus on recommending tags because: (1) it is the most studied textual feature on the Web 2.0, being of more interest to the community; (2) our characterization and classification results indicate that this is a feature of quality (with respect to several aspects) when used in isolation; (3) recommending new terms to other features does not make much sense as they are typically composed of full sentences with semantic meanings, while the the recommended tags can be, at least partially, automatically evaluated, as we shall see.

We should note that our goal here is not to propose new tag recommendation mechanisms. Rather, our primary goals are to assess the potential benefits of using TITLE , DESCRIPTION ment viable, we here adopt a simple approach to recommend tags, which consists of extracting new and possibly  X  X  X elevant X  X  (according to given criteria) terms from the other textual features and adding them as new expand the contents of the TAGS feature by introducing new recommended terms extracted from the other three features, and then evaluate the quality of such recommendations. We note that, despite the great variety of different tag recommendation methods and tag analyses available in the literature ( Menezes et al., 2010; Sigurbjornsson &amp; van Zwol, 2008; Bel X m et al., tifying the quality of different sources of candidate terms for recommendation purposes. This is our objective here, which distinguishes the present effort from previous studies.

Three problems must be tackled in order to effectively recommend new tags: (1) maximize the amount of relevant terms being recommended; and (2) recommend more relevant terms before irrelevant ones, thus minimizing noise in initial sug-gestions; while, (3) minimizing the amount of irrelevant terms. In other words, the problem of recommending tags is here treated as a problem of ranking the best candidate tags to recommend . This rank will be produced by applying a quality metric (or combinations of metrics) to candidate terms extracted from other features of the same object and ranking those terms according to the value of the metric (s).

Note that determining whether a term is relevant to be recommended as a new tag could require laborious manual inspection, which might be affected by the subjectivity of human judgments. Thus, following the approach adopted in ( Ren-dations only terms that already appear in the TAGS feature of the target object, i.e., the current content of the considered as the  X  X  X old standard X  X . Even though this is a somewhat limited evaluation, in the sense that we cannot say whether a recommended tag that is not currently in the object is not relevant, it allows for an automatic evaluation and serves our purpose of comparing the quality of each textual feature when applied to such a task. In other words, our results can be seen as a lower bound for the effectiveness of a tag recommendation approach that exploits contents from the other features. We also note that this scenario covers common situations such as: (1) when a user is uploading a new object and has filled the contents of (some of) the other features (e.g., built for content already existent in the system.

Next, we first present the metrics used to rank the candidate terms as well as the metrics used to evaluate the recom-mendations (Section 7.1 ), and then discuss the main results (Section 7.2 ). 7.1. Metrics
In order to rank terms extracted from TITLE , DESCRIPTION similar vector space model as the one described in Section 6.1 to represent the contents of each such feature. Moreover, we also adopt a term weighting scheme, here applied as an estimate of the quality of a term as a candidate for recommendation.
In other words, given an instance f of a feature F and a term weighting scheme ws , we produce a set of tag recommendations by ranking all terms in f according to ws .
 As in Section 6 , we consider five different weighting schemes (i.e., quality metrics), namely, TS , TS IFF , TF , TF IFF and IFF . We note that, for computing such metrics, we consider the content of the  X  X  X mpty X  X  so that such content would not influence the results of the metrics. For instance, when computing the TS of a can-didate term t , we only consider its spread in the other three features as an estimate of its quality.
The ordered set of terms of f (ranked according to ws ) is here taken as an ordered set of tag recommendations (or simply a of Rec ( f , ws ) are considered as more promising ones for recommendation, and thus should be suggested first. We here eval-uate the quality of each textual feature F , for each considered weighting scheme ws , by assessing the effectiveness of the f of F .

Note that, despite related to some of the analyses performed in Section 5.3 , our goal here is different. We are evaluating ranked lists of terms , therefore we here take special look at the order in which the candidate terms are ranked. This order is not directly captured by any of the considered weighting schemes. Thus, we believe that evaluating how they perform in this of documents, we evaluate the effectiveness of each recommendation using metrics more pertinent to such task, as discussed next.

A good tag recommender would ideally recommend relevant terms before any noise is suggested. To measure this capa-mon IR evaluation metric which not only considers if the elements from an answer set are relevant, but also their positions in the ranking. Thus, higher AP values are obtained for recommendations that include terms of the dard X  X ) in the top positions of the ranking.
 ranked terms from the recommendation that appear in the TAGS 2 Rec ( f , ws ) and 0 &lt; i 6 k }.
 AP is then defined as the average P @ k for each possible value of k , that is: where rel ( k ) is a binary function indicating if the term at position k is relevant or not, that is, if it belongs to f
We can also compute the mean of AP values, i.e., MAP , for all recommendations. Given j F j the number of instances f of feature F , the MAP of all recommendations produced using F is computed as: 7.2. Most relevant results We perform experiments using stemmed data sets from the four applications, namely, CiteULike, LastFM, YahooVideo and
YouTube. Results are computed over randomly selected samples of 50,000 objects, one from each application. Table 14 shows MAP results, along with corresponding 90% confidence intervals, for all three textual features and five term weighting schemes across the four applications. Note that, for both CiteULike and YahooVideo, we disregard didate terms and from the computation of the weighting schemes) as this feature is vastly unused in these applications. For each considered feature, best results, including statistical ties at the 90% confidence level, are shown in bold.
We start by noting that, in general, rankings produced based on the IFF metric alone tend to have the lowest MAP values among all considered weighting schemes, likely because this metric, as an estimate of discriminative power, considers the importance of a term across the whole collection of objects, and not to the specific object that is target of the recommenda-MAP values, for most features and applications. Moreover, the use of such heuristics along with IFF , in the TS IFF and TF IFF schemes, rarely produces significant gains over using only the heuristics (one exception is rather may be very detrimental to the recommendation (e.g., see results for TS on YouTube). Thus, based on our heuristic metrics, descriptive power does seem to be more important for tag recommendation.
 Comparing the descriptive heuristics, we note that, in many cases, such as in YahooVideo and YouTube (for all features),
TS brings great improvements over TF and over TF IFF . The superiority of TS over TF as well as of TS IFF over TF IFF in most cases (with a few notable exceptions) indicates that simply taking very frequent terms in the feature instance, as done by TF , regardless of whether they occur in other features of the object, may not be a good strategy for tag recommendation. In contrast, taking terms that are spread across multiple features, tends to lead to better suggestions in general. Indeed, in the very few cases in which TS does not lead to the highest MAP values, the differences from the best result are under 5%. One worth mentioning case is TITLE in LastFM: the results for all weighting schemes are statistically tied at 90% confidence level. This is probably due to most terms of TITLE instances having the same weight, for any of the considered weighting schemes. This happens because most terms (i.e., artist names) occur only once in a all other textual features (maximum TS ), while being very specific to the object and thus not occurring in other (maximum IFF ). Ultimately, all five weighting schemes lead to the same rank of candidate terms.
 Moreover, considering the results produced by any given weighting scheme, values, followed by DESCRIPTION and, if applicable, COMMENTS
TITLE outperforms DESCRIPTION in 39% and 22%, respectively. On the other two applications, the gains are more modest (up to 10%) but still significant. This is consistent with our characterization results that showed that power. We note however that, since MAP is based on the fraction of returned items that are relevant and their relative posi-tions, the smaller sizes of TITLE does not severely impact our reported results, not even for LastFM. Nevertheless, this factor might be of relevance in practice, as the smaller sizes ultimately limit the number of candidate terms that can be extracted from that feature (see further discussion below).

Comparing the best results for each application, we note that the MAP values obtained for the two video applications are much higher than the results produced for LastFM and CiteULike. This is possibly due to the smaller overlaps between the contents of multiple features of the same object in LastFM and CiteULike (see Table 10 in Section 5.5 ). Such smaller overlaps terms. Moreover, in the case of CiteULike specifically, Table 5 (Section 5.3 ) shows that any given feature in CiteULike has lower AFS value, a trend also observed for TS values, than the same feature on the other applications, thus implying in less descriptive features (according to our heuristics), which ultimately impact recommendation effectiveness on that application.

In sum, TITLE seems to be the most promising feature for providing term candidates for tag recommendation. Moreover descriptive capacity which is strongly present in TITLE instances, appears to play a very important role for the effectiveness of that task. We should note, however, that the smaller sizes of most term suggestions. In contrast, DESCRIPTION and COMMENTS , usually carrying many more terms, might provide more suggestions.
Nevertheless, in practice, presenting a large amount of suggestions to the user may likely overwhelm her. We argue that, instead, only the most promising terms, that is, those more highly ranked, should be recommended. In that case, extracting we emphasize, once again, that our analyses can only measure whether the produced rankings include terms that we know to be relevant, that is, terms that are already present in the which requires manual inspection, is left for future work. 8. Conclusions and future work
The advent of the Web 2.0 drastically changed the way users explore Web applications. Currently, users may act not only as consumers but also as creators of content, generating what is known as social media . Social media includes not only the main object of interest in an application (e.g., videos on YouTube) but also several features associated with that content. However, by offering no guarantee of quality, social media poses challenges to the IR community. Indeed, common IR services, such as content recommendation, searching and directory organization, still rely mostly on the use of textual features (e.g., tags) associated with the multimedia objects. Nevertheless, the question of which textual features provide potentially better data sources for supporting effective IR services remains open.
 Web 2.0 applications. To provide insights on the matter, we sampled data from with objects in CiteULike, LastFM, YahooVideo and YouTube. Our characterization of these features, using different heuristic metrics, revealed that collaborative features, including TAGS from all analyzed applications. However, if present, they tend to provide more content than restrictive features, such as
We also found that, in general, the smaller TITLE and TAGS ing to our heuristics. We should note, however, that, on both CiteULike and LastFM, in terms of descriptive power if only the top-5 most descriptive terms are considered, implying that, on those two applica-tions, DESCRIPTION instances do carry a few very descriptive terms. Finally, we showed that every feature suffers from semantic problems, such as presence of noise and polysemy, and that there exists a significant amount of content and information diversity across features associated with the same object.

As case studies, we also assessed the quality of the four textual features when applied to two relevant IR tasks, namely object classification and tag recommendation. Our classification results showed that weighting schemes that explore dis-criminative power have better effectiveness either in isolation or combined with other metrics. They also revealed that if present, are the most promising feature used in isolation, and that combining content from all features can lead to some classification improvements. In particular, a simple combination strategy based on bag of words may be as effective or at most only slightly worse than concatenating features as different feature spaces. Moreover, our results showed that, although widely present and highly ranked according to both descriptive and discriminative power heuristics, the worst classification results, being severely impacted by the typically small amount of content.

Our tag recommendation results showed that, in contrast, TITLE MAP results. We believe that this is likely due to TITLE , like with a few keywords. Thus, terms extracted from the TITLE tures. We note however that such conclusions are constrained by our evaluation methodology, which considers only previ-ously assigned tags as relevant. A more thorough evaluation, based on manual inspection by a group of volunteers, is left for future work.

These results illustrate that different quality aspects may impact differently the effectiveness of different services. In other words, the most promising textual feature may vary depending on the target service. We here analyzed two specific
IR tasks, while also producing characterization results that uncover knowledge and raise insights that may be quite valuable for future designs and developments. As future work, we intend to explore the metrics and knowledge developed in this study to improve actual IR services and to design different strategies for feature quality enhancement. Another interesting vidual and social behavior aspects ( Santos-Neto et al., 2010 ).
 Acknowledgements
This work is partially supported by the INCT-Web (MCT/CNPq Grant 57.3871/2008-6), and by the authors individual grants and scholarships from CNPq, FAPEMIG, and CAPES. Flavio Figueiredo and Jussara Almeida were also sponsored by Universo Online (UOL), through its UOL Bolsa Pesquisa program, process number 20090215103600.
 References
