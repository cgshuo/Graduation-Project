
In the past 10 years, the general objective of text summarization has been refined into more specific tasks . Such summarization tasks include: (i) Generic Multi Document Summari za tion: aims at summarizing a cluster of topically related document s, such as the top r esults of a search engine query; (ii) in Update Summarization, a set of documents is summarized while assuming the user has already read a summary of earlier doc-uments on the same top ic; (iii) in Query -Focused Summarization, the summary of a documents set is produced to convey an informative answer in the context of a specific query . The importance of these specialized tasks is that they help us dis-t inguish criteria that lead to the selection of con-tent in a summary: centrality, novelty, relevance, and techniques to avoid redundancy.

We present in this paper a variant summariza-tion task which combines the two aspects of up-date and query -focused summar ization. The task is related to exploratory search ( Marchionini , 2006 ) . In contrast to classical information seek-ing, in exploratory search, the user is uncertain about the information available, and aims at learning and understanding a new topic ( White a nd Roth , 2009 ) . In typical exploratory search behavior , a user post s a series of queries , and based on information gathered at each step, de-cides how to further explore a set of documents. The metaphor of berrypicking introduced in ( Bates , 1989 ) captures this interactive process. At each step , the user may zoom in to a more specific information need, zoom out to a more general que ry , or pan sideways , in order to inves-tigate a new aspect of the topic . 
We define Query -Chain Focused Summariza-tion as follows : for each query in an exploratory search session , we aim to extract a summary that answers the information need of the user , in a manner similar to Query -Focused Summariza-tion , while not repeating information already provided in previous steps , in a manner similar to Update Summarization . In contrast to query -focused summarization, the context of a sum-mary is not a single query, but the set of queries that led to the current step, their result sets and the corresponding summaries.
 We have constr ucted a novel data set of Query -Sets with matchin g manual summarizations in the consumer health domain (Cline and Haynes , 2001) . Q ueries are extracted from PubMed search logs (Dogan et al . , 2009) . We have ana-lyzed this manual dataset and confirm that sum-maries written in the context of berry -picking are markedly different from those written for similar queries on the same document set, but without the query -chain context.

We have adapted well -known multi -document algorithms to the task , and present bas eline algo-rithms based on LexRank ( Erkan and Radev , 2004 ) , KLSum and Topic Sum ( Haghighi and Vanderwende , 2009 ) . We introduce a new algo-rithm to address the task of Query -Chain Fo-cused Summarization, based on a new LDA topic model variant, and present an evaluation which demonstrates it improves on these baselines.
The paper is structured as follows. Section 2 formu lates the task of Query -Chain Focused Summarization. Sect ion 3 reviews related work. In S ection 4 , we describe the data collection pro-cess and the resulting dataset. We then present our algorithm , as well as the baseline algorithms use d for evaluation. We conclude with evalua-tion and discussion.
In this work, we focus on the zoom in aspect of the exploratory search process described above. We formulate the Query -Chain Focused Summarization (QCFS) task as follows:
Given an ordered chain of queries Q and a set of documents D , for each query Q q mary S i is generated from D answering the assumption that the user has already read the summaries S i -1 for queries
A typical example of query chain in the con-sumer health domain we investigate includes the following 3 successive queries: ( Causes of asth-ma, Asthma and Allergy, Asthma and Mold Al-lergy ). We consider a single set of documents relevant to the domain of Asthma as the refer-ence set D . The QCFS task consists of generat-ing one summary of D as an answer to each que-ry, so that the successive answers do not repeat information already provided in a previous an-swer. W e first review the closely related tasks of Update Summarization and Query -Focused Summarization. We also review key summariza-tion algorithms that we have selected as baseline and ad apted to the QC FS task .

Update Summarization focuses on identifying new information relative to a previous body of information, modeled as a set of documents. It has been introduced in shared tasks in DUC 2007 and TAC 2008. This task consists of producing a multi -document summary for a document set on a specific topic, and then a multi -document summary for a different set of articles on the same topic published at later dates. This task helps us understand how update summaries iden-tified and focused on new information while re-ducing redundancy compared to the original summaries. 
The TAC 2008 dataset includes 48 sets of 20 documents, each cluster split in two subsets of 10 documents (called A and B). Subset B docu-ments were more recent. Original summaries w ere generated for the A subsets and update summaries were then produced for the B subsets. Human summaries and candidate systems are evaluated using the Pyramid method ( Nenkova and Passonneau , 2004 ) . For automatic evaluation, ROUGE ( Lin , 2004) variants hav e been pro-posed ( Conroy et al . , 2011 ) . In contrast to this setup, QCFS distinguishes the subsets of docu-ments considered at each step of the process by facets of the underlying topic, and not by chro-nology. In addition, the document subsets are not identified as part of the task in QCFS (as op-posed to the explicit split in A and B subsets in Update Summarization).

Most systems working on Update Summariza-tion have focused o n removing redundancy. Du-alSum ( Delort and Alfonseca , 2012 ) is notable in attem pting to directly model novelty using a spe-cialized top ic -model to distinguish words ex-pressing background information and those in-troducing new information in each document.

In Query -Focused Summarization (QFS) , the task consists of identifying informatio n in a doc-ument set that is most relevant to a given query. This differs from generic summarization, where one attempts to identif y central information. Q FS helps us d i stinguish models of relevance and centrality. Unfortunately, detailed analysis of the datasets produced for QFS indicates that these two notions are not strong ly distinguished in practice: ( Gupta et al . , 2007 ) observed that in QFS datasets, up to 57% of the words in the doc-ument sets we re closely related to the query (through simple query expansion). They note that as a consequence, a generic summarizer forms a strong baseline for such biased QFS tasks.

We address this limitation of existing QFS da-tasets in our definition of QCFS: we identify a chain of at least 3 related queries which foc us on different facets of the same central topic and re-quire the generation of distinct summaries for each query, with little repetition across the steps.
A specific evaluation aspect of QFS measures responsiveness (how well the summary answers the specifi c query). QFS must rely on Infor-mation Retrieval techniques to overcome the scarceness of the query to establish relevance. As evidenced s ince ( Dau me and Marcu , 2006 ) , Bayesian techniques have proven effective at this task: we construct a latent topic mo del on the basis of the document set and the query. This topic model effectively serves as a query expan-sion mechanism, which helps assess the rele-vance of indi vidual sentences to the original que-ry.

In recent years, three major techniques have emerged to perform multi -document summariza-tion: graph -based methods such as LexR ank (Er-kan and Radev , 2004) for multi document sum-marization and Biased -LexRank ( Otterba cher et al . , 2008 ) for query focused summa rization , lan-guage model methods such as KLSum ( Haghighi and Vanderwende , 2009 ) and variants of KLSum based o n topic models such as BayesSum ( Dau-me and Marcu , 2006) and TopicSum (Haghighi and Vanderwende , 2009) .

LexRank is a stochastic graph -based method for computing the relative importance of textual units in a natural text. The LexRank algorithm builds a weighted graph  X  = (  X  ,  X  ) where each vertex in  X  is a linguistic unit (in our case sen-tences) and each weighted ed ge in  X  is a measure of similarity between the nodes. In our imple-mentation, we model similarity by computing the cosine distance between the  X  X  X   X   X  X  X  X  vectors representing each node. After the graph is gener-ated, the PageRank algorithm ( Page et al . , 1999 ) is used to determine the most central linguistic units in the graph. To generate a summary we use the  X  most central lexical units, until the length of the target summary is reached. This method has no explicit control to avoid redun-dancy among the selected sentences, and the original algorithm does not address up date or query -focused variants . Biased -LexRank ( Otter-bacher et al . , 2008) makes LexRank sensitive to the query by introducing a prior belief about the ranking of the nodes in the graph, whic h reflects the similarity of sentences to the query. Pag-eRank spreads the query similarity of a vertex to its close neighbors, so that we rank higher sen-tences that are similar to other sentences which are similar to the query. As a result, Biased -LexRank overcomes the lexical sparseness of the query and obtained state of the art results on the DUC 2005 dataset.

KLSum adopts a language model approach to compute relevance: the documents in the input set are modeled as a distribution over words (the original algorithm uses a unigram distribution over the bag of words in documents D ). KLSum is a sentence extraction algorithm: it searches for a subset of the sentences in D with a unigram distribution as similar as possible to that of the overall c ollection D, but with a limited length. The algorithm uses Kullback -Lieber (KL) diver-gence  X  X  X  (  X  | |  X  ) =  X  log  X  (  X  (  X  ) pute the similarity of the distributions. It searches performed in a gre edy manner , adding s entences one by one to S until the length L is reached, and choosing the best sentence as measured by KL -divergence at each step. The original method has no update or query focusing capability, but as a general modeling framework it is easy to adapt to a wide range of specific tasks.

TopicSum uses an LDA -like topic model ( Blei et al . 2003 ) to classify words from a number of document sets (each set discussing a different topic) as either general non -content words, topic specific words and document specific word (this category refers to words that are specific to the writer and not shared acro ss the document set). After the words are classified, the algorithm uses a KLSum variant to find the summary that best matches the unigram distribution of topic specif-ic words. This method improves the results of KLSum but it also h as no update summary or query answering capabilities. 
We now describe how we have constructed a dataset to evaluate QCFS algorithms , which we are publishing freely . We selected to build our dataset in the Consumer Health domain, a popu-lar domain in the web ( Cline and Haynes 2001 ) providing medical information at various levels of complexity , ranging from layman and up to expert information , because consumer health il-lustrates the need for exploratory search. The PubMed repository, while primarily serving the academic community , is also use d by laymen to ask health related questions. The PubMed que-ry logs ( Dogan et al. , 2009 ) provide user quer ies with timestamps and anonymized user identifica-tion. They are publically ava ilable and include over 600K queries per day. In this dataset, Dogan and Murray found that query reformulation (typ-ical of exploratory search) is quite frequent: " In our dataset, 47% of all queries are followed by a new subsequent query. These users did no t select any abstract or full text views from the result set. We make an operational assumption that these users X  intent was to modify their search by re-formulating their query. " We used these logs to extract laymen queries relating to four topics: Asthma , Lung Cancer , Obesity and Alzheimer X  X  disease . We e xtracted a single day query log. From these , we extracted sessions which con-tained the terms  X  A st hma  X ,  X  Lung C ancer  X  ,  X  O besity  X  or  X  A lzheimer  X  . Sessions containing search tags (such as  X  X Author] X ) were removed to reduce the number of academic searches. The sessions were then manually examined and used to create zoom -in query chains of length 3 at most . The queries appear below:
We asked medical expe rts to construct f our docu ment collections from well -known and reli-able consumer health websites relating to the four subjects (Wikipedia, WebMD, and the NHS), so that they would provide general infor-mation relevant to the queries.

We then asked medical students to manually produce summaries of these four document col-lections for each query -chain. The medical stu-dents were instruc ted construct a text of up to 250 words that provides a good answer to each query in the chain. For each query in a chain the summarizers should assume that the person read-ing t he summaries is familiar with the previous summaries in the chain so they shoul d avoid re-dundancy.

Three distinct human summaries were pro-duced for each chain. For each chain, one sum-mary was produced for each of the three queries , where the person producing the summary was not shown the next steps in the chain when an-swering the fi rst query.

To simulate the exploratory search of the user we provided the annotators with a Solr 1 query interface for each document collection. The in-terface allowed querying the document set, read-ing the documents and choosing sentences which answer the query. After choosing the sentences , annotator s can copy and edit the resulting sum-mary in order t o cre ate an answer of up to 250 words. After processing the first two query chain summaries , the annota tors hel d a post -hoc dis-cussion about the different summaries in order to adjust their conception of the task.

The statistics on the collected dataset appear in the T ables below:
A key aspect of the dataset is that the same documents are summarized for each step of the chains, and we expect the summaries for each s tep to be different (that is, each answer is indeed responsive to the specific query it addresses). In addition, each answer is produced in the context of the previous steps, and only provides upda ted information with respect to previous answers. To ensure that the dataset indeed reflects these two aspects (responsiveness and freshness), we em-pirically verified that summar ies created for ad-vanced queries are different from the summaries created for the same queries by summarizers who did not see the previous summaries in the chain. We asked from additional annotators to create manual summaries of advanced queries from the query chain without ever seeing the queries from the beginning of the chain . Fo r example, given the chain ( asthma causes  X  asthma allergy  X  asthma mold allergy ), we asked summarizers to produce an answer for the second query ( asthma allergy ) without seeing th e first step , on the same input documents.
 We used ROUGE to perform this validation: ROUGE compares a summary with a set of ref-erence summaries and source documents. We first computed the mean ROUGE score of the second query summaries. The mean ROUGE score is the mean score of each manual summary vs . all other summaries about the same query. We got (  X  1 = 0 . 52 ,  X  2 = 0 . 22 ,  X  X  X  4 = 0 . 13 ) . The mean ROUGE scores of the same second query summaries by people who did no t see the previ-ous query were markedly lower: (  X  1 = 0 . 40 ,  X  2 = 0 . 22 ,  X  X  X  4 = 0 . 01 ). We only verified t he asthma dataset in this man ner. The results, except for the R2 test, had statistical ly significant difference with 95% confidence interval. All the data, code and an annotated example can be found in our site 2 .

We show below slightly shortened manual summaries created in answer to the following query chain: " obesity screening -&gt; body mass index -&gt; BMI Validity ". We manually annotated the summaries to highlight how the exploration process develops and indicate some of the infor-mation retrieval challenge s faced when address-ing the task: underlined terms are directly rele-vant to the current query; bold italic terms have likely triggered the curiosity of the reader and led to asking the next query. In this example, a user asks about obesity screening, and reformulates his query using the technical term of BMI, which is prevalent in the produced answer.
 adapted the previously mentioned methods to the QCFS task , thus producing 3 strong baselines. We then desc ribe our new algorithm for QCFS . 5.1 Focused KLSum a simple document selection step in the algo-rithm. The method is: given a query step  X  , we first select a focused subset of documents from  X  ,  X  (  X  ) . We then apply the usual KLSum algo-rithm over  X  (  X  ) . This approach does not ma ke any effort to reduce redundancy from step to step in the query chain. In our implementation, we compute  X  (  X  ) by selecting the top -10 documents in  X  ranked by  X  X  X   X   X  X  X  X  scores to the query , as implemented in SolR. 5.2 KL -Chain -Update cated variation of KLSum that answers a query chain (instead a single query). When construct-ing a summary , we update the unigram distribu-tion of the constructed summary so that it in-cludes a smoothed distribution of the previous su mmaries in order to eliminate redundancy be-tween the successive steps in the chain. For ex-ample , when we summarize the documents that were retrieved as a result to the first query , we calculate the unigram distribution in the same manner as we did in Focused KLSum; but for the second query , we calculate the unigram distribu-tion as if all the sentences we selected for the previous summary were selected for the current query too , with a damping factor. In this variant, the Unigram Distribution estimate o f w ord X is computed as : 5.3 ChainSum the QC FS task. We developed a novel Topic Model to identify words that are associated to the current query and not shared with the previous que ries . We achieved this with the following model . For each query in a chain, we consider the documents  X   X  which are "good answers" to the query; and  X   X  which are the documents used to answer the previous steps of the chain. We assume in this model that these document subsets are observable (in our implementation, we select these subsets by ranking the do cuments for the query based on TFxIDF similarity). 1.  X  is the general words topic , it i s intended 2.  X   X  is the document specific topic ; it repre-3.  X  is the new content topic , which should 4.  X  captures old content from  X   X  ,  X   X  is 5.  X  capture s redundant information between 6. For documents from  X   X  we draw from the 7. For documents from  X   X  , we draw from the 8. For documents in  X   X  (  X   X   X   X   X  ) we draw 
The plate diagram of this generative model is shown in Fig. 3 .

We implemented inference over this topic model using Gibbs Samp ling (we distribute the code of the sampler together with our datas et). After the topic model is applied to the current query, we apply KLSum only on words that are assigned to the new content topic . Fig. 4 summa-rizes the algorithm data flow.

When running this topic model on our dataset, we observ e:  X   X  mean size was 978 words and 375 unique words.  X   X  mean size was 1374 words and 436 unique words.  X   X  and  X   X  mean on average 159 words. These figures show there is high lexical overlap between the summaries answering query qi and qi+1 and highlight the need to distingu ish new and previously exposed content.

In the ChainSum model, the topic R aims at modeling redundant information between the previous summaries and the new summary. We intend in the future to exploit this information to construct a contrastive model of c ontent selec-tion. In the current version, R does not play an active role in content selection. We, therefore, tested a variant of ChainSum that did not in-clude  X   X  and obtained results extremely similar to the full model, which we report below. 5.4 Adapted LexRank where nodes repre sent the sentences from the text and weighted edges repre sent the cosine -distance of each sentence's TFx IDF vec-tors. After creating the graph , PageRank is ru n to rank sentences. We adapted LexRank to QCFS in two main ways: w e extend the sentence represen-tation scheme to capture semantic information and refine the model of sentences similarity so that it captures query answering instead of cen-trality. We tagged each sentence with Wikipe dia terms using the Illinois Wikifier ( Ratinov et al ., 2011 ) and with UMLS ( Bodenreider , 2004 ) terms using HealthTermFinder ( Lipsky -Gorman and Elhadad , 2011 ) . UMLS is a rich medical on-tology, which is appropriate to the consumer health domain.
 the sum of Lexical Semantic Similarity (LSS) function s ( Li et al ., 2007 ) on lexical terms , Wik-ipedia terms and UMLS terms: Where : Instead of using the cosine distance, in order to incorporate advanced word/term similarity func-tions. For lexical terms , we used the identity function, for Wikipedia term we used Wikiminer ( Milne , 2007 ), and for UMLS we used Ted Pedersen UMLS similarity function ( McInnes et al . , 2009) . Finally , instead o f PageRank , we used Sim Rank ( Haveliwala , 2002 ) to identify the nodes most simi lar to the query node and not only the central sentences in the graph . 6.1 Evaluation Dataset We worked on the dataset we created for QCFS and added semantic tags: 10% of the to-kens had Wikipedia annotations and 33% had a UMLS annotation. 6.2 Results scores of (r1 = 0.281 , r2 = 0.061 , su4 = 0.100 ), KL -Chain -Update ( r1 = 0.424 , r2 = 0.149 , su4 = 0. 193 ), ChainSum (r1 = 0. 44988 , r2 = 0. 1587 , su4 = 0. 20594 ), ChainSum with t Simplified T opic model (r1 = 0. 44992, r2 = 0.15814 , su4 = 0.20 507 ) and for Modified -LexRank (r1 = 0.4 44 , r2 = 0.1 51 , su4 = 0. 201 ). All of the modified ver-sions of ou r algorithm performed better tha n Fo-cused KLSum with more than 95% confidence. lored for the needs of exp loratory search system . This task combines elements of question answer-ing by sentence extraction with those of update summarization.
 inition of a new summarization task that corre-sponds to exploratory search behavior and the contribution of a novel data set containing human summaries. This dataset is annotated with Wik-ipedia and UMLS terms for o ver 30% of the to-kens. We controlled that the summaries cover only part of the input document sets (and are, therefore, properly focused) and sensitive to the position of the queries in the chain.
 baseline method s based on KL -Sum show a sig-nificant improvement when penalizing redun-dancy with the previous summarization.
 chains, other user actions such as  X  zoom out X  or  X  X witch topic X  were left to future work. This pa-per concentrated on  X  X oom in X  query chains, oth-er user actions such as  X  X oom out X  or  X  X witch topic X  were left to future work. The task remains extremely challenging, and we hope the dataset avai lability will allow further research to refine our understanding of topic -sensitive summariza-tion and redundancy control.
 task -specific evaluation metric that exploits the structure of the chains to better assess relevance, redundancy and contrast.
 Acknowledgments ter of Science (Grant #3 -8705) and by the Lynn and William Frankel Ce nter for Computer Sci-ences, Ben -Gurion University. We thank the reviewers for extremely h elpful advice.

