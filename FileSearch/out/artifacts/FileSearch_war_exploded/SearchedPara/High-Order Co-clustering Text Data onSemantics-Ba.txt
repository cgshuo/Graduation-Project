 Clustering is an indispensable text mining technique such as query search results organization, automatic abstracting, and etc.. The goal of text clustering is to group documents with similar themes together while separate those with differ-ent topics. In order to achieve this goal, the fundamental problem is creating a reasonable and information-abundant representation model. Traditional doc-ument representation model in text clus tering is term-based vector space model (term VSM) [1]. However, term VSM only covers the syntactic information of a document discarding the semantic information because it assumes all terms independent.

In order to consider the semantic information, latent topic models (e.g., LDA [3] and pLSI [10]) were recently propose d to identify the topics, and terms be-longing to one topic are taken to be relevant to each other. This kind of models, to some extent, makes up for the shortage of term VSM, but cannot discover as much semantic information as described in text data only by analyzing syntactic information via statistic methods. Another way to overcome the weakness of the term VSM is incorporating background knowledge (ontology or encyclopedia) into text representation model [11,2,12,19,14,13,15] as concept features to build VSM model.

Even though the background knowledge was used in the literatures [11,12,19,14,13,15] to improve document clustering performance, their final re-sults only contain the document clusters rather than word clusters and even concept clusters. Actually, clustering words is also very important in text clus-tering besides dividing documents into different groups. Such clustering can be called co-clustering or bi-clustering. Actually, bi-clustering was very popular in biological data analysis [4]. For text mining, Dhillon et al.[6] gave an information theoretic co-clustering to group documents and words simultaneously. However, this simple co-clustering can not deal with complicated data structure, say, het-erogeneous data. Gao et al.[8] designed an extended co-clustering with consistent information theory, which can handle heterogeneous data, e.g., documents, terms and document labels. Recently, Dai et al.[ 5] and Wang et al.[20] applied this idea to self taught learning to do document classification with the aid of auxiliary data.

In this paper, we make use of the background knowledge (Wikipedia) to rep-resent text corpus via a high-order structure. The structure contains three parts, terms, documents and Wikipedia concepts. The Wikipedia concepts will be used to represent a document if they are relat ed to the terms appearing in the given document. Meanwhile, the high-order structure is combined with the high-order co-clustering algorithm [8,9,20] to simultaneously obtain the term groups, con-cept groups and document groups (We nam ed this kind of co-clustering proce-dure as HOCOClu). Experimental on real text data have shown that HOCOClu can get better document clustering perfo rmance than co-clustering on document-term, document-concept and even docume nt-(term+concept) respectively. Fur-thermore, the term clusters and concept c lusters make sense by investigating the text data.

The rest of the paper is organized as follo ws: Section 2 introduces the proposed high-order structure by utilizing Wikiped ia concepts. In Section 3, the high-order co-clustering (HOCOClu) is firstly combined with the high-order structure to simultaneously find the term group, concept group and document group. The experimental results will be listed and discussed in Section 4. Finally, we conclude the paper in Section 5. In this section, we will present a high-o rder structure to represent document set, as shown in Fig.1. The structure consists of three types of objects: C = { c T = t 1 ,t 2 ,  X  X  X  ,t m with m terms. In other words, the documents are represented with both terms and concepts. In this structure, the relationship between docu-ment and term is measured by the term frequency-inverse document frequency ( tf  X  idf ).
The key step to build the high-order structure is to select the concepts for each document and calculate the contribution of concepts to the document. Wikipedia is, here, used as the concept resources. G iven a document, if its term is an anchor in the corresponding Wikipedia articles (in Wikipedia, the title of each article is a concept) [16], this Wikipedia con cept is taken to be related to this term. Usually, there are almost twenty Wikipedia concepts related to one term [13]. In this paper, following [17], the most obvious sense (concept) for each term is kept, while the others are removed in ord er to improve the processing efficiency by discarding insignificant concepts.

Fig.2 gives the framework on how to build the relationship between Wikipedia concept and document. According to t he obvious measure, one term can be connected with one Wikipedia concep t or not if the term is not an anchor, while one concept can be semantically related to more than one term as shown in Fig.2. Once obtaining the related concepts for each document, we adopt a context-based method [17] to evaluate the semantic relatedness between term and its relevant Wikipedia concept in one document, i.e. R ij in Fig.2. and where T is the term set of the j th document d j , t l is a term in d j except for t , i.e., t l  X  T and t l = t . c l is the Wikipedia concept related to term t l . SIM ( c i ,c l ) indicates the semantic relatedness between two concepts which is computed according to the hyperlink structure of Wikipedia [18]. In Eq.(2), A and B are the sets of all articles that link to concepts c i and c l respectively, and W is the set of all articles in Wikipedia. If A and B do not have any common article, i.e., | A  X  B | =0,weset SIM ( c i ,c k ) = 0. Eq.(2) is based on term occurrences on Wikipedia-pages. Pages that contain both terms indicate relatedness, while pages with only one of the terms suggest the opposite.
Rel ( t, c i | d j ) indicates the semantic relatedness between term t and concept c in document d j according to term t  X  X  context. Higher value of Rel ( t, c i | d j ) shows that concept c i is more semantically related to term t because c i is much more similar to the relevant concepts of other terms in d j (other terms are the context of term t in d j ). Then, the importance of concept c i in document d j can be calculated as follows.
 where w ( t, d j )istheweight( tf idf is used here) of term t in document d j .That is, the importance of concept c i in document d j is the weighted sum of the related terms X  weight based on the semantic relatedness between c i and its related terms.
So far, a document is represented with a high-order structure which effi-ciently integrates the syntactic information and semantic information (provided by Wikipedia) and contains three types o f objects, term, document and concept. Next, we will make use of this high-order representation structure to simultane-ously find the clusters of documents, terms and concepts. In this section, we will show how to determine tripartite clusters based on the high-order representation structure. Gao et al. [8,9] proposed a kind of co-clustering methods to cluster high-order heterogeneous data. Among them, CIT [9] and CoCC[5,20] were based on consistency information theory [6] and proven to be more efficient and effective than CBGC [8]. All these three methods are used on the heterogeneous text data with category-document-term format, here category is the document category label. H owever, in real applications, very few documents have category information. In this section, we will show how to ap-ply the consistency information theory on text data represented via high-order structure to identify document clusters, term clusters and concept clusters simul-taneously. Also, we adopted a more effect ive optimizing process [5] to implement this high-order co-clustering, named as HOCOClu. To the best of our knowledge, our work is the first one to use high-order co-clustering to find concepts, docu-mentsandtermsclusterssimultaneously.
The goal of HOCOClu can be taken as finding the tripartite graph of the high-order representation, as shown in Fig.3. With this tripartite graph, we can group objects C , D and T into different clusters denoted as  X  C ,  X  D ,and  X  T respectively, e.g., the data objects are separated by the dotted line. The whole clustering procedure can be implemented via the c onsistency information theory [6] by optimizing the following objective function.
 where  X  is a trade-off parameter to balance the influence between the syntac-tic information and the semantic information. D KL ( p || q ) is the KL-divergence. p ( d j ,c l ) indicates the joint probability between concept c l and document d j and the weight of concept c l in document d j . Similarly, p ( d j ,t i ) indicates the joint probability between term t i and document d j ,where v ( d j ,t i )is tf  X  idf value when calculating p ( d j ,t i ).

In Eq.(4), q 1 ( C, D ) is the distribution between co ncept cluster and document cluster, similarly, q 2 ( T,D ) for term cluster and document cluster, which are formulated as follows. and where  X  T ,  X  D ,and  X  C are the group of terms, document, and concept which are iteratively choose via and Eq.(7-9) are conditions to appropriatel y cluster concepts, terms and documents respectively. Therefore, Eq.(4) can be t aken as a consistent fusion of two pair-wise co-clustering sub-problems ( D -C co-clustering and D -T co-clustering) by sharing the same document variable D and document clustering result  X  D . Such consistent fusion can make the overall partitioning be globally optimal under objective function Eq.(4). Based on the above definition, the best  X  T ,  X  D ,and  X  C can be simultaneously returned by Algorithm 1.
 4.1 Dataset In this section, we would test our proposed highi-order representation structure and HOCOClu co-clustering on real data sets (20Newsgroups and Reuters-21578) with the aid of Wikipedia. Five test sets were created from 20Newsgroups. 20NG-Diff4 with four substatially different classes, and 20NG-Sim4 with four similar classes with 1000 documents in each class. Another three data sets, Binary, Multi5 and Multi10 were created following [6]. Among them, Binary contains two similar classes with 250 documents in each class, Multi5 covers five categories with 100 documents per category, and Multi10 includes ten groups with 50 documents per group. Reuters-21578 consists of short news articles dating back to 1987. We created the R Min20Max200 set following [14] which contains the Reuters categories that have at lest 20 and at most 200 docu-ments and R Top 10 consisting of the largest 10 classes. All these seven data sets in Table 1 only contain the single-label documents. For the multi-label document clustering, we will study in the future.

For each data set, the words were extract ed by preprocessing steps, selecting only alphabetical sequences, stemming them and removing stop words. For data sets 20NG-Diff4, 20NG-Sim4, R Min20Max200 and R Top 10, we removed the infrequent words that appeared in less t hree documents. For data sets Binary, Multi5 and Multi10, we adopted the feature selection method following [6], i.e., selecting the top 2000 words according to the MI score between words and corpus, so that it is more reasonable to compare with the clustering result of the original co-clustering algorithm proposed in [6]. The Wikipedia concepts for these words in each data set were d etermined via the method in Section 2. 4.2 Clustering Results and Discussion Since the category of each document was known in these data sets, we used the external cluster validation method to evaluate the clustering results by calculat-ing the correspondence between the clust ers generated by a clu stering algorithm and the inherent categories of the data set. Table 2 lists the three evaluation func-tions, Entropy [21], FScore [21] and the normalized mutual information ( NMI ) [22], which were used to evaluate clustering results.These functions can be in-terpreted as follows. The smaller the Entropy , the better the clustering perfor-mance. The larger the FScore / NMI , the better the clustering performance.
Next, we will demonstrate the performance of HOCOClu on text data repre-sented via high-order structure by comparing with the original co-clustering on document-term (D-T), document-con cept (D-C) and document-(term+concept) (D-(T+C)) models.
In the experiments, the term clusters an d concept clusters were initialized by k -means clustering algorithm on term-document TFIDF matrix and concept-document TFIDF matrix respectively. Fo r document clusters initialization, in co-clustering of D-T, documen t clusters were obtained by k -means on document-term TFIDF matrix, while, in co-clustering of D-C, document clusters were got by k -means on document-concept TFIDF marix. In HOCOClu, we ensembled the document clustering results of k -means on the above two matrices with the method proposed in [7]. The number of document clusters is set to be the true number of classes. For term and concept, we set the number of term clusters and concept clusters with different values from m/ 10 to m/ 100 ( m is the number of terms or concepts), finally, the best clustering result for each dataset was shown.
For trade-off parameter  X  in Eq.(4), we constructed a series of experiments to show how  X  affects the clustering performance. Fig.4 presents the document clustering FScore curve given by HOCOClu along with changing  X  on three small data sets. From this fig ure, it can be seen that, when  X  decreases, im-plying the weights of the concept (i.e., semantic) information being lower, the performance of HOCOClu declines. On the other hand, when  X  is sufficiently large, i.e.,  X &gt; 1, meaning the weights of the term (i.e., syntactic) information being lower, the performance of HOCOClu also decreases slightly. This indicates that clustering performance benefits from both term and concept information, esp., when treating syntactic information and semantic information equally. In the following experiments, we set the trade-off parameter  X  to be 1, which is the best point in Fig.4.

Table 3 gives the comparison clustering results on seven data sets. Three figures in each cell represent the values of Entropy (En), FScore (F) and NMI respectively. The good clustering resu lts are marked in the bold case. We can see that the proposed HOCOClu on high-order structure can achieve the better clustering performance than all co-clustering results on D-T, D-C and D-(T+C), because HOCOClu efficiently makes us e of the term and concept information than the other three methods.
 Meanwhile, HOCOClu has ability to identify the word and concept clusters. Next, we will take Multi5 data set as an example to show the advantage of HOCOClu.
 Table 4 indicates the confusion matrix of Multi5 obtained by HOCOClu. In our experiments, the document clustering performance was best when the number of word clusters and number of c oncept clusters were set to be 50 (i.e., 2000/40). From Table 4, we can see document cluster C 1 D talks about sci.space , C 2 D about rec.sport.baseball , C 3 D mainly about rec.motorcycles , C 4 D mainly about comp.graphics and C 5 D about talk.politics.mideast .

Table 5 gives the term cluster IDs and concept cluster IDs corresponding to each Multi5 document cluster. In order to check whether the word cluster or con-cept cluster is related to the document cluster (i.e., related to the cluster topic), we list ten words and five concepts (separ ated by space) of each representative word or concept cluster (two word cluste rs and two concept clusters shown here) for Multi5 in Table 6. Obviously, the words or concepts in the representative clusters are semantically related t o the topic of document clusters. In this paper, we presented a semantics-based high-order structure for text rep-resentation with the aid of Wikipedia. The proposed structure contains three types of objects, term, document and co ncept. The contribution of concept to document is calculated based on the se mantic relatedness between concept and its related terms in the current document. This proposed structure takes into account both semantic information and syntactic information. Combining the high-order co-clustering algorithm, the clusters of terms, documents and con-cepts can be simultaneously identified. E xperimental results on real data set have shown that the high-order co-clustering on high-order representation struc-ture outperforms the co-clustering on d ocument-term, document-concept and document-(term+concept) model.

Even though the concept information was used in the proposed method, there is abundant information in Wikipedia, e.g., category information. Such category information is also helpful in text clustering [13]. In the future, we plan to build text representation model by integrating Wikipedia category, so that the rela-tionship between Wikipedia concepts could be taken into account.
 This work was supported by the Fundamental Research Funds for the Central Universities (2010RC029), the National Natural Science Foundation of China (60905028, 90820013, 60875031, 61033013), 973 project (2007CB311002) and the Project-sponsored by SRF for ROCS, SEM.

