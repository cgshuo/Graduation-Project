 This paper presents efficient shared-memory parallel implemen-tations and the first comprehensive experimental study of graph eccentricity estimation algorithms in the literature. The implementa-tions include (1) a simple algorithm based on executing two-pass breadth-first searches from a sample of vertices, (2) algorithms with sub-quadratic worst-case running time for sparse graphs and non-trivial approximation guarantees that execute breadth-first searches from a carefully chosen set of vertices, (3) algorithms based on prob-abilistic counters, and (4) a well-known 2-approximation algorithm that executes one breadth-first search per connected component. Our experiments on large undirected real-world graphs show that the al-gorithm based on two-pass breadth-first searches works surprisingly well, outperforming the other algorithms in terms of running time and/or accuracy by up to orders of magnitude. The high accuracy, efficiency, and parallelism of our best implementation allows the fast generation of eccentricity estimates for large graphs, which are useful in many applications arising in large-scale network analysis. H.2.8 [ Database Management ]: Database Applications X  Data Mining ; D.1.3 [ Programming Techniques ]: Concurrent Program-ming X  Parallel Programming Algorithms, Experimentation, Measurement, Performance The eccentricity of a vertex in a graph is defined to be the largest distance from the vertex to any other reachable vertex. Computing the eccentricities of vertices in a graph is a well-studied problem due to its many applications in the analysis of networks (see, e.g., [49]). For example, the eccentricity of a vertex can be used to compute its eccentricity centrality score, defined to be the inverse of its ec-centricity. This can be used to measure a vertex X  X  accessibility in the graph (i.e.,  X  X entral X  vertices tend to have a higher eccentricity centrality score). Eccentricities have been used to study charac-teristics of routing networks [32, 33] X  X  low average eccentricity indicates that the devices are mostly near each other, whereas a high average eccentricity implies that devices are spread out. A vertex X  X  eccentricity value in a routing network could be indicative of a de-c  X  vice X  X  worst-case response time [46]. Graph eccentricities also find applications in biological networks [38], location analysis [12], and state-transition graphs of finite-state machines that arise in formal hardware verification [35]. The eccentricity distributions of graphs have been analyzed to classify the roles of vertices [28, 48, 46]. In addition, for certain algorithms on matrices, it is desirable to identify a starting vertex with a certain eccentricity criteria (see, e.g., [26]).
The exact eccentricity of every vertex in an unweighted graph can be computed simply by executing a breadth-first search (BFS) starting from each vertex, taking a total of O ( mn ) work. More generally, computing the graph eccentricities can be solved exactly using an all-pairs shortest paths (APSP) algorithm. However, these algorithms inherently require  X ( n 2 ) work (see, e.g., [51, 15, 50] and the references therein), which is prohibitive for large graphs. A natural question to ask is whether there exists sub-quadratic work algorithms for graph eccentricity. There have been several papers describing methods for efficiently approximating all of the eccentric-ities of a graph without resorting to APSP, while most other papers focus on efficiently computing or approximating the diameter of a graph. In this paper, we focus on the problem of approximating all eccentricities as this has more applications than the diameter prob-lem. Due to the large sizes of current graphs, we are also interested in parallel solutions to the problem.

For a connected, undirected graph, it is well-known that a 2-approximation for all vertex eccentricities can be achieved by per-forming a BFS from an arbitrary vertex. As far as we know, the only algorithms with a sub-quadratic worst-case work complex-ity for sparse graphs that provide a better provable approximation guarantee for eccentricities are the ones by Roditty and Vassilevska Williams [40] and Chechik et al. [16]. Roditty and Vassilevska Williams [40] present an algorithm for undirected, unweighted graphs that generates eccentricity estimates  X  e ( v ) for each vertex true eccentricity of v . The algorithm requires O ( m Chechik et al. [16] describe an algorithm for undirected, weighted graphs that generates an estimate  X  e ( v ) for each vertex v , such that (3 / 5) e ( v )  X   X  e ( v )  X  e ( v ) , and requires O (( m log m ) This paper presents the first empirical evaluation of parallel imple-mentations of (variants of) these two algorithms.

As for empirical work on eccentricity computation, Kang et al. [28] present HADI, a parallel MapReduce algorithm based on the Flajolet-Martin counters for estimating the number of distinct ele-ments in a multiset [23]. The algorithm is more general in that it can be used to estimate the neighborhood sizes of vertices [36]. Boldi et al. [10] present an improved algorithm for estimating neighbor-hood sizes using the HyperLogLog counters of Flajolet et al. [24], and their algorithm can be used for eccentricity estimation. This paper will study these two algorithms. We note that the problem of estimating the eccentricity distribution (i.e., estimate the counts of vertices with each eccentricity value) has also been studied [19, 46]; however, the techniques are not applicable to the more general problem of generating eccentricity estimates for all vertices.
In addition to the estimation algorithms described above, we also study a simple method of running two-pass BFS X  X  from a sample of vertices simultaneously, which we refer to as k -BFS. We show that k -BFS generates surprisingly good estimates with high efficiency, and describe bit-level optimizations to improve its performance.
We give shared-memory parallel implementations of all of the algorithms using the recent Ligra graph processing framework [41], and empirically evaluate them on a variety of large-scale undirected, unweighted real-world graphs. As far as we know, this is the first comprehensive comprehensive study comparing all of the different eccentricity algorithms in the literature. For all of the algorithms, we present experiments showing their running time, accuracy, and parallel scalability on a modern multicore machine. For k -BFS and the algorithms based on probabilistic counters [36, 28, 10], we study the running time versus the accuracy by varying the number of BFS X  X  or number of counters used. We show that k -BFS is much more accurate (at most 0.01% error on the real-world graphs for which we could compute the true eccentricities) for a given running time than the algorithms based on probabilistic counters. k -BFS is also much more accurate than the simple 2-approximation algorithm on the real-world graphs, although the 2-approximation algorithm achieves reasonable accuracy and is faster. Compared to the algorithms of Roditty and Vassilevska Williams [40] and Chechik et al. [16], k -BFS achieves comparable accuracy while being orders of magnitude faster. In addition, k -BFS is orders of magnitude faster than two exact parallel eccentricity algorithms that we implement, and achieves up to 38x parallel speedup on a 40-core machine with hyper-threading. Due to the efficiency, parallelism, and high accuracy of our implementation of k -BFS, we are able to use it to quickly generate eccentricity distribution plots for several of the largest publicly available real-world graphs, and produce estimates that are useful for other applications in graph analytics. We denote an unweighted graph by G ( V, E ) , where V is the set of vertices and E is the set of edges in the graph. The number of vertices in a graph is n = | V | , and the number of edges is m = | E | . The vertices are assumed to be indexed from 0 to n  X  1 .

We use d ( v, w ) to refer to the shortest distance from vertex v to vertex w in G ( d ( v, w ) =  X  if unreachable). For a set of vertices S , we define d ( v, S ) to be max s  X  S d ( v, s ) ; in other words, it is the maximum distance from v to any vertex in S . We define v in G , D = max v  X  V e ( v ) to be the diameter of the graph, and R = min v  X  V e ( v ) to be the radius of the graph. We will use  X  e ( v ) to refer to an estimate of the eccentricity of vertex v .
A compare-and-swap (CAS) is an atomic instruction supported on modern multicore machines that takes three arguments X  X  mem-ory location ( loc ), an old value ( oldV ), and a new value ( newV ); if the value stored at loc is equal to oldV it atomically stores newV at loc and returns true , and otherwise it does not modify loc and returns false . An AtomicOR takes two arguments, a location x and a value y , performs a bitwise-OR of the value at location x and the value y , and stores the result in x . It can be implemented using a loop with a CAS until the result of the bitwise-OR is successfully stored at the location or until the result is equal to the value already at location x . Throughout the paper, we use the notation &amp; x to denote the memory location of variable x , and  X  |  X  to denote the bitwise-OR operator.

Algorithms in this paper are analyzed in the work-depth model [7], where work is equal to the number of operations required and depth is equal to the number of time steps required. Concurrent reads and writes are allowed in the model, with which CAS can be simulated. We make the standard assumption that  X (log n ) bits fit in a word. We will use the basic parallel primitives, prefix sum and filter [7]. Prefix sum takes an array X of length n , an associative binary operator  X  , and an identity element  X  such that  X  X  X  x = x for any x , and returns the array (  X  ,  X  X  X  X [0] ,  X  X  X  X [0]  X  X [1] , . . . ,  X  X  X  X [0]  X  X [1]  X  . . .  X  X [ n  X  2]) , as well as the overall sum  X  X  X  X [0]  X  X [1]  X  . . .  X  X [ n  X  1] . Filter takes an array X of length n and a predicate function f , and returns an array X 0 of length n containing the elements in x  X  X such that f ( a ) returns true, in the same order that they appear in X . Filter can be implemented using prefix sum, and both require O ( n ) work and O (log n ) depth [7].
A breadth-first search (BFS) algorithm takes an unweighted graph G ( V, E ) and a source vertex r  X  V , and computes d ( r, v ) for all vertices v reachable from r . A standard parallel implementation of BFS takes O ( m + n ) work and O (min( n, D log n )) depth by proceeding in iterations and in iteration h visiting all vertices at distance h away from r [7].

A well-known 2-approximation algorithm for graph eccentricity works as follows: For each component in the graph, run a BFS from an arbitrary vertex and use the maximum distance found as the eccentricity estimate for all vertices in the component. Using the triangle inequality, the estimates  X  e ( v ) for each vertex v can be shown to satisfy (1 / 2) e ( v )  X   X  e ( v )  X  2 e ( v ) . Our implementations are written using the recent Ligra shared-memory graph processing framework [41]. We choose to use Ligra because it is a high-level framework that allows graph traversal algorithms to be expressed easily, and implementations using Ligra have been shown to outperform other high-level graph processing frameworks. The Ligra framework itself is very lightweight, incur-ring minimal overheads compared to code written without outside of a high-level framework, and the simplicity of the implementations makes them easier to understand and more accessible. Many of the implementation ideas described in this paper can be applied to fully hand-written implementations of the algorithms as well. For the implementations in this paper, we found the performance overhead of the Ligra system itself to be at most 5% (usually much less) compared to equally-optimized code that was fully hand-written. Since the largest publicly-available real-world graphs fit in shared memory, we did not find it necessary to use distributed-memory frameworks. Furthermore, graph algorithms in shared-memory have been shown to be more efficient than distributed-memory on a per-core, per-dollar, and per-joule basis.

Ligra supplies a vertexSubset data structure used for representing a subset of the vertices, and provides two simple functions, one for mapping over vertices and one for mapping over edges. VER -TEX M AP takes as input a vertexSubset U and a function F , and applies F to all vertices in U . 1 F can side-effect data structures associated with the vertices. EDGE M AP takes as input a graph G ( V, E ) , vertexSubset U , boolean update function F , and boolean conditional function C ; it applies F to all edges ( u, v )  X  E such that u  X  U and C ( v ) = true (call this set of edges E a a vertexSubset U 0 containing vertices v such that ( u, v )  X  E F ( u, v ) = true . Again, F can side-effect data structures associ-1: 2: 4: 6: ated with the vertices. The programmer must ensure the parallel correctness of the functions passed to VERTEX M AP and EDGE
A key feature of Ligra that makes it efficient is that it has two implementations of EDGE M AP , a version for sparse input vertexSub-sets that writes data from just the vertices in the input, and a version for dense input vertexSubsets that reads data from all vertices in the graph satisfying the conditional function. Ligra automatically switches between the two versions of EDGE M AP based on the size of the input vertexSubset. This optimization is very beneficial for graph traversal algorithms where the size of the active vertex set changes over time. The idea was first used in BFS by Beamer et al. [5]. Ligra also supports graph compression, which leads to im-provements in space usage as well as performance [43]. We refer the reader to [41, 43] for implementation details of Ligra. Ligra compiles with either Cilk Plus or OpenMP.
 BFS in Ligra. We describe how to implement BFS in Ligra (pseu-docode shown in Figure 1). The distances of all vertices are ini-tialized to  X  , indicating that they have not yet been visited (Line 1). The starting vertex r has its distance set to 0 (Line 7) and is placed on the initial frontier, which is represented as a vertexSubset (Line 8). An EDGE M AP is applied on the frontier vertices until the frontier becomes empty (Lines 9 X 10). The EDGE M AP uses an Update function that visits the unexplored neighbors by updating their distance values atomically using a compare-and-swap (Lines 2 X 3). The Cond function (Lines 4 X 5) simply checks if a vertex has been visited. Newly visited vertices in each iteration are placed on the next frontier. The algorithm terminates when the frontier becomes empty, as this means that all vertices reachable from r have been visited. The implementation can be shown to take O ( m + n ) work and O (min( n, D log n )) depth, matching that of the standard parallel BFS algorithm. Takes and Kosters describe an algorithm for exact eccentricity com-putation, and show that it is faster than APSP in practice [46]. We de-scribe their algorithm for undirected graphs, and refer to it as TK . We assume a connected graph; otherwise, the algorithm is separately run on each connected component. The algorithm is based on repeatedly selecting a vertex, executing a BFS from it to compute its eccentric-ity, and using the result to bound the eccentricity of the remaining vertices with the following property: for all vertices v, w  X  V , max( e ( w )  X  d ( w, v ) , d ( w, v ))  X  e ( v )  X  e ( w ) + d ( w, v ) .
Each vertex v maintains a lower bound e L ( v ) and an upper bound e ( v ) on its eccentricity. A set W of vertices is initialized to contain all vertices. The algorithm proceeds in rounds. In each round, a vertex w  X  W is selected and a BFS is executed from w . Then for all v  X  W , e L ( v ) is updated to be max( e L ( v ) , e ( w )  X  d ( w, v ) , d ( w, v )) , and e U ( v ) is updated to be min( e d ( w, v )) . Afterward, vertices v in W where e L ( v ) = e removed, as their exact eccentricities have been determined. The algorithm terminates when W becomes empty. In the worst case, the overall work is O ( mn ) , however Takes and Kosters show that it is much lower in practice [46]. In each round, the vertex w to execute a BFS from can be selected using a heuristic, and the heuristic shown to work best is to alternate between a vertex with the highest e value and a vertex with the lowest e L ( v ) value [45].

This algorithm can easily be parallelized by executing each BFS in parallel, and performing the updates to the lower/upper bounds in parallel. Selecting the vertex to perform a BFS from can be done with a prefix sum computation over the e U or e L values. Removing vertices from W can be done with a parallel filter. We implement this algorithm in Ligra using the BFS procedure in Figure 1, and use it as a baseline to compare with the eccentricity estimation algorithms. A GPU implementation of a similar algorithm (for diameter computation) is described in [22]. This section describes k-BFS , an eccentricity estimation algorithm that performs two phases of executing multiple BFS X  X  simultane-ously. The algorithm assumes an undirected, unweighted graph. We describe the algorithm for a single connected component; if the graph is not connected, then the algorithm is run on each component, and this will be discussed in more detail at the end of this section.
Conceptually, the algorithm is very simple. Define S to be an initial set of k randomly sampled vertices, and call each of these vertices a source . The algorithm proceeds in two phases. The first phase computes d ( v, S ) for all vertices v  X  V (the maximum distance from v to any vertex in S ). This can be accomplished by performing a BFS from each source vertex, and keeping track of the current level of the BFS. d ( v, S ) is then equal to the highest level of a BFS that visits v . Then define S 0 to be the k vertices with the largest d ( v, S ) values. The second phase of the algorithm computes eccentricity estimates  X  e ( v ) for all v , defined to be the maximum distance from v to a vertex in S  X  S 0 . Computing d ( v, S again be accomplished by performing a BFS from each vertex in S , and keeping track of the levels of each BFS. Then  X  e ( v ) = max( d ( v, S ) , d ( v, S 0 )) . The sources in the second phase are likely to produce good eccentricity estimates as they are likely to be far from many vertices. This algorithm is similar to the double-sweep BFS technique used for diameter estimation [17, 31], except that we execute k BFS X  X  together and also compute eccentricity estimates for the vertices.

A naive implementation of this algorithm simply executes each of the first k BFS X  X  independently in parallel, and then the next k BFS X  X  independently in parallel. Overall, the BFS X  X  take O ( km ) work and O (min( n, D log n )) depth. Finding the k BFS sources for the second phase can be accomplished with a parallel integer sort [39] in O ( n ) work and O (log n ) depth with high probability. work of the algorithm is O ( km ) and depth is O (min( n, D log n )) with high probability. For our implementation, we combine the k BFS X  X  together to improve practical performance, leveraging the fact that there is shared work among the BFS X  X .
 Implementation. We describe our implementation of the first phase of the multiple-BFS algorithm in detail (pseudocode shown in Fig-ure 2). The second phase is similar, except that the BFS source vertices are determined from the first phase instead of being random. The implementation is an extension of the eccentricity estimation algorithm in [41].

For each vertex, we need to keep track of which BFS X  X  have visited it, which is done by keeping one bit per BFS source. In particular, for a word size of w and sample size of k , each vertex maintains d k/w e words, which we refer to as a bit-vector . When word of v  X  X  bit-vector will be set to 1. We will take advantage of bit-level parallelism to set multiple bits together by advancing the frontiers of all BFS X  X  simultaneously. The words in each bit-vector 1: 2: 3: 4: 5: are all initialized to 0, except for the vertices in the sample, each of which have a unique bit set in their bit-vector. To allow our implementation to be as portable across architectures as possible, we use a word size of 64 bits, supported on all modern machines. We note that certain architectures support vector operations on a larger number of bits, and using them may improve the performance of our implementation on these architectures.

The implementation proceeds in iterations, where each iteration advances the search frontier of all k BFS X  X  by one level. The imple-mentation uses two arrays of bit-vectors, Visited and NextVisited (Lines 1 X 2), as well as an array Ecc to keep track of the eccentricity estimates for all vertices (Line 3). Initially, k random vertices are placed on the frontier, represented as a vertexSubset (Line 21). They are initialized on Line 22 with a VERTEX M AP using the Init function (Lines 17 X 19), which sets a unique bit in their bit-vectors in Visited and NextVisited to 1, and their eccentricity estimate to 0.
In each iteration, the implementation applies an EDGE M AP the frontier vertices to visit their unvisited neighbors and advances all BFS searches by one level (Line 25). C true , a function that al-ways returns true , is passed to EDGE M AP because vertices can be visited multiple times, unlike in a single BFS. The Update function (Lines 5 X 13) checks for all j  X  { 0 , . . . , d k/w e X  1 } whether the j  X  X h word of the bit-vector of the frontier vertex s is different from the corresponding word of its neighbor d  X  X  bit-vector in the Visited array (Line 8); if so, this means that at least one BFS has found an unvisited neighbor d , and it performs a bitwise-OR of s  X  X  word with d  X  X  word and passes the result to d using an AtomicOR (Line 9). Atomicity is needed because multiple frontier vertices could visit the same neighbor in parallel. The result is stored in the NextVisited array, which keeps the state of the bit-vector of d for the next itera-tion. This is done to prevent a BFS from visiting vertices more than one hop away in a single iteration (which may happen if d is also on the frontier and processed after s ). In addition, the eccentricity estimate of d is updated to be the current round number, which is equal to the current level of each BFS (Lines 10 X 12). This is done using a read followed by a CAS for performance reasons. If the value was updated to the current round number (the CAS returns true ), then the Update function will return true on Line 13, placing the neighbor on the next frontier.

The EDGE M AP returns a vertexSubset containing the vertices on the next frontier, and a VERTEX M AP is applied to the next frontier (Line 26) to update the vertices X  Visited bit-vectors with the newly computed bit-vectors from the EDGE M AP that are stored in NextVisited. The VERTEX M AP takes as input the Copy function which simply copies the NextVisited entries into the Visited array (Lines 14 X 16).

The code terminates when no new vertices are visited in an itera-tion, which causes EDGE M AP to output an empty frontier. At this point, Ecc [ v ] stores d ( v, S ) where S is the set of initially sampled vertices. The second phase of the algorithm finds S 0 , the k vertices with the largest Ecc values breaking ties arbitrarily (we use a parallel integer sort from [42] for this), places them onto an initial frontier, and proceeds in the same manner as in the first phase. The final result gives estimates  X  e ( v ) = max( d ( v, S ) , d ( v, S
The k -BFS implementation takes advantage of shared work among the different BFS X  X  in several ways. First, when a vertex from multi-ple BFS X  X  whose sources are associated with bits in the same word is on the frontier, only a single word-level operation is needed to visit a shared neighbor that has not yet been visited by any of those BFS X  X , as opposed to needing multiple operations had the BFS X  X  been run separately. Second, even if the sources of multiple BFS X  X  are not associated with bits in the same word, as long as they are in the same cache line, only one cache miss per neighbor is required to load their data, leading to fewer cache misses overall compared to separate BFS X  X . Third, compared to doing separate BFS X  X , in k -BFS vertices are placed on the frontier fewer times, each time doing more work. This leads to fewer edge traversals overall, which is more cache-friendly since each edge traversal typically causes a cache miss.

The work and depth of our k -BFS implementation is bounded by that of the naive implementation, as the worst case corresponds to executing 2 k separate BFS X  X . The space usage is O ((1+ k/ log n ) n ) as one length-k bit-vector is stored per vertex. To reduce the space usage, we can separate the k BFS X  X  into c  X  k groups and run groups of k/c BFS X  X  at a time. Then, the sources for the second phase of the algorithm are the k/c farthest vertices found in the first phase. This reduces the space to O ((1 + k/ ( c log n )) n ) and increases the depth by a factor of c . While k -BFS does not have provable approximation guarantees, we will see that it works very well in practice.
 Multiple Components. To handle graphs with more than a sin-gle connected component, we first run a connected components algorithm to identify all the components and their sizes. This can theoretically be done within the stated work/depth bounds [7]. Our code uses a connected components algorithm by Slota et al. [44], which we implement in Ligra. We run the algorithm on each of the remaining components. We implemented a version that applies the algorithm to each of the components in parallel, as well as a version that processes the components one at a time, and found the latter version to be faster overall due to lower overheads. In this section, we review two BFS-based algorithms for eccentricity estimation with non-trivial approximation guarantees that require o ( mn ) work, and describe how to parallelize them. Roditty and Vassilevska Williams [40] describe an algorithm (which we refer to as RV ) for undirected graphs that returns estimates  X  e ( v ) for each vertex v such that max( R, (2 / 3) e ( v ))  X   X  e ( v )  X  min( D, (3 / 2) e ( v )) with high probability. We assume a single component in the graph; otherwise the algorithm can be run sep-arately for each component. We describe the version of RV for unweighted graphs. For a parameter s =  X ( a random sample S of  X (( n/s ) log n ) =  X ( and computes a BFS for each of these vertices. Following the no-tation of [40], let p S ( v ) be the closest vertex in S to v and N be the s closest vertices to v , including v (breaking ties arbitrar-ily). Let w be the vertex with largest d ( w, p S ( w )) value. RV computes a BFS for w , obtaining N s ( w ) , and computes a BFS for each vertex in N s ( w ) . This gives the exact eccentricity for each vertex in S  X  N s ( w ) . For every vertex v 6 X  S  X  N s ( w ) , define e ( v ) = max(max q  X  S d ( v, q ) , d ( v, w )) and let v the closest vertex to v on the shortest path from v to w . Then  X  e ( v ) = max( e 0 ( v ) , e ( v t )) if d ( v, v t )  X  d ( v max( e 0 ( v ) , min q  X  S e ( q )) otherwise. We refer the reader to [40] for the proof that the  X  e ( v ) values are within the stated accuracy bounds.
The RV algorithm spends O ( m ( n/s ) log n ) = O ( m work to generate the BFS X  X  for S . Computing p S ( v ) for all v can be done with O ( | S | ) comparisons per vertex, for a total of O ( n for all vertices, and requires O ( m ) work. Computing the BFS for all vertices in N s ( w ) requires O ( m  X  e ( v ) for all v takes O ( | S | ) comparisons per vertex, for a total of O ( n Parallelization. The random sample S can be generated in O ( n ) work and O (log n ) depth using a parallel filter. Each BFS can be implemented in O ( m ) work and O (min( n, D log n )) depth. Finding the p S ( v ) values can be done in parallel using prefix sums in O ( n | S | ) work and O (log | S | ) depth [7]. Finding w is done by computing the maximum distance, again using prefix sums, in O ( n ) work and O (log n ) depth. Computing all  X  e ( v ) values can be done in parallel, each one taking O ( | S | ) work and O (log | S | ) depth to compute the maximum and minimum over a set of | S | values. The overall work is O ( m Implementation. We implement the RV algorithm using Ligra. As in k -BFS, we find the connected components of the graph, and run RV for each component. The sample S is generated by picking ( n/s ) log n =  X ( into a packed array in parallel. Each vertex in S maintains a distance array to all other vertices, and fills the array using Ligra X  X  parallel BFS (Figure 1), which also gives its own eccentricity estimate. While all parallel BFS X  X  can execute together, we found it more efficient in practice to execute the parallel BFS X  X  one at a time. To find the vertex w that is farthest from S , each vertex v  X  V computes d ( v, S ) in parallel, and a prefix sum is applied to the results to compute the vertex with maximum d ( v, S ) value.
Then a parallel BFS is executed from w . In addition to computing the distance array for w , we also compute the set N s ( w ) and for each v , the closest vertex v t in N s ( w ) on the path from v to w . The BFS is modified (from Figure 1) to fill an array storing N s ( w ) each time it visits a new vertex until N s ( w ) contains s =  X ( Furthermore, the distance array is modified to store the closest vertex v t in N s ( w ) as well for each vertex outside of N the Update function of BFS is modified to pass the this information from a frontier vertex to a newly visited neighbor.

Next, a parallel BFS is executed from each vertex in N s ( w ) and their exact eccentricities are computed. For each vertex v 6 X  S  X  N ( w ) , we compute e 0 ( v ) = max(max q  X  S d ( v, q ) , d ( v, w )) in parallel. Then we look up d ( v, v t ) , and set  X  e ( v ) = max( e if d ( v, v t )  X  d ( v t , w ) and  X  e ( v ) = max( e 0 ( v ) , min erwise. Note that min q  X  S e ( q ) only needs to be computed once.
The space usage of the implementation is  X ( m the distance arrays of the BFS X  X . As an optimization, we reuse the distance arrays for the BFS X  X  of the vertices in S for the BFS X  X  of the vertices in N s ( w ) , which requires us to compute max q  X  S for all v 6 X  S before executing the BFS X  X  from N s ( w ) . Chechik, Larkin, Roditty, Schoenebeck, Tarjan, and Vassilevska Williams describe a eccentricity estimation algorithm for undirected, weighted graphs, which we refer to as CLRSTV [16]. The algo-rithm returns estimates  X  e ( v ) for each vertex v such that (3 / 5) e ( v )  X   X  e ( v )  X  e ( v ) . CLRSTV is similar to RV, but to obtain the claimed ap-proximation guarantee for the weighted case it differs in four aspects: (1) CLRSTV first applies a transformation to the graph to make it have bounded degree, (2) it sets the parameter s to  X ( and finds the sample S deterministically so that S has a non-empty intersection with N s ( v ) for all v (i.e., a hitting set), (3) it runs a single-source shortest paths algorithm not only from S  X  N but also from vertices one hop away from N s ( w ) (call this set T ), and (4) the estimates  X  e ( v ) for v 6 X  S  X  N s ( w )  X  T are set work of the algorithm is O (( m log m ) 3 / 2 ) , and the proof of the approximation guarantee is discussed in [16].

The original algorithm in [16] is designed for weighted graphs, but because the large real-world graphs that we could obtain for our experiments are unweighted, we simplify the algorithm to give a slightly weaker approximation guarantee for unweighted graphs. The algorithm can be simplified to have the same struc-ture as RV. In particular, no graph transformation is done, we set s =  X ( S  X  N s ( w ) instead of from S  X  N s ( w )  X  T . Now the only differ-ences from RV are whether we find the set S deterministically or via sampling, and how we compute the  X  e ( v ) values. We choose to com-pute S using random sampling as in RV because it is simpler and easily parallelizable. The computation of the  X  e ( v ) values uses the O ( | S | + | N s ( w ) | ) = O ( is O ( m algorithm can be shown to be (3 / 5) e ( v )  X  2  X   X  e ( v )  X  e ( v ) with high probability for undirected, unweighted graphs. 3 Parallelization and Implementation. Using the same paralleliza-tion ideas as we did for RV, we obtain a (modified) CLRSTV al-gorithm with O ( m Our Ligra implementation of CLRSTV is similar to that of RV, except that the step of computing the  X  e ( v ) values uses a different formula, and the BFS from w does not need to compute the closest vertex v t in N s ( w ) for vertices outside of N s ( w ) . This section describes two algorithms for eccentricity estimation that were originally developed for approximating the neighborhood function of a graph, which given a distance h returns the number of pairs of vertices reachable within distance h . The algorithms work by approximating the individual neighborhood function for each vertex v , which gives the number of vertices reachable from v within distance h , using probabilistic counters that estimate the number of distinct elements in a multiset. The following descriptions assume an undirected, unweighted graph. The ANF (approximate neighborhood function) algorithm [36] uses Flajolet-Martin (FM) counters [23] to maintain size estimates, where each counter is a bit-vector B of length L =  X (log n ) . Adding an item works by setting a single bit b  X  X  0 , . . . , L  X  1 } with probabil-ity 2  X  ( b +1) . The ANF algorithm maintains k independent counters per vertex (using multiple counters gives more accurate estimates), which approximate the individual neighborhood function for a given distance h , and proceeds in rounds. Initially h = 0 , and the counters are initialized to represent a single item (the vertex itself) by setting a single bit per counter with the appropriate probability. Each round computes the individual neighborhood estimates for the current h using the appropriate formula (see [36, 23]), increments h , and updates the counters. To update the counters to represent a distance of h + 1 , each vertex computes a bitwise-OR of each of its coun-ters with all of its neighbors corresponding counters, all of which represent the neighborhood size at distance h (see [36] for details).
Kang et al. [28] adapt the ANF algorithm for eccentricity esti-mation by iterating until none of the counters change in a given iteration  X  D . The estimated diameter of the graph is then estimated eccentricity for a vertex v is the largest h such that the neighborhood functions for h and h + 1 give the same value. We note that Kang et al. [28] actually compute the effective eccentricity , which for each vertex gives an estimate of the smallest value h such that at least 90% of the vertices in the graph are within h hops away. Their algorithm is parallelized using MapReduce.
 Implementation. In this paper, we compute estimates of the true eccentricities, and implement the algorithm in Ligra. Since we are interested in estimating the true eccentricities, we do not need to output the individual neighborhood functions for each distance. Instead we only need to keep track of the round in which any of a vertex X  X  counters have last changed, as this is an approximation of the distance to the furthest reachable vertex.
 We implement the algorithm in Ligra, and refer to it as FM-Ecc . The implementation maintains two arrays of length n , each entry storing k FM counters associated with a vertex. The size of each counter is set to 32 bits. The counters are initialized using a TEX M AP , which uses a function that sets a single bit in each counter with the appropriate probability. All vertices are placed onto the ini-tial frontier. An EDGE M AP is used to perform an AtomicOR of the counters of the frontier vertices with their neighbors X  counters, and vertices whose counters have changed in a given round will be active in the following round. The logic of the rest of the implementation is the same as the first phase of k -BFS (Figure 2).

As all vertices can be active in each round and the number of rounds is bounded by D , the overall work of the algorithm is O ( kmD ) and depth is O ( D log n ) (each application of can be shown to take O (log n ) depth). The space usage is O ( kn ) as k counters are stored per vertex; as in k -BFS, this can be reduced to O ( kn/c ) for c  X  k , with the depth increasing by a factor of c . Recently, Boldi et al. [10] describe HyperANF, an improved al-gorithm for approximating the neighborhood function. The high-level idea is similar to ANF, but instead of using the FM counters, they use the more recent HyperLogLog counters [24]. The Hyper-LogLog counters take less space than the FM counters, requiring  X (log log n ) bits as opposed to  X (log n ) bits. Thus they are able to fit more than one counter in a single word and use broadword programming to update multiple counters with a single operation. They use HyperANF to estimate the effective diameter of graphs as well as other distance statistics. The implementation in [10] uses parallelism by splitting the vertices among processors, keeps track of modified counters, and processes only vertices with modified counters when this set is small enough.
 Implementation. As in ANF, the HyperANF algorithm can be modified to not output the individual neighborhood functions when approximating true eccentricities. To perform a fair comparison with other methods, we implement this algorithm in Ligra, and refer to it as LogLog-Ecc . Ligra automatically performs the opti-mization of processing only vertices with modified counters when this set is small enough, as discussed in Section 2. The high-level structure of the algorithm is similar to FM-Ecc, except for how counters are initialized and combined. We implement an atomic version (using CAS) of the broadword programming procedure to perform a bitwise-OR of multiple counters within a single word described in [10]. The implementation uses 64-bit words and fits 10 HyperLogLog counters per word.

The work of the algorithm is O ((1 + k log log n/ log n ) mD ) as O (log n/ log log n ) counters fit in each word, and the depth is O ( D log n ) . The space complexity is O ((1+ k log log n/ log n ) n ) , and again can be reduced to O ((1 + k log log n/ ( c log n )) n ) by increasing the depth by a factor of c . This section experimentally compares the performance and accuracy of our parallel implementations of approximate and exact algorithms for graph eccentricity on undirected, unweighted graphs. Experimental Setup. The experiments are performed on a 40-core (with two-way hyper-threading) machine with 4  X  2 . 4 GHz Intel 10-core E7-8870 Xeon processors (with a 1066MHz bus and 30MB L3 cache), and 256 GB of main memory. All implementations are written using Ligra, and compiled with Cilk Plus for parallelism. Cilk Plus automatically assigns work to available processors using a work-stealing scheduler. Using the scheduler, an implementation with work W and depth D using p processors has an expected running time of W /p + O ( D ) [8]. The code is compiled with g++ version 4.8.0 (which supports Cilk Plus) with the -O2 flag. The results reported are based on a median over multiple trials. Input Graphs. We use a set of undirected, unweighted real-world and synthetic graphs, whose size, diameter, and average eccen-tricity are shown in Table 1. The first 8 graphs are real-world graphs obtained from the Stanford Network Analysis Project ( //snap.stanford.edu/data ). nlpkkt240 is a graph obtained from is a graph of the Twitter network [29]. Yahoo is a web graph ob-datatype=g . The two synthetic graphs are generated using graph generators from the Problem Based Benchmark Suite [42]. randLo-cal is a random graph where every vertex has five edges to neighbors chosen with probability proportional to the difference in the neigh-bor X  X  ID value from the vertex X  X  ID. 3D-grid is a grid graph in 3-dimensional space where every vertex has six edges, each con-necting it to its 2 neighbors in each dimension. We symmetrized the graphs for the experiments, and removed all self and duplicate edges. The number of edges reported in Table 1 is the number of undirected edges, although our implementations store each edge in both directions. We note that all of the graphs fit in main memory, and the Yahoo graph is the largest graph considered in previous work on eccentricity estimation.
 Accuracy Measures. We use two metrics to measure the accuracy of the algorithms. The first is the average relative error , defined to tations always compute  X  e ( v ) = 0 , so in this case we define the contribution of v to the average relative error to be 0). The second measure is the correctness ratio , which is the number of vertices with a correct eccentricity estimate divided by n . While both mea-sures should be minimized, we believe that for many applications, the average relative error more closely indicates the usefulness of the approximation, i.e. estimates  X  X lose X  to the true value are still likely to be useful whereas estimates  X  X ar X  from the true value are unlikely to be useful. We note that all of the algorithms, except for RV and the simple 2-approximation algorithm, never overestimate the eccentricity of a vertex. Accuracy versus Running Time. We ran the implementations of k -BFS, FM-Ecc, and LogLog-Ecc with varying values of k . To demonstrate the importance of the second phase of k -BFS, we also experimented with a variant of k -BFS that only performs a single phase of BFS X  X , which we refer to as k-BFS-1Phase . All of the bit-vectors and counters fit in memory for the values of k used. Each implementation was run multiple times for different values of k using all 40 cores with hyper-threading and plots of the average relative error versus running time are shown in Figure 3 for the subset of graphs in Table 1 that we were able to compute the true eccentricities of.

Overall, k -BFS achieves significantly (up to orders of magnitude) lower error for a given time budget than the other three implemen-tations on real-world graphs. The detailed accuracy and running time data for k -BFS is shown in Table 2, and we see that it is able to achieve less than 10  X  4 (0.01%) error for all of the real-world graphs. In contrast, using about the same amount of time as k -BFS for the largest k we tried, the other three implementations achieve errors ranging from 0.1% to 59%, as shown in Table 3. Overall, k -BFS also outperforms the other three implementations in terms of correctness ratio. On the real-world graphs, k -BFS achieves a correctness ratio of at least 96% and at least 99.9% in most cases (see Table 2), while the other three implementations achieve much lower correctness ratios overall (see Table 3).

Intuitively, k -BFS works very well in practice because the first phase identifies many of the  X  X eriphery X  vertices (vertices on the edge of the component). Since the eccentricity of a vertex is mea-sured by its distance to the component X  X  periphery, the second phase uses these periphery nodes to generate very accurate eccentricity estimates for most of the vertices. For the real-world graphs, k -BFS is much more accurate for a fixed running time than k -BFS-1Phase due to having the second phase.

For the two synthetic graphs (randLocal and 3D-grid), the curves in Figure 3 for k -BFS and k -BFS-1Phase are similar, with k -BFS-1Phase being slightly faster for a given accuracy, and both k -BFS and k -BFS-1Phase outperform FM-Ecc and LogLog-Ecc. k -BFS-1Phase is slightly better than k -BFS here because there are no pe-riphery vertices in these graphs, and so the second phase of k -BFS does not provide much improvement in accuracy.

Overall, k -BFS-1Phase is more accurate for a given running time than FM-Ecc and LogLog-Ecc. FM-Ecc and LogLog-Ecc have similar curves, with LogLog-Ecc being more efficient as it fits multiple counters per word.
 Comparison to 2-Approximation Algorithm. As a baseline, we also compare with the 2-approximation algorithm described in Sec-tion 2 that runs a BFS from an arbitrary vertex in each component in the graph, and we refer to it as Simple-Approx . Our implementation first finds the connected components of the graph, and then picks a random vertex in each component to run a parallel BFS from. The running time and accuracy for Simple-Approx is shown in Ta-ble 3. Not surprisingly, Simple-Approx is faster than all of the other implementations, as it performs just a single BFS per component. Observe that for the real-world graphs, k -BFS achieves up to orders of magnitude lower average relative errors than Simple-Approx. However, k -BFS is 2.2 X 15.8x slower as it performs more BFS X  X .
On the two synthetic graphs, Simple-Approx performs extremely well because the range of eccentricities of the vertices is small (in fact, all vertices have the same eccentricity in 3D-grid, and 99.8% have the same eccentricity in randLocal), and so using the eccentricity of a random vertex as the estimate for the remaining vertices gives high accuracy. Simple-Approx is more accurate than k -BFS-1Phase ( k = 2 15 ) on all of the graphs except com-Youtube and roadNet-CA, and more accurate than FM-Ecc (512 counters) and LogLog-Ecc (5120 counters) on all but the roadNet-CA graph. Observe that the average relative error of Simple-Approx is much lower than the worst-case 2-approximation. Overall, Simple-Approx is reasonably accurate, and can be used over k -BFS if some accuracy can be sacrificed in exchange for lower running time.
 Comparison to RV and CLRSTV. We experiment with RV and CLRSTV, which have non-trivial theoretical guarantees on the es-timates produced. Due to the high running time and space usage of RV and CLRSTV, we were only able to run experiments on four of the graphs. The parallel running time, average relative er-ror, and correctness ratio for the inputs are shown in Table 3. RV and CLRSTV achieve reasonably low average relative errors, with CLRSTV achieving better accuracy overall but with a slightly higher running time. The relative error of the implementations is much lower than the theoretical worst case discussed in Section 4.
Both implementations are more than two orders of magnitude slower than k -BFS for a similar accuracy (for com-Youtube, as-skitter, and wiki-Talk, refer to the running time of k -BFS for k = 2 in Table 2, and for roadNet-CA refer to the running time for k = 2 10 ). The performance agrees with the work bounds for k -BFS, RV, and CLRSTV. In particular, k -BFS requires O ( km ) work, whereas RV and CLRSTV require O ( m constant factors). For k -BFS to achieve a comparable accuracy to RV and CLRSTV, usually k is orders of magnitude faster than RV and CLRSTV in practice while obtaining similar accuracy. Note that RV and CLRSTV can achieve much higher accuracy than k -BFS-1Phase, FM-Ecc, LogLog-Ecc, and Simple-Approx (see Table 3).
 Comparison to Exact Algorithms. We study the performance of two parallel algorithms that compute exact vertex eccentricities X  the TK algorithm discussed in Section 2.2 (run on each component in the graph), and one which runs k -BFS-1Phase for each set of n/k vertices in the graph, which we refer to as k-BFS-Exact . Parallel running times are shown in Table 3 for the three graphs on which the algorithms finished in a reasonable amount of time.

Observe that TK is much faster than k -BFS-Exact for two of the inputs as the number of BFS X  X  is much lower than n . However, for as-skitter, TK is slower than k -BFS-Exact because the number of BFS iterations is only reduced to about 0 . 75 n , and TK has additional overheads compared to k -BFS-Exact. Due to their quadratic work complexities, both TK and k -BFS-Exact are orders of magnitude slower than k -BFS. Compared to RV and CLRSTV, TK is sometimes faster and sometimes slower. The running time of TK highly varies, as the reduction in number of BFS X  X  performed is strongly related to the graph structure.
We note that Takes and Kosters [46] also describe a pruning strat-egy that removes all degree-1 vertices from the graph to reduce the number of BFS X  X  needed to compute eccentricities. While we did not implement this strategy in TK, it can be applied to all of the im-plementations in this paper to possibly improve their performance. Parallelism. The parallel speedups of k -BFS, FM-Ecc, and LogLog-Ecc on 40-cores with hyper-threading for a subset of the graphs are shown in Table 4. We observe that the implementations achieve reasonably good speedup, ranging from 14x to 47x on 40 cores with two-way hyper-threading. The parallel speedup tends to be better for larger graphs, as there is more work to offset the overheads of parallelism. Figure 4 plots the self-relative parallel speedup versus thread count for all of the implementations on several input graphs, showing improvements in speedups as the thread count increases. Sharing work in k -BFS. Observe from Table 2 that the parallel running time of k -BFS increases as we increase k , but usually sub-linearly with k . This is because k -BFS takes advantage of shared work among the different BFS X  X , as described in Section 3. As an illustration of one of the benefits of k -BFS, Figure 5 plots the number of edge traversals of k -BFS in the largest component as a function of k for the com-Youtube graph, compared to the number of edge traversals required by running k separate BFS X  X  in each phase of the algorithm (naive BFS). We see that k -BFS does fewer edge traversals, with the difference being larger for larger k . This improves cache performance since each edge traversal typically corresponds to a cache miss. The same trend was observed for the other inputs as well. Due to the efficiency and high accuracy of k -BFS, we are able to quickly generate eccentricity distributions for some of the largest real-world graphs studied in the literature. Figure 6 plots the eccen-tricity distributions generated using k -BFS with k = 2 6 largest graphs X  X witter, com-Friendster, and Yahoo. Generating the distribution for the largest graph, Yahoo, took only 11 minutes.
For Twitter and com-Friendster, we observe that the diameter is relatively small (23 and 37, respectively, as estimated by k -BFS), while the diameter for Yahoo is large (estimated by k -BFS to be 2919). There is a single peak in the plot for the Twitter graph at an eccentricity of 15, which is close to its average eccentricity of 15.2. For com-Friendster, there are two peaks, the first at 0 caused by the many disconnected vertices (almost half of the vertices in the graph are singletons), and the second at 22 (about 29% of the vertices have this value). There are two obvious peaks in the eccentricity plot for the Yahoo graph at 0 (almost half of the vertices are disconnected) and 1552 (about 30% of the vertices have this value), and another flatter peak at around 800 X 1100. k -BFS estimates the average eccentricity of the Yahoo graph to be 770.9, and excluding the singleton vertices, the average eccentricity is about 1513. The high average eccentricity of the vertices is due to the long  X  X hiskers X  [28] present in the graph. We note that Kang et al. [28] have previously studied the effective eccentricities (the distance at which 90% of the vertices can be reached) of the Yahoo graph using MapReduce. For a connected, undirected graph with diameter D , Aingworth et al. [1] describe an algorithm to generate an estimate  X  D such that (2 / 3) D  X   X  D  X  D in O ( m rithm extends to graphs with non-negative edge weights, generating an estimate  X  D such that (2 / 3) D  X  w max  X   X  D  X  D , where w the maximum edge weight. Roditty and Vassilevska Williams [40] improve the work of the diameter estimation algorithm of [1] to O ( m rithm can be used for eccentricity estimation, which we described in Section 4.1. The diameter approximation of weighted graphs was improved by Chechik et al. [16], who present two algorithms that generate estimates  X  D such that (2 / 3) D  X   X  D  X  D , with the first algorithm requiring O ( m 3 / 2 gorithm requiring O ( mn 2 / 3 log 5 / 3 n ) work. They also describe how to generate an additive n -approximation to the diameter in O ( n 1  X  ( m + n log n )) work. Finally, they present an algorithm for eccentricity estimation, which we described in Section 4.2. There have been several other papers on approximating the graph diam-eter or radius [17, 18, 37, 9, 6] as well as on their exact computa-tion [45, 11, 31, 21, 20]. Diameter estimation has also been studied in the external-memory setting [34, 2, 3] and parallel/distributed setting [28, 10, 27, 14]. Leskovec et al. study the evolution of the diameter of real-world graphs over time [30].

Almeida et al. [4] describe a distributed algorithm for exact eccen-tricity computation, which essentially does a BFS from each vertex, hence requiring O ( nm ) work. Cardoso et al. [13] and Garin et al. [25] describe a distributed algorithm for eccentricity estimation using probabilistic counters, where combining counters uses the minimum operator. The algorithm is equivalent to executing one phase of BFS X  X  from multiple sources and for each vertex, using the maximum distance from a source as its eccentricity estimate.
Related to our k -BFS implementation, Then et al. [47] recently describe an algorithm for executing multiple BFS X  X  using bit-level optimizations. Their algorithm, however, is optimized for the case where BFS X  X  are executed from a large fraction of the vertices in the graph. In contrast, our algorithm is optimized for the case where only a small set of vertices execute BFS X  X . Another difference is that their algorithm obtains parallelism across several multiple-BFS instances, whereas k -BFS has parallelism within a single multiple-BFS instance. This is advantageous when BFS X  X  are executed from a small number of sources, as in eccentricity estimation. We have presented a comprehensive study of parallel algorithms for eccentricity estimation on large-scale undirected real-world graphs. Our study shows that k -BFS achieves high accuracy, efficiency, and parallelism, and we believe that the implementation will be useful in the analysis of large-scale networks. An interesting direction for future work is to prove approximation guarantees for k -BFS (or variants of it). We are also interested in evaluating eccentricity estimation algorithms for directed and/or weighted graphs. Acknowledgments. This work is partially supported by the Na-tional Science Foundation under grant CCF-1314590, and by the Intel Labs Academic Research Office for the Parallel Algorithms for Non-Numeric Computing Program. Thanks to Guy Blelloch, Laxman Dhulipala, and Dan Larkin for helpful discussions.
