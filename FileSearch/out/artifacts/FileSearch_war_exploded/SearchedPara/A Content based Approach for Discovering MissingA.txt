 Although anchor text provides very useful information for web search, a large portion of web pages have few or no incoming hyperlinks (anchors), which is known as the an-chor text sparsity problem . In this paper, we propose a lan-guage modeling based technique for overcoming anchor text sparsity by discovering a web page X  X  plausible missing an-chor text from its similar web pages X  in-link anchor text. We design experiments with two publicly available TREC web corpora (GOV2 and ClueWeb09) to evaluate different approaches for discovering missing anchor text. Experimen-tal results show that our approach can effectively discover plausible missing anchor terms. We then use the web named page finding task in the TREC Terabyte track to explore the utility of missing anchor text information discovered by our approach for helping retrieval. Experimental results show that our approach can statistically significantly improve re-trieval performance, compared with several approaches that only use anchor text aggregated over the web graph. Categories and Subject Descriptors: H.3.3 [ Informa-tion Storage and Retrieval] : Information Search and Retrieval X  Search process,Retrieval models ;H.3.5 [ Informa-tion Storage and Retrieval] :Online Information Services X  Web-based services General Terms: Algorithms, Experimentation Keywords: anchor text, anchor text sparsity, language models, relevance models, content similarity, web search
There are rich dynamic human generated hyperlink struc-tures on the web. Most web pages contain some hyperlinks, referred to as anchors , that point to other pages. Each an-chor consists of a destination URL and a short piece of text, which is called anchor text . Anchors play an important role in helping web users conveniently navigate to their inter-ested web information. Although some anchor text only functions as a navigational shortcut which does not have di-rect semantic relation to the destination URL (e.g., X  X lick Table 1: Summary of in-link statistics on two TREC web corpora used in our study. here X  and  X  X ext X ), many times anchor text provides suc-cinct description of the destination URL X  X  content, e.g.  X  X I-GIR 2010(Geneva, Switzerland) X  is from an anchor linked to http://www.sigir2010.org/ . Anchor text instances are usually reasonable queries that web users may issue to search for the associated URL and have been used to simulate plau-sible web queries relevant to the associated web pages in some web search research [15]. Therefore, anchor text is highly useful for bridging the lexical gap between user is-sued web queries and the relevant web pages. It is arguably the most important piece of evidence used in web ranking functions[14].

However, previous research has shown that the distribu-tion of the number of inlinks on the web follows a power law [1], where a small portion of web pages have a large number of inlinks while most have few or no inlinks. Thus, most web pages do not have in-link associated anchor text, a problem originally referred to as the anchor text sparsity problem by Metzler et al. [14]. This problem presents a major obsta-cle for any web search algorithms that want to use anchor text to improve retrieval effectiveness. Table 1 shows the an-Metzler et al. [14] proposed aggregating , or propagating , an-chor text across the web hyperlink graph so that web pages X  lack of anchor text can be enriched with their linked web pages X  associated anchor text. Table 1 shows that the num-ber of URLs associated with some anchor text (original or propagated) in the two TREC web corpora is significantly increased by using their linked-based anchor text enrichment approach. Nevertheless, in Table 1 we notice that large por-tion of web pages still do not have any associated anchor text after having been enriched. This observation motivated us to consider another possible approach, which utilizes the content similarity between web pages, to alleviate anchor text sparsity. gov2-summary.htm
Specifically, we hypothesize that the anchor text associ-ated with a web page X  X  inlinks typically has close semantic relations to the web page so that web pages that are simi-lar in content may be pointed to by anchors having similar anchor text. Under this assumption, in this paper we pro-pose a language modeling based technique for discovering a web page X  X  plausible missing in-link anchor text by using its most similar web pages X  in-link anchor text. We then test the effectiveness of our approach by using the discovered missing anchor text information for some TREC web search tasks. We find that even on the GOV2 data where a seri-ous anchor text sparsity problem exists as shown in Table 1, our approach can significantly improve retrieval perfor-mance. Our content based approach can be combined with the hyperlink based approach to further reduce anchor text sparsity and benefit web search. Our enriched document and anchor text representations can also be used for many other tasks beyond web search, including estimating better document models and extracting advanced textual features for content match and document classification.

Our work has four chief contributions: 1) although con-tent similarity has been used widely in other applications, we are the first to propose using web content similarity to address the anchor text sparsity problem. 2) We develop a language modeling based technique, which stems from ideas in one effective retrieval technique  X  relevance based lan-guage models [10], to effectively discover plausible missing anchor text information and use it for retrieval. 3) We em-pirically show that our approach performs better than Met-zler et al.  X  X  linked-based approach [14] in terms of discovering plausible missing anchor terms in two standard large TREC web corpora. 4) We show that our approach statistically significantly improves retrieval effectiveness, compared with several approaches that only use aggregated anchor text over the web graph, in the web named page finding task of the TREC Terabyte track [4].

We begin by reviewing related work in  X  2. Next, we describe different approaches of discovering missing anchor text to enrich document representations in  X  3. Then we describe the experimental setup and results of evaluating different approaches for anchor text discovery in  X  4. After that, we present how to use discovered anchor text infor-mation for retrieval in a language modeling approach and report the experimental results in  X  5. We conclude in  X  6.
Metzler et al. [14] first directly addressed the anchor text sparsity problem by using the web hyperlink graph and prop-agating anchor text over the web graph. Our work also ad-dresses the same problem but using a different approach, which is based on the content similarity between web pages. Our approach is similar in nature to other similarity based techniques, such as cluster-based smoothing from the lan-guage modeling framework[8, 9, 11], except we focus on en-riching web documents X  anchor text representation by using their similar documents X  associated anchor text.

Anchor text can be modeled in many different ways. West-erveld et al. [20] and Nallapati et al. [15] model anchor text in the language modeling approach [17] and calculate an associated anchor language model to update the original document model for retrieval. Fujii [6] further considers dif-ferently weighting each line of anchor text associated with the same page thus obtaining a more robust anchor language model. Here, we also adopt the language modeling approach but focus on discovering a plausible associated anchor lan-guage model for web pages with no or few inlinks. Our ap-proach can be easily used together with any language model based retrieval model (e.g., Ogilvie and Callan X  X  model [16]) that takes document structure into account.

Our approach of overcoming anchor text sparsity stems from ideas in the relevance based language models(RMs), proposed by Lavrenko and Croft [10]. Their original work introduces the RMs to find plausible useful terms missing in the original query for query expansion. Here we adapt the RMs to compute a web content dependent associated anchor language model for discovering missing anchor terms and using anchor text for retrieval. Thus, our approach, al-though similar in spirit to, differs from document expansion [18] and graph-based document smoothing[13].
We now describe three different approaches for discover-ing plausible missing anchor text for web pages with few or no inlinks. The goal of each is to produce a ranked list of plausible anchor text terms for a page.
To overcome anchor text sparsity, Metzler et al. [14] origi-nally proposed to augment web pages with auxiliary anchor text (denoted as  X   X  X  X  X  ) that is derived by aggregating anchor text over the web graph. We first briefly review the proce-dure they have used to build  X   X  X  X  X  , which is very important for our discussions and comparisons in this research. Given a web page  X  0  X  X  URL  X   X  pages  X   X  X  X  (  X  0 ), within the same site (domain), that link to  X  the procedure collects all anchor text  X  from pages (denoted as  X   X  X  X  X  (  X  0 )) that are linked to any page in  X   X  X  X  (  X  outside the site. The anchor text set  X  is known as external anchor text and is used as  X   X  X  X  X  for  X   X 
Figure 1 illustrates the procedure by using a real-world example from the TREC GOV2 collection. We collect the auxiliary anchor text  X   X  X  X  X  for the page  X  0 .  X  0  X  X  original anchor text (denoted as  X   X  X  X  X  X  X  ), which comes from all pages (denoted as  X   X  X  X  X  X  X  (  X  0 )) that are directly linked to  X  outside the site, consists of lines including X  X ptima National Wildlife Refuge X  and  X  X ptima NWR X .  X  0  X  X   X   X  X  X  X  consists of lines including X  X klahoma Refuge Websites X  X nd X  X klahoma National Wildlife Refuges X .

Note that the above procedure does not use any anchor text associated with internal inlinks, because internal inlinks are typically generated by the owner of the site for naviga-tional purposes and their associating anchor text tends to be navigational in nature (e.g.,  X  X ome X , X  X ext page X , etc.; re-fer to [14] for more discussions on this issue). We emphasize that in the remainder part of this paper we follow Metzler et al. and do not use the anchor text associated with internal inlinks in any way.

In this paper we are specifically interested in the effective-ness of using  X   X  X  X  X  to serve as a surrogate for possibly missing original anchor text. In other words, we consider how effec-tively we may use  X   X  X  X  X  to discover plausibly missing original anchor text of the URL of the interest so that anchor text sparsity can be effectively reduced. Therefore, we focus on the discovered anchor terms themselves in the  X   X  X  X  X  . We use two typical methods to rank the relative importance of each anchor term  X  . The first method, denoted as AUX-TF , is to use each term  X   X  X  term frequency  X  X  X   X  X  X  X  (  X  ) in the  X 
P text for discovering more anchor text for a web page (  X  in this example). The page  X  0 is a GOV2 web page, The second method, denoted as AUX-TFIDF , is to use each term  X   X  X   X  X  X   X  X  X  X   X   X  X  X  X  (  X  ) score, computed by multiplying  X  X  X   X  X  X  X  (  X  ) with  X   X  X   X  X  X  X  score in the web collection. The qual-ity of the discovered anchor term rank lists produced from these two link based approaches implies the effectiveness of using auxiliary anchor text as a surrogate of missing original anchor text. We will compare these two approaches with our content based approach in  X  4.
Note that in the link based approach, a web page  X  0 still cannot obtain the auxiliary anchor text if it has no internal inlinks or if all pages in its  X   X  X  X  (  X  0 ) have no external anchor text. Indeed, Metzler et al. reported only 38% anchor text sparsity reduction on a web sample with the link based ap-proach[14]. Therefore, we propose a content based approach, which does not have specific link structure requirements on the target web page, to discover its plausible missing an-chor text. Intuitively, our approach assumes that web pages that are similar in content may be described by similar as-sociated anchor text. For example, in Figure 1, the target page  X  0 , which is about Optima national wildlife refuge, is similar in content with the page  X  4 , which is about Buffalo Lake national wildlife refuge. We observe that the anchor term  X  X WR X , which appears in  X  0  X  X  and  X  4  X  X   X   X  X  X  X  X  X  but not in  X  0  X  X   X   X  X  X  X  , can be used to partially describe both  X   X  4 although two pages are concerned about different places.
We consider a language modeling approach to better use document similarity and anchor text information, based on ideas from the relevance-based language models ( RM )[10]. In brief, given a query  X  , RM first calculates the posterior  X  (  X   X   X   X  ) of each document  X   X  in the collection  X  generating the query  X  , then calculates a query dependent language model  X  (  X   X   X  ): where  X  is a word from the vocabulary  X  of  X  . Similarly, given an target page  X  0 , our approach aims to calculate a relevant anchor text language model ( RALM )  X  (  X   X   X  0 ) by: where  X   X  denotes the complete original anchor text that should be associated with  X   X  but may be missing ,  X  de-notes the complete original anchor text space for all pages,  X  (  X   X   X   X  ) is a multinomial distribution over the anchor text vocabulary  X   X  . To compute  X  (  X   X   X   X  0 ) in Equation 2 where  X  0 and  X   X  information may be missing, we view each page  X   X  X  content as its anchor text  X   X   X  X  context and use  X   X   X  X  doc-ument language model  X   X  = {  X  (  X   X   X   X  ) } as  X   X   X  X  contextual model. Then we can calculate a translation model  X  (  X   X   X   X  by using  X  0 and  X   X   X  X  contextual models and use  X  (  X   X  to approximate  X  (  X   X   X   X  0 ). This contextual translation ap-proach is also used in Wang and Zhai X  X  work [19].
When calculating a page  X   X   X  X  document language model {  X  (  X   X   X   X  ) } , we employ Dirichlet smoothing on the maximum likelihood (ML) estimate of observing a word  X  in the page (  X   X  X  (  X   X   X   X  )) with the word X  X  collection probability  X  (  X   X  X  X  ): where  X   X  smoothing parameter (  X  = 2500 in our experiments). Then given two pages  X  0 and  X   X  , we use the Kullback-Leibler divergence (KL)  X  X  X  X  (  X  X  X  X  X  X  ) between their document models  X  0 and  X   X  to measure their similarity and view that as the contextual similarity between the associated anchor text  X  and  X   X  . Then the contextually based translation probability  X  (  X   X   X   X  0 ) is calculated by: This  X  (  X   X   X   X  0 ) is then used to approximate  X  (  X   X   X   X  tion 2 to get: A few transformations of Equation 4 can obtain: which is the likelihood of generating  X  0  X  X  context  X  0 from  X   X  X  context  X   X   X  X  smoothed language model and being nor-malized by  X  0  X  X  context length. This likelihood can be eas-ily obtained by issuing  X  0 as a long query to any language model based search engine. In addition, we use the observed incomplete original anchor text language model  X   X  X  X  X  (  X   X   X  associated with  X   X  to approximate  X  (  X   X   X   X  ) in Equation 5, and let  X   X  X  X  X  (  X   X   X   X  ) = 0 if  X   X  has no  X   X  X  X  X  X  X  . In this way, the RALM  X  (  X   X   X  0 ) can be computed.

In practice, for efficiency the RALM of the target page  X  0 is computed from  X  0  X  X  top- X  most similar pages X   X   X  X  X  X  X  X  (original anchor text) because  X  (  X   X   X   X  0 ) in Equation 4 is very small for the other pages. Due to the anchor text sparsity, we set  X  = 2000 in our experiments. Because some of these similar pages do not have associated  X   X  X  X  X  X  X  , we use another parameter  X  to denote the number of most similar pages whose associated original anchor text is not missing and contributes information in the RALM, and we tune  X  in the experiments. Intuitively, increasing  X  can increase the number of anchor text samples to better estimate RALM but may also introduce more noise when the sample size is large.

The probability  X  (  X   X   X  0 ) of an anchor term  X  in the RALM directly reflects the goodness of the term  X  used as original anchor text for the page  X  0 , thus we use the anchor terms that have the largest probabilities  X  (  X   X   X  0 ) in the RALM to evaluate the effectiveness of our content based approach. Theoretically our approach can associate any web page with some anchor term distribution information if there is some anchor text in the corpus, thus it can further reduce the anchor text sparsity.
The keyword based approaches come from the intuition that important keywords in a web page may be good de-scription terms for the page, thus may be arguably used as anchor text. We use three typical term weighting schemes to identify the keywords and rank the words in a web page X  X  content. The first method, denoted as DOC-TF , uses each word  X   X  X  term frequency  X  X  X   X  weighting. The second method, denoted as DOC-TFIDF , uses each word  X   X  X   X  X  X   X  plying  X  X  X   X  third method, denoted as DOC-OKAPI , uses each word  X   X  X  Okapi BM25 score  X  X  X  25  X  where  X  X  X  X  X  X  X  is the average document length of the pages in the collection. We use the typical setting  X  1 = 2 ,  X  = 0 . 75 in Equation 7 in our experiments.

The top ranked terms in a page  X  0 by three methods are used as the possible missing original anchor terms for  X  0 We will use three keyword based methods as baselines in  X  4.
We now compare the capability of discovering missing an-chor text by different approaches described in  X  3, including two link based approaches (AUX-TF and AUX-TFIDF), our content based approach (RALM), and three keyword based approaches (DOC-TF, DOC-TFIDF and DOC-OKAPI).
We use two publicly available large TREC web collec-tions ( GOV2 and ClueWeb09-T09B ). GOV2 is a stan-dard TREC web collection [4] crawled from government web sites during early 2004. The ClueWeb09 collection is a much larger and more recent web crawl, which contains over 1 bil-lion pages. ClueWeb09-T09B is a subset of ClueWeb09 and contains about 50 million English web pages. Compared with GOV2 crawled only from the gov domain, ClueWeb09-T09B is crawled from the general web thus is a less bi-ased web sample; in another aspect, GOV2 contains rela-tively high quality government web pages thus having less noise than ClueWeb09-T09B. Thus we use both GOV2 and ClueWeb09-T09B in our experiments to show how different approaches perform in web collections that have different dex both collections by removing a standard list of 418 IN-QUERY [2] stopwords and applying Krovetz stemmer. In a separate process, we run Indri Search Engine X  X  harvestlinks utility on the two collections to collect web page inlinks and raw anchor text information where we do not perform stop-ping or stemming.

To evaluate the quality of discovered anchor text for a web page  X  0 , we utilize the original anchor text  X   X  X  X  X  X  X  associated with all inlinks of  X  0 . Specifically, we first hide the page  X   X  X   X   X  X  X  X  X  X  , apply different anchor text discovery approaches on  X  0 , then compare the discovered anchor text with  X  0  X   X  X  X  X  X  X  . This procedure can be run automatically so that we can leverage large volumes of web pages to evaluate the performance of different approaches with no human labeling effort. More specifically, we consider each anchor term in a page  X  0  X  X   X   X  X  X  X  X  X  as a good description term, or a relevant term, for  X  0 while terms not in  X   X  X  X  X  X  X  as non-relevant ones; in this way, we can generate term relevance judgments for  X  0 Then we employ each different approach to discover a ranked list of plausible missing anchor terms for  X  0 and then use the relevant judgments to evaluate the ranked anchor term list. Note that for fair comparison  X  0  X  X   X   X  X  X  X  X  X  is not used in Equation 2 for calculating RALM in our approach. In the experiments, we perform slight stopping on the raw anchor text by removing a short list of 39 stopwords, which includes 25 common stopwords[12, pp.26] and 14 additional anchor or part of URLs  X  it is common that anchor text contains some URL.

We calculate some typical TREC style evaluation mea-surements including Mean Average Precision ( MAP ), Mean Reciprocal Rank( MRR ), Precision at the number of rele-vant terms( R-Prec ), Precision at  X  ( P@  X  ) and also nor-malized discounted cumulative gain ( NDCG ) [7]. In the experiments, we are specifically interested in the quality of top ranked discovered anchor terms; thus, we only use the edu, net, html, htm, click, here, next, home . top-20 terms in the discovered term rank lists by different approaches to calculate the measurements.

Note that web pages that can be used in our evalua-tion procedure need to satisfy two requirements: (1) they need to have some associated  X   X  X  X  X  X  X  and (2) they can collect some auxiliary anchor text from the web graph as described in  X  3.1. Thus, for each of two collections, we randomly sam-ple 150 pages satisfying the two requirements for training and another 150 pages for testing. On both training sets, RALM X  X  parameter  X  = 15 described in  X  3.2 achieves the highest MAPs.
The performance of discovering original anchor text by dif-ferent approaches on the testing set of GOV2 and ClueWeb-09-T09B are shown in Table 2 and Table 3, respectively. The results show that our approach (RALM) can effectively discover missing original anchor terms. On both collections RALM performs statistically significantly better than two link based approaches (AUX-TF and AUX-TFIDF). This indicates that, for discovering a page X  X  missing anchor text, the anchor text associated with the similar pages provides more useful information than that associated with the linked web neighbors. The numbers of discovered relevant anchor terms by different approaches, shown in the last column of two tables, also indicate that only using auxiliary anchor text misses more original anchor text information than our content based approach.
 Another observation is that RALM performs worse on ClueWeb09-T09B and not statistically significantly better on GOV2 than the keyword based approaches. This indi-cates that words having high IR utility like  X  X  X  or  X  X  X   X   X  X  X  X  scores are often good description terms for the page and used by human being as the anchor text. Removing a long list of stopwords from web page content has also helped the keyword based approaches to effectively select good descrip- X  X  X  X  (AUX-TF, DOC-TF) 30.5% 26.0% Table 4: The average percentage  X  X  X  X  (  X ,  X  ) of the terms discovered by the  X  approach appearing in the ones discovered by the  X  approach. tion words from the web content. One plausible reason that RALM performs relatively poorly on ClueWeb09-T09B is that, compared with the high quality GOV2 pages, ClueWeb pages are crawled from the general web, where the inlinks and anchor text may be generated in a more noisy way (e.g. spam), degrading RALM X  X  performance. To better under-stand the performance of different approaches, in Table 5 and Table 6 we show the top-10 words of the anchor term rank lists discovered by different approaches for one evalua-tion web page in GOV2 and ClueWeb09-T09B, respectively.
Although using keyword information can discover some good anchor terms, the content-generated anchor terms do not help bridging the lexical gap between a web page and varied queries that attempt to search the page. Indeed, hu-man generated anchor text is highly useful for reducing the word mismatch problem because the lexical gap between anchor text and queries is relatively small[14]. Here, we do some lexical gap analysis to show that our approach can also discover anchor terms similar in nature to human-generated ones but different from content-generated ones.

For each web page  X  in the testing set, we calculate the percentage  X  X  X  X   X  (  X ,  X  ) of the terms discovered by the  X  ap-proach also appearing in the ones discovered by the  X  ap-proach, then compute the average percent  X  X  X  X  (  X ,  X  ) with all the pages. We use the outputs from the keyword based DOC-TF, the link based AUX-TF, and the RALM in this analysis. Table 4 shows three average percentages  X  X  X  X  (  X ,  X  ) which we have specific interest in. We observe that AUX-TF X  X  discovered terms have much higher average per query overlap ratio with RALM X  X  than with DOC-TF X  X . More-over, RALM X  X  discovered anchor terms have small overlap with DOC-TF X  X .
We now describe how we use the discovered anchor text by different approaches for retrieval in a language model-ing approach [17]. We point out that our focus here is not to evaluate different schemes to aggregate or combine an-chor text [14]; instead, we focus on comparing the utility of RALM and auxiliary anchor text for helping retrieval.
We follow the typical language modeling based retrieval approach[17] and score each web page  X  for a query  X  by the likelihood of the page  X   X  X  document language model  X  (  X   X   X  ) generating the query  X  : When using Dirichlet smoothing, the document language model  X  (  X   X   X  ) can be calculated by Equation 3 and then used in Equation 8 for retrieval. We call this baseline QL . We only fix  X  = 2500 in Equation 3 for the document models used to calculate RALM, but tune the  X  for QL to achieve the best retrieval performance in our experiments in  X  5.2.
We follow the mixture model approach [15, 16] to use the discovered anchor text information for helping retrieval. In this approach, a web page  X   X  X  document language model is assumed to be a mixture of multiple component distributions where each component is associated with a prior probability, or a mixture weight. Therefore, we can estimate a language model  X  (  X   X   X  ) from anchor text discovered by each different approach for the page  X  and use  X  (  X   X   X  ) as a component of  X   X  X  document model thus obtaining a better document language model  X   X  (  X   X   X  ): where  X  (  X   X   X  ) is the original smoothed document model in the QL baseline. Then we can plug  X   X  (  X   X   X  ) into equation 8 for retrieval. We compare the retrieval performance of document language models updated by different discovered anchor text information.

We consider three different anchor text sources to update a web page  X   X  X  document model: (1) the observed origi-nal anchor text  X   X  X  X  X  X  X  associated with  X  , (2) the auxiliary anchor text  X   X  X  X  X  of  X  , and (3) the RALM computed by our approach for  X  . We estimate the anchor text language model  X  (  X   X   X   X  X  X  X  X  X  ) and  X  (  X   X   X   X  X  X  X  ) by using the ML estimate of observing each word  X  in  X   X  X  X  X  X  X  and  X   X  X  X  X  , respectively. Here, we design the following five retrieval methods that use the above three anchor text sources: 1. M-ORG , which only uses the observed original anchor text language  X  (  X   X   X   X  X  X  X  X  X  ). 2. M-AUX , which only uses the auxiliary anchor text lan-guage  X  (  X   X   X   X  X  X  X  ). 3. M-ORG-AUX , which uses both  X  (  X   X   X   X  X  X  X  X  X  ) and  X  (  X   X   X  to update the document model  X  (  X   X   X  ) by: M-ORG-AUX 0.3711 57.5  X  = 0 . 95 ,  X  = 0 . 99 M-RALM 0.3388  X  53.6  X  = 20 ,  X  = 0 . 95 M-ORG-RALM 0.3975  X   X  59.7  X ,  X  = 0 . 95 ,  X  = 20 Table 7: Retrieval performance of different ap-proaches with TREC 2006 NP queries. The star indicates statistically significant improvement over MRRs of M-ORG and M-ORG-AUX by one-sided t-test (  X  &lt; 0 . 05 ). The triangle indicates statistically significant improvement over MRRs of QL and M-AUX by one-sided t-test (  X  &lt; 0 . 05 ). 4. M-RALM , which only uses the RALM  X  (  X   X   X  0 ) in Equa-tion 2. The original anchor text of  X  0 is not used in Equation 2 for calculating RALM. 5. M-ORG-RALM , which uses both  X  (  X   X   X   X  X  X  X  X  X  ) and the RALM  X  (  X   X   X  0 ) in Equation 2 by: The original anchor text of  X  0 is not used in Equation 2 for calculating RALM.

Note that we can update each page X  X  document model offline, thus this computationally expensive procedure has little impact on the online query processing time. Moreover, different from experiments in  X  4.1, we use all anchor terms instead of the top-20 most important terms discovered by different approaches.
We use the TREC web named page finding tasks in Ter-abyte Track[4, 5] to evaluate the performance of different retrieval methods described in  X  5.1. The objective of the named page (NP) finding task is to find a particular page in the GOV2 collection, given a topic that describes it. We use the NP topics and their relevance judgments for our experi-ments. In this experiment, we used Porter stemmer and did not remove stopwords when indexing the GOV2 collection.
For each NP query, we first run it against the GOV2 collection to obtain the QL baseline; then we use five re-trieval methods described in  X  5.1 to rerank the top-100 web pages returned by QL. The reranked lists are evaluated by two TREC measurements previously used for the task [5]: MRR which is the mean reciprocal rank of the first correct answer and the %Top10 which is the proportion of queries for which a correct answer was found in the first 10 search results. We use the TREC 2005 NP topics (NP601-872) for training and the TREC 2006 NP topics (NP901-1081) for testing. We first tune the Dirichlet parameter  X  = 500 for QL to achieve the highest MRR on the training set and obtain QL X  X  top-100 web pages for reranking. We then fix  X  = 500 to calculate the smoothed document model com-ponent  X  (  X   X   X  ) in the five retrieval methods but tune the mixture parameters  X  and  X  for them to achieve the highest MRRs with the training queries. For the two approaches that use RALM, the parameter  X  of RALM is also tuned. After that, we run different methods on the testing set.
Table 7 shows the retrieval performance of different meth-ods and the tuned parameters in each method. We observe: (1) M-ORG-RALM performs statistically significantly bet-
AUX-TFIDF  X  X  X   X  X  X  X   X  X  X  X  (  X  ) RALM  X  (  X   X   X  0 ) Rel. as using the original anchor text does.
AUX-TFIDF  X  X  X   X  X  X  X   X  X  X  X  (  X  ) RALM  X  (  X   X   X  0 ) Rel. similar pages X  anchor text. ter than M-ORG. This indicates that missing anchor text discovered by RALM provides additional information not in the original anchor text so that combining them can further improve the retrieval performance. (2) M-ORG-RALM and M-RALM performs statistically significantly better than M-ORG-AUX and M-AUX, respectively. This indicates that in GOV2 missing anchor text information discovered by our content based approach helps retrieval more effectively than
In Table 7, we observe that the auxiliary anchor text helps the performance very little in this task. There are two plau-sible reasons: first, TREC NP queries are short queries and Metzler et al. observed that auxiliary anchor text does not help or even hurts the performance of short navigational web queries[14]; second, the anchor text sparsity problem is seri-ous on the GOV2, thus very small percentage of pages can collect some auxiliary anchor text as shown in Table 1 to benefit the search task. However, even when serious anchor text sparsity exists and queries are short, our content based approach still helps improving retrieval effectiveness.
We expect our technique can enhance the retrieval perfor-mance of general web search engines where there are large portion of short navigational queries. As is well known, in the general web search environment there are many low-quality web pages and spam; thus, we need to address issues about web page quality and noise filtering for better bene-fitting general web search. We leave this as future work.
In this paper, we proposed a language modeling based technique to overcome the anchor text sparsity problem by using web content similarity. Our approach computes a relevant anchor text language model, called RALM, from its similar web pages X  associated anchor text to discover its plausible missing anchor text. Compared with a link based approach [14], our content based approach has no specific link structure requirements on the web page of interest and thus can further reduce anchor text sparsity.

We designed experiments with two TREC web corpora to evaluate the effectiveness of discovering missing anchor terms by three different approaches: the link based approach, the RALM approach, and the keyword based approach. Ex-perimental results show that the RALM approach can ef-fectively discover missing original anchor text and performs statistically significantly better than the two link based ap-proaches on both collections. Moreover, RALM X  X  discovered anchor text is similar in nature to auxiliary anchor text while different from the keywords in the web page.

By using the mixture model[15, 16], we used different dis-covered anchor text information within the language mod-eling framework for retrieval. We evaluated using differ-ent approaches for improving retrieval effectiveness with the TREC named page finding task. The results show that (1) RALM helps retrieval more than using the auxiliary anchor text collected over the web graph and (2) combining RALM and the original anchor text can statistically significantly im-prove the retrieval performance of only using the original an-chor text. Furthermore, RALM can help improving retrieval effectiveness for short navigational queries even when serious anchor text sparsity exists. This makes RALM a promising technique for improving general web search engines. There are several interesting directions of future work. Metzler et al. found that auxiliary anchor text can effec-tively help longer, informational queries [14]; we will explore how well RALM can help long informational queries. We also want to explore using RALM X  X  discovered missing an-chor text information beyond the language modeling based retrieval framework, e.g. using it to extract useful features for learning-to-rank retrieval approaches [3].
This work was supported in part by the Center for Intelli-gent Information Retrieval and in part by NSF IIS-0910884. Any opinions, findings and conclusions or recommendations expressed in this material are the authors X  and do not nec-essarily reflect those of the sponsor.
