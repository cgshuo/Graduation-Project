 that uses information concerning the relational structure of the problem. common set of links is evidence for similarity between such p ages. For instance, if W link to W way of expressing this dependency is through the following M arkov network [5]: Here F variables to C variables (e.g., F in this section. The semantics of the graph, for a fixed input f eature set { F C domains it could be the case that the most sensible model woul d state that C about C given C hidden common causes generating this pair of random variables.
 well S and P (due to the TV set market).
 Competitor ( S, P ) is given in Figure 1(a) as unlabeled gray vertices. Consider a linear regression model for this setup. We assume that for each object O the stock price O where each The fact that there are several hidden common causes between M and S can be modeled by the covariance of to be non-zero. The same holds for  X  be zero (  X  model popular in economics [12].
 Figure 1(c). The undirected representation encodes that Figure 2. Directed mixed graphs are not a good representatio n for this sequence structure. To summarize, the decision between using a Markov network or a DMG reduces to the following modeling issue: if two unlinked object labels y of relationships exists between y intermediated points between two classes labels y points in this chain besides the endpoints.
 ments in text classification are described in Section 4. which we review and compare to our proposed model.
 in the input space X , there is a corresponding function value f x Figure 3: (a) A prediction problem where y two datapoints. Dependencies between f represented in the picture. Indicators  X  models such as seemingly unrelated regression and structur al equation models [11]. where the ij th entry of  X  is given by a Mercer kernel function K ( x ditioning on relational indicators . Let  X  The indicator values are observed for each pair of data point s ( x given relational structure. A model for P (  X  into the Gaussian process by conditioning on all indicators  X  trates a problem with datapoints { ( x variables. Each y interpreted as the cumulative distribution of f variable with zero mean and variance  X  2 .
 In the example of Figure 3(a), one has two relations: ( x rated by conditioning on the evidence (  X  the training set. The prediction task is to estimate y Markov blanket for f where g g + ? i , with ? i as a normal random variable with zero mean and variance v i =  X  2  X   X  2  X  term being the variance of  X  covariance matrix  X  only if objects i and j are related (that is, bi-directed edge y Parameterizing  X  models 3(b) is that now the Markov blanket for f  X  terms), following the motivation presented in Section 1. following Gaussian process prior (implicitly conditioned on x ): where R = K +  X  between labels. Ideally, the parameterization of  X  corresponding graph thus contain unobserved and observed l abel nodes. 3.1 Method I allows for more efficient inference and is done as follows: Finally, set  X  every clique, and every observed node y will never be in a same clique in G 3.2 Method II appear in many common cliques are more likely to have more hid den common causes, and hence produce much weaker associations but does not introduce spu rious relations. create an independent standard Gaussian variable for each e dge y however, has shortcomings of its own: for all pairs ( y that U 0 close to zero even if y Methods I and II according to the marginal likelihood of the m odel. 3.3 Algorithm Recall that our model is a Gaussian process classifier with er ror terms parameterization of the full error covariance matrix: posterior of f , we approximate the posterior density P ( g |D ) , D = { ( x the given training data. The approximate posterior has the f orm Q ( g )  X  X  ( g ) Q section. Since the covariance matrix  X  over each datapoint: P ( y | g ) = Q n classification can be used [8] (with the variance given by  X  instead of K ).
 class.  X  X itations X  denotes the citation count within the tw o paper classes. 4.1 Political books and XGP achieved 0.98 (the difference between XGP and GPC hav ing a std. deviation of 0.02). 4.2 Cora 4.3 WebKB to generate the results for XGP. Mean and standard deviation of AUC results are reported. prediction performance in problems traditionally approac hed by Markov network structures. provide alternative ways of learning our models, including multiple types of relationships. Acknowledgements: We thank Vikas Sindhwani for the preprocessed Cora database .
