 1. Introduction
One way to improve the performance of information systems is to build user models into systems and customize the system to a user X  X  specific need. User models can be either group models built for distinctive groups of users or individual models built for individual users. Stereotype is a widely used technique in user modeling for group modeling as well as for creating initial individual user models. A stereotype is a common user characteristic/trait that is shared by many users (Rich, 1979, 1989).
Examples of stereotypes may be  X  X xpert users X  or  X  X ovice users X . Stereotypes are created on the assumption that the presence of particular characteristics in one member of the stereotype would imply that of others (Harvey, Smith, &amp; Lund, 1998). Therefore, a stereotype normally contains the common knowledge about a group of users. A new user will be assigned into related stereotype(s) if some of his/her characteristics match the ones contained in the stereotype(s).

Though stereotyping usually involves intensive knowledge engineering on the part of the system administrator (Mostafa, Quiroga, &amp; Palakal, 1998), the advantage of using the stereotype technique is that the knowledge about a particular user will be inferred from the related stereotype(s) as much as possible, without explicitly going through the knowledge elicitation process with each individual user. Another advantage is that the information about user groups/stereotypes can be maintained with low redundancy (Fink &amp; Kobsa, 2000; Rich, 1989). Kuflik, Shapira, and Shoval (2003) have found that using a stereotype was better than using a personal-based user profile in information filtering systems.

Nevertheless, using stereotypes is not without problems. Most stereotypes are formed based merely on users X  external characteristics and on subjective human judgment, usually of a number of users/experts (Shapira, Shoval, &amp; Hanani, 1997). The user X  X  knowledge about the system and/or task is not involved. It is common that such stereotypes do not represent their members accurately. The issue of inaccuracy of stereotypes has been pointed out by many researchers, e.g., Beaumont (1998), Bellika, Hartvigsen, and Widding (1998), Brajnik, Guida, and Tasso (1990) and Shapira et al. (1997).

The lack of accuracy is liable to lead to conflicts between the individual models and the assignment to various stereotypes (Shapira et al., 1997), which can affect accurate construction of individual user models. Consequently, system functions adapted to individual user models will fail to achieve their goals.
A common practice for the systems using stereotypes is to continuously check user responses, detect and resolve the conflicts between stereotypes and specific user knowledge values, and then update stereotypes and user models. However, we hardly see the evidence on the improvement of user models through such conflict-resolving and stereotype/model updating. Because of the inaccuracy problem, it is important that the user classes represented by the stereotypes be as homogeneous as possible, and this homogeneity should be based on the users X  knowledge of specific domain or task (Beaumont, 1998).

This paper reports the results of an empirical test on the accuracy of some common stereotypes of information retrieval (IR) systems. Since the user X  X  IR knowledge, the knowledge about IR system com-ponents, and the relationships among them are important to information searching performance (Allen, 1996), we argue that the stereotypes of IR systems be based on the user X  X  IR knowledge, as well as the domain knowledge that is related to the specific search task. We test the accuracy or homogeneity of 11 commonly used user stereotypes by exploring the differences among the members of these stereotypes in terms of their IR knowledge. A stereotype should include only those members with similar level of IR knowledge. We believe such a stereotype is homogeneous and is more accurate than the one based just on the user X  X  explicit characteristics. We assume such an accurate stereotype adapts better to the individual users of IR systems/services.
 The fact that different people have different levels of knowledge is obvious to human experts.
However, it is hard for a computer system to intelligently identify the differences. More important, we need to know not only that there exist differences, but also how the users differ and what the exact differences are. Without systematic investigations, such facts cannot be known even to human experts.
By conducting this study, we propose a formal method for exploring the differences among the members of a stereotype. This kind of formal method is necessary for IR systems to build better, accurate ste-reotypes.

The remainder of this paper is organized as follows: Section 2 reviews literatures on stereotype-based user modeling; Section 3 describes the research design; Section 4 analyzes experiment results; Sections 5 and 6 present and discuss the experiment results, and Section 7 concludes the paper and discusses future re-search directions. 2. Related work
Using stereotype technique for user modeling is first seen in Rich X  X  GRUNDY (1979, 1989), where literatures are recommended to the users of specific stereotypes. However, as Allen (1990) points out,  X  X  ... data on the performance of GRUNDY are sketchy. It was shown to perform better than chance, averaged across a large number of its predictions; however, there was no test of whether the adaptive mechanism improved performance X  X  (p. 518).

Paterno and Mancini (2000) developed three stereotypes in their museum system: experts, tourists, and students of art. Different user models are constructed based on the stereotypes and on the different tasks.
They observe 30 users X  behaviors in initial information access, navigation, and information presentation, and find that users are able to perform desired tasks and that the system is generally easy to use. However, they do not mention how the three stereotypes fit the users, although they suggest future studies on modeling users X  preference.

Art Technology Group X  X  Personalization Server (Fink &amp; Kobsa, 2000) manually developed group profiles and associated rules that allowed the server to assign a user to one or more user groups. These group profiles and rules resemble the stereotype approach. The group profiles contain relevant user characteristics, such as age and gender. The rules take into account not only the user characteristic, but also system usage (e.g., pages visited, products bought) and the environment information (e.g., domain name, browser type).

Brajnik et al. (1990) report the UM-tool, a generic user modeling tool based on stereotypes. Users are assigned to stereotypes based on users X  background information acquired through interviews, and their computer usage history stored in the system. The authors do not explain how the stereotypes are created or how effective they are in user representation.
 Fernandez-Manjon, Fernandez-Valmayor, and Fernandez-Chamizo (1998) model UNIX users in their
Aran system based on the following stereotypes: intermediate-user, novice-user, editor-user, programmer-user, network user, and math-user. These domain-oriented stereotypes try to represent users X  experience and interests with respect to different UNIX contexts and subdomains. They classify users to different stereotypes based on the key characteristics of users. However, there is no evaluation on the identified key characteristics, or the sufficiency and accuracy of the stereotype.

Many other systems such as the work by Chin (1989) and Kobsa, Muller, and Nill (1994) also use stereotype-based user modeling. However, we hardly see any of the above systems test the accuracy of stereotypes. As Bellika et al. (1998) point out, an important problem with stereotype-based user modeling is incorrect classification of users and inconsistent knowledge representation in user models.

Various ways are proposed to improve the accuracy of stereotypes. Mitchell, Woodbury, and Norcio (1994) use fuzzy set theory to develop fuzzy user classes. The authors claim that this method can measure the differences between users more accurately than stereotypes. Bushey, Manuney, and Deelman (1999) suggest categorizing users based on the user characteristics or behaviors that are important to the design of the related system. They propose a method to categorize users based on their performance data, rather than some explicit characteristics not necessarily related to the task performance. Shapira et al. (1997) suggest a hybrid approach to develop stereotypes. Such an approach includes the user questionnaire, cluster analysis based on the data collected from the questionnaire, and an assessment of user clusters by field experts who know the users. They also emphasize the importance of relating stereotypes with tasks, in their words,  X  X  X he environment within which the system is to be applied X  X . This approach is used in modeling the experimental participants of an information filtering system and it appears to be effective in their study (Shapira, Shoval, &amp; Hanani, 1999). 3. Methodology
We studied the accuracy of 11 commonly used stereotypes using discriminant analysis. We modeled users X  knowledge about IR systems, and analyzed if the stereotypes could represent members X  IR knowl-edge consistently, i.e., if members of a stereotype would have the same knowledge level. We assumed a system that could adapt to users with different levels of task knowledge differently would have better performance than one that could not. The stereotypes, participants, and knowledge elicitation and mod-eling are introduced below. 3.1. Stereotypes investigated in the study
As shown in Fig. 1, we investigated 11 stereotypes along four dimensions of user characteristics: edu-cational and professional status, native language, academic discipline, and level of computer experience.
Educational and professional status was the educational degree level if a user was studying at school or the working status if the person was employed by the time of this study. The reason for choosing this characteristic was that it normally reflects a person X  X  knowledge and skills in a professional field.
A user X  X  first language is the language the user acquired at home during his or her childhood. It is also called the user X  X  native language. This characteristic was chosen because IR systems are closely related to languages.

Academic discipline or background refers to the major area of knowledge a user was studying. Different disciplines have different bodies of knowledge and different approaches to exploring knowledge. These differences may have an impact on users X  mental models.

Computer experience is referred to as a person X  X  experience in using any of a list of computer appli-cations such as database management, electronic mail, information retrieval, etc. Since IR systems are presumably computerized systems, a person X  X  computer experience was considered important in shaping the person X  X  views of IR systems.

The stereotypes based on the above characteristics have been frequently used in IR user studies. For example, librarians (search intermediaries) and graduate students are commonly studied subject types (e.g, Ma, 2002; Saracevic &amp; Kantor, 1988; Yee, 1993). Shaw (1996) and Neuman (1995) investigate the search behaviors of undergraduate students and high school students. Charoenkitkarn (1996) studies the effect of native language on users X  search performance. Native English speakers are also the targeted users for IR systems participated in the TREC 2003 Question Answering Track (Voorhees, 2003). Borgman (1989) examines the effects of academic orientation, among other variables, on IR performance; the subjects in-clude undergraduates of Engineering, Psychology, and English majors. Qiu (1993) compares search pat-terns of different user groups in hypertext systems based on the user X  X  academic background, as well as some other aspects. Ellis, Cox, and Hall (1993) compare researchers X  information seeking patterns in
Physical Sciences and Social Sciences. Su (2003) compares search performance of sciences, social sciences, and humanities.

In our study, a user can belong to different stereotypes when categorized on different dimensions. For example, a student can belong to stereotypes of undergraduate, engineering, native English speaker, and medium-level computer experience. 3.2. Participants
Our participants were mainly students, with librarians included as the benchmark group. We recruited 64 participants from four populations: professional librarians and information specialists (abbreviated as  X  X  X ibrarians X  X  hereafter), graduate students (both doctoral and master X  X ), undergraduate students, and high school students. Younger students (such as elementary students) were not considered as the major type of  X  X  X nd users X  X  and were therefore not included in our study.

The participants belonged to different stereotypes (as shown in Fig. 1) based on their self-reported demographic characteristics. Table 1 summarizes the distribution of participants in different stereotypes along each of the four dimensions of user characteristics, as shown column-wise. Please note that parti-cipants in the two academic disciplines were only university students (both graduate and undergraduate). 3.3. Eliciting and modeling of users X  knowledge about IR systems
Various knowledge elicitation and representation techniques exist. In this study, we used the repertory grid technique (RGT). The RGT was invented as a tool for and is based on Kelly X  X  personal construct theory (Kelly, 1955), which asserts that people understand the world (events, people, etc.) through their personal construct systems. The personal construct system that each person develops is the set of repre-sentations or model of the world that the person has developed. It is acquired through the person X  X  social experience.

The RGT, in its simplest form, involves the generation of a list of concepts (elements) about things or events to be investigated, and the generation of attributes (constructs) based on the list of concepts.
A concept is defined as anything that can be compared or contrasted. For example, people, vegetables or notions such as occupations, feelings, situations, events, etc. can all be elements. If the problem is to choose a future career, the concepts may be different jobs. Concepts used in a study may be elicited from the subject or provided by the tester, or both, and they need to be well known and personally meaningful to the subject (Shaw, 1980).

An attribute (construct) is a bipolar dimension that, to some degree, is a property of each concept. A construct is a way in which some things (elements) are seen as alike yet still different from others. Examples of attributes for concepts about people may be: Don X  X  believe in God/Very religious ; Not athletic/Athletic ; Understands me better/Doesn X  X  understand at all ; Sociable/Not sociable , etc. (Fransella &amp; Bannister, 1977).
Like concepts, attributes can also be elicited from the subjects or provided by the tester. There are several ways to elicit attributes. The classic method used by Kelly is to consider various triads (groups of three concepts) selected successively from the whole concept list. The subject or person(s) from which attributes are to be elicited is first presented with three concepts and asked to specify some important aspects in which two of them are alike. Then the subject is asked in which aspects the third concept differs from the other two. Often the subject will indicate spontaneously which two concepts are being judged alike. The subject X  X  description of the similarity forms one pole of the attribute and the answer to the question concerning the difference is the contrast pole. Such a process is called a sort. The examiner records this similarity and contrast as the resulting attribute dimension from the first sort, and proceeds to the second and subsequent sorts using different triads of concepts. There are no rules on how many triads of concepts should be presented to the subjects, but between 10 and 25 is a common range (Bannister, 1968; Fransella &amp; Bannister, 1977).

Presently, the most frequently used variation of repertory grids is rating grids. In a rating grid, the subject is asked to evaluate the concepts systematically by using the attribute list to generate the grid of rating numbers. At every intersection of column and row is the subject X  X  rating value of the concepts on the attributes. The grid form was used as both a model elicitation tool and as a formal representation of IR knowledge in this study. Readers are referred to Latta and Swigger (1992) and Zhang and Chignell (2001) for more detailed descriptions of the use of the method in information science. Discussions on the tech-nique X  X  validity and reliability can be found in Bannister (1968), Latta and Swigger (1992), and Shaw and Woodward (1988).

Table 2 lists the nine concepts about IR systems and the three attributes method.

These concepts were suggested and decided by a group of IR experts who were faculty and doctoral students in information science. The nine concepts cover important components of IR process, from users X  information needs as start and documents from databases as the end. The attributes were generated by the same group of experts using the triads method. Eight attributes were generated and rated by the subjects initially. However, only the listed three attributes were able to differentiate features of mental models among the subject groups. Since the purpose of the study was to find out differences between user groups, only the concept ratings against these three were involved in the final data analyses. The concept ratings on other attributes that did not distinguish subject groups were discarded.

These concepts and attributes were then transformed into a rating form that was administered to the participants. For example, the rating form for the concept  X  X  X rowsing X  X , with the three attributes, is shown in Fig. 2.
On the form, all attributes were transformed into five point scales, with  X  X 1 X  X  at the left poles and  X  X 5 X  X  at the right poles. Participants were asked to rate on the form the nine concepts (one by one) against each of the attributes. In case some participants would have difficulty in understanding an attribute or a concept, or they would consider an attribute not applicable to a concept, a  X  X  X ot applicable X  X  option represented by an  X  X  X  X  X  sign was added to the scales to account for this. Participants could simply circle this sign to rate a concept. It was completely up to the participant to interpret the meaning of the concept on the attribute dimension. Participants either individually met with the authors or they sent their ratings to the authors through surface or electronic mail. 4. Data analyses 4.1. Factor analysis (summarization) of raw concept ratings
Each individual rating of a concept on an attribute constitutes a variable. Altogether, there are nine concepts and three attributes, constituting 27 ratings or variables. A vector of 27 variables represents a participant. These ratings needed to be summarized to reveal unexpected dimensions (or factors) among the original variables and to reduce the number of original variables (Mulaik, 1972). Using the principal component approach in factor analysis, with the varimax rotation, the original 27 (9 concepts  X  3 attributes) variables were transformed into principal factors. The first nine factors with the eigenvalue greater than 1 were selected, a norm used in factor analyses. These nine factors accounted for 68% of the total variations from the original ratings. Factor loadings and interpretations are summarized in Table 3.

Each of the nine factors represents certain original variables (ratings). We assigned a name to each factor to interpret the original variables. The naming was based on the interetation of the attribute dimensions of the major concept(s). For example, the first factor was named as  X  X  X urposefulness of Querying X  X  because the major concepts:  X  X  X nformation need X  X ,  X  X  X uery X  X  and  X  X  X earch X  X  were all interpreted as  X  X  X uerying X  X  and the attribute  X  X  X argeted/untargeted X  X  was interpreted as reflecting the purposefulness of actions ( X  X  X uerying X  X  here). A vector of nine factors represents a participant. Each factor has a score that is a weighted com-bination of the observed scores on original variables in the factor (Boyce, Meadow, &amp; Kraft, 1994, p. 84). A high factor score means that the concepts were rated on the high value end of the attribute scale in the factor. A low score means that the concepts were rated on the low value end of the scale. 4.2. Discriminant analysis on factor scores
With the four user characteristics as grouping variables, and the nine factors as predicting variables, we used the discriminant analysis technique to examine if an individual X  X  predicted membership of a stereotype is consistent with the person X  X  actual membership of a stereotype. The purpose was to detect the differences among the members of the same stereotype in terms of their IR knowledge.

Discriminant analysis extracts from the participants X  factor scores a stereotype classification criterion/ rule for classifying each observation in a user stereotype (Huberty, 1994; SAS Institute Inc., 1988). Spe-cifically, we employed the k -nearest-neighbor method of non-parametric discriminant analysis. The par-ticipants X  data set was used as both the training set and the test set to generate and evaluate the classification criterion.

Based on the classification criterion, we calculated the posterior probability of the stereotype mem-bership for each participant based on the participant X  X  factor scores. We used equal prior probability, assuming the equal size of various populations in the study. The participant was predicted to be in the stereotype where the participant X  X  posterior probability was maximal (compared to the posterior proba-bilities for other stereotypes). If a participant X  X  posterior probability of belonging to the actual stereotype was not the maximum one, the participant was judged as being  X  X  X isclassified X  X  into that actual stereotype.
The misclassification rate, referred to as  X  X  X rror rate X  X  in the remainder of the paper, was estimated for each stereotype. The analysis was performed using Windows version of SAS 8.2. 5. Results
For each actual stereotype, we generated two types of results based on discriminant analysis: the pre-dicted stereotype membership for each person and the error rate for the whole actual stereotype.
Table 4 (panels A X  X ) summarize the results of stereotypes on each of the four user characteristic dimensions respectively. Fig. 3a X  X  show the corresponding error rates.

Each table presents the result of a stereotype in two dimensions: The row shows the classification results for the members of an actual stereotype, i.e., how many members of an actual stereotype are classified to which stereotypes. The ratio following a number on the same row is the percentage of that number in the total number of actual members. Each column displays the results for a predicted stereotype, i.e., from which actual stereotype(s) and how many consist of the current predicted stereotype membership. The ratio below a number is the percentage of that number in the total number of predicted members.

For example, Table 4(panel A) describes the results for the four stereotypes of the educational and professional status. Row 3 shows that none of the eight actual librarians is predicted into other stereotypes.
However, these eight actual librarians account for only 89% of the predicted librarians, as shown in Row 5 of Column 3. The other 11% (1) predicted librarian is from the actual high school student group, as shown in Rows 10 and 11 (We evaluate this high school student as being  X  X  X isclassified X  X .). Therefore, the total number of the predicted librarians is 9, as shown in the last row of Column 3, with the total percentage beneath the number. 6. Discussion Our empirical study shows that the librarian is the most accurate stereotype among all 11 stereotypes.
None of the actual librarians was misclassified, and only one of the 56 actual student participants was predicted into the librarian stereotype. Other stereotypes have varying degrees of consistencies in repre-senting members with respect to their IR knowledge. Along the educational and professional status dimension, the stereotype error rate tends to increase as the education level decreases. Undergraduate stereotype has the highest error rate, and therefore is the least accurate stereotype. Most of the pre-assigned undergraduates who are misclassified are predicted as the high school stereotype. A reason may be that these wrongly classified undergraduates have a low level of IR knowledge. High school student stereotype has a high error rate too. The graduate student stereotype is better than undergraduate and high school stereotypes, but still worse than the librarian stereotype.

The amount of education, and thus the IR task knowledge, may be the key to interpret the different accuracies among these stereotypes. Librarians represent those who have completed their formal profes-sional education/training and have obtained practical experience in their field. They apparently have a high level of IR knowledge and this knowledge level seems to be the same among the members. The librarian stereotype thus consistently represents all librarians. This result matches well with the reality that librarians are trained professionals for IR tasks.

Undergraduate and high school participants have lower levels of education, and normally fewer IR skills. For example, many of them never did online searches before participating in this study. These participants appear to have various levels of IR knowledge. Graduate students seem to have more opportunities to use IR systems, and have higher level of IR knowledge.
 Stereotypes on the native language dimension do not show satisfying results. Although the native
English speaker stereotype has zero error rate, the predicted members for this type actually include many non-native English speakers. An explanation is that many non-native English speakers are as IR knowledgeable as native English speakers. This indicates that the stereotype based on native language may be inappropriate for user modeling.
The science/engineering user stereotype appears to be more homogeneous than the social sciences/ humanities stereotype. The number of the actual social sciences/humanities participants who are predicted as other types of participants is higher than that of sciences/engineering students. This indicates that the social sciences/humanities stereotype may not be accurate for IR user modeling. Two reasons may account for the above phenomenon. One is that students in science/engineering majors may have more opportunities to access information systems and obtain more IR knowledge. They have similar levels of IR knowledge.
The other reason may be that the training in science/engineering majors is more standardized than that in social sciences/humanities majors. Standardized training can decrease the difference of IR knowledge levels among students in science/engineering majors compared with those in social sciences/humanities majors.
Users X  prior computer experience does not seem to help with accurate stereotyping. Both high-level and medium-level actual stereotypes have over 50% of their members predicted as members in other stereotypes on this dimension. The low-level stereotype has a high proportion of its members predicted to be in the same type. These members, however, account for only 43% of the predicted stereotype. 57% of its members are from other actual groups. A reason might be that it is hard to measure  X  X  X omputer experience X  X . Dif-ferent measures may generate different results.

We also observe that the medium type along a dimension (such as the medium level of computer experience between the high and low) suffers the highest error rates. This may be because that the medium type is hard to quantify. There are no very clear features/attributes to separate the medium type from the others at the ends of the dimension, which suggests that just a few very distinctive stereotypes be con-structed.

It should be pointed out that when the total 64 participants were divided into different groups, the sample size was really small. Therefore, the results should be interpreted as exploratory. Larger scale studies should be conducted to confirm the findings. 7. Conclusion
This paper contributes in two ways to stereotype modeling in IR systems. First, we suggest that users X  IR knowledge should be considered for stereotype modeling for IR systems, and we argue the stereotypes based on the IR knowledge can be more homogeneous and accurate than the ones that are based just on the user X  X  explicit characteristics. We assume the system or service built for stereotypes based on users X  IR knowledge would perform better than those based just on the user X  X  demographic characteristics. This is based on the assumption that IR knowledge is important to the user X  X  IR performance (Allen, 1996).
Second, we propose a formal method to empirically distinguish individual users in terms of their IR knowledge, and to evaluate the accuracy of a stereotype. Our method categorizes users X  membership according to the demographic characteristics, and then generates users X  new membership according to their
IR knowledge using discriminant analysis. We compare the degree of similarity between the actual and predicted assignment to see if members of a actual stereotype have the same knowledge level, so as to study the accuracy of the stereotypes.

We conducted empirical tests on the accuracy of 11 commonly used stereotypes. The results show that our method well captures the difference among members of a stereotype, e.g., librarians and information specialists is the most accurate stereotype; other stereotypes such as high-school students have varying degree of IR knowledge among their members. The results reflect the fact that librarians and information specialists are professionally trained for IR tasks and this training makes them different from other types of users. Our method shows promise in detecting the differences among stereotype members and thus in helping developing more accurate stereotypes and individual user models. The method can be used in the design of stereotype-based systems, such as many recommender systems (Motaner, Lopez, &amp; Lluis De La
Rosa, 2003; Sollenborn &amp; Funk, 2002), in two aspects: (a) Detecting the accuracy or homogeneity of the stereotypes that have already been developed for the system, and (b) revising and/or forming new mem-berships for stereotypes that can be more accurate or homogeneous (Zhang, 2003).

This study has limitations in stereotype evaluation in real applications. We would like to implement the stereotypes in a system and to evaluate the reliability of stereotypes according to the goal of the stereotypes, e.g., to empirically compare the performance of a system that uses stereotypes with and without the use of
IR knowledge, or stereotypes with and without consistent representation of members X  IR knowledge. It may also help to invite human experts to assign participants into various stereotypes based on the par-ticipants X  concept ratings. The assignments by the experts and by the discriminant analysis can then be compared to better evaluate the stereotypes. In addition, we would like to invite more participants and experiment on a larger scale of data.

Another interesting research direction for our future work is to consider the situation when a user be-longs to multiple stereotypes. This is an important research issue, since the IR knowledge a user has may relate him/her perfectly to one of the actual stereotypes based on one categorization dimension, but may raise the error rate of another stereotype on another dimension.
 Acknowledgements
We wish to thank the volunteers who participated in this study, and we appreciate the valuable com-ments from Paul Kantor and referees on an earlier draft of this paper.
 References
