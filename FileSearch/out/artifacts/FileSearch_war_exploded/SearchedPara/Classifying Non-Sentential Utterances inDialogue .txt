 Potsdam University King X  X  College London King X  X  College London the classification of non-sentential utterances (NSUs) in dialogue. We introduce a fine-grained taxonomy of NSU classes based on corpus work, and then report on the results of several machine learning experiments. First, we present a pilot study focused on one of the NSU classes in the taxonomy X  X are wh -phrases or  X  X luices X  X  and explore the task of disambiguating between experiments show that, for the taxonomy adopted, the task of identifying the right NSU class can be successfully learned, and hence provide a very encouraging basis for the more general enterprise of fully processing NSUs. 1. Introduction
Non-sentential utterances (NSUs) X  X ragmentary utterances that do not have the form of a full sentence according to most traditional grammars, but that nevertheless convey a complete clausal meaning X  X re a common phenomenon in spoken dialogue. The following are two examples of NSUs taken from the dialogue transcripts of the British
National Corpus (BNC) (Burnard 2000): (1) a. A: Who wants Beethoven music?
Arguably the most important issue in the processing of NSUs concerns their resolu-considered non-clausal. In the first of the examples, the NSU in bold face is a typical  X  X hort answer, X  which despite having the form of a simple NP would most likely be understood as conveying the proposition Richard and James want Beethoven music .The realized by a bare wh -phrase, the meaning conveyed by the NSU could be paraphrased as the question When is Ruth X  X  birthday? most prototypical NSU classes, recent corpus studies (Fern  X  andez and Ginzburg 2002;
Schlangen 2003) show that other less well-known types of NSUs X  X ach with its own resolution constraints X  X re also pervasive in real conversations. This variety of NSU classes, together with their inherent concise form and their highly context-dependent meaning, often make NSUs ambiguous. Consider, for instance, example (2): (2) a. A: I left it on the table.
 or as an acknowledgment, depending on whether it is uttered with raising intonation or not. In (2b), on the other hand, the NSU is readily understood as a short answer, whereas in (2c) it fills a gap left by the previous utterance. Yet in the context of (2d) it will most probably be understood as a sort of correction or a  X  X elpful rejection, X  as we shall call this kind of NSU later on in this article.
 order to resolve NSUs appropriately systems need to be equipped in the first place with the ability of identifying the intended kind of NSU. How this ability can be developed is precisely the issue we address in this article. We concentrate on the task of automatically classifying NSUs, which we approach using machine learning (ML) techniques. Our aim in doing so is to develop a classification model whose output can be fed into a dialogue processing system X  X e it a full dialogue system or, for instance, an automatic dialogue summarization system X  X o boost its NSU resolution capability.
 notate our data with small sets of meaningful features, instead of using large sets with the aim of obtaining a better understanding of the different classes of NSUs, their distribution, and their properties. For training, we use four machine learn-memory-based learner TiMBL (Daelemans et al. 2003), the maximum entropy algo-rithm MaxEnt (Le 2003), and the Weka toolkit (Witten and Frank 2000). From the 398 advantage of using several systems that implement different learning techniques is that this allows us to factor out any algorithm-dependent effects that may influence our results.
 classes we adopt, present a corpus study done using the BNC, and give an overview of the theoretical approach to NSU resolution we assume. After these introductory sections, in Section 3 we present a pilot study that focuses on bare wh -phrases or sluices.
This includes a small corpus study and a preliminary ML experiment that concentrates on disambiguating between the different interpretations that sluices can convey. We obtain very encouraging results: around 80% weighted F-score (an 8% improvement over a simple one-rule baseline). After this, in Section 4, we move on to the full range of NSUs. We present our main experiments, whereby the ML approach is extended to the task of classifying the full range of NSU classes in our taxonomy. The results we achieve on this task are decidedly positive: around an 87% weighted F-score (a 25% improvement over a four-rule baseline where only four features are used). Finally, in
Section 5, we offer conclusions and some pointers for future work. 2. A Taxonomy of NSUs
We propose a taxonomy that offers a comprehensive inventory of the kinds of NSUs that can be found in conversation. The taxonomy includes 15 NSU classes. With a few modifications, these follow the corpus-based taxonomy proposed by Fern  X  andez and
Ginzburg (2002). In what follows we exemplify each of the categories we use in our work and characterize them informally.

Clarification Ellipsis (CE). We use this category to classify reprise fragments used to clarify an utterance that has not been fully comprehended. (3) a. A: There X  X  only two people in the class
Check Question. This NSU class refers to short queries, usually realized by convention-alized forms like alright? and okay? , that are requests for explicit feedback. (4) A: So &lt; pause &gt; I X  X  allowed to record you.

Sluice. We consider as sluices all wh -question NSUs, thereby conflating under this form-based NSU class reprise and direct sluices like those in (5a) and (5b), respectively. taxonomy of Fern  X  andez and Ginzburg (2002) reprise sluices are classified as CE. In the taxonomy used in the experiments we report in this article, however, CE only includes clarification fragments that are not bare wh -phrases. (5) a. A: Only wanted a couple weeks.

Short Answer. This NSU class refers to typical responses to (possibly embedded) wh -questions (6a)/(6b). Sometimes, however, wh -questions are not explicit, as in the context of a short answer to a CE question, for instance (6c). (6) a. A: Who X  X  that?
Plain Affirmative Answer and Plain Rejection. The typical context of these two classes of NSUs is a polar question (7a), which can be implicit as in CE questions like (7b). As shown in (7c), rejections can also be used to respond to assertions. (7) a. A: Did you bring the book I told you? material, characterized by the presence of a  X  yes  X  X ord( yeah , aye , yep ...) or the negative interjection no .

Repeated Affirmative Answer. We distinguish plain affirmative answers like the ones in (7) from repeated affirmative answers like the one in (8), which respond affirmatively to a polar question by verbatim repetition or reformulation of (a fragment of) the query. (8) A: Did you shout very loud?
Helpful Rejection. The context of helpful rejections can be either a polar question or an assertion. In the first case, they are negative answers that provide an appropriate alternative (9a). As responses to assertions, they correct some piece of information in the previous utterance (9b). (9) a. A: Is that Mrs. John &lt; last or full name &gt; ? 400
Plain Acknowledgment. The class plain acknowledgment refers to utterances (like yeah , mhm , ok ) that signal that a previous declarative utterance was understood and/or accepted. (10) A: I know that they enjoy debating these issues.

Repeated Acknowledgment. This class is used for acknowledgments that, as repeated affirmative answers, also repeat a part of the antecedent utterance, which in this case is a declarative. (11) A: I X  X  at a little place called Ellenthorpe.

Propositional and Factual Modifiers. These two NSU classes are used to classify propo-sitional adverbs like (12a) and factual adjectives like (12b), respectively, in stand-alone uses. (12) a. A: I wonder if that would be worth getting?
Bare Modifier Phrase. This class refers to NSUs that behave like adjuncts modifying a contextual utterance. They are typically PPs or AdvPs. (13) A: [...] they got men and women in the same dormitory!
Conjunct. This NSU class is used to classify fragments introduced by conjunctions. (14) A: Alistair erm he X  X , he X  X  made himself coordinator.

Filler. Fillers are NSUs that fill a gap left by a previous unfinished utterance. (15) A: [...] t wenty two percent is er &lt; pause &gt; 2.1 The Corpus Study
The taxonomy of NSUs presented herein has been tested in a corpus study carried out using the dialogue transcripts of the BNC. The study, which we describe here briefly, supplies the data sets used in the ML experiments we will present in Section 4. and Ginzburg (2002). It was created by manual annotation of a randomly selected conversations between two dialogue participants, and 25 files are multi-party tran-scripts. The total of transcripts used covers a wide variety of domains, from free conver-sation to meetings, tutorials and training sessions, as well as interviews and transcripts of medical consultations. The examined subcorpus contains 14,315 sentences. Sentences in the BNC are identified by the CLAWS segmentation scheme (Garside 1987) and each unit is assigned an identifier number.
 subcorpus. These results are in line with the rates reported in other recent corpus studies of NSUs: 11.15% in (Fern  X  andez and Ginzburg 2002), 10.2% in (Schlangen and Lascarides 2003), 8.2% in (Schlangen 2005). 3 together with an additional class Other introduced to catch all NSUs that did not fall in any of the classes in the taxonomy. All NSUs that could be classified with the tax-onomy classes were additionally tagged with the sentence number of their antecedent utterance. The NSUs not covered by the classification only make up 1.2% (16 instances) of the total of NSUs found. Thus, with a rate of 98.8% coverage, the present taxonomy offers a satisfactory coverage of the data.
 assess the reliability of the annotation, a small study with two additional, non-expert annotators was conducted. These annotated a total of 50 randomly selected instances (containing a minimum of two instances of each NSU class as labeled by the expert annotator) with the classes in the taxonomy. The agreement obtained by the three annotators is reasonably good, yielding a  X  score of 0.76. The non-expert annotators were also asked to identify the antecedent sentence of each NSU. Using the expert annotation as a gold standard, they achieved 96% and 92% accuracy in this task.
Acknowledgment, which accounts for almost half of all NSUs found. This is followed in frequency by Short Answer (14.5%) and Plain Affirmative Answer (8%). CE is the most common class among the NSUs that denote questions (i.e., CE, Sluice, and Check
Question), making up 6.3% of all NSUs found. 2.2 Resolving NSUs: Theoretical Background and Implementation
The theoretical background we assume with respect to the resolution of NSUs derives from the proposal presented in Ginzburg and Sag (2001), which in turn is based on the theory of context developed by Ginzburg (1996, 1999).

NSUs X  X ncluding Short Answer, Sluice, and CE X  X ouched in the framework of Head-driven Phrase Structure Grammar (HPSG). They take NSUs to be first-class gram-matical constructions whose resolution is achieved by combining the contribution of the NSU phrase with contextual information X  X oncretely, with the current question under discussion ,or QUD , which roughly corresponds to the current conversational topic. 4 402 answer to an explicit wh -question, like the one shown in (16a). (16) a. A: Who X  X  making the decisions?
In this dialogue, the current QUD corresponds to the content of the previous utterance X  the wh -question Who X  X  making the decisions? Assuming a representation of questions as lambda abstracts, the resolution of the short answer amounts to applying this question to the phrasal content of the NSU, as shown in (16b) in an intuitive notation. sluicing, the current QUD is a polar question p? , where p is required to be a quan-instance: (17) a. A: A student phoned.

In the case of reprise sluices and CE, the current QUD arises in a somewhat less direct way, via a process of utterance coercion or accommodation (Larsson 2002; Ginzburg and Cooper 2004), triggered by the inability to ground the previous utterance (Traum 1994; (sub)utterance which the addressee cannot resolve. For instance, if the original utterance is the question Did Bo leave? in (18a), with Bo as the unresolvable sub-utterance, one possible output from the coercion operations defined by Ginzburg and Cooper (2004) is the question in (18b), which constitutes the current QUD , as well as the resolved content of the reprise sluice in (18a). (18) a. A: Did Bo leave?
The interested reader will find further details of this approach to NSU resolution and its extension to other NSU classes in Ginzburg (forthcoming) and Fern  X  andez (2006). (Ginzburg, Gregory, and Lappin 2001; Fern  X  andez et al., in press), which provides a pro-cedure for computing the interpretation of some NSU classes in dialogue. The system currently handles short answers, direct and reprise sluices, as well as plain affirmative answers to polar questions. SHARDS has been extended to cover several types of clarification requests and used as a part of the information-state-based dialogue system
CLARIE (Purver 2004b). The dialogue system GoDiS (Larsson et al. 2000; Larsson 2002) also uses a QUD -based approach to handle short answers. 3. Pilot Study: Sluice Reading Classification
The first study we present focuses on the different interpretations or readings that sluices can convey. We first describe a corpus study that aims at providing empirical evidence about the distribution of sluice readings and establishing possible correlations between these readings and particular sluice types. After this, we report the results of a pilot machine learning experiment that investigates the automatic disambiguation of sluice interpretations. 3.1 The Sluicing Corpus Study
We start by introducing the corpus of sluices. The next subsections describe the annota-tion scheme, the reliability of the annotation, and the corpus results obtained. were able to use an automatic mechanism to reliably construct our subcorpus of sluices.
This was created using SCoRE (Purver 2001), a tool that allows one to search the BNC using regular expressions. 404 consisting of just a wh -word). We distinguish between the following classes of bare sluices: what , who , when , where , why , how ,and which . Given that only 15 bare which were found, we also considered sluices of the form which N . Including which N , the corpus contains a total of 5,343 sluices, whose distribution is shown in Table 2.
 in the dialogue transcripts of the BNC. The sample was created by selecting all instances of bare how (50) and bare which (15), and arbitrarily selecting 100 instances of each of the remaining sluice classes, making up a total of 665 sluices.
 corpus. The inclusion of sufficient instances of the lesser frequent sluice types would have involved selecting a much larger sample. Consequently it was decided to abstract over the true frequencies to create a balanced sample whose size was manageable enough to make the manual annotation feasible. We will return to the issue of the true frequencies in Section 3.1.3. semantic categories X  X rawn from the theoretical distinctions introduced by Ginzburg and Sag (2001) X  X orresponding to different sluice interpretations. The typology reflects the basic direct/reprise divide and incorporates other categories that cover additional readings, including an Unclear class intended for those cases that cannot easily be classified by any of the other categories. The typology of sluice readings used was the following:
Direct. Sluices conveying a direct reading query for additional information that was explicitely or implicitly quantified away in the antecedent, which is understood without difficulty. The sluice in (19) is an example of a sluice with direct reading: It asks for additional temporal information that is implicitly quantified away in the antecedent utterance. (19) A: I X  X  leaving this school.

Reprise. Sluices conveying a reprise reading emerge as a result of an understanding problem. They are used to clarify a particular aspect of the antecedent utterance corre-sponding to one of its constituents, which was not correctly comprehended. In (20) the reprise sluice has as antecedent constituent the pronoun he , whose reference could not be adequately grounded. (20) A: What a useless fairy he was.

Clarification. As reprise, this category also corresponds to a sluice reading that deals with understanding problems. In this case the sluice is used to request clarification of the entire antecedent utterance, indicating a general breakdown in communication. The following is an example of a sluice with a clarification interpretation: (21) A: Aye and what money did you get on it?
Wh -anaphor. This category is used for the reading conveyed by sluices like (22), which are resolved to a (possibly embedded) wh -question present in the antecedent utterance. (22) A: We X  X e gonna find poison apple and I know where that one is.

Unclear. We use this category to classify those sluices whose interpretation is difficult to grasp, possibly because the input is too poor to make a decision as to its resolution, as in the following example: (23) A: &lt; unclear &gt;&lt; pause &gt; 3.1.2 Reliability. The coding of sluice readings was done independently by three dif-ferent annotators. Agreement was moderate (  X  = 0.59). There were important differ-ences among sluice classes: The lowest agreement was on the annotation of how (0.32) and what (0.36), whereas the agreement on classifying who was substantially higher (0.74).
 annotations. Two of the coders had worked more extensively with the BNC dialogue transcripts and, crucially, with the definition of the categories to be applied. Leaving the third annotator out of the coder pool increases agreement very significantly (  X  = 0.71).
The agreement reached by the more expert pair of coders was acceptable and, we believe, provides a solid foundation for the current classification. 3.1.3 Distribution Patterns. The sluicing corpus study shows that the distribution of read-ings is significantly different for each class of sluice. The distribution of interpretations is shown in Table 3, presented as row counts and percentages of those instances where 406 at least two annotators agree, labeled taking the majority class and leaving aside cases classified as Unclear .
 tations (a chi square test yields  X  2 = 438 . 53, p  X  0 . 001). The most common interpretation for what is Clarification, making up more than 65%. Why sluices have a tendency to be
Direct (68.7%). The sluices with the highest probability of being Reprise are who (84.4%), which (91.6), which N (78.8%), and where (62.2%). On the other hand, when (63.3%) and how (79.3%) have a clear preference for Direct interpretations.
 the overall frequencies of sluice types found in the BNC. Now, in order to gain a complete perspective on sluice distribution in the full corpus, it is therefore appropriate to combine the percentages in Table 3 with the absolute number of sluices contained in the BNC. The number of estimated tokens is displayed in Table 4.
 almost 70% of why sluices are Direct, the absolute number of why sluices that are Reprise exceeds the total number of when sluices by almost 3 to 1. Another interesting pattern revealed by this data is the low frequency of when sluices, particularly by comparison with what one might expect to be its close cousin, where . Indeed the Direct/Reprise splits are almost mirror images for when versus where . Explicating the distribution in
Table 4 is important in order to be able to understand among other issues whether we would expect a similar distribution to occur in a Spanish or Mandarin dialogue corpus; similarly, whether one would expect this distribution to be replicated across different domains.
 what cla 2,040 whichN rep 135 why dir 775 when dir 90 what rep 670 who dir 70 who rep 410 where dir 70 where rep 250 when rep 35 what dir 240 whichN dir 24 is invited to check a sketch of such an explanation for some of the patterns exhibited in
Table 4 in Fern  X  andez, Ginzburg, and Lappin (2004). 3.2 Automatic Disambiguation
In this section, we report a pilot study where we use machine learning to automatically disambiguate between the different sluice readings using data obtained in the corpus study presented previously. 3.2.1 Data. The data set used in this experiment was selected from our classified corpus of sluices. To generate the input data for the ML experiments, all three-way agreement instances plus those instances where there is agreement between the two coders with the highest agreement were selected, leaving out cases classified as Unclear. The total data set includes 351 datapoints. Of these, 106 are classified as Direct, 203 as Reprise, 24 as
Clarification, and 18 as Wh -anaphor. Thus, the classes in the data set have significantly skewed distributions. However, as we are faced with a very small data set, we cannot afford to balance the classes by leaving out a subset of the data. Hence, in this pilot study the 351 data points are used in the ML experiments with their original distributions. experiment we will present later on X  X nstances were annotated with a small set of features extracted automatically using the POS information encoded in the BNC. The annotation procedure involves a simple algorithm which employs string searching and pattern matching techniques that exploit the SGML mark-up of the corpus. The BNC was automatically tagged using the CLAWS system developed at Lancaster University (Garside 1987). The  X  100 million words in the corpus were annotated according to a list of these codes can be found in Burnard (2000). The BNC POS annotation process is described in detail in Leech, Garside, and Bryant (1994).
 tures can therefore not use any intonational data, which would presumably be a useful 408 source of information to distinguish, for instance, between question-and proposition-denoting NSUs, between Plain Acknowledgment and Plain Affirmative Answer, and between Reprise and Direct sluices.
 features and their values is shown in Table 5. Besides the feature sluice , which indicates the sluice type, all the other features are concerned with properties of the antecedent utterance. The features mood and polarity refer to syntactic and semantic properties of the antecedent utterance as a whole. The remaining features, on the other hand, features ( quant , deictic , proper n , pro , def desc , wh ,and overt ) are not annotated value if the element or construction in question appears in the antecedent and it matches the semantic restrictions imposed by the sluice type. For instance, when a sluice with value where for the feature sluice is annotated, the feature deictic , which encodes the presence of a deictic pronoun, will take value yes only if the antecedent utterance contains a locative deictic like here or there . Similarly the feature wh takes a yes value only if there is a wh -word in the antecedent that is identical to the sluice type. allows us to express, for instance, that the presence of a proper name is irrelevant to determining the interpretation of say a when sluice, although it is crucial when the sluice type is who . The feature overt takes no as value when there is no overt antecedent expression. It takes yes when there is an antecedent expression not captured by any expression defined by another feature.

Table 5. The automatic annotation procedure was evaluated against a manual gold standard, achieving an accuracy of 86%. 3.2.3 Baselines. Because sluices conveying a Reprise reading make up more than 57% baseline that always predicts the class Reprise. This yields a 42.4% weighted F-score.
We use the implementation of a one-rule classifier provided in the Weka toolkit. For each feature, the classifier creates a single rule which generates a decision tree where the root is the feature in question and the branches correspond to its different values. The leaves are then associated with the class that occurs most often in the data, for which that value holds. The classifier then chooses the feature which produces the minimum error. sluice . The classifier produces the one-rule tree in Figure 1. The branches of the tree correspond to the sluice types; the interpretation with the highest probability for each type of sluice is then predicted.
 sluice type and preferred interpretation that were discussed in Section 3.1.3. There, we pointed out that these correlations were statistically significant. We can see now that they are indeed a good rough guide for predicting sluice readings. As shown in Table 6, the one-rule baseline dependent on the distribution patterns of the different sluice types yields a 72.73% weighted F-score.
 performing 10-fold cross-validation. They are presented as follows: The tables show the recall, precision, and F-measure for each class. To calculate the overall performance of the algorithm, these scores are normalized according to the relative frequency of corresponding class and then dividing by the total number of datapoints in the data set. The weighted overall recall, precision, and F-measure, shown in boldface for each baseline in Table 6, is then the sum of the corresponding weighted scores. For each of the baselines, the sluice readings not shown in the table obtain null scores. 3.2.4 ML Results. Finally, the four machine learning algorithms were run on the data set annotated with the 11 features. Here, as well as in the more extensive experiment we will present in Section 4, we use the following parameter settings with each of the learners. Weka X  X  J4.8 decision tree learner is run using the default parameter settings.
With SLIPPER we use the option unordered , which finds a rule set that separates each class from the remaining classes using growing and pruning techniques and in our case yields slightly better results than the default setting. As for TiMBL, we run it using the modified value difference metric (which performs better than the default overlap metric ), and keep the default settings for the number of nearest neighbors ( k = 1) and feature weighting method ( gain ratio ). Finally, with MaxEnt we use 40 iterations of the default L-BFGS parameter estimation (Malouf 2002).
 although there are some significant differences amongst the learners. MaxEnt gives the lowest score (73.24% weighted F-score) X  X ardly over the one-rule baseline, and more than 8 points lower than the best results, obtained with Weka X  X  J4.8 (81.80% weighted F-score). The size of the data set seems to play a role in these differences, indicating that
MaxEnt does not perform so well with small data sets. A summary of weighted F-scores is given in Table 7. 410
Appendix A. The results yielded by MaxEnt are almost equivalent to the ones achieved with the one-rule baseline. With the other three learners, the use of contextual features improves the results for Reprise and Direct by around 5 points each with respect to the one-rule baseline. The results obtained with the one-rule baseline for the Clarification reading, however, are hardly improved upon by any of the learners. In the case of
TiMBL the score is in fact lower X 72.16 versus 78.70 weighted F-score. This leads us to conclude that the best strategy is to interpret all what sluices as conveying a Clarification reading.
 type, was not predicted by the one-rule baseline nor by MaxEnt, now gives positive results with the other three learners. The best result for this class is obtained with Weka X  X  J4.8: 80% F-score.
 root of the tree corresponds to the feature wh , which makes a first distinction between
Wh-anaphor and the other readings. If the value of this feature is yes , the class Wh-anaphor is predicted. A negative value for this feature leads to the feature sluice .The of the sluice types what , where , which ,and whichN in a way parallel to the one-rule baseline. Additional features are used for when , why ,and who . A Direct reading is predicted for a when sluice if there is no overt antecedent expression, whereas a Reprise reading is preferred if the feature overt takes as value yes . For why sluices the mood of the antecedent utterance is used to disambiguate between Reprise and Direct: If the antecedent is declarative, the sluice is classified as Direct; if it is non-declarative it is interpreted as Reprise. In the classification of who sluices three features are taken into account: quant , pro ,and proper n . The basic strategy is as follows: If the antecedent utterance contains a quantifier and neither personal pronouns nor proper names appear, the predicted class is Direct, otherwise the sluice is interpreted as Reprise. 3.2.5 Feature Contribution. Note that not all features are used in the tree generated by
Weka X  X  J4.8. The missing features are polarity , deictic ,and def desc . Although they don X  X  make any contribution to the model generated by the decision tree, examination of the rules generated by SLIPPER shows that they are all used in the rule set induced by this algorithm, albeit in rules with low confidence level. Despite the fact that SLIPPER uses all features, the contribution of polarity , deictic ,and def desc does not seem to be very significant. When they are eliminated from the feature set, SLIPPER yields very similar results to the ones obtained with the full set of features: 81.22% weighted F-score versus the 81.66% obtained before. TiMBL on the other hand goes down a couple of points, from 79.80% to 77.32% weighted F-score. No variation is observed with MaxEnt, which seems to be using just the sluice type as a clue for classification. 4. Classifying the Full Range of NSUs
So far we have presented a study that has concentrated on fine-grained semantic dis-tinctions of one of the classes in our taxonomy, namely Sluice, and have obtained very encouraging results X  X round 80% weighted F-score (an improvement of 8 points over a simple one-rule baseline). In this section we show that the ML approach taken can be successfully extended to the task of classifying the full range of NSU classes in our taxonomy.

Plain Acknowledgement and Check Question, and then, in Section 4.6, report on a follow-up experiment where all NSU classes are included. 4.1 Data
The data used in the experiments was selected from the corpus of NSUs following some simplifying restrictions. Firstly, we leave aside the 16 instances classified as Other in the corpus study (see Table 1). Secondly, we restrict the experiments to those NSUs whose antecedent is the immediately preceding utterance. This restriction, which makes the feature annotation task easier, does not pose a significant coverage problem, given that the immediately preceding utterance is the antecedent for the vast majority of NSUs (88%). The set of all NSUs, excluding those classified as Other , whose antecedent is the immediately preceding utterance, contains a total of 1123 datapoints. See Table 8. 412 classified as Plain Acknowledgment and Check Question. Taking the risk of end-ing up with a considerably smaller data set, we decided to leave aside these meta-communicative NSU classes given that (1) plain acknowledgments make up more than 50% of the subcorpus leading to a data set with very skewed distributions; (2) check questions are realized by the same kind of expressions as plain acknowledgments ( okay , right , etc.) and would presumably be captured by the same feature; and (3) a priori these two classes seem two of the easiest types to identify (a hypothesis that was confirmed after a second experiment X  X ee Section 4.6). We therefore exclude plain acknowledg-ments and check questions and concentrate on a more interesting and less skewed data set containing all remaining NSU classes. This makes up a total of 526 data points (1123  X  582  X  15). In Subsection 4.6 we shall compare the results obtained using this restricted data set with those of a second experiment in which plain acknowledgements and check questions are incorporated. 4.2 Features
NSU classification was identified. In particular three types of properties that play an important role in the classification task were singled out. The first one has to do with semantic, syntactic, and lexical properties of the NSUs themselves. The second one refers to the properties of its antecedent utterance. The third concerns relations between the antecedent and the fragment. Table 9 shows an overview of the nine features used. 4.2.1 NSU Features. A set of four features are related to properties of the NSUs. These are nsu cont , wh nsu , aff neg ,and lex . The feature nsu cont is intended to distin-guish between question-denoting ( q value) and proposition-denoting ( p value) NSUs.
The feature wh nsu encodes the presence of a wh -phrase in the NSU X  X t is primarily introduced to identify Sluices. The features aff neg and lex signal the appearance of particular lexical items. They include a value e(mpty) which allows us to encode the absence of the relevant lexical items as well. The values of the feature aff neg indicate the presence of either a yes or a no word in the NSU. The values of lex are invoked by the appearance of modal adverbs ( p mod ), factual adjectives ( f mod ), and prepositions ( mod ) and conjunctions ( conj ) in initial positions. These features are expected to be crucial to the identification of Plain/Repeated Affirmative Answer and Plain/Helpful Rejection on the one hand, and Propositional Modifiers, Factual Modifiers, Bare Modifier Phrases, and Conjuncts on the other.
 non-empty values. This option, however, leads to virtually the same results. Hence, we opt for a more compact set of features. This also applies to the feature aff neg . 4.2.2 Antecedent Features. We use the features ant mood , wh ant ,and finished to encode properties of the antecedent utterance. The first of these features distinguishes between declarative and non-declarative antecedents. The feature wh ant signals the presence of a wh -phrase in the antecedent utterance, which seems to be the best cue for classi-fying Short Answers. As for the feature finished , it should help the learners identify
Fillers. The value unf is invoked when the antecedent utterance has a hesitant ending (indicated, for instance, by a pause) or when there is no punctuation mark signalling a finished utterance. 4.2.3 Similarity Features. The last two features, repeat and parallel , encode similarity relations between the NSU and its antecedent utterance. They are the only numer-ical features in the feature set. The feature repeat , which indicates the appearance of repeated words between NSU and antecedent, is introduced as a clue to identify
Repeated Affirmative Answers and Repeated Acknowledgments. The feature parallel , on the other hand, is intended to capture the particular parallelism exhibited by Helpful
Rejections. It signals the presence of sequences of POS tags common to the NSU and its antecedent.

POS information encoded in the BNC mark-up. However, as with the feature mood in the sluicing study, some features like nsu cont and ant mood are high level features that do not have straightforward correlates in POS tags. Punctuation tags (that would correspond to intonation patterns in spoken input) help to extract the values of these features, but the correspondence is still not unique. For this reason the automatic 414 feature annotation procedure was again evaluated against a small sample of manually annotated data. The feature values were extracted manually for 52 instances ( the total) randomly selected from the data set. In comparison with this gold standard, the automatic feature annotation procedure achieves 89% accuracy. Only automatically annotated data is used for the learning experiments. 4.3 Baselines
We now turn to examine some baseline systems that will help us to evaluate the classification task. As before, the simplest baseline we can consider is a majority class baseline that always predicts the class with the highest probability in the data set. In the restricted data set used for the first experiment, this is the class Short Answer. The majority class baseline yields a 6.7% weighted F-score.
 error is aff neg . The one-rule baseline produces the one-rule decision tree in Fig-ure 3, which yields a 32.5% weighted F-score (see Table 10). Plain Affirmative Answer is the class predicted when the NSU contains a yes -word, Rejection when it contains a no -word, and Short Answer otherwise.

Running Weka X  X  J4.8 decision tree classifier with these features creates a decision tree with four rules, one for each feature used. The tree is shown in Figure 4.
 between question-denoting ( q branch) and proposition-denoting NSUs ( p branch). Not surprisingly, within the q branch the feature wh nsu is used to distinguish between capture the classes Conjunct, Propositional Modifier, Factual Modifier, and Bare Modi-fier Phrase. The e(mpty) value for this feature takes us to the last, most embedded node of the tree, realized by the feature aff neg , which creates a sub-tree parallel to the one-rule tree in Figure 3. This four-rule baseline yields a 62.33% weighted F-score. Detailed results for the three baselines considered are shown in Table 10. 4.4 Feature Contribution As can be seen in Table 10, the classes Sluice, CE, Propositional Modifier, and Factual
Modifier achieve very high F-scores with the four-rule baseline X  X etween 97% and 100%. These results are not improved upon by incorporating additional features nor by using more sophisticated learners, which indicates that NSU features are sufficient indicators to classify these NSU classes. This is in fact not surprising, given that the disambiguation of Sluice, Propositional Modifier, and Factual Modifier is tied to the presence of particular lexical items that are relatively easy to identify ( wh -phrases and certain adverbs and adjectives), whereas CE acts as a default category within question-denoting NSUs.
 features are used. These are Repeated Affirmative Answer, Helpful Rejection, Repeated
Acknowledgment, and Filler. Because they are not associated with any leaf in the tree, they yield null scores and therefore don X  X  appear in Table 10. Examination of the confusion matrices shows that around 50% of Repeated Affirmative Answers were classified as Plain Affirmative Answers, whereas the remaining 50% X  X s well as the overwhelming majority of the other three classes just mentioned X  X ere classified as
Short Answer. Acting as the default class, Short Answers achieves the lowest score: 63.09% F-score.
 finished ), as a next step these were added to the NSU features used in the four-rule tree. When the antecedent features are incorporated, two additional NSU classes are predicted. These are Repeated Acknowledgment and Filler, which achieve rather 416 positive results: 74.8% and 64% F-score, respectively. We do not show the full results obtained when NSU and antecedent features are used together. Besides the addition of these two NSU classes, the results are very similar to those achieved with just NSU features. The tree obtained when the antecedent features are incorporated to the NSU tree in Figure 5. As can be seen in Figure 5, the features ant mood and finished con-tribute to distinguish Repeated Acknowledgment and Filler from Short Answer, whose F-score consequently rises, from 63.09% to 79%, due to an improvement in precision.
Interestingly, the feature wh ant does not have any contribution at this stage (although weighted F-score obtained when NSU and antecedent features are combined is 77.87%. A comparison of all weighted F-scores obtained will be shown in the next section, in Table 11.
 for Repeated Affirmative Answer and Helpful Rejection, which obtain null scores. 4.5 ML Results
In this section we report the results obtained when the similarity features are included, thereby using the full feature set, and the four machine learning algorithms are trained on the data.
 niques, they all yield very similar results: around an 87% weighted F-score. The max-imum entropy model performs best, although the difference between its results and those of the other algorithms is not statistically significant. Detailed recall, precision, and F-measure scores are shown in Appendix B.
 features yields a 62.33% weighted F-score, whereas the incorporation of antecedent features yields a 77.83% weighted F-score. The best result, the 87.75% weighted F-score obtained with the maximal entropy model using all features, shows a 10% improvement over this last result. As promised, a comparison of the scores obtained with the different baselines considered and all learners used is given in Table 11.
 90%). In the three baselines considered, Short Answer acts as the default category.
Short Answer when only NSU features are used is  X  47%. When antecedent features are incorporated precision goes up to  X  72%. Finally, the addition of similarity features raises the precision for this class to  X  82%. Thus, by using features that help to identify other categories with the machine learners, the precision for Short Answers is improved by around 36%, and the precision of the overall classification system by almost 33%: from 55.90% weighted precision obtained with the four-rule baseline, to the 88.41% achieved with the maximum entropy model using all features.
 Repeated Affirmative Answer and Helpful Rejection are predicted by the learners.
Although this contributes to the improvement of precision for Short Answer, the scores yielded by these two categories are lower than the ones achieved with other classes. Re-peated Affirmative Answer achieves nevertheless decent F-score, ranging from 56.96% with SLIPPER to 67.20% with MaxEnt. The feature wh ant , for instance, is used to distinguish Short Answer from Repeated Affirmative Answer. Figure 6 shows one of the sub-trees generated by the feature repeat when Weka X  X  J4.8 is used with the full feature set.
 39.92% F-score for this class. The maximal entropy model, however, yields only a 10.37%
F-score. Examination of the confusion matrices shows that were classified as Rejection,  X  26% as Short Answer, and  X  edgement. This indicates that the feature parallel , introduced to identify this type of NSUs, is not a good enough cue.
 418 4.6 Incorporating Plain Acknowledgment and Check Question
As explained in Section 4.1, the data set used in the experiments reported in the previous section excluded the instances classified as Plain Acknowledgment and Check Question in the corpus study. The fact that Plain Acknowledgment is the category with the highest probability in the subcorpus (making up more than 50% of our total data set X  X ee performance of the learners by inflating the results. Therefore it was left out in order to work with a more balanced data set and to minimize the potential for misleading results. As the expressions used in plain acknowledgments and check questions are very similar and they would in principle be captured by the same feature values, check
Acknowledgment and Check Question were incorporated to measure their effect on the results. In this section we discuss the results obtained and compare them with the ones achieved in the initial experiment.
 aff neg . This value is invoked to encode the presence of expressions typically used in plain acknowledgments and/or check questions ( mhm , right , okay , etc.). The total data set (1,123 data points) was automatically annotated with the features modified in this way, and the machine learners were then run on the annotated data. 4.6.1 Baselines. Given the high probability of Plain Acknowledgment, a simple majority class baseline gives relatively high results: 35.31% weighted F-score. The feature with the minimum error used to derive the one-rule baseline is again aff neg , this time with the new value ack as part of its possible values (see Figure 7). The one-rule baseline yields a weighted F-score of 54.26%.
 of 67.99%. In this tree the feature aff neg is now also used to distinguish between CE and Check Question. Figure 8 shows the q branch of the tree. As the last node of the four-rule tree now corresponds to the tree in Figure 7, the class Plain Affirmative Answer is not predicted when only NSU features are used.

Acknowledgments, and Fillers are predicted, obtaining very similar scores to the ones achieved in the experiment with the restricted data set. The feature ant mood is now also used to distinguish between Plain Acknowledgment and Plain Affirmative Answer.
The last node in the tree is shown in Figure 9. The combined use of NSU features and antecedent features yields a weighted F-score of 85.44%. 4.6.2 ML Results. As in the previous experiment, when all features are used the results obtained are very similar across learners (around 92% weighted F-score), if slightly lower with Weka X  X  J4.8 (89.53%). Detailed scores for each class are shown in Appen-dix C. As expected, the class Plain Acknowledgment obtains a high F-score ( all learners). The F-score for Check Question ranges from 73% yielded by MaxEnt to 90% obtained with SLIPPER. The high score of Plain Acknowledgment combined with its high probability raises the overall performance of the systems almost four points over the results obtained in the previous experiment: from
F-score. The improvement with respect to the baselines, however, is not as large: we now obtain a 55% improvement over the simple majority class baseline (from 35.31% to 92.21%), whereas in the experiment with the restricted data set the improvement with respect to the majority class baseline is 81% (from 6.67% to 87.75% weighted F-score.). experiment.
 is slightly higher than before (due to the reasons mentioned previously), the scores for some NSU classes are actually lower. The most striking cases are perhaps the classes
Helpful Rejection and Conjunct, for which the maximum entropy model now gives null scores (see Appendix C). We have already pointed out the problems encountered with
Helpful Rejection. As for the class Conjunct, although it yields good results with the other learners, the proportion of this class (0.4%, 5 instances only) is now probably too low to obtain reliable results.
 more than 10 points (from 93.61% to 82.42% F-score). The tree in Figure 7 provides a clue to the reason for this. When the NSU contains a yes -word (second branch of the tree) the class with the highest probability is now Plain Acknowledgment, instead of Plain Affirmative Answer as before (see tree in Figure 3). This is due to the fact that, 420 at least in English, expressions like yeah (considered here as yes -words) are potentially ambiguous between acknowledgments and affirmative answers. the problems it entails are also noted by Schlangen (2005), who addresses the problem of identifying NSUs automatically. As he points out, the ambiguity of yes -words is one of the difficulties encountered when trying to distinguish between backchannels (plain acknowledgments in our taxonomy) and non-backchannel fragments. This is a tricky problem for Schlangen as his NSU identification procedure does not have access to the context. Although in the present experiments we do use features that capture contextual information, determining whether the antecedent utterance is declarative or interrogative (which one would expect to be the best clue to disambiguate between Plain
Acknowledgement and Plain Affirmative Answer) is not always trivial. 5. Conclusions
In this article we have presented results of several machine learning experiments where we have used well-known machine learning techniques to address the novel task of classifying NSUs in dialogue.
 out using the dialogue transcripts of the BNC, and then sketched the approach to NSU resolution we assume.
 taxonomy. We analyzed different sluice interpretations and their distributions in a small corpus study and reported on a machine learning experiment that concentrated on the task of disambiguating between sluice readings. This showed that the observed correlations between sluice type and preferred interpretation are a good rough guide for predicting sluice readings, which yields a 72% weighted F-score. Using a small set of features that refer to properties of the antecedent utterance, we were able to improve this result by 8%.
 in the sluicing experiment to the full range of NSU classes in our taxonomy. In order to work with a more balanced set of data, the first run of this second experiment was carried out using a restricted data set that excluded the classes Plain Acknowledgment the NSUs, their antecedents and relations between them, and employed a series of simple baseline methods to evaluate the classification task. The most successful of these the NSUs themselves. This gives a 62% weighted F-score. Not surprisingly, with this baseline very high scores (over 95%) could be obtained for NSU classes that are defined in terms of lexical or construction types, like Sluice and Propositional/Factual Modifier. and improved the result of the four-rule baseline by 25%, obtaining a weighted F-score of around 87% for all learners. The experiment showed that the classes that are most difficult to identify are those that rely on relational features, like Repeated Affirmative Answer and especially Helpful Rejection.

Acknowledgment and Check Question in the data set and ran the machine learners again. The results achieved are very similar to those obtained in the previous run, if slightly higher due to the high probability of the class Plain Acknowledgment. The experiment did show however a potential confusion between Plain Acknowledgment and Plain Affirmative Answer (observed elsewhere in the literature) that obviously had not shown up in the previous run.
 identifying the correct NSU class is a necessary step towards the goal of fully processing
NSUs in dialogue. Our results show that, for the taxonomy we have considered, this task can be successfully learned.
 is the choice of features employed to characterize the utterances. In this case we have opted for rather high-level features instead of using simple surface features, as is com-mon in robust approaches to language understanding. As pointed out by an anonymous reviewer, it would be worth exploring to what extent the performance of our current approach could be improved by incorporating more low-level features, for instance by the presence of closed-class function words.
 volves other tasks that have not been addressed in this article and that are subjects of our future research. For instance, we have abstracted here from the issue of distinguishing
NSUs from other sentential utterances. In our experiments the input fed to the learners was in all cases a vector of features associated with an utterance that had already been singled out as an NSU. Deciding whether an utterance is or is not an NSU is not an easy task. This has for instance been addressed by Schlangen (2005), who obtains rather low scores (42% F-measure). There is therefore a lot of room for improvement in this respect, and indeed in the future we plan to explore ways of combining the classification task addressed here with the NSU identification task.
 order to actually resolve them, however, the output of the classifier needs to be fed into some extra module that takes care of this task. A route we plan to take in the future is to integrate our classification techniques with the information state-based dialogue system prototype CLARIE (Purver 2004a), which implements a procedure for NSU resolution based on the theoretical assumptions sketched in Section 2.2. The taxonomy which we have tested and presented here will provide the basis for classifying NSUs in this dialogue processing system. The classification system will determine the templates and procedures for interpretation that the system will apply to an NSU once it has recognized its fragment type. 422 Appendix A: Detailed ML Results for the Sluice Reading Classification Task Appendix B: Detailed ML Results for the Restricted NSU Classification Task 424 Appendix C: Detailed ML Results for the Full NSU Classification Task Acknowledgments References 426
