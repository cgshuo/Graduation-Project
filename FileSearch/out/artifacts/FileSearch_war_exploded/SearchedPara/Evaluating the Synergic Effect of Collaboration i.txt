 It is typically expected that when people work together, they can often accom plish goals that are difficult or even impossible for individuals. We consider this notion of the group achieving more than the sum of all individuals X  achievements to be the synergic effect in collaboration. Similar expectation exists for people working i n collaboration for information seeking tasks. We, however, lack a methodology and appropriate evaluation metrics for studying and measuring the synergic effect. In this paper we demonstrate how to evaluate this effect and discuss what it means to various collaborative information seeking (CIS) situations. We present a user study with four different conditions: single user, pair of users at the same computer, pair of users at different computers and co-located, and pair of users remotely located. Each of th ese individuals or pairs was given the same task of information seeking and usage for the same amount of time. We then combined the outputs of single independent users to form artificial pairs, and compared against the real pairs. Not surprisingly, partici pants using different computers (co -located or remotely located) were able to cover more information sources than those using a single computer (single user or a pair). But more interestingly, w e found that real pairs with their own computers (co-located o r remotely located) were able to cover more unique and useful information than that of the artificially created pairs. This indicates that those working in collaboration achieved something greater and better than what could be achieved by adding independen t users, thus, demonstrating the synergic effect. Remotely located real teams were also able to formulate a wider range of queries than those pairs that were co-located or artificially created. This shows that the collaborators working remotely were able t o achieve synergy while still being able to think and work independently. Through the experiments and measurements presented here, we have also contributed a unique methodology and an evaluation metric for CIS.
 H.3: INFOR MATION STORAGE AND RETRIEVAL H.3.3: Information Search and Retrieval : Search process ; H.5.3 [ Information Interfaces and Presentation ]: Group and Organization Interfaces  X  Collaborative computing, Computer -supported cooperative work Experimentat ion; Human Factors; Measurement Collaborative information seeking; Synergic effect; Evaluation. It is often argued that people should work together to solve hard problems (e.g., [1,2]). Information seeking is, mistakenly, not seen as a tough problem that could be effectively solved through collaboration [ 9,10 ]. Instead, IR researchers tend to spend their efforts on improving the systems that could boost an individual X  X  search effectiveness . While this approach has achieved remarkable f eat as evident by systems, theories, and their effects reported in the literature, including that of this forum, it is time to revisit the notion of collaboration and social/int eractive elements of searching. In this paper we show how and why people workin g in collaboration for an information -seeking task could do far better than those working individually. More specifically, we look at the notion of synergy, where the whole becomes more than the sum of all, and demonstrate that an appropriate setup of coll aborative information seeking (CIS) could lead to covering information that could otherwise not be discovered by individuals. We also show how remotely located collaborators achieve synergy by exercising independence and diversity.
 In the next section, we review some of the relevant literature on CIS and collaboration in general focusing on two specific aspects: synergy and evaluation. Our specific research questions and hypotheses are also included in this section. To address these questions and specifica lly the hypotheses, we conducted a laboratory study with 10 single users, and 30 pairs of users in three different setups. The details of this study are given in Section 3. Following that, we describe what and how we evaluated in Section 4. Using these eva luation measures, we present the results and relevant discussion in Section 5. The paper is concluded in Section 6 with implications and directions for further research.
 A brief overview of some of the related research is presented here to situa te our work in the context of IR in general and CIS in particular. This review will also inform our research questions and hypotheses listed later in this section. Synergy, from the Greek synergos (which means working together) is when two or more things, individuals, disciplines, etc. interact producing a result that is greater than the sum of the individual products of the involved parts [1]; this is commonly explained through the expression 1+1&gt;2 . Synergy has bee n widely studied in a variety of contexts, which includes , chemistry, medicine, organizations , and education [1]. In the particular context of collaborative information seeking (CIS), little is known about the synergic effect a nd to what extent, if any, it affects users X  behaviors, the process es, and the results produced by individuals searching information in collaboration , in comparison to those working individually. Furthermore, collaboration has been conceptually defined in different ways and to date there is no clear difference between it and related concepts such as cooperation [21]. Authors such as Fidel et al. [4]; Golovchinsky et al. [9], and Shah [ 21] have further defined and explored the conceptual implications of coll aboration in order to have a more suitable definition for CIS. Unlike the study of information seeking performed by single users, little is known about how users seek information in collaboration with others. During the past decade, some have explored dif ferent aspects of CIS in both naturalistic and experimental settings. The focus, however, has been on describing users X  behaviors as well as the information seeking process es of teams. For example, Hyldegard [12, 13 ] studied the applicability of Kuhlthau X  X  information search process [15] in the context of groups. Similarly, Shah &amp; Gonz X lez -Ib X  X ez [22] attempted to map the stages in Kuhlthau X  X  model to collaborative information seeking. Both studies revealed that even though some stages of this model may app ly to CIS, they do not cover the social dimension of CIS.
 More importantly, it has been recognized that collaborative search is a phenomenon widely present in social context where collaboration -in a more general sense -is required. For example, Morris [17] conducted a survey in a large company that revealed that workers engage in a variety of activities in which they collaborate searching for information. Through this survey, the author also identified obstacles that impact t he collaborative information seeking process, different tasks that motivate collaborative search, and common methods that are employed to share search results during the process.
 Specific studies have been conducted with the aim of comparing individual and collaborative search. For instance, Joho, Hannah, &amp; Joemon [14] compared the search process of single users with collaborative -concurrent search. Through this study, the authors found that those working collaboratively were able to reduce overlapping i n t erms of the webpages covered during a recall -oriented search task; however, as the authors pointed out, this redundancy reduction did not improve the retrieval effectiveness. In a related study, Pickens et al. [19] and Shah et al. [23] found that collabora tive search, with algorithmic mediation to enhance the collaboration proce ss among participants, end up with better res ults than those obtained by merging of single users X  results. Similarly, Foley [7] demonstrate d that in order to enhance the performance in synchronous collaborative information retrieval (SCIR), it is necessary to have an appropriated division of labor and also a mediated support for sharing knowledge. On a more theoretical side , Gonz X lez -Ib X  X e z [10] proposed a conceptual map to study the behavior of users in the information seeking process of both individuals and collaborative teams. As the author suggested, such a map and the use of mixed methods would help to understand different dimensions involved in CIS, i ncluding the synergic effect of collaboration.
 Though early studies have suggested the importance of providing support for collaboration in situations involving information search practices [25] , and many have devised tools and technologies to facilitate CIS [16, 18, 26 ]; it has been recognized by several authors (e.g., [5, 20]) that there is a significant lack of theories and tools to investigate and support CIS . Moreover, to our knowledge, beyond the coverage of these and other works, none have studied specifically the synergic effect in co llaborative information seeking, which is one of the core values and advantages of collaboration. From reviewing the literature, it is clear that (1) while collaboration is often conside red a useful approach to solve a complex problem, it has been largely ignored when it comes to information seeking; (2) system -focused CIS works have primarily looked at creating algorithms and tools that provide improved effectiveness in collaboration, bu t ignore the actual user interaction and social component; and (3) in terface -focused CIS works have predominantly studied various usability issues with the interface/system in a CIS situation. These explorations have not addressed some of the basic questio ns about working in collaboration, including the following that we are interested in investigating here . To narrow our investigation for better articulation of our goals and experiments, we propose the following hypotheses. Note that they are postulated for a recall -oriented exploratory search task. The following section describes a user study that we designed to test these hypotheses and address the research questions listed before. We conducted a laboratory study involving a total of 70 particip ants  X  10 participants as single users, and 6 0 participants as collaborative teams . This section describes the study procedure, the subjects, the system, the task, and the experimental conditions. Participants in this study were asked to sign up in pairs with someone with whom they had previous experience collaborating. In addition, they were informed of their compensation for participating in the study, which consisted of $10 per person and the possibility to obtain additional prizes if they were among the three best performing teams ($50, $25, and $15 per person additionally) at the end of the study. Overall, 60 participants in 30 pairs , all of them students from Rutgers University , were recruited and randomly assigned to th ree experimental scena rios (Table 1 ). In addition, 10 more participants were recruited to work individually in the same search task performed by teams. We developed a plugin for the Firefox web browser , called Coagmento, 1 which provided appropriate tools and support for the participants working in various conditions. A screenshot of this plugin within Firefox is shown in Figure 1. As shown, the plugin included a toolbar and a sidebar. The toolbar had the following Publicly available from http://www.coagmento.org . buttons: (1) Home  X  for taking the participant to appropr iate questionnaires, (2) Bookmark  X  for bookmarking a webpage, (3) Snip  X  for collecting a snippet using highlighted text from a webpage, and (4) Editor  X  for accessing a shared editor for writing the report.
 The sidebar had two major components: a chat -box and a resources panel. The chat -box allowed the collaborators in a given team to communicate with each other. The researcher conducting the study also used it to provide instructions to the participants. The resources panel included tabs for bookmarks, s aved snippets, and executed queries. Each experimental session lasted less than an hour and was structure d in six parts as depicted in Figure 2 and described below. The researcher conducting the study communicated with the participants through the chat -box at different times during the study instructing them to start/stop the task or fill in a questionnaire. To study the difference between individual information seeking and CIS, as well as to understand how various CIS settings can affect a collaborative team X  X  effectiveness in accomplishing an information -seeking task, we conducted experiments with four different conditions: single participants, two participants at the same computer, two participants in the same room but different computers, and two participants in different rooms with individual computers . In order to have a baseli ne to study the synergic effect of collaboration, we artificially created pairs of users from C1 (single users). We generated all possible combinations of pairs in groups of 5, reaching a total of 49 groups and creating 245 artificial teams in total. This was done in order to cover all possible pairs of users while avoiding a given user appear ing in more than one team within the same group of teams.
 These five condit ions are summarized in Table 1. Setups for four of these condition s are also depicted in Fig ure 3 . Note that in the real experiment, those in C5 condition were located in different rooms separated by walls, and not just a partition. T hey could not see or talk to each other directly, and the only communication channel they had was the text -box pro vided with the system. We chose  X  X ulf oil spill X  as t he topic for this experimentation since it was quite popular and relevant at the time the study was being conducted. Our preliminary investigations, including a few pilot runs, indicated that there was a huge amount of material on this topic, and that the participants would find it interesting and challenging enough as an exploratory search task. Each participant was given the following task description.  X  X  leading newspaper has hired your team to create a comprehensive report on the causes, effects, and co nsequences of the recent gulf oil spill. As a part of your contract, you are required to collect all the relevant information from any available online sources that you can find.
 To prepare this report, search and visit any website that you want and look f or specific aspects as given in the guideline below. As you find useful information, highlight and save relevant snippets. Make sure you also rate a snippet to help you in ranking them based on their quality and usefulness. Later, you can use these snippets to compile your report, no longer than 200 lines, as instructed. Your report on this topic should address the following issues: description of how the oil spill took place, reactions by BP as well as various government and other agencies, impact on econo my and life (people and animals) in the gulf, attempts to fix the leaking well and to clean the waters, long -term implications and lessons learned. X  The participants saw this description on the screen (phase 4 in the study), and were also given a printed c opy to refer to during their session.

Figure 3: Experimental setups for four different conditions. In order to evaluate the effectiveness of the participants in various conditions, we employed a number of traditional and non -traditional evaluation measures , which are presented below. Here we also describe other useful constructs and definitions that will later be used while reporting and discussing the results. In order to compute quantities such as coverage, we needed a universal set of webpages. Given that the search domain for our experiments was the open web, we needed a more confined set that we could use to compare with. We decided to take the union of all the webpages visited by all of our participants (total 70). In other words, the universe of webpages was defined by combining the visited webpages of each participant/team in every condition. Here, Coverage(t) is the coverage (webpages visited) by participant /team t . This corresponds to the web pages that participants either bookmarked using the toolbar or from where one or more snippets were collected . Once again, we took the union of all such webpages by each participant/team to form a universe of relevant webpages.
 Here, RelevantCoverage(t) is the set of webpages that participant/team t visited and found as relevant. Two of the most common evaluation measures in IR are precision and recall, which for our purpose here, are defined as the following.
 To combin e precision and recall into one measure of effectiveness, we use the traditional formulation of F -measure as defined below. We defined coverage of a given team/participant as the total number of distinct webpages visited within the universe of webpages.
 Cov erage ( t ) = { wp i : wp i was visited by t ! wp i " U } ... ( 6 ) We also considered a particular region of the coverage of teams/participants that was unique within the universe. We called such region unique coverage, which consists of all webpages within the coverage of a given team/pa rticipant t that were visited only by t . We defined relevant coverage as the region of coverage of a given team/participant that intersects with the universe of relevant webpages . In a simila r way, we called unique relevant coverage to the set of webpages within the unique coverage of a given team/participant that intersects with the universe of all relevant webpages. In addition to relevance, we also stu died the usefulness of webpages that teams/participants visited during the task. We used an implicit measure based on the dwell time o n a webpage as described in [27], which is su pported by previous findings [8]. As reported in these prior works, we considered a webpage to be useful if a team/user spent at least 30 seconds on it. Using the log data, we computed dwell time on a given webpage by a user, and if it was greater than or equal to 30 second s, marked it as useful for that user. Note that we only considered content pages, discounting any search engine homepage or search engine results pages ( SERPs ). To evaluate effectiveness of a team/participant in discovering hard to find information, we devised a new measured, called likelihood of discovery . We assume that webpages with a hig h likelihood are easier to find and are common among the majority of the users. On the other hand, those webpages with a low likelihood are diffi cult to reach and probably beyond the first results page of search engines. A participant/team finding these webpages are being more effective in discovering information that is not just relevant, but also diverse.
 In order to operationalize this idea , we used a formulation similar to that of inverse document frequency (IDF) . Using the frequency of each webpage in our log data , we computed its likelihood to be visited; in addition, we multiplied each webpage X  X  likelihood by -1 in order to denote the IDF . As a result, each webpage was assigned with a normalized value between -1 and 0 . In this sense, those webpages with a value close to 0 are rare (and even unique) to be reached by teams/participants, while those close to -1 are more likely to be visited. In addition to the sources that teams/participants visited during the tasks, we also studied how they approached the task in terms of the queries they issued to find information. We studied how similar or different were the queries formulate d by participants in real teams and artificial teams.
 In order to evaluate query diversity, we used Lavenshtein distance to compute the distance between pairs of queries for each real team and also for all combinations of users in artificial teams. Based o n the results of this computation, for a given pair of queries; the closer the distance to 0 the higher the similarity between them. On the other hand, the higher the distance between queries, more different (therefore diverse) were the queries formulated within a team. To study if collaboration has some negative implications for users in terms of cognitive load experienced ; we asked our participants to respond a questionnaire after finishing their task. This questionnaire was a simplified v ersion of NASA X  X  Task Load indeX (TLX) [6]. 2 This instrument had the following questions. All these questions were responded using a 5 -point Likert scale, where higher values in the responses indicate a more negative perception o f the user with respect to of the areas considered in the above questions. Taken from http://www.cc.gatech.edu/classes/AY2 005/cs7470 fall/papers/manual.pdf In this section we present our results and their related discussion s. To facilitate this, we first provide a summary of all the universal sets used in Table 2 . These sets were used to compute other constructs, such as precision, recall, and unique relevant coverage. Note that the numbers in this table represent the combined output of all the 70 participants. Table 2 : Summary of various universes used in our an alysis. Universe of all webpages (U) 562 Universe unique webpages 377 Universe all relevant webpages (U r ) 228 Universe unique relevant webpages 159 The analyses reported in this section were done using one -way ANOVA. We tested for homogeneity of variance and performed appropriate post -hoc tests to measure the difference between the conditions. To begin our analysis, we first looked at simple precision and recall for each condition as defined in the previous sect ion. A summary of these measures is given in Table 3, with variance analysis in Table 4.
 We found no difference between any of the conditions for precision. This is not surprising considering how it was computed and that it was relatively easy to find rele vant results from the web, giving almost everyone a very high value for precision. This high precision was also due to the fact that the relevant set was constructed using the union of the relevance judgments provided by each participant.
 Ta ble 3 : Relevanc e measures  X  means and standard deviations We did, however, find differences among the conditions for recall. Not surprisingly, the single users (C1) had lower recall compared to every other condit ion except C3, where two collaborators used the same computer. Similarly, C3 had lower recall than C2, C4, or C5. In other words, teams with individual computers for their collaborators were able to achieve higher recall than those with a single or a share d computer. This was expected since the assigned task was recall -oriented and exploratory in nature, and given the limited amount of time, those with more resources achieved more results. If the task was non -dividable (e.g., brainstorming), we may not have found these differences.
 Using F -measure, once again, we found that in real collaboration, those with individual computers (C4 and C5) outperformed those with shared computers (C3).
 Table 4 : Means differences between conditions (row -column) . 
Bold values correspond to statistical significant difference at While precision and recall correspond to relevance in traditional IR sense, they are not very appropriate in the present CIS setting given that the participan ts were searching the web, where a huge amount of information on the given topic existed, and that each participant/team was given the same amount of limited time, in which they could easily find a good amount of r elevant information. Given this, a more im portant and interesting aspect to investigate here is coverage  X  a measure of the amount of information explored. Tables 5 and 6 report various kinds of coverage quantities , including the analysis of variance between different conditions . Just as recall, w e found those with two people and two computers were able to cover more information than those with on ly one person and/or one workstation . To extend our investigation for coverage, we looked at unique coverage for each individual/team, which is defined as the set of unique webpages that one covered and others did not. We found that C4 and C5 came o ut on the top with this measure, indicating their effectiveness in covering information that others could not. In fact, C5 outperformed not only C1 and C3, but a lso C2. In other words, when two real collaborators worked in remote CIS, they were able to cover more unique information than artificially created pairs of collaborators. Given that C2 and C4 had no difference, we can say that there is a value in remote c ollaboration when the task has clear independent components, and at the same time, having interactions that one finds in a real collaboration helps over completely working independently as C2 participants did.
 We also looked at how much of the unique cover age was actually relevant, and found that C5 did better than C1 and C3. While we found no difference between C2 and C4 or C5, we can clearly see that those in C5 were able to get to the information that is not discovered by single users (C1), collaborators at the same computer (C3), or artificially created collaboration (C2). At the same time, the amount of unique relevant information that they discovered was found to be significantly more than what was found by C1 or C3. In other words, remotely located te ams were able to leverage real interactions (as opposed to no interactions in artificial teams), and at the same time carry on independent exploration to achieve the synergic effect through collaboration. Figure 4 provides a depiction of various forms of coverage for C2, C3, C4 , and C5. The figure is drawn to scale, with the area of a coverage region, proportion al to the value of that kind of coverage for a given condition. Visually also, we could see that C5 has more coverage (total and unique), as well as more unique relevant coverage. Moving beyond information exploration and relevancy, we looked at the usefulness of viewed information . As defined in the previous section, this referred to visiting webpages that one spends considerable amount of time (30 seconds or more). We found (Tables 7 and 8) that C1 and C3 visited significantly fewer useful webpages than those by C4 and C5 participants. More importantly, real teams with individual computers (C4 and C5) outperformed artificial teams (C2) when it came to visitin g useful webpages , including the webpages that were unique to a given team. In other words, with real collaboration, the teams were able to avoid overlapping their exploration and reach out to more sources of information. This is important for exploratory and recall-oriented search task like the one used for our study. Table 6: Means differences between conditions (row -column) . 
Bold values corres pond to statistical significant difference at Figure 4: Depiction of coverage by various conditions (drawn to scale). 
Table 7 : Usefulness of explored information  X  means and Table 8 : Means differences between conditions (row -column). 
Bold values correspond to statistical significant difference at In addition, we found that C4 and C5 participants visited many more difficult to reach (less likely visited) webpages than those in C1. This shows that collaboration, in this case, allowed a unit (a pair or users) get to the information that was otherwise ignored by those who worked alone. In fact, even when we combined two independent participants X  explorations (C2), those who collaborated while remotely located (C5) could discover more information. Once again, C5 participants leveraged on the interactions through real collaboration while exercising their independence and exploring individual information trails. To understand searching beh avior of various individuals and teams, we looked at their querying effectiveness. Given that the assigned task was exploratory in nature, that there was plethora of useful information on the web, and that the time was limited, it was important that the pa rticipants construct a variety of queries and cover as large a ground as they could. To measure this, we computed query diversity for each participant/team as d efined in the previous section (Table 9) . A general overview of query distances for each conditi on is given in Figure 5. As we can see, those in C4 had more queries with higher distances among them. In other words, those in C5 tried more diverse queries.

Table 9 : Query diversity  X  means and standard deviations for Figure 5 : Query distance for C2, C4, and C5 using Lavenshtein algorithm.
 Using ANOVA, we found (Table 10) that those in real collaborat ions with individual computers and remotely located (C5) had higher diversity in their queries than those co -located using different computers (C4) . C5 participants also exhibited a larger variety in their queries than those by artificially created teams (C2). Combining these two facts indicates that (1) participants remotely located were able to successfully divide the task up and explore unique information through different queries, and (2) these participants had a chance to work more independently than those in the same space . We believe that query simila rity for teams in C4 is due to space sharing, which may influence the way in which users formulate d their queries. Even though most teams split up the task, the physical closeness of users could enable them to hear what their peers think aloud; or even have brief conversations (facilitat ed by face-to-face interaction). This may have influenced implicitly common queries between those participants . column) . Bold values corres pond to statistical significant On the other hand, we think that users working remotely located and using only text chat for communicating with each other (C5) were more able to genera te different queries because they were less directly influenced by their peers during the task. Fidel et al. [5] showed that collaboration induces additional cognitive load, what they referred to as the collaborative load . Often, this is the price to pay for gaining the advantages of collaborating. We used NASA X  X  TLX instrument to measure user perceived cognitive load during the task (see the previous section). For analysis, we combined the responses obtained from six different questions and created an index, since these responses were found to be statistically reliable for this instrument. A summary of what we found for the four real conditions (C2 was artificially created) is given in Table 11.

Table 11 : Cognitive load  X  means and standard d eviations for column) . Bold values corres pond to statistical significant Using this index, we performed the ANOVA and found no difference between the four conditions (Table 12). This indicates that the particip ants who worked in collaboration (C3, C4, and C5) experienced no more cognitive load than what was reported by those in C1. In other words, the collaborators in our experiment (at least C4 and C5) were able to gain the advantages of creating synergy withou t additional mental load. Collaboration is often a useful approach for solving a complex problem, but it has its costs and overheads [6]. One typically gets involved in collaboration if it has good benefit -to-cost ration, or if the given problem is too difficult to be solved without collaboration [21]. It is, however, difficult to measure if and how a collaborative endeavor would pay off. Traditional objective or quantitative approaches used in IR are insufficient, and subjective or qualitative a pproaches may be expensive or difficult to employ. In this paper we proposed and demonstrated a unique framework for evaluating various aspects of collaborative information seeking (CIS), especially the synergic effect.
 Using a user study with 70 participa nts working on an information -seeking task in different setups, we showed how various evaluation measures could be defined and operationalized. Here we summarize our findings and link them back to the research questions (RQ) and hypotheses (HT) listed in S ection 2.2. We first argued that the traditional measures of evaluating relevance are not appropriate for such situations, and proposed either modified or new kinds of evaluations. These included coverage, usefulness, likelihood of discovery, and query di versity. We believe this itself is an important contribution to the community, helping the researchers evaluate and design CIS systems and interfaces (RQ4). The results of the experiments reported here indicated that in a recall -oriented exploratory search task, two collaborators working at the same computer achieve similar results to the individual users (RQ1, HT1). It also became clear that those in remote collaboration were able to work more independently than those that were co -located (H T2). Independen ce is considered an important characteristic of a successful collaboration [24 , 21 ]. Our results also provided a strong support for synergic effect in remotely located collaborators. In particular, we showed that two pe ople working in collaboration (C 4 and C5) is not the same as having the outcomes of two completely independent individuals combined (C2); they do better in terms of discovering more and diverse information for an information -seeking task. Not only that, but the cognitive load in a real collab orative situation was found to be no more than what was perceived by those workin g individually (RQ5). Thus, the synergic effect of the whole being greater than the sum of all was demonstrated and evaluated (RQ3, HT3).
 We also want to point out a few of th e limitations of our experiments. The study reported here was conducted with synchronous CIS task. The findings may be impacted if the collaborators were working asynchronously. Due to the nature of the laboratory study, we also gave limited amount of time to the participants. It has been shown that collaborations lasting longer and done over multiple sessions may produce significantly different results and user experiences [21 , 22 ]. Having more than two participants per team could also lead to different gr oup dynamics, influencing the results. Despite these limitations, w e believe the methodology and the evaluation measures proposed and demonstrated here could help us further investigations of CIS with different setups, including asynchronous, non -time boun d, multi -session , and non-dividable tasks, as well as collaborations that involve more than two participants. These contributions could be helpful for not just CIS, but IR in general. [1] Buckminster Fuller, R. (1975). SYNERGETICS [2] Corning, P. A. (1995). Synergy and self -organization in the [3] Denning, P. J. (2007). Mastering the mess. Communications [4] Denning, P. J., &amp; Yahol kovsky, P. (2008). Getting to  X  X e X  . [5] Fidel, R., Bruce, H., Pejtersen, A. M., Dumais, S. T., Grudin, [6] Fidel, R., Mark Pejtersen, A., Cleal, B., &amp; Bruce, H. (2004). [7] Foley, C. (2008). Division of Labour and Sharing of [8] Fox, S., Karnawat, K., Mydland, M., Dumais, S., &amp; White, [9] Golovchinsky, G., Pickens, J., &amp; Back, M. (2008). A [10] Gonz X lez -Ib X  X e z, R. (201 0). A proposal for studying users X  [11] Hart, S. G., &amp; Staveland, L. E. (1988). Development of [12] Hyldegard, J. (2006). Collaborative information behaviour -[13] Hyldegard, J. (2009). Beyond the search process -exploring [14] Joho, H., Hannah, D., &amp; Joemon , J. M. (2008). Comparing [15] Kuhlthau, C. C. (1991). Inside the Search Process: [16] Morris, M. R., &amp; Horvitz, E. (2007). SearchTogether: An [17] Morris, M. R. (2008). A survey of collaborative web search [18] Morris, M. R., Teevan, J., &amp; Bush, S. (2008). Enhancing [19] Pickens, J., Golovch insky, G., Shah, C., Qvarfordt, P., &amp; [20] Shah, C. (2009). Lessons an d Challenges for Collaborative [21] Shah, C. (2010). Working in Collaboration -What, Why, and [22] Shah, C., &amp; Gonzalez -Ibanez, R. (2010). Exploring [23] Shah, C. , Pickens, J., &amp; Golovchinsky, G. (2010). Role -[24] Surowiecki, J. (2004). Wisdom of Crowds : Why the Many [25] Twidale, M. B. T., Nichols, D. M. N., &amp; Paice, C. D. (1997). [26] Twidale, M. B., &amp; Nichols, D. M. (1996). Collaborative [27] White, R. W., &amp; Huang, J. (2010). Assessing the scenic 
