 In many domains of information retrieval, system estimates of document relevance are based on multidimensional qual-ity criteria that have to be accommodated in a unidimen-sional result ranking. Current solutions to this challenge are often inconsistent with the formal probabilistic framework in which constituent scores were estimated, or use sophis-ticated learning methods that make it difficult for humans to understand the origin of the final ranking. To address these issues, we introduce the use of copulas , a powerful sta-tistical framework for modeling complex multi-dimensional dependencies, to information retrieval tasks. We provide a formal background to copulas and demonstrate their effec-tiveness on standard IR tasks such as combining multidimen-sional relevance estimates and fusion of results from multi-ple search engines. We introduce copula-based versions of standard relevance estimators and fusion methods and show that these lead to significant performance improvements on several tasks, as evaluated on large-scale standard corpora, compared to their non-copula counterparts. We also inves-tigate criteria for understanding the likely effect of using copula models in a given retrieval scenario.
 Information Systems [ Information Retrieval ]: Retrieval models Relevance models; Multivariate relevance; Ranking; Proba-bilistic framework; Data fusion.
In response to user queries, today X  X  search systems typi-cally return lists of documents ranked by system estimates of relevance. In traditional IR retrieval models, each doc-ument X  X  relevance towards the query is expressed as term overlap between query and document [42]. Early on, re-searchers began exploring alternative, non-topical document quality criteria such as document recency, credibility or mon-etary cost. More recently, through a combination of im-proved algorithms and greatly increased data scale, signif-icant gains in ranking quality and user satisfaction based on employing non-topical factors such as textual complexity [12] or suitability for children [17] have begun influencing the ranking process. Given a scenario such as child-friendly information search, non-topical quality criteria can clearly have a strong influence on usefulness of a document for a specific user. A perfectly relevant document that is not understandable due its complex sentence structure or ex-cessive use of jargon will have significantly diminished user relevance.

Beyond the value of individual relevance factors, there can be complex, non-linear dependencies between relevance factors. For example, relevance criteria such as topicality and credibility might appear independent for some docu-ment subsets, but extreme values in one dimension may in-fluence the other in a way that is not easily captured by state-of-the-art approaches. As a concrete example, take TREC 2010 X  X  faceted blog distillation task [32], that aims at retrieving topically relevant non-factual blog feeds. Here, the relevance space has two dimensions: topicality and sub-jectivity. Figure 1 shows the distribution of relevance scores for Topic 1171,  X  mysql  X , across these two relevance dimen-sions. We can note an apparent correlation in the lower left part of the graph that weakens as scores increase. To under-line this, we computed Pearson X  X   X  between the two dimen-sions for the lower score third (  X  = 0 . 37), the upper region (  X  =  X  0 . 4), as well as the overall distribution (  X  = 0 . 18). Apparently, the dependency structure of the joint distribu-tion of relevance, in this case, is not easily described by a linear model. Consequently, we can expect dissatisfying per-formance of linear combination models. And, indeed, when inspecting the performance of a linear combination model with empirically learned mixture parameters  X  , Topic 1171 receives an average precision of only 0.14, well below the method X  X  average across all topics of 0.25. In the course of this work, we will discuss practical means of addressing cases like the present one and will finally revisit this example to demonstrate the effect of our proposed method.

While the machine learning, information retrieval, data mining and natural language processing communities have significant expertise in estimating topical document rele-vance and additional criteria in isolation, the commonly ap-plied combination schemes have tended to be ad hoc and ig-nore the problem of modeling complex, multi-dimension de-pendencies. In practice, they follow statically weighted lin-Figure 1: Distribution of bivariate relevance scores for TREC 2010 Blog Track Topic 1171,  X  X ysql X . ear combinations with empirically determined mixture pa-rameters [42] or deploy sophisticated learning to rank tech-niques that tend to offer only limited insight to humans about why they were weighted highly for relevance. Ideally, we would demand realistic, yet formally-grounded combina-tion schemes that can lead to results that are both effective and with human-interpretable justification.

In a different context, the field of quantitative risk man-agement has devised copulas , a flexible, varied class of prob-ability density functions that are designed to capture rich, non-linear dependencies efficiently in multi-dimensional dis-tributions. Copulas work by decoupling the marginal distri-butions of the data from the underlying dependency struc-ture of the joint distribution. In particular, copulas can account for so-called tail dependencies, i.e., dependencies that play up at the extreme values of the interacting distri-butions. As an example, let us consider two commodities traded on the stock market, such as rare earth metals and pork bellies. The two commodities are sufficiently differ-ent to make the related market segments quasi-independent. However, extreme market situations have been shown to cause investor panics that reach across otherwise indepen-dent segments and cause previously unseen interrelation-ships [9].

This work makes three contributions to the state of the art in relevance modelling. (1) We give a detailed introduc-tion to the formal framework of copulas and describe how to estimate them from empirical data. (2) Based on a number of sizeable standard data sets such as the Blogs08 collection [32], we demonstrate the merit of using copulas for multivari-ate relevance estimation. (3) In a related effort, we address the task of score fusion based on historic submissions to the TREC ad hoc task.
 The remainder of this paper is structured as follows: Sec-tion 2 gives a historic overview of IR relevance frameworks, prior work on multidimensional relevance models, score fu-sion approaches, as well as, examples of copula applications from different fields. Section 3 formally introduces the the-oretical foundation of copulas and details key techniques in their application. In Sections 4 and 5 we demonstrate their merit at the tasks of estimating multidimensional relevance scores as well as fusing prior TREC runs. Section 6 further discusses the experimental results and aims at identifying those domains of IR for which copulas are most promising. Section 7 concludes the paper with a concise summary of our findings.
Over the past decades, a wide range of partially over-lapping relevance frameworks have been proposed, a few prominent examples include [44, 22, 34, 8]. They unani-mously consider relevance as a complex, potentially multi-dimensional concept that may be composed from a number of constituents. In the further course of this section, we will focus on the practical implementation of formal relevance estimation schemes employed in information retrieval and related disciplines. Schamber et al. [45] radically revised the definition of relevance, causing a growing interest in proba-bilistic relevance modelling in the research community. First openly applied at the third TREC competition, the BM25 retrieval model [43] represents a performance landmark that is still valid today (with slight variations such as the 2004 integration of multiple weighted fields [42]). In 1996, Persin et al. [38] introduced the idea of retrieval result lists ranked by their probability of relevance, as an alternative to the previously dominant binary retrieval scenario. Two years later, Ponte and Croft proposed the use of language mod-elling techniques to determine topical relevance [39]. One of the first notions of non-topical relevance was expressed in Kleinberg X  X  work on hubs and authorities [26] in which the author introduces two document-specific relevance notions independent of the query. Lavrenko and Croft [28, 29] pur-sued a line of work on dedicated relevance models.
 While the formal combination of several individual relevance facets in one model has not been extensively studied, there has been an interesting thread of research on score fusion. The task is to combine the result rankings of multiple inde-pendent retrieval systems in order to compensate for local inaccuracies of single engines. Early approaches to the task were based on evidence aggregation in the form of products and sums of scores across individual systems [19]. The fused ranking is based on the absolute value of the cross-system ag-gregates. Vogt et al. [51] first introduced linear interpolation of multiple model rankings for system fusion. Aslam and Montague [5] proposed a probabilistic rank-based method for direct combination of multiple engines. Later on, they devised a similar method based on a majority voting scheme between various retrieval systems [35]. [36] proposed a score normalization scheme that is more robust to outliers in the distribution of relevance than the previously used min/max technique. There has been an extensive body of work on estimating the distribution of relevance scores for document ranking. Recent examples include the work by Arampatzis and Stephenson [4], Kanoulas et al. [25], and, Cummins [14]. Manmatha et al. [33] estimated a search engine X  X  score distri-bution as a mixture of normal and exponential distributions, for relevant and non-relevant documents respectively. They used the resulting distributions for score fusion across mul-tiple engines, but did not attempt to model dependencies in the joint score distribution, instead treating the scores as independent and averaging probabilities, or discarding  X  X ad X  engines altogether.
 In 2002, Wu and Crestani [52] introduced the first of what would become a group of fusion approaches that define an explicit weighting scheme under which the original result lists are combined. [7] and [15] employ various quality no-tions such as the degree to which a document satisfies a given relevance criterion to dynamically adapt the weight-ing scheme to the underlying distribution of relevance. In 2005, Craswell et al. investigated relevance model combi-nation by linearly combining constituent scores in the log domain [13]. Tsikrika and Lalmas applied Dempster-Shafer theory for the aggregation of independent relevance criteria in web retrieval in the form of belief functions [49]. Gerani et al. [21] propose non-linear score transformations prior to the standard weighted linear combination step. Their solid results demonstrate the need for models whose capabilities go beyond linear dependency structures between relevance dimensions.
 In recent years, the variety of IR applications has become significantly more diverse. As a consequence, universal rele-vance models have become less viable in many areas. Tasks such as legal IR, expert finding, opinion detection or the re-trieval of very short documents (e.g., tweets) have brought forward strongly customised relevance models tailored to-wards satisfying a given task (e.g., [24, 6]). Especially for the retrieval of structured (XML) documents, score combina-tion schemes are of central importance to combine evidence across multiple structural fields within a document. De-spite the numerous potential issues pointed out by Robert-son et al. [42], most state-of-the-art approaches to XML re-trieval rely on linear models [31]. An advance towards the formal combination of several independent relevance crite-ria in the form of prior probabilities for language models has been made by Kraaij et al. [27] for the task of entry page search. To date, however, most universally applica-ble relevance models still rely on pure linear combinations of relevance criteria that disregard the underlying data dis-tribution or potential dependencies between the considered dimensions.
 Learning to rank (L2R) has been established as an alterna-tive approach for signal combination. The aim is to apply machine learning methods to either directly infer a docu-ment ranking or a ranking function from a wide range of features, potentially including the previously-discussed rele-vance criteria [10, 40, 30]. The downside of this approach is that the resulting models tend to yield only limited insight for humans. The classic approach of developing a unify-ing formal retrieval model would in our view provide better means to increase not just overall performance, but also our qualitative understanding of the problem domain.
 By introducing copulas for information retrieval, this work proposes a way for closing the gap between linear com-binations (that break with the probabilistic framework in which the constituent scores were estimated) and non-linear machine-learned models (that offer only limited insight to scientists and users).
 Copulas have been traditionally applied for risk analyses in portfolio management [18] as well as derivatives pricing [9] in quantitative finance. Recently, however, there are several successful examples from unrelated disciplines. Renard et al. estimate water flow behaviour based on Gaussian copu-las [41]. Onken et al. apply copulas for spike count analy-sis in neuroscience [37]. In meteorology, copulas have been used to combine very high-dimensional observations for the task of climate process modelling [47]. To the best of our knowledge, there has been no prior application of the copula framework to information retrieval problems.
At this point, we will give a brief introduction of the gen-eral theoretical framework of copulas, before applying them to various IR tasks in subsequent sections. For a more com-prehensive overview, please refer to [46] for more detail and pointers to further reading.
 The term copula was first introduced by Sklar [48] to de-scribe multivariate cumulative distribution functions ( cdfs ) that allow for a formal decoupling of observations from de-pendency structures. Formally, given a k -dimensional random vector with continuous margins we can map our observations to the unit cube [0 , 1] k as This is where our copulas come into play. A k -dimensional copula C describes the joint cumulative distribution function of random vector U with uniform margins.
 This approach has two obvious practical benefits: (1) Sepa-rating marginals and dependency structure allows for more straightforward estimation or approximation of each com-ponent in isolation. (2) An explicit model of dependency is scale-invariant. The copula describes a reference case of dependency on the unit cube [0 , 1] k that can be applied to arbitrary random vectors without further adjustment. A number of key properties make copulas an appealing the-oretical framework for a wide number of applications, so we summarize those now.
Before applying the copula framework to problems in in-formation retrieval, let us visit a number of extreme con-ditions of dependency that frequently occur in IR scenar-ios. (1) Independence of observations is a frequently as-sumed simplification in IR theory that leads to convenient (if na  X   X ve) probabilistic models. In the copula framework, independence of events can be captured by the so-called in-dependence copula C indep : which is equivalent to the product across all constituent probabilities in U . (2) Co-monotonicity describes the case of perfect positive correlation between observations u : (3) counter-monotonicity of observations is given in the opposite case of perfect negative correlation: Consequently, each copula lies within the so-called Fr  X echet-H  X  offding bounds [23]:
After having covered the foundations of copula theory let us inspect some concrete examples of copulas that will be used in the course of this work. Three general families of standard copulas have been proposed in the literature, whose corresponding equations are given right after their introduc-tion in this paragraph: (1) Elliptical copulas are directly derived from known distributions and are based on stan-dard distribution functions such as the Gaussian distribution or Student X  X  t distribution. Equation 1 shows the Gaus-sian copula that requires the observed covariance matrix  X   X  R k  X  k as a parameter.  X  denotes the cdf of a standard normal distribution and  X   X  1 its inverse. (2) Archimedean copulas are popular as they can be explicitly stated (note that due to their distribution dependency that is not the case for elliptical copulas) and typically depend on only a single degree of freedom. The parameter  X  expresses the strength of dependency in the model. Equation 2 shows the Clayton copula whose  X  -range is [  X  1 ,  X  ) \ { 0 } .  X  =  X  1 represents counter-monotonicity,  X   X  0 gives the independence copula and  X   X   X  approaches co-monotonicity. Finally, (3) Ex-treme value copulas are robust in cases of extreme obser-vations. The Gumbel copula (Equation 3) has a parameter space of  X  in [1 ,  X  ). For  X  = 1 we obtain the independence copula, and, for  X   X  X  X  we approach co-monotonicity.
 Figure 3.2 shows contour plots of a number of bivariate stan-dard copulas. The concrete choice of copula family and instantiation has been frequently reported to depend on the application domain [46]. If no prior knowledge about the dependency structure, e.g., prevalence of asymptotic or tail dependencies, is available, practitioners often resort to goodness-of-fit tests or measures of tail dependency in order to choose an appropriate model. We will describe the use of these techniques in the subsequent sections when applying copulas for information retrieval problems.
In the case of elliptical copulas, the fitting process is lim-ited to calculating means and covariance matrices from the available observations. Here, the only degree of freedom is the concrete choice of distribution function (e.g., Gaussian vs. Student) that best approximates the original distribution that generated the observations. In the non-elliptical case, the task is to determine optimal settings of  X  . Commonly, this is achieved by means of maximum likelihood estimates based on the available observations. This is also the ap-proach chosen in this work. It should be noted that there are methods for direct empirical estimations of entire copula functions. The interested reader can find a good overview by Charpentier et al. [11] as a starting point for this line of re-search, the inclusion of which would however go beyond the scope of this initial exploration of copulas for information retrieval.
In the previous section, we described the theoretical foun-dations of copulas including concrete ways of computing C ( U ) from multivariate observations U . We now detail their application for relevance estimation in information retrieval. First, we separately estimate the probability of relevance P rel ( d ) and non-relevance P each of the k criteria (dimensions)  X  for example, topicality, recency, readability, etc. Next, we assume random observa-tions U rel and U non to derive from these distributions and base two distinct copulas, C rel and C non on them. Recall that these copulas should capture the dependencies between relevance criteria, in either the relevant ( C rel non-relevant ( C non ) documents retrieved. Since it is difficult to predict where these dependencies have the most effect, it is natural to consider three different general approaches of combining multivariate observation scores U into a single probability of relevance that can be used for resource rank-ing. (1) CPOS ( U rel ) multiplies the independent likelihood of observing U rel with the relevance copula C rel , capturing only dependencies between the likelihoods of relevance. (2) CNEG ( U rel ,U non ) normalizes the probability of relevance by the non-relevance copula C non ( U non ), capturing only the de-pendencies between the likelihoods of non-relevance. (3) CODDS ( U rel ,U non ), finally, multiplies the probability of rel-evance by the ratio of the two copulas, modelling simulta-neously the dependencies between both previous notions. (c) Gumbel copula with  X  = 2 . 0 .

As performance baselines, we will compare to three popu-lar combination methods from the literature: (1) SUM ( U rel sums up the relevance scores across all criteria k and uses the sum as the final ranking criterion [19]. (2) PROD ( U builds the product across all constituents [19]. Probabilisti-cally, this combination scheme assumes independence across all criteria and can be expected to be too na  X   X ve in some set-tings where dependence is given. (3) Weighted linear com-binations LIN  X  ( U rel ) build a weighted sum of constituents u rel , i with mixture parameters  X  i optimized by means of a parameter sweep with step size 0 . 1 [51]. It should be noted that all optimizations and parameter estimations, both for the baselines as well as for the copula models are conducted on designated training sets that do not overlap with the fi-nal test sets. We relied on the original training portion of the respective corpora. In the case that the original corpus did not specify a dedicated training set, we used a stratified 90%/10% split.

Based on three different standard datasets and tasks, we will highlight the merit of using copulas over the traditional approaches. Each of the settings specifies 2 individual rele-vance criteria ( k = 2) which are crucial for user satisfaction given the retrieval task. Table 1 gives a high-level overview of the relevant corpora that we used. Each of them will be described in more detail in the three following sections. De-pending on the strength of tail dependency in the data, we will see varying improvements for the three inspected set-tings. Comparable as the scenarios appear, there seem to be significant underlying differences in the distribution of relevant documents that influence the benefit from the use of copulas. In Section 6, we will dedicate some room to a de-tailed investigation of when the use of copula-based retrieval models is most promising.
When conducting marketing analyses for businesses, re-searching customer reviews of products or gauging political trends based on voter opinions, it can be desirable to fo-cus the search process on subjective, non-factual documents. The Text REtrieval Conference (TREC) accounted for this task within the confines of their Blog Track between the years 2006 and 2010 [32]. The aim of the task is to retrieve blog feeds that are both topically relevant and opinionated. Our experimental corpus for this task is the Blogs08 collec-tion specifically created for the venue. The dataset consists of 1.3 million blog feeds and is annotated by more than 38k manually created labels contributed by NIST assessors. Each document is represented as a two-component vector U rel . The first component refers to the document X  X  topical relevance given the query and the second represents its de-gree of opinionatedness. In order for a document to be con-sidered relevant according to the judges X  assessments, it has to satisfy both conditions. Topical relevance was estimated by a standard BM25 model and opinionatedness was deter-mined using the output of a state-of-the-art open source clas-sifier [1]. After an initial evaluation of the domain, we chose Clayton copulas (Equation 2) to represent the joint distri-bution of topicality and opinionatedness. Table 2 shows a juxtaposition of performance scores for the baselines as well as the various copula methods. The highest observed perfor-mance per metric is highlighted by the use of bold typeface, statistically significant improvements (measured by means of a Wilcoxon signed-rank test at  X  = 0 . 05-level) over all competing approaches are denoted by an asterisk. Of the baseline methods, the score product PROD performs best. However, introducing the use of copulas, we observe that the highest performance was achieved using the CPOS copula, which gave statistically significant gains in MAP, Bpref and precision at rank 10 over all the baseline methods.
At this point, we revisit the example query (Topic 1171) that was discussed in the introduction and depicted in Fig-ure 1. For this topic, we observed a clear non-linear depen-dency structure alongside a lower-than-average linear combi-nation performance of AP = 0 . 14. When applying CPOS to the topic, however, we obtain AP = 0 . 22, an improvement of over 50%.
Finding and re-finding resources on the Internet are fre-quently accompanied and aided by bookmarking. What started as a local in-browser navigation aid, has in recent years become an active pillar of the social web society. Col-laborative bookmarking platforms such as Delicious , Furl , or Simpy allow users to maintain an online profile along with bookmarks that can be shared among friends and col-laboratively annotated by the user community. Research into tagging behaviour [2] found that a significant amount of the tags assigned to shared media items and bookmarks are of subjective nature and do not necessarily serve as ob-jective topical descriptors of the content. This finding sug-gests that bookmarking has a strong personal aspect which we will cater for in our experiment. Vallet et al. [50] com-piled a collection of more than 300k Delicious bookmarks and several million tags to describe them. For a share of 3.8k bookmarks and 180 topics, the authors collected manual rel-evance assessments along two dimensions, topical relevance of the bookmark given the topic and personal relevance of the bookmark for the user. This dataset is one of the very few corpora whose personalized relevance judgements were made by the actual users being profiled. We conduct a re-trieval experiment in which we estimate topical and personal relevance for each document and use Gumbel copula models to model the joint distribution of facets. The set of rele-vant documents comprises only those bookmarks that satisfy both criteria and were judged relevant in terms of topicality and personal relevance. Table 3 shows an overview of the resulting retrieval performances. CNEG stands out as the strongest copula-based model but the overall ranking of sys-tems depends on the concrete metrics evaluated. For some metrics such as precision at rank 10 and MRR, the linear combination baseline prevails, BPREF and precision at 5 documents favour CNEG.
The third application domain that we will inspect is con-cerned with the retrieval of child-friendly websites. Chil-dren, especially at a young age, are an audience with specific needs that deviate significantly from those of standard web users. Even for adult users it has been shown that focussing Table 4: Copula-based relevance estimation perfor-mance for child-friendly websites ( k = 2 ).
 the retrieval process on material of appropriate reading level can benefit user satisfaction [12]. In the case of children, this tendency can be expected to be even more pronounced since young users show very different modes of interaction with search engines that reflect their specific cognitive and motor capabilities [16]. Consequently, dedicated web search en-gines for children should focus their result sets on topically relevant, yet age-appropriate documents. [17] constructed a corpus of 22k web pages, 1,000 of which were manually annotated in terms of topical relevance towards a query as well as the document X  X  likelihood of suitability for children. According to the authors, the class of suitable documents en-compasses those pages that were topically relevant for chil-dren, presented in a fun and engaging way and textually not too complex to be understood. In our retrieval experiment, we account for both criteria and require documents to be both on topic as well as suitable for children in order to be considered relevant. Table 4 gives an overview of the result-ing retrieval performance. In this setting, the various copula models show comparable result quality as the non paramet-ric baselines. Linear combinations with empirically learned weights, however, were consistently the strongest method. We intend to explore the reasons for this in future work. However we note that the distribution of child-suitable rat-ings has a very large mode at zero, with only a small num-ber of non-zero scores taking a limited number of possible discrete values -limiting the amount of useful dependency information available that copulas could exploit.
Previously, we investigated the usefulness of copulas for modelling multivariate document relevance scores based on a number of (largely) orthogonal document quality criteria. Now, we will address a different, closely related problem: score fusion (also known as an instance of data fusion). In this setting, rather than estimating document quality from the documents, we attempt to combine the output of several independent retrieval systems into one holistic ranking. This challenge is often encountered in the domains of metasearch or search engine fusion. To evaluate the score fusion perfor-mance of copula-based methods, we use historic submissions to the TREC Adhoc and Web tracks. We investigate 6 years of TREC (1995 -2000) and fuse the document relevance scores produced by several of the original participating sys-tems. Intuitively, this task closely resembles the previously addressed relevance estimation based on individual docu-ment properties. In practice, as we will show, the scenario differs from direct relevance estimation in that retrieval sys-tems rely on overlapping notions of document quality (e.g., a variant of tf/idf scoring) and are therefore assumed to show stronger inter-criteria dependencies than individual facets of document quality might. Systematically, however, we ad-dress a set of document-level scores U ( k ) rel , originating from k retrieval systems, exactly in the same way as we did docu-ment quality criteria in the previous section.
 As performance baselines, we will rely on two popular score fusion schemes, CombSUM and CombMNZ [19]. CombSUM adds up the scores of all k constituent retrieval models and uses the resulting sum as a new document score. CombMNZ tries to account for score outliers by multiplying the cross-system sum by NZ ( U ), the number of non-zero constituent scores.
 We introduce statistically principled, copula-based exten-sions of these established baseline methods: correspond-ing to CombSUM and CombMNZ, we define CopSUM and CopMNZ that normalize the respective baseline methods by the non-relevance copula.
 Due to the close relationship to the baseline methods, the effect of introducing copulas is easily measurable. Based on empirical evidence, we employ Clayton copulas to estimate C
Table 5 compares the baselines and copula methods in terms of MAP gain over the best, worst and median historic system run that were fused. Each performance score is aver-aged over 200 repetitions of randomly selecting k individual runs with k ranging from 2 to 10 for each year of TREC. Sta-tistically significant improvements over the respective base-line method, i.e. of CopSUM over CombSUM and CopMNZ over CombMNZ, are determined by a Wilcoxon signed-rank test at  X  = 0 . 05 level and are denoted by an asterisk.
Regarding the baseline methods, CombSUM and CombMNZ perform equally well on average, but with a clear dataset bias. On TREC 4, 8 and 9, CombSUM performs consistently better than CombMNZ. For TREC 5, 6 and 7, the inverse is true. With the exception of TREC 4, the fused rankings do not match the performance of the single strongest run that contributed to the fusion.

Introducing the copula methods led to consistent improve-ments over their non-copula baseline counterparts. In 104 out of 168 cases, the copula-based fusion methods gave sta-tistically significant gains, with only 14 out 168 performing worse than the corresponding baseline method. The copula-based methods achieved, on average, 7% gains over the cor-responding baseline when comparing to the strongest fused system, 4% gain on median systems and 2% gain on the weakest systems.
 There are significant differences in fusion effectiveness be-tween individual editions of TREC. Comparing TREC 4 and TREC 6, for example, we observe that TREC 6 fusion re-sults typically showcase performance losses in comparison to the best original run and very high gains for the weakest systems. We seek an explanation in the imbalance in per-formance of the original systems. Very weak systems have the potential of decreasing the overall quality of the fused result list by boosting the scores of non-relevant documents. As the number of very weak systems increases, so does the chance for performance losses introduced by fusion. When inspecting the number weak submissions (defined as having an MAP score that is at least 2 standard deviations lower than the average score across all participants) included in our fusion experiments, we find that, indeed, our TREC TREC 4.

In order to further investigate the influence of weak runs on overall fusion performance and to measure the proposed methods X  robustness against this effect, we turn to the 10-system fusion scenario and inject more and more weak sys-tems among the regular ones. Figure 3 shows how the fusion improvement over the single strongest system of TREC 4 is affected as the number of weak submissions ranges from 0 to 9 out of 10. As before, each data point is an average across 200 fusions of randomly drawn runs. In the ideal set-ting, in which there are no weak systems, we note higher performance gains than in the uncontrolled scenario that was shown in Table 5. As the number of weak systems in-jected into the fusion increases, performance scores quickly drop. As noted earlier, CombSUM performs slightly better on TREC 4 than CombMNZ. This difference, however, is not further influenced by the number of weak systems. The copula-based fusion methods are more resistant to the influ-ence of weak systems. We note the divide between copula-methods and baseline approaches growing as the number of weak systems increases. Each baseline system score is well-separated from the respective copula-based variant. Error bars in Figure 3 were omitted to prevent clutter.
In Section 4, we investigated three different domains in which we apply copulas to model the joint distribution of multivariate relevance scores. For each of these settings, we could observe varying degrees of usefulness of the pro-posed copula scheme. While for child-friendly web search, the linear baseline performed best, we achieved significant improvements in the opinionated blog retrieval setting. At this point, we investigate the reason for this seeming imbal-ance in performance gains in order to find a way of deciding for which problem domains the application of copulas is most promising.

One of the key properties of copulas is their ability to account for tail dependencies. Formally, tail dependence de-scribes the likelihood that component u rel , i within the obser-vation vector U ( k ) rel will take on extremely high or low values, as another component u rel , j with i 6 = j also takes an extreme value. The strength of this correlation in extreme regions is expressed by the tail dependency indices I U and I L for upper and lower tail dependency, respectively. Higher values of I signal stronger dependencies in the respective tail regions of the distribution.
 The literature has brought forward a number of estimators of the tail indices. We use the R implementation of Frees et al.  X  X  method [20].

Tail index estimates serve as good tools for separating do-mains where we are likely to observe performance gains (blog and bookmark retrieval) and those that do not match linear combination performance (child-friendly search). Based on the respective copula models that we fit to our observations, the blog retrieval ( I L = 0 . 07) and personalized bookmark-ing ( I U = 0 . 49) show moderate tail dependencies while the child-friendly web search task has no recognizable depen-dency among extrema ( I L = I U = 0). Since the compari-son of absolute tail index scores across observations is not meaningful, we are interested in a method to further nar-row down the expected performance. To this end, we took a closer look at the actual data distribution, and investigated goodness-of-fit tests that are used to determine how well an assumed theoretical distribution fits the empirical observa-tions. The higher the likelihood of our observations to have been generated by the copula models that we estimated, the higher resulting performance we can expect. We apply a standard Anderson-Darling test [3] to determine how well the observations are represented by the copula models. In the personalized bookmarking setting, we obtain p = 0 . 47 and for the blog data p = 0 . 67 for the null hypothesis of the observations originating from the present copula model. As we suspected based on the tail dependency strength, the child-friendly web search data only achieved a probability of fit of p = 0 . 046. Figure 3: Performance in terms of MAP when 0 ... 9 out of 10 fused original systems are weak.

To summarize, in this section, we have shown how a com-bination of tail dependence indices and goodness-of-fit tests can be used to help differentiate between domains that may benefit from copula-based retrieval models and those that may not.
In this work we introduced the use of copulas , a power-ful statistical framework for modeling complex dependen-cies, for information retrieval tasks. We demonstrated the effectiveness of copula-based approaches in improving per-formance on several standard IR challenges. First, we ap-plied copulas to the task of multivariate document relevance estimation, where each document is described by several po-tentially correlated relevance criteria. We learned and eval-uated copula models for three different IR tasks, using large-scale standard corpora: (1) opinionated blog retrieval; (2) personalized social bookmarking; and (3) child-friendly web search, obtaining significant improvements on the first two of these tasks. Second, we introduced copula-based versions of two existing score fusion methods, COMB-Sum and COMB-MNZ, and showed that these improve the performance of score fusion on historic TREC submissions, in terms of both effectiveness and robustness, compared to their non-copula counterparts. Finally, we investigated the performance dif-ferences of copula models between different domains, and proposed the use of tail dependency indices and goodness-of-fit tests to understand the likely effect of using copulas for a given scenario.

In future work, there are a number of interesting chal-lenges remaining in applying copula-based models to in-formation retrieval. (1) The independence assumption be-tween individual terms in queries and documents is a long-standing simplification in document and language modelling. Most attempts at incorporating more powerful dependency models into the retrieval process resulted in limited perfor-mance improvements at best. We would like to investigate the use of copulas in order to more realistically approximate the complex underlying term dependency structure. (2) During our investigation of the blog retrieval scenario, we en-countered examples of non-linear multivariate distributions of relevance and briefly pointed out the different correlation regimes that exist within the joint distribution. While the current single-copula models have been shown to outperform linear combination models at capturing such structures, we would like to proceed to inspecting mixture models in which individual copulas account for certain data ranges to rep-resent the underlying regimes better than a single holistic model could. (3) This work represents an exploratory study that aims to introduce the copula framework to the infor-mation retrieval community. For reasons of simplicity and brevity, it is based on data-driven estimation of copula pa-rameters  X  . It would, however, be interesting to build on the large body of previous work on formal modelling of the probability of relevance, to derive custom information re-trieval copulas from the assumed distribution of relevance among documents. [1] Alias-i. LingPipe 3.9.2. http://alias-i.com/lingpipe, [2] M. Ames and M. Naaman. Why we tag: motivations [3] TW Anderson and D.A. Darling. A test of goodness of [4] Avi Arampatzis and Stephen Robertson. Modeling [5] J.A. Aslam and M. Montague. Bayes optimal [6] K. Balog, L. Azzopardi, and M. de Rijke. Formal [7] G. Bordogna and G. Pasi. A model for a SOft Fusion [8] P. Borlund. The concept of relevance in IR. JASIST , [9] J.P. Bouchaud and M. Potters. Theory of financial [10] C. Burges, T. Shaked, E. Renshaw, A. Lazier, [11] A. Charpentier, J.D. Fermanian, and O. Scaillet. The [12] K. Collins-Thompson, P.N. Bennett, R.W. White, [13] N. Craswell, S. Robertson, H. Zaragoza, and [14] Ronan Cummins. Measuring the ability of score [15] C. da Costa Pereira, M. Dragoni, and G. Pasi. [16] A. Druin, E. Foss, L. Hatley, E. Golub, M.L. Guha, [17] C. Eickhoff, P. Serdyukov, and A.P. de Vries. A [18] P. Embrechts, F. Lindskog, and A. McNeil. Modelling [19] E. Fox and J. Shaw. Combination of multiple searches. [20] E.W. Frees and E.A. Valdez. Understanding [21] S. Gerani, C.X. Zhai, and F. Crestani. Score [22] S.P. Harter. Psychological relevance and information [23] W. H  X  offding. Scale-invariant correlation theory. [24] X. Huang and W.B. Croft. A unified relevance model [25] Evangelos Kanoulas, Keshi Dai, Virgil Pavlu, and [26] J.M. Kleinberg. Authoritative sources in a hyperlinked [27] W. Kraaij, T. Westerveld, and D. Hiemstra. The [28] V. Lavrenko and W.B. Croft. Relevance based [29] V. Lavrenko and W.B. Croft. Relevance models in [30] T.Y. Liu. Learning to rank for information retrieval. [31] W. Lu, S. Robertson, and A. MacFarlane.
 [32] C. Macdonald, R.L.T. Santos, I. Ounis, and [33] R. Manmatha, Toni M. Rath, and Fangfang Feng. [34] S. Mizzaro. Relevance: The whole history. JASIS , [35] M. Montague and J.A. Aslam. Condorcet fusion for [36] M. Montague and J.A. Aslam. Relevance score [37] A. Onken, S. Gr  X  unew  X  alder, M.H.J. Munk, and [38] M. Persin, J. Zobel, and R. Sacks-Davis. Filtered [39] J.M. Ponte and W.B. Croft. A language modeling [40] F. Radlinski and T. Joachims. Query chains: learning [41] B. Renard and M. Lang. Use of a gaussian copula for [42] S. Robertson, H. Zaragoza, and M. Taylor. Simple [43] S.E. Robertson, S. Walker, M.M. Hancock-Beaulieu, [44] T. Saracevic. Relevance reconsidered. In Conference [45] L. Schamber, M.B. Eisenberg, and M.S. Nilan. A [46] T. Schmidt. Coping with copulas. Risk Books: Copulas [47] C. Schoelzel, P. Friederichs, et al. Multivariate [48] A. Sklar. Fonctions de r  X epartition `a n dimensions et [49] T. Tsikrika and M. Lalmas. Combining evidence for [50] D. Vallet and P. Castells. Personalized diversification [51] C.C. Vogt and G.W. Cottrell. Fusion via a linear [52] S. Wu and F. Crestani. Data fusion with estimated
