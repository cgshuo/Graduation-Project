 We describe a mixed-modality method to index and search software documentation in three ways: plain text, OCR text of embedded figures, and visual features of these figures. Us-ing a corpus of 102 computer books with a total of 62,943 pages and 75,800 figures, we empirically demonstrate that our method achieves better precision/recall than do alterna-tives based on single modalities.
 H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval Algorithms
When interacting with a software application, users may encounter an unfamiliar dialog box and wish to search online documentation or tutorial articles for information relevant to this dialog box. When browsing the search results and judging the relevancy of each retrieved page, users may base their judgements on whether the page contains any figure related to the dialog box, since such figure would be a strong predictor for relevancy.

However, conventional search engines are often designed to search by keywords and take little or no advantage of visual cues. In the scenario described above, users can po-tentially benefit from the ability to use the image of the dialog box directly to initiate search, instead of describing the dialog box with a set of keywords. The image of the dialog box can then be analyzed by the search engine to retrieve pages with visually relevant figures.

Therefore, we present a mixed-modality method in Sec-tion 2 to index and search software documentation, such as computer books, online tutorials and user manuals, based on not only surrounding text but also the textual and vi-sual content of the figures, characterized by OCR and visual descriptors respectively. In Section 3 we present empirical evaluation results based on a corpus of 102 books, 62,943 pages, and 75,800 figures, demonstrating that our method provides better recall/precision than do single-modality al-ternatives using only text, OCR, or visual features. embedded figure, and (3) the visual features of the figure. gions. A large sample of these descriptors are clustered to form a codebook of visual words. Then, an image can be indexed based on the visual words its visual descriptors are closest to in the same way a text document can be indexed based on the stems its words are mapped to.

To search the index, we can compute the tfidf scores and retrieve the best matches in each of the three indexes. Then, we can combine the scores by taking a linear combination and generate the final ranked list of matches.
To evaluate our approach, we built a large corpus that con-sists of 62,943 pages extracted from the electronic editions of 102 popular computer books covering various operating sys-tems (e.g., Windows XP, MacOS) and software applications (e.g., Photoshop, Office). From these pages we extracted 75,800 figures. To form a query set, we downloaded screen-shot images of 500 dialog boxes from a tutorial website for Windows XP 1 . For each dialog box, the corpus may or may not contains pages about it. The search objective is to find a set of relevant pages in the corpus whenever possible.
We experimented with four methods: (1) keyword-only , where we used the Ferret search engine 2 to index all 62,943 pages based on text. Also, we generated search terms for each test dialog box based on two observations. First, we observed that users tend to choose search terms from words in the title bar, heading, tab, and/or the first sentence in the instruction; thus, we manually entered these words as search terms for each test dialog box. Second, we observed that users are unwilling to type too many search terms; thus, we capped the number of search terms to ten excluding stop-words. (2) OCR-only , where we used the Tesseract OCR engine 3 to process both the figures in the corpus and query images. We restricted the number of legal characters to 50, including numbers, alphabets and certain special characters, which gave us a total of 125,000 possible 3-grams in our index, (3) image-only , where we extracted SURF features and trained a visual codebook of 500,000 words, and (4) OCR+image , where we combined the scores of (2) and (3) and re-rank the matches accordingly.

We measured coverage , recall ,and precision . Coverage measures how many of the 500 dialog boxes were mentioned http://www.leeindy.com/ http://ferret.davebalmain.com/ http://code.google.com/p/tesseract-ocr/ http://www.vision.ee.ethz.ch/ surf/
