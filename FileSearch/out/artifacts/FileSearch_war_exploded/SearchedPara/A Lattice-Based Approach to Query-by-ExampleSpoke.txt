 Recent efforts on the task of spoken document retrieval (SDR) have made use of speech lattices: speech lattices contain in-formation about alternative speech transcription hypothe-ses other than the 1-best transcripts, and this information can improve retrieval accuracy by overcoming recognition errors present in the 1-best transcription. In this paper, we look at using lattices for the query-by-example spoken doc-ument retrieval task  X  retrieving documents from a speech corpus, where the queries are themselves in the form of complete spoken documents (query exemplars). We extend a previously proposed method for SDR with short queries to the query-by-example task. Specifically, we use a re-trieval method based on statistical modeling: we compute expected word counts from document and query lattices, es-timate statistical models from these counts, and compute relevance scores as divergences between these models. Ex-perimental results on a speech corpus of conversational En-glish show that the use of statistics from lattices for both documents and query exemplars results in better retrieval accuracy than using only 1-best transcripts for either docu-ments, or queries, or both. In addition, we investigate the effect of stop word removal which further improves retrieval accuracy. To our knowledge, our work is the first to have used a lattice-based approach to query-by-example spoken document retrieval.
 H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval X  retrieval models ; I.2.7 [ Artificial In-telligence ]: Natural Language Processing X  speech recogni-tion and synthesis Algorithms, Experimentation, Performance, Theory
In a query-by-example information retrieval task, one is given a collection of documents, and a query which is itself a full-fledged document  X  a query exemplar  X  and the task is to find documents in the collection which are similar in sub-ject matter to this exemplar. When both the query and the document collection are in the form of speech recordings, one obvious way to perform query-by-example retrieval is to run automatic speech recognition (ASR) on the recordings to ob-tain 1-best transcripts for both queries and documents, and use these transcripts for retrieval. However, this approach suffers from two problems:
To overcome the problem of recognition errors, one way is to work not with only one transcription hypothesis for each utterance, but several hypotheses presented in a lattice data structure. A lattice is a connected directed acyclic graph in which each edge is labeled with a term hypothesis and a likelihood value[11]; each path through a lattice gives a hypothesis of the sequence of terms spoken in the utterance.
Since the information in a lattice has a statistical inter-pretation, a retrieval model based on statistical inference, such as the statistical modeling retrieval approach of Song and Croft[28], will seem to be a more natural and more prin-cipled approach to lattice-based retrieval. We thus extend the statistical lattice-based retrieval method of Chia et al.[7], which has been shown to work well for retrieving Mandarin Chinese conversational speech with written keyword queries, to the query-by-example task. In our method, we generate a lattice for each speech segment in the collection corpus and the query exemplars, and compute the expected word count  X  the mean number of occurrences of a word given a lattice  X  for each word in each lattice. Using these ex-pected counts, a statistical language model is estimated for each spoken document and each query, and a document X  X  relevance to a query can then be computed as a Kullback-Leibler divergence between the document model and query model[15]. To mitigate the problem of noise in the retrieval process caused by non-content words in queries, we perform stop word removal, or stopping, which is commonly used in information retrieval (IR) tasks.

The rest of this paper is organized as follows. In Section 2 we review related work in the areas of spoken document retrieval, IR in general, use of spoken queries, and query by example. Section 3 describes statistical retrieval methods for performing query by example using 1-best transcripts and lattices, the details of the experimental setup for com-paring the two methods, and the results. Experiments with stop word removal using different stop lists are described in Section 4. Finally, Section 5 concludes our discussions and outlines our future work.
Spoken document retrieval (SDR) with short textual quer-ies is a relatively well-studied task. A straightforward way to perform SDR is to use the 1-best ASR transcripts of spo-ken documents for retrieval  X  indeed, in the SDR track of the Ninth Text REtrieval Conference (TREC-9), in which participants were required to perform retrieval from a col-lection of news broadcasts, it was found that X  X or each of the participants, retrieval from the transcripts created by their own recognizer was comparable to the retrieval from the hu-man reference transcripts X  X 30]. Despite this, it is opined that 1-best transcripts are still not of sufficient quality when the speech are of a more challenging nature, and in such cases the WER can be 50% or higher[25, 19]. For such conditions, the use of lattices has been found to be useful for improving retrieval accuracy.

Lattices were first introduced by James and Young[12] as a representation for indexing spoken documents, as part of a method for vocabulary-independent keyword spotting. The lattice representation was later applied to the task of spo-ken document retrieval by James[11]: James counted how many times each query word occurred in each phone lat-tice with a sufficiently high normalized log likelihood, and these counts were then used in retrieval under a vector space model with tf  X  idf weighting. Jones et al.[14] combined re-trieval from phone lattices using variations of James X  method with retrieval from 1-best word transcripts to achieve better results.

Since then, a number of different methods for SDR us-ing lattices have been proposed. For instance, Siegler[26] used word lattices instead of phone lattices as the basis of retrieval, and generalized the tf  X  idf formalism to allow uncertainty in word counts. Saraclar and Sproat[25] per-formed word-spotting in word lattices by looking for query word occurrences whose expected counts were above a cer-tain threshold; they also computed the expected count of the occurrences of query word pronunciations, when using phone lattices for word-spotting. Chelba and Acero[5] pre-processed lattices into more compact Position Specific Pos-terior Lattices (PSPL), and computed an aggregate score for each document based on the posterior probability of edges and the proximity of search terms in the document; the PSPL representation was further refined by Zhou et al.[36]. Mamou et al.[19] converted each lattice into a word confu-sion network[20], and estimated the inverse document fre-quency ( idf ) of each word t as the ratio of the total number of words in the document collection to the total number of occurrences of t . Hori et al.[10] combined confusion networks for words and phones to do open-vocabulary word-spotting. Chia et al.[7] performed lattice-based retrieval using a sta-tistical retrieval framework[28], instead of the vector space model with tf  X  idf weighting.
The use of lattices of spoken queries for IR was studied by Colineau and Halber[8], who measured the precision and recall for extracting keywords from queries in speech form. In their task, the queries were specified as short natural language sentences, such as  X  X hat do you have on John Kennedy? X ; these were recognized and transcribed by apply-ing grammar parsing, followed by rescoring with a domain-specific n -gram model, onto a word lattice. While this method is useful for short queries which are relatively formulaic and specify the information need concisely, it is not directly ap-plicable to our task, where the query exemplars comprise unrestricted speech and may contain much non-topical ma-terial; a different approach is thus needed in our case.
The statistical language modeling approach to retrieval was used by Ponte and Croft[22] for IR with text docu-ments, and it was shown to outperform the tf  X  idf approach for this task; Song and Croft[28] improved on this method and framed the problem of relevance ranking as a query likelihood computation. Lafferty and Zhai[15] showed that ranking by query likelihood is equivalent to ranking accord-ing to the Kullback-Leibler divergence between query and document models.
Stop word removal, being a standard IR technique, is often simply treated as a given. However, studies have been con-ducted on how different stop lists can impact performance. Sinka and Corne[27] performed classification and clustering of web documents in English using two different stop lists, and also formulated a method for deriving stop lists using word entropies. Carvalho et al.[4] compared the result of stopping using three different stop lists, for the task of Por-tuguese question answering.
In the area of query by example, Chen et al.[6] used news-wire (text) articles as query exemplars for retrieving news broadcast (speech) recordings; for retrieval, they used the statistical language modeling approach with 1-best tran-scripts of the collection corpus. The tracking task in the Topic Detection and Tracking (TDT) project has also been viewed as a form of query by example[32]. Efforts to solve this task  X  for instance, He et al.[9] and Lo and Gauvain[16, 17]  X  have made use of only ASR transcripts; this is likely because topic tracking, like SDR, can be done accurately even with 1-best transcripts when the rate of transcription errors is low enough[1].
The contributions of our work are as follows. First, we ex-tend the use of the statistical model in lattice-based SDR to the query-by-example task. Our work can also be considered an extension of Chen et al. X  X  work on query-by-example[6] to spoken queries, and an extension of Lafferty and Zhai X  X  for-mulation of statistical IR as Kullback-Leibler divergence[15] to the use of expected counts from speech lattices. In addi-tion, we study the effect of stop word removal using different stop lists in the context of query-by-example SDR.
We now describe two retrieval methods for performing query-by-example SDR: a baseline statistical method which works on 1-best transcripts of both queries and documents, and a statistical method which is capable of working with lattice statistics of queries and documents. Our baseline retrieval method is motivated by Song and Croft[28], and uses the language smoothing methods of Zhai and Lafferty[35]. This method is used to perform retrieval on the documents X  1-best ASR transcripts and reference human transcripts.

Let C be the collection of documents to retrieve from. For each document d contained in C , and each query q , the rele-vance of d to q can be defined as Pr( d | q ) . This probability cannot be computed directly, but under the assumption that the prior Pr( d ) is uniform over all documents in C , we see that this means that ranking documents by Pr( d | q ) is equiva-lent to ranking them by Pr( q | d ) , and thus Pr( q | d ) can be used to measure relevance[2]. More precisely, we use as our relevance score the logarithm of Pr( q | d ) :
Now express q as a series of words drawn from a vocab-ulary V = { w 1 , w 2 ,  X  X  X  w V } ; that is, q = q 1 q 2  X  X  X  q K is the number of words in the query, and q i  X  V for 1  X  i  X  K . Then given a unigram model derived from d which assigns a probability Pr( w | d ) to each word w in V , we can compute Rel( d , q ) as follows: where c ( w ; q ) is the word count of w in q .

Lafferty and Zhai[15] showed that this method of relevance scoring happens to be equivalent to ranking documents by the negative Kullback-Leibler divergence  X   X  KL ( q , d ) of the unsmoothed empirical distribution of words in q from a (pos-sibly smoothed) distribution of words in d , since  X   X  KL ( q , d ) = 1 where K and H q are constants which do not depend on d .
Before using Equation 1, we must estimate a unigram model from d : that is, an assignment of probabilities Pr( w | d ) for all w  X  V . One way to do this is to use a maximum likelihood estimate (MLE)  X  an assignment of Pr( w | d ) for all w which maximizes the probability of generating d . The MLE is given by the equation where c ( w ; d ) is the number of occurrences of w in d , and | d | is the total number of words in d . However, using this formula means we will get a value of zero for Pr( q | d ) if even a single query word q i is not found in d . To overcome this problem, we smooth the model by assigning some probability mass to such unseen words. Specifically, we adopt a two-stage smoothing method[35]:
Pr( w | d ) = (1  X   X  ) c ( w ; d ) +  X  Pr( w |C ) Here, U denotes a background language model, and  X  &gt; 0 and  X   X  (0 , 1) are parameters to the smoothing procedure. This is a combination of Bayesian smoothing using Dirichlet priors[18] and Jelinek-Mercer smoothing[13].

The parameter  X  can be set empirically according to the nature of the queries. For the parameter  X  , we adopt the estimation procedure of Zhai and Lafferty[35]: we maximize the leave-one-out log likelihood of the document collection, namely `  X  1 (  X  |C ) = This can be done by using Newton X  X  method to solve the equation
With this, we can now compute the relevance of d to q according to Equation 1.
We now describe our statistical lattice-based method for query-by-example SDR. In contrast to the above baseline method, our proposed method works on the lattice represen-tation of spoken documents, as generated by a speech rec-ognizer. It extends Chia et al. X  X [7] SDR method for textual queries, by incorporating uncertainty of query word counts into the retrieval process.
As in the baseline method, we estimate a statistical model for each document d in the collection corpus C . This is done by generating lattices for the document and computing the expected count of each word in each document.

First, each spoken document in the collection corpus is divided into M short speech segments. Let o denote the acoustic observations comprising such a speech segment; we then use a speech recognizer to generate a lattice[33] from each o .

We can consider the lattice generation process as proceed-ing in two steps. First we output a lattice L containing only acoustic likelihood information; we can consider L to com-prise a set of nodes S and a set of edges A  X  S  X  S , where each edge a  X  A is labeled with a word hypothesis t [ a ] and an acoustic probability p [ a ] . Each path  X  = a 1 a 2  X  X  X  a the lattice contains a hypothesis of the series of words spo-ken in this speech segment, t [  X  ] = t [ a 1 ] t [ a 2 ]  X  X  X  t [ a with acoustic probabilities Pr( o 1 | a 1 ) = p [ a 1 ] , Pr( o observations for the time interval corresponding to the edge a and the word t i hypothesized by the speech recognizer, such that o 1 o 2  X  X  X  o N = o . We have model, to yield an expanded lattice in which paths are weight-ed by their posterior probabilities Pr(  X  | o ) rather than their acoustic likelihoods Pr( o |  X  ) .

In theory, the rescoring can be done by simply multiply-ing acoustic probabilities with n -gram probabilities, since Pr( t | o )  X  Pr( t , o ) = Pr( t ) Pr( o | t ) for each transcription hypothesis t . In practice, it has been found to be useful[5] to assign a higher weight to the n -gram probabilities by apply-ing a scaling factor  X  &gt; 1 , and to introduce a word insertion penalty  X   X  0 ; thus, the paths in the rescored lattice are weighted by where |  X  | is the length of  X  .

The lattice is then pruned, by removing paths in the lat-tice whose log joint probabilities ( ln  X  Pr(  X , o ) ) are not within a threshold  X  doc of the best path X  X  log probability; specif-ically, by removing nodes and edges which occur only on such low-probability paths[21]. After pruning, we obtain a rescored and pruned lattice L 0 with nodes S 0 and edges A in this new lattice, each edge a i is labeled with a joint prob-Pr( a i | o i ) .

Next, we compute the expected count of each word in each document. For each word w and each speech segment o , the expected count of w in o is where the sum is taken over all lattice paths  X  , and c ( w ;  X  ) denotes the word count of w in the hypothesized transcript given by  X  . We can also analogously compute the expected length of o :
A naive implementation of Equations 5 and 6 will be ex-tremely inefficient, since the total number of paths in each lattice is roughly exponential in the length of the speech segment [26]. To compute E[ c ( w ; o )] and E[ | o | ] efficiently, one practical approach is to use a dynamic programming algorithm, based on the standard forward-backward algo-rithm[24].

Finally, for each document d comprised of M speech seg-ments represented by M series of acoustic observations o (1) o (2) ,  X  X  X  o ( M ) , we can compute the expected count of w in d and the expected length of d : and E[ | d | ] ; thus Pr( w | d ) = (1  X   X  ) E[ c ( w ; d )] +  X  Pr( w |C )
In addition, we use an approximate procedure for esti-mating  X  , by considering the leave-one-out log likelihood of a virtual data set with word counts which are close to the expected counts. Specifically, we replace c ( w ; d ) and | d | in Equation 3 with rounded-off expected counts and
We process the query exemplars in a similar way as the documents: each query exemplar q is divided into short speech segments, lattices are generated from the speech seg-ments, the lattices are then pruned according to a log path probability threshold  X  qry , and the expected count of each word w in q and the expected length of q is computed.
We can then estimate a probability model for each q by normalizing the expected word counts:
In Section 3.1.1, we stated that the use of the query like-lihood for relevance scoring is equivalent to ranking docu-ments by the negative Kullback-Leibler divergence; we use the same measure for lattice-based retrieval. Given proba-bility models for the distribution of words in both d and q , the negative Kullback-Leibler divergence is where again H q is a constant. Thus we can compute the relevance of d to q as Rel lat ( d , q ) = where Pr( w | d ) is as estimated in Equation 7.
To evaluate our proposed retrieval method, we performed experiments using the Fisher English Training corpus re-leased by the Linguistic Data Consortium (LDC2004S13 and LDC2005S13). This is a conversational telephone speech corpus comprising 11,699 recorded conversations each tak-ing up to 10 minutes, for a total of more than 1,920 hours, corresponding to approximately 109Mb of transcribed text.
ENG01 . Professional Sports on TV. Do either of you have a favorite TV sport? How many hours per week do you spend watching it and other sporting events on TV? Figure 1: An example of a topic specification in the Fisher corpus The conversations were sampled at 8kHz, and have been broken up into speech segments of up to 30 seconds each.
Each conversation was initiated by a topic statement cho-sen from a list of 40 topics, and speakers mostly adhered to the suggested topics. Each topic was specified with a topic identifier, a topic title, and a verbose topic description; an example of a topic specification is given in Figure 1.
We divided the speech corpus into three portions:
As each telephone conversation pertains to a single topic, we decided to simply treat each conversation in S 3 as a pos-sible unit of retrieval  X  a  X  X ocument X . Each document com-prises multiple consecutive speech segments.

To obtain query exemplars for our task setup, we decided to select 40 conversations as exemplars to represent the 40 topics. To do this, we performed a preliminary round of text retrieval on the reference transcripts of the conversations in S , using the titles and verbose descriptions of the 40 topic specifications as queries; for each topic, we then selected the highest-ranked conversation as an exemplar. (We used this procedure instead of randomly selecting exemplars, as the latter might cause us to select exemplars which are unrepre-sentative of their topics, and retrieval results obtained with them would not be useful for our subsequent analysis.)
We then assigned 8 of the query exemplars to be devel-opment queries, and the remaining 32 as test queries. The exemplars, along with the Fisher topics they represent, are listed in Table 1. The remaining 5,054 conversations in S were designated as the collection corpus ( C ) for our experi-mental setup.

To obtain ground truth relevance judgements, we adopted the following procedure: a document in C was deemed to be relevant to a query if and only if the document and query exemplar were about the same Fisher topic, according to the topic assignment tables provided by the LDC with the corpus. An examination of a small sample of the corpus showed that the conversations did tend to stay with their initial topics; thus we had reason to believe that the LDC X  X  topic annotations are good indicators of the various docu-ments X  subject matter. To process the conversations in the corpus, we used the HTK speech recognition engine[34] for acoustic model train-ing, lattice generation, and lattice rescoring. We divided each speech segment into 25-millisecond speech frames with a frame shift of 10 milliseconds, and coded each frame as a 39-dimensional feature vector consisting of 12 mel frequency cepstral coefficients (MFCCs) and normalized energy, and their first and second order derivatives; speaker-based cep-stral mean and variance normalization were also applied.
Using the acoustic modeling data in S 1 , we trained a set of 33,830 tied-state cross-word triphone models for 39 English phonemes, and two silence models. Each triphone was mod-eled as a left-to-right 3-state HMM with each state having 8 Gaussian mixture components. Pronunciations of words were obtained from the CMU Pronouncing Dictionary 1 .
We then used HTK to generate lattices for the speech segments in the collection corpus C and the query exemplars. The lattices were then rescored with a trigram model trained from the reference transcripts of the conversations in S 2 size of this model training data is approximately 65Mb. 1-best ASR transcripts were decoded by finding the highest-probability hypothesis from each of the rescored lattices. We computed the word error rate of the 1-best transcripts of the collection and queries: the WER was found to be 48.1%. All words in the collection corpus and the queries were then stemmed using the Porter stemming algorithm[23].

Besides HTK, several other tools were used to help with the preprocessing. Pruning of the rescored lattices was done using the AT&amp;T FSM library[21] to prune the rescored lat-tices, and expected count computation was done using the SRILM toolkit[29]. To make retrieval efficient, we stored expected word counts in B-tree index structures via library routines in the CMU Lemur toolkit 2 .
We then performed retrieval on the document collection using the algorithms in Section 3, using the reference tran-scripts, the 1-best ASR transcripts, and expected counts from lattices of the query exemplars and the collection cor-pus. For the retrieval parameters, we set  X  = 0 . 7 , which was suggested by Zhai and Lafferty[35] to give good retrieval performance for verbose queries; values of  X  (obtained by ftp://ftp.cs.cmu.edu/project/speech/dict/cmudict. 0.6 http://www.lemurproject.org/ solving Equation 4) were found to range between 1,300 and 2,600. For the background language model U , we used the language model derived from the collection corpus C .
The results of retrieval were checked against the ground truth relevance judgements, and evaluated in terms of the non-interpolated mean average precision (MAP): where L denotes the total number of queries, R i the total number of documents relevant to the i th query, and r i,j the position of the j th relevant document in the ranked list output by the retrieval method for query i .

For the lattice-based retrieval method, we performed re-trieval with the development queries using different values of the query lattice pruning threshold  X  qry from 20 to 300, and values of the document lattice pruning threshold  X  doc from 20 to 200; we then used the values of  X  qry and  X  doc with the best MAP to do retrieval with the test queries.
As a further comparison, we also performed retrieval on the collection corpus using the original Fisher topic specifi-cations (as in Figure 1) as queries.
The results of our experiments are summarized in Table 2. We see that when performing retrieval using lattices (Lat  X  Lat), the MAP of the development queries was highest at  X  qry = 240 and  X  doc = 120 , at which point the MAP of the test queries was 0.7079. A one-tailed paired Student X  X  t -test with 31 degrees of freedom between the 1-best retrieval results (1-best  X  1-best) and the lattice-based retrieval re-sults yields t = 3 . 58 , and a one-tailed Wilcoxon signed-rank test[31] yields w + = 440 ; these indicate that the improve-ment due to lattice-based retrieval in this case was significant at the 99.95% confidence level.

We also found that using expected counts of lattices from both queries and documents (Lat  X  Lat) resulted in better retrieval performance than using lattices for documents only (1-best  X  Lat), or using lattices for queries only (Lat  X  1-best). This shows that the alternative hypotheses contained in query and document lattices were able to reinforce one another to yield better retrieval accuracy.

However, when compared against the performance ob-tainable by using short queries  X  namely, using the origi-nal Fisher topic specifications as queries  X  our lattice-based query-by-example method still fell short. In fact, even when the reference transcripts of documents and query exemplars Table 3: Most frequent words (after stemming) in the topic specification for ENG01 , and in the ref-erence transcripts of the corresponding exemplar fe_03_02783 are used in query-by-example retrieval (Ref  X  Ref), we found that the MAP achieved on the test queries is only 0.7468, which is still lower than the MAP of 0.7613 obtain-able using topic specifications, even when working merely with 1-best transcripts of the collection corpus (Top  X  1-best). This suggests that the very nature of the query ex-emplars presents difficulties in the way of accurate retrieval. To bridge the accuracy gap between query by example and SDR with short queries, we tried applying stop word removal on both the document collection and the query exemplars. As illustrated in Table 3, the distribution of words is differ-ent in the two types of queries; the short topic specification contains stop words but still has a high concentration of content words specifying the topic, while the most frequent words in the query exemplar are all stop words (such as  X  X he X ) or filler words (such as  X  X eah X ); thus by filtering away stop words, we expect to be able to reduce the difference between the nature of the two types of queries.

For reference and 1-best transcripts, stopping can be done by excluding stop words from the vocabulary V , and omit-ting such words from the transcripts of d and q . For lattice-based retrieval, stop word removal can be achieved by treat-ing all stop words in documents and queries as having an expected count of zero, and also leaving them out of the computation of E[ | d | ] and E[ | q | ] .
We tested the effect of stop word removal on the Fisher corpus retrieval task described in Section 3.3. We experi-mented with using two different stop lists:
The results are shown in Table 4. When performing re-trieval using lattices with stopping using the gla stop list (Lat gla  X  X  X  X  Lat), the MAP of the development queries was highest at  X  qry = 240 and  X  doc = 120 , at which point the MAP of the test queries was 0.7364; a t -test shows that the improvement over retrieval using 1-best transcripts (1-best  X  X  X  X  1-best) was significant at the 99.99% confidence level ( t = 4 . 24 , w + = 483 ). When the smart stop list was used, the MAP of lattice-based retrieval (Lat smart  X  X  X  X  X  X  Lat) was also found to be significantly better than 1-best retrieval (1-best  X  X  X  X  X  X  1-best) at the 99.95% confidence level under the t -test ( t = 3 . 76 ), and at the 99.99% confidence level under the Wilcoxon test ( w + = 472 ). Thus, the use of lattices for query-by-example SDR still produces better performance than query-by-example SDR with 1-best transcripts, even with the use of stopping.

When compared to lattice-based retrieval without stop word removal (Lat  X  Lat), the results were also better. The MAP increased from 0.7079 to 0.7364 with the use of the gla stop list (Lat gla  X  X  X  X  Lat), while using smart (Lat smart caused the MAP to increase to 0.7569. We therefore see that stop word removal can help to boost retrieval accuracy for the query-by-example task.

Also, we found a significant difference between the effects of the two different stop lists: for lattice-based retrieval, switching from the gla stop list to the smart stop list re-sulted in a MAP increase which was significant at the 97.5% confidence level ( t = 2 . 02 , w + = 379 ). This suggests that http://www.dcs.gla.ac.uk/idom/ir_resources/ linguistic_utils/stop_words http://members.unine.ch/jacques.savoy/clef/ englishST.txt the precise choice of stop list can also have an impact on query-by-example performance, and thus this issue merits attention.
We have presented a method for performing query-by-example SDR using lattices of documents and spoken queries, based on a statistical language modeling retrieval frame-work. Results show that our new method can significantly improve the retrieval MAP compared to using only the 1-best ASR transcripts.

Furthermore, retrieval results using stop word removal with two different stop word lists showed that using lattices gave better retrieval accuracy than using 1-best transcripts for both stop word lists; thus it can be seen that lattice-based retrieval yields a consistent improvement over 1-best retrieval across a variety of retrieval setups.

For future work, we would like to extend our statistical lattice-based retrieval framework to other speech processing tasks, such as spoken document classification. [1] J. Allan. Robust techniques for organizing and [2] A. Berger and J. Lafferty. Information retrieval as [3] C. Buckley. Implementation of the SMART [4] G. Carvalho, D. M. de Matos, and V. Rocio.
 [5] C. Chelba and A. Acero. Position specific posterior [6] B. Chen, H.-M. Wang, and L.-S. Lee. A discriminative [7] T. K. Chia, H. Li, and H. T. Ng. A statistical [8] N. Colineau and A. Halber. A hybrid approach to [9] D. He, H. R. Park, G. C. Murray, M. Subotin, and [10] T. Hori, I. L. Hetherington, T. J. Hazen, and J. R. [11] D. A. James. The Application of Classical Information [12] D. A. James and S. J. Young. A fast lattice-based [13] F. Jelinek and R. L. Mercer. Interpolated estimation [14] G. J. F. Jones, J. T. Foote, K. S. Jones, and S. J. [15] J. Lafferty and C. Zhai. Document language models, [16] Y.-Y. Lo and J.-L. Gauvain. The LIMSI topic tracking [17] Y.-Y. Lo and J.-L. Gauvain. Tracking topics in [18] D. J. C. Mackay and L. C. B. Peto. A hierarchical [19] J. Mamou, D. Carmel, and R. Hoory. Spoken [20] L. Mangu, E. Brill, and A. Stolcke. Finding consensus [21] M. Mohri, F. Pereira, and M. Riley. The design [22] J. M. Ponte and W. B. Croft. A language modeling [23] M. F. Porter. An algorithm for suffix stripping. [24] L. R. Rabiner. A tutorial on hidden Markov models [25] M. Saraclar and R. Sproat. Lattice-based search for [26] M. A. Siegler. Integration of Continuous Speech [27] M. P. Sinka and D. W. Corne. Towards modernised [28] F. Song and W. B. Croft. A general language model [29] A. Stolcke. SRILM  X  an extensible language modeling [30] E. M. Voorhees and D. Harman. Overview of the [31] R. E. Walpole and R. H. Myers. Probability and [32] C. L. Wayne. Topic detection and tracking in English [33] F. Weng, A. Stolcke, and A. Sankar. Efficient lattice [34] S. Young, G. Evermann, M. Gales, T. Hain, [35] C. Zhai and J. Lafferty. A study of smoothing [36] Z.-Y. Zhou, P. Yu, C. Chelba, and F. Seide. Towards
