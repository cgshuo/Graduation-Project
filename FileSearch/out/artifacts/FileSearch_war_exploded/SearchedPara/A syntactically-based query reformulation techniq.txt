 1. Introduction
The task of an information retrieval (IR) system is to retrieve documents from a collection, in response to a user need, which is expressed in the form of a query. Retrieval consists in matching the query against the doc-uments, and in returning the ones that appear closest, in ranked lists of assumed relevance ( van Rijsbergen, 1979 ). Usually, the words found in the queries and documents are associated with individual weights, which capture the importance of these words to the content of the corresponding queries and documents. Such weights, commonly referred to as term weights, can be computed using various term weighting models. On top of these term weighting models, other performance-boosting techniques are often used to increase retrieval performance, especially for ad hoc retrieval. Ad hoc retrieval is denned as the prototypical document retrieval task, where the document set is a static collection of text documents, and where a subset of the documents are to be retrieved in response to a user X  X  query. The ad hoc retrieval task is similar to how a researcher might use a library  X  the collection is known, but the questions likely to be asked are not known ( Voorhees &amp; Harman, 2005 ). One of the performance-boosting techniques that are often used to increase retrieval performance is pseudo-relevance feedback (PRF) ( Salton &amp; Buckley, 1990; Salton, Fox, &amp; Voorhees, 1985 ). In PRF, the query is usually enriched with more relevant terms, and thus reformulated, in order to facilitate the retrieval of documents relating to the user need. PRF is often realised on the basis of the lexical features of the queries, such as word frequency counts and co-occurrence statistics ( Amati &amp; van Rijsbergen, 2002; Efthimiadis &amp;
Biron, 1993; Fuhr &amp; Robertson, 1992; Rocchio, 1971; Xu &amp; Croft, 1996 ). By lexical, we denote the informa-tion that relates to the canonical form of words, such as apple for example. By shallow syntactic, we in fact denote the surface-syntactic information that relates to the part of speech of words, such as noun for example. Table 1 illustrates this correspondence between lexicon and syntax in language, with a few examples.
Generally in natural languages, words are lexical manifestations of meaning, while sentences are shallow syntactic arrangements of parts of speech. Our assumption is that, in textual IR, apart from lexical informa-tion, shallow syntactic information, namely part of speech classification information, can also be used to refor-mulate the queries, and increase their likelihood of fetching more relevant documents. We propose syntactically-based query reformulation (SQR) as a technique that aims to automatically enhance the perfor-mance of an IR system, on the basis of natural language shallow syntactic knowledge.

A lot of research in the area of IR has focussed on the automatic processing of words, most probably moti-vated by the fact that words are the explicit carriers of the information intended in any language communi-cation. However, syntax also plays an implicit role in communicating meaning, namely by regulating the relations that bind words together into coherent sentences. We believe that shallow syntactic information may model the structure of language, by showing which shallow syntactic structures are more likely to occur and/or co-occur. In order for this shallow syntactic information to be considered as shallow syntactic evi-dence, it needs to be extracted from a relatively large language sample, which represents language use in gen-eral, and not domain-specific language use.

The shallow syntactic structures that we use in SQR consist of shallow syntactic blocks, that may be arbi-trarily set to any length. shallow syntactic blocks are sentential fragments, where lexical tokens have been replaced by their corresponding shallow syntactic category, namely part of speech. We call these shallow syntactic structures, part of speech blocks (POS blocks). The following example illustrates this point, with a sentential lexical fragment and its corresponding POS block. The length of the POS block used in this exam-ple is set at four tokens.
Such fixed-length POS blocks are extracted from large language samples, and smoothed on the basis of their frequency and the overall size of the language sample used. Both the length of the POS block, and the types of shallow syntactic (part of speech) categories that are contained within POS blocks, may be tai-lored to fit specific and distinct research needs. For example, longer POS blocks may be preferred for a shallow syntactic modeling of the argumentative structure of documents; similarly, the shallow syntactic category of proper nouns may be specified as a compulsory presence within all POS blocks, when seeking named entities.
Our underlying assumption, which is laid out and justified in Section 3.1 , is that POS blocks that occur more often in language may be more content-bearing, than other less frequently occurring POS blocks. This follows from the fact that arrangements of parts of speech have different properties than lexical items, with regard to frequency of occurrence (as is discussed in Section 3.1 ). Following from this, our aim is to reduce the original queries to their content-rich fragments only, using POS blocks. Thus, we adopt the following steps, which are further detailed in Section 4 .
 Firstly, we extract frequently occurring POS blocks from a large representative language sample.
Secondly, we select the lexical parts of the queries that correspond to these frequently occurring POS blocks.
 Thirdly, we reduce the query to the lexical form of the selected frequently occurring POS blocks.
In this respect, our proposed technique reformulates queries by reducing the amount of assumed content-poor and hence noisy fragments found in them. The resulting query becomes our syntactically-based reformu-lated query.

The main objective of this work is to further test the assumption that POS blocks that occur more often in language may be more content-bearing, than other less frequently occurring POS blocks, through an applica-tion, namely IR, for which we have some quantitative measure of performance, namely mean average precision (MAP). SQR applies that assumption ( Lioma &amp; Ounis, 2006 ). Other applications testing this claim may also be done in the fields of automatic summarisation, automatic indexing, and so on.

The remainder of this paper is organised as follows. Section 2 discusses studies relating to this work. Section 3 introduces the motivating intuition behind SQR, and the research questions addressed in this paper. Section 4 presents our methodology. Section 5 details the experimental settings used to evaluate our technique, and the corresponding evaluation outcomes. Section 6 discusses further applications of SQR in IR. Section 7 provides our concluding remarks and some future research directions. 2. Related studies The use of natural language syntax to enhance retrieval performance is not a new idea. In the context of
IR, efforts have been made to use shallow syntactic information to enhance retrieval, ranging from using pseudo-syntactic rules ( Bruandet, 1987 ), to developing conceptual frames ( Croft &amp; Lewis, 1987 ), and focussing on specific surface syntactic groups ( Smeaton &amp; van Rijsbergen, 1988 ). However, numerous as such studies may have been ( DeJaco &amp; Garbolino, 1986; Jacobs, 1992; Jacobs &amp; Rau, 1988; Karlgren, 1993; Mauldin, Carbonell, &amp; Thomason, 1987; Salton, 1991; Smeaton, 1986, 1999; Sparck-Jones &amp; Tait, 1984; Strzalkowski, 1992; Walker, Karlgren, &amp; Kay, 1977; Xu &amp; Croft, 1996; Zukerman &amp; Raskutti, 2002 ), they have not made use of the type of shallow syntactic structures, namely POS blocks, which we present in this work.

POS blocks should not be confused with textual chunks, which are defined as non-recursive and typically non-overlapping cores of intra-clausal constituents ( Abney, 1991, 1996; Ramshaw &amp; Marcus, 1995 ). Chunks can be (and most of the times are) non-overlapping, while POS blocks are always overlapping. Chunks aim to model word groups that are grammatically related, noun phrases for example. POS blocks do not aim to model words groups that are grammatically related, they only aim to model tags (and hence their correspond-ing words) that occur next to one another. More simply, even through a POS block may include a noun phrase, or any other constituent, it does not necessarily have to, and more importantly, it does not aim to.
Hence, chunking can be viewed as a search process, with applications like named entity identification, while extracting POS blocks cannot be viewed as a search process. Chunks can vary in length, while POS blocks are always fixed-length .
 The extraction of POS blocks for SQR is similar as a process to the class-based n -gram model ( Brown, to determine the probability of occurrence of a shallow syntactic class, given its preceding shallow syntactic class, and the probability of occurrence of a particular word, given its own shallow syntactic class. This model differs from our here-proposed approach in that it addresses the problem of shallow syntactic tagging, and has never been applied to Information Retrieval. More importantly, in SQR we treat POS blocks as tokens, and we do not examine the relations between their members. Moreover, a plethora of other studies have examined the distribution of character or word n -grams, as in the case of language modeling for information retrieval ( Croft &amp; Lafferty, 2002; Hiemstra, 2001; Kraaij, 2004 ), for example. Specifically, both in language modeling and in SQR, the length of the patterns or n -grams to be extracted is fixed, while the extraction of the patterns the way in which POS blocks are extracted in SQR). Nevertheless, in language modeling for Information
Retrieval, n -grams tend to consist of  X  X exical X  units, such as character or word groups or subgroups, and not of shallow syntactic categories, such as parts of speech. The POS blocks used in SQR do not replace the terms of the documents and queries, but are used on top of them, as an extra layer of processing. On the contrary, IR language modeling is a type of weighting and retrieval model, which can be used on its own to weight and match n -grams, and therefore to retrieve documents that are relevant to a query. SQR is a query reformulation technique, which cannot be used on its own, but only on top of any existing weighting model.

SQR differs from conventional pseudo-relevance feedback (PRF) techniques in two distinct ways. Firstly, conventional PRF mainly consists in processing lexical items, namely words. On the contrary SQR focuses mainly on arrangements of parts of speech, namely POS blocks. Secondly, PRF is often a query expansion mechanism, which adds assumed relevant words to the queries, in order to boost the presence of content-bear-ing items in them, thus increasing query size, and reweights the resulting query terms. To the contrary, SQR, reduces query size by keeping in the query only sentential fragments that are assumed to be content-bearing, on the basis of their syntax. SQR does not reweight the terms of the reformulated query in itself, but relies on the term weighting model used to do so. 3. Syntactically-based query reformulation (SQR) 3.1. Motivation
One of the core functions of natural language is to communicate information. The role of natural language syntax is to regulate the arrangement of words into phrases, so that these phrases are well-formed and com-municate the information intended. There is more than one way of saying the same thing. Hence, there is more than one syntactic arrangement that communicates the same piece of information. We capture syntactic arrangement in language, using recurrent arrangements of parts of speech, namely POS blocks. We assume that the most frequently occurring POS blocks in language must be the ones that capture the most information content possible in the least effort-consuming way, in line with the principle of least effort ( Zipf, 1949 ). The validity of this statement is well-known in the field of linguistics, where complicated and/or difficult syntactic structures and features are used less frequently with time, until they become extinct from language as a whole ( Kiparsky, 1976 ). It is this exact relation between syntax and information content that we wish to model and utilise in order to enhance retrieval performance, in the framework of an IR system.

Our intuition might appear at first glance to contradict one of the main tenets of textual IR, which holds that the relation between the frequency of occurrence of words and the amount of content that they convey is approximately inversely proportional ( Bookstein &amp; Swanson, 1974; Damerau, 1965; Harter, 1974; Sparck-
Jones, 1972; Yu &amp; Salton, 1976 ). Nevertheless, our intuition does not relate to single words, namely lexical items, but to arrangements of parts of speech, namely POS blocks. Specifically, we assume that the relation between the frequency of occurrence of POS blocks in a representative language sample and the amount of content that they bear is approximately directly proportional. Note that the evaluation of this assumption is not within the scope of this paper. Even so, experimental results suggest that this assumption holds ( Lioma this study.

We model language use statistically, using language samples of considerable size. We consider the most fre-quent POS blocks that are contained within these samples to be representative of the overall  X  X asiest X  way of communicating as much content as possible. Such POS blocks form the shallow syntactic evidence, which we employ to reformulate the queries as part of our SQR technique. SQR reformulation consists in reducing the query to only those sentential fragments that correspond to POS blocks assumed to be content-rich, on the basis of their high frequency.

We refer to our technique as syntactically-based, rather than syntactic, query reformulation, because it does not make any attempt to process syntax, such as exploring the relations that bind the members of a sentence for example. SQR uses surface-syntactic blocks, namely POS blocks, induced from text, and treats these POS blocks as individual units, i.e. tokens. We assume that for these  X  X urface-syntactic tokens X , high frequency of exact theoretical claim that we implement as part of the retrieval process, namely in SQR, for which we have some quantitative measure of performance, such as MAP for example. 3.2. Our research questions
Within the above-described framework of syntactically-based query reformulation, we purport to address the following research questions: 1. Does the relative size of the language samples from which we draw shallow syntactic evidence for SQR affect retrieval performance? 2. Is SQR an effective and robust query reformulation technique for IR systems? 3. Can SQR be effectively combined with pseudo-relevance feedback (PRF)?
We investigate the aforementioned research questions as follows. It has been shown that collection size has an effect on retrieval performance ( Hawking &amp; Robertson, 2003; Macdonald et al., 2005 ), and when using external collections for query expansion ( Diaz &amp; Metzler, 2006; Macdonald et al., 2005 ). We ask if collection size also has an effect on SQR. To answer this first question, we use shallow syntactic evidence induced from various language samples of different sizes. To answer the second question, we compare SQR to a strong pseudo-relevance feedback technique. To answer the third question, we apply SQR and PRF together. We evaluate our approach on two standard Text REtrieval Conference (TREC) English test collections, using three statistically different term weighting models. We consider short queries as the least noise-bearing queries possible, and long queries as the most noise-bearing queries. Thus, we compare the effect of SQR on long que-ries, using short queries as our baseline, and observe the amount, if any, of noise reduction, as reflected upon retrieval performance.
 4. SQR methodology
This section presents the steps realised in order to apply SQR within the context of an IR application. These steps, which are presented in details below, are: Step 1. Draw shallow syntactic evidence from language samples.
 Step 2. Represent the queries syntactically.

Step 3. Reformulate the queries using shallow syntactic evidence. 4.1. Step 1. Drawing shallow syntactic evidence from language samples
We set off with various language samples of different sizes, and induce shallow syntactic evidence from them as follows. We feed each language sample separately into an automatic shallow syntactic tagger, for example the Tree Tagger ( Schmidt, 1997 ). The shallow syntactic analysis is realised on a part of speech level. Thus, we obtain syntactically tagged versions of these samples, from which we extract POS blocks. These blocks are of fixed length, which is set empirically, depending on the application (see Section 1 ). The blocks are overlapping, and are extracted as indicated in the following example. The collective number of POS blocks of length n that can be extracted from a sentence that contains l terms is: l ( n 1). Thus, no more blocks are extracted after there remain less than n terms in a sentence. In the following example, a sample sentence is presented both in its lexical and shallow syntactic form, while the length of the POS blocks extracted is set at four tokens. Note that, in this example, even though farm is a noun, it is tagged as an adjective, because it occupies a pronominal position, and thus assumes the syntactic role of an adjective.
This step is realised during indexing time. The relevant computational cost is low, as both shallow syntactic tagging and block extraction are speedy processes, the aggregation of which does not require much disk space. 4.2. Step 2. Representing the queries syntactically
We syntactically tag the queries using the same automatic shallow syntactic tagger as in Step 1. We extract from the shallow syntactic representations of the queries the top k most probable POS blocks drawn from the language samples. The decision regarding the number k of the most probable POS blocks of the language samples we use is made empirically. The following example illustrates Step 2. In this example, we select
POS blocks of frequency higher than 1. Note that, in this example, even though Chevrolet is a proper noun, it is tagged as an adjective, because it occupies a pronominal position, and thus assumes the syntactic role of an adjective.
We map the two most frequent POS blocks from Step 1 to the tagged query. Fragments that have not been mapped appear in reduced font size. The selected mapped fragments appear in brackets. Overlapping frag-ments are highlighted in bold:
In the preceding example, brackets numbered 1 and 2 contain query fragments that correspond to each of the two POS blocks employed, respectively. Thus, bracket numbered 1 contains the fragment the types of
Chevrolet , and bracket numbered 2 contains the fragment types of Chevrolet trucks . 4.3. Step 3. Reformulating the queries using shallow syntactic evidence
We use the original lexical query text that corresponds to the selected POS blocks of the queries as the sole content of the reformulated query. If there exists an overlap between terms that are shared by two or more
POS blocks, we keep one occurrence of the overlapping terms, so as to avoid word repetition and thus lexical misrepresentation in the query. The output of this process becomes our new query. The following example illustrates Step 3.

Lexical fragments corresponding to the most frequent POS blocks extracted from the language sample (Step 1) and mapped to the query (Step 2):
Final reformulated query without overlap:
The computational overhead of steps 2 and 3 for small values of k , such as the ones used in this paper, is negligible, compared to the cost associated with core retrieval processing. 5. Evaluation This section presents the experiments conducted in order to investigate the research questions formulated in Section 3.2 , across a selection of retrieval settings. Section 5.1 introduces the experimental settings we employ.
Section 5.2 presents our evaluation results and discusses our findings. 5.1. Experimental settings
We induce shallow syntactic evidence from five language samples of significantly different sizes, in order to investigate the effect of language sample size upon retrieval performance when using SQR. Specifically, the first language sample we use is the English component of the second release of the Europarl parallel corpus, namely Europarl_En (75MB) 1 , to be referred to as LS1. This corpus contains parliamentary proceedings of the European Parliament, crawled from the proceedings dating between 1996 and 2003. Our second language sample, to be referred to as LS2, contains journalistic articles collected from the Los Angeles Times archives in 1994 (425MB) 2 . Our third language sample, to be referred to as LS3, is the TREC which contains newswire stories collected between 1988 and 1990. The fourth and fifth language samples we use, to be referred to as LS4 and LS5 correspondingly, are two standard TREC Web test collections, namely WT2G (2GB), from TREC-8, and WT10G (10GB), from TREC-9 and TREC-10, respectively. Both LS4 and
LS5 were crawled from the Web in 1997. We syntactically tag these document collections using the TreeTagger ( Schmidt, 1997 ), which is a probabilistic part of speech tagger.

We estimate the probability of occurrence of the shallow syntactic blocks induced from the above language samples, using Good-Turing statistical smoothing ( Manning &amp; Schutze, 1999 ). Good-Turing smoothing esti-mates the probability of occurrence P GT of a POS block as where N denotes the number of POS block tokens extracted from the language sample, and r * is an adjusted frequency, given by: where r is the frequency of a given unique type of POS block, E is the expectation of a random variable, and Nr is the frequency of r (also known as count-count) of a given unique type of POS block.

We empirically vary the k most probable POS blocks found in each of the language samples that we use for query reformulation, to the following values: 100, 50, 30, 10, and 5. Additionally, we empirically set the size of POS blocks to four tokens. We have also experimented with varying the n size of POS blocks to 3, 5, and 6.
These experiments have produced retrieval performances that are consistent with the scores reported in this paper. The choice of four tokens as the size of POS blocks follows from the intuition that the size of the
POS block should be large enough for the block to contain shallow syntactic constraints, yet no so large as to suppress constraints that make use of strict adjacency.

Table 2 displays the five most frequent POS blocks extracted from each of the five language samples employed. POS blocks common to two or more language samples in the top five most frequent places appear in boldface, followed by a bracketed ratio of the language samples they occur in. We can see that LS1 shares the fewest most frequent common POS blocks with the other language samples, namely three, whereas LS2,
LS3, and LS5 share four, and LS4 shares five. This is symptomatic of the fact that LS1 is representative of elaborate spoken language, as it is the transcription of the proceedings of the European Parliament, while the remaining language samples are in fact collections of written language, with a focus on factual, rather than elaborate language.

For our retrieval experiments, we use ad hoc queries from the TREC Web track. Specifically, we use the ad hoc queries from the TREC-8 Web track, numbered 401-450, when retrieving documents from the WT2G col-lection, and the ad hoc queries from the TREC-9 Web track, numbered 451-500, when retrieving documents from the WT10G collection. For both sets of queries, respective relevance assessments are provided. Each query contains three fields, namely a title ,a description , and a narrative field. The title contains a few key-words, which are essential to the core content of the query. In the vast majority of cases, the title contains no noise, either in the form of stopwords, or in the form of vague/generic/irrelevant terms. The description of a query usually consists of a single sentence, which expands on the query content conveyed in the keywords in the title . Very often, the description sentence is a straight-forward question. In the vast majority of cases, the type of noise that is contained in the description sentence consists of simple stopwords. These stopwords are removed during the stage of stopword removal, which takes place prior to retrieval. The narrative of a query usually consists of more than one sentence, elaborating on what might be of relevance, and what might not be of relevance, to the topic in question. The narrative differs from the title and description fields of the query, not simply because it is longer, but mainly, because it contains much more noise, and more impor-tantly, because it contains a different type of noise, namely noise that cannot be entirely removed only by dropping stopwords. Specifically, this type of noise consists of periphrastic structures, i.e. phrases introducing a point, rather than the factual point itself. These periphrastic structures contain both stopwords and other words that are neverthess vague/generic/irrelevant to the topic. We address exactly this type of noise with
SQR. As was mentioned in Section 1 , SQR is an application of an automatic information processing model, which identifies and categorises content, on the basis of shallow syntactic recurrence statistics, and absolutely not on the basis of the kind of lexical and morphological evidence that defines stopwords. Thus, SQR is not analogous to stopword removal, as a noise reduction process. Hence, we apply SQR to the narrative field of the queries, not only because this field contains longer sentences, but also, and in fact mainly, because the nar-rative field contains vague/general words and phrases, which are not always relevant to the topic. We use all three query fields to match query terms to documents.

The last two language samples from which we draw POS blocks, namely LS4 and LS5, are also the test collections from which we retrieve relevant documents, namely WT2G and WT10G. Even though these are
Web collections, as mentioned in Section 1 , we are not experimenting with Web retrieval, but with the ad hoc task of the relevant Web track. We select these two Web collections, instead of a non-Web TREC collec-tion, such as the AP collection, for example, because we have more queries available for WT2G and WT10G. Thus, these two collections provide a better testbed for our experiments.

We do not expect using the same language sample from which we extract shallow syntactic evidence as a test collection for retrieval to affect retrieval performance, as this shallow syntactic evidence is used to refor-mulate the queries, and does not affect the test collections used for retrieval in any way. This point is exper-imentally confirmed and discussed in Section 5.2 .

During indexing, we remove stopwords, and stem the collections and the queries, using Porter X  X  stemming algorithm 4 . We use the Terrier ( Ounis et al., 2005, 2006 ) IR platform, and apply three different term weighting schemes to match query terms to document descriptors. By doing so, we aim to test the SQR technique on top of three statistically different term weighting approaches. Specifically, we use the classical TF  X  IDF weighting scheme ( Robertson, 1995; Sparck-Jones, 1972 ), the well-established probabilistic BM25 weighting scheme ( Robertson, 1995 ), and the more recent PL2 weighting scheme from the Divergence From Randomness (DFR) framework ( Amati, 2003 ). In DFR, term relevance is measured in terms of the divergence of the actual term distribution from that obtained under a random process. For all three weighting schemes, we calculate the query term weight as follows: qtw  X  qtf mum qtf among all query terms.

We use the default values of all parameters of the above weighting schemes ( Amati, 2003; He &amp; Ounis, 2003; Robertson, 1995 ), which are shown in Table 3 . We use default values, instead of tuning the term weighting parameters, because our focus lies in evaluating our query reformulation technique, and not in optimising overall retrieval performance. If these term weighting parameters are optimised, retrieval perfor-mance may be further improved. We measure retrieval performance using the Mean Average Precision (MAP) measure.

After we have examined the retrieval performance associated with SQR across the aforementioned different settings, we compare SQR against the recent Bo1 pseudo-relevance feedback (PRF) model from the DFR framework, which is based on the Bose-Einstein statistics. Bo1 measures the importance of a term by the divergence of its distribution in a pseudo-relevance document set from a random distribution ( Amati, 2003 ). The Bo1 PRF model has been shown to be a very effective and robust query expansion mechanism ( Amati, 2003; Amati &amp; van Rijsbergen, 2002; Macdonald et al., 2005; Plachouras, He, &amp; Ounis, 2004 ). Bo1 estimates the weight w ( t ) of term t in the expanded query as follows: where tfx is the frequency of the query term in the x top-ranked documents, P of the term in the collection, and N is the number of documents in the collection. The Bo1 PRF mechanism expands the query by merging the extracted terms with the original query terms. The query term weight qtw in the expanded query is given by the following formula: the term with the maximum w ( t ) in the top-ranked documents. If an original query term does not appear in the most informative terms extracted from the top-ranked documents, its query term weight remains equal to the original one.

Bo1 has two parameters, namely the relevant terms added, and the top retrieved documents from which those relevant terms are selected. These two parameters are given as a ratio of relevant terms / top retrieved documents. We tune this ratio empirically, on the basis of the corresponding relevance assessments available for the queries and collections employed, so as to maximise retrieval performance. This provides us with a strong PRF baseline, against which we can compare our proposed SQR technique. The selected most relevant terms from the top retrieved documents are displayed in Table 4 .

In order to undergo a fair comparison between SQR and PRF, we empirically select the settings associated with the highest retrieval performance for SQR, on the basis of the language sample size and top k shallow syntactic blocks.

To recapitulate, we organise our evaluation in three parts, which are listed below. It should be noted that each part uses a different baseline.
 Part 1 (Section 5.2.1 ): we use different term weighting models at default settings as a baseline. We apply
SQR on top of these weighting models, and investigate the effect of varying two parameters of SQR, namely (i) the size of the language sample from which shallow syntactic evidence is induced, and (ii) the number k of top most frequent POS blocks used to reformulate the queries. We vary language sample size by using five differently sized language samples. We also test the effect of employing different values of the top k most frequent POS blocks upon SQR, by testing five different k values. This part answers our first research ques-tion, which was presented in Section 3.2 (page 7).
 Part 2 (Section 5.2.2 ): we use the Bo1 PRF model, set at empirically optimised settings, as our new baseline.
Against this new baseline, we compare the SQR runs that are associated with the highest retrieval perfor-mance, as per Part 1. This part answers our second research question, which was presented in Section 3.2 (page 7).

Part 3 (Section 5.2.3 ): we select the single best between PRF and SQR as our new baseline. Against this new baseline, we now compare a technique that combines SQR + PRF. For the SQR + PRF combination, we use the previously reported optimal settings of SQR and PRF (Part 2). This part answers our third research question, which was presented in Section 3.2 (page 7).
 5.2. Evaluation results 5.2.1. Evaluation of SQR with respect to (i) language sample size, and (ii) the number k of top most frequent POS blocks
In our evaluation experiments, we first set out to investigate the effect of the size of the language sample used to draw shallow syntactic evidence for SQR, upon retrieval performance. To that end, we employ a base-line which consists of the three term weighting models, with each of the two test collections, and the original queries. We apply SQR using shallow syntactic evidence drawn from five language samples of different sizes.
Additionally, we test the effect of varying the number k of top most frequent POS blocks used for SQR upon retrieval performance. Tables 5 and 6 contain the corresponding MAP scores, when retrieving from the WT2G and WT10G test collections, respectively.

From Tables 5 and 6 we see that, overall, SQR can markedly improve retrieval performance, compared to the baseline consisting solely of the weighting models. The size of the language sample from which we draw shallow syntactic evidence does not seem to have a great effect on retrieval performance, as can be seen from the variance of the MAP scores across the language samples and as per weighting model used (column r
Tables 5 and 6 ). Specifically, there is little variation in retrieval performance as language sample size changes, suggesting that the language samples used are representative of general language use, to the extent that their size does not affect their representativeness. Tables 5 and 6 also indicate that when we vary the amount of shallow syntactic evidence employed, in terms of the k most probable POS blocks, retrieval performance remains relatively unaffected, as can be seen from the variance of the MAP scores across the range of k values
Selecting the top 5 and top 10 most probable POS blocks works best, when retrieving relevant documents both from WT2G and WT10G (see shaded cells in Tables 5 and 6 ). This may be explained by the fact that there is a noted division in the way POS blocks are distributed, in terms of their frequency and frequency rank. As Fig. 1 graphically displays, there are a few very highly probable POS blocks, after which the remaining
POS blocks decrease in probability of occurrence sharply. The top 5 and top 10 POS blocks that seem to work best for SQR correspond to these few highly probable POS blocks.

Additionally, we observe that all three term weighting models perform relatively consistently with SQR, throughout the variations in language sample size and number k of most frequent POS blocks used. TF  X  IDF and BM25 are shown to benefit less from SQR than does PL2. Notably, PL2 with SQR marks the overall highest MAP score, when retrieving relevant documents both from WT2G and WT10G.

Overall, SQR is associated with an improvement in retrieval performance over the baseline of using solely weighting models, which is sometimes statistically significant, with one exception. This exception consists of the extreme case of using the largest of the used k values of POS blocks, namely 100, from the smallest lan-guage sample, namely LS1. In this case, SQR results in a slight deterioration in retrieval performace for
TF  X  IDF and BM25 only, and solely with the WT2G test collection ( Table 5 ). This case is described as extreme for the following two reasons: (i) the language sample used represents an atypical domain of language, namely the spoken language used (ii) by inducing the highest of the used values k , namely 100, POS blocks from such an atypical (language-
Additionally, from Tables 5 and 6 , we can see clearly that inducing shallow syntactic evidence from the test collection used for retrieval does not affect SQR performance. This observation provides an answer to our first research question, which was posed in Section 3.2 . The best MAP score when retrieving documents from the
WT2G test collection is associated with POS blocks induced from LS5, which is the WT10G collection; sim-ilarly, the best MAP score when retrieving documents from the WT10G test collection is associated with POS blocks drawn from LS4, which is the WT2G collection. This may be explained by the fact that, generally, the lexical properties of test collections might differ from one test collection to another, as some collections might include unique terms that other collections may not include. On the contrary, the shallow syntactic evidence used by SQR is not unique to specific collections, but actually present in all of them. This follows from the fact that POS blocks are very highly recurrent in language. The specific type of POS blocks used in SQR consists of combinations of 14 unique types of part of speech, which are contained in Table 1 . Put more simply, it is as if a whole document collection contained solely 14 unique terms. SQR extracts POS blocks of those 14 unique terms (which are in fact the 14 parts of speech). Since the POS blocks used in SQR are present in all language samples, the only difference between the type of POS blocks induced from different language samples is their frequency ranking. Drawing POS blocks from the same collection that is used to retrieve documents does not affect retrieval performance, because it does not add any unique information to the query that the other lan-guage samples do not. This reasoning is based on the assumption that shallow syntactic evidence is induced from representative language samples.

The above observations collectively indicate that the proposed SQR technique is a robust automatic query reformulation technique for IR. 5.2.2. Evaluation of SQR with respect to pseudo-relevance feedback (PRF) Having thus  X  X oadtested X  our SQR technique, we now wish to compare it against a strong, state-of-the-art
PRF technique. To this end, and in order to have a fair comparison between the two, we use the tuning set-tings that are associated with the best retrieval performance, for both PRF and SQR, as mentioned in Section 5.1 . We optimise Bo1, so as to have a strong baseline against which to compare our empirically-tuned SQR technique. Also, we empirically select the best reported settings for SQR (as per Tables 5 and 6 ), namely: (i) k = 5, and language sample size = 10GB (LS5), when retrieving from WT2G, and (ii) k = 5, and language sample size = 2GB (LS4), when retrieving from WT10G.
This empirical selection is the choice of language sample size and k settings, associated with the best retrie-val performance, as displayed in Tables 5 and 6 . Table 7 displays the MAP scores of the optimised PRF and SQR runs, separately for WT2G and WT10G.

From Table 7 we can conclude that our proposed SQR technique is at least comparable to the Bo1 PRF mechanism. For the WT2G collection, SQR is almost negligibly outperformed by PRF with TF-IDF and
BM25, but outperforms PRF with PL2, and achieves the best overall performance. With the WT10G collec-tion, SQR outperforms PRF at all times, with a statistically significant best overall score for PL2. The general trend emerging from these runs is that SQR benefits PL2 much more that it benefits TF-IDF and BM25. Over-all, our proposed SQR technique is shown to perform satisfactorily, robustly, and comparably to a strong
PRF baseline, as it outperforms PRF, for the majority of runs, while also producing the best overall MAP score, for both test collections. 5.2.3. Evaluation of SQR and PRF combined
Having tested the robustness and effectiveness of our proposed SQR as a query reformulation technique for IR, we assess its compatibility with conventional PRF. By doing so, we wish to evaluate the theoretical assumption of our SQR model, which holds the following: just as in natural language, communication is achieved by the combination of lexical and shallow syntactic features, similarly in IR, the automatic processing of lexical information may be successfully assisted, in a compatible way, by the automatic processing of shallow syntactic information. We test this by combining SQR with PRF, and comparing them to the best corresponding run achieved either by SQR or PRF alone. By  X  X ombining X , we denote the process of: (i) dropping query terms using SQR, followed by; (ii) expanding the remaining query terms using PRF.

Table 8 includes the relevant MAP scores. For the combined SQR + PRF runs, we use the same weighting model and PRF parameters that we used for the corresponding single SQR and PRF runs separately ( Tables 3 and 4 , p. 16).

Table 8 reveals that SQR can be combined with PRF successfully, indicating that the two techniques com-plement one another. The MAP scores of the merged SQR and PRF runs achieved lead to a pronounced improvement in retrieval performance. More importantly, we see that lexical relevance feedback and shallow syntactic query reformulation can work together successfully, in an equally robust way.
 Table 9 displays the difference of: (1) the best SQR, (2) the best PRF,and (3) the best PRF + SQR runs in
MAP over a baseline that uses only the weighting model. The best SQR and PRF scores are displayed in Table 7 , while the best PRF + SQR scores are displayed in Table 8 . The baseline scores that use only the weighting: model are displayed in Tables 5 and 6 , for WT2G and WT10G, respectively. We observe that: (i) PL2 benefits from SQR more than do TF  X  IDF and BM25, for both collections, and (ii) PL2 benefits from PRF less than do TF  X  IDF and BM25, for both collections. This difference seems to
Finally, we summarise our results and present them alongside those related to shorter queries, namely T and TD. For these runs, we employ the same default values of all weighting model and PRF parameters dis-played in Tables 3 and 4 , respectively (p. 16). We compare the retrieval performance associated with T and TD queries, without and with PRF, to the best retrieval performance associated with long queries (TDN), when using either SQR alone, or SQR + PRF. Table 10 displays the relevant scores associated with these runs.
The retrieval performance scores displayed in Table 10 indicate the following points: (i) TDN queries assisted by SQR perform better than the best-performing between T and TD queries that (ii) TDN queries assisted by SQR + PRF perform markedly better than the best-performing between T and
Note that the MAP scores displayed in the SQR + PRF column of Table 10 compare favourably to the high-scoring equivalent TREC runs, namely, 0.324 (and 0.383 when using Web evidence) ( Robertson &amp;
Walker, 2000 ), for TREC-8, and 0.269 ( Fujita, 2001 ), for TREC-9. On these grounds, we may conclude that the underlying assumption implemented in SQR, namely the fact that content fragments may be approxi-mately identified on the basis of shallow-syntactic recurrence statistics, seems to be valid. This is justified by the experimental evidence presented in this section, which shows that long queries, when assisted by SQR, tend to outperform shorter queries, when in fact, shorter queries are proven to be most efficient.
We have thus seen that SQR is a robust query reformulation technique, which is not greatly affected by the size of the language sample from which shallow syntactic evidence is drawn, provided that this sample is rep-resentative of language use in general. This point means that SQR is a flexible automatic query reformulation technique, portable to language samples of different size. Additionally, SQR works similarly robustly across a varied number of top most probable POS blocks used as shallow syntactic evidence. The advantage of this feature is that SQR can lead to improved retrieval performance, using few POS blocks, which keeps the pro-cessing time and general computational cost associated at low levels. We have also shown that the assistance of SQR to retrieval performance is due to the fact that it reduces the amount of noise in the queries success-fully, since it outperforms even shorter queries, that are generally considered more effective. Finally, the eval-uation results presented in this work indicate that SQR is at least comparable to, if not significantly better than, the effective Bo1 PRF technique, while the combination of the two proves to be overwhelmingly effective in terms of retrieval performance. 6. Applications
In this paper, we have applied SQR to long queries, for two main reasons. Firstly, long queries are more likely to contain full sentences, which we can POS tag and hence extract POS blocks from, than shorter que-ries. Secondly, long queries are more likely to contain noise, which we can attempt to reduce using POS blocks, than shorter queries. The type of long queries used in our experiments are typical of the TREC ad hoc style, which was created by TREC as part of a controlled experimental environment, with the aim to allow for a measurable and analytical evaluation of IR as a process ( Voorhees &amp; Harman, 2005 ). It is exactly under the same light that we consider long queries, as the best setting which would allow us to measure and analyse our motivation, namely that there seems to exist an approximately proportional relation between the fre-quency and content associated to POS blocks (see Section 3.1 ). Hence, in this paper we report on the valida-tion of our assumption, within a controlled experimental setting of long queries. We consider this to be a first step towards applying this assumption in different ways, and towards more realistic retrieval applications, such as a filter that cleans document collections from noise, or as a criterion of index compression, for example.
Indeed, we successfully applied the latter in our participation in the Terabyte ad hoc track of TREC 2006 summaries have been used ( Sakai &amp; Sparck Jones, 2001 ). Such application, namely index filtering, indexing compression, summary-assisted PRF, have been reported to be very beneficial for early-precision, a feature which is most attractive to real-user retrieval tasks, since most users do not look further than the top ten-twenty retrieved documents. Lastly, more general applications of our assumption could be found in fields that aim to automatically identify and process content, such as document summarisation, for example. 7. Conclusion
We described a syntactically-based query reformulation (SQR) technique, applied to enhance the perfor-mance of an IR system. We formulated three research questions, namely: (i) is the size of the language sample from which shallow syntactic evidence is drawn important? (ii) is SQR an effective query reformulation tech-nique for IR? (iii) can SQR be successfully combined with pseudo-relevance feedback? We investigated these questions using five language samples of different sizes, across two standard test collections, and three statis-tically different term weighting models. We compared SQR to a state-of-the-art pseudo-relevance feedback (PRF) technique, firstly by comparing the two techniques to one another, and secondly by merging them into one relevance feedback combination. We foun that SQR is generally not affected by the size of the language sample from which shallow syntactic evidence is drawn, nor by any linguistic similarity between the language sample and the test collection used for retrieval. Moreover, experimental findings indicate that SQR is an over-all robust and effective query reformulation technique. Additionally, SQR is shown to be at least comparable, if not significantly better, to a good PRF technique, and even to complement PRF, as the best overall retrieval performance is marked when both techniques are merged.

In the future, we wish to investigate combining POS blocks of different lengths for SQR, as well as spec-ifying shallow syntactic categories within the POS blocks used in SQR.
 References
