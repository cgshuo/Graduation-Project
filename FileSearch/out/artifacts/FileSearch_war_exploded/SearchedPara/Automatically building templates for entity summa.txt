 1. Introduction
In this paper, we study the task of automatically generating templates for entity summaries. An entity summary is a short tions are examples of entity summaries we consider. Summaries of entities from the same category usually share some com-mon structure. For example, biographies of physicists usually contain facts about the nationality, educational background, affiliation and major contributions of the physicist, whereas introductions of companies usually list information such as the industry, founder and headquarter of the company. Our goal is to automatically construct a summary template that out-lines the most prominent types of facts for an entity category, given a collection of entity summaries from this category.
Such kind of summary templates can be very useful in many applications. First of all, they can uncover the underlying structures of summary articles and help better organize the information units, much in the same way as infoboxes do in Wikipedia. In fact, automatic template generation provides a solution to induction of infobox structures, which are still highly incomplete in Wikipedia Wu and Weld (2007) . A template can also serve as a starting point for human editors to cre-ate new summary articles. Furthermore, with summary templates, we can potentially apply information retrieval and extraction techniques to construct summaries for new entities automatically on the fly, improving the user experience for search engine and question answering systems.
 Despite its usefulness, the problem has not been well studied. The most relevant work is by Filatova, Hatzivassiloglou, and McKeown (2006) on automatic creation of domain templates, where the definition of a domain is similar to our notion of an  X  frequent parse tree patterns from sentences containing these verbs to construct a domain template. There are two major lim-come these two limitations. Automatic template generation is also related to a number of other problems that have been studied before, including unsupervised IE pattern discovery Sudo, Sekine, and Grishman (2003), Shinyama and Sekine (2006), Sekine (2006), Yan, Okazaki, Matsuo, Yang, and Ishizuka (2009) and automatic generation of Wikipedia articles Sau-per and Barzilay (2009) . We discuss the differences of our work from existing related work in Section 7 .
In this paper we propose a novel approach to the task of automatically generating entity summary templates. We first develop an entity-aspect model that extends standard LDA to identify clusters of words that can represent different aspects bel X  X  may be clustered together from biographies of physicists to represent one aspect, even though they may appear in dif-ferent sentences from different biographies. Simultaneously, the entity-aspect model separates words in each sentence into background words, document words and aspect words, and sentences likely about the same aspect are naturally clustered together. After this aspect identification step, we mine frequent subtree patterns from the dependency parse trees of the clustered sentences (Section 4 ). Different from previous work, we leverage the word labels assigned by the entity-aspect model to prune the patterns and to locate template slots to be filled in.

We evaluate our method on five entity categories using Wikipedia articles (Section 5 ). Because the task is new and thus there is no standard evaluation criteria, we conduct both quantitative evaluation using our own human judgment and qual-itative comparison. Our evaluation shows that our method can obtain better sentence patterns in terms of f1 measure com-pared with two baseline methods, and it can also achieve reasonably good quality of aspect clusters in terms of purity.
Compared with standard LDA and K-means sentence clustering, the aspects identified by our method are also more mean-ingful. Finally, we build an application which leverage our generated templates to construct entity summarization system.
The results further prove that our automatically generated entity templates are useful (Section 6 ). 2. The task
Given a collection of entity summaries from the same entity category, our task is to automatically construct a summary template that outlines the most important information one should include in a summary for this entity category. For exam-physicist include his/her educational background, affiliation, major contributions, awards received, etc.
However, it is not clear what is the best representation of such templates. Should a template comprise a list of subtopic of the templates as well as our observations from Wikipedia entity summaries. First, since we expect that the templates can be used by human editors for creating new summaries, we use sentence patterns that are human readable as basic units of the templates. For example, we may have a sentence pattern  X  X  ENT graduated from? University X  X  for the entity category we observe that information about entities of the same category can be grouped into subtopics. For example, the sentences  X  X  X ohr is a Nobel laureate X  X  and  X  X  X instein received the Nobel Prize X  X  are paraphrases of the same type of facts, while the describe the person X  X  educational background. Therefore, it makes sense to group sentence patterns based on the subtopics they pertain to. Here we call these subtopics the aspects of a summary template.

Formally, we define a summary template to be a set of sentence patterns grouped into aspects. Each sentence pattern has a placeholder for the entity to be summarized and possibly one or more template slots to be filled in. Table 1 shows some sentence patterns our method has generated for the  X  X  X hysicist X  X  category.
 2.1. Overview of our method
Our automatic template generation method consists of two steps: 2.1.1. Aspect identification
In this step, our goal is to automatically identify the different aspects or subtopics of the given summary collection. We simultaneously cluster sentences and words into aspects, using an entity-aspect model extended from the standard LDA model that is widely used in text mining Blei, Ng, and Jordan (2003) . The output of this step are sentences clustered into aspects, with each word labeled as a stop word, a background word, a document word or an aspect word. 2.1.2. Sentence pattern generation
In this step, we generate human-readable sentence patterns to represent each aspect. We use frequent subtree pattern of aspect words, background words and stop words, while document words become template slots whose values can vary from summary to summary. 3. Aspect identification lection. Here we propose a principled method based on a modified LDA model to simultaneously cluster both sentences and words to discover aspects.

We first make the following observation. In entity summaries such as the introduction sections of Wikipedia articles, category, some words are generally used in all aspects of the collection. Third, some words are clearly associated with the are specifically associated with the subject entities being discussed.

To capture background words and document specific words, Chemudugunta, Smyth, and Steyvers (2007) proposed to introduce a background topic and document-specific topics. Here we borrow their idea and also include a background topic as well as document-specific topics. To discover aspects that are local to one or a few adjacent sentences but may occur in many documents, Titov and McDonald (2008) proposed a multi-grain topic model, which relies on word co-occurrences within short paragraphs rather than documents in order to discover aspects. Inspired by their model, we rely on word co-occurrences within single sentences to identify aspects. 3.1. Entity-aspect model
We now formally present our entity-aspect model. First, we assume that stop words can be identified using a standard nomial word distributions). There is a background model / B that generates words commonly used in all documents and all aspects. There are D document models w d (1 6 d 6 D ), where D is the number of documents in the given summary collection, and there are A aspect models / a (1 6 a 6 A ), where A is the number of aspects. We assume that these word distributions have a uniform Dirichlet prior with parameter b .
 multinomial distribution p that controls in each sentence how often we encounter a background word, a document word, or an aspect word. p has a Dirichlet prior with parameter c .

Let S d denote the number of sentences in document d , N d , s denote the number of words (after stop word removal) in sen-is generated from the background model, the document model, or the aspect model. Table 3 shows the process of generating
The number of aspects A is also manually set. 3.2. Inference
With the assignment, sentences are naturally clustered into aspects, and words are labeled as either a background word, a document word, or an aspect word.
 using Gibbs sampling, which is commonly used for inference for LDA models Griffiths and Steyvers (2004) . Due to space lim-it, we give the formulas for the Gibbs sampler below without derivation.
 following formula:
We then sample a value for y d , s , n for each word in the current sentence using the following formulas: excluded.

With one Gibbs sample, we can make the following estimation: Here the counts include all sentences and all words.

In our experiments, we set a =5, b = 0.01 and c = 20. We run 100 burn-in iterations through all documents in a collection samples, and average over these 10 samples to get the estimation for the parameters.
 clustered into A aspects, where each word is labeled as either a stop word, a background word, a document word or an aspect word. 3.3. Comparison with other models
A major difference of our entity-aspect model from standard LDA model is that we assume each sentence belongs to a single aspect while in LDA words in the same sentence can be assigned to different topics. Our one-aspect-per sentence assumption is important because our goal is to cluster sentences into aspects so that we can mine common sentence patterns for each aspect.

To cluster sentences, we could have used a straightforward solution similar to document clustering, where sentences are represented as feature vectors using the vector space model, and a standard clustering algorithm such as K-means can be applied to group sentences together. However, there are some potential problems with directly applying this typical docu-ment clustering method. First, unlike documents, sentences are short, and the number of words in a sentence that imply its aspect is even smaller. Besides, we do not know the aspect-related words in advance. As a result, the cosine similarity be-tween two sentences may not reflect whether they are about the same aspect. We can perform heuristic term weighting, but the method becomes less robust. Second, after sentence clustering, we may still want to identify the aspect words in each ter as aspect words may not work well even after stop word removal, because there can be background words commonly used in all aspects. 4. Sentence pattern generation At the pattern generation step, we want to identify human-readable sentence patterns that best represent each cluster. we use dependency parse trees.
 other hand, document words are entity-specific and therefore should not appear in the generic template patterns; instead, they correspond to template slots that need to be filled in. Furthermore, since we work on entity summaries, in each sen-tence there is usually a word or phrase that refers to the subject entity, and we should have a placeholder for the subject entity in each pattern.

Based on the intuitions above, we have the following sentence pattern generation process. to locate the subject entities: For each summary document, we first find the top three frequent base noun phrases that are subjects of sentences. For example, in a company introduction, the phrase  X  X  X he company X  X  is probably used frequently as a sentence subject. We use Stanford typed dependencies to help locate subject of the sentence. Then for each sentence, we one of the top three subject base noun phrases occurs, and if so, it is tagged as the subject entity. Otherwise, we tag the assigned by the entity-aspect model with a new label E . 2. Reconcile document words : Although entity aspect model can recognize document words in each sentence, we still need to define some heuristic rules to improve this labeling precision. For example, a labeled sentence  X  X  X e/E is/S currently/A working/D on/S quantum/B computation/D X  X , here word  X  X  X orking X  X  should be tagged as aspect word. Also in this sentence  X  X  X dobe/E was/S founded/A in/S December/D 1982/A X  X , the word  X  X 1982 X  X  should be tagged as document word. These wrong labeling will led to generate meaningless patterns. So we need to reconcile wrong document words labeling. Firstly if the word tagged as Document word, but its POS is verb, then change its label to aspect word, else if its POS is Adverb, then change its label to background word. Secondly, recognize Time phrase, then for the word contained by Time phrase tagged as document word. tence we obtain a dependency parse tree where each node is a single word and each edge is labeled with a dependency rela-words labeled with D by a question mark to indicate that these correspond to template slots. For the other words, we attach their labels to the tree nodes. Fig. 2 shows an example labeled dependency parse tree. 4. Prune dependency parse trees : We found that long complex sentence may produce wrong patterns. For example,  X  X  X hile/
S working/B for/S General_Electric/D,/S Giaever/E earned/A a/S Ph.D./A from/S the/S Rensselaer_Polytechnic_Institute/D in/S 1964/D X  X , would get the pattern like  X  X ? ENT earned from? in? X  X . This pattern cannot easily recognized as good pattern by human, so we need extract important clauses and remove redundant subtree from original dependency tree. we lever-age English grammatical rules to find clauses, the tree prune procedure is below, frequent subtree pattern mining algorithm proposed in Zaki (2002) , to find all subtrees with a minimum support of K . 6. Prune patterns : We remove subtree patterns found by FREQT that do not contain ENT or any aspect word. We also remove small patterns that are contained in some other larger pattern in the same cluster. 7. Covert subtree patterns to sentence patterns : The remaining patterns are still represented as subtrees. To covert them back to human-readable sentence patterns, we map each pattern back to one of the sentences that contain the pattern to order the tree nodes according to their original order in the sentence.

In the end, for each summary collection, we obtain A clusters of sentence patterns, where each cluster presumably cor-responds to a single aspect or subtopic. 5. Evaluation
Because we study a non-standard task, there is no existing annotated data set. We therefore created a small data set and made our own human judgment for quantitative evaluation purpose. 5.1. Data
We downloaded five collections of Wikipedia articles from different entity categories. To obtain these collections, we set are given in Table 4 . 5.2. Quantitative evaluation
To quantitatively evaluate the summary templates, we want to check (1) whether our sentence patterns are meaningful and can represent the corresponding entity categories well, and (2) whether semantically related sentence patterns are grouped into the same aspect. It is hard to evaluate both together. We therefore separate these two criteria. 5.2.1. Quality of sentence patterns percentage of our sentence patterns that are meaningful, and the percentage of true meaningful sentence patterns of each category that our method can capture. The former is relatively easy to obtain because we can ask humans to judge the qual-ity of our patterns. The latter is much harder to compute because we need human judges to find the set of true sentence patterns for each entity category, which can be very subjective.

We adopt the following pooling strategy borrowed from information retrieval. Assume we want to compare a number of methods that each can also generate sentence patterns from a summary collection. We take the union of the patterns generated by these different methods and mix them randomly. We then ask a human judge to decide whether each sentence pooling strategy should suffice.
 We compare our method with the following three baseline methods:
Baseline 1 : In this baseline, we use the same subtree pattern mining algorithm to find sentence patterns from each sum-mary collection. We also locate the subject entities and replace them with ENT . However, we do not have aspect words or document words in this case. Therefore we do not prune any pattern except to merge small patterns with the large ones that contain them, and the patterns generated by this method do not have template slots.

Baseline 2 : In the second baseline, we apply a verb-based pruning on the patterns generated by the first baseline, similar (2006) , and then prune patterns that do not contain any of the top-20 verbs. collection, and M ( v i ) is the number of documents in the collection that contains v i .

Baseline 3 : In the third baseline, we replace subtree pattern mining algorithm with sequence mining algorithm, we use a tool named  X  X  X refixspan X  X  4 to find sentence patterns. Other processing procedures are the same as our method.
In Table 5 , we show the precision, recall and F1 of the sentence patterns generated by our method and the three baseline methods for the five categories. For our method and baseline 1 and 2 methods, we set the minimal support of the subtree line 3 method, we set minimal support K is 3 and minimal pattern length L is 3; We can see that overall our method gives better precision and f1 measures than the three baseline methods for most categories. Our method achieves a good balance between precision and recall. BL-1 have a higher recall than our method. It is expected because our method does more pat-sequence patterns from the original sentences cannot capture the syntactic relations between words on dependency parse trees.

There are some advantages of our method that are not reflected in Table 5 . First, many of our patterns contains template slots, which make the pattern more meaningful. In contrast the baseline patterns do not contain template slots. Because the human judge did not give preference over patterns with slots, both  X  X  ENT won the award X  X  and  X  X  ENT won the? award X  X  were judged to be meaningful without any distinction, although the former one generated by our method is more meaningful.
Second, compared with BL-2, our method can obtain patterns that do not contain a non-auxiliary verb, such as  X  X  ENT was? director X  X . 5.2.2. Quality of aspect clusters
We also want to judge the quality of the aspect clusters. To do so, we ask the human judges to group the ground truth sentence patterns of each category based on semantic relatedness. We then compute the quality of our sentence pattern clusters compared with the human created clusters. We use purity as our evaluation measure. The results are shown in Table 6 .
 5.3. Qualitative evaluation
In Table 7 , we show the top-10 frequent words of three sample aspects as found by our method, a standard LDA method, and a K-means method. 6. Application 6.1. Overview
In this section, we study how to use the templates generated by our method to construct summaries for new entities. Our goal is to measure the impact of automatically generated templates on improving extractive summarization. We propose a novel template-based approach which consists of three main steps listed below: vious generated patterns.
 Sentence ranking : In this step, we use the selected patterns as hints to get ranked sentences returned by search engine.
Sentence selection : Finally, we select related sentences from ranked sentence list. We use Integer Linear Programming posed by Sauper and Barzilay (2009) .

We evaluate our method using Wikipedia. Our evaluation shows that our method obtains better ROUGE recall score com-pared with baseline method. 6.2. Pattern selection the most representative and generalized patterns for reconstructing summary sentences. Firstly, we run sequence mining algorithm using the tool prefixspan 5 on the pattern collection to find frequent subpatterns, then remove small subpatterns tains top 15 keywords, the top 15 keywords are computed based on the frequency after remove stop words from original pat-6.3. Sentence ranking
After finding the most representative and generalized patterns, we replace  X  X  ENT  X  X  with concrete entity Mention and re-known for  X  X , then send it to Google API again. we get ranked sentence list for each aspect from the returned snippets. 6.4. Sentence selection
We model sentence selection in Integer linear programming framework and solve it using a open source tool named otherwise. Our objective function is to minimize the ranks of the sentences selected for the final entity summary: 6.4.1. Exclusivity constraints 6.4.2. Redundancy constraints
Our similarity function is define as below, 6.5. Evaluation
To assess the quality of the resulting overview articles, we compare them with the original human-authored entity sum- X  X  X tephen Hawking X  X , then automatic evaluate summarization performance using ROUGE toolkit Lin (2004) . ROUGE is an eval-uation metric employed at the Text Analysis Conference summarization task, which assumes that proximity to human-authored text is an indicator of summary quality. We use ROUGE toolkit to compute recall, precision, and F-score for ROUGE-1. The results are shown in Table 8 .

To assess the usefulness of sentence selection model in entity summary generation process, we design a baseline method which select sentences from snippets using regular expression to do pattern matching and other parts are the same as our method. Table 9 shows the baseline results.

This evaluation results in Table 8 indirectly prove our patterns are useful in summarization scenario. However, the ROUGE score shows that our template based entity summary generation procedure still need improvement in the future. 7. Related work
The most related existing work is on domain template generation by Filatova et al. (2006) . There are several differences between our work and theirs. First, their template patterns must contain a non-auxiliary verb whereas ours do not have this patterns are fully independent of each other, whereas we group semantically related patterns into aspects, which provides users with a more meaningful representation as well as alternative patterns to state the same fact. Third, in their work, named entities, numbers and general nouns are treated as template slots. In our method, we apply the entity-aspect model to automatically identify words that are document-specific, and treat these words as template slots. As we have shown ear-uments are event-centered while ours are entity-centered. Therefore we can use heuristics to anchor our patterns on the subject entities.

Sauper and Barzilay (2009) proposed a framework for learning to automatically generate Wikipedia articles. There is a fundamental difference between their task and ours. The articles they generate are long, elaborate documents consisting ing Wikipedia articles. We focus on learning sentence patterns of the short, concise introduction section of Wikipedia articles.

Our entity-aspect model is related to a number of previous extensions of LDA models. Chemudugunta et al. (2007) pro-posed to introduce a background topic and document-specific topics. Our background and document language models are and McDonald (2008) exploited the idea that a short paragraph within a document is likely to be about the same aspect. Our tence patterns. The way we separate words into stop words, background words, document words and aspect words bears similarity to that used in Daum X  and Marcu (2006, 2009) , but their task is multi-document summarization while ours is to induce summary templates. 8. Conclusions
In this paper, we studied the task of automatically generating templates for entity summaries. We proposed an entity-aspect model that can automatically cluster sentences and words into aspects. The model also labels words in sentences as either a stop word, a background word, a document word or an aspect word, We then applied frequent subtree pattern mining to generate sentence patterns that can represent the aspects. We took advantage of the labels generated by the en-tity-aspect model to prune patterns and to locate template slots. We conducted both quantitative and qualitative evaluation using five collections of Wikipedia entity summaries. We found that our method gave overall better template patterns than two baseline methods, and the aspects clusters generated by our method are reasonably good.

There are a number of directions we plan to pursue in the future in order to improve our method. First, we can possibly apply linguistic knowledge to improve the quality of sentence patterns. Currently the method may generate similar sentence sentences. For example, a sentence pattern may contain an adjective but not the noun it modifies. We plan to study how to use linguistic knowledge to guide the construction of sentence patterns and make them more meaningful. Second, we have not quantitatively evaluated the quality of the template slots, because our judgment is only at the whole sentence pattern level. We plan to get more human judges and more rigorously judge the relevance and usefulness of both the sentence pat-rather than treating all words labeled with D as template slots.
 References
