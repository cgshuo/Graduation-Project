 Similarity-based neighborhood methods , a simple and popu-lar approach to collaborative filtering, infer their predictions by finding users with similar taste or items that have been similarly rated. If the number of users grows to millions, the standard approach of sequentially examining each item and looking at all interacting users does not scale. To solve this problem, we develop a MapReduce algorithm for the pairwise item comparison and top-N recommendation prob-lem that scales linearly with respect to a growing number of users. This parallel algorithm is able to work on parti-tioned data and is general in that it supports a wide range of similarity measures. We evaluate our algorithm on a large dataset consisting of 700 million song ratings from Yahoo! Music.
 H.4.m [ Information Systems Applications ]: Miscella-neous Scalable Collaborative Filtering, MapReduce
Today X  X  internet users face an ever increasing amount of data, which makes it constantly harder and more time con-suming to pick out the interesting pieces of information from all the noise. This situation has triggered the development of recommender systems: intelligent filters that learn about the users X  preferences and figure out the most relevant in-formation for them.

With rapidly growing data sizes, the processing efficiency and scalability of the systems and their underlying compu-tations becomes a major concern. In a production environ-ment, the offline computations necessary for running a rec-ommender system must be periodically executed as part of larger analytical workflows and thereby underly strict time and resource constraints. For economic and operational rea-sons it is often undesirable to execute these offline compu-tations on a single machine: this machine might fail and with growing data sizes constant hardware upgrades might be necessary to improve the machine X  X  performance to meet the time constraints. Due to these disadvantages, a single machine solution can quickly become expensive and hard to operate.

In order to solve this problem, recent advances in large scale data processing propose to run data-intensive, analyt-ical computations in a parallel and fault-tolerant manner on a large number of commodity machines. Doing so will make the execution independent of single machine failures and will furthermore allow the increase of computational performance by simply adding more machines to the cluster, thereby obviating the need for constant hardware upgrades to a single machine. Another economic advantage of such an approach is that the cluster machines can be temporally rented from a cloud computing infrastructure provider.
When applied to recommender systems, this technical ap-proach requires the rephrasing of existing algorithms to en-able them to utilize a parallel processing platform. Such platforms are able to run on a cluster of up to several thou-sand machines and to store and process amounts of data that were previously considered unmanageable. They typi-cally employ a shared-nothing architecture together with a parallel programming paradigm and store the data in repli-cated partitions across the cluster. They provide the de-sired horizontal scalability when the number of machines in the cluster is increased. Furthermore they relieve the pro-grammer from having to cope with the complicated tasks of scheduling computation, transferring intermediate results and dealing with machine failures.

We rephrase and scale out the similarity-based neighbor-hood methods, a standard approach in academic literature [22]. They have the advantage of being simple and intuitive to understand, as they are directly inspired by recommen-dation in everyday life, where we tend to check out things we heard about from like-minded friends or things that seem similar to what we already like. They capture local associ-ations in the data which increases serendipity [22] and they are necessary as part of ensembles to reach optimal predic-tion quality [5]. The item-based variants [23] of the neigh-borhood methods are highly stable and allow computation of recommendations for new users without the need to rebuild the model. Additionally, they are able to provide instant justifications for their recommendations by presenting the list of neighbor items and the ratings the user already gave to these as explanation. Due to these properties, neighbor-hood methods are often preferred in industrial use cases [19, 1, 24, 7], although alternative approaches such as the latent factor models are superior in the task of predicting ratings.
We improve the scalability of the similarity-based neigh-borhood methods by rephrasing the underlying algorithm for pairwise comparisons to MapReduce [8], a popular parallel programming paradigm that has originally been proposed by Google. We demonstrate our approach using Apache Hadoop [2], a widely used, open source platform which im-plements the MapReduce paradigm.

We already contributed an implementation of the approach presented here to Apache Mahout [3], an open-source li-brary of scalable data mining algorithms, where it forms the core of the distributed recommender module.

In this paper, we provide the following contributions:
This paper is organized as follows: After a brief intro-duction to the MapReduce paradigm for parallel processing, we describe the algorithmic challenges of our approach in Section 2 and related work in Section 3. We describe and in detail derive our algorithm in Section 4. Finally, we evaluate our solution on various datasets in Section 5.
MapReduce [8], which is inspired by functional program-ming, has become a popular paradigm for data-intensive parallel processing on shared-nothing clusters.

The data to process is split and stored block-wise across the machines of the cluster in a distributed file system (DFS) and is usually represented as ( key,value ) tuples. In order to efficiently parallelize the computation and offer tolerance against machine failures, data is replicated across the clus-ter. As the computation tasks should be moved to the data, the runtime system assigns tasks to process data blocks to the machines holding the replicas of these blocks. The com-putation code is embedded into two functions: map : ( k 1 , v 1 )  X  list( k 2 , v 2 ) reduce : ( k 2 , list( v 2 ))  X  list( v 2 ) The data flow in a MapReduce pass is illustrated in Fig-ure 1. At the beginning the map function is invoked on the input data in parallel on all the participating machines in the cluster. The output tuples are grouped (partitioned and sorted) by their key and then sent to the reducer machines in the shuffle phase. The receiving machines merge the tu-ples and invoke the reduce function on all tuples sharing the same key. The output of that function is written to the distributed file system afterwards.

An optional third function called combine can be spec-ified. It is invoked locally after the map phase and can be used to preaggregate the tuples in order to minimize the amount of data that has to be sent over the network, which is usually the most scarce resource in a distributed environ-ment. combine : ( k 2 , list( v 2 ))  X  list( v 2 ) In addition, Hadoop offers initialize functions that are in-voked before the map and reduce functions and the system provides a means to broadcast small files to all worker ma-chines in the cluster via a distributed cache .
Let A be a | U | X | I | matrix holding all known interactions between a set of users U and a set of items I . A user u is represented by his item interaction history a u  X  , the u -th row of A . The top-N recommendations for this user correspond to the first N items selected from a ranking r u of all items according to how strongly they would be preferred by the user. This ranking is inferred from patterns found in A .
Notation hints: a u  X  denotes the u -th row of the interac-tion matrix A , a  X  i denotes the i -th column of A , | U | denotes the number of users which is equal to the number of rows in A . foreach i  X  v denotes iteration over the indexes of non-zero entries of a vector v , foreach ( i,k )  X  v denotes iteration over the indexes and the corresponding non-zero values of a vector v .
 In order to get a clearer picture of the neighborhood ap-proach, it is useful to express the algorithm in terms of lin-ear algebraic operations. Neighborhood-based methods find and rank items that have been preferred by other users who share parts of the interaction history a u  X  . Let A be a binary matrix with A ui = 1 if a user u has interacted with an item i and A ui = 0 otherwise. For pairwise comparison between users, a dot product of rows of A gives the number of items that the corresponding users have in common. Similarly, a dot product of columns of A gives the number of users who have interacted with both items corresponding to the columns.

When computing recommendations for a particular user with User-Based Collaborative Filtering [21], first a search for other users with similar taste is conducted. This trans-lates to multiplying the matrix A by the user X  X  interaction history a u  X  , which results in a ranking of all users. Sec-ondly, the active user X  X  preference for an item is estimated by computing the weighted sum of all other users X  prefer-ences for this item and the corresponding ranking. In our simple model this translates to multiplying the ranking of all users with A T . This means the whole approach can be summarized by the following two multiplications:
To exploit the higher stability of relations between items, another variant of the neighborhood methods called Item-Based Collaborative Filtering [23] was developed, which looks at items first and weighs their cooccurrences. This approach computes a matrix of item-to-item similarities and allows for fast recommendations as the model does not have to be recomputed for new users. Expressing the item-based ap-proach translates to simply moving the parentheses in our formula, as A T A gives exactly the matrix of the item cooc-currences:
This shows that both user-and item-based collaborative filtering share the same fundamental computational model. In the rest of the paper, we will focus on the more popular item-based variant.
The standard sequential approach [19] for computing the item similarity matrix S = A T A is shown in Algorithm 1.
Algorithm 1 : sequential approach for computing item cooccurrences foreach item i do
For each item i , we need to look up each user u who interacted with i . Then we iterate over each other item j from u  X  X  interaction history and record the cooccurrence of i and j . We can mathematically express the approach using three nested summations:
If we wish to distribute the computation across several machines on a shared-nothing cluster, this approach be-comes infeasible as it requires random access to both users and items in its inner loops. Its random access pattern can-not be realized efficiently when we have to work on parti-tioned data.

Furthermore, at a first glance, the complexity of the item-based approach is quadratic in the number of items, as each item has to be compared with every other item. However, the interaction matrix A is usually very sparse. It is common that only a small fraction of all cells are known 1 and that the number of non-zero elements in A is linear in the number of rows of A . This fact severely limits the number of item pairs to be compared, as only pairs that share at least one inter-acting user have to be taken into consideration. It decreases the complexity of the algorithm to quadratic in the number of non-zeros in the densest row rather than quadratic in the number of columns. The cost of the algorithm is expressed in the datasets we used for our experiments, this ratio varies from 0.1% to 4.5% as the sum of processing the square of the number of in-teractions of each single user. Unfortunately, collaborative filtering datasets share a property that is common among datasets generated by human interactions: the number of in-teractions per user follows a heavy tailed distribution which means that processing a small number of  X  X ower users X  dom-inates the cost of the algorithm.
 We will develop a parallelizable formulation of the compu-tation to scale out this approach on a parallel processing platform. As we need to utilize a distributed filesystem, our algorithm must be able to work with partitioned input data. Furthermore, it must scale linearly with respect to a grow-ing number of users. We will enable the usage of a wide range of similarity measures and add means to handle the computational overhead introduced by  X  X ower users X .
Most closely related to our work is a MapReduce formula-tion of item-based collaborative filtering presented by Jiang et al. [17]. However they do not show how to use a wide va-riety of similarity measures, they do not achieve linear scal-ability as they undertake no means to handle the quadratic complexity introduced by  X  X ower users X  and only present ex-periments on a small dataset. Another distributed imple-mentation of an item-based approach is used by Youtube X  X  recommender system [7], which applies a domain specific way of diversifying the recommendations by interpreting the pairwise item similarities as a graph. Unfortunately this work does not include details that describe how the similar-ity computation is actually executed other than stating it uses a series of MapReduce computations walking through the user/video graph. Furthermore, a very early implemen-tation of a distributed item-based approach was applied in the recommendation system of the TiVo set-top boxes [1], which suggests upcoming TV shows to its users. In a pro-prietary architecture, show correlations are computed on the server side and preference estimation is afterwards con-ducted on the client boxes using the precomputed correla-tions.

There have also been several works on parallelizing latent factor models: The recommender system of Google News [6] uses a MapReduce based implementation combining Prob-abilistic Latent Semantic Indexing and a neighborhood ap-proach with a distributed hashtable that tracks item cooc-currences in realtime. This solution is tailored towards the outstanding infrastructure of Google and might not be prac-tical in other scenarios. Another parallelizable implementa-tion of a latent factor model was presented by Zhou et al. [26], where a factorization of the Netflix dataset using Alter-nating Least Squares is conducted. Mahout [3] contains a MapReduce port of this approach. Similarly, Gemulla et al. [15] propose a stratified version of Stochastic Gradi-ent Descent for matrix factorization on MapReduce. Due to Hadoop X  X  inability to efficiently execute iterative algo-rithms, these implementations show unsatisfactory perfor-mance. Dataflow systems with explicit iteration support such as Stratosphere [12] or specialized systems for machine learning such as GraphLab [20] will pose a solution for the efficient distributed execution of such algorithms in the near future.
This section discusses the step-by-step development of our algorithmic framework. We start with showing how to con-duct distributed item cooccurrence counting for our simple model that uses binary data. After that we generalize the approach to non-binary data and enable the usage of a wide variety of similarity measures. Finally, we discuss means to sparsify the similarity matrix, conduct batch recommen-dation and apply selective down sampling to achieve linear scalability with a growing number of users.
In order to scale out the similarity computation from Al-gorithm 1, it needs to be phrased as a parallel algorithm, to make its runtime speedup proportional to the number of machines in the cluster. This is not possible with the stan-dard sequential approach, as it requires random access to the rows and columns of A in the inner loops of Algorithm 1, which cannot be efficiently realized in a distributed, shared-nothing environment where the algorithm has to work on partitioned data.

We need to find a way of executing this multiplication that is better suited to the MapReduce paradigm and has an access pattern that is compatible to partitioned data. The solution is to rearrange the loops of Algorithm 1 to get the row outer product formulation of matrix multiplication. Because the u -th column of A T is identical to the u -th row of A , we can compute S with only needing access to the rows of A :
Following this finding, we partition A by its rows (the users) and store it in the distributed file system. Each map function reads a single row of A , computes the row X  X  outer product with itself and sends the resulting intermediary ma-trix row-wise over the network. The reduce function simply has to sum up all partial results, thereby computing a row of S per invocation (Algorithm 2).

This approach allows us to exploit the sparsity of the inter-mediary outer product matrices by making the map function only return non-zero entries. At the same time we apply a combiner (which is identical to the reducer) on the vectors emitted by the mappers, which makes the system minimize the amount of data that has to be sent over the network. Additionally we only compute the upper triangular half of S , as the resulting similarity matrix is symmetric.
Real world datasets contain richer representations of the user interactions than a simple binary encoding. They ei-ther consist of explicit feedback like numerical ratings that the users chose from a predefined scale or of implicit feedback where we count how often a particular behavior such as a click or a page view was observed. We need to be able to choose from a variety of similarity measures for comparing these item interactions, in order to be able to find the one that best captures the relationships inherent in the data. From now on, we drop the assumption that A contains only binary entries and assume that it holds such explicit or im-plicit feedback data.

Algorithm 2 : computing item coocurrences function map( a u  X  ): function combine( i,c 1 ,...,c n ): function reduce( i,c 1 ,...,c n ): Expressing arbitrary similarity measures : We incor-porate a wide range of measures for comparing the inter-actions of two items i and j by integrating three canonical functions into our algorithm. We first adjust each item rat-ing vector via a function preprocess() :
Next, the second function norm() computes a single num-ber from the preprocessed vector of an item:
These preprocessing and norm computations are conducted in an additional single pass over the data, which starts with A T , applies the two functions and transposes A T to form A .
The next pass over the data is a modification of the ap-proach presented in Section 4.1. Instead of summing up cooccurrence counts, we now compute the dot products of the preprocessed vectors.
We provide those together with the numbers we computed via the norm function to a third function called similar-ity() which will compute a measure-specific similarity value (Algorithm 3).

With this approach we are able to incorporate a wide vari-ety of different similarity measures which can be rephrased as a variant of computing a dot product. Note that this technique preserves the ability to apply a combiner in each pass over the data and is therefore highly efficient.
Table 1 describes how to express several common simi-larity measures through these canonical functions, including cosine, Pearson correlation and a couple of others evaluated by Google for recommending communities in its social net-work Orkut [24].
 Example : The Jaccard coefficient between items i and j (two columns from the interaction matrix A ) is computed as the ratio of the number of users interacting with both items to the number of users interacting with at least one of those items. It can easily be expressed by our algorithmic framework, as this example shows: Pass 1 : We start by having preprocess binarize the vectors: The second function that is invoked for each of the vectors is norm . We let it return the L 1 norm, which gives us the number of non-zero components of each of the binary vec-tors:
Pass 2 : Finally the function similarity will be called given the dot product between the preprocessed vectors and their precomputed norms. We have to rearrange the formula of the Jaccard coefficient so that it can be computed from the numbers we have at hand: jaccard ( i,j ) =
Algorithm 3 : computing arbitrary item similarities function map( a u  X  ): function combine( i,d 1 ,...,d n ): function initialize_reducer(): function reduce( i,d 1 ,...,d n ):
In order to be able to handle cases with an enormous number of items, we add means to decrease the density of the similarity matrix S to our final implementation.
To get rid of pairs with near-zero similarity, a similarity threshold can be specified, for which we evaluate a size con-straint to prune lower scoring item pairs early in the process [4] and eventually remove all entries from S that are smaller than the threshold. Note however that this threshold is data dependent and must be determined experimentally to avoid negative effects on prediction quality.

Furthermore, it has been shown that the prediction qual-ity of the item-based approach is sufficient if only the top fraction of the similar items is used [23], therefore we add another MapReduce step that only retains these top similar items per item in a single pass over the data.
Although the similarity matrix is usually used to compute recommendations online, some use cases such as generating personalized newsletters require batch recommendation for
Table 1: expressing measures with the canonical functions all users. To achieve that, we need another function called recommend() that is invoked with the similarity matrix S and item interaction history a u  X  of an active user u and returns the top-N items to recommend to that user. A vari-ety of strategies can be applied in the estimation procedure, ranging from simple weighted sum estimation [23], improved by baseline estimates [18], to more advanced techniques that incorporate domain specific knowledge and aim to diversify recommendations [7].

If the similarity matrix fits into the memory of a single mapper instance, the most efficient way of embedding the recommendation computation is by executing a broadcast join [9] of the users X  item interaction histories and the spar-sified similarity matrix. As shown in Algorithm 4, the sim-ilarity matrix is broadcasted to all worker machines in the cluster via the distributed cache and the recommendations are computed in a map-only job over the users X  interaction histories. Such a job is highly efficient as no reducer is re-quired, which obviates the need for the shuffle phase and its associated sorting and network overhead.

In cases with an extreme number of items, the similarity matrix might not fit into the mappers X  memory any more. In such a case a less performant repartition join [13] has to be used where the items each user has interacted with and their corresponding rows from the similarity matrix are sent over the network to a reducer that joins them after receival.
Algorithm 4 : batch recommendation for all users function initialize_mapper(): function map( u,a u  X  ):
Recall that our goal is to develop an algorithmic frame-work that scales linearly with respect to a growing user base. As described in Section 2.2, the cost of the item-based ap-proach is dominated by the densest rows of A , which cor-respond to the users with the most interactions. This cost, which we express as the number of item cooccurrences to consider, is the sum of the squares of the number of interac-tions of each user.

The number of interactions per user usually follows a heavy tailed distribution as illustrated in Figure 3 which plots the ratio of users with more than n interactions to the number of interactions n on a logarithmic scale. There-fore, there exists a small number of  X  X ower users X  with an unproportionally high amount of interactions. These dras-tically increase the runtime, as the cost produced by them is quadratic with the number of their interactions.
If we only look at the fact whether a user interacted with an item or not, then we would intuitively not learn very much from a  X  X ower user X : each additional item he interacts with will cooccur with the vast amount of items he already preferred. We would expect to gain more information from users with less interactions but a highly differentiated taste. Furthermore, as the relations between items tend to stabilize quickly [22], we presume that a moderately sized number of observations per item is sufficient to find its most similar items.

Following this rationale, we decided to apply what we call an interaction-cut : we selectively down sample the interac-tion histories of the  X  X ower users X .

We apply this by randomly sampling p interactions from each such user X  X  history, thereby limiting the maximum num-ber of interactions per user in the dataset to p . Note that this sampling is only applied to the small group of  X  X ower users X , it does not affect the data contributed by the vast majority of non- X  X ower users X  in the long tail. Capping the effort per user in this way limits the overall cost of our approach to | U | p 2 . We will experimentally show that a moderately sized p is sufficient to achieve prediction quality close to that of unsampled data. An optimal value for p is data dependent and must be determined by hold-out tests.
In this section we present the results of a sensitivity analy-sis for the interaction-cut and conduct an experimental eval-uation of our parallel algorithm on a large dataset 1 .
We will show that the prediction quality achieved by using an interaction cut quickly converges to the prediction qual-ity achieved with unsampled data. Subsequently, we will analyze the relationship between the size of the interaction-cut, the achieved quality and the runtime for the similarity computation in our large dataset. After that we will study the effects on the runtime speedup if we add more machines to the Hadoop cluster as well as the scaling behavior with a growing user base.

Prediction was conducted with weighted sum estimation enhanced by baseline estimates [18]. In a preprocessing step that has negligible computation cost, we estimate global user and item biases b u and b i that describe the tendency to deviate from the average rating  X  . This gives us the simple code to repeat our experiments is available at http://github.com/dima-tuberlin/publications-ssnmm Figure 2: sensitivity of the probability of interaction with an item to an interaction-cut of size p in the Movielens dataset baseline prediction b ui =  X  + b u + b i for the rating of a user u to an item i . To finally predict the rating r use the normalized weighted sum over the user X  X  ratings to the k most similar items of i , incorporating the baseline predictions:
We conducted a sensitivity analysis of the effects of the interaction-cut. We measured the effect on the probability of interaction with an item and on prediction quality for varying p on the Movielens 2 dataset consisting of 1,000,209 ratings that 6,040 users gave to 3,706 movies.

For our first experiment, we ranked the items by their probability of interaction as shown in the plot in the top left corner in Figure 2. Next, we applied the interaction-cut by sampling down the ratings of users whose number of interactions in the training set exceeded p and repeated this for several values of p . In the remaining plots of Figure 2, we retained the order of the items found in the unsampled data and plotted their probabilities after applying the interaction-cut for a particular p . We see that for p  X  500 there is no observable distortion in the ranking of the items, which is a hint that this distribution is independent of the data omitted by sampling down the interactions of the  X  X ower users X .
In our second experiment, we computed the prediction quality (by mean average error) achieved by a particular p by randomly splitting the rating data into 80% training and 20% test set based on the number of users. For this experi-ment, we additionaly used the Flixster 3 dataset consisting of 8,196,077 ratings from 147,612 users to 48,794 movies. Again, we applied the interaction-cut by sampling down the ratings of users whose number of interactions in the training set exceeded p and used the 80 most similar items per item for rating prediction. For these small datasets, the tests were conducted on a single machine using a modified version of Mahout X  X  [3] GenericItemBasedRecommender .

Figure 4 shows the results of our experiments. Note that the right-most data points are equivalent to the prediction quality of the unsampled dataset. In the Movielens dataset we see that for p &gt; 400 the prediction quality converges to the prediction quality of the unsampled data, in the Flixster http://www.grouplens.org/node/73 http://www.cs.sfu.ca/  X  sja25/personal/datasets/ Figure 3: long tailed distribution of the number of interactions per user in vari-ous datasets dataset this happens at p &gt; 750. There is no significant de-crease in the error for incorporating more interactions from the  X  X ower users X  after that. This confirms our expectation that we can compute recommendations based on the user data from the long tail and only samples from the  X  X ower users X  without sacrificing prediction quality.
The following experiments were conducted on a Hadoop cluster with a MapReduce implementation of our approach. The cluster consisted of six machines running Apache Hadoop 0.20.203 [2] with each machine having two 8-core Opteron CPUs, 32 GB memory and four 1 TB disk drives. The ex-periments for showing the linear speedup with the number of machines were run on Amazon X  X  computing infrastructure, where we rented m1.xlarge instances, 64-bit machines with 15 GB memory and eight virtual cores each.
To test our approach in a demanding setting, we used a very large ratings dataset 1 which represents a snapshot of the Yahoo! Music community X  X  preferences for various songs that were collected between 2002 and 2006. The data consists of 717,872,016 ratings that 1,823,179 users gave to 136,736 songs.

We used the 699 million training ratings provided in the dataset to compute item similarities and measured the pre-diction quality for the remaining 18 million held out rat-ings. We computed the 50 most similar items per item with Pearson correlation as similarity measure with a threshold of 0.01. Figure 5 shows the results we got for differently sized interaction cuts. We see that the prediction quality converges for p &gt; 600, similar to what we have observed for the smaller datasets in Section 5.1. We additionally mea-sured the root mean squared error and observed the same behavior, the prediction quality converged to an error of 1 . 16 here. We see the expected quadratic increase in the runtime for a growing p , which is weakened by the fact there is a quickly shrinking number of users with more than p interac-tions (from Figure 3 we know for example that only approx-imately 9% of the users have more than 1000 interactions).
The computational overhead introduced by the  X  X ower users X  is best illustrated when we compare the numbers for p = 750 and p = 1000: we see a decrease of only 0 . 0007 in the mean average error, yet the higher value of p accounts R2 -Yahoo! Music User Ratings of Songs with Artist, Album, and Genre Meta Information, v. 1.0, http://webscope.sandbox.yahoo.com/ for a nearly doubled runtime. We conclude that we are able to achieve the convergence of the prediction quality in this large dataset with a p that is extremely low compared to the overall number of items and thereby results in a com-putation cost that is easily manageable even by our small Hadoop cluster.

In order to conduct a comparison to a single machine im-plementation, we tried the item-based kNN recommender of MyMediaLite [14], the GenericItemBasedRecommender provided by Apache Mahout [3] and the ItemRecommender from LensKit [11]. Unfortunately, not one of these was able to complete the computation due to problems with memory handling, although we ran them on a machine which had 48 GB of RAM available.

Based on our findings, we chose to set p to 600 for the following scalability experiments. Figure 6: speedup for a growing number of ma-chines in Amazon EC2
The major promise of parallel processing platforms is seam-less horizontal scale out by simply adding more machines to the cluster. This requires the computation speedup to be proportional to the number of machines. To experimen-tally evaluate this property of our algorithm, we made use of the ElasticMapReduce computing infrastructure provided by Amazon, which allows us to run our algorithm on a cus-tomly sized Hadoop cluster. We repeatedly ran the similar-ity computation with an increasing number of cluster ma-chines. Figure 6 shows the linear speedup as expected by us. With 15 machines we were able to reduce the runtime of the similarity computation to less than one hour.
Finally, we evaluate the scaling behaviour of our approach in the case of a rapid growth of the number of users. The Yahoo! Music dataset is already partitioned into several files, with each file containing approximately 77 million rat-ings given by 200,000 unique users. In order to simulate the growth of the user base, we used an increasing number of these files as input to our parallel algorithm and mea-sured the duration of the similarity computation. Figure 7 shows the algorithm X  X  runtime when scaling from 200,000 to 1,8 million users. We see a perfectly linear increase in the runtime which confirms the applicability of our approach in scenarios with enormously growing user bases.

As the speedup with the number of machines as well as the runtime for a growing number of users scale linearly, we can counter such growth by simply adding more machines to the cluster to keep the computation time constant.
We showed how to build a scalable, neighboorhood-based recommender system based on the MapReduce paradigm. We rephrased the underlying pairwise comparison to run on a parallel processing platform with partitioned data and described how a wide variety of measures for comparing item interactions easily integrate into our method. We introduced a down sampling technique called interaction-cut to handle the computational overhead introduced by  X  X ower users X .
For a variety of datasets, we experimentally showed that the prediction quality quickly converges to that achieved with unsampled data for moderately sized interaction-cuts. We demonstrated a computation speedup that is linear in the number of machines on a huge dataset of 700 million interactions and showed the linear scale of the runtime with a growing number of users on that data.

In future work we intend to explore how our method could be used to scale out recommendation approaches that incor-porate similarity computations on large networks [16, 25]. A special thanks goes to Ted Dunning, Zeno Gantner and Moritz Kaufmann. The research leading to these results has received funding from the European Union (EU) in the course of the project  X  X OBUST X  (EU grant no. 257859) and used data provided by  X  X ahoo! Academic Relations X . [1] K. Ali and W. van Stam. Tivo: Making show [2] Apache Hadoop, http://hadoop.apache.org. [3] Apache Mahout, http://mahout.apache.org. [4] R. J. Bayardo, Y. Ma, and R. Srikant. Scaling up all [5] R. M. Bell and Y. Koren. Lessons from the netflix [6] A. S. Das, M. Datar, A. Garg, and S. Rajaram. [7] J. Davidson, B. Liebald, J. Liu, P. Nandy, [8] J. Dean and S. Ghemawat. Mapreduce: simplified [9] D. DeWitt, R. Gerber, G. Graefe, M. Heytens, [10] T. Dunning. Accurate methods for the statistics of [11] M. D. Ekstrand, M. Ludwig, J. A. Konstan, and J. T. [12] S. Ewen, K. Tzoumas, M. Kaufmann, and V. Markl. [13] S. Fushimi, M. Kitsuregawa, and H. Tanaka. An [14] Z. Gantner, S. Rendle, C. Freudenthaler, and [15] R. Gemulla, E. Nijkamp, P. Haas, and Y. Sismannis. [16] M. Jamali and M. Ester. Trustwalker: a random walk [17] J. Jiang, J. Lu, G. Zhang, and G. Long. Scaling-up [18] Y. Koren. Factor in the neighbors: Scalable and [19] G. Linden, B. Smith, and J. York. Amazon.com [20] Y. Low and J. Gonzalez and A. Kyrola and [21] P. Resnick, N. Iacovou, M. Suchak, P. Bergstrom, and [22] F. Ricci, L. Rokach, B. Shapira, and P. B. Kantor. [23] B. Sarwar, G. Karypis, J. Konstan, and J. Riedl. [24] E. Spertus, M. Sahami, and O. Buyukkokten.
 [25] P. Symeonidis, E. Tiakas, and Y. Manolopoulos. [26] Y. Zhou, D. Wilkinson, R. Schreiber, and R. Pan.
