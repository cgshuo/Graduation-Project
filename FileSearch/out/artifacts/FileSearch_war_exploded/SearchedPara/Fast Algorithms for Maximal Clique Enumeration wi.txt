 Maximal clique enumeration ( MCE ) is a long-standing problem in graph theory and has numerous important applications. Though extensively studied, most existing algorithms become impractical when the input graph is too large and is disk-resident. We first pro-pose an efficient partition-based algorithm for MCE that addresses the problem of processing large graphs with limited memory. We then further reduce the high cost of CPU computation of MCE by a careful nested partition based on a cost model. Finally, we paral-lelize our algorithm to further reduce the overall running time. We verified the efficiency of our algorithms by experiments in large real-world graphs.
 H.2.8 [ DATABASE MANAGEMENT ]: Database Applications X  Data mining ; G.2.2 [ DISCRETE MATHEMATICS ]: Graph The-ory X  Graph algorithms Algorithms, Experimentation, Performance Maximal clique enumeration, I/O efficient, parallel algorithm, mas-sive networks, sparse graphs
Maximal clique enumeration ( MCE ) [3, 9] is one of the fun-damental problems in graph theory. It is closely related to many other important graph problems, such as maximal independent sets (or minimal vertex covers), graph coloring, maximal common in-duced subgraphs, maximal common edge subgraphs, etc. Besides graph theory, MCE has numerous other applications, such as social network analysis [20], hierarchy detection through email networks [16], study of structures in behavioral and cognitive networks [5], statistical analysis of financial networks [7], clustering in dynamic networks [28], the detection of emergent patterns in terrorist net-works [6], etc. Moreover, MCE is also closely tied to various applications in computational biology [1], including the detection of protein-protein interaction complex and clustering protein se-quences.

The problem of MCE is NP-hard theoretically but because of its significance in real applications, many practical algorithms [1, 3, 9, 10, 18, 19, 21, 23, 24, 25, 26, 28, 29, 30, 31] have been proposed. However, many of these algorithms have become impractical to-day due to the fast growing size of graphs in the real world. For example, the Web graph has ove r 1 trillion webpa ges (by Google in 2008); most social networks (e. g., Facebook, Link edIn, Twitter, Google+) and communication networks (e.g., MSN, phone, SMS, and email networks) have up to billions of users; other networks such as transportation networks, citation networks, stock-market networks, etc., are also massively large.

The existing algorithms are in-memory algorithms, which re-quire space that is asymptotically linear in the size of the input graph. However, the massive size of many graphs has outpaced the advance in the memory available on commodity hardware. MCE computation accesses vertices in a rather arbitrary manner and this results in random access. When memory is insufficient and the graph has to be resident on disk, random disk access incurs high I/O cost.

To process such large graphs, Cheng et al. proposed efficient algorithms [12, 13] which enumerate maximal cliques in local sub-graphs that fit in main memory as to eliminate the high I/O cost due to random access. However, MCE intrinsically has a high CPU time complexity and thus reducing the I/O cost alone does not fully address the problem.

To address the intensity of MCE computation, several parallel algorithms [17, 27] have been propos ed. However, they still require a copy of the entire input graph to be resident in memory at each computing node, which ends at the same predicament faced by the existing in-memory sequential MCE algorithms.
 MapReduce has been popularly used to handle massive data. A MapReduce algorithm [33] was proposed recently for MCE. The MapReduce model, however, may not be efficient for enumeration tasks such as MCE and triangle listing [14, 15] in large graphs due to the huge amount of intermediate data produced in the shuffling phase between Mappers and Reducers.
In this paper, we propose new algorithms for MCE that achieve the following three objectives. 1. We reduce the I/O cost by a partition-based strategy, which 2. We reduce the cost of CPU computation by investigating a 3. We further reduce the overall running time by parallelizing
Extensive experiments on large real-world datasets verify the ef-fectiveness of our algorithms in reducing both the I/O cost and CPU cost. The results show that our algorithms significantly outperform the state-of-the-art MCE algorithms [12, 13, 18, 19, 29], especially in the case when the input graph is too large to fit in main memory. Paper organization. The remaining of the paper is organized as follows. Section 2 defines the problem and basic notations. Sec-tion 3 describes a conventional algorithm for MCE and points out its weaknesses. Section 4 discusses the details of our algorithms. Section 5 reports the experimental results. Section 6 discusses the related work and Section 7 concludes the paper.
Let G =( V,E ) be a simple undirected graph. We define the size of G , denoted by | G | ,as | G | =( | V | + | E | ) of vertices S  X  V ,wedefinethe induced subgraph of G by S as G S =( V S ,E S ) ,where V S = S and E S = { ( u, v ): S, ( u, v )  X  E } . We define the set of adjacent vertices of a vertex v in G as adj ( v )= { u :( u, v )  X  E } ,andthe degree of v in G as deg ( v )= | adj ( v ) | . Similarly, we define adj ( v,G ( u, v )  X  E S } and deg ( v,G S )= | adj ( v,G S ) | .

We assume that all graphs are stored (whether in memory or on disk) in adjacency list representation, where vertices are assigned unique IDs and ordered according to their IDs.
 A clique C in a graph G is a subset of vertices, where C  X  such that G C is a complete subgraph of G . C is called a maximal clique in G if there exists no clique C in G such that C  X  Problem definition. The problem of MCE is: given a graph G , find the set of all maximal cliques in G .

We focus on sparse graphs ,forwhich | E | B  X | V | ,and B is the disk block size. Most large and many fast growing real-world networks are sparse. For processing these large sparse graphs, ran-dom disk access in the process of MCE is prohibitively expensive since data transferred from/to disk is in blocks, while the exact amount of data used in each step of MCE is much less than the amount of data transferred for each random access, since deg B for most v  X  V .
 Algorithm 1 IM-MCE Input :agraph G =( V,E ) Output : all maximal cliques in G Procedure 2 IM-MCE-Step ( C, todo , done )
We first discuss the conventional algorithms for MCE and their shortcomings. Algorithm 1 and Procedure 2 sketch a general algo-rithm framework for MCE, which is adopted by most existing MCE algorithms [9, 23, 29]. The state-of-the-art parallel MCE algorithm [27] is also developed based on this framework. This framework is also used in later sections to explain our algorithms.

Algorithm 1 calls Procedure 2. In Procedure 2, we use the no-tations todo , doing ,and done to represent the set of vertices to be processed , being processed ,and already processed , respectively.
The algorithm starts from the set of all vertices in G , i.e., todo V ,and done =  X  , by calling Procedure 2. Procedure 2 recursively calls itself to grow a clique C in a depth-first manner until C be-comes maximal; that is, when todo =  X  , i.e., there is no more vertex for C to grow with, and done =  X  , i.e., C has not been enu-merated before (if done =  X  ,then  X  C  X  C such that C has been found as a maximal clique).

The depth-first MCE process essentially constructs a search tree, or more precisely a search forest joined by a (virtual) root. At each call of Procedure 2, we first pick a pivot vertex v p , which is used to prune all the neighbors of v p , because any maximal clique con-taining a vertex u  X  adj ( v p ) can be enumerated from either v another vertex v  X  adj ( u ) ,where v/  X  adj ( v p ) . Therefore, all the search subtrees rooted at each u  X  adj ( v p ) can be pruned.
We then construct doing , which is the set of vertices being pro-cessed in the current call of Procedure 2. For each vertex v doing , we grow the current clique C by adding v to C .Tofurther grow ( C  X  X  v } ) to a bigger clique in the subsequent recursive call, we need to make sure that every vertex in todo must be a neighbor of v . After the recursive call returns, we add v to done to indicate that v has been processed in the current search subtree. Shortcomings. Algorithm 1 requires O (3 | V | / 3 ) time in the worst case, but has shown to be the optimal worst-case complexity [29]. In practice, the existing algorithms that adopt the framework of Al-gorithm 1 are reasonably fast for processing relatively small real-world graphs. However, these algorithms are in-memory algorithms and require  X ( | V | + | E | ) memory space. If memory is not suffi-cient or the input graph is disk-resident, the process immediately becomes impractical. As we observe in Line 8 of Procedure 2, the MCE process requires accesses to the neighbor set of each ver-tex v  X  doing , i.e., adj ( v ) , which may scatter in different lo-cations in the graph resident on disk. Thus, random disk access leads to huge I/O cost and severely degrades the performance of the in-memory algorithm. In addition, some branches of the search tree constructed by Algorithm 1 can be potentially parallelized. In-tuitively, the search subtrees rooted at each of the siblings of the search tree can be constructed i n parallel with careful design. We address these issues in Section 4.
In this section, we address the shortcomings of the conventional algorithm for MCE. In particular, we have the following three main objectives: (1) reducing the I/O cost, (2) reducing the CPU cost, and (3) further reducing the overall running time by parallelization. When the input graph cannot fit in main memory, reducing the I/O cost by avoiding random disk accesses becomes the key to the efficiency of MCE. The main idea of our algorithm is to repeatedly extract a subgraph that fits in memory and compute the maximal cliques locally from the subgraph. However, we should also pre-serve both the global correctness and completeness of the results computed from the local subgraphs.
 We first define the subgraph to be extracted for MCE as follows.
Definition 1(S EED V ERTICES /S UBGRAPH ). Given a graph G =( V,E ) , denote S to be a set of seed vertices selected from V ,where S  X  V .The seed subgraph of G , denoted by G ( V S ,E S ) , is defined as the induced subgraph of G by S .
Using the seed subgraph G S alone is not sufficient to ensure the completeness of MCE since some seed vertices may form a clique with vertices not in G S . To address this, we extend G S
Definition 2(E XTENDED V ERTICES /S UBGRAPH ). Given a set of seed vertices S of a graph G =( V,E ) ,thesetof extended seed vertices ,orsimply extended vertices , denoted by S + , is defined as S + = S  X  X  v : v  X  adj ( u ) ,u  X  S } .The extended seed sub-graph ,orsimply extended subgraph , denoted by G S + , is defined as the induced subgraph of G by S + .

Note that the above definition of subgraph is different from the one used in [12, 13], where they do not include the edges among the vertices in ( S + \ S ) . This difference renders the design of the algo-rithm totally different, since t he subgraphs in [12, 13] still need to access the input graph for MCE while ours do not. Compared with the algorithms in [12, 13], our algorithm is simpler, more efficient, and natural to be parallelized.
 The following example illustrates the concepts.

Example 1. Consider the graph G shown in Figure 1. If we choose S = { 0 , 1 , 2 } as the set of seed vertices, we obtain the seed subgraph G S as shown in Figure 2 (left). It is easy to see that the seed subgraph alone cannot produce any maximal clique of G .So we extend the seed vertices to be S + = { 0 , 1 , 2 , 3 , tain the extended subgraph G S + , as given in Figure 2 (right). The extended subgraph makes possible the local enumeration of global maximal cliques, i.e., { 0 , 1 , 2 , 3 } , { 1 , 4 , 5 } , tures fully the neighborhood information of seed vertices.
With the above formulation of extended subgraph, we first prove that the maximal cliques computed locally in each extended sub-graph are also globally maximal.
 Let M ( G ) and M ( G S + ) be the set of maximal cliques in G and G S + , respectively. Let and M S ( G S + )= { C : C  X  X  ( G S + ) ,C  X  S =  X  X  be the set of maximal cliques, respectively in G and G S + , that contain at least one vertex in S .

P ROOF . We first prove M S ( G S + )  X  X  S ( G ) .Webeginby proving that  X  C  X  X  S ( G S + ) , C  X  X  ( G ) . Suppose C/ M ( G ) ,then  X  C  X  X  ( G ) such that C  X  C .Then  X  v  X  C and v/  X  S + (otherwise C  X  X  S ( G However, v/  X  S + implies that ( v,u ) /  X  E for some vertex u ( C  X  S ) , which contradicts that C is a clique. Thus, C  X  X  ( and it follows that C  X  X  S ( G ) since C  X  S =  X  .
 We then prove M S ( G )  X  X  S ( G S + ) .  X  C  X  X  S ( G ) ,wehave C  X  X  ( G ) and C  X  S =  X  .Let u  X  ( C  X  S ) .Then  X  v  X  C \{ we have ( u, v )  X  E and thus v  X  S + . Therefore, C  X  X  ( and it follows that C  X  X  S ( G S + ) .

Lemma 1 implies that all maximal cliques in G that contain at least one seed vertex in S can be computed from G S + alone. This leads to the design of an algorithm that repeatedly extracts a sub-graph G S + that can fit in main memory, computes M S ( G G
S + in memory, and then continues to select another set of seed vertices from ( V \ S ) , until all vertices in V are processed. The fol-lowing theorem guarantees the correctness and completeness of the set of maximal cliques so computed.

T HEOREM 1. Let S = { S 1 ,...,S k } ,where 1  X  i  X  k S i and S i  X  S j =  X  for i = j .Let M S = 1  X  i  X  k M S i ( G Then, M S = M ( G ) .
 1. Thus, M S  X  X  ( G ) .Next,  X  C  X  X  ( G ) ,  X  S i such that C is because 1  X  i  X  k S i = V ). Thus, M ( G )  X  X  S .

Theorem 1 immediately leads to the design of an efficient partition-based algorithm, as shown in Algorithm 3.

The algorithm sequentially scans the input graph G to select a set of seed vertices S , extracts G S + by another scan of G , computes M
S ( G S + ) from G S + , and then continues to select another set of seed vertices until all vertices in V are processed as seed vertices.
The algorithm determines the size of G S + in Line 4, where a parameter  X  deg is used to estimate the size of G S + ,and Algorithm 3 Partition-Based MCE Input :agraph G =( V,E ) Output : all maximal cliques in G is to ensure the estimated size is less than M 1 ,where M is the available memory size. Here,  X  deg can be the average or maximum vertex degree in G .If  X  deg is the average vertex degree, it is pos-sible that | G S + | &gt;M ; however, this can be easily fixed by further splitting G S + into smaller extended subgraphs with smaller seed vertex sets. Since only some selection of S can lead to the case  X  |
G S + | &gt;M  X , in practice the total number of extended subgraphs to be extracted remains relatively stable. If  X  deg is the maximum vertex degree, we can guarantee that | G S + | X  M .However,this may lead to under-utilization of the available memory. This is also true for splitting a large G S + into smaller extended subgraphs in the case of setting  X  deg as the average vertex degree. However, we shall show in Section 4.2 that fully utilizing the available memory may actually lead to an even higher cost in the in-memory compu-tation of MCE.

After computing the set of local maximal cliques in G S + M ( G least one seed vertex to ensure the correctness according to Lemma 1. However, it is possible that a maximal clique C is enumerated in more than one G S + , if the vertices in C are in different sets of seed vertices, i.e.,  X  u, v  X  C such that u  X  S i and v  X  S j ,where u and i = j . To avoid duplicate output of a maximal clique, Line 8 sets another condition, ( u  X  C s.t. u  X  S ) . Here, u  X  S means that u is selected as a seed vertex (in an earlier iteration of Lines 5-10), before any vertex in S is selected (in the current iteration). Thus, Lines 7-9 do not output a maximal clique C if C contains a vertex that has already been selected as a seed vertex before S is selected as the seed vertex set.
 Greater Pruning. The condition  X  ( u  X  C s.t. u  X  S )  X  X lso implies that we do not need to include into G S + any edge  X  u, w  X  S + and u, w  X  S . Furthermore, we can push the condi-tion into the in-memory MCE process to achieve greater pruning as follows. Recall that the search tree (implicitly) constructed by Al-gorithm 1 is a prefix tree. During the MCE process, we can process the seed vertices in S before the vertices in S + \ S . Then we do not grow the current clique C by any vertex v  X  S in Line 6 of Pro-cedure 2. In this way, we avoid duplicate enumeration of maximal cliques as well as achieve greater pruning effect for MCE. Complexity Analysis. The following theorem gives the complex-ity of Algorithm 3. We use the following standard I/O complexity
We assume that if | S | =1 ,then | G S + | X  M . In the case of a high-degree vertex v , | G S + | with S = { v } can be large. For sparse real-world graphs, we may effectively reduce | G S + | by sorting the vertex set V in ascending order of the vertex degree, since we ac-tually do not need to include into G S + any edge ( u, w ) are ordered before v ,forall u, w  X  S + (as implied by Line 8 of Algorithm 3). notations [2] in the complexity analysis: M is the main memory size, B is the disk block size, scan ( N )= X ( N/B ) I/Os, where 1 B  X  M/ 2 and N is the amount of data being read/written from/to disk.

T HEOREM 2. Algorithm 3 requires O ( k  X  scan ( | G | )) I/Os, O CPU time, and O ( M ) memory space, where k = min { | V | ( |
V |} , and T is the CPU time complexity of the in-memory algo-rithm for computing MCE in an extended subgraph G S + with  X  M .

P ROOF . Algorithm 3 makes ( k +1) scans of G , once in Line 1and k times in Line 5, where k is the total number of extended subgraphs G S + extracted. From Line 5 we have (  X  deg  X | cM , but we can easily move the last vertex in S to the next round of seed vertex selection to obtain | G S + | X  (  X  deg  X | S Thus, we have ((  X  deg ) 2  X | S | )= O ( M ) since | S + | ( |
S | ) .Since V is divided into k sets of seed vertices, we have k O one vertex and thus k  X | V | .

The selection of seed vertices and extraction of extended sub-graphs require only linear time; thus, the CPU time is bounded by the number of times the in-memory algorithm is invoked to com-pute MCE on an extended subgraph G S + , where the maximum size of any G S + is M (we refer the readers to [29] for the details on the CPU time complexity analysis on the in-memory algorithm). The memory space requirement is | G S + | = O ( M ) .
In processing large graphs that cannot fit in main memory, the bottleneck of the in-memory algorithms is the huge I/O cost due to random disk access. However, by processing these large graphs using Algorithm 3, the bottleneck may no longer be the I/O cost, but rather the cost of the in-memory computation of MCE in each extracted extended subgraph.
The cost of in-memory MCE computation can be broken down into two main types: the cost of constructing the search tree and the cost of set intersection .

Conventional algorithms focus on reducing the cost of construct-ing the search tree by employing various pruning techniques to re-duce the size of search tree. Although these pruning techniques are vital for efficient MCE, we have observed that we can further reduce the cost of CPU computation as follows.

Among all individual operations in the process of MCE, the most expensive one is the set intersection: two in Line 8 of Procedure 2 (and the set subtraction in Line 5 may also be considered as one). These set intersections can be significantly more costly to process than the other operations that require only constant time. There are approximation algorithms for set intersection, but for MCE we require an exact answer. To compute X  X  Y = Z ,if exact set intersection is required, then the cost of the intersection is at least ( | X | + | Y | ) .However,if | Z | is significantly smaller than |
X | and | Y | , then the majority of the processing is  X  X asted X  on processing non-contributing elements in X and Y , i.e., elements in X \ Z and Y \ Z . Thus, the objective here is to reduce the number of non-contributing elements in X and Y .

Reducing the number of non-contributing elements in X and Y in Procedure 2 means removing non-contributing elements in todo , done , and in particular adj ( v ) since adj ( v ) is often significantly larger than todo and done (except in the first call of Procedure 2).
To reduce the portion of the non-contributing elements in todo , done ,and adj ( v ) , we transform the search space from G to G In this way, the search space is immediately reduced from V to S for todo and done , and from adj ( v,G ) to adj ( v,G S + which S + and hence G S + should we select in order to minimize the cost?
To avoid expensive random disk access, G S + should have size smaller than M . For this purpose, Algorithm 3 naturally fulfills the requirement. However, Algorithm 3 attempts to fully utilize the available memory, i.e., | G S + | is close to M . But the analysis in Section 4.2.1 suggests that a smaller G S + gives a smaller cost of set intersection.

Extracting a smaller G S + in Algorithm 3 can be easily done by setting a smaller c in Line 4 of Algorithm 3. However, doing so proportionally increases the number of scans of G in Line 5 and hence the overall I/O cost. The following lemma enables us to totally avoid any extra I/O in extracting smaller G S + .
L EMMA 2. Given two sets of seed vertices, S big and S small S Then we also have E S + E } X  E is a subgraph of G S +
Lemma 2 shows that it is possible to extract a smaller extended subgraph G S + (which is resident in memory) instead of from the original input graph G (which is resident on disk). However, we still need to determine how small this G S +
We propose a cost model for choosing the right size for the ex-tended subgraph. Suppose that we now have a set of extended sub-graphs G  X  resident in main memory for MCE computation. Each subgraph G S + contains only one extended subgraph G S + extracted in Lines 4-5 of Algorithm 3. We determine (iteratively) whether extracting smaller extended subgraphs from G  X  can achieve better efficiency for in-memory MCE computation.

For each G S + smaller extended subgraphs { G S +  X  } , each of them is roughly of the same size. Let G  X  be the set of all such smaller extended subgraphs extracted from G  X  .

First, having a smaller adj ( v,G S +  X  ) , instead of adj the process of MCE leads to a lower cost of set intersection. The gain obtained by preferring G  X  over G  X  can be estimated as follows. where | T S +  X  | is the number of nodes in the search tree T structed by Algorithm 1 with G S +  X  as the input, and deg is the average degree of a vertex v in G S +  X  ; and similarly for and deg ( v,G S +
In Equation (1), the gain obtained from the reduction in the cost of set intersection with a smaller adj ( v,G S +  X  ) instead of adj is reflected by the average degree deg ( v,G S +  X  ) and deg And we multiply the average de gree by the number of nodes in the search tree because at each node in the search tree, we need to perform the set intersection as shown in Line 8 of Procedure 2.
If choosing G  X  instead of G  X  only results in a gain for MCE, then the optimal choice is by setting S  X  = { v } for each single vertex v  X  V . However, preferring G  X  over G  X  mayalsoresult in some loss due to the extraction of a large number of smaller ex-tended subgraphs and the construction of their search trees during the process of MCE.

First, if we further split G S + have extra extraction costs. Second, the size of the search tree con-structed from G S + constructed from the set { G S +  X  } extracted from G S + preferring G  X  over G  X  is given as follows.
 In Equation (2), the first half gives the cost of extracting all G from each G S + to extract each G S +  X  ,weadd | G S + half is the difference in the total size of the search trees constructed from all G S +  X  and from all G S +
Equation (1) and Equation (2) represent a tradeoff: we trade off (1) the cost of set intersection with (2) the cost of subgraph extrac-tion and that of search tree construction ; that is, using smaller ex-tended subgraphs reduces (1) but increases (2), while using larger extended subgraphs results in the opposite. We quantify this trade-off by the following cost model.

Finally, in Equation (1) and Equation (2), | G S +  X  | and known after we extract the subgraphs, while | T S +  X  | and be estimated by a variation of Knuth X  X  method [22] for estimating the size of a backtracking tree of MCE proposed in [12].
With the results of the previous subsections, we propose an im-proved partition-based algorithm for MCE, as shown in Algorithm 4. We name this algorithm as SeqMCE (for Seq uential MCE ), to distinguish from the parallel algorithm proposed in Section 4.3.
As shown in Algorithm 4 and Procedure 5, the extraction of ex-Algorithm 4 SeqMCE Input :agraph G =( V,E ) Output : all maximal cliques in G Procedure 5 MCE-Step ( H =( V H ,E H )) tended subgraphs is processed at two levels, one aiming to mini-mize the I/O cost while the other aiming to reduce the CPU cost.
The first level is to extract bigger extended subgraphs, which is done in a similar way as in Algorithm 3 but by replacing Lines 6-9 with a call to Procedure 5. At this level, the size of each G should be close to M to minimize the number of scans of G and hence the I/O cost.

Then, at the second level as shown in Procedure 5, we extract smaller extended subgraphs from each bigger extended subgraph H obtained at the first level. Whether or not to further extract smaller extended subgraphs is determined by the cost model given by Equation (3).

Computing the optimal G  X  to maximize TotalGain is similar to finding the optimal graph partition from H , which is APX-hard [4]. Thus, this process alone would dominate the overall cost of MCE. However, if we do not select the seed vertices from H randomly but rather select them sequentially, the selection process can be made much more efficient.

Another challenge of applying Equation (3) is that Equation (3) consists of Equations (1) and (2), both of which require the entire sets of extended subgraphs, G  X  and G  X  ,tobeknown. Evenifwe select the seed vertices sequen tially, there are s till expone ntially many different permutations of G  X  and G  X  .

To avoid the costly intermediate process of subgraph extraction, we consider the search space as a binary tree, where the set of nodes at each level of the tree is a set of extended subgraphs. We construct the binary tree level-wise (Lines 1-10). The first level consists of only one extended subgraph, i.e., H itself. The two children of any internal node in the tree are constructed by dividing the set of seed vertices of the parent sequentially into two sets of seed vertices with equal size, and then extract the corresponding extended subgraphs from the parent (the correctness of the extraction is guaranteed by Lemma 2). Then at each level, we compute TotalGain .Westop constructing a new level of the binary tree when there is no longer a gain. Then (in Lines 10-12), we take the corresponding G set of extended subgraphs, and process MCE in each G S +  X  is done before in Algorithm 3.

The above scheme for subgraph extraction, though heuristic, is effective and efficient for the following important reasons: first, it fits seamlessly into the design of the partition-based algorithm; second, it supports effective parallelization; lastly, it avoids extra overhead in re-assigning the vertex order or relabeling the vertices, Algorithm 6 Par MC E Input :agraph G =( V,E ) Output : all maximal cliques in G Procedure 7 MCE-Job ( H =( V H ,E H )) which is necessary for the task of MCE when vertices are selected out of order into G S +  X  .
Existing parallel algorithms for MCE [17, 27] are in-memory al-gorithms that require the entire graph to be resident in main mem-ory of each computing node, and thus are not scalable as the graph size increases and main memory is not sufficient to hold the graph.
We adopt our partition-based MCE framework and propose a parallel algorithm for MCE that requires only limited memory at each computing node. We describe the algorithm, named as ParMCE (for Par allel MCE ), in Algorithm 6 and Procedure 7.

Algorithm 6 uses one computing node as the master node .The master node reads the input graph G from disk, extracts extended subgraphs from G , and distributes them to the idle nodes, if any. To utilize parallelism (e.g., multi-co res, hyper threading, etc.) within the master node, we create two threads in the master node. One thread extracts extended subgraphs and pushes it into the pool of un-computed subgraphs, and the other one takes the subgraphs from the pool and sends them to any available idle nodes. In case that the total number of available computing nodes becomes too large so that the computation at the mast er node might become a bottleneck, our algorithm easily allows us to split the input graph and create multiple master nodes to serve the increasing number of comput-ing nodes.

Procedure 7 then describes that when a computing node receives an extended subgraph H (distributed by Algorithm 6), it further extracts smaller extended subgraphs to improve the efficiency of MCE, as done in Procedure 5. If we have extra idle computing nodes, we may further divide extended subgraphs into smaller ones to maximize parallelization (note that the bottleneck is at the MCE computation rather than at the subgraph extraction, both of which are now performed in-memory). This task itself can also be easily parallelized, by simply distributing the extended subgraphs in new computing nodes and call Procedure 7. But since this process is not the bottleneck, it should be p arallelized only when computing nodes are excessive.

After we obtain the set of extended subgraphs G  X  , we distribute each small extended subgraph G S +  X   X  X   X  to an idle computing node, X .Thenat X , we apply the in-memory algorithm to process MCE in G S +  X  . The fine-grained parallelization of MCE proposed in [27] can be further applied if X has multiple cores or threads, or if there are more idle computing nodes available. In this section, we evaluate the performance of our algorithms. We compared with the algorithm that has the optimal time com-plexity for MCE in memory proposed by Tomita et al. [29] (de-noted by TomitaTT ), a recent algorithm for MCE in d -degenerate graphs proposed by Eppstein et al. [18, 19] (denoted by Epp-steinLS ), and the I/O-efficient MCE algorithm by Cheng et al. [13] (denoted by ChengKFYZ ).

For all the sequential programs, we ran the experiments on a machine with an Intel Xeon 2.67GHz CPU and 4GB RAM, running CentOS 5.4 (Linux). For the parallel program (using the MPI), we ran the experiments on a cluster, with each computing nodes having 2.93GHz CPU and 4GB RAM.
 Datasets. We use the following four datasets: blog , LJ , Web , and BTC .The blog network is collected from the top-15 popular queries published by Technorati (technorati.com) every three hours from Nov 2006 to Mar 2008. For each query, the top-50 results are retrieved. In the blog network, vertices are blogs and edges indi-cate that two blogs appear in the search result of the same query. LJ is the free online community called LiveJournal , where vertices are members and edges represent friendship between members. The LJ dataset is available from snap.stanford.edu. The Web graph is obtained from the YAHOO webspam dataset (barcelona.research. yahoo.net/webspam), where vertices are webpages and edges are hyperlinks. The BTC dataset is a semantic graph converted from the Billion Triple Cha llenge 2009 RDF dataset (http://vmlion25.deri.ie), where each vertex represents an object such as a person, a docu-ment, and an event, and each edge represents the relationship be-tween two vertices such as  X  X s-author X ,  X  X inks-to X , and  X  X as-title X . We list some details of the datasets (number of vertices and edges, physical storage size) in Table 1.

Among the four datasets, blog and LJ are two relatively smaller graphs, while Web and BTC are two larger graphs. We use the smaller graphs to compare the performance of our algorithm with the in-memory algorithms, while we use the larger graphs to eval-uate the performance of our algorithms when memory of a single machine is insufficient to hold the input graph.
We first examine whether the reduction of set intersection cost by extracting smaller extended subgraphs, rather than extracting extended subgraphs that fill the available memory, can indeed im-prove the efficiency of MCE computation. We also assess the ef-fectiveness of our cost model, given in Section 4.2.3, for choosing the right size for the extended subgraphs to be extracted. We test the settings on SeqMCE , i.e., Algorithm 4.
 Figure 3 reports the running time of SeqMCE for blog and Web . We extract extended subgraphs of size from 0.001% to 10% of the available memory size at the second level of Algorithm 4, i.e., each G { 0 . 001% , 0 . 01% , 0 . 1% , 1% , 10% } and M is the available memory size, which is set to 1 GB in this experiment. We also report the running time of SeqMCE that extracts extended subgraphs by our cost model, represented by  X  CM  X  on the x-axis. From Figure 3, the sizes of the extended subgraphs extracted by  X  CM  X  are around 2% for blog and 0.05% for Web . Figure 3: Running time (wall-clock time in seconds) of SeqMCE for MCE with varying sizes of extended subgraphs
The results show that for both graphs, the efficiency of MCE is first improved significantly when smaller extended subgraphs are used at the second level of the algorithm, but there is an optimal point after which further decreasing the size of the extended sub-graphs reports a significantly deteriorated performance. The trend of the running time for different sizes of extended subgraphs can be explained by our analysis of the gain and loss, i.e., Equation (1) and Equation (2), in Section 4.2.3, which represent a tradeoff between (1) the cost of set intersection and (2) the cost of subgraph extrac-tion and search tree construction . The same tradeoff can be also be clearly observed from the LJ and BTC graphs (details omitted due to lack of space).

Having shown the tradeoff, we show that the results obtained by the cost model is near the optimal point. Relatively speaking, the running time reported for the cost model for blog is further away from the optimal point than that for Web . This is because the blog graph is small and hence determining the right subgraph size by the cost model takes a considerable portion of the total time. When the input graph is large, the cost of computation involving the cost model becomes negligible compared to the cost of MCE.

In the subsequent experiments, the algorithm SeqMCE always extracts extended subgraphs by the cost model.
We now compare our algorithm SeqMCE with other state-of-the-art sequential algorithms for MCE.

Table 2 reports the running time of the four algorithms. For the smaller datasets, blog and LJ , EppsteinLS is the fastest. The Epp-steinLS algorithm uses a d -degeneracy ordering 2 to limit the depth
Agraphis d -degenerate if it has a maximum k -core number of d [18]. of recursive calls in the TomitaTT algorithm to d ,where d is not large for many real-world sparse graphs. Our algorithm SeqMCE can effectively reduce the cost of set intersection as well as limit the depth of recursive calls; however, it is a semi-streaming algorithm designed for processing large datasets, while EppsteinLS is an in-memory algorithm. Thus, SeqMCE needs to scan the input graph many times, which explains why it is slower than EppsteinLS .How-ever, EppsteinLS uses significantly more memory than SeqMCE and ran out of memory for the larger graphs, Web and BTC .On the contrary, with the available memory set at only 1 GB, SeqMCE still processes MCE efficiently on both large graphs, and is up to an order of magnitude faster than the state-of-the-art I/O-efficient MCE algorithm, ChengKFYZ .

Compared with the in-memory algorithm TomitaTT , which also ran out of memory for Web and BTC , SeqMCE is up to two or-ders of magnitude faster for processing blog and LJ . Since the smaller datasets can be processed in memory, the main difference between SeqMCE and TomitaTT for in-memory processing is the new adoption of cost reduction on set intersection, we conclude that the performance improvement of SeqMCE over TomitaTT is mainly due to our proposal of cost reduction on set intersection. This result thus further demonstrates the effectiveness of the CPU cost reduction method proposed in Section 4.2. We now evaluate the performance of the parallel algorithm (i.e., Algorithm 6), denoted by Par MC E . There are some recent algo-rithms for parallel MCE [17, 27, 33], but we were not able to obtain their code for comparison.

We report the running time of Par MC E in Figure 4. To show that our method is truly effective in handling graphs that cannot fit in memory, we set the available memory to only 64 MB for blog and LJ , and 256 MB for the two large graphs Web and BTC .
The figures show that when two machines are used, the running time is nearly halved for all the datasets. The efficiency continues to improve when more machines are used, although the speed-up becomes milder when more machines are used. With only eight machines, we can finish computing MCE in the largest dataset in about an hour.
Maximal clique enumeration has been studied extensively and comprehensive reviews can be found in [10] and [8] (the latter also discusses maximum clique finding). The first algorithms were the backtracking method [3, 9] that use O ( | V | 2 ) memory space. Then effective pruning strategy by selecting good pivots was employed Figure 4: Running time (wall-clock time in seconds) of ParMCE to further reduce the search space [10, 23, 29]. Algorithm for d -degenerate graphs was also proposed [18, 19], which achieves a time complexity of O ( d | V | 3 d/ 3 ) by utilizing a d -degeneracy order-ing. However, all these studies did not focus on reducing the mem-ory complexity and require  X ( | V | + | E | ) memory space, which may not be practical for large disk-resident graphs.
 Recently, several parallel algorithms were proposed for MCE. Most of these parallel algorithms [17, 27] still require the entire input graph to be resident in main memory of each computing node, and thus are not practical for processing large graphs that cannot fit in memory. A MapReduce algorithm [33] was also proposed for MCE. The MapReduce model, however, is slow in practice for enumeration tasks such as MCE and triangle listing in large graphs, mainly because of the huge amount of intermediate data produced in the shuffling phase between Mappers and Reducers.

Algorithm for output-sensitive MCE was also introduced which is based on reverse search [30]. The time delay was reduced to O ( d 4 max ) for sparse graphs using matrix multiplication [25], where d max is the maximum degree of a graph; but the algorithm requires O ( | V | X | E | ) preprocessing time. Other algorithms, such as comput-ing a k -clique by joining two ( k  X  1) -cliques [24], by making use of triangles [31], and enumerating maximal cliques of size larger than a threshold [26], were also studied. However, all these algorithms require memory space at least  X ( | V | + | E | ) .

Stix [28] proposed a streaming algorithm that updates the set of maximal cliques upon each edge insertion, but the update is expen-sive and it also requires to keep all maximal cliques in memory, whichhasbeenverifiedin[12].

Recently, Cheng et al. [12, 13] proposed an efficient algorithm that recursively extracts a core part of the input graph for local MCE computation. This idea is similar to the algorithm described in Sec-tion 4.1, but a fundamental difference in the formulation of the sub-graphs to be extracted, i.e., their subgraphs still need to access the input graph for MCE while ours do not, making it nontrivial to par-allelize their algorithm as what we do. We have also verified in Section 5 that our algorithm is significantly more efficient.
Other related works that also avoid random disk access by ex-tracting small subgraphs include core/truss decomposition [11, 32], triangle listing [14, 15]. However, the subgraphs used in these works cannot guarantee the maximality of the cliques computed and their algorithm frameworks are also not suitable for the task of MCE. We presented efficient algorithms to reduce both the I/O cost and CPU cost of MCE in massive networks. We verified the perfor-mance of our algorithms comparing with the state-of-the-art algo-rithms for MCE [12, 13, 18, 19, 29]. Our results first demonstrate that, by reducing the cost of set intersection in MCE, we are able to achieve significant speed-ups in both cases when the input graph can fit in main memory or cannot fit in main memory. Then, we also showed that our parallel algorithm for MCE significantly speeds up the MCE computation compared with the sequential algorithm.
The authors would like to thank the reviewers of this paper for their constructive comments. This research is supported in part by the A*STAR Thematic Strategic Research Programme Grants (102 158 0034 and 112 172 0013). [1] F.N.Abu-Khzam,N.E.Baldwin,M.A.Langston,andN.F.
 [2] A. Aggarwal and S. Vitter, Jeffrey. The input/output [3] E. A. Akkoyunlu. The enumeration of maximal cliques of [4] K. Andreev and H. Racke. Balanced graph partitioning. In [5] H. R. Bernard, P. D. Killworth, and L. Sailer. Informant [6] N. M. Berry, T. H. Ko, T. Moy, J. Smrcka, J. Turnley, and [7] V. Boginski, S. Butenko, and P. M. Pardalos. Statistical [8] I. M. Bomze, M. Budinich, P. M. Pardalos, and M. Pelillo. [9] C. Bron and J. Kerbosch. Algorithm 457: finding all cliques [10] F. Cazals and C. Karande. A note on the problem of [11] J. Cheng, Y. Ke, S. Chu, and M. T.  X zsu. Efficient core [12] J. Cheng, Y. Ke, A. W.-C. Fu, J. X. Yu, and L. Zhu. Finding [13] J. Cheng, Y. Ke, A. W.-C. Fu, J. X. Yu, and L. Zhu. Finding [14] S. Chu and J. Cheng. Triangle listing in massive networks. To [15] S. Chu and J. Cheng. Triangle listing in massive networks [16] G. Creamer, R. Rowe, S. Hershkop, and S. J. Stolfo. [17] N. Du, B. Wu, L. Xu, B. Wang, and P. Xin. Parallel [18] D. Eppstein, M. L X ffler, and D. Strash. Listing all maximal [19] D. Eppstein and D. Strash. Listing all maximal cliques in [20] K. Faust and S. Wasserman. Social network analysis: [21] K. Gouda and M. J. Zaki. Efficiently mining maximal [22] D. E. Knuth. Estimating the efficiency of backtrack [ 23] I. Koch. Enumerating all connected maximal common [24] F. Kose, W. Weckwerth, T. Linke, and O. Fiehn. Visualizing [25] K. Makino and T. Uno. New algorithms for enumerating all [26] N. Modani and K. Dey. Large maximal cliques enumeration [27] M. C. Schmidt, N. F. Samatova, K. Thomas, and B.-H. Park. [28] V. Stix. Finding all maximal cliques in dynamic graphs. [29] E. Tomita, A. Tanaka, and H. Takahashi. The worst-case time [30] S. Tsukiyama, M. Ide, H. Ariyoshi, and I. Shirakawa. A new [31] L. Wan, B. Wu, N. Du, Q. Ye, and P. Chen. A new algorithm [32] J. Wang and J. Cheng. Truss decomposition in massive [33] B. Wu, S. Yang, H. Zhao, and B. Wang. A distributed
