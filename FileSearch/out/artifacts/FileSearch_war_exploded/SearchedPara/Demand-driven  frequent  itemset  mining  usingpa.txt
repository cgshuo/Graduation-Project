
The problem of mining freque nt itemsets in a set of transactions wherein each trans-action is a set of items was first introduced by Agrawal et al. (1993). In its simplest market-basket scenario.
 (Chaudhuri 1998), as a majority of datasets in the real world are stored in the rela-relational form. An item is described by a set of attributes (Han and Fu 1995; Srikant and Agrawal 1995). For instance, a system event can be described by attributes such teresting association rules, such as  X  TCPConnectionClose (E
Vesuvio (H OST ) often occurs together with Security (C ATEGORY ) problems during Authorization (A PPLICATION ), X  relate to items each described by a sub-events that occurred together (occurred dur ing the same transaction as indicated by their TIDs).
 attributes, E VENT T YPE and H OST , so its pattern structure is
R 2 contains two events, described by C ATEGORY , H OST pattern structures convey important knowledge about the patterns. More important, structures. Our work in mining frequent itemsets in event databases brings forth some interesting issues and challenges. E
XPONENTIAL S EARCH S PAC E . Multiattribute items incur a huge search space. The item in R 1 is defined by two attributes: E VENT T YPE challenge: in a dataset with N attributes, items can be defined in as many as 2 different ways or have as many as 2 N  X  1 different structures. Furthermore, they challenge: patterns containing k items can have as many as structures (Proposition 1).
 U
SER P REFERENCES . Traditional association rule algorithms return all frequent patterns where E VENT T YPE and S OURCE are involved in describing the patterns. both the S OURCE and C ATEGORY attributes. Furthermore, he wants to preclude all mining tasks that contain elements for which C ATEGORY which H OST belongs to the ibm.com domain. Note that such user preferences can-not be enforced by preprocessing the data to exclude certain attributes or values, as these attributes and values may combine in different ways to form patterns that are interesting to the user. Thus, we need a mech anism to specify user preferences and incorporate them into the mining process.
 R
ELATIONAL S ENSITIVITY . The patterns embedded in the event data, unlike super-market purchasing patterns that are more or less stable over the time, are constantly evolving as old problems in the system are being solved and new types of prob-lems are being generated. The mining algorithms should focus on newly generated our mining algorithms to be relational sensitive (Chaudhuri 1998), as it is often in-efficient and unnecessary to first convert the data to a mining format, then discover
The dynamic nature of the data prevents us from running a mining algorithm once and for all, and saving the results for future analysis.

In this paper, we present the HIFI 1 algorithm that meets the above challenges (Wang et al. 2002). We ensure maximum information sharing during the mining process. The framework is also conducive to incorporating user preferences for ef-ficient mining and it mines multiattribut e datasets directly without encoding the data.
 related works. In Sect. 3, we give an overview of our approach. Section 4 introduces some definitions and notations used in this paper. We present the HIFI framework in Sect. 5. Section 6 details the implementation of the HIFI framework. We show experiment results in Sect. 7 and conclude in Sect. 8.
Agrawal and Srikant (Agrawal and Srikant 1995; Srikant and Agrawal 1996) led of k literals) are generated by joining patterns on level k tribute A ,wecreateanitem A a ). However, the pattern structure is lost through such are used in describing each item in the patte rns. As a result, user preferences cannot be incorporated in the mining process. The algorithm mines (unnecessarily) all fre-quent patterns before the answers conformed to user X  X  mining targets can be filtered out during the final step. It will apparently slow down the mining process when the dataset is large and high dimensional, and it also creates difficulty in understanding their pattern structures as in the HIFI approach.
 ciation rules based on item taxonomy and hierarchy. These approaches are fur-straints. In Table 2, we compare them with HIFI. With a prespecified hierarchy to fit into the given taxonomy or hierarchy. In contrast, our framework enables us to mine patterns without a prespecified item hierarchy. HIFI offers the max-imum flexibility in defining patterns: patterns containing k items can have different pattern structures. The search space of hierarchy and taxonomy is much smaller and their discoveries are limited by the taxonomy or the fixed hierarchy they use.
 data clusters using templates expressed as second-order predicates. Fu and Han (1995) and Kamber et al. (1997) extend meta-queries to relational databases and multidimen-junction of predicates instantiated on a single record. In contrast, our work considers multiattribute patterns formed from multiple records.

More recently, there has been work on a different kind of multiattribute mining X  circumstances (Grahne et al. 2001) and dynamic grouping (Perng et al. 2001), where an even higher level of flexibility in mining frequent itemsets.
Our approach introduces the concept of pattern structure to identify particular forms
R is (E VENT T YPE ,H OST ), as R 1 contains a single item described by two at-tributes, E VENT T YPE and H OST . Similarly, pattern R H
OST )(E VENT T YPE ,H OST ), as it contains two items each described by a set of having only one attribute, thus, all patterns share the same pattern structure.
Multiattribute association mining should center on pattern structures because mining performance can be improved and (ii) more important, user preferences can be incorporated into the mining process. A u ser can specify desired pattern structures and irrelevant pattern structures by using a rule-like language. As a result, the search space can often be reduced significantly. Although knowledge representation and in-duction are beyond the scope of this paper, our framework provides a foundation for domain knowledge-based data mining.

Different pattern structures in our framework are related by the antimonotonicity property (or the so-called downward closure property). This property not only holds when we expand the size of a pattern from k -items to ( k we expand the structure of a pattern by using more attributes to describe the existing items in the pattern. Working together, they provide a rich set of relationships among exploited to the full extent. Given a k -itemset pattern, each item i defined by N attributes, we show that it has k i N i parent patterns. The antimonotonicity property holds between each parent and child pattern , based on which candidate patterns can without presorting, thus avoiding expensive pattern generation methods that require sorting.

Last, the effectiveness of association r ule mining is often limited by the difficulty in interpreting the numerous rules it produces . Our restructured search space provides us an organized view of the resulting frequent itemsets.
Let D be a set of transactions, where each tran saction contains a set of records and fundamental concept of item is defined differently.
 of attribute-value pairs { t 1 = v 1 , ..., t k = v k } attribute t i . We call T the item structure of I .
 is {C ATEGORY = Security, S EVERITY = High}, and its item structure is {C S
EVERITY }. We also use security, high to denote the structured item when no confusion arises.

Definition 2. A record R is an instance of a structured item I if R . t i = v i ,  X  t i  X  X  t 1 , ..., t k } .
 of I .

Definition 3. A pattern is a set of structured items. A k -itemset pattern is a pattern containing k structured items.
 { i
Definition 4. A transaction S supports pattern X if each structured item I maps to a unique record R  X  S such that R is an instance of I . Pattern X has support s in dataset D if s % of the transactions in D support X . i :{S OURCE = DHCP} and an instance of i 2 :{H OST = Etna}, which makes it an instance of i 3 = i 1  X  i 2 :{S OURCE = DHCP, H OST = Etna} as well. Also, we can see 2-itemset patterns supported by transaction 1001 (the first 5 records): and { i 3 , i 4 } .
 cause i 1 and i 2 are supported by one record and one record only in transaction 1001.
According to the definition, in order to support { i 1 , i at least two records that are instances of i 1 and i
Definition 5. We define a pattern structure c as a multiset, c each T i is a nonempty set of attributes. A pattern of structure c has the form p {
I , ..., I we use ( A )( AB )( C ) to represent pattern structure c {
A } , T example.
 (
A )( B ) has two items, which are described by attribute A and B , respectively; while and B at the same time.

If there is only one attribute, then all patte rn structures are of the form (A)(A)...(A), and the problem degenerates to traditional association rule mining. However, as shown by Proposition 1, the mining space grows exponentially when the number of attributes increases.

Proposition 1. Given a dataset with N different attributes, there are a total of different pattern structures for k -itemset patterns.

Proof. An equivalent problem is as follows: how many different outcomes are there after a throw of k dice, each having m sides? The answer is in m = 2 N  X  1 ways. Thus, there are a total of 2 N + for k -itemset patterns.

The complexity of finding all patterns is much greater than the complexity of finding the structures. Thus, to handle s uch a huge mining space, we need to explore (
AB ) is a specialization of structure ( A ) and structure explores these links by defining the successor/predecessor relationships.
Definition 6. Given a pattern structure c ={ T 1 , ..., T immediate successors are in one of the following forms: 1. {
T 1 , ..., T m , T m + 1 } ,where T m + 1 ={ t } 2. { T 1 , ..., T j  X  X  t } , ..., T m } , t  X  T j .

Figure 2 depicts a graph of pattern struct ures tightly coupled by the predecessor/ successor relationships for two attributes A and B. Note that a pattern structure c {
T , ..., T attributes of c . The benefits of structuring the search space in the level-wise, tightly coupled form are the following:  X 
The framework reveals all the relationships among pattern structures. These re-lationships are essential for candidate generation and pruning.  X  Instead of joining the patterns on level L to derive candidate patterns on level
L + 1 and then using the patterns on level L again for pruning, we can localize the candidate generation and pruning pr ocedure to each pattern structure. For  X  The framework requires no data encoding. Each structure represents a mining
The extended downward closure propert y, which holds between each pair of parent and child pattern structures, enables us to eliminate candidate patterns.
Proposition 2. (Downward closure property) Let p ={ I 1 structure c , and the support of p is less than min _ sup ,then 1. the support of pattern p a ={ I 1 , ..., I k , I a 2. the support of pattern p b ={ I 1 , ..., I k  X  1 , I Proof. Similar to the Apriori property (Agrawal and Srikant 1994). granularity, i.e. to the pattern structure level. It enables us to localize candidate gen-eration and pruning for specific pattern structures.
The introduction of pattern structures makes frequent itemset mining a two-step pro-expandable to include new types of preferences.  X  include in the mining.

Example: P1. Include all pattern structures that involve attributes A and/or B.  X 
Target exclusion. The user specifies a set of rules to exclude certain pattern struc-tures.

Example: P2. Exclude all pattern stru ctures where attributes A and B are used together to describe one of the items.  X 
Value exclusion. The user specifies a set of rules concerning the values of at-tributes in the pattern structures.

Example: P3. Exclude all patterns that contain (A = and b  X  X  b 2 , b 3 } .

User preferences can be expressed in a Prolog-like language. The order in which inclusion and exclusion rules are applied m ay affect the final search space. Obviously, a pattern structure is affected by certain valu e-exclusion rules, then all of its descen-portant property because candidate generatio n and pruning for the structure depends on patterns of its predecessors.
 In Fig. 3, we show the search space afte r applying user preferences P1, P2 and
P3. Pattern structures with value-exclus ion constraints are marked with a dotted box. (A)(A)  X  X  X  (A), and k -item candidate patterns can be generated by merge joining ( k  X  1 ) -item patterns. In HIFI, candidate generation takes two steps: the generation of pattern structures and the generation of ca ndidate patterns in each structure. Because framework when applicable. We show that candidate patterns of a structure can be generated by joining the patterns of any two of its predecessor structures. Although referred to as a simple join, which can be implemented efficiently if the patterns are joins without presorting.

To facilitate further discussions, we assume that there exists an order (e.g. alphabet-ical order) among the attributes in A . Given a set of attributes T to denote the ordered sequence of the same attributes. Assuming T such sequences, we say T 1 T 2 if T 1 holds lexicographical precedence over T
Thus, we can uniquely represent a structure c by its ordered version c where T 1 , ..., T k . Figure 2 shows each pattern structure using the ordered rep-resentation, assuming alphabetical order among the attributes, i.e. A can be represented by a table with L columns: t 11 , ..., is the (ordered) attributes of T i .Weuse c . t ij to denote the column defined by the j th attribute of T i , and we use c k to denote a parent structure of c resulting from taking out the k th column from c . For instance, the patterns of structure c its three parent structures are represented by tables of two columns.
We show how patterns of parent structures can be joined to produce patterns of child structures. To generate the candidate patterns of structure patterns of its parents c 3 and c 1 as follows:
Example 1. Join c 3 = ( A )( B ) and c 1 = ( BC ) to derive c
However, not all join operations can be expressed as succinctly. For example, to generate all possible patterns of ( A )( AC ) by joining c we will have to use the following SQL:
Example 2. Join c 3 = ( A )( A ) and c 1 = ( AC ) to derive c
The reason for the complexity is because of the following: if p a pattern of ( A )( A ) , then patterns derived from p for compare attribute A of c 1 with both attributes of c 3 operations can be avoided.

We show that the above join operations are complete, meaning that the result of the join contains all the patterns that have a support greater than min _ sup .
Proposition 3. (Join property I) Given a pattern structure c on level L , L structures contain all the frequent patterns of c .

Proof. Let p be a pattern of structure c ,andlet c i and c
Removing the i th column and then the j th column of p , we get two subpatterns, p and p j . Because p is a pattern of c , according to the antimonotonicity property, p and p j must be patterns of c i and c j , respectively. Thus, pattern p can be derived by joining p i and p j on their common columns.

Join Property I also implies that joining the patterns of a structure X  X  immediate predecessors.

According to Join Property I, for any pattern structure on level L , L
Other predecessors can be used to further pr une the candidate patterns according to the antimonotonicity property.
 ample 1, without indexing or sorting. We denote join operations in Example 1 as joins in the simple form, while join operations in Example 2 with a disjuntive WHERE condition in the nonsimple form. Joins in the simple form can be implemented effi-implemented efficiently if patterns in c 3 and c 1 are ordered by attribute B . indices on table c 3 are required: one on column t 11 , the other on t have to do a linear scan on table c 3 . (Assuming c each tuple of c 3 , check if there is a tuple in c 1 that satisfies the join condition using is expensive.
 if patterns of c can be derived by joining c i with c c ={ T
We call c i a simple parent of c if T i = T k  X  T i  X 
Proposition 4. Patterns of structure c can be derived by joining c simple form, if both c i and c j are simple parents of c .

Proof. (Sketch) Let c i ={ T 1 , ..., T k , ..., T n } be a simple parent of c . Because
T
T i in c i and c ; in other words, only one item structure, that is, T map to T k . Then, the SQL statement will use the mappings between the two simple parents to c i as the WHERE condition, that is, WHERE F
Proposition 5. (Join Property II) Given a pattern structure c on level L ,thereexist at least two predecessor structures, c i and c j , that can be joined in the simple form to generate patterns for c .

Proof. Assuming c = T 1 , ..., T k , | T 1 |= m and | T k c  X  n + 1 can be joined in the simple form. Given | T 1 |= m , c attribute of T 1 results in a new sequence of attributes T , we have either T or T  X  T 1 , ..., T k , which means there can be no T
Similar reasoning applies to c L  X  n + 1 , which is the parent structure of c after removing of the first attribute of T k . Because m + n  X  L , we know m to Proposition 4, c m and c L  X  n + 1 can be joined in the simple form. (i.e., patterns of ( AB )( CD ) are ordered by their values of A ,then B , C and D ).
Then, candidate patterns (of a child structure) derived by merge joining the ordered still maintain the order. We can repeat this process to generate patterns on all levels through merge joining without resorting the data.
 (
AB )( CD )( EF ) on level 6. These can be generated by joining the patterns of c (
AB )( CD )( E ) and c 5 = ( AB )( CD )( F ) . We can merge join c derived by merge joining two of its parents using their existing order? of c must have the same first ( L  X  2 ) attributes. Thus, the only two parents that can last and the next-to-last column of c , respectively. However, sometimes c do not have the same first L  X  2 attributes. Take c for example. Parent structure c 4 does not exist in the form of (
A )( AB ) because ( A )  X  ( AB ) . Thus, patterns of c first L  X  2 = 2 attributes as c 3 = ( AB )( B ) . On the other hand, even if c be joined in the simple form. An example of such a case is
L  X  3 can be derived by merge joining the patterns of c conditions are satisfied: (i) c L and c L  X  1 can be joined in the simple form and (ii) c and c L  X  1 share the same first L  X  2 attributes.

Proof. Condition (i) guarantees only a single ordering of the patterns is required and (ii) guarantees they can be merge joined.

InTable4,weshowparents c L and c L  X  1 of each pattern structure on level L in the previous section.
 The main procedure for mining the HIFI framework is outlined in Algorithm 6.1.
We start by generating frequent itemsets o n the first level, where each pattern struc-candidate patterns on the second level. For instance, if a patterns of structure ( A ) and ( B ) on the first level, then a (Algorithm 6.2), and on line 7, we eliminate infrequent patterns. Next, we generate vious section (Algorithm 6.4). We repeat the process until no more patterns can be generated.

Algorithm 6.1 HIFI(Dataset: D , MinSupport: min _ sup ) shown in Algorithm 6.2 scans the dataset once to count the occurrences of each pattern is supported by a different record in a transaction. Efficient access to all valid edges on the second level correspond to values of attribute B, and so on. Star  X * X  is the item tree shown in Fig. 5 represent the following items: c 1 , c 2 , a 2 a 2 , c 2 . When we scan the data, we store the occurrences of each valid item in the item a 2 means both record 1 and record 3 o f transaction 3 are instances of a 2 .
The reasons that we use the occurrence buffers are (i) we need to diff erentiate records in each transaction in order to support the c ounting for  X  X xclusive X  itemsets; (ii) we cannot afford to check all the patterns for each transaction coming in. The occurrence buffers allow us to perform in a batch mode. Whenever the buffer is filled, we count the support for each pattern by simultaneously scanning the occurrence buffers of records in a same transaction. For example, as shown in Fig. 5, pattern a 2 a 2 consists of two items, a 2 ,and a 2 , b 1 . When we scan the occurrence buffers of a 2 and a 2 , b 1 , we find that they are both supported in transactions 1 and 2, but only in transaction 1, are they supported by different records; thus, the count of the pattern is increased by 1.

Algorithm 6.2 countSupport(Dataset: D , PatternSet: candidates )
Algorithm 6.3 StructureGen(Se tOfStructures: parentStructures )
Based on the structures on the previous level, a naive way of generating all the optimal because each structure can have mu ltiple successors, and they are generated
Based on a parent structure p = T 1 , ..., T k , we create a subset of its child structures using the following methods: (i) adding a single attribute item T
T k T k + 1 ; (ii) adding a new attribute to an existing item T
T such that T k T . For instance, given a dataset with only 2 attributes, A and B , and a structure ( A )( AB ) , we create ( A )( AB )( B according to (ii). Other child structures of ( A )( AB proof here due to lack of space.
 The core of HIFI is the candidate generation procedure shown in Algorithm 6.4. major components: candidate generation by j oining two of its parents and candidate filtering by the rest of the parents.

Algorithm 6.4 CandidateGen(SetOfStructures: Structures )
To study the effect of the HIFI algorithm, we performed experiments using both synthetic datasets and real-life dataset s. The tests were carried out on a Pentium III machine running Linux OS 2.2.1 with a 766-MHz CPU, 256-M memory. Detailed experimental results are also available in Wang et al. (2002) and Perng et al. (2001).
Hierarchy and Taxonomy. Hierarchy is based on the ML_T* series algorithms (Han and Fu 1995). Given a dataset with N attributes, Hierarchy enumerates all N sible hierarchies of N levels, and for each hierarchy, we use ML_T* to find frequent itemsets. Furthermore, we also find cross-level patterns by combining frequent item-sets on different hierarchy levels (Han and Fu 1995). Taxonomy is based on the
EstMerge algorithm (Srikant and Agrawal 1995). A record with N attributes are en-coded into 2 N  X  1 items, such that a transaction with k records contains as many as k ( 2 N  X  1 ) items. Taxonomy then performs an Apriori search in the extended trans-actions.
 of attributes; I , average records per transaction; N , average number of distinct values for each attribute; and P , number of maximum patterns. Both I and N are the
Poisson distribution parameters. In a ddition, the number of items in a maximum pattern is decided by the Poisson distribution with attributes in an item is decided by the Poisson distribution with dataset, for instance, D100K.A5.I8 is the dataset generated by setting D
A = 5and I = 8.
 First, we study the benefits of the mergen join. Figure 6(a) shows on different
HIFI levels the percentage of structures the patterns of which can be derived by merge join. Note that (i) the percentage is actually larger when the dataset has more attributes, which makes merge join even more valuable in mining high-dimensional data; (ii) structures on lower levels, which usually have more candidate patterns, have candidates is avoided. Figure 6(b) shows the speedup in the candidate generation number of candidates. The dataset we used in this test is D100K.A?.I5. The greatest speedup occurs on levels 4 and 5, where both the number of structures and the number of candidates reach the maximum.

Figure 7 shows the scalability of the HIFI algorithm and compares HIFI with the Hierarchy and the Taxonomy approaches under varying parameter D, A, I. As expected and shown in Fig. 7(a), the execution time increases linearly with the size of the dataset. Figure 7(b), however, shows that the performance is heavily dependent significantly as we increase I , the average number of records in a transaction. This is because, when I becomes larger, patterns will contain more items and require more passes of data scans to discover.
 We compare the performance of HIFI agai nst the other two approaches, namely
Hierarchy and Taxonomy, under varying parameter D (60 K, 200 K, 500 K), A (4, 5, 6), and I (5, 6), and the results are shown in Fig. 7(d)(e)(f). We found that, with other approaches. However, parameter A and I have a much more negative influence on the scalability of Hierarchy and Taxonomy than on that of HIFI. This is so because theincreaseof A and I increases search space size exponentially; HIFI scales bet-ter than Hierarchy and Taxonomy because HIFI manages to prune a large amount of candidate itemsets by taking advantage of the antimonotonic relationships in the tightly coupled mining space. Also, the Hierarchy and Taxonomy approaches only discover a subset of the patterns discovered by HIFI; thus, with regard to their effec-Hierarchy and Taxonomy.

We also applied the HIFI algorithm on a real-life dataset, NETVIEW, which is generated by a production network at a financial service company. Events in the dataset are grouped into transactions by their timestamps based on a 30-second inter-val and, on average, each transaction contains about 10 events. Each event has mul-tiple attributes, which include Event , Host , Severity ,and Category . Overall, there are 241 event types, 2526 hosts, 6 severity levels, and 17 event categories. Typ-ically, mining is performed on data generated in a weekly window using different user preferences.

The HIFI (Wang et al. 2002) and the FARM (Perng et al. 2001) algorithms are currently being incorporated into IBM X  X  system management product EventMiner into the understanding of the operational environment. For instance, we noticed that many of the patterns discovered by HIFI are not discovered by previous arts. Here, we present some simple patterns discovered by HIFI. support. It means when CiscoLinkUp takes place on host 31, some other hosts in the AbstractNetwork category often issue a MinorAlarm message.  X 
Patterns of structure (H OST )(H OST ) show that events from host 3 frequently come together with events from host 2, 7, 8, 12, 16, 17, and 31, which is a hint (H OST ,E VENT )(H OST ) , we found that the corresponding events are MiddleLayer ManagerUp for host 17, MiddleLayerManagerDown for host 16,
SyslogAbort for host 8 and TCPConnectionClose for the rest. This find-ing provides information for building t he behavior model of the operational en-vironment.
Discovering association rules from large datasets has been the focus of much re-item, and a pattern is often composed of items defined by different sets of attributes. This adds up to an exponential search space, and only part of it is explored by approaches using hierarchies and taxonomy (Han and Fu 1995; Srikant and Agrawal 1995).
We present the search space of frequent pa tterns in a novel architecture, where pattern structures are tightly coupled in the antimonotonic relationships (Wang et al. 2002). Using such relationships and an efficient candidate generation algorithm based on merge join, our approach is able to prune away a large amount of candidate patterns, thus greatly improving the mining performance.

Unlike the level-wise mining algorithms used in Apriori and its extensions (Agrawal and Srikant 1995; Srikant and Agrawal 1996), our algorithm localizes the of query structures, we are able to find their frequent itemsets by exploring a much smaller search space than the o ther approaches. Further more, our algorithm is rela-a relational table into items. The organization of the search space is also conducive demand-driven, interactive and online database centric mining where the data to be mined are generated on the fly by other query tools.
 Based on these encouraging results, many additional topics can be addressed.
First, domain knowledge can be used to provide guidance in the search. For in-stance, combinations of attributes or attribute values that are deemed not interesting by domain experts can be excluded from the search space. In other scenarios, users want to focus their search on patterns that belong to a set of pattern templates, and the HIFI framework can be used to figure out a least-expensive strategy for mining such patterns. Second, our approach addre sses the flexibility issue in defining items.
There is an orthogonal issue in defining transactions, for instance, transactions in an event dataset can be defined by grouping items based on timestamps, hosts, event types or any subset of attributes. This orthogonal issue was explored in Perng et al. (2001) and it can be combined with HIFI to a ttain an even higher level of flexibility in mining frequent itemsets.

