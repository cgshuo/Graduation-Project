 From learning sounds to learning the meanings of words, social interactions are extremely important for children X  X  early language acquisition (Baldwin, 1993; Kuhl et al., 2003). For example, children who engage in more joint attention (e.g. looking at par-ticular objects together) with caregivers tend to learn words faster (Carpenter et al., 1998). Yet compu-tational or formal models of social interaction are rare, and those that exist have rarely gone beyond the stage of cue-weighting models. In order to study the role that social cues play in language acquisition, this paper presents a structured statistical model of grounded learning that learns a mapping between words and objects from a corpus of child-directed utterances in a completely unsupervised fashion. It exploits five different social cues , which indicate which object (if any) the child is looking at, which object the child is touching, etc. Our models learn the salience of each social cue in establishing refer-ence, relative to their co-occurrence with objects that are not being referred to. Thus, this work is consis-tent with a view of language acquisition in which children learn to learn , discovering organizing prin-ciples for how language is organized and used so-cially (Baldwin, 1993; Hollich et al., 2000; Smith et al., 2002).

We reduce the grounded learning task to a gram-matical inference problem (Johnson et al., 2010; B  X  orschinger et al., 2011). The strings presented to our grammatical learner contain a prefix which en-codes the objects and their social cues for each ut-terance, and the rules of the grammar encode rela-tionships between these objects and specific words. These rules permit every object to map to every word (including function words; i.e., there is no  X  X top word X  list), and the learning process decides which of these rules will have a non-trivial proba-bility (these encode the object-word mappings the system has learned).

This reduction of grounded learning to grammat-ical inference allows us to use standard grammati-cal inference procedures to learn our models. Here we use the adaptor grammar package described in Johnson et al. (2007) and Johnson and Goldwater (2009) with  X  X ut of the box X  default settings; no parameter tuning whatsoever was done. Adaptor grammars are a framework for specifying hierarchi-cal non-parametric models that has been previously used to model language acquisition (Johnson, 2008).
Social cue Value child.eyes objects child is looking at child.hands objects child is touching mom.eyes objects care-giver is looking at mom.hands objects care-giver is touching mom.point objects care-giver is pointing to
A semanticist might argue that our view of refer-ential mapping is flawed: full noun phrases (e.g., the dog ), rather than nouns, refer to specific objects, and nouns denote properties (e.g., dog denotes the prop-erty of being a dog). Learning that a noun, e.g., dog , is part of a phrase used to refer to a specific dog (say, Fido) does not suffice to determine the noun X  X  mean-ing: the noun could denote a specific breed of dog, or animals in general. But learning word-object rela-tionships is a plausible first step for any learner: it is often only the contrast between learned relationships and novel relationships that allows children to in-duce super-or sub-ordinate mappings (Clark, 1987). Nevertheless, in deference to such objections, we call the object that a phrase containing a given noun refers to the topic of that noun. (This is also appro-priate, given that our models are specialisations of topic models).

Our models are intended as an  X  X deal learner X  ap-proach to early social language learning, attempt-ing to weight the importance of social and structural factors in the acquisition of word-object correspon-dences. From this perspective, the primary goal is to investigate the relationships between acquisition tasks (Johnson, 2008; Johnson et al., 2010), looking for synergies (areas of acquisition where attempting two learning tasks jointly can provide gains in both) as well as areas where information overlaps. 1.1 A training corpus for social cues Our work here uses a corpus of child-directed speech annotated with social cues, described in Frank et al. (to appear). The corpus consists of 4,763 orthographically-transcribed utterances of caregivers to their pre-linguistic children (ages 6, 12, and 18 months) during home visits where children played with a consistent set of toys. The sessions were video-taped, and each utterance was annotated with the five social cues described in Figure 1.
Each utterance in the corpus contains the follow-ing information:  X  the sequence of orthographic words uttered by  X  a set of available topics (i.e., objects in the non- X  the values of the social cues, and  X  a set of intended topics , which the care-giver Figure 2 presents this information for an example ut-terance. All of these but the intended topics are pro-vided to our learning algorithms; the intended top-ics are used to evaluate the output produced by our learners.

Generally the intended topics consist of zero or one elements from the available topics, but not al-ways: it is possible for the caregiver to refer to two objects in a single utterance, or to refer to an object not in the current non-linguistic context (e.g., to a toy that has been put away). There is a considerable amount of anaphora in this corpus, which our mod-els currently ignore.

Frank et al. (to appear) give extensive details on the corpus, including inter-annotator reliability in-formation for all annotations, and provide detailed statistical analyses of the relationships between the various social cues, the available topics and the in-tended topics. That paper also gives instructions on obtaining the corpus. 1.2 Previous work There is a growing body of work on the role of social cues in language acquisition. The language acqui-sition research community has long recognized the importance of social cues for child language acqui-sition (Baldwin, 1991; Carpenter et al., 1998; Kuhl et al., 2003).

Siskind (1996) describes one of the first exam-ples of a model that learns the relationship between words and topics, albeit in a non-statistical frame-work. Yu and Ballard (2007) describe an associative learner that associates words with topics and that exploits prosodic as well as social cues. The rela-tive importance of the various social cues are spec-ified a priori in their model (rather than learned, as they are here), and unfortunately their training cor-pus is not available. Frank et al. (2008) describes a Bayesian model that learns the relationship between words and topics, but the version of their model that included social cues presented a number of chal-lenges for inference. The unigram model we de-scribe below corresponds most closely to the Frank et al. model. Johnson et al. (2010) reduces grounded learning to grammatical inference for adaptor gram-mars and shows how it can be used to perform word segmentation as well as learning word-topic rela-tionships, but their model does not take social cues into account. This section explains how we reduce ground learn-ing problems with social cues to grammatical in-ference problems, which lets us apply a wide vari-ety of grammatical inference algorithms to grounded learning problems. An advantage of reducing grounded learning to grammatical inference is that it suggests new ways to generalise grounded learn-ing models; we explore three such generalisations here. The main challenge in this reduction is finding a way of expressing the non-linguistic information as part of the strings that serve as the grammatical in-ference procedure X  X  input. Here we encode the non-linguistic information in a  X  X refix X  to each utterance as shown in Figure 2, and devise a grammar such that inference for the grammar corresponds to learn-ing the word-topic relationships and the salience of the social cues for grounded learning.

All our models associate each utterance with zero or one topics (this means we cannot correctly anal-yse utterances with more than one intended topic). We analyse an utterance associated with zero topics as having the special topic None , so we can assume that every utterance has exactly one topic. All our grammars generate strings of the form shown in Fig-ure 2, and they do so by parsing the prefix and the words of the utterance separately; the top-level rules of the grammar force the same topic to be associated with both the prefix and the words of the utterance (see Figure 3). 2.1 Topic models and the unigram PCFG As Johnson et al. (2010) observe, this kind of grounded learning can be viewed as a specialised kind of topic inference in a topic model, where the utterance topic is constrained by the available ob-jects (possible topics). We exploit this observation here using a reduction based on the reduction of LDA topic models to PCFGs proposed by Johnson (2010). This leads to our first model, the unigram grammar, which is a PCFG. 1 Sentence  X  Topic t Words t  X  t  X  T 0 Topic t  X  T t Topic None  X  t  X  T 0 Topic t  X  T None Topic t  X  t  X  T T t  X  t Topical c 1  X  t  X  T Topical c Topical c T None  X  t NotTopical c 1  X  t  X  T NotTopical c NotTopical c Words t  X  Word None (Words t )  X  t  X  T 0 Words t  X  Word t (Words t )  X  t  X  T Word t  X  w  X  t  X  T 0 , w  X  W
Figure 4 presents the rules of the unigram gram-mar. This grammar has two major parts. The rules expanding the Topic t nonterminals ensure that the social cues for the available topic t are parsed un-der the Topical nonterminals. All other available topics are parsed under T None nonterminals, so their social cues are parsed under NotTopical nontermi-nals. The rules expanding these non-terminals are specifically designed so that the generation of the so-cial cues corresponds to a series of binary decisions about each social cue. For example, the probability of the rule is the probability of an object that is an utterance topic occuring with the child.eyes social cue. By es-timating the probabilities of these rules, the model effectively learns the probability of each social cue being associated with a Topical or a NotTopical available topic, respectively.

The nonterminals Words t expand to a sequence of Word t and Word None nonterminals, each of which can expand to any word whatsoever. In prac-tice Word t will expand to those words most strongly associated with topic t , while Word None will expand to those words not associated with any topic. Sentence  X  Topic t Collocs t  X  t  X  T 0 Collocs t  X  Colloc t (Collocs t )  X  t  X  T 0 Collocs t  X  Colloc None (Collocs t )  X  t  X  T Colloc t  X  Words t  X  t  X  T 0 Words t  X  Word t (Words t )  X  t  X  T 0 Words t  X  Word None (Words t )  X  t  X  T Word t  X  Word  X  t  X  T 0 Word  X  w  X  w  X  W 2.2 Adaptor grammars Our other grounded learning models are based on reductions of grounded learning to adaptor gram-mar inference problems. Adaptor grammars are a framework for stating a variety of Bayesian non-parametric models defined in terms of a hierarchy of Pitman-Yor Processes: see Johnson et al. (2007) for a formal description. Informally, an adaptor gram-mar is specified by a set of rules just as in a PCFG, plus a set of adapted nonterminals . The set of trees generated by an adaptor grammar is the same as the set of trees generated by a PCFG with the same rules, but the generative process differs. Non-adapted nonterminals in an adaptor grammar expand just as they do in a PCFG: the probability of choos-ing a rule is specified by its probability. However, the expansion of an adapted nonterminal depends on how it expanded in previous derivations. An adapted nonterminal can directly expand to a subtree with probability proportional to the number of times that subtree has been previously generated; it can also  X  X ack off X  to expand using a grammar rule, just as in a PCFG, with probability proportional to a con-stant. 2
Thus an adaptor grammar can be viewed as caching each tree generated by each adapted non-terminal, and regenerating it with probability pro-portional to the number of times it was previously generated (with some probability mass reserved to generate  X  X ew X  trees). This enables adaptor gram-mars to generalise over subtrees of arbitrary size. Generic software is available for adaptor grammar inference, based either on Variational Bayes (Cohen et al., 2010) or Markov Chain Monte Carlo (Johnson and Goldwater, 2009). We used the latter software because it is capable of performing hyper-parameter inference for the PCFG rule probabilities and the Pitman-Yor Process parameters. We used the  X  X ut-of-the-box X  settings for this software, i.e., uniform priors on all PCFG rule parameters, a Beta(2 , 1) prior on the Pitman-Yor a parameters and a  X  X ague X  Gamma(100 , 0 . 01) prior on the Pitman-Yor b pa-rameters. (Presumably performance could be im-proved if the priors were tuned, but we did not ex-plore this here).

Here we explore a simple  X  X ollocation X  extension to the unigram PCFG which associates multiword collocations, rather than individual words, with top-ics. Hardisty et al. (2010) showed that this signifi-cantly improved performance in a sentiment analy-sis task.

The collocation adaptor grammar in Figure 5 gen-erates the words of the utterance as a sequence of collocations, each of which is a sequence of words. Each collocation is either associated with the sen-tence topic or with the None topic, just like words in the unigram model. Figure 6 shows a sample parse generated by the collocation adaptor grammar.
We also experimented with a variant of the uni-gram and collocation grammars in which the topic-specific word distributions Word t for each t  X  T (the set of non-None available topics) expand via Word None non-terminals. That is, in the variant grammars topical words are generated with the fol-lowing rule schema: In these variant grammars, the Word None nontermi-nal generates all the words of the language, so it de-fines a generic  X  X ackground X  distribution over all the words, rather than just the nontopical words. An ef-fect of this is that the variant grammars tend to iden-tify fewer words as topical. We performed grammatical inference using the adaptor grammar software described in Johnson and Goldwater (2009). 3 All experiments involved 4 runs of 5,000 samples each, of which the first 2,500 were discarded for  X  X urn-in X . 4 From these samples we extracted the modal (i.e., most frequent) analysis, which we evaluated as described below. The results of evaluating each model on the corpus with social cues, and on another corpus identical except that the social cues have been removed, are presented in Fig-ure 7.

Each model was evaluated on each corpus as fol-lows. First, we extracted the utterance X  X  topic from the modal parse (this can be read off the Topic t nodes), and compared this to the intended topics an-notated in the corpus. The frequency with which the models X  predicted topics exactly matches the intended topics is given under  X  X tterance topic ac-curacy X ; the f-score, precision and recall of each model X  X  topic predictions are also given in the table.
Because our models all associate word tokens with topics, we can also evaluate the accuracy with which word tokens are associated with topics. We constructed a small dictionary which identifies the words that can be used as the head of a phrase to refer to the topical objects (e.g., the dictionary in-dicates that dog , doggie and puppy name the topi-cal object DOG ). Our dictionary is relatively conser-vative; between one and eight words are associated with each topic. We scored the topic label on each word token in our corpus as follows. A topic label is scored as correct if it is given in our dictionary and the topic is one of the intended topics for the utter-ance. The  X  X ord topic X  entries in Figure 7 give the results of this evaluation.
Finally, we extracted a lexicon from the parsed corpus produced by each model. We counted how often each word type was associated with each topic in our sampler X  X  output (including the None topic), and assigned the word to its most frequent topic. The  X  X exicon X  entries in Figure 7 show how well the entries in these lexicons match the entries in the manually-constructed dictionary discussed above.
There are 10 different evaluation scores, and no model dominates in all of them. However, the top-scoring result in every evaluation is always for a model trained using social cues, demonstrating the importance of these social cues. The variant colloca-tion model (trained on data with social cues) was the top-scoring model on four evaluation scores, which is more than any other model.

One striking thing about this evaluation is that the recall scores are all much higher than the precision scores, for each evaluation. This indicates that all of the models, especially the unigram model, are la-belling too many words as topical. This is perhaps not too surprising: because our models completely lack any notion of syntactic structure and simply model the association between words and topics, they label many non-nouns with topics (e.g., woof is typically labelled with the topic DOG ). 3.1 Evaluating the importance of social cues It is scientifically interesting to be able to evalu-ate the importance of each of the social cues to grounded learning. One way to do this is to study the effect of adding or removing social cues from the corpus on the ability of our models to perform grounded learning. An important social cue should have a large impact on our models X  performance; an unimportant cue should have little or no impact.
Figure 8 compares the performance of the uni-gram and collocation models on corpora containing a single social cue to their performance on the cor-pus without any social cues, while Figure 9 com-pares the performance of these models on corpora containing all but one social cue to the corpus con-taining all of the social cues. In both of these evalua-tions, with respect to all 10 evaluation measures, the child.eyes social cue had the most impact on model performance.

Why would the child X  X  own gaze be more impor-tant than the caregiver X  X ? Perhaps caregivers are fol-lowing in , i.e., talking about objects that their chil-dren are interested in (Baldwin, 1991). However, an-other possible explanation is that this result is due to the general continuity of conversational topics over time. Frank et al. (to appear) show that for the cur-rent corpus, the topic of the preceding utterance is very likely to be the topic of the current one also. Thus, the child X  X  eyes might be a good predictor be-cause they reflect the fact that the child X  X  attention has been drawn to an object by previous utterances.
Notice that these two possible explanations of the importance of the child.eyes cue are diametrically opposed; the first explanation claims that the cue is important because the child is driving the discourse, while the second explanation claims that the cue is important because the child X  X  gaze follows the topic of the caregiver X  X  previous utterance. This sort of question about causal relationships in conversations may be very difficult to answer using standard de-scriptive techniques, but it may be an interesting av-enue for future investigation using more structured models such as those proposed here. 5 This paper presented four different grounded learn-ing models that exploit social cues. These models are all expressed via reductions to grammatical in-ference problems, so standard  X  X ff the shelf X  gram-matical inference tools can be used to learn them. Here we used the same adaptor grammar software tools to learn all these models, so we can be rel-atively certain that any differences we observe are due to differences in the models, rather than quirks in the software.

Because the adaptor grammar software performs full Bayesian inference, including for model param-eters, an unusual feature of our models is that we did not need to perform any parameter tuning what-soever. This feature is particularly interesting with respect to the parameters on social cues. Psycholog-ical proposals have suggested that children may dis-cover that particular social cues help in establishing reference (Baldwin, 1993; Hollich et al., 2000), but prior modeling work has often assumed that cues, cue weights, or both are prespecified. In contrast, the models described here could in principle discover a wide range of different social conventions.
Our work instantiates the strategy of investigating the structure of children X  X  learning environment us-ing  X  X deal learner X  models. We used our models to investigate scientific questions about the role of so-cial cues in grounded language learning. Because the performance of all four models studied in this paper improve dramatically when provided with so-cial cues in all ten evaluation metrics, this paper pro-vides strong support for the view that social cues are a crucial information source for grounded language learning.

We also showed that the importance of the differ-ent social cues in grounded language learning can be evaluated using  X  X dd one cue X  and  X  X ubtract one cue X  methodologies. According to both of these, the child.eyes cue is the most important of the five so-cial cues studied here. There are at least two pos-sible reasons for this: the caregiver X  X  topic could be determined by the child X  X  gaze, or the child.eyes cue could be providing our models with information about the topic of the previous utterance.

Incorporating topic continuity and anaphoric de-pendencies into our models would be likely to im-prove performance. This improvement might also help us distinguish the two hypotheses about the child.eyes cue. If the child.eyes cue is just provid-ing indirect information about topic continuity, then the importance of the child.eyes cue should decrease when we incorporate topic continuity into our mod-els. But if the child X  X  gaze is in fact determining the care-giver X  X  topic, then child.eyes should remain a strong cue even when anaphoric dependencies and topic continuity are incorporated into our models. This research was supported under the Australian Research Council X  X  Discovery Projects funding scheme (project number DP110102506).

