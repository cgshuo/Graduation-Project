 A main aim of IR is to determine the relevance of each document in a col-lection with respect to the user X  X  infor mation need (represented as a query). Relevance has been regarded as a concept in a probabilistic view for decades [8]. The probability ranking principle (PRP) justified that ranking documents in the order of decreasing probability of relevance can optimize the rank effective-ness [11,12]. This implies that a precise estimation for the probability of relevance can yield an optimal rank and the rank eff ectiveness can empirically indicate the quality of the estimation. Many retrieval models that explicitly or implicitly estimate the probability of relevance are mainly for the ranking purpose. For instance, the classical probabilistic models [12] usually estimate the odds-ratio of the probability of relevance. The final relevance scores are often obtained via some rank-equivalent and approximate calculations. The language modeling (LM) approaches [10,15] can also be considered as estimating the probability of relevance under the generative relevance framework [4].

However, a fundamental research problem arises: can an optimal (or even ideal) rank guarantee that the estimated probabilities are precise and without a risk? For instance, suppose that based on the actual relevance judgements of a group of users, the probabilities of relevance for two documents d 1 and d 2 are p 1 =0 . 74 and p 2 =0 . 26, respectively. Therefore, the correct rank is d 1 at first and then d 2 . Assume that we have two sets of estimated probabilities by two models. One model gives p 1 =0 . 71 and p 2 =0 . 29, while the other gives p 1 =0 . 92 and p 2 =0 . 08. Both models give a correct ra nk. However, the second model overestimates d 1 and underestimates d 2 . Theoretically, this example indicates that part of the estimation risk could be independent of the rank.

The rank-independent risk is not only of theoretical importance for risk mod-eling, but also for a wide range of retrieval tasks, where the initial estimation for document relevance is not the final decisi on. For example, in pseudo-relevance feedback (PRF), the estimated relevance probabilities from the first-round re-trieval largely determine the document weights used for query expansion and thus play an important role in the PRF models [5]. In meta-search [7], as an-other example, the relevance scores or probabilities obtained from different search engines should be fused before the final estimation for the document relevance. Therefore, it is necessary to control the estimation risk at the very early stage before it spreads and gets more complicated in the later stages.

It is important to clarify that the rank-dependent risk refers to the rele-vance probability estimation risk that can influence the rank, while the rank-independent risk does not. Since in practice the ideal rank is usually unavail-able, both types of risks may exist in the estimated relevance probabilities. In this paper, we focus on the latter, which, to our best knowledge, has not yet been paid much attention in the literatur e. Therefore, we aim to single out the effect of the rank-independent risk associat ed to the different estimated relevance probabilities when two resultant ranks are identical.

We propose an easy-to-implement risk management method to adjust the rank-independent risk adaptively for an estimated probability distribution. For a given retrieval model, the proposed method can be regarded as the micro-level adjustment, as opposed to the re-ranking approaches (tackling the rank-dependent risk). The latter can be regarded as the macro-level adjustment and is out of the scope of this paper. Our proposed method is applied and evalu-ated in the pseudo-relevance feedback an d relevance feedback. The hypothesis is that the management of the rank-independent risk associated to the esti-mated probabilities in the first-round retrieval can improve the performance of the second-round retrieval. Experimental results on several large-scale TREC collections have shown the effectiveness of our method.
 Researchers have paid much attention to the probabilistic characteristics of rel-evance [8] over decades. The probabilistic ranking principle (PRP) [11] suggests that the document ranking in the order of decreasing probability of relevance of documents can give the optimal rank effect iveness (e.g., in terms of the expected precision [11]) and minimize the overall risk [13]. The risk here refers to the re-trieval risk [11,13], which is based on the loss function associated with a decision on whether or not to retrieve a document. Th erefore, the retrieval risk is closely related to the rank effectiveness. The risk minimization framework [3,16] sug-gests that the optimal ranking strategies can be obtained through considering suitable loss functions in different IR tasks, and the retrieval risks are formu-lated not only in terms of relevance, but also other factors such as novelty and redundancy. In this paper, we focus on relevance only and leave the extension to other factors as future work.

Recently, several approaches have been proposed for modeling the risk in es-timating relevance probabilities or scores. In [17], it is argued that the formula-tion in most estimates of document relevance only provide the point estimation, i.e., the mean, but ignoring the second-moment estimation, i.e., the variance. The variance in computing the relevance score of each individual document can, however, reflect the uncertainty of the corresponding estimation [17]. Wang and Zhu [14] integrated the similar relevance estimation X  X  uncertainty and the inter-document dependency into a Portfolio Theory (PT) based framework. In the above two models, a parameter is involved to adjust the level of uncertainty of the relevance estimation. Different par ameter settings can yield different doc-ument rankings, thus presumably satisfying different kinds of user preference, or different performance metrics for different IR tasks [17,14]. It turns out that in the literature, little attention has been paid to the modeling of the rank-independent risk, which is the aim of this paper. Our main contributions are:  X  We propose to study the rank-independent risk in estimating the probability  X  A risk management method is proposed to control such risk.  X  The above method has been effectively a pplied to pseudo-relevance feedback The probability of relevance of each document corresponds to one basic retrieval question [4]: what is the probability of this document d beingrelevanttoaquery q ? Accordingly, it can be formulated as p ( r | d, q )[12].Let X  p ( r | d, q )denotethe prior p ( d ), we can normalize it as where D is the document set. This normalization is for the further analysis on the estimated relevance probabilities for all the documents in D . S q ( d ) denotes the estimated relevance probability (after normalization) of the document d with respect to the query q .Let S q denote the estimated relevance distribution for all the documents in D .

Our proposed rank-independent risk modeling is expected to be applicable to most retrieval models that can estimate the probability of relevance. In this paper, our focus is on the language modeling (LM) approaches [10,15]. Lafferty and Zhai [3,4] linked the LM approaches to the probability of relevance p ( r | d, q ). As explained in the introduction, we are going to explore the rank-independent risk associated with any two rank-equival ent relevance distributions. Therefore, we first show two rank-equivalent LM approaches as follows. 3.1 Rank-Equivalent LM Approaches The query-likelihood (QL) approach [10,15] is a standard language modeling (LM) approach for the first-round retrieval. It is formulated as: q  X  X  length, and  X  d is a smoothed language model for a document d .
The Negative KL-Divergence (ND) [3 ] between the query language model  X  q and document language model  X  d is formulated as where H (  X  q , X  d ) is the cross entropy between  X  q and  X  d ,and H (  X  q )istheentropy of the  X  q . According to the deviation in [3,9], if a maximum-likelihood estimator is used to estimate the query language model  X  q ,then The above equation shows that  X  H (  X  q , X  d ) is logarithmically proportional to the in terms of ranking documents. Since in Eq. 3, the H (  X  q ) is independent of document ranking, it turns out that negative KL-divergence is rank-equivalent to the query-likelihood approach. 3.2 Difference between the Two Rank-Equivalent Estimations We now present the difference between the t wo document relevance distributions estimated by the QL model and ND model. For a given q , the document relevance distribution estimated by the QL model is denoted as: where D is a set consisting of all concerned documents.

The document relevance distributio n estimated by the ND model can be de-fined as the normalized exponential of the negative KL-divergence: The exponential transformation (i.e exp {} ) is to transform the divergence value to a probability value. Since the H (  X  q )inEq.3isaconstantforevery d  X  D ,it can be eliminated in the normalization process of Eq. 6. We then get After normalizing p ( q |  X  d )by Z QL (i.e. d  X  D p ( q |  X  d )), we have It shows that in the estimated ND distribution S ND q , the relevance probabilities are raised to the powers of 1 m q of S QL q ( d ), turning to [ S QL q ( d )] ization. As a result, comp ared with the QL relevance distribution in Eq. 5, the ND relevance distribution in Eq. 6 is often more smooth, in a sense that there are less very large or very small probabilities. We will show in the next section that the rank-independent risk of a relevance distribution is related to its smooth-ness. The powers-based idea in Eq. 8 then motivates the distribution remodeling process of our proposed risk manageme nt method (detailed in Section 3.4). 3.3 Entropy-Based Risk Measurement In our work, the concerned probability distribution is the estimated document relevance distribution S q (see Eq. 1) generated by a retrieval model. For in-stances, S q can be S QL q (see Eq. 5) or S ND q (see Eq. 6). The entropy of an estimated S q is defined as: where H is the Shannon entropy of the distribution S q ,and D is the document set, which can be the whole document collection or the top n ranked documents. The entropy H ( S q ) generally indicates the smoothness of the distribution S q . In general, the larger entropy of S q implies a higher degree of smoothness, i.e., there are less probabilities which are relatively too large or too small in S q .
Our assumption here is that the larger the entropy (i.e., the higher degree of smoothness) is, the less rank-independent risk would be with the corresponding distribution, if true relevance judgements are not available to support the ratio-nality of some too large or too small relevance probabilities. In this case, a higher risk would be posed, if a document with relatively too small/large probability is actually relevant/irrelevant.
 Let us further illustrate the intuition of this assumption through an example. Given two documents d 1 and d 2 , suppose two models give different estimations ( p 1 =0.71 p 2 =0.29) and ( p 1 =0.92 p 2 =0.08), respectively, for ( d 1 , d 2 ). The first distribution (0.71, 0.29) is more smooth than the second one (0.92, 0.08). If we do not have any relevance judgements of document d 1 and d 2 , the possible (binary) relevance judgements for ( d 1 , d 2 ) can be (1, 1), (1,0), (0,1) and (0,0), where 1 denotes relevance and 0 denotes i rrelevance. We can see that only in the second case, i.e., (1,0), it is sure that d 1 is more relevant than d 2 and it would be reasonable that d 1  X  X  probability is much bigger than that of d 2 . However, in all other possible cases, the first distribution (which is more smooth) is better than the second one. Specifically, in the cases (1,1) and (0,0), there is no distinction between two relevance judgements, sugge sting that the smoother distribution is safer. In the case (0,1), it turns out that d 1 should not have too large probability. Thus, the first distribution has a less risk since it is better in most (3 out 4) cases. 3.4 Powers-Based Risk Management (PRM) Method We will present a novel risk management method and provide a theoretical anal-ysis to show that the method can make every pair of probabilities in an estimated distribution become more smooth so as to reduce (overall) rank-independent risk (without changing the original document rank). This method can remodel an estimated distribution and the remode ling method is motivated by the powers-based idea described in Eq. 8 Specifica lly, given a retrieval model and its es-timated document relevance distribution S q , the remodeling method will raise every probability in S q to the powers ( f ( q )) and then normalize the revised probabilities. It can be formulated as: where S q denotes the remodeled distribution, and the powers f ( q )( &gt; 0) is a function for the query q . f ( q )cannotonlybe m q in Eq. 8, but also can be other functions (detailed later). Here, we first explain the relations between this re-modeling method and the rank-independent risk measurement. This remodeling algorithm preserves the original document rank and has a property described in Proposition 1 (See Appendix A for the formal Proof). This proposition proves that in Eq. 10, the bigger f ( q ) value (i.e. b in the Proposition), the smaller the relative difference between any two probabilities in the distribution S q and thus the higher degree of overall smoothness of the distribution.
 Proposition 1. Given a distribution S q ,suppose S q ( d i ) and S q ( d j ) are the estimated relevance probabilities of any two document d i and d j , respectively. be smaller than that between [ S q ( d i )] 1 a and [ S q ( d j )] 1 a . Intuitively, a bigger f ( q ) value will exclude too large or too small probabilities in the distribution S q , making the distribution become smoother. Thus, we can draw the observation that in Eq. 10, the bigger the f ( q ) value is, the larger the entropy H ( S q ) of the distribution S q will be. This has been verified based on several probability distributions (e.g., exponential distribution) and estimated document relevance distributions from the TREC data.

In this paper, we adopt two options for f ( q ), each corresponding to an in-stantiated algorithm of our method. The first option is m q as used in the Eq. 8, where m q is the length of query q .Wedenotethisoptionas Since m q is often greater than 1, it turns out that the estimated distribution (i.e. S ND q ( d ) in Eq. 8) by the ND model is often more smooth than the one (i.e. S q ( d )inEq.8)bytheQLmodel.

The second option of f ( q ) can be an adjustable parameter  X  as follows: This option allows us have different remodeled distributions and a bigger  X  generally leads to a smoother remodeled distribution. The proposed Powers-based Risk Management (PRM) method (in Eq. 10) can be viewed to some extent as a micro adjustment (or called fine adjustment) for the estimated document relevance distribution. Generally, the applications of the proposed method are those tasks where the initial estimation of the document relevance is not the final decision. In this paper, the tasks we focus on are the pseudo-relevance feedback (PRF) and th e relevance feedback (RF), where the relevance estimation in the first-round retrieval can indicate feedback documents X  weights used in the second-round retrieval.

Relevance Model (RM) [5] is a typical language modeling approach for the second-round retrieval. For each query q , based on the given document set D ( |
D | = n ), the RM 1 is formulated as: number of terms with top probabilities in p ( w |  X  R ) will be selected to estimate the expanded query model, which is then used for the second-round retrieval. In the RM [5], the document prior p (  X  d ) is often assumed to be uniform. It turns out that the estimated document relevance distribution (i.e. S QL q ( d )inEq.5)bythe QL model plays an important role in the RM, since theoretically it distinguishes RM from a mixture of document language models (say d  X  D p ( w |  X  d )).
The Relevance Model was initially derived for the PRF task, where the doc-ument set D is set as the top n retrieval documents in the first-round retrieval. It can also be used in the RF task [1], by selecting all the truly relevant doc-uments (based on the relevance data available for the standard benchmarking collections) in top n documents as the document set D in Eq. 13.

For both PRF and RF tasks, the risk management method will remodel the distribution S QL q (see Eq. 5) obtained from the first round retrieval by the QL model. Our hypothesis is that the management of the rank-independent risk in the distribution S QL q could improve the retrieval performance of both tasks.
We would like to mention that other factors, e.g., the query-drifting after query expansion [2] or the combination coefficient for the feedback model [6], etc., also have a direct influence on the rank performance of the PRF and RF tasks. However, in this paper, we mainly focus on the usefulness of managing the rank-independent risk in estimating relevance probabilities of documents. 5.1 Evaluation Configuration Evaluation Data. The evaluation involves three standard TREC collections, including WSJ (87-92, 173,252 docs), AP (88-89, 164,597 docs) in TREC Disk 1 &amp; 2, and ROBUST 2004 (528,155 docs) in TREC Disk 4 &amp; 5. Both WSJ and AP data sets are tested on queries 151-200, while the ROBUST 2004 is tested on queries 601-700. The title field of the queries are used. Lemur[9] 4.7 is used for indexing and retrieval. All collectio ns are stemmed using the Porter stemmer and stop words are removed in the indexing process.
 Evaluation Set-up. The first-round retrieval is carried out by a baseline lan-guage modeling (LM) approach, i.e., the query-likelihood (QL) model [15,10] in Eq. 2. The smoothing method for the document language model is the Dirichlet prior [15] with  X  = 1000, which is a default setting in Lemur toolkit, and also a typical setting for query-likelihood model.

After the first-round retrieval, the top n ranked documents are selected as the pseudo-relevance feedback (PRF) d ocuments for the PRF task. The truly relevant documents in the PRF documents are selected as the relevance feedback (RF) documents for the RF task. We report the results with respect to n = 30. Nevertheless, we have similar observations on other n (e.g., 50, 70, 90). The Relevance Model (RM) in Eq. 13, is selected as the second baseline method, where the document prior is set as uniform. The number of expanded terms is fixed as 100. 1000 retrieved documents by the KL-divergence model are used for performance evaluation in both the first-round retrieval and second-round retrieval.

The Mean Average Precision (MAP), wh ich reflects the overall rank perfor-mance, is adopted as the primary evaluation metric. The Wilcoxon signed rank test is the measure of the statistical significance of the improvements over base-line methods.
 Evaluation Procedure. We aim to test the performance of different Powers-based Risk Management (PRM) algorithms (see Section 3.4). We denote these algorithms (corresponding to f ND ( q )and f  X  ( q )) as PRM ND and PRM  X  ,re-spectively. For both PRF and RF tasks, the risk management method will re-model the estimated document relevance distribution, i.e., the S QL q in Eq. 8. Then, the remodeled document relevance distribution will be input to the RM to construct the expanded query model for the second-round retrieval. Note that the PRM ND is to remodel the S QL q in the first-round retrieval. 5.2 Evaluation on Risk Management Method for PRF Task The experimental results for differen t PRM algorithms are summarized in Ta-ble 1. We can easily observe that RM significantly outperforms LM on every collection, which demonstrates its effect iveness for the second-round retrieval.
For PRM ND, we can observe that PRM ND can improve the RM on every collection, and the improvements are statistically significant on AP8889 and ROBUST2004 collections. This indicates that if we transform the negative KL-divergences to probabilities, these transformed probabilities can be used as the document weights in the RM, for which the document weights are usually from the query-likelihood model.

For PRM  X  , the results show that the PRM  X  with its best  X  can significantly improve RM. It is also necessary to test PRM  X   X  X  performance on different  X  . Recall that the bigger the  X  is, the smoother (i.e., with larger entropy) the remod-eled probabilities are. The pseudo-relevance feedback performance of PRM  X  are showninFig.1,fromwhichweca n generally conclude that when  X &gt; 1 (i.e. to smooth the original probabilities in S QL q ( d )inEq.5),PRM  X  can significantly improve RM. On the other hand, when  X &lt; 1 ( i.e., to force the original prob-abilities to be less smooth), the performance is always below that of the RM. Since the results usually reach a peak and then drop down, we can not say that the smoother estimated relevance probability distribution entails the better PRF performance. 5.3 Evaluation on Risk Management Method for RF Task This experiment is to test the performance of our PRM algorithms in the RF context. Note that the involved documents in the RF task are all relevant (i.e. with the same relevance status 1). Therefore, a more smooth relevance distribu-tion over documents would have less risk.

The results are summarized in Table 2. We can observe that PRM ND can significantly improve RM on all collections. In some cases, the significance level is 0.01. For PRM  X  , the results show that the PRM  X  with  X  = 4 can significantly (with significance level 0.01) improve RM on all collections. Concerning the in-fluence of different  X  on PRM  X   X  X  performance, Fig. 1 shows that the larger the  X  is, the less the rank-independent risk would be and hence the better performance can be achieved.
 This further indicates the importance of managing the rank-independent risk. In above experiments, we can observe that with the same ranking different esti-mated probabilities can have quite different impact on the next-round retrieval performance. It also shows that the remodeling algorithm is effective in reducing the rank-independent risk. In this paper, we propose to look at the rank-independent risk in estimating the probability of relevance. This paper aims to answer how to compare the estimation risks between two rank-equiva lent retrieval models, and how properly modeling of such risk can improve the retrieval performance.

Specifically, we first show that even though two language modeling approaches (i.e., QL and ND models) are rank-equivale nt, their estimated relevance distri-butions are different and the distribution of the ND model is more smooth than the one of the QL model. In addition, a risk management method, which is based on the powers-based remodeling idea motivated from the distribution dif-ference (see Eq. 8) of QL and ND models, is proposed to generally manage the rank-independent risk for a given retrieval model. We apply the proposed risk management method to the pseudo-relevan ce feedback and relevance feedback. Experimental results on several TREC co llections demonstrat e the effectiveness of the proposed method.

In the future, it would be very interesting to derive an optimization method to automatically obtain the optimal  X  of PRM  X  for different query. To this end, machine learning algorithms could be helpful . In addition, we would like to incorporate relevance judgements from users/assessors into the risk modeling and management process. For example, we can add some constraints designed by using relevance judgements or implicit feedback features that can indicate the document relevance. Moreover, in the ps eudo-relevance feedback task, we would consider how to implement the rank-independent risk management after the first-round retrieved documents are re-ranked. Furthermore, we are also interested to incorporate the proposed risk modeling into the score distribution calibration for the classical probabilistic models.
 Proof. For simplicity, in this proof, let S i and S j denote S q ( d i )and S q ( d j ), respectively. Without loss of generality, we assume that S i &gt;S j &gt; 0. Then, we have means that the right hand side of Eq. 14 is less than 1. Therefore, we have The proposition then follows. Acknowledgments. This research is funded in part by the UK X  X  EPSRC (EP/F014708/2), the China X  X  NSFC (61070044) and the EU X  X  Marie Curie Actions-IRSES (247590).

