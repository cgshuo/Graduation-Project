 Mining frequen t closed itemsets provides complete and non-redundan t results for frequen t pattern analysis. Extensiv e studies have prop osed various strategies for e X cien t frequen t closed itemset mining, such as depth- X rst searc h vs. breadth- X rst searc h, vertical formats vs. horizon tal formats, tree-structure vs. other data structures, top-do wn vs. bottom-up traversal, pseudo projection vs. physical projection of conditional database, etc. It is the right time to ask \ what are the pros and cons of the strategies? " and \ what and how can we pick and integrate the best strategies to achieve higher performanc e in gener al cases? "
In this study , we answ er the above questions by a system-atic study of the searc h strategies and develop a winning algorithm CLOSET +. CLOSET + integrates the advantages of the previously prop osed e X ectiv e strategies as well as some ones newly developed here. A thorough performance study on synthetic and real data sets has shown the advantages of the strategies and the impro vemen t of CLOSET + over ex-isting mining algorithms, including CLOSET , CHARM and OP , in terms of runtime, memory usage and scalabilit y. H.2.8 [ Database Managemen t ]: Database applications| Data Mining Mining metho ds and algorithms Frequen t closed itemsets, association rules  X 
The work was supp orted in part by U.S. National Sci-ence Foundation NSF IIS-02-09199, Univ ersity of Illinois, Microsoft Researc h, and IBM Facult y Award.
 ones newly developed here. A thorough performance study on synthetic and real data sets has shown the advantages of the strategies and the impro vemen t of CLOSET + over ex-isting mining algorithms, including CLOSET , CHARM and OP , in terms of runtime, memory usage and scalabilit y.
The remaining of the paper is organized as follows. In Sec-tion 2, we brie X  X  revisit the problem de X nition of frequen t closed itemset mining and the related work. In Section 3, we presen t an overview of the principal searc h strategies de-veloped before and analyze their pros and cons. In Section 4, we devise algorithm CLOSET + by integrating some win-ning strategies as well as some novel ones developed here. A thorough performance study of CLOSET + in compari-son with several recen tly developed e X cien t algorithms is reported in Section 5. We conclude this study in Section 6.
A transaction datab ase T DB is a set of transactions, where each transaction, denoted as a tuple h tid; X i , contains a set of items (i.e., X ) and is associated with a unique transac-tion identity tid . Let I = f i 1 ; i 2 ; : : : ; i n g be the complete set of distinct items appearing in T DB . An itemset Y is a non-empt y subset of I and is called an l -itemset if it con-tains l items. An itemset f x 1 ; : : : ; x l g is also denoted as x  X   X   X  x l . A transaction h tid; X i is said to contain itemset Y if Y  X  X . The number of transactions in TDB contain-ing itemset Y is called the support of itemset Y , denoted as sup ( Y ). Given a minim um supp ort threshold, min sup , an itemset Y is frequent if sup ( Y )  X  min sup .

Definition 1 (Frequent closed itemset). An item-set Y is a frequen t closed itemset if it is frequen t and there exists no prop er superset Y 0  X  Y such that sup ( Y 0 ) = sup ( Y ).

Example 1. The  X rst two columns in Table 1 show the transaction database TDB in our running example. Supp ose min sup = 2, we can  X nd and sort the list of frequen t items in supp ort descending order. The sorted item list is called f list . In this example f list = h f:4, c:4, a:3, b:3, m:3, p:3 i . The frequen t items in each transaction are sorted according to f list and shown in the third column of Table 1. Itemset f c is a frequen t 2-itemset with supp ort 3, but it is not closed, because it has a superset f cam whose supp ort is also 3. f acm is a frequen t closed itemset.
 Related work Popular algorithms for mining frequen t closed itemsets include A-close [12], CLOSET [14], MAFIA [5] and CHARM [18]. A-close uses a breadth- X rst searc h to  X nd the frequen t closed patterns. In dense datasets or datasets with long patterns, breadth- X rst searc hes may en-coun ter di X culties since there could be many candidates and length-( k + 1) candidates. Due to its too many database scans, it is not suitable for mining long patterns. In con-trast, a depth- X rst searc h metho d traverses the lattice in depth- X rst order, and the subtree of an itemset is searc hed only if the itemset is frequen t. Moreo ver, when the item-sets becomes longer, depth- X rst searc h shrinks searc h space quickly. As a result, the depth- X rst searc h metho d is usually a winner for mining long patterns. Some previous studies (e.g., [14, 5, 18]) clearly elaborate that the depth- X rst searc h metho ds are usually more e X cien t than the breadth- X rst searc h metho ds like A-close .
 Horizon tal vs. vertical data formats. The transaction information can be recorded in two alternativ e formats. The horizon tal format is an intuitiv e bookkeeping of the transac-tions. Each transaction is recorded as a list of items. In the vertical format, instead of recording the transactions explic-itly, a tid-list is kept for every item, where the identities of the transactions containing the item are listed. A-close and CLOSET use the horizon tal data format, while CHARM and MAFIA use the vertical one.

A vertical format-based metho d needs to main tain a tid-set for each frequen t itemset. When the database is big, each tidset is on average big, and many such intermediate results will consume a lot of memory . In contrast, if we prop-erly choose a compressed structure like FP-tree , a horizon tal format-based metho d will not cause too much space usage, because the itemsets can share common path if they share common pre X x, and each of their tidlists is represen ted by a coun t. Moreo ver, for a vertical format-based algorithm, one intersection operation can only  X nd one frequen t item-set. For a horizon tal format-based metho d like CLOSET , one scan of a projected database can  X nd many local fre-quen t items which can be used to grow the pre X x itemset to generate frequen t itemsets. In this paper, we will com-pare CLOSET+, a horizon tal format-based algorithm, with CHARM , a vertical format-based one, in terms of scalabilit y and e X ciency in both runtime and space usage.
 Data compression techniques. A transaction database is usually huge. If a database can be compressed and only the information related to the mining is kept, the mining can be e X cien t. Recen tly, some data compression metho ds have been devised. FP-tree and Di X set are two typical examples.
An FP-tree [8] of a transaction database is a pre X x tree of the lists of frequen t items in the transactions. The idea can be illustrated in the following example.

Example 2. The FP-tree of our running example is con-structed as follows: Scan the database once to  X nd the set of frequen t items and sort them in the supp ort descending order to get the f list (see Example 1). To insert a transac-tion into the FP-tree , infrequen t items are remo ved and the remaining items in the transaction are sorted according to the item ordering in f list , i.e., the least frequen t item at the leaf, and the items with higher global supp ort at a higher level in the FP-tree . Fig. 1(b) shows the FP-tree .
The FP-tree structure has several advantages in mining frequen t itemsets. First, FP-tree often has a high compres-sion ratio in represen ting the dataset because (1) infrequen t items identi X ed in the  X rst database scan will not be used in the tree construction, and (2) a set of transactions shar-ing the same subset of items may share common pre X x paths from the root in an FP-tree . According to our experience, for
In this section, we devise a new frequen t closed itemset mining algorithm, CLOSET +, by integrating some winning searc h strategies and developing some novel ones.
CLOSET + follows the popular divide-and-c onquer paradigm which has been shown one possible instance in Example 4 and the depth- X rst search strategy which has been veri X ed a winner for mining long patterns by several e X cien t fre-quen t pattern mining algorithms. It uses FP-tree as the compression technique. A depth- X rst searc h and horizon tal format-based metho d like CLOSET + will compute the local frequen t items of a certain pre X x by building and scanning its projected database. Therefor, a hybrid tree-projection metho d will be introduced to impro ve the space e X ciency .
Unlik e frequen t itemset mining, during the closed itemset mining process there may exist some pre X x itemsets that are unpromising to be used to grow closed itemsets. We should detect and remo ve such unpromising pre X x itemsets as quickly as possible. Besides adopting the above men-tioned item merging and sub-itemset pruning metho ds, we also prop osed the item skipping technique to further prune searc h space and speed up mining.

Previous algorithms need to main tain all the frequen t closed itemsets mined so far in memory in order to check if a newly found closed itemset candidate is really closed. If there exist a lot of frequen t closed patterns, such kind of closure checking will be costly in both memory usage and runtime. We have also designed an e X cien t subset-che cking scheme: the combination of the 2-lev el hash-indexed result tree based metho d and the pseudo-pr ojection based upwar d checking metho d, which can be used to save memory us-age and accelerate the closure-c hecking signi X can tly. In the following, the above mentioned mining techniques and the CLOSET + algorithm are developed step by step.
Most previously developed FP-tree -based metho ds, such as FP-gro wth and CLOSET , grow patterns by projection of conditional databases in a bottom-up manner [8, 14]. How-ever, such a searc h order may not always lead to the best per-formance for di X eren t kinds of data sets. In CLOSET +, a hy-brid tree-projection metho d is developed, which builds con-ditional projected databases in two ways: bottom-up phys-ical tree-projection for dense datasets and top-down pseudo tree-projection for sparse datasets .
For dense datasets, their FP-tree s can be hundreds (or even thousands) times smaller than their corresp onding orig-inal datasets due to compression. Its conditional projected FP-tree s are usually very compact as well. Each projected FP-tree is much smaller than the original FP-tree , and min-ing on such a compact structure will also be e X cien t. As a result, for dense datasets CLOSET + still physically builds projected FP-tree s and it is done recursiv ely in a bottom-up manner (i.e., in supp ort ascending order).

To assist the physical FP-tree projection, there is a header table for each FP-tree , which records each item's ID, coun t, and a side-link pointer that links all the nodes with the same itemID as the labels. The global FP-tree in our exam-compact and does not shrink quickly. Instead of physically building projected FP-trees , a new metho d is developed for sparse datasets: top-down pseudo projection of FP-tree . Un-like bottom-up physical projection of FP-trees , the pseudo projection is done in the f list order (i.e., in supp ort de-scending order).

Similar to bottom-up physical projection, a header table is also used to record enough information such as local fre-quen t items, their coun ts and side-link pointers to FP-tree nodes in order to locate the subtrees for a certain pre X x item-set. Based on our running example, the top-do wn pseudo-projection metho d is illustrated as follows. Fig. 3(a) shows the global FP-tree and the status of global header table. Ini-tially only the child nodes (e.g., nodes f :4 and c :1) directly under the root node are linked from the global header table. Because we build FP-tree according to the f list order, all the projected transactions containing item f can be found from the subtree under the node with label f :4 (i.e., the dashed polygon in Fig. 3(a)).

By following the side-link pointer of item f in the global header table of Fig. 3(a), we can locate the subtree under node f :4. The local frequen t items can be found by scanning this subtree and used to build the header table for pre X x itemset f :4, as shown in Fig. 4(a). Here only the child nodes directly under node f :4 are linked from the header table H f :4 . Based on the header table H f :4 , we can mine the frequen t itemsets with pre X x f . (1) First, we'll mine closed itemsets containing f c :3. By scanning the subtree under node c :3 in Fig. 4(a), we can  X nd the local frequen t items and build the header table for pre X x itemset f c :3, as shown in Fig. 4(b). Items a and m have the same supp ort as that of pre X x f c :3, according to the item merging technique, they can be merged with f c :3 to form a new pre X x f cam :3 (and it is also a closed itemset) and we will not need to mine closed itemsets with pre X x f ca :3 or f cm :3. Although f ca :3 cannot be used as a pre X x to grow closed itemsets, we still need to follow item a 's side-link pointer to  X nd all the child nodes directly under node a :3 and make them linked from header table H fc :3 . Because item b is infrequen t in H fc :3 , one can safely prune it from the header tables at higher lev-els.
 Pro of . Assume item x at level l is a local frequent item of pre X x itemset X l and has the same support in level k 's header table of pre X x itemset X k , where (0  X  k &lt; l ) ^ ( X k  X  X ) . Let X 0 k = X k [ x and X 0 l = X l [ x , it is obvious that k  X  X 0 l ) ^ ( sup ( X 0 k ) = sup ( X 0 l )) , which means any frequent itemset grown from X 0 k can be subsume d by a corresponding frequent itemset grown from X 0 l . As a result it is non-close d and item x can be prune d from header table at level k .
This pruning metho d can be used in both bottom-up phys-ical and top-do wn pseudo tree-pro jection paradigms. For example, in Fig. 2(c), there is a local frequen t item a with supp ort 3 for pre X x itemset m :3. We also  X nd that item a appears in the global header table (see Fig. 2(a)) with the same supp ort, item a can be safely pruned from the global header table. Similarly , from Fig. 4, we know that items a , m , and p in H fc :3 have the same supp orts as those in H f :4 , these items can be safely remo ved from H f :4 .
The searc h space pruning metho ds can only be used to remo ve some pre X x itemsets that are unpromising to be used as a start point to grow closed itemsets, but they can-not assure that a frequen t pre X x itemset is closed. When we get a new frequen t pre X x itemset, we need to do two kinds of closure checking: the superset-che cking checks if this new frequen t itemset is a superset of some already found closed itemset candidates with the same supp ort, while the subset-che cking checks if the newly found itemset is a sub-set of an already found closed itemset candidate with the same supp ort. Because both bottom-up physical projection and top-do wn pseudo projection work under the divide-and-conquer and depth- X rst-se arch framew ork, the following the-orem states that CLOSET + only needs to do subset-c hecking in order to assure a newly found itemset is closed.
Theorem 4.1. ( Subset checking ) Under the framework of divide-and-c onquer and using the item merging pruning metho d introduced in Lemma 3.1, a frequent itemset found by CLOSET + must be closed if it cannot be subsume d by any other already found frequent closed itemset.
 Pro of . Let the list of items in which order CLOSET + mines frequent closed itemset be m list = h I 1 , I 2 , . . . , I n i and the current frequent itemset CLOSET + has just found is S c = I c 1 I c 2 : : : I cx . Also, we de X ne the relationship between two items I m and I n as I m &lt; I n if item I m is located before item I n in m list . For any two itemsets S 1 = I 11 I 12 : : : I 1 i and S 2 = I 21 I 22 : : : I 2 j where i &gt; 0 and j &gt; 0 , we de X ne S 1 &lt; S 2 if there exists an integer k ( k  X  1) , I 1 k &lt; I 2 k and I 1 l = I 2 l (for all l &lt; k ) hold. Following we will prove itemset I c 1 I c 2 : : : I cx cannot be subsume d by a later found frequent closed itemset, which also means I c 1 I c 2 : : : I cx can-not subsume any other already found closed itemsets.
First, for any frequent itemset S l = I l 1 I l 2 : : : I ly which is mine d later than S c , we can classify it into one of the two categories: (1) S l is gener ated by growing S c ; (2) S l is not gener ated by growing pre X x S c . If S l belongs to the  X rst cat-egory, S l is a superset of S c , but because we have applie d the item merging technique, which means all the local items with the same support as S c 's must have been include d in S c , sup ( S l ) must be smaller than sup ( S c ) . As a consequenc e, S c
At the status shown in Fig. 5, we may later get a frequen t itemset ca :3. By using itemID a and supp ort 3 as hash keys and following the corresp onding hash link, we will  X nd the node labeled as a :3,3 with a length greater than 2, we then check if ca :3 can be absorb ed by the path from node a :3,3 to the root. Unfortunately it cannot pass the checking and will not be inserted into the result tree .
 Pseudo-pro jection based upward checking. Although the result tree can compress the set of closed itemsets a lot, it still consumes much memory and is not very space-e X cien t for sparse datasets. Can we totally remo ve the re-quiremen t of main taining the set of closed itemsets in mem-ory for subset-c hecking? As we know, the global FP-tree contains the complete information about the whole set of frequen t closed itemsets, thus we can use the global FP-tree to check if a newly found frequen t itemset is closed. In such a way, we do not need any additional memory for storing the set of already mined closed itemsets and once a newly found itemset has passed the checking, it will be directly stored in an output  X le, F.

The problem becomes how to do subset-che cking based on the global FP-tree. As we know, in the top-do wn pseudo projection metho d, all the tree nodes and their corresp ond-ing pre X x path s w.r.t. a pre X x itemset, X , can be traced by following its side-link pointer recorded in its header ta-ble. We can use the following lemma 4.2 to judge whether a newly found frequen t itemset is closed.

Lemma 4.2. For a certain pre X x itemset, X , as long as we can  X nd any item which (1) appears in each pre X x path w.r.t. pre X x itemset, X , and (2)do es not belong to X , any itemset with pre X x X will be non-close d, otherwise, if there's no such item, the union of X and the complete set of its local frequent items which have the same support as sup ( X ) will form a closed itemset.
 Pro of . The  X rst part is easy to prove: If we can  X nd an item, i x , which (1)app ears in each pre X x path w.r.t. pre X x itemset, X , and (2) does not belong to X , this will mean ( X  X  ( X [ f i x g )) and ( sup ( X ) = sup ( X [ f i x g )) hold, as a result, X or any itemset with pre X x X will be non-closed.
For the second part, because we cannot  X nd any item which (1)app ears in each pre X x path w.r.t. pre X x itemset, X , and (2) does not belong to X , and any other possible items that always appear together with X can only belong to the set of X 's local frequen t items, as a result, the union of X and the complete set of its local frequen t items which have the same supp ort as sup ( X ) must form a closed itemset.
Here we'll use some examples to illustrate the upwar d subset-che cking . Assume the pre X x X = c :4, we can locate nodes c :1 and c :3 by following the side-link pointer of item c in Fig 3(b) and  X nd there is only one item, f , which appears in pre X x itemset c :4's pre X x paths to the root, and it only co-occurs 3 times with pre X x c :4. In addition, there's no local frequen t item of pre X x c :4 with supp ort 4, thus c :4 is closed and will be stored in output  X le F. Using this metho d, we can easily  X gure out that pre X x am :3 is not closed, be-cause in the pre X x paths of nodes m :2 and m :1, there are two other items, f and c , which appear with am 3 times.
By integration of the techniques discussed above, we de-rive the CLOSET + algorithm as follows.
 T10I4DxP1k 200k-1400k 978 10(31) T10I4D100kPx 100k 4333-29169 10(31) have been used in the previous performance studies [18, 19]. Two other datasets, retail-chain and big-market are di X eren t retail transaction datasets.

Synthetic datasets: The synthetic datasets were generated from IBM dataset generator, with an average transaction length 10 and average frequen t itemset length 4. To test the scalabilit y against base size, we generated dataset series T10I4DxP1k by varying the number of transactions from 200K to 1400K and  X xing the number of unique items at 1k. To test scalabilit y in terms of number of unique items, we generated dataset series T10I4D100kPx by  X xing number of transactions at 100k and setting the number of distinct items at 4333, 13845, 24550, 29169 (generated by setting nitems parameter at 5k, 25k, 125k, 625k respectiv ely). retail-c hain 2788(0.025), 5147(0.015), 17399(0.005) big-mark et 14881(0.005), 24478(0.003), 92251(0.001) T10I4D100k 46993(0.05), 71265(0.03), 283397(0.01) Table 3: Num ber of frequen t closed itemsets vs. rel-ative supp ort threshold. Fig. 6. Runtime ( mushr oom ). Comparison with OP . Our experimen ts show that due to generating a huge number of frequen t itemsets, even the best frequen t itemset mining algorithm like OP cannot comp ete with CLOSET +. Fig. 6 and Fig. 7 show the experimen tal results for mushr oom and gazel le datasets.
 Comparison with CHARM and CLOSET . We used all the 6 real datasets to test CLOSET +'s performance and memory usage in comparison with two other closed itemset mining al-gorithms: CHARM and CLOSET . For dense dataset conne ct , Fig. 8 and Fig. 9 show the results. Fig. 8 shows CLOSET + can be orders of magnitude faster than CLOSET . When the supp ort is not too low (i.e., higher than 20%), CLOSET + is several times faster than CHARM . When supp ort thresh-old is further lowered, they will have similar performance, but at supp ort 10%, CHARM cannot run by reporting an error 0 REALLOC: Not enough core 0 . From Fig. 9 we know overall CLOSET + uses less memory than CHARM . For ex-ample, at supp ort 85%, CLOSET + consumes about 1MB while CHARM consumes about 15MB.

Pumsb* is another dense dataset. Fig. 10 and Fig. 11 depict the results. Both CLOSET + and CHARM have sig-ni X can tly better performance than CLOSET and once the supp ort is lower than 20%, CLOSET just cannot  X nish run-ning. Overall CLOSET + and CHARM have very similar per-formance when the supp ort threshold is not too low. At low supp ort threshold like 15%, CHARM will outp erform CLOSET +. Fig. 11 shows that CLOSET + uses much less memory than CHARM .

Fig. 12 and Fig. 13 demonstrate the results for mushr oom dataset. We can see CLOSET can be orders of magnitude slower than CLOSET + and CHARM , and CLOSET even can-not  X nish running once the supp ort is less than 0.1%. But there is no clear winner between CLOSET + and CHARM : At high supp ort threshold, CLOSET + is several times faster than CHARM ; and at very low supp ort threshold, CHARM is a little better than CLOSET +. But CLOSET + always beats CHARM in terms of memory usage.
Fig. 14 and Fig. 15 presen t the evaluation results for sparse dataset gazel le . Fig. 14 shows that CLOSET + and CHARM are faster than CLOSET . At high supp ort CLOSET + and CHARM have similar performance, and at a little lower supp ort, CHARM is several times faster than CLOSET +, but once we continued lowering the supp ort threshold to 0.005%, CHARM could not run by reporting an error 0 REALLOC: Not enough core 0 . From Fig. 15, we see that CHARM con-sumes about two orders of magnitude more memory than CLOSET + at low supp ort.

Fig. 16 and Fig. 17 demonstrate the results for retail-chain dataset. CLOSET + runs the fastest among the three algo-rithms and uses less memory than CHARM : When supp ort threshold is set to 0.005%, CLOSET + runs almost 5 times faster than CHARM , but uses only 1/9 of the memory that the slope ratio for CHARM is much higher than that for CLOSET +.

We also tested the scalabilit y of CLOSET + in terms of number of distinct items using T10I4D100KPx series with number of distinct items set at 4333, 13845, 24550 and 29169, respectiv ely, and minim um supp ort set at 0.005%. From Fig. 21, we can see that initially these three algo-rithms have very similar performance when the number of distinct items is small, but once the number of distinct items increases, the runtime of CHARM and CLOSET will have a much bigger jump than CLOSET +, which means CLOSET + also has better scalabilit y than both CHARM and CLOSET in terms of the number of distinct items.
 The above experimen tal results show that: (1) Although CHARM adopts the Di X set technique which can reduce space usage signi X can tly [18], it still consumes more memory than CLOSET +, and in some cases it can use over an order of magnitude more memory than CLOSET +. (2) Due to the new techniques developed here, such as the hybrid tree-projection mining strategy , the item-skipping pruning metho d, and the subset-c hecking techniques(i.e., the two-lev el hash-indexed result-tr ee and pseudo-pro jection based upwar d checking ), CLOSET + can be orders of magnitude faster than CLOSET , and is very e X cien t with low supp ort even in the case CLOSET and CHARM cannot run. (3) CLOSET + has linear scalabil-ity and is more scalable than CHARM and CLOSET in terms of both base size and the number of distinct items.
Frequen t pattern mining has been studied extensiv ely in data mining researc h. In this study , we have re-examined some previously prop osed metho dologies, and mainly fo-cused on the new techniques developed for CLOSET +, a highly scalable and both runtime and space e X cien t algo-rithm for dense and sparse datasets, on di X eren t data dis-tributions and supp ort thresholds.

The thorough performance evaluation in this study reveals that: (1) For mining frequen t patterns, one should work on mining closed patterns instead of all patterns because the former has the same expressiv e power as the latter but leads to more compact and meaningful results and likely better e X ciency . (2) There is a popular myth: Algorithms based on the vertical-format are better than those based on the horizon tal-format. Our study shows that an algorithm based on the vertical format, due to its necessit y to identify tids (even using the Di X set compression technique) will likely take more memory than an FP-tree -based algorithm and is less scalable if the latter is implemen ted nicely . (3) Mul-tiple, integrated optimization techniques for database pro-jection, searc h space pruning, and pattern closure-c hecking are needed for high performance pattern mining. Often, di X eren t data characteristics may require di X eren t mining metho dologies, e.g., in CLOSET+, we use the top-down pseudo tree-projection and upwar d subset-che cking for sparse datasets, whereas for dense datasets, the bottom-up physi-cal tree-projection and a compressed result-tr ee have been adopted.

Curren tly CLOSET + has been successfully emplo yed to mine non-redundan t association rules. In the future, we will explore more applications, including association-based classi X cation, clustering, and dependency/link age analysis in large databases.
