 Evaluation in Information Retrieval (IR) has long focused on effectiveness and efficiency. However, new and emerging access tasks now demand alternative evaluation measures which go beyond this traditional view. A retrieval system provides a means of gaining access to documents, therefore intuitively, our view of the collection is shaped by the re-trieval system. In this paper, we outline some emerging information access related sce narios that require knowledge about how the retrieval system affects the users X  ability to access information. This provides the motivation for the proposed evaluation measures and methodology where the focus is on capturing the behavior of the system, in terms of how retrievable it makes individual documents within the collection. To demonstrate the utility of the proposed meth-ods, we perform an extensive analysis on two TREC collec-tions showing how the measures can be applied to evalu-ate different information access questions. For higher order information access tasks that are inherently dependent on retrievability, our novel evaluation methodology emphasizes that effectiveness is an insufficient characterization of a re-trieval system. This paper provides the foundations for the evaluation of higher order access related tasks.
 H.3.3 [ Information Search and Retrieval ]: Retrieval Models Measurement, Performance, Experimentation
With search playing an increasingly crucial role in the ac-cess to information, there are growing concerns over the role of this technology [8, 11]. This is because as larger amounts of information is being made available online, Information Retrieval (IR) systems, as exemplified by the many search engines available today, are becoming the primary means of accessing this information [10]. Consequently, issues are being raised from a number of different areas questioning the influence that IR systems have on the access to infor-mation. For example, media regulators are concerned over whether search engines are biased towards particular web-sites over others [16, 9], while e-Government administrators in the U.S. are now legally required to ensure that all gov-ernment information online is accessible through search en-gines [11]. Legal and patent searchers also need to ensure that they have IR systems which enable them to find all doc-uments in the collection relevant to their information need. These situations require a way to evaluate an IR system in terms of how much access they provide into the underlying collection. To address these types of questions, we must de-velop new measures that indicate how easily documents or sets of documents in the collection can be accessed given the IR system. These measures can then be used to com-pare the influence of different IR systems on the access to information.

Given these motivations, it is the purpose of this paper to develop suitable methods to quantify the influence that a retrieval system has on the access to information. This influ-ence of an IR system, exerted either explicitly or implicitly, is evoked at two junctions: deciding which documents are used as input into the index of the system, and when pro-ducing a ranked list of documents as output in response to a query. The ability to find a document through the retrieval process is therefore a combination of whether or not the doc-ument is indexed, and then whether or not a document can be retrieved through querying. The first factor has already been well studied [14, 6], illustrating the importance of mak-ing content discoverable by search engine crawlers to ensure inclusion in the index. The second factor is more subtle and is less understood. This is because it is generally assumed that if a document is indexed, it can be retrieved; the only impeding factors being: However, the combination of these factors means that some documents are more easily accessible than others. This is because, search is a process of discrimination: a user de-ciding how to pose the query, the IR system attempting to discriminate between relevant and non-relevant content, and the user wading through portions of the results retrieved by the system trying to find content relevant to his/her need. The main objective of the retrieval system is to maximize effectiveness, which implicitly leads to favoring certain types of documents over others; and so it can be said that the sys-tem is inherently biased 1 . Applying or removing particular retrieval biases have been shown to significantly improve or degrade effectiveness. For example, using inlinks to favor more popular documents [5], or accounting for document length normalization [13] can lead to significant improve-ments to retrieval effectiveness. The dual nature of discrim-ination, e.g.  X  X avor longer over shorter X , means that some documents can become more retrievable at the expense of others. The interest of this paper lies in understanding what influence the pursuit of increased effectiveness, and the as-sociated accumulation of biases, has on all documents in the collection, not just the set of relevant documents.
The contribution in this paper is two-fold. Firstly, we propose a retrieval-oriented methodology for evaluating the influence of an IR system on information access. Secondly, we provide a quantitative measure of a document X  X  retriev-ability and demonstrate how it can be used in the evalua-tion of a number of different information access scenarios. A document X  X  retrievability captures the ease with which the document can be retrieved given a particular IR sys-tem, and enables for instance, a way to assess the extent to which information within e-Government websites is accessi-ble. Examples to this effect are shown in Section 4, where we perform an extensive analysis of several standard retrieval models on two different TREC collections, the AQUAINT (News) collection and the .GOV (Web) collection. We first calibrate and test the proposed measure of retrievability and verify that the measure is working as expected, before con-ducting an analysis on how the retrievability imposed by the standard IR models affects the access into the documents indexed by the system. Then, we conduct a final series of experiments where we relate the influence a retrieval sys-tem exerts on the access to the collection and the retrieval effectiveness it achieves. Our main findings are: Finally, in Section 5, we conclude that retrievability provides a useful indicator of the interaction between the collection and the retrieval system, before providing an outline of di-rections for future work.
In this paper, the term  X  bias  X  is used according to the defini-tion in [9], to refer to situations when a group of documents is favored or preferred over others.
Information Retrieval is the area that deals with the stor-age, organization and access of information [15]. The pur-pose of an IR system is to deliver relevant content to the user X  X  request and it should do this effectively and efficiently. There is an extensive amount of literature that aims to ad-dress these facets of IR. Largely, research is concerned with effectiveness. That is, retrieving relevant information in re-sponse to a query. The necessary pre-cursor to relevance is therefore retrieval, i.e., an indexed document must be re-trieved, before it can be judged relevant or not[3]. This condition determines whether or not a document can be ac-cessed through the system, and how easily the document can be accessed.

First of all, the document must be present in the index of the IR system; otherwise there is no possibility for the document to ever be presented in a ranked list of results. Providing effective responses to queries therefore relies on possessing the right content in the collection. In the context of web search, it is vitally important for web-site owners to increase the visibility of their web pages to search engine crawlers in order to increase the likelihood of their pages being indexed by the system. Dasgupta et al [6] refer to this as the discoverability of pages on the web, and Upstill et al [14] refer to this as the crawlability . In the current paper, we assume that the documents of concern have been indexed, and turn our attention to the second junction of retrieval: the ranking function.

An IR system is evaluated in terms of being able to iden-tify those documents in the collection that match the user X  X  request. The inability of a retrieval system to do so leads to analysis of the result sets so that the ranking algorithm can be subsequently improved. For example, in [13] a bias towards length is corrected, and in [10] a method to remove bias due to popularity is suggested. Length and (link-based) popularity are just two features of a document that might in-fluence its position in ranked results produced by a retrieval function. Consistently favoring the retrieval of documents based on such features will invariably lead to a persistent retrieval bias, where documents with such features are in general more likely to be retrieved.

Because of this, a number of studies have been conducted which attempt to determine whether search engines are bi-ased. Such studies have considered a range of possible bi-ases, for example, if one site has more coverage than an-other [8], whether sites in particular geographical locations are favored [16], or whether search engines are biased given a particular topic [9].

However, the studies performed have used crude measures based on coverage to determine the existance of bias, and the later studies were performed using only a handful of samples. In this paper, we propose a robust measure for quantifying the level of access afforded to individual docu-ments, which can be used to determine whether there is any relative retrieval bias. It should be pointed out that search is a process of discrimination; and as such, retrieval systems will be naturally biased in some way; so simply detecting bias is not enough. So as part of this work, we examine whether the retrieval bias actually impedes one X  X  access to information in the collection, and if so, to what extent. This has not been considered in prior work where the assumption has been that  X  X ias is bad X . This latter stance might be justi-fied in certain situations. For instance, perceived bias in web search rankings has led to legal action being taken against a well known search engine company 2 . While, in the area of e-Government, ensuring that online content is accessible is a very important concern, because citizens of a demo-cratic country have a right to access the information. If the information is hidden from the public then this could jeop-ardize the integrity of the government. The importance of e-Government content being made accessible through search technology was highlighted in a recent report 3 . This resulted in changes to U.S. legislation 4 , which requires that govern-ment websites be monitored and assessed in terms of how  X  X earchable X  they are, to ensure that all government infor-mation is accessible through search engines.

Our main objective is to more precisely understand how the retrieval system affects one X  X  ability to access the con-tent housed within the index/collection. In order to do so, we introduce a measure of retrievability in the following sec-tion. This measure will enable the evaluation of a number of different higher order information access tasks, such as search engine bias and e-Governement accessibility.
Given a collection D , an IR System accepts a user query q and returns a ranking of documents R q which are deemed to be relevant to q from within D .Wecanconsiderthe retrievability of a document as a system dependant factor that measures how likely the document is to be returned to the user, with respect to the collection D and the ranking function used by the system. Consider Q , the universe of all possible queries. Each q  X  Q is associated with a weight o which indicates how likely it is that a user will issue this query to the IR system. Such a weight can be used to cap-ture query popularity, for example, to associate q = X  X elebrity gossip X  with a higher weight than q = X  X nformation retrieval X .
Intuitively, the retrievability of a document d should be high if: 1. there are many queries in Q which can be expressed in 2. when retrieved, the rank k dq of the document d is as
Thus we formulate the following measure of the retriev-ability of d : f ( k dq ,c ) is a generalized utility/cost function where k the rank of d in the result for q ,and c denotes the maximum rank that a user is willing to proceed down the ranked list. The function f ( k dq ,c ) returns a value of 1 if k dq  X  0 otherwise. Note that different functions could be used to reflect the likelihood of a user examining documents at a see http://www.searchenginewatch.com
Hiding in Plain Sight: Why Important Government Infor-mation Cannot be Found Through Commercial Search En-gines, Center for Democracy and Technology, http://www. ombwatch.org/info/searchability.pdf U.S. Legislation: E-Government Act 2002, and the e-Government Reauthorization Act 2007 particular rank k (e.g. rank dependant top-heavy functions for web-search) 5 .

Defined in this way, the retrievability of a document is essentially a cumulative score that is proportional to the number of times the document can be retrieved within that cutoff c over the set Q . A document that is never returned in the top-c results for any query will have an r ( d ) value of zero. While, normalizing the r ( d ) values would then pro-vide an indication of the likelihood of a document being retrieved by the system. However, even though we have a measure that satisfies our intuitions, we require a method for approximating the retrievability of a document in an op-erational setting.
Clearly, it is impractical to calculate the absolute r ( d ) scores because the set Q would be extremely large and re-quire a significant amount of computation time as each query would have to be issued against the index for a given re-trieval system. So, in order to perform the measurements in a practical way, we need to obtain a reasonable approx-imation of the relative document retrievability and arrive at some estimate of retrievabilty  X  r ( d ). Our approach in this paper is to use a subset of all possible queries that is sufficiently large and which contains relatively probable or possible queries. For instance, we could use a historical set of queries that have been received by the system in the past, i.e., a query log. Alternatively, we could adopt a simulation based methodology by using an approach like Query Based Sampling [4]. The latter is the approach we take here. Since, we are using a subset of queries, it is worth noting that the estimate of  X  r ( d ) provides a relative measurement of the re-trievability of a document. This enables the ready compari-son between two different retrieval systems, when the same set of queries has been used to draw the samples which are used in the estimation. For instance, for a given subset of queries, if the retrievability of document d under system A, is r A ( d ) = 40, and for system B, it is r B ( d ) = 10, then system A makes d four times more retrievable than system B.
In this section, we first outline the experimental setup, be-fore calibrating the measureme nts taken. We then introduce a global measure of retrievability bias that provides a single measure to quantify the inequality between documents in the collection. Following this, an analysis is performed in a controlled environment on two standard IR test collections in order to simulate a number of different possible informa-tion access evaluations. We consider the following different scenarios:
For an alternative derivation of the retrievability measure based on the concept of  X  X ccessibility X  from Transportation Planning we refer the reader to [2], which also describes other more sophisticated utility/cost functions. These different scenarios follow from our initial motivations for considering this work, as well as being diverse enough to help identify other areas of application. Our analysis reveals that the retrieval systems evaluated exhibit different retrieval biases. To determine whether such retrieval biases impede or restrict one X  X  ability to access the documents in an adverse manner, we conduct a further analysis. In these follow-up experiments, we examine the relationship between effectiveness and retrievability. Datasets We used two different TREC Collections: (1) The .GOV dataset is a collection of just over 1.2 million web documents that consists of a crawl over 643 sub-domains of the U.S. Government website. The collection comes with a set of 225 topics from the Web Track 2004, which we shall use during the analysis. We also use the associated links file, which contains a list of all links between documents in the domain; (2) The AQUAINT dataset is a collection of just over 1 million newswire articles from three different news providers (APW, NYT and XIE). We also use the fifty TREC topics from the ROBUST Track  X 05.

Retrieval Models For the purposes of this paper, we used three standard IR models to ensure that we evalu-ate the measure using known functions. This is so we can check that the measure is performing as expected, reflect-ing known behavior of these algorithms. The models we AQUAINT TFIDF LM1000 BM25 BM25i Table 1: Performance of each algorithm on AQUAINT and .GOV: Mean Average Precision (MAP), Mean Reciprocal Rank (MRR), Precision at 5 and 10 documents (P@5, P@20) with respect to their query sets employed were TFIDF, a Language Modelling approach us-ing Bayes Smoothing with  X  = 1000 denoted by LM1000, and the OKAPI retrieval function (BM25). On the .GOV dataset, we also used BM25 combined with an inlink prior using the method and parameters suggested in [5], which we shall denote as BM25i. By design, BM25i favors documents with more inlinks, while TFIDF is known to be biased to-wards longer documents. LM1000 and BM25 are provided to contrast these methods and show the differences between systems more clearly.

System Effectiveness Table 1 provides an overview of the effectiveness of each retrieval model over a number of performance metrics with their respective TREC topics. Sig-nificance testing 6 of the MAP values reveals that LM1000 is significantly better than TFIDF and BM25 on AQUAINT. On .GOV, BM25i is significantly better than the other re-trieval systems, and BM25 and LM1000 were both signifi-cantly better than TFIDF. Notably the difference in perfor-mance on AQUAINT between the systems is small, while the difference on .GOV is quite large. It appears that on the AQUAINT collection, any of the three retrieval meth-ods considered would provide equivalent performance. The following sections show that despite similar effectiveness, the
Conducted using a paired t-test, p&lt; 0 . 05 [12] three system vary in terms of what areas of the collection they favor, and to what extent.

All experiments reported were conducted using the LEMUR toolkit 7 , which was used to index both the collections, with Porter stemming and the removal of stopwords.
In order to estimate the r ( d ) values for each document, a number of approximation choices need to be made. For this initial set of experiments, we would like to ensure that the choice of the parameters is appropriate and that our prior knowledge of known retrieval biases in the chosen set of retrieval systems are brought out. For the experiments re-ported here, we assume the simple binary function for f ( . ) (indicating just presence or absence in result sets of size c ) and set the weight of each query to a constant (we use o = 1). The latter assumption indicates that we wish that each q  X  Q contributes equally to the retrievability score r ( d )ofall d , situations where the query distributions are skewed (e.g. head and tail queries for web-search) might be better handled by alternative choices of o q . While this is a simple configuration, it provides a very intuitive retrievabil-ity measurement, from which to obtain a good idea about how the measure behaves and what it means.

The next choice is the set of queries to be used. For each collection (i.e., .GOV and AQUAINT), we created the ref-erence set of queries Q by extending, and exploiting, the idea behind Query Based Sampling [4]. The idea was to extract a sufficiently large sample of documents on which to base our estimate of retrievability by issuing millions of queries to probe the collection via the IR system. To do so, a reference set of queries for each collection was created con-taining queries of one or two terms. The single term queries were constructed by taking each term in the vocabulary that occurred 5 times or more and posing the term as a query. The bi-term queries were constructed by taking each bigram in the collection (i.e., every pair of consecutively occurring terms) that occurred at least 20 times, and ranking them by number of occurrences before truncating the list at 2 million. Each bigram in the set was then posed as a query. Table 2 provides details of the two datasets as well as the query sets used to obtain a reasonable approximation of r ( d ). No. of single term queries 663,158 881,230 Total no. of docs retrieved 100,147,410 213,829,937 Expected ( X  r ( d )@ c = 100) 96.9 171.4 Table 2: Details of the TREC datasets and large scale retrieval simulations.

A large scale retrieval simulation was then conducted, where we took each q  X  Q and issued it to a given retrieval system, collecting up to 100 results for each query. This was performed for each system and on each collection. The to-tal number of documents retrieved during the course of the http://www.lemurproject.org simulations for a given run is also shown in Table 2, along with the number of times we would expect to retrieve any given document in the top 100. Since our approximation of r ( d ) is essentially a cumulative measure which reflects how many times document d is going to be retrieved, it is a func-tion of the size of the result set that is returned to the user, characterized by the parameter c . For each collection and retrieval model, we computed the r ( d ) values over the all the documents in the collection for c =10 , 20 , 30 , 50 &amp; 100.
In order to choose a suitable c parameter, we investigated the correlation between retrievability measurements given the different values of c . In a series of pairwise comparisons, with c = 10, we found that a significant correlation exists between the measurements at different values of c , regard-less of the retrieval system. This suggests that changing the c parameter will not dramatically alter the relative estimate of r ( d ), but only affects the magnitude of the approxima-tion. Ideally, the c parameter should be chosen to reflect the particular scenario in order to obtain a more accurate estimate. For instance, in web-search a low c would be more accurate because users are unlikely to go beyond the first page of results; whereas a high c would be more appropriate for a more thorough legal or patent searcher. For the pur-poses of the analysis shown in the remainder of this paper, we only report experiments using c = 100 as this provides the largest sample 8 .

The first interesting observ ation from examining the re-trievability scores of documents, is that there are many doc-uments in the collection that attract a very low retrievabil-ity score, while there are few documents that attract a very high retrievability score. When comparing different retrieval models, we witnessed that this trend was more pronounced in TFIDF and BM25i than LM1000 and BM25. In order to quantify the difference in retrievability amongst the pop-ulation of documents, we require a global measure of the retrievability bias.
By examining the distribution of r ( d )scoresofallthedoc-uments, it is possible to assess the inequality between docu-ments within a collection by using the Lorenz Curve [7]. In Economics and the Social Sciences, a Lorenz Curve is used to visualize the inequality of the wealth in a population. This is performed by first sorting the individuals in the pop-ulation in ascending order of their wealth and then plotting a cumulative wealth distribution. If the wealth in the pop-ulation was distributed equally then we would expect this cumulative distribution to be linear. The extent to which a given distribution deviates from equality is reflected by the skew in the distribution. We employ the same idea in the context of a population of documents, where their wealth is represented by r ( d ) and plot the result in Figure 1. The more skewed the plot, the greater the amount of inequality, or bias within the population. In the example, TFIDF dis-plays the most inequality, whereas BM25 results in the least. The Gini Coefficient G was proposed as a way to summarize the amount of bias in the Lorenz curve [7], and is computed as follows:
The same analysis at different values of c yielded similar findings to those reported here. .GOV BM25 G 0.63 0.60 0.57 0.55 0.52 Table 3: Gini coefficient values for all the retrieval models considered with different values of c .As c increases, G steadily decreases indicating that lesser bias is experienced when considering deeper ranked list. Also shown is the Pearson X  X  correlation coef-ficient between retrievability values calculated with c =10 and other values of c . A statistically signif-icant relationship exists between all the pairs, in-dicating that the measure is relatively stable with respect to choice of c . where the retrievability values, r ( d i ), have been sorted in ascending order and N is the number of documents in the collection. If G = 0 then no bias is present because all the documents are equally retrievable (i.e., r ( d i )= r ( d i, j), whereas if G = 1 then only one document is retrievable and all other documents have r ( d ) = 0. By comparing the Gini coefficient of different retrieval methods, we can obtain a bird X  X  eye view of the retrievability bias imposed on the collection of documents by different retrieval systems, given the reference set of queries.

We can see in Table 3 that as c is increased, the Gini coefficient tends to slowly decrease. This suggests that the amount of inequality within the population is mitigated by the willingness of the user to search deeper into the rank-ing. Consequently, if a user is only willing to examine the top documents, they will be subject to a greater degree of retrieval bias.

In terms of the bias induced by the tested retrieval mod-els, we note that TFIDF has the greatest inequality between documents over both datasets while BM25 appears to pro-vide the least inequality. The introduction of a prior (a fa-voritism towards documents with more inlinks) is reflected in the Gini coefficient for BM25i being more than that for BM25, i.e., it is more biased. In it interesting to compare the overall effectiveness of the system to the retrievability bias (summarized by the Gini coefficient). We can clearly see that global retrieval bias can be very harmful to effec-tiveness, in the case of TFIDF on .GOV, but conversely retrieval bias can be beneficial to effectiveness as in the case of BM25i versus BM25. At this level, there appears to be no relationship between global retrieval bias and overall ef-fectiveness. Intuitively, this is correct, because bias can be both damaging and supportive in the process of searching for relevant content. In section 4.6, we examine at a lower level the relationship between effectiveness and retrievability and show a lower level relationship between the two types of measures.
In this subsection, we consider how we can examine the influence of an IR system on different logical divisions of the collection. Such analysis would be useful in order to identify whether certain sites are favored over others, or whether there are sections of the collection that need to be improved. Both AQUAINT and .GOV, can be logically sub-divided into meaningful divisions. For the e-Government scenario, the curator might be interested in noting which parts of the .GOV domain are more readily accessible through the search engine; and if it is found that some sections of the domain are noticeably less retrievable then remedial action that more effectively exposes the content in the domain can be taken. Alternatively, when comparing retrieval systems, preference may be given to one that makes a larger fraction of the domain visible; for instance, this may be the case with the AQUAINT collection where all the systems deliver comparable retrieval effectiveness. .GOV: For the current set of experiments, we associate every document in .GOV with a sub-domain based on their URL strings. We can then consider each sub-domain in turn, calculating the average r ( d ) value for all the documents in a given sub-domain for a given retrieval algorithm. Simi-larly for the AQUAINT collection of newswire articles, the three different sources (APW , NYT and XIE) provide three parts to the collection and the average retrievability of the documents from a particular source can be calculated. Ta-ble 4 shows the top 10 and bottom 10 sub-domains of .GOV, which contained more than 100 documents. Most of the do-mains which had less than 100 documents, tended to have lower mean r ( d ) than those shown. Interestingly, we found that no correlation existed between the size of the domain and the mean retrievability of the domain. However, we did find a significant positive correlation between the total re-trievability of each domain and the size domain, which is to be expected because a larger domain will have more docu-ments which could be retrieved.

The relative difference between the top 10 domains and bottom 10 domains, is by orders of magnitude, where doc-uments in some domains are, on average, 200 times more likely to be retrieved than documents in other domains. This is quite a disparity; for smaller domains the problem tends to be substantially worse. Some of the least retrievable sub-domains consist of considerable numbers of documents. If search is the primary information access mechanism on this collection, then this represents a problem for the site admin-istrator because there are parts of the collection which are relatively inaccessible.

AQUAINT: Table 5 shows how differently the three re-trieval methods retrieve different parts of the AQUAINT collection. In particular, the mean r ( d ) value for NYT doc-uments is almost 8 times larger than the equivalent number for XIE when using TFIDF retrieval. What this means is that even if relevant content existed within XIE, the TFIDF retrieval algorithm will be unlikely to retrieve it such that it is accessible within the top 100 results. BM25 on the other hand is the least biased of the three algorithms consid-ered providing the least imbalance amongst the sub-divisions of the collection. A one-way ANOVA test on each of the Avg. doc length 426 (385) 798(745) 205(186) Table 5: Mean r ( d ) values for each document in AQUAINT grouped by source (median values shown in brackets). groups, and follow up significance test shows that there is a significant difference between the retrievability of documents from each of the different sources (regardless of retrieval al-gorithm). Consequently, we find that none of these retrieval models provide unbiased access across the different sources.
An alternative study is to analyse the document features to determine if there is a relationship with retrievability. While one must have an idea of the features in mind apriori , it is possible to build up a picture of what features may affect the retrievability of documents. On one hand, this provides a diagnostic tool for IR practitioners and researchers, and on the other hand it provides a investigative tool for search en-gine optimizers. Such an analysis can be seen as a first step towards taking remedial action -if we know that documents with particular characteristics are being penalised by the re-trieval algorithm, as the administrator of the collection, we might want to ensure that the documents in the collection possess the positive characteristics of a highly retrievable document.

For the purposes of demonstration, we consider three doc-ument features that could be used to characterize documents in the .GOV collection and examine how they relate with r ( d ). Thefeaturesweconsider,aredocumentlength n ( d ), the number of inlinks to a document, n ( i, d ) and the number of outlinks from a document, n ( o, d ). However, any feature that is available could be used, and if we had access to the retrieval function instead then we could precisely evaluate the influence of each feature. To perform the analysis: for each of the three features, we sorted the documents in in-creasing order of their value for that feature. We then placed documents into bins of size 2000 leading to 613 groups over the .GOV collection. For each bin, we then calculated the mean and median r ( d ) for documents in that bin. This is plotted on the Y-axes in Figure 2, with the particular fea-ture on the X-axis. For comparison, we provide a dotted line representing the situation where the documents have been randomly assigned to bins. The differences between the re-trieval systems can be clearly seen from the plots shown in Figure 2.

As one would expect, TFIDF tends to favor longer doc-uments, and BM25i tends to favor documents with more inlinks. But perhaps surprisingly, TFIDF, LM1000 and to a lesser extent BM25 tend to favor documents with fewer out-links. For instance, TFIDF tends to retrieve documents with fewer outlinks on average three times more than documents with many outgoing links. On the other hand, BM25i ap-pears to favor documents with more outlinks. The observed increased positive correlation of r ( d ) with inlinks when mov-ing from BM25 to BM25i, as well as dependance of TFIDF on length, again provides a sanity check for our measure. However, the observation on outlinks illustrates non-obvious behavior of the retrieval algorithms that is elucidated using our measure. There are of course two problems with this naive treatment: 1. We have not considered correlations between the fea-2. We have had to decide beforehand what features of This second point means that a search engine optimizer who only observes a bias in retrievability but does not have any knowledge of the underlying retrieval mechanism cannot definitively say whether the feature is the cause, or whether it was just a correlation. Even taking this into account, the feature analysis serves as very helpful diagnostic and inves-tigative tool to aid in determining how the retrieval system behaves given a particular feature.
So far we have examined what levels of retrievability dif-ferent retrieval systems provide to (individual and groups of) documents in a collection. We have seen that the algo-rithms differ significantly and substantially in terms of the retrieval biases that they impose on the population of doc-uments. In this subsection, we specifically examine whether such retrieval bias actually impedes one X  X  ability to access content within the collection. That is, given that the IR system favors certain documents over others, does it mean that less retrievable documents will be significantly harder to find. Or conversely, if some documents are less retriev-able, then removing them from the collection is unlikely to have an impact on effectiveness because they are unlikely to be retrieved (by definition of r ( d )). In order to examine these two premises, we construct two separate experiments.
Experiment 1 We replicate a standard IR evaluation environment by considering each collection with its corre-sponding set of TREC topics. We speculate that the less re-trievable documents are unlikely to be retrieved in response to these topics for a given algorithm; and so if these docu-ments are removed, no significant degradation to effective-ness will be witnessed. The extent to which we can remove documents, will depend on two things: (1) the quality of the documents within the collection, and (2) the extent to which the retrieval system is biased. Such that if a system is more biased, in terms of its Gini coefficient, it is likely that more documents can be removed, because a greater proportion of the documents are unlikely to be retrieved.

Given the set of TREC Topics, we calculated traditional measures of retrieval effectiveness on the collection of docu-ments. Using the hypothesis that the removal of documents with low r ( d ) is unlikely to significantly affect effectiveness, we successively removed a fraction f of each of our two col-lections. To pick the documents that get removed, we first arranged the documents from the index in decreasing order of their r ( d ) values calculated using each retrieval method in turn. We then progressively removed documents from the lower end, and measured the retrieval effectiveness (using Mean Average Precision calculated on the top 1000 docu-ments returned for each query on this reduced collection. A plot showing the percentage drop in MAP versus fraction of the collection removed is shown in Figure 3.

On AQUAINT, we found that for any particular model, there was significant degradation in performance once 10% or more of the collection was removed, except for TFIDF where over 30% of the collection was removed before there was a significant degradation in performance. While, on .GOV, we see quite a different picture. Up to 50% of the collection could be removed before there was a significant degradation in performance for BM25 and LM1000. Whereas for BM25i and TFIDF, up to 70% and 80% of the collection could be removed, respectively. Surprisingly, for BM25i we found that when 30% to 50% of the collection was removed, retrieval effectiveness on the reduced collection was actually better than on the complete collection, and this difference was statistically significant. This experiment provides ev-idence to suggest that the more biased the algorithm, the more of the collection can be removed without a significant drop in performance.

The above experiment also indicates that the different re-trieval algorithms make certain parts of the collection virtu-ally inaccessible, to the point that the documents are ex-pendable (i.e. can be removed without a significant loss to effectiveness). This influence of a retrieval system over the collection is not reflected in standard effectiveness based evaluations and highlights how retrievability provides a dis-tinctly different dimension to IR system evaluation.
Experiment 2 For the next experiment, we consider the hypothesis that if we were trying to retrieve documents of varying retrievability, we would expect that it would be more difficult to formulate a query which would retrieve docu-ments of low retrievability than those of high retrievability; even if we pose a query which is specifically crafted to re-trieve that specific document. In order to test this hypoth-esis, we divide the collection of documents into four bins, according to their retrievability values. The first bin con-tains the 25% of the documents with the lowest retrievabil-ity, while the fourth bin contains the 25% of documents with the highest retrievability for a given retrieval method. From each bin, we simulated known-item queries using the method proposed in [1] with the suggested parameter settings 9 .For each of the quartiles, a document was chosen at random and query terms were randomly selected from this document ac-cording to the probability of the term being present in the document. A total of 1000 queries were simulated for each quartile. These queries were then issued against the collec-tion, and a set of results were generated using one of the retrieval methods and the position of the target document in the returned result list was used to calculate the Mean Reciprocal Rank (MRR). The MRR of known-item searches for documents in each quartile represent the effectiveness of future users X  searches for the documents in that group. The results are provided in Table 6.
 AQUAINT LM1000 0.21 0.26 0.28* 0.28 Table 6: Effectiveness of known-item searches mea-sured by MRR. An  X * X  indicates that the effective-ness of the queries in this set was not significantly different to the performance of the queries in the 4th quartile using the Kolmogorov-Smirnov test be-tween the two distributions (p &gt; 0.05). For all other results, there was a significant difference between the performance of the other quartiles and the 4th quartile.
The specific parameters used we re: the term-frequency (or popular) term selection strategy, the mean length of queries was set to four, and the probability of a noisy query term was set to 0.2.
It can be seen from the table that documents with lower r ( d ) are more difficult to find when compared to document that are more retrievable. This is despite the fact that the queries were specifically designed to bring back those doc-uments. While the results are dependant on the method used to generate the simulated topics, they show that across retrieval algorithms, the documents in the fourth bin are sig-nificantly easier to find (through a search) than documents in first bin. Note that TFIDF on the .GOV collection pro-vides the most extreme example where the effectiveness for documents in the fourth bin is over five times greater than in the first bin. For BM25i, even though it is very effec-tive at retrieving the documents in the fourth bin, it is four times worse at retrieving documents in the first bin. The disparity between bins for BM25 is less than a factor of two. When we consider these results against the global measure of retrievability bias, it indicates that if a system is highly biased, then the disparity in retrieval effectiveness over the collection is likely to be greater than if the system is less biased.

Finally, these results show that the bias of the retrieval system imposed upon a collection of documents can seri-ously affect the retrieval effectiveness of attempting to ac-cess documents which are less retrievable. The more biased the system the greater the disparity in retrieval effectiveness measured on known-item searches.
In this paper, we have proposed a methodology to evalu-ate retrieval systems based on the access they provide into a collection of documents. This required a measure to capture the retrievability of documents. This measure was designed to reflect the ease with which the document can be found through the retrieval system. The motivation for such a measure stems from the concern over biases of search en-gines and retrieval systems, and the need to ensure that content is accessible through such systems. This is because of the growing reliance of users to engage such systems in order to find content. Since effective retrieval necessarily involves a preference for one set of documents (i.e., the rel-evant ones) over another (the non-relevant), the existence of some bias is inevitable. We have demonstrated that the proposed measure of retrievability provides a useful way in which to quantify the retrieval bias of a system and how it can be used in the evaluation of higher order information access tasks.

Given the presence of retrieval bias, it was also important to determine whether such bias had any impact on one X  X  ability to access the content through the system. In this paper, we have tested to see if the imposition of such bias actually has an impact, negative or otherwise, on system effectiveness. Experiments conducted on AQUAINT and .GOV yielded two main findings: Finally, measuring retrievability provides a novel way in which to assess a retrieval system X  X  influence on the access to documents in a collection. This paper has provided a methodology that can be used to assess the impact of IR sys-tems on collections; so that the collection and/or retrieval system can be improved to facilitate better access. While the main goal of retrieval is to maximize the system per-formance for specific sets of information needs, our findings suggest that it is also important to consider the impact that the retrieval system X  X  bias has on the access to the entire collection. Determining the trade off between effectiveness and retrievability poses an interesting direction for future work, along with (i) exploring alternatives to estimate and approximate the retrievability values; (ii) improving the ac-curacy of the estimate by considering different cost functions and query sets, and (iii) examining different application ar-eas (e.g. measuring search engine bias, or the accessibility of e-Government information).
The first author would like to thank the Information Re-trieval Facility ( www.ir-facility.org ) for the use of their computational resources. [1] L. Azzopardi and M. de Rijke. Automatic construction [2] L. Azzopardi and V. Vinay. Accessibility in [3] M. Baillie, L. Azzopardi, and I. Ruthven. Evaluating [4] J. Callan and M. Connell. Query-based sampling of [5] N. Craswell, S. Robertson, H. Zaragoza, and [6] A. Dasgupta, A. Ghosh, R. Kumar, C. Olston, [7] J. L. Gastwirth. The estimation of the lorenz curve [8] S. Lawrence and C. L. Giles. Accessibility of [9] A. Mowshowitz and A. Kawaguchi. Assessing bias in [10] S. Pandey, K. Dhamdhere, and C. Olston. Wic: A [11] V. Petricek, T. Escher, I. J. Cox, and H. Margetts. [12] M. Sanderson and J. Zobel. Information retrieval [13] A. Singhal, C. Buckley, and M. Mitra. Pivoted [14] T. Upstill, N. Craswell, and D. Hawking. Buying [15] C. J. van Rijsbergen. Information Retrieval . [16] L. Vaughan and M. Thelwall. Search engine coverage Figure 2: Retrievability plots across document de-pendent factors. Top to Bottom: TFIDF, LM1000, BM25, BM25i. Subplots left to right: document length n ( d ) , number of inlinks n ( i, d ) and number of outlinks. n ( o, d ) . The Y-axis denotes how retrievable a document is, according to our estimate r ( d )
