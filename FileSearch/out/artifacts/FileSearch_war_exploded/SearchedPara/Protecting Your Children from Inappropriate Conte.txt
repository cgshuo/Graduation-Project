 Mobile applications (Apps) could expose children or adoles-cents to mature themes such as sexual content, violence and drug use, which harms their online safety. Therefore, mobile platforms provide rating policies to label the maturity levels of Apps and the reasons why an App has a given maturity level, which enables parents to select maturity-appropriate Apps for their children. However, existing approaches to im-plement these maturity rating policies are either costly (be-cause of expensive manual labeling) or inaccurate (because of no centralized controls). In this work, we aim to design and build a machine learning framework to automatically predict maturity levels for mobile Apps and the associated reasons with a high accuracy and a low cost.

To this end, we take a multi-label classification approach to predict the mature contents in a given App and then label the maturity level according to a rating policy. Specifically, we extract novel features from App descriptions by leverag-ing deep learning techniques to automatically capture the semantic similarity between words and adapt Support Vec-tor Machine to capture label correlations with pearson cor-relation in a multi-label classification setting. Moreover, we evaluate our approach and various baseline methods using datasets that we collected from both App Store and Google Play. We demonstrate that, with only App descriptions, our approach already achieves 85% Precision for predicting mature contents and 79% Precision for predicting maturity levels, which substantially outperforms baseline methods. H.2.8 [ Database Management ]: Database Applications X  Data mining ; H.4 [ Information Systems Applications ]: Miscellaneous Algorithms, Measurement, Experimentation c  X  Figure 1: The mature contents and maturity levels of Mobile Apps; Content Rating; Text Mining; Deep Learning; Pearson correlation.
Mobile devices are becoming more and more popular in the past few years. However, Apps could expose children or adolescents to mature themes such as sexual content, vio-lence, and drug use, which are harmful to their growth and development. Indeed, research from psychology has long es-tablished that teenagers who are exposed to content that glamorizes drug use, sex, or violence tend to engage in those activities themselves [27, 13, 28].

Therefore, similar to the conventional video game and movie industry, mobile platforms provide mechanisms to rate the maturity levels of Apps, which enables parents to select maturity-appropriate mobile Apps for their children. For instance, App Store has a maturity rating policy, which consists of four maturity levels, i.e., 4+, 9+, 12+ , and 17+ . Apps with different maturity levels are suitable for users with different ages, e.g., Apps with 17+ maturity level are appropriate for users who are at least 17 years old. In ad-dition to the maturity level, App Store also identifies the detailed mature contents which make an App be rated as a specific maturity level. These mature contents are helpful for users to better understand the App, and to guide devel-opers to modify their Apps in order to increase their audi-ence population. For instance, Figure 1 shows the mature Figure 2: Inconsistency of maturity ratings for the same contents and maturity levels of three Apps on App Store. Google Play also establishes a rating policy that includes four maturity levels, i.e., Everyone, Low Maturity, Medium Maturity, and High Maturity , and they are corresponding to the four maturity levels on App Store.

Existing approaches to implement these maturity rating policies are either costly or inaccurate. Specifically, Apple Inc. hires employees to manually examine each submitted App in order to identify its maturity level and the associ-ated reasons. Given the large amount of new Apps, e.g., 20,000 new Apps were submitted to App Store per month as of 2014 [2], this manual labeling approach is very costly and time-consuming. Unlike the centralized rating service provided by App Store, Google Play requires developers to label the maturity levels for their own Apps according to the Google Play X  X  rating policy. These self-reported matu-rity levels are determined at the time when developers sub-mit their Apps to Google Play and remain unchanged un-til users report inappropriateness. Although Google Play X  X  strategy is scalable and less costly, the maturity ratings re-ported by developers could be inaccurate. Moreover, var-ious reports [15, 25] have shown growing concerns among parents who have experienced inaccurate maturity ratings of Apps. Figure 2 shows an example of the mislabelling of the maturity ratings by the Android developers. Chen et al. [5] rated the maturity of apps using a keyword match-ing method through manual identification of the sensitive words that are highly correlated with apps X  maturity in App descriptions. Their method achieves limited labeling accu-racy because it does not consider the semantic meanings of words. Moreover, their method cannot produce evidences about why an App has a worse rating than another one.
In this work, we aim to design a scalable framework that automatically labels App maturity level and the associated reasons accurately. Our framework, called Automatic App Maturity Rating (AAMR) , takes a rating policy and the de-scription of an App as input, and predicts the mature con-tent in the App and the maturity level of the App. We choose App description because it describes the content and functionality of an App, and thus it is a good indicator of the mature content (if any) in the App. However, App de-scription based rating analysis faces several challenges: for users to quickly understand the Apps.
 phrase has different meanings at different contexts; and nat-ural languages have synonyms , i.e., the same mature theme can be expressed with different words or phrases.
 tents are likely to co-occur in Apps while some mature con-tents are mutually exclusive.

To address these challenges, we propose a two-stage ma-chine learning approach to first predict the mature contents and then label the maturity levels. Specifically, we extract novel features from App descriptions. In particular, we use word to vector model [23, 24], which leverages deep learn-ing technique to automatically capture the semantic simi-larity between words; to mitigate the language ambiguity, we use the bag-of-words feature to capture the context and global word distributions. With these features, we map the mature-content prediction to be a multi-label classification problem and we adapt SVM to capture label correlations. We choose Support Vector Machine (SVM) [7] for rating prediction since it was shown to outperform other classifiers for short text analysis [11]. We evaluate our approach and various baseline methods using large-scale datasets that we collected from both App Store and Google Play. We demon-strate that, using only App descriptions, our approach can already achieve 85% Precision on maturity content predic-tion and 79% Precision on maturity level prediction, and our approach substantially outperforms baseline methods in-cluding both automatic and human-labeling approaches.
The key contributions are summarized as follows:
In this section, we first introduce the maturity rating poli-cies adopted by App Store and Google Play, and then we discuss the limitations of current approaches to implement the policies, which is followed by our design goals of our automatic App maturity rating system. App Store X  X  Rating Policy: Table 1 shows the rating policy of App Store [16]. In this policy, Apps are classified into four categories, i.e., 4+ , 9+ , 12+ , and 17+ . Apps with the maturity level of 17+ are appropriate for users who are at least 17 years old. The maturity level of an App is related to the following mature contents: violence, sexual/maturity, profanity/humor, alcohol/drug/tobacco,etc. Moreover, dif-ferent intensity of mature contents results in different matu-rity levels. For example, an App is rated as 9+ if it contains infrequent or mild  X  X ealistic violence X , but it is rated as 12+ if the realistic violence is frequent or intense .
 Google Play X  X  Rating Policy: Similar to App Store, Google Play X  X  policy also consists of four maturity levels. However, unlike App Store X  X  policy whose maturity levels are directly related to numeric ages, Google Play X  X  four lev-els are everyone , low maturity , medium maturity , and high maturity [1]. Table 2 illustrates Google Play X  X  rating policy. Comparing the two policies: We note that the two policies are equivalent under some conditions [5], i.e., some Apps have the same maturity level under the two rating policies. For instance, an App with only frequent profanity is labeled as 12+ by the App Store X  X  policy and medium by the Google Play X  X  policy, respectively. Thus, 12+ in the App Store X  X  policy is equivalent to medium in the Google Play X  X  policy for this App. Table 3 shows the mappings between Google Play X  X  maturity levels and the App Store X  X  maturity levels. The conditions for such mappings were discussed and obtained by Chen et al. [5]. However, App Store and Google Play treat different content as mature. For example, Google Play adopts  X  X ate X  and  X  X ocation X  as mature, but these con-tent is not considered by App Store. Moreover, Google Play also considers the intensity of mature content when labeling maturity levels, but the definition of intensity is a slightly different from that of App Store. In our experiments, we will use these conditions to obtain groundtruth mature contents and maturity levels of Google Play Apps.
An implementation of a policy is to label the maturity level of an App according to the rating policy. Existing im-plementations adopted by App Store and Google Play are either costly or inaccurate. On App Store, Apple Inc. hires trained employees to comprehensively evaluate each submit-ted App to label its maturity level, and such evaluations could include meta-data (e.g., App description, icon, and screenshots of the App) analysis and code analysis. Given the large amount of new Apps, e.g., 20,000 new Apps were Table 3: Mappings between maturity levels in Google submitted to App Store per month as of 2013 [2], this man-ual labeling approach is very costly and time-consuming. Unlike the centralized rating service provided by App Store, Google Play requires developers to label the maturity levels for their own Apps according to the Google Play X  X  rating policy. Although Google Play X  X  strategy is scalable and less costly, the maturity ratings reported by developers could be inaccurate. For instance, in our dataset collected from Google Play, 45% Apps have incorrect developer-provided maturity levels.
To overcome the limitations of existing policy implementa-tions, we propose an automatic App maturity rating frame-work. We have the following design goals for such a frame-work.
 Policy Independent: Different platforms could have dif-ferent rating policies. For instance, although Google Play X  X  policy and App Store X  X  policy are equivalent in some cases, they are different in general. Therefore, we aim to design our framework independent of policies. In other words, the input of our framework includes the specifications of a pol-icy, and our framework produces the maturity level of an App under this policy.
 Mature Contents to Support Maturity Levels: Pre-dicting the mature contents in an App help users better understand the App and guide developers to modify their Apps in order to increase the number of potential users. For instance, an App is labeled as 17+ by App Store because of frequent sexual content. The developer could decrease the number of dirty words so that it is labeled as 12+ , which makes people whose ages are between 12 and 17 years old Figure 3: Overview of Automatic App Maturity Rating (AAMR) framework. potential users of this App. Therefore, we aim to design our framework to give both the maturity levels and also the maturity words that support our rating.
 Scalable and Accurate: Given the large number of new submitted Apps per month, we aim to design our framework to be scalable. In particular, our framework should produce maturity analysis results for Apps immediately after they are submitted. Moreover, our framework should produce the maturity analysis results with a high accuracy.
Figure 3 illustrates the machine learning framework for automatic App maturity rating. In the off-line learning phase, we extract features from App descriptions and learn a multi-label classifier to predict the mature content in a given App. In the on-line prediction phase, our framework performs maturity rating in a two-stage approach, i.e., we first use the learned multi-label classifier to predict mature content in a new App and then label the maturity level ac-cording to the given rating policy.

We extract our features from sensitive words in maturity rating policies that directly refer to mature content, aug-mented sensitive words that are semantically similar to sen-sitive words, and bag-of-words model. Natural languages have synonyms, e.g., different words could represent the same mature content. Thus, we use augmented sensitive words that are synonyms of sensitive words to enrich our features. We obtain augmented sensitive words via recent word-to-vector techniques [24][23]. Specifically, a word-to-vector algorithm learns a vector representation for each word from a corpus of text, and two words are semantically similar if they have close vector representations. Moreover, natural languages are ambiguous, e.g., a sensitive word in an App description might not indicate that the App has mature con-tent. Therefore, we use bag-of-words model to capture the context of sensitive words to mitigate the ambiguity issue.
We find that mature contents are correlated, i.e., some mature contents have very high co-occurrences in App de-scriptions. Therefore, we adapt multi-label Support Vec-tor Machine (SVM) [7] to capture such correlations. We choose SVM because App descriptions are short and previ-ous work [6] showed that SVM outperforms other classifiers for short text classification.

In the next few subsection, we will illustrate each compo-nent in more details.
We extract features from sensitive words in rating policies, augmented sensitive words that are semantically similar to sensitive words.
A maturity rating policy has clear definitions on maturity content. Therefore, we extract sensitive words from a rating policy that directly refer to mature content. For instance, the list of sensitive words we extract from the App Store X  X  rating policy shown in Table 1 include violence, horror, hu-mor, profanity, sex, nudity, mature, alcohol, tobacco, drug, gambling , and treatment, etc . We note that the list of sen-sitive words could be different for different rating policies because they could treat different contents as mature. For instance, Google Play also defines hate as mature content while App Store does not. We use binary features to rep-resent sensitive words. Specifically, a feature has a value of 1 if the corresponding sensitive word appears in an App description, otherwise the feature has a value of 0.
Chen et al. [5] proposed to take these sensitive words as features and use them to directly learn a classifier to pre-dict the maturity levels of Apps. The keyword-matching method, however, suffers from two limitations. First, nat-ural languages have synonyms, e.g., battle is semantically similar to violence . Keyword-matching will miss these syn-onyms, which results in high false negatives. Second, natural languages are ambiguous, e.g., that a sensitive word appears in an App X  X  description does not necessarily mean the App contains the corresponding mature content, which results in false positives. To address these limitations, we leverage fea-ture augmentation to consider words that are semantically similar to sensitive words to capture the context.
We first show an example to illustrate the issues intro-duced by synonyms. The following is a part of the descrip-tion of an App named  X  X njustice: Gods Among Us X :
In the above description, there are several words (shown in bold) strongly indicating violence content which is not suit-able for children under a certain age. However, the sensitive words cannot represent such mature content. To address this problem, we leverage word-to-vector (word2vec) tech-niques developed by Google [24, 23] to augment sensitive words with semantically similar words. Table 4: Top-10 words that have the highest similarities
A word2vec algorithm learns a vector representation for each word from a corpus of texts in an unsupervised setting. These vector representations capture a large number of pre-cise semantic word relationships. Specifically, two words are treated as semantically similar if they have close vector rep-resentations. As a word embedding technique, word2vec, can be viewed as a representational layer in a deep learn-ing [4] architecture which transforms a word into a posi-tional representation of the word relative to other words in the training dataset, where the position is represented as a data point in the new vector space.

In experiment, in order to get  X  semantic  X  meaning of words, we run the word2vec tool [32] using the app descriptions from more than 350,000 Apps in Google and App Store. For instance, Table 4 shows the top-10 words that have the highest similarities with the sensitive word sex . The simi-larity between two words is defined as the cosine similarity of the two corresponding vector representations.

For each sensitive word, we choose the top-200 words that have the highest cosine similarities, and we call these words augmented sensitive words . Note that some words might ap-pear more than once in our augmented sensitive words be-cause they might be similar to more than one sensitive word. Suppose we have n augmented sensitive words, we extract a feature vector with length n for each App. Specifically, if an augmented sensitive word appears in an App X  X  description, the corresponding feature has a value that equals the cosine similarity between the augmented sensitive word and the corresponding sensitive word, otherwise the corresponding feature has a value of 0. Given that the text data is very sparse, in some cases, App descriptions do not contain any sensitive word or aug-mented sensitive word. Moreover, whether a sensitive or an augmented sensitive word really indicates mature con-tent depends on the context. Therefore, we further extract bag-of-words features from App descriptions.
 We adopt term frequency-inverse document frequency (TF-IDF) to weight each word. TF-IDF is widely used in infor-mation retrieval and text mining [22, 30, 33]. The TF-IDF weights evaluate how important a word is to a document in a corpus of documents. Specifically, the TF-IDF weight of a word is composed by two parts. The first part com-putes the normalized Term Frequency (TF) and the sec-ond part is the Inverse Document Frequency (IDF). For-mally, the TF-IDF weight of a word is calculated as follows, w i,j = TF i,j  X  log ( N DF i ) , where TF i,j is the term frequency of t i in document d j , N is the total number of documents in the corpus, and DF i is the total number of documents that contain t i .
We concatenate the features extracted from sensitive words, augmented sensitive words, and bag-of-words model. For the sensitive word features, we extract twelve sensitive words from the App Store policy and thirteen sensitive words from the Google Play policy. For each sensitive word, we have the top-200 words with the highest cosine similarity. We use the most frequent 2000 words for the bag-of-words features. In total, we have 4,412 features for Apps on App Store and 4,613 features for Apps on Google Play.
After feature extraction and feature learning from apps X  descriptions, the next step is to build a machine learning classifier that can automatically classify an app into its cor-responding maturity level.

More formally, let x i  X  &lt; p be the p -dimensional feature vector carried by each App i . Let y i  X  { 0 , 1 } C be the C -dimensional binary vector denoting the maturity contents for App i . Then Y = [ y 1 , y 2 ,... y n ] corresponds to the ma-turity contents for all the apps. Furthermore, let z  X  &lt; be the set of the maturity levels of the mobile Apps, where z = [ z 1 , z 2 ,... z n ] and z i is the maturity level for each App i . More specifically, the mature content is defined by a matu-rity rating policy. For instance, both App Store and Google Play define 17 different mature contents, which are shown in Table 1 and Table 2, respectively, and y i indicates what kind of mature content among the 17 contents the App i contains.
In the off-line training phase, we have a set of training mobile App data L = ( x 1 , y 1 , z 1 ) , ( x 2 , y 2 , z where x i is a feature vector extracted from an App descrip-tion, y i is the binary label vector indicating the maturity content, and z i is the label of the maturity level, where 1  X  i  X  n .

In the on-line prediction phase, we perform maturity anal-ysis in a two-stage approach, i.e., App description( x )  X  maturity content (y)  X  maturity level( z ) , where in the first stage, the maturity contents is inferred from the app feature vectors, and in the second stage, the maturity level is predicted by combining maturity content based on the rating policy. For instance, if every element in y is zero, then the App has the lowest maturity level (i.e., 4+ on App Store and Everyone on Google Play).

Note in this procedure, an app is generally assigned to multiple (i.e., one or more) maturity content tags. This is known as  X  X ulti-label learning X  in machine learning. Fortu-nately, we develop a method that can automatically adapt SVM to support multi-label classification by incorporating label correlations, with minimum efforts.
As is illustrated before, to support multi-label classifica-tion task, we need to develop a method that is fast, scalable, and accurate. Linear classification method is a good fit to achieve these goals given the large number of new Apps and Figure 4: Correlation matrix for 11 types of matu-rity content, where ( k,` ) element indicates the cor-relations between the tag k and ` . For illustration purpose, infrequent/mild and frequent/intensive are grouped together. their meta data. Hence, in the context of app maturity level prediction, we use linear SVM as our method due to its strong capability in handling short text classification tasks such as [34, 29, 35, 11]. This is also further confirmed by our experiments.

Recall that in standard multi-class SVM problem, it finds the maximum-margin hyperplane [8] that has the largest separation (margin) among data points from different classes. To be exact, it optimizes: where w k is the decision hyperplane for k -th class (1  X  k  X  K ), y i is the label for data x i ,  X  i is the slack variable, and C is the constant. According to  X  X oss + Regularization X  format, Eq.( 1) can be written as a sum over of a hinge loss and ` 2 regularization, i.e., where ( x ) + = max { x, 0 } , W = [ w 1 w 2  X  X  X  w K ] is the hyper-plane matrix. Let  X ( W ) = P K k =1 k w k k 2 2 , Eq.(2) is equiva-lent to: where f ( W ; X , Y ) = P n i =1 (1  X  w T y i x i + max k 6 = y
To support multi-label classification, the idea is to trans-form the multi-label classification problem into multi-class problem by considering label correlations. The key obser-vation is that many tags always co-occur together. The in-tuition is that the elimination of label correlations can help improve the performance, which is also confirmed in past researches [9][36][19].

In the context of maturity content rating, the two rating policies, which are shown in Table 1 and Table 2, both con-sider seventeen maturity content for maturity rating. Fig-ure 4 shows the correlations among the maturity content considered by the App Store X  X  rating policy. The figure was plotted based on the statistics from the real-world app dataset used in experiments. For illustration purpose, we combine the infrequent/mild and frequent/intense levels for the same maturity content together. However, we still treat the infrequent/mild and frequent/intense as two labels in our experiments.

We observe that there exists high correlations among some maturity content. For example, the profanity or crude hu-mor and the mature/suggestive themes are highly positively correlated. Another example is that the cartoon or fan-tasy violence and horror/fear theme are highly positively correlated. In addition to the highly positive correlations, some maturity content are negatively correlated. In par-ticular, the two levels of a maturity content are exclusive (i.e., perfectly negatively correlated). For instance, for the App shown in Figure 1. left , one of its maturity content is infrequent/mild sexual content and nudity , and thus fre-quent/intense sexual content and nudity will not be a ma-turity content of this App. The above observations moti-vate us to utilize correlations between maturity content to improve the accuracy of classification.

We adapt the standard multi-label SVM to capture label correlations. In particular, we use pearson correlation coeffi-cient R = [ R k` ]  X &lt; K  X  K to capture the label co-occurrence, i.e., R where Y  X &lt; n  X  K is the class label matrix for maturity con-tent, and The label correlation matrix R measures the linear correla-tion (dependence) between each pair of classes k and ` , each entry of which is a number in the interval [-1, +1]. Positive number indicates positive correlation, 0 indicates no correla-tion, and negative indicates negative correlation. We would like to emphasize here that without the centering of the val-ues corresponding to each label, it is impossible to capture the negative correlations.

Then the class label matrix is modified to: and finally we solve the following optimization problem Eq.(6) using the same method as in Eq. (3):
Please note that our method can be applied to any generic loss functions such as logistic loss, LASSO, etc , but not lim-ited to hinge loss shown in Eq.(6). The pearson correlation is indeed a generic method to eliminate both the label cor-relations and the feature correlations, which can be easily adapted to solve many other correlation problems emerged in data mining and machine learning communities.
 Here, we give an example to illustrate how Y looks like. Suppose we have the class label corresponding to app i , Y [0 , 1 , 0 ,... 0 , 0 , 1]. After multiplying the correlation matrix Using the threshold calibration in [36], we obtain a threshold t in the training phase for mapping the continuous Figure 5: The distribution of ground truth maturity levels of Apps in our three datasets. the binary label. In particular, if  X  Y ik  X  t , then otherwise  X  Y ik is 1. In the training phase, we obtain W via solving Eq.( 6) with X train and  X  Y train . In the testing phase, we first get  X  Y test with W and X test , and then we use t to map  X  Y test to Y . In our experiments, we use the widely used package LibSVM [6] to implement adapted multi-label SVM classification with label correlations.
Recall that our automatic App maturity rating system not only predicts the maturity level for a given App but also provides the reasons why the App has the specific maturity level. Therefore, we evaluate both the maturity level pre-diction performance and the reason prediction performance. In the following, we introduce the datasets that we collected from both App Store and Google Play, evaluation metrics we adopt, training and testing, and compared approaches. Crawling App Store and Google Play: We wrote crawlers in python to collect 105,108 free iOS Apps and 105,287 paid iOS Apps from App Store, and 261, 947 An-droid Apps from Google Play. Our crawls include the de-scription and maturity level of each App. App Store also labels the mature contents that makes an App be rated as a specific maturity level. So we also crawl the mature contents for App Store Apps. We crawled our datasets between July 2014 and September 2014.
 Obtaining groundtruth: Apple Inc. hires trained em-ployees to manually perform maturity analysis for each sub-mitted App. Therefore, for Apps from App Store, we take the maturity levels and the associated mature contents crawled from App Store as their groundtruth. Google Play Apps are rated by App developers, so their labels are not accurate. Chen et al. [5] characterized the conditions when the App Store X  X  maturity rating policy and the Google Play X  X  pol-icy are equivalent, and we leverage their results to locate Apps whose Android version and iOS version have the same maturity levels and mature contents. Then, we take the ma-turity levels and mature contents of the iOS version as the groundtruth of the corresponding Android Apps. In sum-mary, we have the following three datasets with groundtruth maturity information:  X  Dataset 1 consists of 105,108 free Apps in App Store. Figure 6: Top-10 categories of Google Play Apps in the Dataset 3 that have the most incorrect developer-provided maturity levels.  X  Dataset 2 consists of 105,287 paid Apps in App Store.  X  Dataset 3 consists of 14,000 Apps in Google Play. We distinguish between free Apps and paid Apps for App Store because we find that predicting their maturity lev-els achieves substantially different performances. However, most of Google Play Apps that have groundtruth maturity information are free, and thus we do not further classify them into free Apps and paid Apps.

Figure 5 shows the distribution of groudtruth maturity levels of Apps in our three datasets. We observe imbalanced distributions. For instance, 73% of Apps in the Dataset 1 have a maturity level of 4+, but only 5% of Apps in the Dataset 1 have a maturity level of 17+.
 Unreliable Google Play ratings: A Google Play App is said to have an inaccurate maturity level if the maturity level provided by the developer does not match the groundtruth. Overall, we find that 45% of Apps in the Dataset 3 have inac-curate maturity levels. Figure 6 shows the fraction of Google Play Apps in the Dataset 3 that have inaccurate developer-provided maturity levels for top-10 App categories. We ob-serve that Apps in the social networking category are most likely to be incorrectly rated by developers.
Since we classify both the maturity levels and the associ-ated reasons, we evaluate various approaches in two aspects: 1) mature content classification, which is a multi-label task, and 2) maturity level classification, which is a multi-class task. Due to the extremely imbalanced label distributions as we show in Figure 5, we do not compare the algorithms in terms of accuracy. For instance, a classifier that always pre-dicts 4+ as the maturity level can already achieve accuracies of more than 0.70.
 Mature content classification: The mature content pre-diction is a multi-label classification problem. Thus, we adopt the metrics Precision, Recall, and F1-value [37], which are widely used to evaluate multi-label classification sys-tems. Specifically, we denote by C the labels (possible ma-ture contents). For each label `  X  C , we denote by TP ` , FP TN ` , FN ` the number of true positives, false positives, true negatives, and false negatives, respectively. Then, for each label `  X  C , we have its precision, recall, and F1-value as P tively. Then we compute the overall Precision, Recall, and F1-value by averaging the precisions, recalls, and F1-values over all labels, respectively. Maturity level prediction: For maturity level prediction, we also use Precision, Recall, and F1-value as our evalua-tion metrics. More formally, let TP k ,FP k ,TN k ,andFN true positives, false positives, true negatives, and false nega-tives for Apps with maturity level k , respectively. Then for each level k , we have precision as P k = TP k TP R Precision, Recall, and F1-value are computed by averaging over all the maturity levels.
For each of the three datasets, we sample 50% of it uni-formly at random and treat them as the training data, and the rest of it is treated as the testing data. We repeat the experiments for 10 trials and average all metrics over them.
We describe compared approaches for mature content pre-diction and maturity level prediction separately.
 Mature content prediction: Recall that we extract fea-tures from sensitive words, augmented sensitive words, and bag-of-words model. Moreover, we consider label correla-tions. We aim to study the impact of each part. To this end, we add each part to our framework incrementally. Specifi-cally, we compare the following methods:  X  AAMR-I : Our framework AAMR with only features  X  AAMR-II : Our framework AAMR with features from  X  AAMR-III : Our framework AAMR with features from  X  AAMR-IV : Our framework AAMR with features from Maturity level prediction: We compare the following approaches to perform maturity level predictions:  X  Human Labeling (HL): We asked 3 users (they are our  X  Developer Report (DR): Google Play requires devel- X  ALM [5]: To the best of our knowledge, only Chen  X  Multi-Class Classification (MCC): The multi-class Table 5: Comparisons against human-based maturity  X  AAMR : Our two-stage approach first predicts the ma-
Figure 7 shows the Precision, Recall, and F-value of the four methods AAMR-I, AAMR-I, AAMR-III, and AAMR-IV on the three datasets. We observe that feature augmenta-tion, bag-of-words feature, and label correlation all improve mature content prediction.
We report results for automatic maturity level prediction approaches and human labelling approaches separately. Comparing automatic prediction approaches: Figure 8 shows the maturity level prediction performances among the three automatic approaches, i.e., ALM, MCC, and AAMR. We observe that both MCC and AAMR achieve much better performances than ALM. Specifically, MCC achieves around 0 . 44 larger F-value than ALM on average, and AAMR achieves 0 . 39 larger F-value than ALM on average. These observa-tions indicate that our features are much better than sensi-tive words which are used by ALM. MCC achieves slightly better performances than our two-stage AAMR method. This is because our AAMR makes some incorrect predic-tions about mature contents, which are subsequently used to label maturity levels. The two-stage approach enlarges the impact of the incorrect mature content predictions. How-ever, MCC method cannot identify the mature contents in an App.
 Comparing with human-based methods: We com-pare our method AAMR with two human-based manually labelling approaches, i.e., HL and DR. Due to the limited human resources we have, we sample 500 Apps from the Dataset 3. For each sampled App, we ask three users (our colleagues) to rate the maturity level via reading the App de-scription. The final maturity level of an App is determined by majority voting among the labels of the three users. If the three human labels do not agree upon a maturity level, we do not consider the App. After majority voting, we ob-tained 441 Apps that have agreed human labels. We also predict the maturity levels for these Apps using our learned AAMR model.

Table 5 shows the Precision, Recall, and F value of human labelling (HL), developer report (DR), and our proposed automatic method AAMR. We observe that our method AAMR achieves much more accurate results than DR (14% improvement in terms of F value) and HL (38% improve-ment in terms of F value). Human users can hardly achieve satisfactory accuracy in rating maturity levels. In our exper-iments, human users only achieve 38% F values. We specu-late the reason is that developers mainly describe function-ality in App descriptions, and humans might not be able to correlate non-sensitive words to maturity levels. Moreover, DR achieves better performances than HL. We speculate the reason is that App developers have better understand-ing about their Apps and thus could provide more accurate maturity levels.

We further study how users and Android developers la-bel maturity levels incorrectly. An App is underrated if the provided maturity level is lower than its groundtruth, other-wise an App is overrated . We find that App developers are more likely to underrate the maturity levels. Specifically, around 80% of incorrect labels provided by App developers are underrated while 46% of users-provided incorrect labels are underrated. We speculate the reason is that App de-velopers underrated their Apps so that more people could become their users.
We demonstrate that feature learning (augmentation), bag-of-words features, and label correlations are all necessary, i.e., incorporating them increases the performances of our framework. Moreover, our approach substantially outper-forms previous automatic approaches and human-based man-ual labelling approaches.
We review related work on mobile App maturity rating, text analysis, and other App-related studies.
 Maturity rating: To the best of our knowledge, little re-search has been conducted for App maturity rating. Chen et al. [5] studied the severity of unreliable maturity ratings for mobile Apps on Google Play. By comparing the matu-rity ratings of such Apps on Google Play and App Store, they measured the severity of inaccurate maturity ratings of Google Play apps. The reason is that Google Play requires developers themselves to rate their own Apps and develop-ers tend to underclaim the maturity level in order to attract a boarder range of users.
 Text analysis: The bag-of-words model was first docu-mented by Harris [14] and has been widely used for docu-mentation classification [17]. Google developed the word to vector technique [23, 24], which learns a vector representa-tion for each word to capture the syntactic and semantic word relationships. The algorithm takes a text corpus as in-put and produces the word vectors as output. Fan et al. [11] demonstrated that linear SVM [7] outperforms other classi-fiers for short text classification. Kong et al. [20] predicted the permission required by mobile apps from app descrip-tions using structure feature learning technique.
 Other App-related studies: Many recent app-related work focus on security and privacy issues. Specifically, they either reveal the potential security risks in the Android plat-form [12, 3, 18] or enhance the overall Android security via retrofitting the android platform or adding more features into it [26, 10, 31]. In addition, users privacy preference can also be utilized in personalized mobile app recommen-dation [21]. These work are orthogonal to ours.
The scope of this work is to propose a framework that automatically predicts mature contents and maturity levels for mobile Apps from app descriptions. We map the ma-ture content prediction to a multi-label classification prob-lem and then use the predicted mature contents to label the maturity levels. First, we extract novel features from App descriptions using deep learning techniques by considering the semantics of words. Second, we adapt SVM to cap-ture label correlations in a multi-label setting. The exper-iment results on real-world datasets demonstrate that our approach can achieve relatively high accuracies with only App descriptions, and that our approach substantially out-performs baseline methods,.

A few interesting directions include incorporating more features from information sources such as user comments, UI screenshots, and dynamic running behaviors of Apps into our framework, which makes our system more robust to at-tacks such as app description obfuscations, as well as rating maturity levels of dynamic contents such as advertisements in Apps.
 Acknowledgment: We thank reviewers for their valuable comments.
