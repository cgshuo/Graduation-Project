 1. Introduction
In information retrieval, and other related areas, summarisation is traditionally used to create short  X  X nippets X  of retrieved documents, which are displayed in the ranked list of results. Based on these snippets, and other information such as docu-ment title, the user can then decide if the corresponding document might contain relevant information. A summary provides an overview of the textual contents of a document and thus, it facilitates the information finding process of the user ( Tombros &amp; Sanderson, 1998 ).

In structured document retrieval, it is not only documents that are returned in response to a query, but also, portions of ture of documents. Nowadays, structured document retrieval is mainly studied in the context of XML documents where the logical structure of documents is provided via the XML markup ( Lalmas &amp; Tombros, 2007 ). The logical units (e.g. sections, subsections, etc.) of documents, called elements, form a hierarchical structure in an XML document. This hierarchical struc-ture of a document can be overwhelmingly rich, hence, users need to gain an overview of the logical structure in order to find the document portion(s) that might contain the specific information they are looking for. In other words, the structure also needs to be  X  X ummarised X  and a structure summary needs to be displayed. This paper is concerned with the generation of such structure summaries.

We view text summarisation ( Nenkova &amp; McKeown, 2011 ) and structure summarisation as highly related: while a snip-pet is a selection of sentences, phrases, etc. of the textual content of a document, a structure summary is a selection of  X  elements that provides an overview of the logical structure of the document. In addition, selected elements can also provide context to one another when displayed to the user. Based on the similarity between text summarisation and structure sum-marisation, we create structure summaries using similar methods to those known in text summarisation. Structure summa-ries can then be displayed to users as tables of contents.

Traditionally, one chooses the elements to be displayed in a table of contents (ToC) by simply selecting all the sections, subsections, etc. However, we have shown in previous work that some portions of documents might be more important to a user, and thus, these portions should be made more prominent in the table of contents ( Szl X vik, 2006b ). For example, for some sections, we might need to include paragraphs in the corresponding ToC, while other sections (being unimportant or not relevant) might be completely omitted from it. The  X  X ight X  ToC should be determined automatically. The structure summarisation discussed in this paper is used to automatically determine which portions of documents are  X  X orthy X  of inclu-sion in a ToC.

In this paper, we investigate structure summarisation as a means to create overviews of the logical structure of XML doc-uments. We are interested to learn if and how structure summaries can be created automatically. At this stage, it should be noted that we are not focusing on finding a  X  X est X  way to generate structure summaries, nor do we aim at studying the impact of structure summaries on users of retrieval systems, but our focus is on investigating how structure summaries can be cre-ated automatically.

After providing background of our work (Section 2 ), we describe two methods for creating structure summaries (Section 3 ); we investigate the summaries obtained using the second method in subsequent sections. our structure summariser is trained, and we continue with the description of the evaluation methodology followed in our work three methods for training our summariser. We close with the conclusions and future work (Section 7 ). 2. Background
XML retrieval, which we consider equivalent to structured document retrieval from this point on in the paper, focuses on the logical structure of documents in a way that allows corresponding IR systems to return document portions, called ele-ments. This way the searching process becomes more focused and information within documents becomes directly acces-sible: when a user searches for information, the system can display a single element X  X  content. However, elements (i.e. ment that is retrieved as well as to one another. Also, the logical structure of the whole document is known, and this allows to display not only the content of a particular element but the logical structure of the document, sub-structure of the re-trieved element, or sections of the document that are also relevant to the user X  X  query. For example, if a section is relevant then the document it is in is also relevant to some extent (as it contains at least one relevant section). At least some of the subsections of a relevant section are also relevant as they make up the relevant content of the section. All the above elements text of one another to make an informed decision as to where to start reading a document, or where to go next if the first choice does not lead to the most relevant content. As research has showed, displaying the logical structure of the document, both when the whole document X  X  content is found relevant or when only a relatively small element is returned, is something provided in order to support users in their searching process, to provide context to retrieved elements.

Overviews of hierarchically structured information can be displayed in various ways. Several overview methods have been proposed over time, for example, TileBars ( Hearst, 1995 ) visualise frequency and distribution of terms, and Partial
Treemaps ( Gro X johann, Fuhr, Effing, &amp; Kriewel, 2002 ) present the relative relevance of elements within a document. Never-theless, highly graphical overview presentation methods have not made lasting impact yet, mostly because time is needed way to gain an overview of the document is through a table of contents (ToC). ToC-like displays are often used when the set of data is a set of documents, for example, WebTOC automatically generates a hierarchical table of contents of a web site ( Nation, 1998 ), and categories (topics) of documents are automatically determined and displayed in a hierarchical, table of contents like manner in the work by Lawrie (2003) . Results of web clustering are also often presented in a way that resem-bles tables of contents ( Carpineto, Osin  X  ski, Romano, &amp; Weiss, 2009 ).

A table of contents typically gives an overview of the logical structure of a book or article. When a ToC is not available directly for a document there is a need to create one automatically. Elements corresponding to sections and subsections that are  X  X oC-worthy X  need to be selected and their titles displayed in the ToC. For example, a ToC-worthy element cannot be too small, e.g. a paragraph containing only one short sentence is probably not ToC-worthy. Fig. 1 shows various possible ToCs for a document and illustrates how important it is to select ToC-worthy elements that are meaningful and important to display a on it, they find that the target element contains only this word as text. The section  X  X onors X  (rightmost column) might or might not be important to display in a ToC, but whether it needs to be shown may depend on several factors, such as how many honours are listed there, how detailed the listing is, and how prestigious the honours are (note that relevance to a particular information need, or query, is not considered in this work).

When selecting elements that are ToC-worthy, a decision has to be made whether a reference to that element X  X  contents should be included in the ToC. This makes ToC generation a binary classification task, i.e. an element should be classified either into the ToC-worthy class or the not-ToC-worthy class. Classification is also used similarly in traditional extractive type summarisation where units of a text, usually sentences, that are selected into the summary-worthy class form a sum-mary of the textual contents of a document. As ToC generation works very similarly to the above mentioned summarisation it can also be regarded as a kind of summarisation, i.e. that of the structure of documents.

Before discussing structure summarisation in more detail, let us consider document text summarisation first. Summari-sation of the text can be done manually, as it had been done until the middle of the last century, or automatically. Research into automatic text summarisation dates back to the 1950s ( Luhn, 1958 ). Summaries that are extracts (i.e. not abstracts where the summary sentences are reconstructed grammatically) are usually created by assigning some scores to units that are to be extracted. Units are usually sentences or a series of adjacent words and expressions. They receive scores based on various unit features, such as length, location, and number of query words. The individual scores are then combined into a unit score and this score determines whether a unit is to be included in the document summary. A known and successful method for classifying sentences into summary-worthy and not-summary-worthy classes is presented in Kupiec, Pedersen, and Chen (1995) .

The structure of single documents can also be summarised, and this type of summarisation is the focus of this paper. Sum-marising the document structure is often done manually resulting in static tables of contents. For example, someone deter-mines that it is sections and subsections that should be in the ToC, and this rule is applied no matter how long a document is, how rich and deep logical structure it has, etc. As manual textual summarisation evolved into automatic summarisation in the middle of the last century, ToC creation should also be done automatically. The static nature of manually created ToCs, or  X  more precisely  X  the vague definition of what should be in a ToC (i.e. is it sections and sub-sections to be included or sec-
Larsen, &amp; Tombros, 2007 ), in the context of XML retrieval ( Szl X vik, 2006b ). We also found in our study that a ToC should reflect the user X  X  query and that it is not enough to determine ToC-worthiness only based on type (e.g. section, paragraph) of an XML element but other features, such as content length and depth in the structure, need also be considered. In other words, we found that there is a need for automatically identifying ToC-worthy elements, and for dynamically generating ta-bles of contents for single documents.

Using text extraction methods as a basis, we proposed a ToC generation method in Szl X vik, Tombros, and Lalmas (2007) , where ToC-worthiness is determined by a score that is a linear combination of feature scores of the element. Scores are given based on element length, depth in the structure, and relevance to the query, and if the score is above a certain threshold the element X  X  title or label is displayed in the document X  X  ToC. As determined through a user study reported in the same paper, the importance of the relevance feature is high when creating tables of contents, thus the ToC is expected to be query-based.
However, other features are also important in ToC generation, and so, features such as element length or depth cannot be ignored. In the work above, the threshold of ToC-worthiness and the individual weights of features were determined by the users themselves, a method which provided information about the importance of various features, including relevance.
The aim of our current work is to determine these weights automatically. We also believe that several other features can also play important roles in determining whether an element is ToC-worthy and so the quality of automatic ToCs could be improved.

The following sections discuss how ToCs can be generated automatically (Section 3.1 ), which features might play impor-tant roles in structure summarisation (Section 3.2 ), how a structure summariser can be trained (Section 4 ), and how the quality of summaries can be measured using objective methods (Section 5 ). The quality of structure summaries will be mea-sured against manually created ToC-worthy element sets. 3. Structure summarisation
In this section, we describe a basic structure summarisation method, and we propose a probabilistic structure summari-sation method (Section 3.1 ). We used the former in Szl X vik et al. (2007) to explore what elements should be displayed in the tables of contents, i.e. what are the properties of ToC-worthy elements. Based on the outcomes of this earlier approach, in this paper we propose a method that is the adaptation of the method by Kupiec et al. (1995) , a successful probabilistic text summarisation method. We also introduce the set of features used in the probabilistic structure summariser (Section 3.2 ). 3.1. Structure summarisation methods
We view structure summarisation to be closely related to extractive type text summarisation ( Gupta &amp; Lehal, 2010 ). Fol-lowing this view, we assume that methods used in text summarisation can also be used to select XML elements that are later included in the tables of contents.

In Szl X vik et al. (2007) , we presented a method based on early text summarisation methods, where the scores of individ-ual features of an element are combined linearly (Eq. (1) ).
 where S ( e ) denotes the overall score of element e , F is the set of features, W ( f ) is the weight of feature f and S score that is given to element e based on feature f .

For the above method to work, one needs users to determine the weights, and also users are those who should determine a threshold value that is used to generate a score cut-off value. Elements with scores above this value are considered ToC-worthy while others are excluded from the ToC of the document. The above approach also requires an assumption, that is, if a subsection is found ToC-worthy then its parent section has to be ToC-worthy, too, no matter what its score is. Asking users to manually set weights is not an ideal solution, and the parent X  X hild ToC-worthiness assumption might also not be entirely valid. One of the main aims of the work presented in this paper is to overcome these shortcomings.

The method proposed in this paper uses probabilistic element classification to extract the best elements that are worth including in a structure summary (ToC). Our method is based on the fundamental text summarisation method introduced by
Kupiec et al. (1995) which uses a probabilistic framework to extract summary-worthy sentences in order to create text sum-maries of documents.

Our probabilistic structure summarisation method uses (Na X ve) Bayesian classification as follows. For each XML element e , the probability that it is included in a structural summary T given k features f pressed using Bayes X  rule as shown in Eq. (2) : where P ( e 2 T ) denotes the probability that element e is ToC-worthy, P ( f being observed given that element e is ToC-worthy, and P ( f The Na X ve Bayes assumption is then used, which assumes that the features are statistically independent with respect to
ToC-worthiness (Eq. (3) ). Note that although this assumption is clearly not accurate in many applications, including structure summarisation, the classification method works well, often outperforming more sophisticated classifiers on many datasets ( Witten, Frank, &amp; Hall, 2011 ).

P ( e 2 T ) has the same value for each element. P ( e 2 T ), as well as P ( f observed given that element e is ToC-worthy, can be estimated directly from a training set. P ( f be estimated because of the classification method used (see below in Eq. (4) ). The probability estimation can be done by counting feature occurrences. The Bayesian classification function assigns a score for each element e which can be used to select elements for inclusion in a structural summary as described below.

The element selection (classification) function is shown in Eq. (4) . With the help of this function we can calculate whether the element is more likely to be ToC-worthy (ToC-worthy is one of the classes) or not-ToC-worthy (the other class of the higher than zero, the element is more likely to be ToC-worthy than not-ToC-worthy, thus element e is included in the table of contents.
 where P ( e R T ) denotes the probability that element e is not-ToC-worthy, and b served for element e . The use of logarithm makes it possible to express Eq. (4) using sums (see Eq. (5) ), which prevents rep-resentational issues potentially arising from multiplying very low fractions.

The above method allows that a user, to whom a ToC will be displayed, does not need to set weights and threshold values manually. Further advantages of this method are that, in addition to often being a machine learning algorithm to try first, it does not require a number of classifier parameters to be set/tuned (unlike other classification methods, such as decision trees, and support vector machines), and the model it produces is easy to interpret (unlike a multi-layer neural network, for example). Nonetheless, other methods could be used, which we leave to future work.

To build models, the method requires a set of features and a set of training data. The following subsection describes the feature set considered in our work. 3.2. Features
In this section we present the set of features that is used in our work, together with the justification for their choice, as well as the chosen bins for each feature. We do not look at query-based features because we are focusing on generating ta-bles of contents that are generally useful. Query-based features (such as relevance Szl X vik et al., 2007 ) can be added to the list and their effect investigated in the future. The list of chosen features is the following:
The items of the above list were chosen heuristically, based on personal experience with the used document collection (described in Section 4.1 ) and with users of IR and summarisation experiments ( Malik, Klas, Fuhr, Larsen, &amp; Tombros, 2006; Szl X vik et al., 2007 ). Also, an attempt was made to capture as different aspects of ToC-worthiness as possible. The cor-relation matrix of our attributes ( Table 1 ) shows that, indeed, there tends to be low correlation between attributes, which indicates that using the Na X ve Bayes method to build structure summaries was appropriate.

There are several other types of features that could be investigated (such as query-based features, link information within an element X  X  content, etc.) which we leave to future work. In the next sections, we describe how to estimate the various probabilities described previously in this paper, and then we describe the evaluation methodology followed in our work. 4. Training the structure summariser
In this section we introduce the document collection used, and discuss how our structure summariser can be trained. 4.1. Document collection
There are several XML document collections that can be considered for the ToC generation task. The most frequently used collections in structured document retrieval experiments are the IEEE, Wikipedia and Lonely Planet collections. These can also be used when investigating ToC generation. The logical structure of these collections are relatively similar to one an-other. For example, when investigating the usefulness of three element features in structure summarisation we found that the Wikipedia 2 articles. The collection was chosen because it has been used more recently in XML retrieval research (in the context of INEX Fuhr, Kamps, Lalmas, &amp; Trotman, 2007 ), and also, because the documents are accompanied by retrieval result sets (runs) and relevance assessments, which are used in the evaluation of our structure summaries (see next section). The doc-ument collection consists of the full-texts, marked-up in XML, of 659,388 articles of the Wikipedia project, and totalling more than 60 GB (4.6 GB without images) and 30 million in number of elements. sets in relation to the documents of the above collection. 4.2. Training data
According to the summarisation method proposed in this paper, after the set of features is selected, their weights need to be determined. The weights can be expressed in terms of probabilities, as described in Section 3.1 . These weights are deter-mined in an automatic manner through training. Sections 4.2.1, 4.2.2 and 4.2.3 describe three sets of training data that are used to train the structure summariser. 4.2.1. Manually created ToCs
Ideally, a training set for structure summarisation would consist of a series of structure summaries. For the Wikipedia collection, however, such tables of contents were not available, or when they were they consisted of elements selected by their type or depth only (We will use such ToCs as baselines for our experiments). To be able to investigate the effectiveness of several features and feature combinations, a set of example ToCs needs to be created. Therefore, we recruited 25 users with various levels of computer science experience. Their task was to select ToC-worthy elements for up to 20 documents.
These elements were used as positive examples of ToC-worthiness, while other elements from the same document were con-sidered as negative examples. Documents were randomly selected from the collection and assigned to each user. The inter-face shown in Fig. 2 was used. Through this experiment, we acquired ToC-worthy element sets for 322 documents. The number comes from that fact that users were allowed to quit the experiment at any time for any reason. For further details of this experiment, including an analysis of agreement levels between users, see ( Szl X vik, 2008 , chap. 6).

We would like to emphasise that user involvement here is different from that described in Szl X vik et al. (2007) : here we use users only to create a data set for training and evaluation, and we do not ask every individual who uses the summariser to set some weights manually. Once the data set is used for training a particular summariser, no further user involvement is needed, unless one incorporates query-based or user-activity (e.g. click) based features into the summariser, which are be-yond the scope of this paper.

Creating a training data set manually is an expensive and time consuming task. Also, the number of 322 ToCs (that would need to be used both for training is testing) is relatively low. We therefore look at alternative data sets with more documents to train the summariser. We discuss these in Sections 4.2.2 and 4.2.3 . In our work, the manually created ToCs are also used to evaluate the  X  X  X uality X  X  of these alternative sets. The sizes of training data sets are shown in Table 2 . 4.2.2. Creating ToCs using retrieval runs In our work, we assume that the averages of properties of relevant elements over a number of queries (in IR) can predict
ToC-worthiness (in structure summarisation). For example, in relation to the length feature, if a high portion of relevant ele-ments (over a number of queries) are longer than 10,000 characters, then an arbitrary element of an arbitrary document from the collection is more likely to be ToC-worthy if its textual content is longer than 10,000 characters. Based on this assump-tion, XML elements that are returned in high quality retrieval results can be used to estimate ToC-worthiness, i.e. to train a structure summariser. Retrieval runs were chosen as an alternative training data set because, unlike example ToCs, a high number of retrieval runs are available for the Wikipedia collection.

The training method described in this subsection takes the INEX 2006  X  X  X elevant In Context X  X  retrieval result set X  X  high quality runs as training examples (containing 7438 documents) ( Kamps, Lalmas, &amp; Pehcevski, 2007 ). The INEX Relevant In
Context task  X  X  X equired systems to return for each article an unranked set of non-overlapping elements, covering the relevant ments for each document, which is very similar to sets of example ToC-worthy elements. Elements listed in chosen runs are considered to be positive examples of ToC-worthiness for the corresponding documents, while not listed elements from the same document are treated as not-ToC-worthy. Non-overlapping elements in the data set are not considered to be an issue because of the training method and the number of documents in the set. High quality runs, that received the highest evaluation scores at INEX, were considered for training in order to maximise the quality of generated ToCs. 4.2.3. Creating ToCs using IR relevance assessments
Another alternative training set to the manually created ToCs, and that presented in the previous section, is obtained by analysing IR relevance assessments in which element relevance values (in response to a query) are determined by human assessors. To follow the assumption introduced in Section 4.2.2 , XML elements that are assessed relevant at INEX can be used to estimate ToC-worthiness.

The assessments set used is that of INEX 2006, where documents for 114 topics have been assessed. This gives 5460 doc-uments with relevant elements.

Using the above three training data sets, i.e. the manual ToC set, retrieval run set, and assessment set, the probabilities shown in Tables 3 and 4 can be obtained. As Table 3 shows, using the run training, an average of 2.35% of a document X  X  ele-ments are ToC-worthy. This number is lower than that of 8.16% we found previously in Szl X vik (2006b) , while the corre-sponding values for the assessment and manual ToC trainings are closer to 8.16%. The above low number might indicate that run training might result in ToCs with low recall values which we will examine in Section 6 .

The other training probabilities are shown in Table 4 , where f bold will be explained in Section 6.2 .) 5. Evaluation
In this section, we describe the evaluation methods adopted in this paper for evaluating structure summariser versions, as well as the evaluation measures used. 5.1. Evaluation methods
For training using retrieval runs (Section 4.2.2 ) and relevance assessments (Section 4.2.3 ) the holdout method is used while cross-validation is employed for the training with manual ToC sets (Section 4.2.1 )( Kohavi, 1995 ). The holdout method involves using one set of documents for training and another set for testing or evaluation. The data set used for evaluation is the set of the 322 manually created ToCs introduced earlier. The document sets used both in training by run and assessment data are disjoint from the set of documents used for evaluation, to avoid any unwanted effect of a document being in both sets.

As the training and evaluation data sets are the same in case of summarisation using manual ToC training (due to the set X  X  high creation costs), another evaluation methodology should be followed. To maximise efficiency, robustness and reliability, the k -fold cross validation method ( Kohavi, 1995 ) is used. This method splits the document set into two parts, uses one part for training and the other for evaluation, then the initial document set is split again and the newly obtained two parts are used as previously. This procedure is repeated k times and the average of evaluation results from each  X  X old X  are averaged to obtain overall evaluation scores. Research shows that the choice of k = 10 gives one of the most reliable results ( Kohavi, 1996 ), hence k = 10 is used in this paper as well. 5.2. Evaluation measures
To evaluate structure summarisation results (i.e. ToC-worthy elements selected for XML documents), the measures of re-call and precision are used. Recall and precision are widely used in text summarisation ( Mani, 2001 ), and  X  because of the extractive nature of our summariser  X  adopting content based text summarisation evaluation measures such as ROUGE ( Lin, 2004 ) is not needed. To calculate recall and precision values, macro evaluation is used: where N is the number of documents whose ToCs are used in evaluation, Selected are selected by the summariser being evaluated and ToCworthy by participants of manual structure summary building.

It is often desirable to use only one number to describe the performance of a system. This way, various methods can di-rectly be compared and a ranked order easily obtained. Thus, results by the F on recall, are also reported in addition to recall and precision values.

This measure is chosen because we found that user would rather tolerate not-ToC-worthy elements displayed (and pos-sibly ignores them) than accept if important (i.e. ToC-worthy) elements are omitted from the structural overview ( Szl X vik, 2008 , chap. 4). This finding is a clear indication that recall should be favoured over precision. 6. Results and discussion
This section discusses the evaluation results of our proposed structure summarisation method. First, we introduce two baseline functions that allow us to compare our summariser with static ways of building ToCs, and present results based on them. Then we provide structure summarisation results that consider features individually, which is followed by results and their discussions obtained when features are combined and various training methods are used. 6.1. Baseline results In order to compare our results with traditional ToC creation practice, we defined two baseline functions. The first, named
DepthMax3 , considers a situation in which every element up to depth level three deeper as not-ToC-worthy. This function would produce an exactly three level deep ToC. The other baseline function, named SectionsAndArticle , identifies section and article elements as ToC-worthy, and every other type of element as not-ToC-worthy.
The assumption behind it is, naturally, that it is only sections and the root element (a necessity with XML documents) that should be shown in a table of contents.

The results based on the baseline functions are shown in Table 5 . It is clear that a fixed depth ToC brings unacceptably low precision, showing that, in many cases, deeper elements are also ToC-worthy, or elements not deep in the hierarchy are not-
ToC-worthy. Also, a sections-and-articles-only approach produces considerably low recall, despite high precision scores. This shows that users indeed find sections and articles ToC-worthy, but several other types of elements should also be displayed in a ToC, as indicated by a relatively low recall score. Furthermore, subsequent subsections of this paper will demonstrate that our proposed method can easily outperform these two baselines in terms of the F 6.2. Individual features
Individual features generally do not perform particularly well. Similarly to text summarisation (and other areas where learning is used), combinations of features rather than individual features, should be used to yield better performance ( Edmundson, 1969; Kupiec et al., 1995 ). However, it is still worth studying individual features as their discrimination power might suggest which features are more important than others, which ones are more worth selecting for a summariser that uses feature combinations.
 The estimated probability values of various features and bins are shown in Table 4 where some values are marked bold.
The features corresponding to the marked numbers are those that can produce non-empty ToCs on their own, i.e. their dis-crimination power is high enough to allow at least some of the elements to be classified as ToC-worthy. The ratio of the prob-ability of ToC-worthiness ( P ( f ij j e 2 T )) to the probability of non-ToC-worthiness ( P ( f generally low ratio of ToC-worthy elements ( Table 3 ) in the training set. As Table 4 shows, only few of the features can pro-duce non-empty ToCs on their own. For instance, when using run training ( Table 4 , columns 2 X 3, see features whose corre-sponding probability values are marked bold), an element can only be ToC-worthy if it is at depth level one or it has no siblings. This would, most of the time, return the article element when the any of these two features are used. Returning the article only can be acceptable for very short documents but not for all documents in the collection, as only one item in the ToC cannot really be considered as an overview of the logical structure. With respect to assessment training, an ele-ment can also be ToC-worthy when its textual content is longer than 10,000 characters ( Table 4 , middle columns). With elements or its corresponding text is longer than 1000 characters.

The evaluation results when using individual features only are shown in Table 6 , where the IDs of the summariser ver-sions are made up of two character groups as follows: the training used is denoted by either R (run), A (assessment) or M (manual); the second group of characters identifies the used features ( D  X  depth, Ty  X  type, Se  X  sequence number, L  X  length, of text they probably want that part of the document mentioned in the corresponding ToC. For the other training types, title information is not discriminative enough to classify any element as ToC-worthy, however it might still be a useful feature in combination with others, which is also going to be examined in the next section. 6.3. Feature combinations and training methods
This section presents and discusses summariser evaluation results when combinations of features are considered. We also discuss the effectiveness of the three training methods used. The evaluation results are presented in Table 7 .
The top 15 summariser versions with respect to recall are shown in Table 7 a, together with their recall scores. The best summariser in this sense is A _ DTyLSeTiCG which is the summariser version that is trained by retrieval assessments (hence mariser ID). Table 7 a shows that to obtain high recall not less than four features need to be incorporated into the summar-which shows that these features are important to use if the goal is to achieve high recall. As we can also see, the assessment trainings (ID-s starting with an A in Table 7 a) usually outperform the manual and run trainings for recall. This shows that manual ToCs training can indeed be substituted with another type of training if high recall is to be achieved. The top 15 summariser versions with respect to precision are listed in Table 7 b, where we can observe the following:
Firstly, none of the training methods used (denoted by R , A , M , respectively) dominate the list of top 15 summarisers which shows their comparability, i.e. training with assessments and runs can produce comparable results to those obtained by training with manual ToCs. Secondly, all the summariser versions in the top 15 use at most four features. For example, the top summariser with respect to precision uses the depth feature ( D ), the sequence number ( Se ), title information ( Ti ) and number of sibling elements ( Si ). This shows that to achieve high precision, the number of features used should be low. Also, we can see in the second column of Table 7 b that eleven summariser versions have exactly the same precision values. As further examination showed that their corresponding recall values are also the same, it is suspected that the out-puts of these summarisers are exactly the same ToC-worthy element sets. In addition to these eleven summarisers, the in our feature set. Table 7 b further shows that the top four summarisers also use the depth ( D ) feature. Thus, in order to achieve high precision, the title and depth features should be included in a summariser, and an additional one or two other features might be also used.

Table 7 c shows the top 15 summarisers with respect to the F letters of the ID-s in Table 7 c show that the effectiveness of the three used training methods are comparable. This means that training with manual ToCs can be generally substituted by training with assessments or runs, hence, there is no need to cre-ate expensive data sets specifically for structure summarisation but other, existing, data sets (from IR) can be used.
Our work shows that, generally, individual features do not perform particularly well, which is in accordance with findings i.e. the type and length features for recall-oriented summarisers, and the title and depth features for precision-oriented summarisers.

We have also found that for high recall, more than four features need to be used, however, to obtain high precision scores, not more than four features should be incorporated into the summariser. The number and particular choice of features to be used, therefore, depends on what is to be emphasised (i.e. precision or recall). In this work, we have also used the F for evaluation which combines recall and precision. Accordingly, the top summariser versions with respect to the F ( Table 7 c) use between four and seven features (combination of what we have found desirable for high precision and recall), depth, type, and length features (discussed in previous paragraphs). In addition, there is one feature that appears to be increasingly important when the F 2 evaluation measure is used: the number of siblings ( Si ). The number of siblings appears in 12 out of 15 top summarisers in Table 7 c( F 2 measure), as opposed to lower occurrences in Table 7 a and b (recall and pre-cision, respectively). It seems that the siblings feature captures something that leads to high evaluation scores when recall and precision are combined. The above results show that, depending on how effectiveness is measured, or what combina-tions of measures are used in evaluation, different features might emerge as highly effective ones. Hence, it is important to determine the purpose of the summariser, i.e. what is meant to be  X  X  X ffectiveness X  X . This could be determined via user stud-ies which we consider for future work.

To examine various features and their effects, we propose that several data sets from IR can be (re)used. We have found that the effectiveness of summarisers trained using retrieval runs and assessments (which are available in IR test collections) are comparable to that of summarisers trained using manual ToC sets (which are expensive to create). With the use of al-ready existing data sets, the investigation of various structure summarisers can be made quicker and cheaper. 7. Conclusions and future work
In this paper, we have investigated a method for the automatic summarisation of the logical structure of documents. The proposed summariser selects elements of XML documents that are worth displaying in tables of contents. We have presented and studied several element features that have been used in the summariser to identify ToC-worthy elements. We have also shown that automatically generated structure summaries easily outperform traditional ways of creating them (baselines).
Despite the fact that, as a feature, relevance to a query is considered highly important by users ( Szl X vik et al., 2007; Kazai &amp; Trotman, 2007 ), in this work, we have considered query-independent features only. We believe that adding query-depen-dent features would significantly improve the quality and usefulness of ToCs in the searching process of users; however, we also believe that establishing which query-independent features play important roles in the identification of ToC-worthy ele-ments has also high importance. We propose that the use of query-independent features should serve as basis for structure summarisation, while query-dependent (relevance-oriented) features should be used to refine the ToCs and tailor them to the user X  X  information need.

We have also shown in this paper, that it is possible to effectively (re)use several data sets from information retrieval evaluation for training a structure summariser. The quality of summarisers trained with the alternative data sets have matched the quality of those trained using the data set created specifically for the purpose of structure summarisation. With data set containing example ToCs can be used for evaluation purposes only.

A possible future step in this research can be the thorough investigation of the impact of structure summarisation in an information seeking setting. We believe that the appropriate use (display) of structure summaries will enhance user expe-rience, bring increased user satisfaction, and users will also find relevant content more efficiently as well as effectively. Acknowledgements This work was partly funded by the Nuffield Foundation (Grant NAL/01081/G) and the DELOS Network of Excellence in
Digital Libraries, to which we are very grateful. We are especially thankful to participants of the manual ToC creation study for their precious time and great effort. We would also like to acknowledge the reviewers for their useful comments that have helped us improving the paper.
 References
