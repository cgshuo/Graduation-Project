 Traditional boosting algorithms for the ranking problems usually employ the pairwise approach and convert the document rating preference into a binary-value label, like RankBoost. However, such a pairwise approach ignores the information about the magnitude of preference in the learning process. In this paper, we present the directed distance f unction (DDF) as a substitute for binary labels in pairwise appro ach to preserve the magnitude of preference and propose a new boosting algorithm called MPBoost, which applies GentleB oost optimization and directly incorporates DDF into the exponential loss function. We give the boundedness property of MPBoost through theoretic analysis. Experimental results demonstrate that MPBoost not only leads to better NDCG accuracy as compared to state-of-the-art ranking solutions in both public and comme rcial datasets, but also has good properties of avoiding the overfitting problem in the task of learning ranking functions. H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval; I.2.6[ Artificial Intelligence ]: Learning Algorithms, Performance, Experimentation pairwise-preference Boosting [9, 10] is one of the state-of-the-art algorithms in machine learning. Based on boosting, many algorithms have been proposed to solve supervised learning tasks, including [10] for filtering. All of these have theoretically and empirically demonstrated that boosting algor ithm and its variants have excellent advantages in terms of convergence of loss function, low generalization error, little re striction on the form of weak learners, etc. As an interesting and significan t application, boosting is also successfully employed in the learning to rank problem for Web search, where a set of queries Q , and a set of documents D each query qQ  X  are given. For each document of a given query, document. Current popular strategy to apply boosting for ranking, information as the binary classifi cation data, which belongs to the document x 1 is rated higher than another document x 2 Otherwise, ( x 1 , x 2 ) is labeled as a negative instance. However, this kind of pairwise transformati on from ranking to classification neglects the scale or magnitude of the difference between rating richer representations can be achieved if the magnitude of rating differences can be considered in boosting algorithm. Therefore, designing a new boosting algorithm for ranking with consideration of the magnitude of difference between rating pairs is very desirable. Some attempts have been made to address similar issues. In the RankBoost algorithm [7, 8], the magnitude issue is mentioned in *This work was done when the firs t author was visiting Microsoft Research Asia. leveraged multiple hyperplanes to preserve the magnitude of rating differences on the basis of the RankSVM algorithm [13, 17] and demonstrated the importance of preference magnitude. More recently, [4] analyzed the stability bounds of magnitude-preserving loss functions for ge neralization error and proposed two magnitude-preserving ra nking algorithms, MPRank and SVRank, with reports of the im provement on mis-ordering loss. In this paper, we observe that it is fairly straightforward to apply the preference magnitude into the exponential loss function of boosting to improve the accuracy of ranking. To preserve the magnitude of rating differences, we propose directed distance approach to ranking. The exact form of DDF can vary with Thus, we present three kinds of DDFs and list the appropriate scope of usage. Then, on the basis of DDF, we propose a novel ranking algorithm, MPBoost, ba sed on GentleBoost [10]. It directly leverages the exponentia l loss function with DDF as the substitute for binary labels, which makes this algorithm suitable for magnitude-preserving ranking. We also prove a theorem about public and one commercial datasets all illustrate that MPBoost with DDF can significantly outperform traditional pairwise ranking algorithms as well as the state-of-the-art ranking methods like ListNet. Also, the experiment demonstrates that the application of DDF can lead MPBoost to avoid the overfitting problem. concept of magnitude-preserving labels: directed distance function (DDF), with three exempl ary functions. In Section 4, we propose the MPBoost algorithm, which applies exponential loss functions with DDF. We report the experiment results in Section 5 and conclude the paper in Section 6. information retrieval research. One of the main approaches to pairwise approach, the learning to rank task is transformed into a binary classification task base d on document pairs (whether the first document or the second s hould be ranked first given a query). [13, 17] proposed using the SVM techniques to build the classification model, which is referred to as RankSVM. [7, 8] AdaBoost algorithm. [1] also adopt ed the approach and developed a method called RankNet, which le verages the cross entropy as neural network model. Recently, the concept of magnitude-preserving ranking was introduced in [4, 21]. [21] leveraged multiple hyperplanes to preserve the magnitude of rating differences on the basis of the RankSVM and proposed a method called  X  X ultiple Hyperplane Ranker X  (MHR). In [4], the authors analyzed the stability bounds of magnitude-preserving loss func tions for generalization error. Based on the results, they proposed two algorithms, MPRank and SVRank and reported empirical results which showed improvements on mis-ordering loss. However, the loss functions in [4] are regularization-based, and the  X  -admissibility requirement of cost function limits the forms of functions to some extent. For example, exponential co st functions can hardly meet the requirement with a small constant  X  . AdaBoost [9] is a state-of-the-art classification algorithm which stage-wise combines a number of weak learners and generates a strong hypothesis. [10, 22] analyzed the theoretical advantage of the boosting algorithm from the aspect of margin and VC-boosting algorithms. In real appli cation, GentleBoost [10] is a variation of boosting algorithm whic h employs the Newton step to minimize the exponential loss function. It has been shown in [10] that GentleBoost has similar pe rformance to AdaBoost, and often outperforms the latter especially when stability is an issue. Since our work is under the pairwise ranking approach, we will rank. Next, we will propose the directed distance function to preserve the magnitude of rating difference and illustrate three examples. The pairwise approach for ranking is defined as follows in [3]. A ranking dataset includes a set of queries qQ  X  , and a set of documents for each query iq x D  X  . The associated relevance rating of document i x for query q is represented as relevance ratings are discrete and ordered, with values such as { Probably Relevant, Possibly Relevant, Not Relevant }. Furthermore, there exists a total ordering &gt; between various relevance levels, e.g. Probably Relevant &gt; Possibly Relevant &gt; Not Relevant. In this paper, we assume that numerical ratings values are available because ranking performance measurements often take ratings as a component in calculation. Also, for the learning tasks, documents are repr esented by a vector of feature weights obtained by some query-document function  X  . For instance, the document (, ) ( , ) iqiqi qid qx x x x  X  == ......, . The goal of the learning to rank procedure in the pairwise document space such that: () () qi qj qi qj i.e. if document should be larger than the pairwise-preference learning a nd the binary classification: a classifier can be introduced to maintain the given preference relations on the left of inequality (1). Thus, for pairwise approach, we define a preference set containing document pairs for each query q : 
Sxxyrr =  X  , where the binary label qij y satisfies: Although the binary label defined in (2) is suitable for leveraging well-studied classification tools, this transformation from ranking to classification also loses cons iderable amount of information. For example, mis-ranking two doc uments with ratings 5 and 1 should receive more  X  X unishment X  than mis-ranking two documents with ratings 5 and 4, because the former will cause larger decrease in ranking measurements like NDCG [16]. In this sense, the binary label constructe d by (2) only reflects the desired order of two documents, omitting the useful information hidden in the magnitude of rating differences. On the other hand, far richer representations can be achieved if the magnitude of rating differences is considered. Therefore, we need to modify the traditional definition of labels in (2) to preserve the magnitude information. Specifically, we define the label to be the directed distance from a placing a document with rating r a before another document with rating r b . As shown in Figure 1, although the concrete values of directed distances can vary, the magnitude of rating differences should be preserved, as well as the advantage of placing high-rated documents in front. Now, we formally present the requirements (3) and (4) on the concept of directed distance function (DDF) : (, ) dist  X   X  , which subsequent analysis: sgn( ( , )) sgn( ) ij i j where and the modified preference set is (5), which will be leveraged into the exponential loss function in our boosting approach for learning to rank. Compared to  X  -admissibility presented in Definition 2 of [4], the basic requirements on DDF allow ma ny more candidate functions in real application. For instance, we will propose three possible directed distance functions which have performed superiorly in the empirical analysis. An intuitive integration of prefer ence magnitude into DDF is the linear function based on the difference of the preference difference. We call this functi on as Linear Directed Distance (LDD) and show its equation in formula (6). The coefficient used to meet the requirements (3) and (4). (, ) ( ) ij i j dist r r r r LDD is an easy and simple function to consider the preference magnitude. But when the numerical difference between ratings is Figure 1. Three ratings abc rrr &gt;&gt; , where the directed values of the distances can vary, but it should follow that: differences. 2. (, ) 0 ca of placing documents with higher ratings in front. Figure 2. Curves of LDD, LOGDD and LOGITDD under different values of rating differences. The parameters are  X  = , 3  X  = and 0.5  X  = . the experiment. In this case, functions that can smooth large rating differences should help, as proposed in Section 3.2.2 and 3.2.3. Logarithmic Directed Distance function (7) takes the form of logarithms, in order to smooth the grading difference between ratings. Like LDD, LOGDD utili zes the positive parameter Compared with LDD, LOGDD can be utilized when the rating uniform. It is also obvious to note that due to the logarithmic nature, the output range of L OGDD will even tually be smaller than that of LDD and be more suitable to apply in the exponential loss function. On the other hand, LOGDD is much smoother than LDD in term of the output value, as shown in Figure 2. Logistic Directed Distance functi on (8) leverages the well-studied logistic function. Like LDD and LOGDD, LOGITDD applies the positive parameter  X  . Compared with LDD and LOGDD, the output range of LOGITDD is always in (1, 0.5] [0.5,1)  X  X  X   X  , which makes LOGITDD be smoother than both th e above functions. It follows that it X  X  easier to tune the related parameters when considering the preference magnitude. Also note that when applying LOGITDD, the only parameter  X  should be carefully chosen to make dist r r non-negligibly different for disparate values of || rr  X  . For example, if the grading differences between ratings are usually large, a relatively small  X  should be selected to avoid parameter in our experiments. In summary, the introduction of directed distance function is to substitute conventional binary-val ue labels and assign magnitude-preserving property to the ranking algorithm like the one we will introduce in Section 4. Following the introduction to DDF in Section 3, we will present a novel boosting algorithm, MPBoost. Specifically, we apply the GentleBoost approach [10] to define loss function and design optimization methods. For convenience, we combine the preference sets over all queries: 
SS = U and define the index set I over S : 
Iijxx rr = X  . Note that for sake of simplicity, we will sometimes omit the query subscript in the following discussion, i.e. whenever ( , ) ij assume that the documents i x and j x belong to the same query q , and qi qj rr  X  . Similarly, ( , ) ij dist r r means ( , ) Next, to leverage DDF in the MP Boost algorithm, we require that the condition parameters within the applied directed distance functions, like  X  and  X  in LDD, LOGDD and LOGI TDD. This condition will be utilized in the following analysis. Now, the loss function MPBoost employs is as follows: where the strong hypothesis ( ) Fx is a score function based on an additive model. In other words, F is initially set as 0. Then, in the i Algorithm 1 MPBoost algorithm for generating a ranking function. Output : ranking function ( ) Fx 2: Generate index set {( , ) | (( , ),dist( , )) S} 4: for 1... tT = do 5: Fit the weak ranker t f , such that: 7: end for 8: Output the final ranking function after m rounds, In order to minimize loss function, MPBoost needs to find the best weak learner in each round. For example, if the current hypothesis is F , and the next weak learner to be added is f , then the additive loss function should be: ()
JF f e  X  X  X  X  +  X   X  X  X  + 1 ([ ( , ) ( ( ) ( ))] 1)) where we apply second-order Taylor approximation and the condition (9). Thus, we can determine the best f to be added to F by minimizing ( ) J Ff + , equivalent to minimizing a weighed squared loss: document pair. Now, we formally present our algorithm: MPBoost, which is based on GentleBoost [10], with changes in the loss function calculation and weight modification. In MPBoost, the initial weight of each document pair is uniform. During each iteration, a weak ranker is chosen to minimize (12). Then, the weights are updated with help of the normalizer t Z : The final ranking function is the summation over all weak rankers. The procedure details of MPBoos t are presented in Algorithm 1. Note that MPBoost bears some resemblance to RankBoost, which applies the optimization scheme of AdaBoost. In the RankBoost framework, a bound on the ranking loss is offered in Theorem 1 [8]. Now, with the employment of GentleBoost approach and property of RankBoost. We forma lize this the following theorem. Theorem 1. Assuming the notation of Algorithm 1, the normalized ranking loss (mis-ordering) of F is bounded: where [[ ]]  X  is defined to be 1 if predicate  X  holds and 0 otherwise. with the introduction of DDF. rr  X  . Thus, In view of the bound established in Theorem 1, we are guaranteed to produce a combined ranking with low ranking loss if on each Actually, minimizing () wse J f (12) is exactly minimizing a second-order Taylor approximation of t Z (13), when (9) is satisfied: Therefore, in theory, the MP Boost algorithm can achieve low mis-ordering loss (16) via stage-wise gradient descent method. And it has been proved that minimizing the number of mis-orderings is equivalent to maximizing a lower-bound on ranking performance metrics [6]. The empirical analysis in Section 5 also substantiates the result. We utilized three data sets in the experiments: OHSUMED [14], a benchmark data set for documen t retrieval downloadable from LETOR 3.0 [18, 19]; Web-1, a Russian web search dataset [15]; Web-2, an English web search dataset obtained from a popular search engine. OHSUMED [14] is a collection for information retrieval research. It is a subset of MEDLINE, a database on medical publications. OHSUMED contains a total of 348,556 records (out of over 7 million) from 270 medical journals during the period of 1987-1991. The fields of a record embrace title, abstract, MeSH OHSUMED, there are 106 queries, each with a number of query-document pairs, each of which described by 45 features. In OHSUMED, relevance grades are from {0, 1, 2}. We conducted experiments on each of the 5 subfolders in OHSUMED, each containing training/validation/test data. Web-1 is the public training data from  X  X nternet Mathematics normalized features of query-document pairs as well as relevance judgments made by Yandex search engine assessors. There are 97,290 query-document pairs within a total of 9,124 queries. Each query-document pair is described by 245 features. All features are either binary value from {0, 1}, or continuous values from [0, 1]. In Web-1, relevance grades are continuous values from range [0, 4], with higher values represents higher relevance. We used five-train/validation/test sets. Web-2 is from a commercial Englis h search engine. There are 50, 000 query-document pairs within a total of 467 queries. Each query-document pair is described by 1779 features. In Web-2, relevance grades are from {0, 1, 2, 3, 4}. Again, we used five-fold cross validation, with 3+1+1 splits between train/validation/test sets. In the experiment, we apply the Normalized Discounted Cumulative Gain(NDCG) [16] as the performance measure. NDCG can handle multiple levels of relevance and it favors algorithms that give higher ranks to highly relevant documents than marginally relevant one s. Furthermore, lower ranking be examined by a user. In acco rdance with these principles, computing NDCG values follow the following four steps: 1) Compute the gain of each document the list 3) Cumulate these discounted gain of the list 4) Normalize the discounted cumulative gain of the list following: list receives a NDCG score of 1. The final NDCG score is the average over all queries. In addition, in the validation pha se of our experiments, we set NDCG@5 as the criteria for selecting the best parameters. We present the ranking accuracy of MPBoost algorithm on the methods. Specifically, on OHSU MED dataset, we apply RankBoost [7, 8], ListNet [2] a nd AdaRank-NDCG [23] as the baseline methods. The measurem ent on these algorithms comes from [19]. Also, we make MPBoost with binary labels as another baseline method, which is repres ented by MPBoost.BINARY in the figures. Also in following figures, MPB oost.LDD, MPB oost.LOGDD and MPBoost.LOGITDD respectively represent the MPBoost algorithm with DDF in form (6), (7 ) and (8). These three versions of algorithms leverage the magn itude-preserving property. The parameters tuned in the experiments include  X  in (6),  X  in (8) and the number of boosting rounds T . Note that for MPBoost, the condition (9) shoul d be considered during the tuning. And we use the validation set to tune these parameters independently. In the experiments, we leverage decision stumps as the weak appendix. On OHSUMED, we conducted fi ve-fold cross validation experiments using the data split provided in LETOR. Every ranking measurement was calculated as the mean over five folders. As shown in Figure 3, the three versions of magnitude-preserving MPBoost algorithms outpe rform nearly all baselines in NDCG@1 to NDCG@10 by 1 point to 6 points gain. Although MPBoost.LDD and MPBoost.L OGDD lag behind ListNet and AdaRank-NDCG by about 0.5% in NDCG@1, MPBoost.LOGITDD consistently achieves the best NDCG accuracy across NDCG@1 to NDCG@10. Furthermore, MPBoost.BINARY, which is the MPBoost algorithm with binary NDCG@6 and NDCG@7. Thus, we leverage MPBoost.BINARY as the baseline in the following experiments. We conducted experiments on Web-1 via five-fold cross-validation, and the reported result is the average over five folders. As shown in Figure 4, the advant age of MPBoost with magnitude-preserving loss functions is not as clear as that in OHSUMED. Still, an average of 0.16% and 0.13% NDCG advantage is gained by MPBoost.LOGDD and MPBoost.LOGITDD over MPBoost.BINARY. However, MPBoost.LDD lags behind MPBoost.BINARY by an av erage of 0.24% NDCG. The experiment on Web-2 was also conducted via five-fold cross-validation, and the reported result is an average over five folders. From Figure 5, we can see that MPBoost with magnitude-preserving loss functions signifi cantly outperform the version with binary labels by an aver age of 1.5% (MPBoost.LDD), 2.2% (MPBoost.LOGDD) and 1.8%(MPBoost.LOGITDD) and MPBoost.LOGITDD performs the best in NDCG@1 and NDCG@2, while MPBoost.LOGDD achieves the best in the rest positions. In this section, we will try to present some analysis in light of the consistent and excellent experime ntal results generated from our MPBoost algorithm as compared to the state-of-the-art ranking algorithms. Firstly, the loss function (10) applied in MPBoost utilizes the concept of directed distance func tion over ratings, thus preserving experiment on three datasets, all three versions of MPBoost with DDF outperform MPBoost with binary labels. Hence, it substantiates that magnitude-preserving labels can lead the algorithm to grasp the inherent pattern in document vectors and in general yield higher performance. Secondly, MPBoost pursued th e ranking problem with quasi-GentleBoost approach. In [10], Ge ntleBoost has been empirically proved to perform better than AdaBoost, while RankBoost is constructed on the basis of Ad aBoost. Thus, MPBoost.BINARY and the other three versions of MPBoost with magnitude-preserving property consistently outperform RankBoost in OHSUMED dataset. It is interesting to observe th at among the three versions of MPBoost applying LDD, LOGDD and LOGITDD, the logarithm-based one achieved the best performance in two datasets; while the linear-based one fall behind the other two in all three datasets. The linear-based DDF, LDD, tends to output too large values for large rating grades, thus forcing the parameter  X  small values (e.g. the best  X  in the Figure 6. Average NDCG@5 on test set over 5 folds in Web-2 Dataset. For MPBoost.L DD, MPBoost.LOGDD and MPBoost.LOGITDD, the parameters  X  ,  X  and  X  are set as the one achieving the best p erformance on validation set. Web-2 dataset was in average 0.062). The consequence is that the ability to differentiate smaller rating differences degrades. The logistic-based DDF, LOGITDD, has similar problems in that the overall output interval is (1, 0.5] [0.5,1)  X  X  X   X  problem appears quite often. On the contrary, LOGDD generates moderate range of output while pr eserving the original magnitude properly. In the boosting method, overfitting is a thorny issue that needs to be handled [5]. Specifically, while the empirical error will keep decreasing in the training phase, the performance on test set may degrade as the number of training rounds increases. In our experiments, we observe that MPBoost with binary label suffers from overfitting, while MPBoost with magnitude-preserving properties perform well as training goes on. To demonstrate, in the previous experiment on Web-2, we record iterations. Figure 6 shows the av erage NDCG@5 on test set over five folders in the Web-2 dataset. As shown in the figure, MPBoost.BINARY suffers from serious overfitting problem after peaking at around the 90th iterati on. However, MPBoost.LDD, MPBoost.LOGDD and MPBoost.LOGITDD can achieve comparatively stable ranking accuracies as the number of training rounds increase and avoid the overfitting issue to some extent. We attribute this phenomenon to the incompleteness of traditional binary-label pairwise approach to ranking, combined with the distribution of different magnitude of ratings. On one hand, the calculation of metrics such as NDCG deals with the magnitude of ratings , not directly with the pairwise order. On the other hand, in the Web-2 dataset, the relevance ratings come from {0, 1, 2, 3, 4}, which give considerable impetus to place document with ratings like 3 and 4 in front. Thus, ignoring the magnitude of ratings and only retaining the relative order, bi nary labels lose a large amount of information during the transformation from ranking to classification. Thus, the traine d model deviates from correctly capturing the way to improve metrics like NDCG, and test data substantiates the incompleteness. On the contrary, the other three versions of magnitude-preserving MPBoost all apply loss functions w ith DDF. Thus, in the training phase, these three versions can learn a more suitable model to match the goal of ranking in the sense of improving NDCG. sufficient long time of training. In this paper, we propose a new approach to magnitude-preserving ranking: directed dist ance function (DDF). Compared to previous schemes [4], DDF im poses less restriction on the form of functions while retaining the magnitude of rating differences. We also present three kinds of directed distance functions: LDD, LOGDD and LOGITDD, which can be applied under different circumstances due to the output range. The parameters in these DDFs can be easily adapted to m eet requirements of different ranking algorithms. Based on DDF, we propose a ne w boosting method for ranking problem: MPBoost. MPBoost in corporates directed distance GentleBoost-like optimization. Th e ranking loss, or misordering, of MPBoost is still bounded, like RankBoost, which is based on AdaBoost. Experimental results with three datasets indicate that the MPBoost method, when combined with magnitude-preserving DDF, outperforms binary-label-b ased MPBoost and existing state-of-art approaches like RankBoost, ListNet and AdaRank-NDCG. Furthermore, MPBoos t with DDF tend to avoid overfitting in training. For future work, we plan to study the theoretical advantage in our method. We also intend to appl y mixed directed distance for different pairs of ratings to more accurately depict the magnitude issue. The work of Chenguang Zhu was supported in part by the National Basic Research Program of China Grant 2007CB807900, 2007CB807901, the National Natu ral Science Foundation of China Grant 60604033, 60553001, and the Hi-Tech research and Development Program of China Grant 2006AA10Z216.We would also express our sincere acknow ledgement to Xiaohui Wu and Teng Gao for their great help for our work. [1] Burges, C., Shaked, T., Renshaw, E., Lazier, A., Deeds,M., [2] Cao, Z., Qin, T., Liu, T., Y ., Tsai, M.-F., and Li, H. (2007). [3] Carvalho, V. R., Elsas, J. L ., Cohen, W. W., and Carbonell, [4] Cortes, C., Mohri, M., and Rastogi A. (2007). Magnitude-[5] Ditterich, T. G. (1999). An experimental comparison of three [6] Elsas, J., Carvalho, V. R., a nd Carbonell, J. G. (2008). Fast [7] Freund, Y., Iyer, R., Schapire, R.E., and Singer Y.(1998). An [8] Freund, Y., Iyer, R., Schapire, R. E., and Singer Y.(2003). [9] Freund Y. and Schapire, R. E. (1995). A decision-theoretic [10] Friedman, J., Hastie, T. and Tibshirani, R.(2000) Additive [11] F X rnkranz, J., and H X llermeier, E. (2003). Pairwise [12] He J. and Bo T. (2007). Asymmetric gradient boosting with [13] Herbrich, R., Graepel, T., and Obermayer, K. (1999). [14] Hersh, W. R., Buckley, C., Leone, T. J., and Hickam, D. H. [15] Internet Mathematics Contest 2009 training data (Learning to [16] Jarvelin, K., and Kekanainen, J. (2000). Ir evaluation [17] Joachims, T. (2002) Optimizing search engines using [18] Liu, T. Y., Qin, T., Xu, J., Xiong, W. Y., and Li, H. (2007). [19] Liu, T. Y., Zhang, R. C. (2008) Learning to Rank (LETOR) [20] Mason, L., Baxter, J., Bartlett, P. and Frean, M. (2000). [21] Qin, T., Liu, T. Y., Lai, W., Zhang, X. D., Wang, D. S., and [22] Schapire, R. E., Freund Y., Bartlett, P. L., and Lee, W. S. [23] Xu, J. and Li, H. AdaRa nk: A Boosting Algorithm for [24] Yang, P., Shan, S., Gao, W., Li, S. and Zhang, D. (2004) The weak ranker based on decision stumps, () d rx , is defined as the following: k is the serial number of the feature () d rx selects. threshold for this feature. a and 0 are the only two possible values () d rx can take. We choose 0 here because in pairwise approach, only the difference of the two values () matters. In other words, we only need ( ) ( ) calculation. To find the best () d rx , we can iterate k . When k is determined, we only have to iterate (1) n + values for  X  ...... .Thus, the only problem is to find the best a when k and  X  are determined. Notice that both () di () rx can possibly take two values, yielding 4 combinations. Therefore, we should sp lit the sum into 4 cases. Define: Suppose the best a for specific k and  X  is , k a  X  . We have: 
J fwdistrra wdistrr Consequently, in order to minimize () wse t J f , we take the partial derivative and obtain the best , k a  X  : Thus, after iterating through all possible k  X  X  and  X   X  X , we can get the minimum () wse t J f and the corresponding best parameters. 
