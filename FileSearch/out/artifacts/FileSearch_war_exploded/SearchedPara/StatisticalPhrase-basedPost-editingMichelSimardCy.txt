 The quality of machine translation (MT) is gener -ally considered insuf cient for use in the eld with-out a signicant amount of human correction. In the translation world, the term post-editing is often used to refer to the process of manually correcting MT output. While the con ventional wisdom is that post-editing MT is usually not cost-ef cient compared to full human translation, there appear to be situations where it is appropriate and even protable. Unfortu-nately , there are few reports in the literature about such experiences (but see Allen (2004) for exam-ples).

One of the characteristics of the post-editing task, as opposed to the revision of human translation for example, is its partly repetiti ve nature. Most MT systems invariably produce the same output when confronted with the same input; in particular , this means that the y tend to mak e the same mistak es over and over again, which the post-editors must correct repeatedly . Batch corrections are sometimes pos-sible when multiple occurrences of the same mis-tak e appear in the same document, but when it is repeated over several documents, or equi valently , when the output of the same machine translation system is handled by multiple post-editors, then the opportunities for factoring corrections become much more comple x. MT users typically try to reduce the post-editing load by customizing their MT sys-tems. Ho we ver, in Rule-based Machine Translation (RBMT), which still constitutes the bulk of the cur -rent commercial offering, customization is usually restricted to the development of  X user dictionaries X . Not only is this time-consuming and expensi ve, it can only x a subset of the MT system' s problems.
The adv ent of Statistical Machine Translation, and most recently phrase-based approaches (PBMT , see Marcu and Wong (2002), Koehn et al. (2003)) into the commercial arena seems to hold the promise of a solution to this problem: because the MT sys-tem learns directly from existing translations, it can be automatically customized to new domains and tasks. Ho we ver, the success of this operation cru-cially depends on the amount of training data avail-able. Moreo ver, the current state of the technology is still insuf cient for consistently producing human readable translations.

This state of affairs has prompted some to ex-amine the possibility of automating the post-editing process itself, at least as far as  X repetiti ve errors X  are concerned. Allen and Hogan (2000) sketch the out-line of such an automated post-editing (APE) sys-tem, which would automatically learn post-editing rules from a tri-par allel corpus of source, raw MT and post-edited text. Elming (2006) suggests using tranformation-based learning to automatically ac-quire error -correcting rules from such data; howe ver, the proposed method only applies to lexical choice errors. Knight and Chander (1994) also argue in fa-vor of using a separate APE module, which is then portable across multiple MT systems and language pairs, and suggest that the post-editing task could be performed using statistical machine translation tech-niques. To the best of our kno wledge, howe ver, this idea has never been implemented.
 In this paper , we explore the idea of using a PBMT system as an automated post-editor . The un-derlying intuition is simple: if we collect a paral-lel corpus of raw machine-translation output, along with its human-post-edited counterpart, we can train the system to translate from the former into the lat-ter. In section 2, we present the case study that mo-tivates our work and the associated data. In section 3, we describe the phrase-based post-editing model that we use for impro ving the output of the auto-matic translation system. In section 4, we illus-trate this on a dataset of moderate size containing job ads and their translation. With less than 500k words of training material, the phrase-based MT system already outperforms the rule-based MT base-line. Ho we ver, a phrase-based post-editing model trained on the output of that baseline outperforms both by a fairly consistent mar gin. The resulting BLEU score increases by up to 50% (relati ve) and the TER is cut by one third. 2.1 Context The Canadian government' s department of Human Resources and Social De velopment (HRSDC) main-tains a web site called Job Bank , 1 where poten-tial emplo yers can post ads for open positions in Canada. Ov er one million ads are posted on Job Bank every year , totalling more than 180 million words. By virtue of Canada' s Of cial Language Act, HRSDC is under legal obligation to post all ads in both French and English. In practice, this means that ads submitted in English must be translated into French, and vice-v ersa.

To address this task, the department has put to-gether a comple x setup, involving text databases, translation memories, machine translation and hu-man post-editing. Emplo yers submit ads to the Job Bank website by means of HTML forms containing  X free text X  data elds. Some emplo yers do period-ical postings of identical ads; the department there-fore maintains a database of pre viously posted ads, along with their translations, and new ads are sys-tematically check ed against this database. The trans-lation of one third of all ads posted on the Job Bank is actually recuperated this way. Also, emplo yers will often post ads which, while not entirely identi-cal, still contain identical sentences. The department therefore also maintains a translation memory of in-dividual sentence pairs from pre viously posted ads; another third of all text is typically found verbatim in this way.

The remaining text is submitted to machine trans-lation, and the output is post-edited by human ex-perts. Ov erall, only a third of all submitted text re-quires human interv ention. This is nevertheless very labour -intensi ve, as the department tries to ensure that ads are posted at most 24 hours after submis-sion. The Job Bank currently emplo ys as man y as 20 post-editors working full-time, most of whom are junior translators. 2.2 The Data HRSDC kindly pro vided us with a sample of data from the Job Bank . This corpus consists in a collec-tion of parallel  X blocks X  of textual data. Each block contains three parts: the source language text, as submitted by the emplo yer , its machine-translation, produced by a commercial rule-based MT system, and its nal post-edited version, as posted on the website.
The entire corpus contains less than one million words in each language. This corresponds to the data processed in less than a week by the Job Bank . Basic statistics are given in Table 1 (see Section 4.1). Most blocks contain only one sentence, but some blocks may contain man y sentences. The longest block contains 401 tok ens over several sentences. Ov erall, blocks are quite short: the median number of tok ens per source block is only 9 for French-to-English and 7 for English-to-French. As a conse-quence, no effort was made to segment the blocks further for processing.

We evaluated the quality of the Machine Transla-tion contained in the corpus using the Translation Edit Rate (TER, cf. Sno ver et al. (2006)). The TER counts the number of edit operations, including phrasal shifts, needed to change a hypothesis trans-lation into an adequate and uent sentence, and nor -malised by the length of the nal sentence. Note that this closely corresponds to the post-editing op-eration performed on the Job Bank application. This moti vates the choice of TER as the main metric in our case, although we also report BLEU scores in our experiments. Note that the emphasis of our work is on reducing the post-edition effort , which is well estimated by TER. It is not directly on quality so the question of which metric better estimates translation quality is not so rele vant here.

The global TER (over all blocks) are 58.77% for French-to-English and 53.33% for English-to-French. This means that more than half the words have to be post-edited in some way (delete / substi-tute / insert / shift). This apparently harsh result is some what mitigated by two factors.

First, the distrib ution of the block-based TER 2 sho ws a lar ge disparity in performance, cf. Figure 1. About 12% of blocks have a TER higher than 100%: this is because the TER normalises on the length of the references, and if the raw MT output is longer than its post-edited counterpart, then the number of edit operations may be lar ger than that length. 3 At the other end of the spectrum, it is also clear that man y blocks have low TER. In fact more than 10% Figure 1: Distrib ution of TER on 39005 blocks from the French-English corpus (thresholded at 150%). have a TER of 0. The global score therefore hides a lar ge range of performance.

The second factor is that the TER measures the distance to an adequate and uent result. A high TER does not mean that the raw MT output is not understandable. Ho we ver, man y edit operations may be needed to mak e it uent. Translation post-editing can be vie wed as a simple transformation process, which tak es as input raw tar get-language text coming from a MT system, and produces as output tar get-language text in which  X er -rors X  have been corrected. While the automation of this process can be envisaged in man y dif fer -ent ways, the task is not conceptually very dif fer -ent from the translation task itself. Therefore, there doesn' t seem to be any good reason why a machine translation system could not handle the post-editing task. In particular , given such data as described in Section 2.2, the idea of using a statistical MT system for post-editing is appealing. Porta ge is precisely such a system, which we describe here.

Portage is a phrase-based, statistical machine translation system, developed at the National Re-search Council of Canada (NRC) (Sadat et al., 2005). A version of the Portage system is made available by the NRC to Canadian uni versities for research and education purposes. Lik e other SMT systems, it learns to translate from existing parallel corpora.

The system translates text in three main phases: preprocessing of raw data into tok ens; decoding to produce one or more translation hypotheses; and error -dri ven rescoring to choose the best nal hy-pothesis. For languages such as French and English, the rst of these phases (tok enization) is mostly a straightforw ard process; we do not describe it any further here.

Decoding is the central phase in SMT , involv-ing a search for the hypotheses t that have high-est probabilities of being translations of the cur -rent source sentence s according to a model for P ( t | s ) . Portage implements a dynamic program-ming beam search decoding algorithm similar to that of Koehn (2004), in which translation hypotheses are constructed by combining in various ways the tar get-language part of phrase pairs whose source-language part matches the input. These phrase pairs come from lar ge phr ase tables constructed by col-lecting matching pairs of contiguous text segments from word-aligned bilingual corpora.

Portage' s model for P ( t | s ) is a log-linear com-bination of four main components: one or more n -gram tar get-language models, one or more phrase translation models, a distortion (w ord-reordering) model, and a sentence-length feature. The phrase-based translation model is similar to that of Koehn, with the exception that phrase probability estimates P (  X  s |  X  t ) are smoothed using the Good-T uring tech-nique (Foster et al., 2006). The distortion model is also very similar to Koehn' s, with the exception of a nal cost to account for sentence endings.

Feature function weights in the loglinear model are set using Och' s minium error rate algorithm (Och, 2003). This is essentially an iterati ve two-step process: for a given set of source sentences, generate n -best translation hypotheses, that are representati ve of the entire decoding search space; then, apply a variant of Po well' s algorithm to nd weights that op-timize the BLEU score over these hypotheses, com-pared to reference translations. This process is re-peated until the set of translations stabilizes, i.e. no new translations are produced at the decoding step.
To impro ve raw output from decoding, Portage re-lies on a rescoring strate gy: given a list of n -best translations from the decoder , the system reorders this list, this time using a more elaborate loglinear model, incorporating more feature functions, in ad-dition to those of the decoding model: these typ-ically include IBM-1 and IBM-2 model probabili-ties (Bro wn et al., 1993) and an IBM-1-based fea-ture function designed to detect whether any word in one language appears to have been left without satisf actory translation in the other language; all of these feature functions can be used in both language directions, i.e. source-to-tar get and tar get-to-source.
In the experiments reported in the next section, the Portage system is used both as a translation and as an APE system. While we can think of a number of modications to such a system to better adapt it to the post-editing task (some of which are discussed later on), we have done no such modications to the system. In fact, whether the system is used for trans-lation or post-editing, we have used exactly the same translation model conguration and training proce-dure. 4.1 Data and experimental setting The corpus described in section 2.2 is available for two language pairs: English-to-French and French-to-English. 4 In each direction, each block is avail-able in three versions (or slices ): the original text (or sour ce ), the output of the commercial rule-based MT system (or baseline ) and the nal, post-edited version (or refer ence ).

In each direction (French-to-English and English-to-French), we held out two subsets of approxi-mately 1000 randomly pick ed blocks. The valida-tion set is used for testing the impact of various high-level choices such as pre-processing, or for obtain-ing preliminary results based on which we setup new experiments. The test set is used only once, in order to obtain the nal experimental results reported here.
The rest of the data constitutes the training set, which is split in two. We sampled a subset of 1000 blocks as train-2 , which is used for optimiz-ing the log-linear model parameters used for decod-ing and rescoring. The rest is the train-1 set, used for estimating IBM translation models, constructing phrasetables and estimating a tar get language model. The composition of the various sets is detailed in Table 1. All data was tok enized and lowercased; all evaluations were performed independent of case. Note that the validation and test sets were originally made out of 1000 blocks sampled randomly from the data. These sets turned out to contain blocks identical to blocks from the training sets. Consider -ing that these would normally have been handled by the translation memory component (see the HRSDC worko w description in Section 2.1), we remo ved those blocks for which the source part was already found in the training set (in either train-1 or train-2 ), hence their smaller sizes.

In order to check the sensiti vity of experimental results to the choice of the train-2 set, we did a run of preliminary experiments using dif ferent sub-sets of 1000 blocks. The experimental results were nearly identical and highly consistent, sho wing that the choice of a particular train-2 subset has no in-uence on our conclusions. In the experiments re-ported belo w, we therefore use a single identical train-2 set.

We initially performed two sets of experiments on this data. The rst was intended to compare the performance of the Portage PBMT system as an al-ternati ve to the commercial rule-based MT system on this type of data. In these experiments, English-to-French and French-to-English translation systems were trained on the source and reference (manually post-edited tar get language) slices of the training set. In addition to the tar get language model estimated on the train-1 data, we used an external contrib ution, Table 2: Experimental Results: For TER, lower (er -ror) is better , while for BLEU, higher (score) is bet-ter. Best results are in bold. a trigram tar get language model trained on a fairly lar ge quantity of data from the Canadian Hansard.
The goal of the second set of experiments was to assess the potential of the Portage technology in au-tomatic post-editing mode. Again, we built systems for both language directions, but this time using the existing rule-based MT output as source and the ref-erence as tar get. Apart from the use of dif ferent source data, the training procedure and system con-gurations of the translation and post-editing sys-tems were in all points identical. 4.2 Experimental results The results of both experiments are presented in Ta-ble 2. Results are reported both in terms of the TER and BLEU metrics; Baseline refers to the commer -cial rule-based MT output.

The rst observ ation from these results is that, while the performance of Portage in translation mode is approximately equi valent to that of the base-line system when translating into French, its perfor -mance is much better than the baseline when trans-lating into English. Two factors possibly contrib ute to this result: rst, the fact that the baseline system itself performs better when translating into French; second, and possibly more importantly , the fact that we had access to less training data for English-to-French translation.

The second observ ation is that when Portage is used in automatic post-editing mode, on top of the baseline MT system, it achie ves better quality than either of the two translation systems used on its own. This appears to be true regardless of the translation direction or metric. This is an extremely interesting result, especially in light of how little data was actu-ally available to train the post-editing system.
One aspect of statistical MT systems is that, con-trary to rule-based systems, their performance (usu-ally) increases as more training data is available. In order to quantify this effect in our setting, we have computed learning curv es by training the Portage translation and Portage APE systems on subsets of the training data of increasing sizes. We start with as little as 1000 blocks, which corresponds to around 10-15k words.

Figure 2 (ne xt page) compares the learning rates of the two competing approaches (Portage transla-tion vs. Portage APE). Both approaches display very steady learning rates (note the logarithmic scale for training data size). These graphs strongly suggest that both systems would continue to impro ve given more training data. The most impressi ve aspect is how little data is necessary to impro ve upon the baseline, especially when translating into English: as little as 8000 blocks (around 100k words) for di-rect translation and 2000 blocks (around 25k words) for automatic post-editing. This suggests that such a post-editing setup might be worth implementing even for specialized domains with very small vol-umes of data. 4.3 Extensions Given the encouraging results of the Portage APE approach in the abo ve experiments, we were curi-ous to see whether a Portage+Portage combination might be as successful: after all, if Portage was good at correcting some other system' s output, could it not manage to correct the output of another Portage translator?
We tested this in two settings. First, we actu-ally use the output of the Portage translation sys-Table 3: Portage translation -Portage APE system combination experimental results. tem obtained abo ve, i.e. trained on the same data. In our second experiment, we use the output of a Portage translator trained on dif ferent domain data (the Canadian Hansard), but with much lar ger amounts of training material (over 85 million words per language). In both sets of experiments, the Portage APE system was trained as pre viously , but using Portage translations of the Job Bank data as input text.
 The results of both experiments are presented in Table 3. The rst observ ation in these results is that there is nothing to be gained from post-editing when both the translation and APE systems are trained on the same data sets (Portage Job Bank + Portage APE experiments). In other words, the translation system is apparently already making the best possible use of the training data, and additional layers do not help (but nor do the y hurt, interestingly).

Ho we ver, when the translation system has been trained using distinct data (Portage Hansar d + Portage APE experiments), post-editing mak es a lar ge dif ference, comparable to that observ ed with the rule-based MT output pro vided with the Job Bank data. In this case, howe ver, the Portage trans-lation system beha ves very poorly in spite of the im-portant size of the training set for this system, much worse in fact than the  X baseline X  system. This high-lights the fact that both the Job Bank and Hansar d data are very much domain-specic, and that access to appropriate training material is crucial for phrase-based translation technology .

In this conte xt, combining two phrase-based sys-translation). tems as done here can be seen as a way of adapting an existing MT system to a new text domain; the APE system then acts as an  X adapter X , so to speak. Note howe ver that, in our experiments, this setup doesn' t perform as well as a single Portage transla-tion system, trained directly and exclusi vely on the Job Bank data.

Such an adaptation strate gy should be contrasted with one in which the translation models of the old and new domains are  X mer ged X  to create a new translation system. As mentioned earlier , Portage allo ws using multiple phrase translation tables and language models concurrently . For example, in the current conte xt, we can extract phrase tables and lan-guage models from the Job Bank data, as when train-ing the  X Portage Job Bank  X  translation system, and then build a Portage translation model using both the Hansar d and Job Bank model components. Loglin-ear model parameters are then optimized on the Job Bank data, so as to nd the model weights that best t the new domain.

In a straightforw ard implementation of this idea, we obtained performances almost identical to those of the Portage translation system trained solely on Job Bank data. Upon closer examination of the model parameters, we observ ed that Hansar d model components (language model, phrase tables, IBM translation models) were systematically attrib uted negligeable weights. Again, the amount of training material for the new domain may be critical in chos-ing between alternati ve adaptation mechanisms. We have proposed using a phrase-based MT sys-tem to automatically post-edit the output of an-other MT system, and have tested this idea with the Portage MT system on the Job Bank data set, a corpus of manually post-edited French-English ma-chine translations. In our experiments, not only does phrase-based APE signicantly impro ve the quality of the output translations, this approach outperforms a standalone phrase-based translation system.
While these results are very encouraging, the learning curv es of Figure 2 suggest that the output quality of the PBMT systems increases faster than that of the APE systems as more data is used for training. So while the combination strate gy clearly performs better with limited amounts of training data, there is reason to belie ve that, given suf cient training data, it would eventually be outperformed by a direct phrase-based translation strate gy. Of course, this remains to be veried empirically , some-thing which will obviously require more data than is currently available to us. But this sort of beha vior is expectable: while both types of system impro ve as more training data is used, ine vitably some de-tails of the source text will be lost by the front-end MT system, which the APE system will never be able to retrie ve. 5 Ultimately , the APE system will be weighted down by the inherent limitations of the front-end MT system.

One way around this problem would be to modify the APE system so that it not only uses the base-line MT output, but also the source-language input. In the Portage system, this could be achie ved, for example, by introducing feature functions into the log-linear model that relate tar get-language phrases with the source-language text. This is one research avenue that we are currently exploring.

Alternati vely , we could combine these two in-puts dif ferently within Portage: for example, use the source-language text as the primary input, and use the raw MT output as a secondary source. In this perspecti ve, if we have multiple MT systems available, nothing precludes using all of them as pro viders of secondary inputs. In such a setting, the phrase-based system becomes a sort of combination MT system . We intend to explore such alternati ves in the near future as well.
 The work reported here was part of a collaboration between the National Research Council of Canada and the department of Human Resources and Social De velopment Canada. Special thanks go to Souad Benayyoub, Jean-Fr  X  ed X  eric H  X  ubsch and the rest of the Job Bank team at HRSDC for preparing data that was essential to this project.

