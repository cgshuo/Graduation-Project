 Linear Classification has achieved complexity linear to the data size. However, in many applications, data contain large amount of samples that does not help improve the quality of model, but still cost much I/O and memory to process. In this paper, we show how a Block Coordinate Descent method based on Nearest-Neighbor Index can significantly reduce such cost when learning a dual-sparse model. In par-ticular, we employ truncated loss function to induce a series of convex programs with superior dual sparsity, and solve each dual using Indexed Block Coordinate Descent, which makes use of Approximate Nearest Neighbor (ANN) search to select active dual variables without I/O cost on irrelevant samples. We prove that, despite the bias and weak guar-antee from ANN query, the proposed algorithm has global convergence to the solution defined on entire dataset, with sublinear complexity each iteration. Experiments in both sufficient and limited memory conditions show that the pro-posed approach learns many times faster than other state-of-the-art solvers without sacrificing accuracy.
 I.5.2 [ Pattern Recognition ]: Design Methodology  X  Clas-sifier design and evaluation Algorithms, Performance, Experimentation
Linear classification has become one of the standard ap-proaches dealing with large-scale analysis in pattern recog-nition and data mining. Recent advances in training lin-ear model has achieved complexity linear to the data size [10, 17, 8], where problem with several gigabytes of data can be solved in reasonable time. With the development of more and more efficient algorithms, the learning bottle-neck has shifted from computation to the I/O between disk and memory [24]. The situation becomes especially critical when data cannot fit into memory, where repeated data ac-cess through comparatively expensive I/O could encumber the performance of any efficient algorithm. How to reduce such I/O cost becomes a focus in recent research on large-scale linear classification.

One popular approach that addresses the memory limi-tation is solving optimization in an online fashion, where samples are removed from memory after each parameter up-date. However, recent studies [24, 2] have shown that online methods (e.g. [16, 17]) need large number of iterations to ob-tain reasonable accuracy, and since the sample are reloaded for each single update, the training time of online solver is dominated by disk I/O. To balance the time spent on I/O and computation, Yu et al. proposed a Block Minimization framework that better utilizes data in memory by splitting data into several blocks and solving one block at a time. The algorithm yields better trade-off between I/O and computa-tion, but still needs tens of rounds of data loading before converges to a reasonable model. In [2], Kai et al. point out that, when taking Block Minimization as Block Coordi-nate Descent on the dual problem, some dual variables are more important than others. Therefore, caching informative samples in memory based on gradient information can result in faster convergence. However, the caching technique only applies to samples already read into memory, so a Block Co-ordinate Descent with cache still needs to traverse the whole dataset several times before convergence. For a dual-sparse model, this appears to be not cost-effective since only a few informative samples are relevant to improve model, while most I/O time are spent on useless ones. This motivates us to think that if one can organize data beforehand so learning only needs to read relevant samples into memory.

This paper aims to demonstrate how a Nearest-Neighbor index can improve the I/O efficiency in large-scale learning, especially when memory is limited. In practice, this is ben-eficial since many state-of-the-art indexing methods for Ap-proximate Nearest Neighbor (ANN) search (e.g. Locality-Sensitive Hashing, Metric Tree etc.) do not require repeated data access, and thus only need small memory and one pass of data loading to be built. Furthermore, an index can often be reused for models trained on the same data. Scenarios such as parameter tuning, cross-validation, multi-class clas-sification, feature selection, and data incremental learning all require executing the training algorithm multiple times.
In [6], Dhillon et al. are the first to apply Nearest-Neighbor search on sparse optimization problem. They show using ANN search to replace brute-force selection can significantly improve the efficiency of Greedy Coordinate Descent. How-ever, their experiments also indicate that, when minimiza-tion over single coordinate can be solved cheaply, ANN search becomes the bottleneck of learning, and a simple cyclic co-ordinate descent can be more efficient than greedy approach due to the saved search cost. In this paper, we address this issue by proposing Indexed Block Coordinate Descent, a Nearest-Neighbor-based Block Coordinate Descent algo-rithm that balances the cost of search and minimization. In our experiment that considers both I/O and CPU time, even for problem with cheap coordinate minimization step, the proposed algorithm can be orders of magnitude faster than cyclic method. In addition, Indexed Block Coordinate De-scent has global convergence that does not require guaran-tees from ANN search. This milder condition of convergence is crucial, since most ANN search algorithms are not de-signed for learning, and only answer maximum normalized-inner-product query [15] or nearest-to-hyperplane query [9], that does not aim to find samples with largest gradient as required by greedy approaches in [6].

The sparsity of underlying problem is also crucial to the efficiency of Nearest-Neighbor-based approaches. Although the solution of SVM is known to be dual sparse, when data are non-separable, the number of support vectors under stan-dard L1 (hinge) or L2-loss is actually linear to the size of data [19]. In [5], Collobert et al. propose using ramp-loss to achieve better dual-sparsity, which is also known to be more robust than L1/L2-loss in noisy setting [22][21]. Although learning ramp-loss SVM is a non-convex problem, Collobert et al. point out the computational advantage of trading con-vexity for sparsity. They decompose the non-convex problem into a series of convex ones using Concave-Convex Procedure (CCCP) [25], where one can learn a non-linear SVM much more efficiently due to the superior dual-sparsity.
While CCCP can also solve linear SVM with ramp-loss by integrating with state-of-the-art linear solvers such as LIBLINEAR [7], Pegasos [17] or SV M perf [10], there is little efficiency gain like that in nonlinear case, since the complexity of linear solver has little dependency on sparsity. However, in this paper, combining the sparsity induced by ramp-loss with Nearest-Neighbor-based coordinate descent, we obtain a practical algorithm that can solve large-scale problem in sublinear time.

The paper is organized as follows. In Section 2, we pro-pose a new relaxation method that solves not only ramp-loss, but general truncated-loss problem by a series of convex pro-grams with superior sparsity. In Section 3, we introduce In-dexed Block Coordinate Descent to solve each dual-sparse convex program, and present how this framework can be combined with state-of-the-art primal or dual solvers. Sec-tion 4 gives some implementation details including the In-dexing method used in our implementation. Section 5 con-duct experiments on four large-scale data sets under suffi-cient and limited memory conditions. In both conditions, our algorithm learns times faster than other state-of-the-art solvers without sacrificing accuracy. The general truncated-loss function is defined as: where L ( z )canbe L 1 -loss max(1  X  z, 0), L 2 -loss max(1 z, 0) 2 , or other convex loss function. When L ( z )=max(1 z, 0), (1) is also called ramp-loss. Unlike standard convex loss functions that give outlier loss that can potentially grow to infinity, (1) take sample with error more than s as outliers, and assign at most 1 + s loss to them. The truncated loss function (1) is more like 0/1-loss as in accuracy, and thus is more robust to noise [3]. More importantly, none of the outliers under (1) will become support vectors. Thus the number of support vectors does not grow linearly with data size [5]. However, the learning problem is non-convex. When L ( z )=max(1  X  z, 0), (2) can be divided into a convex part J vex ( w ) and a concave part J cav ( w ) and solved with Concave-Convex Procedure (CCCP) [25] by a series of convex programs [25] shows that this algorithm strictly decrease the original objective J ( w ), and more recently, [18] shows this algorithm globally converges to a stationary point of J ( w ). However, the formulation (3) is specific to hinge-loss. For other loss functions, it is not obvious how to derive the convex part and concave part. Another concern is, outliers are not excluded from the formulation (4), and thus cannot be filtered during training.

Here we propose a new convex relaxation method for gen-eral truncated-loss (1) that excludes outliers from each sub-problems. First, we define the sets of outliers and non-outliers as OUT ( w )= l | L ( y l w T x l ) &gt; 1+ s and IN ( w )= l |
L ( y l w T x l )  X  1+ s . In each iteration, this method solves which is very intuitive: solve convex problem of the original loss L ( z ) for only non-outliers, and fix loss of outliers to 1+s. The formulation is motivated by the observation that truncated loss R s ( z ) is upper-bounded both by L ( z )and 1+ s ,so 1 2 = 1 2 w  X  1 2  X  1 2 where D = IN w + OUT w . In other words, (5) mini-mizes an upper bound of (2), which strictly decrease (2) when IN ( w t +1 ) = IN ( w t ) (or, equivalently, OUT ( w OUT ( w t )) 1 .When IN ( w t +1 )= IN ( w t ), (2) equals to (5), and thus we get a minimum of (2). Since IN ( w t )wouldnot be the same as IN ( w 0 ) ...IN ( w t  X  1 ), (5) converges in finite iterations as CCCP.

For L ( z ) being hinge-loss, we can further show that the sequence { w t }  X  t =0 produced by (5) has linear convergence rate by following theorem. The reasoning is similar to the CCCP convergence rate proof in [23].

Theorem 2.1. The sequence { w t }  X  t =0 produced by (5) con-verges to a stationary point of (2) with at least linear con-vergence rate.

Proof Sketch. Since the objective in (5) is an upper bound for (2), a sample changing from IN ( w t )to OUT ( w or OUT ( w t )to IN ( w t +1 ) can be seen as a process of de-scent. Therefore, we interpret (5) as an alternating mini-mization (block coordinate descent) between d  X  R m and ( w ,  X  )  X  ( R n , R m ) on the problem which satisfies the form of non-smooth, separable problem studied in [20]. Thus we can find an equivalent Coordi-nate Gradient Descent procedure introduced in [20], which produces the same sequence as that produced by block co-ordinate descent on (6). By theorem 1,2 and 4 of [20], the sequence converges to a stationary point of (6) with at least linear convergence rate. A detailed version of this proof is in appendix. 2
One advantage of (5) is it ignores outliers and solves the original problem of L ( z ) on non-outliers. Therefore, any solver for the original loss can be utilized to solve (5). Fur-thermore, the relaxation has no assumption on L ( z ). Thus it is a general approach for solving the truncated version of any convex loss function. Even in problems such as regres-sion or clustering, one can use a truncated-version of loss function to obtain a sparser, outlier-free model for indexed learning.
The decrease is strict since R ( y l w T x l ) &lt; 1+ s for l/ OUT ( w ), and R ( y l w T x l ) &lt;L ( y l w T x l )for l/ http://www.csie.ntu.edu.tw/ r00922017/kdd2013appendix
The problem (2) is non-convex, so initialization w 0 of (5) will affect convergence result. A good choice is solving the original convex loss problem to get w  X  L and set w 0 = w Then since (5) guarantees to decrease the objective function (2) with loss function more similar to 0/1 loss, we are likely to get a solution w  X  with higher accuracy. However, this kind of initialization cannot gain learning efficiency since it needs to solve the original problem. A practical choice for indexed learning is to solve the convex loss problem on ran-dom samples to give an initial w 0 with reasonable accuracy.
Without outliers, we obtain convex problem (5) with su-perior dual-sparsity. In this section, we first introduce the framework of Indexed Block Coordinate Descent. Then in Sec.3.2, we introduce the ANN search technique to find in-formative samples for the learning algorithm. In Sec.3.3 and 3.4, we demonstrate how to integrate our framework with state-of-the-art primal and dual solvers.
The Indexed Block Coordinate Descent algorithm 1 solves dual form of the convex program (5) where  X  Q = Q + D ,Qis N by N matrix with Q ij = y i y j D is diagonal matrix. For L1-loss SVM, D ii =0and U = C . For L2-loss SVM, D ii =1 / (2 C )and U =  X  . e is N by 1 vector [1 , 1 ..., 1] T .

In the linear case, we can maintain a relation between primal and dual variables so the gradient of each dual variable can be computed effi-ciently as  X  l f (  X  )=( Q  X  ) l  X  1+ D ii  X  l = y l w T x l  X  1+ D ii The active dual variables are defined by coordinates with non-zero projected gradient:
In Algorithm 1, we maintain a set S that aims to include all active dual variables, and any sample l with  X  l &gt; 0are put in S . Instead of loading all samples in IN ( w t )into memory, we request active variables by queryIndex ( w ( t,k w , s , n ), which searches for n samples /  X  S ( k ) satisfying
It guarantees sample to be non-outlier with non-zero pro-jected gradient, since  X  l =0 for all  X  l /  X  S and  X  l f (  X  )= y by queryIndex(.) are put into S .Note queryIndex(.) does not specify which search method to use. Any search method that can find n samples satisfying (10), when there exist, Algorithm 1 Indexed Block Coordinate Descent repeat until problem (13) defined on S ( k ) reach S and | N | &lt;n yields convergence. However, search method with more ef-ficiency and less bias can result in faster convergence. We will introduce an ANN search method in Sec.3.2 that tries to find w T x l close 0, in which case, the query result tends to have large gradient no matter labels y l are 1 or -1.
In algorithm 1, we denote fresh samples returned by queryIn-dex(.) as N ,where n e is the number of traversed instances for finding | N | X  n samples satisfying (10). Though samples from N have larger gradient, they come from cost of search. To balance cost between search and optimization, we com-pose a block B with N and the other min ( n e  X  X  N | , | S samples from reader r that cyclically traverses S ( k ) ,where M is the sample size limited by memory. Then we solve the block minimization problem defined by B with  X  B = D\ B Then the cost for each iteration is where T search ( n e ) is the time for search method to traverse n e examples. T opt ( | B | ) is the time for solving (11). For a ANN search method with precision prec [ n ]= n/n e , n e and | B | are sublinear to the data size. The hope is that neither T search ( n e )nor T opt ( | B | ) becomes the bottleneck. Practically, both T search and T opt may involve disk I/O when samples used are not cached in memory. We will discuss details of memory management in Sec.4.2.

The Indexed Block Coordinate Descent algorithm 1 can be viewed as a reverse process of the well-known Shrinking strategy. In Shrinking strategy, the problem size is shrunk in each iteration by removing inactive variables that tends to be unchanged. However, since the strategy often needs to traverse data many times before shrinking problem to a smaller size, it is not suitable to be applied when data cannot fit into memory. On the other hand, the Indexed Block Coordinate Descent algorithm tries to maintain a set of most active dual variables S ,andincreases S until it contains all variables with non-zero projected gradient. When combined with Nearest-Neighbor index, this strategy can avoid much I/O, especially when memory is limited.

Although the block minimization (11) decreases objective in (7) each iteration, it does not imply global convergence. In Here we assume L ( z ) is monotonically decreasing for L ( z ) &gt; 0so L  X  1 (1 + s ) exists. the following, we give the convergence theorem for algorithm 1, beginning with a lemma.
 Lemma 3.1. Let  X   X  be the optimal solution of (7). Let V = { l |  X   X  l &gt; 0 } X  S  X  IN ( w t ) and  X  S = D\ S . Then the optimal solution of following problem is also the optimal solution of (7).

The proof for above lemma is simple and thus omitted due to space limitation. 4 The next theorem gives the convergence of algorithm 1.

Theorem 3.2. The sequence {  X  ( t,j ) }  X  j = k produced by al-gorithm 1 linearly converges to the problem (13) with S = S k ) , and the sequence S (1) , S (2) , ... S ( k ) converges to S no more than T n + T iterations, where T n  X  | S  X  | n , T not in S  X  .

Proof. As shown in [8], the formulation (13) satisfies the form of convex, smooth problem studied in [13]. Since each so the sequence {  X  ( t,j ) }  X  j = k satisfies Gauss-Seidal update rule of Block Coordinate Descent. By the extended version of theorem 2.1 in [13], the sequence {  X  ( t,j ) }  X  j = k convergence rate to the optimal solution of (13) defined by S = S ( k ) . And since | S ( k ) | monotonically increases by each iteration, we have where n i = | N | for iterations that have | N | &lt;n ,and n for i =1 ..T since the algorithm stops when | N | &lt;n .By
The size of final set of active variables | S  X  | is at most |D| . However, we observed | S  X  | |D| for a dual-sparse problem. Setting n = 1, Theorem 3.2 proves our algo-rithm converges to the optimal solution of (7). However, in practice, including n more active dual variables only de-crease the objective (2) by at most n C (1 + s ), which ap-proximately increase training accuracy only by n |D| .Toget within tolerance of the best training accuracy Acc ,wecan set n = (1  X  Acc ) |D| . For example, to reach =1%toler-ance for a model with 99% training accuracy on a data with 1 , 000 , 000 samples, n can be set as 100. A more practi-cal stopping condition uses sampling to replace the stopping condition in algorithm 1, in which the algorithm stops when |
N | /n e &lt;n / |D| . The sampling-based stopping condition is practical since it avoids algorithm 1 from traversing the whole index through expensive I/O. In the next section, we will introduce the ANN search method for the queryIndex(.) (10), and in Sec.3.3 and Sec.3.4, we describe the dual and primal optimization methods we use for the block minimiza-tion (11). http://www.csie.ntu.edu.tw/ r00922017/kdd2013appendix
Here we consider how informative samples can be ob-tained via ANN search. The queryIndex(.) function searches for samples satisfying l | L  X  1 (1 + s )  X  y l w T x l  X  samples near to the decision boundary. In [9], a Local-ity Sensitive Hashing (LSH) method EH-Hash was proposed to handle the nearest-to-hyperplane query occurs in Active Learning, where their goal is to minimize the search time for obtaining a new informative sample in active learning. Here, we apply a similar technique that transforms the nearest-to-hyperplane problem into standard ANN query. The tech-nique costs cheaper than EH-Hash when combined with the Tree-based index in our implementation.

Given a query vector q , the nearest neighbor is defined as the vector x l most similar to q ,thatis, where  X  v means normalized v . However, sample closest to a hyperplane of normal vector w is defined as We connect these two problems using kernel of degree-2 polynomial feature expansion V ( x ) =[ x 2 1 , x , lem in original space can be reduced to ANN problem in embedded space. We first considers the case when data are normalized: Where  X  V ( X  w ) is the transformed nearest-neighbor query, and (-V ( X  w ) ) T V ( X  x l ) can be computed efficiently by which is not like the case in EH-Hash, where quadratic cost was required to generate Gaussian-distributed hashing func-tion in the embedded space. The above transformation guar-antees  X  (  X  w T  X  x l ) 2 to be a normalized inner product in some Hilbert space, and thus, can use indexing structure designed for nearest-neighbor problem.

For unnormalized sample x l with length x l 2 .Let R = min l x l 2 , since query (10) defined by queryIndex ( . )isa range query, we can search for a larger range that includes the target samples required by (10), while the search algorithm for unnormalized data will be the same as normalized ones.
In this paper, for both L 1 -loss and L 2 -loss cases, we use coordinate descent solver proposed by [8] to solve the dual of block minimization problem (11) In each inner iterations, the algorithm traverses all variables in block B in a random permuted order, and solve each co-ordinate minimization problem via the closed form solution where  X  l f (  X  ) is computed via (9), and maintains primal vector w by As shown in [8], the algorithm converges linearly to the op-timal solution of (11). However, for some ill-conditioned problems, linear convergence can be very slow [7]. For loss with second-order information, a primal solver like [11] with super-linear convergence is more suitable. The next section introduce the primal problem derived from block (11). To apply primal solver on problem (11), we derive the Lagrangian dual of (11) with  X  l  X   X  B fixed at  X  ( t,k ) results in the primal formulation of (11) 5 where .For L 2 -loss L ( z )= max (1  X  z, 0) 2 ,wehave generalized Hessian and gradient of (16) as where X = x 1 , ... x |D| T , y = y 1 , ..., y |D| T , I is identity matrix and I = l | 1  X  y l w T x l &gt; 0 . With (17), one can apply any primal solver that uses second-order information to get faster convergence. In our implementation, we use theTrust-RegionQuasi-NewtonMethodproposedin[11], which is also included in the LIBLINEAR package [7].
When setting B = S ( k ) ,wehave v  X  B =0since  X  ( t,k ) for l  X   X  S ( k ) . Inthiscase,(16)becomesa L 2 -loss problem defined on S ( k ) . It means, each time we solve a L 2 -loss prob-lem on a subset of data S ( k ) , we get descent in terms of dual objective, and by Theorem 3.2, after T n + T iterations, we obtain all samples of non-zero loss and get a global optimum of (7).
In this section, we address some details of implementing an indexed optimization algorithm. First, we discuss chal-lenges of building index for learning problem, as opposed to other problem like retrieval. Then we discuss the Caching, Shrinking strategy for learning with less memory and time.
There are two practical challenges of building index for selecting informative training samples. First, the index for ANN search is biased towards the reference points (or hash functions) selected. While such bias is acceptable for appli-cations like multimedia retrieval, training on biased samples http://www.csie.ntu.edu.tw/ r00922017/kdd2013appendix will seriously slow down the convergence of algorithm (1). Second, to be reused for different models, the index should be built on disk. However, not all indexing methods are cost-effective to be built on disk. For example, Locality-sensitive hashing (LSH), one of the most popular methods recent years, requires a sample to be stored in different hash tables, which can significantly increase the storage cost [14].
Here we propose Metric Forest that addresses these is-sues. Unlike LSH, a tree-structured index has size com-parable to the original data. In [12], a tree-based index-ing, called Metric Tree, is modified to solve ANN problem, and is shown to be more efficient than LSH. A Metric Tree organizes points as a binary tree, in which each subtree N ( v )rootedonanode v partitioned into points closer to v.lpv and those closer to v.rpv ,where v.lpv and v.rpv are pivot points selected from data, and the metric is de-fined by angle distance in the embedded space: d (  X  x 1 ,  X  x cos  X  1 ( V ( X  x 1 ) T V ( X  x 2 ) )=cos  X  1 ((  X  x 1 T  X  x pose Metric Forest, which shares the basic idea of Metric tree, but is different in three ways:
Since reference points closer to the root of a Metric Tree are used more frequently, query results are often biased to-ward those points. [12] propose a variant called Spill-Tree to allow overlap between different partitions, thus alleviate the bias problem. However, the overlap between partitions sig-nificantly increases storage, and under limited memory, even building a tree is time-consuming. In our design, data are split into R random blocks which can be put into memory, and a Metric Tree is built for each block. When performing search, all trees are traversed in an order determined ran-domly. Such design warrants the query results not biased to a small number of reference points.

While binary tree works well in memory, it is inefficient on disk because the seeking time on disk is more expensive. Here we use, instead, K-way partitioning such that the tree depth is smaller and each tree node contains more points. For each subtree N ( v )rootedonnode v , points are parti-tioned into N ( v.pv 1 )... N ( v.pv K )s.t. v.pv k is nearest to the points in N ( v.pv k )among v.pv 1 ... v.pv K .
In [12], Defeatist Search is used to obtain efficient approxi-mate search, in which only the nearest partition N ( v.pv a query point is visited. However, in our algorithm, we need to increase, or decrease, search range according to the cur-rent margin 1 / w 2 . Therefore, we exploit Best-Bin-First search strategy, as proposed in [1], in which each partition N ( v.pv k ) is put in a priority queue, and ranked by their distance to the query, where the distance is measured by their nearest boundary to the query point, as shown in Fig-ure 1. Let v.pv q be the closest pivot point to the query, the nearest boundary of partition N ( v.pv k ) is defined by the hyperplane passing ( v.pv k + v.pv q ) / 2withnormalvector v.pv k  X  v.pv q .
In our implementation, we use Least-Recently-Used (LRU) cache for frequently-used tree nodes to minimize I/O be-tween memory and index. This is effective since, in queryIn-dex(.) , we only traverse branches of Metric Tree near deci-sion boundary. As model changes over time, samples away from the boundary will not be traversed again, and thus, are removed when cache is full. Since we keep samples with larger projected gradient in memory, I/O of unneces-sary data is minimized. When sample are added into active set S , we moved the memory quota from cache to S ,until LRU cache has memory quota less than a threshold m LRU , when we will write some samples in S into disk in a sliding window fashion, and read those samples back when they are traversed.
In search of queryIndex(.) , samples not satisfying (9) for w t,k ) are likely not satisfying (9) for w ( t,k +1) , w ( This is like the case in most SVM solver, where many bounded variables, that is,  X  l =0or  X  l = C , tends to be un-changed if their gradients are large to the bounded direc-tion. Here we apply a similar technique to shrink vari-ables that are not likely to satisfy (10) in later iterations, y old. The shrunk variables would not be traversed in a Metric Tree, and a tree node is not traversed if all of its descendants are shrunk. When the shrunken problem converges, we re-cover all of shrunken variables and solve the non-shrunk problem to check convergence. This is repeated until the original non-shrunk problem is solved.
In this section, we conduct experiments that compare our algorithm (Index-L1-Dual, Index-L2-Dual, and Index-L2-Primal) with state-of-the-art linear SVM solvers LIB-LINEAR (L1-Dual, L2-Dual, and L2-Primal), online Pegasos (Online-L1 and Online-L2), and truncated-loss batch solver (Trunc-L1-Dual, Trunc-L2-Dual, and Trunc-L2-Primal) that uses the same truncated-loss function (1) as our method, but employs LIBLINEAR as inner procedure for each convex relaxation (5). There are, of course, other state-of-the-art solvers. However, as our contribution is an indexed learning framework, a comparison between methods with/without our technique is more essential than that between differ-ent solvers. In limited memory condition, we compare In-dexed Block Coordinate Descent with online Pegasos and LIBLINEAR-CDBLOCK (Block-L1-Dual, Block-L2-Dual), a limited-memory version of LIBLINEAR proposed in [24], and refined in [2]. The initialization for both truncated-loss solvers uses 10,000 random samples solved by the corre-sponding convex loss solver in LIBLINEAR. In our experi-ments, both I/O and Initialization are included into training time.
Our experiments conducted on 4 large-scale public datasets increasing size: Covtype, Kddcup1999, PAMAP and Mnist8m. Their statistics are summarized in Table 1.

Table 2 shows the statistics of index built. The construc-tion time and storage size are generally linear to the data size. In our experiment, we found that the parameter tree size (e.g. #samples / tree) and tree width (e.g. #pivot-points) do not affect result significantly. However, tree size should be large enough s.t. each tree contains useful sam-ples, and small enough s.t. each tree can be built in mem-ory. Tree width should be large enough s.t. the average depth= log width ( Size ) is reasonably small (3 to 5 in our ex-periment). Since KDDCUP1999 and PAMAP have more irrelevant samples than other datasets, their tree size are set to be larger than that of others.
 We scale features in all dataset to between 0 and 1. For Covtype and KDDCUP1999, we randomly selected 1/3 of samples for testing and the remaining 2/3 samples for train-ing. For PAMAP and Mnist8m, to test on limited memory, we only used 1/10 for testing and left 9/10 for training. All of these data have multi-classes, and thus the index built can be reused for different models. Due to space limitation, we only show figures for one of those 1-against-all models for each dataset. For Covtype, the given result is on the model of class 2 against the other 6 classes, which follows the choice in [4]. For KDDCUP1999, we show result of the class normal against all the other 22 abnormal classes. For PAMAP, we classify action  X  X itting X  from the other 23 ac-tions. For mnist8m, we show result of class digit-1 against all the other 9 digits. Throughout the experiments, we set parameters n = 1000 for Indexed Block Coordinate Descent, s = 1 for truncated-loss, and c =1forSVMmodel.
 Figure 2, 3, 4 show the testing error of L 1 -Dual , L 2 and L 2 -Primal solvers under sufficient memory, where In-dexed Block Coordinate Descent saves much I/O time by selecting only relevant samples into memory. Unlike on-line solver, it converges to much more accurate solution as that produced by truncated-loss batch solver. Though truncated-loss learning problem is non-convex, where dif-ferent solvers may converges to different solutions, the in-dexed solver achieves similar accuracy as the truncated-loss
Covtype, Kddcup1999, PAMAP can be found at UCI Ma-chine Learning Repository, while Mnist8m can be found in LIBSVM Datasets.
 batch solver in most of cases, but indexed solver was orders of magnitude faster than the batch one. In Figure 6, we shows result of our algorithm for a spectrum of parameter c =0 . 01 , 0 . 1 , 10 , 100 on KDDCUP1999 dataset.
Figure (5) shows the limited-memory experiments con-ducted on PAMAP and Mnist8m. We compare Indexed Block Coordinate Descent with Online Pegasos and LIBLINEAR-CDBLOCK, where data size is 10 times larger than mem-ory space, under which the batch version of LIBLINEAR and truncated-loss solvers suffer from severe swaps and can hardly progress. For Mnist8m, we limit memory size to 2GB, and uses 20 blocks with 1GB cache for LIBLINEAR-CDBLOCK. For PAMAP, we limit memory size to 400MB, and uses 10 blocks with 200MB cache for LIBLINEAR-CDBLOCK. In Figure (5), the Indexed Block Coordinate Descent has almost the same performance as in sufficient-memory condition, where it achieves higher accuracy by se-lecting informative samples under truncated-loss into mem-ory. Though datasize was 10 times larger than the mem-ory size, the indexed solver is not affected much since the memory is still large enough for maintaining only relevant samples.

Finally, in Figure 7, we show the number of support vec-tors, and corresponding testing error of our algorithm and LIBLINEAR for a spectrum of s , which shows the signifi-cant effect of truncated-loss on increasing the dual-sparsity of SVM. When s = 1, it means outlier should have error more than the current margin. This choice yields great per-formance for most data. However, in Mnist8m dataset, s =2 is a better choice, and for PAMAP dataset, s =0 . 5wasbet-ter.
In many applications, large-scale data contain only some relevant samples that can effectively improve the accuracy of model. While random sampling, or online learning can over-look those rare but crucial samples, batch learners generally cost too much memory and I/O time. In this paper, we Figure 5: Limited-Memory Solvers. For Mnist8m, we limit memory size to 2GB, and uses 20 blocks with 1GB cache for LIBLINEAR-CDBLOCK. For PAMAP, we limit memory size to 400MB, and uses 10 blocks with 200MB cache for LIBLINEAR-CDBLOCK. Figure 6: L1-Dual Solvers on KDDCUP1999 for c = 0.01, 0.1, 10, 100. Figure 7: Testing Error vs. #SV for a spectrum of s. (L1-Dual Solver) propose Indexed Block Coordinate Descent algorithm that makes use of Approximate Nearest Neighbor (ANN) search to select active dual variables without I/O cost on irrele-vant samples. Though building index takes time linear to the data size, in practice, this is beneficial since people often learn several models from the same data. Scenarios such as parameter tuning, model selection, cross-validation, multi-class classification, feature selection, and data incremental learning all require multiple passes of training. In the case of limited memory, our approach can save much I/O cost since building index do not require much memory and only requires a pass of data reading. This indexed learning ap-proach can be potentially apply to a general class of large-scale learning problem, through defining new truncated-loss function for convex-loss problems in classification, regression or clustering.
This work is supported by National Science Council, Na-tional Taiwan University and Intel Corporation under Grants NSC 101-2911-I-002-001, NSC 101-2628-E-002-028-MY2 and NTU 102R7501. [1] J. S. Beis and D. G. Lowe. Shape indexing using [2] K.-W. Chang and D. Roth. Selective block [3] O.Chapelle,C.B.Do,Q.V.Le,A.J.Smola,and [4] R. Collobert, S. Bengio, and Y. Bengio. A parallel [5] R. Collobert, F. Sinz, J. Weston, and L. Bottou. [6] I. S. Dhillon, P. D. Ravikumar, and A. Tewari. [7] R.-E. Fan, K.-W. Chang, C.-J. Hsieh, X.-R. Wang, [8] C.-J. Hsieh, K.-W. Chang, C.-J. Lin, S. S. Keerthi, [9] P. Jain, S. Vijayanarasimhan, and K. Grauman. [10] T. Joachims. Training linear SVMs in linear time. In [11] C.-J. Lin, R. C. Weng, and S. S. Keerthi. Trust region [12] T. Liu, A. W. Moore, A. G. Gray, and K. Yang. An [13] Z.-Q. Luo and P. Tseng. On the convergence of [14] Q. Lv, W. Josephson, Z. Wang, M. Charikar, and [15] P. Ram and A. G. Gray. Maximum inner-product [16] S. Shalev-Shwartz, K. Crammer, O. Dekel, and [17] S. Shalev-Shwartz, Y. Singer, and N. Srebro. Pegasos: [18] B. K. Sriperumbudur and G. R. G. Lanckriet. On the [19] I. Steinwart. Sparseness of support vector machines. [20] P. Tseng and S. Yun. A coordinate gradient descent [21] L. Wang, H. Jia, and J. Li. Letters: Training robust [22] Z. Wang and S. Vucetic. Fast online training of ramp [23] I. E. Yen, N. Peng., P. Wang, and S. Lin. On [24] H.-F. Yu, C.-J. Hsieh, K.-W. Chang, and C.-J. Lin. [25] A. L. Yuille and A. Rangarajan. The concave-convex
