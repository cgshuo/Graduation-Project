 1. Introduction
Bulmer (2004) defines a questionnaire as,  X  X  X ny structured research instrument which is used to collect social research data in a face-to-face interview, self-completion survey, telephone interview or Web survey. It consists of a series of questions set out in a schedule, which may be on a form, on an interview schedule on paper, or on a Web page X  X  (p. XIV). Questionnaires in interactive information retrieval (IR) studies are typically self-admin-istered via electronic or pen-and-paper mode, or via interview, and have been used to elicit a variety of types of information from system users including factual, behavioral and attitudinal.

Questionnaires can be comprised of closed questions, open questions or a mixture of both. Closed questions are questions that provide a fixed set of responses with which subjects must respond. It is common practice for usability questionnaires to include closed questions in the form of statements such as, the system was easy to learn to use . Subjects are typically provided with 5 X 7-point Likert-type scales for responding, where one scale end-point represents strong agreement and the other represents strong disagreement. The use of semantic dif-ferentials is another common way to respond to closed questions. Open questions, on the other hand, do not provide a response set and subjects are able to provide any type of response they feel is appropriate. For instance, open questions included on usability questionnaires in interactive IR experiments often identify a particular feature of the system and ask subjects for their impressions of the feature. It is also common to have open questions that ask subjects to identify the most positive and negative things about the system. Each type of question has its own set of merits and demerits, which is why a combination of question types is often used.
The questionnaire is a vital part of interactive IR studies since it is one of the primary vehicles for eliciting data from subjects. However, it is generally believed that subjects have a tendency to inflate their ratings of systems during usability evaluations ( Czerwinski, Horvitz, &amp; Cutrell, 2001; Nielsen &amp; Levy, 1994 ). For instance, Nielsen and Levy (1994) conducted a meta-analysis of 57 human X  X omputer interaction (HCI) studies with the goal of analyzing the relationship between performance and preference. Nielsen and Levy (1994) dem-onstrated that the performance of 101 of the 127 systems studied was rated higher than the neutral point of the scale used to make the ratings, and that users assigned a majority of systems a score of four on a 5-point scale.
This meta-analysis provides some evidence for the notion that subjects inflate their ratings of systems or that usability questionnaires are not particularly sensitive, which makes obtaining valid data from subjects a chal-lenge. Moreover, in experiments with two or more systems, this may create problems since there is a good chance that the systems will be rated the same by subjects. Indeed, it is conjectured that subjects in most stud-ies of interactive IR rarely rate systems poorly even when they clearly violate basic usability principles. Since researchers are interested in obtaining as valid data as possible from subjects, identifying better ways of col-lecting usability data from subjects is important. Doing so will further allow researchers to better understand interactive IR processes and differences in systems X  support for such processes.

There have been a number of studies about interactive IR evaluation, including those presenting general frameworks and guidelines (cf., Borlund, 2003; Dumais &amp; Belkin, 2005; Tague-Sutcliffe, 1992; Thomas &amp;
Hawking, 2006; Toms, Freund, &amp; Li, 2004 ). However, studies investigating the impact of various choices of experimental components on study results are rare, with few exceptions (cf., Borlund, 2000 ). In contrast with other disciplines, where studies of methods and experimental design comprise an important portion of the literature, the impact of different experimental designs decisions, such as batch-mode vs. individual session, interview vs. electronic questionnaire, or open vs. closed questions, is virtually ignored in the context of inter-active IR experimentation. In this study we explore the extent to which questionnaire mode impacts subjects X  usability ratings of two systems and their responses to open questions. 2. Literature review
There is a great deal of literature in psychology, public opinion polling and public health which has inves-tigated the impact of mode effects and question type on subjects X  responses to questionnaires (see Groves, 1989; Richman, Kiesler, Weisband, &amp; Drasgow, 1999; Tourangeau, Rips, &amp; Rasinski, 2000 for reviews). Mode effects occur as a result of using a particular technique for collecting questionnaire data. Studies of mode effects have compared and contrasted responses elicited via a number of different questionnaire modes, includ-ing pen-and-paper, electronic, telephone and face-to-face interviews. These studies have demonstrated very complex (and often varying) relationships between questionnaire mode, question content, question type, and subjects X  responses. However, one of the most common results is that subjects are more willing to report sensitive information, and socially disapproved or illegal behaviors in self-administered questionnaires than in questionnaires administered via interviews ( Tourangeau et al., 2000 ). One of the most popular explanations of why differences have been observed between modes is social desirability responding, or  X  X  X he tendency by respondents, under some circumstances and modes of administration, to answer questions in a more socially desirable direction that they would under other conditions or modes of administration X  X  ( Richman et al., 1999, p. 755 ). 2.1. Social desirability responding Social desirability responding has been conceptualized as topic-and personality-based ( Brewer, Hallman,
Fiedler, &amp; Kipen, 2004 ). Topic-based conceptualizations claim that the topic or content of particular ques-tions prevent subjects from responding truthfully. For instance, questions about behaviors like drug use will be underreported, while behaviors like helping others or voting will be over-reported. Personality-based con-ceptualizations  X  X  X ink socially desirable responding to a personality trait characterized by lower self-reports of unflattering behaviors X  X  ( Brewer et al., 2004, p. 876 ). In such explanations it is not the topic of the question that inhibits or encourages socially acceptable responses, but an inherent trait that causes people to portray themselves in the most flattering way.

Results of research about social desirability responding and questionnaire mode have been mixed, although it is generally accepted that different modes are associated with different levels of social desirability responding, and specifically that social desirability responding will occur most often in the interview mode. Despite this, interviews are still used in many situations because the benefits (interviews are generally thought to be more motivating, reduce non-response and encourage longer, more elaborated responses) out-weigh the potential for response bias ( Richman et al., 1999 ).

Researchers have noted that self-administered questionnaires increase subjects X  willingness to disclose infor-mation about sensitive topics ( Tourangeau et al., 2000 ). Initial studies compared differences between tradi-tional modes: pen-and-paper, telephone interview and face-to-face interview. Once computer technology became established as a technique for collecting data, research also compared electronic modes with more tra-ditional modes. Some have found less social desirability responding with electronic questionnaires ( Martin &amp;
Nagao, 1989; Weisband &amp; Kiesler, 1996 ), while others have found more social desirability responding with electronic questionnaires ( Lautenschlager &amp; Flaherty, 1990 ) and still others have found no differences between modes ( Booth-Kewley, Edwards, &amp; Rosenfeld, 1992 ). Kiesler and Sproull (2001) found that subjects X  responses to closed questions on an electronic questionnaire were less socially desirable than responses on a pen-and-paper version of the same questionnaire. These researchers also found that responses to open ques-tions were relatively longer and more disclosing in the electronic mode. A meta-analysis of a large number of studies about social desirability responding and questionnaires demonstrated that were no differences between electronic and pen-and-paper modes, but that there were differences between electronic and interview modes ( Richman et al., 1999 ). Clearly, the relationship between questionnaire mode and response behaviors is quite complex.

One explanation for why there is less social desirability responding in electronic questionnaires is related to anonymity ( Richman et al., 1999 ). Some researchers claim that electronic questionnaires provide greater levels of anonymity to subjects than pen-and-paper or interview questionnaires. Once a subject submits his responses to an electronic questionnaire there is no physical instantiation of these responses  X  essentially answers  X  X  X is-appear into the computer X  X  ( Richman et al., 1999, p. 756 ). However, people X  X  perception of computers have likely changed since many of the original studies were conducted and in more recent studies researchers have suggested that instead of heightened anonymity, questionnaires administered electronically might invoke the  X  X  X ig brother X  X  feeling. This, in turn, might increase the occurrence of social desirability responding ( Rosenfeld, Booth-Kewley, Edwards, &amp; Thomas, 1996 ).

Many previous studies of social desirability responding and mode effects have been undertaken in the con-text of public opinion polling and healthcare, where subjects are often responding to questions about their lifestyle choices; stakes for responding  X  X  X esirably X  X  might necessarily be higher than in situations where sub-jects are asked to report their attitudes about a computer system. However, findings from such studies provide one possible explanation for why subjects tend to inflate their ratings of systems in interactive IR experiments.
Subjects may view the success or failure of a system as a reflection of their own abilities rather than as a reflec-tion of the system X  X  abilities. For instance, subjects might believe that responding negatively to questions such as how easy a system was to learn to use, how easy a system was to use, or how satisfied they were with their performances, reflects on them rather than the system. When people are part of the process as they are in inter-active IR, they may feel that they are at least in part responsible for a system X  X  performance. Thus, people may view negative ratings of systems as negative ratings of themselves, and avoid using such ratings. 2.2. Acquiescence
Another factor that is relevant to understanding subjects X  tendencies to inflate their ratings of systems is the notion of acquiescence.  X  X  X cquiescence, or agreeing-response bias, refers to a presumed tendency for respon-dents to agree with attitude statements presented to them X  X  ( Schuman &amp; Presser, 2004, p. 203 ). It is important to note that acquiescence is a presumed tendency; many studies have remarked how difficult it is to clearly demonstrate that such a bias exists, although it seems like a reasonable explanation for why subjects tend to inflate their ratings of systems. Schuman and Presser (2004/1996) indicate that psychologists tend to view acquiescence as a personality trait, while sociologists and survey investigators tend to characterize acquies-cence as a form of deference or as the result of a respondent X  X  ignorance about a particular topic. Although it is often suggested that reversing the direction of items can correct acquiescence, changing the wording of items can change their meaning and in many cases this does not address a larger agreement issue: subjects are agreeing not so much with the statement, but with the experiment. This phenomenon is known as a demand effect. 2.3. Demand effects
Demand effects occur when subjects have an expectation of how they should behave in a particular research setting. These effects are described as demand effects because subjects may perceive that there is a demand for them to behave in a particular way. The source of the demand can be the researcher who behaves differently when subjects are evaluating the researcher X  X  system of choice, placing a demand on subjects to react a par-ticular way to one system and not another. The source of the demand can also be the subject X  X  interpretation of the experimental situation, in which case the subject X  X  behavior is contingent on him interpreting what desired effects are in any given experimental situation. In the context of interactive IR experiments, it seems reasonable for subjects to interpret that desired effects are likely to translate into positive system ratings, which might suggest why subjects tend to inflate their ratings of systems. In addition, subjects may not want to offend the researcher by rating a system poorly. 2.4. Cognitive and physical effort
An alternative perspective on differences observed due to questionnaire mode concerns the varying amounts of cognitive and physical effort required of subjects to complete questionnaires ( Schwarz, Strack, Hippler, &amp;
Bishop, 1991; Tourangeau, 1984 ). Researchers generally agree that answering a question requires subjects to perform several tasks: interpret the question and understand its meaning; generate an opinion or reflect on past behaviors, which typically consist of retrieving information from memory; and communicate and possibly edit responses. Questionnaire mode most often impacts efforts related to interpreting the question, under-standing its meaning and communicating and editing responses.

Cognitive efforts related to interpreting and understanding questions are similar in pen-and-paper and elec-tronic modes since the question (and often response set) are visible to subjects. However, cognitive effort is high in the interview mode since this information is usually read to the subject and he must keep it in memory while interpreting and forming a response. Different levels of effort are also required for communicating responses. To communicate responses via pen-and-paper questionnaire a subject writes, via electronic ques-tionnaire types, and via interview questionnaire speaks. While the effort required to communicate responses in each of these modes is ultimately related to the subject X  X  abilities to write, type or speak (or read for that matter), in general, one can assume that writing requires the most effort and speaking the least.
Editing one X  X  response requires both cognitive and physical effort, since it involves aspects of response gen-eration and communication. The amounts of cognitive effort in the pen-and-paper and electronic modes are somewhat similar. Pen-and-paper mode requires slightly more physical effort than the electronic since a person has to add/remove material with a pen or pencil. The cognitive demands of editing one X  X  response in the inter-view mode are high, while the physical are still relatively low. A person must remember his previous response, identify what parts he wants to edit, and then manage to communicate both the location of the edit as well as the actual content of the edit.

In the context of responding to questionnaires in interactive IR experiments, subjects may have a more dif-ficult time answering questions via interview mode because of cognitive demands. This effect is likely to be particularly marked for open questions, which already place high cognitive demands on the subject when for-mulating responses. Subjects may provide lengthier responses to open questions in the interview mode because this mode requires the least physical effort to communicate responses ( Groves, 1978 ). However, these responses may not necessarily be of a higher quality, given the higher cognitive demands of this mode. 2.5. Open and closed questions
In constructing questionnaires, another major decision that must be made concerns question format. Com-mon question formats include open and closed, and each has its own set of merits and demerits. Payne (2004/ 1951) identifies the merits of open questions and states that,  X  X  X he free-answer is uninfluenced, it elicits a wide variety of responses, it makes a good introduction to a subject, it provides background for interpreting answers to other questions. It can be used to solicit suggestions, to obtain elaborations, [and] to elicit reasons X  X  (p. 143). In addition, open questions allow subjects to report their own feelings, beliefs and impressions with-out being encumbered by a response set. Drawbacks to open questions are that they take longer to administer and responses are often difficult to interpret and analyze. People typically use different words to describe the same thing and some subjects are better at clarifying and explaining their responses than others. Since different subjects are likely to contribute different amounts of feedback, there is a danger that the opinions of a small number of subjects will dominate and bias results. In relation to questionnaire mode, subjects in a pen-and-paper or electronic questionnaire might skip open questions, only provide partial responses or provide responses which do not make sense. When open questions are administered in the interview mode, interviewers can ask for elaborations, explanations, and clarifications, and can probe subjects in a number of other ways.
However, when administered during an interview, the impact of the interviewer on the interaction is more salient ( Groves, 1978 ).

Krosnick (1991) and Brewer et al. (2004) provide some evidence that suggests that subjects often engage in a type of satisficing ( Simon, 1957 ) in order to reduce the cognitive burden placed on them when responding to open questions. Brewer et al. (2004) describe respondents X  satisficing response behavior:  X  X  X ecause formulating a full and complete answer may too effortful, they may offer a comprise response that they think is  X  X ood enough X  X  X  (p. 876). Krosnick (1991) distinguishes between two types of satisficing, weak and strong. In the case of weak satisficing, subjects X  responses only reflect a small portion of what they know because to search their entire memories for exhaustive responses would be too effortful. Often subjects truncate this search process as soon as enough information has come to mind to form a reasonable judgment ( Krosnick, 1991 ).

In the case of strong satisficing, subjects X  responses reflect little or nothing about what they actual know and believe. One tactic identified by Brewer et al. (2004) is to answer the first in a series of related questions truth-fully and then repeat that response (or variations of) in response to remaining questions. In the context of interactive IR, subjects who are fatigued at the end of a long experiment or those who try quickly to complete a questionnaire might engage in such tactics. For instance, a subject might identify one feature of the system or one usage instance and stay focused on this feature or instance when responding to questions, rather than con-sidering his entire search experience and all features of the system. It is unclear how mode might be related to satisficing behavior in face-to-face interviews. For instance, the interview mode might help mitigate satisficing because subjects may feel more accountable for producing unique responses. On the other hand, more satis-ficing might be observed in the interview mode because the pace of the interview may make subjects feel more pressure to respond quickly. With this added pressure, subjects may not take the time to produce measured responses.

Closed questions allow researchers to gather a larger amount of data in less time using standard ques-tions and scales for eliciting responses. Data elicited from closed questions are usually easier to analyze and interpret. Although it is possible for subjects to skip one or more closed questions, in general, the amount and form of data elicited from subjects via closed questions is much more homogenous than that elicited via open questions. Closed questions also have several limitations. As noted previously, questions can be subject to response biases such as acquiescence. Moreover, scales are not based on a true number line and scale values are subject to individual interpretation; one subject X  X  6 may be another subject X  X  5 .
Response sets provided for closed questions do not always capture the extent of a person X  X  opinions and researchers can introduce many biases when they construct scales and scale labels. With respect to interac-tive IR, data elicited via closed questions are often skewed toward positive ratings with little variance.
Researchers often find themselves comparing system ratings that differ by a single point or by tenths of points. Thus, it is imperative that researchers began to identify better ways of designing and administering closed questions. 2.6. Purpose of study
The literature reviewed above presents several factors that have been proposed to impact subjects X  response behaviors to questionnaires. In this study we investigate some of these factors in the context of an interactive
IR experiment. Specifically, we investigate the relationship between questionnaire mode and subjects X  responses to a usability questionnaire comprised of closed and open questions administered during an inter-active IR experiment. The questionnaire modes we investigate are pen-and-paper, electronic and interview. At least one study has been conducted in the area of library science comparing mode effects in the context of a survey administered to academic reference librarians ( Hayslett &amp; Wildemuth, 2004 ). This study compared pen-and-paper and electronic versions of a survey and focused primarily on response rates and sampling bias.
Aside from the studies cited earlier ( Rosenfeld et al., 1996; Weisband &amp; Kiesler, 1996 ), we were unable to find any studies of questionnaire mode effects in the HCI literature. To our knowledge, no study of questionnaire mode has been conducted in the context of interactive IR.

This study investigates the impact of social desirability responding and demand effects on subjects X  responses to closed questions, which were designed to elicit numeric ratings of two systems. Our hypothesis is that subjects will rate a system more positively in the interview mode than in the pen-and-paper and elec-tronic modes. Social desirability theory predicts that subjects will want to present themselves as favorably as possible. Research examining social desirability responding indicates that the greatest potential for this to occur will be in the interview mode.

We also wanted to investigate if the proximity of the researcher to the subject would create greater demands effects, which would also lead to more positive system ratings. The interview mode produces the greatest demand effect in this regard since the researcher is sitting next to the subject, engaged in a pseudo-conversation and making eye contact with the subject. We thus believed that this might also contribute to more positive ratings of the system from subjects in the interview mode vs. the pen-and-paper and electronic modes. It should be noted that we consider the source of the demand in this situation the subject X  X  interpretations of the situation and not the researcher X  X  behaviors.

Unfortunately, it is not possible to separate social desirability responding and demand effects in this instance, since they both offer plausible explanations for why differences may occur and previous research pre-dicts that these differences will be in the same direction. Although our findings will not allow us to state def-initely which of these variables had the greatest impact on the results, our findings can potentially allow us to say something about the relationship between questionnaire mode and subjects X  responses.

This study also investigates the relationship between questionnaire mode and subjects X  responses to open questions. Our hypothesis is that subjects will provide the shortest and least informative responses in the pen-and-paper mode because it requires more physical effort for subjects to complete. We hypothesize that subjects will provide the lengthiest and most informative responses in the interview mode, since this mode requires the least physical effort for subjects, there is a greater motivation for subjects to respond and since interviews are generally believed to elicit better responses to open questions. Although the higher cognitive demands in this mode may act to reduce the informativeness of responses, we believe the decreased physical demands will exert a stronger influence on behavior.
 3. Method
The purpose of this study was to investigate the effects of questionnaire mode on subjects X  evaluations of retrieval systems. This investigation was conducted within the context of an interface design study whose pur-pose was to investigate the effectiveness of search term highlighting on retrieval performance. The investiga-tion of mode effects was integral to the study, and the experiment was purposefully designed to explore both mode effects and interface effects. We chose to use an experimental IR system, rather than a baseline system (such as Google) because we felt that this would give subjects more things to discuss during the questionnaires.
If we presented a system with which subjects were already familiar, it is unlikely that they would have much to say.

Specific details of the interface investigation are not reported in this paper, but they are mentioned since they are necessary to understand the study design. The study was a 2  X  3 between-subjects factorial design; that is, each subject used one interface and experienced one questionnaire mode. Search term highlighting had two levels (highlighting or no highlighting) and questionnaire mode had three (pen-and-paper, electronic and interview). We created a balanced design where questionnaire mode was distributed equally across high-lighting levels.

Two researchers conducted this study and several steps were taken to minimize researcher effects. First, we systematically rotated researchers across conditions so that each researcher administered an equal number of cases with each mode X  X nterface combination. Second, a detailed experimental protocol was developed and used during the study which prescribed how the experiment would be administered (including scripts for inter-acting with subjects). Finally, during analysis, we included researcher as a variable to determine if there were significant differences in subjects X  responses based on researcher. 3.1. Participants
An email was sent to the entire undergraduate population at the University of North Carolina to solicit subjects for this study. We accepted the first 48 people who responded to the email and accepted additional students as alternates. Subjects were assigned randomly to condition and compensated US$20.00 for partic-ipation. In total, 52 subjects completed the experiment, but only 51 are used in analysis (17 per mode). One subject was considered an outlier and excluded from the study because this subject X  X  age was over seven stan-dard deviations above the mean. The mean and standard deviation for age including the outlier was 21.21 and 4.19; excluding the outlier the figures were 20.67 and 1.47. Since the subject differed so substantially from the rest of the subjects on this variable, we felt her exclusion would provide us with a more cohesive sample.
Sixty-seven percent of the subjects were female and 33% were male. Ninety percent of the subjects were undergraduates, while 4% were graduates and 6% were  X  X ther X  (e.g., continuing education students). Thirty percent of the subjects were social science majors, 24% were humanities majors, 22% were science majors, 22% were part of the professional schools (e.g., business) and 2% did not respond to this question. The means and standard deviations for subjects X  search experiences and frequencies of Web searching were as follows: 3.04 (.49) and 3.73 (.53). For search experience,  X  X 1 X  X  indicated very inexperienced and  X  X 4 X  X  indicated very expe-rienced. For frequency of Web searching,  X  X 1 X  X  indicated less than monthly and  X  X 4 X  X  indicated daily. As expected, this group is fairly experienced with respect to searching for information and searches the Web with a great deal of frequency. 3.2. Interface and system
In this study, subjects used the conteXtual relevance feedback system (XRF) to find and save documents for four search topics ( Harper &amp; Kelly, 2006 ). The XRF interface allows users to save documents in piles and perform relevance feedback using documents contained within these piles. Fig. 1 shows the XRF interface for a search on the topic  X  X  X ropical storms, loss of life and damage, X  X  where the user is sorting documents according to the name of each identified storm. A query is entered by selecting the search pile (1), search results are displayed in (2), and the full texts of documents are displayed in (3).
XRF enables a user to save documents in one or more piles (4) and create labels for these piles. Piles have preset colors to help with identification. A document can be moved into and out of a pile by using the arrow under each pile (5). Small pile icons below the title in each result show piles in which a document is saved (6). The contents of a pile can be reviewed by clicking on a pile. Relevance feedback is invoked by clicking the
Similar button on a pile (7), where the relevance feedback (RF) process assumes that all documents in a pile are relevant. Support for highlighting varied in this study, with one system providing no highlighting and another providing some. In each opened document, the  X  X ighlighting X  system highlighted subjects X  current query terms (grey highlight), previous query terms (bold face), and terms used by subjects as pile labels (pile color highlight). Interested readers are referred to Harper and Kelly (2006) for more information about the
XRF system; we note that the interface described therein did not support highlighting. 3.3. Collections and tasks The TREC-8 Interactive Track collection, consisting of a corpus of 210,158 articles from the Financial Times of London 1991 X 1994, a set of aspectual search topics, and a set of relevance judgments was used in this study ( Hersh &amp; Over, 1999 ). An example aspectual recall tasks entitled  X  X ropical storms X  is displayed in Fig. 2 .

We wanted subjects to complete tasks that required them to find comprehensive and exhaustive informa-tion, so we modified the six TREC aspectual topics. The new topics asked subjects to find documents covering as many aspects as possible (comprehensiveness) and also as many documents as possible relating to each aspect (exhaustiveness). We further added text describing an information seeking scenario by combining the description and instance fields. The modified version of the tropical storms topic is displayed in Fig. 3 .
The original text from the TREC-8 Interactive Track topic is italicized. We used five of six available topics in this study: one topic was used in video demonstrations of each system and four were used by subjects.
A Latin-square was used to rotate topics. 3.4. Procedures
The study took approximately 1.5 h to complete. When subjects arrived to the laboratory, they completed a consent form and a background questionnaire. Following this, subjects were presented with a video demon-stration of the system that they would use with an example search scenario. Next, subjects were presented with their first search scenario. Subjects had up to 15 min to search for documents related to each scenario. After subjects completed one scenario, they were asked to complete a Post-Search Questionnaire, which asked them to make estimates of a number of things, such as the amount of time they spent searching for documents related to their first query, and the number of documents they saved. Data from these questionnaires are not reported in this paper; however, the mode of delivery for these questionnaires matched the mode used in the Exit Questionnaires. In total, subjects completed four search topics and four Post-Search Question-naires. At the end of the study, subjects completed the Exit Questionnaire. The Exit Questionnaire is the focus of this study. 3.5. Exit questionnaire
The content of the Exit Questionnaires was identical across modes. The first 21 questions were closed ques-tions designed to assess the usability of the systems. These were not questions per se, but statements where subjects indicated the strength of their agreement. All 21 of these questions were assessed with 7-point scales, where 1 = strongly disagree and 7 = strongly agree. Circles, or radio buttons, corresponded to each numeric value on the scale and subjects marked the value that represented their beliefs.

A set of eight general usability questions were taken from the USE Questionnaire ( Lund, 2001 ), while the remainder of the questions ( n = 13) were developed to evaluate features specific to the XRF systems. The con-tent of all questions and the order in which they were asked are displayed in Table 3 . These questions were designed to evaluate the following aspects of usability, with the number(s) corresponding to the question(s) assessing each aspect in parenthesis: ease of learning (2), ease of use (3 X 8), usefulness (9 X 10), effectiveness (11 X 19) and satisfaction (20 X 21). We included one additional question to assess the similarity of the search methods used by subjects in this study to those they normally use when searching the Web.

The following four open questions were included as questions 22 X 25 of the Exit Questionnaire: (a) What were the most positive things about using this system and why? (b) What were the most negative things about using this system and why? (c) How would you improve this system and why? (d) Is there anything else that you would like to tell us about this system and your experiences using it? These general questions are typical of those asked during interactive information retrieval studies.

In the pen-and-paper mode, subjects were provided with a print version of the questionnaire, which was produced using a word processing program. In the electronic mode, subjects were provided with an electronic version of the questionnaire, which was produced using XHTML and CSS. Subjects viewed the electronic questionnaire with the Firefox Web browser.

We used separate programs to produce the pen-and-paper and electronic questionnaires because we did not want the pen-and-paper questionnaire to look like a printed Web document. The reason for this was that we did not want subjects to even consider the questionnaire mode  X  if subjects were given a questionnaire that had clearly been printed from the Web, then they might wonder why they were not completing the questionnaire on the Web. For each questionnaire, we used identical layout, font and design, including identically sized visible areas for responses. Additional space for responding to open questions in the pen-and-paper mode was found on the back of the questionnaire (and in the margins), while the text areas in the electronic mode were set to wrap and to have no limit on the number of possible rows (i.e., all or part of the response was visible to subjects at all time and subjects could type as much as they liked).

For the interview mode, a protocol was developed which prescribed how interviews were to be conducted, including instructions for asking follow-up questions. It should be noted that scripting interviews in this man-ner necessarily means that the interviewer loses some flexibility, which of course, is one of the benefits of con-ducting interviews. However, we wanted to minimize interviewer effects as much as possible, especially since one interviewer was more experienced than the other. In general, follow-up questions were only asked if the subject did not answer the entire question or if the researcher did not understand the subject X  X  response. Inter-viewers were instructed to maintain a neutral facial expression and to nod if appropriate. For closed questions, interviewers presented subjects with a scale for responding (to reduce cognitive effort), read each statement to subjects, and asked subjects to simultaneously point to and say aloud the number on the scale that represented their opinions. The interviewer recorded subjects X  responses on a printed version of the questionnaire. It is obvi-ously impossible to make all interviews identical, even in cases when there is a single interviewer. However, large-scale survey efforts employing multiple interviewers (cf., US Census) have demonstrated that through training, differences can be minimized. Interviews were also recorded and transcribed later for analysis. 3.6. Analysis of responses
The analysis of quantitative data from the closed questions was somewhat straight-forward  X  numeric scores were entered into SPSS and analyzed. However, the analysis of data from the open questions was a bit more difficult. We initiated a content analysis of responses to the open questions to divide subjects X  responses into units that could be analyzed, a process known as  X  X  X nitizing. X  X  Neuendorf (2002) describes the unit in content analysis as,  X  X  X n identifiable message or message component, (a) which serves as the basis for identifying the population and drawing a sample, (b) on which variables are measured, or (c) which serves as the basis for reporting analyses. Units can be words, characters, themes, time periods, interactions, or any other result of  X  X reaking up a  X  X ommunication X  into bits X  ( Carney, 1971, p. 52 ). X  X  (p. 71).  X  X  X nitizing X  X  is the process of separating a stream of actions or words into discrete units. Unitizing spoken words, particularly those created in a conversational setting such as an interview, are notoriously difficult because people usually do not speak in complete sentences and can often have many fits and false starts. Neuendorf (2002) cites evi-dence ( Newtson, Engquist, &amp; Bois, 1977 ) that demonstrates that while people are generally good at experienc-ing a stream of words as coherent units auditorily, attempts to instruct coders to unitize such speech acts formally are often met with failure, especially when articulating unitization rules.

To unitize subjects X  responses to open questions, two of the researchers coded responses independently with the goal of subdividing responses into the smallest possible unit. This usually meant subdividing based on mentions of features (e.g., piles, highlighting, Similar feature) and reasons. For instance, in response to the first question regarding the most positive things about the system and why, subjects would often identify one or more features and state why they felt they were positive. In many other cases, subjects only identified a feature without providing a reason. We thus refer to our units as features/reasons, which reflect that units can consist of a feature, if no reason is provided, or a feature X  X eason pair, if both a feature and reason are provided. The initial coding process captured raw occurrences, so if the same feature, or feature X  X eason pair was mentioned more than once, it was counted as many times as it appeared. Table 1 presents example responses from Subjects 13 and 34 along with how these responses were divided into units. Subject 13 was in the pen-and-paper condition and Subject 34 was in the electronic condition. These responses are represen-tative of all subjects X  responses.
 Before the unitizing process began, coders met to discuss the process and to code several responses together.
After coding units independently, coders meet to compare results and discuss differences. In general, there was very low agreement about what constituted a unit  X  in most cases agreement was around 60% and with inter-view responses, this number was even lower. Responses produced in the pen-and-paper and electronic modes tended to be in the form of sentences or phrases, which made unitizing responses a bit easier (as demonstrated by the examples in Table 1 ). Responses produced in the interview mode were usually quite long, with a lot of incomplete utterances, false starts and backtracking. Because of the low agreement, coders engaged in a con-sensus method to resolve disagreements and produce a final listing of units for each response.

We developed several measures to characterize subjects X  responses. Our hypothesis regarding these responses indicated expected differences in the lengths and informativeness of responses. We operationalized length as the raw number of words in each response. This measure included stop words because in most cases stop words were necessary to understand the meaning of subjects X  responses. We conceptualized informative-ness using several different dimensions and operationalized these dimensions accordingly. All of these mea-sures are based on the basic units (features/reasons). Table 2 presents all such dimensions and how we measured them. 4. Results
Before reporting results regarding the two main hypotheses of this study, it is necessary to rule out the pos-sibility that any potential differences were caused by experimenters. T -tests were conducted using experimenter as the independent variable and responses to closed question, and measures of length and informativeness of responses to the open-ended questions as dependent variables. Results from these tests were non-significant allowing us to eliminate the possibility that potential differences were due to experimenter effects. 4.1. Closed questions
The first hypotheses stated that subjects in the interview mode would rate systems more favorably than sub-jects in the pen-and-paper and electronic modes. Table 3 shows each closed question, means and standard deviations for scores in each mode, F -scores associated with the one-way ANOVA and results from the post-hoc tests (Diff.). The degrees of freedom for all tests is 2,48. Statistically significant F -scores are marked with asterisks. Results reported in the Diff. column indicate between which modes statistically significant dif-ferences were detected with Scheffe X  X  post-hoc tests; for example, E &gt; I means that the mean score for the elec-tronic mode was significantly higher than the mean score for the interview mode. For 10 of the 21 questions, statistically significant differences in scores were detected between modes, and in all of these cases scores assigned by subjects in the electronic mode were significantly higher than scores assigned by subjects in the interview mode.

Although no support was found for our first hypothesis, these results provide some insight into mode effects in interactive IR experiments. Readers are reminded that the basic assumption is that lower scores represent more critical, and hence more valid, scores since subjects tend to inflate their ratings of systems. Overall, the general trend was that scores assigned by subjects in the interview mode were lower than scores assigned by subjects in the pen-and-paper and electronic modes. Table 4 shows each pattern that occurred in the data, the frequency with which these patterns occurred, and the number of statistically significant results that each pattern represented. Patterns represent relationships between scores, so that I &lt; P &lt; E means that the average score for the interview mode was the lowest, for the pen-and-paper mode second lowest and for the electronic mode highest.

This data seem to suggest a trend for subjects in the interview mode to be more critical in their ratings of systems than subjects in the electronic mode. For 18 out of 20 questions, subjects X  responses in the electronic mode were higher than subjects X  responses in the interview and pen-and-paper modes, and subjects X  mean rat-ings in the electronic mode were never the lowest. Moreover, all 10 of the statistically significant differences displayed in Table 3 were in the same order. In most cases, differences between the interview and pen-and-paper modes were negligible, which suggests that subjects in the pen-and-paper mode were also more critical than subjects in the electronic mode. However, post-hoc tests did not demonstrate significant differences between the pen-and-paper and electronic modes.

Another way to examine the data is to explore the extent to which subjects X  ratings vary. This analysis pro-vides an indirect way to explore acquiescence, where greater acquiescence will be indicated by less variance across ratings assigned by a single subject. In other words, little variance will be observed in cases where a subject responded to all questions with the same or similar scores (more acquiescence) and greater variance will be observed in cases where a subject used a variety of ratings (less acquiescence). Since all ratings were on the same scale and in the same direction, we created variables representing the average and standard devi-ation for each subject across all 21 closed questions. The means and standard deviations for the average and variance variables are presented in Table 5 . Overall, subjects in the electronic mode had a significantly higher average rating than subjects in the pen-and-paper and interview modes, F (2,48) = 4.42, p &lt; .01, which is con-sistent with the previous results, and a lower variance than subjects in the pen-and-paper and interview modes, although not significantly so. It is interesting to note that again, scores elicited via the pen-and-paper and interview questionnaires are similar to one another, but slightly different from those elicited via the electronic.
Overall, it appears that all subjects, regardless of mode, are only using a small range of numbers to describe their attitudes about the system. 4.2. Open questions
The second hypothesis stated that subjects X  responses to open questions in the interview mode would be longer and more informative than subjects X  responses in the pen-and-paper and electronic modes. Fig. 4 displays the mean lengths of subjects X  responses to all four questions according to mode, along with bars rep-resenting the standard deviations ( X one standard deviation). The average lengths and standard deviations of subjects X  responses in the pen-and-paper, electronic and interview modes were: 80.82 (27.47), 136.65 (86.35) and 262.76 (118.36), respectively. These differences were highly significant, F (2,48) = 19.94, p &lt; .000 ( R 2 = .45) and post-hoc tests detected statistically significant differences for all pairs. These results provide support for our hypothesis regarding length of subjects X  responses to open-ended questions according to ques-tionnaire mode.

Looking at the lengths of subjects X  responses provides only one way to compare differences between modes; we were also interested in looking at the informativeness of subjects X  responses. There is more physical effort involved in responding to pen-and-paper questionnaires than electronic questionnaires, and more effort with electronic than with interview. The average lengths of subjects X  responses reflect this to a certain extent and are therefore, not too surprising. Examining the informativeness of subjects X  responses is crucial since subjects in the interview mode could merely be repeating themselves, discussing irrelevant topics, or simply using more words to express the same number of ideas. Fig. 5 shows the average number of units identified in subjects X  responses, as well as the number of unique units per question and per response. With respect to total units, we see the same trend in Fig. 4 , although not quite as pronounced (pen-and-paper: 7.59 (1.66); electronic: 8.76 (2.74); and interview 13.47 (5.32)). These difference were statistically significant, F (2,48) = 12.76, p &lt; .000 ( R 2 = .35) and post-hoc tests detected significant differences between all pairs. Thus, it appears that subjects in the interview mode are expressing more ideas than subjects in the pen-and-paper and electronic.
When we look at the uniqueness measures, however, the figures converge. These results indicate that over-all, each mode is eliciting a similar amount of usable feedback. While it appears that subjects in the interview mode are expressing more ideas, many of these ideas are the same; subjects are not necessarily providing any new ideas, but rather seem to be repeating their ideas. The means for unique units per question for pen-and-paper, electronic and interview were 6.29 (1.96), 6.35 (1.41) and 6.88 (2.45), respectively, and the means for unique units per response were 5.65 (1.73), 5.50 (1.62), and 5.88 (2.18), respectively. None of these differences were statistically significant. Thus, subjects identified similar amounts of unique units at the question level and at the response level regardless of mode. Note that for each measure, greater standard deviations occurred in the interview mode, which demonstrates that there was greater variability in this mode.

Subjects in the pen-and-paper and electronic modes provide a similar amount of usable feedback as subjects in the interview mode, but do they do so in fewer words? Fig. 5 seems to suggest this, but to investigate this question we look at the efficiency of subjects X  responses. As a reminder, efficiency was measured by the number of unique units identified by a subject divided by the total number of units identified by that subject. Fig. 6 shows the efficiency of subjects X  responses. There were significant differences in the efficiency measure across condition, F (2,48) = 13.08, p &lt; .000 (pen-and-paper: .83 (.18), electronic: .76 (.17) and interview: .53 (.17)).
Post-hoc tests showed that differences between all pairs were statistically significant. Combined with the results in the preceding paragraph, these results suggest that while subjects in the pen-and-paper mode identify the same amount of unique units as subjects in the electronic and interviews modes, they do so in fewer words, with less repetition. It appears that the extra effort required to communicate one X  X  responses in the pen-and-paper mode may have some impact on the conciseness of subjects X  responses.

After examining subjects X  responses to open questions, we noticed two other behaviors, which might be construed as indicators of satisficing. We noticed a number of back-references in subjects X  responses. Back-ref-erencing describes the behavior where a subject responds to one question by referencing his response to another question. The most egregious example of this was subjects writing  X  X  X ee previous response, X  X  which was observed three time in each of the pen-and-paper and electronic modes, but only once in the interview mode. The v 2 test did not show that this distribution was statistically significant, but it appears that back-referencing is more likely to happen in self-administered questionnaire modes. The interview mode may make back-referencing more difficult to do because of the interaction with the interviewer. Given that subjects in the interview mode used significantly more units to communicate a similar amount of unique feedback, it may be the case that back-referencing in the interview mode happens through repetition, rather than explicit reference to previous responses. We observed one extreme satisificing tactic in the electronic condition: a subject copy-and-pasted his response to one question into the response area for another question. Although we only observed a single instance of this behavior, it seems reasonable that electronic questionnaires might encourage more extreme forms of satisficing. 5. Discussion
In this study we investigated the relationship between questionnaire mode and subjects X  responses to a usability questionnaire comprised of closed and open questions administered during an interactive IR exper-iment. Social desirability theory has been used extensively to explain a number of differences that have been observed between questionnaire modes, and it was used in part to motivate the hypotheses of this study. Three questionnaire modes (pen-and-paper, electronic and interview) were explored with 51 subjects who used one of two information retrieval systems. We hypothesized that subjects X  quantitative ratings of systems would be more positive in the interview mode than in the pen-and-paper and electronic modes. We further hypothesized that subjects would provide longer and more informative responses to open questions in the interview mode than in the pen-and-paper and electronic modes.

Results demonstrated that subjects X  quantitative ratings of systems in the electronic mode were significantly more positive than subjects X  ratings in the pen-and-paper and interview modes. The general trend was that scores assigned by subjects in the interview mode were lower than scores assigned by subjects in the pen-and-paper and electronic modes, and that scores assigned by subjects in the interview and pen-and-paper modes were more alike than those assigned by subjects in the electronic mode. Overall, subjects in the inter-view and pen-and-paper modes were more critical in their ratings of systems than subjects in the electronic mode. These results suggest that researchers should use the interview mode to elicit responses to closed questions.

Although no support was found for the first hypothesis, our results provide some important insight into mode effects in interactive IR experiments. Social desirability theory would predict that subjects X  ratings in the interview mode would be more positive than subjects X  ratings in the pen-and-paper and electronic modes. However, it does not appear that this theory offers a good explanation of what happened in this study. It may be the case that subjects in the interview mode believed that their responses were more likely to be valued and used since they were presenting them directly to a researcher and, as a result, were more critical in providing feedback. Accordingly, it may be the case that subjects in the electronic mode were least critical because they believed they were submitting feedback to a  X  X lack hole X  and it was unclear to them when a person would review their comments. In some ways, this is related to the idea of anonymity described earlier, only in an opposite way than was proposed by previous researchers ( Richman et al., 1999 ).

Another possible explanation for these results is related to the mode of delivery for the entire experiment and the concept of flow ( Csikszentmihalyi, 1997 ). All subjects used a computer to complete the primary por-tion of the experiment (using an experimental IR system). While subjects in the pen-and-paper and interview modes switched interaction styles after using the system to complete the Exit Questionnaire, subjects in the electronic mode did not switch styles, and instead followed a link to an electronic version of the questionnaire and continued to use the mouse, keyboard and monitor to communicate responses. It is proposed that the flow that was maintained in the electronic mode caused subjects to be less critical and thoughtful. In this situation, flow prevented users from recognizing a task switch and adjusting their behaviors accordingly. Subjects X  flows in the pen-and-paper and interview modes were interrupted (or perhaps disrupted is more appropriate) which functioned to signal the end of one part of the experiment and perhaps gave them a few moments to reflect on their experiences before they began the next. This, in turn, may have caused them to be more critical and thoughtful. Overall, these results suggest that in interactive IR experiments, some interruption should happen to signal the end of one part of the experiment before subjects move on to another part. Eliciting usability ratings through interviews appears to be one way to accomplish this. This can also be accomplished by pro-viding subjects with a break or asking subjects to complete an unrelated task away from the computer.
Another possible explanation for why subjects X  ratings were more critical in the interview mode is related to the pace of the questionnaire. Previous studies have demonstrated that the pace of an interview can often impact response quality, especially in the context of telephone interviews ( Groves, 1978 ). It is conjectured that in this study, the interview mode slowed down the pace of the questionnaire which may have resulted in more critical ratings. Subjects were unable to rapidly skim questions and response sets; instead they were required to listen carefully and move at a pace set by the interviewer.

Although there were significant differences in usability ratings according to mode, overall, subjects X  ratings were still quite high and most subjects used only a small range of numbers on the higher end of the scale to characterize their opinions. Subjects still seem to exhibit acquiescence, whether in relation to their tendency to agree with attitude statements or in relation to their tendency to agree with the experiment (demand effects).
Although we did not vary the direction of our statements to test for item acquiescence, acquiescence still appears to be a problem that needs to be addressed.

With respect to subjects X  responses to open questions, results of the study demonstrated that subjects in the interview mode provided significantly longer responses to the four open questions. Subjects X  responses in this condition also contained significantly more units, although there were no differences in the number of unique units subjects identified across mode. While subjects in the interview mode identified more units than subjects in the pen-and-paper and electronic modes, many of these units were repetitions of previously identified units.
Results showed that subjects in the pen-and-paper mode were significantly more efficient in communicating their responses than subjects in the electronic or interview modes, and that subjects in the electronic mode were significantly more efficient than subjects in the interview mode.

Overall, subjects in the pen-and-paper mode produced the most concise responses, which is likely related to the increased physical effort involved with producing responses. Although the lower physical effort associated with the interview mode may have encouraged subjects to produce more data, it was not necessarily better data. Instead, it appears that the interview mode may have encouraged subjects to engage in more satisficing behaviors. While the decreased pace provided by the interview mode was beneficial for closed questions, it may have caused subjects to respond more quickly to open questions to avoid uncomfortably long silences.
Because these responses were generated more quickly, they likely contained more repetition. There was also no visual instantiation of subjects X  responses, and this too, may have caused more repetition. The pen-and-paper and electronic modes were also not free of satisficing behaviors. There were six instances of back-referencing in the pen-and-paper and electronic modes, while only one instance was observed in the interview mode. Each mode apparently elicits a different type of satisficing behavior.
It is important to note one very important benefit to having subjects respond to open questions via pen-and-paper or electronic questionnaire: subjects X  responses in the pen-and-paper and electronic modes were much more well-formed than subjects X  responses in the interview mode. This made analyzing the data much easier. Subjects X  responses in the pen-and-paper and electronic modes were typically well-formed sentences.
Subjects X  responses in the interview mode were typically disjointed utterances, with a number of fits and starts, and lots of back-tracking and digression. Even though there was more physical effort required of subjects to communicate their responses in the pen-and-paper and electronic modes, this extra effort seems to have resulted in much more well-formed, concise and understandable responses. Editing can be cumbersome in the pen-and-paper mode and this may have also impacted the conciseness of subjects X  responses in this mode.
The relative well-formedness of subjects X  responses had many implications for the overall costs associate with conducting this research. Not only were there greater research costs associated with analyzing data pro-duced in the interview mode, there were also greater costs associated with transcribing interviews. The process of unitization was extremely arduous in this condition, which likely affected the reliability of the coding. Sur-prisingly, costs associated with entering data collected via pen-and-paper mode were minimal because data was entered by the researcher via Web form immediately following subjects X  experimental sessions or during consecutive experimental sessions. Thus, with respect to eliciting subjects X  responses to open questions, results of this study suggest that pen-and-paper or electronic questionnaires are optimal; if the latter technique is used it is important to include a break between subjects X  uses of the system and their completion of questionnaires.

While this work represents a reasonable contribution to the literature, its results are by no means definitive and there are some limitations to consider. As with all other studies, the particular users, tasks and systems may have influenced the results. Our users were paid undergraduate students from one university in the US who completed four tasks with two experimental systems. Although we included a variable for interviewer and were careful with our study design, there still may have been some differences caused by interviewer.
Finally, the process of unitizing responses was very difficult; our attempts to independently code units resulted in low reliability, which required us to follow this approach with the consensus method. 6. Conclusions
Questionnaires are an important part of interactive IR studies and are used extensively to collect a wide range of measures from users, most notably system usability ratings. However, subjects X  general tendencies to inflate their ratings of systems call into question the validity of data collected via questionnaires and make it virtually impossible to make reliable system comparisons. This, in turn, places severe limitations on what researchers are able to learn about interactive IR systems and processes. Therefore, identifying better ways of collecting usability data from subjects should be one of the chief concerns for interactive IR researchers.
In this study, we investigated some factors related to subjects X  response behaviors to questionnaires. There are two main findings of our study. First, closed questions administered in the interview mode elicited lower ratings on average than pen-and-paper or electronic modes. Second, open questions in the pen-and-paper or electronic modes resulted in more concise and coherent responses, with no appreciable reduction in informa-tion content, even though responses in interview mode tended to be significantly longer. Furthermore, verbal responses obtained in interview mode were far harder to process and analyze. These results suggest that, at least for some types of interactive IR experiments, the post-system questionnaire takes the form of an inter-view for closed questions, followed by pen-and-paper or electronic mode for open questions. The interview serves the useful purpose of clearly demarcating the performance of the task(s) from the evaluation of the sys-tem. It seems likely that this would also hold for post-task questionnaires, but this would need to be confirmed by further study.

Our results provide some insight into the relationship between questionnaire mode and subjects X  response behaviors. Although it may seem cliche  X  to say so, more work should clearly be done in this area. It is vital to the field of interactive IR that adequate attention is paid to understanding more about how research design can potentially impact results, to developing and improving our data collection techniques, and to training and educating new researchers on the intricacies of behavioral research. It is sometimes easy to forget that our study results are only as good as our study methods.
 Acknowledgements
We would like to thank Professors Barbara Wildemuth and Stephanie Haas from UNC for their helpful discussions about this paper.
 References
