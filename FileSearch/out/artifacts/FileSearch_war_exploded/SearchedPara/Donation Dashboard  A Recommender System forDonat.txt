 In this paper we present Donation Dashboard, a system that recommends non-profit organizations to users in the form of a portfolio of donation amounts. Recommendations are made using our Eigentaste 2.0 constant-time collaborative filtering algorithm in combination with a new method for generating a weighted portfolio of recommendations. The key challenge is to generate a customized portfolio that doe s not necessarily exclude items already rated by the user. Un-der our method, the weights for items in the portfolio that have not yet been rated by the user are normalized factors of their predicted ratings, and the weights for items previ-ously rated by the user are normalized factors of the actual ratings. Donation Dashboard 1.0 launched in April 2008, and as of May 8 2009 we have collected over 59,000 ratings of 70 nonprofit organizations from over 3,800 users.
In this working paper we describe our experience devel-oping Donation Dashboard, including the design of the sys-tem and our new method for portfolio generation. We use Normalized Mean Absolute Error (NMAE) to measure the accuracy of Eigentaste using our dataset of non-profit orga-nization ratings and we compare that with the global mean algorithm. We analyze the data collected since the launch of the site, and we have made our dataset available to the public. Donation Dashboard and the Donation Dashboard dataset are accessible at: http://dd.berkeley.edu http://dd.berkeley.edu/dataset H.3.3 [ Information Search and Retrieval ]: Information filtering; H.3.3 [ Information Search and Retrieval ]: Re-trieval models Algorithms, Human Factors, Experimentation Figure 1: Screenshot of a single non-profit in Dona-tion Dashboard 1.0, with the continuous rating bar at the bottom. mend the best set of items as opposed to maximizing the probability that individual items will each be rated highly . Recommendation lists generated by the latter strategy tend to exhibit low levels of diversification; that is, items of th e user X  X  favorite and/or most frequently rated genre are rec-ommended more frequently, commonly referred to as the portfolio effect . Ziegler et al. study ways of mitigating the portfolio effect by improving topic diversification in recom -mendation lists [17]. They model and analyze properties of recommendation lists, including a metric for intra-list si mi-larity that quantifies the list X  X  diversity of topics. Zhang and Hurley [16] model the goals of maximizing diversity while maintaing similarity as a binary optimization problem and seek to find the best subset of items over all possible subsets .
While most recommender systems are  X  X ingle-shot, X  con-versational recommender systems allow users to continuall y refine their queries. Like Donation Dashboard, conversa-tional recommenders will often present items that have been previously recommended. Recent papers on conversational recommender systems include: [9, 15, 4, 14]. McGinty and Smyth consider various forms of user feedback in [11].
We also label the rating scale of Donation Dashboard in a specific way so as to encourage correct behavior. More generally, Cosley et al. [6] investigate the rating scales u sed with recommender interfaces and the effects of their designs on user actions.
In this section we describe the algorithm and interface design of Donation Dashboard 1.0, including a review of the user experience and strategic choices we faced.
Non-profit organizations are presented one at a time in the form of a name, logo, motto, website URL, and short description. Figure 1 shows a screenshot of a sample display .
We chose the short descriptions by manually selecting statements from non-profit information sources that we felt best described the mission and activities of each organiza-tion. We aimed to be as unbiased as possible while allowing a time. Every time he rates an organization the system displays the next one, until he has rated all 15. At this point the system presents the user with a portfolio of donation amounts as described below in Section 3.5. The user can then opt to continue rating items to improve his portfolio.
The first five non-profit organizations presented is a fixed  X  X auge set X  of items that every user is asked to rate, as de-scribed further in Section 3.4. The next five are a  X  X eed set, X  or organizations with the least amount of ratings in the sys-tem; using a seed set ensures that as long as the total num-ber of organizations in the system is relatively small they will each end up with a significant number of user ratings. The last five of organizations are those with the highest pre-dicted ratings via the collaborative filtering algorithm us ed (see Section 3.4). If the user opts to continue rating items to improve his portfolio, he is presented with organization s in descending order of their predicted ratings.
Donation Dashboard 1.0 currently uses Eigentaste 2.0, a constant online-time collaborative filtering algorithm th at addresses the cold-start problem by collecting real-value d ratings of a X  X auge set X  X f items from all users [7]. This gaug e set consists of the organizations with the highest variance in user ratings, as this allows for the quick identification of users with similar interests.

Eigentaste 2.0 is divided into an offline phase and an on-line phase. Offline, principal component analysis (PCA) is applied to the ratings matrix and the users are projected onto the two-dimensional eigenplane. Due to a high concen-tration of users around the origin, a median-based cluster-ing algorithm referred to as recursive rectangular cluster ing is used on the lower-dimensional space to divide users into clusters. This method ensures that cluster cell size decrea ses near the origin, resulting in evenly populated clusters.
Online, the position of a new user in the eigenplane can be determined in constant-time by taking the dot product of the user X  X  ratings vector of the gauge set and the first two principal components of the ratings matrix. Given the position of new user u , we can determine to which cluster u belongs, and the predicted rating for user u of item i is the average rating of i by all users in the same cluster.
Several factors motivate the choice of a weighted item portfolio over competing alternatives such as an un-weight ed list of recommendations or a single organization. First, th e natural visualization of a portfolio as a pie chart provides a more engaging user experience. Second, it emphasizes the point that we are not trying to determine which nonprofits are  X  X ad X  or  X  X ood, X  but rather that we seek a customized recommendation of donation amounts .

One of the greater challenges in recommending a portfolio of donation amounts to nonprofit organizations arises with the idea that we do not want to exclude organizations that the user has already rated; this is because the item class is not meant for one-time consumption, unlike movies or books. The concept of a portfolio is also applicable to other item classes such as stock investments. A less obvious but equally relevant application is the recommendation of musi c playlists; in this case, we might have a set of songs that the user enjoys and the problem would be to determine with what frequency each song or artist should be played. the most rated organization has received 3,061 ratings and the least rated organization has received 491 ratings. The average rating for a nonprofit is -0.65, where ratings can range between -10.00 and 10.00. A histogram of all ratings is shown in Figure 3.

The most highly rated organizations are Doctors With-out Borders, Kiva, the Public Broadcasting Service, Planne d Parenthood Federation of America and Engineers Without Borders, with average ratings of 2.68, 2.29, 2.28, 1.60 and 1.46, respectively. The organizations with the lowest rati ngs are the NRA Foundation, the Heritage Foundation, People for the Ethical Treatment of Animals, Boy Scouts of America and Prison Fellowship, with average ratings of -6.37, -5.10 , -4.90, -3.74 and -3.65, respectively. Those with the larges t variance in ratings are National Public Radio, the Humane Society of the United States, the Wikimedia Foundation, the American Society for the Prevention of Cruelty to Animals and St. Jude Children X  X  Research Hospital.

We use Mean Absolute Error (MAE) as described in [3] to evaluate Eigentaste 2.0 on the Donation Dashboard dataset, and we also list the Normalized Mean Absolute Error (NMAE). We compare to the global mean algorithm and the results are as follows:
The error initially decreases as the cluster count increase s; however, the error increases once the cluster count reaches 16. As we collect more data and more users, the MAE for higher cluster counts should decrease. Note that users are currently divided into 8 clusters on the live system, which i s optimal for now.
One of the greater challenges with this type of applica-tion is determining mathematically what makes one portfo-lio necessarily better than another, and subsequently to de -sign a good portfolio generation algorithm. A possible mea-sure of comparison is the diversity of items recommended in the portfolio, in which case we may seek to recommend a set of organizations that cover a range of issues with mini-mal overlap. Our next step in this project involves a more in-depth study of portfolio generation methods, particula rly when the portfolio may include items already rated by the user.

Data collected indicate that rating individual items on an absolute scale has significant dependencies on the order in which the items are presented. To mitigate this bias, we are developing a graphical model and algorithm that use relative ratings to generate portfolios.

We plan to compare Donation Dashboard to other algo-rithms, particularly Probabilistic Latent Semantic Analy sis (pLSA) [10], and to implement attack prevention protocols in the future so that non-profits are not able to promote themselves via false accounts and ratings. Some recent pa-pers on security of recommender systems are [13, 5, 12].
