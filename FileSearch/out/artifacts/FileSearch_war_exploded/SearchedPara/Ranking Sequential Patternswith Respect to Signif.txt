 1.1 Motivation Frequent sequential pattern mining, introduced in [1], has established itself as one of the most important data mining frameworks with broad applications including analysis of time-related processe s, telecommunications, bioinformatics, business, software engineering, Web click stream mining, etc [9]. The problem is defined as follows. Given a collection of ite mset-sequences (sequence database of transactions) and a minimum frequency (support) threshold, the task is to find all subsequence patterns, occurring across the itemset-sequen ces in the collection, whose frequency is greater than the mi nimum frequency threshold. The main focus of the research on sequential pattern mining has been on devising efficient algorithms for discovering frequent sequential patterns (see [9] for a review). Although state of the art mining algorithms can efficiently derive a complete set of frequent sequential patterns under certain constraints, the main problem is that the set of frequent sequential patterns is still too large for effective usage [9]. The two most effective methods for reducing the large set of frequent sequential patterns have been: closed sequential pattern mining [12] and maximal sequential pattern mining [5]. However no methods for assessing interestingness of sequential patterns have been proposed while such methods are very important to advance the applicability of frequent sequential pattern mining. By comparison, such methods have been proposed for subsequence patterns in the sliding window model [6] and for itemsets (see [11] for a review).
 1.2 Overview of the Method We approach the problem by first building a probabilistic reference model for the collection of itemset-sequences and the n deriving an analytical formula for the relative frequency for sequential patterns. Given such a model we discover se-quential patterns that are under-represented or over-represented with respect to the reference model, where a pattern is under-represented if it is too infrequent in the input collection of itemset-sequences and a pattern is over-represented if it is too frequent in the input collection of itemset-sequence. According to this notion a sequential pattern is significant if the probability that it would occur by chance a specific number of times, in the reference model, is very small. Note that the frequency of occurrence alone is not enough to determine significance, i.e., an infrequent sequen tial pattern can be more significant than a frequent one. Furthermore an occurrence of a subsequence pattern may be meaningless [6] if it occurs in an sequence of an appropriately large size. Our algorithm for ranking sequential patterns with respect to significance works as follows: (I) we find frequent sequential patterns using PrefixSpan [10] for a given minimum support threshold; (II) we compute their frequencies and variances of the frequencies in the reference model and (III) we rank the frequent sequential patterns with respect to significance by computing the divergence ( Z-score )be-tween the empirical (actual) frequencies a nd frequencies in the reference model. Given the reference model a presence of significant divergence between the ac-tual and computed frequency of a sequential pattern indicates that there is a dependency between itemsets/items in that pattern. In order to capture these dependencies our reference model consists of two sub-models: (I) sequence-wise reference model : treats itemsets as alphabet symbols and represents an indepen-dence model where itemsets occur independently of their order in an itemset-sequence and (II) itemset-wise reference model : provides decorrelated frequencies of itemsets for the sequence-wise refere nce model. By decorrelated frequencies we mean that given an attribute (item) a 1 and attribute a 2 the frequency of itemset ( a 1 ,a 2 ) is computed using a maximum entropy model , where the marginal em-pirical probabilities are preserved. The reason we use such a model for itemsets is that unlike in the case of frequent itemset mining, we do not consider empty itemsets (empty attribute sets) and therefore the independence model for item-sets [3] is inappropriate as an itemset-wise reference model. In particular, using the independence model for sparse non-empty itemsets (the average number of ones in a row is much smaller than the number of attributes) would artificially overestimate the probability of the empty itemset causing a distortion of proper proportions of probabilities of non-empty itemsets. Note, that the sequence-wise reference model can be easily extended to Markov models in the spirit of [7]. The reason we consider the sequence-wise model to be independence model in this paper is because of the following reasons: (I) it is the model of choice if the Markov reference model is not known; (II) it has an intuitive interpretation as a method for discovering dependencies and (III) it leads to exact polynomial formulas for computing the frequencies of sequential patterns. 1.3 Multi-stream of News Stories and the Reference Model We demonstrate the applicability of the presented method for discovering de-pendencies between streams of news sto ries, which is an important problem in multi-stream text mining and the topic detection and tracking research [2]. For this purpose we generated a collection of i temset-sequences from a multi-stream of news stories that was gathered from RSS feeds of major world news agencies [8]. Every itemset-sequence in that coll ection consists of stream identifiers of stories in a cross-stream cluster of new s stories reporting the same news event, where the sequence is ordered according t o the timestamps of the stories. Every itemset contains stream identifiers of documents published within the same time granularity. As an example itemset-sequence in that collection consider [(AP, MSNBC), UPI] that corresponds to three articles on the same news event (e.g., an earthquake in Italy), where the first two of them were published by AP and MSNBC within the same time granularity and followed by an article by UPI. Thus, clearly the empty itemset () does not occur in our data set. We stated the following research questions with respect to this collection of i temset-sequences: (I) what is the relationship between frequency, significance and content similarity in the discovered significant sequential patterns? and (II) what are the depen-dencies between the news sources in term s of sequential patterns of reporting the same news events?
As an example of the application of the reference model consider a case where the input collection of itemset-sequences contains a frequent sequential pattern s =[(AP, MSNBC), UPI], that consist of two itemsets s 1 =(AP, MSNBC) and s 2 =UPI that are correlated by occurring fr equently together. Then since the sequence-wise reference model assumes independence between the elements, the frequency of s computed from the sequence-wise reference model will be much smaller then its actual frequency leading to a high significance rank of s .Further-more, s 1 =(AP, MSNBC) contains two items a 1 =AP and a 2 =MSNBC which are correlated by occurring frequently t ogether in the same itemsets. Then there are two possibilities for computing the frequency of s in the sequence-wise refer-ence model: (I) we use the empirical frequency of s 1 or (II) we use a frequency of s 1 provided by the itemset-wise referen ce model. Then since the itemset-wise reference model provides decorrelated f requencies of itemsets while preserving marginal frequencies of the items (th e publishing rates of AP and MSNBC), the frequency of s 1 computed from the itemset-wise r eference model will be smaller that its empirical frequency leading to an even higher significance rank of s . 1.4 Related Work and Contributions Thus, we present a reliable universal method for ranking sequential patterns with respect to significance that builds on the previous work [6], where a frame-work for assessing significance of subsequence patterns in the sliding window model was presented. The challenges of analysing itemset-sequences with re-spect to the previous work on sequences in [6] stems from the following facts: (I) itemset-sequences have variable sizes; ( II) itemset-sequences contain itemsets (unordered sets) and (III) we do not co nsider empty itemsets. We address the first problem by modeling the frequency of an itemset-sequence using a proba-bilistic discrete mixture model and we approach the second and third problem by using an appropriate maximum entropy itemset-wise reference model.
To the best of our knowledge this is the first algorithm for ranking sequential patterns with respect to significance wh ile there has been an extensive research on mining frequent sequential patterns (see [9] for a review).
 The paper is organized as follows. Section 2 reviews theoretical foundations, Section 3 defines the problem, Section 4 presents the sequence-wise reference model, Section 5 presents th e itemset-wise reference model, Section 6 presents the algorithm for ranking sequential patte rns with respect to significance, Section 7 presents experimental results and finally Section 8 pres ents conclusions. In this section we review some concepts that are necessary in order to explain our framework. 2.1 Sequential Pattern Mining In this section we review the problem of sequential pattern mining [1]. Let A = { a I = { a 1 ,a 2 ,...,a |I| } is called an itemset or element and is also denoted by ( a 1 ,a 2 ,...,a |I| ). An itemset-sequence s =[ s 1 ,s 2 ,...,s m ] is an ordered list of itemsets, where s i  X  X  . The size of the itemset-sequence is denoted by | s | and the length of itemset-sequence s is defined as l = m i =1 | s i | . An itemset-sequence s =[ s 1 ,s 2 ,...,s m ]isa subsequence of itemset-sequence s =[ s 1 ,s 2 ,...,s m ], de-noted s s , if there exist integers 1  X  i 1  X  i 2 ...  X  i m such that s 1  X  s i s the support (frequency) of an itemset-sequence s , denoted by sup S ( s ), is defined as the number of itemset-sequences s ( i )  X  S that contain s as a subsequence. itemset-sequences that contain s as a subsequence. Given a relative support threshold minRelSup an itemset-sequence s is called a frequent sequential pat-tern if rsup S ( s )  X  minRelSup . The problem of mining sequential patterns is to find all frequent sequential patterns in S given minRelSup . The support has an anti-monotonic property meaning that sup S ( s )  X  sup S ( s )if s s . A pattern s is called a closed frequent sequential pattern if none of its frequent supersequences has the same support. A pattern s is called a maximal frequent sequential pattern if none of its frequent supersequences is frequent. Table 1 presents an example collection of itemset-sequen ces, where itemset-sequence id =1hassize s =3, length l = 4 and consists of three elements (itemsets): (1 , 3), 1 and 1. Given minRelSup =0 . 5, s =[(1 , 3) , 1] is a frequent sequential pattern that is con-tained in itemset-sequences: id =1 , 3, where rsup S ( s )=0 . 5. 2.2 Significance of Subsequence Patterns In this section we review the framework introduced in [6]. Let e =[ e 1 ,e 2 ,...,e m ] be a sequence of symbols. Let  X  n ( e | w )= n i =1 I i be a random variable that rep-resent the actual frequency (support) of size w windows containing at least one occurrence of e as a subsequence in an event sequence of size n + w  X  1( n shifts of the window), where I i is an indicator function equal to 1 if the i -th shift con-window ending at a given position in the event sequence contains at least one oc-currence of e as a subsequence. The superscript  X  means  X  X t least one occurrence as a subsequence X  and is used to distinguish this probability from a probability of e as a string. Clearly, I 1 ,I 2 ,...,I n is a sequence of dependent random vari-ables because a given subsequence pattern occurring in the input sequence may occur in many consecutive windows depending on its span. Therefore, because of the sliding window overlap  X  n ( e | w )doesnothavea Binomial distribution random variable that represents the actual relative frequency of size w windows containing at least one occurrence of e as a subsequence in an event sequence,
Let W  X  ( e | w )bethe set of all distinct windows of length w containing at least where P ( x ) is the probability of string x in a given Markov model. W  X  ( e | w ) can be enumerated using an enumeration graph. The enumeration graph for a subsequence pattern e =[ e 1 ,e 2 ,...,e m ] is shown in Figure 1. In particular for the 0-order Markov reference model P  X  ( e | w ) can be expressed as follows ence model. Then P  X  ( e | w ) is the probability of getting from state 0 to m in w steps. Paper [6] also presented an efficient O ( w 2 ) dynamic programming algo-rithm for computing P  X  ( e | w )from(1).Itwasshownthatif Var [  X  n ( e | w )] &gt; 0 then  X  n ( e | w ) satisfies the Central limit theorem (CLT) and this fact was used to set a lower and upper significance thresholds for  X  n ( e | w ). The problem of ranking sequential patte rns (itemset-sequences) with respect to significance can be defined as follows.

Given: (I) collection o f itemset-sequences S = { s (1) ,s (2) ,...,s ( n ) } ,where s and (II) minimum relative support threshold minRelSup for sequential patterns. Task: rank the discovered sequential patterns with respect to significance.
Note that in our method the main purpose of the support threshold for se-quential patterns is to limit the search space of possible significant patterns. The sequence-wise reference model trea ts itemsets as alphabet symbols and rep-resents an independence model where itemsets occur independently of their order in an itemset-sequence. In order to prese nt the sequence-wise reference model we introduce the element-wise representation of a collect ion of itemset-sequences, that is a sequence of itemset-sequences R =[ r (1) ,r (2) ,...,r ( |R| ) ]overanitem-r presents the element-wise sequence of ite mset-sequences for the collection from decimal numbers. Note that for the sequence-wise reference model  X  is provided by the itemset-wise reference model and includes all non-empty subsets of A . 4.1 Generative Process Now consider the sequence-wise refere nce model as a generative process, that generates itemset-sequences in R as follows: 1. it first generates the size of the itemset-sequence from a distribution  X  = 2. it generates a sequence of itemsets r ( i ) of size m from distribution  X  = Let P ( r ( i ) ,m ) be the joint probability of a particular itemset sequence r ( i ) of size m to be generated by the process. Then given the independence assumption of the presented generative process we factorize P ( r ( i ) ,m ) as follows r ( i ) to be generated given the size m .

We compute the parameters of the sequence-wise reference model from S number of occurrences of it emset-sequences of size m in S and (II)  X  is computed form the itemset-wise reference model, that is presented in Section 5 and whose purpose is to provide decorrelated frequ encies of itemsets. Note that we could in S and N n (  X  j ) is the number of occurrences of itemset  X  j in S . However the ML estimator for the itemsets does not provide decorrelated frequencies. 4.2 Relative Frequency In this section we consider occurre nces of a sequential pattern as a subsequence (gaps between elements of the sequential pattern are allowed) in the sequence-wise reference model represented by its element-wise representation R .Let  X  n ( s ) be a random variable representing the actual relative frequency of a sequential pattern s occurring as a subsequence in R . Recall that the relative frequency of a sequential pattern s is equal to the fraction of itemset-sequences in R that contain it as a subsequence. This means that even if s occurs many times in a given itemset-sequence s  X  X  we count it only as one occurrence. Let  X  n ( s )= s occurring as a subsequence in R ( sup R ( s )), where I i is an indicator function equal to 1 if the i -th itemset-sequence contains s . Then clearly, I 1 ,I 2 ,...,I n is a sequence of independent random variables because occurrences of a pattern as a subsequence in itemset-sequences are independent of each other. Therefore,  X  n ( s )hasthe Binomial distribution and  X  n ( s )= probability that s exists as a subsequence in a n itemset-sequence in R .Thus, clearly  X  n ( s )and  X  n ( s ) both satisfy CLT. However, since itemset-sequences have variable sizes, for a given s , P  X  ( s ) depends on the distribution of the sizes of itemset-sequences in R .

Let P  X  ( s, m ) be the joint probability that an itemset-sequence s of size | s | exists as a subsequence in another itemset-sequence s of size m  X | s | in R .Then following (2) we factorize P  X  ( s, m ) as follows where P  X  ( s | m ) is the probability that s occurs given an itemset-sequence of size m in R . In order to obtain the formula for P  X  ( s ) we marginalize from (3) as follows: Thus, P  X  ( s ) is expressed as a discrete mixture model ,wherethe mixing coeffi-cients (  X  1 ,  X  2 , ..., X  M ) model the fact that an occurrence of s as a subsequence in an itemset-sequence s depends on the size of s andmaypossiblyoccurinany itemset-sequence s  X  X  for which | s | X | s | .Inotherwords, P  X  ( s )isaweighted combination of contributions from itemse t-sequences of all possible relevant sizes in R .
 given an itemset-sequence of size m in R can be obtained as follows. Let X i = Then clearly, the enumeration graph for W  X  ( s | m ) can be obtained from the enumeration graph fo r a sequence of items e =[ e 1 ,e 2 ,...,e m ] by substitut-ing X 1 , X 2 ,..., X | s | for items e 1 ,e 2 ,...,e m in Figure 1. Then the formula for P  X  ( s | m ) can be obtained from (1) by substituting marginal probabilities of item-Markov sequence-wise reference model , can be obtained from (1) as follows: from the itemset-wise reference model. Formula (5) can be computed in O ( m 2 ) using the dynamic programming algorithm given in [6]. Thus computing P  X  ( s ) from (4) takes O ( M 3 )time.

The presented sequence-wise reference model can be easily extended to more application specific models. As a first extension, we could assume that itemset-sequences ar e generated using a Markov model and use the algorithm for computing P  X  ( s | m ) from [7] for Markov models. As another extension, we could assume that the distribution of itemsets  X  depends on the size of an itemset-sequence (e.g., itemsets having large cardinality are more likely to occur in shorter itemset-sequences). The itemset-wise reference model treat s itemsets as binary vectors and provides decorrelated frequencies of itemsets. In o rder to present the itemset-wise refer-ence model we introduce the item-wise representation, that is a multi-attribute sequence corresponding to attribute (item) a i  X  X  and b ( j ) t  X  X  0 , 1 } is the value at time point t .Thus, B represents S as a sequence of time ordered itemsets. Figure 3 presents the item-wise multi-attribute sequence for the collection from Table 1.

Note that we do not consider empty itemsets (binary vectors consisting of all zeros in B ) because a lack of attributes is mean ingless in our framework. There-Therefore we build a maximum entropy model of the form [4] where Z is the normalizing constant, I ( i ) is an indicator function equal to one if b ( i ) t = 1. We build (6) using Generalized Iterative Scaling (GIS) algorithm by finding  X  i for i =1 ,... |A| ,  X  c and Z under constraints that empirical P ( b ( i ) t =1),where  X  c is the correction feature that ensures that the num-ber of parameters (features) for every binary vector in (6) is constant. Let Sum = bility of attribute i computed from the model given the current estimates of the parameters.

The iterative scaling algorithm pro ceeds as follows: (I) initialization:  X  i =1,  X  c =1, Z = 1 Sum ; and (II) iteration: repeat for i=1 to  X  i =  X  (I) it preserves empirical marginal probabilities; (II) it is defined only for all non-empty subsets of A and (III) it gives as much independence to the attribute streams as possible given the constraints.

Table 2 presents marginal probabilities of the itemsets of size two from Figure 3 obtained using the independence model and the maximum entropy model. Thus, Table 2 shows the following facts: (I) although the empty itemset does not occur in Figure 3 the independence model assigns a bigger probability (1 . 94 e  X  01) to the empty itemset than to the occurri ng itemsets of size two; and (II) the ME model, as expected, assigns greater probabilities to the occurring itemsets than the independence model. Given a collection of itemset-sequences S = { s (1) ,s (2) ,...,s ( n ) } ,where s 1. run PrefixSpan for a given value of minRelSup to obtain a set of frequent 3. for every frequent sequential pattern s =[ s 1 ,s 2 ,...,s | s | ], where s  X  X  and The reference model will be violated in S in two cases: (I) the sequence-wise reference model is violated by correlated i temsets and (II) the itemset-wise ref-erence model is violated by correlated items in itemsets. In this section we present our experimen tal results on the multi-stream of news stories of size 224062 stories that have been retrieved, via RSS feeds, from the following thirteen news sources: ABC news (ABC), Aljazeera (ALJ), As-sociated Press (AP), British Broadcast Co. (BBC), Canadian Broadcast Co. (CBC), Xinhua News Agency (CHV), Central News Agency Taiwan (CNE), CNN, MSNBC, Reuters (REU), United Press International (UPI), RIA Novosti (RIA) and Deutsche Welle (DW). The stories were retrieved over a period of thirteen months from the 12-th of November 2008 to the 3rd of January 2010. We implemented a clustering algorithm that uses a time-window of a given dura-tion (e.g., 24 hours) and is an incremental variant of a non-hierarchical document clustering algorithm using a similarity measure based on nearest neighbors. We ran the algorithm for the following parameters: (I) the time-window size w =24 hours; (II) the document similarity threshold  X  d =0 . 5thatisusedtoidentify nearest neighbors for a new arriving document to the window and (III) the time quantization step size Q t = 1 hour. As a result we obtained a collection of itemset-sequences S of size | S | = 32464, where there are 109964 itemsets, the maximum itemset-sequence size M = 25, the average item set-sequences size is 3 . 5 and the average itemset size is 1 . 2. 7.1 From Clusters to Itemset-Sequences Let D = { d (1) ,d (2) ,...,d ( |D| ) } be a multi-stream of news stories (documents), where d ( i ) t is a document in stream i at a time point t and has three attributes: (I) (reporting the same event in our case) defined as a sequence of documents or-dered with respect to publishing timestamp d i .timestamp .Weconvert C to an is the set of all stream ident ifiers of the news sources in D . As a result of the con-version each itemset s i contains stream identifiers of documents with the same timestamp ( d i .timestamp ) and the itemset-sequence s is ordered with respect to the timestamps of the itemsets. As a n example consider itemset-sequence [(1 , 3) , 1 , 1] in Table 1, where s 1 =(1 , 3) means that two documents: the first from source 1 and the second from source 3 were published (within the time granularity Q t ) before a document from stream s 1 and 1 respectively. Further-more, for every itemset-sequence, we r ecorded content similarity between the stories corresponding to its elements in terms of the cosine similarity measure . In order to asses the nature of content similarity between documents in a given itemset-sequence s we define the average content similarity AvgSim S ( s )andthe variance of the content similarity VarSim S ( s ) between documents in an itemset-sequence s of length l occurring as a subsequence over the whole collection of itemset-sequences S are expressed as follows and
VarSim S ( s )= where j 1  X  j 2 ...  X  j l are the positions where s occurs in s as a subsequence and sim ( d i ,d j )isthe cosine similarity or content similarity between documents i and j . Thus, (7) computes the average cont ent similarity over all itemset-sequences containing s as a subsequence. We also use StdDevSim S ( s )todenote
VarSim S ( s ). 7.2 Baseline: Most Frequent Patterns As a baseline against which we compare the performance of the ranking algo-rithm we use the top-20 most frequent sequential patterns of size greater than one, where we also removed patterns con taining the same symbol, which corre-sponds to frequent updates of the same news event. Table 3 presents the top-20 most frequent sequential patterns of size greater than one. 7.3 Significant Patterns In the first experiment we rank the top-20 most frequent patterns from Table 3 with respect to significance. Figure 4 p resents the results. As it turns out the most frequent pattern in Table 3 is also the most significant one but for the following patterns there is not any obvious relationship between the significance rank and the frequency rank. The dependency between AP and MSNBC can be explained by the fact that as we saw in the recorded stories MSNBC is reusing some content from AP.
In the second experiment we set minRelSup =0 . 01 and found the most significant over-represented ( sigRank &gt; 0) sequential patterns. Table 4 presets the top-20 most significant sequential patterns, where among the top patterns we removed patterns containing the same symbol and patterns having significant supersequences. Note however that the top-20 most significant patterns for the whole collection may not be the same since the patterns in Table 4 were obtained using minRelSup =0 . 01. In general the lower the value of minRelSup the higher the chance that the reference model will discover long significant patterns having low support. By comparing the results from Table 3 and from Table 4 we can make the following observations: (I) the most significant patterns are generally longer than the most frequent ones since the sequence-wise reference model leverages rank of correlated longer patterns and (II) there is a prevalence of patterns involving BBC in the first position and ALJ in the following positions. The dependency between BBC and ALJ may be related to the fact that, as we found out from the BBC web site, BBC sign ed a news exchange agreement with ALJ and as the pattern suggests this exchange seems to be really  X  X ne-way X  from BBC to ALJ. Furthermore, ALJ tends to provide many updates of the same news event. Also, although [AP, MSNBC] is the most frequent pattern it has significance rank nine in Table 4 as a result of the reference model leveraging rank of the longer patterns involving BBC and ALJ.

Figure 5 presents a graph of the significance rank ( x -axis) versus the average content similarity AvgSim S and the standard deviation StdDevSim S ( y -axis) for the top-20 most significant sequential patterns from Table 4. Figure 5 shows two facts: (I) the average content similarity is above the document similarity threshold  X  d =0 . 5 and (II) the value of the standard deviation is relatively low for all patterns. These results suggest that temporally correlated news streams tend to be also correlated with respect to their content.
 We presented a reliable general method fo r ranking frequent sequential patterns (itemset-sequences) with respect to si gnificance. We demonstrated the appli-cability of the presented method on a multi-stream of news stories that was gathered from RSS feeds of the major world news agencies. In particular we showed that there are strong dependencie s between the news sources in terms of temporal sequential patterns of reporting the same news events and content sim-ilarity, where the frequency and significance rank are correlated with the content similarity.

