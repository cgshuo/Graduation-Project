 Compared with single-document summarizations, information diversity and informa-tion redundancy are two main difficulties to be overcome in multi-document summa-rization tasks. The strategies for multi-document summarization roughly fall in three categories according to the way summaries are created. 
The first kind of strategy ranks and extracts sentences from multi-documents simi-proaches, the ranking here should consider not only sentence representative, but also ranks sentences first and then adjusts summary by mutual information overlap of sen-convergence of the adjusting process is questionable. 
The second strategy is based on sentence or paragraph clustering. Sentences are ex-summary to get a good coverage of the topics and remove redundancy. 
Many methods have been proposed on clustering-based multi-document summari-zation. Boros [3] clustered sentences to produce summaries with cluster number pre-tences clustered with a similarity threshold for clusters predefined.

The third strategy is based on information fusion [1], which tries to combine simi-lar sentences across documents to create new sentences based on language generation technologies. Although this could model human X  X  efforts in summarization process, it 
In this paper, we adopt the second strategy ----sentence-clustering-based summari-zation. 
The remainder of the paper is organized as follows. In section 2, we talk about the In section 5, we present the experiments and results on DUC 2004 data. In section 6, we give out the conclusion. In this paper, we adopt the sentence-clustering-based strategy to determine main top-ics from source documents to reach good representative and avoid redundancy. 
One immediate problem occurs that how many clusters are appropriate for sen-cluster number or similarity threshold in clustering process. However, the predefining depends on experience or guess, being hard to meet requirements from varies summa-automatically infer the cluster number. 
Another problem is how to select representative sentences from clusters. Here, the tence based on the overall performance of the whole summary. Almost all clustering but also observing their overall performance. the cosine distance. the cluster number, we need to find k which meets (2). Here, the criterion is set up based on resampling based stability. c ( c  X  ij ) equals 1, otherwise, 0. Then the stability is defined in (3). clustering result on full sentence set P . decrease when increasing the value of k. Therefore to avoid the bias that small value of k is to be selected as cluster number, we use the cluster validity of a random pre-function can be defined as (4). pendent of k (Levine et al. 2001). 
After the number of optimal clusters is decided, k-means algorithm is implemented to cluster. Each output cluster is supposed to denote one topic in the document collec-tion. within the range 8~12 only in following experiments. based on terms they contain to select the representative sentence from each cluster. 4.1 Local Search Strategy use the terms extracted from the clusters, rather than those from the document collec-tions, because they are supposed to contribute more in representative. 
Equation (5) describes the weight scheme for a term in a cluster. term, cf is cluster frequency (number of clusters which contain the term). 
The weight of a sentence s is calculated as follows: Here sl is the length of the sentence (number of the words in the sentence). 4.2 Global Search Strategy performance of the whole summary. Thus a global criterion to evaluate the summary is necessary. For a summary sm , let sml be its length (word number in the summary) the criterion is defined as below. length. expect the summary to contain more terms, more longer terms, and as short as possible. global criterion. 
For search heuristics, we adopt a strategy of deleting one sentence each time. A sen-tence, whose deletion maximizes the criterion among all sentences, is removed from the 
If there is a length limitation for the summarization task, further adjustment (add-ing or removing sentences) of the summary is necessary. The same global criterion is applied. 5.1 Data Set TDT documents. Each set contains 10 documents. Four human model summaries are provided for each set for evaluation. 5.2 Evaluation Method ROUGE [7] is used to automatically evaluation our experiment results. We follow the same requirement of DUC04 task2 ----produce summaries of no more than 665 bytes from each set of 10 documents. All ROUGE evaluation configurations follow the con-figurations in DUC04: stop words included, Porter-stemmed, 4 human model summa-ries used, and the first 665 bytes cutoff. 5.3 Results Comparison with DUC04 reports Table 1 lists ROUGE scores of our summaries and of DUC04 runs. 
We can find that our scores are much higher than the median, a bit lower than the best, in fact ranking the second in average among the task participants. Different Recommendations To check effectiveness of the hybrid strategy, we test global search with different lo-cal recommendations, including recommendation of top 1, 2, 3 or all sentences from each cluster. based on the global criterion. 
From table 2, we get several findings. One is that the hybrid strategy is better than search provide good heuristics for better sentence selection. 
Another finding is that the performance with 3 sentence selection is better than that selected. This means that the sentences which form the optimal summary may be not necessarily the top sentences recommended by individual clusters. solution tends to be found. However, note that better solutions depend on not only the recommended, it would be more likely to reach local optimal, which may underlie the lower performance for global output. dation stratedgy. Validation Vs. Non-validation To learn whether the automatic determination of the cluster number helps to improve the quality of the summary, remembering that the optimal cluster number was decided within range 8~12 by cluster validation, now we got 5 runs by denoting cluster num-cluster number, the optimal clustering might be missed, which would affect the over-all performance. summarization. It mainly consists of three steps: sentence clustering, local recommen-tences, we rank the sentences based on the terms they contain. For global selection of sentences, we propose a global criterion, and seek the optimal summaries based on this criterion. 
We have some findings from experiments. First, clustering is useful to remove the optimal cluster numbers varies for different source documents. Furthermore, the com-formance. 
