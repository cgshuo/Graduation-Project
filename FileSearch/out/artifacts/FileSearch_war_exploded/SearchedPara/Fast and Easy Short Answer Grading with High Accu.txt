 Short-answer questions are a useful device for elic-iting student understanding of specific concepts in a subject domain. Numerous automated graders have been proposed for short answers based on their se-mantic similarity with one or more expert-provided correct answers (Mohler et al., 2011; Heilman and Madnani, 2013; Ramachandran et al., 2015). From an application perspective, these systems vary con-siderably along a set of key dimensions: amount of human effort involved, accuracy, speed, and ease of implementation. We explore a design that seeks to optimize performance along all these dimensions.
Systems developed for the more general task of short-text semantic similarity provide a good start-ing point for such a design. Major progress has been made in this task in recent years, due primarily to the SemEval Semantic Textual Similarity ( STS ) task (Agirre et al., 2012; Agirre et al., 2013; Agirre et al., 2014; Agirre et al., 2015). However, the utility of top STS systems has remained largely unexplored in the context of short answer grading. We seek to bridge this gap by adopting the feature set of the best performing STS system at SemEval-2015 (Sultan et al., 2015). Besides high accuracy, this system also has a simple design and fast runtime.

Textual similarity alone, however, is inadequate as a measure of answer correctness. For example, while the Sultan et al. (2015) system makes the gen-equally to the meaning of a sentence, domain key-words (e.g.,  X  X utation X  for biological evolution) are clearly more significant than arbitrary content words (e.g.,  X  X onsideration X ) for academic text. As another example, question demoting (Mohler et al., 2011) proposes discarding words that are present in the question text as a preprocessing step for grading. We augment our generic text similarity features with such grading-specific measures.

We train supervised models with our final fea-ture set; in two different grading tasks, these mod-els demonstrate significant performance improve-ment over the state of the art. In summary, our contribution is a fast, simple, and high-performance short answer grading system which we also release as open-source software at: https://github. com/ma-sultan/short-answer-grader . A comprehensive review of automatic short answer grading can be found in (Burrows et al., 2015). Here we briefly discuss closely related work.

Early short answer grading work relied on pat-terns (e.g., regular expressions) manually extracted from expert-provided reference answers (Mitchell et al., 2002; Sukkarieh et al., 2004; Nielsen et al., 2009). Such patterns encode key concepts repre-sentative of good answers. Use of manually de-signed patterns continues to this day, e.g., in (Tan-dalla, 2012), the winning system at the ASAP an-man intervention that natural language processing can help to eliminate. Ramachandran et al. (2015) propose a mechanism to automate the extraction of patterns from the reference answer as well as high-scoring student answers. We adopt the simpler no-tion of semantic alignment to avoid explicitly gener-ating complicated patterns altogether.

Direct semantic matching (as opposed to pattern generation) has been explored in early work like (Leacock and Chodorow, 2003). With advances in NLP techniques, this approach has gained pop-ularity over time (Mohler et al., 2009; Mohler et al., 2011; Heilman and Madnani, 2013; Jimenez et al., 2013). Such systems typically use a large set of similarity measures as features for a supervised learning model. Features range from string similar-ity measures like word and character n -gram over-lap to deeper semantic similarity measures based on resources like WordNet and distributional meth-ods like latent semantic analysis ( LSA ). However, a large feature set contributes to higher system run-time and implementation difficulty. While following this generic framework, we seek to improve on these criteria by employing a minimal set of core similar-ity features adopted from (Sultan et al., 2015). Our features also yield higher accuracy by utilizing more recent measures of lexical similarity (Ganitkevitch et al., 2013; Baroni et al., 2014), which have been shown to outperform traditional resources and meth-ods like WordNet and LSA .

Short-text semantic similarity has seen major progress in recent times, due largely to the SemEval Semantic Textual Similarity ( STS ) task (Agirre et al., 2012; Agirre et al., 2013; Agirre et al., 2014; Agirre et al., 2015). STS systems can serve as a source of important new features and design elements for au-tomatic short answer graders (B  X  ar et al., 2012; Han et al., 2013; Lynum et al., 2014; H  X  anig et al., 2015).
Surprisingly, few existing grading systems utilize simple and computationally inexpensive grading-specific techniques like question demoting (Mohler et al., 2011) and term weighting. Our model aug-ments the similarity features using these techniques. Following feature extraction, our system trains a su-pervised model for grading. As we discuss in Sec-tion 4, this can be a regressor or a classifier depend-ing on the task. This section describes our features; specifics of the models are given in Section 4. 3.1 Features 3.1.1 Text Similarity
Given reference answer R = ( r 1 ,...,r n ) and stu-dent response S = ( s 1 ,...,s m ) (where each r and s is a word token), we compute three generic text similarity features.

Alignment . This feature measures the proportion of content words in R and S that have a semantically similar word in the other sentence. Such pairs are identified using a word aligner (Sultan et al., 2014). The semantic similarity of a word pair ( r i ,s j ) is a weighted sum of their lexical and contextual simi-larities. A paraphrase database ( PPDB , Ganitkevitch et al. (2013)) identifies lexically similar word pairs; contextual similarity is computed as average lexical similarity in (1) dependencies of r i in R and s j in S , and (2) content words in [-3, 3] windows around r i in R and s j in S . Lexical similarity scores of pairs in PPDB as well as weights of word and contextual similarities are optimized on an alignment dataset (Brockett, 2007).

To avoid penalizing long student responses that still contain the correct answer, we also employ a second version of this feature: the proportion of aligned content words only in R . We will refer to this feature as coverage of the reference answer X  X  content by the student response.

Semantic Vector Similarity. This feature em-level semantic vector is computed for each input sentence as the sum of its content word embeddings (lemmatized). The cosine similarity between the R and S vectors is then used as a feature. While the alignment features distinguish only between para-phrases and non-paraphrases, this feature enables in-tegration of finer-grained lexical similarity measures between related concepts (e.g., cell and organism ). 3.1.2 Question Demoting
We recompute each of the above similarity fea-tures after removing words that appear in the ques-tion text from both the reference answer and the stu-dent response. The objective is to avoid rewarding a student response for repeating question words. 3.1.3 Term Weighting
To be able to distinguish between domain key-words and arbitrary content words, in our next set of features we assign a weight to every content word in the reference and the student answer based on a variant of tf-idf . While general short-text similar-ity models typically use only idf (inverse document frequency) to penalize general words, the domain-specific nature of answer grading also enables the application of a tf (term frequency) measure.
To fully automate the process for a question and reference answer pair, we identify all content words in the pair. The top ten Wikipedia pages re-lated to these words are retrieved using the Google API . Each page is read along with all linked pages crawled using Scrapy (Myers and McGuffee, 2015). The in-domain term frequency ( tf d ) of a word in the answer is then computed by extracting its raw count in this collection of pages. We use the same set of tools to automatically extract Wikipedia pages in 25 different domains such as Art, Mathematics, Reli-gion, and Sport. A total of 14,125 pages are re-trieved, occurrences in which are used to compute the idf of each word.

We augment our alignment features X  X oth orig-inal and question-demoted X  X ith term weights to generate new features. Each word is assigned a weight equal to its tf d  X  idf score. The sum of weights is computed for (1) aligned, and (2) all con-tent words in the reference answer (after question demoting, if applicable). The ratio of these two numbers is then used as a feature. We compute only coverage features (Section 3.1.1) to avoid comput-ing term weights for each student response. Thus the process of crawling and reading the documents is performed once per question; all the student re-sponses can subsequently be graded quickly. 3.1.4 Length Ratio
We use the ratio of the number of words in the stu-dent response to that in the reference answer as our final feature. The aim is to roughly capture whether or not the student response contains enough detail. We evaluate our features on two grading tasks. The first task, proposed by Mohler et al. (2011), asks to compute a real-valued score for a student response on a scale of 0 to 5. The second task, proposed at SemEval-2013 (Dzikovska et al., 2013), asks to as-sign a label (e.g., correct or irrelevant ) to a student response that shows how appropriate it is as an an-swer to the question. Thus from a machine learning perspective, the first is a regression task and the sec-ond is a classification task. We use the NLTK stop-words corpus (Bird et al., 2009) to identify function words. Results are discussed below. 4.1 The Mohler et al. (2011) Task The dataset for this task consists of 80 undergradu-ate Data Structures questions and 2,273 student re-sponses graded by two human judges. These ques-tions are spread across ten different assignments and two tests, each on a related set of topics (e.g., programming basics, sorting algorithms). A refer-ence answer is provided for each question. Inter-annotator agreement was 58.6% (Pearson X  X   X  ) and .659 ( RMSE on a 5-point scale). Average of the two human scores is used as the final gold score for each student answer.

We train a ridge regression model (Scikit-learn (Pedregosa et al., 2011)) for each assignment and test using annotations from the rest as training ex-amples. A dev assignment or test is randomly held out for model selection. Out-of-range output scores, if any, are rounded to the nearest in-range integer. Following Mohler et al. (2011), we compute a single Pearson correlation and RMSE score over all student responses from all datasets. Average results across 1000 runs of the system are shown in Table 1. Our model shows a large and significant performance im-provement over the state-of-the-art model of Mohler et al. (two-tailed t -test, p &lt; .001). Their system em-ploys a support vector machine that predicts scores using a set of dependency graph alignment and lex-ical similarity measures. Our features are similar in intent, but are based on latest advances in identifica-tion of lexical similarity and monolingual alignment.
Ramachandran et al. (2015) adopt a different setup to evaluate their model on the same dataset. For each assignment/test, they use 80% of the data for training and the rest as test. This setup thus en-ables in-domain model training. Their system auto-matically generates regexp patterns intended to cap-ture semantic variations and syntactic structures of good answers. Features derived from match with such patterns as well as term frequencies in the stu-dent response are used to train a set of random for-est regressors, whose predictions are then combined to output a single score. Results in this setup are shown in Table 2. Again, averaged over 1000 runs, our model performs better on both evaluation met-rics. The differences are smaller than before but still statistically significant (two-tailed t -test, p &lt; .001). 4.2 The SemEval-2013 Task Instead of a real-valued score, this task asks to assign one of five labels to a student response: correct , par-tially correct/incomplete , contradictory , irrelevant , and non-domain (an answer that contains no domain content). We use the S CI E NTS B ANK corpus, con-taining 9,804 answers to 197 questions in 15 science domains. Of these, 3,969 are used for model training and the remaining 5,835 for evaluation. A reference answer is provided for each question.

The test set is divided into three subsets with vary-ing degrees of similarity with the training examples. The Unseen Answers ( UA ) dataset consists of re-sponses to questions that are present in the training set. Unseen Questions ( UQ ) contains responses to in-domain but previously unseen questions. Three of the fifteen domains were held out for a final Unseen Domains ( UD ) test set, containing completely out-of-domain question-response pairs. For this task, we train a random forest classifier with 500 trees in Scikit-learn using our feature set.

Table 3 shows the performance of our model (av-eraged over 100 runs) along with that of top sys-ETS (Heilman and Madnani, 2013) employs a lo-gistic classifier combining lexical and text similar-ity features. SoftCardinality (Jimenez et al., 2013) employs decision tree bagging with similarity fea-tures derived from a set cardinality measure X  X oft cardinality X  X f the question, the reference answer, and the student response. These features effectively compute text similarity from commonalities and dif-ferences in character n -grams.

Each cell on columns 2 X 4 of Table 3 shows a weighted F 1 -score on a test set computed over the five classes, where the weight of a class is pro-portional to the number of question-response pairs in that class. The final column shows a similarly weighted mean of scores computed over the three test sets. On each test set, our model outperforms the top-performing models from SemEval (signifi-cant at p &lt; .001). Its performance also suffers less on out-of-domain test data compared to those mod-els. w/o alignment .519 .938 w/o embedding .586 .892 w/o question demoting .571 .903 w/o term weighting .590 .889 w/o length ratio .591 .888 4.3 Runtime Test Given parsed input and having stop words removed, the most computationally expensive step in our sys-tem is the extraction of alignment features. Each content word pair across the two input sentences is assessed in constant time, giving the feature extrac-tion process (and the whole system) a runtime com-plexity of O ( n c  X  m c ), where n c and m c are the num-ber of content words in the two sentences. Note that all alignment features can be extracted from a single alignment of the input sentences.

Run on the Mohler et al. dataset (unparsed; about 18 words per sentence on average), our system grades over 33 questions/min on a 2.25 GH z core. 4.4 Ablation Study Table 4 shows the performance of our regression model on the Mohler et al. dataset without different feature subsets. Performance falls with each exclu-sion, but by far the most without alignment-based features. Features implementing question demoting are the second most useful. Length ratio improves model performance the least.

Surprisingly, term weighting also has a rather small effect on model performance. Further inspec-tion reveals two possible reasons for this. First, many reference answers are very short, only con-taining words or small phrases that are necessary to answer the question (e.g.,  X  X ush X ,  X  X nqueue and de-queue X ,  X  X y rows X ). In such cases, term weighting has little or no effect. Second, we observe that in many cases the key words in a correct answer are ei-ther not domain keywords or are unidentifiable using tf-idf . Consider the following:  X  Question: What is a stack?  X  Answer: A data structure that can store ele-Important answer words like  X  X ast X ,  X  X dded X ,  X  X irst X , and  X  X emoved X  in this example are not domain key-words and/or are too common (across different do-mains) for a measure like tf-idf to work. We present a fast, simple, and high-performance short answer grading system. State-of-the-art mea-sures of text similarity are combined with grading-specific constructs to produce top results on multi-ple benchmarks. There is, however, immense scope for improvement. Subtle factors like differences in modality or polarity might go undetected with coarse text similarity measures. Inclusion of text-level paraphrase and entailment features can help in such cases. Additional term weighting mecha-nisms are needed to identify important answer words in many cases. Our system provides a simple base model that can be easily extended with new features for more accurate answer grading.
 This material is based in part upon work supported by the National Science Foundation under Grants EHR /0835393 and EHR /0835381. We thank Lak-shmi Ramachandran for clarification of her work.
