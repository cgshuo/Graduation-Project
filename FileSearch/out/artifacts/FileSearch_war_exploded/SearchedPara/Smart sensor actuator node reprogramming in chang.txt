 fi 1. Introduction
Sensors (or detectors) are devices that permit the measurement of chemical or physical variables, transforming them into electrical signals, in order to interpret the signals from various sensors and send the sensed data or make a decision according to them.
Microcontroller boards are an economic, small and fl exible solution, and thus are the most common controller used in a sensor node, also known as a Mote, commonly used in Wireless sensor network ( Yick et al., 2008; Sengupta et al., 2013 ), but also used in other important technologies such as Embedded systems ( Marwedel, 2006; Mamdoohi et al., 2012 )andReal-timesystems( Kopetz, 1997;
Wang et al., 2010 ). Motes are nowadays widely employed in all kind of industrial applications, in several of them, the problem requires an action upon the environmental conditions, in this case a sensor/ actuator node is required.

In cases when the environmental conditions evolve over time, the original sensor/actuator programming can lead to incorrect decisions, and thus it is necessary to change or adapt the decision-making process to the new conditions ( Sayed-Mouchaweh and Lughofer, 2012 ). The traditional option to solve this problem has been to send the sensed data to a central unit, where a person interprets the data and reprogram the microcontroller with the new set of rules ( Han et al., 2005; Wang et al., 2006; Shaikh et al., 2010 ). Different reprogramming techniques have been proposed as a way of dyna-mically changing the behaviour of the sensors without having to manually reprogram them, because traditional reprogramming requires in most cases the interruption of the process for loading the new binary code, with the consequent loss of time and energy, involved in the communication process to the central unit ( Rassam et al., 2013; Aiello et al., 2011 ). A fi rst step towards reducing the previous effects has been to incorporate machine learning systems in the decision-making process, automating the response of the micro-controller without interrupting its execution and sending just a small 2012; Farooq et al., 2010 ). However, recent advances in the comput-ing power of sensors permit the inclusion of learning systems in the microcontroller (  X  on-chip  X  learning), adapting the sensor/actuator behaviour dynamically according to the sensed data ( Aleksendri et al., 2012; Mahmoud et al., 2013 ).

Arti fi cial Neural Networks (ANNs) ( Haykin, 1994 ) are a kind of machine learning models, inspired on the functioning of the brain, that can be utilised in clustering and classi fi cation problems, having been applied successfully in several fi elds, including pattern recognition ( Dhanalakshmi et al., 2011 ), stock market prediction ( Park and Shin, 2013 ), control tasks ( Zhai and Yu, 2009 ), medical diagnosis and prognosis ( Kodogiannis et al., 2007 ), and so on. Despite years of research in the fi eld of ANN, selecting a proper architecture for a given problem remains a dif fi cult task ( G X mez et al., 2009; Hunter et al., 2012; Lakshmi and
Subadra, 2013 ), and several strategies have been proposed for solving or alleviating this issue. In particular, Constructive Neural
Networks (CoNNs) offer the possibility of generating networks that grows as input information is received, matching the com-plexity of the data ( Franco et al., 2009 ). Moreover, the training procedure in CoNN, considered a computationally expensive problem in standard feedforward neural networks, can be done on-line and relatively fast. C-Mantec is a recently introduced CoNN algorithm that implements competition between neurons, also incorporating a built-in fi ltering scheme to avoid over fi problems. These two characteristics permit the algorithm to generate compact neural architectures with very good general-isation capabilities, making the algorithm suitable for its applica-tion to devices with limited resources like microcontrollers. The main limitations of these devices are memory size and computing speed, and thus an ef fi cient implementation of the algorithm is needed. Despite the existence of alternative evolving models ( Lughofer, 2011; Angelov, 2010; Huang et al., 2005 ), C-Mantec has been selected based mainly on the three following features: dynamic generation of compact architectures, good prediction ability and robustness to parameter setting.

In the present work, we have fully implemented the C-Mantec ( Subirats et al., 2012 ) constructive neural network model in an
Arduino UNO board, including the whole learning process to implement the automatic reprogramming process for decision-making into the sensor/actuator in changing environments, avoid-ing communication to other devices.

The Arduino UNO board was used ( Oxer and Blemings, 2009 )as it is a popular, economic and ef fi cient open source single-board microcontroller that allows easy project development ( Lian et al., 2013; Cela et al., 2013; Ortega-Zamorano et al., 2013; Kornuta et al., 2013 ). We have also proposed three case studies of different nature which require reprogramming to the decision-making process, demonstrating that the time involved in the sensor reprogramming is signi fi cantly lower than in the traditional case, without the need to send any information to another device (as a control unit), saving a large fraction of the required energy resources.
 The paper is structured as follows: fi rst, we brie fl y describe in
Section 2 the C-Mantec constructive neural network algorithm used, followed by a description of the Arduino UNO microcon-troller board in Section 3 . Section 4 includes the details of the implementation with the results obtained shown in Section 5 .
Thereafter, three case studies are evaluated in Section 6 , checking the ef fi ciency of the system in them, to fi nally extract the conclusions in Section 7 . 2. C-Mantec, constructive neural network algorithm C-Mantec ( Subirats et al., 2012 ) (Competitive Majority Network
Trained by Error Correction) is a novel neural network constructive algorithm that utilises competition between neurons and a modi perceptron learning rule (thermal perceptron Frean, 1990 ) to build single hidden layer compact architectures with good prediction capabilities for supervised classi fi cation problems. As a CoNN algo-rithm, C-Mantec generates the network topology on-line during the learning phase, avoiding the com plex problem of selecting an ade-quate neural architecture. The novelty of C-Mantec in comparison to previous proposed constructive algorithms is that the neurons in the single hidden layer compete for learning the incoming data, and this process permits the creation of very compact neural architectures. The binary activation state (S) of the neurons in the hidden layer depends on N input signals,  X  i , and on the actual value of the N synaptic weights (  X  i )andbias( b ) as follows:
S  X  1 ( where h is the synaptic potential of the neuron de fi ned as h  X   X 
In the thermal perceptron rule, the modi fi cation of the synaptic pattern) according to the following equation:  X  X  t S  X   X  i T fac ;  X  3  X  where t is the target value of the presented input, and  X  the value of input unit i connected to the output by weight difference in the standard perceptron learning rule is that the thermal perceptron incorporates the T fac factor. This factor, whose value is computedasshowninEq. (4 ), depends on the value of the synaptic potential and on an arti fi cially introduced temperature ( T ).
T  X  T
The value of T decreases as the learning process advances according to Eq. (5 ), similar to a simulated annealing process.
T  X 
T where I is a cycle counter that de fi nes an iteration of the algorithm on one learning cycle, and I max is the maximum number of iterations allowed. One learning cycle of the algorithm is the process that starts when a random chosen pattern is presented to the network and fi nishes after checking that the output of the network is equal to the target for this pattern, or when a chosen neuron (the neuron with largest T fac value or a new added neuron) modi fi es its synaptic weights to learn the actual presented pattern.
The C-Mantec algorithm has three parameters to be set at the time of starting the learning procedure, and several experiments have shown the robustness of the algorithm that operates fairly well in a wide range of parameter values. The algorithm has the following three parameters:
I max : Maximum number of learning iterations allowed for each neuron in one learning cycle. g fac : Growing factor that determines when to stop a learning cycle and includes a new neuron in the hidden layer. : Determines in which case an input example is considered as noise and removed from the training dataset according to the following condition: delete  X  x i  X   X  N LT Z  X   X   X   X  s  X  ;  X  6  X  where x i represents an input pattern, N is the total number of patterns in the dataset, N LT is the number of times that pattern x has been presented to the network on the current learning cycle, and where  X  and s correspond to the mean and variance of the distribution for all patterns on the number of times that the algorithm has tried to learn each pattern in a learning cycle. The learning procedure starts with one neuron present in the single hidden layer of the architecture and an output neuron that computes the majority function of the responses of the hidden neurons (a voting scheme). The process continues by presenting an input pattern to the network and if it is misclassi fi learned by one of the present neurons whose output did not match the target pattern value if certain conditions are met, otherwise a new neuron will be included in the architecture to learn it. Among all neurons that misclassi fi ed the input pattern, the one with the largest T fac will learn it but only if this T larger than the g fac parameter of the algorithm, a condition included to prevent the unlearning of previous stored information.
If no thermal perceptron meeting these criteria are found, a new neuron is added to the network, starting a new learning cycle that includes the resetting of all neurons temperature to T 0 . Also at the end of a cycle the noisy patterns fi ltering procedure (Eq. (6) )is applied. The algorithm continues its operation iteratively repeat-ing the previous stages until all patterns in the training set are correctly classi fi ed by the network. During the learning process catastrophic forgetting is prevented as synaptic weights are only modi fi ed if the change involved is small (controlled by the value of g fac and by an annealing process that reduces the temperature as learning proceeds), as if this is not the case the algorithm introduces a new neuron in the architecture ( Subirats et al., 2012 ).
The Flowchart of C-Mantec algorithm is shown in Fig. 1 , the most relevant function is represented in boxes, the decision-making in diamonds and the most important states (start and fi nish) in ovals. 3. The Arduino UNO board
Arduino is a single-board microcontroller designed to make the process of using electronics in multidisciplinary projects more accessible. The hardware consists of a simple open source hardware board designed around an 8-bit Atmel AVR microcon-troller, though a new model has been designed around a 32-bit Atmel ARM. The software consists of a standard programming language compiler and a boot loader that executes on the microcontroller.

Arduino is a descendant of the open-source Wiring platform and is programmed using a Wiring-based language (syntax and libraries); similar to C  X  X  with some slight simpli fi cations and modi fi cations, and a processing-based integrated development environment. Arduino boards can be purchased pre-assembled or do-it-yourself kits, and hardware design information is avail-able. The maximum length and width of the Arduino UNO board are 6.8 and 5.3 cm respectively, with the USB connector and power jack extending beyond the former dimension.
 The Arduino UNO is based on the ATmega328 chip ( Atmel, Datasheet 328 ). It has 14 digital input/output pins, which can be used as input or outputs, and in addition, has some pins for specialised functions, for example 6 digital pins can be used as PWM outputs. It also has 6 analogy inputs, each of which provide 10 bits of resolution, together with a 16 MHz ceramic resonator, USB connection with serial communication, a power jack, an ICSP header, and a reset button. The ATmega328 chip has 32 KB of memory storage (0.5 KB are used for the bootloader). It also has 2 KB of SRAM and 1 KB of EEPROM. A picture of the Arduino UNO board is shown in Fig. 2 .

The Arduino UNO has a communication protocol for its inter-action with a computer, with another Arduino board or other microcontrollers. The ATmega328 provides serial communication (UART) which is available on digital pins 0 (RX) and 1 (Tx), also has I2C and SPI communication. 4. Implementation of the C-Mantec algorithm
The neural network model comprises two phases (learning and execution). In the learning phase, the synaptic weights for the neural network are computed from a set of patterns stored in the memory of the microcontroller, while in the execution phase, the microcontroller obtains the response to sensed input data accord-ing to the previously learned model. The learning phase comprises two different states (loading of input patterns and neural network learning). Data can be loaded into the microcontroller EEPROM memory on-line by I/O pins or by a serial communication USB port, but in both cases, the patterns have to be stored into the EEPROM memory, after, the neural network learning state starts.
We explain next, the main technical issues considered for the implementation of the algorithm according to the two learning modes mentioned before: 4.1. Loading of patterns
Thepatternshavetobestoredinthememorymicrocontroller because the learning process works in cycles and uses the pattern set repeatedly. For Boolean functions, it is only necessary to store the true value function outputs because the inputs are repre-sented by the memory position. For example, for the pattern  X  01101001  X  - X  0  X  , the input  X  01101001  X  corresponds to the decimal number 105 and thus the value  X  0  X  is stored in the position of memory 105. The microcontroller has 1 KB of EPROM memory, i.e., 8192 bits (2 ) limiting the number of Boolean inputs to 13. For the case of using an incomplete truth table, because the noisy pattern elimination stage is used or because of the nature of the function, the memory is divided into two parts, a fi rst one that corresponds to the function outputs and a second part to indicate the inclusion or not of a given pattern in the learning set. In the case of using an incomplete truth table the maximum size of the input dimension is reduced to 12.

For the case of using real-value patterns is necessary to know in advance the actual number of bits that are used to represent each input variable. Eight bits have been used to represent each variable, taking into account that these values have to be normal-ised between 0 and 255. The following equation permits to compute the maximum number of input patterns: N
N I  X  N P = 8 r 1024 ;  X  7  X  where N I is the number of inputs and N P is the number of patterns. N depends on the number of entrie sandthenumberofbitsusedfor each entry. 4.2. Neural network learning
C-Mantec is an algorithm which adds neurons when they are become necessary, action that is not easily implemented in microcontroller, because the use of memory is static so the maximum number of neurons, which are stored in SRAM, must be previously de fi ned. From this memory, with a capacity of 2 KB, we will employ less than 1 KB for storing the variables of the program; and thus saving at least 1 KB of free memory for saving the variables related to the neurons.

The microcontrollers are device s with limited computing speed so for obtaining more velocity in the learning process we have changed the data type of the variables associated to neurons. The representation is the usual data type used in this kind of algorithm to represent all variables but this representation is not the most ef fi ciency. We have selected fi xed point to represent the variables associated to neurons for this we have to change the data type of these variables to integer. This paradigm shift has incited to essential changes in the way to program this algorithm but in return a higher learning speed and a smaller size of each variable has been obtained.
Regarding the variables associated to neurons, it is given next some details about the representation used:
T fac : Represented as a fl oat type and occupying 4 bytes.
Number of iterations: An integer value with a range between 1000 and 65,535 iterations, so a 2 bytes integer variable was used.

Synaptic weights: Almost all calculations are based on these variables, so to speed up the computations an integer variable of 2 bytes long was used.

Synaptic potential (h): It is calculated as a result of a summa-tion of the synaptic weights, so not to saturate this value a type long of 4 bytes in length is used.

According to the previous de fi nitions, the maximum number of neurons ( N N ) that can be implemented should verify the following constraint: 4 N
N  X  2 N N  X  2 N N  X  N I  X  1  X  X  4 N N r 1024 ;  X  8  X  where N I is the number of inputs. For a worst case (maximum number of inputs is 13), the maximum number of neurons is 28. If during the learning process, the architecture reaches this max-imum number of neurons, the algorithm fi nishes and the sensor outputs an error message.

The synaptic weights and synaptic potential have been imple-mented with 10 bits precision for the decimal part, so that the value of the weights will be between 32 and 32. Integer data types with values between 32,768 and 32,767 were used. The representation of the synaptic potential is done in a similar way, except that for this value 4 bits were used, allowing values between 2,097,152 and 2,097,152. The computation of T fac done using a fl oat data type because it involves an exponential operation that can only be done with this type of data, but as its computation involves integer values, two different conversions must be done. The fi rst conversion is done in Eq. (3) , where T values must be converted to fi xed point representation, operation done by multiplying the T fac value by 1024. The second conversion is performed in Eq. (4) where the synaptic potential ( h ) has to be converted to fl oating point representation for the calculation of an exponential function. In this case, ten right logical shifts were used in the synaptic potential, to then convert its data type to point for the fi nal calculation of the T fac . In order to avoid the saturation of the synaptic weights value, when any of these values are larger than 32 or lower than 32, all weights are divided by two (a right logical shift) when any of them reach an absolute value of 30. This change does not affect the algorithm execution because neural networks are invariant to this kind of scaling. 5. Results
We fi rst tested the correct functioning of the microcontroller implementation of the C-Mantec algorithm comparing to the original published results ( Subirats et al., 2012 ) in terms of the number of neurons generated in the neural network architectures, analysing as well the execution time for the microcontroller using fl oating point and integer representation. A set of 14 Boolean functions have been used for the comparison, including 12 single output functions from the MCNC circuits testing benchmark plus two XOR functions with two and three inputs. The C-Mantec algorithm was run with the following parameter con fi guration: g  X  0.05 and I max  X  1000. The results are shown in Table 1 where the fi rst two columns indicate the function reference name and its number of inputs, and the third, fourth and fi fth columns show that the number of neurons obtained in the original C-Mantec publication ( Subirats et al., 2012 ), and the integer and point microcontroller implementation, respectively. Two last col-umns in Table 1 show the learning times in seconds (s) for the integer and fl oating point implementation of the microcontroller.
The displayed values (mean 7 standard deviation) are computed from 50 random samples.

Fig. 3 shows the learning time for all 14 benchmark functions included in Table 1 both for the integer and fl oating point implementation. The top graph in the fi gure shows on a logarith-mic scale for the y -axis the learning time while the bottom graph shows the comparative speed between both representations used (integer and fl oating point).

Further, Fig. 4 shows the temporal evolution for the most complex analysed function (Alu2k), where the top graph shows the execution times related to the addition of a new neuron in the constructive network architecture while Fig. 4 bottom shows the relative speed between the two implementations (integer and fl oating point). The time shown in the top graph of Fig. 4 includes several learning cycles until an input pattern wrongly classi the network is presented and there is no neuron in the architec-ture that can be selected to learn it (according to the values of T and g fac ).

The C-Mantec algorithm includes three procedures that are modi fi ed depending on the data type ( fl oating or integer) repre-sentation used. These three functions that can be observed in Fig. 1 are the following: Input a random pattern , Select neuron with largest
T fac value , and Modify selected neuron weights . The algorithm includes other two procedures ( Add a new neuron and Eliminate noisy examples ) that are not altered if the data representation is changed.

For each of the procedures that change according to the representation used we computed the mean execution time averaged across 50 samples for both cases. Fig. 5 shows the results for each procedure for integer and fl oating point representations. Fig. 5 a shows the whole network execution time as the number of neurons increases from 1 to 20 for different input pattern dimen-sions (2, 5, 10, 15), while Fig. 5 b shows a comparison between both implementations, where it can be appreciated the number of times that the integer representation is faster than the fl oating point one. Fig. 5 c and d shows the execution time and relative compar-ison (respectively) for the computation of the maximum value of the T fac for both representations as the number of neurons present in the architecture increases from 1 to 20. Fig. 5 e and f shows the same as the previously described case but for the procedure Modify selected neuron weights . 6. Case studies
In this section, in order to test the ef fi ciency of using neural networks in applications where microcontrollers for sense and/or act on the environment might be required, three case studies of different nature have been considered. The fi rst case deals with detecting fi res by an alarm system that can be reprogrammed according to the security level required. The second analysed case is a control system for the activation of a solenoid valve that depends on environmental variables, while the third case corre-sponds to a person fall detection problem with the constraint of using only a limited set of data patterns. The parameter settings of the C-Mantec algorithm were the same for all analysed cases: g fac  X  0.05, I max  X  1000 and values did not improve signi fi cantly the performance on each of the problems and then decided to use this set of parameter in all three cases. We note that it has been previously veri fi ed that
C-Mantec is very robust against changes on the parameter values ( Subirats et al., 2012, 2013; Urda et al., 2013; Luque-Baena et al., 2013 ). 6.1. Fire alarm system Fire alarm systems are widely used in inhabited premises.
We analyse the case of a system whose security levels can be reprogrammed according to the user convenience, by de fi ning activation thresholds as a function of the sensed environmental variables (temperature, smoke and gas). A schematic drawing of a room where a fi re alarm is installed is shown in Fig. 6 including the three typical variables that can affect the system.
We have represented the different states of the fi re alarm system using a truth table which is shown in Table 2 .
Each different sensor (Gas, Smoke and Temperature) can be in the range from 1 to 8. As C-Mantec is a binary classi fi alarm levels were codi fi ed in binary notation, as indicated in the last column of the table. A given functioning state of the alarm system includes setting the alarm levels for each of the detectors states, i.e., choosing the alarm level for a given sensor inputs.
Table 2 corresponds to a case in which each alarm level is different for every different input, as this is the worst possible case.
We fi rst checked the time employed by the system to learn the initial state, measuring also the number of neurons included by the C-Mantec constructive algorithm and the energy consumption.
Table 3 shows the results averaged across 50 samples, where execution time is measured in milliseconds (ms) while energy consumption is measured in nano Amperes per hour (nAh).
Next, we have tested the time needed to relearn a modi fi of the state of the alarm, that corresponds to a change of values in the fourth column in Table 2 . The results are shown in Table 4 where mean, maximum and minimum reprogramming time are displayed together with the measured consumption. 6.2. Weather prediction
Weather prediction is a relevant issue in human daily life, related for example for making good decisions in agriculture (moment of harvest, time of watering and crop type). Weather prediction is a very complex problem and neural network based system has been widely used for this task ( Taylor and Buizza, 2002; Chakraborty et al., 2004 ).

We have consider a system that consists in a sensor/actuator that measures fi ve environmental variables (Temperature ( T ), Wind speed ( W s ), Wind direction ( W d ), Humidity ( H ) and Solar
Irradiance ( I )) and takes a decision, which can be to irrigate or not the surrounding land. Fig. 7 displays a real picture of a constructed sensor/actuator node that may be used for the described problem.
To analyse a possible scenario where this node may be used, we consider the case of determine whether to water or not the surrounding land according to the value of the fi ve environmental variables mentioned before. For the implementation these vari-ables have been discretised using a 12 bits representation where the number of bits used for each variable can be seen from Table 5 together with the discretisation intervals.

The evaluation of the implementation of the neural network model for controlling the actuator system has been carried out starting from a null initial state. Afterwards, random input condi-tions were considered. An example of a generic instance can be:
In the previous case, the indicated instance corresponds to a input of the truth table of the form  X  0100 00 00 10 11  X   X  1  X  as it was indicated that the solenoid valve, controlling the watering system, should open for the indicated condition. The whole truth table for the current discretisation used comprises 4096 different instances.

We measured the evolution of the time needed by the system as the number of de fi ned instances is increased, together with the level of accuracy. Fig. 8 top shows time and consumption averaged across 30 random executions (Mean) and also one randomly chosen execution of the system in order to appreciate the existing variability of the learning process. Accuracy, computed as the fraction of correct responses over the whole truth table is shown in table Fig. 8 bottom. Note that to compute the accuracy, we suppose that the fi nal whole truth table is known at every given time, even if the actuator only gets this information one pattern at a time, implying that in practice the accuracy (as it is computed) can be only analysed at the end of the process. If we had computed the accuracy over the presented pattern, this would have always been equal to 1.
 A singular behaviour in the initial instances can be observed in Fig. 8 , the neural network model has to be modi fi ed because almost all instances are misclassi fi ed thus at the beginning of the process, the learning time is higher and the accuracy increases because the number of classi fi ed instances grows. When the model has already learned almost all instances the learning time grows linearly depending on the number of instances as the results showed, and thus the accuracy is one in practice. In this moment if a instance are misclassi fi ed then the neural network model have to learn this instances and the learning time is the largest as Fig. 8 top shows for one execution.
 6.3. Fall detection system
Monitoring elderly or disabled people is in growing demand in modern societies, mainly because these people are dependent and unable to care for themselves. In particular, one important pro-blem is to detect ef fi ciently when a person falls. Different ways of addressing this problem have been proposed, like using a video surveillance systems, although if such system has the drawback of a strong sensitivity to light changes. A more ef fi cient system to detect a person fall can be constructed using a sensor attached to the person body. The sensor, in this case, should include a 3-axis accelerometer together with a microcontroller that analyses the person relative inclination angle and movement, in order to detect abnormal situations, such as a strong downward movement or any sudden or violent displacement. Fig. 9 shows a schematic drawing of the described system.

Deducing the logic which describes the behaviour of the falls is a complex and very time consuming task. However, using a neural network based system simpli fi es enormously this task as a training process based on observed patterns can be implemented.
In such a system, the accelerometer senses the position of the device, acquiring the position in relationship to axes x , y , z . These coordinates are then sent to the microcontroller, in values normal-ised between 0 and 255 so they can be stored in a variable of  X  byte  X  type. A movement is considered a fall when in a short period of time (1 s), the position of the device changes from an the system consists in the initial and fi nal position of the move-ment plus the output that indicates whether the person has suffered or not a fall  X  X  x 0 ; y 0 ; z 0 ; x 1 ; y 1 ; z patterns that can be stored on the system is delimited by Eq. (7 ), that in the present case leads to a maximum number of patterns to be stored equal to 167 patterns. As this number is quite small for such a complex problem, only patterns that a  X  supervisor sidered wrong will be used. For example, a pattern is sensed every second and the system determines whether or not a fall has occurred. If the supervisor (that during the training of the system can be the person carrying the device) resolves that the decision of the neural network model is wrong, this pattern is stored in memory and a new neural network model is calculated. The previous description is a modi fi cation to the standard C-Mantec algorithm that includes a more ef fi cient storage of patterns for maximising the use of the limited memory resources of a micro-controller. For testing the system, a sensor node has been attached to a subject producing common movements such as sitting down, walking and stopping. The microcontroller has periodically gath-ered accelerometer data every second and 30,000 patterns have been obtained as data set.

Afterwards, the neural network based systems were checked, starting the process with no patterns in the memory, to only store one instance of the problem when the classi fi cation obtained does not match the supervisor output, that in our case, was the person wearing the system.

Fig. 10 top shows the time evolution of the system as the number of patterns presented increase. In the fi gure, the mean of 30 executions is displayed together with a single case to observe the level of variability existing in the problem. Fig. 10 bottom shows the level of accuracy obtained as the patterns are presented to the system (the graph also includes the mean and a single observed case).

It is worth mentioning that the way this problem is treated is different from the previous two cases, as patterns are stored in the microcontroller memory only if they are misclassi fi ed. 7. Conclusions
The C-Mantec constructive neural network algorithm has been successfully implemented in a microcontroller board, adapting it to overcome the limitations imposed by the limited resources of memory and computing speed of the hardware device. The correct implementation of the algorithm has been veri fi ed in comparison to the original published results, ob taining that as the number of inputs is increased, the microcontroll er implementation needs a small number of extra neurons and it is possible to observe a little reduction in performance accuracy due to rounding effects. Further-more, a thorough comparison of the differences of using fl point or fi xed precision representations has be carried out, conclud-ingthatbetterresultscanbeobtainedwithan8-bit fi xprecision representation leading to computation times approximately times faster than using the standard fl oating point representation.
The implemented algorithm has been employed as a sensor/ actuator system and applied to three case studies in order to demonstrate the ef fi ciency and versatility of the resulting applica-tion. The three case studies chosen are problems de fi ned in changing environments, and thus the decision-making of the sensor/actuator has to be adapted accordingly to the observed changes, thus needing a retraining of the neural network model that controls the decision process.

The observed reprogramming times are signi fi cantly low in the three case studies, being the energy consumption of the device also quite small, and even if a comparison to the traditional case in which the new code has to be transmitted from a central control unit has not been analysed, the results suggest a very important potential reduction.
 As an overall conclusion, we have shown the suitability of C-
Mantec for its application in a dynamic task using an Arduino UNO microcontroller. Nowadays, given the existence of devices with much more powerful computing resources than the considered board, the present study con fi rms the potential of the proposed algorithm for its application in real tasks where sensors/actuators are needed.
 Acknowledgements
The authors acknowledge support from Junta de Andaluc X a through Grant Nos. P10-TIC-5770 and P08-TIC-04026, and from CICYT (Spain) through Grant No. TIN2010-16556 (all including FEDER funds).
 References
