 In this paper we evaluate the performance of different collab-orative algorithms in cold-start situations, where the initial lack of ratings may affect the quality of the algorithms. The evaluation has been performed on the pay-per-view datasets collected by two IP-television providers over a period of sev-eral months. The analysis shows that item-based algorithms perform better with respect to SVD-based ones in the early stage of the cold-start problem. Moreover, the accuracy of SVD-based algorithms, when using few latent factors, de-creases with the time-evolution of the dataset. On the con-trary, the same algorithms used with a large-enough number of latent features increase their accuracy with time and may outperform the item-based algorithms.
 H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval X  Information filtering Algorithms, Performance
Cold-start problems refer t o situations where there are only a few ratings on which to base recommendations [10]. When bootstrapping a new recommender system, the num-ber of ratings per user and item is low and this can signifi-cantly degrade the performance of collaborative algorithms.
The cold-start evaluation of collaborative recommender systems has not been extensively covered in the literature, mainly because of the characteristics of the public available datasets, all of which have a substantial large density and fairly long user profiles with respect to many real-life ap-plications. Moreover, few of these datasets provide the ad-ditional information about when the ratings have been col-lected. When this information is missing, the only approach available to simulate the cold-start problem requires to sub-sample the dataset [6]. Previous works mainly focus on the design of new collaborative algorithms that, compared with state-of-the-art algorithms, are able to improve the perfor-mance on data-sparse domains without excessively worsen-ing the performance when data is plentiful [1, 6].
In this paper we benchmark two collaborative algorithms against a non-personalized recommendation method on the cold-start problem. The evaluation has been performed on the datasets collected by two IP-television providers. The datasets have been implicitly obtained by analyzing the pay-per-view movies purchased by the users over a period of sev-eral months. The time evolution of the datasets has allowed the cold-start evaluation of two different recommender algo-rithms: an item-based and a SVD-based. This paper differs from previous works because, instead of designing a new algorithm suited to the cold-start problem, we try to pro-vide guidelines to help in the selection and tuning of differ-ent state-of-the-art collaborative algorithms in the different stages of the recommender systems life-cycle. Moreover, we combine the experience achieved on two real-life cold-start applications to obtain a deeper understanding of the perfor-mance characteristics of recommender systems. The main results suggest adopting item-based algorithms in the early stage of the cold-start period and eventually switching to SVD-based algorithms.
The two state-of-the-art collaborative algorithms described in the following section are based on a user-rating matrix (URM)thatwerefertoas R . The element r pi represents the rating of user p on item i . Since we deal with implicit, bi-nary datasets, r pi can be either 1 or 0, according to whether user p has watched or not item i , respectively.

Item-based collaborative algorithms capture the funda-mental relationships among items [8] by means of a m  X  m matrix, referred to as D ,wheretheelement d ij expresses the similarity between item i and item j .Inourtestswehave adopted the Deshpande and Karypis X  algorithm [3], without using any shrinking factor and computing the cosine simi-larity between items. The algorithm has been enhanced by means of a k NN ( k -nearest-neighborhood) approach.
Collaborative algorithms based on dimensionality X  X educ-tion techniques describe users and items by means of a lim-ited set of hidden features . There exist several methods for computing the hidden features that minimize a given pre-diction error. We have based our analysis on the classical singular value decomposition (SVD) (e.g., [4, 9]). By means ofSVD,theURMcanbefactorizedas R = U  X  S  X  V T , where the diagonal matrix S contains the first l singular val-ues (i.e., l features). In order to factorize matrix R ,wehave preferred the classical SVD decomposition, which minimizes the Frobenius norm of the erro r between actual and pre-dicted ratings, with respect t o the regularized SVD, which minimizes the round mean squared error [7, 2]. Assuming that u p represents the p -row of U and v i the i -row of V , the prediction  X  r pi can be computed as Since U and V have orthonormal columns, we can derive that u p  X  S = r p  X  V ,where r p is the p -th row of R (i.e., the profile vector of user p ). Consequently, (1) can be reformu-lated as By means of (2) we are able to predict at real X  X ime the ratings of any items not watched by user p .
In the next sections we are going to investigate how the learning rate of the tested algorithms evolves with time (i.e., with increasing user profile length, increasing views per items, and increasing number of users and of items). Moreover, we are going to investigate if and how the algorithm parame-ters (e.g., latent size l , neighborhood size k ) should be tuned when the dataset evolves with time.

The dataset evolution is driven by three inputs: (i) exist-ing users watch new movies, (ii) new users join the system, (iii) new items are added to the catalog.

The first input has the effect of increasing the dataset den-sity, the average user profile length, and the average number of views per item. The second input happens when new or existing users watch their first movie. Such input has the effect of decreasing both dataset density and average user profile length, as the new users that join the system have watched one movie. Similarly, the third input factor hap-pens when at least one user watches for the first time either an existing or a new movie. The effect is that both dataset density and average number of views per item decrease.
Since we only have binary ratings, we are forced to use classification accuracy metrics (e.g., recall) for the systems evaluation. We have adopted a k -fold methodology. The URM is divided into k folds by partitioning the rows (i.e., the users) into k disjoint sets (in our tests k = 10): k  X  1 folds are used to train the algorithm while the remaining fold is used to evaluate the algorithm. The process is repeated k times, selecting a different test fold at each iteration. Within the test fold, users are tested by using a leave-one-out approach: for each tested user, each rated item is removed in turn from the user profile, and recommendations are generated on the basis of the remaining user ratings. If the removed item is recommended to the user within the first N positions we have a hit (in our tests N = 5). The recall is computed as the percentage of hits with respect to the number of tests.
Many works in the literature simulate the cold-start phase of a recommender system by randomly sub X  X ampling the set of ratings [6]. However, we have observed that this approach introduces a number of anomalies in the dataset:
Similar problems happen with some of the public datasets that have timestamps. For instance, the timestamps in the Movielens dataset do not refer to when the users watched the movies but, rather, refer to when the users rated them.
As an example, we have sub X  X ampled the dataset of the first IPTV operator, TV1, by randomly timestamping the implicit ratings (views). Figure 1 shows the statistical prop-erties of the sub-sampled dataset over time: the number of views and the dataset density (a), the average number of views per item and per user (b), the number of active users and of active items (c).

If we compare such random evolution with the actual dataset evolution shown in Figure 2, the most noticeable dif-ferences are in the time X  X volution of the density and of the number of items. This happens because the TV1 dataset has a fairly large number of ratings per item. Thus, the random sub X  X ampling of ratings is not able to reduce the number of items till the dataset density has been greatly reduced. The final effect shown in Figure 1(c) is that the random selec-tion mechanism simulates a cold-start problem in which the number of items is almost constant. However, within a real cold-start problem, the rate at which new items are added to the catalog is more regular with time, as in Figure 2(c).
In this section we discuss the quality of the recommender algorithms presented in Section 2 on two datasets that we will refer to as TV1 and TV2, respectively: TV1 has been collected over a period of one year, while TV2 has been collected over a 6 X  X onth period. The two datasets can be downloaded from memo.elet.polimi.it .

Figure 2 reports the properties of the TV1 dataset as a function of time: number of views (i.e., ratings) and dataset density (a), average number of views per item and per user (b), number of users and items (c). We can observe that the number of ratings grow almost linearly with time, while the URM density tends to flatten.

Figure 3 shows the recall as a function of time: cos knn refers to the k NN item-based algorithm and svd refers to the dimensionality-reduction-based algorithm. For the k NN algorithm, we have tested different values of the number k of nearest-neighborhoods items. For the SVD algorithm, we have tested different values of the latent-size parameter l . In addition, we have also plotted the recall of a triv-ial, basic algorithm, referred to as toprated . Such algorithm suggests, for any user profile, the list of the 5 most-popular items (discarding items already rated/viewed by the user). Surprisingly, we observe that the recall for some of the al-gorithms decreases with time before reaching a steady-state value. This result apparently is in contrast with other results Figure 1: TV1 dataset with random timestamps : number of ratings and density of URM (a), average views per item and per user (b), number of users and items (c). stating that the learning rate during the cold-start phase in-creases with the density of the dataset [5]. However, we have to consider that, although the dataset density increases over time, the number of active items in a real cold-start problem (as opposed to a simulated cold-start) increases as well. In-deed, the larger the number of items, the harder it is for an algorithm to select items that users have effectively watched. Figure 3 shows also that the k NN algorithm always outper-forms the SVD algorithm in the early stage of the system, when there are few ratings and items in the dataset.
Figure 4 reports the properties of the TV2 dataset over time. Compared to TV1, the number of active users, active items and ratings is about one order of magnitude larger. However, TV2 dataset is one order of magnitude sparser, and its density is constant with time (in 6 months there is a minimal increase from 0.11% to 0.17%). This happens because the rating at which new users and items join the Figure 2: TV1 dataset: number of ratings and den-sity of URM (a), average views per item and per user (b), number of users and items (c). dataset overcame balance the rating at which exiting users watch new items.

Recalls reported in Figure 5 confirm that k NN has gen-erally a better quality than SVD. We can observe that the average quality of the k NN algorithms is higher if compared to the quality obtained on TV1.
The first result of the paper outlines that the item-based algorithm performs better with respect to the SVD-based algorithm in the early stage of the cold-start problem. The second result shows that the accuracy of the SVD-based al-gorithm when using few latent factors decreases with the time-evolution of the dataset.

Further, ongoing tests are meant to evaluate other aspects of the cold-start problem, such as: differentiating the eval-uation between existing and new users (with respect to a
Figure 3: Recommender quality over time in TV1. certain time), or comparing the quality of collaborative al-gorithms with the quality of content-based algorithms. [1] H. J. Ahn. A new similarity measure for collaborative [2] R. Bell, Y. Koren, and C. Volinsky. Modeling [3] M. Deshpande and G. Karypis. Item-based top-n [4] G.W.Furnas,S.Deerwester,S.T.Dumais,T.K.
 [5] J. Herlocker, J. Konstan, L. Terveen, and J. Riedl. [6] Z. Huang, H. Chen, and D. Zeng. Applying associative [7] A. Paterek. Improving regularized singular value [8] B. Sarwar, G. Karypis, J. Konstan, and J. Reidl. [9] B. Sarwar, G. Karypis, J. Konstan, and J. Riedl. [10] J. Schafer, D. Frankowski, J. Herlocker, and S. Sen. Figure 4: TV2 dataset: number of ratings and den-sity of URM (a), average views per item and per user (b), number of users and items (c).
Figure 5: Recommender quality over time in TV2.
