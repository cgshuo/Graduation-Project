 David Wingate wingated@umich.edu Satinder Singh baveja@umich.edu One of the basic problems in modeling controlled, par-tially observable, stochastic dynamical systems is rep-resenting and tracking state. In a reinforcement learn-ing context, the state of the system is important be-cause it can be used to make predictions about the fu-ture, or to control the system optimally. Often, state is viewed as an unobservable, latent variable, but models with predictive representations of state (Littman et al., 2002) propose an alternative: PSRs represent state as statistics about the future .
 The original PSR models used the probability of spe-cific, detailed futures called tests as the statistics of interest. Recent work has introduced the more gen-eral notion of using parameters that model the distri-bution of length n futures as the statistics of interest (Rudary et al., 2005; Wingate, 2008). To clarify this, consider an agent interacting with the system. It ob-serves a series of observations o 1 ...o t , which we call a history h t (where subscripts denote time). Given any history, there is some distribution over the next n ob-servations: p ( O t +1 ...O t + n | h t )  X  p ( F n | h t is the random variable representing an observation i steps in the future, and F n is a mnemonic for future ). We emphasize that this distribution directly models observable quantities in the system.
 The Exponential Family PSR is a new family of models of partially observable, stochastic dynamical systems. EFPSR models assume that the distribution p ( F n | h t ) has an exponential family form, and that the param-eters of that distribution are the state of the system (Wingate, 2008). This idea has been shown to unify a number of existing models of dynamical systems: for example, if p ( F n | h t ) is assumed to be Gaussian (and certain other choices are made), the model can capture any domain modeled by a Kalman filter.
 Existing algorithms for learning EFPSR models from data are based on maximizing exact likelihood, but the algorithms are slow. This paper presents an effi-cient algorithm for one particular EFPSR, named the Linear-Linear EFPSR. We begin by presenting an ap-proximate likelihood function, and then show that the terms needed to maximize it can be efficiently com-puted by virtue of the linearity of the Linear-Linear EFPSR X  X  state update. The resulting algorithm is computationally efficient, and can be interpreted in terms of stationary distributions of observations, fea-tures and states. It allows us to begin to learn models of domains which are too large (in terms of the amount of data required, and in terms of the complexity of the observation space) to tackle with any other EFPSR. | h t ) p ( F n | h t , o t +1 ) We first review the EFPSR family of models, including how state is represented and how it is maintained. State . The EFPSR defines state as the parameters of an exponential family distribution modeling p ( F n | h t which is a window of n future observations. To em-phasize that these parameters represent state, we will refer to them as s t . The form of the distribution is: p ( F n = f n | h t ; s t ) = exp s  X  t  X  ( f n )  X  log Z ( s with both {  X  ( f n ), s t }  X  R l  X  1 . The vector  X  ( f n ) is a feature vector which controls the particular form of the distribution. For example,  X  ( X ) = [ X, X 2 ], yields a Gaussian, but  X  ( X ) = [ X, log( X )] yields a gamma. Since the distribution is over the future,  X  can be thought of as features of the future . As the agent interacts with the system, p ( F n | h t ) changes because h t changes; therefore the parameters s t and hence state change. The feature vector  X  ( f n ) does not change over time.
 Maintaining State . In addition to selecting the form of p ( F n | h t ), there is a dynamical component: given the parameters of p ( F n | h t ), how can we incorporate a new observation to find the parameters of p ( F n | h t , o t +1 That is, how can we update state? Our strategy is to extend and condition .
 Extend. We assume that we have the parameters of p ( F n | h t ), denoted s t . We extend the distribution of F n | h t to include O t + n +1 , which forms a new vari-able F n +1 | h t , and we assume it has the distribution distribution over ( n + 1) observations.
 To perform the extension, we define an extension func-tion which maps the current state vector to the param-eters of the extended distribution: where  X  is a vector of parameters controlling the ex-tension function (and hence, the overall dynamics). The extension function helps govern the kinds of dy-namics that the model can capture. For example, in the PLG family of work, a linear extension allows the model to capture linear dynamics (Rudary et al., 2005), while a non-linear extension allows the model to capture non-linear dynamics (Wingate, 2008). Condition. Once we have extended the distribution to model the n + 1 X  X t observation in the future, we then condition on the actual observation o t +1 , which results in the parameters of p ( F n | h t +1 ): which is our state at time t + 1.
 The entire process of extending and conditioning is il-lustrated in Fig. 1. We have drawn graphs to suggest that there can be structure in the distributions, and to informally hint at the fact that the form of the dis-tribution does not change over time. This, and other constraints on the features and extension function, are discussed in detail elsewhere (Wingate, 2008). 2.1. The Linear-Linear EFPSR The EFPSR is a family of models. Specific members of the family are chosen by selecting two things: the features  X  , and an extension function. For example, if p ( F n | h t ) is Gaussian, and a special extension function is chosen, the predictively defined version of a linear dynamical system (Kalman filter) is recovered. The Linear-Linear EFPSR chooses features and an ex-tension function designed to make it both analytically tractable and efficiently approximable. The extension function is linear, and features are chosen such that conditioning is always a linear operation (hence the name,  X  X inear-Linear X ). In this paper, we also as-sume the base observations are vectors of binary ran-dom variables. If they are not, we assume that bi-nary features are extracted from the observations, and discard the original observations (we call these atomic features, to distinguish them from the higher-order fea-tures defined by  X  ).
 Features. Let each base observation O t be a vector strict all features comprising the feature vector  X  to be conjunctions of the atomic binary variables in the base observations. For example, if each O t  X  { 0 , 1 } 3 there could be a feature  X  ( o t ) k which is a conjunc-tion of the second and third components of the ob-this way, the resulting distribution can be conditioned with an operator that is nonlinear in the observation o t +1 , but linear in the state s t . We therefore define the linear conditioning operator G ( o t +1 ) to be a ma-trix which transforms s + t into s t +1 : s t +1 = G ( o t +1 See (Wingate, 2008) for details.
 Extension function . We choose a linear extension: A  X  R k  X  l and B  X  R k  X  1 are our model parameters. The combination of a linear extension and a lin-ear conditioning operator means that the en-tire extend-and-condition operation (ie, state update) is a linear operation: This will be critical in the sequel. We now briefly sketch how to learn a Linear-Linear EFPSR model from data by maximizing exact likeli-hood. We do this to point out the two primary com-putational bottlenecks that motivate this paper. We assume we are given a sequence of T observa-tions, [ o 1 o T ], which we stack to create a sequence The likelihood of the training data is p ( o 1 , o 2 ...o Q t =1 p ( o t | h t ), but we will find it more convenient to measure the likelihood of the corresponding f t  X  X : p ( o 1 , o 2 ...o T )  X  n equivalent to maximizing the standard likelihood). The expected log-likelihood of the training f t  X  X  under the model defined in Eq. 1 is Our goal is to maximize this quantity. Any opti-mization method can be used to maximize the log-likelihood. Two popular choices are gradient ascent and quasi-Newton methods, such as (L-)BFGS, which require the gradient of the likelihood with respect to the parameters, which we will now compute.
 We can differentiate with respect to our parameters: and with respect to each state: where E s sufficient statistics at time t .
 The gradient of s t with respect to A is given by where  X  is the Kronecker product, and I is an identity matrix the same size as A . The gradient of the state with respect to B is Note that the gradients at time t are temporally re-cursive  X  they depend all previous gradients. There are two bottlenecks which motivate this paper: 1. Computing E s 2. The gradients are temporally recursive, but can To summarize, the exact learning algorithm does not scale well with either the number of training samples T , the dimension of the observations, the window size n , or the number of features |  X  | .
 We now turn to the main contribution of this paper. In order to achieve an efficient learning algorithm, we will present an approximate expression for likelihood, named c LL , and show that its gradient can be effi-ciently computed. We will also examine what happens in the limit as T  X   X  . The quantity c LL could be used with any model, not just the Linear-Linear EF-PSR, but we will show that the Linear-Linear EFPSR allows us to compute the needed terms easily. We now present our approximate log-likelihood c LL , which is an approximate lower bound on the exact like-lihood. To begin, we will make one central assumption: Assumption 4.1. We assume that Cov[ s t ,  X  ( f t )] = 0 and that Cov[ s t , o t ] = 0,  X  t .
 This assumes that the state does not covary with observable quantities. It implies that E[ s  X  t  X  ( f t )] = E[ s t ]  X  E[  X  ( f t )], which will be repeatedly used in the following derivation. This is not as severe of an as-sumption as it may appear to be  X  in particular, that this does not imply that s t and  X  ( f t ) are independent. We derive c LL using Assumption 4.1 and a lower bound based on Jensen X  X  inequality: where we have defined the operator The fourth line in the derivation follows because of Assumption 4.1. The fifth line is obtained by a double application of Jensen X  X  inequality: E[  X  log Z ( s t )] = E  X  log( Algorithm 1 LEARN-EFPSRS-W-APPROX-LL Input: E T [ o t ], E T [  X  ( f t )]
Initialize A = 0, B = 0. repeat until c LL is maximized Return A , B The second and third lines follow because of the con-vexity of the functions  X  log and exp, and the fourth line follows by Assumption 4.1.
 The approximate log-likelihood involves several new terms, which we now explain. Consider E T [ s t ]. Be-cause this is an unconditional expectation, as T  X  X  X  , this can be interpreted as the stationary distribution of states induced by a particular setting of the param-eters of the model.
 At first glance, this term would appear to defeat the point of our approximations: it appears to depend on T and on the model parameters, which means that we would have to recompute it, at cost T , every time the parameters change (as they would inside any sort of optimization loop). Fortunately, because it is the stationary distribution of states, it can be efficiently computed in the case of the Linear-Linear EFPSR as the solution to a linear system of equations in a way that does not depend on T .
 The other terms have similar interpretations. E
T [  X  ( f t )] is empirically observed stationary distribu-tion of features of n -step windows of observations. Since it does not depend on the model parameters, it can be computed once at the beginning of learning in a single pass through the data. The quantity log Z (E T [ s t ]) is the log partition function Z computed using the vector E T [ s t ], and can be computed in the same way as the partition function associated with any ordinary state s t . 4.1. Computing the Approximate Likelihood Can the approximate log-likelihood c LL and its deriva-tives be computed efficiently? The answer is yes: Ap-pendix A shows that in the case of the Linear-Linear EFPSR, both c LL and the derivative of c LL with respect to the model parameters can be computed efficiently. The computation does not depend on T (the amount of training data), and only involves the solution to two sparse linear systems of equations. Inference must be performed on the graphical model only once. In addi-tion, the expensive matrix-matrix multiplications are completely eliminated. 4.2. Algorithm Summary Let us pause for a brief summary. The exact log-likelihood LL in Eq. 2 is intractable to maximize. However, we have introduced c LL , and shown that it and its derivatives can be computed efficiently. Putting everything together, we see that this learning algorithm is attempting:  X  to find a setting of the parameters A and B  X  which generate a stationary distribution of states  X  based on a transition operator defined using the  X  which imply a stationary distribution of features With gradients in hand, any optimization method may be used to find the optimal settings for A and B . The final gradient algorithm is shown in Algorithm 2 (in Appendix A), and a simple companion steepest de-scent optimizer is shown in Algorithm 1. We briefly turn our attention to the parameter matri-ces A and B . So far, we have implicitly assumed that the matrix A is reasonably sized, but this assumption is false in the case of a large number of features. To clarify this, recall that our state s t is a vector  X  R When we extend and condition, we implicitly compute s , which is a vector of parameters describing n + 1 observations: s + t = As t + B . If we assume that there are k extended features, the A matrix is  X  R k  X  l . One of the goals of EFPSRs is to be able to use many features in order to capture state. If the number of features l is very large (say, tens of thousands, or even millions), the number of extended features k will be even larger, and the matrix A will be too large to work with. For example, if there are 10,000 features, and if the extended distribution has 15,000 features, the c LL has another property which suggests a solution to this problem: the gradients  X  A c LL have a natu-ral rank-one form, and therefore mesh well with sin-gular value decomposition (SVD) update algorithms (Brand, 2006). Instead of maintaining the full matrix A , we can maintain a low-rank SVD of A . Given the SVD of A and a rank-one gradient update, the param-eters of the updated SVD can be efficiently computed. The entire process can be meshed with a rank-aware line search. The advantage is that the full matrix A is never computed, but exact line searches can be con-ducted. See (Wingate, 2008) for more details. We now evaluate the quality of our approximations. For large problems, we cannot compute the exact like-lihoods to compare with, and since we are using ap-proximate likelihoods, it is not clear what a compari-son would mean. Instead, we use reinforcement learn-ing to help measure the quality of the model: we use the states generated by the EFPSR as the input to an reinforcement learning planner. We conclude that our model is good if the RL agent is able to use it gener-ate performance comparable to that of the true model. For comparison, we also tested RL using the raw ob-servations as state (called the  X  X eactive, X  or first-order Markov policy), and a random policy. 6.1. Planning in the EFPSR We used the Natural Actor Critic (or NAC) algo-rithm (Peters et al., 2005) to test our model. NAC requires two things: a stochastic, parameterized pol-icy and the gradients of the log probability of that policy. We used a softmax function of a linear pro-jection of the state: the probability of taking ac-tion a i from state s t given the policy parameters  X  is p ( a i ; s t ,  X  ) = exp s  X  t  X  i /  X  is to be learned. See (Wingate, 2008) for details. 6.2. Bouncing Ball The first test domain is called the Bouncing Ball do-main. In this domain, the observations are factored in a way that is closely related to the dynamics of the sys-tem. This domain was hand-crafted to be compatible with the EFPSR: the domain has significant structure in the observations, and basically requires the use of a model which is able to capture that structure. Figure 2 describes the domain pictorially. The left fig-ure shows the the ball bouncing. At each timestep, the agent observes an 11x10 array of pixels which may be black or white. One of these pixels represents the  X  X all, X  which bounces diagonally around the box (shown as a gray trail in the figure). The agent has two actions: 0 means  X  X o nothing, X  and 1 means  X  X e-verse the direction of the ball. X  The reward signal is shown in the middle. This domain is episodic: every 50 timesteps, the ball is reset to a random position. We define three different versions of the domain. In the noiseless version, the agent sees the exact position of the ball. This domain is second-order Markov with 11  X  10 = 110 observations. The second version adds a p =1% chance of flipping white pixels to black. This domain is no longer second-order Markov, and has 2 110 possible observations. The third version uses p =10%. Figure 2 shows the features we used, which are hand-coded to correspond with the known dynamics. We set n = 2 and added singleton features for each observa-tion. Pairwise features were added for each variable to its diagonal neighbors in the next timestep (to capture the diagonal motion of the ball). The extended distri-bution p ( F 3 | h t ) used quartets consisting of an action and observation at time t , and diagonal observations at time t +1 and t +2. There were 584 features describing p ( F 2 | h t ) and 1,292 features describing p ( F 3 | h We used the timeless gradients, the low rank approx-imation of A , and 100,000 training samples. Figure 3 collects the results. The EFPSR is able to consis-tently improve over the best reactive policy, generat-ing a policy with 30% higher reward in the noiseless version, a policy with 25% higher reward when p =1%, and a policy with 13% higher reward when p =10%. It is an open question as to whether different feature sets would improve these results further. 6.3. Robot Vision Domain Together, the combination of the Linear-Linear EF-PSR, the approximate maximum likelihood objective function, and the low-rank decomposition of the pa-rameter matrix allow experimentation on domains with hundreds of observation variables and tens of thousands of features, which is larger than any other model with a predictive representation of state. Here, we apply the entire suite of techniques to the task of visual navigation, where a robot must navigate a maze using only features of camera images as observations. Figure 4 explains the setup. The latent state space consists of a position x, y and orientation  X  . The ex-periments used two different maps (bottom left). The agent has four actions: move forward, move backward, turn left and turn right. We tested two kinds of dy-namics: in the  X  X oarse X  dynamics, the agent took large steps and turns, and in the  X  X ine X  dynamics, the agent took small steps and turns. The initial observations are 64x64 color images, from which binary features are extracted (upper left). We tried two different sets of binary features. The first set consisted of 884 fea-tures like edges, corners and colors, and the second feature set was a post-processed version of the first. The idea of the second set was to create higher-order features which represented things like walls and hall-ways. To do this, images from Maze #1 were clustered according to the latent states, and then the binary fea-tures were averaged together to create a sort of filter. New images were tested against each filter, triggering if the response exceeded a threshold. There were 373 of these features. Note that while the images were all taken from Maze #1, they were also used in Maze #2, where the colors, hall geometry, etc. were all different. We set n = 3. For the feature vector  X  (), we used  X  X treamer features. X  These connect each observation variable only to its temporal successors (Fig. 4, right). There were between 12,000 and 50,000 total features in the final feature set. We trained on 200,000 sam-ples generated with a random policy. For the NAC parameters, we used a TD rate of  X  = 0 . 85, a step-size  X  = 10 . 0, gradient termination test  X  = 0 . 001 and remembering factor  X  = 0 . 0.
 Figure 5 shows the results. The random policy per-formed the same in both domains, regardless of map or dynamics. Higher rewards were obtained in general with coarse dynamics, regardless of map, feature set, or learning algorithm (presumably because the agent can reach high-reward regions more quickly). The difference between the two feature sets that is most interesting. Using feature set #1, the EFPSR performs just under the performance of the reactive policy, regardless of map or dynamics. Perhaps this means that the EFPSR was unable to capture any meaningful dynamics, and instead learned to predict the identity function, with some noise. This would result in a policy equivalent to the reactive policy. The results are reversed for feature set #2. Here, the EFPSR consistently outperforms the reactive policy. Together, these observations imply a coherent story. For both feature sets, we used the same set of streamer features. One plausible explanation for the results is that low-order conjunctions of more abstract features gives more modeling benefit than low-order conjunc-tions of granular, low-level features. It is easy to imag-ine that low-order conjunctions of granular features is insufficient to capture useful abstract structure in the domain. For example, to represent the corner of a wall, the agent might need a conjunction of 10 features, but we only had fourth order conjunctions. This was part of the motivation for feature set #2: because the cam-era images were clustered according to latent states, they were typically images of the same thing, from slightly different positions and angles. Using this fea-ture set, the highest-order conjunction was still four or five, but these conjunctions may represent more ab-stract knowledge: if one feature represents  X  X ink wall X  and another represents  X  X ink corner, X  perhaps a low-order conjunction could express  X  X  X  X  looking at a pink wall, but if I turn left, I X  X l see a pink corner. X  The idea that low-order conjunctions of more abstract features gives more modeling benefit than low-order conjunc-tions of granular, low-level features suggests several directions for future improvement of these results. Not reflected in the performance graphs is the com-putation required. Learning the model was relatively easy, taking only about 30 seconds. Because of the intensive rendering and relatively large size of the do-mains, the NAC algorithm required about a day to generate the policies to be reported. Informal calcula-tions indicated that it would take about a week to get a single gradient with exact likelihood. We have presented a computationally efficient learning algorithm for the Linear-Linear EFPSR model and il-lustrated it on two domains. Our main contribution is an approximate likelihood, and the insight that maxi-mizing it is equivalent to attempting to match station-ary distributions. This idea may find traction in other learning problems. While evaluation of the model and learning algorithm is challenging, it is only by virtue of these approximations that we were able to attempt at all domains like the Bouncing Ball or the Robot Vi-sion domain, which have continuous state spaces, rich observations, and tens of thousands of features. For both domains, we obtained better-than-reactive con-trol policies, suggesting that information from history has successfully been incorporated into the state repre-sentation. This is a positive result considering the size of the data set and the number of features involved. Future work needs to address the problem of learning good atomic features and the graphical structure, since these appear to be key factors affecting performance. To compute c LL we must compute three terms: E
T [ s t ] (the stationary distribution of states), E T [  X  ( f (which is computed once from data), and the log par-tition function log Z (E T [ s t ]). We begin with E T [ s Recall that our goal is to compute this term in a way that is independent of T . This will be possible using Assumption 4.1, the linearity of the state update, and an insight related to stationary distributions:
E T [ s t ] = E T [ G ( o t ) ( As t  X  1 + B )] where I is an appropriately sized identity matrix, and where B G = G (E T [ o t ]) B . The second line follows by Assumption 4.1.
 The third and fourth lines are both interesting for dif-ferent reasons. The fourth line follows by the linear-ity of the operator G ( ). The matrix G (E[ o t ]) can be interpreted as the expected transition operator, and is a simple function of the stationary distribution of observations E T [ o t ]. The third line follows by the lim-iting properties of our expectations: we assume that E
T [ s t ] = E T [ s t  X  1 ] because as T  X   X  , both represent the stationary distribution of states.
 The result is that E T [ s t ] can be computed as the so-lution to a linear system of equations. Note that G ( E T [ o t ]) will typically be very sparse, and a designer may force the A part to be sparse or low-rank. If so, a matrix-vector product can be computed efficiently, and an iterative solver should be used to solve Eq. 5. Computing Derivatives . We now compute the derivatives of c LL with respect to A and B : We begin with the left-hand term: This result has an appealing intuitive interpretation. E
E T [ s t ] [  X  ( F )] can be interpreted as the expected fea-tures that would be obtained if inference were per-formed using E T [ s t ] as the state  X  in other words, it represents the stationary distribution of features un-der the model. Since E T [  X  ( f t )] represents the em-pirically observed stationary distribution, we see that the gradient wishes to match the two. If we use a variational method to compute the log partition func-tion log Z (E T [ s t ]), which is needed to determine the value of the log-likelihood, then the expected features E
E T [ s t ] [  X  ( F )] are available as a byproduct of the opti-mization. This is a pleasing efficiency.
 However, we are not done. We still must find the tran-sition parameters which allow us to move the expected sufficient statistics closer:  X  E T [ s t ]  X  X  We now find it convenient to remember that the full derivative also includes the term  X  c LL / X  E T [ s t  X , which is a column vector. Let  X   X   X   X  ( I  X  G (E[ o t ]) A )  X  X  X  1 G (E[ o t ]). Then: Algorithm 2 GRADS-OF-APPROX-LL
Input: E T [ o t ], E T [  X  ( f t )], A , B // Compute stationary distribution of states
E T [ s t ] = ( I  X  G (E T [ o t ]) A )  X  1 B // Use E T [ s t ] to perform inference Compute E E // Compute the approximate log-likelihood: c LL =  X  E T [ s t ]  X  E E // Compute the gradient:  X  = E[  X  ( f t )]  X  E E
 X  =  X   X  ( I  X  G (E[ o t ]) A )  X  X  X  1 G (E[ o t ])  X  A c LL =  X   X  E T [ s t ]  X   X  note: a rank-one matrix  X  B c LL = G (E T [ o t ])  X   X  Return c LL ,  X  A c LL ,  X  B c LL The derivative with respect to B is similar: The completed algorithm is shown in Algorithm 2. Both authors were supported by NSF grant IIS-0413004. Any opinions, findings, and conclusions or recommendations expressed are those of the authors and do not necessarily reflect the views of the NSF.
