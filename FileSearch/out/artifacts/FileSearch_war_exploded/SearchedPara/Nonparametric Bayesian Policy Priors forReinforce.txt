 stochastic, and partially observable domain, given both da ta from independent exploration and ex-model-based RL algorithms [1, 2, 3, 4] to learn the world X  X  dy namics. These approaches build mod-els to predict observation and reward data given an agent X  X  a ctions; the action choices themselves, since they are made by the agent, convey no statistical infor mation about the world. In contrast, imitation and inverse reinforcement learning [5, 6] use exp ert trajectories to learn reward models. These approaches typically assume that the world X  X  dynamic s is known.
 ries. Data from independent observation gives direct infor mation about the dynamics, while expert demonstrations show outputs of good policies and thus provi de indirect information about the un-derlying model. Similarly, rewards observed during indepe ndent exploration provide indirect infor-mation about good policies. Because dynamics and policies a re linked through a complex, nonlinear function, leveraging information about both these aspects at once is challenging. However, we show that using both data improves model-building and control pe rformance.
 We use a Bayesian model-based RL approach to take advantage o f both forms of data, applying ous work [7, 8, 9, 10], the model prior p ( M ) was defined as a distribution directly on the dynamics and rewards models, making it difficult to incorporate exper t trajectories. Our main contribution is a new approach to defining this prior: our prior uses the assum ption that the expert knew something about the world model when computing his optimal policy. Dif ferent forms of these priors lead us to three different learning algorithms: (1) if we know the expe rt X  X  planning algorithm, we can sample models from p ( M | D ) , invoke the planner, and weigh models given how likely it is t he planner X  X  can similarly weight world models according to how likely it is that probable policies produced the expert X  X  data; and (3) we can search directly in the policy sp ace guided by probable models. We focus on reinforcement learning in discrete action and ob servation spaces. In this domain, one of our key technical contributions is the insight that the Baye sian approach used for building models of transition dynamics can also be used as policy priors, if we e xchange the typical role of actions and observations. For example, algorithms for learning partia lly observable Markov decision processes (POMDPs) build models that output observations and take in a ctions as exogenous variables. If we reverse their roles, the observations become the exogeno us variables, and the model-learning algorithm is exactly equivalent to learning a finite-state c ontroller [11]. By using nonparametric Our framework has several appealing properties. First, our choices for the policy prior and a world model prior can be viewed as a joint prior which introduces a b ias for world models which are model and then plan with it. Our method can also be used with ap proximately optimal expert data; in the model. For example, in Sec. 4 an application where we extr act the essence of a good controller from good X  X ut not optimal X  X rajectories generated by a random ized planning algorithm. { S immediate reward for each state-action pair, while  X   X  [0 , 1) is the discount factor. We focus on learning discrete state, observation, and action spaces.
 POMDP models. Given data D from an unknown , the agent can compute a posterior over pos- X  X avor simpler models, X  and strong structural assumptions , such as topological constraints among states. Bayesian nonparametric approaches are well-suite d for partially observable environments because they can also infer the dimensionality of the underl ying state space. For example, the re-cent infinite POMDP (iPOMDP) [12] model, built from HDP-HMMs [13, 14], places prior over POMDPs with infinite states but introduces a strong locality bias towards exploring only a few. The decision-theoretic approach to acting in the Bayesian R L setting is to treat the model M as additional hidden state in a larger  X  X odel-uncertainty X  PO MDP and plan in the joint space of models and states. Here, P ( M ) represents a belief over models. Computing a Bayes-optimal policy is computationally intractable; methods approximate the opt imal policy by sampling a single model models and choosing actions based on a vote or stochastic for ward search [1, 4, 12, 2]; and by trying to approximate the value function for the full model-uncert ainty POMDP analytically [7]. Other approaches [15, 16, 9] try to balance the off-line computati on of a good policy (the computational complexity) and the cost of getting data online (the sample c omplexity).
 observable reinforcement learning setting X  X s to consider a parametric family of policies, and at-tempt to estimate the optimal policy parameters from data. T his is the approach underlying, for example, much work on policy gradients. In this work, we focu s on the popular case of a finite-state over next-nodes n  X  to which the agent may transition after taking action a from node n . The policy Nodes are discrete; we again focus on discrete observation a nd action spaces. We now describe our framework for combining world models and expert data. Recall that our key assumption is that the expert used knowledge about the under lying world to derive his policy. Fig. 1 Figure 1: Two graphical models of expert data generation. Le ft: the prior only addresses world dynamics and rewards. Right: the prior addresses both world dynamics and controllable policies. shows the two graphical models that summarize our approache s. Let M denote the (unknown) world model. Combined with the world model M , the expert X  X  policy  X  expert X  X  and agent X  X  data D is a sequence of actions a access to all histories, but the true world model and optimal policy are hidden.
 Both graphical models assume that a particular world M is sampled from a prior over POMDPs, g
M ( M ) prior g world model M , executes a planning algorithm plan ( M ) to construct an optimal policy  X  expert then executes the policy to generate expert data D where  X  However, the graphical model in Fig. 1(a) does not easily all ow us to encode a prior bias toward more controllable world models. In Fig. 1(b), we introduce a new graphical model in which we allow additional parameters in the distribution p (  X  form where we interpret g model . We can write the distribution over world models as If f M (  X  e ) is a delta function on plan ( M ) , then the integral in Eq. 2 reduces to where  X  M controllers as discussed in Sec. 2, the policy prior g number of nodes used the policy, while g visited states in the world. The function f likely it is that the expert uses the policy  X  Finally, we note that p ( D histories D world models given data D The model in Fig. 1(a) corresponds to setting a uniform prior on g distribution over policies given data D We next describe three inference approaches for using Eqs. 4 and 5 to learn. #1: Uniform Policy Priors (Bayesian RL with Expert Data). If f actions for a particular world model M . Eq. 4 allows us to compute a posterior over world models that accounts for the quality of this match. We can then use th at posterior as part of a planner by posterior is given by: We assume that we can draw samples from p ( M | D o,r assumption in Bayesian RL [12, 9]; for our iPOMDP-based case , we can draw these samples using the beam sampler of [17]. We then weight those samples by p ( D a yield the importance-weighted estimator Finally, we can also sample values for q ( a ) by first sampling a world model given the importance-weighted distribution above and recording the q ( a ) value associated with that model. #2: Policy Priors with Model-based Inference. The uniform policy prior implied by standard Bayesian RL does not allow us to encode prior biases about the policy. With a more general prior (graphical model 1(b) in Fig. 1), the expectation in Eq. 6 bec omes where we still assume that the expert uses an optimal policy, that is, f Eq. 7 can result in somewhat brittle and computationally int ensive inference, however, as we must compute  X  policy, whereas a more realistic assumption might be that th e expert uses a near-optimal policy. We now discuss an alternative that relaxes f prefers policies that achieve higher rewards in world model M : f policies that yield high value. Substituting this f We again assume that we can draw samples from p ( M | D o,r additionally assume that we can draw samples from p (  X 
E [ q ( a )]  X  X As in the case with standard Bayesian RL, we can also use our we ighted world models to draw samples from q ( a ) . #3: Policy Priors with Joint Model-Policy Inference. While the model-based inference for pol-icy priors is correct, using importance weights often suffe rs when the proposal distribution is not near the true posterior. In particular, sampling world mode ls and policies X  X oth very high dimen-sional objects X  X rom distributions that ignore large parts o f the evidence means that large numbers of samples may be needed to get accurate estimates. We now des cribe an inference approach that alternates sampling models and policies that both avoids im portance sampling and can be used even in cases where f expectation E [ q ( a )] simply as the average over the action values q ( a | M The inference proceeds in two alternating stages: first, we s ample a new policy given a sampled model. Given a world model, Eq. 5 becomes where making g controllers. We then approximate f of the iPOMDP prior and count data D a scaled by some temperature parameter a . As a is increased, we recover the desired f Next we sample a new world model given the policy. Given a poli cy, Eq. 4 reduces to We apply a Metropolis-Hastings (MH) step to sample new world models, drawing a new model M  X  from p ( D o,r this ratio is likely to be ill-defined; as when sampling polic ies, we apply a tempering scheme in the inference to smooth f smoothed version  X  f for the inference. While applying MH can suffer from the same i ssues as the importance sampling in the model-based approach, Gibbs sampling new policies remo ves one set of proposal distributions from the inference, resulting in better estimates with fewe r samples. 3.1 Priors over State Controller Policies We now turn to the definition of the policy prior p (  X  there are some practical considerations. Mathematically, the policy prior serves as a regularizer to avoid overfitting the expert data, so it should encode a prefe rence toward simple policies. It should also allow computationally tractable sampling from the pos terior p (  X  In discrete domains, one choice for the policy prior (as well as the model prior) is the iPOMDP [12]. To use the iPOMDP as a model prior (its intended use), we treat actions as inputs and observations as outputs. The iPOMDP posits that there are an infinite numbe r of states s but a few popular states are visited most of the time; the beam sampler [17] can efficie ntly draw samples of state transition, observation, and reward models for visited states. Joint in ference over the model parameters T,  X  , R and the state sequence s allows us to infer the number of visited states from the data. treating the observations as inputs and the actions as outpu ts. Now, the iPOMDP posits that there is a state controller with an infinite number of nodes n , but probable polices use only a small subset of the nodes a majority of the time. We perform joint inferenc e over the node transition and policy parameters  X  and  X  as well as the visited nodes n . The  X  X olicy state X  representation learned is not Assuming that the training action sequences are drawn from t he optimal policy, the learner will learn just enough  X  X olicy state X  to control the system optim ally. As in the model prior application, using the iPOMDP as a policy prior biases the agent towards si mpler policies X  X hose that visit fewer nodes X  X ut allows the number of nodes to grow as with new expert experience. 3.2 Consistency and Correctness In all three inference approaches, the sampled models and po licies are an unbiased representation true model and policy posteriors conditioned on their respe ctive data D some mild conditions on the world and policy priors to ensure consistency: since the policy prior and model prior are specified independently, we require that there exist models for which both the policy prior and model prior are non-zero in the limit of data . Formally, we also require that the expert provide optimal trajectories; in practice, we see th at this assumption can be relaxed. Figure 2: Learning curves for the multicolored gridworld (l eft) and snake (right). Error bars are 95% confidence intervals of the mean. On the far right is the snake robot. 3.3 Planning with Distributions over Policies and Models noted in Section 2, computing the Bayes optimal action is typ ically intractable. Following similar work [4, 1, 2, 12], we interpret these samples as beliefs. In t he model-based approaches, we first solve each model (all of which are generally small) using sta ndard POMDP planners. During the node state of the policies (in the policy-based approaches) , is updated after each action-observation pair. Models are also reweighted using standard importance weights so that they continue to be an unbiased approximation of the true belief. Actions are chos en by first selecting, depending on the approach, a model or policy based on their weights, and then p erforming its most preferred action. we found empirically that this simple, fast approach to acti on selection produced nearly identical We first describe a pair of demonstrations that show two impor tant properties of using policy priors: (1) that policy priors can be useful even in the absence of exp ert data and (2) that our approach works even when the expert trajectories are not optimal. We t hen compare policy priors with the basic iPOMDP [12] and finite-state model learner trained wit h EM on several standard problems. In all cases, the tasks were episodic. Since episodes could be o f variable length X  X pecifically, experts generally completed the task in fewer iterations X  X e allowed each approach N = 2500 iterations, or interactions with the world, during each learning trial. Th e agent was provided with an expert tra-jectory with probability . 5 n were provided in the last quarter of the iterations. We ran ea ch approach for 10 learning trials. Models and policies were updated every 100 iterations, and e ach episode was capped at 50 iterations (though it could be shorter, if the task was achieved in fewer iterations). Following each update, we ran 50 test episodes (not included in the agent X  X  experience ) with the new models and policies to 50 samples were collected, 10 iterations apart, after a burn -in of 500 iterations. Sampled models were solved using 25 backups of PBVI [18] with 500 sampled bel iefs. One iteration of bounded policy iteration [19] was performed per sampled model. The fi nite-state learner was trained using finite learners were trained from scratch during each update ; we found empirically that starting from random points made the learner more robust than starting it a t potentially poor local optima. Policy Priors with No Expert Data The combined policy and model prior can be used to encode a prior bias towards models with simpler control policies. T his interpretation of policy priors can be useful even without expert data: the left pane of Fig. 2 sho ws the performance of the policy prior-biased approaches and the standard iPOMDP on a gridwo rld problem in which observations for planning). This domain has 26 states, 4 colors, standard NSEW actions, and an 80% chance of a successful action. The optimal policy for this gridworld w as simple: go east until the agent hits a wall, then go south. However, the varied observations made the iPOMDP infer many underlying states, none of which it could train well, and these models al so confused the policy-inference in Approach 3. Without expert data, Approach 1 cannot do better than iPOMDP. By biasing the agent towards worlds that admit simpler policies, the model-base d inference with policy priors (Approach 2) creates a faster learner.
 Policy Priors with Imperfect Experts While we focused on optimal expert data, in practice pol-ulated snake manipulation problem with a 40-dimensional co ntinuous state space, corresponding to (x,y) positions and velocities of 10 body segments. Actio ns are 9-dimensional continuous vec-tors, corresponding to desired joint angles between segmen ts. The snake is rewarded based on the distance it travels along a twisty linear  X  X aze, X  encouragi ng it to wiggle forward and turn corners. We generated expert data by first deriving 16 motor primitive s for the action space using a cluster-ing technique on a near-optimal trajectory produced by a rap idly-exploring random tree (RRT). A reasonable X  X ut not optimal X  X ontroller was then designed usi ng alternative policy-learning tech-niques on the action space of motor primitives. Trajectorie s from this controller were treated as expert data for our policy prior model. Although the traject ories and primitives are suboptimal, Fig. 2(b) shows that knowledge of feasible solutions boosts performance when using the policy-based technique.
 Tests on Standard Problems We also tested the approaches on ten problems: tiger [20] (2 s tates), network [20] (7 states), shuttle [21] (8 states), an adapted version of gridworld [20] (26 states), an adapted version of follow [2] (26 states) hallway [20] (57 states), beach (100 states), rocksam-problem, the agent needed to track a beach ball on a 2D grid. Th e image-search problem involved identifying a unique pixel in an 8x8 grid with three type of fil ters with varying cost and scales. We compared our inference approaches with two approaches th at did not leverage the expert data: expectation-maximization (EM) used to learn a finite world m odel of the correct size and the infinite POMDP [12], which placed the same nonparametric prior over w orld models as we did.
Figure 3: Performance on several standard problems, with 95 % confidence intervals of the mean. Fig. 3 shows the learning curves for our policy priors approa ches (problems ordered by state space size); the cumulative rewards and final values are shown in Ta ble 1. As expected, approaches that expert data. The policy-based approach is successful even a mong the larger problems. Here, even though the inferred state spaces could grow large, policies remained relatively simple. The opti-mization used in the policy-based approach X  X ecall we use the stochastic search to find a probable policy X  X as also key to producing reasonable policies with li mited computation.
 Table 1: Cumulative and final rewards on several problems. Bo ld values highlight best performers. Several Bayesian approaches have been developed for RL in pa rtially observable domains. These include [7], which uses a set of Gaussian approximations to a llow for analytic value function updates in the POMDP space; [2], which jointly reasons over the space of Dirichlet parameters and states when planning in discrete POMDPs, and [12], which samples mo dels from a nonparametric prior. Both [1, 4] describe how expert data augment learning. The fir st [1] lets the agent to query a state oracle during the learning process. The computational bene fit of a state oracle is that the informa-tion can be used to directly update a prior over models. Howev er, in large or complex domains, the tions. While policy information may be much easier to specify  X  X ncorporating the result of a single as model-spaces grow large. Our policy priors approach uses entire trajectories; by learning policies rather than single actions, we can generalize better and eva luate models more holistically. By work-ing with models and policies, rather than just models as in [4 ], we can also consider larger problems with performance guarantees such as [4], would be an interes ting extension to our approach. We addressed a key gap in the learning-by-demonstration lit erature: learning from both expert and agent data in a partially observable setting. Prior work use d expert data in MDP and imitation-learning cases, but less work exists for the general POMDP ca se. Our Bayesian approach combined priors over the world models and policies, connecting infor mation about world dynamics and expert instead of simply putting a prior over the dynamics, our prio r provides a bias towards models with simple dynamics and simple optimal policies. We show with ou r approach expert data never reduces performance, and our extra bias towards controllability im proves performance even without expert priors to address more problems is an interesting direction for future work. [1] R. Jaulmes, J. Pineau, and D. Precup. Learning in non-sta tionary partially observable Markov [2] Stephane Ross, Brahim Chaib-draa, and Joelle Pineau. Ba yes-adaptive POMDPs. In Neural [3] Stephane Ross, Brahim Chaib-draa, and Joelle Pineau. Ba yesian reinforcement learning in [5] Pieter Abbeel, Morgan Quigley, and Andrew Y. Ng. Using in accurate models in reinforcement [6] Nathan Ratliff, Brian Ziebart, Kevin Peterson, J. Andre w Bagnell, Martial Hebert, Anind K. [7] P. Poupart and N. Vlassis. Model-based Bayesian reinfor cement learning in partially observ-[8] M. Strens. A Bayesian framework for reinforcement learn ing. In ICML , 2000. [9] John Asmuth, Lihong Li, Michael Littman, Ali Nouri, and D avid Wingate. A Bayesian sam-[10] R. Dearden, N. Friedman, and D. Andre. Model based Bayes ian exploration. pages 150 X 159, [11] E. J. Sondik. The Optimial Control of Partially Observable Markov Proces ses . PhD thesis, [12] Finale Doshi-Velez. The infinite partially observable Markov decision process. In Y. Bengio, [13] Matthew J. Beal, Zoubin Ghahramani, and Carl E. Rasmuss en. The infinite hidden Markov [14] Yee Whye Teh, Michael I. Jordan, Matthew J. Beal, and Davi d M. Blei. Hierarchical Dirichlet [15] Tao Wang, Daniel Lizotte, Michael Bowling, and Dale Sch uurmans. Bayesian sparse sampling [16] J. Zico Kolter and Andrew Ng. Near-Bayesian exploratio n in polynomial time. In International [17] J. van Gael, Y. Saatci, Y. W. Teh, and Z. Ghahramani. Beam sampling for the infinite hidden [18] J. Pineau, G. Gordon, and S. Thrun. Point-based value it eration: An anytime algorithm for [19] Pascal Poupart and Craig Boutilier. Bounded finite stat e controllers. In Neural Information [20] M. L. Littman, A. R. Cassandra, and L. P. Kaelbling. Lear ning policies for partially observable [21] Lonnie Chrisman. Reinforcement learning with percept ual aliasing: The perceptual distinc-[22] T. Smith and R. Simmons. Heuristic search value iterati on for POMDPs. In Proc. of UAI 2004 ,
