 Statistical approaches to semantic parsing have re-cently received considerable attention. While some methods focus on predicting a complete formal rep-resentation of meaning (Zettlemoyer and Collins, 2005; Ge and Mooney, 2005; Mooney, 2007), others consider more shallow forms of representation (Car-reras and M ` arquez, 2005; Liang et al., 2009). How-ever, most of this research has concentrated on su-pervised methods requiring large amounts of labeled data. Such annotated resources are scarce, expensive to create and even the largest of them tend to have low coverage (Palmer and Sporleder, 2010), moti-vating the need for unsupervised or semi-supervised techniques.

Conversely, research in the closely related task of relation extraction has focused on unsupervised or minimally supervised methods (see, for example, (Lin and Pantel, 2001; Yates and Etzioni, 2009)). These approaches cluster semantically equivalent verbalizations of relations, often relying on syn-tactic fragments as features for relation extraction and clustering (Lin and Pantel, 2001; Banko et al., 2007). The success of these methods suggests that semantic parsing can also be tackled as clustering of syntactic realizations of predicate-argument rela-tions. While a similar direction has been previously explored in (Swier and Stevenson, 2004; Abend et al., 2009; Lang and Lapata, 2010), the recent work of (Poon and Domingos, 2009) takes it one step further by not only predicting predicate-argument structure of a sentence but also assigning sentence fragments to clusters of semantically similar expres-sions. For example, for a pair of sentences on Fig-ure 1, in addition to inducing predicate-argument structure, they aim to assign expressions  X  X teelers X  and  X  X he Pittsburgh team X  to the same semantic class Steelers , and group expressions  X  X efeated X  and  X  X ecured the victory over X  . Such semantic rep-resentation can be useful for entailment or question answering tasks, as an entailment model can ab-stract away from specifics of syntactic and lexical realization relying instead on the induced semantic representation. For example, the two sentences in Figure 1 have identical semantic representation, and therefore can be hypothesized to be equivalent.
From the statistical modeling point of view, joint learning of predicate-argument structure and dis-covery of semantic clusters of expressions can also be beneficial, because it results in a more compact model of selectional preference, less prone to the data-sparsity problem (Zapirain et al., 2010). In this respect our model is similar to recent LDA-based models of selectional preference (Ritter et al., 2010; S  X  eaghdha, 2010), and can even be regarded as their recursive and non-parametric extension.

In this paper, we adopt the above definition of un-supervised semantic parsing and propose a Bayesian non-parametric approach which uses hierarchical Pitman-Yor (PY) processes (Pitman, 2002) to model statistical dependencies between predicate and ar-gument clusters, as well as distributions over syn-tactic and lexical realizations of each cluster. Our non-parametric model automatically discovers gran-ularity of clustering appropriate for the dataset, un-like the parametric method of (Poon and Domingos, 2009) which have to perform model selection and use heuristics to penalize more complex models of semantics. Additional benefits generally expected from Bayesian modeling include the ability to en-code prior linguistic knowledge in the form of hy-perpriors and the potential for more reliable model-ing of smaller datasets. More detailed discussion of relation between the Markov Logic Network (MLN) approach of (Poon and Domingos, 2009) and our non-parametric method is presented in Section 3.
Hierarchical Pitman-Yor processes (or their spe-cial case, hierarchical Dirichlet processes) have pre-viously been used in NLP, for example, in the con-text of syntactic parsing (Liang et al., 2007; John-son et al., 2007). However, in all these cases the effective size of the state space (i.e., the number of sub-symbols in the infinite PCFG (Liang et al., 2007), or the number of adapted productions in the adaptor grammar (Johnson et al., 2007)) was not very large. In our case, the state space size equals the total number of distinct semantic clusters, and, thus, is expected to be exceedingly large even for moderate datasets: for example, the MLN model in-duces 18,543 distinct clusters from 18,471 sentences of the GENIA corpus (Poon and Domingos, 2009). This suggests that standard inference methods for hi-erarchical PY processes, such as Gibbs sampling, Metropolis-Hastings (MH) sampling with uniform proposals, or the structured mean-field algorithm, are unlikely to result in efficient inference: for ex-ample in standard Gibbs sampling all thousands of alternatives should be considered at each sampling move. Instead, we use a split-merge MH sampling algorithm, which is a standard and efficient infer-ence tool for non-hierarchical PY processes (Jain and Neal, 2000; Dahl, 2003) but has not previously been used in hierarchical setting. We extend the sampler to include composition-decomposition of syntactic fragments in order to cluster fragments of variables size, as in the example Figure 1, and also include the argument role-syntax alignment move which attempts to improve mapping between seman-tic roles and syntactic paths for some fixed predicate.
Evaluating unsupervised models is a challenging task. We evaluate our model both qualitatively, ex-amining the revealed clustering of syntactic struc-tures, and quantitatively, on a question answering task. In both cases, we follow (Poon and Domingos, 2009) in using the corpus of biomedical abstracts. Our model achieves favorable results significantly outperforming the baselines, including state-of-the-art methods for relation extraction, and achieves scores comparable to those of the MLN model.
The rest of the paper is structured as follows. Sec-tion 2 begins with a definition of the semantic pars-ing task. Sections 3 and 4 give background on the MLN model and the Pitman-Yor processes, respec-tively. In Sections 5 and 6, we describe our model and the inference method. Section 7 provides both qualitative and quantitative evaluation. Finally, ad-ditional related work is presented in Section 8. In this section, we briefly define the unsupervised semantic parsing task and underlying aspects and as-sumptions relevant to our model.

Unlike (Poon and Domingos, 2009), we do not use the lambda calculus formalism to define our task but rather treat it as an instance of frame-semantic parsing, or a specific type of semantic role label-ing (Gildea and Jurafsky, 2002). The reason for this is two-fold: first, the frame semantics view is more standard in computational linguistics, sufficient to describe induced semantic representation and conve-nient to relate our method to the previous work. Sec-ond, lambda calculus is a considerably more power-ful formalism than the predicate-argument structure used in frame semantics, normally supporting quan-tification and logical connectors (for example, nega-tion and disjunction), neither of which is modeled by our model or in (Poon and Domingos, 2009).
In frame semantics, the meaning of a predicate is conveyed by a frame , a structure of related con-cepts that describes a situation, its participants and properties (Fillmore et al., 2003). Each frame is characterized by a set of semantic roles (frame el-ements) corresponding to the arguments of the pred-icate. It is evoked by a frame evoking element (a predicate). The same frame can be evoked by differ-ent but semantically similar predicates: for exam-ple, both verbs  X  X uy X  and  X  X urchase X  evoke frame Commerce buy in FrameNet (Fillmore et al., 2003).
The aim of the semantic role labeling task is to identify all of the frames evoked in a sentence and label their semantic role fillers. We extend this task and treat semantic parsing as recursive prediction of predicate-argument structure and clustering of argu-ment fillers. Thus, parsing a sentence into this rep-resentation involves (1) decomposing the sentence into lexical items (one or more words), (2) assigning a cluster label (a semantic frame or a cluster of ar-gument fillers) to every lexical item, and (3) predict-ing argument-predicate relations between the lexical items. This process is illustrated in Figure 1. For the leftmost example, the sentence is decomposed into three lexical items:  X  X avens X  ,  X  X efeated X  and  X  X teelers X  , and they are assigned to clusters Ravens , WinPrize and Steelers , respectively. Then Ravens and Steelers are selected as a Winner and an Opponent in the WinPrize frame. In this work, we define a joint model for the label-ing and argument identification stages. Similarly to core semantic roles in FrameNet, semantic roles are treated as frame-specific in our model, as our model does not try to discover any correspondences be-tween roles in different frames.

As you can see from the above description, frames (which groups predicates with similar meaning such as the WinPrize frame in our example) and clus-ters of argument fillers ( Ravens and Steelers ) are treated in our definition in a similar way. For con-venience, we will refer to both types of clusters as semantic classes . 1
This definition of semantic parsing is closely re-lated to a realistic relation extraction setting, as both clustering of syntactic forms of relations (or extrac-tion patterns) and clustering of argument fillers for these relations is crucial for automatic construction of knowledge bases (Yates and Etzioni, 2009).
In this paper, we make three assumptions. First, we assume that each lexical item corresponds to a subtree of the syntactic dependency graph of the sentence. This assumption is similar to the ad-jacency assumption in (Zettlemoyer and Collins, 2005), though ours may be more appropriate for lan-guages with free or semi-free word order, where syn-tactic structures are inherently non-projective. Sec-ond, we assume that the semantic arguments are lo-cal in the dependency tree; that is, one lexical item can be a semantic argument of another one only if they are connected by an arc in the dependency tree. This is a slight simplification of the semantic role labeling problem but one often made. Thus, the ar-gument identification and labeling stages consist of labeling each syntactic arc with a semantic role la-bel. In comparison, the MLN model does not explic-itly assume contiguity of lexical items and does not make this directionality assumption but their clus-tering algorithm uses initialization and clusterization moves such that the resulting model also obeys both of these constraints. Third, as in (Poon and Domin-gos, 2009), we do not model polysemy as we assume that each syntactic fragment corresponds to a single semantic class. This is not a model assumption and is only used at inference as it reduces mixing time of the Markov chain. It is not likely to be restrictive for the biomedical domain studied in our experiments.
As in some of the recent work on learning se-mantic representations (Eisenstein et al., 2009; Poon and Domingos, 2009), we assume that dependency structures are provided for every sentence. This as-sumption allows us to construct models of seman-tics not Markovian within a sequence of words (see for an example a model described in (Liang et al., 2009)), but rather Markovian within a dependency tree. Though we include generation of the syntac-tic structure in our model, we would not expect that this syntactic component would result in an accurate syntactic model, even if trained in a supervised way, as the chosen independence assumptions are over-simplistic. In this way, we can use a simple gener-ative story and build on top of the recent success in syntactic parsing. The work of (Poon and Domingos, 2009) models joint probability of the dependency tree and its latent semantic representation using Markov Logic Net-works (MLNs) (Richardson and Domingos, 2006), selecting parameters (weights of first-order clauses) to maximize the probability of the observed depen-dency structures. For each sentence, the MLN in-duces a Markov network, an undirected graphical model with nodes corresponding to ground atoms and cliques corresponding to ground clauses.
The MLN is a powerful formalism and allows for modeling complex interaction between features of the input (syntactic trees) and latent output (seman-tic representation), however, unsupervised learn-ing of semantics with general MLNs can be pro-hibitively expensive. The reason for this is that MLNs are undirected models and when learned to maximize likelihood of syntactically annotated sen-tences, they would require marginalization over se-mantic representation but also over the entire space of syntactic structures and lexical units. Given the complexity of the semantic parsing task and the need to tackle large datasets, even approximate methods are likely to be infeasible. In order to overcome this problem, (Poon and Domingos, 2009) group pa-rameters and impose local normalization constraints within each group. Given these normalization con-straints, and additional structural constraints satis-fied by the model, namely that the clauses should be engineered in such a way that they induce tree-structured graphs for every sentence, the parameters can be estimated by a variant of the EM algorithm.
The class of such restricted MLNs is equivalent to the class of directed graphical models over the same set of random variables corresponding to frag-ments of syntactic and semantic structure. Given that the above constraints do not directly fit into the MLN methodology, we believe that it is more nat-ural to regard their model as a directed model with an underlying generative story specifying how the semantic structure is generated and how the syntac-tic parse is drawn for this semantic structure. This view would facilitate understanding what kind of features can easily be integrated into the model, sim-plify application of non-parametric Bayesian tech-niques and expedite the use of inference techniques designed specifically for directed models. Our ap-proach makes one step in this direction by proposing a non-parametric version of such generative model. The central component of our non-parametric Bayesian model are Pitman-Yor (PY) processes, which are a generalization of the Dirichlet processes (DPs) (Ferguson, 1973). We use PY processes to model distributions of semantic classes appearing as an argument of other semantic classes. We also use them to model distributions of syntactic realizations for each semantic class and distributions of syntactic dependency arcs for argument types. In this section we present relevant background on PY processes. For a more detailed consideration we refer the reader to (Teh et al., 2006).
 The Pitman-Yor process over a set S , denoted PY (  X , X ,H ) , is a stochastic process whose samples G 0 constitute probability measures on partitions of S . In practice, we do not need to draw measures, as they can be analytically marginalized out. The conditional distribution of x j +1 given the previous j draws, with G 0 marginalized out, follows (Black-well and MacQueen, 1973) x where  X  1 ,..., X  K are K values assigned to x ,x 2 ,...,x j . The number of times  X  k was as-signed is denoted j k , so that j = P K k =1 j k . The parameter  X  &lt; 1 controls how heavy the tail of the distribution is: when it approaches 1, a new value is assigned to every draw, when  X  = 0 the PY process reduces to DP. The expected value of K scales as O (  X n  X  ) with the number of draws n , while it scales only logarithmically for DP processes. PY processes are expected to be more appropriate for many NLP problems, as they model power-law type distribu-tions common for natural language (Teh, 2006).
Hierarchical Dirichlet Processes (HDP) or hierar-chical PY processes are used if the goal is to draw several related probability measures for the same set S . For example, they can be used to generate transition distributions of a Markov model, HDP-HMM (Teh et al., 2006; Beal et al., 2002). For such a HMM, the top-level state proportions are drawn from the top-level stick breaking construction  X   X  GEM (  X , X  ) , and then the individual transi-tion distributions for every state z = 1 , 2 ,...  X  z are drawn from PY (  X , X  0 , X  0 ) . The parameters  X  0 and  X  0 control how similar the individual transition dis-tributions  X  z are to the top-level state proportions  X  , or, equivalently, how similar the transition distribu-tions are to each other. Our model of semantics associates with each seman-tic class a set of distributions which govern the gen-eration of corresponding syntactic realizations 2 and the selection of semantic classes for its arguments. Each sentence is generated starting from the root of its dependency tree, recursively drawing a seman-tic class, its syntactic realization, arguments and se-mantic classes for the arguments. Below we de-scribe the model by first defining the set of the model parameters and then explaining the generation of in-dividual sentences. The generative story is formally presented in Figure 2.

We associate with each semantic class c , c = 1 , 2 ,..., a distribution of its syntactic realizations  X  . For example, for the frame WinPrize illus-trated in Figure 1 this distribution would concen-trate at syntactic fragments corresponding to lexical items  X  X efeated X  ,  X  X ecured the victory X  and  X  X on X  . The distribution is drawn from DP ( w ( C ) ,H ( C ) ) , where H ( C ) is a base measure over syntactic sub-trees. We use a simple generative process to define the probability of a subtree, the underlying model is similar to the base measures used in the Bayesian tree-substitution grammars (Cohn et al., 2009). We start by generating a word w uniformly from the treebank distribution, then we decide on the num-ber of dependents of w using the geometric distribu-tion Geom ( q ( C ) ) . For every dependent we generate a dependency relation r and a lexical form w 0 from P ( r | w ) P ( w 0 | r ) , where probabilities P are based on add-0.1 smoothed treebank counts. The process is continued recursively. The smaller the parameter q ( C ) , the lower is the probability assigned to larger sub-trees.

Parameters  X  c,t and  X  + c,t , t = 1 ,...,T , de-fine a distribution over vectors ( m 1 ,m 2 ,...,m T ) where m t is the number of times an argument of type t appears for a given semantic frame occur-rence 3 . For the frame WinPrize these parameters would enforce that there exists exactly one Winner and exactly one Opponent for each occurrence of WinPrize . The parameter  X  c,t defines the probabil-ity of having at least one argument of type t . If 0 is drawn from  X  c,t then m t = 0 , otherwise the number of additional arguments of type t ( m t  X  1 ) is drawn from the geometric distribution Geom (  X  + c,t ) . This generative story is flexible enough to accommodate both argument types which appear at most once per semantic class occurrence (e.g., agents), and argu-ment types which frequently appear multiple times per semantic class occurrence (e.g., arguments cor-responding to descriptors).

Parameters  X  c,t , t = 1 ,...,T , define the dis-tributions over syntactic paths for the argument type t . In our example, for argument type Opponent , this distribution would associate most of the probability mass with relations pp over , dobj and pp against . These distributions are drawn from DP ( w ( A ) ,H ( A ) ) . In this paper we only consider paths consisting of a single relation, therefore the base probability distribution H ( A ) is just normalized frequencies of dependency relations in the treebank.
The crucial part of the model are the selection-preference parameters  X  c,t , the distributions of se-mantic classes c 0 for each argument type t of class c . For arguments Winner and Opponent of the frame WinPrize these distributions would assign most of the probability mass to semantic classes de-noting teams or players. Distributions  X  c,t are drawn from a hierarchical PY process: first, top-level pro-portions of classes  X  are drawn from GEM (  X  0 , X  0 ) , and then the individual distributions  X  c,t over c 0 are chosen from PY (  X , X , X  ) .

For each sentence, we first generate a class corre-sponding to the root of the dependency tree from the root-specific distribution of semantic classes  X  root . Then we recursively generate classes for the entire sentence. For a class c , we generate the syntactic realization s and for each of the T types, decide how many arguments of that type to generate (see GenSemClass in Figure 2). Then we generate each of the arguments (see GenArgument ) by first gen-erating a syntactic arc a c,t , choosing a class as its filler c 0 c,t and, finally, recursing. In our model, latent states, modeled with hierarchi-cal PY processes, correspond to distinct semantic classes and, therefore, their number is expected to be very large for any reasonable model of semantics. As a result, many standard inference techniques, such as Gibbs sampling, or the structured mean-field method are unlikely to result in tractable inference. One of the standard and most efficient samplers for non-hierarchical PY processes are split-merge MH samplers (Jain and Neal, 2000; Dahl, 2003). In this section we explain how split-merge samplers can be applied to our model. 6.1 Split and Merge Moves On each move, split-merge samplers decide either to merge two states into one (in our case, merge two semantic classes), or split one state into two. These moves can be computed efficiently for our model of semantics. Note that for any reasonable model of semantics only a small subset of the entire set of se-mantic classes can be used as an argument for some fixed semantic class due to selectional preferences exhibited by predicates. For instance, only teams or players can fill arguments of the frame WinPrize in our running example. As a result, only a small number of terms in the joint distribution has to be evaluated on every move we may consider.

When estimating the model, we start with assign-ing each distinct word (or, more precisely, a tuple of a word X  X  stem and its part-of-speech tag) to an individual semantic class. Then, we would iterate by selecting a random pair of class occurrences, and decide, at random, whether we attempt to perform a split-merge move or a compose-decompose move. 6.2 Compose and Decompose Moves The compose-decompose operations modify syntac-tic fragments assigned to semantic classes, com-posing two neighboring dependency sub-trees or decomposing a dependency sub-tree. If the two randomly-selected syntactic fragments s and s 0 cor-respond to different classes, c and c 0 , we attempt to compose them into  X  s and create a new semantic class  X  c . All occurrences of  X  s are assigned to this new class  X  c . For example, if two randomly-selected oc-currences have syntactic realizations  X  X ecure X  and  X  X ictory X  they can be composed to obtain the syn-tactic fragment  X  X ecure dobj  X  X  X  X  victory X  . This frag-ment will be assigned to a new semantic class which can later be merged with other classes, such as the ones containing syntactic realizations  X  X efeat X  or  X  X in X  .

Conversely, if both randomly-selected syntactic fragments are already composed in the correspond-ing class, we attempt to split them. 6.3 Role-Syntax Alignment Move Merge, compose and decompose moves require re-computation of mapping between argument types (semantic roles) and syntactic fragments. Comput-ing the best statistical mapping is infeasible and proposing a random mapping will result in many attempted moves being rejected. Instead we use a greedy randomized search method called Gibbs scan (Dahl, 2003). Though it is a part of the above 3 moves, this alignment move is also used on its own to induce semantic arguments for classes (frames) with a single syntactic realization.

The Gibbs scan procedure is also used during the split move to select one of the newly introduced classes for each considered syntactic fragment. 6.4 Informed Proposals Since the number of classes is very large, selecting examples at random would result in a relatively low proportion of moves getting accepted, and, conse-quently, in a slow-mixing Markov chain. Instead of selecting both class occurrences uniformly, we se-lect the first occurrence from a uniform distribution and then use a simple but effective proposal distri-bution for selecting the second class occurrence.
Let us denote the class corresponding to the first occurrence as c 1 and its syntactic realization as s 1 with a head word w 1 . We begin by selecting uni-formly randomly whether to attempt a compose-decompose or a split-merge move.

If we chose a compose-decompose move, we look for words (children) which can be attached below the syntactic fragment s 1 . We use the normalized counts of these words conditioned on the parent s 1 to select the second word w 2 . We then select a random occurrence of w 2 ; if it is a part of syntactic realiza-tion of c 1 then a decompose move is attempted. Oth-erwise, we try to compose the corresponding clus-ters together.

If we selected a split-merge move, we use a dis-tribution based on the cosine similarity of lexical contexts of the words. The context is represented as a vector of counts of all pairs of the form (head word, dependency type) and (dependent, depen-dency type). So, instead of selecting a word occur-rence uniformly, each occurrence of every word w 2 is weighted by its similarity to w 1 , where the simi-larity is based on the cosine distance.

As the moves are dependent only on syntactic rep-resentations, all the proposal distributions can be computed once at the initialization stage. 4 We induced a semantic representation over a collec-tion of texts and evaluated it by answering questions about the knowledge contained in the corpus. We used the GENIA corpus (Kim et al., 2003), a dataset of 1999 biomedical abstracts, and a set of questions produced by (Poon and Domingos, 2009). A exam-ple question is shown in Figure 3.

All model hyperpriors were set to maximize the posterior, except for w ( A ) and w ( C ) , which were set to 1 .e  X  10 and 1 .e  X  35 , respectively. Inference was run for around 300,000 sampling iterations until the percentage of accepted split-merge moves became lower than 0.05%.

Let us examine some of the induced semantic classes (Table 1) before turning to the question an-swering task. Almost all of the clustered syntactic realizations have a clear semantic connection. Clus-ter 6, for example, clusters lymphocytes with the ex-ception of thymocyte, a type of cell which gener-ates T cells. Cluster 8 contains verbs roughly corre-sponding to Cause change of position on a scale frame in FrameNet. Verbs in class 9 are used in the context of providing support for a finding or an action, and many of them are listed as evoking elements for the Evidence frame in FrameNet.
Argument types of the induced classes also show a tendency to correspond to semantic roles. For ex-ample, an argument type of class 2 is modeled as a distribution over two argument parts, prep of and prep from . The corresponding arguments define the origin of the cells ( transgenic mouse, smoker, volun-teer, donor, . . . ).

We now turn to the QA task and compare our model ( USP-BAYES ) with the results of baselines considered in (Poon and Domingos, 2009). The first set of baselines looks for answers by attempting to match a verb and its argument in the question with the input text. The first version ( KW ) simply re-turns the rest of the sentence on the other side of the verb, while the second ( KW-SYN ) uses syntactic in-formation to extract the subject or the object of the verb.

Other baselines are based on state-of-the-art re-lation extraction systems. When the extracted rela-tion and one of the arguments match those in a given question, the second argument is returned as an an-swer. The systems include TextRunner ( TR ) (Banko et al., 2007), RESOLVER ( RS ) (Yates and Etzioni, 2009) and DIRT (Lin and Pantel, 2001). The EX-ACT versions of the methods return answers when they match the question argument exactly, and the SUB versions produce answers containing the ques-tion argument as a substring.

Similarly to the MLN system ( USP-MLN ), we generate answers as follows. We use our trained model to parse a question, i.e. recursively decom-pose it into lexical items and assign them to seman-tic classes induced at training. Using this semantic representation, we look for the type of an argument missing in the question, which, if found, is reported as an answer. It is clear that overly coarse clusters of argument fillers or clustering of semantically re-lated but not equivalent relations can hurt precision for this evaluation method.

Each system is evaluated by counting the answers it generates, and computing the accuracy of those answers. 5 Table 2 summarizes the results. First, both USP models significantly outperform all other baselines: even though the accuracy of KW-SYN and TR-EXACT are comparable with our accuracy, the number of correct answers returned by USP-Bayes is 4 and 11 times smaller than those of KW-SYN and TR-EXACT, respectively. While we are not beating the MLN baseline, the difference is not significant. The effective number of questions is rel-atively small (less than 80 different questions are an-swered by any of the models). More than 50% of USP-BAYES mistakes were due to wrong interpre-tation of only 5 different questions. From another point of view, most of the mistakes are explained by overly coarse clustering corresponding to just 3 classes, namely, 30%, 25% and 20% of errors are due to the clusters 6, 8 and 12 (Figure 1), respec-tively. Though all these clusters have clear semantic interpretation (white blood cells, predicates corre-sponding to changes and cykotines associated with cancer progression, respectively), they appear to be too coarse for the QA method we use in our exper-iments. Though it is likely that tuning and differ-ent heuristics may result in better scores, we chose not to perform excessive tuning, as the evaluation dataset is fairly small. There is a growing body of work on statistical learn-ing for different versions of the semantic parsing problem (e.g., (Gildea and Jurafsky, 2002; Zettle-moyer and Collins, 2005; Ge and Mooney, 2005; Mooney, 2007)), however, most of these methods rely on human annotation, or some weaker forms of supervision (Kate and Mooney, 2007; Liang et al., 2009; Titov and Kozhevnikov, 2010; Clarke et al., 2010) and very little research has considered the un-supervised setting.

In addition to the MLN model (Poon and Domin-gos, 2009), another unsupervised method has been proposed in (Goldwasser et al., 2011). In that work, the task is to predict a logical formula, and the only supervision used is a lexicon providing a small num-ber of examples for every logical symbol. A form of self-training is then used to bootstrap the model.
Unsupervised semantic role labeling with a gen-erative model has also been considered (Grenager and Manning, 2006), however, they do not attempt to discover frames and deal only with isolated pred-icates. Another generative model for SRL has been proposed in (Thompson et al., 2003), but the param-eters were estimated from fully annotated data.
The unsupervised setting has also been consid-ering for the related problem of learning narrative schemas (Chambers and Jurafsky, 2009). However, their approach is quite different from our Bayesian model as it relies on similarity functions.

Though in this work we focus solely on the un-supervised setting, there has been some success-ful work on semi-supervised semantic-role label-ing, including the Framenet version of the prob-lem (F  X  urstenau and Lapata, 2009). Their method exploits graph alignments between labeled and un-labeled examples, and, therefore, crucially relies on the availability of labeled examples. In this work, we introduced a non-parametric Bayesian model for the semantic parsing problem based on the hierarchical Pitman-Yor process. The model defines a generative story for recursive gener-ation of lexical items, syntactic and semantic struc-tures. We extend the split-merge MH sampling algo-rithm to include composition-decomposition moves, and exploit the properties of our task to make it effi-cient in the hierarchical setting we consider.
We plan to explore at least two directions in our future work. First, we would like to relax some of unrealistic assumptions made in our model: for ex-ample, proper modeling of alterations requires joint generation of syntactic realizations for predicate-argument relations (Grenager and Manning, 2006; Lang and Lapata, 2010), similarly, proper model-ing of nominalization implies support of arguments not immediately local in the syntactic structure. The second general direction is the use of the unsuper-vised methods we propose to expand the coverage of existing semantic resources, which typically require substantial human effort to produce.
 Acknowledgements
