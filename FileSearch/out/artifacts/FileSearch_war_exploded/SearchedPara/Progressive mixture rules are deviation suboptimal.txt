 work here is related to the second step of this scheme.
 to be suboptimal (see the second part of Theorem 2 and also [1] ). Section 4). We assume that we observe n pairs of input-output denoted Z the risk (or generalization error): ` : Y  X Y  X  R  X  X  +  X  X  .
 2 a  X  y  X  X  . The loss function is These assumptions imply that rules defined below.
 of input-output is where by convention we take  X  probability distribution  X   X  equivalently for any g  X  G ,  X   X  function such that The progressive indirect mixture rule produces the prediction function From the uniform exp-concavity assumption and Jensen X  X  ine quality,  X  h take  X  h predicted output for any x  X  X  is the progressive mixture rule.
 in front of min studies the behaviour of the excess risk, that is the random v ariable R ( X  g )  X  min Theorem 1 Any progressive indirect mixture rule satisfies Let y lower bounds of [12]. Theorem 2 If B , sup which produces a prediction function  X  g erm in argmin Let y probability distribution generating the data for which { -1;+1 } classication with exponential or logit losses).
 Theorem 3 If B , sup have Let y ` such that Proof 1 See Section 5.
 than the expectation convergence rate.
  X  g empirical risk in the star  X  G =  X  algorithms producing a mixture expert inside the convex hul l of the set of experts. 5.1 Proof of the upper bound Let Z 1  X  , we have Using [12, Theorem 3.8] and the exp-concavity assumption, w e have Let  X  g  X  argmin Merging (3), (4), (5) and (6), with probability at least 1  X  2 , we get 5.2 Sketch of the proof of the lower bound the i in { 0 , . . . , n } ,  X  the excursion event.
 tions, for any y  X  X  X  ] a ; +  X  [ , on a neighborhood of a , we have: ` 00 y and  X  y 5.2.1 Probability distribution generating the data and firs t consequences. that the output distribution satisfies for any x  X  X  where y We have Therefore g W The weight given by the Gibbs distribution  X  Equality (9) leads us to consider the event: have The event E variables W and  X  1 with probability (1  X   X  ) / 2 .
 From (9), on the event E This means that  X  (see (8)). 5.2.3 Lower bound of the probability of the excursion event. Let N be a positive integer. Let  X  start with the following lemma for sums of Rademacher variab les (proof omitted). Lemma 1 Let m and t be positive integers. We have Let  X  0 omitted) Lemma 2 For any set A  X  ( integer, we have We may now lower bound the probability of the excursion event E  X  . We still use W we obtain computations, we obtain M  X   X  have to be appropriately chosen.
 To control the probabilities of the r.h.s., we use Stirling X  s formula and get for any s  X  [0; N ] such that N  X  s even, and similarly n  X  M is even. Indeed, from (10), (16) and (17), we obtain lim n  X  +  X  term. We obtain the following lower bound on the excursion pr obability Lemma 3 If  X  = p C 5.2.4 Behavior of the progressive indirect mixture rule on t he excursion event. From now on, we work on the event E by definition of  X  h In particular, for any n large enough, we have ` [ y independent from  X  . From the convexity of the function y 7 X  ` ( y we obtain for some constant C with C &gt; 0 independent from  X  .
 From (19), we obtain with C on the event E with probability at least 1 /n C 4 for some C depending only on the loss function, the symmetry parameter a and the output values y
