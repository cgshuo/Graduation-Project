 multiplier method (ADMM) [ 12 ], RDA-ADMM [ 12 ] and online proximal gradi-direction, and achieve linear convergence rate.
 has H  X  older continuous gradients with degree v : when v = 1 and becomes non-smooth Lipschitz-continues function when v =0. M problems of the following form: Let g ( x )= 1 n n i =1 g i ( x ).
 which cannot deal with training data appearing in succession, such as audio the data probably cannot be loaded into the memory simultaneously in batch objective functions which have H  X  older continuous gradients. setting with respect to x  X  , defined as And if =1 / cannot be loaded into the memory at the same time and show that the SUG will be shown in another paper or a long version of this paper. 1.1 Notations and Lemmas there are no other statements.
 have where h ( y ) is the fixed regularizer.
 Lemma 2. If g satisfy condition ( 1 ) ,assume &gt; 0 and M&gt; ( then for any pair x, y we have If  X  x is the Bregman mapping at x obtained by ( 6 ) , then we have Throughout this work, we denote  X  ( M v , ):=( 1 ) 1  X  v 1+ v M arg min x  X  ( x ) , then we have important difference: we only meet and process one sample (one function) at whose proofs we also draw some ingredients in ours. 2.1 Online Universal Prime Gradient Method (O-UPGM) Lemma 2 shows that the Bregman mapping can move the current point more online algorithms. In UGM, the Bregman mapping is employed to update the x Here we offer the general online universal primal gradient method (O-UPGM) Bregman mapping is also employed to update the x t in each iteration seeing current sample, while unlike UGM that the average of these x solutions after all the iterations.
 Algorithm 1. A generic O-UPGM bound and the convergence rate for UPGM for general convex function with H  X  older continuous gradients.
 we have paper.
 L +1 =  X  ( M v , ). Thus Theorem 1 becomes the sequence { x t } be generated by O-UPGM with fixed steps L Then we have the standard regret bound Remark 1. All of our online algorithms (O-UPGM and the following O-UDGM) need to first assume a fixed accuracy , and then the smaller the , the more accurate the solution. For example, if we assume =1 /T , then we will have a regret bound of O (1) after T iterations. And if =1 / ( T ), then we will fixed T , we can obtain an O (1) bound after T iteration. 2.2 Online Universal Dual Gradient Method (O-UDGM) The original batch UDGM is based on updating a simple model for objective for online or large scale problems.
 Algorithm 2. A generic O-UDGM the sequence { x t } be generated by the general O-UDGM. Then we have We have the following remarks regarding the above result: Remark 2. If we replace Step 2 and 3 in Algorithm 2 with x +1 = arg min x and respectively, then L t +1 =  X  ( M v , ) and Theorem 2 becomes the sequence { x t } be generated by O-UDGM with fixed steps L Then we have the standard regret bound the sequence { x t } be generated by the specific O-UDGM with x and ( 17 ). Then we have deal with situation that the data probably cannot be loaded into the memory increasing. We summarize the SUG method in Algorithm 3 .
 Algorithm 3. SUG: A generic stochastic universal gradient method 3.1 Convergence Analysis of SUG Theorem 3. Suppose g i ( x ) satisfy condition ( 1 ) and M for i =1 , ..., n , d ( x ) satisfy  X  d ( x )  X  X  X  d ( y )  X  convex with  X  h  X  0 , then the SUG iterations satisfy for k where  X  = 1 n M  X  We have the following remarks regarding the above result: satisfy  X  Inequality ( 21 ) gives us a reliable stopping criterion for SUG method. any &gt; 0, Thus we have the following high-probability bound.
 Corollary 4. Suppose the assumptions in Theorem 3 hold. Then for any &gt; 0 and  X   X  (0 , 1) , we have provided that the number of iterations k satisfies and nonsmooth problems, we propose efficient online and stochastic gradient algorithms to optimization the intermediate classes of convex problems with tions C 1 ,v ( R p ).
 In this appendix, we will present some applications of our methods. 1. Lasso Problem The lasso problem is formulated as follows: where a t ,x  X  R p and b t is a scalar.
  X  x 1 , d ( x )= 1 2 x 2 , then g ( x )= a T t x  X  b t 2 are and  X  x = arg min respectively.
 Then we have 2. Steiner Problem In continuous Steiner problem we are given by centers c i the total distance to all other centers. Thus, our problem is as follows: such as new shop opening or new warehouse establishing. Thus our online and stochastic gradient algorithms are needed.
 Let h ( x )=0, d ( x )= 1 2 x 2 , then  X  ( x, y )= 1 2 x  X  x =0and x =0.
 The Bregman mapping associate with 1 m m i =1 x  X  c i and the component function x  X  c i are  X  x = arg min and  X  x = arg min respectively In online UDGM and SUG for Steiner problem, we have where g i ( x i )= x i  X  c i and  X  g i ( x i )= x i  X  c i x +1 = arg min x  X  t +1 ( x ) = arg min x 1 2 x 0
