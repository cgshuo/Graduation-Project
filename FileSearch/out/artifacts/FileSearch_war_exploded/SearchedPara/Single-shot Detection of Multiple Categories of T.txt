
In this paper, we address the problem of detecting multiple topics or categories of text where each text is not assumed to belong to one of a number of mutually exclusive cate-gories. Conventionally, the binary classification approach has been employed, in which whether or not text belongs to a category is judged by the binary classifier for every cat-egory. In this paper, we propose a more sophisticated ap-proach to simultaneously detect multiple categories of text using parametric mixture models (PMMs), newly presented in this paper. PMMs are probabilistic generative models for text that has multiple categories. Our PMMs are es-sentially different from the conventional mixture of multi-nomial distributions in the sense that in the former several basis multinomial parameters are mixed in the parameter space, while in the latter several multinomiai components are mixed. We derive efficient learning algorithms for PMMs within the framework of the maximum a posteriori estimate. 
We also empirically show that our method can outperform the conventional binary approach when applied to multi-topic detection of World Wide Web pages, focusing on those from the "yahoo.corn" domain. As a large quantity of text is being stored in the World 
Wide Web, electric mail, digital libraries, and so on, auto-matic text categorization is becoming a more important and fundamental task in information retrieval and text mining. 
In particular, since a document usually consists of several topics, detecting multi-topic or multi-category of text is of great practical value. Hence, this type of detection prob-lem has become a challenging research theme in the field of machine learning. 
This detection problem is different from the traditional pat-tern classification problems such as character recognition and speech recognition in the sense that each sample is not permission and/or a fee. SIGKDD 022 Edmonton, Alberta, Canada 
Copyright 2002 ACM 1-58113-567-X/02/0007 ...$5.00. word wl occurrence in d n among the vocabulary V :&lt; wl,...,wv &gt;. Here, V is the total number of words in the vocabulary. Thus, in the BOW, each x is a point in V-dimensional Euclidean space and w is assumed to be gener-ated by a multinomial distribution over the words: p(w; 0) tor and the ith element 0~ denotes a probability that wi appears and therefore 01 &gt; 0 and ~/V=i 0i = 1. We define a category vector y'~ = (y~,... ,y~) for d '~, where y~ takes a value of 1(0) when d ~ belongs (does not belong) to the/th category. Here, L is the total number of categories and L categories are assumed to be known. Note that we assume that at least one component in y~ takes a value of 1. Now, let us consider how word-frequency vectors of multi-category text are distributed. As a simple example, sup-pose that we have two multinomial distributions .h.,l, and .A/12 for category C1 and C2, each of which are specified by ~(C1) -~ (0.7,0.1,0.2) and ~a(C2) = (0.1,0.7,0.2), respec-tively. This corresponds to the case where (L, V) = (2, 3). 
Figure l(a) shows samples {x ~} denoted by 'o'('+') which are artificially generated by ~(C1) (~(C2)). The sum of el-ements of each frequency vector are distributed from about 100 to 800. Clearly, a parameter vector ~ lies on a two-dimensional simplex (ie., 01 + 02 + 03 = 1), shown as a large triangle in Figure 1(c). Let C~,2 denote a multi-category class which belongs to both C1 and C2. Then, it may be assumed that text belonging to CL2 has words related to C1 and C2. For example, it seems reasonable to assume that a document having two topics such as "sports" and "music" would consist of a mixture of characteristic words related to both topics, where the mixing weight values between these two categories would not always be equal (i.e., 0.5). According to the above idea, we generated C~,2 samples, denoted by '~' in Figure l(b), by computing the weighted sum of two arbitrarily selected word-frequency vectors each of which belongs to C1 and C2, respectively;. One can see that samples in C1,2 are distributed between C1 and C2 sam-ples. The important point to note is that these samples in Ci,2 can never be generated by a mixture of two multinomial distributions .A41 and .M2. Clearly, the maximum likelihood estimate of ~(Ck) is pro-= arg moaX{~ log P(z"Iy", O) + logp(O)}. (6) 
J(O; V) =/2(0; 7)) + (~ -1) E E'l X g Oa, (8) /2(0; 7)) = E E x~ log E hrOu. (9) Table 1: Summary of Detection Problems Problem Name V L PMC ANW Arts &amp; Humanities 23146 26 44.4% 111.1 Business &amp; Economy 21924 30 42.4% 102.1 Computers &amp; Internet 34096 33 30.2% 128.2 Education 27534 33 33.1% 111.8 Entertainment 32001 21 27.7% 145.7 Health 30605 32 46.8% 108.8 Recreation &amp; Sports 30324 22 30.8% 129.9 Reference 39679 33 14.5% 163.7 Science 37187 40 32.0% 173.3 Social &amp; Science 52350 39 21.6% 154.4 Society &amp; Culture 31802 27 40.4% 176.2 For l = 1,...,L, m = 1,...,L, n 0 h~h,~Ol,~,i At,,n,~(O) = ctz,,nOt~ 
Note that Al,~,i + Am,l,i = 1 and q~,m,i = qm,l,ln hold. Let O denote the estimated parameter. Then, applying 
Bayes' rule, the optimum category vector y* for x* of a new sample is defined as: y* = argmaxy P(ylw*; (~) under a uniform class prior assumption. This maximization prob-lem belongs to the zero-one integer problem (i.e.,, NP-hard problem). Clearly, an exhaustive search is prohibitive for a large L. To solve this problem, we utilize a simple heuristic greedy-search algorithm. That is, first, only one Yh value is set to 1 so that P(ylx*; O) is maximized. Then, for the rest elements, only one Yz~ value is set to 1 with Yh is fixed. This procedure is repeated until P(YlZ*; O) cannot increase. Namely, this algorithm successively determines an element in y so as to improve the posterior probability until its value does not improve. This algorithm is of great efficiency be-cause it requires the calculation of the posterior probability at most L(L + 1)/2 times, while the exhaustive search needs 2 L -1 times. We designed a series of problems for detecting multiple top-ics of World Wide Web pages. We focused on the "ya-hoo.corn" domain to collect Web pages because this do-main is a famous portal site and most related pages linked from this domain are registered by site recommendation and therefore links may be reliable. Yahoo consists of 14 top-level categories and each of these categories is classified into a number of second-level subcategories. We formalized a multi-category detection problem by regarding each list of the second-level subcategories as categories to be detected. 
Let yn = (y~ .... ,y~) and ~'~ = (~,..., ~) be an actual and predicted category vectors for x n. Then, in our case, P and R per each sample can be computed as Using Eq. (17), we can obtain Fn = 2P,~R,~/(P,~ -t-Rn). Finally, we compute F averaged over all N test samples: 
We did not perform any feature transformation such as TFIDF (see, e.g., [14]) because we wanted to purely evaluate the basic performance of each detection method. For every de-tection problem, we evaluated all the methods mentioned above by using five pairs of training and test sample sets. 
In our first set of experiments, the number of training (test) samples was set to 2,000 (3,000). In our second set of exper-iments, we focused on detection performance for test sam-pies with unseen category vectors that did not appear in the training samples. For convenience, hereafter we call this test data uc-test saraples to discriminate between the two kinds of test data. Since ue-test samples always had mul-tiple topics and their frequency vectors were substantially different from those in the training samples, they are avail-able to severely evaluate the generalization ability of each of the detection methods. In our third set of experiments, to evaluate the robustness of the PMM approach, we reduced the number of training samples from 2,000 to 500. 
We compare the mean of F values over five trials on the test (Tables 2 and 4) or the ue-test (Tables 3 and 5) sam-pies. Moreover, the standard deviation of the five trials is also shown in each parenthesis. For the lack of space, we omitted the standard deviations in Tables 3 and 5. PMMs took about five minutes for trainig (2,000 data) and about just one minute for test (3,000 data) on 2.0 Ghz pentium, averaged over 11 problmes. 
By tuning parameters, SVM produced fairly better results than the NB method, although SVM is also binary approach. 
The detection performance by SVM, however, were lower than those by PMMs in almost all problems. These exper-imental results support our claim that since SVM does not consider generative models of multi-category text, it has an 
Table 5: Detection performance for uc-test samples using 500 training samples. Problem NB SVM kNN NN PMM1 PMM2 Arts. 9.7 18.4 24.9 18.7 27.8 28.9 Busi. 30.3 33.7 35.2 33.4 32.6 34.0 Comp. 17.8 15.9 24.9 18.6 30.3 31.1 Edu. 5.7 16.4 20.0 15.5 25.5 26.8 Enter. 11.9 19.8 25.2 21.6 29.6 30.6 Health 22.8 29.8 34.9 29.4 32.7 33.9 Rec. 10.4 15.0 23.7 14.5 28.6 30.1 Ref. 7.3 13.6 22.9 15.5 27.0 27.7 Sci. 5.2 14.6 21.8 11.9 25.1 25.9 Soc.&amp;Sci. 12.9 16.8 22.8 17.7 23.9 26.6 
Soc.&amp;Cul. 12.4 17.2 24.4 18.3 27.2 28.3 
In this paper we have presented a novel approach based on parametric mixture models for multi-topic detection of text, and efficient algorithms for both learning and prediction. 
Clearly, some work rem~ns in order to extend our approach and algorithms, and to ewluate them by using a wider vari-ety of problems. Neverthless, we have taken some important steps along the path, and we are encouraged by our current results on the important problem of detecting multiple top-ics of real World Wide Web pages. 
Recently, sophisticated distributional mixture models for text modeling, pLSI/aspect model [5] and Latent Dirichlet 
Allocation Model [2], have been proposed to represent sev-eral latent subtopics. Combine these models with our models might be interesting. [1] C. Bishop. Neural networks for pattern recognition. 
Clarendon Press, Oxfor, 1996. [2] D. M. Blei, A. Y. Ng, and M. I. Jordan. Latent Dirichlet allocation, to appear in Advances in Neural 
Press. [3] A. P. Dempster, N. M. Laird, and D. B. Rubin. 
Maximum likelihood from incomplete data via the EM 39:1-38. 1977. 
