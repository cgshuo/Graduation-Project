 The web contains lots of interesting factual information ab out entities, such as celebrities, movies or products. This pap er describes a robust bootstrapping approach to corroborate facts and learn more facts simultaneously. This approach starts with retrieving relevant pages from a crawl reposi-tory for each entity in the seed set. In each learning cycle, known facts of an entity are corroborated first in a relevant page to find fact mentions. When fact mentions are found, they are taken as examples for learning new facts from the page via HTML pattern discovery. Extracted new facts are added to the known fact set for the next learning cycle. The bootstrapping process continues until no new facts can be learned. This approach is language-independent. It demon-strated good performance in experiment on country facts. Results of a large scale experiment will also be shown with initial facts imported from wikipedia.
 I.5.1 [ Pattern Recognition ]: models X  statistical and struc-tural ; I.2.6 [ Artificial Intelligence ]: Learning X  knowledge acquisition ; H.3.3 [ Information Systems ]: Information Search and Retrieval Algorithms, Experimentation Web Mining, Information Extraction, Bootstrapping
There are all kinds of factual information on the web. If we can collect them and provide a way to search them, it would be very helpful for answering questions or for improving web search in general. The web also contains lots of redundant information about common entities. For example there are hundreds of thousands of web pages about Angelina Jolie and thousands of them mention that her birthday is June 4th, 1975. If we learn that the Angelina Jolie X  X  birthday is June 4th, 1975 from some source, then all other occurrences of this fact on the web will become training examples for extraction.

One the other hand, if we are not confident about the birthday fact extracted from a particular source, the redun -dancy of information on the web can also help us to verify it. This is called fact corroboration in this paper, which is to find mentions of existing facts on the web. If we are confident that the existing facts are correct, the mentions identified by fact corroboration can be taken as examples for fact extraction. This is the basic idea of this paper.
In this paper, we are interested in facts of common en-tities in the world. Facts are represented in the form of attribute-value pairs. For instance  X  X irthday: June 4, 197 5 X  and  X  X irth Name: Angelina Jolie Voight X  are two facts for the entity  X  X ngelina Jolie X . Facts can exist in free text or semi-structured text. For fact corroboration, both types o f text are considered. But for extraction of new facts, only semi-structured text is considered. In many cases, facts ap -pear in a regular format. One common example is a two-column table where the first column contains the attribute names and the second one contains the values. But it can be any other HTML format which gets rendered into a two-column format. In either case if we can identify the repeated pattern from a page and have some way to verify that they contain facts, it is possible to extract them automatically . The system described in this paper is called GRAZER. It starts with facts imported from one website and takes them as known facts (seed facts). Then it tries to find men-tions of the seed facts on other web sites. This involves retrieving relevant pages for each entity and then corrob-orates facts in them. Once it finds mentions of facts in a page, a high-precision pattern discovery is applied to the surrounding area to find repeated HTML patterns. If a pat-tern can be found and it contains one of the example facts, GRAZER will extract all the facts that match the pattern and add them into the known fact set. The enlarged known fact set will be used in the next learning step. This is a bootstrapping process and the known fact set keeps grow-ing larger. The learning process continues until a stopping criterion is satisfied.
 Two experiments are done to test the GRAZER system. One experiment is on country facts. Evaluation result will be shown and discussed. The other one is a large scale ex-periment. The seed set contains 11.4 million facts imported
Attribute Name ENTITY NAME Angelina Jolie Academy Awards Best Supporting Actress from en.wikipedia.org. By using a crawl repository of the web, GRAZER is able to find 5.1 million mentions of the seed facts and extract 11.0 million new facts.
MapReduce[5] is a programming model for processing large data sets in parallel. It is the computing model the GRAZER system is based on. It automatically divides input data into chunks and distributes them to worker machines. User-defined logic is invoked on each worker machine to process the chunks. Mapreduce handles parallelization and machine failures automatically. A user only needs to specify the de-sired parallelism and resources needed by a task.
A MapReduce is composed of mappers and reducers. Map-pers take the input data as key-value pairs. After process-ing each pair, a mapper outputs new key-value pairs for reducers. The intermediate data is sorted by keys and then shuffled to reducers. Each reducer can process the key-value pairs again and output new values. Intermediate values with the same key are always processed by one reduce step of a reducer. A good MapReduce should have evenly distributed keys so that each mapper or reducer does approximately the same amount of computation. Then, the whole MapReduce task can terminate quickly.

MapReduce can handle terabytes of data with thousands of machines. In this paper, many steps of our algorithm are implemented in the MapReduce framework. 1) Fact: an attribute-value pair with a list of sources (urls ) where the fact is mentioned. Attribute and value are both strings. 2) Entity: formed by a list of facts. At least one of the facts has to be a name fact, in which the attribute name is always  X  X NTITY NAME X  and the value is the entity name. The name fact indicates the name of the entity. There could be multiple name facts for an entity. Table 1 shows an exam-ple entity  X  X ngelina Jolie X  extracted from en.wikipedia.o rg. All the facts have a single source, the wikipedia page. 3) Relevant page: a page that is relevant to an entity. A good relevant page is a page with an entity as its main subject, e.g. a factual page of an entity. 4) Pattern: it refers to any contiguous HTML tag sequence that repeats at least two times in a page. The repetitions have to be contiguous also. Plain text in a pattern is ignored with matching the pattern. 5) Pattern instance: each repetition of a pattern. The tag sequences of pattern instances need not be exactly the same. They can be approximately equal based on some similarity measurement. 6) Textblock: consecutive text in HTML with the same text state. It is basically the text between any two HTML a textblock should be in the same text state, e.g. font size, color, face, etc.
Figure 1 illustrates the approach with input of one entity on a single machine. In practice, the system is parallelized so that it can handle input with millions of entities.
Relevant pages for an entity are retrieved from a crawl repository of the web by matching with the entity name. It would be very inefficient to do this for every entity. In practice relevant pages for all entities are retrieved by go ing through the crawl repository once.

GRAZER uses anchors (incoming links) of a page as clues for the page subject. A relevant page of an entity must have an anchor whose text matches the entity name. Other heuristics are also used to check the content of the page. Relevant pages for all entities are saved in a repository.
Each entity is represented by a list of facts, one of which is a name fact indicating the name of the entity. In the fact corroboration step, GRAZER loops through every entity in the seed set and retrieves its relevant pages from the saved page repository via the entity name. It then tries to corrob-orate facts of the entity inside each page. Mentions of facts are annotated in the page.

The basic idea to corroborate a fact is to look for men-tions of the fact in web pages about the same object. For example if we have a page about  X  X ngelina Jolie X , we can just search for text  X  X irthday X  and  X  X une 4th, 1975 X . Since we are interested in facts in semi-structured text, the at-tribute name and value will appear close to each other when the formatting HTML tags are removed. Therefore when we have a relevant page of the object, matching a fact can be simply finding the attribute name and value appearing close to each other in plain text. Plain text here refers to the text between HTML tags. In this paper, we do not try to tackle the problem where attribute and value are next to each other in the rendered page but not in HTML text, e.g. they may appear in the same column but different rows in a table. Some facts such as birthdays are pretty unique while others are not, such as gender facts. We need to use some technique to ignore the latter.

When fact mentions are found and annotated in a page, they will be used as examples to extract more facts from the page. The learning process starts with pattern discov-ery in the surrounding area of the fact mentions to find any repeated HTML pattern. If a pattern can be found, the page becomes a candidate page for extraction. In extrac-tion, if one of the pattern instance contains a known fact, new facts will be extracted from other pattern instances. Attribute names and values are extracted by aligning each pattern instance with the known example. When new facts are extracted, they are added back into the seed fact set for the next page. So as the learning continues, the seed set grows larger.
Existing facts from any source of text can be used, as long as they are represented by attribute-value pairs and the name of the entity is given. In this paper initial facts are generated by scraping en.wikipedia.org using manually -generated scripts. Wikipedia facts are a good source becaus e it covers many knowledge domains and this algorithm will corroborate and learn facts for each domain. If seed facts ar e from a specific domain, then the corroboration and learning will be confined by the domain.

It is also possible to use automatically generated low-precision facts as seeds. In this case scoring will be nec-essary to evaluate confidence of facts. Scoring can be based on the number of corroborated sources of a fact and the reliability of each source. Good facts should be commonly referenced, thus they should have many sources from high quality sites. Incorrect facts should have very few mention s. Low-precision seed input is not the focus of this paper.
To retrieve relevant pages of objects, one straightforward solution is to query the object name through a search engine. However, this solution does not scale well when the number of objects is large. The solution in GRAZER is to match anchor text of a page with entity names.

For example, if a page contains company profile of  X  X a-hoo Inc. X , most probably it has an anchor pointing to it with text  X  X ahoo Inc. X . To improve the precision of the re-sult, other heuristics can be used also. One heuristic is to require the name to appear in the page title, which is true for many auto-generated factual pages. Another is to re-quire the name to appear exclusively (surrounded by some HTML tags) in the visual part of a page. This helps to ig-nore noisy anchor text. A stricter constraint is to require the name to appear in a salient position in a page, such as in heading. GRAZER does not require the relevant pages to be of 100% precision, so we just require the name to appear on the page.

All the entity names in the seed set are extracted into a list. The page retrieval algorithm is implemented in a MapReduce. The mapper works on every page (with an-chor information) in the crawl repository. Each mapper also loads the list of entity names into memory. For each page, the mapper checks whether any of the anchors match an en-tity name in the list. Matching is on normalized text with punctuation and capitalization removed. When it finds a match, it further verifies that the name appears in the page body. A mapper outputs key-value pairs. If the name can be found in the page, the mapper outputs the entity name as the key and content of the page as the value. If the page does not match any name, the mapper outputs nothing. The reducer simply combines the pages for the same key (entity name) into a list. The MapReduce algorithm is as follows: Mapper: Reducer:
Sometimes different entities share the same name, e.g.  X  X ndependence Day X  can be a movie or a holiday. For these entities, all the relevant pages are mixed together as they are indexed by the same entity name. However, we need to find facts of entities in the pages. Chances are that differ-ent entities have different facts. So we should not find the fact  X  X irector: Roland Emmerich X  on a holiday page or fact  X  X ate: July 4 X  on a movie page. Pages that contain multi-ple entities may cause problem for corroboration and they should be avoid in relevant pages. To address this issue, we require that a relevant page should not be ambiguous: it can not be associated with more than one entities.

The distribution of relevant pages across entities is not uniform. For popular entities there could be hundreds of thousands of them, while for less popular entities there cou ld be just a few. In bootstrapping, processing hundreds of thousand relevant pages could take significantly longer tim e than a small number of pages. In practice, to make sure that the MapReduce in the bootstrapping step runs smoothly, a threshold is used to limit the number relevant pages per entity.
The corroboration algorithm basically searches for values of all facts in an entity in a relevant page. If a value mention can be found, it then searches for the attribute name of the fact before or after the value mention. If both the attribute and value can be found, they become a mention of the fact. There could be mentions of more than one fact in one page.
Corroboration can be wrong with common facts such as gender, which have only two values X  X ale X  X nd X  X emale X . The value of a gender fact also depends on the entity name: if there is another person named  X  X ngelina Jolie X  on the web, it is very likely that her gender is also female.
To avoid this kind of errors, ideally we should estimate the probability of all the attribute-value pairs appearing ran -domly in a relevant page of the entity. But it is difficult to have a good estimate with limited seed data. In practice, we compute the probability p of all the fact values appear-ing randomly in a page given their attribute names. If p is below a threshold, all the fact mentions are deemed as valid mentions. Probability of a value v appearing randomly for an attribute A is, The probability of all value mentions appear randomly in a page given the attribute names is Intuitively, facts with a large set of values (such as birthd ays) are less likely to incur coincidence than facts with a small set of values (such as gender). This simple heuristic works well on the wikipedia seed facts.

Within a mapper, corroboration of facts of the entity is invoked on each relevant page. The pseudo-code of the cor-roboration algorithm is shown below. The input to the func-tion is an entity E with name X and a relevant page P for name X . procedure CorroborateFacts(Entity E , Page P ) for each fact F in entity E , do end for
The attribute and value matching are based on the plain text of page P . All HTML tags are ignored. So a fact can be corroborated either in free text or in structured text . Searching a fact value can be an exact string match or a flexible match of tokens in the value. A few things have been done to improve coverage of corroboration: 1) Values of a fact tend to vary, e.g.  X  X une 4, 1975 X  and  X 4-June-1975 X . The match of fact value in page P is based on normalized text (lowercased and with punctuation re-moved). The flexible match also allows tokens of a value to appear in any order in text. In practice, we compare two values after lexicographical sorting of tokens. In the prev i-ous example, the two values will match since they have the same token sequence  X 1975 4 June X  after sorting. Searching a value in this way can be achieved by constructing a special regular expression.
 This does not solve the value variation problem completely. For instance it will never be able to match  X  X une 4, 1975 X  with  X 06/04/1975 X . However, as we learn new facts from relevant pages, chances are that some of the new facts will be variations of original facts. In our experiments, this ha p-pens frequently for popular entities. The learned facts wil l help to cover more variations of fact values.

Sophisticated matching can be used also for common value types, e.g. dates, numbers, etc. This is a better way to han-dle value variations of known types, but it can not solve the variation problem for all values and it is language-depende nt. 2) Synonyms of attribute names can be used, e.g.  X  X ate of Birth X  and  X  X irthdate X  for attribute  X  X irthday X . In our ex-periment, we do use synonyms generated from other sources. They are helpful but not indispensable. 3) Stopwords between M a and M v are not counted. They include common words like  X  X  X ,  X  X he X ,  X  X f X , etc. This will affect corroboration in free text but not in structured text. 4) For some facts, the attribute name does not appear explicitly in text, e.g. address or phone number facts. In this case matching of attribute name can be optional. If the value is matched in a page P , we will take P as a new source for fact F . However, this kind of fact mention will not become an example for the learning phase. The learning phase requires examples to have attribute name and value.
The corroboration algorithm needs to go over the entities in the seed set and for each entity it retrieves the relevant pages by the entity name. Then it needs to corroborate each fact of the entity in the relevant pages. This is implemented as a MapReduce in which the mapper takes joined inputs of entities and relevant pages, keyed by entity names. Entitie s and relevant docs are serialized to strings. The output key is still the entity name. The output value is the corrobo-rated entity. The reducer in this MapReduce is an identity reducer. It just passes input key-value pairs to output. The mapper task is shown below: Mapper:
Input: (key=entity-name,
Output:(entity-name, new-entity)
Using the annotated examples generated from fact corrob-oration, this step will try to extract new facts appearing in a repeated format around examples in structured text. The example annotation must contain both attribute name and value. As we only try to learn new facts in structured text, corroborated examples in free text should be ignored. The extraction step happens immediately after corroboration.
Pattern discovery is applied to the enclosing node of ex-amples in structured text to find repeated HTML patterns. The algorithm used here is similar to the one described by [1]. In our case, we need to find data records that contain attribute-value pairs. An example pattern is shown in Fig-ure 2.

The algorithm is a top-down process applied to the DOM tree of a page. It tries to find clusters of nodes under the same parent that have similar HTML format (or tags). Simi-larity is based on edit distance between the HTML tag (with attributes) sequences. Plain text is dropped since it is not important in terms of format. Some HTML tags are also ignored as they are not important for formatting. Examples are comment tags, script tags and anchors.

The pseudo-code of the algorithm is given below. This is only for illustration of the algorithm. Details like bounda ry checking and optimization are ignored for simplicity. procedure DiscoverPatterns(HtmlNode Node ) for (int i=0; i &lt; size( Node.Children ); i++) do end for for (i=0; i &lt; size( NodeData ); i++) do end for if not PatternList.empty() then end if for each node N not covered by P max do end for
In the pseudo-code, the function GetHtmlT ags ( node ) re-turns the tag sequence under node without the special tags. The tag sequence includes attributes of a tag. Similarity between two tag sequences s 1 and s 2 is defined as:
Function IsSimilar ( s 1 , s 2 ) returns true if Sim ( s 1 0 . 8. Allowing inexact match in patterns is crucial for ro-bustness because there are many small variations from page to page. For example &lt; br &gt; and &lt; b &gt; tags can be inserted anywhere in text to change the format slightly.

If multiple patterns are found under the same parent, the pattern with the largest span will be preferred. E.g. if the html text is  X  &lt; b &gt;&lt; /b &gt;  X .
 The pattern discovery algorithm is a top-down process. When it finds a pattern, it will not look inside each pattern for sub-patterns. It may find undesirable large patterns and ignore smaller patterns inside. In our algorithm we specify the maximum number of textblocks a pattern could have. If a discovered pattern has too many textblocks, it is discarde d and the discovery process continues down the tree. The upper bound of textblocks is set to 3 for discovering fact patterns. A pattern can contain an arbitrary HTML tag sequence. In our experiment the two most common patterns discovered are: &lt;li&gt;&lt;b&gt;Attribute:&lt;/b&gt;Value&lt;/li&gt; and If a pattern can be matched and it contains an example fact, the extraction process will start to extract facts from it.
If we can find an HTML pattern in which one pattern in-stance contains a example fact, it is likely that other patte rn instances also contain facts about the same entity. Figure 3 illustrates the extraction process. We will refer to the pattern instance that contains the corroborated fact as P I
If the number of textblocks in P I e is more than two, we need to make sure that one of them contains the example attribute and another one contains the example value. If thi s is true, the positions of the two corresponding textblocks in P I e are recorded. For example the attribute name may appear in the first textblock ( attribute index = 1) and the value may appear in the second one ( value index = 2). This should handle the case where attribute name appears after the value. When extracting from other pattern instances, we require that they must have the same number of textblocks as P I e . Then the textblock at position attribute index will be the attribute name and textblock at value index will be the value. New facts will be created from the extracted attribute-value pairs.

When the number of textblocks in a pattern instance is different from P I e , there must be some format variation in fact representation. There could be different scenarios for this. One example is that the value string is too long so that a line break &lt; br &gt; has to be inserted. In this case, spe-cial handling can be designed to extract the fact. However, the format change could also be a result of inconsistent con-tent: e.g. multiple sub-facts appear in one pattern instanc e. In this case, extraction is not trivial. In our experiments, we ignore the pattern instances with different numbers of textblocks for precision reason.

If the number of textblocks in each pattern instance is one, that means the example attribute and value appear in the same piece of text. In this case, we require a delimiter (e.g.  X : X ) to separate them. If such a delimiter exists, we will loo k for it when extracting from other pattern instances. The delimiter will be used to separate the attribute and value. I f no delimiter can be found, no extraction will be attempted.
New facts extracted from a page are added to the seed en-tity. Both the new facts and the original facts will be taken as seeds for corroboration in the following pages. So the see d fact set will grow larger as learning proceeds. This increas es the chances of corroboration and therefore the chance of extracting new facts. Figure 4 shows a diagram of the boot-strapping process.

The learned facts will act as seeds only for the pages after them. To resolve this problem, we go through the relevant pages multiple times. Convergence is an important issue for bootstrapping. The process may never stop if learning fails to converge. The bootstrapping process described here con-verges very well. This is because incorrect facts extracted from one page are unlikely be corroborated in other pages. Therefore the chance of error propagation is small. In prac-tice we can stop the algorithm when no more new facts can be extracted from any page. In experiments, learning often terminates within a few of rounds for an entity with average number of relevant pages.

The algorithm is shown below in pseudo-code. This hap-pens in a mapper with the input key-value pair as an entity E and the set of relevant pages set [ P ] of E . If new facts are extracted from page P , ExtractF acts ( E, P ) returns an augmented entity with the extracted facts appended to it. procedure Bootstrap( E , set [ P ]) terminate = false ; for (round=1; terminate is false; round++) do end for
The result of the GRAZER is an enlarged known facts set. It contains corroborated seed facts, uncorroborated s eed facts, corroborated new facts and uncorroborated new facts . All corroborated facts have at least two sources and uncor-roborated facts have only one source. The number of sources of a fact can be used as a signal to control the trade-off be-tween precision of recall of the facts we select from the out-put. In general facts with more sources are more reliable. To determine the quality of facts, other signals can also be used, such as the reliability and diversity of sources. But this is out of the scope of this paper. In the experiments of this paper, all facts (corroborated or uncorroborated) a re considered in evaluation.
Two experiments has been done. The first experiment is a small scale one on country facts. The seed facts are the capital cities of countries. The second experiment is a large scale one with seed facts imported from table facts on en.wikipedia.org.
In the first experiment, the seed set contains 230 country or territory entities. Each entity has only one fact: the capital city, e.g.  X  X apital:Paris X  for entity X  X rance X . So there 230 seed facts in all. For each entity, 500 relevant pages are kept. The learning result is shown in Table 2.
 Table 2: The Learning Results on Country Entities * Corroborated new facts refer to the extracted facts being corroborated later. Each fact has 14.0 sources in average. Table 3: Seed entity example  X  X ndependence Day X 
The new facts are from 54,010 unique pages. An average of 5.3 facts are extracted from each page. These pages are from 8,558 unique sites. The learning result contains 2,400 unique attribute names and they cover a broad set of facts for each country. Each entity contains 510 facts in average after learning. Some of the facts are specific to a country.
The result shows that corroborated facts have much higher precision than uncorroborated facts. If an application nee ds high quality facts, it could use only the well corroborated facts. If an application is interested in more facts, it coul d use the uncorroborated facts also.
In this experiment, the seed facts are imported from en.wiki -pedia.org, which cover a wide range of entities in the world. Relevant pages are retrieved from the crawl repository whic h contains a few billion documents. The experiment was run in MapReduce and it takes a few hours to finish.
The initial facts were extracted from en.wiki-pedia.org vi a an importer. The importer extracts from the fact tables on wikipedia. Entity names are extracted from the first sentence of the page. It generated 1.75 million entities and 12.6 million facts. Table 3 shows an example of a movie entity  X  X ndependence Day X . Table 4 shows the top 10 types of entities in the seed set.
Relevant pages are retrieved from the crawl repository us-ing the algorithm described in section 3.5. Only HTML pages are considered. All the relevant pages are ranked by pagerank and the top 500 are kept for each entity. As a result, 98M relevant pages are retrieved for 1.59M entities , which cover about 90.8% of the seed entities. The average number of pages per entity is 61.6. The distribution of rel-evant pages per entity (without thresholding) is shown in Figure 5, in which the x-axis represents entities sorted by Table 6: Stats of the Learning Results Per Type on Wikipedia Seeds Geo-location 1,510.4 253.6 181.1 Organization 278.0 55.8 56.1 *Corroborated Facts include corroborated seed facts and the number of relevant pages. The y-axis is the number of relevant pages in logarithmic scale.
The input to the bootstrapping module is the 1.59M en-tities (11.4M facts) and the 98M relevant pages. Entities without relevant pages are ignored. In the first experiment, bootstrapping only goes through the relevant page set once. The first column in Table 5 shows the learning results.
From this result, we can see that about 18.2% of seed entities or 12.2% of seed facts are corroborated. We think most of the facts do not get corroborated for two reasons: 1) there is less redundancy about less popular entities on the web; 2) many facts are specific to wikipedia and we can not find them in other sites by shallow string matching. In this result, the average number of corroborated sources is 3.7 fo r seed facts and 6.7 for learned facts. This may indicate that the learned facts reflect better the common representations of facts on the web.

In another experiment, the bootstrapping process went over the relevant page set twice for each entity. In this case facts extracted from the last page in the first round will be corroborated in the pages before it in the second round. This should increase the coverage of corroboration. The results are shown in the second column of Table 5.

This experiment shows that the number corroborated new facts increased by 32%. They should be from the learned facts in the first round. The number of corroborated seed facts did not change because they were searched for already in the first round. The new facts extracted increased by 23% in the second round because of the new training examples generated from the enlarged seed set. Column 3 in Table 5 shows the result of learning after three rounds. The number of corroborated new facts and learned new facts is increased by 5.8% and 15.6% respectively. For entities with lots of relevant pages, bootstrapping with more rounds would gen-erate more facts. But it also increases the running time on these entities dramatically, which is not efficient for MapRe -duce. Table 6 shows the numbers of corroborated facts and Table 7: Facts learned for the entity  X  X ndependence Day X  from www.infoplease.com.
 Release Date: July 2, 1996 Running time: 135 minutes learned facts by entity type. Entity types are ranked by the number of seed facts they have. This result shows some trends of the web. It contains much redundant information about films, famous people and books. Much of the infor-mation is from online shopping sites or celebrity sites. For these categories we can discover many new sources for the seed facts and extract many new facts. Other types such as animals and buildings are not as popular.

It is difficult to evaluate all the learning results because it involves millions of webpages and facts. In an evaluation of animal facts, the precision of corroborated seed facts an d corroborated new facts are 98.9% and 92.7% respectively. The precision of uncorroborated new facts is 91.9%.
For the seed entity  X  X ndependence Day X  shown in Table 3, GRAZER was able to find 394 total new sources, out of which 382 are correct mentions. It also extracted 226 new facts, out of which 215 are correct. It found 59 new sources for the fact  X  X irector: Roland Emmerich  X  and 18 sources for the learned fact  X  X tarring: Will Smith, Jeff Goldblum, Bill Pullman X .

For example from X  X ttp://www.infoplease.com/movies/ 547 6 X , it corroborated the fact  X  X irector: Roland Emmerich X  and extracted 9 more facts (shown in Table 7). From  X  X ttp:// movies.aol.com/movie/independence-day/2318/main X , it also corroborated the fact  X  X irector: Roland Emmerich X  and ex-tracted 7 more facts (shown in Table 8).
Data extraction from the web has been researched ex-tensively in the last decade. Areas include extraction from free text using NLP techniques and extraction from semi-structured HTML text. The extraction part of this paper is targeted on semi-structured text, while corroborating fac ts considers free text also. Extraction from HTML text often Table 8: Facts learned for the entity  X  X ndependence Day X  from movies.aol.com.
 Theatrical Release Date: 07/03/1996 involves generating wrappers for a particular site. Wrap-pers can be generated either from example pages or by au-tomatic exploration of regular structures in the DOM (Doc-ument Object Model) tree of an HTML page. Supervised approaches use labeled example pages to generate wrappers for a specific format [10, 9, 4]. Wrappers are generated via different learning approaches and they can have good accu-racy in extraction. Wrappers are usually rules in a specific language. A wrapper will encode the hierarchical structure of examples and use it to extract content from other pages. If the format of the target pages changes, the wrappers gener-ated may fail to work. In this paper, we focus on extracting facts in a regular format from web pages. The examples we use are content-based: we use the corroborated content (facts) on a page as keys. Wrappers are learned dynami-cally around these keys. So if the page format changes but the content stays the same, a different wrapper should be learned. As we do not need to produce examples manually for generating wrappers, this approach scales well to a larg e number of websites.

Automatic approaches generate wrappers by exploring reg-ularity in HTML layout of a page. They do not need training examples and they aim to find repeated HTML patterns in a page. Only HTML tags are considered in a pattern. Text are either ignored or converted into a special tag as they do not contribute to the formatting. IEPAD [3] discovers re-peated patterns on a PAT tree from HTML. It converts text into a special tag. Since there is no prior information where the data region is, users need to select from the candidate patterns for extraction. Another approach [1] is based on the observation that data records usually appears under one parent node in DOM and they are contiguous. Similarity of patterns is measured upon edit distance between HTML tag sequences. The pattern match is inexact (using a threshold in edit distance) so that it can handle small format varia-tions that are common in many web pages. It demonstrated good performance on a variety of sites.

All of the automatic approaches aim to find repeated pat-terns that contain the data. But as open-domain approaches, they do not attempt to extract the data and assign attributes to them. In our case, we use a pattern discovery approach similar to [1], but we use the content examples to locate the data region and to label the extracted data.

Bootstrapping has been applied to many areas to extract more data using limited seed data. KnowItAll [6] used boot-strapping to extract patterns and facts simultaneously fro m text. The relevant pages are retrieved from a search engine via a query composed of keywords in a pattern. The initial seed set contains a few hand-generated patterns. Recent work [7] used more expressive patterns and assumed less re-dundancy of the information on the web. It showed good performance in experiments. [11, 8] used bootstrapping in the area of question answering and showed interesting re-sults. But all of these approaches are based on plain text and they involve more NLP techniques. This paper is fo-cused more on structured text in HTML.

In [2] a similar idea is used to bootstrap book title and author pairs from the web. It starts with a few example pairs and learns patterns and new pairs simultaneously. As a result lots of new title and anchor pairs can be learned. The patterns used are any HTML text that contains both title and author. They can be very general and therefore have low precision in extraction. Sophisticated scoring metric s have to be used to make bootstrapping converge. In this paper, we focus on repeated patterns of multiple facts withi n a page, which should be more reliable. In experiments our algorithm is very stable in terms of convergence.
The paper presents an algorithm to find relevant pages about entities and extract new facts from them by corrob-orating existing facts. Mentions of existing facts in a page are used as examples in extraction. Fact corroboration and extraction of each entity is a bootstrapping process. It ter -minates well within a few learning rounds. In an experi-ment with wikipedia facts as seeds, the algorithm was able to corroborate millions of new sources and extract the same magnitude of new facts. The accuracy of the result is sat-isfactory for many applications. The algorithm is based on string match and HTML pattern discovery, so it is language-independent. It has been applied to several other languages and generated similar results as English.

Shallow matching of strings can limit the recall of corrob-oration. For values of a particular type, e.g. date or geo-location, special recognizers could be used to identify the m in text. Then corroboration can be based on the semantic values of facts. This should be a better way to handle value variations. But it is language-dependent. It is worth to try if we want to improve result for a specific language.
In this paper, only repeated html patterns are considered for extraction. Many facts exist in non-repeated formats. Other high-precision wrappers can also be applied to extrac t new facts. This should increase the coverage of extraction. Currently corroborated facts in free text are not considere d as examples for extraction, but they could be training ex-amples for extraction in the NLP domain. [1] R. G. Bing Liu and Y. Zhao. Mining data records in [2] S. Brin. Extracting patterns and relations from the [3] S.-C. L. Chia-Hui Chang. Iepad: Information [4] W. Cohen, M. Hurst, and L. Jensen. A flexible [5] J. Dean and S. Ghemawat. Mapreduce: Simplified [6] O. Etzioni, M. Cafarella, D. Downey, S. Kok, [7] R. Feldman, B. Rosenfeld, S. Soderland, and [8] S. Harabagiu, M. Pasca, and S. Maiorano.
 [9] I. Muslea, S. Minton, and C. A. Knoblock.
 [10] D. W. N. Kushmerick and R. Doorenbos. Wrapper [11] D. Ravichandran and E. Hovy. Learning surface text
