 1. Introduction
Intelligent video surveillance has got more research attention hazards of the world. Multi-pose human or human head detection and tracking from video sequences is a key issue for intelligent video frameworkproposedby Viola and Jones (2001 ), subsequent face detectionresearchhasgrowntremendously(see Kang et al. (2014) ) for the most recent partial literature survey). Yuk et al. (2006) proposed a probabilistic based shape contour matching algorithm to shape is an ellipse or a special shape. Long and Huang (2006) used an adaptive neural network model to detect inter-pixel correlation extract HoG feature, which limits its actual application. Zhu and Ramanan (2012 )proposedauni fi ed model for face detection, pose track multiple humans in high-density crowds in the presence of extreme occlusion. Huang and Long. (2006) integrated neural net-works, and employed the optimal recovery theory for image enhance-ment and surveillance. Gorbenko and Popov (2012 )presentedaface detection system to handle compressed video streams of different and localization. In contrast to other multiview approaches that ing two manually indicated reference points for initialization.
Kalman fi lter ( Hu et al., 2012; Yu et al., 2005 ) has been used solve the problem of target tracking when there is nonlinearity or non-Gaussian process in actual case. Nummiaro et al. (2003) pro-The color model cannot change much in the case of plane rotation that of the background. Vermaak et al. (2003) developed a method for tracking through all targets being jointed in a particle in low accuracy. Schulz et al. (2001) introduced an approach to computation becomes complex with i ncreasing of number of targets. tracking as follows: (1) Occlusion is the most common problem in number. (3) The similarity of targets' appearance makes it hard to distinguish targets of different trajectories.

Aiming at improving some of the limits mentioned above, a novel system has been deviced to detect and track multiple human sub-phases of our developed system are shown in Fig. 1 ,whose main components consist of head detection, head preprocessing, head tracking, online head collecting and training, the identity judgment of head trajectories, and head validation. Comparative of applications.
 described in Section 2 .In Section 3 ,wediscusstheheadtracking results and analysis are shown in Section 5 followed by some conclusions in Section 6 . 2. Head detection
An overview of the head detection module is shown in Fig. 2 . 2.1. Foreground segmentation
For the sake of reducing head detection time and eliminate false head detection from background, multi-scale wavelet trans-formation (WT) across frame difference is developed to segment break a single into similar (low-pass) and discontinuous (high-pass) sub-signals ( Guan, 2010 ), which effectively combines the two basic properties into a single approach.

A morphological dilation and erosion post-processing can be applied to the extracted foreground mask for maintaining pixel segment completeness. Some intermediate results are shown in
Fig. 3 . 2.2. Head training and detection
Based on the foreground segmented, we detect heads only over the ROI, which is speci fi ed by the extracted foreground bounding with a rectangle (seen from Fig. 1 b), instead of the whole image. as follows. We collected 12,000 heads and 22,000 negative images without heads from different videos and cropped them to size of 20 20 pixels. We train these samples using the AdaBoost cascade detector ( Viola and Jones, 2001 ) under the gentle Adaboost mode using 45 1 rotated features. We detect heads based on the trained detector at an interval of one frame. Some examples of detection results are given in Fig. 4 . 3. Head tracking
An overview of the head tracking module is shown in Fig. 5 .The head tracking module employs a multi-target tracking mechanism, and the main functionalities that it implements are the head fi Rect_pre to judge whether Rect_dect and Rect_pre are reasonably close enough. If the judgement is true, we use the two position window to adjust the predicted detection window in this trajectory Rect_pre is used to adjust the predicted detection window for the next frame, with its own counter decreased by 1. If the counter is is larger than its upper limit, it will no longer be increased.
One particle fi lter ( Czyz et al., 2007 )isemployedforeachtracked head, namely, each head trajectory contains a particle fi detected and the initial weight of each particle is set w paper. Ali and Dailey (2012) also proposed to use particle an appearance model for head tracking, however, in this paper, we weight for each propagated particle by an appearance model, and resample the particles as follows, respectively. (a) Predict: A second-order linear autoregressive dynamical motion (b) Update: We get appearance model for each propagated particle (c) Appearance model construction: We select chromaticity H histogram bins. We set width with 8 bins for H and S components, respectively, so the bins of the chromaticity histogram is m  X  8 8. Select V components to establish oriented gradient histogram and the bins are also set as n  X  8. The bins of the normalized integrated histogram is p  X  m n  X  512. De fi ne a normalized integrated histogram as follows: 8 &lt; :  X  2  X  where u  X  0 ; ... ; m 1; v  X  0 ; ... ; n 1; r  X  0 ; ... ; represents the bin of color histogram whose index is u ; q v the bin of oriented-gradient histogram whose index is v ; q the bin of normalized integrated histogram whose index is r .
Before initializing a new trajectory, we compute the histogram h j for the detected head region and save it for comparison with histograms extracted from the subsequent frames. For each subsequent frame, we use Bhattacharyya metric to measure the similarity coef fi cient between them ( Aherne et al., 1998 ) according to the computed results of model histogram h and observed histogram h 0 . The similarity coef fi cient is described as follows: d  X  h ; h 0  X  X  (d) Resample: We resample the particles to avoid degenerate 4. Head validation An overview of the head validation module is shown in Fig. 6 .
Head validation consists of fi ve functions including ellipse detection, false detection suppression, recovery from miss, data details are described as follows. 4.1. Ellipse detection
For a detected head, we fi rst compute n landmark points on the boundary between the head and the background, whose coordi-elliptical equations, such as  X  r r c  X  2 a 2  X  b  X  r r c  X  2  X  a 2  X  t t c  X  2  X  a 2 b 2  X  6  X   X  r r c  X  2  X  k  X  t t c  X  2  X  a 2  X  7  X  where a and b are respectively the lengths of the major and minor and b , r c and t c are the center of the ellipse.

To be used in a real-time system, we require the computation of preferred (or in most cases demanded). To this end, our objective we adopted the absolute error, that is more robust, the computa-tional cost would be of at least one order higher owing to the dif fi culty of having a straightforward closed form solution.
For similar reasons, here we adopt Eq. (7) as our elliptical equation. This is because one of its derivative is pivotal in E  X  r c ; t c ; k ; a  X  X   X  landmark points, in which the centerized coordinates are denoted as ( x 1 , y 1 ), ... ,( x n , y n ) x  X  1 n  X  n x  X  r i x ; y i  X  t i y  X  1 r i r n  X  x  X  r c xy c  X  t c y In this way, our objective function is equivalently turned into E  X  x c ; y c ; k ; a  X  X   X 
The minimization of the above equation is obtained when the fi rst order derivative of each variable (  X   X  E =  X  k  X  X  0,  X   X   X 
E conditions:  X   X  y i y c  X  2  X  X  x i x c  X  2  X  k  X  y i y c  X  2 a 2  X  0  X  11  X   X  a  X  X  x i x c  X  2  X  k  X  y i y c  X  2 a 2  X  0  X  12  X   X   X  x i x c  X  X  X  x i x c  X  2  X  k  X  y i y c  X  2 a 2  X  0  X  13  X   X  k  X  y i y c  X  X  X  x i x c  X  2  X  k  X  y i y c  X  2 a 2  X  0  X  14  X  be eliminated:  X   X  X  x i x c  X  2  X  k  X  y i y c  X  2 a 2  X  0  X  15  X   X   X  y i y c  X  X  X  x i x c  X  2  X  k  X  y i y c  X  2 a 2  X  0  X  16  X 
From Eq. (15) ,wehave  X  x  X  X  x i x c  X  2  X  k  X  y i y c  X  2 a 2  X  0  X  17  X   X  y  X  X  x i x c  X  2  X  k  X  y i y c  X  2 a 2  X  0  X  18  X  we would not have Eq. (15) and thus further derivation are not possible.

So Eqs. (13) and (16) becomes  X  x  X  X  x i x c  X  2  X  k  X  y i y c  X  2 a 2  X  0  X  19  X   X  y  X  X  x i x c  X  2  X  k  X  y i y c  X  2 a 2  X  0  X  20  X 
And we also have  X  y y c  X  X  x i x c  X  2  X  k  X  y i y c  X  2 a 2  X  0  X  21  X 
With the above equation, Eq. (11) can be simpli fi ed to  X  y 2  X  X  x i x c  X  2  X  k  X  y i y c  X  2 a 2  X  0  X  22  X 
By expanding Eq. (19) ,wehave  X  x
Note that in the above equation, we eliminate all  X  n i  X  1 x i  X  0.

Similarly, from Eqs. (20) and (22) we have k  X  k  X  y
A simpli fi ed form of Eq. (15) give that x
Substituting the above equation into Eq. (25) leads to fi nally a is computed by a  X  4.2. False detection suppression
During detecting head as mentioned above, some tracking errors may be exist due to false detection. To overcome this problem, a head count method is developed as follows. When a head is detected, we do not consider it as a true head directly at fi head count hc with trajectory c is set 0.5. For the updated head's position in each subsequent frame, we con fi rm the estimated position through detection. When the trajectory c is con fi when the head count hc fi rst reaches to the value O tr . 4.3. Recovery from miss
We applied trajectories identity judgment, which mainly uses sliding average over a small interval of time to recover the to store the static human head detection result within a small interval of time, and then the maximal count of the detection results are used as the output for the current time stamp. 4.4. Robust supervised distance function learning Get some estimated head positions for each existing trajectory T and detected head positions at the current frame H H current frame). The incidence matrix R is de fi ned as R  X  denoting the incidence relation between the head H i and the trajectory T j .
 Euclidean distance d ji between H i and T j , j  X  1 ; ... ; Find a minimum distance d ji ( min ) and its corresponding T p ji ( j a p ) as 0. There is only one value 1 in each column at the matrix R , while there may be more than one value 1 for each row at the matrix R .

In recent years, supervised distance function learning becomes a hot topic, in which, a rarely studied but commonly encountered sub-problem is about how to set the desired distances between labeled sample points. Here we propose a proximal method for robust distance function learning and graph-based learning to compute the robust distance matrix D , according to the desired distance matrix D 0  X  { d ji }.

Our proposed proximal method consists of two alternative updating: Expectation, and Maximization. As a general model, we want to minimize the following objective function: J  X  U ; D  X  X  f  X  U ; D  X  X jj D D 0 jj 2 F  X  29  X  association parameter U to the robust distance matrix D . jj returns the Frobenius norm of a given matrix.

Generally, if we denote the second term in Eq. (29) as another Newton's method can be written as J  X  U ; D  X  X  min f  X  U ; D  X  X  g  X  D  X  where U t is the current U value at the t th step and  X  is the step size.
 So the U value at the t  X  1th step is
U  X  arg min f  X  U ; D  X  X   X  arg min f  X  U ; D  X  X   X  arg min f  X  U ; D  X  X 
Speci fi cally, in the expectation step, we aim to fi nd a better robust distance matrix D under the current orthogonal projection matrix U , as another word, our goal is to optimize the following objective function: min
TrU T X  X  diag  X  De  X  D  X  X T U  X jj D D 0 jj 2 F  X  32  X  in which, diag ( x ) constructs a matrix using the elements in the vector x as the diagonal elements of the matrix, and all other
Initially, the orthogonal projection matrix U is set to be the identity matrix I .

For expression convenience and the ease of analyzing the problem, we denote B  X  X T UUX T , and Eq. (32) is equivalent to the following: J  X  D  X  X  Tr  X  diag  X  De  X  D  X  B T  X jj D D 0 jj 2 F following two linear algebra properties are employed: Tr diag  X  De  X  B  X  X  De  X  T b e
Db  X  TrDbe T  X  35  X  in B .
 from Eq. (35) : arg min J  X  D  X  X  D 0 1 2 B  X  36  X 
In the maximization step, the robust distance matrix D is and we minimize the following objective function: min
It is not dif fi cult to see that, the closed form solution of the above equation is derived via eigen decomposition of the matrix X in the descending order of the eigen-values.

Owing to the property of minimization of Eqs. (32) and (37) ,we have the following two inequalities: step, respectively. From Eqs. (38) and (39) , we can see that, the proposed Expectation Maximization (EM) algorithm makes the objective function always going down at each step. 4.5. Occlusion handling
Occlusion as a challenging problem is prone to causes detection missing and tracking errors. Especially when two or more heads address this problem, we take the case of overlap between two heads for discussion. Do a judgment about occlusion before the Euclidean distance d ij between each two trajectories separately.
Introduce a threshold d t set 75% of the maximum width of the the two trajectories under occlusion. In this case, we do not use the head detection result to correct the trajectory even if the trajectory is con fi rmed to be associated with the detected head.
The head count is updated as mentioned above. When occlusion is over, the similarity of appearance model (the color and oriented gradient histograms) and the data association strategy are used to decide the correspondence between the detected head and the handing strategy is similar for the case of overlap among more than two heads. 5. Experimental results and analysis
To test performance of the proposed approach, we have done experiments on a large number of real-world video sequences. To evaluate its performance with that of state-of-the-art methods, we performance at the same situations. All the experiments are performed in C  X  X  project with the OpenCV library v2.4.6,
Pentium(R) 2.6 GHz CPU and 3 GB RAM memory. A screenshot of the user interface of our C  X  X  written VS 2012 project is research, and we provide the download of our complete software package at https://sites.google.com/site/facedetectrecog for non-commercial usage. 5.1. Tested videos
The fi rst selected dataset is CAVIAR from  X  http://homepages.inf. tion 384 288 pixels. In the CAVIAR dataset, the color of human's head is similar with that of background. There are inter-object occlusions and frequent interactions between human in the dynamic scenes, which makes it dif fi cult to detect and track multiple heads.
 The second selected dataset is UT-Interaction dataset from
Ryoo and Aggarwal (2010 ), which contains 6 classes of human human interaction, hand shaking, punching, pushing, hugging, kicking and pointing with resolution 780 480 pixels. Several pairs of interacting persons execute activities simultaneously in the scene. Each video has different background, scale and illumi-nation. The motion of head is instantaneous and fast with no rule. Besides, occlusion occurs due to movement of hand.

The third selected dataset is collected by us captured more than 10,000 frames with resolution 400 304 pixels in a pedestrian street. Some pedestrians frequently go in and out the scene and the head is smaller than that of the above datasets. 5.2. Choice of parameters
In order to build a fair comparison with state-of-the-arts, some parameters mentioned above must be kept the same in the whole test.
 We adopt Precision (also called positive predictive value), and Recall with different d t and O tr values for the selected three datasets are shown in Figs. 8  X  10 .

One can note that the Precision increases with the increasing of d , while Recall decreases no matter which dataset is selected from Figs. 8 to 10 . On the contrary, Precision decreases, while Recall increases with increasing of O tr . We choose d t  X  1.0 and O which makes a trade-off between the Precision and Recall for all the three datasets. 5.3. Performance evaluation with state-of-the-arts
To test whether heads can be tracked ef fi ciently or not using oriented gradient one, CAVIAR dataset and the particle fi on the appearance of color histogram proposed in Czyz et al. (2007 ) are selected. There we detect head on the fi rst frame and track head frame by frame. Some tracking results are given in Fig. 11 .
 that of background, while heads are tracked reliably in our proposed method. It highlights that the developed method can be used to track head ef fi ciently by qualitative comparisons.
The above results show qualitative information about the effectiveness of our method in tracking heads. It is necessary to evaluate our whole performance with that of state-of-the-arts including Yuk et al. (2006) and Ali and Dailey (2012 ) quantita-tively. The evaluation method proposed in Ali and Dailey (2012 )is used to test performance as shown in Table 1 . It should be stated detection is con fi rmed continuously, the other is that a true trajectory drifts for some reasons.

To establish a fair comparison, the above selected three video sequences are manually segmented to generate the ground-truth. Some results for the CAVIAR dataset are shown in Table 2 .
From Table 2 , we can fi nd that the proposed method has the highest MT , the lowest values in Frag , and IDS among the three methods. Although the FAT is higher than that of method ( Yuk The performance of our method is the best among the investigated methods as a whole.

In terms of the running time, the average processing time for each frame is about 180 ms for our method, while the methods in processing time among the investigated methods.

Some head tracking examples are given in Fig.12 for the CAVIAR dataset.
 Some results for the UT-Interaction dataset are shown in Table 3 .

From Table 3 , one can note that the proposed method has the highest MT , and the lowest IDS among the three methods. Although the FAT is higher than that of Yuk et al. (2006 ), the ML and Frag in our paper are much lower than that of Yuk et al. (2006 ).The performance of our method is the best among the investigated methods as a whole also.

In the average running time for each frame, our method is about 410 ms, while the method in Ali and Dailey (2012 )is 2400 ms. The reason for Ali and Dailey (2012 ) with great running time is as follows. The method in Ali and Dailey (2012 ) detects heads over the whole scene frame by frame. We detect heads only over the ROI at an interval of one frame. For the method in Yuk et al. (2006 ), the average runtime time for each frame is 380 ms, which is similar to ours. Some head tracking examples for the UT-Interaction dataset are shown in Fig. 13 .
 Some results for our dataset are shown in Table 4 .

From Table 4 , one can fi nd that the proposed method has the investigated methods for oursevles datasets. Some examplary frames are shown in Fig. 14 for the dataset.

In the terms of the running time, the average runtime per frame in the paper is about 125 ms, while the methods in Ali and Dailey (2012 ) is 150 ms, 240 ms. It indicates that our method is more suitable for general real-time sureveillance in an ordinary handware.

To further emphasize the performance of our method in deal-ing with occlusion especially two or more heads overlap each other frequently, another indoor video is selected where several people walk in a line with 27 times cross movements at a short instant (about 20 frames). It is easy to cause the IDS problem in this selected video. Some examples are shown in Fig. 15 . We test it in this video and compare it with that of methods in in Table 5 .
 handling occlusions among all the three compared methods. 6. Conclusions
A novel system is developed to detect and track multiple human heads using ef fi cient human head validation via ellipse detection. A robust supervised distance function learning is employed to set the robust desired distances between labeled sample heads and trajectories. The solution is obtained by a proximal method using a graph-based learning mechanism. The presented system is real-time and can work using any video camera devices even under a cluttered scene without any assump-tions of the video contents and contexts. Experimental results verify the ef fi cacy of our system and its architecture. Acknowledgments This work is partially supported by the National Natural Science Foundation of China (Grant nos. 11176016 and 60872117), and the Specialized Research Fund for the Doctoral Program of Higher Education (Grant no. 20123108110014).
 References
