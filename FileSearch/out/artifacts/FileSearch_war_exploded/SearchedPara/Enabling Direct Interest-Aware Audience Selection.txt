 Advertisers typically have a fairly accurate idea of the inter-ests of their target audience. However, today X  X  online adver-tising systems are unable to leverage this information. The reasons are two-fold. First, there is no agreed upon vocab-ulary of interests for advertisers and advertising systems to communicate. More importantly, advertising systems lack a mechanism for mapping users to the interest vocabulary.
In this paper, we tackle both problems. We present a sys-tem for direct interest-aware audience selection. This system takes the query histories of search engine users as input, ex-tracts their interests, and describes them with interpretable labels. The labels are not drawn from a predefined tax-onomy, but rather dynamically generated from the query histories, and are thus easy for the advertisers to interpret and use. In addition, the system enables seamless addition of interest labels provided by the advertiser.

The proposed system runs at scale on millions of users and hundreds of millions of queries. Our experimental evaluation shows that our approach leads to a significant increase of over 50% in the probability that a user will click on an ad related to a given interest.
 H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval X  Clustering ; H.2.8 [ Database Man-agement ]: Database Applications X  Data mining Algorithms, Experimentation Online Targeted Advertising, Query-Log Clustering  X  W ork done while the author was an intern at Microsoft Research.  X  Work done while the author was at Microsoft Research.
In online advertising, advertisers want to target a specific audience that is more likely to engage with their campaign. Typically, advertisers are capable of describing this audience fairly accurately. However, in today X  X  online advertising sys-tems, they do not have the option to explicitly specify the characteristics of the users that they wish to target, except for broad demographic information. Instead, they bid on query terms, which act as a proxy for the user interests. But queries can be misleading when taken out of context. For example, if a user queries for  X  X elmets X , it is not obvi-ous if she is looking for bike helmets or motorcycle helmets. Ads for both will appear, since this is a term related to both bike and motorcycle helmet companies. If we knew that the user who posed the query has a long-term interest in biking, then it would become clear that the query is more likely to be about bike helmets. The techniques that we present in this paper allow the bike helmet advertiser to directly specify that she prefers users who are interested in biking and the advertising system to identify the users who are interested in biking. Thus, a match between users and advertisers with intersecting interests can be easily made. We call this capa-bility direct interest-aware audience selection .

Enabling advertisers to directly specify user interests is extremely powerful. For instance, part of the appeal of ad-vertising on social media sites such as Facebook is the abil-ity for advertisers to directly select their audience based on their expressed interests, as well as their X  X ikes X  and friends An expensive restaurant can select users who have specified interest in X  X ood and Wine X , while a company that sells out-doors equipment can advertise to users who have declared interest in  X  X amping X . This option is not available when advertising on search engines. The aforementioned restau-rant would have to guess the terms with which a user will express their interest, and bid on these terms. In the case of  X  X ood and Wine X , this translates to a large set of terms related to restaurants, fine dining, wine selection, entertain-ment arrangements, etc. This places a huge burden on the advertisers to come up with the right terms, and they still run the risk of triggering incorrectly, or missing an impor-tant term.

Unlike social media users, search engine users do not ex-plicitly state their interests and preferences. However, they give abundant implicit information about their interests
S ee https://www.facebook.com/business#!/business/ads/. through their actions, and more specifically their queries. Users query about anything and everything that is on their mind. Compiling the long-term ( e.g., year-long) query his-tory of a user reveals a variety of interests: ephemeral inter-ests that correspond to short-term tasks such as buying a new washing machine; routine interests that correspond to queries that enable everyday tasks such as reading a news-paper or checking email; activities that correspond to long-term interests of the user such as diet, sports, gaming, and health care.

In this paper, we consider the problem of extracting user interests from query histories for enabling direct interest-aware audience selection. Given a collection of multiple user histories, we will produce a set of interest labels , and train a model that assigns interest labels to users. The labels are not drawn from a predefined taxonomy, but rather dy-namically generated from the query histories, and thus easy for the advertisers to interpret and use for targeting specific users. A direct interest-aware audience selection capability is important for both sponsored search and display adver-tising. In the former, the targeted user interests would be provided by the advertisers alongside the usual bid terms; in the latter the interests would be specified together with demographic and other behavioral targeting information.
We contrast our approach to other audience selection ap-proaches, where users may be associated to interests, but these interests cannot be directly used by the advertisers. For example in the work of Ahmed et al. [1] interests are represented by topics produced by a topic model, and used for ad click prediction. Advertisers do not have the option to directly specify the interests they want to target. Further-more, the produced interests are not easily interpretable, and thus they cannot be used for direct targeting.
In a nutshell, our approach is as follows: First, we cluster the query history of each individual user in order to identify groups of queries that are about the same interest. For the clustering, we use a measure of semantic similarity between terms that we obtain by exploiting temporal relationships between queries. Figure 1(a) shows an illustrative example of two clusters obtained using our approach from a user his-tory in our data set. Notice, for instance, the queries  X  X ak-land raiders X  and  X  X ouston texans X  being clustered together but having no terms in common.

Given the clustering of the query history, we extract a short description for each cluster consisting of the most pop-ular query terms present in the cluster. Then, we generate the interest labels by finding terms that occur frequently across multiple user histories, and selecting a subset of these terms as our interest vocabulary. Table 1 lists some of the terms that our approach extracted as part of the interest vocabulary. We can see that the interests are represented using commonly used vocabulary found in search queries. In order to map the clusters into this vocabulary, we train a classifier using massive amounts of automatically created training data constructed from the queries in the labeled clusters. The classifier can then be applied to new users to map them to the set of interest labels. For the same user shown in Figure 1(a), Figure 1(b) shows the top four inter-ests inferred by our approach.

Our contributions include the following:
We note that although in this work we consider the prob-lem of audience selection, our work can also be applied to other tasks, such as personalization of search user experi-ence. In this case, the user interests could be used to provide context for a query, and tailor the search engine response to the needs of the specific user.

The rest of the paper is structured as follows. In Section 2, we present related work. In Section 3, we provide an overview of our approach. In Section 4, we present the de-tails of the modeling phase of our approach. In Section 5, we present experimental results. Finally, in Section 6, we make concluding remarks and give directions for future work.
Computational advertising is an emerging research field that considers the application of computational and algo-rithmic techniques to online advertising. We refer the reader to the course notes of Introduction to Computational Adver-tising 2 for a thorough review of the field. Behavioral target-ing, the use of prior user history for improving the effective-ness of an online campaign, is a prominent research topic within this field and has received considerable attention [1, 5, 9, 14, 21, 22]. Pandey et at. [14], Chen et al. [5], and Yan et al. [22] model the user as a bag of events, such as clicks to pages or queries. Jaworska et al. [9] represent users as a vector of categories, by mapping their web page visits to a predefined taxonomy. A machine-learning model is then trained in order to predict whether a user will click on an ad. Tyler et al. [21] model the problem of audience selection as an information retrieval problem, where there is a repository of users, and some users that are known to respond well to a campaign are used as queries over the repository. Users are modeled again as a bag of events, queries, and web page clicks. Recent work [12, 17, 2] has also considered the use of social network information (friendships, email communica-tion) for improving behavioral targeting. The scalability of the behavioral targeting problem has been addressed either with Map-Reduce implementations [5, 14] or sampling [1]. The closest work to our approach is the recent work by Ahmed et al. [1]. They consider the problem of behavioral h ttp://www.stanford.edu/class/msande239/ targeting in display advertising, and use a generative topic model to define interests over histories of multiple users. Then, they use the interests of users who have clicked on an ad as features in a classifier that predicts whether a user will click on the ad. Their technique assumes the existence of previous ad click activity for the given ad. Furthermore, their interest topics are not directly used by the advertis-ers. In contrast, we associate users to a concise set of in-terpretable labels and empower the advertisers to directly specify such interest labels together with their ads.
Query logs are instrumental in the improvement of search engines, and they have been under intense analysis in the past few years. There is a voluminous literature on different aspects of query-log analysis. One important problem is that of breaking up a query history into sessions [7, 10, 11, 13, 16, 18], dealing with the fact that temporal coherence does not necessarily imply thematic coherence. This is a challeng-ing task, since different tasks tend to be interleaved or span long periods of time. Temporal correlation between queries over large number of sessions has been exploited to define semantic correlations between queries [3, 8, 20] for tasks like reformulations or query suggestions. One key differentiation of our work is that we use temporal correlations between queries to define similarity between terms , and then we uti-lize this similarity to cluster queries. In contrast, previous works define relationships directly between queries. Related to our approach is the work by Richardson [19] that discovers long-term relationships between terms in query histories.
In this paper, we address the problem of identifying user interests from search query histories, and describing them using a concise vocabulary. Given a user u and their query history Q u , we want to assign user u a set of labels L u from a larger vocabulary L of possible interests. The choice of the vocabulary L is of paramount importance in enabling the advertisers to select the appropriate audience for their campaign. We propose a methodology for generating the interest vocabulary L , and an algorithm for mapping the user history to this interest vocabulary.

Our approach has two phases: the modeling phase, and the inference phase. In the modeling phase we use query histories from multiple users to generate the vocabulary of interests, and train a machine learning model that maps collections of queries to labels in our vocabulary. In the in-ference phase, we apply our labeling algorithm to user query histories to obtain a labeling of the users in our interest vo-cabulary. We now discuss the details of the two phases. The modeling phase takes as input a collection Q = { Q 1 , ..., Q m } of query histories of m users, and produces a vocabulary of interest labels L = {  X  1 , ...,  X  K } , and a model M that assigns interest labels to collections of queries. This phase can be decomposed into three steps: First, we clus-ter the individual query histories in order to extract themes ; Then we use the produced clusterings to generate the label vocabulary. When available, we also augment the interest vocabulary with advertiser provided interest labels; Finally, we train a machine learning model that maps themes into the label vocabulary. The model itself is trained using data obtained automatically from the clusters. The pipeline of these three steps is shown in Figure 2. Q uery History Clustering: Users express their interests in their search queries, but not necessarily in a temporally and syntactically coherent way. Queries pertaining to wildly different interests are interleaved over short intervals of time, while queries that refer to the same interest recur over the span of weeks, months, or even years, each time slightly mu-tated, using different terms and touching different aspects. As a first step towards extracting interests from query his-tories, we organize queries of individual users into themes : semantically coherent clusters that are potentially related to the same interest. We extract these themes by clustering the user history. For our clustering, we use a similarity measure that captures the semantic correlation between queries, as this is observed over the query histories of millions of users. We discuss our similarity measure, and clustering algorithm in detail in Section 4.1.

Formally, given the history Q u of an individual user u , the query history clustering step produces a clustering C u = { c 1 , ..., c T u } , where each cluster of queries c  X  C u corre-sponds to a semantically coherent theme that is candidate for capturing an interest. Given a collection of user query histories Q = { Q 1 , ..., Q m } the output of the clustering step is a collection of clusterings C = { C 1 , ..., C m } , one for each individual user.
 Label Generation: The clustering step does a good job in bringing together queries that are semantically related, and organizing the user query history into themes. Manual inspection reveals some clearly defined interests: an long-term engagement in online gaming, a prolonged search for a new house, or a long-standing quest for medical advice. These groups of queries make intuitive sense to a human observer, but they are not actionable for advertisers who cannot afford to go through millions of query clusters to find the ones that are of interest to their campaign. We thus need a concise way to describe the interests we observe. Using the themes we have identified, we will extract a set of interest labels, which will define the vocabulary with which advertisers can select the users they are interested in.
The label extraction process identifies key terms that can be used for labeling query clusters of individual users. It then aggregates these terms over the full history collection to identify terms that pertain to a large number of users. These terms will define the interest label vocabulary. We describe the details of this process in Section 4.2. In sum-mary, given the collection of clusterings C output from the history clustering step, the label generation step will produce an interest label vocabulary L = {  X  1 , ...,  X  K } that describes the space of possible interests of a search user.

Optionally, we can also incorporate advertiser-provided interest labels to the label set, thus allowing the adver-tisers to dynamically modify the set of interest labels. In Section 5.3, we show the performance of our system when advertiser-provided interest labels such as  X  X ebkinz X  and  X  X ego X  are included.
 Model training: We observed that the themes extracted from the clustering step align well with intuitively defined interests, and we used this fact to create our interest vocab-ulary L . Given a new user u , with query history Q u , which is clustered into a set of themes C u , we want to be able to map these themes into the space of interest labels L . In this step we train a discriminative model M that performs this task: given a cluster of queries c , it produces a probability distribution P (  X  | c ) over the interest labels  X   X  L .
In order to train the model we need training data: clusters that are labeled within our label vocabulary L . We obtain this data automatically from the clustering collection C we produced in the clustering step.

In summary, in the model training step, we take as input the interest vocabulary L produced in the label generation step, and the clustering collection C produced in the cluster-ing step, and we produce a machine learning model M that maps a cluster of queries c into the label vocabulary L . We describe the details of this step in Section 4.3. At the inference phase, given a user u , with query history Q u , we will assign a set of labels L u  X  L from the vocabulary of labels L produced in the modeling phase. This phase can be decomposed into three steps: the history clustering step, the interest assignment step, and the interest aggregation step. The pipeline for the inference procedure is shown in Figure 3.
 Query History Clustering: In this step we extract the main themes in the query history of the user, using the same clustering techniques that we described in the mod-eling phase, which we describe in detail in Section 4.1. Given the input history Q u we produce a clustering C u = { c 1 where each cluster c  X  C u corresponds to a theme in the user history.
 Interest Assignment: In this step we apply the machine learning model M that we trained in the modeling phase to the clustering C u . For every cluster c  X  C u , we obtain a probability distribution over the label set L . That is, for each label  X   X  L , we obtain the probability P (  X  | c ) that the cluster c should be labeled with label  X  .
 Interest Aggregation: Given the clustering C u and the probability distributions P (  X  | c ) defined for each cluster c , we can aggregate them in a number of ways to obtain the consolidated user interest profile. In our case, we associate the user u with the set of labels L u that have probability P (  X  | c ) above a certain threshold  X  p (set to 0.75 in our exper-iments), for some cluster c  X  C u . While being simple, the experimental results indicate that this aggregation scheme is robust and works well in practice. It is possible to ex-p loit a variety of other signals for weighting the probability scores of the clusters such as the cluster size, the time in-terval over which the queries were asked, etc., but we leave this as potential problem for future investigation.
We will now describe in details the three steps of the mod-eling phase: query history clustering, label generation, and model training.
An interest manifests itself in the query history over mul-tiple queries that cover different aspects of the interest. For example, a user who has an interest in  X  X ootball X  will pose multiple queries about different football teams, NFL, or game schedules. All these queries are thematically related around the interest  X  X ootball X . Identifying such groups of thematically related queries poses the challenge of defining a suitable similarity measure between queries. Typical syn-tactic similarity measures such as Jaccard coefficient or edit distance are not sufficient, since they do not capture the di-versity in the way people query about a topic. Queries like  X  X edding gown X  and  X  X loral arrangements X  are semantically close under a  X  X edding X  interest, yet far apart under any measure of textual similarity. Temporal affinity is a popu-lar method for capturing such semantic correlations: related queries are likely to appear close in the user history. There is considerable amount of work in partitioning a user history into sessions , temporally coherent clusters of queries [4, 7, 10]. However, temporal coherence does not always guaran-tee thematic coherence: thematically diverse interests may be interleaved over a short period of time. Conversely, a thematically coherent interest may span several days, weeks or months in the history of a user. Therefore, sessions do not necessarily capture interests fully and accurately.
Although temporal affinity is not sufficient to capture in-terests in a single user history, when aggregating millions of user histories, it provides a strong signal for semantic sim-ilarity. This idea has been previously explored to extract correlations between queries for tasks such as query sug-gestions and query reformulations [23, 3]. In our approach we will use temporal co-occurrence of queries over multiple user histories in order to define semantic similarity between terms . We will then use this measure of similarity to group queries into clusters, which capture themes in the user his-tory, and are candidates to be mapped to user interests.
Formally, let Q denote a collection of user histories. A user history Q u is a sequence Q u = h ( q 1 , t 1 ) , ..., ( q of query, time-stamp pairs ( q i , t i ), where query q i at time t i . We partition the query history into sessions using the usual 30-minute timeout rule: a timeout of more than 30 minutes between two queries defines the beginning of a new session. The session contains the set of queries between two timeouts. Formally, a session is a maximal subset S  X  Q u of the query history, such that for any two query-timestamp
Let S denote the set of all sessions defined over the history collection Q . Each session S  X  S can be thought of as a bag of words, S = { w 1 , ..., w k } , consisting of all the terms of the queries contained in S . Let P ( w i , w j ) be the number of ses-sions where words w i and w j occur together. Let N be the total number of co-occurrences, that is, N =  X  i  X  j P ( w For a pair of terms ( w i , w j ) we define the co-occurrence fre-quency f ( w i , w j ) as the fraction of co-occurrences that con-tain both terms w i and w j . That is, f ( w i , w j ) = P ( w S imilarly, for a term w i , we define the frequency f ( w term w i to be the fraction of co-occurrences that contain
I n order for two terms to be similar we would like them to have high co-occurrence frequency. However, high co-occurrence frequency by itself is not sufficient to determine similarity. Terms that have high frequency on their own are likely to participate in pairs with high co-occurrence frequency. For example, queries  X  X acebook X  and  X  X oogle X  are prominent in the search logs, and they exhibit high co-occurrence frequency with each other and with other terms, yet this does not imply semantic similarity. To normalize for this effect, we divide the co-occurrence frequency with the probability that the two terms co-occur in the same session by chance. This ratio, or more precisely the log of this ratio, is the point-wise mutual information (PMI) between the two terms, a commonly used similarity measure in text mining and natural language processing [6]. Formally, it is defined as follows: A known drawback of PMI is that it favors rare co-occurrences. Two terms that appear only once in the the query histories in the same session, have the highest possible PMI. This is undesirable, since we would like the pair of terms to have some support in order to be deemed similar. We address this issue by using the discounted PMI (dPMI) measure [15]:
Given the similarity measure between terms, we can ex-tend it to queries, or collection of queries. We represent those as bags of terms. Given two bags of terms X = { x as follows: That is, the similarity of the two bags of terms is the average dPMI similarity of the pairs of terms in the cross-product between the two bags.

Note that a collection of terms may contain the same term multiple times. According to our similarity definition this term will appear multiple times in the sum, and thus con-tribute more to the similarity. This follows the intuition that terms that are frequent should have more impact on the similarity of the collection of queries. It is also possible that X and Y share a term w . In this case we need to define a measure of similarity of a term to itself. We compute this using the definition of PMI, where we define f ( w, w ) = f ( w ). Therefore, we have: T his definition captures nicely the intuition that two col-lections that share a rare term ( e.g.,  X  X quarium X ) are more similar than two collections that share a frequent term ( e .g.,  X  X acebook X ).

Note that our final query similarity measure makes use of both semantic and syntactic similarity between queries. Semantic similarity is explicitly introduced by using the tem-poral co-occurrence of terms, while syntactic similarity is a side benefit of reducing the similarity of queries to compar-isons between terms.

Equipped with a similarity measure between queries and sets of queries, we can now apply any standard clustering al-gorithm for grouping the queries into interests. We opt for hierarchical agglomerative clustering. The algorithm pro-ceeds iteratively, starting with a set of singleton clusters each consisting of a single query, and at each iteration it merges the clusters with the highest similarity. It continues until the similarity of the most similar pair drops below a predefined threshold.

Therefore, given a collection of m user query histories Q = { Q 1 , ..., Q m } , we have obtained a collection of m clusterings C = { C 1 , ..., C m } . A cluster c  X  C u in the clustering of user u is a set of queries that are thematically related, and are candidates for defining an interest of user u . In the following section, we will show how we generate the interest labels from the clusterings C , and then label the clusters in C u with the appropriate interest label.
In the label generation step, we exploit the collection of clusterings C that we obtained in the clustering step to au-tomatically generate a rich set of terms that can serve as the vocabulary L of interest labels.

Given the collection C , we first perform a pruning step to remove clusters with number of distinct queries below a certain threshold (set to 30 in our experiments). Such clusters are too small to capture a prevalent interest of the user. Let C p denote the new clustering collection. For each cluster c  X  C p we produce a set T c containing the top-q most frequent terms, where q = 5 in our experiments (we exclude stop words, etc. ). These top frequently-occurring terms represent a  X  X ynopsis X  of the cluster, capturing the underlying  X  X heme X  of its queries. Let T =  X  T c denote the union of all terms that are among the most frequent terms T of at least one cluster, for at least one user. We keep as our label set L the terms in T that appear in the query history of at least  X  u users, where  X  u = 100 in our experiments. That is, our label set consists of terms that appear as a theme for at least 100 users. We also do minimal human inspection to remove certain labels that do not correspond to interests ( e.g.,  X  X ap X  and  X  X tore X ), and canonicalize certain synonymous terms ( e.g.,  X  X ecipes X  and  X  X ecipe X ).

Table 1 shows a subset of the 300 labels that were gener-ated using this approach. We can see that the labels cover a wide spectrum of interests, and at different levels of gran-ularity. For instance, while a label like  X  X ames X  tends to be more encompassing, a label such as  X  X asketball X  is more spe-cific. There are subtle variants of similar kind of interests, e.g.,  X  X ood X ,  X  X ecipes X ,  X  X aking X  and  X  X ining X , to name a few. There are also time bounded interests such as  X  X edding X  and  X  X oofing X . Furthermore, these interest labels are of high value to the advertisers. To verify this, we performed the following check: we obtained the top 1,000 unigram adver-tising bid terms (in terms of the revenue that they generate in a major sponsored search engine), and we computed the overlap with the list of generated interest labels. It turns out that 12.5% of these 1,000 top advertising terms are actually included in our list.

It is important to note that our approach is not restricted to using these automatically-generated interest labels. Any list of interest labels provided by the advertisers can be used, as long as it contains terms that appear in the query clus-ters. In Section 5.3, we experimentally show the effectiveness of our approach not only in the scenario of automatically-generated labels but also in the case of additional labels provided by the advertisers.
Given the label set L = {  X  1 , ...,  X  K } , in this step we train a machine learning model M which given a cluster c , pro-duces the probability P (  X  | c ) that the cluster c belongs to the interest described by label  X  , for every label  X   X  L . We use a multiclass logistic regression model parameterized by W to compute P (  X  | c ). The parameter matrix W is a collection of weight vectors { w k } , one for each label  X  k  X  L such that each component w jk measures the relative importance of the j th feature for predicting k th label. The multiclass logistic regression learns a mapping from the feature vector of c , denoted by z ( c ) to the label y , using the following softmax logistic function: where b j (1  X  j  X  K ) are bias terms.

In our setting, the feature vector z ( c ) corresponds to a summary constructed from the cluster, c . In particular, we used lexical features consisting of all unigrams (terms) ap-pearing in the queries in the cluster. We then converted them to boolean features representing the presence or ab-sence of these unigrams. We did not consider the frequency of occurrence of these terms as that would require that the clusters are normalized for many factors such as number of queries in the cluster, number of unigrams in the cluster, etc. In fact, our experimental evaluation shows that these simple binary features perform effectively.

The weight vector in Equation 1 is learned from a la-( x j , y j ) corresponds to the feature extracted from a cluster and its corresponding interest label. In particular, W is learned so as to maximize the conditional log-likelihood of the labeled data:
Labeled data for training: Manual construction of a la-beled training set can be too expensive and time-consuming. Our approach enables an effective way to obtain large amounts of automatically labeled training data. For some label  X   X  L , let C  X  denote the set of clusters, from any clustering C such that for all c  X  C  X  ,  X   X  T c , that is, the label  X  is one of the top terms in the cluster c . We treat the clusters in C as positive examples for the label  X  , and we use this data to train our model. In order for our approach to be successful, we need clusters in C  X  to be homogenous, with highly fre-quent terms being semantically related. We manually evalu-ated the clusterings for their homogeneity and we confirmed that this is indeed the case. The homogeneity of the clus-t ers follows from the way we have constructed our similarity function to capture the semantic similarity of terms.
Note that our approach generalizes naturally to the case that the set of labels we want to train against is provided from some external source ( e.g., provided by the advertis-ers). Let L  X  denote this provided set of labels. For each label  X   X  L  X  we can use the process described above to obtain the set C  X   X  of clusters that have  X   X  in their top terms. Then we can train our model against these externally provided labels. We experiment with this case in Section 5.3.
We now report the results of a large-scale, end-to-end eval-uation that we performed on our system. In Section 5.1, we present the experimental setup. In Section 5.2, we describe our methodology and metrics for evaluating on advertising data. Finally, in Section 5.3, we present our key findings.
We now provide the details of the data and parameters used for the different components of our system. The clus-tering algorithm uses the similarity measure defined in Sec-tion 4.1, which relies on a discounted PMI computation for unigram pairs over user sessions. We computed discounted PMI over the sessions of 2.2 million users over 16 months of queries from the query log of the Bing search engine. We used the standard definition of session, where a session con-sists of all consecutive queries until there is a period of 30 minutes inactivity [4].

To create the set of interest labels, we ran the clustering algorithm on 580,000 users. To ensure good quality key-words, we constrained ourselves to clusters with at least 30 queries. As a result, we obtained 1,042,729 clusters mapped to their top-5 keywords. We aggregated this mapping by counting for each keyword the number of users who have at least one cluster that is mapped to the keyword. We then produced a list of all keywords that are associated to at least 100 users; after removing stop-words, plurals etc, this resulted in 5,500 keywords. The size of this list was manageable enough to be processed editorially in order to detect highly frequent terms related to interests. After ed-itorial processing, we obtained a list of 332 interest labels. To verify that these interest labels are of high value to the advertisers, we performed the following check: we obtained the top 1,000 unigram advertising bid terms in terms of the revenue that they generate in a major sponsored search en-gine, and we computed the overlap with our list of interest labels. It turns out that 12.5% of these 1,000 top advertising terms are actually captured by our list. This indicates that the interest labels are, indeed, monetizable.

We note that the creation of a meaningful taxonomy, or class system, is an extremely hard undertaking that merits its own scientific field. Complete automation is nearly im-possible, and probably not desirable, since introducing some human intuition can improve the quality significantly. Our approach simplifies the vocabulary creation process signifi-cantly, by offering a manageable set of labels for the data analyst to process. Processing is also simplified, consisting mostly of filtering out uninteresting or very specific terms. This is a considerably easier task compared to deriving such class labels from scratch. More importantly, the produced labels capture the underlying trends in the data, they can be updated dynamically as query histories get updated, and they come together with training data.

The Model Training component used the clustering of 120,000 users to create 116,839 (user,cluster,label) training examples. The classifier uses lexical features: we used a feature vector consisting of 56,983 binary features (these features correspond to unigrams that appeared in at least 20 queries among the 120,000 users). A logistic regression model was learned using these training examples.

At inference time, the Interest Aggregation component mapped users to the labels for which they had at least one cluster with classification score above a threshold. Unless otherwise stated, we used threshold  X  p = 0 . 75. As we explain in the next section, the system was tested on 150,000 users using 16 month of query activity. Running the system at this scale was enabled by our implementation on large-scale Map-Reduce distributed data processing system.
The main goal of this evaluation is to study the effective-ness of our interest-aware audience selection system. Our hypothesis is that users who match advertiser-specified in-terests are (on average) more likely to click on ads related to the interest than the users currently selected via keyword match.

User click probability. Let A denote a set of ads, e.g., the set of ads in a specific advertising campaign, or all the ads related to a specific interest. Let U denote the set of users that are candidates to be shown the ads in A , and let U
A  X  U denote the set of users that are actually impressed with at least one ad from the set A . Also let C A  X  U A denote the subset of these users that clicked on at least one of the ads that they were impressed. We define the user click probability of the set A with respect to the user set U as follows: T hat is, P U ( A ) is the fraction of users being impressed with ads from A , that actually clicked on at least one ad from the set A .

This metric is reminiscent of the standard notion of click-through rate (CTR) 3 which, like user click probability, is also a ratio between clicks and impressions. However, CTR is the probability that, given an impression of an ad, the ad will be clicked, while user click probability is the prob-ability that given a user who is impressed an ad, the user will at some point click on the ad. Although related, the two metrics capture different information. We believe that user click probability metric fits nicely with the goal of au-dience selection, which is to select users to whom to impress advertisements.

Now, let U  X  denote a set of users tagged with an interest label  X  . Also, let A  X  denote a set of ads that are related to the interest  X  (we discuss later how we obtain this set). The user click probability P U  X  ( A  X  ) is the probability that a user who is assigned the interest label  X  will click on an ad related to the interest  X  . Therefore, mathematically, our hypothesis is with interest  X  are more likely to click on an ad related to  X  , than users drawn from the general population of all users U who are impressed with the ad. h ttp://www.stanford.edu/class/msande239/
To test our hypothesis, we used the sponsored search logs o f the Bing search engine for a 2-month period, which does not overlap with the time period used to compute user in-terests 4 . For each label  X  in the vocabulary L , we applied our algorithm to the set of users U in the 2-month query log, and we generated a subset of users U  X   X  U that were assigned this label. Next, for each label  X  we need to ob-tain a set of ads A  X  that are related to interest  X  . It is not immediate how to obtain such a set, since currently, adver-tisers do not provide interest labels, and thus there exists no test set of ads labeled with interest labels that we could use for the evaluation. We tackle this problem by making the following approximation: we use the readily available, existing bid terms from advertisers as a proxy for interest labels. More specifically, let a be an ad, and let B a denote the set of all bid keywords associated with this ad. We say that ad a is labeled with interest label  X  if  X   X  B a . We define the set A  X  as the set of ads that are labeled with the label  X  .

Given the set of ads A  X  , we use U A  X   X  to denote the subset of users from U  X  that are impressed an ad in A  X  . Ideally, we would like to have control over the set U A  X   X  in our ex-periments. However, since we do not have such control, beled with  X  , that are impressed an ad from A  X  in the ex-isting logs. The set C A  X   X  of users tagged with the interest label  X  that clicked on the ads in A  X  is defined similarly. The user click probability of A  X  with respect to set U is P respect to U  X  is P U  X  ( A  X  ) = | C A  X   X  | / | U A  X   X  To make it more concrete, consider the following example. We have a set of four users U = { Alice, Bob, Cathy, David } , and a set of three ads A = { a 1 , a 2 , a 3 } . Advertisement a tagged with bid keywords B 1 = {  X  X asino X ,  X  X otel X  } , a 2 keywords B 2 = {  X  X egas X ,  X  X asino X  } , and a 3 with keywords B 3 = {  X  X egas X ,  X  X otel X  } . In our logs, Alice is shown ads { a 1 , a 2 , a 3 } , Bob is shown ads { a 2 , a 3 } , Cathy is shown ads { a 1 , a 3 } and David is shown ad { a 3 } . Alice clicked on ads a and a 2 , Bob clicked on a 3 , and Cathy and David did not click on any of the ads.

Suppose now that our interest label  X  is  X  X asino X , and that our algorithm tagged users U  X  = { Alice, Bob, David } with the interest label  X  . We have that A  X  = { a 1 , a 2 U
A  X  = { Alice, Bob, Cathy } is the set of users that were im-pressed with an ad in A  X  . The set of users tagged with the label  X  that are also impressed with an ad in A  X  is U  X  = { Alice, Bob } . Only Alice clicked on an ad related to the interest  X  , therefore, C A  X  = C A  X   X  = { Alice } . The user click probability with respect to the set of all users U is P
U ( A  X  ) = | C A  X  | / | U A  X  | = 1 / 3 while the user click probabil-ity with respect to the set of users U  X  that we tagged with our example, among the users impressed with an ad in A  X  there is a 33% probability for one of them to click on an ad, while this probability increases to 50% when a user is drawn from the set of users tagged with the interest label  X  .
User coverage. We also consider a measure of coverage which is defined as the fraction of users who are assigned an interest profile (set of labels) of a minimum given size. The goal is to show that a large fraction of users get assigned
W e further restricted the users to  X  X igh engagement X  users, as determined by the rules of the search engine company. Figure 4: Scatter plot of User Click Probability of b aseline vs. our system. interest labels. Let U be the universe of users considered in an experiment (in our case, the users in the 2-month snap-shot of the sponsored search logs). Let V k be the users in U that are assigned a profile consisting of at least k labels by the interest-aware system. Then, the measure user coverage for profile of size k is defined as follows:
R ecall that in our system, we only keep the labels whose classification score for some cluster is above a classification threshold. Thus, there is a clear tradeoff between user cov-erage and user click probability. A high user coverage means that more users will be assigned interest labels, but this may come at the expense of retrieving  X  X oor-quality X  users who might not click on ads. We now discuss the main results from our experiments.
Effect on user click probability. We first compare the user click probability of our system against that of the baseline, for every label  X  for which there were at least 30 impressed ads in the 2 month time period that we considered for the advertising data (126 labels in total). In Figure 4, we present a scatter plot of the user click probability of the two alternatives we consider for every label  X  . The x -axis corre-sponds to the user click probability P U ( A  X  ) of the baseline system, and the y -axis corresponds to the user click proba-bility P U  X  ( A  X  ) of our system. We can observe that the points for most labels lie above the diagonal (more precisely, for 97 out of 120 labels). This means that our system outperforms the baseline for 81% of the labels.

To get an understanding of the performance for individ-ual interest labels, we indicate on Figure 4 some labels for which we do particularly well, and some labels for which the baseline outperforms our approach. For example, we have large gains for labels such as  X  X aseball X  and  X  X ewelry X  which represent permanent (or long-term) interests of users, and for long term tasks such as  X  X edding X . Arguably, consid-ering entire histories to make a determination of the user interests helps for these long-term interests. Some of the worst performing labels for our system are broad, high-level interests which do not necessarily lead to higher user click probabilities. Examples include terms such as  X  X cience X  and  X  X aw X . Figure 5: User Coverage for our interest-aware sys-t em.

To get an aggregated view of the results, we computed the average user click probability over all labels l . For our sys-tem, the average user click probability is 0.131; for the base-line, it is 0.087. This represents a 50.5% increase over the baseline. We used a one-tailed t-test, and verified that this difference is statistically significant with 95% confidence, thus establishing our hypothesis that on average it is more likely for a user who is tagged with an interest label to click on an ad related to that interest than a user drawn from the general population.

Our results demonstrate that given an interest of the ad-vertiser, our technique produces an audience that has in-creased probability of clicking to an ad related to that in-terest. Of course, this audience must be of significant size. That is, U A  X   X  , the labeled users to whom the ads related to  X  are impressed, should be a sizeable fraction of U A  X  all the users to whom an ad related to  X  were impressed. In our experiments U A  X   X  is on average 10% of U A  X  , indicating that we capture a sizeable fraction of impressed users.
Effect on user coverage. Since, in our system, a profile consists of all labels above a classification threshold, users may get profiles of different sizes (in terms of the number of labels), or even no profile at all. We measured user coverage for different minimum profile sizes for our interest-aware sys-tem. In Figure 5, we give size k of the profile in terms of number of labels on the x -axis; and the user coverage for profiles that have at least k labels, UserCoverage k , on the y -axis. We can observe that the coverage is as high as 0.93 and 0.78 for histories with at least 1 and 2 labels respec-tively. It remains at reasonable levels even for histories of size at least five (0.27). This implies that the number of users retrieved by our system is significant for the different labels.

Sensitivity to classification threshold. We also per-formed a sensitivity analysis of the classification score thresh-old  X  p . In particular, we considered different instantiations of our system, where we varied the classification threshold  X  and measured the corresponding average user click prob-ability. The results are shown in Figure 6. We can observe that the average user click probability is not overly sensitive to the classification threshold: for classification threshold Figure 6: Average User Click Probability for differ-e nt classification score thresholds.
 Table 2: Performance on advertiser-provided inter-e sts. The first column shows the User Click Proba-bility of the Baseline, while the second column the User Click Probability of our method.  X  = 0 . 95 it is 0.158, decreasing gently to 0.130 for thresh-old  X  p = 0 . 6.

Enabling advertisers to extend the vocabulary of interests. So far, we have presented results on a set of inter-est labels derived from the clusters themselves (Label Gen-eration component). However, our system is by no means restricted to that list: it enables advertisers to flexibly pro-vide new interest labels and add them to the interest label set. To show that this is possible, we performed an exper-iment where we added some highly monetizable interests to the list of labels, and then performed the same evalu-ation as before but with the extended list. In particular, we added three popular tourist destinations (Vegas, Disney, and Hawaii), and two popular toys (Lego and Webkinz). We show the results on Table 2. We can observe that our system outperforms the baseline on the five interests. This means that click-through rate increases for interests that are flexibly added by the advertisers to the interest vocabulary.
In this paper, we presented a system for direct interest-aware audience selection. Our system takes the query histo-ries of search engine users as input, extracts their interests, and describes them with interpretable labels that enable ad-vertisers to easily target users.

We showed the effectiveness of our approach through a large-scale evaluation using advertising data. The results indicate that our system associates users to interest labels that are highly useful for advertisers to better target rele-vant users. Our system can lead to an increase in user click probability of over 50% compared to the baseline system.
We are planning to extend our work in multiple direc-tions. Extensions include employing additional signals such as clicked URLs and timestamps, and studying the effect of interests on conversion rates, in addition to clicks. One p articularly interesting direction involves building upon the output of the clustering algorithm to infer a  X  X ime signature X  for different types of interests, based on the distribution over time of the queries in an interest cluster. This can enable us to better understand the nature of users interests. For instance, one may expect that the time signature of a time-bounded task such as planning a wedding would be different from that of a more permanent interest such as gardening. We would like to thank Patrick Pantel, Michael Gamon, and Maria Christoforaki for their insightful feedback. We would also like to thank Josh Bonis and Gil Lapid Shafriri for providing support and access to multiple resources needed for this project. [1] A. Ahmed, Y. Low, M. Aly, V. Josifovski, and A. J. [2] E. Bakshy, D. Eckles, R. Yan, and I. Rosenn. Social [3] P. Boldi, F. Bonchi, C. Castillo, D. Donato, A. Gionis, [4] L. Catledge and J. Pitkow. Characterizing browsing [5] Y. Chen, D. Pavlov, and J. F. Canny. Large-scale [6] K. Church and P. Hanks. Word association norms, [7] D. He, A. G  X  oker, and D. J. Harper. Combining [8] H. Hwang, H. W. Lauw, L. Getoor, and A. Ntoulas. [9] J. Jaworska and M. Sydow. Behavioural targeting in [10] R. Jones and K. L. Klinkner. Beyond the session [11] A. Kotov, P. N. Bennett, R. W. White, S. T. Dumais, [12] K. Liu and L. Tang. Large-scale behavioral targeting [13] Q. Mei, K. Klinkner, R. Kumar, and A. Tomkins. An [14] S. Pandey, M. Aly, A. Bagherjeiran, A. Hatch, [15] P. Pantel and D. Lin. Discovering word senses from [16] B. Piwowarski, G. Dupret, and R. Jones. Mining user [17] F. J. Provost, B. Dalessandro, R. Hook, X. Zhang, [18] F. Radlinski and T. Joachims. Query chains: learning [19] M. Richardson. Learning about the world through [20] E. Sadikov, J. Madhavan, L. Wang, and A. Y. Halevy. [21] S. K. Tyler, S. Pandey, E. Gabrilovich, and [22] J. Yan, N. Liu, G. Wang, W. Zhang, Y. Jiang, and [23] Z. Zhang and O. Nasraoui. Mining search engine query
