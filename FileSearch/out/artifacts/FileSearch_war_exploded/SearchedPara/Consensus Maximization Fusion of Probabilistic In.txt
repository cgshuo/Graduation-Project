 The abundance of unstructured text on the web such as news, discussion forums, wiki pages, etc. has in-creased the interest of the research community in the automatic extraction of information at scale. Infor-mation extractors can be used to construct or expand Knowledge Bases (KBs) through a process known as Knowledge Base Population (KBP) or Construction. Facts in a KB are typically modeled as ( subject , relation , object ) triples such as ( Facebook , org:city of headquarters , Menlo Park ).

No current information extractor is perfectly accurate and different models exhibit different strengths and weaknesses. As a result, many state-of-the-art KBs in academia and industry employ multiple complementary information extractors for KBP. NELL(Mitchell and Fredkin, 2014) employs rules, statistically-learned pattern extractors, and context extractors among others. YAGO (Suchanek et al., 2007) uses heuristic extractors at text and on-tological levels and Google has multiple extractors crawling text, tables, and HTML.

Different extraction systems may also agree or disagree on the information they extract. Con-sider three systems that extract the facts ( Facebook , org:city of headquarters , Menlo Park ), ( Facebook , org:city of headquarters , Palo Alto ), and ( Face-book , org:city of headquarters , Menlo Park ) with probabilities 0 . 6 , 0 . 3 , and 0 . 5 respectively. The Palo Alto extraction is erroneous and should be removed. The two Menlo Park extractions should be promoted by agreement and have their confidences increased.
The aggregation of facts from multiple extractors into a single probabilistic triple is known as Knowl-edge Fusion (KF) (Dong et al., 2014) and can be modeled as an ensemble learning problem. Previous ensemble approaches at the output layer divide into unsupervised and supervised methods (Gao et al., 2010). Unsupervised methods establish a consen-sus or majority vote among extractors without distin-guishing the merit of each, but perform poorly if all the extractors are weak. Supervised methods such as stacking achieve better performance by learning weights for each extractor and combining them as a weighted sum. The difficulty in obtaining training data usually results in high precision, but low recall among all facts.

As a solution to the low recall problem we present a probabilistic ensemble fusion model based on Consensus Maximization (CM) (Gao et al., 2009), which is a semi-supervised learning method able to combine the strengths of both supervised and unsu-pervised approaches. In the knowledge fusion do-main, where the number of unsupervised systems can be rather large compared to those with manu-ally labeled data, Consensus Maximization Fusion is able to leverage both for improved performance. We apply our CM Fusion approach to the NIST Slot Filling Validation (SFV) task, an ensemble learning problem that aims to combine multiple in-formation extractors participating in the NIST En-glish Slot Filling (ESF) task. Our experiments show an improved F1 score relative to the current state-of-the-art SFV systems.

We make the following overall contributions in this paper:  X  Present a novel probabilistic fusion system that  X  Develop an application of our system to the  X  Outline an evaluation of our system that im-
Though we choose to focus on the SFV task in this paper, there are clear applications beyond this and to the Knowledge Fusion problem in general. The remainder of this paper is organized as follows. In Section 2, we discuss background material on the ESF and SFV tasks as well as the Consensus Maxi-mization algorithm. Section 3 outlines our CM Fu-sion system and how it maps into the knowledge fu-sion problem. Our experiments are detailed in Sec-tion 4 and we conclude in Section 5. Here we present the appropriate background knowl-edge on the English Slot Filling (ESF) and Slot Fill-ing Validation (SFV) tasks that are part of the NIST Text Analysis Conference (TAC). We also introduce the Consensus Maximization framework that forms the foundation of CM Fusion. 2.1 Knowledge Base Construction: Slot Filling A knowledge base is a repository of information about people, places, and things. The usual repre-sentation is as ( subject , relation , object ) triple. The subject and object are entities such as Facebook or Menlo Park . The relation is some property that holds between the subject and object and usually adheres to a fixed ontology such as headquarters in . Knowledge Base population in-volves the generation of triples from unstructured text sources.
 To facilitate and encourage further research into KBP, NIST has organized a series of workshops known as the Text Analysis Conference (TAC). The English Slot Filler (ESF) task involves connecting a ( subject , relation , * ) pair with a set of corresponding object attributes. Teams compete with each other to develop the best system for the job (Surdeanu and Ji, 2014).

Each team receives as input a set of queries in XML format and a text corpus. The queries are empty slots such ( Facebook , org:city of headquarters , * ) and the corpus is a formatted set of web pages, newswire, and discussion forums. For each query slot, the systems extract the appropriate attribute as either a single value or list of values or NIL . Overall evaluation is determined by final F1 score across all queries. Query evaluation occurs after submission by a team of human judges. Teams do not know their final accuracy at submission time. 2.2 Ensembling ESF: Slot Filling Validation The massive quantity of data makes manual labeling impossible and thus system evaluation very difficult. A parallel TAC task is Slot Filling Validation (SFV), which aims to develop a meta-classifier for the pur-pose of validating individual systems. SFV systems receive as input the set of query results from each ESF system. Without any truth information, they must evaluate the evidence from each query and re-turn a final slot value. The process of sorting con-flicting and complementary information from dif-ferent systems and aggregating into a unified KB is equivalent to the Knowledge Fusion problem.
Sys. Slot Filler Provenance Prob 03 1 Menlo Park D1:683-692 0.087 03 2 Menlo Park D1:683-692 0.197 12 4 Menlo Park D2:655-665 0.987 10 3 Menlo Park D3:683-692 1.000 13 3 San Francisco D4:974-986 1.000 07 4 San Francisco D5:3534-3544 0.210 09 1 Chinatown D6:3520-3529 0.200 16 1 California D6:7250-7258 1.000 10 2 California D7:263-267 1.000
Table 1 shows an example set of query re-sults from different systems for the ( Facebook , org:city of headquarters , *) slot. Included with the slot filler are provenance information about where the filler was mentioned in the corpus and the sys-tem X  X  confidence probability.
 As stated in the introduction, previous work in SFV has used majority voting (Sammons et al., 2014) or stacking (Viswanathan et al., 2015) to com-bine the output of multiple ESF systems. The lack of ground truth motivated the majority voting ap-proach of (Sammons et al., 2014), which performs decently in precision and recall. Because of the an-nual nature of the TAC-KBP competition, some sys-tems repeat submissions in successive years. While current truth data is not available, for a small num-ber of systems there is data available from previous years on a different set of queries. (Viswanathan et al., 2015) uses those systems as training data in a stacking ensemble. Viewing each triple as a binary classification problem into true or false, they extract features based on the probabilities each ESF system gave to each fact (or 0.0 if they did not extract the fact) and train an L1-regularized SVM with a linear kernel. While this gives high precision, the lack of systems participating in multiple years leads to very low recall among all extracted facts. 2.3 Consensus Maximization Consensus Maximization (Gao et al., 2009) is an ensemble learning method that merges both super-vised and unsupervised learning models into a sin-gle cohesive framework. While unsupervised mod-els don X  X  provide class labels, they do provide con-straints that reduce the hypothesis space of the over-all learning problem. Examples in the same cluster are likely to receive the same class label and may im-prove prediction accuracy through increased model diversity.

Like majority voting and stacking, CM operates at the output level of each individual model and aims to predict the conditional probabilities of each ex-ample belonging to each class. By providing class labels, supervised models naturally partition the ex-ample space into groups and give those groups la-bels. Unsupervised models do the same partition-ing, but cannot label the groups. Using the super-vised methods and their examples as a starting point, CM propagates labels to other unsupervised clusters containing the same examples and to other examples within those clusters.

Formally, the framework of Consensus Maxi-mization is a constrained optimization problem over a bipartite graph. The two sets of nodes are object nodes and group nodes. Each Object node repre-sents a single example given to each system for clas-sification/clustering. Group nodes represent within-system partitions. There are as many partitions as there are classes. For c classes and m systems, there will be v = cm total groups.

The conditional probabilities are expressed in the form of matrices U n  X  c for objects and Q v  X  c for groups, where rows are object or group nodes from the graph and columns are classes. Rows or columns of the matrix are denoted with the vector ~u i or ~q i . Each element u iz  X  U n  X  c represents the conditional probability of object i belonging to class z . Simi-larly, each element q jz  X  Q n  X  v represents the condi-tional probability of group j belong class z as shown in the following equations:
The adjacency matrix of the bipartite graph is rep-resented by A n  X  v where a ij = 1 when x i has been assigned to group g j by its respective model, or 0 otherwise. Initial group label assignments are pro-vided by supervised models and encoded in the ma-trix Y v  X  c , where y jz = 1 if the group g j comes from supervised learning and belongs to class z , or 0 oth-erwise. The sum of the rows of Y determines if a group node belongs to a supervised or unsupervised model. Let k j = scenario. Note that k j = 0 when there is no super-vised evidence for group j . The consensus among models is formulated as the following optimization problem:
The first term finds consensus across models by minimizing the deviation between the conditional probability of object i in the vector ~u i , and the con-ditional probability of the groups, ~u j , where it has been originally assigned by the input models. The second term penalizes deviations of the group prob-ability ~q j from its initial assignment ~y j , with  X  being the penalization factor that has to be paid for violat-ing supervised predictions. Consensus Maximiza-tion does not require labeled objects to estimate the probability matrices Q and U . Nevertheless, a small amount known labels can help bias the optimization and improve the model. If l objects have known la-bels, the matrix F n  X  c encodes this information such that f iz = 1 when x i  X  X  true label is z and 0 other-wise. In this matrix, i is the index of an object node, and z a class label. The sum of rows in F determine if an object has a label assigned, h i = For instance, ~ f i = ~ 0 represents an objects without labeled data. The third term in the objective function penalizes deviations of the conditional probability of an object ~u i from its known label ~ f i ; when h i = 0 this term is dismissed. Labeled variables incorpo-rated in CM are not final. The value of  X  is the pe-nalization factor for violating label constraints. For a fuller treatment of the methodology, see (Gao et al., 2009). In this section we describe the ensemble CM Fu-sion system we built for combining multiple infor-mation extractors. Similar to (Viswanathan et al., 2015), we view the slot filling problem in terms of a binary classification task. The set of all extrac-tions across all systems produce an initial knowl-edge base of facts. This KB is noisy and redundant and contains a lot of conflicting and complementary evidence. For each fact in the KB, we fuse decisions coming from each system into a true class and a false class.

Figure 1 shows the architecture of the system. Of-fline all individual extractors operate over the same corpus and produce their extractions. A preprocess-ing step canonicalizes strings and clusters systems producing the same extraction. The feature extrac-tion step converts set of probabilities about each system X  X  fact into the unsupervised feature vector. For the systems for which previous years evaluation data is available we train 6 different meta-classifiers. Their decisions on the same corpus comprise the su-pervised feature vector. The supervised and unsu-pervised data are passed into the consensus maxi-mization component that produces a final aggregated probability for each value. As part of a final cleaning process, constraints are applied that remove certain mutually exclusive conflicting facts.

For the remainder of this section we describe each component of our Consensus Maximization Fusion system in more detail. 3.1 Preprocessing Our system takes as input extractions produced us-ing a number of information extractors that differ in their methodology and application. Their output is uniform and their  X  X illed slots X  correspond to a set of extracted facts. Each fact is processed by trans-forming all text into lower case and deleting trailing spaces. Using exact string match we map each fact onto a cluster of the multiple systems that extracted it. 3.2 Stacked Ensemble Consensus Maximization Fusion is able to take into account supervised and unsupervised systems for knowledge fusion. The supervised portion is able to weight certain systems using previously labeled data and combine their decisions into a meta-classifier. The unsupervised portion operating over unlabeled data treats every system equally and is closer in spirit to a majority vote.

Of the extractors submitting results to ESF, about 10% had participated in previous years where la-beled data was available. This idea was used in (Viswanathan et al., 2015) to generate a series meta-classifiers using stacking. A meta-classifier com-bines the outputs of multiple systems using learned weights. (Viswanathan et al., 2015) generate a to-tal of 6 meta-classifiers for comparison that differ in classifier type and feature vector. The basic fea-ture vector includes one entry for each system and with the value being that system X  X  extraction prob-ability. Two additional feature vectors were gener-ated by adding relational information for each sys-tem and both relational information and provenance information. These three feature vectors were in-dependently trained using logistic regression (LR) and SVM for a total of 6 meta-classifiers. CM Fu-sion runs each trained meta-classifier over the same ESF corpus and uses their output as an additional 6 systems in the ensemble combination. Though we use only 6 our method generalizes to any number of meta-classifiers. 3.3 Fusing Systems In total there are S systems submitting runs to be ensemble by CM Fusion. N of these systems have evaluation data for the training a total of M meta-classifiers. The top half of Figure 2 shows the col-lection of supervised meta-classifiers and remaining unsupervised systems.
 g 1 ,...,g 2 m Group nodes from meta-classifiers g 2 m +1 ,...,g t Group nodes from unmapped runs
Consensus Maximization operates as a bipartite graph between object nodes representing examples to classify and group nodes representing classes within each system. In the knowledge fusion do-main, each object node pertains to a specific fact triple under consideration. The two classes for each system ( Yes / No ) all combined represent the set of group nodes. The translation component of CM Fu-sion generates the bipartite graph in Figure 2 as input to the consensus maximization component.

The overall goal of consensus maximization is to combine labeled predictions from the M supervised meta-classifiers and consensus predictions from the unsupervised systems using a constrained optimiza-tion framework. As recalled in Section 2, CM opti-mizes U and Q , which are conditional probabilities between object nodes (facts) and classes ( Yes / No ) and between group nodes (output classes for each system) and classes ( Yes / No ) respectively. Because we are only interested in the Yes class probabili-ties, the matrices collapse into vectors ~u and ~q as per equation 3. ~u i is initialized as an uninformed prior with value 0 . 5 for each object node. Since un-supervised output labels are known a priori, ~q j re-mains unchanged in the optimization process and encodes the pertinence of each group node j to a Yes / No class. Finally ~y j includes the supervised meta-classifier output for each group. Running the optimization program generates a final ~u i that gives a posterior probability for each fact. Table 2 maps all CM symbols in equation 3 to their corresponding elements in the SFV problem definition. 3.4 Enforced Constraints After applying consensus maximization, there may be some facts that can be eliminated using function-ality constraints. A slot is functional when it has only one possible value for its filler. Functional slots with different fillers may have certain facts elimi-nated based on mutual exclusion. As an example, consider the various slot filler responses in Table 1. An organization can only have its headquarters in one city at any given time. In this case Menlo Park should be chosen based on it having the maximum probability and all other facts eliminated. When the extraction probability is the same for multiple sys-tems, one of the extractions is chosen at random. Nevertheless, this is hardly ever the case. We evaluate our system using a collection of queries supplied by TAC-KBP SFV. In this section we more fully describe the dataset and our experiment methodology. We compare to the current state-of-the-art ESF and SFV systems and show an improve-ment in F1 score. Finally, we provide an analysis of our results. 4.1 Datasets Three datasets were used for training and testing spread across the three years that the English Slot Filling task has been run. Each team in the compe-tition submits multiple models vying for the highest score on unseen training data. Each submission is viewed as a different system in our ensemble. 2013 contains 52 systems. 2014 contains 65 and 2015 contains 69. In 2013 and 2014 we use only ESF data, but 2015 adds Cold Start Knowledge Base (CSKB) data. The total number of labeled queries in both 2013 and 2014 were 100 each. 2015 had 9340 un-labeled queries. In addition, 2015 had 164 labeled queries supplied for initial system assessment that was incorporated into the training data for 2015 sub-missions.

We performed two distinct experiments and com-pared multiple baselines for each. Table 3 shows results where we trained on 2013 only and tested on 2014 data only. Table 4 results are for training on 2013 and 2014 data and testing on 2015 data. 4.2 0-Hop and 1-Hop Queries The 2015 ESF task introduced more complex queries into the fold. 1-hop queries use the result of a previous query to answer a new query. For example, consider a query ask-ing for all companies headquartered in the same city as Facebook. The equivalent 1-hop query is ( Facebook , org:city of headquarters , ?x ), ( ?x , gpe:headquarters in city , *). The second part of the query can be seen as a join to the first part using a common answer in both. The terminology exploits the idea of  X  X opping X  from one query to another. By contrast, a regular slot filling task is a 0-hop query. 4.3 Evaluation All systems were evaluated using the standard met-rics of precision, recall, and F1 which is the har-monic mean of precision and recall. Precision is the amount of correctly extracted facts compared to the total facts extracted by the system. Recall is the amount of correct facts compared all facts in the ground truth. In review: SVM + PROV + REL 0.729 0.298 0.423
The 0-hop queries are scored trivially on the cor-rectness of the slot filler. The 1-hop queries require two verifications to undergo scoring. The 0-hop query from which it was derived must both exist and be correct. Any slot fillers that don X  X  meet this crite-ria are omitted even if their 1-hop slot was ultimately correct. For example, the 1-hop extraction ( Palo Alto , gpe:headquarters in city , Hewlett-Packard ) will be ignored even though it is correct because the 0-hop query ( Facebook , gpe:city of headquarters , Palo Alto ) that derives it is incorrect (Facebook is headquarted in Menlo Park). 4.4 Results Table 3 shows results using 2013 ESF systems as training data for the meta-classifiers and 2014 ESF systems as testing data. The first 6 rows are a combi-nation of classifiers and feature vectors using stack-ing as described in (Viswanathan et al., 2015). MV refers to the majority voting described (Sammons et al., 2014). Majority voting accepts a fact if a cer-tain number of systems above some learned thresh-old have extracted it. Both of these systems show-cased results on the 2014 SFV task. The Stanford system (Angeli et al., 2014) was the best perform-ing ESF system during the 2014 competition. The final row is our CM Fusion algorithm. Only 0-hop queries are used for these results.

Table 4 showcases two of the three runs we offi-cially submitted as part of the recent 2015 SFV task 2013 &amp; 2014 0.528 0.481 0.504 2013 &amp; 2014 0.393 0.097 0.155 2013 &amp; 2014 0.503 0.307 0.381 for different query types. For each query, the first two rows refer to our CM Fusion system with dif-ferent training data and the final row the best per-forming 2015 ESF system for that particular query type. 2015 results are not officially published yet for either ESF or SFV. Nevertheless, at 2015 workshop announcing the results, CM Fusion was awarded as the top ranked SFV system by F1 score. 4.5 Analysis The main drawback between previous ensemble sys-tems that utilize only supervised systems is high pre-cision, but very low recall. This is evidenced in the disproportion between precision and recall in the first 6 systems of Table 3. Majority voting, while learning a threshold using supervision, does include some unsupervised consensus.

The main idea behind CM Fusion is to take into account the answers from potentially well-ranked extractors that stacking meta-classifiers omit due lack of training data. CM Fusion outperforms both approaches in terms of F1 by greatly increasing the recall while maintaining high precision. It also pro-duces better results than the best performing 2014 ESF system. On 2015 data in Table 4, both systems outperform the best performing 2015 ESF system.
The benefit of using CM Fusion over other su-pervised ensemble models is the ability to use un-supervised systems that lack sufficient training data. When these systems agree on extractions, their con-sensus can be a source of discriminative power which CM Fusion is able to harness. For our submis-sion trained only on 2014 ESF data, Figures 3 and 4 break down the difference between extracted facts derivable only from supervised systems and those able to be attained from unsupervised systems.
Specifically, Figure 3 measures the number of ex-tracted facts supported by a consensus of the num-ber of systems on the x-axis that also agree with the output of a supervised system. Blue bars represent the amount of correct extractions and red bars in-correct extractions. When only supervision is used there is good precision across the board. Figure 4 shows the number of extracted facts supported only by a consensus of systems on the x-axis without any supervision. The disparity in the scale of facts ex-tracted supports the recall of supervised systems. When only a few systems agree on an extraction, about as many good facts are extracted as bad ones. As consensus increases, precision greatly improves. This idea of unsupervised consensus is exactly what improves CM Fusion over supervised ensemble ap-proaches.

To understand the impact of the number of unsu-pervised systems fused and the quality of the fused runs in the ensemble, we applied CM Fusion in an incremental fashion by adding one unsupervised system at a time and scored the produced ensem-ble at each step. Techniques for ranking unsuper-vised systems are outside the scope of this work, for simplicity we examine the best-and worst-cases for adding systems incrementally. Ranking each unsu-pervised system by its final individual F1 score on the SFV 2015 data, we ran CM Fusion adding the best performing run (CMF-BEST) and worst per-forming run (CMF-WORST) at each step. The re-sults are displayed in Figure 5. For comparison, we also included the results of the best performing ESF system (BBN-F1) and the score of a supervised-only ensemble (SUP-ONLY). Fusing unsupervised systems with lower accuracy negatively affects the quality of the ensemble compared to supervised-only. When ensembleing very similar runs, such as runs submitted by the same team, the diversity of the systems is compromised and may lower the ensem-ble quality. On the other hand, unsupervised sys-tems with higher accuracy can rapidly increase the quality of the ensemble above the best individual re-sults and reach the highest ensemble score. Never-theless, more does not necessarily mean better, as shown in the plateau of the best-case plot in Fig-ure 5, when noisy systems are added to the ensemble the F1 score is maintained. The average case would lie somewhere in between the extremes of the best-and worst-case. As is shown, a large number of sys-tems are extremely error-prone, but the combination using CM Fusion produces a result ultimately supe-rior. This paper presented our Consensus Maximization Fusion of multiple probabilistic information extrac-tors. This approach combines supervised stacking meta-classifiers with unsupervised extraction out-puts in an ensemble classifier. Our system outper-formed the current state-of-the-art ensemble mod-els submitted to the TAC-KBP Slot Filler Valida-tion task in 2014. CM Fusion was also chosen as the leading system at the 2015 TAC-KBP Workshop. This is the first cross-model ensemble approach that fuses multiple knowledge graphs obtained from both supervised and unsupervised information extractors. Optimal performance is attained when the extractors represent different systems running over the same corpus and the shared extraction density is high.
The canonicalization of facts in CM Fusion rep-resents a new state-of-the-art contribution to entity-centric information extraction compared to tradi-tional document-centric approaches. While our ap-proach has been experimentally verified using TAC-KBP data, it generalizes to overlapping ensemble of knowledge bases. Some such as NELL (Mitchell and Fredkin, 2014) have a small supervised hu-man feedback component and others such as Ope-nIE (Banko et al., 2007) are entirely unsupervised. Future work concerns using CM Fusion to to align and canonicalize multiple such knowledge bases to solve the knowledge fusion problem.
 This work is partially supported by NSF IIS Award #1526753, DARPA under FA8750-12-2-0348-2 (DEFT/CUBISM) and graduate fellowships from Fulbright and Sandia National Labs
