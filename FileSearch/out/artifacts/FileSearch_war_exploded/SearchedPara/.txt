 Patent documents are important intellectual resources of protecting interests of individuals, organizations and com-panies. Different from general web documents, patent doc-uments have a well-defined format including frontpage, de-scription, claims, and figures. However, they are lengthy and rich in technical terms, which requires enormous human ef-forts for analysis. Hence, a new research area, called patent mining, emerges in recent years, aiming to assist patent ana-lysts in investigating, processing, and analyzing patent doc-uments. Despite the recent advances in patent mining, it is still far from being well explored in research communities. To help patent analysts and interested readers obtain a big picture of patent mining, we thus provide a systematic sum-mary of existing research efforts along this direction. In this survey, we first present an overview of the technical trend in patent mining. We then investigate multiple research questions related to patent documents, including patent re-trieval, patent classification, and patent visualization, and provide summaries and highlights for each question by delv-ing into the corresponding research efforts.
 Patent Mining; Patent Information Retrieval; Patent Clas-sification; Patent Visualization; Patent Valuation; Cross-Language Patent Mining; Patent Application Patent application is one of the key aspects of protecting intellectual properties. In the past decades, with the ad-vanced development of various techniques in different appli-cation domains, a myriad of patent documents are filed and be approved. They serve as one of the important intellec-tual property components for individuals, organizations and companies. These patent documents are open to public and made available by various authorities in a lot of countries or regions around the world. For example, World Intellectual Property Organization (WIPO) 1 reported 1.98 million total patent applications filed worldwide in 2010.
 Patent documents have great research values, beneficial to the industry, business, law, and policy-making communities. If patent documents are carefully analyzed, important tech-nical details and relations can be revealed, leading business http://www.wipo.int/ipstats/en/general info.html. trends can be illustra ted, novel industrial solutions can be inspired, and consequently vital investment decisions can be made [15]. Thus, it is imperative to carefully analyze patent documents for evaluating and maintaining patent values. In recent years, patent analysis has been recognized as an im-portant task at the government level. Public patent author-ities 2 in United States, United Kingdom, China and Japan have invested various resources to improve the performances of creating valuable patent analysis results for various patent analysis tasks.
 However, patent analysis is a non-trivial task, which often requires tremendous amount of human efforts. In general, it is necessary for patent analysts to have a certain degree of expertise in different research domains, including infor-mation retrieval, data mining, domain-specific technologies, and business intelligence. In reality, it is difficult to find and train such analysts to match those multi-disciplinary requirements within a relatively short period of time. An-other challenge of patent analysis is that patent documents are often lengthy, and full of technical and legal terminolo-gies. Even for domain experts, it may also require a lot of time to read and analyze a single patent document. There-fore, patent mining plays an important role in automatically processing and analyzing patent documents [106; 127]. A patent document often contains dozens of items that can be grouped into two categories: (1) structured items, which are uniform in semantics and format (such as patent num-ber, inventor, filing date, issued date, and assignees); and (2) unstructured items, which consist of text content in differ-ent length (including claims, abstracts, and descriptions of the invention.). Given such a well-defined structure, patent documents are considerably different from general web docu-ments (e.g., web pages), most of which contain unstructured data, involving free texts, links, tags, images, and videos. Hence, the analysis of patent documents might be different from the one for web documents in terms of the format and various application-wise purposes.
 In this survey, we comprehensively investigate multiple crit-ical research questions in the domain of patent mining, in-cluding (1) how to effectively retrieve patent documents based on user-defined queries (See Section 3)? (2) how to efficiently perform patent classification for high-quality maintenance (See Section 4)? (3) how to informatively rep-resent patent documents to users (See Section 5)? (4) how to explore and evaluate the potential benefit of patent doc-uments (See Section 6)? and (5) how to effectively deal with cross-language patent documents (See Section 7)? For http://www.wipo.int/directory/en/urls.jsp.
 [2; 9; 25; 28; 29; 30; 31; 34; 40; 43; 52; 68] [69; 72; 73; 74; 78; 83; 96; 98; 99; 107; 114] each question, we first identify several critical research chal-lenges, and then discuss different research efforts and vari-ous techniques used for addressing these challenges. Table 1 summarizes different patent mining tasks, including patent retrieval, patent classification, patent visualization, patent exploration, and cross-language patent mining. Up-to-date references/lists related to patent mining can be found at http://users.cis.fiu.edu/  X  lzhan015/patmining.html. In the following sections, we will briefly introduce the existing so-lutions to each task based on the techniques being utilized. The rest of the paper is organized as follows. In  X  2, we provide an introduction to patent documents by describing patent document structures, patent classification systems, and various patent mining tasks. Section 3 presents a sum-mary of research efforts for addressing patent retrieval, espe-cially, patent search. In Section 4, we investigate how patent documents can be automatically classified into different pre-defined categories. In Section 5, we explore how patent doc-uments can be represented to analysts in a way that the core ideas of patents can be clearly illustrated and the cor-relations of different documents can be easily identified. In Section 6, we show that the quality of a patent document can be automatically evaluated based on some predefined measurements that help companies decide which patent is more important and should be further maintained for effec-tive property protection. In Section 7, we present different techniques for cross-language patent mining, including ap-proaches to solving machine translation and semantic corre-spondence. Section 8 discusses existing free and commercial patent mining systems that provide various functionalities to allow patent analysts to perform different patent mining tasks. Finally, Section 9 concludes our survey and discusses emerging research-and application-wise challenges in the domain of patent mining. In this section, we first provide a brief overview of patent documents and their structure, and then describe the cur-rent patent classification systems, followed by introducing the tasks in the entire process of patent application. According to World Intellectual Property Organization 3 ,the definition of a patent is:  X  patents are legal documents issued http://www.wipo.int. by a government that grants a set of rights of exclusivity and protection to the owner of an invention. The right of exclu-sivity allows the patent owner to exclude others from making, using, selling, offering for sale, or importing the patented in-vention during the patent term, typically period from the ear-liest filing date, and in the country or countries where patent protection exists.  X  Based upon the understanding of the defi-nition, patent documents are one of the key components that serve to protect the intellectual properties of patent owners. Note that patents and inventions are two different yet in-terleaved concepts: patents are legal documents, whereas inventions are the content of patents. Different countries or regions may have their own patent laws and regulations, but in general there are two common types of patent doc-uments: utility patents and design patents. Utility patents describe technical solutions related to a product, a process, or a useful improvement, etc., whereas design patents of-ten represent original designs related to the specifications of a product. In practice, due to the distinct properties of these two types of patents, the structure of patent document may vary slightly; however, a typical patent document often contains several requisite sections, including a front page, detailed specifications, claims, declaration, and/or a list of drawings to illustrate the idea of the solution.
 Figure 1 shows an example of the front page of a patent document. In general, a frontpage contains four parts, de-scribed as follows: 1. Announcement , which includes Authority Name (e.g. 2. Bibliography , which often includes Title, Inventors, 3. Classification and Reference , which include Inter-4. Abstract , which may contain a short description of the Beside the front page, a patent document contains detailed description of the solution, claims, and/or a list of draw-ings. The description section, in general, depicts the back-ground and summary of the invention, brief description of the drawings, and detailed description of preferred embod-iments. The claim section is the primary component of a patent document, which defines the scope of protection con-veyed by the invention. It often contains two types of claims: (1) the independent claim which stands on itself; and (2) the dependent claims which refer to its antecedent claim. A patent document is often lengthy, compared with other types of documents, e.g., web pages. Although the structure of a patent document is well-defined, a myriad of obscure and ambiguous text snippets are often involved, and various technical terms are often used in the content, which render the analysis of patent document more difficult. Before the publication of patent applications, one or more classification codes are often assigned to patent documents based on their textual contents for the purpose of efficient management and retrieval. Different patent authorities may maintain their own classification hierarchies, such as the United States Patent Classification (USPC) in the United States, the International Patent Classification (IPC) for the World Intellectual Property Organization, and the Derwent classification system fixed by Thomson Reuters. In the fol-lowing, we will introduce the classification taxonomies of IPC and USPC in more details. IPC was established in 1971 based on Patent Cooperation Treaty [22]. This hierarchical patent classification system categorizes patents to different technological groups. There are over 100 countries using IPC system to classify their national patent applications. Specifically, the IPC category taxonomy contains 8 sections, 120 classes, 630 subclasses, 7,200 main groups and approximately 70,000 sub-groups. A typical IPC category contains a class label and a piece of text description to indicate the specific category content. In IPC, all technological fields are first grouped into 8 sec-tions represented by one of the capital letters from A to H , including (A)  X  X uman necessities X ; (B)  X  X erforming op-erations, transporting X ; (C)  X  X hemistry, metallurgy X ; (D)  X  X extiles, paper X ; (E)  X  X ixed constructions X ; (F)  X  X echan-ical engineering, lighting, heating, weapons, blasting X ; (G)  X  X hysics X ; and (H)  X  X lectricity X . Then, within each sec-tion, the technological fields are regrouped into classes as the second level of the IPC taxonomy. Each class consists of one or more subclasses, which are treated as the third level of the taxonomy. Finally, each subclass is further divided into subdivisions referred to as  X  X roups X . As an illustrative example, Figure 2 describes the class label  X  X 01S 3/00 X  and its ancestors.
 The USPC system was developed in 1836, which is the first patent taxonomy established in the world [88]. In USPC, the patent categories are organized as a two-level taxonomy, i.e., class and subclass. Each class has a designated class number, and includes a descriptive title, class schedule, and definitions. Then each class is subdivided into a number of subclasses. A subclass has a number, a title, an indent level indicated by one or more dots, a definition, a hierarchical re-lationship to other subclasses in a class, and relationships to other subclasses in other classes. A subclass is the smallest searchable group of patents in USPC. Based upon the filing status of a patent document, a patent mining system can be decomposed into two modules: (1) Pre-filing module, in which the patent documents are care-fully examined to ensure the non-infringement; and (2) Post-http://www.wipo.int/classifications/ipc/en.
 filing module, in which patent documents are maintained and analyzed. The general architecture of a patent mining system is depicted in Figure 3.
 During the pre-filing process, or say, the application process, there are two major tasks: 1. Classifying the patent application into multiple prede-2. Searching all relevance patent documents from patent Currently in most intellectual property authorities and/or patent law firms, these two tasks are often being conducted manually. In practice, these two tasks, especially the lat-ter one, may require specific domain expertise and a huge amount of time/human efforts.
 The major focus of the post-filing process is to maintain and analyze patent documents in order to provide fully func-tional support to various types of enterprises. For example, a company plans to develop a new product. Prior to the de-sign/implementation of this product, it is essential to deter-mine what related products have already been produced and patented. Therefore, a typical task is to perform a compre-hensive investigation towards the related domain/products by virtue of patent search. By doing this, the company is able to obtain an overview of the general technologies ap-plied in the corresponding domain, as well as the technical details of relevant products. In general, in the process of post-filing , besides the task of patent search, three additional tasks are often involved: 1. Patent visualization, which aims to represent patent 2. Patent valuation, which explores patent documents in 3. Cross-language mining, which localizes patent infor-However, due to the large volume of patent files and di-verse writing styles of patent applications, these processes are time-consuming, and often require a lot of human efforts for patent reading and analysis. The ultimate goal of these efforts is to provide automatic tools to ease the procedure of patent analysis. In the following sections, we will introduce the existing academic/industrial efforts in designing patent mining algorithms and building patent mining applications using the architecture shown in Figure 3. Patent retrieval is a subdomain of information retrieval, in which the basic elements to search are patent documents. Due to the characteristics of patent documents and special
Figure 3: The architecture of a patent mining system. requirements of patent retrieval, patent search is quite dif-ferent from searching general web documents. For example, queries in patent search are generally much longer and more complex than the ones in web search.
 With the domain-specific requi rement of patent retrieval, patent search has gained great attention in the last decade in both academia and industry. Currently, there are nu-merous benchmark collections of patent documents avail-able in information retrieval community, and several work-shops and symposiums on patent retrieval have been or-ganized, including NTCIR 5 ,CLEF 6 and TREC 7 . In 2003, the third NTCIR workshop [44] firstly provided benchmark collections of patent documents for enhancing research on patent information processing. They assigned the  X  X atent Retrieval Task X  to explore the effect of retrieving patent doc-uments in real-world applications. The recent advancement in patent search is driven by the  X  X ntellectual Property X  task initialized by CLEF [87]. Several teams participated in the prior-art search task of the CLEF-IP 2010 and pro-posed approaches to reduce the number of returned patent documents by extracting a set of key terms and expanding queries for broader coverage.
 Despite the recent advances, the task of patent retrieval re-mains challenging from multiple perspectives. We summa-http://research.nii.ac.jp/ntcir/index-en.html. http://ifs.tuwien.ac.at/  X  clef-ip. http://trec.nist.gov.
 rize several challenges related to patent retrieval as listed in Table 2. In the following, we first introduce various types of patent search tasks in Section 3.1, and then discuss existing solutions/approaches to the aforementioned challenges. A summary of patent retrieval techniques is depicted in Fig-ure 4. Specifically, in Section 3.2 we discuss how to improve the readability of patent documents; in Section 3.3 we in-troduce existing methods that assist patent examiners in generating query keywords; and in Section 3.4 we describe the techniques to expand the query keyword set.
 In practice, there are five representative patent search tasks listed as follows: In Figure 5, we provide an overview of the procedure to per-form patent search tasks. As depicted, it contains 4 major steps:
Here the publications are public literatures, including patent documents and scientific papers. Step 1 Construct the retrieval query: Step 2 Perform the query and review the results: Step 3 Refine the retrieval query: Step 4 Analyze the returned results: We take patentability search as an illustrative example to further explain the search procedure. Suppose a patent ex-aminer tries to perform the patentability search for a patent application related to  X  X ersonal Data Mining X . In Step 1, he/she will read the application file and extract keywords such as  X  X ata mining X ,  X  X apture data X , and  X  X orrelation connection link X , and generate the search query based on these keywords. Then he/she will perform the search query within a series of patent databases, such as USPAT and IBM TDB, and iteratively refine the query according to the search results in Step 2 and 3. Finally, he/she will read all 40  X  X its X  (the returned documents) to find a list of relevant documents and write a search report in Step 4. Figure 6 shows a query log of this example 9 .
 InSection2.1,wehaveintroducedthetypicalstructureof patent documents. Besides the structured content in the front page, a patent document, in practice, often contains a large amount of unstructured textual information. In order to ensure the patentability of patent documents and maxi-mize the scope of the protection, patent attorneys or inven-tors, in general, use complex sentences with domain-specific words to describe the invention, which renders patent doc-uments difficult to understand or read, even for domain ex-perts. This phenomenon is more common in the claims, which is the most important part of a patent document, as claims often define the implementation of essential compo-nents of the patent invention. In order to help users quickly grasp the core idea of a patent document, and consequently improve the efficiency of patent retrieval, it is imperative to refine the readability of patent documents.
 A patent document often involves complex structure and/or lexicon. To ease the understanding of patent document, re-searchers usually try to reduce both structural complexity and lexical complexity using techniques of information re-trieval, data mining, natural language processing, etc. For http://portal.uspto.gov/pair/PublicPair. example, in [91], Shinmori et al. utilize nature language pro-cessing methods to reduce the structural complexity. They predefine six relationships (procedure, component, elabo-ration, feature, precondition, composition) to capture the structure information of Japanese patent claims. In addi-tion, they use cue-phrase-based approaches to extract both cue phrase tokens and morpheme tokens, and then employ them to create a structure tree to represent the first in-dependent claim. Their experimental results on NTCIR3 patent data collection indicate that the proposed tree-based approach can achieve better performance in terms of accu-racy. In contrast, Sheremetyeva [90] proposes the similar approach to capture both the structure and lexical content of claims from US patent documents. The author decom-poses the long claim sentences into short segments, and then analyzes the dependence relations among them. After that, a tree-basd representation is provided to capture both con-tent and structure information of claims, and consequently the readability of the patent documents is improved. Besides the complexity, patent documents often contain some spelling errors. Stein et al. [92] indicate that many patents from USPTO contain the spelling errors, e.g.,  X  X amsung Inc X  may be written as  X  X umsung Inc X . Such errors may in-crease the inconsistency of the patent corpus and hence may deteriorate the readability of patent documents. Thus, they provide an error detection approach to identify the spelling errors in the field of patent assignee (e.g., company name). The experiments have shown that both precision and recall can be improved after they correct the spell errors. In general, users may specify only several keywords in ad-hoc web search. Most web-based search systems have the restriction on the length of the input query, e.g., the maxi-mum number of query keywords in Google search engine is 32. One possible reason is that the retrieval response time of search engines increases along with the length of the input. Comparatively in patent retrieval systems, a patent query often consists of tens or even hundreds of keywords on av-erage. A common practice of generating such a query is to manually extract representative terms from original patent documents or add additional technological terms. This is of-ten achieved by patent examiners, which requires a tremen-dous amount of time and human efforts. Also, patent exam-iners are expected to have strong technological background in order to provide a concise yet precise query. To assist patent examiners in generating patent queries, a lot of re-search work has been proposed in the last decade. In general, there are two automatic ways to produce a patent query, i.e., query extraction and query partition . Query extraction aims to extract representative information from an invention that describes the core idea of the inven-tion. The simplest way of query extraction is to extract the abstract which is the summary of the invention given by the patent applicant, or the independent claims which define the scope of the protection. However, the extracted information based on abstracts or claims may not be suitable to form the patent query. The reason is straightforward: applicants often describe the abstract/claim without enough techni-cal details in order to decrease the retrievability of their patent, and the terms in the abstract/claims often contain obscure meaning (e.g.,  X  X omprises X  means  X  X onsists at least of X ) [106].
 To alleviate this issue, Konishi [55] tries to expand the query by selecting terms from the explanative sentences in the de-scription. As mentioned in Section 2, the description section of a patent document consists of the detailed information of the invention. Additional efforts along this direction in-volve [76; 119] that extract query terms from different sec-tions of a patent document to automatically transform a patent file into a query. In [119], different weights are as-signed to terms from different sections of patents. Their experiments on a USPTO patent collection indicate that using the terms from the description section can produce high-quality queries, and using the term frequency weighting scheme can achieve superior retrieval performance. In [76], a patent query is constructed by selecting the most represen-tative terms from each section based on both log-likelihood weighting model and parsimonious language model [38]. While the authors only consider 4 sections, including title, ab-stract, description and claims, they draw the same con-clusion that extracting terms from the description section of a patent document is the best way to generate queries. Mahdabi et al. [73] further propose to utilize the interna-tional patent code as an additional indicator to facilitate automatic query generation from the description section of patents.
 In addition to extracting query terms from a single sec-tion [73; 76; 119], Konishi [55] exploits the combination of queries from multiple sections to build a query. The in-tuition is that the terms extracted from a single section is more cohesive from the ones from different sections, whereas the terms of multiple sections can help emphasize the dif-ferences between sections. Therefore, the generated queries from single sections can be treated as subqueries for search-ing patent documents. The experiments [55] demonstrate that the best retrieval performance could be achieved by combining the extracted terms from the abstract, claims, and description sections.
 However, the aforementioned approaches require to assign weights to terms from different sections. In most cases, the weights of terms are difficult to obtain, and hence have to be heuristically assigned. To further improve the retrieval, Xue and Croft consider to employ additional features, including patent structural features, retrieval-score features, and the combinations of these features to construct a  X  X earning-to-rank X  model [118]. Their experiments on a USPTO patent collection demonstrate that the combination of terms and noun-phrases from the summary field can achieve the best retrieval performance. An alternative way for query generation is to automatically partition the query document into multiple subtopics, and generate keywords based on each subtopic. Along this di-rection, several partition-based approaches have been pro-posed to improve the quality of patent queries. For example, Takaki et al. [95] partition the original query document into multiple subtopics, and then builds sub-queries to retrieval similar documents for each subtopic. A entropy-based  X  X el-evance score X  of each subtopic is defined to determine rele-vance documents. However, this method involves extracting terms from the query document for each subtopic element, and hence the time complexity will increase along with the number of subtopics. Borgonovi et al. [11] present a similar approach to segment original query into subtopics. Instead of extracting terms form subtopics, they treat subtopics as sub-queries, and directly use them to execute the search and merge results obtained from each sub-query as the final result. Another approach [10] splits the original query docu-ment into multiple sentences, and then treats each sentence as an individual query to perform search. The top k rele-vant documents of each sub-query are merged as the final retrieval result. The empirical evaluation demonstrates that this approach is able to achieve reasonable retrieval perfor-mance, and also can significantly improve the running time compared with other baselines. Patent search, as a recall-orientated search task, does not allow missing relevant patent documents due to the highly commercial value of patents and high costs of processing a patent application or patent infringement. Thus, it is impor-tant to retrieve all possible relevant documents rather than finding only a small subset of relevant patents from the top ranked results. To this end, a common practice is to enrich the query keywords in order to improve the keyword cover-age, which is often referred to as query expansion .Recently, many query expansion techniques have been introduced in the field of patent search to improve the effectiveness of the retrieval. As discussed in [69; 78], the methods for tack-ling this problem can be categorized into two major groups: (1) appending-based methods , which either introduce simi-lar terms or synonyms from patent document or external resources, or extract new terms from patent document to expand or reformulate a query; and (2) feedback-based meth-ods , which modify the query based on the retrieved results, e.g. using pseudo relevance feedback or citation analysis. Appending-based methods try to append additional terms to the original keyword set. In practice, the additional terms can be extracted from either the query document or the ex-ternal resources, e.g., Wordnet and Wikipedia. Based on the information sources utilized by query expansion, this type of methods can be further decomposed into two groups: (1) methods that employ the query document as the expansion basis; and (2) methods that use external resources to expand the query.
 Internal methods : This type of techniques exploits the query patent document itself as the resource to expand the original keyword set. The general process is to extract rel-evant or new terms that represent the major idea of the invention. A lot of query expansion approaches fall into this group. For example, Konishi [55] expands query terms by virtue of the  X  X xplanative sentences X  extracted from the de-scription section of the query patent, where the explanative sentences are obtained based on the longest common sub-string with respect to the original keyword set. In addition, several approaches [69; 99] use multi-language translation models to create a patent-related synonyms set (SynSet) from a CLEP-IP patent collection, and expand the original query based on SynSet. Parvaz et al. [73] introduce vari-ous features that can be used to estimate the importance of the noun-phrase queries. In their method, important noun-phrase queries are selected to reformulate original keyword set. These approaches are able to improve the retrieval per-formance; however, the improvement purely based on the extraction paradigm is quite marginal.
 To further enhance the retrieval capability, semantic rela-tions, e.g., the keyword dependencies, between query key-words are often explored. For example, Krishnan et al. [57] propose an approach to identifying the extracted treatment and causal relationships from medical patent documents. In [83], linguistic clues and word relations are exploited to identify important terms in patent documents. Based on the extracted relations between problems and solutions, the original query is reformulated. The evaluation shows that by considering the semantic relations of keywords, the retrieval performance can be improved to a great extent.
 External methods : This type of techniques aims to utilize external resources, e.g., WordNet and Wikipedia, to expand original queries. WordNet is a large lexical database of En-glish that groups different terms into sets of cognitive syn-onyms. It is often employed by researchers from the informa-tion retrieval community to enhance retrieval effectiveness. Recently, WordNet has been used to facilitate the process of query expansion in patent retrieval. For instance, Magdy and Jones [69] build a keyword-based synonym set with ex-tracted synonyms and hyponyms from WordNet, and uti-lize this synonym set to improve the retrieval performance. However, in some cases it cannot obtain reasonable results due to the deficiency of contextual information. To solve this problem, Al-Shboul and Myaeng [2] introduce another external resource, i.e., Wikipedia, to capture the contextual information, i.e., the category dependencies. Based on the category information of Wikipedia, another query candidate set is generated. Finally, the WordNet-based synonym set and the Wikipedia-based candidate set are integrated to re-fine the original query.
 Besides the public resources available online, the domain-specific ontology is another reliable resource that can be utilized to expand the keyword set. For example, Mukher-jea et al. [82] apply Unified Medical Language System as an ontology to facilitate keyword-based patent query expansion in biomedical domain, and the result can be refined based on the semantic relations defined by the ontology. Another useful resource is the patent classification information that defines the general topic/scope of patent documents [1; 35]. Mahdabi et al. [75] treat patent classification information as domain knowledge to facilitate query expansion. Based on the international patent classification information, a con-ceptual lexicon is created and serves as a candidate pool to expand the keyword set. To further improve the effective-ness of patent retrieval, the proximity information of patent documents is exploited to restrict the boundary of query ex-pansion. Recently, Tannebaum et al. [99; 100] introduce the query logs as expert knowledge to improve query expansion. Based on the analysis of query logs, they extract the fre-quent patterns of query terms and treat them as rules to expand the original keyword set. The idea of relevance feedback [89] is to employ user feed-backs to improve the search result in the process of infor-mation retrieval. However in practice, it is often difficult to obtain direct user feedbacks on the relevance of the retrieved documents, especially in patent retrieval. Hence, researchers usually exploit indirect evidence rather than explicit feed-back of the search result. Generally, there are two types of approaches to acquire indirected relevant feedback: pseudo relevance feedback and citation analysis .
 Pseudo relevance feedback : Pseudo relevance feedback (Pseudo-RF) [117], also known as blind relevance feedback, is a standard retrieval technique that regards the top k ranked documents from an initial retrieval as relevant doc-uments. It automates the manual process of relevance feed-back so that the user gets improved retrieval performance without an extended interaction [78]. Pseudo-RF has been extensively explored in the area of patent retrieval. Several related approaches have been proposed to employ Pseudo-RF to facilitate the retrieval performance of patent search. In NTCIR3, Kazuaki [52] exploits two relevance feedback models, including the Rocchio [89] model and Taylor ex-pansion based model, and then extends relevance feedback methods to pseudo relevance feedback methods by assuming the top-ranked k documents as relevant documents. In NT-CIR4 [43] and NTCIR5 [96], several participants attempt to utilize different Pseudo-RF approaches to improve the retrieval effectiveness. However, existing studies indicate that Pseudo-RF based approaches perform relatively poor on patent retrieval tasks, as it suffers from the problem of topic drift due to the ambiguity and synonymity of terms [71]. To alleviate the negative effect of topic drift, Bashir and Rauber [8] provide a clustering-based approach to deter-mine whether a document is relevant or irrelevant. Based upon the intra-cluster similarity, they select top ranked doc-uments as relevant feedback from top ranked clusters. Re-cently, Mahdabi et al. [74] utilize a regression model to pre-dict the relevance of a returned document combined with a set of features (e.g. IPC clarity and query clarity). Their experiments demonstrate the superiority of the proposed method over the standard pseudo relevance feedback method. Based on this approach, in [73], they introduce an additional key-phrase extraction method by calculating phrase impor-tance scores to further improve the performance.
 Citation analysis : There are two types of citations as-signed to patent documents: applicant-assigned citations and examiner-assigned citations. The first type of citations are produced by patent applicants, and often appear in the specification of patent applications in a way similar to the case that research papers are cited. Comparatively, citations assigned by patent examiners are often obtained based on the results from patentability search of the patent appli-cation, and hence might be more accurate because of the authority of the examiners.
 Citations are good indicators of relevance among patent doc-uments, and thus are often utilized to improve the search re-sults. For example, Fuji [25] considers the cited documents as relevance feedback to expand the original query. Based on the empirical evaluation, the retrieval performance can be significantly improved by virtue of patents citation informa-tion. In CLEF 2009 IP track, Magdy et al. [68] propose to automatically extract the applicant-assigned citations from patent documents, and utilize these cited documents to fa-cilitate patent retrieval. They further improve the citation feedback method by introducing additional terminological resources such as Wikipedia [72]. Patent classification is an important task in the process of patent application, as it provides functionalities to enable flexible management and maintenance of patent documents. However in recent years, the number of patent documents is rapidly increasing worldwide, which increases the demand for powerful patent mining systems to automatically catego-rize patents. The primary goal of such systems is to replace the time-consuming and labor-intensive manual categoriza-tion, and hence to offer patent analysts an efficient way to manage patent documents.
 Since 1960, automatic classification has been identified as an interesting problem in text mining and natural language processing. Nowadays, in the field of text classification, re-searchers have devised many excellent algorithms to address this problem. However, as we previously described, it is still a non-trivial task in the domain of patent mining due to the complexity of patent documents and patent classifica-tion criteria. There are several challenges during the process of patent classification, including (1) patent documents of-ten involve the sophisticated structures, verbose pages, and rhetorical descriptions, which renders automatic classifica-tion ineffective as it is difficult to extract useful features; (2) the hierarchical structure of the patent classification schema is quite complex, e.g. there are approximately 72,000 sub-groups in the bottom level of IPC taxonomy; and (3) the huge volume of patent documents, as well as the increasing variety of patent topics, exacerbates the difficulty of auto-matic patent classification.
 To overcome these challenges, researchers have put a lot of efforts in designing effective classification systems in the past decades. The major focus along this research direction in-cludes (1) utilizing different types of information to perform classification; and (2) testing the performance of different classification algorithms on patent documents. The bag-of-words (BOW) model is often employed to repre-sent unstructured text document. In the domain of patent document classification, the BOW representation has been widely explored. For example, Larkey [58] proposes a patent classification system in which terms and phrases are se-lected to represent patent documents, weighted by the fre-quency and structural information. Based on the vector space model, KNN (K-Nearest Neighbors) and Na  X   X ve Bayes classification models are employed to categorize US patent documents. The experiments indicate that the performance of KNN-based classifier is better than that of Na  X   X ve Bayes in the task of patent classification. After that, Koster et al. [56] propose a new approach which employs the Winnow algo-rithm [33] to classify patent applications. The BOW-based model is utilized to represent patent documents. Based on their experiment result, they state that the accuracy of using full-text documents is much better than that of abstracts. The popularity of the BOW-based representation is origi-nated from its simplicity. However, it is often difficult to convey the relationships among terms by using the BOW-based model. To address this issue, Kim et al. [49] pro-pose a new approach to facilitate patent classification by introducing the semantic structural information. They pre-define six semantic tags, including technological field, pur-pose, method, claim, explanation and example. Given a patent document, they convert it to the new representation based on these semantic tags. They then calculate the sim-ilarity based on both the term frequency and the semantic tag. Finally, KNN-based model is exploited to automatically classify the Japanese patent documents. The proposed ap-proach achieves 74% improvement over the prior approaches in Japanese patent classification.
 It has been widely recognized that patent classification is difficult due to the complexly structure and professional cri-teria of the current patent classification schema. Hence, be-side exploiting the existing patent classification schema to categorize patent documents, some researchers explore the possibility of using other types of taxonomies to fulfill this task. For example, in [86], Pesenhofer et al. exploit a new taxonomy generated from Wikipedia to categorize patent documents. Cong et al. [66] design a TRIZ-based patent classification system in which TRIZ [4] is a widely used tech-nical problem solving theory. These systems provide flexible functionalities to allow users to search relevant patent doc-uments based on the applied taxonomy. Following the aforementioned efforts, researchers are also in-terested in exploring what types of classification algorithm can help improve the classification accuracy. For example, Fall et al [23; 24] compare the performance of different clas-sification algorithms in categorizing patent documents, in-cluding Na  X   X ve Bayes, Support Vector Machine (SVM), KNN, and Winnow. Besides, they also compare the effect of uti-lizing different parts of patent documents, such as titles, claims, and the first 300 words of the description. Their experiments have shown that SVM achieves the best perfor-mance for class-level patent document categorization, and it is the best way to use the first 300 words of the description for representing patent documents.
 As mentioned in Section 2, the IPC classification system is a five-level classification schema which contains more than 70,000 sub-groups in the bottom level. The fine-grained class label information renders patent classification more difficult. To alleviate this problem, Chen et al. [19] present a hybrid categorization system that contains three steps. Firstly, they train an SVM classifier to categorize patent documents to different sub-cla sses; they then train another SVM classifier to separate the documents to the bottom level of IPC; finally, they exploit KNN classification algorithms to assign the classification code to the given patent document based on the selected candidates. In their experiments, they compare various approaches employed in the sub-group level patent classification and show that their approach achieves the best performance.
 Besides the traditional classification models, hierarchical ap-proaches have also been explored, given the fact that the patent classification schema can naturally be represented as a taxonomy, as described in Section 2. For example, in [13], Cai and Hofmann present a novel hierarchical classification method that generalizes SVM. In their method, structured discriminant functions are used to mirror the class hierar-chy. All the parameters are learned jointly by optimizing a common objective function with respect to a regularized upper bound on the empirical loss. The experiments on the WIPO-alpha patent collection demonstrate the effectiveness of their method. Another hierarchical model involves [103], in which the taxonomy information is integrated into an on-line classifier. The results on the WIPO-alpha and Espace A/B patent collections show that the method outperforms other state-of-the-art approaches significantly.
 The complex structure of patent documents often prevents the analysts from quickly understanding the core idea of patents. To resolve this issue, it would be helpful to visu-alize patent documents in a way that the gist of patents can be clearly shown to the analysts, and the correlations between different patents can be easily identified. This is often referred to as patent visualization , an application of information visualization.
 As introduced in Section 1, a patent document contains dozens of items for analysis, which can be grouped into two categories: In the following, we will discuss how patent documents can be visualized using these two types of data, as well as the integration of them. For the purpose of analysis, structured data in patent doc-uments are often represented as graphs. The primary re-source used for constructing graphs is the citation infor-mation among different patents. By analyzing the citation graph, it is easy to discover interesting patterns with respect to particular patent documents. An example of patent cita-tion graphs is illustrated in Figure 7a. Along this direction, several research work has been published, in which graphs are used to model patent citations. For example, in [42], Huang et al. create a patent citation graph of high-tech elec-tronic companies in Taiwan between 1998 and 2000, where each point denotes an assignee, and the link between two points represents the relationship between them. They cat-egorize the companies into 6 major groups, and apply graph analysis to show the similarity and distinction between dif-ferent groups.
 Citation analysis has been the most frequently adopted tool in visualizing the relationships of patent documents. How-ever in some cases, it is difficult to capture the big picture of all the patent documents purely using a citation graph, as citations are insufficient to grasp the inner relations among patents. To alleviate this issue, Yoon and Park propose a network-based patent analysis method, in which the overall relationship among patents is represented as a visual net-work [123]. In addition, the proposed method takes more diverse keywords into account and produces more meaning-ful indices, which enable deeper analysis of patent docu-ments. Tang et al. [97] further extend this idea by con-structing a patent heterogeneous network, which involves a dynamic probabilistic model to characterize the topical evo-lution of patent documents within the network. Unstructured text in patent documents provides rich infor-mation of the core ideas of patents, and therefore it be-comes the primary resource for patent analysts to perform content analysis. Compared with the citation analysis, the content-based patent map has considerable advantages in latent information extraction and global technology visu-alization. It can also help reduce the burden of domain knowledge dependance. In the last decade, several visual-ization approaches have been proposed to explore the un-derlying patterns of patent documents and present them to users. For example, in [124], Yoon et al. present three types of patent maps, including technology vacuum map, claim point map, and technology portfolio map, all of which are generated from the unstructured text of patent documents. Figure 7b shows a patent landscape map. Similarly, Atsushi et al. [5] propose a technology portfolio map generated using the concept-based vector space model. In their model, they apply single value decomposition on the word co-occurrence matrix to obtain the word-concept matrix, and then exploit the concept-based vector to represent patent documents. To generate the patent landscape map, they employ the hierar-chical clustering method based on the calculated document-concept matrix. More recently, Lee et al. [61] present an approach to generating the technology vacuum map based on patent keyword vectors. They employ principal com-ponent analysis to reduce the space of keyword features to make suitable for use on a two-dimensional map, and then identify the  X  X echnology vacuum areas X  as the blank zones with sparse density and large size in the map. Unstructured text is useful for analyzing the core ideas of patents, and structure data provide evidences on the cor-relations of different patent documents. These two types of information are often integrated together for the pur-pose of visualization. As a representative work, Kim et al. [51] propose a novel visualization method based on both structured and unstructured data. Specifically, they first collect keywords from patent documents under a specific technology domain, and represent patent documents using keyword-based vectors. They then perform clustering on patent documents to generate k clusters. With the cluster-ing result, they form a semantic network of keywords, and then build up a patent map by rearranging each keyword node according to its earliest filing date and frequency in patent documents. Their approach not only describes the general picture of the targeted technology domain, but also presents the evolutionary process of the corresponding tech-niques. In addition, natural language prossing is utilized to facilitate patent map generation [125]. Compared with the traditional technology vacuum map purely built on patent content, this approach integrates bibliographic information of patent documents, such as assignee and file date, to con-struct the patent maps. The generated patent map is able to assist experts in understanding technological competition trends in the process of formulating R&amp;D strategies. Patent documents are the core of many technology orga-nizations and companies. To support decision making, it is imperative to assess the quality of patent documents for further actions. In practice, a common process of evaluating the importance/quality of patent documents is called patent valuation , which aims to assist internal decision making for patent protection strategies. For example, companies may create a collection of related patents, called patent portfo-lio [113], to form a  X  X uper-patent X  in order to increase the coverage of protection. In this case, a critical question is how to explore and evaluate the potential benefit of patent docu-ments so as to select the most important ones. To tackle this issue, researchers often resort to two types of approaches: unsupervised exploration and supervised evaluation .Inthe following, we discuss existing research publications related to patent valuation from these two perspectives. Unsupervised exploration on the importance of patent docu-ments is often oriented towards two aspects: influence power and technical strength . The former relies on the linkage be-tween patent documents, e.g., citations, whereas the latter mainly focus on the content analysis.
 Influence power : The first work of using citations to evalu-ate the influence power of patent documents involves [20]. In this work, a citation graph is constructed, where each node indicates a patent document, and nodes link to others based on their citation relations. The case study of semi-synthetic penicillin demonstrates the effectiveness of using citation counts in assessing the influence power of patents. In [3], Albert et al. further extend the idea of using citation counts, and prove the correctness of citation analysis to eval-uate patent documents. In addition, two related techniques are proposed, including the bibliographic coupling that in-dicates two patent documents share one or more citation, and co-citation analysis that indicates two patent documents have been cited by one or more patent documents. Based on these two techniques, Huang et al. [42] integrate the bibliographic coupling analysis and multidimensional scal-ing to assess the importance of patent documents. Further, ranking-based approaches can also be applied to the process of patent valuation. For example, Fujii [25] proposes the use of PageRank [12] to calculate citation-based score for patent documents.
 Technical strength : Unlike approaches that rely on the anal-ysis of the influence power of patent documents, some re-search publications focus on the analysis of the technical strength of inventions, which is relevant to the content of patents. For instance, Hasan et al. [36] define the tech-nical strength as claim originality, and exploit text min-ing approaches to analysis the novelty of patent documents. They use NLP techniques to extract the key phrases from the claims section of patent documents, and then calculate the originality score based on the extracted key phrases. This valuation method has been adopted by IBM, and is applied to various patent valuation scenarios; however, the term-based approaches suffer the problem of term ambigu-ity, which may deteriorate the rationality of the scores in some cases. To alleviate this issue, Hu et al. [41] exploit the topic model to represent the concept of the patents instead of using words or phrases. In additional, they state that tra-ditional patent valuation approaches cannot handle the case that the novelty of patents evolves over time, i.e., the novelty may decrease along time. Therefore, they exploit the time decay factor to capture the evolution of patent novelty. The experiment indicates that their proposed approach achieves the improvement compared with the baselines. The aforementioned approaches define the importance of patent documents from either content or citation links. In essence, they are unsupervised methods as the goal is to extract meaningful patterns to assess the value of patents purely based on the patent itself. In practice, besides these two types of resources, some other information may also be available to exploit. Some researchers introduce other types of patent related records, such as patent examination results [37], patent maintenance decisions [46], and court judgments [65], to generate predicated models to evaluate patent documents. For example, Hido et al. [37] create a learning model to estimate the patentability of patent appli-cations from the historical Japan patent examination data, and then use the model to predict the examination decision for new patent applications. They define the patentability prediction problem as a binary classification problem (re-ject or approval). In order to obtain an accuracy classifier, they exploit four types of features, including patent docu-ment structure, term frequency, syntactic complexity, and word age [36]. From their experiments, they demonstrate thesuperiorityoftheproposedmethodinestimatingthe examination decision. Jin et al. [46] construct a heteroge-neous information network from patent documents corpus, in which nodes could be inventors, classification codes, or patent documents and edges could denote the classification similarity, the citation relation or inventor cooperation, etc. Based on this heterogeneous network, they define interest-ing features, such as meta features, novelty features, and writing quality features, to created a patent quality model that is able to predict the value of patents and give the maintenance decision suggestion. Liu et al. [65] propose a graphical model that discovers the valid patents which have highly probability to achieve the victory during the patent litigation process. Based on the patent citation count and court judgments, they define a latent variable to estimate the quality of patent documents. They further incorpo-rate various quality-related features, e.g., citation quality, complexity, reported coverage, and claim originality, to im-prove the probabilistic model. The experiments indicate that their approach achieves promising performance for pre-dicting court decisions. Patent documents are quite sensitive to regions, i.e., patents from different regions might be described by different lan-guages. However in reality, patent analysts prefer to receive localized patent information, even if they are described by multiple languages. For example, a patent document is writ-ten by English, but an analyst from Spain expects that this patent can be translated to Spanish for better understand-ing. In addition, international patent documents are re-quired to be written by the language accepted worldwide, which is often referred to as patent globalization. In such cases, cross-language patent mining is needed to support patent localization/globalization.
 In the current stage of cross-language patent mining, the primary task is cross-language information retrieval, which enables us to retrieve information from other languages us-ing a query written in the language that we are familiar with. In general, a cross-language patent retrieval system can be constructed using two techniques: machine translation and semantic correspondence . In the following, we describe the details of these two techniques and discuss existing research efforts on this direction. A well-known technique to address cross-language retrieval is machine translation. By translating a query to the de-sired language, the problem can be reduced to a monolin-gual information retrieval task that various approaches can be employed. Popular machine translation systems, such as Google Translate 10 , Bing Translator 11 , and Cross Lan-guage 12 , have been widely exploited in tackling the prob-lem of cross-language patent retrieval [18; 48; 70; 77]. The NTCIR Workshop holds a machine translation track to en-courage researchers to practice the cross-lingual patent re-trieval task [27]. In [77], Makita et al. present a multilin-gual patent retrieval system based on the method proposed in [26], which employs a probabilistic model to reduce the ambiguity of query translation. As indicated in the report of NTCIR9 Patent Machine Translation task [32], several participants propose word-based and phrase-based transla-tion approaches by exploiting Moses [53], an open source http://translate.google.com. http://www.bing.com/translator. http://www.crosslanguage.co.jp. toolkit for statistical machine translation. Their experi-ments demonstrate that lexicon-based approaches are able to achieve acceptable performance; however, the domain-specific terms and structural sentences of patent documents are difficult to translate. Hence, it is imperative to explore the syntactic structure of patents when performing patent document translation. An alternative way of building a cross-language patent search engine is to explore the semantic correspondence among lan-guages. The basic idea is to first construct the semantic re-lations of a pair of languages, and then interpret the query to another language. In [64], Littman et al. present a novel approach which creates a cross-language space by exploiting latent semantic indexing(LSI) in cross-language information retrieval domain. Base on the research of [64], Li et al. [62] propose a new approach to retrieve patent documents in the Japanese-English collection. They introduce the method of kernel canonical correlation analysis [110] to build a cross-language sematic space from Japanese-English patent doc-uments. The empirical evaluation shows that the proposed method achieves significant improvement over the state-of-the-art. However, it may require a lot of efforts to build a cross-language semantic space, and also the performance of this type of approaches is restricted by the quality of the semantic space. Patent mining aims to assist patent analysts in efficiently and effectively managing huge volume of patent documents. It is essentially an application-driven area that has been extensively explored in both academia and industry. There are a lot of online patent mining systems, either with free access or having commercial purposes. Table 3 lists several representative systems that provide flexible functionalities of patent retrieval and patent analysis (Part of the content is obtained from Intellogist 13 ).
 Patent mining systems, e.g., Google Patent 14 , Baidu Patent and FreePatentOnine 16 , provide free access and basic re-trieval functionalities and are very easy to use for the major-ity. In addition, a list of patent authorities, e.g., USPTO EPO 18 ,WIPO 19 , provide advanced search functions to allow professional users to input more complex patent queries for high-recall retrieval. These authority-based systems usually require more human efforts and domain expertise.
 Some leading companies, e.g., Thomson Reuters, Questel, and Lexisnesxis, offer commercial patent mining systems. Compared with the systems with free access, commercial systems provide more advanced features to assist analysts in retrieval and processing patent documents. These com-mercial systems often have: http://www.intellogist.com. https://www.google.com/?tbm=pts. http://zhuanli.baidu.com. http://www.freepatentsonline.com. http://www.uspto.gov. http://www.epo.org. http://www.wipo.int.
 Recently, several patent mining systems have been proposed in academia, most of which are constructed by utilizing the available online resources. For example, PatentSearcher [40] leverages the domain semantics to improve the quality of dis-covery and ranking. The system uses more patent fields, such as abstract, claims, descriptions and images, to re-trieve and rank patents. PatentLight [14] is an extension of PatentSearcher , which categorizes the search results by virtue of the tags of the XML-structure, and ranks the re-sults by considering flexible constraints on both structure and content. Another representative system is called Patent-Miner [97], which studies the problem of dynamic topic modeling of patent documents and provides the topic-level competition analysis. Such analysis can help patent ana-lysts identify the existing or potential competitors in the same topic. Further, there are some mining systems focus-ing on patent image search. For instance, PATExpert [115] presents a semantic multimedia content representation for patent documents based on semantic web technologies. Pat-Media [112] provides patent image retrieval functionalities in content-based manner. The visual similarity is realized by comparing visual descriptors extracted from patent images. In this survey, we comprehensively investigated several tech-nical issues in the field of patent mining, including patent search, patent categorization, patent visualization, and patent evaluation. For each issue, we summarize the correspond-ing technical challenges exposed in real-world applications, and explore different solutions to them from existing publi-cations. We also introduce various patent mining systems, and discuss how the techniques are applied to these sys-tems for efficient and effective patent mining. In summary, this survey provides an overview on existing patent mining techniques, and also sheds light on specific application tasks related to patent mining.
 With the increasing volume of patent documents, a lot of application-oriented issues are emerging in the domain of patent mining. In the following, we identify a list of chal-lenges in this domain with respect to several mining tasks. Some challenges, such as the scalability and classification issues, are imperative to solve in order to assist patent ana-lysts in efficiently and effectively performing patent analysis tasks. Other challenges can stimulate the emergence of new types of patent-oriented applications, such as evolutionary analysis and drawing-based retrieval. Even though it is im-possible to describe all algorithms and applications in detail for patent mining, we believe that the ideas and challenges discussed in this survey should give readers a big picture of this field and several interesting directions for future studies. The work was supported in part by the National Science Foundation under grants D BI-0850203, HRD-0833093, CNS-1126619, and IIS-1213026, the U.S. Department of Home-land Security unde r grant Award Number 2010-ST-06200039, and Army Research Office under grant nu mber W911NF-1010366 and W911NF-12-1-0431. [1] S. Adams. Comparing the ipc and the us classification [2] B. Al-Shboul and S. Myaeng. Query phrase expansion [3] M. Albert, D. Avery, F. Narin, and P. McAllister. Di-[4] G. S. Alt X  suller. The innovation algorithm: TRIZ, sys-[5] H. Atsushi and T. YUKAWA. Patent map generation [6] L. Azzopardi, W. Vanderbauwhede, and H. Joho.
 [7] R. Bache and L. Azzopardi. Improving access to large [8] S. Bashir and A. Rauber. Improving retrievability [9] S. Bashir and A. Rauber. Improving retrievability of [10] S. Bhatia, B. He, Q. He, and S. Spangler. A scalable [11] F. Borgonovi. Divided we stand, united we fall: Re-[12] S. Brin and L. Page. The anatomy of a large-scale [13] L. Cai and T. Hofmann. Hierarchical document cate-[14] S. Calegari, E. Panzeri, and G. Pasi. Patentlight: a [15] R. S. Campbell. Patent trends as a technological fore-[16] M. Carrier. A roadmap to the smartphone patent wars [17] S. Cetintas and L. Si. Effective query generation and [18] M. Chechev, M. Gonz` alez, L. M` arquez, and [19] Y. Chen and Y. Chang. A three-phase method for [20] P. Ellis, G. He pburn, and C. Oppenhein. Studies on [21] P.  X  Erdi, K. Makovi, Z. Somogyv  X  ari, K. Strandburg, [22] J. Erstling and I. Boutillon. Patent cooperation treaty: [23] C. Fall, A. Torcsvari, K. Benzineb, and G. Karetka. [24] C. Fall, A. T  X  orcsv  X  ari, P. Fievet, and G. Karetka. Auto-[25] A. Fujii. Enhancing patent retrieval by citation anal-[26] A. Fujii and T. Ishikawa. Japanese/english cross-[27] A. Fujii, M. Utiyama, M. Yamamoto, and T. Ut-[28] S. Fujita. Technology survey and invalidity search: [29] D. Ganguly, J. Leveling, and G. Jones. United we fall, [30] D. Ganguly, J. Leveling, W. Magdy, and G. Jones. [31] J. Gobeill, A. Gaudinat, P. Ruch, E. Pasche, [32] I. Goto, B. Lu, K. P. Chow, E. Sumita, and B. K. [33] A. Grove, N. Littlestone, and D. Schuurmans. General [34] H. Gurulingappa, B. M  X  uller, R. Klinger, H.-T. Mevis-[35] C. G. Harris, R. Arens, and P. Srinivasan. Comparison [36] M. A. Hasan, W. S. Spangler, T. Griffin, and A. Alba. [37] S. Hido, S. Suzuki, R. Nishiyama, T. Imamichi, [38] D. Hiemstra, S. Robertson, and H. Zaragoza. Parsi-[39] Q. Honghua and Y. Xiang. Research on a method for [40] V. Hristidis, E. Ruiz, A. Hern  X  andez, F. Farf  X  an, and [41] P. Hu, M. Huang, P. Xu, W. Li, A. K. Usadi, and [42] M. Huang, L. Chiang, and D. Chen. Constructing a [43] H. Itoh. Ntcir-4 patent retrieval experiments at ricoh. [44] M. Iwayama, A. Fujii, N. Kando, and A. Takano. [45] A. Jaffe and M. Trajtenberg. Patents, citations, and [46] X. Jin, S. Spangler, Y. Chen, K. Cai, R. Ma, L. Zhang, [47] Y. Jin. A hybrid-strategy method combining semantic [48] C. Jochim, C. Lioma, H. Sch  X  utze, S. Koch, and [49] J. Kim and K. Choi. Patent document categorization [50] Y. Kim, J. Seo, and W. Croft. Automatic boolean [51] Y. Kim, J. Suh, and S. Park. Visualization of patent [52] K. Kishida. Experiment on pseudo relevance feedback [53] P. Koehn, H. Hoang, A. Birch, C. Callison-Burch, [54] S. Kondo, M. Komachi, Y. Matsumoto, K. Sudoh, [55] K. Konishi. Query terms extraction from patent doc-[56] C. Koster, M. Seutter, and J. Beney. Multi-[57] A. Krishnan, A. F. Cardenas, and D. Springer. Search [58] L. Larkey. Some issues in the automatic classification [59] L. Larkey. A patent search and classification system. [60] C. Lee, Y. Cho, H. Seol, and Y. Park. A stochastic [61] S. Lee, B. Yoon, and Y. Park. An approach to discov-[62] Y. Li and J. Shawe-Taylor. Advanced learning algo-[63] Y.-R. Li. The technological roadmap of cisco X  X  busi-[64] M. Littman, S. Dumais, T. Landauer, et al. Auto-[65] Y. Liu, P. Hseuh, R. Lawrence, S. Meliksetian, C. Per-[66] H. T. Loh, C. He, and L. Shen. Automatic classifica-[67] M. Lupu, F. Piroi, and A. Hanbury. Aspects and anal-[68] W. Magdy and G. Jones. Applying the kiss principle [69] W. Magdy and G. Jones. A study on query expansion [70] W. Magdy and G. J. Jones. An efficient method for us-[71] W. Magdy, J. Leveling, and G. J. Jones. Exploring [72] W. Magdy, P. Lopez, and G. Jones. Simple vs. so-[73] P. Mahdabi, L. Andersson, M. Keikha, and [74] P. Mahdabi and F. Crestani. Learning-based pseudo-[75] P. Mahdabi, S. Gerani, J. X. Huang, and F. Crestani. [76] P. Mahdabi, M. Keikha, S. Gerani, M. Landoni, and [77] M. Makita, S. Higuchi, A. Fujii, and T. Ishikawa. [78] C. Manning, P. Raghavan, and H. Sch  X  utze. Introduc-[79] E. Meij, W. Weerkamp, and M. de Rijke. A query [80] H.-C. Meng. Innovation cluster as the national com-[81] A. Messeni Petruzzelli, D. Rotolo, and V. Albino. De-[82] S. Mukherjea and B. Bamba. Biopatentminer: an in-[83] K.-L. Nguyen and S.-H. Myaeng. Query enhancement [84] S. Oh, Z. Lei, P. Mitra, and J. Yen. Evaluating and [85] K. OuYang and C. Weng. A new comprehensive patent [86] A. Pesenhofer, S. Edler, H. Berger, and M. Ditten-[87] F. Piroi and J. Tait. Clef-ip 2010: Retrieval experi-[88]I.J.Rotkin,K.J.Dood,andM.A.Thexton. Ahis-[89] G. Salton. The SMART retrieval system  X  experiments [90] S. Sheremetyeva. Natural language analysis of patent [91] A. Shinmori, M. Okumura, Y. Marukawa, and [92] B. Stein, D. Hoppe, and T. Gollub. The impact of [93] C. Sternitzke, A. Bartkowski, and R. Schramm. Vi-[94] J. H. Suh and S. C. Park. Service-oriented technology [95] T. Takaki, A. Fujii, and T. Ishikawa. Associative doc-[96] H. Takeuchi, N. Uramoto, and K. Takeda. Exper-[97] J. Tang, B. Wang, Y. Yang, P. Hu, Y. Zhao, X. Yan, [98] W. Tannebaum and A. Rauber. Acquiring lexical [99] W. Tannebaum and A. Rauber. Analyzing query logs [100] W. Tannebaum and A. Rauber. Mining query logs [101] D. Teodoro, J. Gobeill, E. Pasche, D. Vishnyakova, [102] E. Terumasa. Rule based machine translation com-[103] D. Tikk, G. Bir  X  o, and A. T  X  orcsv  X ari. A hierarchical [104] A. J. Trappey, C. Y. Fan, C. Trappey, Y.-L. Lin, [105] Y. Tseng et al. Text mining for patent map analysis. In [106] Y. Tseng, C. Lin, and Y. Lin. Text mining techniques [107] Y. Tseng, C. Tsai, and D. Juang. Invalidity search for [108] Y. Tseng and Y. Wu. A study of search tactics for [109] N. Van Zeebroeck. The puzzle of patent value indica-[110] A. Vinokourov, J. Shawe-Taylor, and N. Cristianini. [111] I. Von Wartburg, T. Teichert, and K. Rost. Inventive [112] S. Vrochidis, A. Moumtzidou, G. Ypma, and I. Kom-[113] R. P. Wagner and G. Parchomovsky. Patent portfolios. [114] J. Wang and D. W. Oard. Combining bidirectional [115] L. Wanner, S. Br  X  ugmann, B. Diallo, M. Giereth, [116] T. Xiao, F. Cao, T. Li, G. Song, K. Zhou, J. Zhu, [117] J. Xu and W. B. Croft. Query expansion using lo-[118] X. Xue and W. Croft. Automatic query generation for [119] X. Xue and W. Croft. Transforming patents into prior-[120] Y. Yang, L. Akers, T. Klose, and C. Barcelon Yang. [121] Y. Y. Yang, L. Akers, C. B. Yang, T. Klose, [122] T. Yeap, G. Loo, and S. Pang. Computational patent [123] B. Yoon and Y. Park. A text-mining-based patent net-[124] B.-U. Yoon, C.-B. Yoon, and Y.-T. Park. On the de-[125] J. Yoon, H. Park, and K. Kim. Identifying technologi-[126] L. Zhang, L. Li, T. Li, and Q. Zhang. Patentline: [127] L. Zhang and T. Li. Data mining applications in  X  NTCIR Patent Data 20 : This data set is provided by  X  WIPO Patent Data 21 : This patent collection is created  X  MAREC Patent Data 22 : MAtrixware REsearch Col- X  ESPACE EP Patent Data 23 :ESPACEEPiscreated http://research.nii.ac.jp/ntcir/permission/ntcir-8/perm-en-PATMN.html. http://www.wipo.int/classifications/ipc/en/ITsupport/ Categorization/dataset/index.html. http://www.ir-facility.org/prototypes/marec. http://www.epo.org/searching/subscription/ep.html. } This paper presents a general formalism for Recommender Systems based on Social Network Analysis. After intro-ducing the classical categories of recommender systems, we present our Social Filtering formalism and show that it ex-tends association rules, classical Collaborative Filtering and Social Recommendation, while providing additional possibil-ities. This allows us to survey the literature and illustrate the versatility of our approach on various publicly available datasets, comparing our results with the literature. Recommender systems; Social Network Analysis; Collabo-rative Filtering; Social Recommenders. Recommender Systems (RSs) help users or groups of users deal with information overload by proposing to them items suited to their interests. The history of RSs started in the late 1990s with work by the GroupLens team at Univer-sity of Minnesota [18] to recommend news, and by Movie-Lens in 1996 to recommend movies, which demonstrated that automated recommendations were very well received by users. Then Amazon, which had been incorporated in 1994, published its patent in 2001 and has been serving recom-mendations ever since, acting as a de facto reference show-case for the efficiency of RSs [33 ]. The Netflix competition (2006-2009) attracted over 41,000 participating teams [6] and turned RS into a hot topic among researchers.
 The first papers on collaborative filtering showed how to use the opinions of similar users to recommend items to the active user [1]. Since then, research in RSs has become very active (see for example a recent RS survey including more than 250 references [8]) and RSs have been successfully used in many industry sectors to recommend items: movies (Netflix [6], MovieLens [41]), products (Amazon.com [33], La Bo X  X te `a Outils [47]), songs [3], jobs to Facebook users (Work4Labs.com [16]), books, friends, banners or content on a social site (Skyrock.com [44]) etc.
 RSs exploit various sources of information: about users (their demographics), about products (their features) and about user interactions with the products [8; 25], either explicit (rating, satisfaction) or implicit (product purchased, book read, song heard, content clicked etc.) More recently, So-cial networks and social media (blogs, social tagging sites, etc) have emerged and become very active. A social site will allow users to construct profiles (public or semi-public), to share connections with other users and to view and tra-verse lists of connections made by others in the system [10]. Authors [8; 51; 56; 57] have thus proposed to also include information from social media (Facebook, Twitter, ...) be-cause social relations obviously influence users X  behaviors. There are two visions for social recommendation [56]: RSs implementations are based on various techniques [1]: content-based, collaborative filtering (both memory-and model-based), hybrid or social [56]. Performances are eval-uated through various criteria [52].
 In this paper, we propose a Social Filtering formalism (SF), based on Social Network Analysis (SNA), which allows us to describe, within the same formalism, both association rules, traditional Collaborative Filtering (CF) and Social Recom-mendation (SR), while providing additional ways to imple-ment a RS, thus producing novel RSs.
 The paper can thus be read as a survey on RSs, with ex-periments illustrating the various RSs presented. We have not tried, in this paper, to optimize hyper-parameters, but rather intended to present a wide repertoire of RSs tested in a uniform setting to allow for comparisons which are usually hard to make since, in the literature, each paper has its own settings and hyper-parameters choices.
 The paper is organized as follows: in section 2, we introduce general concepts and notations, and we review traditional techniques in section 3. In section 4, we introduce our So-cial Filtering formalism and show in section 5 how it relates to conventional approaches. In section 6, we introduce eval-uation metrics and various representative datasets. In sec-tion 7, we present extensive experimental results to illustrate the various RSs presented in the paper: we reproduce known results from the literature, and add new results, showing the benefits brought by our unifying formalism. Our conclusion identifies remaining issues and perspectives in section 8. RSs use available data to generate lists of recommendations. Depending on the application, data can involve: Usage and rating data can be represented by an interaction (or preferences ) matrix R which encodes the actions of users on items. Table 1 below, for example, shows 4 users who rated 5 movies (ratings are shown by numbers). In the case where the user X  X  interaction is a purchase or a click, matrix R is binary, with 1 indicating the item was purchased and 0 it was not. In cases where repeated consumption is possible, values in matrix R indicate the number of times the item was consumed, thus leading to R having the same structure as for ratings. In the following, we will use the word consume to indifferently mean rate, purchase or click.
 It should be noted that collecting implicit user X  X  behavior is usually easier than requiring the user to provide explicit feedback or provide access to his / her social network. Most users purchase but very few items and rate even less: as few as 1% of users who consume an item might also rate it. As a result, matrix R is often very sparse, even more so in the case of ratings.
 We will denote by L (resp. C ) the total number of users or lines in R (resp. total number of items or columns). Matrix R is thus of dimensions L x C . Usually, matrix R is very large: a few millions  X  a few tens-hundreds of thousands. RSs have been studied for more than 15 years now [1; 8]. Traditional techniques are grouped into content-based, Col-laborative Filtering, hybrid, and, more recently, social [8], which we describe in this section. Content-based RS [1; 35; 46] use items (or users) descrip-tions to define items X  (or users) profiles. A user is then recommended items which profiles best match the items he best rated in the past, or items which users with most sim-ilar profiles best rated in the past. Sometimes, users only provide descriptions (instead of ratings), in this case items X  profiles are constructed using these descriptions [16]. To im-plement Content-based RSs, we need a similarity measure (among items or users) and profiles comparison (for example k -nearest neighbors). Collaborative Filtering is certainly the most widely used technique for implementing RSs. There exist two main groups of CF techniques: memory-based (or neighborhood meth-ods [1]), and model-based (or latent factor models [42]). As stated earlier in the introduction, CF methods use the opinion of a group of similar users to recommend items to the active user [1; 27; 32; 42; 54]. According to [42], the two key assumptions behind these systems are: In the literature, there exist two main groups of CF tech-niques [59]: model-based or latent factor models [42; 34; 2] and memory-based or neighborhood methods [1; 32]. Model-based RSs [11; 14; 29; 54; 60; 61] estimate a global model, through machine learning techniques, to produce un-known ratings. This leads to models that neatly fit data and therefore to RSs with good quality. However, learning a mo-del may require lots of training data which could be an issue in some applications. In the literature many model-based CF systems have been proposed: One of the most efficient and best used model-based meth-ods is matrix factorization [42; 59] in which users and items are represented in a low-dimensional latent factors space. The new representations of users (  X  U ) and items (  X  I ) are com-monly computed by minimizing the regularized squared er-ror [59]: where  X  1 and  X  2 are regularization parameters, r u,i is the rating that user u gave to item i ,  X  v u and  X  v i are the new rep-resentations of user u and item i respectively,  X  U and  X  the new representation sets of the sets of users and items, re-spectively. Once the new representations of users and items  X  v u and  X  v i have been computed, we can obtain the predicted rating  X  r u,i as follows: Matrix factorization methods can be generalized to proba-bilistic models called Probabilistic Matrix Factorization [42; 50; 59]. These techniques are more suited to explicit feed-back cases. They usually produce very good results but suffer from extremely large sizes of matrix R [3]. Memory-based CF techniques rely on the notion of similarity between users or items to build neighborhoods methods. If a is the active user for whom we seek recommendations, u another user and i and j two items, we will denote: http://www.netflixprize.com The similarity between users a and u can be defined through many similarity measures, for example cosine, Pearson cor-relation coefficient (PCC) [1] or asymmetric cosine [3] simi-larities (equations (3), (4) &amp; (5) below respectively):
Sim( a, u ) = PCC h Note that, in the binary case, asymmetric cosine for  X  = 1 is equivalent to cosine similarity. The similarity between items i and j can be defined in the same fashion: cosine, Pearson correlation coefficient or asymmetric cosine (equa-tions (6), (7) &amp; (8) below respectively): It should be noted that both users and items similarity mea-sures above take into account the actions on all items (resp. of all users): this is why these measures are called collabo-rative . CF techniques produce, for an active user a , a list of recom-mended items ranked through a scoring function (or aggre-gation function), which takes into account either users most similar to a (user-based CF) or items most similar to those consumed by a (item-based CF).
 Let us thus denote K ( a ) the neighborhood of a and V ( i ) the neighborhood of item i . These neighborhoods can be defined in many ways (for example, N -nearest neighbors, for some given N , or neighbors with similarity larger than a given threshold, using user/item similarity).
 The score functions are then defined for users and items as: where various functions f and g can be used [1]: Another mechanism has been developed [3] to produce local-ity instead of explicitly defining neighborhoods. Functions f and g are defined so as to put more emphasis on high similarities (with high q , q 0 ): For q = 0, this is equivalent to average rating, and for q = 1, this is similar to weighted average rating.
 We then rank items i by decreasing scores and retain the top k items ( i a 1 ,i a 2 ,...,i a k ) which are recommended to a , such that: Notice that while memory-based techniques produce ranked lists of items, model-based techniques predict ratings, through a score which can be used also to rank recommendations. In practice, all CF systems suffer from several drawbacks: The representation of users and items in a low dimensional space in latent factor models mitigates the cold start recom-mendation problem but raises a scalability issue. In general, latent factor methods are known to generally yield better re-sults than neighborhood methods [28; 42; 59]. Traditional RSs, and in particular model-based systems, rely on the (often implicit) assumption that users are indepen-dent, identically distributed (i.i.d). The same holds for it-ems. However, this is not the case on social networks where users enjoy rich relationships with other members on the net-work. It has long been observed in sociology [40] that users X   X  X riends X  on such networks have similar taste (homophily). It is thus natural that new techniques [65] extended previous RSs by making use of social network structures. However, it was realized that the type of interaction taken into account could have a dramatic impact on the quality of the obtained social recommender [65]. In this section, we review three families of social recommender: one based on explicit social links, one based on trust and an emerging family based on implicit links. In this section, we assume that users are connected through explicit relationships such as friend, follower etc. Unsur-prisingly, with the recent thrive of online social networks, it has been found that users prefer recommendations made by their friends than those provided by online RSs, which use anonymous people similar to them [53]. Most Social RSs are based on CF methods: social collaborative recommenders, like traditional CF systems, can be divided into two fami-lies: memory-based and model-based systems. Memory-based methods in social recommendation are sim-ilar to those in CF (presented in section 3.2.2 ), the only difference being the use of explicit social relationship for computing similarities. http://www.essembly.com Model-based methods in social recommenders represent users and items into a latent space vector (as described in sec-tion 3.2.1 ) making sure that users X  latent vectors are close to those of their friends. http://www.yelp.com Finally, a few social RSs combine social and content-based techniques. For example, [17 ] proposes two ways to aggre-gate users X  preferences with those of their friends: enrich users X  profiles with those of their friends or aggregate users X  recommendation scores with those of their social relation-ships. As explained in [38]  X  X rust relationships X  are different from  X  X ocial relationships X  in many respects. Trust-aware RSs are based on the assumption that users have taste similar to other users they trust, while in social RSs, some of the active user X  X  friends may have totally different tastes from him [38]. This was also observed in [65], with the differences between friends and allies, which represents a case where trust is ex-plicitly provided by users.
 In everyday life, people may ask other people (friends, rel-atives, someone they trust) for a recommendation. If the person cannot provide sufficient information, she may indi-cate another person whom she knows which could, and so on. The notion of trust network arises naturally: one tend to have faith in the opinion of people trusted by the peo-ple he trusts himself, transitively. Conversely, the notion of social influence has long been used in marketing, relying on the assumption that users are likely to make choices simi-lar to their role-models [49]. The notion of influence can be seen as close to that of trust: when providing a friend with a referral, a trusted user influences her friend. It has long been known that this  X  X ord-of-mouth effect X  can be used commercially, such as for example in viral marketing. Recently, it was attempted to incorporate trust or influence knowledge into RSs. Beyond the mere expected increase in efficiency, computing trust may also alleviate recurrent prob-lems of traditional RSs, such as data sparsity, cold start or shilling attacks (fake profile injections) to bias recommen-dations. The trust relationship is directional, i.e. the fact that user u 1 trusts user u 2 at some level t does not necessarily mean that u 2 trusts u 1 at the same or another level. Trust can be represented by a binary value, 0 for  X  X ot trusted user X  and 1 for  X  X rusted user X , or through more gradual scales [20; 21; 39] or even with a probabilistic approach [15; 48]. Some models include an explicit notion of distrust [21; 67], but most of them ignore it.
 For RSs, trust is computed over an explicit social network to increase the information available to generate recommen-dations. There exist two cases in the literature: either trust is provided explicitly in a trust network, or it has to be in-ferred.
 In an explicit trust network, we propagate and aggregate trust to infer long chains of trust [ 67; 20]. Trust compu-tation also requires an aggregation strategy, to combine es-timates obtained from different paths from one user to an-other. Several operators may be used like minimum, maxi-mum, (un)weighted sum and average. Different strategies may also be applied: propagate trust first, then aggregate; or aggregate first, then propagate (the latter allowing easier distributed computation).
 In non-explicit trust networks, trust has to be inferred. For example, in [45], the author defines a profile and item-level trust, based on correct previous recommendations. In explicit trust networks, users provide the system with trust statements for their peers, be it on a gradual scale (Moleskiing [5]), allies (Essembly [65]) or lists of trusted and non-trusted people (Epinions [39]). Then, for the rec-ommendation to a specific user u , trust is estimated between u and the relevant users, in order to weigh the recommenda-tion computation, either through trust-based weighted mean (rating from user u for item i is an ordinary weighted mean of the ratings of users which have evaluated i , where the weights are the trust estimates for these users) or Trust-based CF (as in classical CF methods, replacing similarity-based weights by trust-based weights obtained via propaga-tion and aggregation strategies as described above). Recently, a new type of social RSs has been introduced which rely not upon an explicit social network (as in sec-tion 3.3.1 ) but upon networks which can be derived from users X  behaviors and have thus been named implicit net-works . Users will be  X  implicitly  X  connected if, for exam-ple, they take pictures in the same locations [23], they attend the same events or click on the same ads [44]. The implicit users X  social network can then be used, as in section 3.3.1 ) to build recommendations.
 For example, [57] extracts from cooking recipes bipartite graph ( recipes , ingredients ) and sets the weight of the link in the ingredients network as the point-wise mutual informa-tion of the ingredients, extremities of the link. Authors then apply a discriminative machine learning method (stochas-tic gradient boosting trees), using features extracted from the ingredients network, to predict recipe ratings and re-commend recipes. Results show that the structural features extracted from the ingredient networks are most critical for performances.
 Similar approaches have been developed for RSs where no ratings are provided, but only information on whether ob-jects were collected (product purchased, banner clicked, movie watched, song listened to). Social RSs are still relatively new. There is a lot of active research in this area and it should be expected that new re-sults will extend the field of traditional systems to incorpo-rate social information of all sorts. In particular, the field of social recommenders built on implicit social networks seems particularly promising and we will now dig deeper in this direction to produce our Social Filtering formalism. Our Social Filtering formalism (SF) is based upon a bipartite graph and its projections (see [22; 66] for a discus-sion of bipartite graphs). A bipartite graph is defined over a set of nodes separated into two non-overlapping subsets: for example, users and items, items and their features, etc. A link can only be established between nodes in different sets: a link connects a user to the items she has consumed. The bipartite network is then projected into two (unipar-tite) networks, one for each set of nodes: a Users X  and an Items X  networks. In the projection (see Figure 1), two nodes are connected if they had common neighbors in the bipartite graph. The link weight can be used to indicate the number of shared neighbors. For example, two users are linked if they have consumed at least one item in common (we usu-ally impose a more stringent condition: at least K items). The projected networks can thus be viewed as the network of users consuming at least K same items (users having the same preferences) and the network of items consumed by at least K 0 same users (items liked by the same people). Projected networks can then be used to define neighbor-hoods [55] or recommendation algorithms which perform better than conventional CF on the MovieLens dataset [66]. This generic formalism extends these early contributions: we are able to reproduce results from various classical ap-proaches, and we also provide new approaches, allowing more flexibility and potential for improved performances, depending on the dataset.
 In the SF formalism, as in traditional CF, we build rec-ommendations by defining neighborhoods and scoring func-tions. In the case of implicit feedback (binary interaction matrix R ), in essence, the link between two users a and u (resp. i and j ) represents an association rule a  X  u (resp. i  X  j ) with the link weight proportional to the rule support, where support of rule a  X  u (resp. i  X  j ) is defined as: Supp( a  X  u ) = # Items cons. by a and u
Supp( i  X  j ) = # Users who cons. i and j In the case of a non-binary matrix R (ratings), support is similarly defined. Hence, Support is defined in general as: Support is similar to cosine similarity (equations (3) and (6)), so that we can use support as a similarity measure. We will define support-based similarity of users (resp. items) as: In the case of implicit feedback, the confidence of link a  X  u (resp. i  X  j ) is defined, as for association rules, by: Conf( a  X  u ) = # Items cons. by a and u
Conf( i  X  j ) = # Users who cons. i and j For ratings (non-binary matrix R ), confidence will be simi-larly defined. Hence, Confidence is defined in general as: This is again similar to cosine similarity. We can thus also use confidence as a similarity measure. Confidence-based similarity of users (resp. items) is defined as: We might want to define similarity between a and u (resp. i and j ) from both a  X  u and u  X  a links, which is not the case for confidence. Following [3], we thus define the follow-ing Asymmetric Confidence-based Similarity of users/items, where  X  is a parameter to be tuned by cross-validation: This measure is identical to asymmetric cosine, generalizes confidence similarity of link a  X  u (for  X  = 0) and of link u  X  a (for  X  = 1) and, in the case where matrix R is binary, cosine similarity as well (for  X  = 0 . 5). Jaccard Index [36] measures the similarity of lists by count-ing how many elements they have in common. The Jaccard Index of users a and u (resp. items i and j ) is defined (in the binary case) as: According to the definition of the Jaccard index, we thus have for users (and similarly for items): As above, Jaccard index will be similarly defined if matrix R is not binary. Hence, Jaccard index for users / items is defined as: We then define the Jaccard-based Similarity of users / items as: Now that we have defined various similarity measures, we can define the neighborhood K ( a ) of user a (resp. V ( i ) of item i ): we only give the definition for users, items are similar) in any of the following ways (all users or only the Top N or only those with similarity measure larger than a threshold can be chosen): These last two cases are completely novel ways to define neighborhoods: they exploit the homophily expected in so-cial networks (even implicit as here), where users with the same behavior tend to connect and be part of the same com-munity. In CF, users in K ( a ) (with cosine similarity) are such that they have consumed at least one item in common (otherwise their cosine would be 0); in the SF setting such users would be linked in the Users graph ( K  X  1). In the above community-based definitions of K ( a ), users in K ( a ) might not be directly connected to active user a . These definitions thus embody some notion of paths linking users, through common usage patterns. We now define scoring functions as for Collaborative filtering (equations (10), (11) and (12) above) with the various social similarity measures (equations (15), (17), (18) and (20)) and the various neighborhoods we have defined. As in equation (12), scores with locality parameters q and q can be used with any of the similarity measures defined above.
 We then rank-order items i by decreasing scores as in equa-tion (13) and retain the top k ( i a 1 ,i a 2 ,...,i a k ) recommended to a . To implement the SF framework, we need to build the bi-partite network and project it to unipartite networks, af-ter choosing adequate parameters K and K 0 (Figure 1) and eliminating mega-hubs if necessary [44]. Then, depending upon the choice of similarity measure (equations (15), (17), (18) and (19)), neighborhoods and score function (equa-tions (10), (11) or (12)) we obtain a RS which is equivalent to one of the following classical recommender systems: As can be seen above, the SF formalism generalizes various well established recommendation techniques. However, it offers new possibilities as well: the Social Filtering formal-ism thus extends content-based, association rules and CF, with new similarity measures and new ways to define neigh-borhoods.
 By only building once one bipartite graph and the pro-jected unipartite graphs, we have at our disposal, in a unique framework, a full set of similarity measures, neighborhood definitions and scoring functions; thus allowing us to pro-duce many different RSs, evaluate their performances and select the best.
 We will illustrate in section 7 performances on various stan-dard datasets, comparing our SF formalism to conventional techniques and showing original results produced within our SF framework. A RS can be asked either to predict a rating (in the case of explicit feedback) or to rank items (in the case of implicit feedback). The two visions are quite different and globally correspond to model-based and memory-based approaches in CF respectively [1]. Performance metrics differ in these two cases [52].
 Predictive case : we want to evaluate how close predicted rating  X  r ij is to actual rating r ij . We thus use classical per-formance metrics from machine learning: where I is the total number of tested ratings.
 Ranking case : in that case, we want to evaluate whether recommended items were adequate for the user; for exam-ple, recommended items were later consumed. We thus have a Target set for each user which represents the set of it-ems he consumed after being recommended. This can be implemented by splitting the available dataset into Train-ing / Testing subsets (taking into account time stamps if available). In this case, metrics are those classically used in information retrieval: In practical situations, we are also interested in more qual-itative indicators showing whether all users (resp. items) get recommendations (resp. are recommended) or which it-ems are recommended: as we will see, some RSs might be better on performance indicators and poorer on these qual-itative indicators and it will be the user X  X  choice to trade performance decrease for qualitative indicators increase. http://kaggle.com/c/msdchallenge Many more indicators are described in [52], but we will only use these for the experiments described in section 7. To demonstrate the generality of our framework, we present results on various datasets traditionally used in the litera-ture. These datasets, shown in Table 2 are characterized by the number of users, items, preferences ( implicit shown by a count of usage or explicit shown by ratings) and, in some cases, existing explicit social relationships.
 Some performance results of RSs on these datasets are al-ready available in the literature. See for example: We have implemented our SF formalism on the various data-sets presented above. The goal of these experiments is not to demonstrate that our formalism produces better results than other RSs: this would have required systematic runs of multiple splits and optimization of parameter choices; http://files.grouplens.org/datasets/hetrec2011/ hetrec2011-lastfm-2k.zip http://www.cs.ubc.ca/ ~ jamalim/datasets/ http://files.grouplens.org/datasets/movielens/ ml-1m.zip whereas we only performed one run without optimizing pa-rameters.
 We rather intend to show the versatility of our formalism which provides many different ways to assemble the various ingredients and produce old and new RSs. We also give de-tails, which are not always present in the literature, about the settings we used for the experiments. To allow for com-parison, our code is available in open source 8 .
 Our experiments will be presented in two sets: In the experiments reported below, we have used various settings which are not always explicit in the literature: For reference, to compare performances obtained in our ex-periments, we have implemented three classical techniques: https://bitbucket.org/danielbernardes/ socialfiltering Performances shown in Table 3 provide a baseline: figures in bold indicate the best performance of the corresponding category. We run our simulations on an Intel Xeon E7-4850 2,00 GHz (10 cores, 512 GB RAM), shared with members of the team (so concurrent usage might have happened in some of the experiments, with impact on reported time). Computing time in hours is thus indicative only (0:01:00 is 1 min, 4:20:00 is 4 hours 20 min). Our formalism was im-plemented using state-of-the-art libraries, such as Eigen for computing similarities.
 As can be seen, bigrams are very efficient in terms of perfor-mances on all four datasets (see also [47]) and they require no parameters tuning (except the support and confidence threshold). But bigrams have low Users X  and Items X  cover-age, with all recommended items in the Head.
 In contrast, NMF are very sensitive to parameters (maximal rank and maximum number of iterations) and since we did no try to optimize these parameters, we obtain low perfor-mances here. In addition, NMF do not scale well with in-creasing size of datasets. On the other hand, NMF have best Users X  and Items X  coverage (note that after 4 days of comput-ing time, we stopped NMF on MSD). These two techniques illustrate the trade-off one has to make in practice: fine tune parameters vs. default parameters to obtain optimal perfor-mances, and performances vs. coverage. Finally scalability is indeed a critical feature. We have implemented CF with the code provided by the author 11 in [3], in 2 versions: https://github.com/kimjingu/nonnegfac-python http://Eigen.tuxfamily.org Code on http://www.math.uni.pd.it/ ~ aiolli/CODE/MSD Note that in Table 4, the neighborhood chosen simply con-sists of all users / items with non null similarity (the case with q = 5 restores some locality). We found that limiting to a Top N neighbors (we just tested N = 100) usually re-sulted in decreased performances and coverage. Note that, with the datasets sizes we use, a relative difference of 1% is significant.
 We have then implemented our SF formalism to reproduce classical RSs: When comparing CF IB, CF UB, with cosine or Aiolli asym-metric confidence, we do not find one technique systemati-cally best: Note that parameters (  X  = 0 ,q = 5) have not been opti-mized the way they were in [3]: it would certainly be possi-ble to choose, by cross-validation, better parameters. But, as we said, the purpose of these experiments is not to fully optimize settings.
 Comparing Table 3 and Table 4 shows that bigrams seem to perform best (except for MovieLens), but their Users X  cov-erage is poor (because the filter on support and confidence limits the number of rules: the threshold was not optimized). These results show that our formalism covers these various classical techniques: association rules, CF (item or user-based) and CF with locality as in [3]. We now show in Tables 5 and 6 implementations of the SF formalism with various choices of similarity measures (equations, (15), (17), ( 18) and (20)) and neighborhood K ( a ) /V ( i ). We use as score function the weighted aver-age (equations (10) and (11) middle) or, for the asymmetric confidence similarity, the local score of equation (12). In our implementation, we filter the links in the Users X  and Items X  networks with support / confidence less than 1%.
 We thus have three differences with the classical implemen-tations of CF above: one is the choice of neighborhood (first circle or local community in the implicit Users or Items net-work, instead of most similar users/items for CF); next one is the filtering of links in SF which results in a reduction in the numbers of neighbors; and last one is the choice of various similarity measures such as support, confidence, or Jaccard (equations (15), (17) and (20)). Here are our main findings: and Asymmetric Cosine Collaborative Filtering. As was demonstrated in many cases, and very notably in the Netflix challenge [6; 30], ensembles of RSs often get im-proved performances. We have thus implemented a few en-sembles using a basic combination method, originated from [13], and used in [3]. To combine N recommenders, we assemble, for each user a , the N lists of k recommended items pro-duced by the N RSs.
 Let us denote ( i n 1 ,...,i n k ) the list of items recommended to a (omitted in the notation) by RS n (with n = 1 ,...,N ). Some of the lists might have less than k elements. For each list, we assign points to the items in each list as follows: 1st item gets k points, 2nd item k  X  1, etc. The lists are then fused and each item gets the sum of points in the various lists, with the ties resolved through a priority on RSs (the winner comes from the highest priority list).
 In order to explore the potential of ensemble techniques, we tried combinations of pairs of RSs: we tested combina-tions of some of the best SF-based recommenders mentioned above (Tables 4, 5 and 6). Results shown in Table 8 are en-couraging: These preliminary results indeed confirm the potential of en-semble methods. They could certainly be enhanced by test-ing combinations of more systems, optimizing the parame-ters and implementing more adequate aggregation methods (Borda X  X  aggregation method [13] seems rather popular in the literature, but can certainly be improved upon to merge recommendation results). We have presented a simple and generic formalism based upon social network analysis techniques: by building once the projected Users X  and Items X  networks, the formalism al-lows reproducing a wide range of RSs found in the literature while also producing new RSs. This unique formalism thus provides very efficient ways to test the performances of many different RSs on the dataset at hand to select the most ad-equate in that case.
 As can be seen from our experiments, there is no unique silver bullet (so far!): for each dataset, one has to test and try a full repertoire of candidate RSs, fine tuning hyper-parameters (a topic we did not address in this paper) and selecting the best RS for the performance indicator he/she cares for. The richer the repertoire is, the more chances for the final RS to get best performances. This theoretical for-malism thus provides a very powerful way to formalize and compare many RSs.
 In addition, this integrated formalism enables the produc-tion of modular code, uncoupling the similarities and scor-ing functions computation steps. It also allows for elegant implementation of the recommendation engines, as demon-strated in our published open source code 12 .
 Computing the bipartite network and its projections re-quires significant computing resources. It can be considered as a set-up step for our recommender framework. This over-head is only worth it if one wants to produce an ensemble of RSs originating from various choices of parameters (similar-ity measures, neighborhoods and scoring functions) to make comparisons and select the best choice for the dataset at hand. In addition, computation of similarities is the bottle-neck (since obviously it involves all pairs of users or items). However, some similarity measures (asymmetric confidence or Jaccard) are more costly than others (support, confidence and cosine) as the figures about computing time show in the various tables.
 This work opens ways for future research in several direc-tions: https://bitbucket.org/danielbernardes/ socialfiltering Personal communication of one of the authors [58 ]. This work has been partially supported by projects AM-MICO and OFS (funded by BPI France). Access to public datasets and code is gratefully acknowledged. [1] G. Adomavicius and A. Tuzhilin. Toward the next [2] D. Agarwal and B.-C. Chen. Regression-based la-[3] F. Aiolli. Efficient top-n recommendation for very large [4] J. Aranda, I. Givoni, J. Handcock, and D. Tarlow. An [5] P. Avesani, P. Massa, and R. Tiella. A trust-enhanced [6] J. Bennett and S. Lanning. The netflix prize. In Pro-[7] V. D. Blondel, J.-L. Guillaume, R. Lambiotte, and [8] J. Bobadilla, F. Ortega, A. Hernando, and A. Guti  X errez. [9] C. Borgelt. Frequent item set mining. Wiley Interdisci-[10] D. M. Boyd and N. B. Ellison. Social network sites: Def-[11] J. S. Breese, D. Heckerman, and C. Kadie. Empirical [12] D. Carmel, N. Zwerdling, I. Guy, S. Ofek-Koifman, [13] J.-C. de Borda. On elections by ballot. Classics of social [14] L. M. de Campos, J. M. Fern  X andez-Luna, and J. F. [15] Z. Despotovic and K. Aberer. A probabilistic approach [16] M. Diaby, E. Viennet, and T. Launay. Toward the [17] M. Diaby, E. Viennet, and T. Launay. Exploration of [18] M. D. Ekstrand, J. T. Riedl, and J. A. Konstan. Col-[19] M. Gartrell, X. Xing, Q. Lv, A. Beach, R. Han, [20] J. A. Golbeck. Computing and applying trust in web-[21] R. Guha, R. Kumar, P. Raghavan, and A. Tomkins. [22] R. Guimer`a, M. Sales-Pardo, and L. A. N. Amaral. [23] M. Gupte and T. Eliassi-Rad. Measuring tie strength [24] J. He and W. W. Chu. A social network-based recom-[25] Y. Hu, Y. Koren, and C. Volinsky. Collaborative fil-[26] M. Jamali and M. Ester. A matrix factorization tech-[27] D. Jannach, M. Zanker, A. Felfernig, and G. Friedrich. [28] J. Kim and H. Park. Fast nonnegative matrix factor-[29] M. W. Kim, E. J. Kim, and J. W. Ryu. A collabo-[30] Y. Koren, R. Bell, and C. Volinsky. Matrix factoriza-[31] D. D. Lee and H. S. Seung. Learning the parts of [32] D. Lemire and A. Maclachlan. Slope one predictors for [33] G. Linden, B. Smith, and J. York. Amazon. com rec-[34] N. N. Liu, M. Zhao, and Q. Yang. Probabilistic la-[35] P. Lops, M. De Gemmis, and G. Semeraro. Content-[36] L. L  X u and T. Zhou. Link prediction in complex net-[37] H. Ma, I. King, and M. R. Lyu. Learning to recommend [38] H. Ma, D. Zhou, C. Liu, M. R. Lyu, and I. King. Rec-[39] P. Massa and P. Avesani. Trust-aware collaborative fil-[40] M. McPherson, L. Smith-Lovin, and J. M. Cook. Birds [41] B. N. Miller, I. Albert, S. K. Lam, J. A. Konstan, and [42] A. Mnih and R. Salakhutdinov. Probabilistic matrix [43] B. Ngonmang, M. Tchuente, and E. Viennet. Local [44] B. Ngonmang, E. Viennet, S. Sean, F. Fogelman-Souli  X e, [45] J. O X  X onovan and B. Smyth. Trust in recommender [46] M. J. Pazzani and D. Billsus. Content-based recommen-[47] B. Pradel, S. Sean, J. Delporte, S. Gu  X erif, C. Rouveirol, [48] M. Richardson, R. Agrawal, and P. Domingos. Trust [49] M. Richardson and P. Domingos. Mining knowledge-[50] R. Salakhutdinov and A. Mnih. Bayesian probabilistic [51] S. Shang, P. Hui, S. R. Kulkarni, and P. W. Cuff. Wis-[52] G. Shani and A. Gunawardana. Evaluating recommen-[53] R. R. Sinha and K. Swearingen. Comparing recommen-[54] X. Su and T. M. Khoshgoftaar. A survey of collabora-[55] J. Sun, H. Qu, D. Chakrabarti, and C. Faloutsos. Neigh-[56] J. Tang, X. Hu, and H. Liu. Social recommendation: a [57] C.-Y. Teng, Y.-R. Lin, and L. A. Adamic. Recipe rec-[58] K. Verstrepen and B. Goethals. Unifying nearest neigh-[59] C. Wang and D. M. Blei. Collaborative topic model-[60] K. Wang and Y. Tan. A new collaborative filtering [61] Z. Xia, Y. Dong, and G. Xing. Support vector machines [62] B. Xu, J. Bu, C. Chen, and D. Cai. An exploration of [63] C. Yin, T. Chu, et al. Improving personal product [64] R. Zheng, F. Provost, and A. Ghose. Social network col-[65] R. Zheng, D. Wilkinson, and F. Provost. Social net-[66] T. Zhou, J. Ren, M. Medo, and Y.-C. Zhang. Bipar-[67] C.-N. Ziegler and G. Lausen. Propagation models for Computer science is essentially an applied or engineering science, creating tools. In Data Mining, those tools are sup-posed to help humans understand large amounts of data. In this position paper, I argue that for all the progress that has been made in Data Mining, in particular Pattern Mining, we are lacking insight into three key aspects: 1) How pattern mining algorithms perform quantitatively, 2) How to choose parameter settings, and 3) How to relate found patterns to the processes that generated the data. I illustrate the issue by surveying existing work in light of these concerns and pointing to the (relatively few) papers that have attempted to fill in the gaps. I argue further that progress regarding those questions is held back by a lack of data with varying, controlled properties, and that this lack is unlikely to be remedied by the ever increasing collection of real-life data. Instead, I am convinced that we will need to make a science of digital data generation, and use it to develop guidance to data practitioners. Computer science is basically an applied or engineering sci-ence. By this, I do not mean that all work done in our field does or should happen only in relation with a concretely de-fined real-life application. But rather that we use the results of other disciplines, be they mathematics, physics, or oth-ers, and develop what should be understood as tools , devices and algorithms that make it easier for humans to perform certain tasks.
 In Data Mining, those tools come mainly in two forms: 1) supervised methods that learn from labeled data how to pre-dict labels for unseen data or how to characterize predefined subsets, and 2) unsupervised methods. A second dimension along which to characterize them has to do with the scope of their results: a) they apply either to (almost) the entire data set  X  they are global in nature, such as classification models or clusterings, or b) being local , they refer only to a (non-predefined) subset of the data.
 The setting where the lack of supervision and local results intersect, i.e. 2b), it often referred to as  X  X attern Mining X  (
PM ), which is the term I will use hereafter. It holds a great promise: given large amounts of data and little or no supervision, PM can find interesting, hitherto undiscovered, relationships  X  patterns  X  in the data. Those patterns can in turn be exploited in the domains whence the data were generated to, for instance, further research, improve logis-tics, or increase sales. To see why this promise is so great, one only has to consider that supervised modeling already knows one side of relationship it seeks to establish. Super-vised modeling seeks explanations, unsupervised modeling hypotheses.
 Pattern Mining has been an active research field at least since the publication of the seminal paper introducing the Apriori algorithm [3] for frequent itemset mining (FIM). The twenty years since have brought an ever-widening scope of the field, extending the original itemset/association rule setting to semi-structured and structured data, the transac-tional setting to single-instance settings such as episode and network mining, and the application of PM techniques to a variety of different fields. This widening of scope also led to a plethora of techniques, published in a number of journals and conferences.
 As I will argue in detail, however, the increase in number of topics and algorithms has not been paralleled by an equal increase in understanding the strengths and in particular limitations of developed techniques, or by guidelines for their employment. And creating tools (or rather in many cases blueprints) is not enough: to fulfill PM  X  X  promise and make the most of the developed techniques, it is necessary to give potential users an idea how to actually employ those tools. In particular, there are three large gaps in our understanding of pattern mining: 1. We do not know how most pattern mining al-2. We do not know how to choose good parame-3. We do not know how mined patterns relate to In the following three sections, I will illustrate the gaps in our knowledge in detail. In particular, I will pay attention to the work that did attempt to fill those gaps, and proposed ways of researching these issues, but also why it falls short. These problems are not limited to PM  X  evaluations, com-parisons, and the exploration of good parameter settings also often fall short of best practices for supervised settings, and techniques that result in global models. But the prob-lem is particularly pronounced in PM , mainly because the local results require the evaluation of individual patterns and because quality criteria are harder to define. As I will argue, an important contributing factor is the lack of data in general, and of data with controlled, diverse char-acteristics and known ground truth in particular: There is a potential solution to this problem  X  artificial data generation  X  but as I will show, the track record of the PM community w.r.t. data generation is rather weak so far. This has to be remedied if we want to do more than propose tools that might or might not work. While developing generators that lead to data with differing characteristics could turn out to be relatively easy, knowing what kind of generative processes can be expected to occur in real-life data will be more challenging and such knowledge will often not be found with data miners but with real-world practitioners. Hence, I argue that we need to add a deeper understanding of data  X   X  X ata science X  so-to-say  X  to the PM research portfolio, a task that will require the collaboration with researchers and practitioners of other fields that currently is too often more statement than fact. The first glaring problem has to do with the lack of extensive evaluation of data mining algorithms, whether in the original papers, on additional data, or in comparisons with other techniques from the field. As a result thereof, we do not have a clear idea how different algorithms will behave on certain data. We do know some things: dense data sets will result in more patterns and more computational effort, strict formal concept analysis on real-life data will probably not result in good output compression. But apart from that, the body of knowledge is weak.
 Often, the paper that introduces a new algorithm contains the most comprehensive published experimental evaluation of that algorithm in terms of running times, memory con-sumption etc. Even those initial evaluations are often not very extensive, though. In the following, I will give an overview of this phenomenon in different subfields. The seminal FIM paper used an artificial data generator to evaluate their approach on more than 45 data sets, gener-ated by varying parameters. Notably, all those data shared a common characteristic  X  they were sparse. A similarly sys-tematic evaluation can still be found in [61], yet most other early work [22; 64; 44] used far fewer data sets. FIM papers since have followed this trend with few exceptions. Sequence mining was first formalized and proposed at roughly the same time as FIM by the same authors and in [51] twelve data sets are used for evaluation, nine of which artificial, with a similarly declining trend for follow-up work [62; 21; 45]. Sequence mining was transferred from the transactional setting to finding recurrent patterns  X  episodes  X  in single large data sequences in [37] and algorithms from this subfield have typically only been evaluated on a few data sets. With a few exceptions, papers on the generalizations of se-quences  X  trees (introduced in [63]) and graphs [25; 30; 26; 60; 24]  X  have followed the same trajectory, with two of my own papers [11; 68] among the worst offenders in this regard. Graph mining has also been extended to the  X  X ingle large X  setting and different papers [17; 31; 28] have used less than twenty data sets each from various sources.
 Rarely have those algorithms been reevaluated on additional data beyond that used in the original papers, apart from some comparisons in the papers that proposed improve-ments. Even in the latter case, transitivity has often been assumed  X  if algorithm B has been reported to perform bet-ter than algorithm A, comparing to B is considered enough. But obviously, this only holds for the data on which that evaluation has been performed, either locking future evalu-ations into the same restricted data or leading to unjustified generalizations about algorithmic behavior.
 The paper that most incisively demonstrated this problem is arguably the one by Zheng et al. [65]. As described above, the data on which Apriori was evaluated were artificially generated and follow-up techniques mainly used subsets of those data. When comparing the artificial data to real-life data at their disposal, Zheng et al. noticed that the latter had different characteristics. An experimental comparison of the follow-up algorithms showed that claimed improvements in the literature did not transfer to the real-life data  X  the improvements had been an artifact of the data generation process.
 Unfortunately, that paper has remained one of a handful of exceptions. With the exception of the Frequent Item-set Mining Implementations (FIMI) workshops [19; 5], little additional work has been done on FIM. While FIMI was undoubtedly important, there were notable short-comings: the workshop took place only twice, the focus was more on the practical aspects of implementing abstract algorithms than on an assessment of the algorithms themselves, and was limited to the, relatively small, collection of data sets available.
 In graph mining, [59] compared four depth-first search graph miners on new data, notably reporting results that contradict those in [24]. The authors of [40] generated and manipulated data, and most notably find that there is no strong relation between run times and the efficiency of the graph codes, as had been claimed in the literature.
 Episode mining is an area in which evaluations and compar-isons of algorithms are particularly rare. I am not aware of any other work except my own [67], in which I used a data generator to generate data having a range of characteristics and reported results that indicate temporal constraints have more impact than pattern semantics, contrary to claims in the literature.
 Those papers show how important it is to use data with new characteristics, and to pay attention to questions of imple-mentations and hardware when assessing the usefulness of algorithmic approaches. Yet, compared to the body of work describing algorithmic solutions, the body of work compre-hensively evaluating those solutions is rather small. A second problem, somewhat related to the first, is that, for the majority of techniques, it is unclear how parameter settings should be chosen. As a rule of thumb, more lenient parameter settings will lead to longer running times but not even this relationship has been established concretely. At first sight, the situation w.r.t. this problem is better but this impression is deceiving.
 Several papers established a relationship between the distri-bution of mined frequent, closed, and maximal itemsets and algorithmic running times [64; 49; 20; 14]. Apart from the fact that such research does not exist for structured data and patterns, those approaches are faced with the obvious prob-lem that extensive mining, at potentially extensive running times, is needed before the relationship can be established. An improvement consists of estimating support distributions and running times based on partial mining results, or sam-pled patterns, as been the approach of [34; 43; 16; 10; 9]. Those studies only traverse half the distance though  X  in predicting running times and output sizes  X  even though [9] proposes setting a frequency threshold high enough to avoid the exponential explosion of the output set. Whether a threshold setting that allows the operation to finish in a reasonable amount of time will lead to interesting patterns, is largely unexplored.
 A notable exception concerned with mining sequences under regular expression constraints can be found in [6]. By sam-pling patterns fulfilling data-independent constraints under assumptions about the symbol distribution, they derive a model of background noise, and identify thresholds expected to lead to interesting results. A similar idea can be found in [38], which uses sampling and regression to arrive at pattern frequency spectra for FIM. By comparing analytical expres-sions for spectra of random data to the actually derived spectra, they also identify deviating regions, proposing to explore those. Those two papers come closest to actually giving guidance for parameter selection but are limited to frequency thresholds. Yet, the blue print for building up knowledge about the interplay of data characteristics and parameter settings  X  for instance for storage in experiment databases [54]  X  is there, held back by the relatively limited supply of available data sets.
 Apart from the fact that it would be attractive to fix param-eter settings before an expensive mining operation, this is an instance in which the unsupervised nature of pattern mining makes the task harder than for supervised settings. In the latter, validation sets can be used to assess the quality of re-sulting models, and parameters adjusted accordingly. There is work based on related ideas, which derive null models di-rectly from the data, already found patterns, or user knowl-edge and use statistical testing to remove all those patterns that are not unexpected w.r.t. the null hypothesis [7; 18; 39], or uses multiple comparisons correction and hold-out sets [57]. While such approaches promise to remove all sta-tistically unsurprising patterns, unexpected patterns are not necessarily useful ones. This last remark hints at an important issue in PM : which patterns to consider  X  X nteresting X . The field has moved on from the FIM view of (relatively) frequent and incuding (rel-atively) strong implications. Yet what has taken its place are (objectively or subjectively) surprisingness (see above), or effective summarization/compression (e.g. [56]). Such patterns are undoubtedly interesting and useful but they leave us with the third and in my opinion most important gap in our knowledge: it is currently mostly unclear how the patterns that are being mined by PM techniques relate to the patterns actually occurring in the data.
 To be clear about this: we know what types of patterns we have defined, and we have the algorithms to find them according to specific criteria (at least in the case of complete techniques). In FIM, for instance, if there is set of items that is often bought together, it will be found. Yet so will all of its subsets, and maybe intersections with other itemsets, and we are currently lacking the knowledge to decide which of these patterns are relevant. Hence, in many cases we do not know whether we capture meaningful relationships.
 Furthermore, even if we could identify the actual patterns, we would not know how those relate to the processes that generated the data in the first place. To continue with the FIM example, papers will often give the motivation behind performing such mining by stating that supermarkets can group items that are bought together close to each other, to motivate those customers who did not buy them together to do so. An alternative proposal states supermarkets should group such items far apart to motivate customers to tra-verse the entire store space and potentially make additional purchases.
 These strategies implicitly assume two different types of cus-tomer behavior, though: in the latter case, the co-purchases are systematic and can be leveraged to generate additional business. In the former, the co-purchases are somewhat op-portunistic, which also means that using the first strategy could lead to loss of business. Maybe the two types of be-havior actually lead to different expressions of the pattern in the data. And maybe the layout of the supermarket at the time of purchase has an effect on this expression, for instance because it enables the opportunistic behavior. But since there exist no studies relating mined itemsets to the shopping behavior itself, i.e. the generative process of the data, it is unclear, twenty years after FIM was proposed, which of the two assumptions holds.
 This has grave implications. If it is unclear how patterns relate to the underlying processes, it is also unclear how to exploit patterns in the domains that the data originated from. Given that this is the rationale of data mining, the current state of the art in PM research is therefore failing the field X  X  main purpose: supporting real-world decision making. Again, there exist studies that have attempted to fill this gap in a more or less systematic manner [32; 53; 52; 36; 35; 66; 48; 58; 67]. The typical approach consists of embed-ding explicitly defined patterns in the data (with or with-out noise effects), and comparing mined patterns to them to understand the relationships. Of particular interest are experiments in which the data pattern generation takes a different form from the pattern definition, such as in [35] in which Bayes X  Nets were used to generate itemsets. Yet, again, there are not many of them, and their focus has often been only on a single technique.
 The problem of interpretation exists for supervised and/or global approaches as well, but arguably to a lesser degree. If the goal is prediction, high accuracy is an indicator of a good result  X  after all, there are  X  X lack box X  classification tech-niques but no black box PM ones. Similarly, a clustering that exhibits high intra-cluster similarity and inter-cluster dissimilarity is probably a good one  X  incidentally an as-sessment that is related to good summarization. And even if the setting is supervised and local, as in subgroup discov-ery, patterns that correlate strongly with the subgroup can be expected to be meaningful. The preceding sections should not be read as an indictment of all PM research. Quite contrary, many of the found solu-tions are ingenious and elegant, and the impressive tool kit that has been amassed should enable practitioners to ad-dress many real-world problems more effectively. But faced with data, a typical practitioner will not know which tools to choose, how to set the parameters without extensive trial-and-error, and what conclusion to draw from the resulting patterns  X  unless we fill in the gaps.
 There are several factors that influence the described situ-ation. Some of those are related to what kind of research is rewarded, which in turn relates to publication policies. Instead of discussing those, the interpretation of which is necessarily subjective, I want to draw attention to an objec-tive factor that I have also pointed to in each section: Despite what one would expect given the name  X  X ata Mining X , what we lack is data! In reaction to the work of Zheng et al. , the data sets they introduced were added to the benchmark data sets for FIM and reliance on the data generator from [3] was reduced. Several other data sets were added over time and the col-lection is currently downloadable at the FIMI website. 1 Yet the totality of this collection comprises only twelve data sets. This is a far cry from the large amount of data sets avail-able at the UCI repository for machine learning [8] (used for predictive learning, clustering, and subgroup discovery), the UCR collection of data for time series classification and clustering [29], or even the data sets available for multi-label learning. 2 Sequence mining is mainly performed on a small number of biological data sets. Most of the real-life data used in episode mining papers are covered by non-disclosure agree-ments and have therefore never entered the public domain. There are handful of tree mining data sets, mainly based on click streams or website traversal. Graph mining also makes heavy use of a small number of molecular data sets, and network mining data sets have ranged from graph en-http://fimi.ua.ac.be/data/  X  accessed 08/21/2014 http://en.sourceforge.jp/projects/sfnet_mulan/ releases/  X  accessed 08/21/2014 codings of UCI data to snapshots of social, citation, traffic, or biological networks.
 Unless the current collections cover by pure accident all characteristics that can be encountered in the real world, even best-practice evaluations based on them will not give a complete picture of algorithms X  strengths and weaknesses, making it difficult to address the first and second prob-lem. Furthermore, even if we had large amounts of real-life data at our disposal, in most cases we would not know the ground truth of such data and therefore could not address the problem laid out in Section 4. After all, real-life data in supervised settings  X  X nly X  need a label to assess whether relationships are relevant but evaluating patterns in a local unsupervised setting needs a much deeper understanding of the data. And even if we had data available of which we knew the ground truth, we would lack the knowledge about the generative processes leading to this ground truth, as de-scribed above.
 Luckily for computer scientists, there is an alternative to assembling ever increasing collections of real-life data, or rather a complement: artificial data generation. This is the solution that has been chosen in exploring phenomena in the SAT solving community [47], for instance, or for evaluating another unsupervised setting, clustering [46]. The idea is also running like a thread through much of the work I have reviewed so far, whether it is artificial data used for sys-tematically exploring the effects of data characteristics, for identifying where data deviates from random backgrounds, or for matching patterns to generating processes. The problem is, however, that the story of data generation in
PM so far is arguably one of failure. The data gener-ator used in [3] was discredited by Zheng et al. . This has repercussions since the data generators used in sequence and graph (and arguably tree) mining papers base on similar considerations. Cooper et al. [13], attempting to fix a sec-ond problem of that generator, ignored the first one, and introduced new artifacts. The survey undertaken in [12] lists 22 generators for network data, all of which attempt to reproduce certain numerical properties of real-life data, such as degree distribution, or clustering coefficient. With the ex-ception of a few that attempt to model particular types of networks, the authors found that none of the proposals gets it fully right. The reference is admittedly somewhat dated but other work published since [33; 41; 15; 42] comes to the same conclusion.
 Generators leading to data resembling the one already avail-able suffer from the fact that they do not solve the problem of the data bottle neck. Approaches such as [49; 50; 55] take the output of an FIM operation and generate databases that will result in similar output. Thus, they take the existence of data as a given, as do the approaches that create null models based on the data. Furthermore, the data generated by the former is expected to result in the same mix of rel-evant and irrelevant patterns as the old one, and the latter mask the underlying processes.
 While artificial data generation enables us to create data with a wide range of characteristics, assess the effects of different kinds of noise on the ability to recover patterns, and to simulate different generative processes, we have not used this ability to fill in the gaps in our understanding. This will need to change, and I am convinced that to do so we, or at least some of us, have to become data scientists . When media and non-academics refer to data miners or data analysts, the term  X  X ata scientist X  is often used. But what this term implies, in my opinion, is that such a person un-derstands the data, and we do not . Part of this is by design  X  as I wrote in the beginning, the promise of pat-tern mining is to find interesting patterns in a largely unsu-pervised manner. The naive interpretation of this promise, however, is similarly flawed as the claim that in the age of  X  X ig Data X , discovering correlation replaces understanding causation [4]. 3 To fill in the gaps in our understanding as PM researchers, data science needs to be added to our expertise. We need to develop data generators that produce varied characteris-tics in a controlled manner, to enable extensive experiments. Those data generators need to use generative processes to which we can map patterns back, so we start to understand how certain processes manifest in the output of the tools we develop. Concretely, this means exploring different distri-butions (and mixtures thereof) governing the data genera-tion, instead of fixing a single one. It means adding varying degrees of noise to the data. And it means using genera-tive processes that are different from the sought patterns  X  Bayes X  Nets for itemset data, or interacting agents for net-work data, for instance. In fact, there exists already a tool that makes some of this possible, the KNIME data generator [2], which is however neither used widely nor systematically so far.
 Once we have access to such generators, we can follow the approach of existing studies to fill in some of the gaps that currently exist. This means, for instance, evaluating and comparing algorithms over data of different density, pattern length, alphabet size, etc. It means establishing what pro-portion of individual patterns can be recovered under the effects of noise. It means assessing whether highly ranked itemsets represent fragments of embedded patterns, or rep-resent subgraphs of Bayes X  Nets. It also means understand-ing whether data that are generated by the same processes with the same parameters actually have the same character-istics, and whether they give rise to similar result sets, i.e. whether it is appropriate to transfer insights derived on one data set to another that  X  X ooks X  similar. In other words, it should allow us to develop better ways of comparing data sets.
 This is obviously not an exhaustive enumeration and it will take the creativity and effort of the community to get the field to that stage. But even that can only be a step on the way: we can learn how the patterns we mine relate to the patterns in the data, and in turn to the processes that generated them. How itemsets relate to agents that  X  X hop X  according to certain  X  X ehavior X , for instance.
 The knowledge about real-life behavior cannot come from inside our community, however. Instead, it will be found in physics [1], in engineering [27], in the social and life sciences. Once we have understood which method to use, how to set parameters, and how to select relevant patterns and inter-pret them, based on the data available, we can approach practitioners from those fields. Using their knowledge, we
A claim that incidentally experiences tremendous push-back. can generate data that are real life-like, and troubleshoot generators and evaluation methods. In all probability, some of the assumptions from those fields will turn out to be wrong but probably not more wrong than the assumptions we ourselves have made in generating data so far. And if, building on such assumptions, we find them to be wrong (or at least questionable), and feed this information back into the fields whence they originated, even better.
 I have not come here to bury PM but to praise it. I am convinced that the potential of the tools that the community has developed over the last two decades is tremendous. I am, however, challenging the community to develop guidance for how to use those tools. Working closely with practitioners and giving them hands-on guidance, the modus operandi of many application papers, is a worthy endeavour but it is also time-consuming and allows for little generalization. We have to solve the data problem in data mining and we have to do it in a better-founded way than by trying to acquire additional real-life data sets. We need to make a science out of generating digital data.
 I am grateful to Matthijs van Leeuwen, Arno Siebes, and Jilles Vreeken for reviewing preliminary versions of this ar-ticle, giving feedback, sharpening the questions, and dis-cussing possible answers. The author was supported by the FP7-PEOPLE-2013-IAPP project GRAISearch (Grant Agreement Number 612334) at the time of writing. [1] Corsika -an air shower simulation program,. [2] I. Ad  X a and M. R. Berthold. The new iris data: mod-[3] R. Agrawal and R. Srikant. Fast algorithms for min-[4] C. Anderson. The end of theory: The data [5] R. J. Bayardo Jr., B. Goethals, and M. J. Zaki, editors. [6] J. Besson, C. Rigotti, I. Mitasiunaite, and J.-F. Bouli-[7] T. D. Bie. Maximum entropy models and subjec-[8] C. Blake and C. Merz. UCI repository of machine learn-[9] M. Boley, T. G  X artner, and H. Grosskreutz. Formal con-[10] M. Boley and H. Grosskreutz. A randomized approach [11] B. Bringmann and A. Zimmermann. Tree 2 -De-[12] D. Chakrabarti and C. Faloutsos. Graph mining: [13] C. Cooper and M. Zito. Realistic synthetic data for [14] F. Flouvat, F. D. Marchi, and J.-M. Petit. A new classi-[15] A. Freno, M. Keller, and M. Tommasi. Fiedler random [16] F. Geerts, B. Goethals, and J. V. den Bussche. Tight [17] S. Ghazizadeh and S. S. Chawathe. Seus: Structure ex-[18] A. Gionis, H. Mannila, T. Mielik  X ainen, and P. Tsaparas. [19] B. Goethals and M. J. Zaki, editors. FIMI  X 03, Fre-[20] K. Gouda and M. J. Zaki. Genmax: An efficient algo-[21] J. Han, J. Pei, B. Mortazavi-Asl, Q. Chen, U. Dayal, [22] J. Han, J. Pei, and Y. Yin. Mining frequent pat-[23] J. Han, B. W. Wah, V. Raghavan, X. Wu, and R. Ras-[24] J. Huan, W. Wang, and J. Prins. Efficient mining of [25] A. Inokuchi, T. Washio, and H. Motoda. An apriori-[26] A. Inokuchi, T. Washio, K. Nishimura, and H. Motoda. [27] E. F. V. J. J. Down. A plant-wide industrial process [28] U. Kang, C. E. Tsourakakis, and C. Faloutsos. Pega-[29] E. Keogh, Q. Zhu, B. Hu, Y. Hao, X. Xi, L. Wei, and [30] M. Kuramochi and G. Karypis. Frequent subgraph dis-[31] M. Kuramochi and G. Karypis. Finding frequent pat-[32] S. Laxman, P. S. Sastry, and K. P. Unnikrishnan. Dis-[33] J. Leskovec, K. J. Lang, A. Dasgupta, and M. W. Ma-[34] L. Lhote, F. Rioult, and A. Soulet. Average number of [35] M. Mampaey and J. Vreeken. Summarizing categorical [36] M. Mampaey, J. Vreeken, and N. Tatti. Summariz-[37] H. Mannila and H. Toivonen. Discovering frequent [38] A. U. Matthijs van Leeuwen. Fast estimation of the [39] R. Milo, S. Shen-Orr, S. Itzkovitz, N. Kashtan, [40] S. Nijssen and J. Kok. Frequent subgraph miners: run-[41] G. K. Orman, V. Labatut, and H. Cherifi. Qualita-[42] G. K. Orman, V. Labatut, and H. Cherifi. Towards re-[43] P. Palmerini, S. Orlando, and R. Perego. Statistical [44] J. Pei, J. Han, and R. Mao. Closet: An efficient al-[45] J. Pei, J. Han, B. Mortazavi-Asl, H. Pinto, Q. Chen, [46] Y. Pei and O. Za  X  X ane. A synthetic data generator for [47] D. M. Pennock and Q. F. Stout. Exploiting a theory [48] B. A. Prakash, J. Vreeken, and C. Faloutsos. Efficiently [49] G. Ramesh, W. Maniatty, and M. J. Zaki. Feasible [50] G. Ramesh, M. J. Zaki, and W. Maniatty. Distribution-[51] R. Srikant and R. Agrawal. Mining sequential pat-[52] N. Tatti and J. Vreeken. Discovering descriptive tile [53] N. Tatti and J. Vreeken. The long and the short of [54] J. Vanschoren, H. Blockeel, B. Pfahringer, and [55] J. Vreeken, M. van Leeuwen, and A. Siebes. Preserving [56] J. Vreeken, M. van Leeuwen, and A. Siebes. Krimp: [57] G. I. Webb. Discovering significant patterns. Machine [58] G. I. Webb and J. Vreeken. Efficient discovery of the [59] M. W  X orlein, T. Meinl, I. Fischer, and M. Philippsen. [60] X. Yan and J. Han. gspan: Graph-based substructure [61] M. J. Zaki. Scalable algorithms for association mining. [62] M. J. Zaki. Spade: An efficient algorithm for mining [63] M. J. Zaki. Efficiently mining frequent trees in a forest. [64] M. J. Zaki and C.-J. Hsiao. Charm: An efficient algo-[65] Z. Zheng, R. Kohavi, and L. Mason. Real world perfor-[66] A. Zimmermann. Objectively evaluating condensed rep-[67] A. Zimmermann. Understanding episode mining tech-[68] A. Zimmermann and B. Bringmann. Ctc -correlating The realm of knowledge discovery extends across several allied spheres today. It encompasses database management areas such as data warehousing and schema versioning; information retrieval areas such as We b semantics and topic detection; and core data mining areas, e.g., knowledge based systems, uncertainty management, and time-series mining. This becomes particularly evident in the topics that Ph.D. students choose for their dissertation. As the grass roots of research, Ph.D. dissertations point out new avenues of research, and provide fresh viewpoints on combinations of known fields. In this article we overview some recently proposed developments in the domain of knowledge discovery and its related spheres. Our article is based on the topics presented at the doctoral workshop of the ACM Conference on Information and Knowledge Management, CIKM 2011.
 Ranking, Text Mining, Extreme Web, ETL, Pattern Recognition, Resource Monitoring, Version Control, KNN, Semantic Web, Main Memory Database, Data Warehousing, Database Analytics Knowledge discovery is an interdisciplinary field of research, which encompasses diverse areas such as data mining, database management, information retrieval, and information extraction. This inspires doctoral candidates to pursue research in and across these related disciplines, with the core contributions of their dissertation being in one or more of these areas. In this article, we review some of the directions that the researchers of tomorrow pursue. We provide a report of the research challenges addressed by students in the Ph.D. workshop PIKM 2011. This workshop was held at the ACM Conference on Information and Knowledge Management, CIKM 2011. The CIKM conference and the attached workshop encompass the tracks of data mining, databases and information retrieval, thus providing an excellent venue for dissertation proposals and early doctoral work in and across different spheres of knowledge discovery. This workshop was the fourth of its kind after three successful PIKM workshops in 2007 [15, 16], 2008 [11, 14] and 2010 [9, 10]. The PIKM 2011 [8] attracted submissions from several countries around the globe. After a review by a PC comprising 19 experts from academia and industry worldwide, 9 full papers were selected for oral presentation and 4 short papers for poster presentation. The program was divided into 4 sessions: data mining and knowledge management; databases; information retrieval; and a poster session with short papers in all tracks. The first highlight of the PIKM 2011 was a keynote talk on  X  X xtreme We b Data Integration X  by Prof. Dr. Felix Naumann from the Hasso Plattner Institute in Potsdam, Germany [8]. This talk addressed the integration and querying of data from the Semantic We b at large scale, i.e., from vast sources such as DBpedia, Freebase, public domain government data, scientific data, and media data such as books and albums. It discussed the challenges related to the heterogeneity of We b data (even inside the Semantic Web), common ontology development, and multiple record linkage. It also highlighted the problems of We b data integration in general, such as identification of good quality sources, structured data creation, standardization-related cleaning, entity matching, and data fusion. We now furnish a review comprising a summary and critique of the dissertation proposals presented at this workshop, discussing new directions of research in data mining and related areas. We follow the thematic structure of the workshop with 3 topic areas: knowledge discovery, database research, and information retrieval. The knowledge discovery issues surveyed in this article include areas as diverse as pattern recognition in time-series, resource monitoring with knowledge-based models, version control under uncertainty, and random walk k-nearest-neighbors (k-NN) for classification. The database research problems presented here entail aggregation for in-memory databases, evolving extract-transform-load (E-ETL) frameworks, schema and data versioning, and automatic regulatory compliance support. The information retrieval themes involve paradigms such as user interaction with polyrepresentation, ranking with entity relationship (ER) graphs, online conversation mining, sub-topical document structure, and cost optimization in test collections. The workshop also issues a best paper award to the most exciting dissertation proposal, as determined by the PC of the workshop. This year X  X  award went to the proposal  X  X anking Objects by Following Paths in Entity-Relationship Graphs X  [6], in the Information Retrieval track.
 The rest of this article is organized as follows. Sections 2, 3, and 4 discuss the different tracks of PIKM, i.e., knowledge discovery, database research, and information retrieval, respectively. In Section 5, we summarize the hot topics of current research, and compare them with the topics of the previous PIKM workshops. The topics surveyed here are those with main contributions in data mining and knowledge discovery, although some of them overlap with the other two thematic tracks, namely, databases and information retrieval. Many devices today, such as mobile phones, modern vehicular equipment and smart home monitors, contain integrated sensors. These sensors need to capture information with reference to context (e.g., abnormal motor behavior in vehicles) in order to enable the device to adapt to change and cater to users. This entails dealing with temporal data that is evolving in the respective environment. Spiegel et al. [13] addressed this problem. They proposed efficient methods to recognize contextual patterns in continuously evolving temporal data. Their approach incorporated a machine learning paradigm with a three step process: extracting features to construct robust models capturing important data characteristics; determining homogenous intervals and point of change by segmentation; and grouping the time series segments into their respective subpopulation by clustering and classification. This problem was considered interesting by the audience especially due to its application in smart phones where the proposed approach is useful in detecting patterns such as changing product prices to provide better responses to queries. Abele et al. [1] focused on a knowledge engineering problem in the manufacturing domain. More specifically, their problem was on computerizing the monitoring of resource consumption in complex industrial production plants, a task that usually involves tremendous time and manual effort. They proposed a semi-automated method for monitoring through knowledge based modeling with sensors, reasoners, annotations and rule engines, easily adaptable to changes. Their modeling approaches included object-oriented models with UML and domain models in the Semantic Web, with specific use of a data format for plant engineering called AutomationML. Advantages of their monitoring approach were reduction in manual effort, saving of time and resources, application independence and flexibility. Their research was particularly appreciated due to the manner in which they dealt with a domain-specific problem in an application independent manner by proposing knowledge based models that adequately encompassed ontology and logic through formalisms. Collaborative work on documents poses the problem of version control in the presence of uncertainty. Ba et al. [2] tackled this issue with much attention to XML documents. They proposed a version-control paradigm based on XML data integration to evaluate data uncertainty and automatically perform conflict resolution. They outlined the main features of versioning systems and defined a version-space formalism to aid in data collaboration and in the management of uncertainty. In their proposed solution, they incorporated aspects such as XML differencing with respect to trees, delta models for operations like insertions, moves and updates, directed acyclic graphs of version derivations and construction of probabilistic XML documents that minimize uncertainty. This work was found highly appealing due to its contributions to data mining, databases and IR since it addressed the important problem of uncertainty in knowledge discovery, proposed models based on database theoretical concepts and applied these within the context of XML in information retrieval. The challenging problem of multi-label classification formed the focus of the work by Xia et al. [19]. In this problem, an instance belonged to more than one class as predicted by the classifier and the number of potential class labels could be exponential, posing a challenge in predicting the target class. The authors proposed an approach to solve this problem by exploiting the benefits of k nearest neighbors and random walks. They first constructed a link graph based on k-NN, and then executed a random walk on the graph, so as to obtain a probability distribution of class label sets for the target instance. Further, they determined a classification threshold based on minimizing the Hamming Loss that computed the average binary classification error. Although this work did not show any significant experimentation, and thus drew criticism from the audience, it provided a complexity analysis with true and predicted class labels that was found acceptable. The authors also provided good motivation for their work by addressing real-world applications such as functional genomics, semantic scene classification and text categorization. The work presented at PIKM 2011 in the area of database research includes topics in database analytics, main memory databases, evolution of resources using ETL, and schema and data versioning. Some of the hottest topics in current database research include in-memory database systems, columnar database systems, self-managing and self-adapting database systems in the presence of mixed workloads. These topics, and, in general, issues related to large in-memory database systems were discussed by M X ller and Plattner in [7]. The main goal of the proposed work is to design an adaptive engine for aggregation materialization for database analytics. The paper addresses problems related to aggregation materialization identifying relevant cost factors for deciding when the materialization is cost efficient, and also what characteristics a cost model used by this type of adaptive engines must have. The authors provided an excellent motivation for this research by discussing recent trends in database usage where online transactional processing and online analytical processing are all reunified in one single database. Traditional data warehouse systems employ an ETL process that integrates the external data sources into the data warehouse. One of the hardest problems in ETL research is the frequent changes in the schema of the external data sources. Wojciechowski [18] took up the challenge of automating the process of adapting and evolving the ETL process itself to the changes in the structural schemas of the external data sources. The proposed E-ETL framework brings together, in a unique way, techniques from materialized view adaptation, schema and data evolution, versioning of schema and data [17], and multiversion in data warehouse systems. The author presented the current implementation of the E-ETL system and future work on developing an appropriate language for defining ETL operations, as well as extensions to the current set of changes at the external data sources that E-ETL can detect and adapt to. The topic of schema and data versioning systems, which was briefly referenced in [18] in the context of evolvable ETL processing, was the main topic of the paper by Wa l l and Angryk [17]. The authors investigate two different approaches to schema and data versioning: one approach is based on storing the minimal set of changes from the base schema and data to each branch, a sandbox, representing a particular deviation from the base schema The queries run against the branch are mapped accordingly to the base schema and its data, as well as to the branch schema and data to correctly reflect the set of changes in the branch. Another approach is to create copies of the modified tables into the branch, and propagate changes done in the base schema and data to these copies. The most interesting parts of this work are related to investigation into the qualitative and quantitative differences between the two techniques. The authors described the design of the ScaDaVer system meant to be used in assessing and testing different approaches to schema and data versioning. The topic and the motivation for this research are well established in the database area with new importance being brought to by the SaaS systems where multiple instances of the database can be operational in the same time. A unique research on using semantic web technologies to support the management of the compliance systems was presented by Sapkota et al. in [12]. Compliance Management systems, although using computer-assisted processes, still lack one of the fundamental requirements, which is the automation towards updating the system when the regulations change. The paper proposed a Semantic We b methodology to automate these processes. The proposed techniques include automatic extraction and modeling of regulatory information, and mapping regulations to organizational internal processes. The authors describe t implementation of the proposed methodology which has been started using the Pharmaceutical industry as a case study, and is being applied to the Eudralex European regulation for good manufacturing practice in the pharmaceutical industry. The third thematic track of the PIKM workshop was concerned with topics in information retrieval. It comprised 2 poster papers and 3 full papers, including the best paper award winner (see Section 4.5). Zellhoefer et al. [20] introduce a new interactive information retrieval model. They present a prototypical GUI-based system that allows users to interactively define and narrow down the documents they are interested in. The novelty of the proposed approach lies in the inspiration from the cognitively motivated principle of polyrepresentation. The principle X  X  core hypothesis is that a document is defined by different representations such as low-level features, textual content, and the user X  X  context. Eventually, these representations can be used to form a cognitive overlap of features in which highly relevant documents are likely to be contained. In their work, the authors link this principle to a quantum logic-based retrieval model, which enhances its appeal. The work also addresses the issue of information need drifts, i.e., of adjustments of the information needs of the user. This gives the work a refreshingly practical angle. Inches et al. [5] address the problem of data mining in online instant messaging services, twitter messages and blogs. The texts in these new areas of the social We b exhibit unique properties. In particular, the documents are short, user generated, and noisy. The authors investigate two different but related aspects of the content of these colloquial messages: the topic identification and the author identification tasks. They develop a framework in which social-Web specific features, such as emoticons, abbreviations, shoutings, and burstiness are systematically extracted from the documents. These features are used to build up a model of topic representation and author representation. The paper at the workshop described work in progress, with the author identification, and the synthesis of the features still to be explored. In text segmentation, a document is decomposed into constituent subtopics. Ganguly et al. [3] analyze how text segmentation can help for information retrieval. This can happen, for example, by computing the score of a document as a combination of the retrieval scores of its constituent segments, or by exploiting the proximity of query terms in documents for ad-hoc search. Text segmentation can also help for question answering (QA), where retrieved passages from multiple documents are aggregated and presented as a single document to a searcher. Text segmentation can also help segmenting the query, if the query is a long piece of text. This is particularly important for patent prior art search tasks, where the query is an entire patent. Information retrieval systems are usually evaluated by measuring the relevance of the retrieved documents on a test collection. This relevance has to be assessed manually. Since this is usually a costly process, Hosseini et al [4] consider the problem of optimally allocating human resources to construct the relevance judgments. In their setting, there is a large set of test queries, for each of which a large number of documents need to be judged, even though the available budget only permits to judge a subset of them. In their work, the authors propose a framework that treats the problem as an optimization problem. The authors design optimization functions and side constraints that ensure not only that the resources are allocated efficiently, but also that new, yet unseen systems can be evaluated with the previous relevance judgments. It also takes into account uncertainty that is due to human errors. This way, the proposal aims to tackle holistically a problem that is of principal importance in the area of information retrieval in general. The area of Information Retrieval is no longer restricted to text documents. It can equally well be applied to entity-relationship graphs or RDF knowledge bases. Kahng et al. [6] explore this idea by looking at paths in entity-relationship graphs. They put forward two ideas: First, an entity-relationship graph can represent not just factual information, but also other, additional, heterogeneous information. This allows treating tasks that have traditionally been seen as orthogonal within one single model. Second, the paper proposes to take into account the schema of the graph, and in particular the labels along the edges of paths. The paper shows that this allows treating tasks that have traditionally been seen as different, such as information retrieval and item recommendation, in the same model. This contribution earned the work the best paper award of the PIKM 2011. In this article, we have looked at the area of knowledge discovery from the viewpoint of Ph.D. students. We have presented some promising research proposals, which treat not just the area of knowledge discovery but also the neighboring fields of database management and information retrieval. PIKM 2011 was the fourth Ph.D. workshop in a series of such workshops. Looking back, we see that PIKM 2007 concentrated on topics such as fuzzy clustering, linguistic categorization, online classification, rule-based processing and collaborative knowledge management frameworks. In 2008, the focus was on social networking, text mining and speech information retrieval. In 2010, the research areas tilted towards security, quality and ranking, getting more interdisciplinary. In PIKM 2011, three new topics emerged: mining in social media [5], mining in the Semantic We b [6], and main memory databases [7]. We see this as a proof of the growing attraction of these domains. In addition, one theme that caught particular attention across multiple areas was the evolution of resources over time, be it in the area of ETL processing [18], in the area of XML [2], or in database versioning [17]. We also see an overarching topic of process management in general, in the sense of resource monitoring [1] and regulatory compliance [12], as well as in the sense of human cost optimization when producing IR test collections [4]. After four successful Ph.D. workshops in Information and Knowledge Management, the PIKMs in 2007, 2008, 2010 and 2011, we hope to continue these events in future conferences. We believe that such workshops benefit not just Ph.D. students, but also the research community as a whole, since Ph.D. thesis proposals point out new research avenues and provide fresh viewpoints from the researchers of tomorrow. Anisoara Nica holds a Ph.D. in Computer Science and Engineering from the University of Michigan, Ann Arbor, USA, 1999, with the dissertation in the areas of information integration systems and data warehousing. She is currently Distinguished Engineer in the SQL Anywhere Research and Development team of Sybase, An SAP Company, in Waterloo, Canada. Her research interests and expertise are focused on database management systems, in particular query processing and query optimization, data warehousing, distributed, mobile, and parallel databases. Her work experience includes Lawrence Berkeley National Laboratory and International Computer Science Institute in Berkeley. Dr. Nica holds eight patents and has several other patents pending. She has published more than 25 research articles, and she reviews for NSERC, ACM SIGMOD, IEEE ICDE, ACM CIKM .
 Fabian Suchanek is the leader of the Otto Hahn Research Group "Ontologies" at the Max Planck Institute for Informatics in Germany. In his Ph.D. thesis, Fabian developed inter alia the YAGO-Ontology, earning him a honorable mention of the SIGMOD dissertation award. His interests include information extraction, automated reasoning, and ontologies in general. Fabian has published around 30 scientific articles, and he reviews for conferences and journals such as ACL, EDBT, WSDM, WWW, TKDE, TODS, AIJ, JWS, and AAAI.
 Aparna Varde , Computer Science Faculty at Montclair State University (New Jersey), has a Ph.D. from WPI (Massachusetts) with a dissertation in the data mining area. Her interests include scientific data mining and text mining with projects in green IT, nanoscale analysis, markup languages, terminology evolution and collocation, overlapping AI &amp; DB areas. She has 45 publications and 2 trademarks. Her students are supported by grants from PSE&amp;G, NSF, Roche &amp; Merck. She has served as a panelist for NSF; journal reviewer for TKDE, VLDBJ, DKE, KAIS and DMKD; and PC member at ICDM, SDM and EDBT conferences.
 Figure 2: Edge sampling via weighted random walks for training pairs. In our sampling scheme, the first sampling pair is derived from the edge sampling over the whole network. Then, the following training vertices are selected from the weighted random walk on the preceding vertex. vertex. Note that it is straightforward to enlarge the window size to model higher-order proximity.
To embed the user preference into the vertices v , the proposed HPE model updates the vertex representation  X  according to the sampling proximities derived from the pref-erence network. In other words, we treat those indirect connected vertices as the contextual information of the cen-ter vertex, which can be represented as follows:
Maximizing the above posterior probability is equivalent to minimizing the negative log likelihood, so the objective function of the second-order proximity can be represented as follows: where S is a set of sampling pairs and w indicates the weight of the edge. In practice, the observed edges are all posi-tive information and thus falls into the one-class prediction problem. We adopt the widely-used solution called negative sampling to sample the additional user-to-other-entity pairs from the unobserved data. This works well in point-wise optimization functions [ 12 ].

In the heterogeneous preference embedding, a regularized term is adopted to avoid the over-fitting problem. That is due to the fact that we seek to preserve the inference ability that can match the vertices containing the similar contexts, rather than match those vertices containing exactly the same con-text. In addition, we also adopt the asynchronous stochastic gradient descent (ASGD) [8 ] algorithm to optimize Equa-tion 3. The overall procedure is summarized in Algorithm 1.
Three music listening datasets are employed to assess the performance of our proposed method. The first one is the lastfm-dataset-1K dataset, 1 which contains the listening logs http://www.dtic.upf.edu/  X ocelma/ Algorithm 1: Heterogeneous Preference Embedding Input: User Preference Network: G ( V,E ),
Walk Steps: w , Sampling Times: n for v  X  V do 2 Initialize the representation:  X ( v ) and context v for i  X  X  1 ,...,n } do 4 ( v 1 ,v 2 ) = EdgeSampling ( G ) 5 Update  X ( v 1 ), context v 2 by minimizing Eq. (3 ) 6 Update  X ( v 2 ), context v 1 by minimizing Eq. (3 ) 7 for v 0  X  RandomWalk ( v 2 ,w  X  1) do 8 Update  X ( v 1 ), context v 0 by minimizing Eq. (3 ) Output: Vertex representations  X  from the website of Last.fm. The second one, released by EchoNest, 2 is a music taste profile subset derived from the official user dataset of the Million Song Dataset (MSD). The third one is a dataset with user listening logs provided by KKBOX Inc., which is a regional leading music streaming company. The third dataset covers user listening logs from 2014 to 2015. Table 1 lists the statistics of the three datasets.
For query-based recommendations, most users may only care about the top recommendations. Therefore, we adopt 1) precision at k (P@ k ) and 2) mean Average Precision at k (mAP@ k ) as the evaluation metrics, where k indicates the number of the cut-off recommended items. Calculating the precision with a small k is equivalent to examining the possible hit ratio of the users in top recommendations.
In the following experiments, we randomly select 70% of the listening history for each user as the training logs, and put the rest 30% logs in the test set for the off-line evaluation. In the testing stage, we randomly select 5 queries from the test logs of each user, and ask for matching the recommendations to these selected query items.

Note that the ground truth of the query-based recommen-dations shall depend on both the user and the user X  X  query (even when we set  X  = 0 in this work). Therefore the best returned results are varied from person to person. Treating the query as a context information, we assume that a user may tend to listen to similar songs in a short period, thereby considering the songs co-listened within the time period as the ground truth. In the lastfm-1k and KKBOX datasets, we consider the co-listen frequency that is higher than 3 as the ground truth. In MSD, we do not further filter the ground truth because it does not contain the primitive listening logs.
To verify the effectiveness of the proposed method, we compare it with other state-of-the-art approaches, including one simple method (i.e. popularity-based), one CF-based model (i.e. matrix factorization) and two embedding models (i.e. DeepWalk and LINE-2nd). The similarities among the entities are measured by cosine similarity. Below we briefly describe these baseline methods: http://labrosa.ee.columbia.edu/millionsong/tasteprofile
Table 2 lists the experimental results in terms of Precision @10 (upper table) and mAP @10 (bottom table). We can find that the three embedding models, including the proposed HPE model, achieve a comparable performance to the MF model for the three datasets, indicating that the embedding models can capable of capturing the query intentions form the pref-erence network. Note that the recommendation performance drops for the DeepWalk and LINE methods degrades when the dimensions of the vectors become higher. In contrast, the proposed preference embedding with regularized terms can prevent the learned representation from over-fitting, yielding better recommendation performance in most cases.
In this paper, we propose the HPE model that is designed towards the application of  X  X uery-based recommendation. X  The HPE exploits not only the observed preference but also the possible relevant entities via the user preference network. Therefore, by embedding such proximity information, the http://www.mymedialite.net learned representation can better reflect the query intention in comparison to the CF-based model. Moreover, for the proposed preference embedding, we adopt the regularized learning to alleviates the over-fitting issue while learning the embedded representations. Our experiments confirm the effectiveness of the proposed method in making recommen-dations for the task of query-based recommendation.
 Series Editors Randy Goebel, University of Alberta, Edmonton, Canada J X rg Siekmann, University of Saarland, Saarbr X cken, Germany Wolfgang Wahlster, DFKI and University of Saarland, Saarbr X cken, Germany Volume Editors Mohammed J. Zaki Rensselaer Polytechnic Institute Troy, NY, USA E-mail: zaki@cs.rpi.edu Jeffrey Xu Yu The Chinese University of Hong Kong Hong Kong, China E-mail: yu@se.cuhk.edu.hk B. Ravindran IIT Madras, Chennai, India E-mail: ravi@cse.iitm.ac.in Vikram Pudi IIIT, Hyderabad, India E-mail: vikram@iiit.ac.in Library of Congress Control Number: 2010928262 CR Subject Classification (1998): I.2, H.3, H.4, H.2.8, I.4, C.2 LNCS Sublibrary: SL 7  X  Artificial Intelligence ISSN 0302-9743 ISBN-10 3-642-13656-7 Springer Berlin Heidelberg New York ISBN-13 978-3-642-13656-6 Springer Berlin Heidelberg New York The 14th Pacific-Asia Conference on Knowledge Discovery and Data Mining was held in Hyderabad, India during June 21 X 24, 2010; this was the first time the conference was held in India.
 and knowledge discovery in databases (KDD). It provides an international fo-rum for researchers and industry practitioners to share their new ideas, original research results and practical development experiences from all KDD-related areas including data mining, data warehousing, machine learning, databases, statistics, knowledge acquisition and automatic scientific discovery, data visual-ization, causal induction and knowledge-based systems.
 ing: Australia, Austria, Belgium, Canada, China, Cuba, Egypt, Finland, France, Germany, Greece, Hong Kong, India, Iran, Italy, Japan, S. Korea, Malaysia, Mexico, The Netherlands, New Caledonia, New Zealand, San Marino, Singapore, Slovenia, Spain, Switzerland, Taiwan, Thailand, Tunisia, Turkey, UK, USA, and Vietnam. This clearly reflects the truly international stature of the PAKDD conference.
 papers that did not conform to the submission guidelines or that were deemed not worthy of further reviews, 60 papers were rejected with a brief explana-tion for the decision. The remaining 352 papers were rigorously reviewed by at least three reviewers. The initial results were discussed among the reviewers and finally judged by the Program Committee Chairs. In some cases of con-flict additional reviews were sought. As a result of the deliberation process, only 42 papers (10.2%) were accepted as long presentations (25 mins), and an addi-tional 55 papers (13.3%) were accepted as short presentations (15 mins). The total acceptance rate was thus about 23.5% across both categories. shop on Data Mining for Healthcare Management (DMHM 2010), Pacific Asia Workshop on Intelligence and Security Informatics (PAISI 2010), Workshop on Feature Selection in Data Mining (FSDM 2010), Workshop on Emerging Re-search Trends in Vehicle Health Management (VHM 2010), Workshop on Behav-ior Informatics (BI 2010), Workshop on Data Mining and Knowledge Discovery for e-Governance (DMEG 2010), Workshop on Knowledge Discovery for Rural Systems (KDRS 2010).
 Program Committee members (164), external reviewers (195), Conference Orga-nizing Committee members, invited speakers, authors, tutorial presenters, work-shop organizers, reviewers, authors a nd the conference attendees. We highly appreciate the conscientious reviews provided by the Program Committee members, and external reviewers. The Program Committee members were matched with the papers using the SubSift system (http://subsift.ilrt.bris.ac.uk/) for bid matching; we thank Simon Price and Peter Flach, of Bristol University, for developing this wonderful system. Thanks also to Andrei Voronkov for host-ing the entire PAKDD reviewing process on the easychair.org site.
 invaluable suggestions and support throughout the organization process. We thank Vikram Pudi (Publication Chair), Pabitra Mitra (Workshops Chair), Ka-mal Karlapalem (Tutorials Chair), and Arnab Bhattacharya (Publicity Chair). Special thanks to the Local Arrangements Commitee and Chair R.K. Bagga, and the General Chairs: Jaideep Srivastava, Masaru Kitsuregawa, and P. Krishna Reddy. We would also like to thank all those who contributed to the success of PAKDD 2010 but whose names may not be listed.
 was organized by IIIT Hyderabad. It was sponsored by the Office of Naval Re-search Global (ONRG) and the Air Force Office of Scientific Research/Asian Office of Aerospace Research and Development (AFOSR/AOARD).
 cutting edge research in data mining and knowledge discovery. We also hope all participants took this opportunity to share and exchange ideas with each other and enjoyed the cultural and social attractions of the wonderful city of Hyderabad! June 2010 Mohammed J. Zaki Rajeev Sangal IIIT Hyderabad, India Jaideep Srivastava University of Minnesota, USA Masaru Kitsuregawa University of Tokyo, Japan P. Krishna Reddy IIIT Hyderabad, India Mohammed J. Zaki Rensselaer Polytechnic Institute, USA Jeffrey Xu Yu The Chinese University of Hong Kong B. Ravindran IIT Madras, India Pabitra Mitra IIT Kharagpur, India Kamal Karlapalem IIIT Hyderabad, India Arnab Bhattacharya IIT Kanpur, India Vikram Pudi IIIT Hyderabad, India Raji Bagga (Chair) IIIT Hyderabad, India K.S. Vijaya Sekhar IIIT Hyderabad, India T. Ragunathan IIIT Hyderabad, India P. Radhakrishna Infosys SET Labs, Hyderabad, India A. Govardhan JNTU, Hyderabad, India R.B.V. Subramanyam NIT, Warangal, India Osman Abul Muhammad Abulaish Arun Agarwal Hisham Al-Mubaid Reda Alhajj Hiroki Arimura Hideo Bannai Jayanta Basak M.M. Sufyan Beg Bettina Berendt Fernando Berzal Raj Bhatnagar Vasudha Bhatnagar Arnab Bhattacharya Pushpak Bhattacharyya Chiranjib Bhattacharyya Vivek Borkar Keith C.C. Chan Longbing Cao Doina Caragea Venkatesan Chakaravarthy Vineet Chaoji Sanjay Chawla Arbee Chen Phoebe Chen Jake Yue Chen Zheng Chen Hong Cheng James Cheng Alok Choudhary Diane Cook Alfredo Cuzzocrea Sanmay Das Anne Denton Lipika Dey Guozhu Dong Petros Drineas Tina Eliassi-Rad Wei Fan Eibe Frank Benjamin C.M. Fung Sachin Garg Mohamed Gaber Yang-Sae Moon Yasuhiko Morimoto Tsuyoshi Murata Atsuyoshi Nakamura J. Saketha Nath Juggapong Natwichai Richi Nayak Wilfred Ng Mitsunori Ogihara Salvatore Orlando Satoshi Oyama Prabin Panigrahi Spiros Papadimitriou Srinivasan Parthasarathy Wen-Chih Peng Bernhard Pfahringer Srinivasa Raghavan R. Rajesh Naren Ramakrishnan Ganesh Ramakrishnan Jan Ramon Sanjay Ranka Rajeev Rastogi Chandan Reddy Patricia Riddle S. Raju Bapi Saeed Salem Sudeshna Sarkar Tamas Sarlos C. Chandra Sekhar Srinivasan Sengamedu Shirish Shevade M. Shimbo Abdul Nizar Abhinav Mishra Alessandra Raffaeta Aminul Islam Andrea Tagarelli Anitha Varghese Ankit Agrawal Anuj Mahajan Anupam Bhattacharjee Chao Luo Chen-Yi Lin Chih jui Lin Wang Chuancong Gao Chun Kit Chui Chun Wei Seah Chun-Hao Chen Chung An Yeh Claudio Silvestri Da Jun Li David Uthus De-Chuan Zhan Delia Rusu Dhruv Mahajan Di Wang Dinesh Garg Elena Ikonomovska En Tzu Wang Eugenio Cesario Feilong Chen Feng Chen Ferhat Ay Gokhan Yavas Gongqing Wu Hea-Suk Kim Hideyuki Kawashima Hui Zhu Su Ilija Subasic Indranil Palit Jan Rupnik Janez Brank Jeyashanker Ramamirtham Jitendra Ajmera Junjie Wu Kathy Macropol Khalil Al-Hussaeni Kong Wah Wan Krishna Prasad Chitrapura Kunpeng Zhang Kyle Chipman L. Venkata Subramaniam Lang Huo Lei Liu Lei Shi Lei Yang Prithviraj Sen Pruet Boonma Qi Mao Qiang Wang Qingyan Yang Quan Yuan Quang Khoat Than Rahul Chougule Ramanathan Narayanan Raquel Sebastiao Rashid Ali Rui Chen S. Sathiya Keerthi Shailesh Kumar Sai Sundarakrishna Saikat Dey J. Saketha Nath SakethaNath Jagarlapudi Salim Akhter Chowdhury Samah Fodeh Sami Hanhij  X  arvi Satnam Singh Sau Dan Lee Sayan Ranu Sergio Flesca Shafkat Amin Shailesh Kumar Shalabh Bhatnagar Shantanu Godbole Sharanjit Kaur Shazzad Hosain Shenghua Gao Shirish Shevade Shirish Tatikonda Shirong Li Shumo Chu Shuo Miao Sinan Erten Sk. Mirajul Haque Soumen De Sourangshu Bhattacharya Sourav Dutta Srinivasan Sengamedu Srujana Merugu IIIT Hyderabad, India AFOSR, USA AOARD, Tokyo, Japan ONRG, USA Empower People with Knowledge: The Next Frontier for Web Search ... 1 Discovery of Patterns in Global Earth Science Data Using Data Mining ......................................................... 2 Game Theoretic Approaches to Knowledge Discovery and Data Mining ......................................................... 3 A Set Correlation Model for Partitional Clustering ................... 4 iVAT and aVAT: Enhanced Visual Analysis for Cluster Tendency Assessment ...................................................... 16 A Robust Seedless Algorithm for Correlation Clustering .............. 28 Integrative Parameter-Free Clustering of Data with Mixed Type Attributes ...................................................... 38 Data Transformation for Sum Squared Residue ...................... 48 A Better Strategy of Discovering Link-Pattern Based Communities by Classical Clustering Methods ...................................... 56 Mining Antagonistic Communities from Social Networks .............. 68 As Time Goes by: Discovering Eras in Evolving Social Networks ....... 81 Online Sampling of High Centrality Individuals in Social Networks ..... 91 Estimate on Expectation for Influence Maximization in Social Networks ....................................................... 99 A Novel Scalable Multi-class ROC for Effective Visualization and Computation .................................................... 107 Efficiently Finding the Best Parameter for the Emerging Pattern-Based Classifier PCL ................................................... 121 Rough Margin Based Core Vector Machine .......................... 134 BoostML: An Adaptive Metric Learning for Nearest Neighbor Classification .................................................... 142 A New Emerging Pattern Mining Algorithm and Its Application in Supervised Classification .......................................... 150 Hiding Emerging Patterns with Local Recoding Generalization ........ 158 Anonymizing Transaction Data by Integrating Suppression and Generalization ................................................... 171 Satisfying Privacy Requirements: One Step before Anonymization ...... 181 Computation of Ratios of Secure Summations in Multi-party Privacy-Preserving Latent Dirichlet Allocation ....................... 189 Privacy-Preserving Network Aggregation ............................ 198 Multivariate Equi-width Data Swapping for Private Data Publication ... 208 Correspondence Clustering: An Approach to Cluster Multiple Related Spatial Datasets ................................................. 216 Mining Trajectory Corridors Using Fr  X  echet Distance and Meshing Grids ........................................................... 228 Subseries Join: A Similarity-Based Time Series Match Approach ....... 238 TWave: High-Order Analysis of Spatiotemporal Data ................. 246 Spatial Clustering with Obstacles Constraints by Dynamic Piecewise-Mapped and Nonlinear Inertia Weights PSO ................ 254 An Efficient GA-Based Algorithm for Mining Negative Sequential Patterns ........................................................ 262 Valency Based Weighted Association Rule Mining .................... 274 Ranking Sequential Patterns with Respect to Significance ............. 286 Mining Association Rules in Long Sequences ........................ 300 Mining Closed Episodes from Event Sequences Efficiently ............. 310 Most Significant Substring Mining Based on Chi-square Measure ....... 319 Probabilistic User Modeling in the Presence of Drifting Concepts ...... 328 Using Association Rules to Solve the Cold-Start Problem in Recommender Systems ........................................... 340 Semi-supervised Tag Recommendation -Using Untagged Resources to Mitigate Cold-Start Problems ..................................... 348 Cost-Sensitive Listwise Ranking Approach .......................... 358 Mining Wikipedia and Yahoo! Answers for Question Expansion in Opinion QA ..................................................... 367 Answer Diversification for Complex Question Answering on the Web ... 375 Vocabulary Filtering for Term Weighting in Archived Question Search .......................................................... 383 On Finding the Natural Number of Topics with Latent Dirichlet Allocation: Some Observations ..................................... 391 Supervising Latent Topic Model for Maximum-Margin Text Classification and Regression ...................................... 403 Resource-bounded Information Extraction: Acquiring Missing Feature Values On Demand ............................................... 415 Efficient Deep Web Crawling Using Reinforcement Learning ........... 428 Topic Decomposition and Summarization ........................... 440 UNN: A Neural Network for Uncertain Data Classification ............ 449 SkyDist: Data Mining on Skyline Objects ........................... 461 Multi-Source Skyline Queries Processing in Multi-Dimensional Space ... 471 Efficient Pattern Mining of Uncertain Data with Sampling ............ 480 Classifier Ensemble for Uncertain Data Stream Classification .......... 488 Author Index .................................................. 497 Subclass-Oriented Dimension Reduction with Constraint Transformation and Manifold Regularization ........................ 1 Distributed Knowledge Discovery with Non Linear Dimensionality Reduction ....................................................... 14 DPSP: Distributed Progressive Sequential Pattern Mining on the Cloud .......................................................... 27 An Approach for Fast Hierarchical Agglomerative Clustering Using Graphics Processors with CUDA ................................... 35 Ontology-Based Mining of Brainwaves: A Sequence Similarity Technique for Mapping Alternative Features in Event-Related Potentials (ERP) Data ........................................................... 43 Combining Support Vector Machines and the t -statistic for Gene Selection in DNA Microarray Data Analysis ......................... 55 Satrap: Data and Network Heterogeneity Aware P2P Data-Mining ..... 63 Player Performance Prediction in Massively Multiplayer Online Role-Playing Games (MMORPGs) ................................. 71 Relevant Gene Selection Using Normalized Cut Clustering with Maximal Compression Similarity Measure ........................... 81 A Novel Prototype Reduction Method for the K -Nearest Neighbor Algorithm with K  X  1 ............................................ 89 Generalized Two-Dimensional FLD Method for Feature Extraction: An Application to Face Recognition ................................... 101 Learning Gradients with Gaussian Processes ......................... 113 Analyzing the Role of Dimension Arrangement for Data Visualization in Radviz ....................................................... 125 Subgraph Mining on Directed and Weighted Graphs .................. 133 Finding Itemset-Sharing Patterns in a Large Itemset-Associated Graph .......................................................... 147 A Framework for SQL-Based Mining of Large Graphs on Relational Databases ....................................................... 160 Fast Discovery of Reliable k -terminal Subgraphs ..................... 168 GTRACE2: Improving Performance Using Labeled Union Graphs ...... 178 Orthogonal Nonnegative Matrix Tri-factorization for Semi-supervised Document Co-clustering .......................................... 189 Rule Synthesizing from Multiple Related Databases .................. 201 Fast Orthogonal Nonnegative Matrix Tri-Factorization for Simultaneous Clustering ...................................................... 214 Hierarchical Web-Page Clustering via In-Page and Cross-Page Link Structures ...................................................... 222 Mining Numbers in Text Using Suffix Arrays and Clustering Based on Dirichlet Process Mixture Models .................................. 230 Opinion-Based Imprecise Query Answering .......................... 238 Blog Opinion Retrieval Based on Topic-Opinion Mixture Model ........ 249 Feature Subsumption for Sentiment Classification in Multiple Languages ...................................................... 261 Decentralisation of ScoreFinder: A Framework for Credibility Management on User-Generated Contents ........................... 272 Classification and Pattern Discovery of Mood in Weblogs ............. 283 Capture of Evidence for Summarization: An Application of Enhanced Subjective Logic ................................................. 291 Fast Perceptron Decision Tree Learning from Evolving Data Streams ... 299 Classification and Novel Class Detection in Data Streams with Active Mining ......................................................... 311 Bulk Loading Hierarchical Mixture Models for Efficient Stream Classification .................................................... 325 Summarizing Multidimensional Data Streams: A Hierarchy-Graph-Based Approach ................................................. 335 Efficient Trade-Off between Speed Processing and Accuracy in Summarizing Data Streams ....................................... 343 Subsequence Matching of Stream Synopses under the Time Warping Distance ........................................................ 354 Normalized Kernels as Similarity Indices ............................ 362 Adaptive Matching Based Kernels for Labelled Graphs ............... 374 A New Framework for Dissimilarity and Similarity Learning ........... 386 Semantic-Distance Based Clustering for XML Keyword Search ......... 398 OddBall : Spotting Anomalies in Weighted Graphs ................... 410 Robust Outlier Detection Using Commute Time and Eigenspace Embedding ...................................................... 422 EigenSpokes: Surprising Patterns and Scalable Community Chipping in Large Graphs ................................................. 435 BASSET: Scalable Gateway Finder in Large Graphs .................. 449 Ensemble Learning Based on Multi-Task Class Labels ................ 464 Supervised Learning with Minimal Effort ........................... 476 Generating Diverse Ensembles to Counter the Problem of Class Imbalance ....................................................... 488 Relationship between Diversity and Correlation in Multi-Classifier Systems ........................................................ 500 Compact Margin Machine ......................................... 507 Author Index .................................................. 515 The Web has continued to evolve quickl y. With the emergence of cloud comput-ing, we see a new opportunity of creating a cloud platform to leverage developer ecosystem and enabling the development of millions of micro-vertical services and applications to serve users X  various information need. In this new world, there is an opportunity to build a more powerful and intelligent search engine that understands what users are trying to accomplish and helps them learn, decide and take actions. In this talk, I will first discuss a few new trends from cloud computing that will impact web search, and then I will share my thoughts on possible directions to tap into this new wave and develop not only innovative but also potentially disruptive technologies for Web search.
 The climate and earth sciences have recently undergone a rapid transformation from a data-poor to a data-rich environment. In particular, climate and ecosys-tem related observations from remote sensors on satellites, as well as outputs of climate or earth system models from large-scale computational platforms, pro-vide terabytes of temporal, spatial and spatio-temporal data. These massive and information-rich datasets offer huge potential for understanding and predicting the behavior of the Earth X  X  ecosystem and for advancing the science of climate change.

However, mining patterns from Earth Science data is a difficult task due to the spatio-temporal nature of the data. This talk will discuss various challenges involved in analyzing the data, and present some of our work on the design of algorithms for finding spatio-temporal patterns from such data and their appli-cations in discovering interesting relationships among ecological variables from various parts of the Earth. A special focus will be on techniques for land cover change detection (and their use in asse ssing the impact on carbon cycle) and finding teleconnections between ocean and land variables.
 Game theory is replete with brilliant solution concepts such as the Nash equi-librium, the core, the Shapley value, etc. These solution concepts and their ex-tensions are finding widespread use in solving several fundamental problems in knowledge discovery and data mining. The problems include clustering, classifi-cation, discovering influential nodes, social network analysis, etc. The first part of the talk will present the conceptual underpinnings underlying the use of game theoretic techniques in such problem solving. The second part of the talk will delve into two problems where we have recen tly obtained some interesting results: (a) Discovering influential nodes in social networks using the Shapley value, and (b) Identifying topologies of strategically formed social networks using a game theoretic approach.

 Series Editors Randy Goebel, University of Alberta, Edmonton, Canada J X rg Siekmann, University of Saarland, Saarbr X cken, Germany Wolfgang Wahlster, DFKI and University of Saarland, Saarbr X cken, Germany Volume Editors Mohammed J. Zaki Rensselaer Polytechnic Institute Troy, NY, USA E-mail: zaki@cs.rpi.edu Jeffrey Xu Yu The Chinese University of Hong Kong Hong Kong, China E-mail: yu@se.cuhk.edu.hk B. Ravindran IIT Madras, Chennai, India E-mail: ravi@cse.iitm.ac.in Vikram Pudi IIIT, Hyderabad, India E-mail: vikram@iiit.ac.in Library of Congress Control Number: 2010928262 CR Subject Classification (1998): I.2, H.3, H.4, H.2.8, I.4, C.2 LNCS Sublibrary: SL 7  X  Artificial Intelligence ISSN 0302-9743 ISBN-10 3-642-13671-0 Springer Berlin Heidelberg New York ISBN-13 978-3-642-13671-9 Springer Berlin Heidelberg New York The 14th Pacific-Asia Conference on Knowledge Discovery and Data Mining was held in Hyderabad, India during June 21 X 24, 2010; this was the first time the conference was held in India.
 and knowledge discovery in databases (KDD). It provides an international fo-rum for researchers and industry practitioners to share their new ideas, original research results and practical develop ment experiences from all KDD-related areas including data mining, data warehousing, machine learning, databases, statistics, knowledge acquisition and auto matic scientific discovery, data visual-ization, causal induction and knowledge-based systems.
 ing: Australia, Austria, Belgium, Canada, China, Cuba, Egypt, Finland, France, Germany, Greece, Hong Kong, India, Iran , Italy, Japan, S. Korea, Malaysia, Mexico, The Netherlands, New Caledonia, New Zealand, San Marino, Singapore, Slovenia, Spain, Switzerland, Taiwan, Thailand, Tunisia, Turkey, UK, USA, and Vietnam. This clearly reflects the trul y international stature of the PAKDD conference.
 papers that did not conform to the submission guidelines or that were deemed not worthy of further reviews, 60 pape rs were rejected with a brief explana-tion for the decision. The remaining 352 papers were rigorously reviewed by at least three reviewers. The initial resu lts were discussed among the reviewers and finally judged by the Program Committee Chairs. In some cases of con-flict additional reviews were sought. As a result of the deliberation process, only 42 papers (10.2%) were accep ted as long presentations (25 mins), and an addi-tional 55 papers ( 13.3%) were accepted as short pr esentations (15 mins). The total acceptance rate was thus abou t 23.5% across both categories. shop on Data Mining for Healthcare Management (DMHM 2010), Pacific Asia Workshop on Intelligence and Security In formatics (PAISI 2010), Workshop on Feature Selection in Data Mining (FSDM 2010), Workshop on Emerging Re-search Trends in Vehicle Health Management (VHM 2010), Workshop on Behav-ior Informatics (BI 2010), Workshop on Data Mining and Knowledge Discovery for e-Governance (DMEG 2010), Workshop on Knowledge Discovery for Rural Systems (KDRS 2010).
 Program Committee members (164), external reviewers (195), Conference Orga-nizing Committee members, invited speakers, authors, tutorial presenters, work-shop organizers, reviewers, authors and the conference attendees. We highly appreciate the conscientious revie ws provided by the Program Committee members, and external reviewers . The Program Committee members were matched with the papers using the SubSift sy stem (http://subsift.ilrt.bris.ac.uk/) for bid matching; we thank Simon Price and Peter Flach, of Bristol University, for developing this wonderful system. Thanks also to Andrei Voronkov for host-ing the entire PAKDD reviewing process on the easychair.org site. invaluable suggestions and support throughout the organization process. We thank Vikram Pudi (Publication Chair), Pabitra Mitra (Workshops Chair), Ka-mal Karlapalem (Tutorials Chair), and Arnab Bhattacharya (Publicity Chair). Special thanks to the Local Arrangements Commitee and Chair R.K. Bagga, and the General Chairs: Jaideep Srivastava, Masaru Kitsuregawa, and P. Krishna Reddy. We would also like to thank all those who contributed to the success of PAKDD 2010 but whose names may not be listed.
 was organized by IIIT Hyderabad. It was sponsored by the Office of Naval Re-search Global (ONRG) and the Air Force Office of Scientific Research/Asian Office of Aerospace Research and Development (AFOSR/AOARD).
 cutting edge research in data mining and knowledge discovery. We also hope all participants took this opportunity to share and exchange ideas with each other and enjoyed the cultural and social attractions of the wonderful city of Hyderabad! June 2010 Mohammed J. Zaki Rajeev Sangal IIIT Hyderabad, India Jaideep Srivastava University of Minnesota, USA Masaru Kitsuregawa University of Tokyo, Japan P. Krishna Reddy IIIT Hyderabad, India Mohammed J. Zaki Rensselaer Polytechnic Institute, USA Jeffrey Xu Yu The Chinese University of Hong Kong B. Ravindran IIT Madras, India Pabitra Mitra IIT Kharagpur, India Kamal Karlapalem IIIT Hyderabad, India Arnab Bhattacharya IIT Kanpur, India Vikram Pudi IIIT Hyderabad, India Raji Bagga (Chair) IIIT Hyderabad, India K.S. Vijaya Sekhar IIIT Hyderabad, India T. Ragunathan IIIT Hyderabad, India P. Radhakrishna Infosys SET Labs, Hyderabad, India A. Govardhan JNTU, Hyderabad, India R.B.V. Subramanyam NIT, Warangal, India Osman Abul Muhammad Abulaish Arun Agarwal Hisham Al-Mubaid Reda Alhajj Hiroki Arimura Hideo Bannai Jayanta Basak M.M. Sufyan Beg Bettina Berendt Fernando Berzal Raj Bhatnagar Vasudha Bhatnagar Arnab Bhattacharya Pushpak Bhattacharyya Chiranjib Bhattacharyya Vivek Borkar Keith C.C. Chan Longbing Cao Doina Caragea Venkatesan Chakaravarthy Vineet Chaoji Sanjay Chawla Arbee Chen Phoebe Chen Jake Yue Chen Zheng Chen Hong Cheng James Cheng Alok Choudhary Diane Cook Alfredo Cuzzocrea Sanmay Das Anne Denton Lipika Dey Guozhu Dong Petros Drineas Tina Eliassi-Rad Wei Fan Eibe Frank Benjamin C.M. Fung Sachin Garg Mohamed Gaber Yang-Sae Moon Yasuhiko Morimoto Tsuyoshi Murata Atsuyoshi Nakamura J. Saketha Nath Juggapong Natwichai Richi Nayak Wilfred Ng Mitsunori Ogihara Salvatore Orlando Satoshi Oyama Prabin Panigrahi Spiros Papadimitriou Srinivasan Parthasarathy Wen-Chih Peng Bernhard Pfahringer Srinivasa Raghavan R. Rajesh Naren Ramakrishnan Ganesh Ramakrishnan Jan Ramon Sanjay Ranka Rajeev Rastogi Chandan Reddy Patricia Riddle S. Raju Bapi Saeed Salem Sudeshna Sarkar Tamas Sarlos C. Chandra Sekhar Srinivasan Sengamedu Shirish Shevade M. Shimbo Abdul Nizar Abhinav Mishra Alessandra Raffaeta Aminul Islam Andrea Tagarelli Anitha Varghese Ankit Agrawal Anuj Mahajan Anupam Bhattacharjee Chao Luo Chen-Yi Lin Chih jui Lin Wang Chuancong Gao Chun Kit Chui Chun Wei Seah Chun-Hao Chen Chung An Yeh Claudio Silvestri Da Jun Li David Uthus De-Chuan Zhan Delia Rusu Dhruv Mahajan Di Wang Dinesh Garg Elena Ikonomovska En Tzu Wang Eugenio Cesario Feilong Chen Feng Chen Ferhat Ay Gokhan Yavas Gongqing Wu Hea-Suk Kim Hideyuki Kawashima Hui Zhu Su Ilija Subasic Indranil Palit Jan Rupnik Janez Brank Jeyashanker Ramamirtham Jitendra Ajmera Junjie Wu Kathy Macropol Khalil Al-Hussaeni Kong Wah Wan Krishna Prasad Chitrapura Kunpeng Zhang Kyle Chipman L. Venkata Subramaniam Lang Huo Lei Liu Lei Shi Lei Yang Prithviraj Sen Pruet Boonma Qi Mao Qiang Wang Qingyan Yang Quan Yuan Quang Khoat Than Rahul Chougule Ramanathan Narayanan Raquel Sebastiao Rashid Ali Rui Chen S. Sathiya Keerthi Shailesh Kumar Sai Sundarakrishna Saikat Dey J. Saketha Nath SakethaNath Jagarlapudi Salim Akhter Chowdhury Samah Fodeh Sami Hanhij  X  arvi Satnam Singh Sau Dan Lee Sayan Ranu Sergio Flesca Shafkat Amin Shailesh Kumar Shalabh Bhatnagar Shantanu Godbole Sharanjit Kaur Shazzad Hosain Shenghua Gao Shirish Shevade Shirish Tatikonda Shirong Li Shumo Chu Shuo Miao Sinan Erten Sk. Mirajul Haque Soumen De Sourangshu Bhattacharya Sourav Dutta Srinivasan Sengamedu Srujana Merugu IIIT Hyderabad, India AFOSR, USA AOARD, Tokyo, Japan ONRG, USA Subclass-Oriented Dimension Reduction with Constraint Transformation and Manifold Regularization ........................ 1 Distributed Knowledge Discovery with Non Linear Dimensionality Reduction ....................................................... 14 DPSP: Distributed Progressive Sequential Pattern Mining on the Cloud .......................................................... 27 An Approach for Fast Hierarchical Agglomerative Clustering Using Graphics Processors with CUDA ................................... 35 Ontology-Based Mining of Brainwaves: A Sequence Similarity Technique for Mapping Alternative Features in Event-Related Potentials (ERP) Data ........................................................... 43 Combining Support Vector Machines and the t -statistic for Gene Selection in DNA Microarray Data Analysis ......................... 55 Satrap: Data and Network Heterogeneity Aware P2P Data-Mining ..... 63 Player Performance Prediction in Massively Multiplayer Online Role-Playing Games (MMORPGs) ................................. 71 Relevant Gene Selection Using Normalized Cut Clustering with Maximal Compression Similarity Measure ........................... 81 A Novel Prototype Reduction Method for the K -Nearest Neighbor Algorithm with K  X  1 ............................................ 89 Generalized Two-Dimensional FLD Method for Feature Extraction: An Application to Face Recognition ................................... 101 Learning Gradients with Gaussian Processes ......................... 113 Analyzing the Role of Dimension Arrangement for Data Visualization in Radviz ....................................................... 125 Subgraph Mining on Directed and Weighted Graphs .................. 133 Finding Itemset-Sharing Patterns in a Large Itemset-Associated Graph .......................................................... 147 A Framework for SQL-Based Mining of Large Graphs on Relational Databases ....................................................... 160 Fast Discovery of Reliable k -terminal Subgraphs ..................... 168 GTRACE2: Improving Performan ce Using Labeled Union Graphs ...... 178 Orthogonal Nonnegative Matrix Tri-factorization for Semi-supervised Document Co-clustering .......................................... 189 Rule Synthesizing from Multiple Related Databases .................. 201 Fast Orthogonal Nonnegative Matrix Tri-Factorization for Simultaneous Clustering ...................................................... 214 Hierarchical Web-Page Clustering via In-Page and Cross-Page Link Structures ...................................................... 222 Mining Numbers in Text Using Suffix Arrays and Clustering Based on Dirichlet Process Mixture Models .................................. 230 Opinion-Based Imprecise Query Answering .......................... 238 Blog Opinion Retrieval Based on Topic-Opinion Mixture Model ........ 249 Feature Subsumption for Sentiment Classification in Multiple Languages ...................................................... 261 Decentralisation of ScoreFinder: A Framework for Credibility Management on User-Generated Contents ........................... 272 Classification and Pattern Discovery of Mood in Weblogs ............. 283 Capture of Evidence for Summarization: An Application of Enhanced Subjective Logic ................................................. 291 Fast Perceptron Decision Tree Learning from Evolving Data Streams ... 299 Classification and Novel Class Detection in Data Streams with Active Mining ......................................................... 311 Bulk Loading Hierarchical Mixture Models for Efficient Stream Classification .................................................... 325 Summarizing Multidimensional Data Streams: A Hierarchy-Graph-Based Approach ................................................. 335 Efficient Trade-Off between Speed Processing and Accuracy in Summarizing Data Streams ....................................... 343 Subsequence Matching of Stream Synopses under the Time Warping Distance ........................................................ 354 Normalized Kernels as Similarity Indices ............................ 362 Adaptive Matching Based Kernels for Labelled Graphs ............... 374 A New Framework for Dissimilarity and Similarity Learning ........... 386 Semantic-Distance Based Clustering for XML Keyword Search ......... 398 OddBall : Spotting Anomalies in Weighted Graphs ................... 410 Robust Outlier Detection Using Commute Time and Eigenspace Embedding ...................................................... 422 EigenSpokes: Surprising Patterns and Scalable Community Chipping in Large Graphs ................................................. 435 BASSET: Scalable Gateway Finder in Large Graphs .................. 449 Ensemble Learning Based on Multi-Task Class Labels ................ 464 Supervised Learning with Minimal Effort ........................... 476 Generating Diverse Ensembles to Counter the Problem of Class Imbalance ....................................................... 488 Relationship between Diversity and Correlation in Multi-Classifier Systems ........................................................ 500 Compact Margin Machine ......................................... 507
 Series Editors Randy Goebel, University of Alberta, Edmonton, Canada J X rg Siekmann, University of Saarland, Saarbr X cken, Germany Wolfgang Wahlster, DFKI and University of Saarland, Saarbr X cken, Germany Volume Editors Joshua Zhexue Huang Chinese Academy of Sciences Shenzhen Institutes of Advanced Technology (SIAT) Shenzhen 518055, China E-mail: zx.huang@siat.ac.cn Longbing Cao University of Technology Sydney Faculty of Engineering and Information Technology Advanced Analytics Institute Center for Quantum Computation and Intelligent Systems Sydney, NSW 2007, Australia E-mail: longbing.cao-1@uts.edu.au Jaideep Srivastava University of Minnesota Department of Computer Science and Engineering Minneapolis, MN 55455, USA E-mail: Srivasta@cs.umn.edu ISSN 0302-9743 e-ISSN 1611-3349 ISBN 978-3-642-20840-9 e-ISBN 978-3-642-20841-6 DOI 10.1007/978-3-642-20841-6 Springer Heidelberg Dordrecht London New York Library of Congress Control Number: 2011926132 CR Subject Classification (1998): I.2, H.3, H.4, H.2.8, I.4, C.2
LNCS Sublibrary: SL 7  X  Artificial Intelligence PAKDD has been recognized as a major international conference in the areas of data mining (DM) and knowledge discovery in databases (KDD). It provides an international forum for researchers and industry practitioners to share their new ideas, original research results and prac tical development experiences from all KDD-related areas including data mining, machine learning, artificial intelligence and pattern recognition, data warehousing and databases, statistics, knowledge engineering, behavioral sciences, visualization, and emerging areas such as social network analysis.
 (PAKDD 2011) was held in Shenzhen, China, during May 24 X 27, 2011. PAKDD 2011 introduced a double-blind review process. It received 331 submissions af-ter checking for validity. Submissions were from 45 countries and regions, which shows a significant improvement in internationalization than PAKDD 2010 (34 countries and regions). All papers were assigned to at least four Program Com-mittee members. Most papers received more than three review reports. As a result of the deliberation process, only 90 papers were accepted, with 32 papers (9.7%) for long presentation and 58 (17.5%) for short presentation. Workshop on Behavior Informatics (BI 2011), Workshop on Advances and Issues in Traditional Chinese Medicine Clinical Data Mining (AI-TCM), Quality Is-sues, Measures of Interestingness and E valuation of Data Mining Models (QIMIE 2011), Biologically Inspired Techniques for Data Mining (BDM 2011), and Work-shop on Data Mining for Healthcare Management (DMHM 2011). PAKDD 2011 also featured talks by three distinguished invited speakers, six tutorials, and a Doctoral Symposium on Data Mining.
 Program Committee members (203), external reviewers (168), Organizing Com-mittee members, invited speakers, authors, tutorial presenters, workshop orga-nizers, reviewers, authors and the conference attendees. We highly appreciate the conscientious reviews provided by the Program Committee members and external reviewers. We are indebted to the members of the PAKDD Steering Committee for their invaluable suggestions and support throughout the orga-nization process. Our special thanks go to the local arrangements team and volunteers. We would also like to thank al l those who contributed to the success of PAKDD 2011 but whose names cannot be listed.
 ference and workshop proceedings. Tha nks also to Andrei Voronkov for hosting the entire PAKDD reviewing pro cess on the EasyChair.org site.
 tutions. The conference was organized by the Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, China, and co-organized by the Uni-versity of Hong Kong, China and the University of Technology Sydney, Australia. edge research in data mining and knowledge discovery. We also hope all partic-ipants took this opportunity to exchange ideas with each other and enjoyed the modem city of Shenzhen! May 2011 Joshua Huang Honorary Chair Philip S. Yu University of Illinois at Chicago, USA General Co-chairs Jianping Fan Shenzhen Institutes of Advanced Technology, David Cheung University of Hong Kong, China Program Committee Co-chairs Joshua Huang Shenzhen Institutes of Advanced Technology, Longbing Cao University of Technology Sydney, Australia Jaideep Srivastava University of Minnesota, USA Workshop Co-chairs James Bailey The University of Melbourne, Australia Yun Sing Koh The University of Auckland, New Zealand Tutorial Co-chairs Xiong Hui Rutgers, the State University of New Jersey, Sanjay Chawla The University of Sydney, Australia Local Arrangements Co-chairs Shengzhong Feng Shenzhen Institutes of Advanced Technology, Jun Luo Shenzhen Institutes of Advanced Technology, Sponsorship Co-chairs Yalei Bi Shenzhen Institutes of Advanced Technology, Zhong Ming Shenzhen University, China Publicity Co-chairs Jian Yang Beijing University of Technology, China Ye Li Shenzhen Institutes of Advanced Technology, Yuming Ou University of Technology Sydney, Australia Publication Chair Longbing Cao University of Technology Sydney, Australia Co-chairs Rao Kotagiri University of Melbourne, Australia Graham Williams Australian National University, Australia Life Members David Cheung University of Hong Kong, China Masaru Kitsuregawa Tokyo University, Japan Rao Kotagiri University of Melbourne, Australia Hiroshi Motoda AFOSR/AOARD and Osaka University, Japan Graham Williams (Treasurer) Australian National University, Australia Ning Zhong Maebashi Institute of Technology, Japan Members Ming-Syan Chen National Taiwan University, Taiwan, ROC Tu Bao Ho Japan Advanced In stitute of Science and Ee-Peng Lim Singapore Management University, Singapore Huan Liu Arizona State University, USA Jaideep Srivastava University of Minnesota, USA Takashi Washio Institute of Scientific and Industrial Research, Thanaruk Theeramunkong Thammasat University, Thailand Kyu-Young Whang Korea Advan ced Institute of Science and Chengqi Zhang University of Technology Sydney, Australia Zhi-Hua Zhou Nanjing University, China Krishna Reddy IIIT, Hyderabad, India Adrian Pearce The University of Melbourne, Australia Aijun An York University, Canada Aixin Sun Nanyang Technological University, Singapore Akihiro Inokuchi Osaka University, Japan Akira Shimazu Japan Advanced Institute of Science and Alfredo Cuzzocrea Univer sity of Calabria, Italy Andrzej Skowron Warsaw University, Poland Anirban Mondal IIIT Delhi, India Aoying Zhou Fudan University, China Arbee Chen National Chengchi University, Taiwan, ROC Aristides Gionis Yahoo Research Labs, Spain Atsuyoshi Nakamura Hokkaido University, Japan Bart Goethals University of Antwerp, Belgium Bernhard Pfahringer University of Waikato, New Zealand Bo Liu University of Technology, Sydney, Australia Bo Zhang Tsinghua University, China Boonserm Kijsirikul Chulalongkorn University, Thailand Bruno Cremilleux Universit  X  e de Caen, France Chandan Reddy Wayne State University, USA Chang-Tien Lu Virginia Tech, USA Chaveevan Pechsiri Dhurakijpundit University, Thailand Chengqi Zhang University of Technology, Australia Chih-Jen Lin National Taiwan University, Taiwan, ROC Choochart Haruechaiyasak NECTEC, Thailand Chotirat Ann Ratanamahatana Chulalongkorn University, Thailand Chun-hung Li Hong Kong Baptist University, Hong Kong, Chunsheng Yang NRC Institute for Information Technology, Clement Yu University of Illinois at Chicago, USA Dacheng Tao The Hong Kong Polytechnic University, Daisuke Ikeda Kyushu University, Japan Dan Luo University of Technology, Sydney, Australia Daoqiang Zhang Nanjing University of Aeronautics and Dao-Qing Dai Sun Yat-Sen University, China David Albrecht Monash University, Australia David Taniar Monash University, Australia Di Wu Chinese University of Hong Kong, China Diane Cook Washington State University, USA Dit-Yan Yeung Hong Kong University of Science and Dragan Gamberger Rudjer Boskovic Institute, Croatia Du Zhang California State University, USA Ee-Peng Lim Nanyang Technological University, Singapore Eibe Frank University of Waikato, New Zealand Evaggelia Pitoura University of Ioannina, Greece Floriana Esposito Universit` a di Bari, Italy Gang Li Deakin University, Australia George Karypis University of Minnesota, USA Graham Williams Australian Taxation Office, Australia Guozhu Dong Wright State University, USA Hai Wang University of Aston, UK Hanzi Wang University of Adelaide, Australia Harry Zhang University of New Brunswick, Canada Hideo Bannai Kyushu University, Japan Hiroshi Nakagawa University of Tokyo, Japan Hiroyuki Kawano Nanzan University, Japan Hiroyuki Kitagawa University of Tsukuba, Japan Hua Lu Aalborg University, Denmark Huan Liu Arizona State University, USA Hui Wang University of Ulster, UK Hui Xiong Rutgers University, USA Hui Yang San Francisco State University, USA Huiping Cao New Mexico State University, USA Irena Koprinska University of Sydney, Australia Ivor Tsang Hong Kong University of Science and James Kwok Hong Kong University of Science and Jason Wang New Jersey Science and Technology University, Jean-Marc Petit INSA Lyon, France Jeffrey Ullman Stanford University, USA Jiacai Zhang Beijing Normal University, China Jialie Shen Singapore Management University, Singapore Jian Yin Sun Yat-Sen University, China Jiawei Han University of Illinois at Urbana-Champaign, Jiuyong Li University of South Australia Joao Gama University of Porto, Portugal Jun Luo Chinese Academy of Sciences, China Junbin Gao Charles Sturt University, Australia Junping Zhang Fudan University, China K. Selcuk Candan Arizona State University, USA Kaiq huang Chinese Academy of Sciences, China Kennichi Yoshida Tsukuba University, Japan Kitsana Waiyamai Kasetsart University, Thailand Kouzou Ohara Osaka University, Japan Liang Wang The University of Melbourne, Australia Ling Chen University of Technology Sydney, Australia Lisa Hellerstein Polytechnic Institute of NYU, USA Longbing Cao University of Technology Sydney, Australia Manabu Okumura Tokyo Institute of Technology, Japan Marco Maggini University of Siena, Italy Marut Buranarach NECTEC, Thailand Marzena Kryszkiewicz Warsaw University of Technology, Poland Masashi Shimbo Nara Institute of Science and Technology, Masayuki Numao Osaka University, Japan Maurice van Keulen University of Twente, The Netherlands Xiaofeng Meng Renmin University of China, China Mengjie Zhang Victoria University of Wellington, New Zealand Michael Berthold University of Konstanz, Germany Michael Katehakis Rutgers Business School, USA Michalis Vazirgiannis Athens Unive rsity of Economics and Business, Min Yao Zhejiang University, China Mingchun Wang Tianjin University of Technology and Mingli Song Hong Kong Polytechnical University, China Mohamed Mokbel University of Minnesota, USA Naren Ramakrishnan Virginia Tech, USA Ngoc Thanh Nguyen Wroclaw University of Technology, Poland Ning Zhong Maebashi Institute of Technology, Japan Ninghui Li Purdue University, USA Olivier de Vel DSTO, Australia Pabitra Mitra Indian Institute of Technology Kharagpur, Panagiotis Karras University of Zurich, Switzerland Pang-Ning Tan Michigan State University, USA Patricia Riddle University of Auckland, New Zealand Panagiotis Karras National University of Singapore, Singapore Jialie Shen Singapore Management University, Singapore Pang-Ning Tan Michigan State University, USA Patricia Riddle University of Auckland, New Zealand Peter Christen Australian National University, Australia Peter Triantafillou University of Patras, Greece Philip Yu IBM T.J. Watson Research Center, USA Philippe Lenca Telecom Bretagne, France Pohsiang Tsai National Formosa University, Taiwan, ROC Prasanna Desikan University of Minnesota, USA Qingshan Liu Chinese Academy of Sciences, China Rao Kotagiri The University of Melbourne, Australia Richi Nayak Queensland University of Technology, Australia Rui Camacho LIACC/FEUP University of Porto, Portugal Ruoming Jin Kent State University, USA S.K. Gupta Indian Institute of Technology, India Salvatore Orlando University of Venice, Italy Sameep Mehta IBM, India Research Labs, India Sanjay Chawla University of Sydney, Australia Sanjay Jain National University of Singapore, Singapore Sanjay Ranka University of Florida, USA San-Yih Hwang National Sun Yat-Sen University, Taiwan, ROC Seiji Yamada National Institute of Informatics, Japan Sheng Zhong State University of New York at Buffalo, USA Shichao Zhang University of Technology at Sydney, Australia Shiguang Shan Digital Media Research Center, ICT Shoji Hirano Shimane University, Japan Shu-Ching Chen Florida International University, USA Shuigeng Zhou Fudan University, China Songcan Chen Nanjing University of Aeronautics and Srikanta Tirthapura Iowa State University, USA Stefan Rueping Fraunhofer IAIS, Germany Suman Nath Networked Embedded Computing Group, Sung Ho Ha Kyungpook National University, Korea Sungzoon Cho Seoul National University, Korea Szymon Jaroszewicz Technica l University of Szczecin, Poland Tadashi Nomoto National Institute of Japanese Literature, Taizhong Hu University of Science and Technology of China Takashi Washio Osaka University, Japan Takeaki Uno National Institute of Informatics (NII), Japan Takehisa Yairi University of Tokyo, Japan Tamir Tassa The Open University, Israel Taneli Mielikainen Nokia Research Center, USA Tao Chen Shenzhen Institutes of Advanced Technology, Tao Li Florida International University, USA Tao Mei Microsoft Research Asia Tao Yang Shenzhen Institutes of Advanced Technology, Tetsuya Yoshida Hokkaido University, Japan Thepchai Supnithi National Electronics and Computer Technology Thomas Seidl RWTH Aachen University, Germany Tie-Yan Liu Microsoft Research Asia, China Toshiro Minami Kyushu Insti tute of Information Sciences Tru Cao Ho Chi Minh City University of Technology, Tsuyoshi Murata Tokyo Institute of Technology, Japan Vincent Lee Monash Un iversity, Australia Vincent S. Tseng National Cheng Kung University, Taiwan, ROC Vincenzo Piuri University of Milan, Italy Wagner Meira Jr. Universidade Federal de Minas Gerais, Brazil Wai Lam The Chinese University of Hong Kong, China Warren Jin Australian National University, Australia Wei Fan IBM T.J.Watson Research Center, USA Weining Qian East China Normal University, China Wen-Chih Peng National Chiao Tung University, Taiwan, ROC Wenjia Wang University of East Anglia, UK Wilfred Ng Hong Kong University of Science and Wlodek Zadrozny IBM Research Woong-Kee Loh Sungkyul University, Korea Wynne Hsu National University of Singapore, Singapore Xia Cui Chinese Academy of Sciences, China Xiangjun Dong Shandong Institute of Light Industry, China Xiaofang Zhou The University of Queensland, Australia Xiaohua Hu Drexel University, USA Xin Wang Calgary University, Canada Xindong Wu University of Vermont, USA Xingquan Zhu Florida Atlantic University, USA Xintao Wu University of North Carolina at Charlotte, USA Xuelong Li University of London, UK Xuemin Lin University of New South Wales, Australia Yan Zhou University of South Alabama, USA Yang-Sae Moon Kangwon National University, Korea Yao Tao The University of Auckland, New Zealand Yasuhiko Morimoto Hiroshima University, Japan Yi Chen Arizona State University, USA Yi-Dong Shen Chinese Academy of Sciences, China Yifeng Zeng Aalborg University, Denmark Yihua Wu Google Inc.
 Yi-Ping Phoebe Chen Deakin University, Australia Yiu-ming Cheung Hong Kong Baptist University, Hong Kong, Yong Guan Iowa State University, USA Yonghong Peng University of Bradford, UK Yu Jian Beijing Jiaotong University, China Yuan Yuan Aston University, UK Yun Xiong Fudan University, China Yunming Ye Harbin Institute of Technology, China Zheng Chen Microsoft Research Asia, China Zhi-Hua Zhou Nanjing University, China Zhongfei (Mark) Zhang SUNY Binghamton, USA Zhongzhi Shi Chinese Academy of Sciences, China Zili Zhang Deakin University, Australia Ameeta Agrawal York University, Canada Arnaud Soulet Universit  X  e Francois Rabelais Tours, France Axel Poigne Fraunhofer IAIS, Germany Ben Tan Fudan University, China Bian Wei University of Technology, Sydney, Australia Bibudh Lahiri Iowa State University Bin Yang Aalborg University, Denmark Bin Zhao East China Normal University, China Bing Bai Google Inc.
 Bojian Xu Iowa State University, USA Can Wang University of Technology, Sydney, Australia Carlos Ferreira University of Porto, Portugal Chao Li Shenzhen Institutes of Advanced Technology, Cheqing Jin East China Normal University, China Christian Beecks RWTH Aach en University, Germany Chun-Wei Seah Nanyang Technological University, Singapore De-Chuan Zhan Nanjing University, China Elnaz Delpisheh York University, Canada Erez Shmueli The Open University, Israel Fausto Fleites Florida International University, USA Fei Xie University of Vermont, USA Gaoping Zhu University of New South Wales, Australia Gongqing Wu University of Vermont, USA Hardy Kremer RWTH Aachen University, Germany Hideyuki Kawashima Nanzan University, Japan Hsin-Yu Ha Florida International University, USA Ji Zhou Fudan University, China Jianbo Yang Nanyang Technological University, Singapore Jinfei Shenzhen Institutes of Advanced Technology, Jinfeng Zhuang Microsoft Research Asia, China Jinjiu Li University of Technology, Sydney Jun Wang Southwest University, China Jun Zhang Charles Sturt University, Australia Ke Zhu University of New South Wales, Australia Keli Xiao Rutgers University, USA Ken-ichi Fukui Osaka University, Japan KuiYu UniversityofVermont,USA Leonard K.M. Poon Shenzhen Institutes of Advaced Technology, Leting Wu University of North Carolina at Charlotte, USA Liang Du Chinese Academy of Sciences, China Lin Zhu Shanghai Jiaotong University, China Ling Chen University of Technology Sydney, Australia Linhao Xu Aalborg University, Denmark Mangesh Gupte Google Inc.
 Mao Qi Nanyang Technological University, Singapore Marc Plantevit Universit  X  eLyon1,France Marcia Oliveira University Porto, Portugal Ming Li Nanjing University, China Mingkui Tan Nanyang Technological University, Singapore Natalja Friesen Fraunhofer IAIS, Germany Nguyen Le Minh Japan Advanced Institute of Science and Ning Zhang Microsoft Research Asia, China Nuno A. Fonseca LIACC/FEUP University of Porto, Portugal Omar Odibat IIIT, Hyderabad, India Peipei Li University of Vermont, USA Peng Cai East China Normal University, China Penjie Ye University of New South Wales, Australia Peter Tischer Monash University, Australia Petr Kosina University of Porto, Portugal Philipp Kranen RWTH Aachen University, Germany Qiao-Liang Xiang Nanyang Technological University, Singapore Rajul Anand IIIT, Hyderabad, India Roberto Legaspi Osaka University, Japan Romain Vuillemot INSA Lyon, France Sergej Fries RWTH Aachen University, Germany Smriti Bhagat Google Inc.
 Stephane Lallich Telecom Bretagne, France Supaporn Spanurattana Tokyo Institute of Technology, Japan Vitor Santos Costa LIACC/FEUP University of Porto, Portugal Wang Xinchao Ecole Polytechnique Federale de Lausanne (EPFL), Weifeng Su Shenzhen Institutes of Advaced Technology, Weiren Yu University of New South Wales, Australia Wenjun Zhou Rutgers University, USA Xiang Zhao University of New South Wales, Australia Xiaodan Wang Fudan University, China Xiaowei Ying University of North Carolina at Charlotte, USA Xin Liu Tokyo Institute of Technology, Japan Xuan Li Chinese Academy of Sciences, China Xu-Ying Liu Nanjing University, China Yannick Le Bras Telecom Bretagne, France Yasayuki Okabe National Institute of Informatics, Japan Yasufumi Takama National Institute of Informatics, Japan Yi Guo Charles Sturt University, Australia Yi Wang Shenzhen Institutes of Advanced Technology, Yi Xu SUNY Binghamton, USA Yiling Zeng University of Technology, Sydney Yimin Yang Florida International University, USA Yoan Renaud INSA Lyon, France Yong Deng Southwest University, China Yong Ge Rutgers University, USA Yuan YUAN Aston University, UK Zhao Zhang East China Normal University, China Zhenjie Zhang Aalborg University, Denmark Zhenyu Lu University of Vermont, USA Zhigang Zheng University of Technology, Sydney, Australia Zhitao Shen University of New South Wales, Australia Zhiyong Cheng Singapore Management University Zhiyuan Chen The Open University, Israel Zhongmou Li Rutgers University, USA Zhongqiu Zhao University of Vermont, USA Zhou Tianyi University of Technology, Sydney, Australia An Instance Selection Algorithm Ba sed on Reverse Nearest Neighbor ... 1 A Game Theoretic Approach for Feature Clustering and Its Application to Feature Selection .............................................. 13 Feature Selection Strateg y in Text Classification ..................... 26 Unsupervised Feature Weighting Based on Local Feature Relatedness ... 38 An Effective Feature Selection M ethod for Text Categorization ........ 50 A Subpath Kernel for Rooted Unordered Trees ...................... 62 Classification Probabilistic PCA with Application in Domain Adaptation ...................................................... 75 Probabilistic Matrix Factorization Leveraging Contexts for Unsupervised Relation Extraction .................................. 87 The Unsymmetrical-Style Co-training .............................. 100 Balance Support Vector Machines Locally Using the Structural Similarity Kernel ................................................ 112 Using Classifier-Based Nominal Imputation to Improve Machine Learning ........................................................ 124 A Bayesian Framework for Learning Shared and Individual Subspaces from Multiple Data Sources ....................................... 136 Are Tensor Decomposition Solutions Unique? On the Global Convergence HOSVD and ParaFac Algorithms ....................... 148 Improved Spectral Hashing ........................................ 160 High-Order Co-clusterin g Text Data on Semantics-Based Representation Model .......................................................... 171 The Role of Hubness in Clustering High-Dimensional Data ............ 183 Spatial Entropy-Based Clustering for Mining Data with Spatial Correlation ...................................................... 196 Self-adjust Local Connectivity Analysis for Spectral Clustering ........ 209 An Effective Density-Based Hierarchical Clustering Technique to Identify Coherent Patterns from Gene Expression Data ............... 225 Nonlinear Discriminative Embedding for Clustering via Spectral Regularization ................................................... 237 An Adaptive Fuzzy k -Nearest Neighbor Method Based on Parallel Particle Swarm Optimization for Bankruptcy Prediction .............. 249 Semi-supervised Parameter-Free Divisive Hierarchical Clustering of Categorical Data ................................................. 265 Identifying Hidden Contexts in Classification ........................ 277 Cross-Lingual Sentiment Classification via Bi-view Non-negative Matrix Tri-Factorization .......................................... 289 A Sequential Dynamic Multi-class Model and Recursive Filtering by Variational Bayesian Methods ..................................... 301 Random Ensemble Decision Trees for L earning Concept-Drifting Data Streams ........................................................ 313 Collaborative Data Cleaning for Sentiment Classification with Noisy Training Corpus ................................................. 326 Using Constraints to Generate and Explore Higher Order Discriminative Patterns ........................................................ 338 Mining Maximal Co-located Event Sets ............................. 351 Pattern Mining for a Two-Stage Information Filtering System ......... 363 Efficiently Retrieving Longest Common Route Patterns of Moving Objects By Summarizing Turning Regions .......................... 375 Automatic Assignment of Item Weights for Pattern Mining on Data Streams ........................................................ 387 Predicting Private Company Exits Using Qualitative Data ............ 399 A Rule-Based Method for Customer Churn Prediction in Telecommunication Services ....................................... 411 Adaptive and Effective Keyword Search for XML .................... 423 Steering Time-Dependent Esti mation of Posteriors with Hyperparameter Indexing in Bayesian Topic Models .................. 435 Constrained LDA for Grouping Product Features in Opinion Mining ... 448 Semantic Dependent Word Pairs Generative Model for Fine-Grained Product Feature Mining .......................................... 460 Grammatical Depe ndency-Based Relations for Term Weighting in Text Classification .................................................... 476 XML Documents Clustering Using a Tensor Space Model ............. 488 An Efficient Pre-processing Method to Identify Logical Components from PDF Documents ............................................ 500 Combining Proper Name-Coreference with Conditional Random Fields for Semi-supervised Named Entity Recognition in Vietnamese Text ..... 512 Topic Analysis of Web User Behavior Using LDA Model on Proxy Logs ........................................................... 525 SizeSpotSigs: An Effective Deduplicate Algorithm Considering the Size of Page Content ................................................. 537 Knowledge Transfer across Multilingual Corpora via Latent Topics ..... 549 Author Index .................................................. 561 Spectral Analysis of k -Balanced Signed Graphs ...................... 1 Spectral Analysis for Billion-Scale Graphs: Discoveries and Implementation .................................................. 13 LGM: Mining Frequent Subgraphs from Linear Graphs ............... 26 Efficient Centrality Monitoring for Time-Evolving Graphs ............. 38 Graph-Based Clustering with Constraints ........................... 51 A Partial Correlation-Based Bayesian Network Structure Learning Algorithm under SEM ............................................ 63 Predicting Friendship Links in Social Networks Using a Topic Modeling Approach ....................................................... 75 Info-Cluster Based Regional Influence Analysis in Social Networks ..... 87 Utilizing Past Relations and User Similarities in a Social Matching System ......................................................... 99 On Sampling Type Distribution from Heterogeneous Social Networks ... 111 Ant Colony Optimization with Markov Random Walk for Community DetectioninGraphs .............................................. 123 Faster and Parameter-Free Discord Search in Quasi-Periodic Time Series .......................................................... 135 INSIGHT: Efficient and Effective Ins tance Selection for Time-Series Classification .................................................... 149 Multiple Time-Series Prediction through Multiple Time-Series Relationships Profiling and Clustered Recurring Trends ............... 161 Probabilistic Feature Extraction from Multivariate Time Series Using Spatio-Temporal Constraints ...................................... 173 Real-Time Change-Point Detection Using Sequentially Discounting Normalized Maximum Likelihood Coding ........................... 185 Compression for Anti-Adversarial Learning .......................... 198 Mining Sequential Patterns from Probabilistic Databases .............. 210 Large Scale Real-Life Action Recognition Using Conditional Random Fields with Stochastic Training .................................... 222 Packing Alignment: Alignment f or Sequences of Various Length Events .......................................................... 234 Multiple Distribution Data Description Learning Algorithm for Novelty Detection ....................................................... 246 RADAR: Rare Category Detection via Computation of Boundary Degree .......................................................... 258 RKOF: Robust Kernel-Base d Local Outlier Detection ................ 270 Chinese Categorization and Novelty Mining ......................... 284 Finding Rare Classes: Adapting Generative and Discriminative Models in Active Learning ............................................... 296 Margin-Based Over-Sampling Method for Learning From Imbalanced Datasets ........................................................ 309 Improving k Nearest Neighbor with Exemplar Generalization for Imbalanced Classification ......................................... 321 Sample Subset Optimization for Classifying Imbalanced Biological Data ........................................................... 333 Class Confidence Weighted k NN Algorithms for Imbalanced Data Sets ............................................................ 345 Multi-agent Based Classification Using Argumentation from Experience ...................................................... 357 Agent-Based Subspace Clustering .................................. 370 Evaluating Pattern Set Mining Strategies in a Constraint Programming Framework ...................................................... 382 Asking Generalized Queries with Minimum Cost ..................... 395 Ranking Individuals and Gro ups by Influence Propagation ............ 407 Dynamic Ordering-Based Search Algorithm for Markov Blanket Discovery ....................................................... 420 Mining Association Rules for Label Ranking ......................... 432 Tracing Evolving Clusters by Subspace and Value Similarity ........... 444 An IFS-Based Similarity Measure to Index Electroencephalograms ..... 457 DISC: Data-Intensive Similarity Measure for Categorical Data ......... 469 ListOPT: Learning to Optimize for XML Ranking ................... 482 Item Set Mining Based on Cover Similarity .......................... 493 Learning to Advertise: How Many Ads Are Enough? ................. 506 TeamSkill: Modeling Team Chemistry in Online Multi-player Games ... 519 Learning the Funding Momentum of Research Projects ............... 532 Local Feature Based Tensor Ker nel for Image Manifold Learning ....... 544
 Series Editors Randy Goebel, University of Alberta, Edmonton, Canada J X rg Siekmann, University of Saarland, Saarbr X cken, Germany Wolfgang Wahlster, DFKI and University of Saarland, Saarbr X cken, Germany Volume Editors Joshua Zhexue Huang Chinese Academy of Sciences Shenzhen Institutes of Advanced Technology (SIAT) Shenzhen 518055, China E-mail: zx.huang@siat.ac.cn Longbing Cao University of Technology Sydney Faculty of Engineering and Information Technology Advanced Analytics Institute Center for Quantum Computation and Intelligent Systems Sydney, NSW 2007, Australia E-mail: longbing.cao-1@uts.edu.au Jaideep Srivastava University of Minnesota Department of Computer Science and Engineering Minneapolis, MN 55455, USA E-mail: Srivasta@cs.umn.edu ISSN 0302-9743 e-ISSN 1611-3349 ISBN 978-3-642-20846-1 e-ISBN 978-3-642-20847-8 DOI 10.1007/978-3-642-20847-8 Springer Heidelberg Dordrecht London New York Library of Congress Control Number: 2011926132 CR Subject Classification (1998): I.2, H.3, H.4, H.2.8, I.4, C.2
LNCS Sublibrary: SL 7  X  Artificial Intelligence PAKDD has been recognized as a major international conference in the areas of data mining (DM) and knowledge discovery in databases (KDD). It provides an international forum for researchers and industry practitioners to share their new ideas, original research results and prac tical development experiences from all KDD-related areas including data mining, machine learning, artificial intelligence and pattern recognition, data warehousing and databases, statistics, knowledge engineering, behavioral sciences, visualization, and emerging areas such as social network analysis.
 (PAKDD 2011) was held in Shenzhen, China, during May 24 X 27, 2011. PAKDD 2011 introduced a double-blind review process. It received 331 submissions af-ter checking for validity. Submissions were from 45 countries and regions, which shows a significant improvement in internationalization than PAKDD 2010 (34 countries and regions). All papers were assigned to at least four Program Com-mittee members. Most papers received more than three review reports. As a result of the deliberation process, only 90 papers were accepted, with 32 papers (9.7%) for long presentation and 58 (17.5%) for short presentation. Workshop on Behavior Informatics (BI 2011), Workshop on Advances and Issues in Traditional Chinese Medicine Clinical Data Mining (AI-TCM), Quality Is-sues, Measures of Interestingness and E valuation of Data Mining Models (QIMIE 2011), Biologically Inspired Techniques for Data Mining (BDM 2011), and Work-shop on Data Mining for Healthcare Management (DMHM 2011). PAKDD 2011 also featured talks by three distinguished invited speakers, six tutorials, and a Doctoral Symposium on Data Mining.
 Program Committee members (203), external reviewers (168), Organizing Com-mittee members, invited speakers, authors, tutorial presenters, workshop orga-nizers, reviewers, authors and the conference attendees. We highly appreciate the conscientious reviews provided by the Program Committee members and external reviewers. We are indebted to the members of the PAKDD Steering Committee for their invaluable suggestions and support throughout the orga-nization process. Our special thanks go to the local arrangements team and volunteers. We would also like to thank al l those who contributed to the success of PAKDD 2011 but whose names cannot be listed.
 ference and workshop proceedings. Tha nks also to Andrei Voronkov for hosting the entire PAKDD reviewing pro cess on the EasyChair.org site.
 tutions. The conference was organized by the Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, China, and co-organized by the Uni-versity of Hong Kong, China and the University of Technology Sydney, Australia. edge research in data mining and knowledge discovery. We also hope all partic-ipants took this opportunity to exchange ideas with each other and enjoyed the modem city of Shenzhen! May 2011 Joshua Huang Honorary Chair Philip S. Yu University of Illinois at Chicago, USA General Co-chairs Jianping Fan Shenzhen Institutes of Advanced Technology, David Cheung University of Hong Kong, China Program Committee Co-chairs Joshua Huang Shenzhen Institutes of Advanced Technology, Longbing Cao University of Technology Sydney, Australia Jaideep Srivastava University of Minnesota, USA Workshop Co-chairs James Bailey The University of Melbourne, Australia Yun Sing Koh The University of Auckland, New Zealand Tutorial Co-chairs Xiong Hui Rutgers, the State University of New Jersey, Sanjay Chawla The University of Sydney, Australia Local Arrangements Co-chairs Shengzhong Feng Shenzhen Institutes of Advanced Technology, Jun Luo Shenzhen Institutes of Advanced Technology, Sponsorship Co-chairs Yalei Bi Shenzhen Institutes of Advanced Technology, Zhong Ming Shenzhen University, China Publicity Co-chairs Jian Yang Beijing University of Technology, China Ye Li Shenzhen Institutes of Advanced Technology, Yuming Ou University of Technology Sydney, Australia Publication Chair Longbing Cao University of Technology Sydney, Australia Co-chairs Rao Kotagiri University of Melbourne, Australia Graham Williams Australian National University, Australia Life Members David Cheung University of Hong Kong, China Masaru Kitsuregawa Tokyo University, Japan Rao Kotagiri University of Melbourne, Australia Hiroshi Motoda AFOSR/AOARD and Osaka University, Japan Graham Williams (Treasurer) Australian National University, Australia Ning Zhong Maebashi Institute of Technology, Japan Members Ming-Syan Chen National Taiwan University, Taiwan, ROC Tu Bao Ho Japan Advanced In stitute of Science and Ee-Peng Lim Singapore Management University, Singapore Huan Liu Arizona State University, USA Jaideep Srivastava University of Minnesota, USA Takashi Washio Institute of Scientific and Industrial Research, Thanaruk Theeramunkong Thammasat University, Thailand Kyu-Young Whang Korea Advan ced Institute of Science and Chengqi Zhang University of Technology Sydney, Australia Zhi-Hua Zhou Nanjing University, China Krishna Reddy IIIT, Hyderabad, India Adrian Pearce The University of Melbourne, Australia Aijun An York University, Canada Aixin Sun Nanyang Technological University, Singapore Akihiro Inokuchi Osaka University, Japan Akira Shimazu Japan Advanced Institute of Science and Alfredo Cuzzocrea Univer sity of Calabria, Italy Andrzej Skowron Warsaw University, Poland Anirban Mondal IIIT Delhi, India Aoying Zhou Fudan University, China Arbee Chen National Chengchi University, Taiwan, ROC Aristides Gionis Yahoo Research Labs, Spain Atsuyoshi Nakamura Hokkaido University, Japan Bart Goethals University of Antwerp, Belgium Bernhard Pfahringer University of Waikato, New Zealand Bo Liu University of Technology, Sydney, Australia Bo Zhang Tsinghua University, China Boonserm Kijsirikul Chulalongkorn University, Thailand Bruno Cremilleux Universit  X  e de Caen, France Chandan Reddy Wayne State University, USA Chang-Tien Lu Virginia Tech, USA Chaveevan Pechsiri Dhurakijpundit University, Thailand Chengqi Zhang University of Technology, Australia Chih-Jen Lin National Taiwan University, Taiwan, ROC Choochart Haruechaiyasak NECTEC, Thailand Chotirat Ann Ratanamahatana Chulalongkorn University, Thailand Chun-hung Li Hong Kong Baptist University, Hong Kong, Chunsheng Yang NRC Institute for Information Technology, Clement Yu University of Illinois at Chicago, USA Dacheng Tao The Hong Kong Polytechnic University, Daisuke Ikeda Kyushu University, Japan Dan Luo University of Technology, Sydney, Australia Daoqiang Zhang Nanjing University of Aeronautics and Dao-Qing Dai Sun Yat-Sen University, China David Albrecht Monash University, Australia David Taniar Monash University, Australia Di Wu Chinese University of Hong Kong, China Diane Cook Washington State University, USA Dit-Yan Yeung Hong Kong University of Science and Dragan Gamberger Rudjer Boskovic Institute, Croatia Du Zhang California State University, USA Ee-Peng Lim Nanyang Technological University, Singapore Eibe Frank University of Waikato, New Zealand Evaggelia Pitoura University of Ioannina, Greece Floriana Esposito Universit` a di Bari, Italy Gang Li Deakin University, Australia George Karypis University of Minnesota, USA Graham Williams Australian Taxation Office, Australia Guozhu Dong Wright State University, USA Hai Wang University of Aston, UK Hanzi Wang University of Adelaide, Australia Harry Zhang University of New Brunswick, Canada Hideo Bannai Kyushu University, Japan Hiroshi Nakagawa University of Tokyo, Japan Hiroyuki Kawano Nanzan University, Japan Hiroyuki Kitagawa University of Tsukuba, Japan Hua Lu Aalborg University, Denmark Huan Liu Arizona State University, USA Hui Wang University of Ulster, UK Hui Xiong Rutgers University, USA Hui Yang San Francisco State University, USA Huiping Cao New Mexico State University, USA Irena Koprinska University of Sydney, Australia Ivor Tsang Hong Kong University of Science and James Kwok Hong Kong University of Science and Jason Wang New Jersey Science and Technology University, Jean-Marc Petit INSA Lyon, France Jeffrey Ullman Stanford University, USA Jiacai Zhang Beijing Normal University, China Jialie Shen Singapore Management University, Singapore Jian Yin Sun Yat-Sen University, China Jiawei Han University of Illinois at Urbana-Champaign, Jiuyong Li University of South Australia Joao Gama University of Porto, Portugal Jun Luo Chinese Academy of Sciences, China Junbin Gao Charles Sturt University, Australia Junping Zhang Fudan University, China K. Selcuk Candan Arizona State University, USA Kaiq huang Chinese Academy of Sciences, China Kennichi Yoshida Tsukuba University, Japan Kitsana Waiyamai Kasetsart University, Thailand Kouzou Ohara Osaka University, Japan Liang Wang The University of Melbourne, Australia Ling Chen University of Technology Sydney, Australia Lisa Hellerstein Polytechnic Institute of NYU, USA Longbing Cao University of Technology Sydney, Australia Manabu Okumura Tokyo Institute of Technology, Japan Marco Maggini University of Siena, Italy Marut Buranarach NECTEC, Thailand Marzena Kryszkiewicz Warsaw University of Technology, Poland Masashi Shimbo Nara Institute of Science and Technology, Masayuki Numao Osaka University, Japan Maurice van Keulen University of Twente, The Netherlands Xiaofeng Meng Renmin University of China, China Mengjie Zhang Victoria University of Wellington, New Zealand Michael Berthold University of Konstanz, Germany Michael Katehakis Rutgers Business School, USA Michalis Vazirgiannis Athens Unive rsity of Economics and Business, Min Yao Zhejiang University, China Mingchun Wang Tianjin University of Technology and Mingli Song Hong Kong Polytechnical University, China Mohamed Mokbel University of Minnesota, USA Naren Ramakrishnan Virginia Tech, USA Ngoc Thanh Nguyen Wroclaw University of Technology, Poland Ning Zhong Maebashi Institute of Technology, Japan Ninghui Li Purdue University, USA Olivier de Vel DSTO, Australia Pabitra Mitra Indian Institute of Technology Kharagpur, Panagiotis Karras University of Zurich, Switzerland Pang-Ning Tan Michigan State University, USA Patricia Riddle University of Auckland, New Zealand Panagiotis Karras National University of Singapore, Singapore Jialie Shen Singapore Management University, Singapore Pang-Ning Tan Michigan State University, USA Patricia Riddle University of Auckland, New Zealand Peter Christen Australian National University, Australia Peter Triantafillou University of Patras, Greece Philip Yu IBM T.J. Watson Research Center, USA Philippe Lenca Telecom Bretagne, France Pohsiang Tsai National Formosa University, Taiwan, ROC Prasanna Desikan University of Minnesota, USA Qingshan Liu Chinese Academy of Sciences, China Rao Kotagiri The University of Melbourne, Australia Richi Nayak Queensland University of Technology, Australia Rui Camacho LIACC/FEUP University of Porto, Portugal Ruoming Jin Kent State University, USA S.K. Gupta Indian Institute of Technology, India Salvatore Orlando University of Venice, Italy Sameep Mehta IBM, India Research Labs, India Sanjay Chawla University of Sydney, Australia Sanjay Jain National University of Singapore, Singapore Sanjay Ranka University of Florida, USA San-Yih Hwang National Sun Yat-Sen University, Taiwan, ROC Seiji Yamada National Institute of Informatics, Japan Sheng Zhong State University of New York at Buffalo, USA Shichao Zhang University of Technology at Sydney, Australia Shiguang Shan Digital Media Research Center, ICT Shoji Hirano Shimane University, Japan Shu-Ching Chen Florida International University, USA Shuigeng Zhou Fudan University, China Songcan Chen Nanjing University of Aeronautics and Srikanta Tirthapura Iowa State University, USA Stefan Rueping Fraunhofer IAIS, Germany Suman Nath Networked Embedded Computing Group, Sung Ho Ha Kyungpook National University, Korea Sungzoon Cho Seoul National University, Korea Szymon Jaroszewicz Technica l University of Szczecin, Poland Tadashi Nomoto National Institute of Japanese Literature, Taizhong Hu University of Science and Technology of China Takashi Washio Osaka University, Japan Takeaki Uno National Institute of Informatics (NII), Japan Takehisa Yairi University of Tokyo, Japan Tamir Tassa The Open University, Israel Taneli Mielikainen Nokia Research Center, USA Tao Chen Shenzhen Institutes of Advanced Technology, Tao Li Florida International University, USA Tao Mei Microsoft Research Asia Tao Yang Shenzhen Institutes of Advanced Technology, Tetsuya Yoshida Hokkaido University, Japan Thepchai Supnithi National Electronics and Computer Technology Thomas Seidl RWTH Aachen University, Germany Tie-Yan Liu Microsoft Research Asia, China Toshiro Minami Kyushu Insti tute of Information Sciences Tru Cao Ho Chi Minh City University of Technology, Tsuyoshi Murata Tokyo Institute of Technology, Japan Vincent Lee Monash Un iversity, Australia Vincent S. Tseng National Cheng Kung University, Taiwan, ROC Vincenzo Piuri University of Milan, Italy Wagner Meira Jr. Universidade Federal de Minas Gerais, Brazil Wai Lam The Chinese University of Hong Kong, China Warren Jin Australian National University, Australia Wei Fan IBM T.J.Watson Research Center, USA Weining Qian East China Normal University, China Wen-Chih Peng National Chiao Tung University, Taiwan, ROC Wenjia Wang University of East Anglia, UK Wilfred Ng Hong Kong University of Science and Wlodek Zadrozny IBM Research Woong-Kee Loh Sungkyul University, Korea Wynne Hsu National University of Singapore, Singapore Xia Cui Chinese Academy of Sciences, China Xiangjun Dong Shandong Institute of Light Industry, China Xiaofang Zhou The University of Queensland, Australia Xiaohua Hu Drexel University, USA Xin Wang Calgary University, Canada Xindong Wu University of Vermont, USA Xingquan Zhu Florida Atlantic University, USA Xintao Wu University of North Carolina at Charlotte, USA Xuelong Li University of London, UK Xuemin Lin University of New South Wales, Australia Yan Zhou University of South Alabama, USA Yang-Sae Moon Kangwon National University, Korea Yao Tao The University of Auckland, New Zealand Yasuhiko Morimoto Hiroshima University, Japan Yi Chen Arizona State University, USA Yi-Dong Shen Chinese Academy of Sciences, China Yifeng Zeng Aalborg University, Denmark Yihua Wu Google Inc.
 Yi-Ping Phoebe Chen Deakin University, Australia Yiu-ming Cheung Hong Kong Baptist University, Hong Kong, Yong Guan Iowa State University, USA Yonghong Peng University of Bradford, UK Yu Jian Beijing Jiaotong University, China Yuan Yuan Aston University, UK Yun Xiong Fudan University, China Yunming Ye Harbin Institute of Technology, China Zheng Chen Microsoft Research Asia, China Zhi-Hua Zhou Nanjing University, China Zhongfei (Mark) Zhang SUNY Binghamton, USA Zhongzhi Shi Chinese Academy of Sciences, China Zili Zhang Deakin University, Australia Ameeta Agrawal York University, Canada Arnaud Soulet Universit  X  e Francois Rabelais Tours, France Axel Poigne Fraunhofer IAIS, Germany Ben Tan Fudan University, China Bian Wei University of Technology, Sydney, Australia Bibudh Lahiri Iowa State University Bin Yang Aalborg University, Denmark Bin Zhao East China Normal University, China Bing Bai Google Inc.
 Bojian Xu Iowa State University, USA Can Wang University of Technology, Sydney, Australia Carlos Ferreira University of Porto, Portugal Chao Li Shenzhen Institutes of Advanced Technology, Cheqing Jin East China Normal University, China Christian Beecks RWTH Aach en University, Germany Chun-Wei Seah Nanyang Technological University, Singapore De-Chuan Zhan Nanjing University, China Elnaz Delpisheh York University, Canada Erez Shmueli The Open University, Israel Fausto Fleites Florida International University, USA Fei Xie University of Vermont, USA Gaoping Zhu University of New South Wales, Australia Gongqing Wu University of Vermont, USA Hardy Kremer RWTH Aachen University, Germany Hideyuki Kawashima Nanzan University, Japan Hsin-Yu Ha Florida International University, USA Ji Zhou Fudan University, China Jianbo Yang Nanyang Technological University, Singapore Jinfei Shenzhen Institutes of Advanced Technology, Jinfeng Zhuang Microsoft Research Asia, China Jinjiu Li University of Technology, Sydney Jun Wang Southwest University, China Jun Zhang Charles Sturt University, Australia Ke Zhu University of New South Wales, Australia Keli Xiao Rutgers University, USA Ken-ichi Fukui Osaka University, Japan KuiYu UniversityofVermont,USA Leonard K.M. Poon Shenzhen Institutes of Advaced Technology, Leting Wu University of North Carolina at Charlotte, USA Liang Du Chinese Academy of Sciences, China Lin Zhu Shanghai Jiaotong University, China Ling Chen University of Technology Sydney, Australia Linhao Xu Aalborg University, Denmark Mangesh Gupte Google Inc.
 Mao Qi Nanyang Technological University, Singapore Marc Plantevit Universit  X  eLyon1,France Marcia Oliveira University Porto, Portugal Ming Li Nanjing University, China Mingkui Tan Nanyang Technological University, Singapore Natalja Friesen Fraunhofer IAIS, Germany Nguyen Le Minh Japan Advanced Institute of Science and Ning Zhang Microsoft Research Asia, China Nuno A. Fonseca LIACC/FEUP University of Porto, Portugal Omar Odibat IIIT, Hyderabad, India Peipei Li University of Vermont, USA Peng Cai East China Normal University, China Penjie Ye University of New South Wales, Australia Peter Tischer Monash University, Australia Petr Kosina University of Porto, Portugal Philipp Kranen RWTH Aachen University, Germany Qiao-Liang Xiang Nanyang Technological University, Singapore Rajul Anand IIIT, Hyderabad, India Roberto Legaspi Osaka University, Japan Romain Vuillemot INSA Lyon, France Sergej Fries RWTH Aachen University, Germany Smriti Bhagat Google Inc.
 Stephane Lallich Telecom Bretagne, France Supaporn Spanurattana Tokyo Institute of Technology, Japan Vitor Santos Costa LIACC/FEUP University of Porto, Portugal Wang Xinchao Ecole Polytechnique Federale de Lausanne (EPFL), Weifeng Su Shenzhen Institutes of Advaced Technology, Weiren Yu University of New South Wales, Australia Wenjun Zhou Rutgers University, USA Xiang Zhao University of New South Wales, Australia Xiaodan Wang Fudan University, China Xiaowei Ying University of North Carolina at Charlotte, USA Xin Liu Tokyo Institute of Technology, Japan Xuan Li Chinese Academy of Sciences, China Xu-Ying Liu Nanjing University, China Yannick Le Bras Telecom Bretagne, France Yasayuki Okabe National Institute of Informatics, Japan Yasufumi Takama National Institute of Informatics, Japan Yi Guo Charles Sturt University, Australia Yi Wang Shenzhen Institutes of Advanced Technology, Yi Xu SUNY Binghamton, USA Yiling Zeng University of Technology, Sydney Yimin Yang Florida International University, USA Yoan Renaud INSA Lyon, France Yong Deng Southwest University, China Yong Ge Rutgers University, USA Yuan YUAN Aston University, UK Zhao Zhang East China Normal University, China Zhenjie Zhang Aalborg University, Denmark Zhenyu Lu University of Vermont, USA Zhigang Zheng University of Technology, Sydney, Australia Zhitao Shen University of New South Wales, Australia Zhiyong Cheng Singapore Management University Zhiyuan Chen The Open University, Israel Zhongmou Li Rutgers University, USA Zhongqiu Zhao University of Vermont, USA Zhou Tianyi University of Technology, Sydney, Australia Spectral Analysis of k -Balanced Signed Graphs ...................... 1 Spectral Analysis for Billion-Scale Graphs: Discoveries and Implementation .................................................. 13 LGM: Mining Frequent Subgraphs from Linear Graphs ............... 26 Efficient Centrality Monitoring for Time-Evolving Graphs ............. 38 Graph-Based Clustering with Constraints ........................... 51 A Partial Correlation-Based Bayesian Network Structure Learning Algorithm under SEM ............................................ 63 Predicting Friendship Links in Social Networks Using a Topic Modeling Approach ....................................................... 75 Info-Cluster Based Regional Influence Analysis in Social Networks ..... 87 Utilizing Past Relations and User Similarities in a Social Matching System ......................................................... 99 On Sampling Type Distribution from Heterogeneous Social Networks ... 111 Ant Colony Optimization with Markov Random Walk for Community DetectioninGraphs .............................................. 123 Faster and Parameter-Free Discord Search in Quasi-Periodic Time Series .......................................................... 135 INSIGHT: Efficient and Effective Ins tance Selection for Time-Series Classification .................................................... 149 Multiple Time-Series Prediction through Multiple Time-Series Relationships Profiling and Clustered Recurring Trends ............... 161 Probabilistic Feature Extraction from Multivariate Time Series Using Spatio-Temporal Constraints ...................................... 173 Real-Time Change-Point Detection Using Sequentially Discounting Normalized Maximum Likelihood Coding ........................... 185 Compression for Anti-Adversarial Learning .......................... 198 Mining Sequential Patterns from Probabilistic Databases .............. 210 Large Scale Real-Life Action Recognition Using Conditional Random Fields with Stochastic Training .................................... 222 Packing Alignment: Alignment f or Sequences of Various Length Events .......................................................... 234 Multiple Distribution Data Description Learning Algorithm for Novelty Detection ....................................................... 246 RADAR: Rare Category Detection via Computation of Boundary Degree .......................................................... 258 RKOF: Robust Kernel-Base d Local Outlier Detection ................ 270 Chinese Categorization and Novelty Mining ......................... 284 Finding Rare Classes: Adapting Generative and Discriminative Models in Active Learning ............................................... 296 Margin-Based Over-Sampling Method for Learning From Imbalanced Datasets ........................................................ 309 Improving k Nearest Neighbor with Exemplar Generalization for Imbalanced Classification ......................................... 321 Sample Subset Optimization for Classifying Imbalanced Biological Data ........................................................... 333 Class Confidence Weighted k NN Algorithms for Imbalanced Data Sets ............................................................ 345 Multi-agent Based Classification Using Argumentation from Experience ...................................................... 357 Agent-Based Subspace Clustering .................................. 370 Evaluating Pattern Set Mining Strategies in a Constraint Programming Framework ...................................................... 382 Asking Generalized Queries with Minimum Cost ..................... 395 Ranking Individuals and Gro ups by Influence Propagation ............ 407 Dynamic Ordering-Based Search Algorithm for Markov Blanket Discovery ....................................................... 420 Mining Association Rules for Label Ranking ......................... 432 Tracing Evolving Clusters by Subspace and Value Similarity ........... 444 An IFS-Based Similarity Measure to Index Electroencephalograms ..... 457 DISC: Data-Intensive Similarity Measure for Categorical Data ......... 469 ListOPT: Learning to Optimize for XML Ranking ................... 482 Item Set Mining Based on Cover Similarity .......................... 493 Learning to Advertise: How Many Ads Are Enough? ................. 506 TeamSkill: Modeling Team Chemistry in Online Multi-player Games ... 519 Learning the Funding Momentum of Research Projects ............... 532 Local Feature Based Tensor Ker nel for Image Manifold Learning ....... 544 Author Index .................................................. 555 An Instance Selection Algorithm Ba sed on Reverse Nearest Neighbor ... 1 A Game Theoretic Approach for Feature Clustering and Its Application to Feature Selection .............................................. 13 Feature Selection Strateg y in Text Classification ..................... 26 Unsupervised Feature Weighting Based on Local Feature Relatedness ... 38 An Effective Feature Selection M ethod for Text Categorization ........ 50 A Subpath Kernel for Rooted Unordered Trees ...................... 62 Classification Probabilistic PCA with Application in Domain Adaptation ...................................................... 75 Probabilistic Matrix Factorization Leveraging Contexts for Unsupervised Relation Extraction .................................. 87 The Unsymmetrical-Style Co-training .............................. 100 Balance Support Vector Machines Locally Using the Structural Similarity Kernel ................................................ 112 Using Classifier-Based Nominal Imputation to Improve Machine Learning ........................................................ 124 A Bayesian Framework for Learning Shared and Individual Subspaces from Multiple Data Sources ....................................... 136 Are Tensor Decomposition Solutions Unique? On the Global Convergence HOSVD and ParaFac Algorithms ....................... 148 Improved Spectral Hashing ........................................ 160 High-Order Co-clusterin g Text Data on Semantics-Based Representation Model .......................................................... 171 The Role of Hubness in Clustering High-Dimensional Data ............ 183 Spatial Entropy-Based Clustering for Mining Data with Spatial Correlation ...................................................... 196 Self-adjust Local Connectivity Analysis for Spectral Clustering ........ 209 An Effective Density-Based Hierarchical Clustering Technique to Identify Coherent Patterns from Gene Expression Data ............... 225 Nonlinear Discriminative Embedding for Clustering via Spectral Regularization ................................................... 237 An Adaptive Fuzzy k -Nearest Neighbor Method Based on Parallel Particle Swarm Optimization for Bankruptcy Prediction .............. 249 Semi-supervised Parameter-Free Divisive Hierarchical Clustering of Categorical Data ................................................. 265 Identifying Hidden Contexts in Classification ........................ 277 Cross-Lingual Sentiment Classification via Bi-view Non-negative Matrix Tri-Factorization .......................................... 289 A Sequential Dynamic Multi-class Model and Recursive Filtering by Variational Bayesian Methods ..................................... 301 Random Ensemble Decision Trees for L earning Concept-Drifting Data Streams ........................................................ 313 Collaborative Data Cleaning for Sentiment Classification with Noisy Training Corpus ................................................. 326 Using Constraints to Generate and Explore Higher Order Discriminative Patterns ........................................................ 338 Mining Maximal Co-located Event Sets ............................. 351 Pattern Mining for a Two-Stage Information Filtering System ......... 363 Efficiently Retrieving Longest Common Route Patterns of Moving Objects By Summarizing Turning Regions .......................... 375 Automatic Assignment of Item Weights for Pattern Mining on Data Streams ........................................................ 387 Predicting Private Company Exits Using Qualitative Data ............ 399 A Rule-Based Method for Customer Churn Prediction in Telecommunication Services ....................................... 411 Adaptive and Effective Keyword Search for XML .................... 423 Steering Time-Dependent Esti mation of Posteriors with Hyperparameter Indexing in Bayesian Topic Models .................. 435 Constrained LDA for Grouping Product Features in Opinion Mining ... 448 Semantic Dependent Word Pairs Generative Model for Fine-Grained Product Feature Mining .......................................... 460 Grammatical Depe ndency-Based Relations for Term Weighting in Text Classification .................................................... 476 XML Documents Clustering Using a Tensor Space Model ............. 488 An Efficient Pre-processing Method to Identify Logical Components from PDF Documents ............................................ 500 Combining Proper Name-Coreference with Conditional Random Fields for Semi-supervised Named Entity Recognition in Vietnamese Text ..... 512 Topic Analysis of Web User Behavior Using LDA Model on Proxy Logs ........................................................... 525 SizeSpotSigs: An Effective Deduplicate Algorithm Considering the Size of Page Content ................................................. 537 Knowledge Transfer across Multilingual Corpora via Latent Topics ..... 549
 The Asia Information Retrieval Symposium (AIRS) was established by the Asian information retrieval community after the successful series of Information Re-trieval with Asian Languages (IRAL) workshops held in six different locations in Asia, starting from 1996. While the IRAL workshops had their focus on infor-mation retrieval problems involving Asian languages, AIRS covers a wider scope of applications, systems, technologies and theory aspects of information retrieval in text, audio, image, video and multimedia data. This extension of the scope reflects and fosters increasing research activities in information retrieval in this region and the growing need for collaborations across subdisciplines. submissions and their quality, compared to the IRAL workshops. We received 106 papers from nine countries in Asia and North America, from which 28 papers (26%) were presented in oral sessions and 38 papers in poster sessions (36%). It was a great challenge for the Program Committee to select the best among the excellent papers. The low acceptance rates witness the success of this year X  X  conference.
 Springer, the publisher agreed to publish our proceedings in the Lecture Notes in Computer Science (LNCS) series, which is SCI-indexed. We feel that this strongly attests to the excellent quality of the papers.
 all the technical programs at this conference. A tutorial was given on the first day to introduce the state of the art in Web mining, an important application of Web document retrieval. Two keynote speeches covered two main areas of the conference: video retrieval and language issues. There were a total of eight oral sessions run, with two in parallel at a time, and two poster/demo sessions. by the hard-working people behind the scenes. In addition to the Program Com-mittee members, we are thankful to the Organizing Committee (Shao-Ping Ma and Jianfeng Gao, Co-chairs), Interactive Posters/Demo Chair (Gary G. Lee), and the Special Session and Tutorials Chair (Wei-Ying Ma). We also thank the sponsoring organizations: Microsoft Research Asia, the Department of Systems Engineering and Engineering Management at the Chinese University of Hong Kong, and LexisNexis for their financial support, the Department of Computer Science and Technology, Tsinghua University for local arrangements, the Chi-nese NewsML Community for website design and administration, Ling Huang for the logistics, Weiwei Sun for the conference webpage management, EONSO-LUTION for the conference management, and Springer for the postconference LNCS publication. We believe that this conference set a very high standard for a regionally oriented conference, especially in Asia, and we hope that it continues as a tradition in the upcoming years.
 Kam-Fai Wong, Chinese University of Hong Kong, China Hong-Jiang Zhang, Microsoft Research Asia, China Sung Hyon Myaeng, Information and Communications University (ICU), Ming Zhou, Microsoft Research Asia, China Jianfeng Gao, Microsoft Research Asia, China Shao-Ping Ma, Tsinghua University, China Wei-Ying Ma, Microsoft Research Asia, China Gary Geunbae Lee, POSTECH, South Korea Jun Adachi, National Institute of Informatics, Japan Hsin-Hsi Chen, National Taiwan University, Taiwan Lee-Feng Chien, Academia Sinica, Taiwan Tetsuya Ishikawa, University of Tsukuba, Japan Gary Geunbae Lee, POSTECH, South Korea Mun-Kew Leong, Institute for Infocomm Research, Singapore Helen Meng, Chinese University of Hong Kong, China Sung-Hyon Myaeng, Information and Communications University, South Korea Hwee Tou Ng, National University of Singapore, Singapore Kam-Fai Wong, Chinese University of Hong Kong, China Lee-Feng Chien, Academia Sinica, Taiwan (Publicity, Asia) Susan Dumais, Microsoft, USA (Publicity, North America) Jianfeng Gao, Microsoft Research Asia, China (Publication, Co-chair) Mun-Kew Leong, Institute for Infocomm Research, Singapore (Finance) Shao-Ping Ma, Tsinghua University, China (Local Organization, Co-chair) Ricardo Baeza-Yates, University of Chile, Chile (Publicity, South America) Shucai Shi, BITI, China (Local Organization) Dawei Song, DSTC, Australia (Publicity, Australia) Ulich Thiel, IPSI, Germany (Publicity, Europe) Chuanfa Yuan, Tsinghua University, China (Local Organization) PeterAnick,Yahoo,USA Hsin-Hsi Chen, National Taiwan University, Taiwan Aitao Chen, University of California, Berkeley, USA Lee-Feng Chien, Academia Sinica, Taiwan Fabio Crestani, University of Strathclyde, UK Edward A. Fox, Virginia Tech, USA Jianfeng Gao, Microsoft Research Asia, China Hani Abu-Salem, DePaul University, USA Tetsuya Ishikawa, University of Tsukuba, Japan Christopher Khoo, Nanyang Technological University, Singapore Jung Hee Kim, North Carolina A&amp;T University, USA Minkoo Kim, Ajou University, South Korea Munchurl Kim, Information and Communication University, South Korea Kazuaki Kishida, Surugadai University, Japan Kui-Lam Kwok, Queens College, City University of New York, USA Wai Lam, Chinese University of Hong Kong, China Gary Geunbae Lee, POSTECH, South Korea Mun-Kew Leong, Institute for Infocomm Research, Singapore Gena-Anne Levow, University of Chicago, USA Hang Li, Microsoft Research Asia, China Robert Luk, Hong Kong Polytechnic University, China Gay Marchionini, University of North Carolina, Chapel Hill, USA Helen Meng, Chinese University of Hong Kong, China Hiroshi Nakagawa, University of Tokyo, Japan Hwee Tou Ng, National University of Singapore, Singapore Jian-Yun Nie, University of Montreal, Canada Jon Patrick, University of Sydney, Australia Ricardo Baeza-Yates, University of Chile, Chile Hae-Chang Rim, Korea University, South Korea Tetsuya Sakai, Toshiba Corporate R&amp;D Center, Japan Padmini Srinivasan, University of Iowa, USA Tomek Strzalkowski, State University of New York, Albany, USA Maosong Sun, Tsinghua University, China Ulrich Thiel, Fraunhofer IPSI, Germany Takenobu Tokunaga, Tokyo Institute of Technology, Japan Hsin-Min Wang, Academia Sinica, Taiwan Ross Wilkinson, CSIRO, Australia Lide Wu, Fudan University, China Jinxi Xu, BBN Technologies, USA ChengXiang Zhai, University of Illinois, Urbana Champaign, USA Min Zhang, Tsinghua University, China Peter Anick Yunb o Cao Yee Seng Chan Hsin-Hsi Chen Zheng Chen Aitao Chen Tee Kiah Chia Lee-Feng Chien Fabio Crestani Edward A. Fox Jianfeng Gao Hani Abu-Salem Xuanjing Huang Tetsuya Ishikawa Christopher Khoo Jung Hee Kim Minkoo Kim Munchurl Kim Kazuaki Kishida Automatic Word Clustering for Text Categorization Using Global Information Text Classification Using Web Corpora and EM Algorithms Applying CLIR Techniques to Event Tracking Document Clustering Using Linear Partitioning Hyperplanes and Reallocation Summary Generation Centered on Important Words Sentence Compression Learned by News Headline for Displaying in Small Device Automatic Text Summarization Using Two-Step Sentence Extraction Sentence Extraction Using Time Features in Multi-document Summarization Extracting Paraphrases of Japanese Action Word of Sentence Ending Part from Web and Mobile News Articles Improving Transliteration with Precise Alignment of Phoneme Chunks and Using Contextual Features Combining Sentence Length with Location Information to Align Monolingual Parallel Texts Effective Topic Distillation with Key Resource Pre-selection Efficient PageRank with Same Out-Link Groups Literal-Matching-Biased Link Analysis Multilingual Relevant Sentence Detection Using Reference Corpus A Bootstrapping Approach for Geographic Named Entity Annotation Using Verb Dependency Matching in a Reading Comprehension System Sense Matrix Model and Discrete Cosine Transform Query Model Estimations for Relevance Feedback in Language Modeling Approach A Measure Based on Optimal Matching in Graph Theory for Document Similarity Estimation of Query Model from Parsimonious Translation Model Ranking the NTCIR Systems Based on Multigrade Relevance X-IOTA: An Open XML Framework for IR Experimentation Recognition-Based Digitalization of Korean Historical Archives On Bit-Parallel Processing of Multi-byte Text Retrieving Regional Information from Web by Contents Localness and User Location Towards Understanding the Functions of Web Element Clustering-Based Navigation of Image Search Results on Mobile Devices
 Asia Information Retrieval Symposium (AIRS) was established in 2004 by the Asian information retrieval (IR). The scope of the symposium covers applications, systems, video and multi-media data. were presented. It was a great challenge and hard work for the program committee to LNCS (Lecture Notes in Computer Science) proceedings volume, which is SCI-indexed. The technical program included two keynote talks by Prof. Walter Bender and Prof. including one special session and also two poster sessions. 
The technical and social programs, whic h we are proud of, were made possible by members, we are thankful to the Organizing Committee, Poster Chair (Helen Meng), conference website management. LexisNexis, USA; INEK, Korea; and Daum Soft, Korea for their support, as well as these proceeding in their LNCS series. conference as well as the beautiful scenery of Jeju island. August 2005 Gary G. Lee and Akio Yamada General Conference Chairs Sung Hyon Myaeng (ICU, Korea) Dong In Park (KISTI, Korea) Program Chairs Gary G. Lee (POSTECH, Korea) Akio Yamada (NEC, Japan) Poster Chair Helen Meng (Chinese University of Hong Kong, China) Demo/Exhibition Chair Myung-Gil Jang (ETRI, Korea) Publicity Chairs Hsin-hsi Chen (National Taiwan University, Taiwan) MunChurl Kim (ICU, Korea) Mun-kew Leong (I2R, Singapore) Tetsuya Sakai (Toshiba, Japan) Bing Swen (Peking University, China) Robert Luk (Hong Kong Polytechnic University, China) Sponsor Chair Hee Suk Lim (Hansin University) Treasurer/Secretary Bo Yeong Kang (ICU, Korea) Steering Committee Jun Adachi, National Institute of Informatics, Japan Hsin-Hsi Chen, National Taiwan University, Taiwan Lee-Feng Chien, Academia Sinica, Taiwan Gary Geunbae Lee, POSTECH, Korea Mun-Kew Leong, Institute for Infocomm Research, Singapore Helen Meng, The Chinese University of Hong Kong, China Sung Hyon Myaeng, Information and Communication University, Korea Hwee Tou Ng, National University of Singapore, Singapore Tetsuya Sakai, Toshiba, Japan Kam-Fai Wong, The Chinese University of Hong Kong, China Ming Zhou, Microsoft Research Asia, China Organization Committee Bo Yeong Kang (Information &amp; Communications University) Munchurl Kim (Information &amp; Communications University) Myung-Gil Jang (ETRI) Pyung Kim (KISTI) Qing Li (Information &amp; Communications University) Hee Suk Lim (Hansin University) Dong-Chul Lee (Cheju National University) Youngjoong Ko (Dong-A University) Hanmin Jung (KISTI) Kyung Soon Lee (Chonbuk National University) Hosted by Information and Communications University Korea Institute of Science and Technology Information Hani Abu-Salem, DePaul University, USA Terumasa Aoki, University of Tokyo, Japan Kohtaro Asai, Mitsubishi Electric, Japan Ricardo Baeza-Yates, ICREA-UPF, Spain &amp; CWR, Univ. of Chile Hsin-Hsi Chen, National Taiwan University, Taiwan Lee-Feng Chien, Academia Sinica, Taiwan Tat-Seng Chua, National University of Singapore, Singapore Fabio Crestani, University of Strathclyde, Scotland, UK Jianfeng Gao, Microsoft Research Asia, China Tetsuya Ishikawa, University of Tsukuba, Japan Min-Yen Kan, National University of Singapore, Singapore Noriko Kando, National Institute of Informatics, Japan Christopher Khoo, Nanyang Technological University, Singapore Munchurl Kim, ICU, Korea Hoi-Rin Kim, ICU, Korea Juntae Kim, Dongguk University, Korea Kazuaki Kishida, Surugadai University, Japan Kui-Lam Kwok, Queens College, City University of New York, USA Wai Lam, Chinese University of Hong Kong, China Gary Geunbae Lee, POSTECH, Korea JaeHo Lee, University of Seoul, Korea Jae Sung Lee, Chungbuk National University, Korea Jong-Hyeok Lee, POSTECH, Korea Mun-Kew Leong, Institute for Infocomm Research, Singapore Gina-Anne Levow, University of Chicago, USA Hang Li, Microsoft Research Asia, China Ee-Peng Lim, Nanyang Technological University, Singapore Chin-Yew Lin, University of Southern California, USA Robert Luk, Hong Kong Polytechnic University, China Helen Meng, The Chinese University of Hong Kong, China Hiroshi Murase, Nagoya University, Japan Hiroshi Nakagawa, University of Tokyo, Japan Hwee Tou Ng, National University of Singapore, Singapore Jian-Yun Nie, University of Montreal, Canada Sam Gyun Oh, Sungkyunkwan University, Korea Jon Patrick, University of Sydney, Australia Fuchun Peng, BBN Technologies, USA Hae-Chang Rim, Korea University, Korea YongMan Ro, ICU, Korea Tetsuya Sakai, Toshiba Corporate R&amp;D Center, Japan Shin'ichi Satoh, National Institute of Informatics, Japan John R. Smith, IBM T. J. Watson Research Center, USA Padmini Srinivasan, University of Iowa, USA Masaru Sugano, KDDI R&amp;D Laboratories, Japan Aixin Sun, University of New South Wales, Australia Maosong Sun, Tsinghua University, China Ulrich Thiel, Fraunhofer IPSI, Germany Takenobu Tokunaga, Tokyo Institute of Technology, Japan Hsin-Min Wang, Academia Sinica, Taiwan Kam-Fai Wong, CUHK, China Ross Wilkinson, CSIRO, Australia Lide Wu, Fudan University, China Seong-Joon Yoo, Sejong University, Korea ChengXiang Zhai, University of Illinois, Urbana Champaign, USA Min Zhang, Tsinghua University, China Ming Zhou, Microsoft Research Asia, China Justin Zobel, RMIT, Australia Yee Seng Chan, Tee Kiah Chia, Yunbo Cao, Jie Tang, Jun Xu The Reliability of Metrics Based on Graded Relevance Improving Weak Ad-Hoc Retrieval by Web Assistance and Data Fusion Query Expansion with the Minimum Relevance Judgments Improved Concurrency Control Technique with Lock-Free Querying for Multi-dimensional Index Structure A Color-Based Image Retrieval Method Using Color Distribution and Common Bitmap A Probabilistic Model for Music Recommendation Considering Audio Features VisMed: A Visual Vocabulary Approach for Medical Image Indexing and Retrieval Object Identification and Retrieval from Efficient Image Matching: Snap2Tell with the STOIC Dataset Extracting the Significant Terms from a Sentence-Term Matrix by Removal of the Noise in Term Usage Cross Document Event Clustering Using Knowledge Mining from Co-reference Chains Filtering Contents with Bigrams and Named Entities to Improve Text Classification The Spatial Indexing Method for Supporting a Circular Location Property of Object Supervised Categorization of JavaScript TM Using Program Analysis Features Effective and Scalable Authorship Attribution Using Function Words Learning to Integrate Web Taxonomies with Fine-Grained Relations: A Case Study Using Maximum Entropy Model WIDIT: Fusion-Based Approach to Web Search Optimization Transactional Query Identification in Web Search Improving FAQ Retrieval Using Query Log Clustering in Latent Semantic Space Phrase-Based Definitional Question Answering Using Definition Terminology Enhanced Question Answering with Combination of Pre-acquired Answers An Empirical Study of Query Expansion and Cluster-Based Retrieval in Language Modeling Approach Effective Query Model Estimation Using Parsimonious Translation Model in Language Modeling Approach Chinese Document Re-ranking Based on Term Distribution and Maximal Marginal Relevance On Effectiveness Measures and Relevance Functions in Ranking INEX Systems Home Photo Categorization Based on Photographic Region Templates MPEG-7 Visual-Temporal Clustering for Digital Image Collections A Structured Learning Approach to Semantic Photo Indexing and Query Image Retrieval Using Sub-image Matching in Photos Using MPEG-7 Descriptors An Incremental Document Clustering for the Large Document Database Subsite Retrieval: A Novel Concept for Topic Distillation A Rough Set-Based Fuzzy Clustering Effective Use of Place Information for Event Tracking A Classifier Design Based on Combining Multiple Components by Maximum Entropy Principle A Query-by-Singing Technique for Retrieving Polyphonic Objects of Popular Music Integrating Textual and Visual Information for Cross-Language Image Retrieval Practical Application of Associative Classifier for Document Classification A Method for Query Expansion Using a Hierarchy of Clusters Chinese Question Classification from Approach and Semantic Views GJM-2: A Special Case of General Jelinek-Mercer Smoothing Method for Language Modeling Approach to Ad Hoc IR The Empirical Impact of the Nature of Novelty Detection Song Wave Retrieval Based on Frame-Wise Phoneme Recognition trackThem : Exploring a Large-Scale News Video Archive by Tracking Human Relations Topic-Independent Web High-Quality Page Selection Based on K-Means Clustering Improving Text Similarity Measurement by Critical Sentence Vector Model Audio Fingerprinting Scheme by Temporal Filtering for Audio Identification Immune to Channel-Distortion On the Chinese Document Clustering Based on Dynamical Term Clustering Integrating Heterogeneous Multimedia Resources Calculating Webpage Importance with Site Structure Constraints Gene Ontology Classification of Biomedical Literatures Using Context Association An Examination of Feature Selection Frameworks in Text Categorization Query Transitive Translation Using IR Score for Indonesian-Japanese CLIR Development of a Meta Product Search Engine with Web Services An Automatic Code Classification System by Using Memory-Based Learning and Information Retrieval Technique Finding New News: Novelty Detection in Broadcast News Named Entity Tagging for Korean Using DL-CoTrain Algorithm Semantic Categorization of Contextual Features Based on Wordnet for G-to-P Conversion of Arabic Numerals Combined with Homographic Classifiers Robust Matching Method for Scale and Rotation Invariant Local Descriptors and Its Application to Image Indexing Image Feedback Retrieval Based on Vector Space Model Transformation Indexing Structures for Content-Based Retrieval of Large Image Databases: A Review Document Similarity Search Based on Generic Summaries KNM: A Novel Intelligent User Interface for Webpage Navigation Towards Construction of Evaluation Framework for Query Expansion An Efficient Incremental Nearest Neighbor Algorithm for Processing k -Nearest Neighbor Queries with Visal and Semantic Predicates in Multimedia Information Retrieval System Generating Term Transliterations Using Contextual Information and Validating Generated Results Using Web Corpora Handling Orthographic Varieties in Japanese IR: Fusion of Word-, N-Gram-, and Yomi-Based Indices Across Different Document Collections A Term Weighting Approach for Text Categorization A LF Based Answer Indexing Method for Encyclopedia Question-Answering System Approximate Phrase Match to Compile Synonymous Translation Terms for Korean Medical Indexing Protein Function Classification Based on Gene Ontology Extracting and Utilizing of IS-A Relation Patterns for Question Answering Systems An Iterative Approach for Web Catalog Integration with Support Vector Machines Fuzzy Post-clustering Algorithm for Web Search Engine A Relational Nested Interval Encoding Scheme for XML Storage and Retrieval The Design of Webservices Framework Support Ontology Based Dynamic Service Composition Privacy Preserving Decision Tree in Multi Party Environment
 Commenced Publication in 1973 Founding and Former Series Editors: Gerhard Goos, Juris Hartmanis, and Jan van Leeuwen David Hutchison Takeo Kanade Josef Kittler Jon M. Kleinberg Friedemann Mattern John C. Mitchell Moni Naor Oscar Nierstrasz C. Pandu Rangan Bernhard Steffen Madhu Sudan Demetri Terzopoulos Doug Tygar Moshe Y. Vardi Gerhard Weikum Volume Editors Hwee Tou Ng Min-Yen Kan
National University of Singapore, Department of Computer Science 3 Science Drive 2, Singapore 117543 E-mail: {nght, kanmy} @comp.nus.edu.sg Mun-Kew Leong Donghong Ji
Institute for Infocomm Research 21 Heng Mui Keng Terrace, Singapore 119613 E-mail: {mkleong, dhji}@i2r.a-star.edu.sg Library of Congress Control Number: 2006932723 CR Subject Classification (1998): H.3, H.4, F.2.2, E.1, E.2
LNCS Sublibrary: S L 3  X  Information Systems and Application, incl. Internet/Web and HCI ISSN 0302-9743 ISBN-10 3-540-45780-1 Springer Berlin Heidelberg New York ISBN-13 978-3-540-45780-0 Springer Berlin Heidelberg New York Asia Information Retrieval Symposium (AIRS) 2006 was the third AIRS confer-ence in the series established in 2004. The first AIRS was held in Beijing, China, and the 2nd AIRS was held in Cheju, Korea. The AIRS conference series traces its roots to the successful Information Retrieval with Asian Languages (IRAL) workshop series which started in 1996.
 opers to exchange new ideas and the latest results in information retrieval. The scope of the conference encompassed the theory and practice of all aspects of information retrieval in text, audio, image, video, and multimedia data. number since the conference series started in 2004. Submissions came from Asia and Australasia, Europe, and North America. We accepted 34 submissions as regular papers (23%) and 24 as poster papers (16%).
 ence, the seven area chairs, who worked tirelessly to recruit the program com-mittee members and oversaw the review process, and the program committee members and their secondary reviewers who reviewed all the submissions. publisher Springer, compiled the camera ready papers and turned them into the beautiful proceedings you are reading now. We also thank Donghong Ji, who chaired the local organization efforts and maintained the AIRS 2006 website, Kanagasabai Rajaraman for publicity efforts, Yee Seng Chan for helping with the START conference management software, and Daniel Racoceanu for organizing and chairing the special session on medical image retrieval.
 for Infocomm Research and the National University of Singapore for hosting the conference, Springer for publishing our proceedings, and AVC for local organi-zation and secretariat support.
 August 2006 Mun-Kew Leong AIRS 2006 was organized by the Institute for Infocomm Research and co-organi-zed by the National University of Singapore.
 Program Chair Hwee Tou Ng General Chair Mun-Kew Leong Publication Chair Min-Yen Kan Organizing Chair Donghong Ji Organizing Committee GuoDong Zhou Lee-Feng Chien Academia Sinica Noriko Kando National Institute of Informatics Kui-Lam Kwok Queens College, City University of New York Hang Li Microsoft Research Asia Ee-Peng Lim Nanyang Technological University Jian-Yun Nie University of Montreal Hae-Chang Rim Korea University Jun Adachi National Institute of Informatics Robert Allen Drexel University Vo Ngoc Anh University of Melbourne Thomas Baker Goettingen State and University Library Timothy Baldwin University of Melbourne Yunbo Cao Microsoft Research Asia Aitao Chen Yahoo! Hsinchun Chen University of Arizona Hsin-Hsi Chen National Taiwan University Kuang-hua Chen National Taiwan University Lei Chen Hong Kong University of Science and Technology Zheng Chen Microsoft Research Asia Jean-Pierre Chevallet IPAL CNRS, Institute for Infocomm Research Liang-Tien Chia Nanyang Technological University Lee-Feng Chien Academia Sinica Tat-Seng Chua National University of Singapore Hang Cui National University of Singapore Koji Eguchi National Institute of Informatics Patrick Fan Virginia Tech Jianfeng Gao Microsoft Research Fredric C. Gey University of California, Berkeley Dion Hoe-Lian Goh Nanyang Technological University Donna Harman NIST Wen-Lian Hsu Academia Sinica Jimmy Huang York University Makoto Iwayama Hitachi Ltd.
 Myung-Gil Jang ETRI Donghong Ji Institute for Infocomm Research Rong Jin Michigan State University Hideo Joho Glasgow University Gareth Jones City University of Dublin Min-Yen Kan National University of Singapore Ji-Hoon Kang Chungnam National University Tsuneaki Kato Tokyo University Sang-Bum Kim Tokyo Institute of Technology Kazuaki Kishida Keio University Youngjoong Ko Dong-A University Kui-Lam Kwok Queens College, City University of New York Mounia Lalmas Queen Mary, University of London Wai Lam Chinese University of Hong Kong Gary Geunbae Lee Pohang University of Science &amp; Technology Jong-Hyeok Lee Pohang University of Science &amp; Technology Gina-Anne Levow University of Chicago Haizhou Li Institute for Infocomm Research Hang Li Microsoft Research Asia Mingjing Li Microsoft Research Asia Hee Suk Lim Hansin University Chin-Yew Lin Microsoft Research Asia Chuan-Jie Lin National Taiwan Ocean University Tie-Yan Liu Microsoft Research Asia Ting Liu Harbin Institute of Technology Yan Liu The Hong Kong Polytechnic University Robert Wing Pong Luk Hong Kong Polytechnic University Qing Ma Ryukoku University Shaoping Ma Tsinghua University Helen Meng The Chinese University of Hong Kong Satoshi Morinaga NEC Sung Hyon Myaeng Information &amp; Communications University Takashi Nagatsuka Tsurumi University Hiroshi Nakagawa University of Tokyo Hidetsugu Namba Hiroshima City University Jian-Yun Nie University of Montreal Jon Patrick University of Sydney Fuchun Peng Yahoo! Carol Peters Italian National Research Council Ian Ruthven University of Strathclyde Tetsuya Sakai Toshiba Corporate R&amp;D Center Mark Sanderson Sheffield University Jacques Savoy University of Neuchatel Koichi Shinoda Tokyo Institute of Technology Dawei Song The Open University Shigeo Sugimoto University of Tsukuba Hiroya Takamura Tokyo Institute of Technology Katsumi Tanaka Kyoto University Yin-Leng Theng Nanyang Technological University Naohiko Uramoto IBM Shalini R. Urs University of Mysore Hsin-Min Wang Academia Sinica Ji-Rong Wen Microsoft Research Asia Christa Womster University of Hildesheim Kam-Fai Wong Chinese University of Hong Kong Lide Wu Fudan University Xing Xie Microsoft Research Asia Yiyu Yao University of Regina Masaharu Yoshioka Hokkaido University Yong Yu Shanghai Jiaotong University Marcia Zeng Kent State University ChengXiang Zhai University of Illinois, Urbana Champaign Dell Zhang Birkbeck, University of London Jian Zhang National ICT Australia Lei Zhang Microsoft Research Asia Min Zhang Tsinghua University Ming Zhou Microsoft Research Asia Justin Zobel RMIT Bo Chen Yuting Liu Yuanhao Chen Tao Qin Bin Gao Hironori Takeuchi Ming-Qiang Hou Mingfeng Tsai Yang Hu Bin Wang Feng Jing Canhui Wang Hiroshi Kanayama Changhu Wang Jiaming Li Huan Wang Xirong Li Min Xu Jing Liu Xin Yan Song Liu Wujie Zheng Yiqun Liu Query Expansion with ConceptNet and WordNet: An Intrinsic Comparison ...................................................... 1 Document Similarity Search Based on Manifold-Ranking of TextTiles .... 14 Adapting Document Ranking to Users X  Preferences Using Click-Through Data ............................................... 26 A PDD-Based Searching Approach for Expert Finding in Intranet Information Management .......................................... 43 A Supervised Learning Approach to Entity Search .................... 54 Hierarchical Learning Strategy in Relation Extraction Using Support Vector Machines .................................................. 67 Learning to Separate Text Content and Style for Classification .......... 79 Using Relative Entropy for Authorship Attribution .................... 92 Efficient Query Evaluation Through Access-Reordering ................ 106 Natural Document Clustering by Clique Percolation in Random Graphs ................................................ 119 Text Clustering with Limited User Feedback Under Local Metric Learning .................................................. 132 Toward Generic Title Generation for Clustered Documents ............. 145 Word Sense Language Model for Information Retrieval ................. 158 Statistical Behavior Analysis of Smoothing Methods for Language Models of Mandarin Data Sets ...................................... 172 No Tag, a Little Nesting, and Great XML Keyword Search ............. 187 Improving Re-ranking of Search Results Using Collaborative Filtering ... 205 Learning to Integrate Web Catalogs with Conceptual Relationships in Hierarchical Thesaurus ............................................ 217 Discovering Authoritative News Sources and Top News Stories .......... 230 Chinese Question-Answering: Comparing Monolingual with English-Chinese Cross-Lingual Results ............................... 244 Translation of Unknown Terms Via Web Mining for Information Retrieval ......................................................... 258 A Cross-Lingual Framework for Web News Taxonomy Integration ....... 270 Learning Question Focus and Semantically Related Features from Web Search Results for Chinese Question Classification ..................... 284 Improving the Robustness to Recognition Errors in Speech Input Question Answering ............................................... 297 An Adjacency Model for Sentence Ordering in Multi-document Summarization ................................................... 313 Poor Man X  X  Stemming: Unsupervised Recognition of Same-Stem Words ........................................................... 323 NAYOSE: A System for Reference Disambiguation of Proper Nouns Appearing on Web Pages .......................................... 338 Efficient and Robust Phrase Chunking Using Support Vector Machines .................................................. 350 Statistical and Comparative Evaluation of Various Indexing and Search Models .......................................................... 362 Bootstrap-Based Comparisons of IR Metrics for Finding One Relevant Document ........................................................ 374 Evaluating Topic Difficulties from the Viewpoint of Query Term Expansion ....................................................... 390 Incorporating Prior Knowledge into Multi-label Boosting for Cross-Modal Image Annotation and Retrieval ........................ 404 A Venation-Based Leaf Image Classification Scheme ................... 416 Pic-A-Topic: Gathering Information Efficiently from Recorded TV Shows on Travel .................................................. 429 A Music Retrieval System Based on Query-by-Singing for Karaoke Jukebox ......................................................... 445 A Semantic Fusion Approach Between Medical Images and Reports Using UMLS ..................................................... 460 Automated Object Extraction for Medical Image Retrieval Using the Insight Toolkit (ITK) .............................................. 476 Stripe: Image Feature Based on a New Grid Method and Its Application in ImageCLEF .................................................... 489 An Academic Information Retrieval System Based on Multiagent Framework ....................................................... 497 Comparing Web Logs: Sensitivity Analysis and Two Types of Cross-Analysis .................................................... 508 Concept Propagation Based on Visual Similarity ...................... 514 Query Structuring with Two-Stage Term Dependence in the Japanese Language ........................................................ 522 Automatic Expansion of Abbreviations in Chinese News Text ........... 530 A Novel Ant-Based Clustering Approach for Document Clustering ...... 537 Evaluating Scalability in Information Retrieval with Multigraded Relevance ........................................................ 545 Text Mining for Medical Documents Using a Hidden Markov Model ..... 553 Multi-document Summarization Based on Unsupervised Clustering ...... 560 A Content-Based 3D Graphic Information Retrieval System ............ 567 Query Expansion for Contextual Question Using Genetic Algorithms .... 574 Fine-Grained Named Entity Recognition Using Conditional Random Fields for Question Answering ...................................... 581 A Hybrid Model for Sentence Ordering in Extractive Multi-document Summarization ................................................... 588 Automatic Query Type Identification Based on Click Through Information ...................................................... 593 Japanese Question-Answering System for Contextual Questions Using Simple Connection Method, Decreased Adding with Multiple Answers, and Selection by Ratio ............................................. 601 Multi-document Summarization Using a Clustering-Based Hybrid Strategy ......................................................... 608 A Web User Preference Perception System Based on Fuzzy Data Mining Method ................................................... 615 An Analysis on Topic Features and Difficulties Based on Web Navigational Retrieval Experiments ................................. 625 Towards Automatic Domain Classification of Technical Terms: Estimating Domain Specificity of a Term Using the Web ............... 633 Evaluating Score Normalization Methods in Data Fusion ............... 642 WIDIT: Integrated Approach to HARD Topic Search .................. 649 Automatic Query Expansion Using Data Manifold .................... 659 An Empirical Comparison of Translation Disambiguation Techniques for Chinese X  X nglish Cross-Language Information Retrieval ............. 666 Web Mining for Lexical Context-Specific Paraphrasing ................. 673
 Commenced Publication in 1973 Founding and Former Series Editors: Gerhard Goos, Juris Hartmanis, and Jan van Leeuwen David Hutchison Takeo Kanade Josef Kittler Jon M. Kleinberg Alfred Kobsa Friedemann Mattern John C. Mitchell Moni Naor Oscar Nierstrasz C. Pandu Rangan Bernhard Steffen Madhu Sudan Demetri Terzopoulos Doug Tygar Gerhard Weikum Volume Editors Gary Geunbae Lee Pohang University of Science and Technology, E-mail: gblee@postech.ac.kr Dawei Song The Robert Gordon University, E-mail: d.song@rgu.ac.uk Chin-Yew Lin Microsoft Research Asia, E-mail: cyl@microsoft.com Akiko Aizawa National Institute of Informatics, E-mail: aizawa@nii.ac.jp Kazuko Kuriyama Shirayuri College, E-mail: kuriyama@shirayuri.ac.jp Masaharu Yoshioka Hokkaido University, E-mail: yoshioka@ist.hokudai.ac.jp Tetsuya Sakai Microsoft Research Asia, E-mail: tetsuyasakai@acm.org Library of Congress Control Number: 2009935701 CR Subject Classification (1998): H.3, H.4, F.2.2, E.1, E.2
LNCS Sublibrary: SL 3  X  Information Systems and Application, incl. Internet/Web and HCI ISSN 0302-9743 ISBN-10 3-642-04768-8 Springer Berlin Heidelberg New York ISBN-13 978-3-642-04768-8 Springer Berlin Heidelberg New York Asia Information Retrieval Symposium (AIRS) 2009 was the fifth AIRS confer-ence in the series established in 2004. The first AIRS was held in Beijing, China, the second in Jeju, Korea, the third in Singapore and the fourth in Harbin, China. The AIRS conferences trace their roots to the successful Information Retrieval with Asian Languages (IRAL) workshops, which started in 1996. opers to exchange new ideas and the lates t results in information retrieval. The scope of the conference encompassed the theory and practice of all aspects of information retrieval in text, audio, image, video, and multimedia data. papers (22%) and 20 (24%) posters through a double-blind reviewing process. We are pleased to report that the conferenc e proceedings include contributions from not only Asian countries, but also from Finland, Italy, Australia, UK and USA. the honorary conference chair, and to Hokkaido University for hosting the con-ference. We thank the Information Retrieval Facility, Microsoft Research Asia, Ricoh, Ltd., Global COE Program  X  X enter for Next-Generation Information Technology Based on Knowledge Discovery and Knowledge Federation X  and Sap-poro International Communication Plaza Foundation for sponsoring the confer-ence, and Springer for publishing the con ference proceedings as part of their Lecture Notes in Computer Science (LNCS) series. We also thank ACM SIGIR and IPSJ SIGFI for giving the conferen ce an  X  X n cooperation with X  status. compiling the camera-ready papers and liasing with Springer, and the finance chair, Takuya Kida, for successfully managing all money matters. We also thank the publicity co-chairs, Atsushi Fujii, Ian Soboroff, Dawei Song and William Webber, for advertising AIRS, and the area chairs and other program committee members for their high-quality and punctual reviews.
 mitted papers and who continue to contribute to this Asian community of IR research and development.
 August 2009 Gary Geunbae Lee Hsin-Hsi Chen National Taiwan University, Taiwan Wai Lam The Chinese University of Hong Kong, China Gary Geunbae Lee Pohang University of Science and Technology, Alistair Moffat University of Melbourne, Australia Hwee Tou Ng National University of Singapore, Singapore Tetsuya Sakai Microsoft Research Asia, China Dawei Song The Robert Gordon University, UK Masaharu Yoshioka Hokkaido University, Japan Mun Kew Leong National Library Board, Singapore Sung Hyon Myaeng Information and Communication University, Kam-Fai Wong The Chinese University of Hong Kong, China Elizabeth Liddy Syracuse University, USA Tetsuya Sakai Microsoft Research Asia, China Masaharu Yoshioka Hokkaido University, Japan Gary Geunbae Lee Pohang University of Science and Technology, Chin-Yew Lin Microsoft Research Asia, China Dawei Song The Robert Gordon University, UK Finance Chair Takuya Kida, Hokkaido University, Japan Publication Co-chair Akiko Aizawa, National Institute of Publicity Co-chair Atsushi Fujii, University of Tsukuba, Japan Gareth Jones Dublin City University, Ireland Tie-Yan Liu Microsoft Research Asia, China David Losada Universidad de Santiago de Compostela, Teruko Mitamura Carnegie Mellon University, USA Ian Ruthven University of Strathclyde, UK Andrew Turpin RMIT, Australia Min Zhang Tsinghua University, China Hokkaido University, Japan Gianni Amati Fondazione Ugo Bordoni, Italy Leif Azzopardi University of Glasgow, UK Peter Bailey Microsoft Research, USA Mark Baillie University of Strathclyde, UK Alvaro Barreiro University of A Coru X a, Spain Bodo Billerbeck RMIT University, Australia Roi Blanco University of A Coru X a, Spain Rui Cai Microsoft Research Asia, China Enhong Chen University of Science and Technology, China Hsin-Hsi Chen National Taiwan University, Taiwan Xuqi Cheng Chinese Academy of Sciences, China Seungjin Choi Pohang University of Science and Technology, Nick Craswell Microsoft Research Cambridge, UK Shane Culpepper RMIT University, Australia Efthimis N. Efthimiadis University of Washington, USA David Elsweiler Friedrich-Alexander-University of Juan M. Fern X ndez Luna University of Granada, Spain Colum Foley Dublin City University, Ireland Bin Gao Microsoft Research Asia, China Fred Gey University of California, Berkeley, USA Daqing He University of Pittsburgh, USA Ichiro Ide Nagoya University, Japan Hideo Joho University of Glasgow, UK Hiroshi Kanayama Tokyo Research Laboratory, IBM, Japan Jaana Kek X l X inen University of Tampere, Finland Jeongwoo Ko Google, USA Wei Lai Microsoft Research Asia, China Mounia Lalmas University of Glasgow, UK Wai Lam Chinese University of Hong Kong, Hong Kong Ni Lao Carnegie Mellon University, USA Martha Larson Technical University of Delft, Ting Liu Harbin Institute of Technology, China Yiqun Liu Tsinghua University, China Andrew MacFarlane City University of London, UK Stephane Marchand-Maillet University of Geneva, Switzerland Massimo Melucci University of Padova, Italy Helen Meng The Chinese University of Hong Kong, Tatsunori Mori Yokohama National University, Japan Alessandro Moschitti University of Trento, Italy Jian-Yun Nie University of Montreal, Canada Eric Nyberg Carnegie Mellon University, USA Neil O X  X are Dublin City University, Ireland Nils Pharo Oslo University College, Norway Benjamin Piwowarski University of Glasgow, UK Dulce Poncele X n IBM Almaden Research, USA Thomas Roelleke Queen Mary University of London, UK Shinichi Satoh National Institute of Informatics, Japan Falk Scholer RMIT University, Australia Milad Shokouhi Microsoft Research Cambridge, UK Hideki Shima Carnegie Mellon University, USA Luo Si Purdue University, USA Mark Smucker University of Waterloo, Canada Ruihua Song Microsoft Research Asia, China Koichi Takeda Tokyo Research Laboratory, IBM, Japan Jamie Thom RMIT University, Australia Anastasios Tombros Queen Mary, University of London, UK Elaine Toms Dalhousie University, Canada Andrew Trotman University of Otago, New Zealand Anh Vo University of Melbourne, Australia Bin Wang Institute of Computing Technology, Chinese Richard Wang Carnegie Mellon University, USA William Webber University of Melbourne, Australia Iris Xie University of Wisconsin-Milwaukee, USA Guirong Xue Shanghai Jiao-Tong University, China Rong Yan IBM T.J. Watson Research Center, USA Qiang Yang Hong Kong University of Science and Hwan-Jo Yu Pohang University of Science and Technology, Jun Zhao Chinese Academy of Science, China Le Zhao Carnegie Mellon University, USA Fully Automatic Text Categorization by Exploiting WordNet .......... 1 A Latent Dirichlet Framework for Relevance Modeling ................ 13 Assigning Location Information to Display Individuals on a Map for Web People Search Results ........................................ 26 Web Spam Identification with User Browsing Graph .................. 38 Metric and Relevance Mismatch in Retrieval Evaluation .............. 50 Test Collection-Based IR Evaluatio n Needs Extension toward Sessions  X  A Case of Extremely Short Queries ............................... 63 Weighted Rank Correlation in Information Retrieval Evaluation ........ 75 Extractive Summarization Based on Event Term Temporal Relation Graph and Critical Chain ......................................... 87 Using an Information Quality Framework to Evaluate the Quality of Product Reviews ................................................. 100 Automatic Extraction for Product Feature Words from Comments on the Web ........................................................ 112 Image Sense Classification in Text-Based Image Retrieval ............. 124 A Subword Normalized Cut Approach to Automatic Story Segmentation of Chinese Broadcast News ........................................ 136 Japanese Spontaneous Spoken Document Retrieval Using NMF-Based Topic Mo dels .................................................... 149 Finding  X  X ucy in Disguise X : The Misheard Lyric Matching Problem ..... 157 Selecting Effective Terms for Query Formulation ..................... 168 Discovering Volatile Events in Your Neighborhood: Local-Area Topic Extraction from Blog Entries ...................................... 181 A Unified Graph-Based Iterative Reinforcement Approach to Personalized Search .............................................. 193 Exploiting Sentence-Level Features for Near-Duplicate Document Detection ....................................................... 205 Language Models of Collaborative Filtering ......................... 218 Efficient Text Classification Using Term Projection ................... 230 IPHITS: An Incremental Latent Topic Model for Link Structure ....... 242 Supervised Dual-PLSA for Personalized SMS Filtering ................ 254 Enabling Effective User Interactions in Content-Based Image Retrieval ........................................................ 265 Improving Text Rankers by Term Locality Contexts .................. 277 Mutual Screening Graph Algorithm: A New Bootstrapping Algorithm for Lexical Acquisition ............................................ 289 Web Image Retrieval for Abstract Queries Using Text and Image Information ..................................................... 300 Question Answering Based on Answer Trustworthiness ................ 310 Domain Specific Opinion Retrieval ................................. 318 A Boosting Approach for Learning to Rank Using SVD with Partially Labeled Data .................................................... 330 Opinion Target Network and Bootstrapping Method for Chinese Opinion Target Extraction ........................................ 339 Automatic Search Engine Performance Evaluation with the Wisdom of Crowds ......................................................... 351 A Clustering Framework Based on Adaptive Space Mapping and Rescaling ....................................................... 363 Research on Lesk-C-Based WSD and Its Application in English-Chinese Bi-directional CLIR .............................................. 375 Searching Polyphonic Indonesian Folksongs Based on N-gram Indexing Technique ....................................................... 387 Study on the Click Context of Web Search Users for Reliability Analysis ........................................................ 397 Utilizing Social Relationships for Blog Popularity Mining ............. 409 S-node: A Small-World Navigation System for Exploratory Search ...... 420 Efficient Probabilistic Latent Semantic Analysis through Parallelization ................................................... 432
 Commenced Publication in 1973 Founding and Former Series Editors: Gerhard Goos, Juris Hartmanis, and Jan van Leeuwen David Hutchison Takeo Kanade Josef Kittler Jon M. Kleinberg Alfred Kobsa Friedemann Mattern John C. Mitchell Moni Naor Oscar Nierstrasz C. Pandu Rangan Bernhard Steffen Madhu Sudan Demetri Terzopoulos Doug Tygar Gerhard Weikum Volume Editors Pu-Jen Cheng National Taiwan University Department of Computer Science and Information Engineering Taipei 10617, Taiwan R.O.C.
 E-mail: pjcheng@iis.sinica.edu.tw Min-Yen Kan National University of Singapore Department of Computer Science, School of Computing Singapore 117417 E-mail: kanmy@comp.nus.edu.sg Wai Lam The Chinese University of Hong Kong Department of Systems Engineering and Engineering Management N.T., Hong Kong, China E-mail: wlam@se.cuhk.edu.hk Preslav Nakov National University of Singapore Department of Computer Science, School of Computing Singapore 117417 E-mail: nakov@comp.nus.edu.sg Library of Congress Control Number: 2010938622 CR Subject Classification (1998): H.3, H.4, F.2.2, I.4-5, E.1, H.2.8
LNCS Sublibrary: SL 3  X  Information Systems and Application, incl. Internet/Web and HCI ISSN 0302-9743 ISBN-10 3-642-17186-9 Springer Berlin Heidelberg New York ISBN-13 978-3-642-17186-4 Springer Berlin Heidelberg New York The Asia Information Retrieval Societies Conference (AIRS) 2010 was the sixth conference in the AIRS series, aiming to bring together international researchers and developers to exchange new ideas and the latest results in information re-trieval. The scope of the conference encompassed the theory and practice of all aspects of information retrieval in text, audio, image, video, and multimedia data.
 Retrieval with Asian Languages (IRAL) workshop series, started in 1996. It has become a mature venue for information retrieval work, finding support from the ACM Special Interest Group on Information Retrieval (SIGIR); the Association for Computational Linguistics and Chinese Language Processing (ACLCLP); ROCLING; and the Information Processing Society of Japan, Special Interest Group on Information Fundamentals and Access Technologies (IPSJ SIG-IFAT). year. A total of 120 papers were submitted, representing work by academics and practitioners not only from Asia, but also from Australia, Europe, North America, etc. The high quality of the work made it difficult for the dedicated program committee to decide which papers to feature at the conference. Through a double-blind reviewing process, 26 submissions (21%) were accepted as full oral papers and 31 (25%) were accepted as short posters.
 authors who submitted papers for review, the program committee members who constructively assessed the submissions, and the registered conference delegates. We thank them for their support of this conference, and for their long-term support of this Asian-centric venue for IR research and development. most of which is largely unseen by the authors and delegates. We would like to thank Preslav Nakov, our publication chair, who painstakingly worked with each individual paper X  X  authors to ensure formatting, spelling, diction, and grammar were completely error-free. We thank Lun-Wei Ku, our finance chair, who worked hard to reduce the conference registratio n rates. We also thank Chien-Wen Chen and Yen-Chieh Huang, our two AIRS 2010 webmasters, whose timely actions ensured that the website stay in sync with our program as it evolved. We would also like to thank Si-Chen Lee, President of the National Taiwan University, for accepting to be the honorary chair for the conference.
 December 2010 Pu-Jen Cheng The Sixth Asia Information Retrieval Societies Conference (AIRS 2010) was organized by the National Taiwan University in cooperation with ACM SIGIR, ACLCLP, ROCLING, and IPSJ SIG-IFAT.
 Hsin-Hsi Chen National Taiwan University, Taiwan Wai Lam The Chinese University of Hong Kong, China Gary Geunbae Lee Pohang University of Science and Technology, Alistair Moffat The University of Melbourne, Australia Hwee Tou Ng National University of Singapore, Singapore Tetsuya Sakai Microsoft Research Asia, China Dawei Song The Robert Gordon University, UK Masaharu Yoshioka Hokkaido University, Japan Mun Kew Leong National Library Board, Singapore Sung Hyon Myaeng Korea Advanced Institute of Science and Kam-Fai Wong The Chinese University of Hong Kong, China Honorary Conf. Chair Si-Chen Lee Conference Chair Pu-Jen Cheng Publication Chair Preslav Nakov PC Co-chairs Wai Lam Finance Chair Lun-Wei Ku Webmasters Chien-Wen Chen Timothy Baldwin The University of Melbourne, Australia Atsushi Fujii Tokyo Institute of Technology, Japan Winston Hsu National Taiwan University, Taiwan Joemon Jose University of Glasgow, UK Tie-Yan Liu Microsoft Research Asia, China Tetsuya Sakai Microsoft Research Asia, China Kazunari Sugiyama National University of Singapore, Bin Wang Chinese Academy of Sciences, China William Webber The University of Melbourne, Australia Min Zhang Tsinghua University, China Yi Zhang University of California, Santa Cruz, USA Akiko Aizawa National Institute of Informatics, Japan Tomoyoshi Akiba National Institute of Advanced Industrial Enrique Alfonseca Google Zurich, Switzerland Sachi Arafat University of Glasgow, UK Ching Man Au NTT Communication Science Labs, Japan Jing Bai Microsoft, USA Timothy Baldwin The University of Melbourne, Australia Gosse Bouma University of Groningen, Netherlands Deng Cai Zhejiang University, China Rui Cai Microsoft Research Asia, China Ben Carterette University of Delaware, USA Enhong Chen University of Science and Technology, Hsin-Hsi Chen National Taiwan University, Taiwan Seungjin Choi Pohang University of Science and Technology, Shane Culpepper RMIT University, Australia James Curran The University of Sydney, Australia Rebecca Dridan The University of Melbourne, Australia Georges Dupret Yahoo! Labs, USA Atsushi Fujii Tokyo Institute of Technology, Japan Yong Gao Yahoo! Beijing, China Bo Gong Oracle Corporation, USA Jesse Prabawa Gozali National University of Singapore, Singapore Jiafeng Guo Institute of Computing Technology, CAS, Cathal Gurrin Dublin City University, Ireland Ben Hachey Capital Markets CRC, Australia Kenji Hatano Doshisha University, Japan Daqing He University of Pittsburgh, USA Yin He University of Science and Technology, Xiaofei He Zhejiang University, China Tao Hong Zhejiang University, China Winston Hsu National Taiwan University, Taiwan Xuanjing Huang Fudan University, China Adam Jatowt Kyoto University, Japan Hideo Joho University of Tsukuba, Japan Joemon Jose University of Glasgow, UK Hanmin Jung Pohang University of Science and Noriko Kando National Institute of Informatics, Japan In-Su Kang Kyungsung University, Korea Evangelos Kanoulas University of Sheffield, UK Sarvnaz Karimi NICTA, The University of Melbourne, Jaana Kekalainen University of Tampere, Finland Jinyoung Kim University of Massachusetts, Amherst, USA Jungi Kim Pohang University of Science and Sang-Bum Kim SK Telecom, Korea Fuminori Kimura Ritsumeikan University, Japan Kazuaki Kishida Keio University, Japan Iraklis Klampanos University of Glasgow, UK Mounia Lalmas University of Glasgow, UK Wei Lai Microsoft Research Asia, China Andrew Lampert CSIRO, Australia Raymond Lau City University of Hong Kong, Gary Geunbae Lee Pohang University of Science and Jong-Hyeok Lee Pohang University of Science and Kian Chin Lee Multimedia University, Malaysia Hyowon Lee Dublin City University, Ireland Mun-Kew Leong National Library Board, Singapore Guoliang Li Tsinghua University, China Juanzi Li Tsinghua University, China Sujian Li Peking University, China Wenjie Li The Hong Kong Polytechnic University, Tie-Yan Liu Microsoft Research Asia, China Ting Liu Harbin Institute of Technology, China Yi Liu Google Inc., USA Yiqun Liu Tsinghua University, China Yue Liu Chinese Academy of Sciences, China Zhiyuan Liu Tsinghua University, China Si Luo Purdue University, USA Xueqiang Lv Beijing Information Science and Hao Ma The Chinese University of Hong Kong, Qiang Ma Kyoto University, Japan Akira Maeda Ritsumeikan University, Japan David Martinez NICTA, The University of Melbourne, Massimo Melucci University of Padua, Italy Alistair Moffat The University of Melbourne, Australia Tatsunori Mori Yokohama National University, Japan Mas Rina Mustaffa Universiti Putra Malaysia, Malaysia Seung-Hoon Na National University of Singapore, Preslav Nakov National University of Singapore, Hidetsugu Nanba Hiroshima City University, Japan Jianyun Nie University of Montreal, Canada Iadh Ounis University of Glasgow, UK Seong-Bae Park Kyungpook National University, Korea Will Radford The University of Sydney, Australia Reede Ren University of Glasgow, UK Ian Ruthven University of Strathclyde, UK Tetsuya Sakai Microsoft Research Asia, China Falk Scholer RMIT University, Australia Kazuhiro Seki Kobe University, Japan Yohei Seki University of Tsukuba, Japan Satoshi Sekine New York University, USA Hideki Shima Carnegie Mellon University, USA Toshiyuki Shimizu Kyoto University, Japan David Smith University of Massachusetts, Amherst, Ruihua Song Microsoft Research Asia, China Young-In Song Microsoft Research Asia, China Virach Sornlertlamvanich NECTEC, Thailand Mark Stevenson Sheffield University, UK Kazunari Sugiyama National University of Singapore, Bin Sun Peking University, China Le Sun Chinese Academy of Sciences, China Motofumi Suzuki The Open University of Japan, Japan Yu Suzuki Nagoya University, Japan Yee Fan Tan National University of Singapore, Chong Teng Wuhan University, China James Thom RMIT University, Australia Paul Thomas CSIRO, Australia Andew Trotman University of Otago, New Zealand Yuen-Hsien Tseng National Taiwan University, Taiwan Kiyotaka Uchimoto National Institute of Information and Takehito Utsuro University of Tsukuba, Japan Stephen Wan Macquarie University, Australia Bin Wang Chinese Academy of Sciences, China Jun Wang University College London, UK Mingwen Wang Jiangxi Normal University, China William Webber The University of Melbourne, Australia MingFang Wu RMIT University, Australia Yunqing Xia Tsinghua University, China Jun Xu Microsoft Research Asia, China Yinghui Xu Ricoh, Japan Ichiro Yamada National Institute of Information and Mikio Yamamoto University of Tsukuba, Japan Hongfei Yan Peking University, China Rong Yan Carnegie Mellon University, USA Patrick Ye Monash University, Australia Emine Yilmaz Microsoft Research Cambridge, UK Masaharu Yoshioka Hokkaido University, Japan Hwan Jo Yu Pohang University of Science and Jinhui Yuan Tsinghua University, China Min Zhang Tsinghua University, China Yi Zhang University of California, Santa Cruz, USA Jin Zhao National University of Singapore, Singapore Jun Zhao Chinese Academy of Sciences, China Le Zhao Carnegie Mellon University, USA Zhaohui Zheng Yahoo! Labs, USA Tingshao Zhu Chinese Academy of Sciences, China Mathieu Blondel Zhengguang Chen Zhanying He Jinha Kim Sungchul Kim Taeho on Kim Youngdae Kim Jaeyong Lee Fangtao Li Jiyi Li National Science Council, Republic of China (Taiwan) Ministry of Education, Republic of China (Taiwan) Relevance Ranking Using Kernels .................................. 1 Mining YouTube to Discover Extremist Videos, Users and Hidden Communities .................................................... 13 Title-Based Product Search  X  Exemplified in a Chinese E-commerce Portal .......................................................... 25 Relevance Model Revisited: With Multiple Document Representations .................................................. 37 Multi-viewpoint Based Similarity Measure and Optimality Criteria for Document Clustering ............................................. 49 A Text Classifier with Domain Adaptation for Sentiment Classification .................................................... 61 Effective Time Ratio: A Measure for Web Search Engines with Document Snippets .............................................. 73 Investigating Characteristics of Non-click Behavior Using Query Logs ... 85 Score Estimation, Incomplete Judgments, and Significance Testing in IR Evaluation ................................................... 97 Multi-Search : A Meta-search Engine Based on Multiple Ontologies ..... 110 Co-HITS-Ranking Based Query-Focused Multi-document Summarization .................................................. 121 Advanced Training Set Construction for Retrieval in Historic Documents ...................................................... 131 Ontology-Driven Semantic Digital Library ........................... 141 Revisiting Rocchio X  X  Relevance Feedback Algorithm for Probabilistic Models ......................................................... 151 When Two Is Better Than One: A Study of Ranking Paradigms and Their Integrations for Subtopic Retrieval ............................ 162 Connecting Qualitative and Quantitative Analysis of Web Search Process: Analysis Using Search Units ............................... 173 Transliteration Retrieval Model for Cross Lingual Information Retrieval ........................................................ 183 The Role of Lexical Ontology in Expanding the Semantic Textual Content of On-Line News Images .................................. 193 Order Preserved Cost-Sensitive Listwise Approach in Learning to Rank ........................................................... 203 Pseudo-relevance Feedback Based on mRMR Criteria ................. 211 An Integrated Deterministic and Nondeterministic Inference Algorithm for Sequential Labeling ........................................... 221 FolkDiffusion: A Graph-Based Tag Suggestion Method for Folksonomies .................................................... 231 Effectively Leveraging Entropy and Relevance for Summarization ...... 241 Machine Learning Approaches for Modeling Spammer Behavior ........ 251 Research of Sentiment Block Identification for Customer Reviews Based on Conditional Random Fields ............................... 261 Semantic Relation Extraction Based on Semi-supervised Learning ...... 270 Corpus-Based Arabic Stemming Using N-Grams ..................... 280 Analysis and Algorithms for Stemming Inversion ..................... 290 Top-Down and Bottom-Up: A Combined Approach to Slot Filling ...... 300 Relation Extraction between Related Concepts by Combining Wikipedia and Web Information for Japanese Language ............... 310 A Chinese Sentence Compression Method for Opinion Mining ......... 320 Relation Extraction in Vietnamese Text Using Conditional Random Fields .......................................................... 330 A Sparse L 2 -Regularized Support Vector Machines for Large-Scale Natural Language Learning ....................................... 340 An Empirical Comparative Study of Manual Rule-Based and Statistical Question Classifiers on Heterogeneous Unseen Data .................. 350 Constructing Blog Entry Classifiers Using Blog-Level Topic Labels ..... 360 Finding Hard Questions by Knowledge Gap Analysis in Question Answer Communities ............................................. 370 Exploring the Visual Annotatability of Query Concepts for Interactive Cross-Language Information Retrieval .............................. 379 A Diary Study-Based Evaluation Framework for Mobile Information Retrieval ........................................................ 389 Dynamics of Genre and Domain Intents ............................. 399 Query Recommendation Considering Search Performance of Related Queries ......................................................... 410 A Local Generative Model for Chinese Word Segmentation ............ 420 Re-ranking Summaries Based on Cr oss-Document Information Extraction ...................................................... 432 A Two-Stage Algorithm for Domain Adaptation with Application to Sentiment Transfer Problems ...................................... 443 Domain-Specific Term Rankings Using Topic Models ................. 454 Learning Chinese Polarity Lexicons by Integration of Graph Models and Morphological Features ....................................... 466 Learning to Rank with Supplementary Data ......................... 478 Event Recognition from News Webpages through Latent Ingredients Extraction ...................................................... 490 Tuning Machine-Learning Algorithms for Battery-Operated Portable Devices ......................................................... 502 A Unified Iterative Optimization Algorithm for Query Model and Ranking Refinement .............................................. 514 A Study of Document Weight Smoothness in Pseudo Relevance Feedback ....................................................... 527 Modeling Variable Dependencies between Characters in Chinese Information Retrieval ............................................. 539 Mining Parallel Documents across Web Sites ........................ 552 A Revised SimRank Approach for Query Expansion .................. 564 Improving Web-Based OOV Translation Mining for Query Translation ...................................................... 576 On a Combination of Probabilistic and Boolean IR Models for Question Answering ...................................................... 588 Emotion Tag Based Music Retrieval Algorithm ...................... 599 An Aesthetic-Based Approach to Re-ranking Web Images ............. 610
 Commenced Publication in 1973 Founding and Former Series Editors: Gerhard Goos, Juris Hartmanis, and Jan van Leeuwen David Hutchison Takeo Kanade Josef Kittler Jon M. Kleinberg Alfred Kobsa Friedemann Mattern John C. Mitchell Moni Naor Oscar Nierstrasz C. Pandu Rangan Bernhard Steffen Madhu Sudan Demetri Terzopoulos Doug Tygar Gerhard Weikum Volume Editors Mohamed Vall Mohamed Salem Dubai Knowledge Village, P.O. Box 20183, Dubai, United Arab Emirates E-mail: mohamedsalem@uowdubai.ac.ae Khaled Shaalan British University in Dubai, Faculty of Engineering and IT Dubai International Academic City, Block 11, 1st and 2nd Floor P.O. Box 345015, Dubai, United Arab Emirates E-mail: khaled.shaalan@buid.ac.ae Farhad Oroumchian Dubai Knowledge Village, P.O. Box 20183, Dubai, United Arab Emirates E-mail: farhadoroumchian@uowdubai.ac.ae Azadeh Shakery University of Tehran, Faculty of Engineering Department of Electrical and Computer Engineering North Kargar Street, P.O. Box 14395-515, Tehran, Iran E-mail: shakery@ut.ac.ir Halim Khelalfa Dubai Knowledge Village, P.O. Box 20183, Dubai, United Arab Emirates E-mail: halimkehlalfa@uowdubai.ac.ae ISSN 0302-9743 e-ISSN 1611-3349 ISBN 978-3-642-25630-1 e-ISBN 978-3-642-25631-8 DOI 10.1007/978-3-642-25631-8 Springer Heidelberg Dordrecht London New York Library of Congress Control Number: 2011941661 CR Subject Classification (1998): H.3, H.4, F.2.2, I.4-5, E.1, H.2.8 and HCI invited submissions to the following areas of research:  X  Arabic Script Text Processing and Retrieval  X  IR Models and Theories  X  Multimedia IR  X  User Study, IR Evaluation, and Interactive IR  X  Web IR, Scalability and Adversarial IR  X  IR Applications  X  Machine Learning for IR  X  Natural Language Processing for IR in natural language processing. A new track on Arabic Script Text Processing conference.
 grew from the Information Retrieval with Asian Languages (IRAL) workshop from the ACM Special Interest Group and Information Retrieval (SIGIR) and many other associations.
 The Program Committee used a double-blind reviewing process and as result short (poster) papers.
 tremely active Program Committee membe rs without whom the present proceed-of Ali Farghaly (Oracle, USA), Minjie Zhang (University of Wollongong, Aus-Research Asia), Min Zhang Tsinghua (University, China), Wang Bin (Chinese Academy of Sciences, China), Tie-Yan Liu (Microsoft Research Asia) and Chia-Hui Chang (National Central University, Taiwan).
 most of which is largely unseen by the authors and delegates. We would like to thank our Publication Chairs (Azadeh Shakery and Halim Khelalfa) who dictation and grammar wer e completely error-free.
 October 2011 Khaled Shaalan AIRS 2011 was organized by the Faculty of Computer Science and Engineering, University of Wollongong in Dubai in cooperation with ACM/SIGIR. Conference Co-chairs Mohamed Val Salem University of Wollongong in Dubai, UAE Farhad Oroumchian University of Wollongong in Dubai, UAE Program Co-chairs Khaled Shaalan British University in Dubai, UAE Farhad Oroumchian University of Wollongong in Dubai, UAE Publication Co-chairs Azadeh Shakery University of Tehran, Iran Halim Khelalfa University of Wollongong in Dubai Publicity Chair Asma Damankesh University of Wollongong in Dubai Abolfazl AleAhmad University of Tehran, Iran Program Co-chairs Khaled Shaalan British University in Dubai, UAE Farhad Oroumchian University of Wollongong in Dubai, UAE Area Chairs User Study, IR Evaluation, and Interactive IR Tetsuya Sakai, Microsoft Research Asia IR Models and Theories Minjie Zhang, University of Wollongong, Multimedia IR Joemon M Jose, University of Glasgow, UK IR Applications Kazunari Sugiyama, National University of Web IR, Scalability and Adversarial IR Min Zhang, Tsinghua University, China Machine Learning for IR Tie-Yan Liu, Microsoft Research Asia Natural Language Processing for IR Chia-Hui Chang, National Central University, Arabic Script Text Processing and Retrieval Ali Farghaly, Oracle, USA I. Sengor Altingovde I. Abu El-Khair M. Adriani A. Aizawa Z. Al Aghbari H. Aliane H. Amiri M. Asadpour E. Ashoori C. Man Au Yeung S. Bandyopadhyay L.H. Belguith P. Bhattacharyya W. Bin M. Blondel R. Cai T.W. Cai F. Can S. Carberry V. Cavalli-Sforz Chia-Hui Chang ChienChinChen Hsin-Hsi Chen Sung-Kwon Choi G. Dupret Yiqun Liu Zhiyuan Liu Wen-Hsiang Lu Yuanhua Lv Qiang Ma A. Maeda M. Melucci M.R. Mustaffa C. Mahlow M. Maragoudakis K. Megerdoomian D. Metzler R. Mitkov M.F. Moens A. Moffat T. Mori Seung-Hoon Na P. Nakov H. Nanba Kenta Oku M. Okumura F. Oroumchian I. Ounis Dubai Knowledge Village Micorsoft Middle East Query-Dependent Rank Aggregation with Local Models ............... 1 On Modeling Rank-Independent Risk in Estimating Probability of Relevance ....................................................... 13 Measuring the Ability of Score Distributions to Model Relevance ....... 25 Cross-Language Information Retrieval with Latent Topic Models Trained on a Comparable Corpus .................................. 37 Construct Weak Ranking Functions for Learning Linear Ranking Function ........................................................ 49 Is Simhash Achilles? .............................................. 61 XML Information Retrieval through Tree Edit Distance and Structural Summaries ...................................................... 73 An Empirical Study of SLDA for Information Retrieval ............... 84 Learning to Rank by Optimizing Expected Reciprocal Rank ........... 93 Information Retrieval Strategies for Digitized Handwritten Medieval Documents ...................................................... 103 Query Phrase Expansion Using Wikipedia in Patent Class Search ...... 115 Increasing Broadband Subscriptions for Telecom Carriers through Mobile Advertising ............................................... 127 Query Recommendation by Modelling the Query-Flow Graph .......... 137 Ranking Content-Based Social Images Search Results with Social Tags ........................................................... 147 Profiling a Non-medical Professional Searcher on a Medical Domain: What Do Search Patterns and Demographic Details Reveal? ........... 157 Prioritized Aggregation of Multiple Context Dimensions in Mobile IR ....................................................... 169 Searching for Islamic and Qur X  X nic Information on the Web: A Mixed-Methods Approach ....................................... 181 Enriching Query Flow Graphs with Click Information ................ 193 Effect of Explicit Roles on Collaborative Search in Travel Planning Task ........................................................... 205 A Web 2.0 Approach for Organizing Search Results Using Wikipedia ... 215 Recommend at Opportune Moments ................................ 226 Emotion Tokens: Bridging the Gap among Multilingual Twitter Sentiment Analysis ............................................... 238 Identifying Popular Search Goals behind Search Queries to Improve Web Search Ranking ............................................. 250 A Novel Crawling Algorithm for Web Pages ......................... 263 Extraction of Web Texts Using Content-Density Distribution .......... 273 A New Approach to Search Resul t Clustering and Labeling ............ 283 Efficient Top-k Document Retrieval Using a Term-Document Binary Matrix ......................................................... 293 Topic Analysis for Online Reviews wi th an Author-Experience-Object-Topic Mo del ..................................................... 303 Predicting Query Performance Di rectly from Score Distributions ....... 315 Wikipedia-Based Smoothing for Enhancing Text Clustering ........... 327 ASVMFC: Adaptive Support Vector Machine Based Fuzzy Classifier ... 340 Ensemble Pruning for Text Categorization Based on Data Partitioning ..................................................... 352 Sentiment Analysis for Online Reviews Using an Author-Review-Object Model .......................................................... 362 Semantic-Based Opinion Retriev al Using Predicate-Argument Structures and Subjective Adjectives ............................... 372 An Aspect-Driven Random Walk Model for Topic-Focused Multi-document Summarization .................................... 386 An Effective Approach for Topic-Specific Opinion Summarization ...... 398 A Model-Based EM Method for Topic Person Name Multi-polarization ................................................ 410 Using Key Sentence to Improv e Sentiment Classification .............. 422 Using Concept-Level Random Wal k Model and Global Inference Algorithm for Answer Summarization .............................. 434 Acquisition of Know-How Information from Web ..................... 446 Topic Based Creation of a Persian-English Comparable Corpus ........ 458 A Web Knowledge Based Approach for Complex Question Answering ... 470 Learning to Extract Coherent Keyphrases from Online News .......... 479 Maintaining Passage Retrieval Information Need Using Analogical Reasoning in a Question Answering Task ............................ 489 Improving Document Summarization by Incorporating Social Contextual Information ........................................... 499 Automatic Classification of Link Polarity in Blog Entries .............. 509 Feasibility Study for Procedural Knowledge Extraction in Biomedical Documents ...................................................... 519 Small-Word Pronunciation Modeling for Arabic Speech Recognition: A Data-Driven Approach ......................................... 529 The SALAH Project: Segmentation and Linguistic Analysis of h . ad  X  it Arabic Texts .................................................... 538 Exploring Clustering for Multi-document Arabic Summarisation ....... 550 ZamAn and Raqm: Extracting Temporal and Numerical Expressions in Arabic .......................................................... 562 Extracting Parallel Paragraphs and Sentences from English-Persian Translated Documents ............................................ 574 Effect of ISRI Stemming on Similarity Measure for Arabic Document Clustering ...................................................... 584 A Semi-supervised Approach for Key-Synset Extraction to Be Used in Word Sense Disambiguation ....................................... 594 Mapping FarsNet to Suggested Upper Merged Ontology .............. 604 Topic Detection and Multi-word Terms Extraction for Arabic Unvowelized Documents .......................................... 614
 Commenced Publication in 1973 Founding and Former Series Editors: Gerhard Goos, Juris Hartmanis, and Jan van Leeuwen David Hutchison Takeo Kanade Josef Kittler Jon M. Kleinberg Friedemann Mattern John C. Mitchell Moni Naor Oscar Nierstrasz C. Pandu Rangan Bernhard Steffen Madhu Sudan Demetri Terzopoulos Doug Tygar Moshe Y. Vardi Gerhard Weikum Volume Editors Guozhu Dong Wright State University Department of Computer Science and Engineering, USA E-mail: gdong@cse.wright.edu Xuemin Lin University of New South Wales &amp; NICTA, Australia E-mail: lxue@cse.unsw.edu.au Wei Wang University of New South Wales School of Computer Science and Engineering, Australia E-mail: weiw@cse.unsw.edu.au Yun Yang Swinburne University of Technology, Melbourne, Australia E-mail: yyang@ict.swin.edu.au Jeffrey Xu Yu The Chinese University of Hong Kong Department of Systems Engineering and Engineering Management, China E-mail: yu@se.cuhk.edu.hk Library of Congress Control Number: 2007927715 CR Subject Classification (1998): H.2-5, C.2, I.2, K.4, J.1
LNCS Sublibrary: S L 3  X  Information Systems and Application, incl. Internet/Web and HCI ISSN 0302-9743 ISBN-10 3-540-72483-4 Springer Berlin Heidelberg New York ISBN-13 978-3-540-72483-4 Springer Berlin Heidelberg New York The rapid prevalence of Web applications requires new technologies for the de-sign, implementation and management of Web-based information systems, and for the management and analysis of information on the Web. The joint AP-Web/WAIM 2007 conference, combining the traditions of APWeb and WAIM conferences, was an internatio nal forum for researchers, p ractitioners, developers and users to share and exchange cutting-edge ideas, results, experience, tech-niques and tools in connection with all aspects of Web data management. The conference drew together original research and industrial papers on the theory, design and implementation of Web-based information systems and on the man-agement and analysis of information on the Web. The conference was held in the beautiful mountain area of Huang Shan (Yellow Mountains)  X  the only dual World Heritage listed area in China for its astonishing natural beauty and rich and well-preserved culture. These pr oceedings collected the technical papers selected for presentation at the conference, held at Huang Shan, June 16 X 18, 2007.
 full-paper submissions from North America, South America, Europe, Asia, and Oceania. Each submitted paper underwent a rigorous review by three inde-pendent referees, with detailed review rep orts. Finally, 47 full research papers and 36 short research papers were accep ted, from Austria, Australia, Canada, China, Cyprus, Greece, Hong Kong, Japan, Korea, Singapore, Taiwan, and USA, representing a competitive acceptanc e rate of 15%. The contributed papers address a broad spectrum on Web-based information systems, including data mining and knowledge discovery, information retrieval, P2P systems, sensor networks and grids, spatial and temporal databases, Web mining, XML and semi-structured data, query processing and optimization, data integration, e-learning, privacy and security, and st reaming data. The p roceedings also in-clude abstracts of keynote speeches from f our well-known researchers and four invited papers.
 standing researchers in the APWeb/WAIM research areas. We would like to extend our sincere gratitude to the Program Committee members and external reviewers.
 port of this conference, making it a big success. Special thanks go to An-hui University, The Chinese University of Hong Kong, The University of New South Wales, Anhui Association of Science and Technology, Anhui Computer Federation, Hohai University, Huangshan University, National Science Founda-tion of China, and Oracle.
 June 2007 Guozhu Dong APWeb/WAIM 2007 was jointly organized by Anhui University, University of Science and Technology of China, The Chinese University of Hong Kong, and The University of New South Wales.
 Guo Liang Chen, University of Science and Technology of China, China Ra-mamohanarao Kotagiri, University of Melbourne, Australia Guozhu Dong, Wright State University, USA Xuemin Lin, University of New South Wales, Australia Yun Yang, Swinburne University of Technology, Australia Jeffrey Xu Yu, Chinese University of Hong Kong, China Kevin Chen-Chuan Chang, University of Illinois at Urbana-Champaign, USA Flip Korn, AT&amp;T, USA Jian Pei, Simon Fraser University, Canada Mukesh K. Mohania, IBM India Research Laboratory, India Toshiyuki Amagasa, University of Tsukuba, Japan Bin Luo, Anhui University, China Wei Wang, University of New South Wales, Australia Chengfei Liu, Swinburne University of Technology, Australia Guoren Wang, Northeast ern University, China Haixun Wang, IBM T.J. Watson Research Center, USA Jiaxing Cheng, Anhui University, China Nian Wang, Anhui University, China Jianwen Su, UC Santa Barbara, USA Dongqing Yang, Peking University, China Xiaofang Zhou, University of Queensland, Australia X. Sean Wang, University of Vermont, USA Yanchun Zhang, Victoria University, Australia Mikio Aoyama Nanzan University, Japan Vijay Atluri Rutgers University, USA James Bailey University of Melbourne, Australia Sourav S. Bhowmick Nanyang Technological University, Singapore Haiyun Bian Wright State University, USA Athman Bouguettaya Virginia Tech, USA Stephane Bressan National University of Singapore, Singapore Chee Yong Chan National University of Singapore, Singapore Kevin Chen-Chuan Chang University of Illinois at Urbana-Champaign, USA Akmal B. Chaudhri IBM DeveloperWorks, USA Sanjay Chawla University of Sydney, Australia Lei Chen Hong Kong University of Science and Technology, Yixin Chen Washington University at St. Louis, USA Reynold Cheng Hong Kong Polytechnic University, China Byron Choi Nanyang Technological University, Singapore Gao Cong Microsoft Research Asia, China Bin Cui Peking University, China Alfredo Cuzzocrea Univer sity of Calabria, Italy Stijn Dekeyser University of S outhern Queensland, Australia Amol Deshpande University of Maryland, USA Gill Dobbie University of Auckland, New Zealand Xiaoyong Du Renmin University of China, China Gabriel Fung Chinese University of Hong Kong, China Hong Gao Harbin University of Technology, China Guido Governatori Universi ty of Queensland, Australia Stephane Grumbach The Sino-French IT Lab, China Giovanna Guerrini Universita di Genova, Italy Michael Houle National Institute for Informatics, Japan Joshua Huang Hong Kong University, China Ela Hunt ETH Zurich, Switzerland Yoshiharu Ishikawa Nagoya University, Japan Panagiotis Kalnis National University of Singapore, Singapore Raghav Kaushik Microsoft Research, USA Hiroyuki Kitagawa University of Tsukuba, Japan Yasushi Kiyoki Keio University, Japan Flip Korn AT&amp;T, USA Manolis Koubarakis T echnical University of Crete, Greece Chiang Lee National Cheng-Kung University, Taiwan Yoon-Joon Lee Korea Advanced Institute of Science and Chen Li University of California (Irvine), USA Jianzhong Li Harbin University of Technology, China Jinyan Li Institute for Information Research, Singapore Qing Li City University of Hong Kong, China Ee Peng Lim Nanyang Technological University, Singapore Chengfei Liu Swinburne University of Technology, Australia Tieyan Liu Microsoft Research Asia, China Qiong Luo Hong Kong University of Science and Technology, Hongyan Liu Tsinghua University, China Qing Liu University of Queensland, Australia Frank Maurer University of Calgary, Canada Emilia Mendes Auckland University, New Zealand Weiyi Meng Binghamton University, USA Xiaofeng Meng Renmin University of China, China Mukesh K. Mohania IBM India Research Laboratory, India Miyuki Nakano University of Tokyo, Japan Wolfgang Nejdl University of Hannover, Germany Jan Newmarch Monash University, Australia Zaiqing Nie Microsoft Research Asia, China John Noll Santa Clara University, USA Chaoyi Pang CSIRO, Australia Zhiyong Peng Wuhan University, China Evaggelia Pitoura University of Ioannina, Greece Sunil Prabhakar Purdue University, USA Weining Qian Fudan University, China Tore Risch Uppsala University of Sweden, Sweden Uwe Roehm Sydney University, Australia Prasan Roy IBM India Research Laboratory, India Keun Ho Ryu Chungbuk National University, Korea Monica Scannapieco University of Rome  X  X a Sapienza, X  Italy Klaus-Dieter Schewe Massey University, New Zealand Albrecht Schmidt Aalborg University, Denmark Markus Schneider University of Florida, USA Heng Tao Shen University of Queensland, Australia Jialie Shen University of Glasgow, UK Timothy K. Shih Tamkang University, Taiwan Kian-Lee Tan National University of Singapore, Singapore David Taniar Monash University, Australia Changjie Tang Sichuan University, China Yufei Tao Chinese University of Hong Kong, China Minh Hong Tran Swinburne University of Technology, Australia Anthony Tung National University of Singapore, Singapore Andrew Turpin RMIT University, Australia Guoren Wang Northeastern University, China Haixun Wang IBM T. J. Watson Research Center, USA Jianyong Wang Tsinghua University, China Min Wang IBM T. J. Watson Research Center, USA Qing Wang Institute of Software CAS, China Shaojun Wang Wright State University, USA Wei Wang Fudan University, China Wei Wang University of New South Wales, Australia X. Sean Wang University of Vermont, USA Gerald Weber Auckland University, New Zealand Sui Wei Anhui University, China Jirong Wen Microsoft Research Asia, China Raymond Wong University of New South Wales, Australia Jun Yan Wollongong University, Australia Dongqing Yang Peking University, China Jian Yang Macquarie University, Australia Jun Yang Duke University, USA Cui Yu Monmouth University, USA Ge Yu Northeastern University, China Jenny Xiuzhen Zhang RMIT University, Australia Jianliang Xu Hong Kong Baptist University, China Qing Zhang CSIRO, Australia Rui Zhang University of Melbourne, Australia Yanchun Zhang Victoria University, Australia Aoying Zhou Fudan University, China Xiaofang Zhou University of Queensland, Australia Xuemin Lin University of New South Wales, Australia Hongjun Lu Hong Kong University of Science and Jeffrey Xu Yu Chinese University of Hong Kong, China Yanchun Zhang Victoria University, Australia Xiaofang Zhou (Chair) University of Queensland, Australia Guozhu Dong Wright State University, USA Masaru Kitsuregawa University of Tokyo, Japan Jianzhong Li Harbin Institute of Technology, China Qing Li City University of Hong Kong, China Xiaofeng Meng Renmin University, China Changjie Tang Sichuan Universty, China Shan Wang Renmin University, China X. Sean Wang (Chair) University of Vermont, USA Ge Yu Northeastern University, China Aoying Zhou Fudan University, China Carola Aiello Halil Ali Toshiyuki Amagasa Saeid Asadi Eric Bae Manish Bhide Niranjan Bidargaddi Leigh Brookshaw Penny De Byl Ryan Choi Kin Wah Chow Soon Ae Chun Yu-Chi Chung Valentina Cordl Ken Deng Bolin Ding Zhicheng Dou Jing Du Flavio Ferrarotti Sergio Flesca Francesco Folino Gianluigi Folino Mohamed Medhat Gaber Bin Gao Jun Gao Xiubo Geng Gabriel Ghinita Kazuo Goda Nizar Grira Qi Guo Himanshu Gupta Rajeev Gupta Yanan Hao Sven Hartmann Hao He Jun He Reza T. Hemayati Bo Hu Yuan-Ke Huang Ingrid Jakobsen Nobuhiro Kaji Odej Kao Hima Karanam Roland Kaschek Hiroko Kinutani Markus Kirchberg Henning Koehler Yanyan Lan Michael Lawley Massimiliano De Leoni Jianxin Li Xue Li Xuhui Li Xiang Lian Sourashis Roy Ruggero Russo Attila Sali Falk Scholer Basit Shafiq Derong Shen Heechang Shin Takahiko Shintani Houtan Shirani-Mehr Yanfeng Shu Adam Silberstein Guojie Song Shaoxu Song Alexandre De Spindler Bela Stantic I-Fang Su Sai Sun Ying Sun Takayuki Tamura Nan Tang Bernhard Thalheim Wee Hyong Tok Rodney Topor Alexei Tretiakov Paolo Trunfio Yung-Chiao Tseng Bin Wang Botao Wang John Wang Rong Wang Xiaoyu Wang Xin Wang Yitong Wang Youtian Wang Anhui University The Chinese University of Hong Kong The University of New South Wales Anhui Association of S cience and Technology Anhui Computer Federation Hohai University Huangshan University National Science Foundation of China Oracle Data Mining Using Fractals and Power Laws ........................ 1 Exploring the Power of Links in Data Mining ........................ 2 Community Systems: The World Online ............................ 3 A New DBMS Architecture for DB-IR Integration .................... 4 Study on Efficiency and Effectiveness of KSORD ..................... 6 Discovering Web Services Based on Probabilistic Latent Factor Model .......................................................... 18 SCORE: Symbiotic Context Oriented Information Retrieval ........... 30 Process Aware Information Syste ms: A Human Centered Perspective ... 39 IMCS: Incremental Mining of Closed Sequential Patterns ............. 50 Mining Time-Shifting Co-regulation Patterns from Gene Expression Data ........................................................... 62 Tight Correlated Item Sets and Their Efficient Discovery .............. 74 Improved Prediction of Protein Seco ndary Structures Using Adaptively Weighted Profiles ................................................ 83 Framework for Building a High-Quality Web Page Collection Considering Page Group Structure ................................. 95 Multi-document Summarization Using Weighted Similarity Between Topic and Clustering-Based Non-negative Semantic Feature ........... 108 A Fair Load Balancing Algorithm for Hypercube-Based DHT Networks ....................................................... 116 LINP: Supporting Similarity Search in Unstructured Peer-to-Peer Networks ....................................................... 127 Generation and Matching of Ontology Data for the Semantic Web in a Peer-to-Peer Framework .......................................... 136 Energy-Efficient Skyline Queries over Sensor Network Using Mapped Skyline Filters ................................................... 144 An Adaptive Dynamic Cluster-Based Protocol for Target Tracking in Wireless Sensor Networks ......................................... 157 Distributed, Hierarchical Clustering and Summarization in Sensor Networks ....................................................... 168 A New Similarity Measure for Near Duplicate Video Clip Detection .... 176 Efficient Algorithms for Historical Continuous k NN Query Processing over Moving Object Trajectories ................................... 188 Effective Density Queries for Mo ving Objects in Road Networks ....... 200 An Efficient Spatial Search Method Based on SG-Tree ................ 212 Getting Qualified Answers for Aggregate Queries in Spatio-temporal Databases ....................................................... 220 Dynamic Adaptation Strategies for Long-Term and Short-Term User Profile to Personalize Search ....................................... 228 Using Structured Tokens to Identify Webpages for Data Extraction ..... 241 Honto? Search: Estimating Trustworthiness of Web Information by Search Results Aggregation and Temporal Analysis ................... 253 A Probabilistic Reasoning Approach for Discovering Web Crawler Sessions ........................................................ 265 An Exhaustive and Edge-Removal Algorithm to Find Cores in Implicit Communities .................................................... 273 Active Rules Termination Analysis Through Conditional Formula Containing Updatable Variable .................................... 281 Computing Repairs for Inconsistent XML Document Using Chase ...... 293 An XML Publish/Subscribe Algorithm Implemented by Relational Operators ....................................................... 305 Retrieving Arbitrary XML Fragmen ts from Structured Peer-to-Peer Networks ....................................................... 317 Combining Smooth Graphs with Semi-supervised Learning ............ 329 Extracting Trend of Time Series B ased on Improved Empirical Mode Decomposition Method ........................................... 341 Spectral Edit Distance Method for Image Clustering .................. 350 Mining Invisible Tasks from Event Logs ............................. 358 The Selection of Tunable D BMS Resources Using the Incremental/Decrem ental Relationship .............................. 366 Hyperclique Pattern Based Off-Topic Detection ...................... 374 An Energy Efficient Connected Coverage Protocol in Wireless Sensor Networks ....................................................... 382 A Clustered Routing Pro tocol with Distributed Intrusion Detection for Wireless Sensor Networks ......................................... 395 Continuous Approximate Window Queries in Wireless Sensor Networks ....................................................... 407 A Survey of Job Scheduling in Grids ............................... 419 Relational Nested Optional Join for Efficient Semantic Web Query Processing ...................................................... 428 Efficient Processing of Relational Queries with Sum Constraints ....... 440 A Theoretical Framework of Natural Computing  X  M Good Lattice Points (GLP) Method ............................................ 452 Building Data Synopses Within a Known Maximum Error Bound ...... 463 Exploiting the Structure of Update Fragments for Efficient XML Index Maintenance .................................................... 471 Improvements of HITS Algorithms for Spam Links ................... 479 Efficient Keyword Search over Data-Centric XML Documents ......... 491 Promotional Ranking of Search Engine Results: Giving New Web Pages a Chance to Prove Their Values .................................... 503 Adaptive Scheduling Strategy for Data Stream Management System .... 511 A QoS-Guaranteeing Scheduling Algorithm for Continuous Queries over Streams .................................................... 522 A Simple But Effective Event-Driven Model for Data Stream Queries ... 534 Efficient Difference NN Queries for Moving Objects .................. 542 APCAS: An Approximate Approach to Adaptively Segment Time Series Stream .................................................... 554 Continuous k-Nearest Neighbor Search Under Mobile Environment ..... 566 Record Extraction Based on User F eedback and Document Selection ... 574 Density Analysis of Winnowing on Non-uniform Distributions ......... 586 Error-Based Collaborative Filtering Algorithm for Top-N Recommendation ................................................ 594 A PLSA-Based Approach for Building User Profile and Implementing Personalized Recommendation ..................................... 606 CoXML: A Cooperative XML Query Answering System .............. 614 Concept-Based Query Transformation Based on Semantic Centrality in Semantic Peer-to-Peer Environment ................................ 622 Mining Infrequently-Accessed File Correlations in Distributed File System ......................................................... 630 Learning-Based Trust Model fo r Optimization of Selecting Web Services ......................................................... 642 SeCED-FS: A New Approach for the Classification and Discovery of Significant Regions in Medical Images .............................. 650 Context-Aware Search Inside e-Learning Materials Using Textbook Ontologies ...................................................... 658 Activate Interaction Relations hips Between Students Acceptance Behavior and E-Learning ......................................... 670 Semantic-Based Grouping of Search Engine Results Using WordNet .... 678 Static Verification of Access Control Model for AXML Documents ..... 687 SAM: An Efficient Algorithm for F&amp;B-Index Construction ............ 697 BUXMiner: An Efficient Bottom-Up Approach to Mining XML Query Patterns ........................................................ 709 A Web Service Architecture for Bidirectional XML Updating .......... 721 (  X  , k )-anonymity Based Privacy Preservation by Lossy Join ........... 733 Achieving k -Anonymity Via a Density-Based Clustering Method ....... 745 k -Anonymization Without Q-S Associations ......................... 753 Protecting and Recovering Database Systems Continuously ............ 765 Towards Web Services Composition Based on the Mining and Reasoning of Their Causal Relationships ..................................... 777 A Dynamically Adjustable Rule Engine for Agile Business Computing Environments ................................................... 785 A Formal Design of Web Co mmunity Interactivity ................... 797 Towards a Type-2 Fuzzy Description Logic for Semantic Search Engine ......................................................... 805 A Type-Based Analysis for Verifying Web Application ................ 813 Homomorphism Resolving of XPath Trees Based on Automata ........ 821 An Efficient Overlay Multicast Routing Algorithm for Real-Time Multimedia Applications .......................................... 829 Novel NonGaussianity Measure Based BSS Algorithm for Dependent Signals ......................................................... 837 HiBO: Mining Web X  X  Favorites ..................................... 845 Frequent Variable Sets Based Clustering for Artificial Neural Networks Particle Classification ............................................ 857 Attributes Reduction Based on GA-CFS Method ..................... 868 Towards High Performance and High Availability Clusters of Archived Stream ......................................................... 876 Continuously Matching Episode Rules for Predicting Future Events over Event Streams .............................................. 884 Author Index .................................................. 893 Christos Faloutsos is a Prof essor at Carnegie Mellon University. He has received the Presidential Young Investigator Award by the National Science Foundation (1989), the Research Contributions Award in ICDM 2006, nine  X  X est paper X  awards, and several teaching awards. He has served as a member of the executive committee of SIGKDD; he has published over 160 refereed articles, 11 book chapters and one monograph. He holds five patents and he has given over 20 tutorials and 10 invited distinguished lectures. His research interests include data mining for streams and networks, fractals, indexing for multimedia and bio-informatics data, and database performance.
 Jiawei Han is a Professor in the Department of Computer Science at the Uni-versity of Illinois at Urbana-Champaign. He has been working on research into data mining, data warehousing, database systems, mining spatiotemporal data, multimedia data, stream and RFID data, Web data, social network data, and biological data, and software bug mining, with over 300 conference and journal publications. He has chaired or served on over 100 program committees of inter-national conferences and workshops, including PC co-chair of 2005 (IEEE) In-ternational Conference on Data Mining (ICDM), Americas Coordinator of 2006 International Conference on Very Large Data Bases (VLDB). He is also serving as the founding Editor-In-Chief of ACM Transactions on Knowledge Discovery from Data. He is an ACM Fellow and has received 2004 ACM SIGKDD Innova-tions Award and 2005 IEEE Computer Society Technical Achievement Award. His book  X  X ata Mining: Concepts and Techniques X  (2nd ed., Morgan Kaufmann, 2006) has been popularly used as a textbook worldwide.
 Raghu Ramakrishnan is VP and Research Fellow at Yahoo! Research, where he heads the Community Systems group. He is on leave from the University of Wisconsin-Madison, where he is Professor of Computer Sciences, and was founder and CTO of QUIQ, a company that pioneered question-answering com-munities such as Yahoo! Answers, and provided collaborative customer support for several companies, including Compaq and Sun. His research is in the area of database systems, with a focus on data retrieval, analysis, and mining. He has developed scalable algorithms for clustering, decision-tree construction, and itemset counting, and was among the first to investigate mining of continuously evolving, stream data. His work on query optimization and deductive databases has found its way into several commercial database systems, and his work on ex-tending SQL to deal with queries over sequences has influenced the design of win-dow functions in SQL:1999. His paper on th e Birch clustering algorithm received the SIGMOD 10-Year Test-of-Time award, and he has written the widely-used text  X  X atabase Management Systems X  (WCB/McGraw-Hill, with J. Gehrke), now in its third edition.

He is Chair of ACM SIGMOD, on the Board of Directors of ACM SIGKDD and the Board of Trustees of the VLDB Endowment, and has served as editor-in-chief of the Journal of Data Mining and Knowledge Discovery, associate editor of ACM Transactions on Database Systems, and the Database area editor of the Journal of Logic Programming. Dr. Ramakrishnan is a Fellow of the Association for Computing Machinery (ACM), and has received several awards, including a Distinguished Alumnus Award from IIT Madras, a Packard Foundation Fellow-ship, an NSF Presidential Young Investigator Award, and an ACM SIGMOD Contributions Award.
 Kyu-Young Whang is Professor of Computer Science and Director of Advanced Information Technology Research Center (AITrc) at KAIST. Previously, he was with IBM T.J.Watson Research Center from 1983 to 1990. Since joining KAIST in 1990, he has been leading the Odysseus DBMS project featuring tight-coupling of DBMS with information retrieval (IR) and spatial functions. Dr. Whang is one of the pioneers of probabilistic counting, which nowadays is being widely used in approximate query answering, sampling, and data streaming. One of the algo-rithms he co-developed at IBM Almaden (then San Jose) Research Lab in 1981 has been made part of DB2. Dr. Whang is the author of the first main-memory relational query optimization model developed in 1985 and reported in 1990 in ACM TODS in the context of Office-by-Example (OBE). This model influenced subsequent optimization models of commercial main-memory DBMSs. His re-search has covered a wide range of database issues including physical database design, query optimization, DBMS engin e technologies, and more recently, IR, spatial databases, data mining, and XML. Dr. Whang is a Co-Editor-in-Chief of the VLDB Journal, having served the journal for 17 years from its inception as its founding editorial board member. He is a Trustee Emeritus of the VLDB Endowment and served the internation al academic community as the General Chair of VLDB2006, DASFAA2004, and PAKDD2003, as a PC Co-Chair of VLDB2000, CoopIS1998, and ICDE2006, and as an editorial board member of journals such as IEEE TKDE and IEEE Data Engineering Bulletin. He served as an IEEE Distinguished Visitor from 1989 to 1990. He earned his Ph.D. from Stanford University in 1984. Dr. Whang is an IEEE Fellow, a member of the ACM and IFIP WG 2.6.

 Commenced Publication in 1973 Founding and Former Series Editors: Gerhard Goos, Juris Hartmanis, and Jan van Leeuwen David Hutchison Takeo Kanade Josef Kittler Jon M. Kleinberg Alfred Kobsa Friedemann Mattern John C. Mitchell Moni Naor Oscar Nierstrasz C. Pandu Rangan Bernhard Steffen Madhu Sudan Demetri Terzopoulos Doug Tygar Gerhard Weikum Volume Editors Qing Li City University of Hong Kong, Department of Computer Science Kowloon, Hong Kong, China E-mail: itqli@cityu.edu.hk Ling Feng Tsinghua University, Department of Computer Science &amp; Technology Beijing 100084, China E-mail: fengling@tsinghua.edu.cn Jian Pei Simon Fraser University, School of Computing Science Burnaby, BC V5A 1S6, Canada E-mail: jpei@cs.sfu.ca Sean X. Wang University of Vermont, Department of Computer Science Burlington, VT 05405, USA E-mail: sean.wang@uvm.edu Xiaofang Zhou The University of Queensland, School of ITEE Brisbane, QLD 4072, Australia E-mail: zxf@itee.uq.edu.au Qiao-Ming Zhu Soochow University, School of Computer Science &amp; Technology Suzhou, Jiangsu 215006, China E-mail: qmzhu@suda.edu.cn Library of Congress Control Number: 2009922514 CR Subject Classification (1998): H.2-5, C.2, J.1, K.4
LNCS Sublibrary: SL 3  X  Information Systems and Application, incl. Internet/Web and HCI ISSN 0302-9743 ISBN-10 3-642-00671-X Springer Berlin Heidelberg New York ISBN-13 978-3-642-00671-5 Springer Berlin Heidelberg New York APWeb and WAIM are two leading interna tional conferences on the research, development, and applications of Web technologies, database systems, informa-tion management and software engineering, with a focus on the Asia-Pacific re-gion. The previous APWeb conferences were held in Beijing (1998), Hong Kong (1999), Xi X  X n (2000), Changsha (2001), Xi X  X n (2003), Hangzhou (2004), Shang-hai (2005), Harbin (2006), Huangshan (2007), and Shenyang (2008); and the previous WAIM conferences were held in Shanghai (2000), Xi X  X n (2001), Beijing (2002), Chengdu (2003), Dalian (2004), Hangzhou (2005), Hong Kong (2006), Huangshan (2007), and Zhangjiajie (2008). For the second time, APWeb and WAIM were combined to foster closer colla boration and research idea sharing, and were held immediately after IEEE ICDE 2009 in Shanghai.
 who chose APWeb+WAIM as a venue for their publications. Out of 189 submit-ted papers from 21 countries and regions, including Australia, Austria, Bengal, Brazil, Canada, Fra nce, Greece, Hong Kong, India, Ir an, Japan, Kor ea, Macau, Mainland China, Myanmar, The Netherlands, Norway, Taiwan, Thailand, UK, and USA, we selected 42 full papers and 26 short papers for publication. The acceptance rate for regular full papers i s 22%. The contributed papers addressed a wide scope of issues in the fields of Web-age information management and advanced applications, including Web data mining, knowledge discovery from streaming data, query processing, multidimensional data analysis, data manage-ment support to advanced applications, etc.
 the Program Committee members and the reviewers for their invaluable efforts. Special thanks to the local Organizing Committee headed by Jiwen Yang and Liusheng Huang. Many thanks also go to the Workshop Co-chairs (Lei Chen and Chengfei Liu), Tutorial Co-chairs (Liu Wenyin and Vivek Gopalkrishnan), Publicity Co-chairs (Lei Yu and Baihua Zheng), and Finance Co-chairs (Howard Leung and Genrong Wang). Last but not least, we are grateful for the hard work and effort of our webmaster (Zhu Fei, Tony), and to the generous sponsors who enabled the smooth running of the conference.
 April 2009 Qing Li APWeb+WAIM 2009 was organized by Soochow University, China.
 Conference Co-chairs Sean X. Wang University of Vermont, USA Xiaofang Zhou University of Queensland, Australia Qiaoming Zhu Soochow University, China Program Committee Co-chairs Qing Li City University of Hong Kong, China Ling Feng Tsinghua University, China Jian Pei Simon Fraser University, Canada Local Organization Co-chairs Jiwen Yang Soochow University, China Liusheng Huang University of Science and Technology, China Workshop Co-chairs Lei Chen Hong Kong University of Science and Chengfei Liu Swinburne University of Technology, Australia Tutorial/Panel Co-chairs Wenyin Liu City University of Hong Kong, China Vivek Gopalkrishnan Nanyang Technological University, Singapore Industrial Chair Hui Xue Soochow University, China Publicity Co-chairs Lei Yu State University of New York at Binghamton, Baihua Zheng Singapore Management University, Singapore Finance Co-chairs Howard Leung City University of Hong Kong, China Genrong Wang Soochow University, China CCF DB Society Liaison Xiaofeng Meng Renmin University of China, China WISE Society Liaison Yanchun Zhang Victoria University, Australia Webmaster Fei Zhu Soochow University, China Aijun An York University, Canada James Bailey University of Melbourne, Australia Ladjel Bellatreche ENSMA -Poitiers University, France Rafae Bhatti IBM Almaden Research Center, USA Sourav Bhowmick Nanyang Technological University, Singapore Haiyun Bian Wright State University, USA Klemens Boehm Universi ty of Karlsruhe, Germany Athman Bouguettaya Virginia Polytechnic Institute and State Stephane Bressan National University of Singapore, Singapore Ji-Won Byun ORACLE, USA Jinli Cao La Trobe University, Australia Badrish Chandramouli Duke University, USA Akmal Chaudhri IBM DeveloperWorks, USA Enhong Chen University of Science and Technology of China, Jian Chen South China University of Technology, China Lei Chen Hong Kong University of Science and Qiming Chen Hewlett-Packard Laboratories, USA Yi Chen Arizona State University, USA Hong Cheng University of Illinois at Urbana-Champaign, Reynold Cheng Hong Kong Polytechnic University, China David Cheung The University of Hong Kong, China Dickson Chiu Dickson Computer Systems, Hong Kong, China Byron Choi Hong Kong Baptist University, China Gao Cong Microsoft Research Asia, China Bin Cui Peking University, China Alfredo Cuzzocrea Univer sity of Calabria, Italy Guozhu Dong Wright State University, USA Wenliang Du Syracuse University, USA Xiaoyong Du Renmin University of China, China Jianhua Feng Tsinghua University, China Yaokai Feng Kyushu University, Japan Eduardo Fernandez Florida Atlantic University, USA Ada Fu The Chinese University of Hong Kong, China Benjamin Fung Concordia University, Canada Gabriel Fung The University of Queensland, Australia Byron Gao University of Wisconsin,USA Hong Gao Harbin Institute of Technology, China Jun Gao Peking University, China Yunjun Gao Singapore Management University, Singapore Zhiguo Gong University of Macau, China Anandha Gopalan Imperial College, UK Vivekanand Gopalkrishnan Nanyang Technological University, Singapore Guido Governatori The University of Queensland, Australia Madhusudhan Govindaraju State University of New York at Binghamton, Stephane Grumbach INRIA, France Ning Gu Fudan University, China Giovanna Guerrini Universit` a di Genova, Italy Jingfeng Guo Yanshan University, China Mohand-Said Hacid Universit  X  e Claude Bernard Lyon 1, France Weihong Han National Laboratory for Parallel and Michael Houle National Institute of Informatics, Japan Ming Hua Simon Fraser University, Canada Xiangji Huang York University, Canada Yan Huang University of North Texas, USA Ela Hunt University of Strathclyde, UK Renato Iannella National ICT Australia (NICTA), Australia Yoshiharu Ishikawa Nagoya University, Japan Yan Jia National University of Defence Technology, Daxin Jiang Microsoft Research Asia, China Ruoming Jin Kent State University, USA Panagiotis Kalnis National University of Singapore, Murat Kantarcioglu UT Dallas, USA Wee Keong Nanyang Technological University, Singapore Markus Kirchberg Institute for Infocomm Research, A*STAR, Hiroyuki Kitagawa University of Tsukuba, Japan Flip Korn AT&amp;T Labs Research, USA Manolis Koubarakis National and Kapodistrian University of Anne LAURENT University of Montpellier 2, France Chiang Lee National Cheng-Kung University Dik Lee Hong Kong University of Science and Wang-Chien Lee Pennsylvania State University,USA Yoon-Joon Lee Korea Advanced Institute of Science and Carson Leung University of Manitoba, Canada Chen Li University of California, Irvine, USA Jinyan Li Nanyang Technological University, Singapore Jiuyong Li University of South Australia, Australia Quanzhong Li IBM, USA Tao Li Florida International University, USA Xue Li The University of Queensland, Australia Zhanhuai Li Northeastern Polytechnical University, China Wenxin Liang Japan Science and Technology Agency (JST) Ee Peng Lim Nanyang Technological University, Singapore Chengfei Liu Swinburne University of Technology, Australia Hongyan Liu Tsinghua University, China Mengchi Liu Carleton University, Canada Qing Liu CSIRO, Australia Tieyan Liu Microsoft Research Asia, China Wei Liu University of Western Australia, Australia Weiyi Liu Yunnan University, China Jiaheng Lu University of California, Irvine, USA Liping Ma Ballarat University, Australia Xiaosong Ma NC State University, USA Lorenzo Martino Purdue University, USA Weiyi Meng Binghamton University, USA Xiaofeng Meng Renmin University of China, China Miyuki Nakano University of Tokyo, Japan Wilfred Ng Hong Kong University of Science and Anne Ngu Texas State University, USA Junfeng Pan Google, USA Chaoyi Pang CSIRO, Australia Zhiyong Peng Wuhan University, China Evaggelia Pitoura University of Ioannina, Greece Marc Plantevit University of Montpellier 2, France Tieyun Qian Wuhan University, China Weining Qian East China Normal University, China Wenyu Qu Tokyo University, Japan Vijay Raghavan University of Louisiana at Lafayette, USA Cartic Ramakrishnan Wright State University, USA Keun Ryu Chungbuk National University, Korea Monica Scannapieco ISTAT, Italy Markus Schneider University of Florida, USA Derong Shen Northeastern University, China Dou Shen Microsoft, USA Heng Tao Shen University of Queensland, Australia Jialie Shen Singapore Management University, Singapore Timothy Shih Tamkang University, Taiwan Youngin Shin Microsoft, USA Adam Silberstein Yahoo, USA Peter Stanchev Kettering University, USA Xiaoping Sun Chinese Academy of Science, China Changjie Tang Sichuan University, China Jie Tang Tsinghua University, China Nan Tang CWI, The Netherlands Zhaohui Tang Microsoft, USA David Taniar Monash University, Australia Yufei Tao Chinese University of Hong Kong, China Maguelonne Teisseire University of Montpellier 2, France Yunhai Tong Peking University, China Yicheng Tu University of South Florida, USA Daling Wang Northeastern University, China Guoren Wang Northeastern University, China Haixun Wang IBM T. J. Watson Research Center, USA Hua Wang University of Southern Queensland, Australia Jianyong Wang Tsinghua University, China Junhu Wang Griffith University, Australia Min Wang IBM T. J. Watson Research Center, USA Shan Wang Renmin University of China, China Tengjiao Wang Peking University, China Wei Wang Fudan University, China Wei Wang University of New South Wales, Australia X. Sean Wang University of Vermont, USA Jirong Wen Microsoft Research Asia, China Raymond Wong University of New South Wales, Australia Raymond Chi-Wing Wong HKUST, China Fei Wu Zhejiang University, China Weili Wu The University of Texas at Dallas, USA Wensheng Wu IBM, USA Xintao Wu University of North Carolina at Charlotte, USA Yuqing Wu Indiana University, USA Zonghuan Wu University of Louisiana at Lafayette, USA Jitian Xiao Edith Cowan University, Australia Xiaokui Xiao Chinese University of Hong Kong, China Hui Xiong Rutgers University, USA Wei Xiong National University of Defense Technology, Guandong Xu Victoria University, Australia Jianliang Xu Hong Kong Baptist University, Hong Kong, Jun Yan University of Wollongong, Australia Xifeng Yan IBM, China Chunsheng Yang NRC, Canada Jian Yang Macquarie University, Australia Shuqiang Yang National Laboratory for Parallel and Xiaochun Yang Northeastern University, China Jieping Ye Arizona State University, USA Laurent Yeh Prism, France Ke Yi Hong Kong University of Technology, China Jian Yin Sun Yat-Sen University, China Cui Yu Monmouth University, USA Ge Yu Northeastern University, China Jeffrey Yu Chinese University of Hong Kong, China Lei Yu State University of New York at Binghamton, Philip Yu University of Illinois at Chicago, USA Xiaohui Yu York University, Canada Lihua Yue Science and Technology University of China, Chengqi Zhang University of Technology, Sydney, Australia Donghui Zhang Northeastern University, USA Qing Zhang CSIRO, Australia Rui Zhang University of Melbourne, Australia Shichao Zhang University of Technology, Sydney, Australia Xiuzhen Zhang RMIT University, Australia Yanchun Zhang Victoria University, Australia Ying Zhang University of New South Wales, Australia Hongkun Zhao Bloomberg, USA Yanchang Zhao University of Technology, Sydney, Australia Sheng Zhong State University of New York at Buffalo, USA Aoying Zhou East China Normal University, China Lizhu Zhou Tsinghua University, China Shuigeng Zhou Fudan University, China Xuan Zhou CSIRO ICT Centre, Australia Yongluan Zhou EPFL, Sweden Qiang Zhu University of Michigan, USA Xingquan Zhu Florida Atlantic University, USA Watve Alok Mafruzzaman Ashrafi Markus Bestehorn Thorben Burghardt Mustafa Canim Cai Chen Rui Chen Shu Chen Xiaoxi Du Frank Eichinger Shi Feng Li zhen Fu Qingsong Guo Yi Guo Christopher Healey Raymond Heatherly Ali Inan Fangjiao Jiang Fangjiao Jiang Hideyuki Kawashima Dashiell Kolbe Lily Li Huajing Li Distributed XML Processing ...................................... 1 Towards Multi-modal Extraction and Summarization of Conversations ................................................... 2 Simple but Effective Porn Query Recognition by k-NN with Semantic Similarity Measure ............................................... 3 Selective-NRA Algorithms for Top-k Queries ........................ 15 Continuous K -Nearest Neighbor Query o ver Moving Objects in Road Networks ....................................................... 27 Data Quality Aware Queries in Collaborative Information Systems ..... 39 Probabilistic Threshold Range Aggregate Query Processing over Uncertain Data .................................................. 51 Dynamic Data Migration Policies for Query-Intensive Distributed Data Environments ................................................... 63 C-Tree Indexing for Holistic Twig Joins ............................. 76 Processing XML Keyword Search by Constructing Effective Structured Queries ......................................................... 88 Representing Multiple Mappings between XML and Relational Schemas for Bi-directional Query Translation ................................ 100 Finding Irredundant Contained Rewritings of Tree Pattern Queries Using Views ..................................................... 113 IRank: A Term-Based Innovation Ra nking System for Conferences and Scholars ........................................................ 126 A Generalized Topic Modeling Approach for Maven Search ............ 138 Topic and Viewpoint Extraction for Diversity and Bias Analysis of News Contents .................................................. 150 Topic-Level Random Walk through Probabilistic Model ............... 162 Query-Focused Summarization by Combining Topic Model and Affinity Propagation ..................................................... 174 Rank Aggregation to Combine QoS in Web Search ................... 186 Personalized Delivery of On X  X ine Se arch Advertisement Based on User Interests ........................................................ 198 Automatic Web Image Annotation via Web-Scale Image Semantic Space Learning .................................................. 211 Representing and Inferring Causalities among Classes of Multidimensional Data ........................................... 223 Efficient Incremental Computation of CUBE in Multiple Versions What-If Analysis ................................................. 235 Incremental Computation for MEDIAN Cubes in What-If Analysis ..... 248 The Tradeoff of Delta Table Merging and Re-writing Algorithms in What-If Analysis Application ...................................... 260 Efficiently Clustering Probabilistic Data Streams ..................... 273 Detecting Abnormal Trend Evolution over Multiple Data Streams ...... 285 Mining Interventions from Parallel Event Sequences .................. 297 Effective Similarity Analysis over Event Streams Based on Sharing Extent .......................................................... 308 A Term-Based Driven Clustering Approach for Name Disambiguation .................................................. 320 Sentiment Clustering: A Novel Method to Explore in the Blogosphere ..................................................... 332 Kernel-Based Transductive Learning with Nearest Neighbors .......... 345 An Improved Algorithm for Mining Non-Redundant Interacting Feature Subsets ......................................................... 357 AutoPCS: A Phrase-Based Text Categorization System for Similar Texts ........................................................... 369 A Probabilistic Approach for Mining Drifting User Interest ............ 381 Temporal Company Relation Mining from the Web ................... 392 StreetTiVo: Using a P2P XML Database System to Manage Multimedia Data in Your Living Room ........................................ 404 Qualitative Spatial Representation and Reasoning for Data Integration of Ocean Observational Systems ................................... 416 Managing Hierarchical Information on Small Screens ................. 429 An Approach to Detect Collaborative Conflicts for Ontology Development .................................................... 442 Using Link-Based Content Analysis to Measure Document Similarity Effectively ...................................................... 455 Parallel Load Balancing Strategies f or Tree-Structured Peer-to-Peer Networks ....................................................... 468 A Verification Mechanism for Secure d Message Processing in Business Collaboration .................................................... 480 Semantic Service Discovery by Consistency-Based Matchmaking ....... 492 A Fast Heuristic Algorithm for the Composite Web Service Selection ... 506 Intervention Events Detection and Prediction in Data Streams ......... 519 New Balanced Data Allocating and Online Migrating Algorithms in Database Cluster ................................................ 526 An Evaluation of Real-Time Transaction Services in Web Services E-Business Systems .............................................. 532 Formal Definition and Detection Algorithm for Passive Event in RFID Middleware ..................................................... 538 Distributed Benchmarking of Relational Database Systems ............ 544 Optimizing Many-to-Many Data A ggregation in Wireless Sensor Networks ....................................................... 550 Bag of Timestamps: A Simple and Effi cient Bayesian Chronological Mining ......................................................... 556 Efficient Hybrid Password-Ba sed Authenticated Group Key Exchange ....................................................... 562 Optimal K-Nearest-Neighbor Query in Data Grid .................... 568 A Routing Scheme with Localized Movement in Event-Driven Wireless Sensor Networks ................................................. 576 Short Text Clustering for Search Results ............................ 584 Load Shedding for Shared Window Join over Real-Time Data Streams ........................................................ 590 A Sliding-Window Approach for Finding Top-k Frequent Itemsets from Uncertain Streams ............................................... 597 A Novel ID-Based Anonymous Signcryption Scheme ................. 604 Pseudo Period Detection on T ime Series Stream with Scale Smoothing ...................................................... 611 An Affinity Path Based Wireless Broadcast Scheduling Approach for Multi-item Queries ............................................... 617 Lightweight Data Mining Based Database System Self-optimization: A Case Study ...................................................... 623 Efficient Detection of Disco rds for Time Series Stream ................ 629 SLICE: A Novel Method to Find Lo cal Linear Correlations by Constructing Hyperplanes ......................................... 635 Complete and Equivalent Query Rewriting Using Views ............... 641 Grubber: Allowing End-Users to Develop XML-Based Wrappers for Web Data Sources ............................................... 647 Performance Analysis of Group Handover Scheme for IEEE 802.16j-Enabled Vehicular Networks ................................ 653 Directly Identify Unexpected In stances in the Test Set by Entropy Maximization ................................................... 659 Decentralized Orches tration of BPEL Processes with Execution Consistency ..................................................... 665 Let Wiki Be More Powerful and Suitable for Enterprises with External Program ........................................................ 671 DepRank: A Probabilistic Measure of Dependence via Heterogeneous Links ........................................................... 677 Author Index .................................................. 683
 Commenced Publication in 1973 Founding and Former Series Editors: Gerhard Goos, Juris Hartmanis, and Jan van Leeuwen David Hutchison Takeo Kanade Josef Kittler Jon M. Kleinberg Alfred Kobsa Friedemann Mattern John C. Mitchell Moni Naor Oscar Nierstrasz C. Pandu Rangan Bernhard Steffen Madhu Sudan Demetri Terzopoulos Doug Tygar Gerhard Weikum Volume Editors Xiaoyong Du Renmin University of China, School of Information, Beijing 100872, China E-mail: duyong@ruc.edu.cn Wenfei Fan
University of Edinburgh, LFCS, School of Informatics 10 Crichton Street, Edinburgh, EH8 9AB, UK E-mail: wenfei@inf.ed.ac.uk Jianmin Wang Tsinghua University, School of Software, Beijing 100084, China E-mail: jimwang@tsinghua.edu.cn Zhiyong Peng Wuhan University, Computer School Luojiashan Road, Wuhan, Hubei 430072, China E-mail: peng@whu.edu.cn Mohamed A. Sharaf The University of Queensland School of Information Technology and Electrical Engineering St. Lucia, QLD 4072, Australia E-mail: m.sharaf@uq.edu.au ISSN 0302-9743 e-ISSN 1611-3349 ISBN 978-3-642-20290-2 e-ISBN 978-3-642-20291-9 DOI 10.1007/978-3-642-20291-9 Springer Heidelberg Dordrecht London New York Library of Congress Control Number: 2011924095 CR Subject Classification (1998): H.2-5, C.2, J.1, K.4
LNCS Sublibrary: S L 3  X  Information Systems and Application, incl. Internet/Web and HCI This volume contains the proceedings of the 13th Asia-Pacific Conference on Web Technology (APWeb), held April 18 X 20, 2011, in Beijing, China. APWeb is a leading international conference on research, development and applications of Web technologies, database systems, information management and software engineering, with a focus on the Asia-Pacific region.
 received 104 submissions overall, from 13 countries and regions. Among these submissions the Program Committee selected 26 regular papers and 10 short papers. The acceptance rate for regular p apers is 25%, while the overall accep-tance rate is 34.6% when short papers are included. The submissions ranged over a variety of topics, from traditional topics such as Web search, databases, data mining, to emerging emphasis on Web mining, Web data management and service-oriented computing. The submissions were divided into nine tracks based on their contents. Each submission was reviewed by at least three members of the Program Committee or ext ernal reviewers. Needless to say, to accept only 36 papers we were forced to reject many high-quality submissions. We would like to thank all the authors for submitting their best work to APWeb 2011, accepted or not.
 pers, which were selected by a separ ate committee chaired by Gao Hong and Egemen Tanin. It also includes papers acco mpanying three keynote talks, given by Philip S. Yu, Deyi Li and Christian S. Jensen.
 ternational Workshop on XML Data Management (XMLDM 2011) organized by Jiaheng Lu from Renmin University of China, and the Second International Workshop on Unstructured Data Management (USD 2011) organized by Tenjiao Wang from Peking University, China. These two workshops were treated as spe-cial sessions of the APWeb conference. The details on the workshop organization can be found in the messages of the workshop organizers.
 Tsinghua University, China. It was also supported financially by the National Natural Science Foundation of China, and the Database Society of China Com-puter Federation.
 people. The Program Committee that select ed the research papers consisted of 118 members. Each of them reviewed a pile of papers, went through extensive discussions, and helped us put together a diverse and exciting research pro-gram. In particular, we would like to extend our special thanks to the Track Chairs: Diego Calvanese, Floris Geerts , Zackary Ives, Anastasios Kementsiet-sidis, Xuemin Lin, Jianyong Wang, Cong Yu, Jeffrey Yu, Xiaofang Zhou, for leading and monitoring the discussions. We would also like to thank the APWeb conference Steering Co mmittee, the APWeb Organiz ing Committee, as well as the local organizers and volunteers in Bei jing, for their effort and time to help make the conference a success. We thank Q ian Yi for maintaining the conference website, and the Microsoft Conference Management Tool (CMT) team for their support and help. Finally, we are extremely indebted to Yueguo Chen for over-seeing the entire reviewing process, and to Tieyun Qian for handling the editing affairs of the proceedings.
 March 2011 Xiaoyong Du We dedicate this book to our respected Prof. Shixuan Sa, who passed away on July 11, 2010 after a long battle against Parkinson disease.
 wrote the first textbook on databases in China, with his former student Prof. Shan Wang. It is this book that introduced many Chinese scholars to database research.
 Sa Shixuan Best Student Paper Award has been established by the National Database Conference of China (NDBC) i n memoriam of his contributions to the database society in China. In fact, the best student paper award was initiated and promoted by Prof. Sa for NDBC in 1984.
 was born in Fuzhou city, Fujian Province on December 27, 1922. He studied mathematics at Xiamen University and graduated in 1945. Prior to his move to Renmin University in 1950, he was a lecturer of mathematics at Sun Yat-Sen University and at Huabei University. He was a founder and the first Chairman of the Department of Economic Information Management at Renmin University, which is the first established academic program on information systems in China. Chinese database community, and a brilliant and wonderful man, a man with great vision, an open mind and a warm heart. Shixuan Sa inspired and encour-aged generations of database researchers and practitioners, and was a pioneer in establishing international collaborations. His contributions to the research com-munity, especially in database development and education, are unprecedented. Above all, he was a man admired for his humor, his modesty, and his devo-tion to his students, family, and friends. In the database world Prof. Sa will be remembered as the man who helped to produce the database community in China.
 Jiaguang Sun Tsinghua University, China Jianzhong Li Harbin Institute of Technology, China Philip Yu University of Illinois at Chicago, USA Ramamohanarao (Rao) Kokagiri The University of Melbourne, Australia Xiaoyong Du Renmin University of China Wenfei Fan The University of Edinburgh, UK Jianmin Wang Tsinghua University, China Xiao Zhang Renmin University of China Chaokun Wang Tsinghua University, China Zhanhuai Li Northwest Polytechnical University, China Wookshin Han Kyungpook National University, Korea Ling Feng Tsinghua University, China Anthony K.H. Tung National University of Singapore Ge Yu Northeast University of China Ee-Peng Lim Singapore Management University, Singapore Hong Gao Harbin Institute of Technology, China Egemen Tanin University of Melbourne, Australia Zhiyong Peng Wuhan University, China Mohamed Sharaf University of Queensland, Australia Shuigeng Zhou Fudan University, China Takeo Kunishima Okayama Prefectural University, Japan Tengjiao Wang Peking University, China Aoying Zhou East China Normal University, China Katsumi Tanaka Kyoto University, Japan Xiaoyong Du Renmin University of China Wenfei Fan The University of Edinburgh, UK Jianmin Wang Tsinghua University, China Diego Calvanese Free University of Bozen-Bolzano Floris Geerts University of Edinburgh Zackary Ives University of Pennsylvania Anastasios Kementsietsidis IBM T.J. Watson Research Center at Hawthorne, Xuemin Lin University of New South Wales Jianyong Wang Tsinghua University Cong Yu Google Research Jeffrey Yu Chinese University of Hong Kong Xiaofang Zhou University of Queensland Toshiyuki Amagasa University of Tsukuba, Japan Denilson Barbosa University of Alberta, Canada Pablo Barcelo Universidad de Chile, Chile Geert Jan Bex Hasselt University, Belgium Athman Bouguettaya CSIRO, Australia Zhipeng Cai Mississippi State University, USA Chee Yong Chan National University of Singapore Jae-Woo Chang Chonbuk National University, Korea Wenguang Chen Peking University, China Haiming Chen Chinese Academy of Sciences, China Hong Chen Renmin University of China Hanxiong Chen University of Tsukuba, Japan Jinchuan Chen Renmin University of China Jin Chen Michigan State University, USA Lei Chen Hong Kong University of Science and Technology Reynold Cheng The University of Hong Kong David Cheung The University of Hong Kong Richard Connor University of Strathclyde, UK Bin Cui Beijing University, China Alfredo Cuzzocrea Univer sity of Calabria, Italy Ke Deng University of Queensland, Australia Ting Deng Beihang University, China Koji Eguchi Kobe University, Japan Ling Feng Tsinghua University, China Yaokai Feng Kyushu University, Japan Jianhua Feng Tsinghua University, China Hong Gao Harbin Institute of Technology, China Xueqing Gong East China Normal University Guido Governatori NICTA, Australia Dirk Van Gucht Indiana University, USA Sven Helmer University of London, UK Zi Huang University of Queensland, Australia Ela Hunt ETH Zurich, Switzerland Seung-won Hwang POSTECH, Korea Yoshiharu Ishikawa Nagoya University, Japan Ik-Soon Kim ETRI, Korea Jin-Ho Kim Kangwon National University, Korea Yasushi Kiyoki Keio University, Japan Kyong-Ho Lee Yonsei University, Korea SangKeun Lee Korea University Qing Li Hong Kong City University Juanzi Li Tsinghua University, China Zhanhuai Li Northwest Polytechnical University, China Yingshu Li Georgia State University, USA Chengfei Liu Swinburne University of Technology, Australia Huan Liu Arizona State, USA Ling Liu Georgia Tech, USA Mengchi Liu Carleton University, Canada Jiaheng Lu Renmin University of China Hua Lu Aalborg University, Denmark Qiang Ma Kyoto University, Japan Shuai Ma University of Edinburgh, UK Ioana Manolescu INRIA Saclay, France Weiyi Meng State University of New York at Binghamton, Yunmook Nah Dankook University, Korea Shinsuke Nakajima Kyoto Sangyo University, Japan Miyuki Nakano University of Tokyo, Japan Joachim Niehren INRIA Lille Nord Europe, France Werner Nutt Free Universi ty of Bozen-Bolzano, Italy Satoshi Oyama Hokkaido University, Japan Chaoyi Pang CSIRO, Australia Mary-Angela Papalaskari Villanova University, USA Sanghyun Park Yonsei University, Korea Seong-Bae Park Kyungpook National University, Korea Zhiyong Peng Wuhan University, China Alex Poulovassilis University of London, UK KeunHo Ryu Chungbuk National University, Korea Marc Scholl Universitaet Konstanz, Germany Mohamed Sharaf University of Queensland, Australia Shuming Shi MSRA, China Jianwen Su U.C. Santa Barbara, USA Kazutoshi Sumiya University of Hyogo, Japan Katsumi Tanaka Kyoto University, Japan David Taniar Monash University, Australia Alex Thomo University of Victoria, Canada Anthony Tung National University of Singapore Stijn Vansummeren Free University of Brussels, Belgium Chaokun Wang Tsinghua University, China Daling Wang Northeastern University, China Tengjiao Wang Peking University, China Hua Wang University of Southern Queensland, Australia Hongzhi Wang Harbin Institute of Technology, China Wei Wang University of New South Wales, Australia Wei Wang Fudan University, China Xiaoling Wang East China Normal University Ji-Rong Wen MSRA, China Jef Wijsen University of Mons-Hainaut, Belgium Peter Wood University of London, UK Yuqing Wu Louisianna State University, USA Yingyuan Xiao Tianjing University of Technology, China Jianliang Xu Hong Kong Baptist University Linhao Xu IBM Research China Xiaochun Yang Northeastern University, China Jian Yin Sun Yat-Sen University, China Haruo Yokota Tokyo Institute of Technology, Japan Jae Soo Yoo Chungbuk National University, Korea Hwanjo Yu POSTECH, Korea Lei Yu State University of New York at Binghamton, Philip Yu University of Illinois at Chicago, USA Ge Yu Northeast University, Northeastern Ming Zhang Beijing University, China Rui Zhang The University of Melbourne, Australia Yanchun Zhang Victoria University, Australia Ying Zhang University of New South Wales, Australia Wenjie Zhang University of New South Wales, Australia Xiao Zhang Renmin University of China Zhenjie Zhang ADSC Singapore Shuigeng Zhou Fudan University, China Xiangmin Zhou CSIRO, Australia Xuan Zhou CSIRO, Australia Md. Hijbul Alam Nikolaus Augsten Sukhyun Ahn Bhuvan Bamba Jiefeng Cheng Hyunsouk Cho Gabriel Pui Cheong Fung Christian Grn Guangyan Huang Reza Hemayati Hai Huang Yu Jiang Neila Ben Lakhal Sau Dan Lee Chunbin Lin Brahim Medjahed Kenta Oku Information Networks Mining and Analysis .......................... 1 iMiner: From Passive Searching to Active Pushing ................... 3 On the Querying for Places on the Mobile Web ...................... 4 Music Review Classification Enhanced by Semantic Information ....... 5 Role Discovery for Graph Clustering ............................... 17 Aggregate Distance Based Clustering Using Fibonacci Series-FIBCLUS ................................................. 29 Exploiting Explicit Semantics-Based Grouping for Author Interest Finding ......................................................... 41 Top-K Probabilistic Closest Pairs Query in Uncertain Spatial Databases ....................................................... 53 CkNN Query Processing over Moving Objects with Uncertain Speeds in Road Networks ................................................ 65 Discrete Trajectory Prediction on Mobile Data ...................... 77 Cleaning Uncertain Streams for Query Improvement .................. 89 Personalized Web Search with User Geographic and Temporal Preferences ...................................................... 95 Web User Profiling on Proxy Logs and Its Evaluation in Personalization .................................................. 107 Exploring Folksonomy and Cooking Procedures to Boost Cooking Recipe Recommendation .......................................... 119 Effective Hybrid Recommendation Combining Users-Searches Correlations Using Tensors ........................................ 131 Maintaining Internal Consistency of Report for Real-Time OLAP with Layer-Based View ................................................ 143 Efficient Approximate Top-k Query Algorithm Using Cube Index ...... 155 DVD: A Model for Event Diversified Versions Discovery .............. 168 Categorical Data Skyline Using Classification Tree ................... 181 Quality Evaluation of Wikipedia Articles through Edit History and Editor Groups ................................................... 188 Towards Improving Wikipedia as an Image-Rich Encyclopaedia through Analyzing Appropriateness of Images for an Article .................. 200 SDDB: A Self-Dependent and Data-Based Method for Constructing Bilingual Dictionary from the Web ................................. 213 Measuring Similarity of Chinese Web Databases Based on Category Hierarchy ....................................................... 225 Effective Term Weighting in ALT Text Prediction for Web Image Retrieval ........................................................ 237 Layout Object Model for Extracting the Schema of Web Query Interfaces ....................................................... 245 Tagging Image with Informative and Correlative Tags ................ 258 Let Other Users Help You Find Answers: A Collaborative Question-Answering Method with Continuous Markov Chain Model .... 264 Hybrid Index Structures for Temporal-Textual Web Search ............ 271 Complex Event Processing over Unreliable RFID Data Streams ........ 278 A Graph Model Based Simulation Tool for Generating RFID Streaming Data ........................................................... 290 Assisting the Design of XML Schema: Diagnosing Nondeterministic Content Models .................................................. 301 Improving Matching Process in Social Network Using Implicit and Explicit User Information ......................................... 313 Dynamic Network Motifs: Evolutionary Patterns of Substructures in Complex Networks ............................................... 321 Social Network Analysis on KAD and Its Application ................. 327 Modeling and Querying Probabilistic RDFS Data Sets with Correlated Triples .......................................................... 333 A Tableau Algorithm for Paraconsistent and Nonmonotonic Reasoning in Description Logic-Based System ................................. 345 Key Concepts Identification and We ighting in Search Engine Queries ... 357 SecGuard: Secure and P ractical Integrity Protection Model for Operating Systems ............................................... 370 A Traceable Certificateless Threshold Proxy Signature Scheme from Bilinear Pairings ................................................. 376 A Mashup Tool for Cross-Domain Web Applications Using HTML5 Technologies .................................................... 382 Change Tracer: A Prot  X  eg  X  e Plug-In for Ontology Recovery and Visualization .................................................... 386 Augmenting Traditional ER Method to Support Spatiotemporal Database Applications ............................................ 388 STOC: Extending Oracle to Support Spatiotemporal Data Management .................................................... 393 Preface to the 2nd International Workshop on Unstructured Data Management (USDM 2011) ....................................... 398 ITEM: Extract and Integrate Entities from Tabular Data to RDF Knowledge Base ................................................. 400 Batch Text Similarity Search with MapReduce ....................... 412 An Empirical Study of Massively Parallel Bayesian Networks Learning for Sentiment Extraction from Unstructured Text .................... 424 Preface of the 2nd International Workshop on XML Data Management .................................................... 436 XPath Rewriting Using Views: The More the Merrier (Keynote Speech) ................................................ 438 OSD-DB: A Military Logistics Mobile Database ...................... 440 Querying and Reasoning with RDF(S)/OWL in XQuery .............. 450 A Survey on XML Keyword Search ................................. 460 Schema Mapping with Quality Assurance for Data Integration ......... 472 Author Index .................................................. 485 T he success o fasearche n g in e in c lo ud c o mput in ge nvi r on me n tre li es on the n umbers o fusersa n dthe i rc li ck -thr o ugh .I f w e take the pre vio us search ke y wo rdsastags o fuserst o stud y a n dd iff ere n t i ate the user in teract ion beha vio rs , based on the i rpre vio us act ion s in stead o fpass iv e ly w a i t in gf o rusers  X  quer i es . How e v er the user search in gbeha vio r i sa ff ected b ylo ts o ffact o rs , a n d i t i s qu i te c o mp l e x a n du n certa in. T he lo gfi l es pr ovi ded b y asearche n g in eha v e rec o rded a ll the in f o rmat ion o f the user in teract ion pr o cess on the i rser v ers o r br ow sers , such as ke ywo rds , c li ck -thr o ugh rate , t i me stamp , t i me on page ,IP push rec o mme n dat ion based on the quer i ed ke ywo rds ?An dh ow t o e x tract user beha vio rm o de l s o fsearch in gact ion s in o rder t o rec o mme n dthe in f o rmat ion t o meet users  X  rea ln eeds m o re t i me ly a n dprec i se ly?
In o rder t o a n s w er these quest ion s ,w ed on X  tth in kthere i s only on ec o rrect W e suggested a n dha v ea l read y stud i ed a c lo ud m o de l t o br i dge the gap bet w ee n w h i ch mea n sthatac on t in u o us o rper io d i chab i tma y reappear aga in a n d aga in, a n de v e n the user in terest i scha n ged fr o mt i me t o t i me .

W earede v e lo p in ga niMin er s y stem at prese n t , am inin gp l atf o rm that ca n act iv e ly push usefu lin f o rmat ion t o users , rather tha n atrad i t ion a l search e n-g in e w h i ch only pass iv e ly w a i ts f o rusers  X  ke ywo rds .To supp o rt th i sk in d o f u l es o fhuge lo gdatapre -pr o cess in g , users  X  act ion sm o de lin g , quer y m o de lin g a n dquer y match in getc .W etr y t oi mp l eme n ta ll o fthem inin gser vi ces on a vi rtua l c lo ud m inin gser vi ce ce n ter .T he supp o rted ser vi ces in c lo ud c o mput in g t o the users  X  e nvi r on me n t such as the prese n t lo cat ion a n dt i me . T he w eb i su n derg oin gafu n dame n ta l tra n sf o rmat ion: i t i sbec o m in gm o b il e quer y fu n ct ion a li t y. T he research c o mmu ni t yi shardat wo rk e n ab lin geffic i e n t supp o rt f o rsuchfu n ct ion a li t y.

W e refer t o p oin ts o f in terest wi th a w eb prese n ce as spat i a lw eb o b j ects o r the lo cat ion o fauser , a n d are re l e v a n tt o ate x targume n t , t y p i ca lly asearch e n g in equer y. T he ta l kc ov ers rece n tresu l ts b y the speaker a n dh i sc oll eagues S pec i fica lly, the ta l kt o uches on quest ion ssuchasthef ollowin g :  X  How t o use user -ge n erated c on te n t , spec i fica lly l arge c oll ect ion s o fG PS  X  How t o ass i g ni mp o rta n ce t o p l aces us in gG PS rec o rds a n d lo gs o fquer i es  X  How t o supp o rt c on t in u o us quer i es that ma in ta in up -t o-date resu l ts as the  X  How t o take in t o acc o u n t the prese n ce o f n earb y p l aces that are re l e v a n tt o
 Volume Editors Jeffrey Xu Yu Chinese University of Hong Kong, Department of Systems Engineering and Engi-neering Management Shatin, N.T., Hong Kong, China E-mail: yu@se.cuhk.edu.hk Masaru Kitsuregawa University of Tokyo, Institute of Industrial Science 4-6-1 Komaba, Meguro-Ku, Tokyo 153-8505, Japan E-mail: kitsure@tkl.iis.u-tokyo.ac.jp Hong Va Leong Hong Kong Polytechnic University, Department of Computing Hung Hom, Kowloon, Hong Kong, China E-mail: cshleong@comp.polyu.edu.hk Library of Congress Control Number: 2006927069 CR Subject Classification (1998): H.2, H.3, H.4, I.2, H.5, C.2, J.1 LNCS Sublibrary: S L 3  X  Information Systems and Application, incl. Internet/Web and HCI ISSN 0302-9743 ISBN-10 3-540-35225-2 Springer Berlin Heidelberg New York ISBN-13 978-3-540-35225-9 Springer Berlin Heidelberg New York The rapid prevalence of Web applications requires new technologies for the design, implementation and management of Web-based information systems. WAIM 2006, following the past tradition of WAIM conferences, was an inter-national forum for researchers, practitioners, developers and users to share and exchange cutting-edge ideas, results, experience, techniques and tools in connec-tion with all aspects of Web data management. The conference drew together original research and industrial papers on the theory, design and implementa-tion of Web-based information systems. As the seventh event in the increasingly popular series, WAIM 2006 made the move towards internationalization by mi-grating out of mainland China into Hong Kong, the Oriental Pearl, and the cultural junction and melting pot between the East and the West. It was suc-cessful in attracting outstanding researchers from all over the world to Hong Kong. These proceedings collected the technical papers selected for presentation at the 7th International Conference on Web-Age Information Management, held in Hong Kong, on June 17 X 19, 2006.
 paper submissions from North America, South America, Europe, Asia, and Ocea-nia. Each submitted paper underwent a rigorous review by three independent referees, with detailed referee reports. Finally, 50 full research papers were ac-cepted, from Australia, China, Germany, Hong Kong, Japan, Korea, Macau, New Zealand, Singapore, Spain, Taiwan, the UK and USA, representing a competitive acceptance rate of 17%. The contributed papers addressed a broad spectrum on Web-based information systems, ranging from data caching, data distribution, data indexing, data mining, data stream processing, information retrieval, query processing, temporal databases, XML and semistructured data, sensor networks, peer-to-peer, grid computing, Web services, and Web searching.
 outstanding researchers in the WAIM research area. We would like to extend our sincere gratitude to the Program Committee members and external reviewers. of this conference, making its every success. Special thanks go to The Chinese University of Hong Kong, City University of Hong Kong, Hong Kong Baptist University, The Hong Kong Polytechnic University, The Hong Kong University of Science and Technology, The University of Hong Kong, Hong Kong Web Society, IEEE Hong Kong Section Computer Society Chapter, and Hong Kong Pei Hua Education Foundation.
 June 2006 Jeffrey Xu Yu It is our great pleasure to welcome you to WAIM 2006. At age 7, WAIM was ready to reach new heights. This year, WAIM featured a couple of new ini-tiatives. We added two accompanying workshops, one co-chaired by Hong Va Leong and Reynold Cheng and the other co-chaired by Lei Chen and Yoshiharu Ishikawa. A project demo session, organized by Joseph Ng, was introduced so that conference participants could get their hands on the research prototypes and interact directly with the researchers who developed them.
 gether an excellent technical program, including three inspiring keynote speeches by leading experts in the field. In addition to the paper sessions, we had a rich tutorial program, put together by Wang-Chien Lee and Frederick Lochovsky, and a panel that was carefully selected by Kamal Karlapalem and Qing Li to reflect timely research issues relevant to this region.
 without the hard work of many members of the Organizing Committee. Hong Va Leong, the Publication Chair, was responsible for the negotiation and logistics for the publication of the proceedings you are reading. Xiaofeng Meng, Huan Liu and Arkady Zaslavsky helped to publicize WAIM 2006 in four key regions, Asia, USA, Australia and Europe.
 and Vincent Ng for carefully managing the budget. Robert Luk and Ben Kao, the Local Arrangement Co-chairs, working with the office staff at the Hong Kong Polytechnic University, Department of Computing, arranged the conference site and social activities to give the conference participants an enjoyable experience during the conference and pleasant memories to bring home.
 Society provided both financial and moral support to WAIM 2006. Additional major funding was received from Hong Kong Pei Hua Education Foundation Limited and IEEE Hong Kong Section Computer Society Chapter. Their gen-erosity is much appreciated.
 June 2006 Dik Lun Lee and Wang Sang WAIM 2006 was jointly organized by The Chinese University of Hong Kong, City University of Hong Kong, Hong Kong Baptist University, The Hong Kong Poly-technic University, The Hong Kong University of Science and Technology, and The University of Hong Kong, and was hosted by the Department of Computing, The Hong Kong Polytechnic University.
 Honorary Conference Keith Chan, The Hong Kong Polytechnic Chair: University, Hong Kong Conference General Dik Lun Lee, Hong Kong University of Science Co-chairs: and Technology, Hong Kong Program Committee Jeffrey Xu Yu, Chinese University of Co-chairs: Hong Kong, Hong Kong Tutorial Co-chairs: Wang-Chien Lee, Pennsylvania State University, Panel Co-chairs: Kamal Karlapalem, International Institute of Publication Chair: Hong Va Leong, The Hong Kong Polytechnic Publicity Co-chairs: Xiaofeng Meng, Renmin University of China, Exhibition and Industrial Joseph Fong, City University of Hong Kong, Liaison Chair: Hong Kong Research Project Exhibition Joseph Ng, Hong Kong Baptist University, and Demonstration Chair: Hong Kong Local Arrangement Robert Luk, The Hong Kong Polytechnic Co-chairs: University, Hong Kong Finance Chair: Vincent Ng, The Hong Kong Polytechnic Steering Committee Sean X. Wang, University of Vermont, USA Liaison: Toshiyuki Amagasa University of Tsukuba, Japan James Bailey University of Melbourne, Australia Sourav S. Bhowmick Nanyang Technological University, Singapore Stephane Bressan National University of Singapore, Singapore Ying Cai Iowa State University, USA Wojciech Cellary The Poznan University of Economics, Poland Chee Yong Chan National University of Singapore, Singapore Kevin Chang University of Illinois at Urbana-Champaign, Arbee L.P. Chen National Tshing Hua University, Taiwan Lei Chen Hong Kong University of Science and Reynold Cheng The Hong Kong Polytechnic University, Kak Wah Chiu Dickson Computer Systems, Hong Kong Chin-Wan Chung KAIST, Korea Gao Cong University of Edinburgh, UK Gill Dobbie University of Auckland, New Zealand Ling Feng Twente University, The Netherlands Ada Fu Chinese University of Hong Kong, Hong Kong Xiang Fu Georgia Southwestern State University, USA Stephane Grumbach Liama The Sino-French IT Lab Institute of Takahiro Hara Osaka University, Japan Haibo Hu Hong Kong University of Science and Joshua Huang University of Hong Kong, Hong Kong Edward Hung The Hong Kong Polytechnic University, Haifeng Jiang IBM Almaden Research Center, USA Hyunchul Kang Chung-Ang University, Korea Ben Kao University of Hong Kong, Hong Kong Hiroyuki Kitagawa University of Tsukuba, Japan Yasushi Kiyoki Keio University, Japan Flip Korn AT&amp;T, USA Chiang Lee National Cheng-Kung University, Taiwan Wookey Lee Sungkyul University, Korea YoonJoon Lee KAIST, Korea Chen Li University of California, Irvine, USA Ee Peng Lim Nanyang Technological University, Singapore Xuemin Lin University of New South Wales, Australia Chengfei Liu Swinburne University of Technology, Australia Guimei Liu National University of Singapore, Singapore Mengchi Liu Carleton University, Canada Tieyan Liu Microsoft Research Asia, China Frederick H. Lochovsky Hong Kong University of Science and Nikos Mamoulis University of Hong Kong, Hong Kong Weiyi Meng Binghamton University, USA Xiaofeng Meng Renmin University of China, China Mukesh K Mohania IBM India Rearch Lab, India Miyuki Nakano University of Tokyo, Japan Wilfred Ng Hong Kong University of Science and Beng Chin Ooi National University of Singapore, Singapore Jian Pei Simon Fraser University, Canada Zhiyong Peng Wuhan University, China Keun Ho Ryu Chungbuk National University, Korea Klaus-Dieter Schewe Massey University, New Zealand Heng Tao Shen University of Queensland, Australia Timothy K. Shih Tamkang University, Taiwan Dawei Song Open University, UK Kazutoshi Sumiya University of Hyogo, Japan Keishi Tajima Kyoto University, Japan Kian-Lee Tan National University of Singapore, Singapore Katsumi Tanaka Kyoto University, Japan Changjie Tang Sichuan University, China David Taniar Monash University, Australia Yufei Tao City University of Hong Kong, Hong Kong Masashi Toyoda University of Tokyo, Japan Anthony Tung National University of Singapore, Singapore Guoren Wang Northeastern University, China Haixun Wang IBM T. J. Watson Research Center, USA Ke Wang Simon Fraser University, Canada Min Wang IBM T. J. Watson Research Center, USA Wei Wang Fudan University, China Wei Wang University of New South Wales, Australia X. Sean Wang University of Vermont, USA Jirong Wen Microsoft Research Asia, China Raymond Wong University of New South Wales, Australia Jianliang Xu Hong Kong Baptist University, Hong Kong Chris Yang Chinese University of Hong Kong, Hong Kong Dongqing Yang Peking University, China Jun Yang Duke University, USA Yun Yang Swinburne University of Technology, Australia Masatoshi Yoshikawa Nagoya University, Japan Cui Yu Monmouth University, USA Ge Yu Northeastern University, China Arkady Zaslavsky Monash University, Australia Chengqi Zhang University of Technology, Sydney, Australia Yanchun Zhang Victoria University, Australia Baihua Zheng Singapore Management University, Singapore Aoying Zhou Fudan University, China Lizhu Zhou Tsinghua University, China Shuigeng Zhou Fudan University, China Xiaofang Zhou University of Queensland, Australia Manli Zhu Institute for Infocomm Research, Singapore Guozhu Dong Wright State University, USA Masaru Kitsuregawa University of Tokyo, Japan Jianzhong Li Harbin Institute of Technology, China Xiaofeng Meng Renmin University, China Baile Shi Fudan University, China Jianwen Su University of California at Santa Barbara, USA Shan Wang Remin University, China X. Sean Wang University of Vermont, USA Ge Yu Northeastern University, China Aoying Zhou Fudan University, China Sihem Amer-Yahia Rebecca Lynn Braynard Huiping Cao Badrish Chandramouli Bo Chen Jinchuan Chen Manoranjan Dash Susumu Date Chun Feng Guang Feng Andrew Flahive Chan Kai Fong Bin Gao Matthew Gebski Byunghyun Ha Rachid Hamadi Gab-Soo Han Wook-shin Han Kenji Hatano Bin He Hao He Qi He Wai-Shing Ho Ming-Qiang Hou Kenneth Hsu Ming Hua Yuan-Ke Huang Lucas Hui Huan Huo Nobuto Inoguchi Yoshiharu Ishikawa Arpit Jain Mingfei Jiang Govind Kabra Odej Kao Panagiotis Karras Roland Kaschek Seung Kim Chui Chun Kit Isao Kojima Man Ki Mag Lau Yuangui Lei Hou U Leong Chengkai Li Chunsheng Li Jianxin Li Li Li Xiaoguang Li Xiang Lian Seungkil Lim Taeso o Lim Junyi Xie Linhao Xu Liang Huai Yang Qi Yang Zaihan Yang Ikjun Yeom Cai Yi Yiqun Lisa Yin Man Lung Yiu Tomoki Yoshihisa Yaxin Yu Yidong Yuan Kun Yue The Chinese University of Hong Kong City University of Hong Kong Hong Kong Baptist University The Hong Kong Polytechnic University The Hong Kong University of Science and Technology The University of Hong Kong Hong Kong Web Society Hong Kong Pei Hua Education Foundation IEEE Hong Kong Section Computer Society Chapter On-Demand Index for Efficient Structural Joins An Efficient Indexing Scheme for Moving Objects X  Trajectories on Road Networks Spatial Index Compression for Location-Based Services Based on a MBR Semi-approximation Scheme KCAM: Concentrating on Structural Similarity for XML Fragments A New Structure for Accelerating XPath Location Steps Efficient Evaluation of Multiple Queries on Streamed XML Fragments Automated Extraction of Hit Numbers from Search Result Pages Keyword Extraction Using Support Vector Machine LSM: Language Sense Model for Information Retrieval Succinct and Informative Cluster Descriptions for Document Repositories LRD: Latent Relation Discovery for Vector Space Expansion and Information Retrieval Web Image Retrieval Refinement by Visual Contents An Effective Approach for Hiding Sensitive Knowledge in Data Publishing Tracking Network-Constrained Moving Objects with Group Updates Dynamic Configuring Service on Semantic Grid Object Placement and Caching Strategies on AN.P2P Role-Based Peer-to-Peer Model: Capture Global Pseudonymity for Privacy Protection A Reputation Management Scheme Based on Global Trust Model for Peer-to-Peer Virtual Communities QoS-Aware Web Services Composition Using Transactional Composition Operator Optimizing the Profit of On-Demand Multimedia Service Via a Server-Dependent Queuing System Service Matchmaking Based on Semantics and Interface Dependencies Crawling Web Pages with Support for Client-Side Dynamism RecipeCrawler: Collecting Recipe Data from WWW Incrementally CCWrapper: Adaptive Predefined Schema Guided Web Extraction MiniTasking: Improving Cache Performance for Multiple Query Workloads Cache Consistency in Mobile XML Databases Bulkloading Updates for Moving Objects Finding the Plateau in an Aggregated Time Series Compressing Spatial and Temporal Correlated Data in Wireless Sensor Networks Based on Ring Topology Discovery of Temporal Frequent Patterns Using TFP-Tree DGCL: An Efficient Density and Grid Based Clustering Algorithm for Large Spatial Database Scalable Clustering Using Graphics Processors TreeCluster: Clustering Results o f Keyword Search over Databases A New Method for Finding Approximate Repetitions in DNA Sequences Dynamic Incremental Data Summarization for Hierarchical Clustering Classifying E-Mails Via Support Vector Machine A Novel Web Page Categorization Algorithm Based on Block Propagation Using Query-Log Information Counting Graph Matches with Adaptive Statistics Collection Tight Bounds on the Estimation Distance Using Wavelet Load Shedding for Window Joins over Streams Error-Adaptive and Time-Aware Maintenance of Frequency Counts over Data Streams Supporting Efficient Distributed Top-k Monitoring Designing Quality XML Schemas from E-R Diagrams Validating Semistructured Data Using OWL Dynamic Data Distribution of High Level Architecture Based on Publication and Subscription Tree A Framework for Query Reformulation Between Knowledge Base Peers An Efficient Indexing Technique for Computing High Dimensional Data Cubes A Scientific Workflow Framework Integrated with Object Deputy Model for Data Provenance On the Development of a Multiple-Compensation Mechanism for Business Transactions OS-DRAM: A Delegation Administration Model in a Decentralized Enterprise Environment
 Commenced Publication in 1973 Founding and Former Series Editors: Gerhard Goos, Juris Hartmanis, and Jan van Leeuwen David Hutchison Takeo Kanade Josef Kittler Jon M. Kleinberg Alfred Kobsa Friedemann Mattern John C. Mitchell Moni Naor Oscar Nierstrasz C. Pandu Rangan Bernhard Steffen Madhu Sudan Demetri Terzopoulos Doug Tygar Gerhard Weikum Volume Editors Lei Chen Hong Kong University of Science and Technology Department of Computer Science Clear Water Bay, Kowloon, Hong Kong, China E-mail: leichen@cs.ust.hk Changjie Tang Sichuan University, Computer Department Chengdu 610064, China E-mail: cjtang@scu.edu.cn Jun Yang Duke University, Department of Computer Science Box 90129, Durham, NC 27708-0129, USA E-mail: junyang@cs.duke.edu Yunjun Gao
Zhejiang University, College of Computer Science 388 Yuhangtang Road, Hangzhou 310058, China E-mail: gaoyj@zju.edu.cn Library of Congress Control Number: 2010929625 CR Subject Classification (1998): H.3, H.4, I.2, C.2, H.2, H.5
LNCS Sublibrary: SL 3  X  Information Systems and Application, incl. Internet/Web and HCI ISSN 0302-9743 ISBN-10 3-642-14245-1 Springer Berlin Heidelberg New York ISBN-13 978-3-642-14245-1 Springer Berlin Heidelberg New York The previous WAIM conferences were held in Shanghai ( 2000), Xi'an (2001), Beijing (2002), Chengdu (2003), Dalian (2004), Hangzhou (2005), Hong Kong (2006), Huangshan (2007), Zhangjiajie (2008), and Suzhou (2009). In 2010, WAIM was held in Jiuzhaigou, Sichuan, China. Thailand, UK, and USA, we selected 58 full papers and 11 short papers for publica-addressed a wide range of topics such as Web, XML, and multimedia data, data proc-essing in the cloud or on new hardware, data mining and knowledge discovery, infor-and Prof. Xiaofang Zhou. A conference like WAIM can only succeed as a team effort. We want to thank the thanks go to the local Organizing Committee headed by Changjie Tang, Aoying Zhou, and Lei Duan. Many thanks also go to our Workshop Co-chairs (Jian Pei and Hengtao Wang and Shuigeng Zhou), Industrial Chairs (Qiming Chen and Haixun Wang), Reg-istration Chair (Chuan Li), and Finance Co-chairs (Howard Leung and Yu Chen). Last conference. Yi Zhang Sichuan University, China Masaru Kitsuregawa University of Tokyo, Japan Qing Li City University of Hong Kong, Hong Kong Lei Chen Hong Kong University of Science and Technology, Changjie Tang Sichuan University, China Jun Yang Duke University, USA Aoying Zhou East China Normal University, China Lei Duan Sichuan University, China Jian Pei Simon Fraser University, Canada Hengtao Shen University of Queensland, Australia Wenyin Liu City University of Hong Kong, Hong Kong Jian Yang Macquarie University, Australia Qiming Chen HP Labs, Palo Alto, USA Haixun Wang Microsoft Research Asia, China Yunjun Gao Zhejiang University, China Hua Wang University of Southern Queensland, Australia Shuigeng Zhou Fudan University, China Howard Leung Hong Kong Web Society, Hong Kong Yu Chen Sichuan University, China Chuan Li Sichuan University, China Xiaofeng Meng Renmin University of China, China Zhiyong Peng Wuhan University, China Jie Zuo Sichuan University, China James Bailey University of Melbourne, Australia Gang Chen Zhejiang University, China Hong Chen Chinese Univeristy of Hong Kong, Hong Kong Yu Chen Sichuan University, China Reynold Cheng The University of Hong Kong, Hong Kong David Cheung The University of Hong Kong, Hong kong Dickson Chiu Dickson Computer Systems, Hong Kong Byron Choi Hong Kong Baptist University, Hong Kong Bin Cui Peking University, China Alfredo Cuzzocrea University of Calabria, Italy Guozhu Dong Wright State University, USA Xiaoyong Du Renmin University of China, China Lei Duan Sichuan University, China Ling Feng Tsinghua University, China Johann Gamper Free University of Bozen-Bolzano, Italy Bryon Gao Texas State University at San Marcos, USA Yong Gao Univeristy of British Columbia, Canada Jihong Guan Tongji University, China Giovanna Guerrini Universit X  di Genova, Italy Bingsheng He Chinese Univeristy of Hong Kong, Hong Kong Jimmy Huang York Univeristy, Canada Seung-won Hwang Pohang University of Science and Technology, Wee Hyong Microsoft Yoshiharu Ishikawa Nagoya University, Japan Yan Jia National University of Defence Technology, China Ruoming Jin Kent State University, USA Ning Jing National University of Defence Technology, China Ben Kao The University of Hong Kong, Hong Kong Yong Kim Korea Education &amp; Res earch Information Service, Nick Koudas Univeristy of Toronto, Canada Wu Kui Victoria University, Canada Carson Leung University of Manitoba, Canada Chengkai Li University of Texas at Arlington, USA Chuan Li Sichuan University, China Feifei Li Florida State University, USA Tao Li Florida International University, USA Tianrui Li Southwest Jiaotong University, China Zhanhuai Li Northwestern Polytechnical University, China Zhoujun Li Beihang University, China Xiang Lian Hong Kong University of Science and Technology, Lipeow Lim University of Hawaii at Manoa, USA Xuemin Lin University of New South Wales, Australia Huan Liu Arizona State University, USA Lianfang Liu Computing Center of Guangxi, China Qizhi Liu Nanjing University, China Weiyi Liu Yunnan University, China Wenyin Liu City Univeristy of Hong Kong Eric Lo Hong Kong Polytechnic University, Hong Kong Zongmin Ma Northeastern University, China Weiyi Meng State University of New York at Binghamton, USA Mohamed Mokbel University of Minnesota, USA Yang-Sae Moon Kangwon National University, Korea Akiyo Nadamoto Konan University, Japan Miyuki Nakano University of Tokyo, Japan Raymond Ng University of British Columbia, Canada Anne Ngu Texas State University at San Marcos, USA Tadashi Ohmori University of Electro Communications, Japan Olga Papaemmanouil Brandeis University, USA Zhiyong Peng Wuhan University, China Evaggelia Pitoura University of Ioannina, Greece Tieyun Qian Wuhan University, China Shaojie Qiao Southwest Jiaotong University, China Markus Schneider University of Florida, USA Hengtao Shen University of Queensland, Australia Yong Tang Sun Yat-sen University, China David Taniar Monash University, Australia Maguelonne Teisseire University Montpellier 2, France Anthony Tung National University of Singapore, Singapore Shunsuke Uemura Nara Sangyo University, Japan Jianyong Wang Tsinghua University, China Ke Wang Simon Fraser University, Canada Tengjiao Wang Peking University, China Wei Wang University of New South Wales, Australia Raymond Wong University of New South Wales, Australia Raymond Chi-Wing Wong Hong Kong University of Science and Technology, Xintao Wu University of North Carolina at Charlotte, USA Yuqing Wu Indiana University at Bloomington, USA Junyi Xie Oracle Corp., USA Li Xiong Emory University, USA Jianliang Xu Hong Kong Baptist University, Hong Kong Jian Yang Macquaire University, Australia Xiaochun Yang Northeastern University, China Ke Yi Hong Kong University of Science and Technology, Hwanjo Yu Pohang University of Science and Technology, Jeffrey Yu Chinese Univeristy of Hong Kong, Hong Kong Lei Yu State University of New York at Binghamton, USA Philip Yu University of Illinois at Chicago, USA Ting Yu North Carolina State University, USA Xiaohui Yu York University, Canada Demetris Zeinalipour University of Cyprus, Cyprus Donghui Zhang Microsoft Jim Gray Systems Lab, USA Ji Zhang University of Southern Queensland, Australia Baihua Zheng Singapore Management University, Singapore Aoying Zhou East China Normal University, China Shuigeng Zhou Fudan University, China Xiangmin Zhou CSIRO, Australia Qiang Zhu University of Michigan at Dearborn, USA Lei Zou Peking University, China Sichuan University Analyzing Data Quality Using Data Auditor (Keynote Abstract) ....... 1 Rebuilding the World from Views (Keynote Abstract) ................ 2 Approximate Query Processing in Sensor Networks (Keynote Abstract) .............................................. 3 Duplicate Identification in Deep Web Data Integration ................ 5 Learning to Detect Web Spam by Genetic Programming .............. 18 Semantic Annotation of Web Objects Using Constrained Conditional Random Fields .................................................. 28 Time Graph Pattern Mining for Web Analysis and Information Retrieval ........................................................ 40 FISH: A Novel Peer-to-Peer Overlay Network Based on Hyper-deBruijn .................................................. 47 Continuous Summarization of Co-evolving Data in Large Water Distribution Network ............................................. 62 Proactive Replication and Search for Rare Objects in Unstructured Peer-to-Peer Networks ............................................ 74 SWORDS: Improving Sensor Networks Immunity under Worm Attacks ......................................................... 86 Efficient Multiple Objects-Orient ed Event Detection over RFID Data Streams ........................................................ 97 CW2I: Community Data Indexing for Complex Query Processing ...... 103 Clustering Coefficient Queries on Massive Dynamic Social Networks .... 115 Predicting Best Answerers for New Questions in Community Question Answering ...................................................... 127 Semantic Grounding of Hybridization for Tag Recommendation ........ 139 Rich Ontology Extraction and Wikipedia Expansion Using Language Resources ....................................................... 151 Fine-Grained Cloud DB Damage Examination Based on Bloom Filters .......................................................... 157 XML Structural Similarity Search Using MapReduce ................. 169 Comparing Hadoop and Fat-Btree B ased Access Method for Small File I/O Applications ................................................. 182 Mining Contrast Inequalities in Numeric Dataset ..................... 194 Users X  Book-Loan Behaviors Analysis and Knowledge Dependency Mining ......................................................... 206 An Extended Predictive Model Markup Language for Data Mining ..... 218 A Cross-Media Method of Stakeholder Extraction for News Contents Analysis ........................................................ 232 An Efficient Approach for Mining Segment-Wise Intervention Rules in Time-Series Streams .............................................. 238 Automated Recognition of Sequential Patterns in Captured Motion Streams ........................................................ 250 Online Pattern Aggregation over RFID Data Streams ................ 262 Cleaning Uncertain Streams by Parallelized Probabilistic Graphical Models ......................................................... 274 Taming Computational Complexity: Efficient and Parallel SimRank Optimizations on U ndirected Graphs ............................... 280 DSI: A Method for Indexing Large Graphs Using Distance Set ......... 297 K-Radius Subgraph Comparison for RDF Data Cleansing ............. 309 A Novel Framework for Processing Continuous Queries on Moving Objects ......................................................... 321 Group Visible Nearest Neighbor Queries in Spatial Databases ......... 333 iPoc: A Polar Coordinate Based Indexing Method for Nearest Neighbor Search in High Dimensional Space .................................. 345 Join Directly on Heavy-Weight Compressed Data in Column-Oriented Database ....................................................... 357 Exploiting Service Context for Web Service Search Engine ............ 363 Building Business Intelligence Applications Having Prescriptive and Predictive Capabilities ............................................ 376 FileSearchCube: A File Grouping Tool Combining Multiple Types of Interfile-Relationships ............................................ 386 Trustworthy Information: Concepts and Mechanisms ................. 398 How to Design Kansei Retrieval Systems? ........................... 405 Detecting Hot Events from Web Search Logs ........................ 417 Evaluating Truthfulness of Modifiers Attached to Web Entity Names ... 429 Searching the Web for Alternative Answers to Questions on WebQA Sites ........................................................... 441 Domain-Independent Classification for Deep Web Interfaces ........... 453 Data Selection for Exact Value Acquisition to Improve Uncertain Clustering ...................................................... 459 Exploring the Sentiment Strength of User Reviews ................... 471 Semantic Entity Detection by Integrating CRF and SVM ............. 483 An Incremental Method for Ca usal Network Construction ............. 495 DCUBE: CUBE on Dirty Databases ................................ 507 An Algorithm for Incremental Maintenance of Materialized XPath View ........................................................... 513 Query Processing in INM Database System ......................... 525 Fragile Watermarking for Color Image Recovery Based on Color Filter Array Interpolation .............................................. 537 A Hybrid-Feature-Based Efficient Retrieval over Chinese Calligraphic Manuscript Image Repository ..................................... 544 Efficient Filtering of XML Documents with XPath Expressions Containing Ancestor Axis ......................................... 551 ACAR: An Adaptive Cost Aware Cache Replacement Approach for Flash Memory ................................................... 558 GPU-Accelerated Predicate E valuation on Column Store .............. 570 MOSS-DB: A Hardware-Aware OLAP Database ..................... 582 Efficient Duplicate Record Detectio n Based on Similarity Estimation ... 595 A Novel Composite Kernel for Finding Similar Questions in CQA Services ......................................................... 608 Efficient Similarity Query in R FID Trajectory Databases .............. 620 Context-Aware Basic Level Concepts Detection in Folksonomies ....... 632 Extracting 5W1H Event Semantic Elements from Chinese Online News ........................................................... 644 Automatic Domain Terminology Extraction Using Graph Mutual Reinforcement ................................................... 656 Semi-supervised Learning from Only Positive and Unlabeled Data Using Entropy ................................................... 668 Margin Based Sample Weighting for Stable Feature Selection .......... 680 Associative Classifier for Uncertain Data ............................ 692 Automatic Multi-schema Integration Based on User Preference ........ 704 EIF: A Framework of Effective Entity Identification .................. 717 A Multilevel and Domain-Independent Duplicate Detection Model for Scientific Database ............................................... 729 Generalized UDF for Analytics Inside Database Engine ............... 742 Efficient Continuous Top-k Keyword Search in Relational Databases .... 755 V Locking Protocol for Materialized Aggregate Join Views on B-Tree Indices ......................................................... 768 Web Information Credibility (Keynote Abstract) ..................... 781 Author Index .................................................. 783
 Commenced Publication in 1973 Founding and Former Series Editors: Gerhard Goos, Juris Hartmanis, and Jan van Leeuwen David Hutchison Takeo Kanade Josef Kittler Jon M. Kleinberg Alfred Kobsa Friedemann Mattern John C. Mitchell Moni Naor Oscar Nierstrasz C. Pandu Rangan Bernhard Steffen Madhu Sudan Demetri Terzopoulos Doug Tygar Gerhard Weikum Volume Editors Hong Gao Harbin Institute of Technology Harbin 150001, Heilongjiang, China E-mail: honggao@hit.edu.cn Lipyeow Lim University of Hawaii Honolulu 96822, HI, USA E-mail: lipyeow@hawaii.edu Wei Wang Fudan University Shanghai 200433, China E-mail: weiwang1@fudan.edu.cn Chuan Li Sichuan University Chengdu 610064, Sichuan, China E-mail: lcharles@scu.edu.cn Lei Chen Hong Kong University of Science and Technology Kowloon, Hong Kong, China E-mail: leichen@cse.ust.hk ISSN 0302-9743 e-ISSN 1611-3349 ISBN 978-3-642-32280-8 e-ISBN 978-3-642-32281-5 DOI 10.1007/978-3-642-32281-5 Springer Heidelberg Dordrecht London New York Library of Congress Control Number: 2012943037 CR Subject Classification (1998): H.2.4, H.2.7-8, H.3.3-5, F.2.2, H.2, H.4, C.2, H.5, G.2.2, I.5.3
LNCS Sublibrary: S L 3  X  Information Systems and Application, incl. Internet/Web and HCI This volume contains the proceedings of the 13th International Conference on Web-Age Information Management (WAIM), held August 18 X 20, 2012, in Harbin, China. WAIM is a leading international conference on research, development, and applications of Web technologies, database systems and software engineer-ing. WAIM is based in the Asia-Pacific region, and previous WAIM conferences were held in Shanghai (2000), Xi X  X n (2001), Beijing (2002), Chengdu (2003), Dalian (2004), Hangzhou (2005), Hong Kong (2006), Huangshan (2007), Zhangji-ajie (2008), Suzhou (2009), Jiuzhaigou (2010), and Wuhan (2011). As the 13th event in the increasingly popular series, WAIM 2012, which was organized and supported by Harbin Institute of Technology, attracted outstanding researchers from all over the world to Harbin, China. In particular, this year WAIM and Microsoft Research Asia jointly sponso red a database summer school, which was collocated with WAIM.
 who chose WAIM as a venue for their publications. Out of 178 submissions from various countries and regions, we select ed 32 full papers and 10 short papers for publication. The acceptance rate for regu lar paper was 18% while the overall ac-ceptance rate is 23.6% including short papers. The contributed papers address a wide range of topics such as spatial databases, query processing, XML and Web data, graph and uncertain data, distributed computing, information extraction and integration, data warehousing and data mining, similarity search, wireless sensor networks, social networks, data security, etc. We are grateful to our dis-tinguished keynote speakers Wenfei Fan, Jiawei Han and Rakesh Agrawal for contributing to the event X  X  success.
 the Program Committee members and the reviewers for their invaluable efforts. Special thanks to the local Organizing Committee headed by Jizhou Luo. Many thanks also go to our Workshop Co-chairs (Xiaochun Yang and Hongzhi Wang), Publication Co-chairs (Lei Chen and Chuan Li), Publicity Co-chairs (Weiyi Meng and Guohui Li), Industrial chairs (Mukesh Mohania and Xiaoxin Wu), Panel Co-chairs (Jian Pei and Zhaonian Zou), and Finance Co-chairs (Howard Leung and Shengfei Shi). Last but not least, we wish to express our gratitude for the hard work of our webmaster (Hai Cao), and for our sponsors who generously supported the smooth running of our conference.
 August 2012 Hong Gao Jianzhong Li Harbin Institute of Technology, China Qing Li City University of Hong Kong Hong Gao Harbin Institute of Technology, China Lipyeow Lim University of Hawaii, USA Wei Wang Fudan University, China Xiaochun Yang Northeast University, China Hongzhi Wang Harbin Institute of Technology, China Jian Pei Simon Fraser University, USA Zhaonian Zou Harbin Institute of Technology, China Mukesh Mohania IBM, India Xiaoxin Wu Huawei, China Lei Chen Hong Kong University of Science and Technology, Chuan Li Sichuan University, China Weiyi Meng Binghamton University, USA Guohui Li Huazhong University of Science and Technology, Jizhou Luo Harbin Institute of Technology, China Howard Leung City University of Hong Kong Shengfei Shi Harbin Institute of Technology, China Xiaofeng Meng Renmin University, China Zhiyong Peng Wuhan University, China Haixun Wang Microsoft Research Asia, China Alfredo Cuzzocrea Univer sity of Calabria, Italy Aoying Zhou East China Normal University, China Bin Cui Peking University, China Carson K. Leung University of Manitoba, Canada Chengkai Li University of Texas at Arlington, USA Chuan Li Sichuan University, China Donghui Zhang Microsoft Jim Gray Systems Lab, USA Feifei Li Florida State University, USA Ge Yu Northeast University, China Giovanna Guerrini Universit` a di Genova, Italy Heng Tao Shen University of Queensland, Australia Hiroaki Ohshima Kyoto University, Japan Hong Chen Chinese University of Hong Kong, Hong Kong Hongzhi Wang Harbin Institute of Technology, China Hong Gao Harbin Institute of Technology, China Hwanjo Yu Pohang University of Science and Technology, Korea Jeffrey Yu Chinese University of Hong Kong Jianyong Wang Tsinghua University, China Jianzhong Li Harbin Institute of Technology, China JianliangXu Hong Kong Baptist University, Hong Kong Jimmy Huang York University, Canada Jizhou Luo Harbin Institute of Technology, China Jun Gao Peking University, China Johann Gamper Free Universi ty of Bozen-Bolzano, Italy Lei Duan Sichuan University, China Lei Chen Hong Kong University of Science and Technology, Lei Zou Peking University, China Ling Feng Tsinghua University, China Ning Jing National University of Defense Technology, China Ning Ruan Kent State University, USA Ning Yang Sichuan University, China Peng Wang Fudan University, China Raymond Ng University of British Columbia, Canada Shuai Ma University of Edinburgh, UK Shuigeng Zhou Fudan University, China Shuming Shi Microsoft Research Asia, China Tao Li Florida International University, USA Toshiyuki Amagasa University of Tsukuba, Japan Wei Wang University of New South Wales, Australia Weiyi Meng State University of New York at Binghamton, USA Xiangliang Zhang King Abdullah University of Science and Technology, Xiaofeng Meng Renmin University of China Xin Dong AT&amp;T Research, USA Xingquan Zhu University of Technology, Sydney Xintao Wu University of North Carolina at Charlotte, USA Xuemin Lin University of New South Wales, Australia Xiaofang Zhou University of Queensland, Australia Xiaochun Yang Northeastern University, China Yanghua Xiao Fudan University, China Yan Jia National University of Defense Technology, China Yang-Sae Moon Kangwon National University, Korea YaokaiFeng Kyushu University, Japan Yi Cai City University of Hong Kong Ke Yi Hong Kong University of Science and Technology, Yoshiharu Ishikawa Nagoya University, Japan Yunjun Gao Zhejianing University, China Yuqing Wu Indiana University at Bloomington, USA Zhanhuai Li Northwestern Polytechnical University, China Zhaonian Zou Harbin Institute of Technology, China Zhiyong Peng Wuhan University, China Zongmin Ma Northeaster n University, China Data Quality: Theory and Practice ................................. 1 Construction of Web-Based, Service-Oriented Information Networks: A Data Mining Persp ective (Abstract) .............................. 17 Electronic Textbooks and Data Mining ............................. 20 Topology-Based Data Compression in Wireless Sensor Networks ....... 22 A Residual Energy-Based Fairness Scheduling MAC Protocol for Wireless Sensor Networks ...................................... 35 Topology-Aided Geographic Routing Protocol for Wireless Sensor Networks ....................................................... 47 Polaris : A Fingerprint-Based Localization System over Wireless Networks ....................................................... 58 A High-Performance Algorithm for Frequent Itemset Mining ........... 71 Mining Link Patterns in Linked Data ............................... 83 Detecting Positive Opinion Leader Group from Forum ................ 95 D X  X ART: A Tool for Building and Populating Data Warehouse Model from Existing Reports and Tables .................................. 102 Continuous Skyline Queries with Integrity Assurance in Outsourced Spatial Databases ................................................ 114 Assessing Quality Values of Wikipedia Articles Using Implicit Positive and Negative Ratings ............................................. 127 Form-Based Instant Search and Quer y Autocompletion on Relational Data ........................................................... 139 Range Query Estimation for Dirty Data Management System .......... 152 Top-k Most Incremental Location Selection with Capacity Constraint ... 165 An Approach of Text-Based and Image-Based Multi-modal Search for Online Shopping .............................................. 172 Categorizing Search Results Using WordNet and Wikipedia ........... 185 Optimal Sequenced Route Query Algorithm Using Visited POI Graph .......................................................... 198 A Packaging Approach for Massive Amounts of Small Geospatial Files with HDFS ..................................................... 210 Adaptive Update Workload Reduction for Moving Objects in Road Networks ....................................................... 216 An Adaptive Distributed Index for Similarity Queries in Metric Spaces .......................................................... 222 Finding Relevant Tweets .......................................... 228 Fgram-Tree: An Index Structure Based on Feature Grams for String Approximate Search .............................................. 241 Efficient Processing of Updates in Dynamic Graph-Structured XML Data ........................................................... 254 Extracting Focused Time for Web Pages ............................ 266 Top-Down SLCA Computation Based on Hash Search ................ 272 Top-K Graph Pattern Matching: A Twig Query Approach ............ 284 Dynamic Graph Shortest Path Algorithm ........................... 296 A Framework for High-Quality Clustering Uncertain Data Stream over Sliding Windows ............................................. 308 Bayesian Network Structure Learning from Attribute Uncertain Data ... 314 Bandwidth-Aware Medical Image Retrieval in Mobile Cloud Computing Network ........................................................ 322 Efficient Algorithms for Constrained Subspace Skyline Query in Structured Peer -to-Peer Systems ................................. 334 Processing All k -Nearest Neighbor Queries in Hadoop ................ 346 RGH: An Efficient RSU-Aided Gro up-Based Hierarchical Privacy Enhancement Pro tocol for VANETs ................................ 352 Locating Encrypted Data Preci sely without Leaking Their Distribution ..................................................... 363 LB-Logging: A Highly Efficient Recovery Technique for Flash-Based Database ....................................................... 375 An Under-Sampling Approach to Imbalanced Automatic Keyphrase Extraction ...................................................... 387 A Tourist Itinerary Planning Approach Based on Ant Colony Algorithm ....................................................... 399 A Transparent Approach for Database Schema Evolution Using View Mechanism ...................................................... 405 WYSIWYE: An Algebra for Expressing Spatial and Textual Rules for Information Extraction ........................................ 419 A Scalable Algorithm for Detecting Community Outliers in Social Networks ....................................................... 434 An Efficient Index for Top-k Keyword Search on Social Networks ...... 446 Engineering Pathway for User Per sonal Knowledge Recommendation ... 459 Pick-Up Tree Based Route Recomme ndation from Taxi Trajectories .... 471 Author Index .................................................. 485 Traditional database systems typically focus on the quantity of data , to support the creation, maintenance and use of large volumes of data. But such a database system may not find correct answers to our queries if the data in the database are  X  X irty X , i.e., when the data do not properly repr esent the real world entities to which they refer.

To illustrate this, let us consider an employee relation residing in a database of a company, specified by the following schema: Here each tuple specifies an employee X  X  name (first name FN and last name LN ), office phone (country code CC ,areacode AC , phone phn ), office address ( street , city , zip code), salary and marital status . An instance D 0 of the employee schema is shown in Figure 1.

Consider the following queries posted on relation D 0 . (1) Query Q 1 is to find the number of employees working in the NYC office (New York City). The answer to Q 1 in D 0 is 3, by counting tuples t 1 ,t 2 and t . However, the answer may not be correct, f or the following reasons. First, the data in D 0 are inconsistent .Indeed,the CC and AC values of t 1 ,t 2 and t 3 have conflicts with their corresponding city attributes: when CC =44and AC = 131, the city should be Edinburgh ( EDI )inthe UK , rather than NYC ; and similarly, when CC =01and AC = 908, city should be Murray Hill ( MH )inthe US .It is thus likely that NYC is not the true city value of t 1 ,t 2 and t 3 . Second, the data in D 0 may be incomplete for employees working in NYC . That is, some tuples representing employees working in NYC may be missing from D 0 . Hence we cannot trust 3 to be the answer to Q 1 . (2) Query Q 2 is to find the number of distinct employees with FN =Mary.In D 0 the answer to Q 2 is 3, by enumerating tuples t 4 ,t 5 and t 6 . Nevertheless, the chances are that t 4 ,t 5 and t 6 actually refer to the same person: all these tuples were once the true values of Mary, but some have become obsolete. Hence the correct answer to Q 2 may be 1 instead of 3. (3) Query Q 3 is to find Mary X  X  current salary and current last name, provided that we know that t 4 ,t 5 and t 6 refer to the same person. Simply evaluating Q 3 on D 0 will get us that salary is either 50k or 80k, and that LN is either Smith or Luth. However, it does not tell us whether Mary X  X  current salary is 50k, and whether her current last name is Smith. Indeed, reliable timestamps for t 4 ,t 5 and t 6 may not be available, as commonly f ound in practice, and hence, we can-not tell which of 50k or 80k is more current; similarly for LN .
 This example tells us that when the data are dirty, we cannot expect a database system to answer our queries correctly, n o matter what capacity it provides to accommodate large data and how efficient it processes our queries.

Unfortunately, real-life data are often dirty : inconsistent, duplicated, inaccu-rate, incomplete and/or out of date. Indeed, enterprises typically find data error rates of approximately 1% X 5%, and for some companies it is above 30% [41]. In most data warehouse projects, data cleaning accounts for 30%-80% of the development time and budget [43], for improving the quality of the data rather than developing the systems. When it comes to incomplete information, it is estimated that  X  X ieces of information per ceived as being needed for clinical de-cisions were missing from 13.6% to 81% of the time X  [38]. When data currency is concerned, it is known that  X 2% of r ecords in a customer file become obsolete in one month X  [14]. That is, in a database of 500000 customer records, 10 000 records may go stale per month, 120000 records per year, and within two years about 50% of all the records may be obsolete.

Why do we care about dirty data? Data quality has become one of the most pressing challenges to data management. It is reported that dirty data cost US businesses 600 billion dollars annually [14], and that erroneously priced data in retail databases alone cost US consumers $2.5 billion each year [16]. While these indicate the daunting cost of dirty data in the US , there is no reason to believe that the scale of the problem is any different in any other society that is dependent on information technology. Dirty data have been a longstanding issue for decades, and the prevalent use of Inte rnet has been increasing the risks, in an unprecedented scale, of creating and propagating dirty data.

These highlight the need for data quality management ,toimprove the quality of the data in our databases such that the data consistently, accurately, com-pletely and uniquely represent the rea l-world entities to which they refer.
Data quality management is at least as important as traditional data man-agement tasks for coping with the quantity of data . There has been increasing demand in industries for developing data-quality management systems, aiming to effectively detect and correct errors in the data, and thus to add accuracy and value to business processes. Indeed, the market for data-quality tools is growing at 16% annually, way above the 7% average forecast for other IT segments [34]. As an example, data quality tools deliver  X  X n overall business value of more than 600 million GBP  X  each year at BT [40]. Data quality management is also a critical part of big data management, master data management ( MDM )[37], customer relationship management ( CRM ), enterprise resource planning ( ERP ) and supply chain management ( SCM ), among other things.

This paper aims to highlight several central technical issues in connection with data quality, and to provide an over view of recent advances in data quality management. We present five important issues of data quality (Section 2), and outline a rule-based approach to cleaning dirty data (Section 3). Finally, we identify some open research problems a ssociated with data quality (Section 4). The presentation is informal, to incite curiosity in the study of data quality. We opt for breadth rather than depth in the presentation: important results and techniques are briefly mentioned, but the details are omitted. A survey of detailed data quality management techniques is beyond the scope of this paper, and a number of related papers are not r eferenced due to space constraints. We refer the interested reader to papers in which the results were presented for more detailed presentation of the results and techniques. In particular, we encourage the reader to consult [3,4,9,1 7,21] for recent surveys on data quality management. In fact a large part of this paper is taken from [21]. We highlight five central issues in connection with data quality: data consistency, data deduplication, data accuracy, information completeness and data currency. 2.1 Data Consistency Data consistency refers to the validity and integrity of data representing real-world entities. It aims to detect inconsistencies or conflicts in the data. In a relational database, inconsistencies ma y exist within a single tuple, between dif-ferent tuples in the same table, and between tuples across different relations.
As an example, consider tuples t 1 ,t 2 and t 3 in Figure 1. There are conflicts within each of these tuples, as well as inc onsistencies between different tuples. (1) It is known that in the UK (when CC = 44), if the area code is 131, then the city should be Edinburgh ( EDI ). In tuple t 1 , however, CC =44and AC = 131, but city = EDI . That is, there exist inconsistencies between the values of the CC , AC and city attributes of t 1 ; similarly for tuple t 2 . These tell us that tuples t 1 and t 2 are erroneous. (2) Similarly, in the US ( CC = 01), if the area code is 908, the city should be Murray Hill ( MH ). Nevertheless, CC =01and AC = 908 in tuple t 3 , whereas its city is not MH . This indicates that tuple t 3 is not quite correct. (3) It is also known that in the UK , zip code uniquely determines street .Thatis, for any two tuples that refer to employees in the UK , if they share the same zip code, then they should have the same value in their street attributes. However, there are conflicts between t 1 and t 2 .
 Inconsistencies in the data are typically identified as violations of data dependen-cies ( a.k.a. integrity constraints [1]). Errors in a single relation can be detected by intrarelation constraints, while errors across different relations can be identi-fied by interrelation constraints.

Unfortunately, traditional dependencie s such as functional dependencies (FDs) and inclusion dependencies (INDs) fall s hort of catching inconsistencies com-monly found in real-life data, such as the errors in tuples t 1 ,t 2 and t 3 above. This is not surprising: the traditional dependencies were developed for schema design, rather than for improving data quality.

To remedy the limitations of traditional dependencies in data quality manage-ment, conditional functional dependencies (CFDs [23]) and conditional inclusion dependencies (CINDs [7]) have recently been proposed, which extend FDs and INDs, respectively, by specifying patte rns of semantically related data values. It has been shown that conditional dependencies are capable of capturing com-mon data inconsistencies that FDs and INDs fail to detect. For example, the inconsistencies in t 1  X  t 3 given above can be detected by CFDs.

A theory of conditional dependencies is already in place, as an extension of classical dependency theory. More specifically, the satisfiability problem, impli-cation problem, finite axio matizability and dependency propagation have been studied for conditional dependencies, from the complexity to inference systems to algorithms. We refer the interested reader to [7,6,23,31] for details. 2.2 Data Deduplication Data deduplication aims to identify tuples in one or more relations that refer to the same real-world entity. It is also known as entity resolution, duplicate detection, record matching, record linkage, merge-purge, database hardening, and object identification (for d ata with complex structures).

For example, consider tuples t 4 ,t 5 and t 6 in Figure 1. To answer query Q 2 given earlier, we want to know whether these tuples refer to the same employee. The answer is affirmative if, for instance, there exists another relation which indicates that Mary Smith and Mary Luth have the same email account.

The need for studying data deduplication is evident: for data cleaning it is needed to eliminate duplicate records; for data integration it is to collate and fuse information about the same entity from multiple data sources; and for mas-ter data management it helps us identify links between input tuples and master data. The need is also highlighted by payment card fraud, which cost $4.84 billion worldwide in 2006 [42]. In fraud detection it is a routine process to cross-check whether a credit card user is the legitim ate card holder. As another example, there was a recent effort to match records on licensed airplane pilots with records on individuals receiving disability benefits from the US Social Security Admin-istration. The finding was quite surprising: there were forty pilots whose records turned up in both databases (cf. [36]).

No matter how important it is, data deduplication is nontrivial. Indeed, tuples pertaining to the same object may have different representations in various data sources with different schemas. Moreover, the data sources may contain errors. These make it hard, if not impossible, to match a pair of tuples by simply checking whether their attributes pairwise equal. Worse still, it is often too costly to compare and examine every pair of tuples from large data sources.

Data deduplication is perhaps the most extensively studied data quality prob-lem. A variety of approaches have been proposed: probabilistic, learning-based, distance-based, and rule-based (se e [15,36,39] for recent surveys).

We promote a dependency-based approach for detecting duplicates, which al-lows us to capture the interaction between data deduplication and other aspects of data quality in a uniform logical framework. To this end a new form of de-pendencies, referred to as matching dependencies , has been proposed for data deduplication [18]. These dependencies help us decide what attributes to com-pare and how to compare these attributes when matching tuples. They allow us to deduce alternative attributes to inspect such that when matching cannot be done by comparing attributes that contain errors, we may still find matches by using other, more reliable attributes. In contrast to traditional dependencies that we are familiar with such as FDs and INDs, matching dependencies are dynamic constraints: they tell us what data have to be updated as a consequence of record matching. A dynamic constraint theory has been developed for matching dependencies, from deduction analysis to finite axiomatizability to inference algorithms (see [18] for details). 2.3 Data Accuracy Data accuracy refers to the closeness of values in a database to the true values of the entities that the data in the database represent. Consider, for example, a person schema: where each tuple specifies the name ( FN , LN ), age , height and marital status of a person. An instance of person is shown below, in which s 0 presents the  X  X rue X  information for Mike.

Given these, we can conclude that the values of s 1 [ age , height ] are more ac-curate than s 2 [ age , height ], as they are closer to the true values for Mike, while s [ FN , status ] are more accurate than s 1 [ FN , status ]. It is more challenging, how-ever, to determine the relative accuracy of s 1 and s 2 when the reference s 0 is unknown, as commonly found in practice. In this setting, it is still possible to find that for certain attributes, the values in one tuple are more accurate than another by an analysis of the semantics of the data, as follows. (1) Suppose that we know that Mike is still going to middle school. From this, we can conclude that s 1 [ age ] is more accurate than s 2 [ age ]. That is, s 1 [ age ]is closer to Mike X  X  true age value than s 2 [ age ], although Mike X  X  true age may not be known. Indeed, it is unlikely that students in a middle school are 45 years old. Moreover, from the age value ( s 1 [ age ]), we may deduce that s 2 [ status ]may be more accurate than s 1 [ status ]. (2) If we know that s 1 [ height ]and s 2 [ height ] were once correct, then we may conclude that s 1 [ height ] is more accurate than s 2 [ height ], since the height of a person is typically monotonically increa sing, at least when th e person is young. 2.4 Information Completeness Information completeness concerns whether our database has complete infor-mation to answer our queries. Given a database D and a query Q ,wewantto know whether Q can be completely answered by using only the data in D .If the information in D is incomplete, one can hardly expect its answer to Q to be accurate or even correct.

In practice our databases often do not have sufficient information for our tasks at hand. For instance, the value of t 1 [ phn ] in relation D 0 of Figure 1 is missing, as indicated by null . Worse still, tuples representing employees may also be missing from D 0 . As we have seen earlier, for query Q 1 given above, if some tuples representing employees in the NYC office are missing from D 0 ,then the answer to Q 1 in D 0 may not be correct. Incomplet e information introduces serious problems to enterprises: it routinely leads to misleading analytical results and biased decisions, and accounts for loss of revenues, credibility and customers.
How should we cope with incomplete information? Traditional work on infor-mation completeness adopts either the Closed World Assumption (CWA) or the Open World Assumption (OWA), stated as follows (see, e.g., [1]).  X  The CWA assumes that a database has collected all the tuples representing  X  The OWA assumes that in addition to missing values, some tuples represent-Database textbooks typically tell us that the world is closed: all the real-world entities of our interest are assumed alr eady represented by tuples residing in our database. After all, database theory is typically developed under the CWA, which is the basis of negation in our queries: a fact is viewed as false unless it can be proved from explicitly stated facts in our database.

Unfortunately, in practice one often finds that not only attribute values but also tuples are missing from our database. That is, the CWA is often too strong to hold in the real world. On the other hand, the OWA is too weak: under the OWA, we can expect few sensible queries to find complete answers.

The situation is not as bad as it seems. In the real world, neither the CWA nor the OWA is quite appropriate in emerging applications such as master data man-agement. In other words, real-life databases are neither entirely closed-world nor entirely open-world. Indeed, an enterprise nowadays typically maintains master data ( a.k.a. reference data ), a single repository of high-quality data that provides various applications with a synchronized, consistent view of the core business en-tities of the enterprise (see, e.g., [37], for master data management). The master data contain complete information about the enterprise in certain categories, e.g., employees, departments, projects, an d equipment. Master data can be re-garded as a closed-world database for the core business entities of the enterprise. Meanwhile a number of other databases may be in use in the enterprise for, e.g., sales, project control and customer support. On one hand, the information in these databases may not be complete, e.g., some sale transaction records may be missing. On the other hand, certain parts of the databases are constrained by the master data, e.g., employees and projects. In other words, these databases are partially closed . The good news is that we often find that partially closed databases have complete information to answer our queries at hand.

To rectify the limitations of the CWA and the OWA, a theory of relative information completeness h as been proposed [20,19], to specify partially closed databases w.r.t. available master data. In addition, several fundamental prob-lems in connection with relative compl eteness have been studied, to determine whether our database has complete information to answer our query, and when the database is incomplete for our tasks at hand, to decide what additional data should be included in our database to meet our requests. The complexity bounds of these problems have been established for various query languages. 2.5 Data Currency Data currency is also known as timeliness . It aims to identify the current values of entities represented by tuples in a dat abase that may contain stale data, and to answer queries with the current values.

The question of data currency would be trivial if all data values carried valid timestamps. In practice, however, one often finds that timestamps are unavail-able or imprecise [46]. Add to this the complication that data values are often copied or imported from other sources [12,13], which may not support a uniform scheme of timestamps. These make it challenging to identify the  X  X atest X  values of entities from the data in our database.

For example, recall query Q 3 and the employee relation D 0 of Figure 1 given earlier. Assume that tuples t 4 ,t 5 and t 6 are found pertaining to the same em-ployee Mary by data deduplication. As remarked earlier, in the absence of reli-able timestamps, the answer to Q 3 in D 0 does not tell us whether Mary X  X  current salary is 50k or 80k, and whether her current last name is Smith or Luth.
Not all is lost. In practice it is often possible to deduce currency orders from the semantics of the data, as illustrated below. (1) While we do not have timestamps associated with Mary X  X  salary, we know that the salary of each employee in the company does not decrease, as commonly and t 5 [ salary ]. Hence we may conclude that Mary X  X  current salary is 80k. (2) We know that the marital status can only change from single to married and from married to divorced; but not from married to single. In addition, employee tuples with the most current marital status also contain the most current last name. Therefore, t 6 [ LN ]= t 5 [ LN ] is more current than t 4 [ LN ]. From these we can infer that Mary X  X  current last name is Luth.
 A data currency model has recently been proposed in [26], which allows us to specify and deduce data currency when temporal information is only partly known or not available at all. Moreover, a notion of certain current query answers is introduced there, to answer queries wit h current values of entities derived from a possibly stale database. In this model the complexity bounds of fundamental problems associated with data currency have been established, for identifying the current value of an entity in a database in the absence of reliable timestamps, answering queries with current values , and for deciding what data should be imported from other sources in order t o answer query with current values. We encourage the interested reader to consu lt [26] for more detailed presentation. 2.6 Interactions between Data Quality Issues To improve data quality we often need to deal with each and every of the five central issues given above. Moreover, th ere issues interact with each other, as illustrated below.

As we have seen earlier, tuples t 1 ,t 2 and t 3 in the relation D 0 of Figure 1 are inconsistent. We show how data deduplication may help us resolve the incon-sistencies. Suppose that the company maintains a master relation for its offices, consisting of consistent, complete and current information about the address and phone number of each office. The master relation is specified by schema: and is denoted by D m , given as follows:
Then we may  X  X lean X  t 1 ,t 2 and t 3 by leveraging the interaction between data deduplication and data repairing processes (for data consistency) as follows. (1) If the values of the CC , AC attributes of these tuples are confirmed accurate, we can safely update their city attributes by letting t 1 [ city ]= t 2 [ city ]:=EDI, and t 3 [ city ] := MH, for reasons remark ed earlier. This yields t 1 ,t 2 and t 3 ,which differ from t 1 ,t 2 and t 3 , respectively, only in their city attribute values. (2) We know that if an employee tuple t  X  D 0 and an office tuple t m  X  D m agree on their address ( street , city , zip ), then the two tuples  X  X atch X , i.e., they refer to the same address. Hence, we can update t [ CC , AC , phn ] by taking the corre-differs from t 2 only in the street attribute. (3) We also know that for employee tuples t 1 and t 2 , if they have the same ad-dress, then they should have the same phn value. In light of this, we can augment t [ phn ] by letting t 1 [ phn ]:= t 2 [ phn ], and obtain a new tuple t 1 .
One can readily verify that t 1 ,t 2 and t 3 are consistent. In the process above, we  X  X nterleave X  operations for resolvi ng conflicts (steps 1 and 3) and operations for detecting duplicates (step 2). On one hand, conflict resolution helps dedu-plication: step 2 can be conducted only after t 2 [ city ] is corrected. On the other hand, deduplication also helps us resolve conflicts: t 1 [ phn ] is enriched only after t [ street ] is fixed via matching.

There are various interactions between data quality issues, including but not limited to the following.  X  Data currency can be improved if more temporal information can be obtained  X  To determine the current values of an entity, we need to identify tuples  X  To resolve conflicts in tuples represen ting an entity, we have to determine These suggest that a practical data quality management system should provide functionality to deal with each and every of five central issues given above, and moreover, leverage the interactions bet ween these issues to improve data quality. There has been preliminary work on the interaction between data deduplication and data repairing [27], as illustrated by the example above. Real-life data are often dirty, and dirty data are costly. In light of these, effective techniques have to be in place to improve the quality of our data. But how? Errors in Real-Life Data. To answer this question, we first classify errors typically found in the real world. There are two types of errors, namely, syntactic errors and semantic errors, as illustrated below. (1) Syntactic errors: violations of domain constraints by the values in our database. For example, name = 1.23 is a syntactic error if the domain of attribute name is string , whereas the value is numeric. Another example is age = 250 when the range of attribute age is [0, 120]. (2) Semantic errors: discrepancies be tween the values in our database and the true values of the entities that our data intend to represent. All the examples we have seen in the previous sections ar e semantic errors, related to data con-sistency, deduplication, accuracy, cu rrency and information completeness. While syntactic errors are relatively easy to catch, it is far more challenging to detect and correct semantic errors . Below we focus on semantic errors. Dependencies as Data Quality Rules. A central question concerns how we can tell whether our data have semantic errors, i.e., whether the data are dirty or clean? To this end, we need data qua lity rules to detect semantic errors in our data and fix those errors. But what data quality rules should we adopt? A natural idea is to use data dependencies ( a.k.a. integrity constraints). Dependency theory is almost as old as relational databases themselves. Since Codd [10] introduced functional dependencies, a variety of dependency lan-guages, defined as various classes of first-order ( FO ) logic sentences, have been developed. There are good reasons to believe that dependencies should play an important role in data quality management systems. Indeed, dependencies spec-ify a fundamental part of the semantics of data, in a declarative way, such that errors emerge as violations of the dependencies. Furthermore, inference systems, implication analysis and profiling methods for dependencies have shown promise as a systematic method for reasoning about the semantics of the data. These help us deduce and discover rules for improving data quality, among other things. In addition, all the five central aspects of data quality  X  data consistency, dedupli-cation, accuracy, currency and informat ion completeness  X  can be specified in terms of data dependencies. This allows us to treat various data quality issues in a uniform logical framework, in which we can study their interactions.
Nevertheless, to make practical use of dependencies in data quality manage-ment, classical dependency theory has to be extended. Traditional dependencies were developed to improve the quality of schema via normalization, and to opti-mize queries and preven t invalid updates (see, e.g., [1]). To improve the quality of the data , we need new forms of dependencies, such as conditional dependencies by specifying patterns of semantically related data values to capture data in-consistencies [23,7], matching dependencies by supporting similarity predicates to accommodate data errors in record matching [18], containment constraints by enforcing containment of certain information about core business entities in master data to reason about information completeness [20,19], and currency constraints by incorporating temporal orders to determine data currency [26].
Care must be taken when designing dependency languages for improving data quality. Among other things, we need to ba lance the tradeoff between expressive power and complexity, and revisit classical problems for dependencies such as the satisfiability, implication and finite axiomatizability analyses. Improve Data Quality with Rules. Afterwecomeupwiththe X  X ight X  X e-pendency languages for specifying data quality rules, the next question is how to effectively use these rules to improve d ata quality? In a nutshell, a rule-based data quality management system should provide the following functionality. Discovering Data Quality Rules . To use dependencies as data quality rules, it is necessary to have efficient techniques in place that can automatically discover dependencies from data. Indeed, it is unrealistic to rely solely on human experts to design data quality rules via an expensive and long manual process, or count on business rules that have been accumula ted. This suggests that we learn infor-mative and interesting data quality rules from (possibly dirty) data, and prune away trivial and insignificant rules based on a threshold specified by users.
More specifically, given a database instance D ,the profiling problem is to find a minimal cover of all dependencies ( e.g., CFDs, CINDs, matching dependen-cies) that hold on D , i.e., a non-redundant set of dependencies that is logically equivalent to the set of all dependencies that hold on D .

To find data quality rules, several algorithms have been developed for discov-ering CFDs [8,24,35] and matching dependencies [44].
 Validating Data Quality Rules .Agivenset  X  of dependencies, either automat-ically discovered or manually designed by domain experts, may be dirty itself. In light of this we have to identify  X  X onsistent X  dependencies from  X  , i.e., those rules that make sense, to be used as data quality rules. Moreover, we need to deduce new rules and to remove redundancies from  X  , via the implication or deduction analysis of those dependencies in  X  .

This problem is, however, nontrivial. It is already NP-complete to decide whether a given set of CFDs is satisfiable [23], and it becomes undecidable for CFDs and CINDs taken together [7]. Nevertheless, there has been an approxima-tion algorithm for extracting a set S of consistent rules from a set S of possibly inconsistent CFDs, while guaranteeing that S is within a constant bound of the maximum consistent subset of S (see [23] for details).
 Detecting Errors . After a validated set of data quality rules is identified, the next question concerns how to effectively cat ch errors in a database by using these rules. Given a set  X  of data quality rules and a database D ,wewantto detect inconsistencies in D , i.e., to find all tuples in D that violate some rule in  X  . When it comes to relative information co mpleteness, we want to decide whether D has complete information to answer an input query Q , among other things.
We have shown that for a centralized database D , given a set  X  of CFDs and CINDs, a fixed number of SQL queries can be automatically generated such that, when being evaluated against D , the queries return all and only those tuples in D that violate  X  [23]. That is, we can effectiv ely detect inconsistencies by leveraging existing facility of commercial relational database systems.
In practice a database is often fragmented, vertically or horizontally, and is distributed across different sites. In this setting, inconsistency detection becomes nontrivial: it necessarily requires certain data to be shipped from one site to an-other. In this setting, error detection with minimum data shipment or minimum response time becomes NP-complete [25], and the SQL-based techniques for de-tecting violations of conditional dependencies no longer work. Nevertheless, effec-tive batch algorithms [25] and incremental algorithms [29] have been developed for detecting errors in distributed data, w ith certain perfor mance guarantees. Data Imputation . After the errors are detected, we w ant to automatically localize the errors, fix the errors and make the data consistent. We also need to identify tuples that refer to the same entity, and for each entity, determine its latest and most accurate values from the data in our database. When some data are missing, we need to decide what data we should import and where to import from, so that we will have sufficient information for tasks at hand. As remarked earlier, these should be carried out by capitalizing on the interactions between processes for improving various aspects of data quality, as illustrated in Section 2.6.
As another example, let us consider data repairing for improving data consis-tency. Given a set  X  of dependencies and an instance D of a database schema R , it is to find a candidate repair of D , i.e., an instance D of R such that D satisfies  X  and D minimally differs from the original database D [2]. This is the method that us national statistical agencies, among others, have been practicing for decades for cleaning census data [33,36]. The data repairing problem is, nev-ertheless, highly nontrivial: it is NP-complete even when a fixed set of FDs or a fixed set of INDs is used as data quality rul es [5], even for centr alized databases. In light of these, several heuristic algor ithms have been developed, to effectively repair data by employing FDs and INDs [5], CFDs [11,45], CFDs and matching dependencies [27] as data quality rules. A functional prototype system [22] has also shown promises as an effective tool for repairing data in industry.
The data repairing methods mentioned above are essentially heuristic: while they improve the consistency of the data, they do not guarantee to find correct fixes for each error detected, i.e., they do not warrant a precision and recall of 100%. Worse still, they may introduce new errors when trying to repair the data. In light of these, they are not accurate enough to repair critical data such as medical data, in which a minor error may have disastrous consequences. This highlights the quest for effective methods to find certain fixes that are guaranteed correct. Such a method has recently be proposed in [28]. While it may not be able to fix all the errors in our database, it guarantees that whenever it updates a data item, it correctly fixes an error without introducing any new error. Data quality is widely perceived as one of the most important issues for informa-tion systems. In particular, the need for studying data quality is evident in big data management, for which two central i ssues of equivalent importance concern how to cope with the quantity of the data and the quality of the data.
The study of data quality management has raised as many questions as it has answered. It is a rich source of questions and vitality for database researchers. However, data quality research lags behind the demands in industry. A number of open questions need to be settled. Bel ow we address some of the open issues. Data Accuracy. Previous work on data quality has mostly focused on data consistency and data deduplication. In contrast, the study of data accuracy is still in its infancy. One of the most pre ssing issues concerns how to determine whether one value is more accurate than another in the absence of reference data. This calls for the development of models, quantitative metrics, and effective methods for determining the relative accuracy of data.
 Information Completeness. Our understanding of this issue is still rudimen-tary. While the theory of relative information completeness [20,19] circumvents the limitations of the CWA and the OWA and allows us to determine whether a database has complete information to a nswer our query, effective metrics and algorithms are not yet in place for us to conduct the evaluation in practice. Data Currency. The study of data currency has not yet reached the matu-rity. The results in this area are mostly theoretical: a model for specifying data currency, and complexity bounds for reasoning about the currency of data [26]. Among other things, effective methods for evaluating the currency of data in our databases and for deriving current values from stale data are yet to be developed. Interaction between Various Issues of Data Quality. As remarked ear-lier, there is an intimate connection between data repairing and data dedupli-cation [27]. Similarly, various interactions naturally arise when we attempt to improve the five central aspects of data q uality: information completeness is intimately related to data currency and consistency, and so is data currency to data consistency and accuracy. These in teractions require a full treatment. Repairing Distributed Data. Already hard to repair data in a centralized database, it is far more challenging to efficiently fix errors in distributed data. This is, however, a topic of great interest to the study of big data, which are typically partitioned and distributed. As remarked earlier, data quality is a cen-tral aspect of big data management, and hence, effective and scalable repairing methods for distributed data have to be studied.
 The Quality of Complex Data. Data quality issues are on an even larger scale for data on the Web, e.g., XML data and social graphs. Already hard for relational data, error detection and repairing are far more challenging for data with complex structures. In the context of XML, for example, the constraints involved and their interaction with XML Schema are far more intriguing than their relational counterparts, even for s tatic analysis [30,32], let alone for data repairing. In this setting data quality remains by and large unexplored. Another issue concerns object identification, i.e., to identify complex objects that refer to the same real-world entity, when the o bjects do not have a regular structure. This is critical not only to data quality, but also to Web page clustering, schema matching, pattern recognition, and spam detection, among other things. The World-Wide Web can be viewed as a gigantic information network, where web-pages are the nodes of the network, and links connecting those pages form an in-tertwined, gigantic network. However, due to the unstructured nature of such a network and semantic heterogeneity of web-links, it is difficult to mine interesting knowledge from such a network except for finding authoritative pages and hubs. Alternatively, one can also view that Web is a gigantic repository of multiple infor-mation sources, such as universities, gove rnments, companies, news, services, sales of commodities, and so on. An interesting problem is whether this view may pro-vide any new functions for web-based information services, and if it does, whether one can construct such kind of semi-stru ctured information n etworks automati-cally or semi-automatically from the Web, and whether one can use such new kind of networks to derive interesting new information and expand web services.
In this talk, we take this alternative view and examine the following issues: (1) what are the potential benefits if one can construct servi ce-oriented, semi-structured information networks from the World-Wide Web and perform data mining on them, (2) whether it is possible to construct such kind of service-oriented, semi-structur ed information networks from the World-Wide Web au-tomatically or semi-automatically, and (3) research problems for constructing and mining Web-Based, service-oriented, s emi-structured info rmation networks.
This view is motivated from our recent w ork on (1) mining semi-structured heterogeneous information networks, and (2) discovery of entity Web pages and their corresponding semantic structures from parallel path structures.
First, real world physical and abstract data objects are interconnected, forming gigantic, interconnected networks. By str ucturing these data objects into multi-ple types, such networks become semi-structured heterogeneous information net-works . Most real world applications that handle big data, including interconnected social media and social networks, scientific, engineering, or medical information systems, online e-commerce systems, and most database systems, can be struc-tured into heterogeneous information networks. For example, in a medical care network, objects of multiple types, such as patients, doctors, diseases, medica-tion, and links such as visits, diagnosis, and treatments are intertwined together, providing rich information and forming heterogeneous information networks. Ef-fective analysis of large-scale heterogeneous information networks poses an inter-esting but critical challenge. Our recent s tudies show that th e semi-structured heterogeneous information network model leverages the rich semantics of typed nodes and links in a network and can uncover surprisingly rich knowledge from interconnected data. This heterogeneou s network modeling will lead to the discov-ery of a set of new principles and methodo logies for mining interconnected data. The examples to be used in this discussion include (1) meta path-based similar-ity search, (2) rank-based clustering, (3) rank-based classification, (4) meta path-based link/relationship prediction, (5) relation strength-aware mining, as well as a few other recent developments.

Second, it is not easy to automatically or semi-automatically construct service-oriented, semi-structured, heterogeneous information networks from the WWW. However, with the enormous size and diversity of WWW, it is impossible to construct such information networks manually. Recentl y, there are progresses on finding entity-pages and mining web structural information using the structural and relational information on the Web. Specifically, given a Web site and an entity-page (e.g., department and faculty member homepage) it is possible to find all or almost all of the entity-pages of the same type (e.g., all faculty members in the department) by growing parallel paths through the web graph and DOM trees. By further developing such methodologies, it is possible that one can construct service-oriented, semi-structured, heterogeneous information networks from the WWW for many critical services. By integrating methodologies for construction and mining of such web-based information networks, the quality of both construction and mining of such information networks can be progressively and mutually enhanced.

Finally, we point out some open researc h problems and promising research directions and hope that the construc tion and mining of Web-based, service-oriented, semi-structured heterogeneous information networks will become an interesting frontier in the research into Web-aged information management sys-tems.
 Rakesh Agrawal, Sreenivas Gollapudi, Anitha Kannan, and Krishnaram Kenthapadi
 Commenced Publication in 1973 Founding and Former Series Editors: Gerhard Goos, Juris Hartmanis, and Jan van Leeuwen David Hutchison Takeo Kanade Josef Kittler Jon M. Kleinberg Alfred Kobsa Friedemann Mattern John C. Mitchell Moni Naor Oscar Nierstrasz C. Pandu Rangan Bernhard Steffen Madhu Sudan Demetri Terzopoulos Doug Tygar Gerhard Weikum Volume Editors Haralambos Mouratidis University of East London School of Computing, IT and Engineering Docklands Campus, 4/6 University Way, E16 2RD London, UK E-mail: H.Mouratidis@uel.ac.uk Colette Rolland Universit X  Paris1 Panth X on Sorbonne
CRI 90 Rue de Tolbiac, 75013 Paris, France E-mail: rolland@univ-paris1.fr ISSN 0302-9743 e-ISSN 1611-3349 ISBN 978-3-642-21639-8 e-ISBN 978-3-642-21640-4 DOI 10.1007/978-3-642-21640-4 Springer Heidelberg Dordrecht London New York Library of Congress Control Number: 2011928907 CR Subject Classification (1998): H.4, H.3, D.2, C.2, J.1, I.2
LNCS Sublibrary: SL 3  X  Information Systems and Application, incl. Internet/Web and HCI A warm welcome to the proceedings of th e 23rd International Conference on Advanced Information Systems Engineering (CAiSE 2011)! The CAiSE series of conferences started in 1989 with the objective to provide a forum for the exchange of experience, research result s, ideas and prototypes in the field of in-formation systems engineering. Twenty-two years later, CAiSE has established itself as a leading venue in the information systems area for presenting and ex-changing results of emerging methods and technologies that facilitate innovation and create business opportunities.
 dition. The theme of CAiSE 2011 was  X  X nformation Systems Olympics: Infor-mation Systems in a Diverse World. X  This year X  X  CAiSE conference theme was linked to the coming London Olympic and Paralympic Games 2012, two inter-national multi-sport events that bring together athletes from all continents to celebrate sporting excellence but also hu man diversity. Diversity is an impor-tant concept for modern information systems. Information systems are diverse by nature ranging from basic systems to complex ones and from small to large. The process of constructing such systems is also diverse ranging from ad-hoc methods to structured and formal methods. Diversity is also present among in-formation systems developers, from novice to experienced. Moreover, the wide acceptance of information systems and their usage in almost every aspect of hu-man life has also introduced diversity among users. Users are both novice and experienced and they demonstrate differe nces related to race, ethnicity, gender, socio-economic status, age, physical abilities, religious beliefs, and so on. It is therefore the responsibility of the information systems engineering community to engineer information systems that operate in such a diverse world. CAiSE conference series. Most of the submissions came from Germany, Spain, Italy, France and China. Following an extensive review process, which included a Program Committee/Program Board meeting during February 13 X 14, 2011 in London, 42 submissions were accepted a s full papers and 5 as short papers. Accepted papers addressed a large varie ty of issues related to the conference and were organized into ten themes: Requirements, Adaptation and Evolution, Model Transformation, Conceptual Design, Domain-Specific Languages, Case Studies and Experiences, Mining and Matching, Service and Management, Val-idation and Quality, Business Process Modeling. The program of the conference was also supplemented by a number of tutorials, 11 workshops, a Doctoral Con-sortium, the CAiSE Forum, and two working conferences. Two keynote speeches were delivered as part of the conference program. Anthony Finkelstein talked about  X  X pen Challenges at the Boundaries Software Engineering and Informa-tion Systems, X  while Dimitrios Beis talked about  X  X nformation Systems for the Olympics Games. X  Moreover, a panel di scussed issues related to  X  X reen and Sustainable Information Systems. X  would not be possible without the valuable help and time of a large number of people. As editors of this volume, we would like to express our gratitude to the Program Committee members, additional reviewers and the Program Board members for their valuable support in selecting the papers for the scientific pro-gram of the conference; to the authors of the papers for sending their work to CAiSE; to the presenters of the papers; and to the participants of the conference for their contribution. We also thank our sponsors and the General Chair and Chairs of the various CAiSE 2011 committees for their assistance in creating an exciting scientific program. We would also like to thank the local Organiz-ing Committee at the University of East London for their hospitality and the organization of the social events of the conference.
 March 2011 Colette Rolland Arne S X lvberg Norwegian Universit y of Science and Technology, Norway Janis Bubenko Jr. Royal Institute of Technology, Sweden Colette Rolland Universit  X  e Paris 1 Panth  X  eon Sorbonne, France Pericles Loucopoulos Loughborough University, UK Haralambos Mouratidis University of East London, UK Colette Rolland Universit  X  e Paris 1 Panth  X  eon Sorbonne, France Elias Pimenidis University of East London, UK Miltos Petridis University of Greenwich, UK Oscar Pastor Valencia University of Technology, Spain Camille Salinesi Universit  X  e Paris 1 Panth  X  eon Sorbonne, France Selmin Nurcan Universit  X  e Paris 1 Panth  X  eon Sorbonne, France Barbara Pernici Politecnico di Milano, Italy Michel L  X  eonard Universit  X  edeGen` eve, Switzerland Bernhard Thalheim Christian Albrechts University Kiel, Germany Cornelia Boldyreff University of East London, UK Jolita Ralyt  X  eUniversit  X  edeGen` eve, Switzerland David Preston University of East London, UK Rebecca Deneckere Universit  X  e Paris 1 Panth  X  eon Sorbonne, France Jaelson Castro Universidade Federal de Pernambuco, Brazil Leszek Maciaszek Macquari e University, Australia Kecheng Liu University of Reading, UK Keng Siau University of Nebraska-Lincoln, USA Mohammad Dastbaz University of East London, UK Michalis Pavlidis University of East London, UK Sambhu Singh University of East London, UK Marco Bajec, Slovenia Nacer Boudjilida, France Eric Dubois, Luxembourg Xavier Franch, Spain Marina Jirotka, UK Moira Norrie, Switzerland Wil van der Aalst, The Netherlands Peggy Aravantinou, Greece P  X  ar  X  Agerfalk, Sweden Hans Akkermans, The Netherlands Antonia Albani, The Netherlands Daniel Amyot, Canada Paris Avgeriou, The Netherlands Luciano Baresi, Italy Ahmad Barfourosh, Iran Zohra Bellahsene, France Valeria De Antonellis, Italy Joerg Evermann, Canada Jo  X  ao Falc  X  ao a Cunha, Portugal Paolo Falcarin, UK Mariagrazia Fugini, Italy Paolo Giorgini, Italy Stefanos Gritzalis, Greece Remigijus Gustas, Sweden Terry Halpin, USA Willem-Jan van den Heuvel, The Netherlands Patrick Heymans, Belgium Jane Huang, USA Matthias Jarke, Germany Paul Johannesson, Sweden Christos Kalloniatis, Greece Dimitris Karagiannis, Austria Panagiotis Karras, Singapore Evangelia Kavakli, Greece Zoubida Kedad, France Marite Kirikova, Latvia Naoufel Kraiem, Tunisia John Krogstie, Norway Wilfried Lemahieu, Belgium Michel Leonard, Switzerland Panos Louridas, Greece Kalle Lyytinen, USA Raimundas Matulevicius, Estonia Jan Mendling, Germany Isabelle Mirbel, France John Mylopoulos, Canada Selmin Nuncan, France Andreas Oberweis, Germany Antoni Olive, Spain Andreas Opdahl, Norway Mike Papazoglou, The Netherlands Jeffrey Parsons, Canada Oscar Pastor, Spain Alberto Abell  X  o David Aguilera-Moncusi Saeed Ahmadi-Behnam Naved Ahmed Reza Akbarinia Fernanda Alencar Raian Ali Christos Anagnostopoulos Birger Andersson Vasilios Andrikopoulos Ion Androutsopoulos Luca Ardito George Athanasopoulos Ahmed Awad Daniele Barone Saeed Ahmadi Behnam Maria Bergholtz Maxime Bernaert Devis Bianchini Riccardo Bonazzi Boris Brandherm Glenn J. Browne Stephan Buchwald Andrea Capiluppi Amit Chopra Remi Coletta Ajantha Dahanayake Fabiano Dalpiaz R  X  ebecca Deneck` ere Olfa Djebbi Vicky Dritsou Fabien Duchateau Rami Eid-Sabbagh Golnaz Elahi Amal Elgammal Thibault Estier Alfio Ferrara Kunihiko Fujita Matthias Galster Dimitris Gavrilis Andrew Gemino Sepideh Ghanavati Emmanuel Giakoumakis Michael Parkin Adamantia Pateli Michalis Pavlidis Raul Mazo Pena Jo  X  ao Pimentel Eric Platon Viara Popova Alireza Pourshahid Ruediger Pryss Ricardo Ramos Jan Recker Evangelos Rekleitis Oscar Romero Christoph Rosenkranz Ulysse Rosselet Khalid Saleem Camille Salinesi Emanuel Santos Sihem Ben Sassi Ricardo Seguel Azalia Shamsaei Omri Shiv Patricio Silva David Simms Jonas Sj  X  ostr  X  om Aidas Smaizys Sergey Smirnov Ten Open Challenges at the Boundaries of Software Engineering and Information Systems (Abstract) .................................... 1 Total Integration: The Case of Information Systems for Olympic Games (Abstract) ................................................ 2 Requirements Management with Semantic Technology: An Empirical Study on Automated Requirements Categorization and Conflict Analysis ........................................................ 3 S
C: Using Service Discovery to Support Requirements Elicitation in the ERP Domain ................................................ 18 Requirements Engineering for Self-Adaptive Systems: Core Ontology and Problem Statement ........................................... 33 A Fuzzy Service Adaptation Based on QoS Satisfaction ............... 48 Dealing with Known Unknowns: Towards a Game-Theoretic Foundation for Software Requirement Evolution ................................ 62 Goal-Based Behavioral Customization of Information Systems ......... 77 From Requirements to Models: Feedback Generation as a Result of Formalization ................................................... 93 A Web Usability Evaluation Process for Model-Driven Web Development .................................................... 108 A Trace Metamodel Proposal Based on the Model Driven Architecture Framework for the Traceability of User Requirements in Data Warehouses ..................................................... 123 Ontological Foundations for Conceptual Part-Whole Relations: The Case of Collectives and Their Parts ............................ 138 Product-Based Workflow Design for Monitoring of Collaborative Business Processes ............................................... 154 Modeling Design Patterns with Description Logics: A Case Study ...... 169 Interactively Eliciting Database Constraints and Dependencies ......... 184 A Conceptual Model for Integrated Governance, Risk and Compliance ..................................................... 199 Using Synchronised Tag Clouds for Browsing Data Collections ......... 214 Revisiting Naur X  X  Programming as Theory Building for Enterprise Architecture Modelling ........................................... 229 A DSL for Corporate Wiki Initialization ............................ 237 The REA-DSL: A Domain Specific Modeling Language for Business Models ......................................................... 252 A Foundational Approach for Managing Process Variability ........... 267 Tangible Media in Process Modeling  X  A Controlled Experiment ....... 283 Experiences of Using Different Com munication Styles in Business Process Support Systems with the Shared Spaces Architecture ......... 299 What Methodology Attributes Are Critical for Potential Users? Understanding the Effect of Human Needs .......................... 314 Exploratory Case Study Research on SOA Investment Decision Processes in Austria .............................................. 329 A Metamodelling Approach for i * Model Translations ................ 337 Automatic Generation of a Data-Centered View of Business Processes ....................................................... 352 Connecting Security Requirements Analysis and Secure Design Using Patterns and UMLsec ............................................ 367 Transforming Enterprise Architecture Models: An Artificial Ontology View ........................................................... 383 Handling Concept Drift in Process Mining .......................... 391 An Iterative Approach for Business Process Template Synthesis from Compliance Rules ................................................ 406 A Design of Business-Technology Alignment Consulting Framework .... 422 ONTECTAS: Bridging the Gap between Collaborative Tagging Systems and Structured Data ............................................. 436 Cognitive Complexity in Business Process Modeling .................. 452 Human-Centered Proces s Engineering Based on Content Analysis and Process View Aggregation ......................................... 467 Process Model Generation from Natural Language Text ............... 482 A Semantic Approach for Business Process Model Abstraction ......... 497 On the Automatic Labeling of Process Models ....................... 512 Pattern-Based Modeling and Formalizing of Business Process Quality Constraints ..................................................... 521 Quality Evaluation and Improvement Framework for Database Schemas -Using Defect Taxonomies ................................ 536 Validation of Families of Business Processes ......................... 551 Using SOA Governance Design Methodologies to Augment Enterprise Service Descriptions .............................................. 566 Management Services  X  A Framework for Design ..................... 582 Bottom-Up Fault Management in Composite Web Services ............ 597 Understanding the Diversity of Services Based on Users X  Identities ..... 612 Request/Response Aspects for Web Services ......................... 627 Using Graph Aggregation for Service Interaction Message Correlation ...................................................... 642 Supporting Dynamic, People-Driven Processes through Self-learning of Message Flows ................................................... 657 Business Process Service Oriented Methodology (BPSOM) with Service Generation in SoaML ............................................. 672 Panel on Green and Sustainable IS (Abstract) ....................... 681 Author Index .................................................. 683
 Commenced Publication in 1973 Founding and Former Series Editors: Gerhard Goos, Juris Hartmanis, and Jan van Leeuwen David Hutchison Takeo Kanade Josef Kittler Jon M. Kleinberg Alfred Kobsa Friedemann Mattern John C. Mitchell Moni Naor Oscar Nierstrasz C. Pandu Rangan Bernhard Steffen Madhu Sudan Demetri Terzopoulos Doug Tygar Gerhard Weikum Volume Editors Haixun Wang Microsoft Research Asia, Beijing, 100190, China E-mail: haixunw@microsoft.com Shijun Li Wuhan University, Hubei 430072, China E-mail: shjli@whu.edu.cn Satoshi Oyama Hokkaido University, Sapporo, Hokkaido 060-0814, Japan E-mail: oyama@ist.hokudai.ac.jp Xiaohua Hu Drexel University, Philadelphia, PA 19104, USA E-mail: xh29@drexel.edu Tieyun Qian Wuhan University, Hubei, 430072, China E-mail: qty@whu.edu.cn ISSN 0302-9743 e-ISSN 1611-3349 ISBN 978-3-642-23534-4 e-ISBN 978-3-642-23535-1 DOI 10.1007/978-3-642-23535-1 Springer Heidelberg Dordrecht London New York Library of Congress Control Number: 2011934878 CR Subject Classification (1998): H.3, H.4, I.2, C.2, H.2, H.5
LNCS Sublibrary: SL 3  X  Information Systems and Application, incl. Internet/Web and HCI WAIM has been a leading international conference on research, development, and applications of Web technologies, database systems, information manage-ment and software engineering. WAIM is focused in the Asia-Pacific region, and previous WAIM conferences were held in Shanghai (2000), Xi X  X n (2001), Beijing (2002), Chengdu (2003), Dalian (2004), Hangzhou (2005), Hong Kong (2006), Huangshan (2007), Zhangjiajie (2008), Suzhou (2009), and Jiuzhaigou (2010). As the 12 th event in the increasingly popular series, WAIM 2011 attracted out-standing researchers from all over the world to Wuhan, China. In particular, this year WAIM and Microsoft Research Asia jointly sponsored a database summer school, which was collocated with WAIM.
 who chose WAIM as a venue for their publications. Out of 181 submitted papers from various countries and regions, we selected 53 full papers for publication. The acceptance rate for regular full papers i s 29%. The contributed papers addressed a wide scope of issues in the fields of Web-age information management and advanced applications, including Web data mining, knowledge discovery from streaming data, query processing, multidimensional data analysis, data manage-ment support to advanced applications, etc.
 the Program Committee Chairs, Program Committee members and the reviewers for their invaluable efforts. Special thanks to the local Organizing Committee headed by Liang Hong and Ming Zhong. Many thanks also go to the Workshop Co-chairs (Chengfei Liu and Liwei Wang), Finance Chairs (Howard Leung and Xuhui Li), and Publicity Co-chairs (Weiyi Meng, Lei Chen and Guohui Li). We also thank the conference sponsors for their generous support.
 September 2011 Haixun Wang Yanxiang He Wuhan University, China Changjie Tang Sichuan University, China Katsumi Tanaka Kyoto University, Japan Zhiyong Peng Wuhan University, China Haixun Wang Microsoft Research Asia Shijun Li Wuhan University, China Satoshi Oyama Hokkaido University, Japan Chengfei Liu Swinburne University of Technology, Australia Liwei Wang Wuhan University, China Mukesh Mohania IBM, India Xiaoxin Wu Huawei, China Xiaohua Hu Drexel University, USA Tieyun Qian Wuhan University, China Weiyi Meng Binghamton University, USA Lei Chen Hong Kong University of Science and Guohui Li Huazhong University of Science and Liang Hong Wuhan University, China Ming Zhong Wuhan University, China Howard Leung City University of Hong Kong Xuhui Li Wuhan University, China Qing Li City University of Hong Kong Xiaofeng Meng Renmin University, China Haixun Wang Microsoft Research Asia Shijun Li Wuhan University, China Satoshi Oyama Hokkaido University, Japan Bin Cui Peking University, China Danushka Bollegala University of Tokyo, Japan Jianyong Wang Tsinghua University, China Kenny Zhu Shanghai Jiao Tong University, China Seung-won Hwang Pohang University of Science and Technology, Wei Wang University of New South Wales, Australia Xiaochun Yang Northeastern University, China Alfredo Cuzzocrea Univer sity of Calabria, Italy Anthony Tung National University of Singapore, Singapore Aoying Zhou East China Normal University, China Baihua Zheng Singapore Management University, Singapore Bin Cui Peking University, China Bingsheng He Chinese University of Hong Kong, Hong Kong Chengkai Li University of Texas at Arlington, USA Danushka Bollegala The University of Tokyo, Japan David Cheung The University of Hong Kong Donghui Zhang Microsoft Jim Gray Systems Lab, USA Fei Xu Microsoft Feifei Li Florida State University, USA Ge Yu Northeastern University, China Guoren Wang Northeastern University, China Guozhu Dong Wright State University, USA Heng Tao Shen University of Queensland, Australia Hiroaki Ohshima Kyoto University, Japan Hong Chen Chinese University of Hong Kong, Hongzhi Wang Harbin Industry University, China Hua Wang University of Southern Queensland, Huan Liu Arizona State University, USA Hwanjo Yu Pohang University of Science and Jaewoo Kang Korea University Jeffrey Yu Chinese University of Hong Kong Jianliang Xu Hong Kong Baptist University, Hong Kong Jianyong Wang Tsinghua University, China Jie Tang Tsinghua University, China Jimmy Huang York University, Canada Jun Gao Peking University, China Ke Wang Simon Fraser University, Canada Kenny Zhu Shanghai Jiao Tong University, China Lei Chen Hong Kong University of Science and Lei Duan Sichuan University, China Lin Li Wuhan University of Technology, China Lei Zou Peking University, China Lipeow Lim University of Hawaii at Manoa, USA Min Wang HP Lab China Nick Koudas University of Toronto, Canada Ning Jing National University of Defense Peiquan Jin University of Science and Technology Peng Wang Fudan University, China Philip Yu University of Illinois at Chicago, USA Qiang Ma Kyoto University, Japan Qiang Zhu University of Michigan at Dearborn, USA Raymond Ng University of British Columbia, Canada Ruili Wang Massey University, New Zealand Ruoming Jin Kent State University, USA Seung-won Hwang Pohang University of Science and Shinsuke Nakajima Kyoto Sangyo University, Japan Shuai Ma University of Edinburgh, UK Shuigeng Zhou Fudan University, China Shuming Shi Microsoft Research Asia Tao Li Florida International University, USA Tengjiao Wang Peking University, China Ting Yu North Carolina State University, USA Toshiyuki Amagasa University of Tsukuba, Japan Wei Wang University of New South Wales, Australia Weiyi Meng State University of New York at Weizhu Chen Microsoft Research Asia Xiaochun Yang Northeastern University, China Xiaofeng Meng Renmin University of China, China Xiaoyong Du Renmin University of China, China Xin (Luna) Dong AT&amp;T Research, USA Xingquan Hill Zhu University of Technology, Sydney Xintao Wu University of North Carolina at Charlotte, Xu Yu Teradata, USA Xuanjing Huang Fudan Universtiy, China Xuemin Lin University of New South Wales, Australia Yan Jia National University of Defense Yanghua Xiao Fudan Universtiy, China Yaokai Feng Kyushu University, Japan Yi Cai City University of Hong Kong, Hong Kong Yi Ke Hong Kong University of Science and Yingzi Jin University of Tokyo, Japan Yoshiharu Ishikawa Nagoya University, Japan Yuchen Fu Soochow University, China Yunjun Gao Zhejiang University, China Yuqing Wu Indiana University at Bloomington, USA Zhanhuai Li Northwestern Polytechnical University, China Zhongfei Zhang State University of New York at Zhongyuan Wang Microsoft Research Asia Analytics for Info-plosion Including Information Diffusion Studies for the 3.11 Disaster ................................................. 1 Using the Web for Collaborative Language Learning and Teaching ..... 2 Data-Driven Modeling and Analysis of Online Social Networks ......... 3 Efficient Filter Algorithms for Reve rse k-Nearest Neighbor Query ...... 18 Keyword Query Cleaning with Query Logs .......................... 31 A Self-adaptive Cross-Domain Query Approach on the Deep Web ...... 43 SPARQL Query Answering with RDFS Reasoning on Correlated Probabilistic Data ............................................... 56 Probabilistic Threshold Join over Distributed Uncertain Data ......... 68 Bayesian Classifiers for Positive Unlabeled Learning .................. 81 Measuring Social Tag Confidence: Is It a Good or Bad Tag? ........... 94 A New Vector Space Model Exploiting Semantic Correlations of Social Annotations for Web Page Clustering ............................... 106 A Generalization Based Approach for Anonymizing Weighted Social Network Graphs ................................................. 118 Incremental Reasoning ov er Multiple Ontologies ..................... 131 General-Purpose Ontology Enrichment from the WWW .............. 144 QuerySem: Deriving Query Semantics Based on Multiple Ontologies .... 157 Getting Critical Categories of a Data Set ........................... 169 MFCluster: Mining Maximal Fault-Tolerant Constant Row Biclusters in Microarray Dataset ............................................ 181 Expansion Finding for Given Acronyms Using Conditional Random Fields .......................................................... 191 Leveraging Communication Information among Readers for RFID Data Cleaning ........................................................ 201 Web Article Quality Assessment in Multi-dimensional Space ........... 214 DRScribe: An Improved Topic-Based Publish-Subscribe System with Dynamic Routing ................................................ 226 An Efficient Quad-Tree Based Index Structure for Cloud Data Management .................................................... 238 Efficient Duplicate Detection on Cloud Using a New Signature Scheme ......................................................... 251 A Secure and Efficient Role-Based Acces s Policy towards Cryptographic Cloud Storage ................................................... 264 Tagging Image by Exploring Weighted Correlation between Visual Features and Tags ............................................... 277 Credibility-Oriented Ranking of Multimedia News Based on a Material-Opinion Model .......................................... 290 Actions in Still Web Images: Visualization, Detection and Retrieval .... 302 Social Analytics for Personalization in Work Environments ............ 314 Finding Appropriate Experts for Collaboration ...................... 327 Location Privacy Protection in t he Presence of Users X  Preferences ...... 340 Layered Graph Data Model for Data Management of DataSpace Support Platform ................................................ 353 A Study of RDB-Based RDF Data Management Techniques ........... 366 Requirement-Based Query and Upd ate Scheduling in Real-Time Data Warehouses ..................................................... 379 Answering Subgraph Queries over Large Graphs ..................... 390 Finding Relevant Papers Based on Citation Relations ................. 403 ASAP: Towards A ccurate, S table and A ccelerative P enetrating-Rank Estimation on Large Graphs ....................................... 415 Efficient Name Disambiguation in Digital Libraries ................... 430 A Classification Framework for Disambiguating Web People Search Result Using Feedback ............................................ 442 Incorporating User Feedback into Name Disambiguation of Scientific Cooperation Network ............................................. 454 Multi-core vs. I/O Wall: The Approaches to Conquer and Cooperate ... 467 W-Order Scan: Minimizing Cache Pollution by Application Software Level Cache Management for MMDB ............................... 480 Index Structure for Cross-Class Query in Object Deputy Database ..... 493 Ensemble Pruning via Base-Classifier Replacement ................... 505 Efficient Approximate Similarity Search Using Random Projection Learning ........................................................ 517 Informed Prediction with Increm ental Core-Based Friend Cycle Discovering ..................................................... 530 Early Prediction of Temporal S equences Based on Information Transfer ........................................................ 542 Mining Event Temporal Boundaries from News Corpora through Evolution Phase Discovery ........................................ 554 Event Detection over Live and Archived Streams ..................... 566 Renda-RX: A Benchmark for Evaluating XML-Relational Database System ......................................................... 578 Energy-Conserving Fragment Met hods for Skewed XML Data Access in Push-Based Broadcast ......................................... 590 Evaluating Probabilistic Spatial-Range Closest Pairs Queries over Uncertain Objects ............................................... 602 Synthesizing Routes for Low Sampling Trajectories with Absorbing Markov Chains .................................................. 614 Approximate Continuous K -Nearest Neighbor Queries for Uncertain Objects in Road Networks ........................................ 627 Parallel Detection of Tempora l Events from Streaming Data ........... 639 Towards Effective Event Detectio n, Tracking and Summarization on Microblog Data .................................................. 652 Author Index .................................................. 665 Internet technologies and online social networks are changing the nature of social interactions. There exist incredible opportunities for learning, social connection, and individual entertainment and enhancement in a wide variety of forms. Peo-ple now have ready access to almost inconcei vably vast information repositories that are increasingly portable, accessible, and interactive in both delivery and formation. Basic human activities have changed as a result, and new possibilities have emerged. For instance, the process b y which people locate, organize, and coordinate groups of individuals with shared interests, the number and nature of information and news sources available, and the ability to solicit and share opinions and ideas across myriad topics have all undergone dramatic change as a result of interconnected digital media . Indeed, recent evidence indicates that 45% of users in the U.S. say that the Internet played a crucial or important role in at least one major decision in their lives in the last two years, such as attain-ing additional career training, helping themselves or someone else with a major illness or medical condition, or making a major investment or financial deci-sion [HR06]. The significant role of online social networks in human interactions motivates our goals of generating a greate r understanding of social interactions in online networks through data analysis, the development of reliable models that can predict outcomes of social processes, and ultimately the creation of applications that can shape the outcome of these processes.

This work centers on social processes related to the diffusion of information and opinions in online social networks. We are interested in questions relating to how a single news item or idea propagates through a social network. We also note that such  X  X nformation campaigns X  do not exist in isolation. So while it is important to understand the dynamics of individual campaigns, we also study the interplay of the many information diffusion processes that take place simultaneously in a network and the relative importance of different information topics or trends over multiple geographical and temporal resolutions. Diffusion of Information and Opinions. Given the notable outcomes and the potential applications of information diffusion over online social networks, it is an important research goal to increase our understanding of information dif-fusion processes. It is also desirable to understand how these processes can be modified to achieve desired objectives. We describe and refine models of infor-mation diffusion for online social networks based on analysis of real-world data sets. The scalable, flexible identification of influential users or opinion leaders for specific topics is crucial to ensuring that information reliably reaches a large audience, and so we propose to develop algorithms for finding influential users in this data-driven framework. It is equally important to identify techniques that can slow or prevent the spread of misinformation, and we create models and algorithms to address this question. We are also interested in the processes by which a social group forms opinions about an idea or product. We create models that accurately capture the opinion formation process in online social networks and develop scalable algorithms and techniques for external intervention that can alter those opinions.
 Information Trend Analysis. In light of an online environment where there is an increasing number of sources of information, understanding information trends is critical. For example, data showing that there is sustained and substantial interest in a particular political issue locally, but not regionally, nationally, or internationally, can signal opportunities for political organization around that topic, political party proclivities in a geographic area, or even where best to focus fund-raising efforts. The relative trendiness of a message or topic, such as when interest in it suddenly spikes, can be more important than the overall pop-ularity of a topic. Finally, underlying trends that illustrate relationships among entities can be used to highlight inform ation that is otherwise obscure. For ex-ample, strangers independently confirming the importance of a certain piece of information (such as a user X  X  feedback rating on eBay) are better indicators of its veracity than are a tightly-connected cluster of information sources reporting the same information in common. Trending tools can thus help to discern these and other distinctions. We are interested in developing tools to discover many types of trends at multiple spatio-temporal resolutions.

In the following sections, we detail our research questions, propose solution techniques, and highlight open problems for further investigation. Social networks have already emerged as a significant medium for the widespread distribution of news and instructions in mass convergence events such as the 2008 U.S. Presidential Election [HP09], the 2009 Presidential election in Iran [Gro09], and emergencies like the landfall of Hurricanes Ike and Gustav in the fall of 2008 [HP09]. Due to the open nature of social networks, it is not only possible to spread factual information, but also to create misinformation campaigns that can discourage or disrupt correct response in these situations. For instance, Twitter has served as forum for spreading both useful information and false rumors about the H1N1 pandemic which altered users X  attitudes about the safety and efficacy of the flu vaccine [Far09].

Given the notable outcomes and the potential applications of information dif-fusion over online social networks, it is an important research goal to increase our understanding of information diffusion processes and to study how these processes can be modified to achieve desired objectives. We have developed pre-liminary models of information and opinion diffusion for online social networks. First, we address the question of how t o formalize a loosely specified model of social diffusion based on data analysis in order to adapt this model to social interactions in the blogosphere. We then consider the problem of limiting the reach of an information campaign and highlight preliminary results and proposed extensions. 2.1 Influence Maximization: The Law of the Few The identification of influential users or opinion leaders in a social network is a problem that has received a significant amount of attention in recent research [KKT03, CYY09, LKG + 07, KSNM09, WCSX10]. Informally, we define an infor-mation campaign as the process by which a news item or idea spreads from the initiators of the campaign, i.e. the users who first learn the news, throughout the social network. This initial set of users is denoted by the set A ,andthesetof users who eventually r eceive this news is the influence set of A , denoted IF ( A ). In the influence maximization problem , given a model of how information dif-fuses in a social network, the objective is to select a set of users A of size k who are to be the initial recipients of the information (through some offline means), so that the size of IF ( A ) is maximized [DR01, RD02].

With an efficient, robust solution to this problem, it would be possible to en-sure the widespread dissemination of important information in a social network. Early works relied on heuristics such a s node degree and distance centrality [WF94] to select the set A . More recently, several inves tigators have considered probabilistic models of information diffusion such as the Independent Cascade [GLM01] and Linear Threshold [Gra78]. The problem of finding the optimal initiator set in this model is NP-hard, but there is a polynomial-time greedy al-gorithm that yields a result that is within 1  X  1 /e of optimal [KKT03]. Work has been done on improving the performance of greedy algorithms for influence max-imization [CYY09, LKG + 07, KSNM09, WCSX10], but the scalability of these algorithms, in the context of social networks spanning millions of users, remains a significant challenge.

We summarize our recent work [BAEA10] on the influential users problem using a different model of diffusion base d on the theories introduced in the popular book  X  X he Tipping Point X  by Malcolm Gladwell [Gla02]. The main idea of  X  X he Tipping Point X  is the crucial roles of three types of  X  X ascinating X  people that the author calls mavens, connectors and salesmen on the effectiveness of an information campaign. These people are claimed to  X  X lay a critical role in the word-of-mouth epidemics that dictate our tastes, trends and fashions X . We study those three types of people or actors and a fourth type of interesting actor that we call a translator .

The first type of actor introduced by Gl adwell is the connector. In terms of a social network graph, we define a connector to be a node that has high de-gree centrality. The second type of important actor in information propagation is the maven. The word  X  X aven X  comes from Yiddish and means one who ac-cumulates knowledge. Gladwell lists three important characteristics for mavens: 1) they seek new knowledge, 2) they sha re the knowledge they acquire with others and 3) an individual hearing something from a maven is very likely to believe the correctness and importance of this piece of information. Translat-ing those features into graph theory, we define mavens to be nodes that start a large number of cascades (they are the original source of new information) and have high influence on their neighbors. The third kind of actor that Glad-well introduces is the salesman, a perso nwithhighcharismawhocansellideas to almost anyone since he never gives up. We define a salesman to be a node that has a large number of trials to activate its neighbors for cascades that the node itself is a part of. We also study another class of actors referred to as translators . These actors act as bridges or translators among different commu-nities and therefore have the power of changing the context in which to present an idea. In order to identify the translators in the blogosphere, first we need to detect the communities. Different fro m many community detection research [Cla05, PSL90, New03, New06, GL08, BGMI05, ZWZ07, Gre07], we define com-munities based on the existence of flow of influence between nodes rather than relying solely on the structure of the graph. Using this definition, we detect overlapping communities using the algorithm presented in [BGMI05] and define translators as the nodes that belong to the most number of communities.
Weblogs have become a predominant way of sharing data online. The blogo-sphere has considerable influence on how people make decisions in areas such as politics or technology [AG05]. We used the August-October 2008 Memetracker data that contains timestamped phrase and link information for news media articles and blog posts from different blogs and news websites. The data set consists of 53,744,349 posts and 2,115,449 sources of information (blogs, news media and sources that reside outside the blogosphere) and 744,189 cascades. Using the methods formalized above, we id entify the mavens, salesmen, connec-tors and translators of the blogosphere and study their effect on the success of cascades. Our initial results are quite promising in that they indicate that al-gorithms for finding the best method of reaching out to certain actors, rather than the entire network, can be a good heuristic to impact influence in social networks. The types of actors identified can also be used to augment the current models of diffusion to capture real world behavior. As part of future work, we also plan to augment our analysis on the intermediaries to investigate if there exists an optimal timing to reach out to a connector, maven, salesman or trans-lator. Are these actors more useful if they adopt and advocate a cascade early or later on? We analyzed the blogosphere data to investigate the validity of the heuristics introduced but the same heuri stics can indeed be evaluated on other social networks. We believe that different social networks provide different types of interactions, which means that certain actors, while not so significant in some networks, can be highly influential in others. 2.2 Limiting Diffusion of Misinformation While a substantial amount of research has been done in the context of influ-ence maximization, a problem that has not received much attention is that of limiting the influence of a malicious or incorrect information campaign. One strategy to deal with a misinformation campaign is to limit the number of users who are willing to accept and spread this misinformation. In this section, we present preliminary work on techniques to limit the number of users who partic-ipate in an information campaign. We call this problem the influence limitation problem .
 In this context, we consider the social network as an edge-weighted graph. The nodes represent users of the netwo rk and edges represent a relationship between these users. Edges can be undirected, indicating that the relationship is symmetric (users are friends with each other), or they can be directed, indicating one-way communication such as a publisher/subscriber relationship (follower relationship in Twitter). We use edge weights to quantify the  X  X nfluence X  that one node has upon another node; the weight of edge e i,j is an indicator of the likelihood that node j will agree with and pass on inf ormation it receives from node i . Nodes that have adopted the idea or information are called active and those that have not are inactive .
 We consider two different information diffusion models, both similar to the Independent Cascade Model (ICM). In ICM, the information diffusion pro-cess takes place in discrete rounds. When node i is first activated in round r , it tries to activate each inactive neighbor j ; it succeeds with probability p i, j ). Whether or not it succeeds, node i does not try to activate anyone in subsequent rounds. In order to capture simultaneous spread of two cascades (the initial misinformation campaign and the limiting campaign), we intro-duced two extensions to ICM called the Multi-Campaign Independent Cas-cade Model (MCICM) and Campaign-Oblivious Independent Cascade Model (COICM) [BAEA11a]. We omit the details of these models due to space limita-tions but note that they are similar to a number of other models in literature [BKS07, DGM06, CNWvZ07, KOW08].

Our objective is to minimize the number of people affected by the misinfor-mation campaign by  X  X accinating X  users through a limiting campaign .Letthe campaign that is to be minimized be campaign C and the initial set of nodes activated in campaign C be A C . The limiting campaign is called campaign L and the initial activation set is A L . For simplicity, we assume that a user can be active in only one campaign, i.e. , once the user has decided on a side, he will not change his mind. Given a network and a diffusion model, suppose that a campaign that is spreading bad information is detected r rounds after it began. Given a budget l , select l individuals for initial activation with the compet-ing information such that the expected number of nodes eventually activated in campaign C is minimized. Let IF ( A C ) denote the influence set of campaign C without the presence of campaign L , i.e the set of nodes that would accept campaign C if there were no limiting campaign. We define the function  X  ( A L ) to be the size of the subset of IF ( A C ) that campaign L prevents from adopting campaign C . Then, the influence limitation problem is equivalent to selecting A
L such that the expectation of  X  ( A L ) is maximized. 1: Initialize A L to  X  2: for i =1to l do 3: Choose node i that maximizes  X  ( A L  X  4: Set A L  X  A L  X  X  i } ; afalsehood).Werefertothisnotionas high-effectiveness property .Inare-cent study [BAEA11a], we have shown that  X  ( A L ) is a monotone, sub-modular function for this setting. We have also proved the same result in the context of Campaign-Oblivious Independent Cascade Model, even without the high-effectiveness property . Therefore, for both diffusion models, the greedy algorithm given in Figure 1 yields an A L for which  X  ( A L )iswithin1  X  1 /e of optimal [BAEA11a]. Another process that is of interest in soc ial networks research is the process by which a group of individuals in a social network form an opinion about a piece of information or an idea and how interpersonal influence affects the opinion formation process. This process is known as opinion dynamics . While this process shares some similarities with the diffusion of an information campaign, it differs in the main respect that the individual opinions evolve over time, continuously changing in response to interactions with other individuals and possibly external inputs.

Research on opinion formation in social groups predates the advent of online social networks by decades, and several formal mathematical models of the opin-ion formation process have been proposed [Fre56, DeG74, Leh75, FJ99, HK02]. These early works are based on the assumption that all individuals interact with all friends simultaneously in a synchronized fashion. In online social networks, however, individuals are spread out across space and time and they interact with different communities and friends at diffe rent times and with different frequen-cies. We investigate how these previousl y proposed models of opinion dynamics can be augmented to incorporate the asynchronous nature of social interactions that arise in online social networks.

We first briefly review the general opin ion dynamics model. The social net-work is modeled as a graph G =( V, E ) where the vertices represent users or agents ,with | V | = n , and the edges represent relationships between users, with |
E | = m . The graph may be directed or undirected. A directed graph is used to model networks where relationships are not symmetric, for example, the follower relationship in Twitter. An undirected graph models a network with symmetric relationships such as the friend relationship in Facebook. We say that agent i is a neighbor of agent j if ( i, j )  X  E . Each individual i has an initial opinion x (0). The opinion is assumed to be a real number, for example a numerical representation of the individual X  X  supp ort for an issue. The agreement process takes place in discrete rounds. In each round, each individual updates his opin-ion based on information exchanged along edges in the network graph, taking a weighted average of his own opinion and the opinions of his neighbors. Let w ij be the weight that agent i places on the opinion of agent j with the normalization requirement that n j =1 w ij = 1. In each round, individual i updates his opinion as follows: where w ij &gt; 0onlyif( i, j )  X  E . We note that this formulation admits the possibility that w ij = 0 even if ( i, j )  X  E , meaning i does not place any weight on the opinion of j even though they are neighbors in the network. The evolution of all of the opinions, called the opinion profile , is captured by the following recursion: x ( t )isthe n -vector of opinions at round t ,and W ( t, x ( t )) is an n  X  n matrix that gives the edge weights, the interpersonal influence, for round t . This general model allows for the possibility that these edge weights may change over time and may depend on the current opinion profile.

In the following sections, we restrict our discussion to the classical model of opinion dynamics that was proposed by De Groot [DeG74] and Lehrer [Leh75], where the edge weights are assumed to remain constant throughout the opinion formation process. The proces s is given by the recursion where W is an n  X  n matrix.

Due to the simple form of this model, it is possible to analyze properties of the opinion formation process and predict the outcome by examining the W matrix. In particular, it can be shown that (1) Individuals reach agreement if and only if |  X  2 ( W ) | &lt; 1, where  X  2 ( W ) is the second largest eigenvalue of W by magnitude; (2) If |  X  2 ( W ) | &lt; 1and W is symmetric, the consensus value is the average of the initial opinions. If W is not symmetric, the consensus value is a weighted average of the initial opinions; and (3) If agreement will occur, the number of rounds required for individuals to be within of the consensus value is log / log(  X  2 ( W )). We next describe several extensions to the classical opinion dynamics that capture the types of social interactions exhibited in online networks.

It has been observed that users inter act frequently with only a small sub-set of their neighbors and that communication is infrequent along many edges [WBS + 09]. It is reasonable to expect that the frequency of interaction will have a large impact on the evolution of opinions in online social networks. To capture the notion of interaction frequency, we associate a (unique) probability of com-munication p ij with each edge ( i, j )  X  E .Thevalue p ij is the probability that agents X  i and j will communicate in each round. The evolution of the opinion profile with probabilistic interactions is given by the following recursion, where  X  ij ( t ) are independent Bernoulli random variables with This model has been adapted from the model for multi-agent consensus in stochastic networks [PBE10]. Each L ij is the weighted Laplacian matrix of the graph G ij =( V, { ( i, j ) } ), the graph that contains the same n vertices as the orig-inal graph G and the single edge ( i, j ). When  X  ij = 1, agents i and j exchange information just as they did in the classical model. When  X  ij = 0, agents i and j do not communicate.

In our recent work [PB10, PBE10], we have derived a matrix-valued, Lyapunov-like operator W (  X  ) that describes the evolution of the covariance of the opinion profile in this stochastic model of opinion dynamics and we have provided anal-ysis similar to the well-known results for classical opinion dynamics. Social networks provide a large-scale information infrastructure for people to discuss and exchange ideas about variety of topics. Detecting trends of such topics is of significant interest for man y reasons. For one, it can be used to detect emergent behavior in the network, for instance a sudden increase in the number of people talking about explosives or biological warfare. Information trends can also be viewed as a reflection of socie tal concerns or even as a consensus of collective decision making. Understanding how a community decides that a topic is trendy can help us better understand how ad-hoc communities are formed and how decisions are made in such communities . In general, const ructing  X  X seful X  trend definitions and providing scalable solutions that detect such trends will contribute towards a better understanding of human interactions in the context of social media.

Before we study the problem of finding trendy topics in a social network, we first need to develop a clear definition of  X  X rendiness X . Assume users of a network can choose to (or not to) broadcast their opinions about various topics at any point in time. Assume further that we can abstract away what the topic is from what a user broadcasts. A simple definition of trendy topics can be the frequent items throughout the entire history of user broadcasts. The problem, defined this way, is simply to find the frequent items in a stream of data, also referred to as heavy hitters .The frequent elements problem is well studied and several scalable, online solutions have been proposed [CCFC02, CM05, MAE06, MM02, DLOM02]. While the heavy hitters view trend definition is compelling because of the existence of scalable algorithms, this simple definition overlooks various important aspects such as the spatio-temporal dimensions of human interaction and the network structure a nd its effect on the emergence of trends. In the following, we explore structural trend analysis as an example that depend on structural connect ions between the users who are broadcasting. Information trends at the granularity of spatio-temporal dimensions remains future work.
Assuming that information diffusion on a social network is a substantial part of the process that creates the information trends, properties that are defined in the context of the network structure are of significant interest. For example, consider a group of friends in a large social network like Facebook discussing an attack. Detecting this new interest of th is specific group on  X  X ttacks X  can be of great importance. Note th at especially for cyber attacks, those people do not necessarily need to be in the same geogra phical region, and in some instances, this geographic information is not even available. In essence, a structural trend is a topic that is identified as  X  X ot X  within structural subgroups of the network. The challenges are to formally define the notions of a structural subgroup and to develop techniques to detect structural trends .
 nodes are people who are not talking about it. Even though both graphs have the same number of people talking about this topic, in the graph on the right, the people talking about the issue are a part of a more clustered subgraph, giving the topic a higher structural significance. The number of pairs of users talking about the topic in the graph on the left is 0 whereas the pair count is 3 for the graph on the right. The detection of structural trends is harder to solve than the traditional definition of trends since in the traditional setting a counter can be updated on each new data item without dependence on any future or past items. This is not the case for the correlated trends. Value addition of a tuple n i ,T j to topic T j ,where n i represents a node in the graph and T j is an arbitrary topic depends on other tuples involving T j and neighbors of n i . The detection of structural trends calls for n ew, graph-oriented solutions.

The exact solution for correlated trends that requires keeping track of all topics for all nodes is not scalable for large networks so we need to explore approxima-tion algorithms. Here we describe possible solutions: (i) Use the activity level and degree of a node as a heuristic to limit the number of nodes monitored per topic; (ii) Only monitor those topics that are frequent in the traditional sense and for such topics find the order of their importance w.r.t. correlated trendiness; (iii) As demonstrated by Mislove et al. [MMG + 07], there is a small subset of nodes in social networks without which the net work would be a collection of many small disconnected subgraphs. This property can be exploited to partition the graph into smaller sub-graphs and apply the counting algorithms in parallel. Query processing requires periodically merging the counts from multiple sub-partitions with the counts from highly connected nodes. This approach is highly scalable in the MapReduce paradigm [DG08].

Another solution we propose to evaluate involves a semi-streaming approxima-tion algorithm. We essentially reduce the problem of evaluating the importance of each topic with respect to the correlated trendiness notion to a problem of counting local triangles in a graph, i.e. counting the number of triangles incident to every node n  X  N in the graph. Consider a social network graph G =( N, E ), a set of all topics T and stream of node-topic tuples S , where each tuple is in the form: n i ,T j s.t. n i  X  N and T j  X  T . Let us create a graph G =( N ,E )s.t. N = N  X  T and E = { ( u, v ) | ( u, v )  X  E  X  ( u, v )  X  S } . The number of connected pairs of users in the social network G that are discussing a specific topic T j is simply the number of triangles incident to node T j in G . Using approximation algorithms based on the idea of min-wise independent permutations similar to [BBCG08], we are able to provide a solution using O( | N | ) space in main mem-ory and performing O(log | N | ) sequential passes over E . Note that the naive solution would need O( | N | X | T | ) space in main memory.

Alternatively, we can define a structu ral trend be the other extreme, where we are interested in the number of unrelated people interested in a specific topic and in trends that results from these unrelated people. We call these uncorrelated trends . Going back to our example of two graphs in Figure 2, for the graph on the left this count will be 3, whereas it will be only 1 for the graph on the right. This definition of trendiness can be used capture the notion of the trustworthiness of a trend. In this case trendiness of a topic will not be biased by a discussion amongst a small clustered group. We note that we have only considered two extremes of structural trends. Identifying alternative definitions of structural trends that will span the entire spectrum remains future work [BAEA11b]. Internet technologies and online social networks are changing the nature of social interactions and user behaviors. Recent e vidence indicates that 45% of users in the U.S. state that the Internet played a crucial or important role in at least one major decision in their lives in the last two years, such as attaining additional ca-reer training, helping themselves or someone else with a major illness or medical condition, or making a major investment or financial decision [HR06]. The sig-nificant role of online social networks in human interactions motivates our goals of generating a greater understanding of social interactions in online networks through data analysis, the development of reliable models that can predict out-comes of social processes, and ultimately the creation of applications that can shape the outcome of these processes. In this paper, we have presented a pre-liminary formulation of modeling and analyzing information diffusion , opinion dynamics ,and information trends in online social networks.
 Acknowledgments. This work was partially supported by the NSF under grant IIS-1135389.

 1 Introduction In recent years, there has been growing needs for Location-Based Services (LBS) and an needs to be answered continuously, while the position of the query object (i.e., the mobile an M k NN query retrieves the k nearest data points of q from P for every timestamp X  X his means that the answer set may need to be updated as q moves. Figure 1 a gives an example set of q is { p 2 , p 3 } ,andat q 2 , the 2NN set of q becomes { p 4 , p 6 } .
There have been various studies on the M k NN query as well as its variants, such as the
A recent study [ 9 ] showed how to process the obstructed M k NN query assuming that the keep monitoring the movement of the query point and recomputing the k NN set repeatedly, which may lead to high communication and computation costs.

In this paper, we address this challenge and propose an efficient algorithm to process the obstructed M k NN query with no predefined query trajectory .

Our basic idea is inspired by V*-Diagram [ 22 ], the state-of-the-art method for the M k NN cles). V*-Diagram is a safe region based technique. Its safe region is formed by two types regions in obstructed space to address the obstructed M k NN query. However, our work is not a trivial extension of V*-Diagram [ 22 ] because our work requires exploiting geometric regions. In obstructed space, the distance between two objects has to take into account the extra distance for bypassing the obstacles. Hence, the safe regions usually have irregular shapes. We address these challenges and make the following contributions: trajectory. We formulate two types of regions, namely the obstructed safe region w.r.t. a integrated safe region for processing the query.  X  We exploit the properties of the obstructed integrated safe region to reduce the query processing cost and obtain a highly efficient algorithm to process the obstructed M k NN query.  X  We perform a comprehensive experimental study on the proposed algorithm. The results show that our algorithm outperforms the baseline algorithm by up to two orders of mag-nitude.

Thispaperisanextendedversionofourearlierpaper[ 16 ].There,weproposedasaferegion based approach to process the obstructed M k NN query. In this paper, we extend our work by (1) significantly improving the performance of our obstructed M k NN query processing used in the obstructed known region to reduce the number of obstacle vertices to be checked the improved algorithm and comparing it with a baseline algorithm adapted from a related to the proposed approach, including formal definitions and proofs to the key techniques and complexity analysis; (5) improving the overall presentation of the paper, including more explanatory figures and clarifications.
 The remainder of this paper is organized as follows: We review related studies in Sect. 2 . conduct an experimental study in Sect. 9 and conclude the paper in Sect. 10 . 2 Related work We first review studies on spatial queries over moving objects in general. Then, we review M k NN queries in unobstructed space and obstructed space, respectively. 2.1 Spatial queries over moving objects Spatial queries over moving objects have been studied extensively. For example,  X altenis et al. [ 28 ] propose the Time-Parameterized R-tree (TPR-tree), which indexes moving points as queries, which query moving objects satisfying certain time-parameterized predicates. Hu et al. [ 13 ] propose a safe region based framework to monitor spatial queries over moving objects for client X  X erver based systems. In such a system, each moving object is a client. It knows the current query result and does not need to report its location updates to the frameworks to monitor spatial queries over moving objects. Their frameworks process the queries incrementally by monitoring the effect of each object location update on the query Xia et al. [ 32 ] propose a six-region approach for the query type. More recently, Cheema et al. [ 3 ] propose a safe region based approach to process the reverse k nearest neighbor (RkNN) query over moving objects. Li et al. [ 17 ] study moving k NN query in weighted Ward et al. [ 30 ] propose a GPU-based algorithm to process the query. This query finds the intersecting pairs from two sets of moving objects. These studies have different problem settings from ours, and their solutions are inapplicable. 2.2 M k NN queries in unobstructed space As a major type of spatial queries, the k NN query has attracted a large body of studies have become more popular. Most of the existing studies on the M k NN queries consider point on a line segment, which can be seen as finding the NNs of a point object moving on processing cost in the expense of query result accuracy.

Many studies use safe region based approaches to process the query. The k th-order Voronoi diagram ( k VD) [ 23 ] is an example. This method requires expensive precomputation and updates for the safe regions. The Retrieve-Influence-Set k NN algorithm (RIS-k NN) [ 35 ] expensive time-parameterized k NN queries every time the query point exits the current safe In this case, the safe region is reduced to a line segment on the predefined trajectory and can be determined by the bisectors between the query point and its nearby data points with low costs. Nutanong et al. [ 22 ] propose the V*-Diagram technique, which maintains some serve as a  X  X ache, X  this technique does not need to retrieve new data points every time the as much cheaper data access cost per data retrieval, and it handles dynamically changing k values. As V*-Diagram is closely related to our proposed technique, we will describe it in detail in Sect. 3.3 .

Besides the safe region based approaches, Li et al. [ 18 ]uses influential neighbor set to bound k NN sets, which reduces k NN recalculation cost while retaining the lowest k NN They propose an incremental computation approach to maintain an answer region for each query, which updates according to the object location updates. The answer region is then where pt denotes the line segment connecting two points p and t ,and o \  X  o denotes an each other if pt intersects the boundary of o but does not cross the o . Figure 2 shows an example, where the dotted area and the horizontal-lined area denote the the shortest path between p 1 and p 2 (i.e., p 1 CDp 2 ). This shortest path is computed by constructing a graph on the vertices of the obstacles, the source point and the destination point and then applying the Dijkstra X  X  algorithm. The detailed shortest path computation process can be found in [ 23 ]. 3.2 Bisector of two points which a point t has the same distance to both p i and p j . Formally,
When obstacles are involved, the definition becomes: is because of the  X  X etoured X  distance caused by the obstacles.

The bisector of p i and p j divides the space into two regions. In one region, every point over p j , denoted by  X ( p i , p j ) . region is called the dominant region of p j over p i , denoted by  X ( p j , p i ) .
In obstructed space, the definition becomes:
In Fig. 3 , the gray region represents  X   X  ( p 1 , p 2 ) , and the white region represents  X  ( p 2 , p 1 ) .
 3.3 V*-Diagram knowledge of the query point location and the safe region, V*-Diagram significantly reduces k NN query).

The safe region used in the V*-Diagram is called the Integrated Safe Region (ISR), which and (ii) the fixed-rank region of the k + x nearest data points, where x is the number of
The V*-Diagram M k NN algorithm computes the integrated safe region as follows. It first q q and z .
 as a region that, when the query point q moves inside the region, p is nearer to q than any data point p outside the known region. Formally, d ( q b , z )) can be tightened by replacing d ( q , p ) with d ( q b , p )  X  d ( q b , q ) : further simplified to be: this ellipse by  X ( q b , p , d ( q b , z )) .

As long as q is inside the intersection of the safe regions w.r.t. the k data points in the known region cannot be closer to q than any of those k data points.
 Step 2 The V*-Diagram algorithm further computes a region called the fixed-rank region Step 3 The intersection of the safe regions w.r.t. the k + x data points and the fixed-rank  X ( computation can be simplified to the following Eq. [ 22 ]:
Figure 4 shows an example where k = 2and x = 2. When the query point q is at  X  p 1 , p 3 , p 2 , p 6  X   X ( q b , p 3 , d ( q b , p 6 )) ,the2NNof q will not change. 3.4 Solution framework In V*-Diagram, the ISR is computed in unobstructed space. It can be expressed as a set of ellipses and lines and hence can be easily maintained. When the space is obstructed, safe region is like and how it can be computed and maintained.
 of the regions used in V*-Diagram, namely the obstructed known region, the obstructed safe region . Based on these regions, we propose the Obstructed M k NN algorithm to process the obstructed M k NN query as follows.  X  Initialization. When an obstructed M k NN query is issued, the algorithm first computes a to reduce the number of safe region recomputation.  X  Maintenance. At every timestamp, the new position of q arrives. The algorithm uses the obstructed safe region and the obstructed fixed-rank region to determine whether a safe region recomputation is required and which data points need to be accessed for the
The above process repeats, and the obstructed M k NN set of q is generated continuously. 4 Obstructed safe region w.r.t. a data point (OSRD) 4.1 Obstructed known region an example, where the shaded polygon ABCD denotes an obstacle and the gray region denotes an obstructed known region.

As can be seen from the figure, the obstructed known region is not a disk in Euclidean space. Next, we describe how to compute the obstructed known region through a concept called the  X  X bstructed disk. X  We use  X ( q b , r ) to denote a disk with center q b and radius r in unobstructed space. points X  distances to q b are less than or equal to r . Figure 6 showsanexample,wherethetwo shaded triangles denote obstacles, and the gray region denotes an obstructed disk.
We have the following observations on an obstructed disk  X   X  ( q b , r ) . centered at q b with a radius of r ), as formalized by Theorem 1 .
  X ( q b , r ) .Wehave: Proof (i)  X   X   X  ( q b , r )  X  v  X  X  q (ii)  X   X   X  ( q b , r )  X  v  X  X  q as  X   X  ( q b , d  X  ( q b , z )) (cf. Fig. 5 ). 4.2 Definition of OSRD Based on the definition of the obstructed known region, we can define the Obstructed Safe Region w.r.t. a Data point p (OSRD) . OSRD is a region where the movement of q will not cause p to be removed from the k + x nearest neighbors of q . As shown in Fig. 5 ,if q has must be less than or equal to the distance between q and any point  X  outside the obstructed d that p stays as one of the k + x NNs of q .
 Formally, OSRD is defined as follows.
 x )  X  ( q b , p , d  X  ( q b , z )) ,isdefinedas: 4.3 Computation of OSRD The definition of OSRD specified a point set but does not provide a clear way to represent method to compute OSRD based on the following three lemmas.

The first lemma establishes that in the region visible to both q b and p , OSRD covers the same region with or without considering the obstacles.
 Lemma 1 In the intersection of the visible regions of q b and p, the obstructed safe region  X  safe region in  X   X  ( q b )  X   X   X  ( p ) is the same as its unobstructed counterpart.  X  part (VEP) and denote it by  X   X  ( q b , p , l ) .

The second lemma establishes that the OSRD of p fully covers the OSRD of certain points in the OSRD of p .
 d d OSRD.
  X  Therefore, t is in  X   X  ( q b , p , l ) .

Basedonthetwolemmasabove,wehavethat  X   X  ( q b , p , l ) can be computed by the intersection of the VEPs of all sub-OSRDs satisfying Lemma 2 . Since the points satisfying Lemma 2 are infinite, we need to identify a subset of such points whose OSRDs together t  X   X   X  (v 1 )  X   X   X  (v 2 ) .  X (
Similarly, we can identify v 2 on the shortest path from p to t . Thus, we have two points v ,v 2  X  X  q b , p } X  V  X ( q d  X  ( q b , p , l ) . Formally, Proof The correctness of the theorem is guaranteed by the lemmas above straightforwardly. without considering the obstacles.

The combination of the VEPs can have three different cases. exactly the same area. (cf. Fig. 7 a,  X   X  ( q b , p 1 , l ) ). compute their union to form  X   X  ( q b , p 3 , l ) .
 of  X  path between q b and p 3 passes D . Thus, in the overlapping visible region JDK , ellipse  X ( can avoid computing parts of  X   X  ( q b , p , l ) covered by more than one VEPs based on the following corollary.
 shortest path from v 1 to q b and v 3 lies on the shortest path from v 1 to p. We have: d d ( 4.4 Algorithm for computing OSRD In this subsection, we present our OSRD computing algorithm OSRDC based on Theorem 2 , as shown in Algorithm 1 . The algorithm starts with computing the visibility graph and  X  and is returned by the algorithm.
 An example Figure 8 shows how an OSRD is computed through computing the VEPs, where of the two points selected for VEP computation. The points selected for VEP computation q shown by the gray ellipse part in Fig. 8 a. Point C is the next point selected by the inner Algorithm 1: Computing OSRD computed as shown in Fig. 8 c X  X , respectively. The combination of all VEPs computed gives us  X   X  ( q b , p , l ) (cf. Fig. 8 f).
 average [ 5 ], where n denotes the number of all obstacle vertices. We denote the number of In all, the algorithm takes O ( log n +| V all | 2 ) time. 5 Obstructed fixed-rank region of distances of the k + x data points in the obstructed known region to q does not change. Figure 9 shows an example. There are three data points p 1 , p 2 and p 3 and two obstacles space into six OFRs. Each OFR corresponds to a distance rank of the three data points. For p 3 is the nearest data point, while p 2 is the second and p 1 is the third.

If we compute the OFR for the k + x data points in the obstructed known region, then unless q moves out of this OFR, we do not need to recompute the k NN set. However, computing and Therefore, instead of computing and storing the OFR, we check whether the dominating moved out of the OFR and reordering for the k + x data points is needed.
 summarizedinAlgorithm 2 .Thealgorithmhastwostages:(i)initializationand(ii)evaluation. The initialization stage (lines 1 to 3) computes a visibility graph on p i and p i + 1 .  X  puted based on the obstacles whose vertices are in both disks because these are the only graph computation cost is constrained.
 or equal to d  X  ( q , p i + 1 ) is returned.
 graph can be reused in the following dominating relationship check. Therefore, the cost is O ( n V log n V ) time, where n V denotes the number of all obstacle vertices. Identifying all log n V + n 2 V 1 log n V 1 ) time.

The evaluation stage involves two shortest path computation using Dijkstra X  X  algorithm visible to each other.
 Algorithm 2: Dominating Relationship Check 6 Obstructed integrated safe region In this section, we combine OSRD and OFR to form a region where the movement of q does not cause its k NN set to change. We call this region the Obstructed Integrated Safe Region (OISR) .
 change by the movement of q . This intersection is the obstructed integrated safe region. Definition 3 ( Obstructed integrated safe region ) The obstructed integrated safe region of L
An example is shown in Fig. 10 ,where k = 2, x = 1, L 3 = p 1 , p 2 , p 3 and the  X  Then, we just need to compute one safe region for  X   X  ( q b , L k + x ) .
 Theorem 3 Let L k + x = p 1 , p 2 ,..., p k + x bealistofdatapointsorderedbytheirdistances to q b . Then,  X  =  X   X  ( L k + x )  X   X   X  ( q b , p k , l ).
 Obstructed M k NN algorithm. Since the OFR part of an OISR is not precomputed or stored but replaced by the dominating relationship check, the OISR cannot be precomputed or stored. Thus, the obstructed M k NN algorithm only uses the OISR conceptually to process the obstructed M k NN query as follows. When an obstructed M k NN query is issued and the the maintenance process. At every timestamp, the new position of q arrives. The algorithm Algorithm 3: Obstructed M k NN processing L + x to determine the new k NN of q . Next, the algorithm checks whether q is in the OSRD OISR and no further processing is required. Otherwise, the current OISR is deprecated, and set of q is generated continuously (i.e., at every timestamp).

Algorithm 3 summarizes the process. 7 Essential auxiliary set In this section, we discuss how to choose the x extra data points when we compute the k + x data points for safe region computation. Our aim is to (i) obtain safe regions as large as the x extra data points, so that the cost of each recomputation is reduced. Note that the x region becomes invalid.

We introduce a concept called the essential auxiliary set (EAS) to constrain our search of should only be chosen from the EAS. Then, we present a way to compute the EAS efficiently. 7.1 Determination of inessential data points If a data point p does not affect the size of an OISR whether or not it is used as an extra data point, then we say p is inessential totheOISRandcallitan inessential data point . Otherwise, we say that p is essential to the OISR and call it an essential data point . The following lemma gives a basic requirement that an essential data point must satisfy. If a data point does not satisfy the following lemma, then it is an inessential data point. Lemma 4 If a data point p is essential to an OISR, then there exists a point q in the OISR where p is its ( k + 1 ) th nearest neighbor.
 Proof Since we are considering the possible extra data points, p must not be one of the that when q moves, p would become one of the k NNs, which means the nearness rank of p the nearness rank of p has to change continuously from the original k +  X  to less than k , the point q .

This lemma gives a way to determine whether p is essential. However, it is too difficult see if p is its ( k + 1 ) th nearest neighbor.

To simplify the determination process, we consider the bisectors between p and the current k NNs.
 Lemma 5 A data point p is inessential to an OISR if and only if the OISR does not overlap any bisector between p and a current kNN.
 Proof (i)  X  p is inessential  X  the OISR does not overlap any bisector between p and a
Suppose the OISR overlaps a bisector between p and a current k NN, denoted by r .Then p would become nearer to q than r does. This violates the condition that p is inessential. p and r .
Suppose p is essential, then there exists a point q satisfying Lemma 4 . When the query neighbor when the query point is at some point q . At this moment, we have (1) q must be at the border of the OISR, since p is becoming one of the k NNs, and the current OISR will as p does, since otherwise p would have already become one of the k NNs. As a result, the OISR must overlap the bisector between p and r .

Therefore, if the OISR does not overlap any bisector between p and a current k NN, then p is inessential.

So far, we have assumed the existence of an OISR for the computation of an inessential data point. However, we do not have an OISR until we have identify the EAS and further the x extra data points. To bypass this dilemma, we use a region that is guaranteed to enclose any OISR that is computed using the current k NNs. Since this region is larger than any of data points.
 Lemma 6 Any OISR of a k NN set R resides in an order-k obstructed Voronoi cell w.r.t. R. Voronoi cell [ 23 ].
 Theorem 4 Given a k NN set R and a data point p, if for every data point r  X  R, the w.r.t. R, then p is inessential to any OISR of R.
 Proof By Lemma 6 ,anyOISRof R is fully contained in the order-k Voronoi cell w.r.t. R . be overlapped by any of the OISRs. Then by Lemma 5 , p is inessential to R .
Figure 11 gives an example, where k =2, x = 3 and the query point is at q . The current p 6 and p 7 contribute as parts of the Voronoi cell boundary.

The aim of EAS is to eliminate points like p 5 from being considered as the extra data  X  ( q , d  X  ( q , p 6 )) , denoted by the loosely dashed curve. 7.2 Computation of the essential auxiliary set Theorem 4 gives a more computable definition of the inessential data points. However, the the order-k Voronoi cell, which can be precomputed easily for a given set of objects and obstacles without sacrificing the algorithm correctness. We also compute the EAS based on the obstructed Delaunay triangulations.
 that no point in P is inside the circumcircle of any triangle in DT ( P ) [ 23 ].
When obstacles are involved, the obstructed Delaunay triangulation (ODT) is defined by by  X  X ircum-obstructed disks X . Figure 12 gives an example, where the dashed circle indicates circum-obstructed disk w.r.t. p 2 , p 7 and p 9 .
 Next, we generalize Theorem 4 based on the ODT.
 Lemma 7 Given a 1 NN data point r, a data point p is inessential to r if p and r are not directly linked by an edge in the ODT.
 Proof We need to prove that if p and r are directly linked by an edge in the ODT then the bisector between r and p is not overlapped the Voronoi cell of r .
 Suppose the bisector is overlapped by the Voronoi cell. Let a moving point d be in the disk and no other object inside, which means that p , r and o define a triangle in the ODT, and we encounter a contradiction that p and r are directly linked by an edge in the ODT. Lemma 8 Given a k NN set R, if for data point r  X  R, a data point p is not directly linked by an edge to r in the ODT ignoring all other data points in R, then p is inessential to R. Proof Every Delaunay triangulation has a corresponding dual Voronoi diagram [ 23 ]. We prove this lemma through the properties of the Voronoi diagram. The order-k Voronoi cell for a set R is intersected by the order-1 Voronoi cell of each data point r  X  R computed when the other objects in R are ignored. If p is not directly linked to r , then by Lemma 7 and Lemma 5 , the bisector between p and r is not overlapped by the Voronoi cell of r .As a result, the bisectors between p and any data point in R is not overlapped by the order-k Voronoi cell of R . Therefore, p is inessential to R .

Lemma 8 needs different ODT X  X , which are constructed by ignoring different sets of objects. The following lemma allows us to eliminate the need of these different ODT X  X . Lemma 9 Given the O DT w.r.t. a data point set R (denoted by O DT R ), and the O DT w.r.t. edge to any data point in R \ PinODT R , then the edges linked to p are exactly the same in both ODT X  X .
The correctness of Lemma 9 can be proved straightforwardly based on the the unobstructed version of the lemma in [ 23 ], and hence, the proof is omitted. Based on the lemmas above, we have the following conclusion.
 Theorem 5 Given a k NN set R, a data point p is inessential to R if there is no edge in the obstructed Delaunay triangulation between p and any data point in R.
 Proof The correctness of the theorem is guaranteed by the lemmas above straightforwardly.
Theorem 5 suggeststhatdatapointsnotdirectlylinkedtothedatapointsin R areinessential and can be pruned from consideration when choosing the extra data points. Therefore, we can keep the data point that linked to the data points in R as the EAS. Formally:
The definition suggests an efficient way to construct the EAS when the extra data points are needed. Note that the extra data points are needed after the k NN retrieval and before constructing the OISR. Therefore, the EAS can be constructed as follows: 1. In the initialization phase, precompute the obstructed Delaunay triangulation based on 3. When processing a M k NN query, after the retrieval of the current k NN set R , compute 4. Sort EAS by based on the distance of the data points to the query point and take the x
Note that the EAS may contain false positives, i.e., inessential data points, since a data point directly linked to the data points in R can still be inessential to R .However,wehave the following theorem that guarantee that on average the number of data points in EAS is constrained, i.e., less than or equal to 6 k .
 Theorem 6 The average number of data points in an EAS is less than or equal to 6 k. Proof In the dual Voronoi diagram of a Delaunay triangulation, an order-k Voronoi cell is constructed by k order-1 Voronoi cells. Since on average each Voronoi cell has six neigh-bors [ 23 ], for a set R of k data points, it has no more than 6 k neighbors in total. 8 Other types of obstacles In this section, we discuss how our techniques can be applied on other types of obstacles. Concave polygons with nonzero extents In this case, we can simply divide each obstacle into multiple convex polygons. Then, our techniques can be applied straightforwardly. Line segments A typical use case of line segments as obstacles is shopping recommendation in large shopping malls. As shown in Fig. 13 , line segments AB , BC ,..., TA form the outline of a shopping mall. A customer in the mall is represented by q .Shemayissuea query to the shopping recommendation server about the types of shops she is interested in. Then, the server can identify the relevant shops (as the data points) and make continuous recommendation based on the distance between the customer and the shops as the customer is form the obstacle vertex set. Then, our techniques can be applied to process the obstructed M k NN queries for this case. 9 Experiments In this section, we present a detailed performance study of our obstructed M k NN algo-algorithm under different values of the algorithm parameter x in Sect. 9.2 . We compare our algorithm with a baseline algorithm in Sect. 9.3 . In the experiments, we measure the number of page accesses, the communication cost (by counting the number of safe region recomputation) and the average response time. 9.1 Experimental setup All experiments were conducted on a desktop computer with a 2.4 GHz Intel i5 CPU and 8 GB main memory.

Datasets We use real datasets as the obstacles and generate synthetic data as the data Blocks dataset from the R-tree Portal. 1 The Hypsography dataset contains 76,999 MBRs of hypsography data from Germany, and the Census Blocks dataset contains 556,696 MBRs of census blocks from the USA. We map these MBRs to a data domain of 200 , 000  X  200 , 000 units.

We generate two types of synthetic datasets around the obstacles: (i) uniform dataset (denoted by  X  X  X ), where the data points follow uniform distribution; (ii) Zipfian dataset (denoted by  X  X  X ), where the data points follow Zipfian distribution with  X  = 0 . 85 as the skew coefficient.
 page size is set to be 2 KB and the buffer size is set to be 16 pages.

We generate two types of query point trajectories: (i) random (denoted by  X  X  X ) ,wherethe movement of the query point follows the Random Waypoint model; (ii) directional (denoted by  X  X  X ) , where the query point moves in straight lines until reaching an obstacle and the movement is reflected by the obstacle.
 Parameters To evaluate the algorithms under various settings, we vary the value of x from point moving speed v q from 100 to 500 units per timestamp. By default, we set x at 10, k average cost per timestamp.
 Baseline algorithm In the comparative study, we adapt the Continuous Obstructed kNN trajectory and feed it into the algorithm. This means the CO k NN algorithm will be run at each timestamp. 9.2 Effect of x is that when x becomes larger, the obstructed known region becomes larger, and hence, the valid periods of the safe regions become longer. The frequency of safe region recomputation points and the obstacles as well as the communication cost to report the updated k NN set decrease. Meanwhile, the computation cost to maintain the safe regions of k + x data points Overall, our proposed algorithm performs best when x = 10. Therefore, we will use this value in our algorithm in the following experiments. 9.3 Comparative study In this subsection, we compare our obstructed M k NN algorithm with a baseline algorithm COkNN [ 9 ]. In the result figures, we denote our algorithm by  X  X MkNN X  while the baseline algorithm by  X  X OkNN. X  9.3.1 Varying the query parameter k Figures 15 , 16 and 17 show the comparative performance when the value of k is varied. From the figures, we can see that our algorithm outperforms COkNN in the three measurements by up to two orders of magnitude (please note the logarithmic scale). ( a ) (b) (c) ( d ) (e) (f)
Figure 15 shows the numbers of page accesses. Our algorithm has much smaller numbers because our safe region based algorithm keeps the number of k NN recomputation low, and hence, it does not require to access the data point as frequently as the COkNN does. The same reason applies, and thus, the communication cost of our algorithm is also much smaller than that of COkNN (cf. Fig. 16 ).
 The above two effects together result in a smaller average response time for our algorithm. As Fig. 17 shows, for COkNN, the average response time is almost one second per timestamp, whichmeansCOkNNistooslowtogeneratevalidqueryresultsformorethanjustonequeryat most cases, which means our algorithm can provide timely query results for tens of queries simultaneously (Please note that the experiments were conducted on an average desktop computer rather than a high performance server machine). This demonstrates the scalability of our algorithm. ( a ) (b) (c) ( d ) ( a ) (b) (c) ( d ) 9.3.2 Varying data point X  X bstacle ratio | P | / | O | Next, we compare our algorithm with COkNN when the ratio of the number of data points over the number of obstacles ( | P | / | O | )isvaried. ( a ) (b) (c) ( d ) ( a ) (b) (c) ( d )
Figures 18 , 19 and 20 give the result. Our algorithm again outperforms COkNN by more than one order of magnitude in the three measurements. We can see from Fig. 20 that as the ratio grows, the average response time of our algorithm first drops and then increases ( a ) (b) (c) ( d ) ( a ) (b) (c) ( d ) known region is large, and it encloses more obstacles. As a result, the maintenance cost of ( a ) (b) (c) ( d ) ( a ) (b) (c) ( d ) the obstructed know region drops and it encloses less obstacles. Thus, the maintenance cost of the safe regions drops. However, when the size of the obstructed known region drops, the frequency of safe region recomputation also increases. This effect becomes the dominating effect and the costs of query processing grow again when | P | / | O | is larger than two. ( a ) (b) (c) ( d )
Note that the communication cost of COkNN stays unchanged when | P | / | O | is varied (cf. Fig. 19 ). This is because COkNN has to be rerun and sends the new k NN set at every size of the k NN set, which is decided by k and x . 9.3.3 Varying query point speed v q We also vary the query point speed v q . Figure 23 shows the response time. OMkNN again outperforms COkNN significantly. It can provide query answers within 0.01s when v q is 500 units per timestamp. Assume that the 200 , 000  X  200 , 000 data space represents a 1,000km  X  1,000km area. Then, OMkNN provides query answers within 0.01 seconds when v q is 180km/h, which is fast enough for most daily life applications. This shows the applicability of OMkNN (Figs. 21 , 22 ).

The comparative performance in I/O cost and communication cost is similar to that of previous experiments.
 In summary, our obstructed M k NN algorithm outperforms the adapted baseline algorithm COkNN by up to two orders of magnitude in computation and communication costs under various settings. 10 Conclusion query for a moving query point in space with obstacles. We proposed a safe region based (OFR). The OSRD guarantees that the objects in the k NN set do not change, while the OFR guarantees that the closeness order of the objects in the k NN set to the query object also does not change. Together, they guarantee that, as long as the query object stays in their we obtained an efficient algorithm to process the M k NN query. We further optimized the algorithm through the essential auxiliary set which help determine the possible new k NNs when the k NN set needs to be updated. As the experiments show, our algorithm outperforms an adapted baseline algorithm by up to two orders of magnitude under various settings.
Compared with an earlier conference version of the paper [ 16 ] where a preliminary query processing algorithm was proposed, this journal article significantly enhanced the query reduce the number of obstacle vertices and objects checked in safe region computation. We renewed the experiments comparing the enhanced algorithm with a more advanced baseline foundations are provided, including formal definitions and proofs to the key techniques and complexity analysis.
 References
 component e rises. Prior to the second ASR-Failure the user needs more than 3 s to answer. That leads to an increase of the component T m . After the third ASR-Failure the dialogue is interrupted and the user has to activate the SDS again to continue the task with the speech modality. After pressing the PTT button the user waits more than 5 seconds to utter a command and a Timeout takes place. Accordingly, the component t rises. Then follows a user request for options that raises the component o . Finally, he utters the right command, but he needs more than 3 s to do it and T m rises. Subsequently, the status of the user for the task  X  X  X hoose a frequency X  X  is calculated (Section 2.3 ) and the vector ~ UM is updated. 2.2 Weight vector The weight vector ~ UM G represents how much each component of ~ UM can tell about the user experience with the system. The idea goes back to the assessment theory MAUT (Multi-Attribute-Utility-Theory) (Schu  X  tz &amp; Scha  X  fer, 2002 ) for products. According to this theory, the overall score of a product is the sum of the weighted values of the evaluated product attributes. In our case, the product is the user status and the attributes are the components of ~ UM . The sum of every component v G _ i of ~ UM G has to be
For the usability tests we assumed that every component of ~ UM G is equally good for predicting the user status. Thus, every v G _ i had the same value = 1/ n ( n being the dimension of the vector ~ UM ). After the tests we classified by hand every dialogue according to the perceived user behaviour in beginner or expert. Altogether we considered 229 dialogues with the reference system and 231 with the prototype. With the help of a multiple linear regression using the user status as dependent variable and the recorded components of ~ UM as independent variables we calculated the real values of the ~ UM G components 1 . Because of a technical flaw, it was not possible to record the onset time T m .

In spite of the differences between the two systems 2 the values for the compo-nents of ~ UM G in the reference system and in the prototype were much the same. This result can be interpreted as an evidence that the components are equally pre-dictive of the user status quite independently of the SDS design. The components e and o turned out to be the best predictors of the user behaviour. The calculated values for the components of ~ UM G for both systems are displayed below: 2.3 Calculation of the user category The calculation of the user category is illustrated in Fig. 2 . The comparison function takes three values as input: the scalar product ~ UM ~ UM G  X  D UM , the threshold value s , and a heurism to avoid chance assignments. D UM is compared to a threshold value s .If D UM &gt; s , the user behaves as a novice, if D UM  X  s , he behaves as an expert. The size of s is twice the value of the least component of ~ UM G , d , and it also depends on two factors. First, s accounts for people X  X  ability to learn by analogy. We assume that when users know how a particular task works, they will learn the use of a similar task faster and in an easier way than the use of a completely different task. Thus, we have grouped the system tasks in families, according to their similarity. If the user already knows how to operate tasks from a certain family, then the value of s becomes s + d when classifying the user for a new function from that family so that he becomes an expert faster for that function.
 Second, the value of s depends on the time characteristics of the interaction. People tend to forget newly acquired knowledge rapidly, unless they revise it fre-quently, until it becomes part of long-term memory (Edelmann, 1996 ). The influence of these psychological insights on the learning behaviour of the users is incorporated into the adaptation concept by adapting the threshold value s to the elapsed time between interactions. The more time elapses since the last interaction the faster users are assigned novice status, i.e. s is reduced and becomes s  X  d .

A heuristic makes sure that wrong category assignments due entirely to chance behaviour are avoided: After a prediction of the user model, look for positive/ negative verification for this prediction in the next interaction (Rich, 1979 ). We fulfil this claim as follows: The comparison function has to yield three times the same result for a certain task before users are assigned another category. Doing so, the probability that the system assigns users a category only by chance is minimized. 3 Adaptation of the system prompts The difference between system prompts for novices and for experts is mainly their explicitness, e.g. while for novices the SDS mentions the available voice commands without waiting for users to ask, experts have to explicitly ask for them. Long and informative prompts would be in the long run annoying to frequent users. Table 3 summarizes some examples. The system prompts type is given in the first column. Prompt types set in brackets were left out for expert prompts. The type of the user X  X  utterances is set in brackets in the second column.

The different system utterances were analysed with respect to the information they convey to users and assigned a DAMSL-category (Allen &amp; Core, 1997 ; Core &amp; Allen, 1997 ). Depending on the semantics they transmit, the information can be presented in different ways (cf. Table 3 ). To this end, we rely on the notion of Grice X  X  conversational implicatures , and the basic principles for their calculation: cooperation principle and conversational maxims (Clark, 1997 , pp. 320 X 330).
Contributions conveying conventional meaning do not necessarily need linguistic signals. For experts, Openings and Closings can be performed by tones, and a Signal-understanding confirming an action requirement like  X  X  X lay CD X  X  can indirectly be accomplished by playing the CD. Contrarily, Signal-understanding confirming a dialogue state transition, e.g.  X  X  X ntertainment X  X , needs linguistic signals to express the confirmation. For novices, these utterances can be completed with the available voice commands to help the user to carry on with the dialogue.

An assertion cannot be completely replaced by non-linguistic signals. This kind of prompt can be expressed in a less verbose manner or, at the most, be inferred from another prompt type through implicature. A very elementary example for the use of implicature in expert prompts is the following combination of Assert and Action-directive: users can deduce from the directive  X  X  X nsert a CD X  X  the assertion  X  X  X D slot is empty X  X .
 Signal-non-understandings could also be replaced by a non-linguistic signal. However, a beep may not fulfil the maxim of quantity, since a tone alone may not be enough to indicate to users what to do next. Besides, it may not fulfil the maxim of manner because in a car environment we can not guarantee the Signal-non-under-standing tone to be discerned from other tones. Therefore, we decided to express non-understanding using utterances like an Action-directive, e.g. asking users to repeat the last utterance.

Other utterances (Info-request, Open-option and Action-directive) cannot be deduced by implicature or replaced by non-linguistic signals. We just shortened them keeping the semantic content, e.g.  X  X  X nter CD number X  X  to  X  X  X umber? X  X  (Info-request). 4 Test design Two test series were carried out to evaluate a prototype with prompt adaptation and other features described in (Hassel, 2006 ; Hassel &amp; Hagen, 2005 ) against a reference system. Beyond those differences both systems had the same functionality and the same GUI (Hagen et al., 2004 ). The test series with the prototype took place in a BMW 7 Series, and for the tests with the reference we used a BMW 5 Series.

Bernsen and Dybkj X r ( 2001 ) pointed out the importance of testing in a real environment. Therefore, both series were conducted in a car under real driving conditions. We did between-subject testing with a total of 30 male and 14 female subjects participating in the tests, 22 subjects in each series. The tests consisted of a driving part (duration: 30 X 45 min) and a questionnaire. Table 4 summarizes the participants X  characteristics.

During the driving part of the test the subjects were asked to complete 11 rep-resentative tasks that are usual in the driving environment, but do not belong to the driving task, e.g. choosing a radio frequency, dialling a telephone number and changing the map scale (Table 5 ). The degree of usualness of the tasks was obtained from an opinion poll among BMW employees and a survey within the scope of a doctoral thesis. Tasks 1 and 2 were repeated at the end of the test (tasks 10 and 11) to test the adaptation of the system and the learning progress of the participants: Could they achieve the task more efficiently? Did they already develop an operating strategy during the test time?
In addition to completing the tasks while driving, users were told to verbalise their thoughts as they used the system. The thinking-aloud method is described by Nielsen page questionnaire. 5 Evaluation To assess the test results we intended to use the evaluation framework PARADISE (Walker et al., 1998 ) that was developed for information dialogues. The method argues that maximizing task performance means to maximize the task success and to minimize the costs. The task success is represented by the Kappa coefficient 3 , j , (Carletta, 1996 ). For the cost factors we use the usual metrics as described in Larsen ( 2003a ) and NIST ( 2001 ).

Achieving the dialogue goal of having a high performance should result in a high user satisfaction. Therefore, performance can be expressed in terms of the user satisfaction. The theory postulates that using the cost factors ( c i in the formula below) and the task success measure j as independent variables and the user sat-isfaction, US, as a dependent variable in a multiple linear regression, it should be possible to predict the US of future interactions just having the values of the inde-pendent variables: US = ( a  X  N ( j ))  X  the model parameters and N () is a function that normalizes the values to z -scores 4 .
In information dialogues completion of a task corresponds to filling a form. The information required to complete the tasks is therefore thoroughly defined and can be represented using an attribute-value-matrix (AVM). Such a matrix consists of a set of ordered pairs of attributes and their possible values. A dialogue is considered successful when all task attributes get a value. PARADISE takes advantage of this task representation to calculate task success using j . This coefficient compares the AVM for the actual set of dialogues with the AVM for the scenario keys, i.e. with the AVM that defines a successful task.

Recently, PARADISE was the subject of several investigations, among others (Aguilera et al., 2004 ; Larsen, 2003b ; Paek, 2001 ; Whittaker, Terveen, &amp; Nardi, 2000 ). The main limitation found was that tasks have to be clearly defined so that they can be described by an AVM. Further, it was pointed out that PARADISE was designed to evaluate only unimodal systems. And lastly, the assumption of a linear relationship between user satisfaction and subjective measures was called into question.

Attempts have been made to revise PARADISE. Hjalmarsson ( 2002 ) proposes a new task definition for the evaluation of multimodal systems with non-AVM-describable tasks. We could not apply this method because they evaluated SDSs for information exchange and the task success was calculated in terms of information success measure to evaluate multimodal systems. They rate tasks as successful or not, but since we wanted to know how well users coped with the tasks, we also discarded this method. In the next sections we describe the changes we carried out to PARADISE in order to apply it to our system. 5.1 Evaluation of task success: a modified j calculation SDSs for the car environment offer users a broad spectrum of tasks, e.g. dialling a telephone number, setting navigation options and tuning a radio frequency. The type of tasks in this environment can be represented by a directed, connected graph 5 with marked and unmarked nodes (Fig. 3 ), through which users navigate and where the task is completed after they reach the desired node. The edges represent the tran-sitions due to user utterances, and the nodes represent states of that dialogue space. Only some edges are drawn, the transitions caused by options and help requests, the command  X  X  X ack X  X , etc. were left out. Unmarked nodes are transitional states: the SDS remains active after users have reached such states, and the dialogue strategy remains user initiated. Marked nodes (drawn with heavy line) are discussed below. Utterances are set in quotation marks.

Fragment A in Fig. 3 shows dialogues in the navigation menu: Users can navigate either to the node View by choosing a view in the navigation menu (north, driving, arrows: in Fig. 3 users chose  X  X  X rrow view X  X ) or they can navigate to the node Scale by saying they want to change the scale of the map. In this last case, the system takes the initiative asking users what scale they want to have (Table 6 ). Fragment B shows a dialogue in the communication menu: users navigate to the node Dial Number , where they are asked to enter a telephone number. This subdialogue is displayed inside the node.

When users reach a marked node, usually either the dialogue is done immediately (node View ), or the system takes the initiative to require information from the users, and then the dialogue is done (nodes Scale and Dial Number ). But whether a task has been completed or not is not always that easy to answer. The crux of the matter is the goal of the users: If they just want to have the phone menu displayed, then the task is done after they reach the node Phone (Fig. 3 ). That means that the tasks of our SDS are not clearly defined in advance. Consequently, they cannot be described in terms of AVMs and we had to define j in a slightly different way to apply this metric to our system.

We called this task success measure j * . Instead of task attributes, we have specified for each task a set of nodes starting from the main menu and following the usual paths to the nodes that represent the test tasks. Figure 4 shows the AVM of task 5 (dial a phone number) represented as a graph in Fig. 3 B. The values in the AVM belongs to the reference system. Since the tasks for the tests are fixed, for each task a subset of nodes defines when it is complete. The black diagonal cells Ready represent the final states.
 In PARADISE only utterances referring to task attributes are recorded in the AVM. We also include those that contribute indirectly to accomplishing the tasks. For this purpose we introduce the following attributes: O PTIONS /H ELP ,S TOP ,R EPEAT , F
AILURE , and B ACK (for the prototype). F AILURE subsumes answer failures due to a voice recognition misunderstanding (grey columns in Fig. 4 ), answer failures due to a wrong user input (last diagonal cells) and correct system answers due to wrong user utterances (grey rows).

PARADISE computes only correctly recognised utterances or  X  X  X isunderstand-ings that are not corrected in the dialogue X  X  because  X  X  X he effect of misunderstandings that are corrected during the course of the dialogue are reflected in the costs asso-ciated with the dialogue X  X  (Walker et al., 1998 ). Such an AVM is supposed to  X  X  X ummarize how well an agent achieves the information requirements of a particular task X  X  (Walker et al., 1998 ). But, since our dialogues are not based on information requirements, we do not have a set of attributes that have to be accomplished for the task to be successful. Therefore, we consider all utterances that occur during the dialogue in order to compute j . Such an AVM summarizes how well users coped with the task.
 between j and j * is the calculation of P ( E ). In PARADISE P ( E ) is calculated using only the columns of the AVM, thus taking only the exchanged information into consideration, independently from who uttered it, system or user. We use the standard formula for P ( E ) which includes rows and columns (Siegel &amp; Castellan, 1988 ), thus taking both system X  X  and user X  X  utterances separately into consideration.
In the next paragraphs we illustrate the calculation of j * on the basis of task 5 (Dial a phone number) for the reference system (Fig. 4 ). In the first place the AVM for the task in question must be completed. The following contributions: raise the value of the AVM cell P HONE _P HONE by one. The misunderstood user contribution: raises the value of the AVM cell R EADY _F AILURE . After the AVM has been com-pleted, P ( A ) and P ( E ) can be computed. For P ( A ) the values of the diagonal cells are added; for P ( E ) the sum of the respective columns and rows are multiplied and the totals are added. In both cases, the summands are divided by the total number of next section we discuss the reasons for the high task completion times. Because this factor highly correlates with the number of turns the given explanation also applies to the observed number of turns. 5.2.1 Discussion of the task completion times Considering the low degree of complexity of the test tasks, the high average task completion times might be irritating. The real dialogue from the reference series shown in Fig. 5 will help explain the high completion times.

The test subject was asked to change the map style (task 5). Under ideal cir-cumstances, it would have taken the user about 5 s to change the map style uttering a voice command (activation of the system plus contribution 8 in the above dialogue). Why did it take the user in the analysed dialogue about 65 s to complete the task? The main reason is the users being novices X  X one of the test subjects had used the assessed speech system before. That explains the pause at the beginning of the dialogue (2). The hesitation in the contribution (3) leads to an unperceived transition for the user: The system understands  X  X  X oard information X  X  and activates the corre-sponding menu and vocabulary. Therefore, when the user asks for options (5) the system tells the speech commands for the board information menu which annoys the user (6) and takes 10 s. After reaching the desired menu (6), the user asks for options again (7). It takes the system 16 s to read the available options. Only then the test subject utters the right command to change the map style (8), thus completing the task. Summing up, the high average task completion times are to a large extent due to faulty user utterances (wrong speech commands, hesitations, etc.), and the reit-eration of long option lists.
 5.2.2 Evaluation of the repetition tasks 10 and 11 The comparison results of the performance for tasks 1 and 2 and their reiterations at the end of the test (tasks 10 and 11) differ from what was expected: we assumed the performance for the repetition tasks would be better in both systems, and especially in the prototype.

The task duration for task 10 is in both systems lower than in task 1, but more remarkable in the reference system. Users of this system needed for task 10 on average only 30% of the time they needed for task 1. Task duration for the repe-tition of task 2 (task 11) decreased only in the reference system. In the prototype these values remained almost the same as for task 2. In the reference system it took 15% of the testers longer to complete task 10 than task 1, and more than twice as many testers of the prototype (38%) needed more time to complete task 10. In the reference system 28% of the test subjects needed longer to complete task 11 than task 2, and in the prototype this was the case for almost twice as many test subjects (48%).

This results can be partly explained by the disparate length of the system prompts: The prototype prompts for novice users offered automatically the available speech commands; in the reference system the prompts only confirmed the user input. Besides, the difference in the prompt explicitness in both systems led to a different user behaviour. It looks as if users of the reference system learned faster that they can speak the tasks they want to activate directly (short-cuts). The help given to the novices in the prototype seems to slow down this insight among the users of this system. They repeatedly applied the same tactics, they followed the menu structure of the system instead of speaking the desired commands directly. 5.3 User satisfaction The questionnaire uses a Likert scale with four choices ranging from strongly op-posed (1) to strongly in favour (4). It consists of four parts: questions about the participant, about his technical background, about user satisfaction, and about the system (this part indicates if the user believed the system to meet the requirements of the Norm DIN EN ISO 9241-10, 1996 ).

We calculated two factors to measure user satisfaction (US 1 and US 2 ). US 1 subsumes three answers to questions about the test:  X  X  X  could complete all tasks without problems X  X ,  X  X  X  find the system easy to use X  X , and  X  X  X  got frequently upset during the test X  X . US 2 subsumes three answers to questions about the system:  X  X  X  would recommend the system X  X ,  X  X  X  really want to have such a system in my car X  X , and  X  X  X  find the system very useful X  X . Figure 6 shows the values for US 1 and US 2 for both systems and over the task completion rate. The higher the values the more satisfied were the users (0: not satisfied at all, 4: very satisfied). For the task completion we defined three categories: the user completed the task alone ( X  X  X es X  X ), the user completed the task with help ( X  X  X artly X  X ) and the user could not complete the task ( X  X  X o X  X ). As Fig. 6 shows, users rated US 1 and US 2 better for the prototype than for the reference system irrespective of the task completion category.
This lack of correlation between the task completion and the user satisfaction was completely unexpected. Only US 1 has some correlation with the task completion rate ( p = 0.58 for the reference system and p = 0.45 for the prototype 8 ). For US 2 we found no correlation at all. In Section 5.4 we discuss the reasons for these findings. For a more detailed review of the subjective assessment of prototype and reference system see (Hassel, 2006 ). 5.4 Discussion of the paradise evaluation Contrary to our expectations, the data from our experiments did not confirm the claim of a correlation between user satisfaction and a success measure together with the cost factors. We have tried every combination of independent and dependent (user satisfaction measures) variables in the performance function. The independent variables were the usual cost factors (number of ASR-Failures, timeouts, barge-in attempts at the beginning of system utterances, number of option and help requests, task duration, and number of turns) and the success measure j * or, alternatively, the success rate. Besides, we have also considered the influence of the gender 9 .
None of variable combinations reached values that could explain the variance 10 as stated in PARADISE: R 2 = 0.92 11 (Walker et al., 1998 ). The best results we obtained were R 2 values of 50% for the reference system and 41% for the prototype. Therefore, we can not apply the multivariate linear regression proposed in PARADISE to calculate a performance function for our systems.
 One reason for the low levels of correlation is that the users X  levels of satisfaction US 1 and US 2 were almost completely unrelated to j * and the success rates. The cause for this finding may lie in the novelty of voice interfaces in the automotive environment. The characteristics of the test subjects largely agreed with those of early adopters: young, urban, and highly educated. For such users, the main goal of Experiments with real customers should be carried out to confirm this hypothesis.
Another reason for the absence of correlation might be the redundancy of the system. Voice interaction is not the only way to provide input but is a possibility in addition to the manual input to operate the comfort tasks available in a car. Therefore, the requirements of the users differ from those for e.g. telephony SDSs, where the voice interface is the sole input possibility. 6 Conclusion We have described the classification of users between beginner and expert, the adaptation of the system prompts to the calculated user expertise, and the evaluation of the performed usability tests.

Our evaluation of the parameters h (Help Requests), o (Option Requests), t (Timeouts) and e (ASR-failures) to predict the user status and the observed user behaviour confirmed the results of (Jokinen, Kanto, Kerminen, &amp; Rissanen, 2004 ): the parameters e and o proved to be the best predictors of the user behaviour. Besides, in spite of the differences between the two systems, the values for all four parameters were in both the reference system and the prototype much the same. This could mean that they are all equally predictive of the user status quite inde-pendently of the SDS design.

The adaptation was assessed in a real driving situation in two test series (a ref-erence system without, and a prototype with adaptation facility). The comparison between systems showed that adaptation contributed to improve usability: All subjective and nearly all objective measures were better for the prototype. While users who could not try adaptation were sceptical about it, the ones that did try wanted to have that feature afterwards 12 . In general, users found the enumeration of the available options a good means to learn the system, but in the long run the enumeration would be tedious. Therefore, and because they knew they could ask for options and help, prototype testers approved of adaptation.

Since no test subject had used the voice interface in the car before, we do not know how experts would cope with the systems. On the one hand, the comparison of tasks 1 and 2 with their repetitions 10 and 11 showed that the learning curve was very steep for the prototype. On the other hand, there is evidence that the extended prompts for novices in the prototype could lead users to operate the system in a less straightforward manner than in the reference system because they did not use shortcuts, i.e. they kept navigating through the menus. The prompts of the prototype become the same as in the reference system when users turn experts. Will experts change their habits and learn the shortcuts? Long-term evaluations have to be performed to investigate the benefit of the proposed features over time.
With reference to the evaluation, all subjective (user satisfaction) and nearly all objective (cost factors) measures were better for the prototype. Contrary to our expectations, we could not find a correlation between user satisfaction and cost factors along with success. Users X  levels of satisfaction were almost completely unrelated to these factors and success measures. One reason for this finding may lie in the novelty of voice interfaces in the automotive environment. The characteristics of the test subjects largely agreed with those of early adopters: young, urban, and highly educated. For such users, the main goal of operating an innovative system is the interaction itself, not task completion. Another reason for the absence of cor-relation might be the redundancy of the system. Voice interaction is not the only way to provide input but is a possibility in addition to the manual input to operate the comfort tasks available in a car. In spite of that, we found j * to be a good measure to characterize how difficult it was for users to accomplish (or try to accomplish) the task.
 References

In the same way that hyperlinks enable the crea-tion of connections betw een documents, current semantic web technologies enable the establish-ment of connections or links between or among pieces of data, information, and knowledge, in what is known as the Linked Data paradigm 1 , with the goal of better exploiting them in linked data-driven Web applications (Hausenblas, 2009). lieve that terminology has much to contribute to this field. In the past, terminology work was ex-tensively applied to the identification of terms and relations for their subsequent transformation into concepts and conceptual relations in ontolo-gies (Velardi et al., 2001; Aussenac-Guilles and 
S X rgel, 2005; Maynard et al., 2008; to mention just a few). Currently, works on terminological variation may play a significant role in the Linked Data linking step. for publishing and connecting distributed data on the Web with the idea of transforming it into a global graph . For this purpose, data must be pre-viously structured according to graph-based models in the form of ontologies, using the stan-dard RDF (Resource Description Framework) syntax. Moreover, these data or information units have to use URIs (Uniform Resource Identifiers) as their names on the Web, and follow the HTTP (Hypertext Transfer Protocol) schema so that users can look up those names and find the in-formation related to them . Finally, data have to be connected to similar data, so that users can explore those data and discover additional data. 
Thus, the more links an RDF dataset has to other datasets, the more useful it will be. one that involves greater difficulties. As stated in (Heath and Bizer, 2011), it is common practice to use the property or relation owl:sameAs to state that one data source in an  X  X DF dataset A X  provides the same information as another data source in an  X  X DF dataset B X . But, is it easy to identify two data sources in different RDF data-sets that mean the same? Can it be done auto-matically, or does it require an expert to analyze and compare the datasets? Ideally, taking into account the number of RDF datasets currently published as Linked Data 2 , this task should be performed automatically. forming data into the linked data format was ini-tially led by English speaking countries, nowadays we find an increasing amount of RDF datasets in languages other than English that need to be linked to similar or related datasets in other languages (G X mez-P X rez et al., 2013). cornerstone in communicative and cognitive ap-proaches to terminology (Cabr X  1995, Daille 2005, Temmerman, 2000) could contribute to the identification of terms that refer to the same on-tological concept, thus attempting to integrate univocity, defended by the traditional theory (W X ster, 1979) with variation in real situations. 
The result of such an analysis could be used in the automatic identification of concepts that mean the same or that hold a certain type of rela-tion. It could also contribute to the definition of the reasons that caused that variation, and pro-pose alternatives to the owl:sameAs property to capture more fine-grained relations between data sources. Finally, from a multilingual per-spective, it could also help to establish cross-lingual relations between RDF datasets in vari-ous languages. 
Data datasets should be enriched with termino-logical variants, as well as with other types of lexical and linguistic information as proposed in (McCrae et al., 2011; Graci a et al., 2012), so that further processes in the Linked Open Data Cloud construction  X  specifically the linking step  X  become smoother and more reliable. Such en-richment could also be very profitable due to its potential exploitation by linked data-driven Web applications. tion of lexical, terminological and semantic vari-ants that has been proposed within the framework of the W3C Ontology-Lexica Com-munity Group 3 to enhance a model of linguistic descriptions intended to enrich domain ontolo-gies and RDF datasets. The model being de-signed in this framework relies on previous computational models of linguistic description, such as LMF (Francopoulo, 2013; ISO 24613), 
SKOS (Miles et al., 2005), or, fundamentally, the lemon model (McCrae et al., 2011). denominative variation in communicative ap-proaches to terminology, in section 2 we revisit previous classifications of terminological vari-ants in the light of the Linked Data paradigm. In other to justify the proposed classification, in section 3 we provide examples of modelling so-lutions for the different types of variants (lexical, terminological and semantic variants). We com-pare the mechanisms provided by available mod-els (SKOS) to represent such variants, in contrast to the richer, more complex model of linguistic descriptions that is being proposed in the W3C 
Ontology-Lexica Community Group and that takes as starting point the lemon model. Finally, in section 4, we present some concluding re-marks and discuss some further lines of research. 
As suggested in Cabr X  (2008), term variants that refer to one and the same concept can be divided into two types: (1) Term variants that are seman-tically coincident but formally different , i.e., terms that mean the same but are expressed by different lexical forms, generally known as synonyms (e.g., eczema vs. skin rash); and (2) 
Term variants that are semantically and formally different , since each one is highlighting one facet or dimension of the same concept (e.g., hospital waste vs. biomedical waste), so that they do not mean exactly the same , but refer to the same concept or real world entity. The same author refers to the latter variants as partial synonyms and leaves open the question of whether the two terms should point to the same concept or each term should point to a different concept, with many assumed commonalities between the two. view of a model that is designed to associate complex linguistic descriptions to conceptual structures (ontologies, RDF datasets), because it informs how lexical and terminological descrip-tions of the concepts are represented. If the con-ceptual structure is already given and contains that conceptual difference (let us say that it makes a distinction between biosanitary waste , in general, and hospital waste , only for the waste produced in hospitals), the two terms will most probably be associated to two different concepts. 
Conversely, if only one concept is represented in the ontology, we may still want to account for both terminological variants in the linguistic model, and explicitly state the motivation behind each denomination. In this way, we would also facilitate the linking of this data source to an-other data source contained in a different dataset and to which only the term biosanitary waste has been associated. the causes that provoked the variation, and has been inspired for the terminological part on the work by Freixa on denominative variation in terminology (2006). In this case, we distinguish between lexical, terminological and semantic or cognitive variants. Each type of variant will be devoted a sub-section below. 2.1 Lexical variants 
For the purposes of this work, lexical variants are defined as those variants that are semantically coincident but formally different , and which are mainly motivated by grammatical requirements, style ( Wortklang ), and linguistic economy (help-ing to avoid excessive denominative repetition and improving textual coherence) 4 . As Freixa (2006: 61) maintains for acronyms and reduc-tions of terms, this lexical variation has a high level of conceptual equivalence. Also, the use of one variant over the other does not really change the intention of the message, but it is rather caused by formal aspects of the text. 2.2 Terminological variants 
As for terminological variants, we understand those variants that are not only formally, but also semantically different , and this difference is in-tentionally caused. As stated in Diki-Kidiri (2000:29 and ff), in order to better understand this type of variants, it may be useful to make a distinction between concept and meaning or sense ( le signifiant, le signifi X  and le concept ), since we could say that these terminological variants refer to the same concept but they repre-sent  X  X he multiple specific perceptions of the same object X . nomination or term itself is a clear indicator of the reasons or causes for variation. As mentioned in Freixa (2006), these reasons can be the origins of the authors, in the case of diatopic variants; the different communicative registers, in the case of diaphasic variants (also termed functional variants); the stylistic or expressive needs of the authors, as for the so-called diastratic variants; and the different conceptualizations, approaches or perspectives underlying them, in what we have termed dimensional variants (dubbed cogni-tive variants in Freixa (2006)). emphasize that it is more common than not to find different conceptualizations of the same domain when different groups approach the same area of knowledge from different perspectives or with different needs. Because of that, some terms may highlight certain properties of a concept, which are not so relevant for other users. This is even more obvious in a multilingual context, in which different geographical, cultural and social groups comprehend reality in different ways. In this sense, we have included a subtype of dimen-sional variants called cross-lingual dimensional variants . lingual variants. It could be argued that these are not terminological variants strictly speaking, but translations. However, we have decided to con-sider them a subtype of variants with the aim of covering those scenarios in the Linked Data con-text in which datasets in different natural lan-guages have to be linked and this linkage becomes essential in this new paradigm. variants. First, we include translations, in the general sense. It is widely accepted that original and target cultures have segmented and catego-rized a bit of reality in a very similar way and have a similar concept a nd equivalent term to refer to it. We do not account for the reasons for this similarity (it may be that one culture has im-ported not only the concept but also the term by providing a loan translation, etc.). However, we account for the case in wh ich the target culture has no equivalent concept and describes the con-cept of the original culture and/or directly reuses the foreign term. also have cross-lingual diaphasic variants , if one language uses the scientific term in all registers, and the other language has two terms: one for an expert-to-expert communication situation and another term for an expert-lay communication situation (e.g., huesos metacarpianos in Spanish vs. Ossa metacarpi and Mittelhandknochen in 
German). The same could happen in the case of diachronic and diastratic variants. a multilingual scenario these terminological vari-ants would be pointing to the same concept or conceptual structure, or even share the same con-ceptualization. This is one of the main differ-ences compared to the variants in section 2.3, namely, the so-called semantic or cognitive vari-ants. 
So, we have classified terminological variants as follows: 2.3 Semantic or Cognitive Variants 
Semantic or cognitive vari ants are mainly caused by different conceptualization and/or motiva-tions. We could say that these term variants are semantically and formally different, as in the case of terminological variants, but they usually point to two closely related, but different, onto-logical concepts, which m eans that they are also conceptually different (Aguado-de-Cea and Montiel-Ponsoda, 2012). lingual level, but we can also found them in monolingual contexts. Let us imagine the case of an ontology or dataset that contains the concept religious building , and another ontology that contains the concept of mosque . At the linguistic level we could say that religious building and mosque are in a relation of hypernymy-hyponymy (one concept is subsumed by the oth-er, but they are referring to two different con-cepts included in two different conceptual structures that have a different granularity level). (general-specific) or horizontal variants. Vertical variants are defined as those variants that refer to concepts that share most properties, but one is more specific than the other (they are not at the same level in a classification tree, but one is more general and the other more specific. See the example of river in English vs. rivi X re and fleuve in French). In the case of horizontal variants, we refer to those terms that point to concepts that share most properties, but one includes proper-ties that the other does not, and vice versa. As a result of these divergences, terms will be point-ing to two different concepts in two different conceptual structures at the same level of speci-ficity in a classification tree, but including une-qual properties. Therefore, we can consider them counterparts or closest equivalents. 
Within this group we find the following types: 
Incorporating all this terminological knowledge in ontologies is important if we aim at optimiz-ing the linking process. Thus, distinguishing dif-ferent forms of term variation turns into a key issue, when we model terminology for practical applications. In the context of linked data this means that we will model the data by means of an existing model such as SKOS or lemon . In this section we present practical modelling ex-amples for kind of term variation. 3.1 Lexical variants 
Lexical variants are modelled by either the mul-tiple forms of the same entry or by means of re-lationships between lexical entries. For example in lemon , we would model orthographic variants as different representations of the same form 6 : 
In the example above, a monolingual lemon lexi-con is defined which contains a lexical entry :theatre_lexicalentry . The concept  X  X heatre X  in a certain ontology is the reference of such lexical entry, which has two associated written represen-tations:  X  X heater X  and  X  X heatre X  for American and British English respectively. 
Alternatively, two different lexical entries could have been defined for each different representa-tion. In that case, a relation can be defined be-tween the lexical entries in lemon in this way: 
In this example  X  X heater X  and  X  X heatre  X  are asso-ciated to two different lexical entries. They are linked by a new relation :diatopicVariant , which is defined as a subtype of a lemon lexical variant. 
Notice that :diatopicVariant does not exist in the lemon model as such, but it can be defined as in the example or, alternatively, an external cate-gory could be used, such as "diatopical" includ-ed in ISOCAT 7 . 
Morphosyntactic variants are also represented as links between lexical entries, as there may be differences in the syntactic properties of the en-tries. Let us consider the term  X  X eer to peer X  and its abbreviated form  X  X 2p X : 
In the previous example :p2p and :peer_to_peer are lexical entries with their respective written representations. Then, ISOCAT categories are defined as lexical variants and used to relate or link both lexical entries 8 . 
We can also use SKOS for representing lexical variants. In that case, we can show two preferred labels for different dialects of a language as fol-lows: 
However, to represent morphosyntactic variants it is necessary to use the extended label model (SKOS-XL) as follows, but, otherwise, it is simi-lar to lemon, where we define a named label en-tity for each label and represent the link between them as a triple. 3.2 Terminological variants 
As terminological variants maintain the meaning of the term while changing the surface form, it is necessary to distinguish between the syntactic and semantic level of the term. For this reason, we characterize terminological variants as links between different senses not between lexical en-tries. In lemon this is easy to model as can be seen in the following example: 
In the example above, an entity of biontology-icd 9 has been associated with two lemon senses. 
Such senses constitute the bridge between the ontology entities and thei r respective lexical en-tries. Then, the temporal dimension ( X  X odern X  vs.  X  X ld X ) can be established as an attribute of the senses (by using the LexInfo vocabulary (Cimiano et al , 2011) in our example). 
Alternatively, a relation at the level of senses can be established in lemon in this way: 
However, there is no way to make this distinc-tion in SKOS and this will lead inevitably to con-fusion about the syntactic and semantic layer. 3.3 Semantic or cognitive variants 
Cognitive variants are distinct but closely related meanings of a word. So, we can model the varia-tion not only as a relationship between words but also as described by a semantic model, i.e., an ontology. As such, we would state OWL axioms to describe the relationshi p, as illustrated in the example below. There, Chancellor of Germany and Prime Minister of Spain are both subclasses of the concept Head of government. 
At the lexicon level we could also establish a relation of horizontal variants between the two terms. This relation is established because we know that the two terms  X  X hancellor of Ger-many X  and  X  X rime Minister of Spain X  are not equal but can be considered similar (or counter-parts) in the two cultural settings, as they have a close antecedent concept,  X  X ead of government X . 
As mentioned in this paper, the Linked Data ini-tiative needs to find ways of linking the huge amount of structured datasets found on the Web in the same or in different languages. We believe that although ontologies aim at achieving uni-vocity in as much as traditional terminology did, the more sociolinguistic cognitive approaches to terminology can also contribute to enrich the current computational models of linguistic de-scriptions. With this purpose, we have revisited previous classifications of term variants in the light of the Linked Data initiative so as to facili-tate the process of recognition of terminological variation. We have proposed a classification of term variants in three wide groups: lexical vari-ants, terminological variants and semantic or cognitive variants. We have also illustrated this classification with the corresponding examples at the ontology level by resorting to different ontol-ogy representation models, such as lemon . With the solutions proposed we also aim to enrich the linguistic ontology models as well as to make them more reliable when applied to Linked Data. 
This work has been sup ported by the BabelData (TIN2010-17550) Spanish project and the FP7 European project Monnet (FP7-ICT-4-248458). 
Aussenac-Gilles, N. and D. Soergel. 2005. Text anal-Cabr X , M.T. 1995. On diversity and terminology. 
Cabr X , M.T. 2008. El principio de poliedricidad: la articulaci X n de lo discursivo, lo cognitivo y lo lin-g X  X stico en Terminolog X a (I). IB X RICA 16: 9-36. 
Cimiano, P., P. Buitelaar, J. McCrae, and M. Sintek. 2011. LexInfo: A declarative model for the lexi-con-ontology interface. Web Semantics: Science, 
Services and Agents on the World Wide Web, 9(1): 29-51. 
Daille, B. 2005. Variations and application-oriented 
Diki-Kidiri, M. 2000. Une aproche culturelle de la Francopoulo, G. (ed.) 2013. LMF Lexical Markup Framework, Wiley-ISTE. G X mez-P X rez, A. Vila-Suero , D. Montiel-Ponsoda, E. 
Gracia, J. and G. Aguado-d e-Cea. 2013. Guidelines for multilingual linked data. In Proceedings of the 3rd International Conference on Web Intelligence, Gracia, J. Montiel-Ponsoda, E. Cimiano, P. G X mez-
Heath, E. and Ch. Bizer. 2011. Linked Data: Evolving 
ISO 24613 -LMF-Lexical Markup Framework, Lan-
Jacquemin, C. 1997. Variation terminologique: re-
Maynard, D., Y. Li, and W. Peters. 2008. NLP Tech-
McCrae, J., G. Aguado de Cea, P. Buitelaar, P. Ci-Miles, A., B. Matthews, D. Beckett, D. Brickley, M. 
Temmerman, R. 2000. Towards new ways of termino-
Velardi, P. Missikoff, M. and R. Basili. 2001. Identi-
W X ster, E. 1979. Introduction to the General Theory In spite of the progress reached in the field of 
Medical Informatics, it is still difficult for Health care professionals and other health actors to reg-ister and codify clinical data in daily practice . 
The link between Electronic Health Records (EHRs) and international English classification systems used to codify the clinical data is often complex, not well integrated and hampered by a translation gap, both in terms of language and of world of reference (patient, general practitioner, medical speciali st, etc.). the goal of semantic interoperability entails con-struction of interface terminologies, defined by (Rosenbloom et al., 2006) as a  X  X  systematic co l-lection of healthcare-related phrases (terms) to supp ort clinicians X  entries of patient -related in-formation into computer programs such as clin i-cal  X  X ote capture X  and decision support tools, facilitating display of computer-stored patient information to clinician users as simple human-readable texts X  X . These kinds of interface term i-nologies can be used for problem list entry, clin i-cal documentation in EHRs, text generation and care provider ord er entry with decision support. 
The question is whether existing interface term i-nologies are sophisticated enough to support s e-mantic interoperable c om munication of the clinical data between partners in a multilingual health care system. Until now, interface term i-nologies have been either limited to one lan-guage (often English), or either providing an interface to only one nomenclature (e.g. SNOMED) or classification (e.g. International Classification of Diseases). tectural structure for a multilingual medical in-terface terminology, following both lexical and terminological ISO standards on multilingual terminologies and using Semantic Web techno l-ogies and languages (RDF/OWL, SPARQL, 
Linked Data, etc.). The ambition of this multilin-gual resource for general practitioners, patients, medical professionals, and allied health person-nel is to span the gap between human language (as addressed in Natural Language Processing systems) and machine language (used to manage concepts and their lexical representations) and to map to a variety of well-respected international medical nomenclatures, thesauri and classific a-tion systems). gives an overview of the state of the art in the field of Medical Terminology. Section 3 d e-scribes the approach to building the structure for a hybrid interface terminology. Section 4 is d e-voted to present a use case on Heart Failure and preliminary results. Finally, Section 5 provides some discussion and conclusions. 
Over the last two decades, research on medical terminologies and classification systems has b e-come a popular topic and much work has been done to map between several nomenclatures ent in structure and purposes and used by phys i-cians during their patient s X  health care visits. on the extensive use of UMLS as a knowledge resource for extracting semantic mappings ( see Fung and Bodenreider, 2005). ogies in the biomedical domain has lead ed to promising results in terms of information int e-gration across heterogeneous resources. Exa m-ples are the use of Resource Development 
Framework (RDF), triple stores and SPARQL queries in integrating consumer-oriented term i-nologies with standard classification systems in 
UMLS (Cardillo et al., 2012), or for aligning standardized nomenclatures with thesauri (Bodenreider, 2008). Many tools have been cr e-ated for automatic alignment between resources. more useful to ontology alignment. Results of these kind of alignment algorithms are recorded in healthcare is the formalization of existing medical terminologies or classification systems in ontologies. SNOMED-CT (Rector and Brandt, 2008) and the upcoming ICD-11 (Tudorache et al., 2010) represent a good example of this pra c-tice. Important is also the creation of medical on-tology repositories, such as Bioportal 10 , a We b-based open repository where users can search and browse biomedical ontologies as well as cr e-ate mappings for them. useful, above all if integrated in EHRs or in Pe r-sonal Health Records (PHRs), since they give structuring and semantics to the recorded info r-mation. With predefined mappings of vocabula r-ies used in the original data, they also allow for aggregation, reuse of knowledge, automated re a-soning on data, search and retrieval of data from diverse original source systems. launched for the creation of consumer-oriented terminologies or classification systems. Exa m-ples of this challenge are: the Open Access Co l-laborative Consumer Health Vocabulary for 
English, developed by (Zeng et al., 2006) and available in the UMLS Metathesaurus; the Italian 
Consumer Medical Vocabulary (ICMV), deve l-oped by (Cardillo, 2011) using a lexi-ontological approach, and the Multilingual Glossary of Popular and Technical Medical Terms, in nine of the European Commission. of application in the domain of medical term i-nology of two ISO terminological standards, namely Lexical Markup Framework (LMF, ISO 24613, 2008) and Te rminological Markup 
Framework (TMF, ISO 16642, 2003), the ab-sence of a multilingual approach. 
We propose a Medical Interface Terminology, that is composed of two types of domain-specific resources: a multilingual reference terminology (linked to international classifications, thesauri and nomenclatures), on the one hand, and unilin-gual end-user lexicons the other hand. nology to be used in the primary domain for in-formation storage, encoding, translation, and retrieval, we used a hybrid approach that consists in the combination between onomasiological and semasiological approaches. On one hand we build the structure of the reference terminolo gy starting from essential concepts of the domain and then look for their lexical representations in different international classificatio ns. On the other hand, we build also the structure of an end-user lexicon following the opposite approach, so starting with a word (or phrase) and looking for its different meanings, and in particular to which concept in the onomasiological resource the wo rd refers to. As mentioned in Section 1., to build this interface terminology we advocate our choice for standard frameworks, using the ISO norms on Terminological Markup Framework (TMF) and on Lexical Markup Framework (LMF), whose meta-models perfectly fit with our requirements. For each type of resource of the interface terminology, we explain our approach to apply the mentioned standards, and to build the two recourses with data categories and do-data category registry (ISO 12620, 1999). Fina l-ly, we describe the approach for publishing these resources as Linked Open Data (LOD). 3.1. Creating a TMF based Reference 
A reference terminology comprehensively and rigorously defines reference concepts and ex-pressions within the biomedical domain, includ-ing interrelations between concepts (Rosenbloom et al., 2006), and provides a common reference point for comparisons and aggregation of data about the entire health care process, recorded by multiple different individuals, systems or institu-tions. It allows the concepts to be defined in a formal and computer-processable way and to be mapped to existing standard nomenclatures and classification systems. This allows to support consistent and understandable coding of clinical concepts and so is a central feature for use in EHRs. lingual reference terminology, following the standard TMF (Romary et al., 2006), conceived to structurally address multi lingualism. Th e TMF meta-model, which keeps the traditional onomasiological view of a terminological entry, decomposes the organization of a terminological database into five basic components: i) the te r-minological resource (Terminological Data Co l-lection); ii) the concept (Terminological Entry) ; iii) the language chosen (Language Selection) ; iv) the term(s) in the language chosen (Term En-try); v) components of the term (Term Comp o-nent Section). internationally accepted data categories, relevant to the functionalities of the reference terminolo-gy (e.g., entrySource , languageIdentifier , preferredTerm ) that have been selected from the already mentioned ISOcat.org platform. Detailed explanations on the TMF meta-model and on the selected data categories can be found in (Roumier et al., 2011). 
Markup Language (TML) that comprises the mappings between the Meta-model, the data cat-egories and their domain values. The resulting 
TML has been serialized using the XML schema we were inspired by the TML created in the TermScienc es 15 project (Khayari et al., 2006). cepts to a respectable minimum (between 7.000 and 15.000 concepts). For each concept, the level of granularity is assessed. Pre-coordinated con-cepts are kept to a minimum (based on frequen-cy-of-use criteria). el (language independent), selected concepts are incorporated in the TMF resource. To each con-cept a definition in English is assigned and i f possible, a perfect match for that concept is looked for in the SNOMED-CT nomenclature, or in the UMLS Metathesaurus. Otherwise, the concept is genuinely defined within the system. 
Each concept in the reference terminology is cat-egorized according to medical categories (sym p-tom, disease, medical procedure, body part, etc.) . 
In addition, to ensure semantic interoperability, links to international terminologies (SNOMED-
CT and UMLS) and classification systems (ICD 
ICPC-2) are provided, with the corresponding qualitative nature of the mapping (exact match, nearly exact, match to higher or lower level of granularity, match not possible). guage dependent) the concept is labeled with one preferred term (the most suitable clinical term used by physicians to coin the concept in this language) and one admitted term (the most suit a-ble lay term used by consumers and patients to coin the concept in that language) for each pa r-ticipating language. In case the concept has a 
SNOMED-CT exact match, a  X  X tandardized term X  is also given (literal translation of the fully specified SNOMED-CT name). hierarchical (broader-narrower) links or links between related concept into our system, nor any other attempt to self-generated ontological map-ping. We decided to rely on the mappings to the international nomenclatures and classifications to explore the semantic relations between the con-cepts in our resource, and not to invest energy in the possibly redundant activity of creating a new ontology. 3.2. Creating an LMF based end-user 
The second type of resource in the interface te r-minology is a series of unilingual end-user lex i-cons that must be linked to the multilingual reference terminology described above. These end-user lexicons (one for each language) can be linked to Natural Language Processing (NLP) applications, and can be oriented to patients and to professionals. common standardized framework for NLP lex i-cons. This model deals with linguistic complex i-ties, and, as TMF, uses the ISOc at source for the association of linguistic data categories (e.g., partOfSpeech , namedEntity ). Furthermore it can be linked to TMF and other concept based repr e-sentation systems. Resources in LMF can also be linked to existing lexical NLP resources (such as 
WordNet). LMF meta-model contains a mech a-nism to deal with multiple senses (Francopoulo et al, 2007 ). words (or phrases) that are selected from everyday interactions between doctors and patients (occurrences in medical records, in guidelines, in web consultations, etc.), based on frequency count and relevance. Various methodologies for human or automated term extraction can be used. For each of the selected words (or phrases), the possible senses (i.e. medical and non-me dical ones ) are clearly defined and entered as such in the LMF end-user lexicon of the originating language. type of resource, the multilingual reference terminology and the end-user unilingual lexicons: 
In the unilingual end-user lexicons, links to syn-onyms for selected words (or phrases) in the s e-lected sense can be provided. At this level, also subtle differences between related languages (e.g. Portuguese in Portugal and in Brazil, Eng-lish in the UK and in the US, Dutch in the Neth-erlands and in Belgium) are addressed. 3.3. Publication in Linked Open Data 
The TMF model of the reference terminology, described in Section 3.1, is implemented as an 
OWL-DL ontology, which is an efficient way of defining the components (represented as hierarchically organized classes) and the vocabulary (consisting of data and object properties that can be easily reused in other data sets) and ensuring the consistency of the data. classes, while data categories are represented as 
OWL object properties. Classes, whenever possible, are linked to similar concepts in other international recognized classifications, available on the web as Linked Data, using the owl:equivalentClass property (which, in this case, should be preferred to owl:sameAs to avoid undesired effects when using reasoners (Halpin et al., 2010). Because a one-to-one correspondence c annot always be found between all the classifications, in some cases entries can be grouped in more general categories with the owl:unionOf property. 
OWL/RDF allows for the publication of the data accessible via SPARQL queries. Linked Data principles encourage reuse, reduce redundancy, maximize its real and potential inter-connectedness, and finally enable network effects to add value to data (Bizer et al., 20 09). proposed model for modeling lexicons and m a-chine-readable dictionaries based on LMF and, similarly to our TMF terminology, using Seman-tic Web technologies and ISOcat data categories (McCrae et al., 2012). The functionality to pub-lish the content of the lexicons in Linked Open 
Data is an established part of the LEMON framework. 
To populate the structure of our interface terminology with a test sample, we have chosen to extract relevant concepts and words or phrases from a Belgian bilingual (Dutch and French) guideline on Heart Failure for general practitioners, published by the Scientific Associations of Primary Care Physicians (Van 
Royen et al., 2011). For this study we worked with the French version as a starting point. We describe a manual concept extraction and an automated term extraction, along with the procedures to populate both the TMF and LMF-
LEMON resources. 4.1. Concept extraction to the TMF 
A general practitioner, expert in medical classif i-cations, analyzed the French version of me n-tioned guideline, and after a careful reading and tagging, selected 168 concepts, relevant for the clinical domain of heart failure and pertaining to the reference world of general practice. entered in the TMF resource, at the Term Entry section of the French language section. concept, together with a reference to the French guideline from which the concept was extracted. 
For each concept, a preferred term (representing the technical term used by physicians) and one or more admitted terms (representing the lay term used by patients for that concept) were chosen . 
For some technical terms the corresponding lay term was a simple description of the term itself (e.g. the French admitted term  X  eau dans le ventre  X  for the technical term  X  ascite  X ).

Entry level was addressed. The corresponding concepts were looked up in ICPC-2, the reference classification for general practice. 
Then, the cross mapping to ICD10 were sought after. Next a search in the SNOMED-CT web browser was made and finally the relevant corresponding definitions were extracted from the UMLS Metathesaurus, using a dedicated tool. In case a perfect match with a SNOMED-CT concept was possible, its Fully Specified 
Name was entered (as well as its French literal translation). In case the concept could not be matched in SNOMED-CT, a definition in 
English was sought (and the event recorded for notification to the international SNOMED-CT governance group IH TS DO ). concepts selected from the guideline, 153 were mapped to ICPC-2, 131 to ICD-10, 161 concepts to SNOMED-CT, and 116 to UMLS definitions. 
SNOMED-CT were the too broad and general nature of the selected concept, and mismatches between  X  X orld of references X . For instance, in the guideline, the concept  X  X exual problems X  was repeatedly used at this broad level to convey information (and thus necessary in the communication). Some concepts (e.g. drug side effect  X  X ry mouth X ) were not within the scope of 
SNOMED-CT, as the category  X  X ymptom and complaint X  (very important for primary care) is alien to SNOMED-CT. Both UMLS and 
SNOMED-CT use semantic types to categorize concepts. However, the conceptual framework of these semantic types is different, which also leads to difficulties in finding an exact conceptual match for locally used medical terms in international nomenclatures. With regard to mapping to ICD-10, we noticed that 15 of the 168 concepts referred to aspects of functional status (e.g. nutritional status, exercise intolerance), which is difficult to represent in 
ICD. This provoked difficulties to map to this classification (oriented towards morbidity and mortality classification). Similarly, a number of terms related to medical procedures could not be mapped. Regarding ICPC-2, some of the selected concepts were mapped to more than one ICPC-2 rubric (e.g.  X  X lcoholic cardiomyopathy X  mapped to  X  X eart disease other X  (K84) and  X  X hronic alcohol abuse X  (P15)) and in most cases they were mapped to broad rag bag ICPC-2 rubrics as in the case of the concept  X  X aladie de Paget X  mapped to the ICPC-2 rubric  X  T99 -M usculoskeletal other diseases X  . section was populated with a translation of the preferred term and of the admitted term. This was performed with the help of Italian speaking domain experts and a terminologist, sustained by mapping to an existing Italian medical vocabulary oriented to healthcare consumers (the already mentioned ICMV). A similar work is to be made for English and Dutch language sections, and further expanded as described above. 4.2. Term extraction to the LMF resource. 
The two versions (Dutch/French) of the heart failure guideline (30 pages of text) were submi t-ted to the term extraction program TExSIS (Macken et al., 2013) , which provides tokeniz a-tion, part-of-speech tagging, lemmatization, d e-tection of phrases, named entity recognition, and bilingual sentence alignment. The term extra c-tion resulted in an aligned bilingual glossary of 774 words and phrases. matching to the French preferred terms of con-cepts in the TMF resource was performed. Su r-prisingly only 77 French preferred terms among 168 had a string match with the 774 word and phrases extracted by TExSIS. These 77 French terms were then entered in the French LMF -
LEMON resource and linked to the identification number of the concepts in the TMF resource. the TMF resource, resulting into an increase of entries in the LMF-LEMON resource to 138 lex-ical entries. For 16 admitted terms there was no difference with the French preferred term. 
Among the 138 lexical entries, 114 were phrases, and were then decomposed into single words, the total of entries to 298 in the LMF-LEMON r e-source. This rapid increase in the number of terms is however unlikely to be linear, since some words are part of several phrases (e.g.  X  insuffisance cardiaque  X   X   X  insuffisance r X nale  X  ). mation, such as part-of-speech tag, lemma, spelling variants and inflected forms. The med i-cal senses of the entries are linked to the refe r-ence terminology, and the other senses to 4.3. Publication of the TMF and LMF-
The tabular data representing the TMF resource on Heart Failure were automatically converted into 16.636 RDF triples, assigning a unique iden-tifier to each element, and serializing it in RDF/XML. We published the resulting file (in can be freely available for computer applications. server along with a SPARQL endpoint. By the combined use of URL rewriting techniques and 
SPARQL construct queries, triples are given a more convenient access. 
Also the LMF-LEMON resource is represented in RDF triples from the start, and hence automa t-
The following table show some statistics on the use case of Heart Failure. TMF Concepts 168 Preferred Terms 168 168 Admitted Terms 168 168 Standardized Terms 161 161 161 UMLS definitions 116 Links to ICD-10 131 Links to ICPC-2 153 TMF Triples 16,636 LMF entries 298 LMF Triples 3,400 
In this article, we presented the architectural structure of a multilingual medical interface te r-minology with an ambitious set of objectives. 
We presented a first attempt, with in a small use case, to create a sophisticated, hybrid medical interface terminolo gy, aiming at multilingual so-lutions, at semantic interoperability, and at open source availability as Linked Open Data in the 
Semantic Web. For resources as precious as lan-guage and international terminologies a propri e-tary approach would not be appropriate. munication is not tested yet in a clinical setting. 
Preliminary results indicate the need for a con-certed quality control of the process of  X  X ord s/concepts  X  sele ction. To improve results, a refinement of the mapping approach (manual, so time and cost consuming) is needed, trying to investigate semi-automatic approaches relying on Semantic Web technologies, as done in (Cardillo et al., 2012).
 ing internationalization of health care (near bo r-der, cross border and intercontinental health care exchange including migrants health issues). In fact, in order to provide semantic interoperability between different healthcare information systems and between different health actors and patients, multilingual access to international terminologies is needed. tem, relying on ISO standards and Semantic Web languages and tools, published as Linked Open 
Data supports : (i) the efficient use of existing medical terminologies and their legacy data in the activity of clinical encoding ; (ii) links b e-tween professional language and lay language, and healthcare information system integration (e.g. between EHRs and PHRs) ; (iii) multilin-gualism in its core approach to semantic interop-erability; (iv) information retrieval, and (v) creation of information through epidemiological research. Our interface terminology will enable physicians to find the right medical entry at the right moment at the point of care, and to int e-grate their data with standard classifications for their encoding, being consistent with the r e-quirements of their Health Authority (e.g. the mandatory use in the General Practice of ICPC, or ICD as coding system for diagnoses or prob-lem lists), and finally to exchange their data with other healthcare professionals and within various healthcare information systems in an interoper a-ble way. coverage of our reference terminology selecting other use cases (e.g. contra-indications of med i-cation) and to extend the range of participating languages. based management platform for the extension and maintenance of the proposed Multilingual Medical Interface Terminology. 
This study has been done under the MERITERM consortium, devoted to joined research activities on medical terminologies and classification sy s-tems. The authors wish to thank the consortium for the support. 
Christian Bizer, Tom Heath, and Tim Berners-Lee. 2009. Linked data-the story so far, International 
Journal on Semantic Web and Information Sy s-tems, 5(3), 1-22.

Olivier Bodenreider. 2008. Comparing SNOMED CT and the NCI Thesaurus through Semantic Web 
Technologies. In proceedings of the 3rd Intern a-tional Conference on Knowledge Representation in Elena Cardillo, Germano. Hernandez, and Olivier 
Elena Cardillo. 2011. A Lexi-ontological Resource Gil Francopoulo, Nuria Bel, Monte George, Nicoletta 
Kin W. Fung and Olivier Bodenreider. 2005. Utilizing 
Harry Halpin, Ivana Herman, and Patrick J. Hayes. 
International Standard Organization. 1999. ISO 
International Standard Organization. 2003. ISO 
International Standard Organization. 2008. ISO 
Lieve Macken, Els Lefever, and Veronique Hoste John McCrae, Guadalupe Aguado-de Cea, Paul and Tobias Wunner (2012). Interchanging lexical resources on the semantic web. Language R e-sources and Evaluation, 46(4):701  X  719.

Majid Khayari, St X phane Schneider, Isabelle Kramer, and Laurent Romary. 2006. Unification of multi-lingual scientific terminological resources using the ISO 16642 standard. The TermSciences initiative, LREC 2006, International Workshop 
Acquiring and representing multilingual, specialized lexicons: the case of biomedicine. http://hal.archives-ouvertes.fr/hal-00022424 . 
Alan. L. Rector and Sebastian Brandt. 2008. Why do it the hard way? The case for an expressive d e-scription logic for SNOMED. Journal of American Medical Informatics Association, 15(6):744  X  751. Laurent Romary, Isabelle Kramer, Susanne Salmon-
Alt, and Joseph Roumier. 2006. Gestion de donn X es terminologiques: principes, mod X les, methods. Terminologie et acc X s  X  l'information. 
Widad Mustafa El Hadi (Ed.), Hermes science publ, Paris, France. S. Trent Rosenbloom, Randolph. A. Miller, Kevin B. 
Johnson, Peter I. Elkin, and Steven H. Brown. 2006. Interface terminologies: Facilitating direct entry of clinical data into electronic health record systems. J Am Med Inform Assoc, May-June, 13(3):277-288. Joseph Roumier, Robert Vander Stichele, Laurent Romary, and Elena Cardillo. 2011. Approach to the Creation of a Multilingual, Medical Interface Terminology. Workshop Proceedings of the 9th International Conference on Terminology and Artificial Intelligence, 13-15.M. Tania Tudorache, Sean M. Falconer, Csongor Nyulas, 
Natalya F. Noy, and Marc A. Musen. 2010. Will semantic web technologies work for the develo p-ment of icd-11? In Proceedings of the International Semantic Web Conference (ISWC2010): 257  X  272. Paul Van Royen, Pierre Chevalier, Gilles Dekeulenaer, Martine Goossens, Philip Koeck, 
Michel Vanhalewyn, and Paul Van den Heuvel. 2011. Recommandation de Bonne Pratique Insuffisance cardiaque. Domus Medica. SSMG. Belgium. 
Qing Zeng and Tony Tse. 2006. Exploring and D e-veloping Consumer Health Vocabularies. Journal of the American Medical Informatics Associ a-tion,13:24  X  29. 
Given the recent trend toward the attainment of dynamic lexical resources (Calzolari, 2008) in service-oriented Web environments, a process that can efficiently, and in some cases on-demand/on-the-fly, interlink lexical elements across resources is in high demand. In particular, an inexpensive computational method for finding possible con-ceptual mates in another language (the target lan-guage (TL)) from a lexical concept in a given lan-guage (the source language (SL)) can contribute to the realization of a virtually combined multilin-gual lexical-semantic resource on top of existing monolingual lexical-semantic resources. Here, a conceptual mate in TL means a lexical concept that denotes almost the same or a closely related concept to the one in SL.

Given this motivation, this paper proposes a method of discovering conceptual mates in the lexical-semantic resources of other languages. To accomplish this, the proposed method integrates two types of cross-lingual semantic relatedness by balancing them using a weighted sum: The first, synonym-based relatedness , estimates the proba-bilistic correspondence between sets of synonyms, while the other, gloss-based relatedness , measures the textual similarity between gloss texts.

For synonym-based relatedness, this pa-per adopts a method recently proposed by us (Hayashi, 2012), which effectively employs a sense-tagged corpus in the TL and existing bilingual dictionaries. Conversely, for gloss-based relatedness, this paper relies on the extended gloss overlap method proposed by Banerjee and
Pedersen (2003). However, because their method was developed to calculate monolingual textual similarities, prior to using them, we translate glosses in one language into another language using off-the-shelf machine translation engines.
This paper empirically discusses an optimum mean for integrating synonym-based and gloss-based semantic relatednesses, while examining the range of gloss extension. In addition, this paper investigates the impact of machine trans-lation by comparing the performances with those achieved when presumably ideal gloss translations are available.

The proposed method can contribute to the real-ization of dynamically combined lexical resources in service-oriented environments (Hayashi, 2011) because it is computationally inexpensive.

Although the proposed method is essentially lan-guage independent, we here set our current task as the discovery of English conceptual mates in Princeton WordNet (PWN) (Miller and Fell-baum, 2007), given a concept in the EDR elec-tronic dictionary (EDR) (Yokoi, 1995). Note that even though the EDR is organized bilingually in
Japanese and English, in principle, we only em-ployed information given in Japanese, leaving the corresponding English information for reference purposes only. (See Appendix-A for a more de-tailed description of EDR.) 2.1 Cross-lingual semantic relatedness
In order to find the best-matched conceptual mates in a TL lexical-semantic resource for a given SL lexical concept, the proposed method establishes a ranked list of candidate lexical concepts in the TL (a partial example is shown in Figure 1) by com-puting cross-lingual semantic relatedness scores score ( s, t ) , which are defined by Formula (1).
Just to be safe, s and t denote lexical concepts in SL and TL, respectively. As clearly indicated in the formula, score ( s, t ) is computed as the weighted sum of pscore ( s, t ) and gscore ( s, t ) ; the former, synonym-based relatedness, refers to cross-lingual semantic relatedness based on syn-onymous words (i.e., words that jointly specify a lexical concept), while the latter, gloss-based re-latedness, measures cross-lingual semantic relat-edness based on the textual similarity between the (extended) glosses. Finally, 0 . 0  X   X   X  1 . 0 indi-cates the blending ratio of these elements. score ( s, t )  X  (1  X   X  ) pscope ( s, t )+  X gscore ( s, t ) 2.2 Synonym-based relatedness: pscore ( s, t )
We adopt the method proposed by Hayashi (2012) to compute the synonym-based related-ness, pscore ( s, t ) . As defined by Formula (2), it is computed as a weighted sum of pscore  X  ( x , which gives the cross-lingual semantic relatedness between x of an SL lexical concept s , and a TL lexical con-cept t . In the formula,  X  ( x tion for assigning a weight to pscore  X  ( x stated,  X  ( x count. pscore ( s, t )  X  2.2.1 pscore  X  ( x
A probabilistic interpretation is given to pscore  X  ( x i , t ) , as shown in Formula (3). pscore  X  ( x, t )  X  p ( t | x ) = p ( t )
In the formula,  X   X  W ( x ) denotes a set of translation words for  X  p ( y j | t ) dictates the posterior probability of  X  p ( y j | x ) represents the translation probability 2.2.2  X  ( x
As proposed by Hayashi (2012), we also utilize the following formula to compute  X  ( x
The terms on the right hand side of the formula are as follows:  X  n idf ( x i ) , which, inspired from the normal- X  tset ( x i , s ) , which measures the representa- X  toverlap ( s, t ) , which measures the similar-2.3 Gloss-based relatedness: gscore ( s, t )
In this paper, we propose that gloss-based relat-edness be computed by Formula (5), which incor-porates the notion of extended gloss overlap first proposed by Banerjee and Pedersen (2003). gscore ( s, t )  X  candidates are relevant and irrelevant, respectively. the cross-lingual cases, we utilize machine trans-lation, for which the relevant function is in the for-mula represented by  X  that translates a text string in SL to the correspond-ing one in TL. In our experiments reported below, we utilized four distinct Web service Japanese-to-English machine-translation engines 1 , includ-ing Google Translate.
 score is computed by averaging over the text similarity scores ( T extSim ( x, y ) ), each resulting from a textual combination defined in R . That is, the pair ( r combination considered by a calculation of text similarity. Here, r inventory { gloss, hyper, hypo } ; while, gloss ( s ) denotes the gloss text given in the lexical resource for the lexical concept s ; and hyper ( s ) / hypo ( s ) denotes the concatenation of the gloss texts given in the lexical resource for the hypernym/hyponym concepts of s . Note that an instance of R rep-resents a particular strategy for extending gloss texts. 3.1 Test dataset
We utilized the same test dataset described by (Hayashi, 2012): comprising 196 query concepts and the relevant judgment annotations. 1. Query concepts: 196 concepts were ran-2. Candidate PWN synsets: By applying the 3. Relevance judgments: We established two 3.2 Evaluation measures
Because of the following obvious correspon-dences, the experimental task itself was quite sim-ilar to that of information retrieval (IR): the whole
PWN is a document set; a PWN synset can be a document; and a given EDR concept corresponds to a query reflecting a user X  X  intent. We therefore adopted the following two IR-based measures to assess the performance of the proposed method.  X  MAP (Mean Average Precision): This mea-3.3 Design of the experiments
We organized the experiments by considering the following parameters. More specifically, we con-ducted one experimental run using a combination of these conditions.  X  Combination of extended gloss text: We con- X  Text similarity measure T extSim ( x, y ) : The 4.1 Main results 4.1.1 S@1 measure tual combinations A through D, where the best scores at both Rel-level and Syn-level are listed.
Displayed immediately below the scores are the type of vector (rV: raw frequency vector, wV: weighted vector) and the optimum blending ratio  X  .
 glosses, even extended ones, are ineffective for improving the S@1 measure, because the best score (0.776) is achieved when  X  is zero. How-ever, the S@1 measures are more important in considering performance at Syn-level.
 (0.474) is achieved through combination A, which uses the weighted vectors with  X  = 0 . 2 . This combination only employs non-extended glosses.
The score declined when we applied more com-plicated textual combinations (B through D), but the differences are not statistically significant (p = 0.594, 0.504, 0.166, respectively; paired t-test).
The results may indicate that the task of finding a synonymous conceptual mate in the first place can be affected by the noises inevitably introduced by more extended textual glosses. 4.1.2 MAP measure
Table 3 summarizes the best results obtained for the MAP measure. Unlike the results obtained using the S@1 measure, these results in general show that extended textual glosses effectively im-prove MAP performance.

In the table, it can be seen that the best score at Rel-level (0.695) is achieved when combination
C is adopted with  X  = 0 . 6 , and the differences among combinations A, B, and D are statistically significant (p = 0.011, 0.035, and 0.027, respec-tively; paired t-test). This suggests that moder-ately extending gloss texts is beneficial because combination C considers more combinations of extended-gloss texts compared to combination B, but not as lavishly as combination D.

At Syn-level, on the other hand, combination D yields the best result, and combination A the sec-ond best. Although MAP performance is vastly more important at Rel-level, we still need to closely investigate the Syn-level results because they may not correlate with the results at Rel-level.
In summary, these results suggest that extended textual glosses effectively improve the MAP mea-sure, in particular at Rel-level. However, there might also be a better solution than the current strategy: more controlled extension strategies that rather deal with the queries in a nonuniform man-ner by taking into account the characteristics of each individual query concept and its textual gloss can and should be developed. 4.1.3 Blending ratio blending ratio  X  . Figure 2 displays S@1 per-formance and Figure 3 displays MAP perfor-mance; both sweep the value of  X  from 0.0 to 1.0. Note that these figures display the results only for the textual combination C with tf  X  idf -based weighted vectors.
 S@1 results: Figure 2 shows that the scores at
Rel-level and at Syn-level decrease sharply imme-diately after  X  departs 0.0; however, the scores are relatively stable in the range  X  = 0 . 4  X  0 . 8 . At
Rel-level, the score at  X  = 0 . 4 is almost the same as that at  X  = 0 . 0 ( 0 . 775  X  0 . 770 ). At Syn-level, the maximum score (0.454) is obtained at  X  = 0 . 6 .

The scores at  X  = 1 . 0 , which indicates that only gloss-based relatedness are employed, are never better than those of any other  X  s. Consequently, it can safely be said that adopting  X  somewhere between 0.4 and 0.5 is not a bad solution when we consider the simultaneous MAP performance dis-cussed below. In the figure, it can also be seen that the difference at the Syn-level performance between  X  = 0 . 0 / 0 . 6 is not statistically signifi-cant (p = 0.089), while that at the Rel-level be-tween  X  = 0 . 6 / 1 . 0 is statistically significant (p = 0.007).
 MAP results: Figure 3 shows that scores at both
Rel-level and Syn-level improve as  X  increases and are almost stable in the range  X  = 0 . 2  X  0 . 6 .

The best scores for both levels are achieved at  X  = 0 . 6 : 0.695 at Rel-level and 0.525 at Syn-level, respectively. In the figure, it can also be seen that the differences in performance between  X  = 0 . 0 / 0 . 6 and  X  = 0 . 6 / 1 . 0 at the Rel-level are both statistically significant (p = 0.001 and 0.000, respectively). Notably, at Rel-level, the score at  X  = 1 . 0 exceeds that at  X  = 0 . 0 ( 0 . 695 &gt; 0 . 623
This suggests that if we can adopt only one type of solution to achieve better MAP performance at Rel-level, it should be gloss-based relatedness, not synonym-based relatedness. In conclusion, we can safely say that the proposed method, balanc-ing pscore ( s, t ) and gscore ( s, t ) by the blending ratio  X  is not only effective but also relatively ro-bust, as the performances examined were not very sensitive to variations in  X  . 4.2 Impact of machine translation To assess the impact of machine translation (MT),
Table 4 compares the results achieved by the machine-translated textual glosses (MT all col-umn) with those yielded by the original English glosses given in the EDR (EDR column). Assum-ing the English glosses are reasonably adequate, they simulate a situation in which we can obtain  X  X deal X  translations.

As can be imagined, the results yielded by the original glosses outperforms those of the MTed glosses in virtually all cases, but the differences are relatively minor and statistically insignificant (p = 0.257, 0.358, and 0.394, respectively; paired t-test). On the other hand, the MAP performance at Rel-level (which we consider the most impor-tant) achieved by MTed glosses is even better than that of the original EDR glosses, but again the difference is statistically insignificant (p = 0.308).
These results may suggest that the issue of vocab-ulary mismatch can be partially solved by incor-porating as many translation texts from different translation engines as possible.
 umn of the table lists the results obtained when we used only Google Translate from among the four available translation engines. The figures in the related columns demonstrate that more translation engines can yield better results: the differences in the S@1 measure are not statistically significant (p = 0.089 for Rel-level and p = 0.138 for Syn-level), but the differences in the MAP measure clearly exhibited statistically significant differences (p = 0.000, 0.0426, respectively).
 tual glosses does not appear to be as harmful as reported in previous ontology-matching litera-ture Fu et al. (2009); McCrae et al. (2011) because the linguistic qualities of translation targets are different (a word/phrase label versus a short para-graph). Moreover, we have demonstrated that re-dundancy in the translated texts brought about by employing multiple translation engines definitely plays a role in improving performance, at least with an appropriate text-representation scheme and a proper text-similarity metric. 4.3 Extrapolated optimal performances
We believe that would be able to achieve better results if we could alter the blending ratio each time we processed a query concept, rather than the current uniform one. To partly examine this idea, we calculated the somewhat extrapolated re-sults shown in Table 5, in which the best figures already shown in the previous tables are replicated in parentheses. The extrapolated figures were cal-culated a posteriori with the assumption that we could choose from { X:textual combination C with  X  = 0 . 6 , Y:textual combination A with  X  = 0 . 2 ,
Z:textual combination A with  X  = 0 . 0 } , the best setting on a query-by-query basis.
 Table 5: Extrapolated optimum performance figures. olated results assert that room for improvement exists. To further break down these results, we investigated the distribution of parameter settings adopted a posteriori: for 126 out of 196 queries, setting X was adopted, setting Y for 42 queries, and setting Z for 28. These figures again suggest that a query-dependent gloss expansion method would be promising, and worth being examined.
Although the present work demonstrates the potential usefulness of extended textual glosses (Banerjee and Pedersen, 2003), the range of contextual neighbors for the extension and the proper weighting of the textual similarities obtained from various combinations remain unidentified. The key to solving this issue is using a machine learning (ML) approach that can capture hidden properties in the lexical resources.
The use of machine translation and its impact in terms of performance have been explored by the research community looking at ontology match-ing . Their primary focus, however, has been on the translation of conceptual/ontological labels, as typically discussed in (Fu et al., 2009). More re-cently, McCrae et al. (2011) made a similar ar-gument and presented a number of strategies that they assert could improve the performance of sta-tistical machine translation of conceptual labels.
In contrast to these research attempts, the present method applies machine translation to linguisti-cally richer textual glosses. However, as the lin-guistic qualities of textual glosses may vary, at least from resource to resource, we may need to devise a method for properly assessing and adjust-ing to them.

From the perspective of multilingual terminol-ogy extraction and alignment, the presented work may not share research interests with terminolo-gists, because our work targets dynamic pairwise alignment of existing lexical concepts. Neverthe-less, we envision that the proposed method may be applicable to cross-lingual terminology align-ment if domain-specific terms are extracted from a corpus with linguistic context, including their tex-tual definitions. That is, we could robustly mea-sure the cross-lingual similarity between linguistic contexts in different languages by applying multi-ple machine translation engines.
This paper demonstrated that extended textual glosses residing in lexical-semantic resources can be effectively employed even in the context of a cross-lingual task, provided multiple machine translation engines can be utilized and the resul-tant translation redundancy yields benefits. The applicability of the method could thereby increase, as the underlying resources (sense-tagged corpora for the synonym-based relatedness and translation engines for the gloss-based relatedness) would be more readily available and sophisticated.
 to be done in some areas of this work: the per-formance could be improved by considering the  X  X ensitivity X  (Kwong, 2012) of a lexical concept or a lexical concept type; and the multilingual-ity can be greatly enhanced by exploiting Web-based multilingual resources, as demonstrated by (Navigli and Ponzetto, 2010). Another direction that can be explored is the type discrimination of lexical-semantic correspondences. A promising solution would be an ML-based approach (e.g., (de Melo and Weikum, 2012)) that takes into ac-count more features acquirable from the lexical re-sources at hand.
 This work was supported by JSPS KAKENHI Grant Number 258201170.

Banerjee, S. and T. Pedersen (2003). Extended
Calzolari, N. (2008). Approaches towards a  X  X ex-de Melo, G. and G. Weikum (2012). Con-Fu, B., R. Brennnan, and D. O X  X ullivan (2009).
Hayashi, Y. (2011). A representation framework correspondences. In Proc. of IWCS 2011 , pp. 155 X 164.

Hayashi, Y. (2012). Computing cross-lingual syn-onym set similarity by using princeton anno-tated gloss corpus. In Proc. of GWC 2012 , pp. 131 X 141.

Hayashi, Y. and T. Ishida (2006). A dictionary model for unifying machine readable dictionar-ies and computational concept lexicons. In Proc. of LREC 2006 , pp. 1 X 6.

Kwong, O. Y. (2012). New Perspectives on Com-putational and Cognitive Strategies for Word Sense Disambiguation . Springer.

Manning, C. D., P. Raghavan, and H. Sch  X  utze (2008). Introdocution to Information Retrieval . Cambiridge University Press.
 McCrae, J., M. Espinoza, E. Montiel-Ponsoda,
G. A. de Cea, and P. Cimiano (2011). Combin-ing statistical and semantic approaches to the translation of ontologies and taxonomies. In Proc. of SSST-5 .

Miller, G. A. and C. Fellbaum (2007). Wordnet. then and now. Language Resources and Evalu-ation 41 , 209 X 214.
 Navigli, R. and S. P. Ponzetto (2010). Babelnet:
Building a very large multilingual semantic net-work. In Proc. of ACL 2010 , pp. 216 X 225.
 Yokoi, T. (1995). The edr electronic dictionary. Communications of the ACM 38 (11), 42 X 44.
 The EDR electronic dictionary, unlike Princeton
WordNet, is not a lexical database based on rela-tional lexical semantics. Instead, it can be seen as a knowledge base that is enriched with linguistic information given in both Japanese and English.
Another big difference lies in the consideration of parts of speech in the conceptual organization: as is well known, PWN maintains POS-dependent lexical semantic networks, while EDR bears only a POS-independent network.

Despite these differences, the information struc-ture of EDR can be modeled in the same way as that of PWN (Hayashi and Ishida, 2006): the set of words associated with a common concept iden-tifier in one or more of the sub-dictionaries can be modeled as a kind of synset.
Automatic Multi-Word Term extraction is an important task in many Natural Language Pro-cessing (NLP) applications (Boulaknadel et al., 2008b; Wen et al., 2007). The aim of the MWT acquisition process is to extract specific domain terms from special language corpora (Korkontze-los et al., 2008). The extraction of MWTs is cru-cial for terminology acquisition, since they are less ambiguous and less polysemous than single word terms, and since their internal structure encodes useful semantic relations (Wen et al., 2008).
There are three main approaches to MWT ex-traction. The first one makes use of linguistic fil-ters. The second one relies on statistical measures based on termhood and/or unithood. Termhood denotes  X  X he degree to which a linguistic unit is re-lated to a specific domain concept X  , and unithood denotes  X  X he degree of strength or stability of syn-tagmatic combinations or collocations X  (Kageura et al., 1996). Lastly, the third approach is hybrid and combines the linguistic and the statistical ap-proaches. Hybrid methods extract MWTs using linguistic filters and then rank the list of candidate MWTs according to statistical measures.

In this paper, we propose a novel, hybrid me-thod for Arabic MWT extraction. Like other hy-brid methods, it includes two main filters. In the first one, we use a part-of-speech (POS) tagger to extract candidate MWTs based on syntactic pat-terns. In the second one, we propose a novel sta-tistical measure, the NLC-value, that unifies the contextual information and both termhood and unithood measures. We compare this measure to alternative ones in the task of MWT extraction : NTC-value (Vu et al., 2008), LLR+C-value (Al Khatib et al., 2010), C/NC-value and LLR.

The remainder of this paper is organized as fol-lows. In the next section, Section 2, we present the related work. Section 3 describes the proposed method to extract MWTs. In Section 4, we present how MWT variation is handled in the proposed method. Section 5 describes the experimental vali-dation and Section 6 concludes this work and pre-sents some perspectives.

Several studies have been conducted on MWT extraction for many languages. These studies have either used a linguistic approach, a statistical ap-proach, or a combination of them (hybrid ap-proach). Most recent MWT extraction methods rely on a hybrid approach to efficiently extract MWTs, due to its higher accuracy compared to the two other approaches (Tadic et al., 2003). The linguistic approach uses technical analysis on the current knowledge of the language and its struc-ture. There are two subcategories : approaches ba-sed on morpho-syntactic patterns (Daille, 1994) and those based on MWT boundary detection (Bourigault, 1994).

The main purpose of applying statistical me-thods for MWT extraction is to rank candidate terms based on a particular measure that gives higher scores to  X  X ood X  candidate terms. Candi-date terms above a particular threshold are selec-ted for further processing. The reliance on fre-quency is based on the simple assumption that a frequent expression indicates an important repre-sentation of the domain in question. Therefore, frequent expressions are assumed to represent im-portant concepts. Given a candidate multi-word term, frequency only counts how often the candi-date occurs in the text, but doesn X  X  give any infor-mation on the strength of the relationship between words composing the candidate multi-word term. Statistical approaches aim at extracting candidate terms from text corpora by means of association measures (Church et al., 1991) that concentrate on termhood and/or unithood to assign a score to candidate MWTs. These measures are based on frequency and co-occurrence information such as the T-score (Church et al., 1991), the loglike-lihood ratio (LLR) (Dunning, 1994), the C/NC-Value (Frantzi et al., 1998), etc.

While linguistic approaches focus on syntac-tic structures, statistical methods focus on the recurrent characteristics of MWTs. Both have their advantages and limitations. As mentioned by Boulaknadel et al. (2008), statistical approaches  X  X re unable to deal with low-frequency MWTs X  while pure linguistic approaches are  X  X anguage dependent and not flexible enough to cope with complex structures of MWTs X  . Hybrid methods try to combine linguistic and statistical techniques to extract MWTs in order to avoid the weaknesses of the two approaches.

Boulaknadel et al. (2008) have relied on a hy-brid method to extract Arabic MWTs. As a first step, candidate terms that fit syntactic patterns are extracted from the output of the part-of-speech (POS) tagging tool proposed by Diab (2004). In the second step, the list of candidate terms is ran-ked according to one of the following association measures : log-likelihood ratio (LLR), Mutual In-formation (MI), FLR, and T-score. These mea-sures have been evaluated on an Arabic corpus and he results obtained show that LLR outperforms the other association measures.

Bounhass et al.(2009) have followed the same approach (using again Diab X  X  (2004) POS tagger and LLR) while focusing on compound nouns and thus using a more restricted set of syntactic pat-terns. For the bigrams, the obtained results outper-form those obtained by Boulaknadel et al. (2008).
A similar study has been conducted by Al Kha-tib et al. (2010), based on the POS tagger propo-sed by (Al-Taani et al. , 2009) and an association measure that combines both termhood and uni-thood through a combination of the C-value and the LLR. Experimental results show promising re-sults for the combined measure.

Most hybrid methods presented previously have been evaluated on 100 (best) candidate MWTs and deal with bi-grams (i.e. candidate MWTs of length 2). Moreover, they rely on LRR or a combination of LRR and C-value (Al Khatib et al., 2010) and ignore contextual information in the ranking step.
To overcome this limitation, we introduce a new association measure that integrates contextual in-formation and both termhood and unithood. Our overall approach is also hybrid and relies on the same linguistic filters as the ones used in the pre-vious studies, based on syntactic patterns applied on the output of the POS tagger developed by (Diab, 2009).
Our method for extracting MWT candidates comprises two major steps : the linguistic and the statistical filters. 46 3.1 Linguistic Filter The proposed linguistic filter extracts candidate MWTs based on two core components ; the POS tagger and the sequence identifier. In the literature, several methods for Arabic POS tagging systems have ben developed. We have used the one pro-posed by (Diab, 2009) as it performs at over 96% accuracy and allows a number of variable user set-tings. The underlying system uses Support Vector Machine (SVM). Figure (1) illustrates the global schema of our linguistic filter. As a first step, our method tags the corpus using the AMIRA toolkit (Diab POS Tagger) which is trained from the Penn Arabic TreeBank (PATB) to assign tags for each word in the corpus. Then, the sequence identifier tokenizes tagged files of the corpus and uses syn-tactic patterns in order to identify candidate terms that fit the rules of the grammar. We have extended the list of syntactic patterns used by Boulaknadel et al. (2008) as follows :  X  ( N oun +( N oun | ADJ )+ | ( N oun | ADJ )+  X  N oun P rep N oun The second major step of the linguistic filter is handling the problem of MWTs variation to im-prove the effectiveness of extracted MWT candi-dates. Several categories of term variation are ta-ken into account by this filter : graphical, inflectio-nal, morpho-syntactic and syntactic variants, and are discussed in Section 4. 3.2 Statistical Filter
In this step, we apply a number of statistical measures to rank the list of candidate MWTs ex-tracted by the linguistic filter. The main objective of our statistical filter is to consider both termhood and unithood measures. 3.2.1 The C -value
The C -value measures the termhood of a can-didate string on the basis of several characteris-tics : number of occurrences, term nesting, and term length. It is defined as : C -Value ( a ) = where | a | denotes the length in words of candidate term a , f ( a ) is the number of occurrences of a and : where T ( a ) denotes the set of longer candidate terms into which a appears ( | T ( a ) | is the cardi-nality of this set).

As one can note, if the candidate term is not nes-ted, its score is solely based on its number of oc-currences and length. If it is nested, then its num-ber of occurrences is corrected by the number of occurrences of the terms into which it appears. 3.2.2 The N C -value
The N C -value combines the contextual infor-mation of a term together with the C -Value. The contextual information is calculated based on the
N value which provides a measure of the termino-logical status of the context of a given candidate term. It is defined as : where C a denotes the set of distinct context words of a , f a ( b ) corresponds to the number of times b occurs in the context of a and n is the total number of terms considered. This measure is then simply combined with the C -value to provide the overall
N C -value measure :
N C  X  value(a) = 0 . 8  X  C  X  value(a) +0 . 2  X  N value(a) 3.2.3 The N T C -value
The aim of the N T C -value (Vu et al., 2008) is to incorporate a unithood feature, through the T-score, to the C/N C -value to improve its per-formance. The T-score measures the adhesion or differences between two words in a corpus of N words as follows :
T s ( w i , w j ) = where p ( w i , w j ) corresponds to the probability of observing the bi-gram w i , w j in the corpus ; p ( w i ) is the probability of word w i in the cor-pus and corresponds to the marginal probability of p ( w i , w ) . The T-score is integrated in the C/N C measures through a re-weighting of the number of occurrences that privileges terms with a positive T-score :
F ( a ) = where min( T s ( a )) corresponds to the minimum T-score obtained from all the word pairs in a . Substituting F ( a ) to f ( a ) in Equation 1 yields the T C -value, which is then combined with the N value as before, leading to the N T C -value : N T C -value(a) = 0 . 8  X  T C value(a) +0 . 2  X  N value(a) The resulting metric (6) thus takes into account both contextual information and termhood and unithood measures. 3.2.4 The N LC -value
We follow here the same development as before but rely this time on the more accurate unithood feature LLR (Dunning, 1994), instead of the T-score, for the combination with the C/NC-value (Frantzi et al., 1998). LLR is a  X  X oodness of fit X  statistics that determines if the words in an ob-served n -gram come from a sample that is inde-pendently distributed (meaning they co-occur by chance) or not. The underlying measure is calcu-lated for bi-grams by the following formula :
LLR ( w j , w j ) = a log( a ) + b log( b ) + c log( c ) + d log( d )  X  ( a + b ) log( a + b )  X  ( a + c ) log( a + c )  X  ( b + d ) log( b + d )  X  ( c + d ) log( c + d ) + N log( N ) with : a : number of terms in which w i and w j co-occur ; b : number of terms in which only w i occurs ; c : number of terms in which only w j occurs ; d : number of terms in which neither w i nor w j appear ;
N : total number of extracted terms.

For terms that consist of more than two terms, we calculate the LLR for each big-ram and then consider the minimum value obtained. The num-ber of occurrences of a term is now re-weighted by this minimum value : F L ( a ) = f ( a )  X  ln(2 + min( LLR ( a ))) which is used instead of f ( a ) in the C -value, leading to the LC -value : L
C-value(a) = with GL ( a ) = 1
This measure is then combined with the N value as before, leading to the N LC -value that inte-grates contextual information and both termhood and unithood :
N LC -value(a) = 0 . 8  X  LC -value(a) +0 . 2  X  N value(a)
As mentioned in the previous section, we have handled the problem of term variation at the lin-guistic step. Our method takes into account four types of variations : graphical variants, inflectional variants, morpho-syntactic variants and syntactic variants. Graphical variants concern orthographic errors occurred in writing a particular letters ( X   X ,  X   X  and  X   X   X ) which are very common in Ara-bic. Furthermore, some letters go through a slight modification in writing, that doesn X  X  necessarily change the meaning of the word. For example, the letter  X   X   X  is replaced by another letter  X   X   X  at the leads to  X  Y X Aymyk X  wlt X   X  meaning  X  X hemi-cal pollution X . Inflectional variants are due to the use of different forms for the words constituting a
MWT ; these different forms are related to gender and number of adjectives, as in  X  Xym X  wl  X  (ocean pollution) and  X  AWym X  wl  X  (pollu-tion of the oceans) and to the presence/absence 48 of a definite article, as in  X   X Ay X  wl  X  (wa-ter pollution) and  X   X Aym X  wl  X  (the water pol-lution). Morpho-syntactic variants affect the in-ternal structure of term as the words it contains are related through derivational morphology. Two patterns control this type of variation in Arabic MWTs :  X  N oun 1 N oun 2  X  N oun 1 Adj : wl  X   X  N oun 1 Adj  X  N oun 1 P rep N oun : We treat these three types of variations by using normalization method and the light stemming al-gorithm described in (Larkey et al., 2007) on each word of each MWT candidate.

Syntactic variants modify the internal structure of the MWT candidate by adding one or more words (as adjectives) but do not affect the gram-matical categories of the content words of the ori-ginal MWT candidate. Such variants can be iden-tified, for a given MWT candidate, by searching for all the stemmed MWT candidates that contain it. All the elements that constitute an addition to the original MWT candidate are then considered as context terms. 5.1 The Corpus
Since there is no standard domain-specific Ara-bic corpus, we have built, in order to evaluate our approach, a new corpus specialized on the envi-ronmental domain with similar properties as the ones described (Boulaknadel et al. , 2008; Boun-has et al., 2009; Al Khatib et al., 2010).

The corpus built contain 1666 files comprising 53569 different tokens (without stop words) ex-tracted from the Web site  X  X l-Khat Alakhdar X  1 . It covers various environmental topics such as pol-lution, noise effects, water purification, soil degra-dation, forest preservation, climate change and na-tural disasters. 5.2 Evaluation and Results
The evaluation of automatic MWTs extraction is a complex process and is usually performed by comparing each MWT candidate extracted to a domain-specific reference list. When there is no reference list available in the language retai-ned, one can first translate the MWT candidates (using a machine translation system or a bilingual dictionary) and use a reference list available in another language. For the evaluation purpose, we have constituted automatically a reference list of all Arabic MWTs available in the latest version of
AGROVOC 2 thesaurus and then use the stemming algorithm to remove prefixes and suffixes for each
MWT in the reference list and the extracted MWT list. The next step consists of using an algorithm that considers a MWT candidate as correct if it is included in this list, noting that the MWT candi-date and the term in the reference list should have the same number of stemmed words. Otherwise, we translate it and consider it as relevant whether its translation is contained in the European termi-nological database IATE 3 . Finally, the precision is calculated using the number of attested MWTs and the number of considred terms.
 We computed the association scores (LLR, C-value, NC-value, NTC-value, LLR+C-value,
NLC-value) for the MWT candidates and retain from each produced ranking for each statistical measure the k -best candidates, with k ranging from 100  X  300 at intervals of 100 . The expe-rimental results illustrated in table 1 show that our method (NLC-value) outperforms the previous methods in term of the quality of the extracted MWTs.
 Stat. measures 100 200 300 LLR 75,0% 70,5% 64 , 3% C-value 71,0% 69,0% 67 , 3% NC-value 74,0% 70,0% 68 , 3% NTC-value 80,0% 71,5% 69 , 7% LLR+C-value 73,0% 72,0% 68 , 3% NLC-Value 82,0% 75,5% 73 , 0%
Furthermore, the combination of the context in-formation and the C -value improves the perfor-mance of the process of MWT extraction because the
N C -value outperforms the C -value for each considered MWT list. The unithood feature LLR outperforms the C/N C -value as expected from previous studies. Figure 2 illustrates the precision obtained for the C/N C -value and the LLR.

The integration of contextual information and the T-score unithood measure to the C -value improves the performance of MWT acquisition, since the N T C -value has better precision than the C/N C -value, as illustrated in Figure 3.

Lastly, the combination of termhood and uni-thood measures ( N T C -value, LLR + C -value, N LC -value) is essential for MWT extraction, since all the measures based on this combination perform better than measures using only term-hood or unithood ( C -value, N C -value, LLR). We note that the statistical measure we have propose, N LC -value, outperforms all other measures. This measure is based on the accurate unithood fea-ture LLR, combined with the N C -value. The
N LC -value method takes advantages from pre-vious works proposed in (Vu et al., 2008) and (Al
Khatib et al., 2010) taken into account contextual information and both termhood and unithood as-sociation measures. Figure 4 presents a comparai-son of the precision for different statistical mea-sures that combine termhood and unithood.

The number of different terms evaluated are 1095 amongst other 1800 terms, moreover the sta-tical measures share 141 terms. The tables 2 and 3 represent the number of terms found in agrovoc and IATE respectively.
 Stat. measures 100 200 300 LLR 35 60 80 C-value 27 59 82 NC-value 32 62 82 NTC-value 35 60 83 LLR+C-value 34 60 84 NLC-Value 41 65 86
In this work, we have presented a hybrid me-thod for Arabic MWT acquisition ; this method takes advantage of existing linguistic and statisti-cal approaches. As a first step, we apply linguistic filters to extract MWT candidates based on syntac-tic patterns using a sequence identifier component.
Then, MWT variants are identified through a mor-phological analysis of he extracted MWTs ba-sed on light stemming. In the statistical step, we 50 have proposed a novel statistical measure, N LC -value, that consists of ranking MWT candidates by considering contextual information and both term-hood and unithood statistical measures.

Experiments are performed for bi-grams and tri-grams on an environment Arabic corpus. The experimental results show that our method out-performs the previous ones in term of quality of the extracted MWTs. In conclusion, the combi-nation of the best association measures that inte-grate contextual information and both termhood and unithood statistical measures improves the performance of the MWT acquisition process.

In a near future, we plan on using the extrac-ted MWTs in an information retrieval system as complex terms often constitute a better representa-tion of the content of a document than single word terms.
 53. 51 52
Although perspectives and terminology used may differ, the importance of terminological r e-lation s in terminology research and management is appreciated by many scholars. Attention has focused at various points on the classification and description of relations that are relevant for terminology work , on methods for extracting these from texts, and on the relevance of and ap-proaches to integrating this information into te r-minology resources . Among the first proposals for relation -enriched terminology resources was 
Meyer et al. X  X  (1992) terminological knowledge base (TKB), a terminology resource that d e-scribes not only a range of concepts but also a variety of relationships that hold between them. even (semi -)automatically from texts (cf.
L X  X omme and Marshman 2006) in the form of knowledge -rich contexts (KRCs ) (Meyer 2001). 
These excerpts of texts often contain knowledge patterns  X  X .e., combinations of terms or other linguistic units that express concepts, linked by lexical markers of the relations between them  X  and can both provide information to assist in un-derstanding the terms and concepts and illustrate the linguistic items in use . As excerpts of  X  X uthentic X  texts, KRCs can also illustrate vari a-tion in concepts  X  expression and the lexical mark ers used in various communicative situ a-tions (e.g. Condamines 2002, 2008; Marshman and L X  X omme 2008; Marshman et al. 2009).
 ers who have investigated various strategies for developing and populating TKBs (e.g. 
Condamines and Rebeyrolle 2000, 2001; Faber et al. 2011; Faber and San Mart X n 2011; Le X n et al. 2011, 2013) in a selection of domains. Some projects have addressed the use of terminological relationships in the form of ontologies (e.g. Cabr X  et al. 2004; Gillam et al. 2005; Maroto and 
Alcina 2009), as part of an incr easing movement towards the integration of terminology and on-tology (e.g. Temmerman and Kerremans 2003; cf. also Roche et al. 2011). Still others (e.g. 
L X  X omme 2013, 2013a) have described lexical relationships between terms. Most of these r e-sources have be en in electronic form, although some specialized print dictionaries (e.g. Dancette and R X thor X  2000) have include d such info r-mation. tized in resources, there is still no standard model for relation choice and representation . This may be true in part because the wide v ariety of users of terminology resources and purposes for their use (e.g. Sager 1990) entails diverse needs in this area. Faber and San Mart X n ( 2011: 48 ) express the need for  X  X ustomized X  design of terminology resources: [I]n order for any know ledge resource to aspire to psychological and explanatory adequacy, its underl y-ing conceptualization and design must be in cons o-nance with the needs and expectations of a specific user group, whose main objective is generally to a c-quire knowledge about the specialized area.
 observation can be questioned : translators (who are seen as the primary users of terminology da-tabases in contexts such as Canada  X  X ) may not be as interested in domain knowledge per se as in terms, equivalents, synonyms and their use (in-cluding the contexts in which they occur) . These and similar observations have led s ome to con-clude that conventional terminology resources such as the large t erm banks, including terminologique (GDT) 2 , are not adequate for translators X  needs. The same can be said for other resources: ontology-based resources may also not be easy to understand and use for non -subject -field specialists such as translators and terminologists (Cabr  X  et al. 2004: 87). some of the resources that are available to speci f-ic groups and how (and how w ell) they meet the needs of these group s. In this paper, we will f o-cus on the needs of trainee translators: individu-als who are likely in need of both subject -field and linguistic knowledge to carry out a transl a-tion task, but may attribute different levels of importance to each kind of knowledge, and may evaluate the resources that supply this knowledge different ly from other groups and from one another . We will gather information about trainee translators X  reactions to resources from a questionnaire completed by users of three terminology resources, and try to extrapolate some guidelines for the creation of effective re-sources based on this feedback.
 of the currently available terminology resources (section 2). We will outline the methodology used to gather information for this pilot study (section 3), and then will present and discuss some findings (section 4), before wrapping up with some brief remarks, suggested guidelines derived from the observations, and ideas for f u-ture work (section 5).
In this section, we will provide a brief overview of the conventional term banks used in the pr o-ject (2.1), as well as a few examples of relation -enriched resources and how they have compl e-mented this basic model with terminol ogical r e-lations (2.2), and then describe the 
CREATerminal prototype used in this study (2.3).
The largest and most widely used term banks today are mainly constructed on traditional mo d-els such as those described by Pavel and Nolet (2001) and Dubuc (2002) , and provide a range of information to translators, students, writers and other users.

Plus term bank (Government of Canada 2013; see also Pavel and Nolet 2001) has very broad cove rage, including over four million terms (most in English and French, but with a growing component of Spanish and Portuguese) from a wide range of domains . In addition to admini s-trative information including dates of modific a-tion and record authors, i ts ter m records contain largely standard term record fields of domain and sub-domain, terms, equivalents, sources, part -of-speech labels, usage labels, definitions, contexts, observations and in some cases phraseologisms (although not all of these fields may appear on each record) . years, the GDT now presents terms (mainly 
French and English, with a small complement of other languages) from a wide range of domains in a term record format that calls particular atten-tion to French terms and to the associated usage information (particularly appropriateness for use in Quebec ). In addition to (mostly French) def i-nitions, some records include illustrations and notes to clarify meaning (including distinctions between relate d terms and concepts) and usage, as well as administrative fields . resources is uneven, with any such information generally found in definitions , contexts or obse r-vations/ notes. 2.2 Enriching term inology resources with 
In filling the gaps in this traditional term record model and developing the idea of TKBs or on-tologies, a number of projects have addressed the needs of users for additional relation info r-mation. Meyer et al. X  X  (1992) COGNITERM project was followed by other projects including 
GenomaKB 3 (Cabr  X  et al. 2004; Feliu et al. 2004) that integrated corpora and bibliographical i n-formation with a terminological database and an ontology to provide an i ntegrated , multilingual resource that would meet the needs of non -sub ject -field specialists in the field of the g e-nome. This type of integration reflects some of the observations of Bowker (2011), which hig h-lighted the usefulness of access to corpus data for translators researching terms. portance of context of use and its potential for disambiguati on in the description of terms and project ( Faber et al. 2011; Le X n et al. 2011, 2013) . This multilingual resource in the field of environmental science provides access not only to definitions of concepts , but also to visual i n-formation in the form of both illustrations and dynamic relation maps that illustrate connections bet ween terms and other elements (including generic-specific, part -whole and various non-hierarchical relations) based on an approach in-spired by Fillmore X  X  Frame Semantics (Faber et al. 2011; Faber and San Mart X n 2011) . The d y-namic visualization options allow the user to view a wide range of connections and to navigate by follow ing links between concepts, in order to better understand the ir complex interconnections. 
CoInfo 5 and the DiC oEnviro 6 , has been created by a team headed by Marie -Claude L X  X omme at the Universit X  de Montr X al X  X  Observatoire de linguistique Sens -Texte. Developed based on corpus data from the perspective of lexico-semantic terminology , and calling upon princ i-ples of Explanatory and Combina torial Lexico g-raphy (Mel X uk et al. 1995; L X  X omme 2012) and later on Frame Semantics, these resources pr o-vide extensive descriptions of links between terms (including nouns, verbs, adjectives and phrases) in the fields of computing and the Inte r-net and of the environment, respectively. In addi-tion to part -of-speech labels, equivalents, contexts and definitions, term s are accompanied by an analysis of their actantial structures and typical actants, as well as a list of terminological relationships that may include synonyms, ant o-nyms, hyponyms, hypernyms, meronyms, and holonyms, as well as a number of  X  custom  X  rel a-tions observed in the corpora. A visual interface , the DiCoInfo v isuel (Robichaud 2012) allows users to view connections between the terms d e-scribed in the DiCoInfo . potential for explicitly describing a wide variety of relationships relevant in terminology, as well as a range of o ptions for making this information easily accessible to users, including increased access to a variety of contexts and options for various approaches to navigation within the r e-source, including a visual interface.
Another in the list of relation -rich resources, but far less developed than those described above, is the CREATerminal prototype. In development since 2007, it aims to provide a useful resource for translators , built based on the content of pop-ularized, bilingual (Engli sh-French) documents in the field of breast cancer (e.g. Marshman and Van Bolderen 2009; Marshman, Gari  X py and Harms 2012) . T he information contained in the 
CREATerminal prototype was extracted from bilingual Canadian web sites ( e.g. Health Can a-da, the Canadian Cancer Society, and the Cana-dian Breast Cancer Foundation) . da tabase with three main tables: one has an entry for each of the approximately 85 concepts cov-ered in the resource, and links the terms ident i-fied fo r each concept with their equivalents in the other language; one includes approximately 250 bilingual contexts showing the terms and their equivalents in use, and the third presents a total of approximately 800 bilingual KRCs that illustrate terminological relations (generic-specific, part -whole, cause -effect and entity -function) that involve the concepts and include lexical relation markers. These KRCs are ann o-tated to identify the relationship present, the rel a-tion marker, the related items , and their sources. record form  X  X hich shows terms and equiv a-lents, and offers buttons to display exam ples and 
KRCs illustrating different relations  X  X r view complete list s of KRCs for each relation type or lexical re lation markers for the relations . neric queries that allow the user to search for specific character strings in term records, exa m-ples and KRCs . 
This pilot project focuses on the comparison of the CREATerminal, TERMIUM  X  Plus and the 
GDT by a sample of students in translation pr o-grams (B.A. and graduate programs) at the Un i-versity of Ottawa. These students were predominantly Anglophone and registered in courses that included a component of terminol o-gy and/or terminography. The students were first introduced to the concept and relevance of rel a-tions in the field of terminology in their courses and with an introductory in -class exercise, and to the CREATerminal model and how to consult and search it. (A ll had previously used both 
TERMIUM  X  Plus and the GDT i n their cours e-work and were assumed to be comfortable with their use.) They were then asked to carry out a translation task and invited to complete a n op-tional, anonymous online questionnaire summ a-rizing their experiences after class.
 short (1-3 sentence) excerpts of popularized texts on breast cancer. A mix of English to French and 
French to English translation was offered, and students were asked to try both (so that they would be translating both into and out of their 
L2 ). Students were asked to pay particular atten-tion to highlighted terms in the excerpts and to look them up in the three terminological r e-sources. All concepts corresponding to the hig h-lighted terms were described in at least two of the terminology resources used in the compar i-son, although occasionally term forms or terms themselves varied.
 cerpts as possible in a thirty -minute period. They then were invited to complete the questionnaire, delivered via the Survey Monkey interface. The first section of the questionnaire gathered general information on the respondents X  perceptions of several subject s: the resource s X  usefulness for understanding concepts in the excerpts and for writ ing about them; what the respondents found most and least useful about each resource; and which resources they would use again for a sim i-lar task. The second section (on a new page) ad-dressed terminological relations speci fically , and asked about students X  perceptions of how well terminological relations were described in each resource, as well as how useful the information about terminological relations in general was for understanding concepts and for writing about them . Respondents were also asked to evaluate the usefulness of individual record fields con-tai ning this relation -related information . Finally, the third section asked students to identify which fields they would consider useful in their own translation -oriented term records (i.e. whether they currently included them, planned to include them, would consider including them, or did not and would not include them). rating scale from 1 to 4, with 1 representing a negative evaluation (e.g.  X  X ot at all useful X  for questions about usefulness, and  X  X o not and will not include X  for questions about term record field s) and 4 representing a positive evaluation (e.g.  X  X ery useful X ,  X  X urrently include X ). Average scores were computed automatically by Survey Monkey based on these scales.
 consult /use  X  option was provided. Participants were also offered the option to list and evaluate additional resources they consulted.
 pate in the survey. A very high dropout rate of almost 50% after the first question suggests that many may have first accessed the questionnaire to familiarize themselves with its contents (as t he main questions could only be accessed after co n-senting to participate) , and either return ed later to com plete it or were dissuaded by the nature or length of the questionnaire. Of the 13 respond-ents who continued to the second question, 7 continued to t he final question. 3.1 Some l imitations of the methodology
An important limitation of this study is the small sample size and the high dropout rate. Important ethical considerations involved in the collection of data from students required great care to avoid coercion and ensure anonymity, which unfort u-nately limited opportunities to encourage partic i-pation and follow up with potential respondents (in addition to imposing significant restrictions on the general methodology) . Moreover, the na-ture of the sample itself should be taken into a c-count, as it consists of students from a single academic setting, and those most likely to partic-ipate were doubtless those who had a particular interest in terminology in general and termin o-logical relations in particular. necessarily restricted by time limitations and coverage limitations for the three banks, and the approach used to introduce variety by giving a choice of excerpts to translate (coupled with the survey -based metho dology) also made it impo s-sible to verify exactly which term records in each resource were consulted by each individual . survey -based methodology for data collection are also significant in themselves. We accessed only respondents X  perceptions of their experience, and thus were not able to objectively measure aspects of this experience, or to provide a fine -grained portrait of how the various resources were act u-ally used.
 en as purely indicative clues to help in identif y-ing key concerns in creating student -friendly, relation -rich terminology resources (and certai n-ly not as evaluations of the quality of any speci f-ic resource) . Given the limitations of the sample , no statistical evaluation of the data will be ca r-ried out beyond the comparison of average scores from multiple-choice questions and pe r-centages of respondents within the group.
In the first section of the questionnaire, respon d-ents were asked to evaluate and compare the use-fulness of the three resources for two main tasks: understanding concepts (i.e. decoding the source text) and writing about concepts (i.e. encoding the target text).
 ful ness of the three resources, the 13 respondents found all of the resources to be between  X  X airly useful X  and  X  X ery useful X  for understanding con-cepts: TERMIUM  X  Plus had the highest average score of 3.46 out of 4, followed by the GDT at 3.17 and finally the CREATerminal at 3.00. For writing about concepts, the scores showed a wi d-er rage and fell just slightly below  X  X airly useful X  into the range of  X  X omewhat useful X . In contrast to the previous ranking, the CREATerminal scored highest, with an average score of 3.36, score of 2.92 and finally the GDT at 2.73.
 for the two purposes most likely reflects the strengths of different types of data . Among the chief complaints were some gaps in information (e.g. of definitions, contexts and cooccurrents in with searching and display in all three resources (e.g. having to scroll down or through various 
Plus and the GDT, or having to work with one query at a time and to close tabs between searc h-es in the CREATerminal).
 each resource the coverage and variety of equiv alents included were valued. Among the stre ngths of TERMIUM  X  Plus , respondents cited broad cov erage of terms and concepts and incl u-sion of bilingual information X  X oth likely to a s-sist with understanding  X  X s well as ease of use and precise searching. The GDT X  X  strengths, as identified by the students, included the notes provided about usage, origin, etc. These might fulfill a decoding or an encoding function. Fina l-ly, the numerous, bilingual KRCs in the CR E-
ATerminal seemed most helpful for writing about concepts.
 the defining and the illustrating functions of te r-minology resources . This may represent an ex-ception to the general observation that translators tend to be most concerned with equivalents and less with definitions, perhaps because these are students working in a largely unfamiliar field  X  or because they were asked specifically abou t the understanding of concepts . questionnaire, 4 of the 7 respondents reported currently storing definitions on their term re c-ords , and 2 of the others reported planning to include them (an overall score of 3.43) . In co n-trast, none of the students reported currently stor-ing relation -related fields, although between 3 and 5 of the respondents (depending on the field) indicated that they would consider including them. The students seemed more likely to co n-sider including conventional term record fields (ranging from a score of 2.5 for phraseologisms to 3.29 for contexts and 3.43 for definitions ) than relation -related fields ( ranging from 1.5 for sources of terminological relations to 2.33 for a context illustrating the relationships).
 when asked about the usefulness of the different types of relations described in the CREATerm i-nal , the 9 respondents indicated that they were useful to varying degrees , with the highest ave r-age score (3.5 out of 4) for the generic -specific relation, followed by part -whole (3.2), entity -function (3.0) and finally cause -effect (2.8). 
When asked abou t specific elements of the ann o-tated KRCs that were helpful for understanding the concepts (excluding the terms themselves) , the highest -ranked fields were the example source ( with an average of 3.2) and the French example (3.17). T he other fields , except for the 
French relation marker (2.67) , scored 3.0, indi-cating that these elements were considered fairly useful. For writing about concepts, the English context explaining the relation was on average ranked most useful (3.75), followed by the Eng-lish lexic al relation marker (3.5) , the French co n-text (3.2) and the English related term/item (3.0) . 
All other field s scored below 3.0. The average score from 10 responses to the final question from the section indicated that the CREATerm i-nal provided the most useful information about terminological relations (with a score of 3.56 out of 4), followed by the GDT at 2.88 and finally TERMIUM  X  Plus at 2.56. observations above, in that information about terminological relations appears to be useful, but not very likely to be stored by students in their own records (perhaps because of the complexity and labour -intensiveness of the task ) and also unlikely to be thoroughly covered in conventio n-al termino logy resources . We thus see the need for  X  X hird -party X  terminology resources that do integrate this information to fill the gap for trai n-ee translators (and those with similar needs) . tions of users when asked whi ch resources they would use for a similar task again. Of the 12 r e-spondents to this question, 83% indicated that use the CREATerminal, and 50% would use the 
GDT. (It should nevertheless be noted that the respondents were mostly Anglophone and that the data suggest that they were paying particular attention to information for encoding in English, which is not the primary purpose of the GDT.)
This study has elicited some encouraging reac-tions from students, indicating that relation -enriched resources can meet some perceived needs in carrying out a translation task. From the literature and findings described above, we can observe a high priority accorded to equivalents (wh ich is not surpri sing) and to the understan d-ing of concepts (e.g. via definitions and KRCs) . 
This may well reflect the nature of the students X  experience, in their need to interpret concepts that are almost inevi tably unfamiliar (and cha l-lenging given the fact that the tra nslations were of excerpts and not whole texts, which would provide more information to help with interpret a-tion). There is also a positive evaluation of the usefulness of relation -related informati on X  particularly for writing  X  X s evidenced by the evaluation of the CREATerminal resource and the willingness to use it again.
 we can derive some preliminary, suggested guidelines for the creation of a relation -rich te r-minology resource that would meet the expect a-tions of the students:
Students X  reactions suggest that for this user group, the user -friendliness of resources is fu n-damental. Regardless of resources X  content (and coverage was highly valued by the respondents) , it seems that easy access to this information may be equally important . The inclusion of visual interfaces such as in the EcoLexicon and the D i-
CoInfo visu el  X  X articularly if these are smoothly integrated into an interface that also allows for easy consultation of textual material X  X re pro m-ising avenues for future development.
 students did find the relation information they consulted helpful, and seemed to be particularly drawn to it in the form of KRCs. This may be due to a focus on information that can be useful for writing about concepts as well as understan d-ing them . In any case, to satisfy the needs of this user group, it seems beneficial to include as wide a range of KRCs as possible (or practical) to take advantage of the dual function of these items (while nevertheless maintaining efficient integr a-tion and organization of the material to ensure easy navigation). Despite the time investment, advantages to including selected KRCs in a r e-source rather than offering (only) direct access to corpus data may include both speed and ease of access to information  X  X articularly for users who are new to the subject field in question and may need assistance for the first stages of r e-search  X  X s well as the ability to exploit the data they contain, e.g. for visual representation of re l-evant relationships . mation where available. A bilingual format is common to TERMIUM (which often includes definitions and contexts in both languages, usua l-ly from comparable resources) and the CR E-
ATerminal (which includes parallel bilingual contexts). As noted by Bowker (2011: 221 ), in spite of traditional terminology guidelines, tran s-lators increasingly tend to value translated sources (parallel corpora, translation memories ) and the rapidity and ease of use these info r-mation sources offer . Although the benefits of comparable corpora in terminology work are well established, it seems that the inclusion of complementary translated information can be an asse t in the eyes of the trainee translators. use of these resources in more detail and to better understand to what extent these preliminary guidelines are relevant , and why . It will be e s-sential to gather more da ta from a wider variety of users in order to identify more generalizable trends in requirements and preferences. A more in-depth study of the use of the resources by pa r-ticipants (e.g. using screen recording and inte r-views , or  X  X esources permitting  X  X sing ey e-tracking and keystroke logging tools to monitor users X  activity ) could allow us to obtain a more accurate and detailed picture of how students use such resources and their contents , and what fa c-tors they take into account in evaluations.
 sign, use and usefulness of student -friendly, rel a-tion -rich resources, we will be better able not only to produce richer and more useful tools but also to better train students to use and even cr e-ate them in the workplace . 
The author wishes to thank the organizations who granted permission to use their texts in the 
CREATerminal project, as well as the numerous research assistants at uOttawa who have contri b-uted to the project. Thanks are also extended to the Social Science and Humanities Research 
Council of Canada and the University of Ottawa and uOttawa Faculty of Arts for funding various stages of the project. 
Bowker, L. 2011.  X  X ff the record an d on the fly: E x-amining the impact of corpora on terminographic practice in the context of translation. X  In J. Mu n-day, K. Wallmach and A. Kruger, eds. Corpus -based Translation Studies: Research and Applic a-tions . 211 -236. Manchester: St. Jerome.

Cabr X , M.T. , C. Bach, R. Estop X , J. Feliu, G. Mar t X nez and J. Vivaldi. 2004.  X  X he GENOMA -KB project: towards the integration of concepts, terms, textual corpora and entities. X  In M.T. Lino, M.F . Xavier, 
F. Ferreira, R. Costa and R. Silva, eds. Procee d-ings of the Four th International Conference on 
Language Resources and Evaluation (LREC ) 2004 . 87-90. Lisbon, Portugal, 26-28 May 2004. https://www -new.comp.nus.edu.sg/~rpnlpir/ proceedings/lrec -2004/pdf/100.pdf. Consulted 4 July 2013.

Condamines, A. 2002.  X  X orpus analysis and conce p-tual relation patterns. X  Terminology 8(1): 141  X 162.
Condamines, A. 2008.  X  X aking genre into account when analysing conceptual relation patterns. X  Co r-pora 3(2): 115  X 140.

Condamines, A. and J. Rebeyrolle. 2000.  X  X onstruc-tion d'une base de connaissances terminologiques  X  partir de textes : exp X rimentation et d X finition d'une m X thode. X  In J. Charlet, M. Zacklad, G. Ka s-sel and D. Bourigault, eds. Ing X nierie des connai s-sances,  X volutions r X centes et nouveaux d X fis . 225 -241. Paris : Eyrolles. http://w3.erss.univ -tlse2.fr/textes/pagespersos/rebeyrol/Articles/conda mines_rebeyrolle_2000_b.pdf. Consulted 4 July 2013.

Condamines, A. and J. Rebeyrolle. 2001.  X  X earching for and identifying conceptual relationships via a corpus -based a pproach to a Terminological Knowledge Base (CTKB): Method and Results. X  
In Bourigault, D., M.C. L X  X omme and C. Jacque-min, eds. Recent Advances in Computational Te r-minology . 127-148. Amsterdam/Philadelphia : John Benjamins. 
Dancette, J. and C. R X thor X . 2000 . Dictionnaire An a-lytique de la Distribut ion . Montreal: Les Presses de l X  X niversit X  de Montr X al.

Dubuc, R. 2002. Manuel pratique de terminologie, 4 e  X dition . Brossard, Quebec: Linguatech  X diteur . 
Faber, P., P. Le X n Ara X z and A. Reimerink. 2011.  X  X nowledge representation in EcoLexicon. X  In N. Talav X n Zan X n, E. Mart X n Monje and F. Palaz X n Romero, eds. Technological Innovation in the Teaching and Processing of LSPs: Proceedings of TISLID X 10 . 367 -385. Madrid: UNED.

Faber, P. and A. San Mart X n. 2011.  X  X inking specia l-ized knowledge and general knowledge in EcoLe x-icon. X  In Roche, C. et al, ed s. Actes de la conf X rence Terminologie &amp; Ontologie : Th X ories et Applications (TOTh) 2011 . 47-61. Annency , 
France, 26 -27 May 2013. http://www.porphyre.org/toth/files/actes/TOTh -2011 -actes.pdf. Consulted 4 July 2013.
 Feliu, J., J.J. Giraldo, V. Vidal, J. Vivaldi and M.T. 
Gillam, L., M. Tariq and K. Ahmad. 2005.  X  X erm i-nology and the construction of ontology. X  Term i-nology 11(1): 55-81.
 Le X n Ara X z, P., A. Reimerink and P . Faber . 2011. 
Le X n Ara X z, P., A. Reimerink and A. Garc X a-Arag X n. L X  X omme, M.C. 2012.  X  Using Explanatory and 
Maroto, N. and A. Alcina. 2009.  X  X ormal description 
Marshman, E. and M. -C. L X  X omme. 2008.  X  X ortabil i-
Marshman, E. and P. Van Bolderen. 2009.  X  X owards 
Marshman, E., J.L. Gari X py and C. Harms. 2012. 
Marshman, E., M.-C. L X  X omme and V. Surtees. 2008.  X  X ortability of cause -effect relation markers across specialized domains and text genres: A comparative evaluation. X  Corpora 3(2): 141-172.
Mel X  X  X k, I., A. Clas and A. Polgu X re. 1995. Introduc-tion  X  la lexi cologie explicative et combinatoire . Louvain la Neuve: Duculot. 
Meyer, I. 2001.  X  X xtracting knowledge -rich contexts for terminography: A conceptual and methodologi-cal framework. X  In D. Bourigault, C. Jacquemin and M. -C. L X  X omme, eds. Recent Advances in 
Co mputational Terminology . 279-302. Amste r-dam/Philadelphia: John Benjamins.

Meyer, I., D. Skuce, L. Bowker and K. Eck. 1992.  X  X owards a new generation of terminological r e-sources: an experiment in building a terminological knowledge base. X  Proceedings of the 15th Interna-tional Conference on Computational Linguistics (COLING -92) . 956 -960. Nantes, France, 23 -28 August 1992. 
Pavel, S. and D. Nolet. 2001. Handbook of Terminol-ogy . Ottawa: Government of Canada, Translation 
Bureau. http://www.btb.gc.ca/publications/ documents/termino -eng.pdf. Consulted 5 July 2013.
Robichaud, B. 2012.  X  X ogic based methods for term i-nological assessment. X  In N. Calzolari, K. Choukri, 
T. Declerck, M. U X ur Do X an, B. Maegaard, J. Ma r-iani, J. Odijk and S. Piperidis, eds. Proceedings of the Eighth International Conference on Language 
Resources and Evaluation (LREC) 2012. 94 -98. I s-tanbul, Turkey, 21-27 May 2012. http://www.lrec -conf.org/proceedings/lrec2012/pdf /1096_Paper.pdf. Consulted 5 July 2013.
 Roche, C. et al., eds. 2011. Actes d e la conf X rence 
Terminologie &amp; Ontologie : Th X ories et Applic a-tions (TOTh) 2011 . Annency, France, 26 -27 May 2013. http://www.porphyre.org/toth/files/ actes/ TOTh -2011 -actes.pdf. Cons ulted 4 July 2013. Sager. J.C. 1990. A Practical Course in Terminology 
Processing . Amsterdam/Philadelphia: John Benj a-mins.

Temmerman, R. and K. Kerremans. 2003.  X  Termo n-tography: Ontology building and the sociocognitive approach to terminology description . X  In E. Haji X  o-v X , A. Kot X   X ovcov X  and J. M X rovsk X , eds. Proceed-ings of the XVII International Congress of 
Linguists . 9-16. Prague, Czech Republic, 24 -29 July 2003. http://www.starlab.vub.ac.be/research/projects/poir ot/Publications/temmerman_art_prague03.pdf . 
Consulted 4 July 2013.
Term extraction plays an important role in a wide range of applications including information retrieval (Yang et al., 2005), keyphrase extrac-tion (Lopez and Romary, 2010), information ex-traction (Yangarber et al., 2000), domain ontol-ogy construction (Kietz et al., 2000), text classi-fication (Basili et al., 2002), and knowledge min-ing (Mima et al., 2006). In many of these ap-plications the specificity level of a term is a rel-evant characteristic, but despite the large body of work in term extraction there are few methods that are able to identify general terms or intermediate level terms. Take for example the following struc-ture from the AGROVOC vocabulary 1 : resources  X  natural resources  X  mineral resources  X  lig-nite , where resources is an upper level term, natu-ral resources and mineral resources are intermedi-ate level terms, and lignite is a leaf. Intermediate level terms are specific to a domain but are broad enough to be usable for summarisation and clas-sification. Methods that make use of contrastive corpora to select domain specific terms favour the leaves of the hierarchy, and are less sensitive to generic terms that can be used in other domains.
Instead, we construct a domain model by iden-tifying upper level terms from a domain corpus.
This domain model is further used to measure the coherence of a candidate term within a domain.
The underlying assumption is that top level terms (e.g., resource ) can be used to extract intermedi-ate level terms, in our example natural resources and mineral resources . Our method for construct-ing a domain model is evaluated directly through an expert survey as well as indirectly based on its contribution to intermediate level term extraction.
While domain modelling is tested and exemplified with English, the ideas presented here are not lan-guage dependent and can be applied to other lan-guages, but this is outside the scope of this work.
We start by giving an overview of related work in term extraction in Section 1. Then, an approach to construct a domain model based on domain co-herence is proposed in Section 2, followed by a method to apply domain models for term extrac-tion. The experimental part of the paper starts with a direct evaluation of a domain model through a user survey (Section 3). A first set of experiments is carried in a standard setting for term evaluation, while the second set of experiments is application-driven, using corpora annotated for keyphrase ex-traction, information extraction, and information retrieval. We conclude this paper in Section 4, giv-ing a few directions for future work.
 Methods for term extraction that use corpus statis-tics alone are faced with the challenge of distin-guishing general language expressions (e.g., last week ) from terminological expressions. A solu-tion to this problem is to use contrastive corpora (Huizhong, 1986). Several contrastive measures are proposed including domain relevance (Park et al., 2002), domain consensus (Velardi et al., 2001), and word impurity (Liu et al., 2005). In this work we propose an approach to compute do-main specificity based on a domain model, that is less sensitive to leaf terms and is better suited for intermediate level terms.

The domain model proposed in this work is de-rived from the corpus itself, without the need for external corpora. An automatic method for iden-tifying the upper level terms of a domain has ap-plications beyond the task of term extraction. Al-though not named as such, upper level terms were previously used for text summarisation (Teufel and Moens, 2002). The authors manually identi-fied a set of 37 nouns including theory , method , prototype and algorithm , without considering a principled approach to extract them. The work presented here is similar to (Barri ` ere, 2007), but instead of re-ranking terms based on their similar-ity to each other we make use of domain model terms, reducing data sparsity issues.

In our experiments we employ two state of the art methods for term extraction, the NC-value ap-proach (Frantzi et al., 2000) and TermExtractor 2 (Velardi et al., 2001). The former is a hybrid method that ranks terms using only corpus statis-tics, while the latter exploits contrastive corpora. NC-value is based on raw frequency counts and considers nested multi-word terms by penalising frequency counts of shorter embedded terms. Ad-ditionally, it incorporates context information in a re-ranking step using top ranked terms. Con-text words (nouns, verbs and adjectives) are identi-fied based on their occurrence with top candidates. Our method is an extension of this approach that uses domain models instead of selecting context words based on frequency alone.

TermExtractor is a popular approach that com-bines different term extraction techniques includ-ing domain relevance, domain consensus and lex-ical cohesion. Domain Relevance ( DR ) compares the probability of a term t in a given domain D i with the maximum probability of the term in other domains used for contrast D j and is measured as:
Domain Consensus ( DC ) identifies terms that have an even probability distribution across the corpus that represents a domain of interest, and is estimated through entropy as follows: where d is a document in the domain D i . Fi-nally, the degree of cohesion among the words w j that compose the term t is computed through a measure called Lexical Cohesion ( LC ). Let | t | be the length of t in number of words, and f ( t,D i ) the frequency of t in the domain D i , then Lexical Cohesion is defined as:
The weight TE used for ranking terms by Ter-mExtractor is a linear combination of the three methods described above: TE ( t,D i ) =  X   X  DR +  X   X  DC +  X   X  LC (4)
While general terms typically have a high do-main consensus, the domain relevance measure boosts narrow terms that have limited usage out-side of the domain. For example the term system is not identified as relevant for Computer Science be-cause it is frequently used in general language and in other specific domains as biology. In this work we take a different approach to compute domain specificity that can be applied for general terms by using a domain coherence measure that does not use external corpora. Two general purpose cor-pora, the Open American National Corpus 3 and a corpus of books from Project Gutenberg 4 , are used as contrastive corpora for our implementa-tion of TermExtractor. The books selected from Project Gutenberg include the bible, the complete works of William Shakespeare, James Joyce X  X  Ulysses and Tolstoy X  X  War and Peace . We con-sider only the default setting of TermExtractor as-signing equal weights to each measure in Equation 4. We begin this section by describing an approach for domain modelling based on domain coherence in Section 2.1. Then, we discuss a modification of the NC-value approach which makes it better suited for intermediate level terms (Section 2.2). We conclude this section by describing a novel method for term extraction using a domain model in Section 2.3. 2.1 Domain modelling A domain model is represented as a vector of words which contribute to determine the domain of the whole corpus. Let  X  be the domain model, and w 1 to w n a set of generic words, specific to the domain, then:
The number of words n can be empirically set according to a cutoff associated weight. Previous work on using domain information for word sense disambiguation (Magnini et al., 2002) has shown that only about 21% of the words in a text actu-ally carry information about the prevalent domain of the whole text, and that nouns have the most significant contribution (79.4%). Several assump-tions are made to identify words that are used to construct a domain model from a domain corpus: 1. Distribution: Generic words should appear 2. Length: Only single-word candidates are 3. Content: Only content-bearing words are of 4. Semantic Relatedness: A term is more gen-The distribution assumption implies that rare terms are more specific, similar with the frequency-based measure previously used for measuring tag generality (Benz et al., 2011). This might not always be the case, for example a sim-ple search with a search engine shows that arte-fact or silverware are more rarely used than the term spoon , although the first two concepts are more generic. However, in this work we are in-terested in extracting basic-level categories as the-orised in psychology (Hajibayova, 2013). A basic-level category is the preferred level of naming, that is the taxonomical level at which categories are most cognitively efficient. A counter example can be found for the length assumption as well, as the longer term inorganic matter is more general than the single word knife , but in this case we would simply consider as a candidate the single word matter which is more generic than the compound term. Both length and frequency of occurrence are proposed as general criteria for identifying basic-level categories (Green, 2005).

The first three assumptions are used for can-didate selection, while the fourth assumption is used to filter the candidates. A possible solution for building a domain model is to use a standard termhood measure for single-word terms. Most approaches for extracting single-word terms make use of contrastive corpora, ranking higher specific words that are rarely used outside of the domain.
But our domain model is further used for term extraction, therefore it is important that we use generic words to insure a high recall.

We interpret coherence as semantic relatedness to quantify the coherence of a term in a do-main. The measure used for semantic relatedness is Pointwise Mutual Information (PMI). First, we extract multi-word terms using a standard term extraction technique, then we use the top ranked terms to filter candidate words using the following scoring function for domain coherence: where  X  is the domain model candidate,  X  is top ranked multi-word term,  X  is the set of top ranked multi-word terms and P (  X , X  ) is the probability that the word  X  appears in the context of the term  X  . In our implementation, the set  X  contains the best terms extracted by our baseline term extrac-tion method described in Section 2.2, but any other term extraction method can be applied in this step. A small sample from domain models extracted us-63 ing our domain coherence method for Computer Science, Food and Agriculture, and the Biomedi-cal Domain, is shown in Table 1. 2.2 Baseline term extraction method Our baseline approach for intermediate level term extraction is frequency-based, similar to the C-value method (Ananiadou, 1994), but we mod-ify its ranking function. The main difference is the way we take into consideration embedded terms. In previous work, this information is used to decrease frequency counts, as shorter terms are counted both when they appear by themselves and when they are embedded in a longer term. We ar-gue that the number of longer terms that embed a term can be used as a termhood measure. In our experiments, this measure only works for embed-ded multi-word terms, as single-word terms are too ambiguous. The baseline scoring method b is defined as: where  X  is the candidate string, |  X  | is the length of  X  , f is its frequency in the corpus, and e  X  is the number of terms that embed the candidate string  X  . The parameter  X  is used to linearly combine the embeddedness weight and is empirically set to 3.5 in our experiments. 2.3 Using domain coherence for term Although we proposed a method to build a do-main model in Section 2.1, the question of how to use this domain model in a termhood measure remains unanswered. Again, the solution is to rely on the notion of domain coherence, which is defined in this case as the semantic relatedness between a candidate term and the domain model described above. The assumption is that a cor-rect term should have a high semantic relatedness with representative words from the domain. This method favours more generic candidates than con-trastive corpora approaches, therefore it is better suited for extracting intermediate level terms.
The same measure of semantic relatedness is used as for the domain model, the PMI measure.
The domain coherence DC of a candidate string  X  is defined as follows: where  X  is a word from the domain model, and  X  is the domain model constructed using Equation 6. Using generic terms to build the domain model is crucial for ensuring a high recall as these words are more frequently used across the corpus. In our implementation context is defined as a window of 5 words.
Evaluating term extraction results across domains is a challenge, because finding domain experts is difficult for more than one domain. An al-ternative is to reuse datasets annotated for appli-cations where term extraction plays an important role, for example, keyphrase extraction or index term assignment. Three technical domain cor-pora are used in our experiments: Krapivin , a cor-pus of scientific publications in Computer Science (Krapivin et al., 2009); GENIA , a corpus of ab-stracts from the biomedical domain (Ohta et al., 2001); and FAO , a corpus of reports about Food and Agriculture (Medelyan and Witten, 2008) col-lected from the website of the Food and Agricul-ture Organization of the United Nations 5 . The
Krapivin corpus provides author and reviewer as-signed keyphrases for each publication. The GE-
NIA corpus is exhaustively annotated with biomed terms, with about 35% of all noun phrases anno-tated as biomed terms. The FAO dataset provides index terms assigned to each document by profes-sional indexers. It is not only the document size that varies considerably across these three cor-pora, but also the number of annotations assigned to each document as can be seen in Table 2.

We evaluate our measure for building a domain model in Computer Science, by identifying a list of general words with the help of a domain ex-pert in Section 3.1. We envision two sets of ex-periments: a standard term extraction evaluation where the top ranked terms are evaluated against the list of unique annotations provided in the eval-uation datasets (Section 3.2.1), and a second set of experiments where each term extraction approach is used to assign candidates to documents in com-bination with a document relevance measure in Section 3.2.2. 3.1 Intrinsic evaluation of a domain model A domain expert was asked to investigate nouns used in the ACM Computing Classification Sys-tem 6 . The expert was provided with the list of nouns and their frequency in the taxonomy and was required to identify nouns that refer to generic concepts. A set of 80 nouns were selected in this manner including system , information , and soft-ware . Only one annotator was involved because of the complexity of the task, that implies the analy-sis and filtering of several hundred words. We esti-mate the inter-annotator agreement by analysing a subset of the selected words through a survey with 27 participants. A quarter of the selected words are combined with the same number of randomly selected rejected words and the resulting list is sorted alphabetically. The Fleiss kappa statistic for interrater agreement is 0.34, lying in the fair agreement range. 80% of the words from our gold standard domain model were selected by at least half of the participants.

We compare our method ( DC ) with two other benchmarks, the contrastive termhood measure used in TermExtractor, and the frequency-based method used by NC-value to select context words (
NCV weight ). Again, context is defined as a window of 5 words. A domain model has many similarities with probabilistic topic modelling, al-though it provides less structure. We compare our approach with a popular approach to topic mod-elling, Latent Dirichlet Allocation (LDA) (Blei et al., 2003). We experimented with different num-bers of topics but we report only the best results achieved for 75 topics ( LDA 75 ).

The results of this experiment are shown in Fig-ure 1, in terms of F-score. Several conclusions can be drawn from this experiment. First, the methods that analyse the context of top ranked terms (i.e., our domain coherence measure, DC , and the weight used for context words in the NC-value, w NCV ) perform better than the contrastive measure used in TermExtractor, with statistically significant gains. Also, our domain coherence method outperforms the more simple frequency-based weight used in NC-value, although this re-sult is not statistically significant. As expected, the words ranked high by TermExtractor are too specific for a generic domain model. The topic modelling approach identifies several words from the gold standard but much less than our approach and these are evenly distributed across latent top-ics. These conclusions will be further investigated across two other domains, using gold standard terms annotated for three different applications in
Section 3. 3.2 Term extraction evaluation results
We implement and compare the baseline method presented in Section 2.2 and the method based on domain coherence described in Section 2.3, against the NC-value and TermExtractor methods, which are used as benchmarks. The same candi-date selection method is used for all the evaluated approaches. Candidate terms are selected through syntactic analysis by defining a syntactic pattern for noun phrases. To assure the results are compa-rable, the same number of context words is used 65 in our implementation of the NC-value approach as the size of the domain model. Two general pur-pose corpora, the Open American National Cor-pus 7 and a corpus of books from Project Guten-berg 8 , are used as contrastive corpora for our im-plementation of TermExtractor. We considered only the default setting for TermExtractor, assign-ing equal weights to each measure. 3.2.1 Standard term extraction evaluation
While keyphrases and index terms suit well our purposes, as they are terms of an intermediate level of specificity, meant to summarise or clas-sify documents, many of the terms annotated in GENIA are too specific. We discard the annotated terms that are mentioned in less than 1% of the documents from corpus, based on our distribution assumption. For each of the three datasets, the top ten thousand ranked terms were evaluated. We in-crementally analysed portions of the ranked lists computed using the baseline approach ( Baseline ), the baseline approach linearly combined with the domain coherence measure ( Baseline+DC ), and the two benchmarks, NC-value and TermExtrac-tor . The precision value for a portion of the list is scaled against the overall number of candidates considered. First, we observe that all methods perform better on the GENIA (Figure 4) and the Krapivin corpus (Figure 2), with the best methods achieving a maximum precision close to 60% at the top of the ranked list.

The Food and Agriculture use case is more chal-lenging, as the best method achieves a precision of less than 20%, as can be seen in Figure 3. Also, the contrastive corpora measure employed in
TermExtractor yields considerably worse results on all three domains, because the extracted terms are too specific. The baseline method, that re-wards embedded terms, outperforms the NC-value method on the Computer Science domain, and in the biomedical domain, but it performs slightly worse on the Agriculture domain. The combina-tion of our baseline method with the domain co-herence measure (referred to as Baseline + DC in the legend) yields the most stable behaviour, out-performing all other measures across the three do-mains, considerably so in the biomedical domain (Figure 4) and at the top of the ranked list in Com-puter Science (Figure 2). In Computer Science, domain coherence significantly outperforms the best performing state-of-the-art method, NC-value (Figure 2). In Biomedicine, the improvement is statistically significant, with a gain of 106% at top 20% of the list (Figure 4). 3.2.2 Application-based evaluation
An important reason for developing termhood measures is that they are needed in specific ap-plications, for example keyphrase extraction and index term extraction. Typically, a termhood mea-sure is combined with different measures of docu-ment relevance in such applications, as the candi-dates are assigned at the document level. We make use of the standard information retrieval measure
TF-IDF in combination with the considered term extraction scoring functions to assign terms to documents. The best results are obtained by us-ing domain coherence as a post-processing step.
In this experiment, the PostRankDC approach was computed by re-ranking the top 30 candidates se-lected using our baseline approach described in 66 Equation 7, based on their domain coherence.

The application-based evaluation proposed in this work allows us to evaluate both precision and recall, and consequently F-score can be used as an evaluation metric. The results for keyphrase extraction in Computer Science are presented in Table 3, while the results for index term extrac-tion in the Agriculture domain are shown in Ta-ble 4. The results for document level term extrac-tion from the Biomed corpus appear in Table 5. All three methods yield a higher performance on the GENIA corpus. The results on the Agricul-ture corpus are again the lowest, because a larger number of candidates has to be analysed.

Our Baseline method outperforms the NC-value approach on the Krapivin corpus and on the GENIA corpus, but not on the FAO corpus. We can observe that the domain coherence approach (
PostRankDC ) improves over our baseline ap-proach ( Baseline ) on all three domains. The im-provement is statistically significant compared to the best state-of-the-art method in Computer Sci-ence, NC-value. NC-value outperforms TermEx-tractor in Computer Science and Agriculture, but TermExtractor performs better in Biomedicine. Although both NC-value and TermExtractor make use of domain-independent features for ranking, their performance varies across domains and ap-plications. At the same time, combining our domain coherence approach ( PostRankDC ) with our baseline method in a post-ranking step dis-plays a more stable behaviour, achieving the best performance on the Computer Science domain (Krapivin) and similar results with the results of the best method in Biomedicine (GENIA) and Agriculture (FAO). In this study, we proposed an approach to iden-tify intermediate level terms through domain mod-elling and a novel domain coherence measure, ar-guing that approaches that make use of contrastive corpora are only suitable for updating existing ter-minology resources with more specific terms and not for summarisation or classification tasks. The contributions described in this work are three-fold:
Experiments discussed in this paper show that term extraction performance depends on the do-main, although systems make use of domain-independent features. Our domain coherence ap-proach based on a domain model performs well across domains, while the performance of the
NC-value and TermExtractor benchmarks is more domain-dependent. The results lead to the conclu-sion that using a domain model is more appropri-ate than using statistical approaches based on con-trastive corpora, for extracting intermediate level terms. Future work will include an unsupervised learning-to-rank approach for term extraction, that will allow a more principled integration of domain coherence measures with standard term extraction features. The method proposed here can be used as a specificity measure, and we currently investi-gate this in the context of constructing generalisa-tion hierarchies of concepts.
 This work has been funded in part by the European
Union under Grant No. 258191 for the PROMISE 67 project, as well as by a research grant from Sci-ence Foundation Ireland (SFI) under Grant Num-ber SFI/12/RC/2289.

Automatic term extraction from texts of a spe-cific domain is one of the well-studied applica-tions in natural language processing and document analysis. During many years of research a lot of useful features of domain term extraction were proposed, including frequency-based and context-based features, word association measures, etc. ((Daille, 1995), (Zhang, 2008)).

Since these features characterize various prop-erties of terms, machine-learning models based on multiple features are now increasingly used for term extraction. It was shown that such models can work considerably better than those based on single features ((Aze et al., 2005), (Loukachevitch, 2012)). Nevertheless, the signifi-cance of particular features for term extraction by machine learning depends on several important as-pects concerning the domain in the target text col-lection, structure of extracted terms, and type of a terminological resource to be developed.

Firstly, specific domains vary in their scope (e.g., the broad social-political domain vs. the relatively narrow banking domain). Besides, domain-specific languages vary in their closeness to the general language (e.g. banking vs. im-munology domain). This enhances or diminishes the role of a reference text collection required to calculate some term features (usually, a news col-lection or a national corpus is used).

Secondly, terms may be single-word and multi-word. To extract single-word terms, word associ-ation measures (mutual information, t-score, etc.) are not applicable; extraction of three-word and longer terms requires special forms of associa-tion measures. It means that extraction models for terms of different lengths can differ.

At last, terms are extracted for various types of terminological resources: terminological dictio-naries, information-retrieval thesauri, ontologies for NLP. Dictionaries are mainly intended to sup-ply terms with definitions, whereas information-retrieval thesauri are to provide concepts (descrip-tors) for domain-specific applications (Z39.19, 2005).

For example, such terms from EuroVoc information-retrieval thesaurus as agricultural product, milk product, European party, eco-nomic consequence denote important concepts in the contemporary socio-political life of European
Union, however, it is difficult to imagine these terms as entries in terminological dictionaries.
Therefore, a particular type of a terminological re-source needs specialized term extraction models (Loukachevitch, 2012).

In this paper we consider the term extraction task specially for thesauri intended to be used in the information retrieval context (search, cat-egorization, clustering and other applications), 69 because we suppose that such terminological resources have specific properties partially ex-plained in specialized standards (Z39.19, 2005).
For this task we experimentally study machine-learning models based on various features for term extraction. Our study is based on two manu-ally created thesauri and two languages: the En-glish version of Eurovoc thesaurus and the Rus-sian Banking thesaurus. We restrict our study to single-word and two-word terms to compare the extraction models for the most frequent types of terms.
Machine-learning or combined approaches to automatic term extraction were studied in a num-ber of works: (Vivaldi et al., 2001), (Aze et al., 2005), (Foo and Merkel, 2010), (Zhang, 2008), (Loukachevitch, 2012).

In most works automatically extracted terms are evaluated on the basis of available terminological resources or expert annotations of domain terms ((Daille, 1995), (Church and Hanks, 1990), (Dun-ning, 1993), (Church and Gale, 1995)). If to con-sider evaluation of machine-learning models for term extraction, in (Aze et al., 2005) experiments were fulfilled for texts in biological and human resources domains with expert annotation of do-main terms. In the work (Foo and Merkel, 2010) two patent collections with term pre-annotation were studied. (Zhang, 2008) extracted terms from the Genia corpus, for which Genia ontology was created, and also utilized an artificial corpus of Wikipedia articles with expert annotation of terms.
In contrast to the above-mentioned works, in our study of term extraction we focus on the specific type of terminological resources  X  the-sauri intended for information-retrieval applica-tions. We take the well-known terminological re-source EuroVoc and Banking thesaurus created for the Central Bank of the Russian Federation. Both resources are used in indexing and retrieval of doc-uments in real information-retrieval systems. 3.1 EuroVoc Thesaurus and Europarl Text
For the English part of our study we took Eu-roVoc thesaurus and Europarl parallel corpus. Eu-roVoc is an official thesaurus of the European
Union and is intended for manual indexing of EU parliamentary documents. It is a multidisciplinary thesaurus covering the EU activities and contain-ing terms in 22 languages of the EU. The En-glish version of EuroVoc comprises 15161 terms ( http://eurovoc.europa.eu/drupal ).

The Europarl parallel corpus was extracted from the proceedings of the European Parliament ( http://www.statmt.org/europarl/ ).
 The English part includes almost 54 mln. words.
In fact, EuroVoc thesaurus is intended just for the description of Europarl documents. There-fore, we can model how EuroVoc thesaurus could be developed from the Europarl corpus. EuroVoc represents a broad socio-political domain, and its language is close to general English. 3.2 Banking Thesaurus for the Central Bank For the Russian part of our study we took the
Banking thesaurus created for the Central Bank of the Russian Federation. It is used in an information-retrieval system for indexing, search and vizualization of information and as a basis for text categorization. The thesaurus includes about 15 thousand terms and comprises the terminology of banking activity, banking regulation, monetary politics and macroeconomics.
 As an appropriate text collection we took 10422 Russian articles from various on-line magazines:
Auditor, RBC, Banking Magazine, etc. These documents contain almost 15.5 mln. words.

Since the banking thesaurus is used in real in-formation retrieval tasks, we can model how it could be developed from the banking text corpus. In contrast to the broad socio-political domain of
EuroVoc, this thesaurus represents relatively nar-row banking domain, and its language is not so close to general language.
In our study we investigated single-word and two-word term extraction separately in order to have possibility to compare corresponding extrac-tion models. As single-word term candidates we consider only Nouns and Adjectives (for Russian language) and Nouns (for English language); as two-word candidates we consider only Adjective + Noun and Noun + Noun (for Russian language) and Adjective + Noun , Noun + Noun , and Noun + 70 of + Noun (for English language) since they cover the majority of terms.

We use several types of enough known features for term extraction proposed in previous works and relatively new topic-based features proposed in (Bolshakova et al., 2013). 4.1 Traditional Features
The first type of traditional features is frequency-based features . The main assumption is that terms differ in their frequency and the distri-bution from other words in the target corpus. We consider the following 8 features: Term Frequency in the collection (tf), Document Frequency (df), TF-IDF, TF-RIDF (Church and Gale, 1995), Do-main Consensus (Sclano and Velardi, 2007), Term Contribution, Term Variance Quality, Term Vari-ance (Liu et al., 2005).

The second type of traditional features is based on the target and reference corpora and sup-poses that term frequencies in the target and ref-erence corpora should be significantly different. We consider 9 such features, namely: Weird-ness (Ahmad et al., 1999), corpus-based TF-IDF (where TF is taken from the target corpus, and IDF is taken from the reference corpus), Rele-vance (Pe  X  nas et al., 2001), Contrastive (Basili et al., 2001) and Discriminative (Wong et al., 2007) Weights , Lexical Cohesion (Park et al., 2002), Reference Weight, KF-IDF (Kurz and Xu, 2002), Loglikelihood (Gelbukh et al., 2010). In our study n-gramm statistics from British Na-tional Corpus ( http://www.natcorp.ox. ac.uk/ ) and Russian National Corpus ( http: //www.ruscorpora.ru ) were used as statis-tical data of a reference corpus for English and Russian collections correspondingly.

The third type of traditional features comprises word-association measures estimating mutual correlation of term candidate usage. They are primarily intended for two-word collocation ex-traction and are not applicable for single-word term extraction. We consider 19 word association measures: Mutual Information (MI) (Church and Hanks, 1990), Augmented MI (Zhang, 2008), Cu-bic MI (Daille, 1995), Normalized Pointwise MI (Bouma, 2009), True MI, Dice Coefficient (DC) (Smadja et al., 1996), Modified DC, Generalized DC (Park et al., 2002), T-Score, Z-Score, Sym-metric Conditional Probability (Lopes and Silva, 1999), Simple Matching Coefficient, Kulczinksy Coefficient, Ochiai Coefficient, Yule Coefficient, Jaccard Coefficient (Daille, 1995), Chi Square, Loglikelihood Ratio (Dunning, 1993), Gravity Count (Daudarvi  X  cius and Marcinkevi  X  cien  X  e, 2005).
The last type of traditional features is context-based features that account for phrases encom-passing term candidates and their left and/or right context. We define a context of a term candidate as the bounds of encompassing noun phrases. In our study 11 known context-based features were con-sidered: C-Value, NC-Value (Frantzi and Anani-adou, 1994), MNC-Value, Token-LR, Token-FLR, Type-LR, Type-FLR (Nakagawa and Mori, 2003),
Sum3, Sum10, Sum50, Insideness (Loukachevitch, 2012).

Besides, we propose a novel context-based fea-ture: Modified Gravity Count (MGCount) . It is based on Gravity Count association measure described in (Daudarvi  X  cius and Marcinkevi  X  cien  X  e, 2005). MGCount for xy phrase is calculated as follows:
M GCount = log where f ( x ) is the frequency of x , f ( y ) is the fre-quency of y , f ( xy ) is the frequency of xy phrase, l ( x ) is the number of different words to the left of x , and r ( y ) is the number of different words to the right of y ; l ( x ) and r ( y ) are considered only within the bounds of encompassing noun phrases. second component of the sum), thus the measure was transformed from the association measure to the context one. 4.2 Topic-Based Features
The next type comprises features based on so-called topic models (Blei and Lafferty, 2009).
Topic models are intended to describe texts in terms of their topics, they determine, which top-ics are related to each document, and which words (or phrases) form each topic. In fact, each topic is represented as a list of frequently co-occurring words (or bigrams) ordered by descending degree of belonging to it. As an example, the first five words and bigrams from the top of four randomly selected topics of the English corpus along with 71 their probabilities of belonging are presented in the Table 1.

Typically, there are two types of topic models: non-probabilistic ones that are based on hard clus-tering methods (K-Means, hierarchical agglom-erative clustering, etc.) and probabilistic ones (PLSI, LDA, etc.) that represent each document as a mixture of topics and each topic is considered as a probabilistic distribution over words (Blei and Lafferty, 2009), (Bolshakova et al., 2013).

The topic-based features are relatively new and are obtained by revealing topics in the target text corpus. These features account for the idea that domain terms should usually correspond to some subtopics of the domain. As it was shown that NMF (Non-Negative Matrix Factorization) algo-rithm with KL-divergence minimization is the best topic model in terms of terminology extraction (Bolshakova et al., 2013), we applied it to reveal subtopics, as well as probabilities in them. Ba-sically, given a non-negative term-document ma-trix V , this algorithm tries to find non-negative term-topic matrix W and topic-document matrix H , such that V = W H . We consider the ver-sion of NMF that minimizes Kullback-Leibler di-vergence D ( V || W H ) (Lee and Seung, 2000).
We consider the following 7 topic-based fea-tures: Term Frequency, TF-IDF, Domain Consen-sus, Maximum Term Frequency (Bolshakova et al., 2013), Term Score (TS) (Blei and Lafferty, 2009), TS-IDF, Maximum Term Score . Most of these features are extensions of the standard frequency-based features applied to the revealed subtopics, considering probabilities of the term candidates in topics as frequencies (cf. Table 2; P i ( w ) denotes a probability of the term candidate w in the topic i , and K is the number of topics).

We also used 6 single-topic document features (documents are regarded as separate topics). In fact, we used all above-mentioned topic-based features except Domain Consensus, since this fea-ture is already considered in the section of tradi-tional frequency-based features (cf. section 4.1). 4.3 Other Features
Other features considered in our study include:  X  5 Linguistic features: Ambiguity (determines  X  Features for term candidates that play subject  X  2 features for term candidates that are in  X  Average position of the first occurrence in
Thus, 27 features belong to this group. To sum up, the full list of features comprises 69 fea-tures for single-word candidates and 88 features for two-word term candidates.
We studied models for single-word and two-word term extraction from two above-described corpora: Russian banking electronic magazines, and English part of parallel corpus Europarl.
To extract single-word and two-word term candidates from these corpora, documents were processed by morphological analyzers. Thus, for English corpus we used Stanford POS tagger ( http://nlp.stanford.edu/ software/corenlp.shtml ), while for Russian corpus we used our own morphological analyzer. Besides, from the set of extracted English term candidates we excluded words from the stop list created for the experiments ( other, another, that, this, those, mrs, sir, etc. ), and word pairs including stop-words were excluded as well.
Having extracted term candidates, we trained combined models comprising the above-described types of term features. The features were com-bined by Gradient Boosting machine learning al-gorithm, which proved to be the best one in our study. Namely, we used an open-source realization of this algorithm from http:// scikit-learn.org . It is well-known that Gradient Boosting has a lot of parameters that need to be tuned. So, in all experiments we fixed all parameters, except the number of trees and maximum allowed depth of trees, that were tuned in each experiment individually. Besides, for training and evaluation four-fold cross valida-tion was applied, which means that every time the training set was three-quarters of the whole list while the testing set was the remaining part.
A term extraction model has to find the best or-der, where real terms should be located at the be-ginning of the ordered list of term candidates. As an evaluation measure, we used Average Precision (AvP) often applied as a measure for term extrac-tion (Zhang, 2008), (Bolshakova et al., 2013). It is defined for a set D of all term candidates with a subset of approved ones D q  X  D as follows:
AvP ( D ) = where r i = 1 if the i -th term  X  D q and r i = 0 otherwise.

At the first step of experiments we separately studied term extraction models for single-word and two-word terms. As baselines we considered several well-known features: Weirdness, TF-IDF, C-Value for single-word models and TF-IDF, C-
Value, Mutual Information for two-word ones. In the Figures 1, 2, 3, 4 plots of AvP on various num-bers of most frequent candidates are presented for these baselines, the best single feature and the re-sulted model combined by Gradient Boosting.

As we can see, the best single feature for single-word terms turned out to be a topic-based feature (either Maximum Term Score or Maximum Term Frequency ), the performance of these features is considerably better than well-known baselines. So it seems that for single-word terms their relation to a domain subtopic is important.

The best single feature for two-word terms was found to be the novel context-based feature Mod-ified Gravity Count . Besides, in all cases we can see the huge improvement of the combined model performance compared to well-known baselines and best single features.

At the second step of experiments we tried to determine the contribution of each above-described group of features to the whole com-bined model. We fixed the number of most fre-quent term candidates to 5000, excluded each of the following groups separately from the whole list: frequency-based features; features, based on the reference corpus; word association measures; context-based features, and topic-based features.
The results of combining the remaining features by Gradient Boosting are presented in the Table 3.
As we can see, features, which are based on the reference corpus, give the most significant contri-bution to the two-word term extraction models re-gardless of the subject domain and language.

Besides, the use of word association measures does not improve the quality of extraction of two-word terms. The latter conclusion contradicts the assumption of numerous studies that association measures should be useful for multi-word term extraction (Zhang, 2008), (Daille, 1995), (Kurz and Xu, 2002). From the other side, this con-clusion can be quite evident because, for exam-ple, EuroVoc includes a lot of terms looking as compositional phrases with free separate usage of components (as European party, European idea, economic consequence etc.). Introduction of such terms into an information-retrieval thesaurus is possible due to multiple principles of term in-clusion in information-retrieval thesauri (Z39.19, 2005).

At the last step of experiments we investigated both models for single-word and two-word term candidates together. We created a unified model for both types of term candidates, taking into ac-count all features except association measures and obtaining as a result the unified list of candidates.
Then we created specific models separately for single-word and two-word term candidates. As the models are specialized, they can be potentially more efficient. We summed up resulted lists of extracted terms according to their probability val-ues generated by Gradient Boosting, and in such a way obtained the summed-up list of term can-74 didates. We should notice that in the case of the unified model there is more data to train it, so this model can be potentially very efficient too.

The comparison of AvP for these two models (for both corpora) shows that summed-up model slightly outperforms the unified one  X  cf. Figure 5.
In addition, as an example of the extracted term candidates, we present in the Table 4 the first 10 elements from the top of the term candidates lists created by unified models for Russian and English corpora (the elements in italics are real terms).
The resulting unified models may be too com-plex in the number of applied features. Some of them may be redundant for Gradient Boosting and have no use in the models, make their training harder. In order to exclude them we applied a step-wise greedy algorithm Add for selecting the most significant features.

The algorithm starts with the empty set of fea-tures, and then at each step it adds the feature that maximizes the overall Average Precision, un-til there is any improvement between successive iterations. As a result, the combinations of only 13 features (out of total 69 features) were found for both corpora (see Table 5). We grouped simi-lar features in the same rows of the table.
Since there are representatives of all above-described groups in both found subsets of features, we conclude that each such group is significant for unified models of term extraction regardless of the scope and language. Besides, we can see that short models for both thesauri are quite similar.
In this paper we modelled single-word and two-word term extraction for the specific type of ter-minological resources  X  information-retrieval the-sauri. Our experiments revealed features signifi-cant for extraction of single-word and two-word terms in the broad EuroVoc and relatively narrow banking domains. We showed that the best fea-tures for single term extraction in both cases are relatively new topic-based features, based on pre-liminary clustering of words in the target text col-lection. The context-based features are the most important for two-word term extraction.

The interesting result of our study is that the use of association measures does not improve the quality of term extraction models intended for information-retrieval thesaurus construction. It was also proved that the unified model can be ap-plied to both single-word and two-word term ex-traction.
 The work is partially supported by Dmitrii
Zimin Dynastia Foundation with financial support of Yandex founders. 75 and Management of Monolingual Thesauri NISO. 76
In modern times, technical progress accelerates the development of new disciplines and sub-disciplines. That is why new and specific termi-nology becomes more necessary. The conse-quences of this interdisciplinarity are multilingual communication problems between non-professionals and experts of a special field or between experts of navigation domains of dif-ferent transportation modes. Especially, these semantic problems include synonymy, antonymy, hypernymy-hyponymy relations and ambiguity etc. The main target of the iglos 1 ter-minology management system as terminological tool of the 21st century is to avoid the multilin-gual misunderstanding between special lan-guages of different domains by comparing the definitions of technical terms in heterogeneous languages. 
Keywords  X  terminology management, terminology, multilingual, semantical problems, localisation, positioning, position, location for different technical languages or varieties at international level. Multilingual communication problems such as synonymy, antonymy, transla-tion problems, ambiguity, risk of confusion etc. between experts of special fields intensify by the transforming technology. People extend and spe-cialise their acquirements and tasks in the fields of technology. In general, the correct translation of a technical term is only possible if one knows the terminology of the special field. and  X  X osition X  and  X  X ositioning X  are in the focus of our terminological contemplation. At this point, we want to analyse which relation exists between these terms in English, German and 
French. Finally, we want to display the equiva-lences in three languages. a cooperation of the Institute for Traffic Safety and Automation Engineering and the Department of German Linguistics of the Braunschweig Uni-versity of Technology.  X  X he project is interdisci-plinary and it consistently grows in complexity and richness of perspective by our dialogue with linguists, terminologists, computer scientists, engineers, translators and users. X  (Arndt; Schn X pp; Schnieder; Yurdakul, 2013). ware platform on a linguistic basis. With this terminology management system, it is intended to accelerate and facilitate a consistent, multilin -gual and unambiguous development of technical terminology for optimising the scientific and commercial communication. The foundation of the iglos system consists in a development of the variety-based trilateral sign model. two lexemes and on the other hand of one rela-tional lexeme which is placed between these both (Schnieder, 2012). or the unit of a word paradigm. In general, a lexeme is concretised by its grammatical word forms. In our case, terms are special lexemes. denomination such as  X  X avigation X ), a definition which relates to the context and a variety as a technical language (e.g. linguistics, technics, etc.). There is at least one relation between two lexemes. This is specified by at least one relational lexeme or a relation type. confusion ( isMixesUpWith ), translation ( hasTranslation ), output ( hasOutput, isOutputOf ), input ( hasInput, isInputOf ), holonymy ( hasPart, isPartOf ), meronymy ( isPartOf, hasPart) , antonymy ( hasAntonym ), synonymy ( isSynonymOf ), polysemy ( isPolysemOf ), homonymy ( hasHomonym ) 
Basically, the iglos sign model enables the speci-fication of terminologies by avoiding termino-logical haziness and creating and visualising concrete relations between terms in a systematic context (variety). These relations are unobstructedly typable. and ambiguity (disambiguation) of terms. On the basis of the variety-based iglos sign model, there is a communication between different languages (multilingualism) on the one hand and between different domains (multidisciplinarity) on the other hand. In our consideration, we want to pre-sent the multilingual problems in relation to ter-minology in the domain of traffic engineering and which solution approaches can be proposed for them on the basis of iglos . eral methodological approaches for avoiding the linguistic problems e.g. between the terms  X  X osi-tion X ,  X  X ositioning X ,  X  X ocation X  and  X  X ocalisa-tion X  in English, German and French: 
German and French in general linguistic usage , in etymologic, grammatical, normative tech-nical and relational perspective. kinds of these semantical problems between the different terms in the navigation domain. raised the four terminological questions: terms can be avoided or minimised by relating them with each other. According to normative sources (e.g. DIN standards), we tried to create relations between these terms. There are various relation types such as  X  X ynonymy X ,  X  X ntonymy X ,  X  X art-whole relation X ,  X  X ypernymy-hyponymy relation X ,  X  X mbiguity X ,  X  X ranslation X   X  X equence X ,  X  X unction X ,  X  X isk of confusion X ,  X  X onverseness X  etc. es, we have found out: 80 the result that  X  X ositioning X  and  X  X ocalisation X  or rather  X  X osition X  and  X  X ocation X  are not syno-nyms. Firstly, they are sequent and secondly there is a risk of confusion between both. 
Whereas,  X  X ositioning X  describes the process for achieving the place in a three-dimensional refer-ence system,  X  X ocalisation X  is a large place in a two-dimensional reference system and enables local orientation on the surface of the earth (see 
Yurdakul; Schnieder; Hodon 2013). type that describes a point or geometry potential-ly occupied by an object or person X  (ISO 19132, 2007) and  X  X ocation X  is the  X  X dentifiable geo-graphic place X  (ISO 19134, 2007) which must be localised by functional resources. are state nominations. In contrast to them,  X  X osi-tioning X  and  X  X ocalisation X  are specific functions.
A function is a specific process that a system is able to perform whereas a state is the entirety of all physical quantities of a physical system. In general, we can notice that there is a causal and functional coherence between these functions and states:  X  X ocalisation X  leads to  X  X ocation X  and  X  X ositioning X  to  X  X osition X  or  X  X ocation X  isOutputOf  X  X ocalisation X  and  X  X osition X  of  X  X o-sitioning X . In addition,  X  X osition X  isInputOf  X  X o-calisation X  and  X  X ocation X  of  X  X ositioning X . 
Finally, all terms together (as hyponyms) are related to  X  X avigation X  (figure 2). to German and French. On the basis of this schedule, it becomes obvious that the terms  X  X o-sition X  and  X  X ositioning X  have similar denomina-tions in French and German whereas  X  X ocation X  and  X  X ocalisation X  are differently denominated. 
The term  X  X ocalisation X  has the same denomina-tion in French, but not in German. In relation to  X  X ocation X , there are different denominations in each of the three languages. tioning X  are internationalisms which is a bor-rowed term and which occurs in several languages.  X  X ocation X  and  X  X ocalisation X  are typ-ically English terms but not especially German. 
Instead of  X  X rtung X , there will be the possibility to use  X  X okalisierung X  as internationalism in German. 
English German French position Position position location Ort lieu positioning Positionierung positionnement localisation Ortung/ 
Table 1: Equivalences of terms in German and French ous and consistent relation types, a terminology building can be constructed. There are various relations between different terms in this building. scribed as an ontology structure with single lex-emes are bound together by relations. 
Furthermore, iglos is a terminology management system of the next generation which collects and integrates different (technical) languages and guides terminologists to construct terminology. munication problems not only between experts of traffic engineering. Linguistic, normative and scientific definitions and the iglos sign model offer several advanced methodological solutions and approaches for identifying and avoiding the-se problems. between the terms  X  X osition X ,  X  X ocation X ,  X  X ocali-sation X  and  X  X ositioning X . These differentiate in causal and functional relation. 
In addition, translations from English into Ger-man and French are not very unproblematic be-cause e.g.  X  X osition X  is translated in German both as  X  X age X  and as  X  X tandort X . At this point, we have a risk of confusion with  X  X ocation X  be-cause of the translation problem in German or in other languages. Finally, we could find out that the four analysed terms exist in all three lan-guages and that the relation between them is the same. determination of relations between these four terms in each three languages. 
Ay  X  e Yurdakul, Eckehard Schnieder, Michal Hodon. 
Eckehard Schnieder. 2012. Qualit X t dynamischer Sa-DIN EN 13848-4: Railway applications -Track -
DIN EN ISO 19148: 2012-06: Geographic infor-
IEC 60351-1: 1976: Expression of the properties of 
ISO 19132: 2007. Geographic information -Location-based services -Reference model. Berlin, Beuth. 
ISO 19134: 2007. Geographic information -Location-based services -Multimodal routing and naviga-tion. Berlin, Beuth. 
Online Etymology Dictionary. http://www.etymonline.com, (citated on September 6th, 2013). 
Oxford Online Dictionaries. oxforddictionaries.com (citated on September 6th, 2013). 
Susanne Arndt, Dieter Schn X pp, Eckehard Schnieder &amp; Ay  X  e Yurdakul 2013. iglos . The intelligent Glossary  X  Terminology Management as Knowledge Network. (poster presentation). 
Braunschweig University of Technology, Institute for Traffic Safety and Automation Engineering, 
Braunschweig. EURO-ZEL 2013  X  21 st Interna-tional Symposium. University of Zilina. Zilina. 
In this article we present a preliminary study of the existin g ontologies and lexica which could be reused in order to characterize neologisms in the realm of the neurosciences.
 manage terminology in an efficient way it is essential to reuse existing resources and tools, rather than starting from scratch with every new project (McCrae et al 2012). Until quite recently, this effort was an almost impossible one, due to the lack of interoperability of resources , which are generally inaccessible for a number of diffe r-ent reaso ns such as the incompatibility of fo r-mats or the inaccessibility of data, among others. and the Linked Data paradigm opens up a new panorama in which data can (and should) be linked and shared across projects and ap plic a-tions. In this paper we focus on the possibilities of applying the principles of Linked Data to the characterization of new concepts within the Ne u-roNEO project and describe existing resources and technologies that might be suitable for our purposes. 
First, we present the main objectives of the Ne u-roNEO project and the role played by conceptual organization (section 2). Then we describe the ontological and lexical resources in the neurosc i-ences that could be reused for the purposes of the 
NeuroNEO project (section 3). In section 4 we focus on the principles of Linked Data and their possible application to terminology projects. In section 5 we review the lemon model proposed by McCrae et al . (2012) for interchanging lexical resources over the semantic web. Finally, we point out the main challenges and pitfalls that lay ahead in the NeuroNEO project.
The NeuroNEO Project (Garc X a Palacios et al, in press) aims to observe, compile and analyze newly -created Spanish terms in the co nstantly -evolving realm of the n eurosciences, with a sp e-cial focus on the very moment and context in which neologisms are born. to collect new lexical units in clo se collaboration with specialists in the field and with specialized translators as necessary collaborators and dec i-sive agents in the dissemination of neologisms. 
For further details on the process of neology detection and handling within NeuroNEO refer to Garc X a Palacios et al. (in press).
 need to observe the place they occupy within the conceptual structure of the discipline. With a view to corroborating whether new terms are semantically motivated, the concept ual structure from the specialists X  point of view needs to be considered, that is, we should observe how ne u-roscientists organize knowledge about their field. motivated and can therefore be easily related to othe r already existing close concepts in the fiel d, the chances for these new terms to be accepted by experts will be higher.
Over the past years, several joint efforts have been made to collaborate in the exchange of o n-tolog ies in the biomedical sciences. One of them is the BioPortal of the National Center for Bi o-medical Ontology (NCB O 1 ) , which provides access to commonly used biomedical ontologies and to tools for working with them. Within the portal two ontologies dedicated to the neurosc i-ences are available: the Computational Neurosc i-ence Ontology 2 and the NIFSTD ontology, described in more detail later. Both can be dow n-loaded in OWL and therefore reused with appl i-cations such as ontology editors. tologies in the biomedical domain is that of the 
OBO Foundry 3 , which aims at making a core of ontolo gies fully interoperable by virtue of a common design philosophy and implementation. terest shown by life scientists in sharing ontol o-gies and conceptual structures across the web. 
EHTOP, European Health Terminol o-gy/Ontology Portal (Grosjean et al, 2011). The 
EHTOP is a repository that provides access to 32 health terminologies and ontologies, both for humans and computers. It focuses on French and 
English, although some data are available in other languages, too. multiple resources of the medical sciences, i n-cluding the neurosciences. However, as far as we know, it is not prepared to link data using RDF or the Linked Data principles and Spanish is not one of the supported langua ges. 3.1 Neuroscience Information Framework 
One of the most outstanding efforts to develop domain ontologies in the field of the neurosc i-ences is being carried out within the Neurosc i-ence Information Framework (NIF 4 ). NIF is a dynamic inven tory of w eb -based neuroscience resources that gives access to data, materials, and tools via the Internet , bringing together the e f-forts of 16 centers for neuroscience research. 
In order to overcome the need for a shared s e-mantic framework for neuroscience, NIF has developed NeuroLex 5 , a lexicon of common neuroscience terminology built from NIFSTD in a modular fashion, with separate modules cove r-ing major domains of neuroscience (e.g., anat o-my, cell types, techniques) . NIF Standard ontology (NIFSTD) is constructed according to the set of best practices established by the OBO 
Foundry project. In this way it was designed to avoid duplication of effort and ensure that work performed under one domain is of maximum utility to the broader communit y by conforming to standards that p romote reuse (Bug et al . , 2008). All data are open access and codified in 
RDF and the entire ontology can be downloaded in OWL. This feature makes it especially useful for our purposes, as we could download parts of the o ntology and include our newly discovered concepts within the already existing NIFSTD structure.
 The main drawback for our purposes is that both 
Neurolex and NIFSTD are codified in English and we want to describe Spanish neologisms. 
The need for  X  X ocalization X  of the ontology has to be assessed before using the NIFSTD ontol o-gy in a Spanish context (Espinoza et al , 2009). 
The term Linked Data refers to a set of best pra c-tices for publishing and connecting structured dat a on the Web . It also refers to data published on the Web in such a way that it is machine -readable, its meaning is explicitly defined, it is linked to other external data sets, and can in turn be linked to from external data sets (Bizer, Heath &amp; Berners -L ee , 2008). ferred to as Web of Data, is described as  X  a web of things in the world, described by data on the Web  X  ( Bizer, Heath &amp; Berners -Lee , 2008 ). publishing data on the Web as Linked Data, known as Linked Data principles: data in RDF (Resource Description Framework) format. RDF provides a generic, graph -based data model with which to structure and link data that describe things in the w orld. In RDF, data are represented as a Subject -Object -Predicate structure, where the objects and subjects are e i-ther resources or literals (van Erp , 2012). Several 
RDF extensions are being designed with the goal to formalize knowledge bases like terminolo gy databases and lexical -semantic resources. As 
Chiarcos et al . (2012) point out,  X  X f (linguistic) resources are published in accordance with these rules, it is possible to follow links between exis t-ing resources to find other, related data and e x-ploit net work effects X .
 this are interoperability of language resources (both at the structure and the conceptual level) and information integration , that is, how to combine heterogeneous information from diffe r-ent sources in an efficient way (Chiarcos et al ., 2012). 
In our quest for a model that enables ontologies to be reused and to overcome th e language barr i-er, we are considering the lemon model (McCrae et a l ., 2012 a ) for interchanging lexica over the Semantic Web.

RDF model that allows lexica to be specified for ontologies and allows these lexica to be pu b-lished on the Web (McCrae et al ., 2012b). The main features o f the model are:
URIs are used to name and dereference linguistic annotations, and links can be easily created b e-tween lexicons using RDF tr iples.
 several lexica in different languages can be ass o-ciated to one and the same ontology. be of use in our building of a Spanish lexicon upon NIFSTD ontology. 
In this paper we have looked at different poss i-bilities to reuse existing resources in the chara c-terization of neologisms. suitable scenario for reusing resources widely accepted by neuroscientists. NIF STD ontology is readily available in OWL format, making it sui t-able for the exchange with applications such as ontology editors. use of resources originally created for the En g-lish language for characterizing neologisms in 
Spanish. In the NeuroNEO project we are going to intervene when the neologism has just been created or has not been created yet. In a certain sense, the Spanish -speaking neuroscientist is working in a bilingual situation (English -
Spanish). On e consequence of this special kind of bilingualism is that there will probably be a single conceptual structure both for English and 
Spanish. This is why we believe that conceptual and linguistic resources created for the English language could be reused f or the purposes of organizing Spanish neologisms in this field.
 to be addressed. We have seen that models such as lemon could be used to share ontology lexica across the web in different languages. the trade -off between effort and quality of the result. The main issues that still lay ahead are the following: will be able to bene fit from previous efforts and contribute to the scientific community with our findings.

This work is supported by the Spanish Ministry of Economy and Competitiveness within the national project N euroNEO "Regulaci X n de los procesos neol X gicos y los neologismos en las  X reas de neurociencias" (code FFI2012 -34596) .
Christian Bizer, Tom Heath , and Tim Berners -Lee. tional Journal on Semantic Web and Information Systems , 5(3): 1 -22. 
Christian Chiarcos, Sebastian Nordhoff, and Seba s-tian Hellmann. 2012. Linked Data and Linguistics: Representing and Connecting Language Data and Language Metadata. Berlin: Springer: 57 -64. Joaqu X n Garc X a Palacios , Jes X s Torres del Rey , Nava Maroto, Daniel Linder, Goedele De Sterck, and
Mi guel S X nchez -Ib X  X ez. In press . NeuroNEO , una investigaci X n multidisciplinar sobre la neolog X a terminol X gica . 
John McCrae , Guadalupe Aguado -de -Cea, Paul Buit e-laar, Philipp Cimiano, Thier ry Declerck, Asunci X n 
G X mez -P X rez, Jorge Gracia, Laura Hollink et al . 2012 a. Interchanging lexical resources on the S e-mantic Web . Language Resources and Evaluation. Springer.
 John McCrae, Elena Montiel -Ponsoda, and Philipp 
Cimiano. 2012b. Integrating WordNet and Wi k-tionary with lemon . In Christian Chiarcos, Seba s-tian Nordhoff, and Sebastian Hellmann. 2012. Linked Data and Linguistics: Representing and Connecting Language Data and Language Metadata. Berlin: Springer:25 -34.
 Julien Grosjean, Tayeb Merabti , Nicholas Griff on, Badisse Dahamna, and Stefan Darmoni. 2011. Multiterminology cross -lingual model to create the 
European Health Terminology/Ontology Portal . 9th International Conference on Terminology and 
Artificial Intelligence, TIA 2011, Paris, 8  X  10 N o-vember 2011 : 119  X  122.

Marieke van Erp. 2012. Reusing Linguistic R e-sources: Tasks and Goals for a Linked Data A p-proach. In Christian Chiarcos, Sebastian Nordhoff, and Sebastian Hellmann. 2012. Linked Data and 
Linguistics: Representing and Connecting La n-guage Data and Language Metadata. Berlin: Springer: 57 -64.
 Mauricio Espinoza, Elena Montiel -Ponsoda, and
Asun ci X n G X mez -P X rez. 2009. Ontology Localiz a-tion. K -CAP2009 -The Fifth International Confe r-ence on Knowledge Capture, California, Tim B erners -Lee. 2006. Linked Data  X  Design Issues. 
Available online at: http://www.w3.org/DesignIssues/LinkedData.html [Last accessed: June 28, 2013].
 William J. Bug, Giorgio A. Ascoli et al . 2008 . The 
NIFSTD and BIRNLex vocabularies: building comprehensive ontologies for neuroscience. Ne u-roinformatics 6: 175 -194.
 Menzo Windhouwer and Sue Ellen Wright. 2012. Linking to Linguistic Data Categories in ISOCat. 
In Christian Chiarcos, Sebastian Nordhoff, and S e-bastian Hellmann. 2012. Linked Data and Lingui s-tics: Representing and Connecting Language Data and Language Metadata. Berlin: Springer:99 -108.
Bilingual lexicons are essential for translation practice, and fundamental for example-based machine translation (Piao, 2002). They are also valuable resources for comparative language studies. However, the instances of unlexicalised concepts in a target language present particular difficulties to the building of bilingual lexicons. 
One of the most appealing instances involving the Chinese language concerns the concept  X  X den-tity X . This concept is deep ly trenched in western tradition and closely associated with other key notions such as  X  X erson X ,  X  X ndividuality X ,  X  X uman rights X . However, personal identity does not op-erate similarly in Eastern cultures. In a Chinese social context, a person X  X  identity very often de-pends on his/her social network  X  e.g., who his/her family members are, who he/she knows well, which social group he/she belongs to. The concept of personal identity is not readily lexi-calised in Chinese (cf. 2.2). There is therefore no clear counterpart of identity in the Chinese lan-guage. However, the present study makes use of parallel corpora to gather the instances of transla-tion of  X  X dentity X  into Chinese, in an attempt to make suggestions for potential candidates that render the concept into Chinese. 
In this section, we will survey the definitions of identity in dictionaries and in the research on 
WordNet, and review previous research on the problem to translate  X  X dentity X  into Chinese. 2.1 Entry in dictionaries and WordNet 
Different dictionaries define  X  X dentity X  in differ-ent scopes and varied fine-grainedness. We sur-veyed several commonly-used dictionaries  X  
Collins Cobuild Advanced Dictionary of English (seventh Edition) , Macmillan English Dictionary for Advanced Learners , Oxford English Diction-ary , English-Chinese Dictionary  X  X  X  X  X  X  X  (by Lu Gusun), and New Time English-English English-Chinese Dictionary (by Yan Yuanshu). 
Each dictionary denotes two to five senses of the entry  X  X dentity X  (we excluded obsolete, technical, regional usage). The typical senses include (a) the distinctive characteristics of a person or a place, (b) who a person is, and (c) sameness be-tween two entities. The online dictionary.com provides a visual thesaurus that displays four major senses of  X  X dentity X  in the form of four branches  X  personal identity, identicalness, iden-tification, and (mathematical) identity operator (http://www.visualthesaurus. com). Such a visual representation closely resembles the four senses used in WordNet. We examine all the senses of  X  X dentity X  in our research, except the mathematic one, e.g., identity operator. 2.2 Difficulty to perceive the concept 
Previous research has documented that Chinese learners of English tend to find the western con-cept of personal identity unfamiliar to their own practice and feel it is difficult to perceive lan-guage use associated with this concept (Richter and Song, 2005). The problem may stem from the Confucius tradition  X  it attaches importance to collectivism, universal virtues, common good of a community, but disfavours personal endeav-ours for self realisation (Yum, 1988). In the so-cial context of China today, individuals X  power and identity heavily derive from their social net-work, family ties, and social in-groupness. Rich-ter and Song X  X  (2005) study demonstrates that 
Chinese speakers encounter much difficult to translate the key concepts such as identity, self, myself into Chinese. Their informants provided a range of solutions, which are understandably of-ten disagreeable and dissatisfactory. 
We examine the major senses of  X  X dentity X , ex-cluding the mathematic use of the word. We col-lected instances of  X  X dentity X  and their translations into Chinese from parallel corpora. 3.1 Parallel corpora 
There are four resources used for data in the pre-sent study: 
We used all the search results (27 in total) of the first three resources and 73 results from the fourth one to build a sample of 100 pairs of Eng-lish-Chinese sentences for this research. The in-stances of reduplications and conspicuous typos and mistranslations were laid out. 3.2 Data analysis method 
A close examination of the 100 instances of  X  X dentity X  in our sample enabled its major senses to emerge and be classified in main groups. We will then investigate the possible equivalents in 
Chinese for each of the senses. 3.3 Results 
There are four major senses that emerged in our sample: (A) self-identity, (B1) public identity, (B2) an entity X  X  distinctive characteristics, and (C) sameness between two things. We can now examine each of the senses and the most fre-quently used items in Chinese to render the sense. 
A. Self-identity . This sense points to a per-son X  X  defining characteristic. It relates to his/her individuality, personality, and manhood. It does not have to be his/her public image, or titles and labels given by his/her social affiliations and networks. It is something deep inside, at the heart of his/her being. People tend to be aware of their own identity and consider it extremely im-portant and dear to them. Our sample contains nine instances of identity using Sense A. The translations into Chinese do not display any read-ily equivalent item. This echoes the finding of previous research that it is difficulty to express this sense of  X  X dentity X  in Chinese. The transla-tions in our sample tend to use paraphrase or specify most probable contextual meaning of the word (Baker, 2011). Here are some examples (underlines added): (1) a . I am conscious of my own identity. b.  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  (2) a. It X  X  a serious situation if one loses b.  X  X  X  X  X  X  X  X  X  X  X  X  X  (3) a. You can lose your identity when you join b.  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  (4) a. They inevitably diminish the new individ-b.  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X 
The translators used their own interpretations in the context to explicate the meaning expressed by  X  X dentity X   X  e.g.,  X  X  X  cun zai  X  X xistence X  or  X  X eing X  in (1b),  X  X  ge xing  X  X n individual X  X  character or disposition X  in (2b) and (3b), and  X   X  ren ge  X  X anhood X  or  X  X ersonality X  in (4b). This indicates the absence of a cover term in 
Chinese to render Sense A of  X  X dentity X . Our sample also suggests other alternatives to render this sense  X  e.g.,  X  X  X  X  X  geren cunzai  X  X xist-ence of an individual X   X  as well as more adaptive translations such as  X  X  X  X  shiluo gan  X  X ense of loss X  to convey the fee ling of losing of one X  X  identity, and  X  X  X  X  X  yinxing maiming  X  X o hide one X  X  family and given na mes X  for the situation in which one purposely conceals of his identity. strumental function  X  to single out an entity from a population by recognising its unique character-istics. Such characteristics are often physical, measurable, and operationalisable to a large scale of population. The typical collocations include identity card, proof of identity, digital identity, and identity parade. Unlike self-identity, such an identity can be designed, assumed or even stolen: (5) a. A credit card is not a valid proof b.  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  (6) a. The journalist did not want to reveal b.  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  (7) a. How do I report online fraud and identity b.  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  ? (8) a. Managing digital identity is a critical is-b.  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  (9) a. This lets you manage identity ranges man-b.  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  (10)a. but for feeling certain that the man had no b.  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X 
Of the four senses of  X  X dentity X , this sense (B1) is most readily translatable into Chinese. There are 74 instances of Sense B1 in our sample. The most frequently used translations are  X  X  shen fen (n= 32) and  X  X  shen fen (n= 12)  X  both are pronounced identically and often used inter-changeably, although the first term tends to be used for identity card and documents (5), while the second for social status of a person or a legal person (6). A similar translation  X   X  X  X  X  X  shenfen jianbie  X  X uthenticating an identity X  (8)  X  is used twice in our sample, which emphasises more on the act of authenticating or appraising. 
Another translation  X   X  X  X  biao shi  X  X arking X  (9)  X  occurs seven times in our sample and ex-presses more technical meaning of marking out distinctive features. There are also several more adaptive translations in our sample, which ex-press the contextual meaning of  X  X dentity X , e.g.,  X  X  X  ren chu  X  X ecognising X  (10),  X  X  shi shui  X  X s who X , and  X  X   X  X s him X . 
B2. Distinctive characteristics . This sense, somehow related to Sense B1, reveals more dis-tinctive characteristics of an entity. The eight instances of Sense B2 tend to be translated into  X  X  te xing  X  X pecial features X ,  X  X  X  te zheng  X  X pecial characteristics X  (n= 3 for the two terms) and  X  X  X  xing xiang  X (special) image X  (n= 2). (11) a. The countries have kept their own distinct b.  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  (12) a. Language and identity in Caribbean litera-b.  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  (13) a. I want to design the corporate identity. b.  X  X  X  X  X  X  X  X  X  X  X  X 
The terms  X  X  X  (11) and  X  X  (12) appear to be close equivalents in Chinese for Sense B2. 
C. Sameness . This sense of  X  X dentity X  points to sameness and identicalness between two enti-ties through rather objective or factual compari-son. It is not difficult to be translated into 
Chinese with several commonly used terms: (14) a. Identity of interests is the bond that unites b.  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  (15) a. As to the cloning of dictators and celebri-b.  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X   X   X  (16) a. Identity and continuity are not the same. b.  X  X  X  X  X  X  X  X  X  X  X  X  X 
Chinese:  X  X  yi zhi  X  X n agreement X  (14),  X  X  X  xiang tong  X  X ameness X  (15),  X  X  X  deng tong  X  X quality (16). Other alternative translations into 
Chinese include  X  X  X  tong yi  X  X nity X  and  X  ...  X   X  X  shi ... wei yiti  X  X aken as a unity X . the most frequently u sed Chinese translations: sense no frequently used translation A 9  X  X  ,  X  X  X  ,  X  X  X  X  X  ,  X  X  X  B1 74  X  X  ,  X  X  ,  X  X  X  ,  X  X  X  ,  X  X  X  ,  X  X  X  B2 8  X  X  ,  X  X  X  ,  X  X  X  C 9  X  X  X  ,  X  X  X  ,  X  X  ,  X  X  X  ,  X  ...  X  X  X  X 
This research demonstrates the value of parallel corpus for bilingual lexicon building. From par-allel language materials, the main senses of  X  X dentity X  emerged. Our results lend support to distinction between personal identity and public identity, a position advocated by John Locke (1632-1704) (see discussion by Richter and 
Song, 2005, p. 92). Our data showed that the former (Sense A in this study) points to a seman-tic vacuum in Chinese  X  an unlexicalised concept  X  while the latter can be readily translated into 
Chinese. From the perspective of frame seman-tics, when Chinese speakers have difficulty to establish a core (keyword) to a semantic network using Sense A of  X  X dentity X , they would have problem to perceive its associated words and col-locations, e.g., self, myself, individuality (Fill-more, Johnson, &amp; Petruck, 2003). However, based on our evidence from parallel corpora, alt-hough Sense A is not lexicalised in Chinese, par-allel corpora provide a variety of solutions that translators have used to solve the problem (cf. 
Table 1). Our investigation also reveals some constrains of using use parallel corpora. We have observed a few cases of typos, mistranslations, rather questionable adaptions in translation. 
The present study investigates the problem of unlexicalised concept in the target language in relation to the building of bilingual lexicon. Par-allel corpora demonstrate advantages of using real-life language materials to allow the main senses of the word in qu estion to emerge. Our results show the distribution pattern of the main senses of  X  X dentity X  and present the most fre-quently used terms in Chinese to render each sense. Such terms are candidates for English-
Chinese lexicons. Our findings reinforce the pre-vious research that self-identity (Sense A in this study) is unlexicalised in the Chinese language and therefore can hardly be captured by a single cover term. It requires paraphrase or contextual-ised renditions as commonly used translation methods. By contrast, the other three senses of  X  X dentity X  are more readily translatable into Chi-nese. A variety of lexical items serve as close equivalents (cf. Table 1). The potential of using parallel corpus deserves to be explored in future studies in relation to the problem of bilingual corpus construction, machine translation and translator training. 
Baker, Mona. (2011). In Other Words: A Coursebook on Translation (2nd ed.). Abingdon Oxon. New York: Routledge. Petruck, Miriam R. L. (2003). Background to 
FrameNet. International Journal of Lexicography, 16 (3), 235-250. 
Piao, Scott Songlin. (2002). Word Alignment in Eng-lish X  X hinese Parallel Corpora. Literary and Lin-guistic Computing, 17 (2), 207-230. concept of identity. In E. Hung (Ed.), Translation and Cultural Change: Studies in History, Norms and Image-Projection (pp. 91 X 110). Amsterdam: John Benjamins. 
Yum, June Ock. (1988). The impact of Confucianism on interpersonal relationships and communication patterns in East Asia. Communication Mono-graphs, 55 , 374-388. 
Depuis quelques a nn X es d X j X , les institutions j u-ridiques nationales et europ X ennes accordent une importance grandis sante  X  la question de la just i-ce en ligne. En effet, les technologies de l X  information et de communication modernes apparaissent d X sormais comme essentielles afin de promouvoir l X  X ffi cacit X  de la justice et de fac i-liter l X  X cc X s au droit, en particulier dans un contexte multilingue. 
Dans cette opti que, une interface juridique eu r o-p X enne, le portail E -Justice, a  X t X  cr X  X e en 2010 afin de permettre aux citoyens, aux entreprises et aux praticiens du droit d X  X cc X der  X  l X  X nformation juridique (aide j uridictionnelle, formation jud i-ciaire, registres fonciers, etc.) gr X ce  X  la mise  X  disp osition de 22 000 pages consultables dans les 22 langues officielles de l X  X nion europ X enne (UE). N X anmoins , l X  X cc X s aux informations jur i-diques ne constitue au mieux qu X  X ne premi X re  X tape. En effet, avoir acc X s  X  l X  X nformation est une chose, pouvoir mettre diff X rents syst X mes juridiques europ X ens en vis - X  -vis et en compre n-dre les subtilit X s en est une autre. 
Un nombre consid X rable d X  X  X udes portant sur le droit europ X en et transnational compar X  a depuis longtemps mis e n  X vidence les innombrables di f-ficult X s linguistiques et techniques li X es  X  la comparaison des syst X mes juridiques (De Groot, Van Laer 2007 ; Bocquet 2008). 
De m X me que les cultures, les traditions et la langue d X  X n pays ont  X volu X  et diverg X  de celles de leurs voisins, les syst X mes juridiques ont  X g a-lement d X velopp X  leurs propres sp X cificit X s (par exemple, Common Law anglais versus syst X me continental fran X a is). Ces diff X rences intercult u-relles et donc jur idiques sont inh X rentes  X  la f a- X on dont une communaut X  de locuteurs d X  X ne langue donn X  e c on X oit le monde, son enviro n-nement et cr X e des concepts pour appr X hender, cat X goriser et nommer cette r X alit X . 
Mais aussi pr X cie uses soient -elles, ces diff X re n-ces, associ X es  X  la complexit X  des textes de loi et  X  une terminologie et phras X ologie marqu X es, sont aussi  X  l X  X rigine des principales difficult X s rencontr X es par l X  X E pour harmoniser ses d X ma r-ches et proc X dures judiciaires ; des difficult X s li X es notamment  X  l X  X bsence d X  X  X uivalences entre concepts de droit ou  X  la superposition entre concepts europ  X ens et nationaux. Nombre de j u-riliguistes (De Groot 1996 ; Grossfeld 2000 ; 
M a gris &amp; al. 1999) estiment que la complexit X  des concepts juridiques interdit toute possibilit X  de transposition linguistique. Qui plus est, l X  X xercice requiert de la part des tr aducteurs et des l X gislateurs, une connaissance approfondie du droit compar X  et des syst X mes juridiques concern X s, toute erreur traductologique ou term i-nologique pouvant mener  X  une application ou une interpr X tation de la loi erron X e ou probl X m a-tique au se in des pays membres (Sacco 1999). C X  X st pourquoi, en compl X ment du portail E -
Justice, il devient indispensable de proposer un outil de facilitation des m X tiers du droit, sous la forme d X  X ne base de connaissances multilingue et interactive, desti n X e aux praticiens du droit, traducteurs et interpr X tes juridiques qui sont les artisans du multilinguisme. Ces trois cat X gories d X  X tilisateurs ont en commun le besoin d X  X cc X der  X  des informations contextualis X es, associ X   X  une compr X hension conceptuelle e t interculturelle fine des documents juridiques multilingues. La mise  X  disposition de simples lexiques d X  X  X uivalences, de traductions align X es ou d X  X nformations juridiques tronqu X es et non hi  X -rarchis X es ne permet plus de g X rer la complexit X  grandissante d es t extes de loi nationaux, transn a-tionaux et europ X ens .

PLATO a donc pour objectif de d X velopper un syst X me intelligent permettant un acc X s multi -niveaux  X  un vaste corpus multilingue compos X s d X  X rticles, de textes de loi et de la jurisprudence r X cente. P lus sp X cifiquement, l X  X util permettra :  X  Une mise en parall X le intelligente de ces corpus par langue et sous -domaines, via une indexation et annotation s X mantique de chaque document.  X  La possibilit X  d X  X x X cuter des requ X tes, via un syst X me d X  X nterrogation multilingue et orient X  utilisateurs, af in de proposer des donn X es cro i-s X es sur des th X matiques juridiques (par exemple, une mise en para ll X le des jurisprudences de pl u-sieurs pays europ X ens sur un sujet ponctuel).  X  Une terminolog ie et une phras X ologie mul tili n-gue exhaustive afin de faciliter l X  X  X aboration, la traduction et l X  X nt erpr X tation des textes et proc  X -dures juridiques . Outre la n X cessit X  de compre n-dre parfaitement le message du l X gislateur, il est essentiel de restituer un texte authentique, en resp ectant la terminologie, le style arch X typique et les combinaisons lexicales propres au discours juridique (Heylen &amp; al. 2010).  X  Des analyses et commentaires d X  X rdre conce p-tuel et intercultur el visant 1) une meilleure ide n-tification et gestion des difficul t X s notionnelles ; 2) une juste compr X hension, interpr X tation et a p-plication de la loi dans les diff X rents syst X mes juridiques concern X s. Les utilisateurs pourront s X  X ppuyer sur un ensemble d X  X xplicitations jur i-diques, d X  X nalyses en droit compar X  , voire de nouvelles pr o positions d X nominatives.  X  De s ontologies j uridiques par (sous -)domaines, associ X es  X  des repr X sentations graphiques des concepts afin de permettre aux utilisateurs de saisir l X  X rticulation hi X rarchique des concepts juridiques . Les ontologies serviront aussi  X  la construction d X  X n mod X le de donn X es fondame n-tal qui interagira de mani X re permanente avec l X  X nsemble de la structure de l X  X util et permettra la construction, l X  X  X hange, l X  X nterrogation, la m i-se  X  disposition et l X  X nterpr X tation des do nn X es .
Une fois identi fi X e dans le syst X me, chaque ca t  X -gorie d X  X tilisateurs sera en mesure de mener des recherches approfondies dans le corpus gr X ce  X  un assistant de recommandation  X lectronique, l X  X  -advisor. Ce dernier, gr X ce  X  l X  X tilisation de technologi es avanc X es en intelligence artificielle, aidera l X  X sager dans ses recherches, en prenant en compte son profil, ses objectifs et besoins, ainsi que l X  X isto rique des interactions pr X c X de n-tes avec la base de connaissances et les raisons motivant la recherche en cours. L X  X nteraction entre l X  X  -advisor et la base de connaissance sera rendue possible p ar la mise en place d X  X ne inte r-face homme -machine sp X cialement con X ue pour 
PLATO. Le r X le de cette interface est de s X  X dapter aux sp X cificit X s ainsi qu X  X ux besoins r X els de chaque utilisateur et au contexte d X  X sage. 
L X  X nterface aura  X  la fois pour r X le d X  X fficher de mani X re personnalis X e les in formations reche r-ch X es par l X  X sager, tout en co l lectant le plus de donn X es possible s X  X gissant des actions men X es par ce dern ier afin d X  X limenter les algorithmes d X  X ntelligence artificielle. 
Quel que soit le ty pe de requ X te saisie, les utili s a-teurs auront la possibilit X  de creuser les  X l X  ments de r X ponse fournis par le syst X me gr X ce  X  un e n-semble de liens menant vers de nouveau x concepts, de nouvelles analyses interculturelles, des convergen ces notionnelles ou des applic a-tions juridiques. Ils pourront obtenir progress i-vement des donn X es de plus en plus sp X cifiques (par exemple, en partant de textes g X n X raux vers des l X gislations ou jurisprudences locales) ; n a-viguer entre plusieurs niveaux d X  X nformations, tout en ayant la possibilit X  de s X lectionner, filtrer, rassembler ou rejeter des donn X es sp X cifiques. 
La base de connaissance PLATO est con X ue pour  X voluer dan s un univers virt uel de connais sances complexes et multidimensionnelles, en gardant syst X  matiquement une trace du raison nement juridique de chaque utilisateur.
Le corpus est un  X l X ment d X terminant de ce projet dans la mesure o X  les textes constit uent  X  la fois des vecteurs de connaissances sp X cialis X es et une source majeure de repr X sentation conce p-tuel le.
 semi -automatis X e, une annotation s X mantique automatique (Federated knOwledge eXtraction (FOX)), fond  X e sur un syst X me de reconnaissa n-ce d X  X ntit X s (nomm X es) et d X  X dentification des relations s X mantiques (via l X  X tilisation de patrons linguistiques), sera d X ploy X e afin de rep X rer au sein du corpus l es informations d X  X rdre termin o-logique, contextuel, notionn el et interculturel. 
Cette activit X  d X  X ndexation est essentielle afin d X  X  X re en mesure de fournir des r X ponses conte x-tualis X es et ajust  X es aux besoins de chaque cat  X -gorie d X  X tilisateurs. du corpus, l X  X  -advis or pourra exploiter plusieurs techniques d X  X ntell igence artificielle afin de me t-tre en place un assistant de recherche qui aidera  X  la navigation au sein des documents. L X  X  -advisor ne remplit p as une simple fonction de reco m-mandation ; il interagit directe ment avec l X  X tilisateur pou r fournir des r X ponses personn a-lis X es. Il s X  X git do nc de mettre en place un v X rit a-ble dialogue entre l X  X ssistant de recommandation et l X  X tilisateur af in de permettre au premier de  X  comprendre  X  v X ritablement la requ X te du s e-cond et ainsi d X  X  juster ses suggestions. Cela s i-gnifie  X galement que l X  X  -advisor ne peut se contenter de fonder ses recommandations sur un simple syst X me de filtrage collaboratif ou de fi l-trage bas X  sur du contenu. Il doit en l X  X ccurrence int X grer des technolo gies qui soient en mesure de prendre en compte les objectifs et les circonsta n-ces d X  X tilisation du traducteur/interpr X te/praticien  X  un moment sp X cifique. C X  X st pourquoi l X  X util combinera des syst X mes de recommandation tr a-ditionnels aux derni X res technolog ies d X  X ntelligence arti ficielle  X  l X  X nstar des m X cani s-mes d X  X rgument ation et des mod X les d X  X gent c o-gnitif. suscit X  un int X r  X t grandissant ces derni X res a n-n X es. Les technologies de filtrage collaboratif ou bas X  su r du contenu (Su et al. 2009; Pazzani &amp; 
Billsus 2007) sont d X sormais suffisamment m  X -res et largement employ X es dans des sph X res tel -les que l X  X  -commerce. N X anmoins, les syst X mes de recomman dation fond X s sur ces m X thodol o-gies tendent  X  recourir  X  des mod X le s d X  X tilisateur si mplifi X s, en plus d X  X  X re cumul a-tifs . Cette approche cumulative ne prend pas en compte la notion d X   X  action s i tu X e  X , autrement dit le fait que l X  X tilisateur inte ragit avec le syst  X -me dans un contexte particulier. La d X finition et la pris e en compte de ce contexte ont un impact direct sur la nature des recomma n dations. 
Aussi, nous proposons de mettre en place un agent virtuel q ui aura une connaissance appr o-fondie de l X  X tilisateur, de ses pr X f X rences, de sa situation professionnelle, des t  X ches qu X  X l doit mener (dans un sous -domaine particulier). Sur la base de ces informations, l X  X gent pourra s X le c-tionner les sources les plus ad X quates et la fa X on de les ex ploiter afin de fournir les meilleures r e-comma n dations possible. Ces donn X es doivent s X  X ppuyer sur des mod X les cognitifs des compo r-tements utilisateurs afin d X  X ctiver la  X  compr  X -hension  X  de l X  X gent s X  X gissant des besoins des utilisateurs et du contexte d X  X sage. 
Afin de fournir un acc X  s simplifi X , interactif et natu rel aux donn X es juridi ques dans un contexte i n terculturel, nous dotons l X  X ssistant de capacit X s conversationnelles ( Loisel et al. 2012 ) . Les inte r-faces de dialogue en langage naturel offrent de nombreux avant a ges (Androut sopoulos et al. 1995) extr X mement int X ressants dans le cadre de ce projet : (i) les inte r faces de dialogue sont plus intuitives ; (ii) les requ X tes des utilis a teurs sont transform X es en repr X sentation syst X me ; (iii) l X  X  tilisation de di a logues interac tifs permet de corriger les inco m pr X hensions.

Toutefois, concevoir un tel agent est d X  X ne grande complexit X . Le syst X me doit non seul e-ment  X tre en mesure d X  X xtraire les donn X es pour les transformer en langage informatique, mais il doit  X galement pouvoir comprendre les processus en jeu. Les hommes ont naturellement le don de communique r et de raisonner. Notre hypoth X se consiste donc  X  penser que la prise en compte du contexte et des strat X gies discursives des utilis a-teurs permettra d X  X dapter la machine  X  l X  X omme et  X  ses besoins. En effet , le dialogue est un moyen de communication e f fi cace n X cessitant peu ou pas de formation (Allen et al. 2000).
S X  X gissant de l X  interface utilisateur, nous som mes  X galement dans un domaine en perp  X -tuelle  X volution : les syst X mes sont d X velopp X s sur internet, ce qui a pour effet de d X multiplier le nombre e t la vari X t X  des profils utilisateurs et des contextes d X  X sage.  X  cela s X  X joute, le fait que les donn X es juridiques sont aussi structurellement en flux constant. Cela implique par cons X quent de mettre en place une toute nouvelle approche en mati X re de conc eption logicielle et graphique, et un processus facilitant l X  X pprentissa ge profe s-sionnel  X   X  la vol X e  X  (Consiglio et Van der Veer, 2011). La multipli cit X  des contextes et des cult u-res d X  X tilisation incite donc au d X veloppement d X  X nterfaces extr X mement fle xibles et adaptables (Van der Veer, 2011), autorisant des contenus dynamiques et multidimensionnels.
PLATO es t agenc X  autour de deux structu res sp X cifiques : un premier axe orient X  contenu v i-sant l X  X  X aboration et l X  X xploitation d X  X n corpus s X man tiquement annot X   X  des fins d X  X nalyse li n-guistique et textuelle et de constitution d X  X ntologies multilingues. Une seconde struct u-re, orient X e ing X nierie et nouvelles technologies, visant le d X veloppement d X  X n e -advisor (acc X s aux donn X es), d X  X ne interface homme -machine (lien entre donn X es, assistant de recommandation et utilisateur) et bien s X r de l X  X rchitecture globale qui soutiendra l X  ensemble, en structurant les i n-formations d X  X n point de vue informatique. 
L X  X nterd X penda nce de l X  X nsemble de ces activi t X  s et cycles de travail constitue un d X fi de taille, n X cessitant de mettre en place une approche par granularit X  fine, associ X e  X  des  X tapes de contr X le r X currentes. Mais elle refl X te  X galement le cara c-t X re pluridisciplinaire extr X mement fort et inn o-vant de PLATO, en particulier, et des ressources onto -terminologiques du futur, en g X n X ral. C X  X st en effet ce travail c ollaboratif entre des sp X ciali s-tes de la structuration des outils, des sp X cialistes de la structuration des contenus, associ X s  X  une  X quipe de j uristes sp X cialis X s en droit compar X  et dont l X  X xpertise est essentielle pour l X  X nterpr X tation des donn X e s juridiques, qui pe r-mettra la conception et  X  terme l X  X  X aboration d X  X n outil v X ritableme nt adapt X  aux besoins des util i-sateurs. Et, c X  X st dans cette a ction de mise en commun des savoir -faire au cours m X me de la recherche que r X side l X  X nnovation.

Allen, J., Ferguson, G., Miller, B., Ringger, E., Siko r-Androutsopoulos, I., Ritchie, G., and Thanisch, P., 
Bocquet, C, La traduction juridique : fondement et 
Consiglio and Van der Veer. Designing an inte r active learning environment for a worldwide distance adult learning community. In Anke Dittmar &amp; P e-ter Forbrig (Eds) Designing Collaborative Activ i-ties -Proceedings of ECCE, 2011. ACM Digital Library, 225 -228 De Groot, G. Law, Legal Language and the Legal System: Reflections on the Problems of Tran slating 
Legal Texts. In Volkmar -Gessner -Holland -Varga (eds.) European Legal Cultures. Aldershot, Da r-mouth, 19 96, pp. 155 -159.

De Groot, G., Van Laer, C. The dubious quality of legal dictionaries. In Translation and Meaning. University of Maastricht, 2007, pp. 173 -187 Grossfeld, B. Comparative Law as a Comprehensive Approach: A European Tribute to Professor J.A. 
H iller. Richmond Journal of Global Law and Bus i-ness, vol. 1, 2000, pp.1 -33.

Heylen, Kris, Hendrik Kockaert &amp; Frieda Steurs. 2010.  X  X he TermWise Knowledge Platform: an e f-ficient translation and terminology management suite for legal translation in Belgium X . TKE 2010 -
Presenting terminology and know l edge engineering resources online: models and challenges: Fiontar, Dublin City University, 11 -14 August.
 Loisel A. , Dubuisson Duplessis G. , Chai g naud N. , Kotowicz J -P , Pauchet A. : A Convers a tional 
Agent for Information Retrieval based on a Study of H u man Dialogues. ICAART (1) 2012: 312 -317
Magris, M., Musacchio, M.T. La terminografia orie n-tata alla traduzione tra pragmatismo e armonizz a-zione. Terminologie et Traduction, 1999, p. 148 -181.

Pazzani &amp; Billsus. 2007.  X  X ontent -based recomme n-dation systems X . Michael J. Pazzani and Daniel 
Billsus. Lecture Notes in Computer Science (LNCS). Volume 4321, pages 325 -341.
 Sacco, R. Langue et Droit. I n Sacco -Castellani (eds.) : Les multiples langues du droit europ X en uniforme. L X  X armattan Italia, Turin, 1999, p. 163 -179.
Su X. and Taghi M. K. A survey of collaborative fi l-ter ing techniques. Adv. in Artif. Intell. 2009, Art i-cle 4, 19 pages. DOI=10.1155/2 009/421425.

Van der Veer. 2011. Culture Centered Design. In P a-trizia Marti, Alessandro Soro, Luciano Ga m berini and Sebastiano Bagnara (Eds) Facing Co m plexity -Proceedings CHItaly . ACM Digital Library. 7 -8
Van der Veer and Vyas. 2011. Non -formal Tec h-n iques for Requirements Elicitation, Modeling, and 
Early Assessment for Services. In : Anke Dittmar &amp; Peter Forbrig (Eds) Designing Collaborative A c-tivities -Proceedings of ECCE 2011. ACM Digital 
Library, 285 -286.
In the context of projects dealing with Knowl-edge-Driven Information Extraction (KDIE), we investigated in details how terms and natural language expressions are used in knowledge or-ganization systems. A goal of this study was to propose a strategy for using those combined in-formation types (concepts, terms and lexical items) in the automated analysis of semi-structured and unstructured texts for extracting domain relevant facts (Declerck &amp; Buitelaar, 2012). 
This work led naturally to consider issues re-lated to the representation of those combined information types, aiming at a formal representa-tion of the used terms and other natural language expressions in order to improve interoperability of this type of language data in the context of multilingual KDIE systems. As basis for our work, we have been looking at approaches by Reymonet et al. (2009), McCrae et al. (2012) and 
Declerck &amp; Lendvai (2010). As a field study for our on-going work, we dealt with a large multi-
In this short paper, we present briefly the the-saurus, discuss our proposal for the modification of its organization and outline future work. 
The thesaurus for the social sciences is a knowl-edge source under continuous development (we are currently using version 0.92). The list of key-words used in TheSoz contains about 12,000 en-tries, of which more than 8,000 are descriptors (accepted keywords, which act as domain terms). 
It is encoded in RDF and SKOS. While the main conceptual elements of the thesaurus are encoded in the core syntax of SKOS, the re-source makes also use of the SKOS-XL proper-language expressions that are attached to the conceptual elements, using the  X  X refLabel X  and  X  X ltLabel X  annotation properties, allowing thus to describe main terms and their variants. The advantage of using SKOS-XL labels, compared to rdfs:label , consists in the fact that the value of such labels is not a literal, but an object, which can thus be related to other knowledge objects. 
The natural language expressions corresponding to the labels are represented by using the SKOS-XL annotation property  X  X iteralForm X . Perl script the main elements from the RDF and 
SKOS code and present those in a tabular fash-ion, an example of which is given below, dis-playing also the terms in the languages covered by TheSoz (English, French and German): concept id "10034303" term  X 10034303" notation  X 3.2.00" broader notation  X 3.2 X  broader notation  X 3 X  In the example above the reader can see how the 
English preferred label  X  X rop-out X  is associated with the concept  X  X chool and Occupation X , which is itself a subclass of the concept  X  X ccu-pation and Qualification X , classified itself as a field of the broader concept  X  X nterdisciplinary 
Application Areas of Social Sciences X . All the language material contained in the labels or used for naming the  X  X otations X  can be re-used in the context of multilingual KDIE applied to text. 
As the reader can see, in this thesaurus all conceptual, terminological and language data are present in the same Knowledge Organization 
System (KOS). Descriptors are introduced using the RDF language, specifying them as being an owl#Class, being itself an rdfs subClassOf a skos 
Concept. &lt;rdf:Description rdf:about="http://lod.gesis.org/thesoz/ext/Descript or" &gt; &lt;rdfs:label xml:lang="en"&gt;Descriptor&lt;/rdfs:label&gt; &lt;skos:definition xml:lang="en"&gt;Descriptors of the TheSoz&lt;/skos:definition&gt; &lt;rdf:type rdf:resource="http://www.w3.org/2002/07/owl#Class "/&gt; &lt;rdfs:isDefinedBy rdf:resource="http://lod.gesis.org/thesoz/ext/theso z_e xt.rdf"/&gt; &lt;rdfs:subClassOf rdf:resource="http://www.w3.org/2004/02/skos/core#
Concept"/&gt; &lt;/rdf:Description&gt; 
A concrete concept in TheSoz looks like: &lt;rdf:Description rdf:about="concept/10034303"&gt; &lt;skos:inScheme rdf:resource="http://lod.gesis.org/thesoz/"/&gt; &lt;prv:containedBy rdf:resource="http://lod.gesis.org/thesoz/"/&gt; &lt;prv:createdBy rdf:resource="http://lod.gesis.org/thesoz/Creation" /&gt; &lt;/rdf:Description&gt; 
The concepts in TheSoz are encoded by num-bers. We like this approach, consisting in not adding any natural language expression to the identification of knowledge objects outside of the annotation properties like  X  X abel X ,  X  X om-ment X  or  X  X efinition X , although this is practiced in a number of cases, and experiments have been done on considering this type of language data used in RDF identifiers for cross-lingual applica-tions (Fu et al., 2012). Those language independ-ent concept IDs can be associated to a plurality of labels containing natural language expres-sions, being either multilingual variations of a preferred label, or different usages of a term in one language.

TheSoz introduces explicitly the knowledge ob-jet  X  X erm X  at the same level as  X  X oncept X : &lt;skosxl:prefLabel 
Here a term is being encoded as a prefLabel (could also be an altLabel) of a concept. A  X  X erm X  is also represented by a number, which is the value of the property  X  X refLabel X . We find this solution more flexible than the one proposed by (Reymonet et al., 2009) where concepts and terms are both encoded as OWL descriptors, and where meta-classes are needed for describing the relations between domain concepts and terms, leading to an OWL full system. But important is the fact that both approaches are describing terms as full knowledge objects. An added-value of having terms as knowledge objects is the fact that relations between terms (and not only be-tween concepts) can be formally described. the domain modeling (using RDF and OWL, also the rdfs:label property for supporting human consumption of the model) from the termino-logical description (modeled with RDF, SKOS and SKOS-XL) and cross-linking both by the use of a mixture of RDF and SKOS properties. The approach by (Reymonet et al., 2009) in-cludes basic linguistic information in the classes introducing the terms. We would also like to de-part from this and have linguistic information encoded as independent but related knowledge objects. And this is something, which is also not realized in TheSoz. There the content associated to the term objects are just displayed as strings within the annotation property  X  X iteralForm X : &lt;rdf:Description rdf:about="term/10034303" &lt;skosxl:literalForm xml:lang="de"&gt;Abbrecher&lt;/skosxl:literalForm&gt; &lt;/rdf:Description&gt; This line specifies that the knowledge object  X  X erm/10034303 X , which is the prefLabel of  X  X oncept/10034303 X , is linguistically realized with the word  X  X bbrecher X  ( drop-out ). Another  X  X erm X  knowledge object is associated to the concept as its altLabel: &lt;rdf:Description rdf:about="concept/10034303"&gt; &lt;skosxl:altLabel rdf:resource="term/10034307" /&gt; &lt;/rdf:Description&gt; with literalForm: &lt;rdf:Description rdf:about="term/10034307"&gt; &lt;skosxl:literalForm xml:lang="de"&gt;Studienabbrecher&lt;/skosxl:literalForm &gt; &lt;/rdf:Description&gt;  X  X tudienabbrecher X  ( university drop-out ) is thus one of the term variant for the object con-cept/10034303. 
What now about the strings displayed as the values of the property  X  X iteralForm X ? As men-tioned above, Reymonet et al. (2009) include basic linguistic information in the ontological elements used for representing the terms. Here we opt rather for a system that encodes linguistic information as an independent knowledge object, as this is described by the lemon model (McCrae et al., 2012). In this model, the semantics of the language data included in labels of ontologies is described by an explicit reference made to a class or a property of the domain ontology under con-sideration. Differently to (McCrae et al., 2012), we link the linguistic description encoded in lemon not to an ontology element but to a term in a corre-sponding knowledge organization system (real-ized as SKOS conceptScheme). The following (simplified) example shows how our lemon rep-resentation of the linguistic information associ-ated to the decomposed literalForm  X  X niversity drop-out X  refers to the term  X 10034307 X , as its semantic  X  X nchor X : :university_drop-out [lemon:writtenRep "univer-sity drop-out"@en] lemon:sense [lemon:reference lemon:decomposition ( :university_comp :drop-out_comp ) ; lemon:phraseRoot [lemon:constituent :NP ; lemon:edge [lemon:constituent :NP ; lemon:edge [lemon:constituent :NN ; lemon:leaf university_comp ] ; lemon:edge [lemon:constituent :NN ; lemon:leaf drop-out_comp ] ]; ] 
In this, lexical data refer only indirectly to classes or properties of ontologies, mediated by term objects in a terminological concept scheme. 
A good example of how a (multilingual) lexicon encoded as a knowledge object could look like is given by the recent RDF version of Wiktionary very early development phase. In this lexicon, lemon references and other relations to lexicon external knowledge objects are fully supported. 
We are currently working in reducing redun-dancy of information in TheSoz and in the lemon model. We note that in both case tokens in a  X  X it-eralForm X  or a lemon entry can be repeated in the concept scheme or in the ontology lexicon when they are referred by different concepts ( lemon ) or terms (TheSoz). We are adding for this a specific representation for every word used in labels, and replace the components of the la-bels by pointers to the uniquely represented word (or lemma combined with morphological infor-mation), which could be in fact a RDF represen-tation of a Wiktionary entry, situating thus the lexicon in the Linked Open Data framework. In doing so, we have a much more compact repre-sentation of the lexicon that is used in knowledge sources. This work consists in fact of a SKOS and lemon formalization of some ideas formerly described in (Declerck &amp; Lendvai, 2010). monized lexical basis for a wide range of Knowl-edge-Driven natural language processing applications. 
The work presented in this paper has been supported by the TrendMiner project, co-funded by the Euro-pean Commission with Grant No. 287863. 
The author is thanking the reviewers for their very helpful comments, which led to substantial changes brought to the final version of the paper. Buitelaar, P., Cimiano, P. Haase, P., Sintek, M. 20 09. Declerck, T,. Buitelaar, P. 2012. Ontologies as a ings of the SWAIE 2012 Workshop: Semantic Web and Information Extractio.n 
Declerck, T., Gromann, D. 2012. Towards the Gen-eration of Semantically Enriched Multilingual 
Components of Ontology Labels. In: Proceedings 
Declerck, T., Lendvai, P. 2010. Towards a standard-ized linguistic annotation of the textual content o f labels in Knowledge Representation Systems. In: 
Proceedings of the seventh international confer-ence on Language Resources and Evaluation , Va-letta, Malta, ELRA. Ell, B., Vrandecic, D., Simperl, E. 2011. Labels in the Web of Data. In Aroyo, L., Welty, C., Alani, H., 
Taylor, J., Bernstein, A. (eds.): Proceedings of the 10th international conference on the semantic web -Volume Part I (ISWC'11), Vol. Part I. Springer-Verlag, Berlin, Heidelberg, pp.162_176. 
Fu, B., Brennan, R., O'Sullivan, D. 2012. A Configu r-able Translation-Based Cross-Lingual Ontology Mapping System to Adjust Mapping Outcomes. Journal of Web Semantics, Vol. 15, pp.15_36. 
Gromann, D., Declerck, T. 2013. Cross-Lingual Cor-recting and Completive Patterns for Multilingual 
Ontology Labels. In Buitelaar, P. and Cimiano, P. (eds) Multilingual Semantic Web , Springer-Verlag (to appear) McCrae, J., Aguado-de-Cea, G., Buitelaar, P., 
Cimiano, P., Declerck, T., G X mez-P X rez, A., Gra-cia, J., Hollink, L., Montiel-Ponsoda, E., Spohr, D ., 
Wunner, T. 2012. Interchanging lexical resources on the SemanticWeb. Journal of Language Re-sources and Evaluation, pp.1_19. Reymonet, A., Thomas, J., Aussenac-Gilles, N. 2009.
Ontology based information retrieval: an applica-tion to automotive diagnosis. In Proceedings of In-ternational Workshop on Principles of Diagnosis.
Silberztein, Max. 2003. NooJ manual . Available at the WEB site http://www.nooj4nlp.net (200 pages) 
Wimalasuriya, D. C., Dou, D. 2012. Ontology-based information extraction: an introduction and a sur-vey of current approaches. Journal of Information Science, Vol. 36, No. 3, pp.306-323. Zapilko, B., Johann Schaible, Philipp Mayr, Brigitt e 
Mathiak. 2012. TheSoz. A SKOS Representation of the Thesaurus for the Social Sciences . Semantic-
Web Journal. 
Les ressources s  X  emantiques sont au c X ur de la recherche d X  X nformation (Vallet et al., 2005). Elles peuvent prendre plusieurs formes qui vont de la liste de termes ` a une ontologie formelle. Dans le cadre du projet L  X  egiLocal qui vise ` a proposer des fonctionnalit  X  es d X  X cc ` es ` a l X  X nformation juridique locale pour les citoyens, les  X  elus et personnels de mairie, la ressource s  X  emantique est une ressource termino-ontologique (RTO) constitu  X  ee de termi-nologies reli  X  ees ` a une ontologie modulaire. Les terminologies doivent servir ` a annoter des docu-ments pour pouvoir r  X  epondre ` a des questions rel-atives ` a l X  X ctivit  X  e juridique d X  X ne commune. Dans le cadre du projet de recherche, cette activit  X  e ju-ridique a  X  et  X  e restreinte aux probl ` emes communaux induits par l X  X ctivit  X  e de randonn  X  ee p  X  edestre. Ainsi un secr  X  etaire de mairie peut rechercher des doc-uments lui permettant de r  X  epondre ` a la question suivante : un quad peut-il circuler sur le GR traversant la commune ? .

Dans ce papier nous relatons l X  X xp  X  erience pluridisciplinaire d X  X ne construction collaborative de la RTO et des enseignements que l X  X n peut en tirer concernant le travail de terminologie ju-ridique, la pluridisciplinarit  X  e et l X  X mpact de l X  X p-plication sur la ressource cr  X  e  X  ee. La section 2 pr  X  esente un bref  X  etat de l X  X rt sur la construction de RTO, puis la section 3 explicite notre travail.
La section 4 met en lumi ` ere les aspects li  X  es ` a la terminologie juridique ainsi que le probl ` eme de lien tr ` es fort entre terminologie et application et pr  X  esente l X  X xp  X  erience men  X  ee.
L X   X  etat de l X  X rt est riche autour des ques-tions de construction d X  X ne RTO. Dans (Charlet et al., 2012), les auteurs pr  X  esentent une exp  X  erience de construction d X  X ne RTO m  X  edicale (
ONTOLURGENCES ) pour la recherche d X  X nforma-tion. Ils s X  X nt  X  eressent notamment aux questions de r  X  eutilisation et d X  X nt  X  egration de ressources exis-tantes. Pour le domaine du droit des collectivit  X  es territoriales, et pour le droit en g  X  en  X  eral, tr ` es peu de ressources existent en dehors de celles de haut niveau telle que LKIF-Core (Hoekstra et al., 2007) qui contient 3 modules consacr  X  es au droit qui constituent une ontologie g  X  en  X  erique du droit.
Dans (Aussenac-Gilles et al., 2002), les auteurs montrent que la nature de l X  X pplication vis  X  ee con-ditionne les diff  X  erentes  X  etapes de construction de produits terminologiques.

Nous nous sommes int  X  eress  X  es sp  X  ecifiquement au processus de validation des termes extraits au-tomatiquement. Cet objectif nous diff  X  erencie des travaux qui s X  X nt  X  eressent ` a la validation de ter-mes pour  X  evaluer la qualit  X  e des outils d X  X cquisition (Zargayouna and Nazarenko, 2010) qui supposent l X  X xistence d X  X ne r  X  ef  X  erence.

Les travaux qui mettent en place un cadre col-laboratif sont de plus en plus nombreux (Sior-paes et al., 2008). Dans un premier temps, nous ne nous posons pas la question de sp  X  ecifications de fonctionnalit  X  es d X  X n outil comme dans (He et al., 2009) mais plut  X  ot des recommandations m  X  ethodologiques qui permettent d X  X xpliciter et de tracer les choix effectu  X  es.
La ressource terminologique ` a construire doit en plusieurs sous-domaines. Pour identifier ces sous-domaines donnant lieu ` a des modules, nous sommes parties de la t  X  ache qui est d X  X ider les secr  X  etaires de mairie (ou DGS 2 ) ` a r  X  ediger des actes conformes ` a la l  X  egalit  X  e et de rendre visible au citoyen une grande partie des actes publi  X  es par les dans (Ressad-Bouidghaghen et al., 2013).

Pour la construction de ces modules, des cor-tains modules sont construits en r  X  e-utilisant des ressources existantes telles que l X  X ntologie de l X  X n-see 3 ou l X  X nnuaire de la DILA 4 .

La construction de la RTO par modules per-met une construction incr  X  ementale par ajout de modules. Dans un premier temps nous avons s  X  electionn  X  e un sous-domaine relatif ` a la randonn  X  ee p  X  edestre qui doit contenir les principales no-tions propres ` a la pratique de la randonn  X  ee du point de vue des collectivit  X  es territoriales. Nous d  X  etaillons dans la suite la constitution du volet ter-minologique du module Randonn  X  ee.

Le corpus est constitu  X  e d X  X n livre (Louarn, 2010). Ce corpus contient moins de 50 000 mots.
Le contenu du livre est un guide pr  X  esentant la re-sponsabilit  X  e des organisateurs de randonn  X  ees, le droit des propri  X  etaires des terrains travers  X  es, des  X  elus et des associations dans la pratique de la ran-donn  X  ee p  X  edestre. Le discours est moins juridique que ne le sont les actes juridiques qui y sont l X  X d  X  equation de son discours ` a la langue juridique utilis  X  ee par les secr  X  etaires de mairie. La taille et la nature du corpus ne permettent pas de d  X  etecter des r  X  egularit  X  es ou de faire des calculs statistiques.
Le corpus a  X  et  X  e trait  X  e par l X  X util d X  X xtraction ter-minologique de notre partenaire (soci  X  et  X  e Temis 5 ). forme TERMINAE (Szulman, 2011) pour visualiser les r  X  esultats de l X  X xtraction et permettre la consti-tution de la terminologie.

Le travail de validation a  X  et  X  e d  X  ecoup  X  e en trois phases : (i) une phase d X  X xplicitation des choix de validation sur la base d X  X n ensemble restreint de candidats termes, (ii) une phase de validation par les terminologues et (iii) une phase de double val-idation avec l X  X xpert juridique.

La premi ` ere phase de validation a impliqu  X  e l X  X xpert juridique et deux terminologues. Elle a port  X  e sur 120 termes candidats choisis en fonction de leur distribution dans le corpus. Cet  X  echantillon a permis de constater qu X  X n filtrage automatique ne peut pas  X  etre effectu  X  e sur cette base. En effet, 44% (resp. 35%) de termes jug  X  es pertinents (resp. non pertinents) par l X  X xtracteur ont  X  et  X  e valid  X  es.
Le but de cette premi ` ere phase est d X  X rriver ` a un accord entre les personnes impliqu  X  ees dans la t  X  ache de validation. Les 120 termes ont  X  et  X  e jug  X  es par chaque intervenant (terminologue, expert ju-ridique). Un guide de validation a  X  et  X  e mis en place et enrichi au fur et ` a mesure des discussions et des choix. Ce guide a permis d X  X xpliciter les choix et de garder trace des d  X  ecisions via des commen-taires normalis  X  es : (i) pour exprimer le doute sur certains termes qui n  X  ecessite des discussions, (ii) argumenter les choix, (iii) proposer de nouveaux termes issus du terme ` a valider.

La liste des termes candidats a  X  et  X  e ainsi divis  X  ee en plusieurs listes :  X  la liste des termes valid  X  es : A chaque terme valide correspond une fiche terminologique d  X  ecrite ci-apr ` es.  X  la liste des termes invalid  X  es : soit des ter- X  la liste noire des candidats termes : des ter-
La fiche terminologique pr  X  esente plusieurs rubriques d  X  ecrivant le terme, comme l X  X nsemble des variantes lexicales. Pour chaque variante lex-icale, un qualificatif d  X  ecrit par la rubrique USE doit  X  etre d  X  efini. Ce qualificatif a trois entr  X  ees ( langue juridique utilise beaucoup d X  X cronymes qui doivent  X  etre pr  X  esents dans la fiche. Ainsi le terme recours pour exc ` es de pouvoir est tr ` es souvent utilis  X  e sous son acronyme REP . La forme inter-dite est pr  X  esente dans la langue juridique. En ef-fet certains termes comme conclusions n X  X nt de sens en droit que s X  X ls sont utilis  X  es au pluriel. Or tout extracteur de terme produit des candidats ter-mes lemmatis  X  es au singulier. Il faut indiquer que le terme conclusion au singulier ne doit pas  X  etre utilis  X  e, cette information est d X  X utant plus utile pour l X  X nnotation o ` u on a g  X  en  X  eralement recours ` a la lemmatisation.

Chaque fiche est d  X  esign  X  ee par un terme et con-tient entre autres les diff  X  erentes variantes lexi-cales. Les fiches proviennent soit des termes can-didats valid  X  es, soit de termes candidats jug  X  es in-valides mais dont une partie est consid  X  er  X  ee valide comme ordre public dans le candidat terme c X ur de l X  X rdre public . Certains termes not  X  es  X  X ermes ? X  sont des termes  X  etudi  X  es pour lesquels la cat  X  egorie n X  X  pas  X  et  X  e  X  etablie et qui ont fait l X  X bjet de discus-sions.
La deuxi ` eme phase de validation a permis de cr  X  eer 998 fiches terminologiques par deux termi-nologues n X  X yant pas de connaissance particuli ` ere l X  X xpert juriste. Ils ont servi ` a  X  eclaircir des no-tions et ` a mettre ` a jour le guide de validation. La deuxi ` eme  X  etape a dur  X  e 60 heures approximative-ment. La premi ` ere phase avait dur  X  e 10 heures ap-proximativement. De d  X  ecoupage a donc permis un gain de temps consid  X  erable en permettant de par-all  X  eliser le processus de validation. Pour conclure sur l X  X xp  X  erience men  X  ee, l X  X nteraction entre expert m  X  etier et terminologues est imp  X  erative pour con-struire une RTO r  X  epondant aux objectifs qui lui sont assign  X  es. L X  X xpert ignore, souvent, ses im-plicites et donc le choix des terminologues lui permet d X  X n prendre conscience et de les g  X  erer.
De m  X  eme, son mod ` ele de raisonnement, non con-scient souvent, oriente ses choix et sa validation.
Nous avons mis en pratique des notions th  X  eoriques connues mais essentielles lors d X  X ne approche pragmatique. Ainsi des choix doivent  X  etre effectu  X  es et sont guid  X  ees par l X  X pplication.
La ressource que nous construisons est utilis  X  ee pour un acc ` es s  X  emantique ` a l X  X nformation. Deux types d X  X cc ` es ont  X  et  X  e d  X  efinis : une recherche d X  X n-formation par mots cl  X  es et une navigation via la structure taxonomique de l X  X ntologie. Ces deux types d X  X cc ` es n  X  ecessitent des choix diff  X  erents : (i) l X  X n tr ` es li  X  e ` a l X  X nnotation des documents et ` a l X -exploitation de ces annotations par le moteur de recherche d X  X nformation, l X  X utre tr ` es li  X  e ` a l X  X ntolo-gie et donc aux choix de mod  X  elisation. Deux ex-emples pour illustrer cet impact :  X  terme simple versus terme compos  X  e : Les ter-mes simples comme randonn  X  ee sont utiles pour la structuration mais s X  X ls sont utilis  X  es pour l X  X nnotation, augmenteront le bruit. les termes compos  X  es comme droit de la ran-donn  X  ee , randonn  X  ee p  X  edestre permettent une recherche plus pr  X  ecise. (Omrane et al., 2011) peuvent servir ` a 105 Les deux types d X  X cc ` es possibles nous ont con-traintes ` a chaque fois d X  X xpliciter le choix fait. La liste des termes produite contient ` a la fois les ter-mes qui serviront ` a l X  X nnotation et ceux qui sont utiles ` a la mod  X  elisation.
Cet article d  X  ecrit un retour d X  X xp  X  erience de con-struction de la partie terminologique d X  X ne RTO par plusieurs intervenants de profils divers. Nous avons exp  X  eriment  X  e un processus de construction collaboratif et incr  X  emental. L X  X bjectif de la col-laboration est double puisqu X  X l s X  X git ` a la fois de s X  X ssurer d X  X ne repr  X  esentation consensuelle et de pouvoir parall  X  eliser certaines t  X  aches. Nous avons mis en place un guide qui explicite les choix ter-minologiques effectu  X  es.

Sur le plan, scientifique, la mise en lumi ` ere de la finalit  X  e du produit terminologique pour les choix de mod  X  elisation montre la justesse du paradigme pr  X  on  X  e par le groupe TIA.

L X  X bjectif de la constitution de la RTO est de permettre un acc ` es s  X  emantique ` a l X  X nformation juridique. Pour atteindre cet objectif il faudrait garantir une bonne couverture de la ressource ce qui pose la question r  X  ecurrente de passage ` a l X   X  echelle. La mise en place d X  X ne construction modulaire et incr  X  ementale garantit, ` a notre sens, de produire des ressources de qualit  X  e et volu-mineuses. La phase de validation par des non ex-perts permet de faciliter le travail de l X  X xpert et de permettre de valider assez rapidement des listes volumineuses de candidats termes.

Ce travail ` a  X  et  X  e partiellement financ  X  e par le pro-jet L  X  egiLocal(FUI 9).

Developing ontologies manually is a complex and time consuming process, which involves both ontology engineers and domain experts. 
Natural language (NL) t echniques have been traditionally used for extracting knowledge from texts to build semantic resources. In fact knowledge acquisition from text plays an i m-portant role in Ontology Engineering. It is divi d-ed into several steps, according to the  X  X ntology le arning layer -cake X  (Cimiano, 2006): (a) ident i-fying and extracting terms, (b) eliciting concepts and relations linking concepts from these terms, (c) organizing concepts and relations into hiera r-chies, and (d) identifying axioms. 
During the ontology building, a wide range of difficulties and handicaps can appear. These situations may have as consequence the inclusion of anomalies in the ontology. Thus, the ontology evaluation process plays a key role in ontology engineering develop ments. Currently, the general trends in ontology evaluation involve different approaches (e.g., the comparison of the ontology to a  X  X old standard X  or the detection of common errors in the ontology). However, what seems to be less present in the ontology e valuation field is the intensive use of NL techniques. For example, some structural or naming errors in the ontology may be automatically pointed out with a lingui s-tic analysis of concept labels. Thus, our intention in this paper is to aim at the use of NL tec h-niques during the ontology evaluation process. In particular, we propose a first attempt of impro v-ing the pitfall detection methods implemented within OOPS! by means of NL techniques.

The remainder of this paper is structured as follows: Section 2 summarizes different NL techniques used in Ontology Engineering. Se c-tion 3 presents the relation between ontology evaluation and NL -based tec hniques. Section 4 briefly describes OOPS!. In Section 5 our pr o-posal towards a language -based enhancement of the pitfall detection process within OOPS! is presented. Finally, Section 6 outlines some co n-clusions and future steps. 
NL techniques traditionally help on the (semi) -automatic building of ontologies and on the po p-ulation of ontologies with instances.
 from text, known as ontology learning me thods, usually implement lexico -syntactic patterns (Hearts, 1992; Montiel -Ponsoda and Aguado de 
Cea, 2010), clustering methods or machine lear n-ing algorithms (essentially unsupervised) (Poelmans et al., 2010), to exploit various li n-guistic clues. Some plat forms exist and impl e-ment one or a combination of these methods using different NLP tools (term or relation e x-tractors, parsers, etc.). Examples are Text2Onto (Cimiano and V X lker, 2005), which discovers concepts and hyperonimic relations between concepts, thanks to lexico -syntactic patterns and associative rules automatically learned from examples and OntoLearn (Velardi et al., 2005), which uses Wordnet (Fellbaum, 1998) for ident i-fying lexical relations .
 like T EXCOMON (Zouaq and Nkambou, 2008) uses linguistic patterns for instance identific a-tion, using named entity recognition techniques. to ontology matching where Euzenat and Shva i-ko (2007) distinguish between language -based methods and methods which are based on li n-guistic resources, whereas the more general class of terminological approaches also includes string -based methods. We can mention the work by Ritze et. al (2010) that show s how complex matching can benefit from NL techniques. 
Ontology evaluation process, which checks the technical quality of an ontology against a frame of reference (Su X rez -Figueroa, 2010), plays a key ro le in ontology engineering projects.
 ation process, there are different approaches (Sabou and Fernandez, 2012; Poveda -Villal X n et al., 2012): (a) comparison of the ontology to a  X  X old standard X , (b) detection of common errors from catalogues in the ontology, (c) use of d i-mensions and criteria for describing the quality and goodness of the ontology, (d) use of the o n-tology in an application and evaluation of the results, (e) comparison of the ontology with a source of data about the domain to be covered, and (f) evaluation by experts who check the o n-tology against the requirements.

In addition, ontology evaluation can be su p-ported by NL techniques in several ways (Ga n-gemi et. al, 2005): mation retrieval or text mining applications and thus concerns objects mentioned in texts.
NLP can be used to identify mentions of instan c-es (i.e. occurrences in text) of classes and rel a-tions which a re mentioned in the text. A corpus -based evaluation of the ontology can reveal i m-portant properties of the ontology that might not be discovered otherwise. ontology is performed, NLP can help in the ide n-tification of new senses of already known i n-stances, for example because the instance is polysemous and/or ambiguous (e.g.,  X  X ashin g-ton X  is a person and a location).

However, ontology evaluation approaches could take more advantage of NL techniques. In this sense, we propose here a first attempt t o-wards a NL -based upgrade of OOPS!. 
OOPS! 1 (Poveda -Villal X n et al., 2012) is a web -based tool, independent of any ontology deve l-opment environment, for detecting potential pi t-falls that could lead to modelling errors. 
Currently, OOPS! provides mechanisms to a u-tomatically detect as many pitfalls as possible, thus it helps developers in the diagnosis activity, which is part of the ontology validation process.
OOPS! takes as input an ontology to be eval u-ated and a pitfall catalogue in order to produce a list of evaluation results. The current version of the catalogue 2 consists on 35 pitfalls. Some e x-amples are c reating synonyms as classes, defi n-ing wrong inverse relationships, miss ing annotations, missing domain or range in prope r-ties, or defining wrong equivalent classes. Up to now, OOPS! detects semi -automatically a subset of 21 pitfalls related to the following dimensions: human understanding, logical consistency, mo d-elling issue s, ontology language specification and real world representation.
In this section we propose a first attempt towards a language -based enhancement of the pitfall d e-tection process within the ontology evaluation tool OOPS!. To do this, we have reviewed the current catalogue of pitfalls in order to determine (a) which pitfalls, already implemented, could be detected in a better way by means of applying linguistic techniques and (b) which one s, not detected yet by OOPS!, could be implemented based on linguistic aspects.
 already detected by OOPS!, we can mention the following ones: classes whose identifie rs are synonyms are crea t-ed and defined as equivalent. Its detection could be improved by using linguistic resources such as WordNet and EuroWordNet, particularly by looking for the synonymy information of the class name. of using ''rdfs:subClassOf'', ''rdf:type'' or ''owl:sameAs' ': the  X  X s X  relationship is created in the ontology instead of using OWL primitives for representing the subclass relationship ( X  X u b-classOf X ), the membership to a class ( X  X nstanc e-
Of X ), or the equality between instances ( X  X ameAs X ). The detection could be enriched by creating specific language -dependent lexico -syntactic patterns to discover the use of  X  X s X  and by using named entity recognition tools for cha r-acterizing the  X  X nstanceOf X  relatio n. two relationships are defined as inverse relations when they are not necessarily. As first attempt, the implementation of this pitfall could be i m-proved by creating specific lexico -syntactic pa t-terns for direct/ inverse relationship name structure. class : a class is created whose identifier is refe r-ring to two or more different concepts (e.g.,  X  X tyleAndPeriod X , or  X  X roductOrService X ). As first attempt, its detection coul d be enhanced by creating specific language -dependent lexico -syntactic patterns and regular expressions to discover the use of  X  X nd X  or  X  X r X  in the concept name. an ontology is imported into another, developers nor mally miss the definition of equivalent pro p-erties in those cases of duplicated relations and attributes (e.g.,  X  X asMember X  and  X  X as -Member X  in two different ontologies). The detection could be enriched by (a) using linguistic resources such as WordNet and EuroWordNet, specifically by looking for the synonymy information of the property name and (b) creating specific la n-guage -dependent lexico -syntactic patterns. fall appears when a relationship (except for the sym metric ones) has not an inverse relationship defined within the ontology. As first attempt, its implementation could be improved by creating specific lexico -syntactic patterns for d i-rect/inverse relationship name structure (e.g., isSoldIn -sells; hasAuthor -isAuthorOf; hasParent -isParentOf). in a hierarchy a class that contains the instances that do not belong to the sibling classes instead of classifying such instances as instances of the class in the upper level o f the hierarchy. This class is normally named  X  X ther X  or  X  X iscellan e-ous X . As first attempt, its detection could be i m-proved by creating a set of lexico -syntactic patterns that represent different ways of naming concepts that are usually miscellaneous entities . 
With respect to those pitfalls not detected yet by OOPS!, we can propose the following ideas for their implementation based on NL aspects : tology element whose name has different mea n-ings is included in the ontology to represent more than one conceptual idea. As first a p-proach, its detection could be implemented by (a) using linguistic resources such as WordNet and 
EuroWordNet, specifically by analyzing the di f-ferent synsets in which t he element name appears and (b) by analysing labels of neighbourhood concepts for disambiguation. that is required and/or useful is not included in the ontology. As first approach and in certain situations, this pitfall could be implemented by using linguistic resources such as WordNet and 
EuroWordNet, specifically by analyzing the a n-tonym information of the relationships name. ontology is imported into ano ther, classes with the same conceptual meaning that are duplicated in both ontologies should be defined as equiv a-lent classes. As first step, this pitfall could be detected by using linguistic resources such as 
WordNet and EuroWordnet, specifically by loo k-ing for the synonymy information of the class name. two classes are defined as equivalent when they are not necessarily. As first step, this pitfall could be implemented by using linguistic r e-sources such as WordNet and EuroWordNet, specifically by looking for the hyperonym i n-formation of the class name. 
In this paper, we have presented the first efforts towards a NL -based enhancement of the pitfall detection process within the ontology ev aluation tool OOPS!. We have reviewed the 35 pitfalls in the OOPS! catalogue and analyzed which pitfall detections could be linguistically improved and which pitfalls could be implemented based on 
NL as first attempt. In summary, we have pr o-posed the impro vement of 7 pitfall detection processes and the automation of 4 pitfalls not detected yet by OOPS!. Thus, we have planned to enhance OOPS! with the NL techniques pr e-sented in this paper.
This work has been supported by the Spanish project B abelData ( TIN2010 -17550 ).

Philipp Cimiano. 2006. Ontology Learning and Pop u-Philipp Cimiano and Johanna V X lker. 2005. J X r X me Euzenat and Pavel Shvaiko. 2007. Ontology 
Christiane Fellbaum (ed.). 1998. Wordnet. An Ele c-
Aldo Gangemi , Carola Catenacci, Massimiliano Cia r-amita, and Jens Lehmann. 2005. Ontology evalu a-tion and validation: An integrated formal model for the quality diagnostic task. Technical report, L a-boratory of Applied Ontologies. CNR, Rome, Italy.
Marti A. Hearst. 1992. Automatic acquisition of h y-ponyms from large text corpora. In Procs. of the 14th International Conference on Computational Linguistics COLING1992, p. 539  X  545.
 Elena Montiel -Ponsoda and Guad a lupe Aguado de 
Cea. 2010. Using natural language patterns for the development of ontologies. In V. Bhatia, P. 
S X nchez -Hernandez &amp; P. P X rez Paredes, Eds., R e-searching specialized languages, p. 211  X  230. John Benjamins Pub.

Jonas Poelmans, Paul Elzinga, Stijn Viaene, and Gu i-do Dedene. 2010. Formal concept analysis in know ledge discovery: a survey. In Proceedings of the 18th international conference on Conceptual structures: from information to intelligence, 
ICCS X 10, p. 139  X  153, Berlin, Heidelberg: Sprin g-er -Verlag.
 Mar X a Poveda -Villal X n, Mari Carmen Su X rez -
Figueroa, and Asu nci X n G X mez -P X rez. 2012. Val i-dating ontologies with OOPS!, in Knowledge E n-gineering and Knowledge Management, Vol. 7603, Lecture Notes in Computer Science (Springer -Verlag, Berlin, 2012), pp. 267  X  281.
 Dominique Ritze, Johanna V X lker, Christian 
Meilicke, an d Ond  X  ej  X  v X b -Zamazal . 2010. Li n-guistic analysis for complex ontology matching. Proceedings of the 5th International Workshop on Ontology Matching (OM -2010), Shanghai, China.
Marta Sabou and Miriam Fernandez. 2012. Ontology (Network) Evaluation. Ontology E ngineering in a Networked World. Su X rez -Figueroa, M.C., G X mez -
P X rez, A., Motta, E., Gangemi, A. (Editors). Pp. 193 -212, Springer. ISBN 978 -3 -642 -24793 -4. Mari Carmen Su X rez -Figueroa. 2010. PhD Thesis: 
NeOn Methodology for Building Ontology Ne t-works: Specif ication, Scheduling and Reuse. Spain. Universidad Polit X cnica de Madrid.

Paola Velardi, Paolo Fabriani and Michele Lissikof. 2005. Using text processing techniques to automa t-ically enrich a domain ontology. In Proceedings of 
ACM -FOIS., Maine, publication of ACM, 270  X  284.
 Amal Zouaq and Roger Nkambou. 2008. Building Domain Ontologies from Text for Educational 
Purpos -es. IEEE Transactions on Learning Tec h-nologies 1(1), pp. 49  X  62. Whatever the domain, specialized texts are char-acterized by terms and relations between terms. Identifying these relations is crucial in many ap-plications in Natural Language Processing (NLP), such as information retrieval, question-answering systems, information extraction in search engines or specialized automatic translation. For instance, a semantic relation that links the terms sucre (sugar) and saccharose will allow to increase re-call in a retrieval information system.

Those relations may be provided by terminolo-gies, but usually those resources are not tuned to the targeted texts (Bourigault and Slodzian, 1999). Relations may also be automatically ac-quired from specialized corpora, through differ-ent strategies. We can take into account morpho-logical (Grabar and Zweigenbaum, 2000), syn-tactic (Jacquemin, 1997) or semantic informa-tion (Jacquemin, 1999), define lexico-syntactic patterns through observation in corpora (Hearst, 1992; Morin, 1999; Auger and Barriere, 2008), use machine learning techniques (Snow et al., 2005) or distributional analysis (Habert and
Zweigenbaum, 2002), etc. All the methods show various limits. Regarding the quality of the results, they can either get a low recall (methods are too restrictive) or a low precision (ambiguities or pol-ysemy are not well identified). Furthemore, ap-proaches usually aim at acquiring only one rela-tion type (for eg., hyperonymy).

Let X  X  take the example of two methods :  X  Lexico-syntactic patterns allow to get a good  X  On the contrary, distributional analysis (DA)
Furthemore, methods based on DA are generally used with big amounts of data and tend to be less efficient with low frequency words (Caraballo, 1999). Results obtained with general language are promising, but improvement is still required with specialized texts, even if good results have already been achieved (Habert and Zweigenbaum, 2002).
As mentionned above, one of DA X  X  limit is low frequency words. Indeed, for those words, sim-ilarity is computed from very little information (i.e. the one in contexts), that leads to gener-ate poorer quality groupings of terms (Caraballo, 1999). We assume that this information could be increased with semantic information as the one contained in an existing resource or acquired by a relation acquisition method, as for example, us-ing hyperonymy relations acquired with patterns. Following this idea, we intend to define a hybrid method that switches words in DA contexts for their hierarchical parent or morphosyntactic vari-ant. This method normalizes contexts (Henneron et al., 2005), to increase their frequency.

We first present the related work, then our hy-brid method and we finally describe the different experiments we led. The results we get are then evaluated in terms of precision. This work uses a DA method, based on the har-rissian hypothesis that states that words appearing in a similar context tend to be semantically close (Harris, 1954). The DA principle has been auto-mated in the 90 X  X , and concepts and procedures used in distributional computations have been well defined (Sahlgren, 2006; Turney and Pantel, 2010; Baroni and Lenci, 2010). However, this area of research still represents some current issues con-cerning the building, the evaluation and the use of distributional resources 1 . We focus here on the building of distributional resources.

In that respect, during the past few years, re-search has shifted from using DA methods for modelling the semantics of words to tuning them for the semantics of larger units such as phrases or entire sentences (Hermann et al., 2013). Most ap-proaches tackle the problem through vector com-position. Mitchell and Lapata (2008) use lin-ear algebraic vector operations, testing both ad-ditive and multiplicative models, and a combina-tion of these models. Grefenstette and Sadrzadeh (2011) apply unsupervised learning of matrices for relational words to their arguments, in order to compute the meaning of intransitive and tran-sitive sentences. Baroni and Zamparelli (2010) use matrices to model meaning, but only for adjective-noun phrases, whereas Grefenstette and
Sadrzadeh (2011) X  X  work also applies to sentences containing combinations of adjectives, nouns, verbes and adverbs. Recently, the framework pro-posed by Grefenstette et al. (2013) combines both approaches.

An important issue in DA improvement focuses on distributional contexts, and more precisely on weighting contexts. Broda et al. (2009) consider that what matters is not the feature X  X  exact fre-quency. They do not use these frequencies as sim-ple weights but rank contexts and take into ac-count this rank in DA. Influence on contexts may also be done by embedding additional semantic in-formation. With a method based on bootstrapping,
Zhitomirsky-Geffet and Dagan (2009) modify the weights of the elements in contexts relying on the semantic neighbours found with a distributional similarity measure. Based on this work, Ferret (2013) faces the problem of low frequency words by using a set of positive and negative examples selected in an unsupervised way from an original distributional thesaurus to train a supervised clas-sifier. This classifier is then applied for reranking the semantic neighbours of the thesaurus selec-tion. With the same purpose of solving the prob-lem of data sparseness, other methods are based on dimensionality reduction, as Latent Semantic
Analysis (LSA) in (Pad X  and Lapata, 2007), or on a bayesian approach of DA (Kazama et al., 2010).
Above work exploits great collection of texts of general language. However, few works are also interested in applying DA to specialized do-mains where text collections are generally smaller and frequencies lower (Habert and Zweigenbaum, 2002; Embarek and Ferret, 2008). As presented previously, Ferret (2013) attempts to exploit ma-chine learning approaches to face the problem of low frequency of words and contexts. In our work, we propose an approach that exploits relations ac-quired with linguistic approaches in order to nor-malize contexts and increase their frequency. As we work with specialized texts, our approach dif-114 fers in considering nouns, adjectives and both sim-ple and complex terms. The contexts in which occurs a target word have associated frequencies which may be used to form probability estimates. The goal of our hybrid method is to influence those distributional context frequencies by normalizing contexts. Indeed, nor-malization tends to decrease diversity in contexts in order to increase contexts X  frequency. Our hy-brid method follows the scheme presented in fig-ure 1.
 Target and context definition During Step 1, we define target words and contexts. Through the literature, syntactic analysis is mainly used to get dependency relations. But as it is time-consuming and heavy, we choose to use instead graphical windows within a sentence and around the target word. As we work on specialized texts, we also identify terms with the term extractor Y A T E A (Aubin and Hamon, 2006).

We define the following parameters:  X  Target words: words are in relation when  X  Distributional contexts: contexts are made of  X  Fixed window size: we tested two different  X  Word form: for both contexts and target
Linguistic approaches During the normaliza-tion process described below, we use three exist-ing linguistic approaches: two methods that aim at acquiring hyperonymy relations and one that al-lows to get morphosyntactic variants.  X  Lexico-syntactic Patterns (LSP): we use the  X  Lexical Inclusion (LI): uses the syntactic  X  Terminological Variation (TV): uses rules
Context normalization Once targets and con-texts are defined comes the core of the hy-brid method with context normalization. During 115 Step 2, we normalize contexts with the relations acquired by the three linguistic approaches we mentionned.

The relations are integrated in contexts in the following way: a word in context is replaced by its hyperonym or its morphosyntactic variant. We define two rules :  X  If the word in context matches with only one  X  If the context matches with several hyper-
We normalize contexts with each method sep-aretly and sequentially: the first normalization is processed on all contexts before the second nor-malization starts, and so on.
 Computation of semantic similarity When contexts have been normalized, similarity between two target words of the same POS tag is computed. As we decrease diversity in contexts during the normalization step, we choose among the exist-ing measures (Weeds et al., 2004) a measure that favors words appearing in similar contexts com-pared to words appearing in many different con-texts.

The Jaccard Index (Grefenstette, 1994) normal-izes the number of contexts shared by two words by the total number of contexts of those two words. sim  X  JACCARD mn = Parameter: threshold We filter the relations according to three parameters, two of them ap-plied on the contexts and the third one on the tar-get.  X  Number of shared contexts: number of lem- X  Frequency of the shared contexts: number of  X  Frequency of the target words: number of oc-
For each parameter, a threshold is automatically computed, according to the corpus. It corresponds to the mean of the values taken by each parameter on the whole corpus.
In order to evaluate the contribution and influence of relations acquired by the three methods, we de-fine several sets of experiments and evaluate the relations acquired on existing resources. 4.1 Corpus
We use the merging of the two corpora pro-vided by DEFT 2013 French challenge 2 : the train-ing corpus (2,388,731 words) and the test corpus (1,539,927 words). They are both French corpora and contain cooking recipes. Each text of the cor-pus is made of a title, ingredients and the body of the recipe, and we use all the information.

We pre-process the corpus within the Ogmios platform (Hamon et al., 2007). We perform mor-phosyntactic tagging and lemmatization with Tree Tagger (Schmid, 1994), and use the term extractor Y A T E
A(Aubin and Hamon, 2006). 4.2 Parameters and models of hybridization
In these different sets we vary two main kinds of parameters (cf. table 1): window size and models of hybridization.

We test two window sizes. With a large one of 20 words around the target (10 before, 10 af-ter, henceforth W10) we may take into account the highest number of possible relations, because the average size of a sentence in French is 20 words and we restricted the relation acquisition to the sentence level. But such a large window may face a lack of specificity and get too much noise. We also test a window of 4 words (2 before, 2 after, henceforth W2). Such a size applied after remov-ing the function words is comparable to a 8 word window applied to the original texts (Rapp, 2003).
We test different models of hybridization. We first use DA on its own, without normalizing the contexts (DAonly). This set is a reference to which compare the hybridization sets. As for the models of hybridization, we first separately evalu-ate the contribution of each method (LSP, LI, TV) in distributional context, and then different types of combinations of the methods integrated in DA. Within these combinations, we first exploit two methods together and then three. Our goal is to evaluate the impact of the order of the methods and the contribution of each method. 4.3 Comparison with existing resources In order to evaluate the quality of the acquired re-lations, we compare our relations with three dif-ferent resources: Agrovoc 3 , of 75,222 relations [AGRO], and UMLS 4 .

With the UMLS resource, we build two dif-ferent resources: one more general [UMLS] of 2,325,006 relations, and a more specific one re-stricted to terms belonging to the Food concept (semantic type T168) [UMLS/Food] of 1,843 re-lations.

We only use the relations for the nouns and terms of our corpus, because adjectives were not represented in the resources. In that respect, we evaluate our work with 1,551 ([AGRO]), 1,800 ([UMLS]) and 871 ([UMLS/Food]) relations.

We use those three resources because of avail-ability. The comparison with UMLS/Food and Agrovoc is justified by the presence of relations between food terms in both resources. But in cooking recipes, we may find other types of re-lations, as the relation between a food term and a term belonging to another semantic class. The comparison with the whole UMLS may allow to detect other relations than ingredient relations.
Even if we can not expect an important overlap between these resources and the corpus, the com-parison of our results to the relations issued from these resources gives an indication of the contri-bution of each proposed hybridization model.

We compute precision for each target term: semantic neighbours (acquired by our method) found in the resource by the semantic neighbours acquired by our method. For each target term, we sorted the semantic neighbours we obtained ac-cording to their similarity measure, and apply four thresholds: precision after examining 1 (P@1), 5 (P@5), 10 (P@10) and 100 (P@100) neighbours.
We proceed to the analysis and discussion of the results we obtain with our hybrid method. Regard-ing the relations provided by the terminologies, we present here the results obtained for nouns and terms only.

We evaluate precison after examining four groups of neighbours. The best results are ob-tained with P@1, and decrease when we consider more neighbours: the more neighbours we con-sider, the lower precision is. For instance, for nouns-W10, precision decreases from 0.089 for P@1 to 0.009 for P@100, when compared with
Agrovoc. We make similar observations on all the sets of results. Best results in first position means that the values of the measures rank quite correctly the proposed relations, and therefore that the choice of the measure was a good choice.

The table 2 presents the results for P@1, given the two window sizes (W10 and W2). We describe here only those results. The relations produced by DA (DAonly) are considered as our baseline. The low precision of our results was expected and 117 can be explained by the fact that even if the re-sources are relevant for our corpus, they are not fully adapted. However, the comparison of the precision values gives important information on the usefulness of the hybridization models.

Results are better for nouns (between 0.056 and 0.111 for nouns-W10 and between 0.024 and 0.073 for nouns-W2, with Agrovoc) than for terms (between 0 and 0.047 for terms-W10, and between 0 and 0.34 for terms-W2, with Agrovoc). This is not surprising because terms do not match easily with other terms in resources. This can be due to two main factors: terms are less frequent and it is difficult to match terms from the terminologi-cal resources in the corpus. As for the window size, we observe that generally W10 gives good results for nouns and W2 is better with terms. But when we look more in details, we observe that the quality of the results depends on the resource used for comparison. For nouns, with Agrovoc and UMLS/food, W10 gives the best results, but when compared with UMLS results are better with W2. The difference is similar with terms, but in this case results are better with W2 when compared with UMLS and UMLS/food, and better with W10 when compared with Agrovoc.
 Linguistic approaches Considering the three methods individually, TV seems to have no in-fluence on the computation of semantic simi-larity; the results obtained with DAonly and DA/TV are identical, except for terms W2 and
W10 when compared with UMLS. Also, in all the hybrid sets, exploiting TV in the distribu-tional contexts doesn X  X  influence the results, ex-cept for DA/LI+LSP+TV with nouns-W10 when compared with Agrovoc and UMLS/food, and DA/TV+LSP with term-W2 when compared with
UMLS. This may be because of the small num-ber of relations used and our current way of DA hybridization with TV.

On the contrary, LSP is the method that most in-fluences the results: most of the time they give bet-ter results than DA. The best hybridization model for terms is the normalization with LSP, whereas for nouns the combination of LI and LSP is the best choice. The order of the methods also mat-ters, but results also differ according to the re-source; DA/LSP+LI (and DA/LSP+LI+TV) give better results when compared with UMLS/food, and DA/LI+LSP (and DA/LI+LSP+TV) give bet-ter results when compared with Agrovoc. What emerge from these results is that generalization with hyperonyms is the best configuration, for both terms and nouns, and that the quality of the hyperonymy relation is important as well. Lexi-cal inclusion used after patterns does not seem to bring new relations but allow to rule out noisy re-lations. By noisy relations, we mean relations not found in the resource. But these relations may be interesting and may be domain relations.

Resources and relation types The relations found by our method in UMLS/Food are co-118 hyponyms (eg: ail (garlic)/oignon ), those found in Agrovoc are both hyperonyms (eg:  X pice (spice)/poivre (pepper) ) and meronyms (eg: miel (honey)/sucre (sugar) ). Relations found in the whole UMLS are the same as those found in UMLS/food. The identification of terms allow to find more relations, between simple terms and complex terms. For instance, in UMLS/food, our method found the co-hyponyms poivre blanc (white pepper)/poivre noir (black pepper) and miel (honey)/fruit that are not identified by taking into account nouns only. In this work, we present our hybrid method based on normalization of distributional contexts. Our method aims at acquiring semantic relations from specialized texts, and is adapted to low frequency words. We normalize contexts with relations ac-quired by three linguistic approaches; two meth-ods of hyperonymy relation acquisition and a method of morpho-syntactic variant acquisition. We focus on relations between nouns and terms. We tested our method on a French corpus com-posed of cooking recipes, varying one parameter in our DA method, the window size, and testing different models of normalization. Normalization obtains the best results when realized with hyper-onyms and also depends on the quality of the hy-peronymy relations. In our method, the hyper-onym used for normalization is the one with the highest frequency. Even if precision values pre-sented in this work are currently low and results differ according to the resource used for evalua-tion, it emerges that the best parameters are for nouns a W10 with LSP, and for terms a W2 with LSP and LI. This set of parameters is to be used for classical types of relations. But other types may be acquired with our DA++ method, especially do-main specific relations. In order to have a better knowledge of the influence of each hybridization model, quality of the results has to be analyzed more deeply by manually checking with the val-idation of a subset of relations, and with a study of relations that are in common or not between the various results sets. For future work, we plan to investigate other strategies of normalization by assigning a weight to the relations proposed by the linguistic methods, or taking into account the level in the hierarchy. In that latter approach, the choice of the hyperonym used for the normaliza-tion could be guided by a distance (Resnik, 1995;
Leacock and Chodorow, 1998). Relations used for normalization can also be issued from terminolog-ical resources. Furthemore, we will intend to com-bine the methods before normalization and exploit other similarity measures.

Cet article d X crit une exp X rimentation visant  X  faciliter la reconnaissance automatique des termes (T) en texte int X gral. Elle contribue  X  un objectif final d X  X ndexation de textes via la re-connaissance automatique de termes en textes connaissance en nous focalisant sur l X  X nvironnement textuel des candidats termes, dans une philosophie similaire  X  celle de (Ba-chimont et al. 2005 ; Bourigault et al. 2001 ; 
Bourigault et al. 2004). Nous nous inscrivons dans le champ de la terminologie textuelle (Bourigault et Slodzian 1999) qui appr X hende les termes dans leur fonctionnement textuel. 
L X  X ypoth X se que nous testons est celle d X  X ne interaction privil X gi X e entre les unit X s de lexique transdisciplinaire scientifique (ULT) et les termes du vocabulaire de sp X cialit X  d X  X ne discipline. Plus pr X cis X ment,  X  la suite de (Kis-ter et Jacquey 2012), nous faisons l X  X ypoth X se que les co-occurrences port X es par une relation syntaxique entre les unit X s du lexique transdis-ciplinaire (ULT) et les termes peuvent  X tre des indices du statut terminologique des termes. Par exemple, dans le domaine des sciences du lan-gage, les termes diglossie et locution apparais-sent dans les textes avec concept et analyser : le concept de diglossie et nous analysons les locu-tions . ATILF, INIST, LIDILEM, LINA, INRIA NGE et 
Saclay : http://www.atilf.fr/ressources/termith/ (page consult X e le 17/09/2013). 
L'exp X rience de (Kister et Jacquey 2012), qui portait sur un petit corpus contrastif scientifique vs vulgarisation, a montr X  l X  X nt X r X t de mener une exp X rience  X  plus grande  X chelle : extension du corpus, automatisation, structuration s X mantique du lexique transdisciplinaire (toutes les ULT ne sont pas  X quivalentes du point de vue de l'hypo-th X se examin X e).

Notre exp X rience, centr X e sur le discours scienti-fique, automatise deux  X tapes cl X s : (1) la projec-tion du lexique transdisciplinaire, (2) la d X tectio n et la qualification des relations syntaxiques T-
ULT. Cette  X tape d X  X utomatisation permet ainsi d X  X ugmenter la taille du corpus de travail. 
Enfin, nous effectuons une analyse quantitative et qualitative des relations d X tect X es. L'analyse qualitative est r X alis X e en fonction d'une pre-mi X re structuration s X mantique du lexique trans-disciplinaire. Des pistes d'exploration sont pro-pos X es pour affiner les proc X dures dans la pers-pective d'une exploitation automatique du lexique transdisciplinaire. 
Le lexique transdisciplinaire des  X crits scienti-fiques est un lexique particulier qui ne renvoie pas aux objets scientifiques des domaines de sp X cialit X , mais plut X t aux discours sur les objets et les proc X dures scientifiques (Tutin 2007). Il es t mis en  X uvre dans la description et la pr X senta-tion de l X  X ctivit X  scientifique et est ainsi partag  X  par la communaut X  scientifique. Il s'agit donc d'un lexique de genre plus que d X  X ne terminolo-gie propre  X  un domaine. Il est en grande partie partag X  par de nombreuses disciplines, m X me si des diff X rences se font jour entre familles de disciplines. 
Ce lexique concerne ainsi des unit X s lexicales qui renvoient aux proc X dures scientifiques (  X tudier, analyser, recenser ), aux opinions ( de notre point de vue, nous pensons ),  X  l' X valuation ( valide, int X ressant, pertinent ), aux artefacts scienti-fiques ( approche, hypoth X se, mod X le ), aux ob-servables ( donn X es, r X sultats ). Il int X gre  X  la fois des mots simples et de tr X s nombreuses s X -quences lexicalis X es aux fonctions diversifi X es (Tutin,  X  para X tre).
 Pour les mots simples,  X  la suite du Vocabulaire 
G X n X ral d'Orientation Scientifique (Phal 1971), plusieurs listes ont  X t X  propos X es pour l'anglais (Coxhead 2000 ; Bolshakova 2008 ; Paquot 2010) et pour le fran X ais (Drouin 2007) et (Tutin 2007). Ces listes ont  X t X  constitu X es sur la base de crit X res statistiques (distribution dans plu-sieurs disciplines, fr X quences, sp X cificit X ). Dans une perspective d X  X ndexation de textes, nous envisageons d X  X tiliser le lexique constitu X  par fusion de ceux de (Drouin 2007) et (Tutin 2007) de deux mani X res :
En approfondissant et en cherchant  X   X valuer cette derni X re hypoth X se, nous esp X rons contri-buer  X  l X  X xtraction et  X  la reconnaissance auto-matique des termes dans les textes, sous l X  X ngle de la d X limitation des unit X s terminologiques d X  X n domaine par le biais du lexique transdisci-plinaire.
Comme bri X vement d X crit dans l X  X ntroduction, plusieurs phases segmentent les traitements r X alis X s sur les textes ( cf . Tableau 1). Phase 4 Filtrage et analyse des r X sultats 3.1 Corpus de travail, ressources lexicales et 
Les articles de l'exp X rimentation sont extraits du appartenant tous au domaine des sciences du langage (42 textes r X partis en 22% d X  X rticles et 78% de communications -155 157 occurrences). 
La ressource terminologique de r X f X rence est entr X es). 
Le lexique transdisciplinaire utilis X  est le r X sult at d'une fusion des lexiques de Tutin (2007) et 
Drouin (2007), restreint aux entr X es nominales et verbales afin de r X duire le bruit, ce qui entra X ne l X  X xclusion des adjectifs. Il comporte 390 noms ( cas,  X tude, travail, type ) et 321 verbes ( agir, consid X rer, correspondre) , soit 711 entr X es. 3.2 Annotation terminologique 
L'annotation terminologique est r X alis X e en deux  X tapes principales. Les extracteurs ACABIT (Daille 1996) et TER-
MOSTAT (Drouin 2003a et b) fournissent des listes de candidats termes en appliquant sur les http://scientext.msh-alpes.fr/scientext-site (page consult X e le 17/09/2013). bibliographiques en sciences humaines et sociales constitu X e par l X  X nist. con X u et maintenu par le centre de documentation de l X  X tilf  X  UMR 7118. articles donn X s en entr X e, un jeu de r X gles lin-guistiques et de crit X res statistiques. 
Apr X s avoir  X t X  projet X s dans les articles sources, les candidats termes sont s X lectionn X s manuellement pour ne retenir que les occur-rences terminologiques valides, qu'il s'agisse d X  X nit X s simples ou complexes. Pour d X termi-ner la validit X  des candidats, nous avons eu recours  X  des experts linguistes qui ont pu, si besoin, utiliser les deux ressources terminolo-giques de r X f X rence, Thesaulangue et le vocabu-laire Francis ( cf . section 3.1). 
Parmi les 43 324 occurrences de candidats, nous avons conserv X  11 772 occurrences jug X es valides (21%). A l X  X ssue de cette s X lection, les candidats valid X s sont consid X r X s comme des occurrences de termes. 3.3 Projection du lexique transdisciplinaire 
La projection du lexique transdisciplinaire est r X alis X e par un module distinct qui prend en entr X e les articles enrichis en termes et le lexique transdisciplinaire s X lectionn X . A l'issue d'un  X tiquetage morpho-syntaxique (TreeTagger et d'une projection du lexique transdisciplinaire, l'ensemble des co-occurrences d'une m X me phrase est report X  en tenant compte de la com-binatoire possible. Une m X me phrase qui con-tient deux termes (T 1 et T 2 ) et 2 ULT (ULT 1 et 
ULT 2 ) diff X rents est report X e 4 fois en mettant en valeur  X  chaque fois un couple (T i , ULT j ) diff X rent. A l'issue de ce traitement, nous obte-nons 53 281 relations de co-occurrence impli-quant 1 273 termes diff X rents : corpus 1 417 relations, mots 8 1 156, mot 1 068, verbe 1 067, sens 814, etc. 
Ensuite, les phrases contenant des co-occurrences T-ULT sont analys X es syntaxiquement ( X  l X  X ide de XIP) afin de d X terminer si la relation de co-occurrence est une relation syntaxique de d X pendance et de pr X ciser la nature de la d X pendance quand il y en a une. le 17/09/22013) par ACABIT et TERMOSTAT, nous avons choisi les formes plein-texte afin d' X viter de niveler des distinctions terminologiques pouvant  X tre importantes. 123 3.4 D X tection et qualification des relations 
Comme indiqu X  pr X c X demment, les phrases trai-t X es sont analys X es en d X pendance avec XIP de fa X on  X  rep X rer automatiquement les relations syntaxiques de d X pendance entre un terme et une ULT. 
Sur les 53 281 co-occurrences T-ULT du corpus, 4 565 relations de d X pendance directes ont  X t X  rep X r X es. Ces relations syntaxiques de d X pen-dance sont class X es par ULT, par nature de la relation de d X pendance et par orientation de cette relation (ULT rectrice ou r X gie). Nous avons ainsi pu identifier les associations lexico-syntaxiques les plus fr X quentes (relation i , ULT j ).
L'analyse quantitative et qualitative des r X sultats obtenus porte sur les seules relations directes et plus particuli X rement sur trois patrons de rela-passives DEEPSUBJ et DEEPOBJ. Ces trois types de relations ont  X t X  choisis, d'une part, pou r leur productivit X  (3 406 sur 4 565) et, d'autre part, pour leur repr X sentativit X . La relation de type NMOD s' X tablit principalement au sein du groupe nominal complexe. Les relations de type 
OBJ et SUBJ apparaissent dans le domaine ver-bal. De cette mani X re, nous esp X rons couvrir tout ou partie des cas les plus int X ressants d'interac-tion entre termes et ULT. 
Nous pr X sentons ci-dessous des exemples de relations entre un [terme] T r X gi et une [unit X  du lexique transdisciplinaire] ULT rectrice :
OBJ : relation entre un verbe et un compl X ment d X  X bjet direct ou l X  X ttribut du sujet. SUBJ : relation entre un pr X dicat et un sujet. 
DEEPSUBJ : relation entretenue par un pr X dicat et u n compl X ment d X  X gent dans une tournure passive. 
DEEPOBJ : relation qui s X  X  X ablit entre un pr X dicat et son sujet dans une tournure passive. 
Nous avons  X cart X  les erreurs d'analyse syn-taxique parmi les 3 406 relations  X tudi X es ce qui nous conduit  X  rejeter 768 analyses erro-n X es : erreur de recteur ou de r X gi, erreur de relation.

Puis, nous avons class X  les 2 638 couples res-tants (T i , ULT j ) : les cas pour lesquels l X  X LT joue un r X le de d X limiteur de terme et ceux pour couple analys X  est ainsi plac X  dans l X  X ne des trois cat X gories suivantes : 
La r X partition obtenue pour les 2 638 relations analys X es est pr X sent X e dans le tableau 2. occurrents n X  X st plus  X  mesurer car l X  X nnotation terminologique a d X j X  rempli cette t X che. ( cf . section 3.2) 124
Pour certaines occurrences, la configuration est tr X s claire : l X  X LT putative a pour fonction d'ex-poser des proc X dures et des outils de l'activit X  scientifique consid X r X e. 
Dans le cas de notre  X tude, les quatre textes [cons ti-quement annot X s .

Le m X me verbe peut avoir une valeur relevant de la langue g X n X rale comme constituant dans l X  X xemple suivant : ments : d X terminant + nom [constituant] ULT le [groupe nominal] T sujet, verbe marcher au pr X sent.
Dans certains contextes, l X  X mbigu X t X  est impor-tante au point de rendre une d X cision spontan X e difficile. Pour ces cas, on s X  X nterroge au sujet de l X  X mploi terminologique ou langue g X n X rale de l X  X LT. ont  X t X  [produites] ULT par des [enfants franco-phones] T [...] 
Dans ce cas, produire a un sens ambigu entre l X  X cception de langue g X n X rale plus proche du verbe support faire et le sens terminologique de production langagi X re par opposition  X  compr X -hension . 
Un dernier type de difficult X s tient  X  notre m X -thode qui isole les phrases et n X  X ffre pas toujours un contexte suffisant pour comprendre le sens de l'occurrence. C X  X st le cas lorsqu X  X n est en pr X -sence d X  X naphores : pour les t X ches scolaires d'[ X criture] T , pr X c X demment cit X  ( cf. section 4.1) 
Dans cet exemple, il est difficile de retrouver l X  X nt X c X dent de la premi X re . 
A c X t X  de l'analyse quantitative, nous avons souhait X  observer plus finement les ULT ou les classes d'ULT les plus susceptibles d'introduire des termes. Nous faisons, en effet, l'hypoth X se que certaines ULT qui ont une fonction m X ta-linguistique ( concept, terme ) ou introduisent des proc X dures scientifiques ( X tudier, analyser ) sont plus propices  X  l'introduction de termes. 
Pour cette t X che, nous avons effectu X  un typage s X mantique des ULT les plus productives en observant  X galement les ULT projet X es n'en-trant pas dans ces relations. Nous nous sommes pour cela bas X s sur un typage effectu X  par Tutin (2007)  X tendu pour cette exp X rience. 5.1 Les ULT nominales introductrices de 
Les noms transdisciplinaires ont  X t X  caract X ris X s dans des grandes classes s X mantiques, cons-truites largement  X  partir de crit X res distribu-tionnels et inspir X es de l'approche de Flaux et 
Van de Velde (2000). Certaines classes sont assez g X n X riques comme les noms de processus ( choix ), les noms humains (personne, individu ), les noms quantitatifs (nombre de, ensemble de ) alors que d'autres sont sp X cifiques de la langue scientifique comme les noms de processus scientifique (  X tude, description, recherche ), les noms d'observables scientifiques avec les objets  X tudi X s par l'activit X  scientifique ( donn X es, pa-ram X tres) , les noms d'artefact scientifique avec les objets construits par la r X flexion scientifique ( approche, m X thode, analyse ). Bien entendu, certains noms peuvent relever de plusieurs ca-t X gories. La cat X gorisation a  X t X  effectu X e en observant l'ensemble des contextes de fa X on syst X matique pour les noms les plus productifs de la liste des 390 noms transdisciplinaires. 
Tableau 3 -ULT nominales introductrices de termes 
Dans la cinqui X me colonne du tableau 3, on peut constater un tr X s net accroissement de la propor-tion de cas satisfaisants ce qui confirme l X  X ypoth X se selon laquelle certaines classes de noms sp X cifiques de la langue scientifique sont clairement des introducteurs privil X gi X s de termes. C'est en particulier le cas des noms de processus scientifiques : artefacts scienti-fiques ( analyse, construction, description, re-cherche,  X tude ) et artefacts scientifiques purs ( structure, repr X sentation, sch X ma, mod X le ) . 
On observe aussi un ensemble de noms a priori moins sp X cifiques : additionnant les fr X quences absolues respectives de chaque ULT cit X e dans la colonne pr X c X dente et pr X sente dans le corpus de travail. 
Contrairement  X  nos attentes, les noms m X talin-guistiques ( terme, notion, concept, mot, nom ) ne sont pas les meilleurs introducteurs de termes bien que les noms terme et mot figurent parmi les ULT les plus fr X quentes du corpus 
Scientext dans son ensemble. Dans notre  X chan-tillon, seuls notion et concept sont utilis X s  X  cette fin, mot, nom et terme sont surtout  X  ce qui est  X videmment  X troitement li X   X  la disci-pline des textes  X  utilis X s comme des termes. 
Par ailleurs, on observe que quelques classes de noms transdisciplinaires sont peu employ X es comme introducteurs de termes :
En r X sum X , si l'hypoth X se est fortement v X rifi X e pour certaines classes s X mantiques de noms (en particulier pour les processus scientifiques et artefacts scientifiques et pour les processus dans leur ensemble), ce n'est pas le cas d'autres cat X gories bien repr X sent X es dans notre genre, principalement pour deux raisons : 
La cat X gorisation s X mantique des noms du lexique transdisciplinaire appara X t donc indis-pensable pour effectuer un filtrage plus effi-cace. 5.2 Les ULT verbales introductrices de termes 
Nous avons  X galement observ X  le fonctionne-ment des verbes introducteurs de termes appa-raissant avec les relations SUBJ/DEEPSUBJ et 
OBJ/DEEPOBJ. La liste de verbes utilis X s, peu filtr X e, a  X t X  caract X ris X e  X  l'aide de classes de quasi-synonymes pour les  X l X ments les plus pro-ductifs ( cf. Tableau 4). 
Tableau 4 : ULT verbales les plus productives comme
Comme dans le cas des noms, certaines classes de verbes apparaissent plus clairement introduc-trices de termes. La cinqui X me colonne du ta-bleau 4 montre une nette augmentation de la proportion de cas pour lesquels l X  X ypoth X se est v X rifi X e, except X  pour la classe utiliser . Cepen-dant, la t X che de caract X risation est rendue plus difficile par une polys X mie plus grande que pour les noms. 
Les verbes s X mantiquement quasi-vides (  X tre, avoir ) sont largement repr X sent X s, ce qui appara X t facilement explicable par les formules d'identifi-cation, souvent utilis X es pour l'introduction de termes comme :
Viennent ensuite les modaux ( devoir, sembler ), largement repr X sent X s, eux aussi, dont nous pen-sons qu X  X ls peuvent  X tre consid X r X s comme des auxiliaires modaux dans la t X che d'analyse syn-taxique. 
A c X t X  de ces verbes  X  tout faire , fr X quents dans la langue g X n X rale, on rel X ve plusieurs classes synonymiques productives comme celle de l'identification ( identifier, distinguer, trouver ), de l' X tude (  X tudier, classer, consid X rer 12 ), de la description ( d X crire, caract X riser ).

Comme pour les noms, certaines classes syno-nymiques paraissent peu repr X sent X es : celles du point de vue ou des liens logiques ( cause, cons X quence ). 
L'examen des classes s X mantiques introduc-trices de termes appara X t comme une piste pro-metteuse, certains champs paraissent clairement privil X gi X s comme introducteurs de termes (processus et artefacts scientifiques, processus de description et d' X tude, formules identifica-toires ). Cela m X rite un approfondissement sur un ensemble de donn X es plus vaste, travail que nous souhaitons entreprendre en extrayant se-mi-automatiquement la combinatoire syn-taxique et s X mantique du lexique transdiscipli-naire  X  partir de corpus arbor X s (sous-cat X gorisation syntaxique, co-occurrents lexico-syntaxiques) de fa X on  X  acc X l X rer et faciliter le processus de cat X gorisation.
L X  X xp X rience d X crite dans cet article examine, sur un corpus de 42 articles scientifiques en sciences du langage, la mani X re et la proportion selon lesquelles les unit X s d X  X n lexique trans-disciplinaire scientifique peuvent jouer un r X le de d X limiteurs de termes. L X  X  X aluation quantita-tive de cette hypoth X se montre que celle-ci est v X rifi X e pour environ 74% des cas trait X s ( cf.
Tableau 2). Nous analysons manuellement 2 638 couples (T i , ULT j ) parmi 3 406 couples qualifi X s par une relation syntaxique de d X pen-dance (NMOD, OBJ, SUBJ et leurs variantes. 
Sur le plan quantitatif, l X  X nalyse manuelle des 2 638 cas pour lesquels il n X  X  a aucune erreur d X  X nalyse syntaxique montre que l X  X ypoth X se est majoritairement valide mais insuffisamment r X guli X re pour utiliser en l X  X  X at les ULT pour filtrer automatiquement les candidats termes propos X s par les extracteurs automatiques de termes, ACABIT et TERMOSTAT. 
L X  X nalyse qualitative des r X sultats les plus fr X -quents fait  X merger une premi X re piste d X  X m X lioration en mettant en valeur plusieurs classes d X  X nit X s nominales et verbales pour les-quelles on constate un net accroissement de la proportion de cas satisfaisants ( cf . Tableau 3 et Tableau 4, section 5). 
Au-del X  de l X  X xp X rience d X crite, plusieurs pistes nous semblent int X ressantes pour am X liorer les r X sultats. 
Tout d X  X bord, l X  X xp X rience men X e dans le do-maine des sciences du langage engendre un biais disciplinaire important : par exemple l X  X mbigu X t X  constat X e pour le verbe produire mentionn X e ci-dessus. Ce biais pourra  X tre r X duit par une exten-sion de l X  X xp X rimentation  X  d X  X utres disciplines scientifiques et la prise en compte des sp X cifici-t X s de chacune d X  X lles avant de projeter le lexique transdisciplinaire. 
Ensuite, l X  X xtension de l X  X xp X rience ne sera que plus productive si on op X re un affinage du typage s X mantique du lexique et en particulier de la combinatoire des ULT.

De plus, il faut envisager de diversifier le rep X -rage des relations syntaxiques. L X  X d X e est de ne plus limiter l X  X xp X rience  X  des relations binaires mais de mettre en place une reconnaissance de patrons identificatoires (Jacques 2011) de la forme un T [est] ULT un T qui [...] , par exemple. 
Enfin, il sera n X cessaire d X  X nalyser les contextes o X  les termes apparaissent sans pour autant  X tre introduits par des ULT. A X t-Mokhtar, S., Chanod, J-P. et Roux, C. (2002). Bachimont B., Baneyx A, Malais X  V, Charlet J et 
Bolshakova E. (2008). Common Scientific Lexicon Bourigault D, Aussenac-Gilles N et Charlet J. (2004 ). 
Bourigault D, Jacquemin C et L X  X omme MC. 
Bourigault, D. et Slodzian, M. (1999). Pour une Coxhead, A. (2000). A New Academic Word List. 
Daille B. (1996). ACABIT : une maquette d'aide  X  la
Drouin P. (2003a). Term extraction using on-
Drouin P. (2003b). Acquisition des termes simples 
Drouin P. (2007). Identification automatique du 
Flaux N. et Van de Velde D. (2000). Les noms en 
Jacques, MP. (2011). Nous appelons  X  X cet Y  X  : X 
Kister L., Jacquey E. (2012). Relations syntaxiques
Paquot M. (2010). Academic vocabulary in learner 
Phal, A. (1971). Vocabulaire g X n X ral d'orientation Tutin A. (2007). Lexique et  X crits scientifiques. 
Tutin A. ( X  para X tre). La phras X ologie transdiscipl i-
Olena.Orobinska@univ -
On c herche souvent  X  construire l X  ont o logie d  X  X n nouveau domaine d X  X ctivit X  ; il est donc utile de di s poser d X  X n proc X d X  rapide et efficace. Dans cet article, nous proposons une m X thode simple pour construire une ontologie  X  partir d X  X n co r-pus et d X  X n pr e mier glossaire ; cette m X thode d X couvre automatiquement les patrons termin o-logiques utiles pour un domaine . Cela pe r met de limiter le r X le de l X  X xpert du domaine  X  la valid a-tion des candidats -termes propos X s ; nous l X  X ppliquons au domaine de la Radioprotec tion. 
Le cadre g X n X ral a  X t X  pr  X  sent X  en langue russe dans (Orobinska, 2012).
 al (2009) : l X  X ng X nierie des ontologies a atteint sa maturit X  en utilisant la diversit X  des m X thodol o-gies, approches et techniques. M ais ce processus reste tr X s lab o rieux et co X teux et il faut chercher des proc X d X s rapides et efficaces, c a pables de d X gager automatiquement des  X l X ments de co n-naissances dans les ressources te x tuelles.
 s X mantiqu e qui encode les concepts, les relations et les axiomes d X finissant le mod X le d'un d o-maine donn X  (De Nicola &amp; al. 2009; Gruber 1995 ; Neches &amp; al. 1991 ) . A la diff X rence de la lexicologie, o X  les notions de  X  concept  X  et de  X  terme  X  sont bien distinctes 1 , ici, pour la con s-truction d X  X ntologies, ces deux mots seront s y-nonymes ; un  X  concept  X  est un  X l X ment constructif d X  X ne ontologie. Les termes corre c-tement d X tect X s dans le corpus vont  X tre impla n-t X s dans notre ontol o gie, soit comme des classes ou sous -c lasses, soit comme de nouveaux indiv i-dus des classes ou sous -classes. 
La section 2 introduit l X  X  X at de l X  X rt sur les techniques d X  X xtraction de termes et de relations  X  partir de ressources textuelles ; nous pr X sentons notre approche d X  X xtraction des nouv eaux cand i-dats -termes dans la section 3 ; la section 4 d  X -taille l X  X mpl X mentation de notre m X thode d X  X nrichissement de la taxonomie pour chaque concept d X  X ne  X  ontologie plate  X  ; la section 5 pr X sente les r X sultats des exp  X  rimentations et la section 6 soul igne d es probl X mes  X  r  X  soudre et introduit quelques perspectives.
Pour enrichir une ontologie  X  partir de textes, on distingue deux cat X gories de m X thodes : les m  X -thodes purement statistiques et celles qui se b a-sent sur le Tr aitement A uto matique de la 
Langue . L X  X pproche linguistique permet de fo r-muler des r X gles d X  X nf X rences ou de trouver les heuristiques. La fusion de ces approches a e n-gendr X  des m X thodes hybrides. L X  X xploitation des connaissances linguistiques est devenue po s-sible gr X ce  X  la mise au point et  X  la diffusion d X  X utils et re s sources sp X cialis X s : analyseurs grammati caux et dictionnaires  X lectroniques. Pour les dictionnaires, la communaut X  de Tex t-
Mining se sert surtout d X  outils comme WordNet, mais beaucoup d X  X utre s ressources utiles sont acce s sibles sur la Toile 2 . A l X  X venir, l X  X ng X nierie des ontologies utilisera plus largement l X  X nsemble de ces o u tils.
 grammaticaux pour extraire les mots li X s par les relations taxonomiques de grand corpus par 
Hearst (1992) , un grand nombre de chercheurs travaille dans cette direction. L X  X tilisation de patrons syntaxiques permet d X  X btenir des  X  un i-t X s de connaissances  X  pertinentes mais leur  X l a-boration reste laborieuse. fectu X es ; nous citons ceux qui ont particuli X r e-ment inspir X  nos propres exp X rimentations. Une longue liste de patrons pour extraire des comp  X -tences dans le domaine du management est pr o-pos X e par Buitelaar &amp; Ei gner ( 2008) ; c e pendant, ces patrons semblent d X finis a priori par des e x-perts linguistes et non issus automatiqu e ment du corpus. semi -automatique d X  X  ntologie et sur l X  X xtraction de relations non taxonomiques sont pr X sent X s dans (Ma kki &amp; al. 2009) pour le domaine de  X  risk management  X . Musti X re &amp; al. ( 2011) pr o-posent des techniques automatiques de traitement du langage et d X  X lignement d X  X ntologies.
 travaillent depuis 20 ans sur la mod X lisat ion te r-minologique  X  base de textes. Leurs r X sultats sont rassembl X s dans la plate -forme 
TERMINAE 3 . Depuis 2009, cette  X quipe d X v e-loppe DAFOE, une plateforme multi m o d X les et multi m X thodes pour construire des ont o logies  X  partir de corpus (Charlet &amp; al, 2 009).
 vise les items sp X cifiques au corpus technique est propos X e par P. Douin (200 3 ) et elle e s t r X alis X e dans le projet TermoStat 4 .
 al, 2005) permet de d X gager  X  la f ois les concepts et rela tions qui les lient. S X nchez (2010) pr X se n te l es recherches men X es sur l X  X tilisation des re s-sources web o X  les textes sont plus struct u r X s et contiennent d X j X  des m X tadonn X es qui facilitent l X  X xtraction de connai s sances.
 va G. (2011) a produit un travail sur la g X n X ral i-sation des stru c tures syntax iques minimales qui permettent de distinguer le sens de phrases .
Nous d X veloppons nos m X thodes pour l X  X  X aboration, l X  X nstallation et l X  X nrichissement de l X  X ntologie de la radioprotection  X  la demande du groupement de recherche et production  X  M X tr o-logie  X  de Kharkov, Ukraine. Cette organis a tion, membre de l X  X IEA , souhaite disposer d X  X ne base de connaissances en anglais, fran X ais et russe. En coll a boration avec ces sp X cialistes, nous avons construit un mod X le du domaine (Fig.1).

Fig .1 . Mod X  le du domaine de radioprotection pour 3.1 Bases th X oriques
Nous posons comme hypoth X se que les in s tances des concepts et des relations qui existent dans l X  X nsemble des textes publi X s dans le d o maine portent la s X mantique du domaine  X  tr a vers les r X gles linguistiques qui les r X unissent. En disp o-sant d X  X ne liste de concepts g X n X riques et en d  X -couvrant empiriquement les r X gles de fo r mation de leurs attributs  X  partir d X  X n corpus, on peut  X largi r l X  X ntologie sous -jacente par d es termes nuanc X s, formant la tax o nomie (Fig. 2).

Fig. 2. Installation de hi X rarchie de con cepts  X  partir -dose efficace : somme des doses  X quivalentes -dose efficace collective : somme de toutes les (i) l X  X nsemble C des identifiants de concepts ; (ii) la relation de subsomption entre les co n cepts qui est transitive, r X flexive et antisym  X  trique (ordre partiel) ; (iii) l X  X nsemble A des attributs des co n-cepts. De telles structures sont nomm X es  X  hi  X -rarchies des concepts  X  ou  X  taxonomies  X  (Cimiano, 2005 ; Ganter, 1999) . logie de noya u par l X  X xtension de termes g X n  X -riques en utilisant des patrons terminologiques . La combinaison de mots cr X ant de nou veaux termes sui t des r X gles syntaxiques que nous allons d  X -couvrir dans le corpus. Les termes sont form X s par une combinaison syntaxique hi  X ra r chis X e de mots, appel X s syntagmes termino logiques ou s y-napsies (Cabr X , 1998) . L X  X pplication de ces sy n-tagmes terminologiques permet l X  X xtraction de termes  X  partir de leur forme sy n taxique . 3.2 Formation des patrons terminol o giques 
Nos patrons terminologiques sont form X s en deux  X tapes  X  partir des fr X quences de structures syntaxiques dans le corpus, celui -ci  X tant pr X al a-blement  X tiquet X , tagu X , par les balises corre s-pondantes aux  X  parties de discours  X  , puis  X  par tir de l X  X nalyse syntaxique des termes du glossai re de domaine. aux patrons sont extraits du corpus , puis valid X s. 
Par construction, tous les fragments extraits co n-tiennent des termes d X riv X s de concepts de l X  X nt ologie sous -jacente . Autr e ment dit, chaque terme nouveau contient un des concepts en tant que radical. m X me racine fo r ment la taxonomie partielle (  X  5 ). Ils sont rajo u t X s dans l'ontologie en tant que sous -cl asses de concepts correspo n dants. termes, pr X  sent X e Fig. 3 , a  X t X  programm X e en Java .
 fectue la conversion des fichiers PDF en format textuel puis le net toyage des texte s obtenus en  X li minant les fragments contena nt des cara c t X res autres que des lettres, chiffres ou signes de pon c-tuation ; le module Etiquetage produit l X  X  X iquetetage des textes par TreeTagger ; l e m o-dule Formation d es patrons recense tous les p a-trons termi nologiques ; le module Extra c tion permet de r X cup X rer les fragments qui correspo n-dent aux patrons et il forme les taxonomies pour chaque racine. Ensuite , les r X sultats sont pr X se n-t X s aux experts du domaine et rajout X s  X  l X  X ntologie initiale s X  X ls sont vali d  X s .
 l X  X ntologie du domaine par la terminologie d X riv X e de 
Nous avons constitu X  un corpus de textes fra n- X ais ab ordant la probl X matique du domaine  X  pa r-tir de documents officiels accessibles en ligne, notamment les normes de s X ret X  et les ra p ports de l X  X AEA et de la Commission internati o nale de protection radiologique, et  X  partir d X  X rticles de revues sp X cialis X es p ubli X s depuis 1999 ; ce co r-pus (texte brut) contient 1 500 000 mots. 
La plupart des textes  X tant initialement en format PDF, on a d X  les convertir dans un format (texte ou HTML) utilisable pour le tra i tement .
A l X  X  X ape suivante les t extes ont  X t X   X tiquet X s avec TreeTagger (Schmid, 1994) de fa X on  X  o b-tenir des f i chiers o X  les textes sont transform X s sous la forme  X  ...pos/lemme pos/l emme pos/lemme... X  o X  : -pos  X  correspond  X  une certaine partie du discours reconnue par TreeTagger. Dans c et a r-ticle, nous utilisons le mot  X  balise  X  pour d X s i-gner une telle partie du discours ; -lemme  X  pr X sente la forme normale d  X  un mot : l  X  infinitif pour les verbes ; le masculin -singulier pour les noms et adjectifs, etc.  X ... d X triment caus X  au personnel m X dical est pris en compte de fa X on subsidiaire . X , NOM/ personnel ADJ/ m X dical VER/  X tre PPP/ prendre PRP/ en NOM/ compte PRP/ de NOM/ fa X on ADJ/ subsidiaire SE NT/.  X  grammaticales pour le fran X ais, ce qui est trop d X taill X  pour nos objectifs ; pour g X n X rer les p a-trons grammaticaux nous avons r X duit  X  17 le nombre de ces formes (Tab. 1). telles que les articles, les propositions et les pr o-noms, qui sont souvent n X glig X  e s par d X  X utres auteurs .
Un patron terminologique est une cha X ne de b a-lises que l X  X nalyseur automatique (Tre e T agger) est capable de distinguer. Par exemple,  X  NOM/PRP/NOM  X  est le patron pour tout fragment qui se co m pose d X  X n nom suivi par une pr X position suivie par un nom.

Tab.1. Notre liste r X duite de 17 balises de p arties de grams des balises grammaticales qui ont rempl a-c X s l es mots dans le corpus ; nous avons utilis X  des grams de taille (N) variant de 2  X  6. Nous avons extrait du corpus tous les fragments de phrases correspondants  X  ces N -grams. 
La s X lection des patrons potentiellement pert i-nents a  X t X  faite  X  partir des ter mes g X n X riques (= concepts) de l X  X ntologie sous -jacente. La liste de ces termes g X n X riques, contenant 38 noms, a  X t X  install X e apr X s consultation d X  X n expert du d o-maine ; ces termes ont  X t X  utilis X s pour cr X er les co n cepts d X  X ntologie sous -jacente. Ci -apr X  s nous utiliser ons le mot  X  concept  X  pour ces termes -l X .
On a retenu les patrons pour lesquels au moins 70% des phrases correspondantes contiennent un tel concept ; ce seuil a  X t X  choisi exp X rimental e-ment. Les scores de cette partie de l X  X xp X rimentation s ont pr X sent X s dans la table 2.
Nous avons enrichi notre liste de patrons  X  l X  X ide du glossaire propos X  en 2007 par la Co m-mission Internationale de Protection Radiol o-gique. Ce glossaire compte 162 termes. La structure syntaxique des certains termes est asse z  X volu X e ce qui permet d X  X  X argir la liste des p a-trons.

Les terme s contiennent en moyenne 2,8 mots ( entre 1 et 10 mots et le plus souvent entre 2 et 5) ; la table 4 en donne des exemples .

N ous avons ajout X  7 nouveaux patrons term i-nologiques (Tab. 3) issus de l X  X nalyse du glo s-saire ; leur longueur peut  X ventuellement d X pa s ser 6 .
Tab.3. Patrons termino logiques fo rm X s  X  partir du 
La liste finale, avec des exemples des termes correspondant  X  chaque patron terminologique, est pr X sent X e dans la table 4.
Une intervention humaine est n X cessaire pour la validation d X finitive des  X  candidats -termes  X  d X tect X s dans le corpus  X  partir des patrons te r-minologiques. Les scores d  X  X  X aluation de ces candidats -termes sont donn  X s dans le t a bleau 5 o X  o n lit le s informations suivantes : la c o lonne 
Patron contient la liste des patrons terminol o-giques; la colonne Total co n tient le nombre des fragments d X tect X s dans le corpus, qui corre s-pondent aux patrons; la colonne Taux des fra g-ments contenant un concept fournit le nombre de fragments o X  entre au moins un des concepts de l X  X ntologie de d X part; la colonne Taux des termes d X riv X s donne le nombre de termes du domaine parmi les fragments pr X c  X  dents.

Par exemple, ligne 1 du tableau 5: dans le co r-pus, on rencontre 14 7 59 expressions diff X rentes contenant le patron  X  NOM + ADJECTIF  X  ; parmi elles , 1 296 co n tiennent un mot de la liste initiale, soit 8, 8 % (0,88=1296/ 14 759). Parmi ces 1 296, 558 expressions sont accept X es par l X  X xpert comme des termes qui compl X teront l X  ontologie, so it 43 % (0,43 = 558/1 296).
Notons que la pertinence d X  X n patron termin o-logique d X pend de sa taille (nombre de pa r ties du discours forma nt le patron terminol o gique) ; les patrons longs sont en moyenne plus pert i nents.
Nous allons utiliser les d X finitions su i vantes : -taxonomie partielle : taxonomie de chaque concept de l X  X ntologie, form X e par ses desce n-dants (termes d X riv X es), ainsi chaque concept est le  X  so m met  X  de sa taxonomie partielle ; -racine du patron terminologique : la stru c-ture mi nimale linguistique  X  laquelle un terme du domaine peut correspondre. Nous distinguons trois types des racines : NOM+ADJ, NOM+PPP et NOM+PRP+NOM ; l X  X nsemble des termes co r respondants  X  une racine forment le niveau I de la taxonomie d X  X n concept ; -terme -descendant , ou descendant : le terme d X riv X , form X   X  la base d X  X ne racine; les desce n-dants forment les niveaux II et III de chaque taxinomie partielle.

L es patrons terminologiques r X unis autour de chaque racine permettent de former les taxon o-mies  X  trois n iveaux (sans compter les niveaux des concepts en tant que tels). Ceci est illu s tr X  dans les figures 4 -a, 4 -b, 4 -c. Fig. 4 -a. Taxonomie avec la  X racine X  NOM+ADJ taxonomies partielles sont pr X sent X s dans le t a-bleau 6.
 chaque type de racine, sont pr X sent X s dans le t a-bleau 7 .
 Tab. 7. Exemples de taxonomies de termes deriv X s 
Notons deux faits : rents niveaux ne sont pas forc X ment embo X t X s ; exemple  X  exposition continue  X  (N OM+ADJ) n X  X st pas retenu au niveau I, car non sp X cifique du domaine, mais  X  exposition continue au rayonnement  X  (NOM+ADJ+PRP+NOM) est retenu au n i veau II pour enrichir l X  X ntologie ; dant du niveau I vers les nive aux II et III est inf  X -rieur au nombre des termes correspondant  X  chaque patron car chaque terme du niveau sup  X -rieur n X  X  pas n X cessairement de terme -fils.
Nous avons montr X  qu X  X  partir d X  X n corpus du d o maine, on peut enrichir une ontologie  X  pl ate  X  (constitu X e d X  X ne premi X re liste de mots) en r e-cherchant les patrons terminologiques des phrases qui contiennent ces mots dans le corpus, puis, inversement, les termes plus ou moins complexes contenus dans les phrases co n struites sur ces patrons. 
No us avons construit un grand corpus original du domaine de la radioprotection (environ 1 500 000 mots) et nous y avons appliqu X  notre m X thode.

Nous avons produit des  X  candidats -termes  X  qui peuvent  X tre soumis  X  un expert pour valid a-tion.

La terminologie d  X  X n domaine de haute -technologie, tel la radioprotection, ne se l i mite pas  X  une liste de mots isol X s, et la fo r mation des termes complexes suit des r X gles syntaxiques propres  X  chaque langue et  X  chaque domaine . 
Nous avons montr X  que la probabilit X  qu X  X ne structure syntaxique fr X quente dans le corpus sp X cialis X  corresponde  X  un terme int X ressant est assez  X l e v X e, jusqu X  X  70% . Cela nous a permis de travai l ler sur les patrons terminologiques et de proposer une m X thode de d X tection de ces p a-trons  X  partir des fr X quences de chaque structure syntaxique dans le corpus. 
La liste des patrons a  X t X  enrichie  X  partir d X  X n glossaire du d o maine . 
Parmi les difficult X s rencontr X es, citons la constitution du corpus et la transformation de textes du format PDF vers le fo rmat TXT ; nous avons cr X  X  un module qui r X alise cela automat i-quement. La faible capacit X  de TreeTagger  X  di s-tinguer certaines formes grammaticales du fran X ais, par exemple adjectif et participe pass X , cr X e du bruit qui perturbe toute la cha X ne de tra i-teme nt ; un perfectionnement des analyseurs li n-guistiques am X liorerait nos r X sultats finaux
Dans le futur, nous envisageons d X  X  X udier une g X n X ralisation des r X gles de construction des p a-trons terminologiques  X  partir d X  X nalyse sy n-taxique.

Aussenac -Gilles Nathalie. 2006. M X thodes asce n-dant e s pour l X  X ng X nierie des connaissances : Sy n-th X se des travaux. Institut de recherches en informatique de Toulouse, p.226.
Buitelaar Paul, Eigner Thomas. 2008. Topic Extra c-
Cabr X , Maria Teresa. 1998. La terminologie. Th X orie, 
Charlet J. et al. Apport des outils de TAL  X  la con s-
Cimiano Philipp, Hotho Andreas and Staab Steffen. De Nicola Antonio, Missikoff Michele and Navigli 
Drouin, Patrick . 2003. "Term extraction using non -
Ganter Bernhard and Wille Rudolf. 1999. Formal 
Gruber Thomas R. 1 995. Towards principles for the 
Hearst Marti A. 1992. Automatic Acquisition of H y-Makki Jawad, Alquier Anne -Marie and Violaine M usti X re S X bastien, Abadie Nathalie, Aussenac -Gilles Neches Robert, Fikes Richard, Finin Tim, Gruber 
Orobinska Olena A. 2012. Automatic Method of D o-
S X nchez David. 2010. A methodology to learn ont o-logical attributes from the Web. Data Knowl. Eng. 69, 6 (June 2010), 573 -597.

Schmid Helmut. 1994. Probabilistic Part -of -Speech tagging using decision trees. In Proceedings of the 
International Conference on New Methods in La n-guage Processing, Manchester, UK, pages 44  X  49.
Simperl Elena, Mochol Malgorzata , B X  rger Tob i as and Igor O. Popov. 2009. Achieving Maturity: The State of Practice in Ontology Engineering in 2009. In Proceedings of the Confederated Intern a tional 
Conferences, CoopIS, DOA, IS, and ODBASE 2009 on On the Move to Meaningful Internet Systems: 
Part II (OTM '09), Robert Meersman, Tharam Di l-lon, and Pilar Herrero (Eds.). Springer -Verlag, Be r-lin, Heidelberg, 983 -991. 
Zolotova G. Syntactic Dictionary . The repertoire of elementary units of Russian syntax .  X  X  X  X  X  X  X  X  X  
 X  X  X  X , 2011, P .356 . La structure  X  enum  X  erative (dor  X  enavant appel  X  ee SE) est une structure textuelle ayant la propri  X  et  X  e d X  X xprimer des connaissances hi  X  erarchiques au travers de diff  X  erents composants. Elle pr  X  esente, au sein d X  X n m  X  eme objet textuel, un th ` eme  X  enum  X  eratif, dit  X  enum  X  erath ` eme , justifiant la r  X  eunion de plusieurs  X  el  X  ements en fonction d X  X ne identit  X  e de statut (Ho-Dac et al., 2010). Sur le plan s  X  emantique elle forme un tout. Sur le plan de la mise en forme, elle peut  X  etre exprim  X  ee selon diff  X  erents modes, allant d X  X ne forme lin  X  eaire discursive ` a une forme visuelle usant de dispositifs typo-dispositionnels. Ces propri  X  et  X  es autorisent son apparition dans tout type de texte, lui permettant par l ` a m  X  eme de rendre compte de connaissances de nature diff  X  erente.

Elle a ainsi fait l X  X bjet de nombreuses  X  etudes au cours desquelles diff  X  erentes typologies ont pu tiellement analys  X  ees dans le cadre de l X  X nalyse du discours. Elles ont d X  X bord donn  X  e lieu ` a des typologies comme celle de (Vergez-Couret et al., aux SE ` a deux temps, ou encore comme celle de (Ho-Dac et al., 2010) o ` u les SE ont  X  et  X  e classifi  X  ees selon leur niveau de granularit  X  e (SE dont les items sont des titres, SE en tant que listes format  X  ees, SE multi-paragraphiques sans marque visuelle, SE intra-paragraphiques). Les SE usant de dispositifs typo-dispositionnels, dites verticales, ont quant ` a elles  X  et  X  e notamment analys  X  ees dans le cadre de la g  X  en  X  eration de texte. Hovy et Arens (1991) distinguent les listes d X  X tems (ensemble de com-posants de m  X  eme niveau), des listes  X  enum  X  er  X  ees (pour lesquelles l X  X rdre des composants est pris en compte), alors que Luc (2001) propose une typologie qui oppose les SE parall ` eles aux SE non parall ` eles. Cette derni ` ere typologie est bas  X  ee sur la composition du mod ` ele rh  X  etorique de la
RST 1 (Mann and Thompson, 1988) et du MAT 2 de Virbel (1989). `
A notre connaissance, les SE n X  X nt pas s  X  emantiques ` a partir de textes. Or ces SE sont tr ` es fr  X  equentes dans les textes scientifiques ou encyclop  X  ediques qui sont justement appropri  X  es pour la construction de ressources s  X  emantiques.
Les m  X  ethodes classiques d X  X xtraction des relations sont le plus souvent limit  X  ees ` a l X  X dentification de relations binaires intra-phrastiques, apr ` es analyse du texte r  X  edig  X  e par des patrons lexico-syntaxiques (Hearst, 1992; Montiel-Ponsoda and de Cea, 2011; Aussenac-Gilles and Jacques, 2008), des techniques de clusterisation ou des algorithmes d X  X pprentissage automatique (es-sentiellement non supervis  X  e) (Buitelaar et al., 2005; Poelmans et al., 2010). L X  X xploitation des SE apparait alors comme un moyen d X   X  elargir les m  X  ethodes classiques d X  X xtraction de relations pour la construction ou l X  X nrichissement de ressources s  X  emantiques telles que les ontologies, les Ressources Termino-Ontologiques (RTO), les thesaurus, etc.

Cet article propose une typologie multi-dimensionnelle qui permettra de cibler puis d X  X xploiter automatiquement les SE porteuses de relations termino-ontologiques. Cette ty-pologie caract  X  erise les SE selon les axes visuel et rh  X  etorique ` a l X  X nstar de (Luc, 2001), mais  X  egalement selon les axes intentionnel et s  X  emantique. C X  X st cette typologie que nous en section 2 quelques d  X  efinitions et propri  X  et  X  es des SE. Vu l X  X nad  X  equation des outils classiques d X  X xtraction de relations pour ce genre de struc-ture textuelle, nous envisageons une approche alternative, ` a base d X  X pprentissage supervis  X  e, n  X  ecessitant une campagne d X  X nnotation bas  X  ee sur cette typologie. La section 4 montre com-ment cette typologie intervient dans le cadre du processus d X  X nnotation, et d  X  ecrit sommairement l X  X util d X  X nnotation d  X  evelopp  X  e pour ces besoins. Nous concluons et pr  X  esentons nos perspectives en section 5. Comme indiqu  X  e pr  X  ec  X  edemment, l X  X cte successifs d X  X n m  X  eme champ conceptuel, ces  X  el  X  ements entretenant un lien hi  X  erarchique direct ou indirect avec un concept classifieur. La par la pr  X  esence d X  X ne amorce (phrase contenant d X  X ne  X  enum  X  eration compos  X  ee d X  X u moins deux items (appartenant au m  X  eme champ conceptuel), et  X  eventuellement d X  X ne cl  X  oture (ou conclusion).
D X  X n point de vue visuel, la SE a la pro-fac  X ons. Elle peut  X  etre  X  enonc  X  ee discursivement en dehors de toute MFM, au sein de la m  X  eme phrase ou ` a travers plusieurs phrases n X  X ppartenant pas n  X  ecessairement au m  X  eme paragraphe. Elle peut  X  egalement  X  etre mise en  X  evidence par l X  X sage de marqueurs typographiques et/ou dispositionnels, marqueurs qui pallient alors les marqueurs lexicaux. Ces marqueurs sont de l X  X rdre de la m  X  etalangue (Harris, 1976; Porhiel, 2007) et permettent alors d X  X rganiser des segments de texte successifs non forc  X  ement contigus.

Diff  X  erentes d  X  efinitions de la SE existent, dont celle de Pascual pour qui  X   X  enum  X  erer, c X  X st conf  X  erer une  X  egalit  X  e d X  X mportance ` a un ensemble d X  X bjets, et ensuite c X  X st ordonner ces objets selon des crit ` eres vari  X  es X  (Pascual, 1991). Ces objets sont consid  X  er  X  es comme visuellement et fonctionnellement  X  equivalents. On parle alors de SE parall ` eles.

D X  X n point de vue rh  X  etorique, l X  X nalyse des SE montre qu X  X l existe des relations de discours entre les diff  X  erents composants. La d  X  efinition de Pascual cit  X  ee ci-dessus correspond au cas o ` u ces relations montrent une  X  egalit  X  e d X  X mportance entre les items. Or des  X  etudes de corpus ont montr  X  e que les SE ne pr  X  esentent pas toutes cette  X  equivalence visuelle et fonctionnelle entre items (Luc, 2001).

Dans un souci de g  X  en  X  eralisation, nous pr  X  ef  X  erons la d  X  efinition propos  X  ee par (Virbel, 1999) qui nous semble mieux prendre en compte ` a la fois les ph  X  enom ` enes architecturaux du texte et l X  X ntention de l X  X uteur :  X  X  X  X cte textuel consiste segments linguistiques qui les d  X  ecrivent, ceux-ci devenant par le fait les entit  X  es constitutives de l X   X  enum  X  eration (les items). X 
D X  X n point de vue intentionnel, ` a l X  X mage des textes qui peuvent  X  etre de diff  X  erents types (narratifs, proc  X  eduraux, descriptifs, etc.), les SE refl ` etent l X  X ntention de l X  X uteur. Nous proposons de reprendre cette typologie des textes pour caract  X  eriser l X  X ntention de l X  X uteur lorsqu X  X l r  X  edige une SE.
 Enfin, d X  X n point de vue s  X  emantique, les
SE peuvent exprimer des connaissances de nature diff  X  erente. Ces connaissances peuvent d  X  ecrire de fac  X on consensuelle ou conjoncturelle le monde r  X  eel ou imaginaire, la langue, les  X  emotions, les sentiments, les opinions, etc. La typologie que nous proposons est bas  X  ee sur s X  X ppuie sur les dimensions visuelle, rh  X  etorique, intentionnelle et s  X  emantique, l X  X bjectif  X  etant ` a terme de rep  X  erer et d X  X xploiter les SE paradigma-tiques b  X  en  X  eficiant de mise en forme et v  X  ehiculant des connaissances propices ` a la construction de ressources s  X  emantiques.

Les diff  X  erentes caract  X  eristiques observ  X  ees au sein de chacune des dimensions sont illustr  X  ees par des exemples extraits du corpus de Virbel (1999) et d X  X n corpus compos  X  e de pages Wikip  X  edia, ce d X  X nrichir l X  X ntologie OntoTopo construite lors du projet GEONTO 3 (Kamel and Rothenburger, 2011). 3.1 Typologie selon l X  X xe visuel Les types d  X  efinis dans cet axe ont pour but d X  X ider au rep  X  erage des SE. Nous distinguons la SE horizontale qui peut b  X  en  X  eficier ou non de mise en forme typographique, de la SE verticale qui b  X  en  X  eficie de mise en forme typographique et dispositionnelle.

La SE horizontale s X  X nscrit dans la lin  X  earit  X  e du texte et ne fait pas usage du  X  X ispositionnel X . Elle est caract  X  eris  X  ee soit par des MIL 4 comme  X  X remi ` erement X ,  X  X euxi ` emement X ,  X  X  X  X bord X ,  X  X nsuite X , etc. qui permettent d X  X ntroduire les items (fig. 3.a), soit par des marqueurs lex-icaux comme  X  X els que X ,  X  X omme X , etc. qui permettent d X  X ntroduire l X   X  enum  X  eration (fig. 3.b). Mais elle peut aussi faire usage de marqueurs typographiques pour d  X  elimiter l X   X  enum  X  eration, comme les parenth ` eses dans (fig. 3.c).

La SE verticale pr  X  esente des discontinuit  X  es par rapport ` a la lin  X  earit  X  e du texte. Des marqueurs typo-dispositionnels sont alors utilis  X  es pour organiser, subdiviser et hi  X  erarchiser les diff  X  erents composants de la SE, comme le montre (fig. 3.d). Les items apparaissent en retrait par rapport ` a l X  X morce, les items sont introduits par des puces, des tirets, etc.

SE verticales et horizontales peuvent  X  etre combin  X  ees et imbriqu  X  ees au sein d X  X ne m  X  eme
SE. C X  X st le cas lorsqu X  X n item d  X  ecrit lui-m  X  eme une SE, avec ou sans mise en forme typo-dispositionnelle (fig. 3.e). 3.2 Typologie selon l X  X xe rh  X  etorique `
A ce niveau nous prenons en compte la nature des relations du discours qui relient les diff  X  erents com-posants de la SE. Les relations entre items peuvent  X  etre de type noyau-satellite ou multi-nucl  X  eaire, selon la RST (Mann and Thompson, 1988). Une relation noyau-satellite relie une unit  X  e du discours plus saillante ` a une unit  X  e du discours qui sup-porte l X  X nformation d X  X rri ` ere-plan, alors qu X  X ne re-lation multi-nucl  X  eaire relie des unit  X  es du discours de m  X  eme importance. Les SE, dont les items mon-trent une  X  egalit  X  e d X  X mportance, suscitent pour nous un int  X  er  X  et particulier, car leur traduction en struc-tures hi  X  erarchiques est assez imm  X  ediate.
Nous distinguons alors les SE paradigma-tiques , les SE syntagmatiques , les SE hybrides et les SE bivalentes , reprenant ainsi en partie la terminologie utilis  X  ee par Luc (2001).
La SE paradigmatique est compos  X  ee d X  X tems ind  X  ependants dans un contexte donn  X  e. Elle porte alors une relation rh  X  etorique multi-nucl  X  eaire entre les items successifs, chacun des items  X  etant li  X  e ` a l X  X morce par une m  X  eme relation de type noyau-satellite (fig. 1.a). Les exemples (a), (b), (c), entre autres, de la fig. 3 sont des cas de SE paradig-matiques. ` A l X  X ppos  X  e, la SE syntagmatique est compos  X  ee d X  X tems qui n X  X nt pas la m  X  eme impor-tance, et qui ne sont donc pas ind  X  ependants. La SE syntagmatique porte alors une relation rh  X  etorique noyau-satellite entre items successifs (fig. 1.b). Le cas (fig. 3.f) en est un exemple.

Lorsqu X  X ne SE porte une relation rh  X  etorique noyau-satellite entre au moins deux items et une relation rh  X  etorique multi-nucl  X  eaire entre au moins deux items, elle est qualifi  X  ee d X  hybride . Enfin, les caract ` eres paradigmatique et syntagmatique peu-vent coexister au sein de la m  X  eme SE, et dans ce cas la SE est dite bivalente (fig. 3.g). 3.3 Typologie selon l X  X xe intentionnel ` A ce niveau nous prenons en compte l X  X ntention de communication de l X  X uteur. Nous avons repris la typologie des textes pour l X  X dapter aux SE, en diff  X  erenciant les SE descriptives , les SE narra-tives , les SE prescriptives , les SE proc  X  edurales , les SE explicatives , et les SE argumentatives . dans nos corpus. L X  X bjectif est de caract  X  eriser les types de SE propices ` a la construction de RTO, pour ensuite proposer un mod ` ele de repr  X  esentation des connaissances adapt  X  e.

La SE descriptive d  X  ecrit une entit  X  e qui peut  X  etre un objet du monde anim  X  e ou pas, artificiel ou naturel (fig. 3.a, fig. 3.b, fig. 3.c), alors que la SE narrative articule une succession d X  X ctions ou d X   X  ev  X  enements, r  X  eels ou imaginaires (fig. 3.j). Les notions de conseil, d X  X ndication, d X  X njonction peuvent  X  etre int  X  egr  X  ees ` a ces types de SE. Dans ce cas la SE est dite prescriptive (fig. 3.i). De plus, lorsque ces conseils, indications, injonctions sont  X  enonc  X  es selon une volont  X  e d X  X rdonnancer (comme dans les modes d X  X mploi, les notices explicatives, les guides d X  X tilisation, les manuels, les recettes de cuisine, etc.), pour atteindre un but donn  X  e, la SE est dite proc  X  edurale (fig. 3.h).

Enfin, la SE explicative r  X  epond en g  X  en  X  eral ` a un questionnement de type  X  X omment ? X ,  X  X ourquoi? X ,  X  X ans quelles circonstances? X  etc. (fig. 3.f). Si des arguments sont avanc  X  es dans le but de d  X  efendre une opinion, dans le but de conva-incre, la SE est dite argumentative (fig. 3.k).
En ce qui concerne cet axe, une m  X  eme SE pourra poss  X  eder plusieurs traits intentionnels. La naisons de types intentionnels les plus fr  X  equentes.
Il existe cependant des SE pour lesquelles au-cune des cat  X  egories de l X  X xe intentionnel pr  X  ecit  X  ees n X  X  pu  X  etre identifi  X  ee. Pour les cat  X  egoriser, nous avons d  X  efini le type SE intentionnelle autre . 3.4 Typologie selon l X  X xe s  X  emantique `
A ce niveau nous rendons compte de la dimen-objectif de construction de ressources termino-ontologiques. Nous avons divis  X  e les SE en trois cat  X  egories : SE ` a vis  X  ee ontologique concerne des connaissances du monde (fig. 3.d et fig. 3.g), SE m  X  etalinguistique concerne la langue (fig. 3.l et fig. 3.m) et SE s  X  emantique autre qui regroupe les SE qui ne sont ni ` a vis  X  ee ontologique, ni m  X  etalinguistiques (fig. 3.o).

Une typologie des relations est associ  X  ee aux types s  X  emantiques  X  ` a vis  X  ee ontologique X  et  X  X   X  etalinguistique X . Les relations is-a (fig. 3.a, fig 3.b, fig. 3.c), part-of (fig. 3.d, fig. 3.g), instance-of (fig. 3.n), ontologique autre (relation ontologique transverse ou d X  X ctance) (fig. 3.i) sont associ  X  ees aux SE ` a vis  X  ee ontologique.
Les relations d X  hyperonymie , de m  X  eronymie , d X  homonymie (fig. 3.m), de synonymie , de multilinguisme (fig. 3.l), lexicale autre (re-lation lexicale moins fr  X  equente d  X  ecrivant la langue, telle que la paronymie qui associe deux mots ` a la graphie/prononciation proches mais aux sens diff  X  erents) sont associ  X  ees aux SE m  X  etalinguistiques.

De fac  X on orthogonale, les connaissances port  X  ees par la SE peuvent  X  etre contextualis  X  ees dans l X  X space (fig. 3.j, fig. 3.n), dans le temps, ou dans tout autre dimension (fig. 3.m), ` a l X  X ide de circonstants. L X  X nnotation de ces derniers permet d X  X nvisager l X  X dentification de relations autres que binaires. Nous distinguons les SE contextuelles des SE non contextuelles . La typologie d  X  ecrite ouvre la voie ` a une car-act  X  erisation plus fine des SE. Corollaire de cette possibilit  X  e, elle offre une latitude plus large pour la discrimination des classes lors d X  X n apprentis-sage supervis  X  e pour l X  X dentification des relations que portent les SE (Fauconnier et al., 2013).
Afin d X   X  eprouver cette typologie de mani ` ere empirique, nous avons d  X  ebut  X  e une campagne d X  X nnotation avec trois annotateurs. La t  X  ache d X  X nnotation elle-m  X  eme se d  X  eroule en trois phases principales qui consistent ` a : (1) d  X  elimiter les diff  X  erents composants de la SE (amorce, items, cl  X  oture) lorsqu X  X lle b  X  en  X  eficie de mise en forme. (2) annoter la SE selon les crit ` eres rh  X  etoriques, intentionnels et s  X  emantiques d  X  efinis ci-dessus. Chaque SE se voit affecter un type rh  X  etorique, un ou plusieurs types intentionnels, un type s  X  emantique. Lorsque la SE est paradigmatique, ` a vis  X  ee ontologique ou m  X  etalinguistique, un type de relation est associ  X  e au type s  X  emantique ( associ  X  ee ou non ` a un contexte). (3) d  X  elimiter, lorsque la SE est paradigmatique et ` a vis  X  ee ontologique ou m  X  etalinguistique, les dans l X  X morce, le concept pr  X  esent dans chacun des items, le circonstant (lorsqu X  X l existe) et la relation entre l X  X morce et chacun des items.

Pour  X  etre men  X  ee ` a bien, cette t  X  ache d X  X nnotation multi-dimensionnelle des SE, cas moins courant en TAL o ` u l X  X n privil  X  egie habituellement des annotations simple label. De plus, il  X  etait aussi indispensable que cet outil supporte le caract ` ere imbriqu  X  e et potentiellement r  X  ecursif des SE. Par exemple, une SE peut contenir d X  X utres SE et elle-m  X  eme  X  etre imbriqu  X  ee au sein d X  X ne structure discursive plus large (e.g : citation) ou  X  etre  X  etal  X  ee sur plusieurs d X  X ntre elles (e.g : un titre et plusieurs paragraphes). Enfin, cet outil devait  X  etre modulable pour  X  etre facilement adapt  X  e ` a d X  X utres types d X  X bjets avec mise en forme (e.g :  X  enonc  X  es d  X  efinitoires, d  X  emonstrations math  X  ematiques, etc.) et plusieurs types de format d X  X ntr  X  ee (e.g : HTML, PDF, etc.).

Les outils d X  X nnotation tels que MMAX2 (M  X  uller and Strube, 2006), MAE (Stubbs, 2011) ou encore Glozz (Widl  X  ocher and Mathet, 2009) ne r  X  epondent pas ou partiellement ` a ces exigences.
MMAX2 et MAE prennent du texte brut en entr  X  ee et ne gardent pas la mise en forme originelle des textes. Glozz, initialement conc  X u pour l X  X nnotation de relations discursives, supporte la mise en forme du texte mais n X  X st, en l X   X  etat, pas adapt  X  e pour une annotation rapide et ergonomique d X  X bjets multi-labels. En outre, la possibilit  X  e de faire  X  evoluer le code source de Glozz n X  X st pas assur  X  ee (licence restrictive).
 Pour toutes ces raisons, nous avons d  X  evelopp  X  e
LARAt (Logiciel d X  X cquisition de Relations par l X  X nnotation de textes 5 ), prononc  X  e /laKa/ . Cet outil Java se veut portable, et open-source.
Dans son  X  etat actuel, LARAt prend en entr  X  ee des fichiers HTML ou XML respectant la norme
TEI , les affiche en respectant leur mise en forme et permet aux annotateurs d X  X nnoter des objets textuels imbriqu  X  es ou  X  eclat  X  es sur plusieurs niveaux textuels (e.g : titres et sous-titres).
Dans la t  X  ache d X  X nnotation des SE, deux types d X  X nnotation sont produits (type 1 et type 2). Les annotations de type 1 concernent ex-clusivement le rep  X  erage en document des SE. avec des annotations de type 2 qui reprennent
Ainsi, ` a chaque annotation de type 1 est associ  X  ee une ou plusieurs annotations de type 2. Cette mani ` ere modulaire de g  X  erer l X  X nnotation facilite les post-traitements et l X  X mploi sp  X  ecialis  X  e de ces derni ` eres (e.g :  X  etude d X  X n ph  X  enom ` ene particulier, recherche d X  X n cas pr  X  ecis pour exemplifier un emploi, etc.). `
A terme, cet outil sera amen  X  e ` a supporter le PDF ainsi que le post-traitement des annota-tions (alignement, Kappa de Cohen et Fleiss pour l X  X ccord inter-annotateurs). `
A noter qu X  X n guide d X  X nnotation accompagne cette campagne d X  X nnotation. Sa r  X  edaction se d  X  eroule de mani ` ere it  X  erative en prenant en compte les retours des annotateurs et les cas ambigus qui posent question. Au terme de la campagne, le corpus annot  X  e, le guide ainsi que LARAt seront distribu  X  es sous licence libre. L X  X nalyse que nous avons men  X  ee sur les SE a permis de d  X  efinir une typologie multi-dimensionnelle, permettant de tenir compte orthogonales. Le but th  X  eorique de ce travail a quant ` a sa forme, sa structure ou sa fonction. D X  X n point de vue pratique, ce travail nous permet d X  X ne part d X  X m  X  eliorer le rep  X  erage des SE dans les textes et, d X  X utre part d X  X dentifier la ou les relations s  X  emantiques qui relient les concepts contenus dans la SE. ` A cet  X  egard, nous avons d  X  evelopp  X  e l X  X util d X  X nnotation LARAt qui permet de cat  X  egoriser les SE extraites de textes suivant les diff  X  erents axes de notre typologie. Une premi ` ere campagne d X  X nnotation ` a l X  X ide de cet outil est en cours. La principale perspective de poursuite de ce travail est son extension ` a d X  X utres objets textuels ayant un impact sur la s  X  emantique des textes tels que la titraille et les  X  enonc  X  es d  X  efinitoires.
 Langues Naturelles (TALN 2009) . 143 144
Tout processus de r  X  ealisation d X  X n syst ` eme re-pose sur une phase de sp  X  ecification des exigences. La v  X  erification de la correction et la consistance de ces sp  X  ecifications n  X  ecessitent l X  X tilisation de m  X  ethodes formelles qui peuvent  X  etre appliqu  X  ees uniquement sur des sp  X  ecifications formelles.
Or, en pratique, les sp  X  ecifications d X  X xigences sont le plus souvent r  X  edig  X  ees en langage na-turel (LN) (Mich et al. , 2004) et sont donc non formelles. Leur v  X  erification n  X  ecessite alors de les transformer en sp  X  ecifications formelles. Se pose ainsi naturellement la question de l X  X u-tomatisation du passage entre sp  X  ecifications LN et sp  X  ecifications formelles. Cette probl  X  ematique approches (Foug ` eres et Trigano, 1997; Ilic, 2007;
Ilieva et Boley, 2008; Kof, 2010; Bajwa et al. , 2012; Guiss  X  e et al. , ). L X  X nsemble des ap-proches pointe la difficult  X  e d X  X ne transformation
LN et sp  X  ecifications formelles. Les mod ` eles in-g  X  en  X  eral semi-formels, comme UML ou SBVR.
Dans le cadre du projet ENVIE VERTE 1 dans lequel se place le travail d  X  ecrit dans cet aricle, nous avons choisi de mod  X  eliser les connaissances du domaine par une ontologie en OWL-DL, une version d  X  ecidable d X  X WL2 et de repr  X  esenter for-mellement les sp  X  ecifications par son peuplement.
Une ontologie mod  X  elise les concepts et leurs propri  X  et  X  es, d  X  efinissant ainsi le vocabulaire concep-tuel d X  X n domaine. Un concept est la description d X  X n ensemble d X  X ndividus (d X  X nstances) ayant une s  X  emantique et des propri  X  et  X  es communes. Une instance de concept est une concr  X  etisation d X  X n concept. Par exemple, dans notre mod  X  elisation du domaine kitchen correspond ` a une ins-tance du concept Location 2 . Une propri  X  et  X  e est d  X  efinie entre deux concepts, i.e. une re-lation, ou entre un concept et un type de donn  X  ee, i.e un attribut. Une instance de pro-pri  X  et  X  e relie donc deux instances de concepts, par exemple Occured-in(movement,kitchen) , ou une instance de concept et une valeur d X  X ttribut, par exemple Has-value (temperature,25). Une ontolo-gie offre un cadre formel permettant d X  X ssocier une s  X  emantique aux termes issus des textes. L X  X s-sociation des instances de concepts et propri  X  et  X  es ` a leurs formulations dans les textes est r  X  egie ` a l X  X ide d X  X ne ontologie lexicale en SKOS (Simple Know-ledge Organization System) contenant la termino-logie li  X  ee au vocabulaire conceptuel.

Peupler une ontologie consiste ` a y ajouter de nouvelles instances sans en changer la structure conceptuelle (Petasis et al. , 2011). Ces nou-velles instances sont associ  X  ees aux concepts et propri  X  et  X  es reconnus dans les textes. L X  X dentifica-tion peut  X  etre centr  X  ee sur la reconnaissance d X  X ns-tances de concepts (Thongkrau et Lalitrojwong, 2012), ou sur la reconnaissance d X  X nstances de re-lation (Nakamura-Delloye et Stern, 2011).

Dans cet article, nous proposons de guider le peuplement de l X  X ntologie par l X  X dentification de triplets de termes dans les textes correspondant ` a des instances de propri  X  et  X  es. Nous distinguons deux types de triplets : complets et partiels. Les tri-plets complets contiennent la mention d X  X ne ins-tance de propri  X  et  X  e ainsi que les mentions des deux instances de concepts qu X  X lle lie. En revanche, les triplets partiels ont pour vocation de reconna  X   X tre des propri  X  et  X  es pour lesquelles une des deux ins-tances n X  X st pas explicitement mentionn  X  ee. Gui-der le peuplement de l X  X ntologie par identification d X  X nstances de propri  X  et  X  es permet de r  X  esoudre des cas d X  X mbigu  X   X t  X  e de termes et d X  X nformations im-plicites pour lesquels les concepts associ  X  es sont trouv  X  es par inf  X  erence dans l X  X ntologie ` a partir des instances de propri  X  et  X  es. De la sorte, l X  X dentifica-tion d X  X nstances de concepts ne repose pas seule-ment sur la reconnaissance de leurs mentions dans les textes mais aussi sur les propri  X  et  X  es concep-tuelles auxquelles elles sont associ  X  ees.

Les triplets correspondant aux instances de pro-par amorc  X age ` a partir d X  X n corpus d X  X pprentis-sage et d X  X ne terminologie de d  X  epart. Ces r ` egles correspondent ` a des formes lexico-syntaxiques r  X  ecurrentes. Nous montrons que cette approche permet d X  X dentifier de mani ` ere fiable des instances de propri  X  et  X  es, et dans un deuxi ` eme temps d X  X nf  X  erer les instances de concepts qui leurs sont associ  X  ees. De plus, l X  X pproche que nous proposons peut s X  X dapter ais  X  ement ` a d X  X utres domaines d X  X pplica-tion d ` es lors que l X  X n peut le d  X  ecrire par une onto-logie.
Le peuplement d X  X ntologie consiste ` a identifier et ` a classer les instances extraites des textes. Se pose le probl ` eme de la reconnaissance de men-tions d X  X nstances de concepts et de propri  X  et  X  es dans les textes. L X  X ypoth ` ese g  X  en  X  eralement faite est que les paires d X  X ntit  X  es apparaissant dans un m  X  eme contexte peuvent  X  etre consid  X  er  X  ees comme des instances de la m  X  eme relation. La d  X  efinition du contexte peut  X  etre restreinte par la pr  X  esence d X  X n verbe et la reconnaissance de son entourage (Lin et Pantel, 2001; Makki et al. , 2008). D X  X utres tra-vaux se fondent sur la classification entre couples d X  X ntit  X  es connues pour  X  etre li  X  ees par une relation s  X  emantique (Hasegawa et al. , 2004; Nakamura-
Delloye et Stern, 2011; Thongkrau et Lalitro-jwong, 2012). La majorit  X  e des approches ex-ploitent des connaissances lexicales et syntaxiques pour la d  X  efinition d X  X n contexte repr  X  esentant une relation s  X  emantique. Cependant, dans (IJntema et al. , 2012), les auteurs avancent que les patrons lexico-s  X  emantiques sont plus ` a m  X  eme de captu-rer dans les textes le contexte s  X  emantique. Alors qu X  X ls proposent un langage d X   X  ecriture manuelle de r ` egles, nous proposons d X  X cqu  X  erir ce type de r ` egle automatiquement, ` a partir des trois niveaux de connaissances lexicale, syntaxique et s  X  emantique.
L X  X nsemble des approches identifient les ins-tances de concept au niveau du texte. La m  X  ethode que nous proposons tire partie des connaissances s  X  emantiques pour inf  X  erer au sein de l X  X ntologie l X  X ppartenance d X  X ne instance ` a un concept. De plus, nous proposons d X  X ller plus loin que la clas-sification d X  X nstances en identifiant dans les textes les mentions d  X  enotant un m  X  eme individu.
Dans la suite de l X  X rticle nous emploierons les notations suivantes. Les noms de concept et de propri  X  et  X  e sont en italique et commencent par une majuscule. Les noms d X  X nstances de concept sont en italique et commencent par une minuscule. Un concept sera not  X  e C i et les instances de concept i . Les propri  X  et  X  es sont d  X  efinies sur un domaine et une image 3 . Les propri  X  et  X  es entre concepts sont sont not  X  ees P k ( i C plet contenant la mention d X  X ne instance de pro-pri  X  et  X  e, t P , entre deux instances de concepts est not  X  ee ( t P , t C d  X  enotent respectivement les instances du concept C i et du concept C j .
Dans les textes, les instances de concepts et de propri  X  et  X  es sont d  X  enot  X  ees par des termes. Gui-der l X  X dentification d X  X nstances de concepts ` a par-conna  X   X tre des triplets ( t P , t C r ` egle d X  X xtraction a comme objectif de reconna  X   X tre la mention d X  X ne propri  X  et  X  e de l X  X ntologie et d X  X x-traire du texte les triplets de termes qui la d  X  enotent. Deux types de triplets sont ` a reconna  X   X tre :  X  triplet complet : les mentions d X  X nstances des  X  triplet partiel : l X  X ne des mentions d X  X nstances Par exemple ` a partir de la phrase : when a person moves into the kitchen, switch on the light. , on peut identifier le triplet ( Occured -in, move, kitchen ) qui d  X  enote une instance de la propri  X  et  X  e Occured in liant une instance du concept P henomenon ` a une insatnce du concept Location . N  X  eanmoins dans cette m  X  eme phrase, l X  X gent qui doit allumer tifier dans ce cas est ( T urn -on 4 , ? A, light ) qui d  X  enote une instance de la propri  X  et  X  e T urn -on une instance non mentionn  X  ee du concept Actuator ` a une instance du concept Physical-process . 4.1 M  X  ethode d X  X cquisition des r ` egles
L X  X cquisition des r ` egles d X  X xtraction se fait au-tomatiquement ` a partir d X  X n corpus d X  X pprentis-sage et d X  X ne termino-ontologie amorce. Elles sont acquises ` a partir des chemins syntaxiques les plus fr  X  equents entre des paires de termes. Ces termes sont issus de classes s  X  emantiques connues pour  X  etre li  X  ees dans l X  X ntologie. L X  X xtraction des deux types de triplets n  X  ecessite l X  X cquisition de deux types de r ` egles d X  X xtraction.
Dans le cas de triplets partiels, les r ` egles sont acquises ` a partir des chemins les plus fr  X  equents entre les termes qui d  X  enotent les instances d X  X ne propri  X  et  X  e et les instances de son domaine ou de son image. Pour les triplets complets, les chemins sont constitu  X  es des deux chemins partiels entre paires de termes d  X  enotant les instances de domaine et
L X  X lgorithme 1 d  X  ecrit le processus d X  X cquisi-tion des r ` egles. Chaque phrase du corpus est ana-lys  X  ee et son arbre de d  X  ependances syntaxiques est engendr  X  e. Puis trois fonctions sont appel  X  ees pour l X  X xtraction de chemins syntaxiques entre termes. Elles permettent d X  X dentifier des chemins liant trois types d X  X nsemble de termes : les tri-plets de termes d X  X ne propri  X  et  X  e, de son domaine et de son image, les paires de terme d X  X ne pro-pri  X  et  X  e et de son image et les paires de terme d X  X ne propri  X  et  X  e et de son domaine. Les chemins extraits sont compar  X  es en fonction de leurs d  X  ependances et des formes lemmatis  X  ees des termes. Les chemins identiques les plus fr  X  equents pour chaque type de paires sont retourn  X  es. Enfin, les r ` egles d X  X xtraction mins retourn  X  es (cf. section 4.3). 4.2 Chemin syntaxique
L X  X nalyse syntaxique des phrases permet d X  X n extraire des arbres de d  X  ependances syntaxiques cf. figure 1. Chaque n X ud est  X  etiquet  X  e par un terme et sa cat  X  egorie morpho-syntaxique (nom, verbe, etc). Les n X uds sont reli  X  es deux ` a deux par des d  X  ependances syntaxiques qui constituent des liens orient  X  es. Un chemin syntaxique est compos  X  e des d  X  ependances syntaxiques liant deux termes dans un arbre de d  X  ependances. La figure 1 repr  X  esente l X  X rbre de d  X  ependances de la phrase  X  X hen a per-son moves into the kitchen, switch on the light. X  5
Par exemple, le chemin syntaxique entre le terme moves d  X  enotant une instance du concept
Phenomenon et le terme kitchen d  X  enotant une ins-tance du concept Location est prep ( moves, into ) -pobj ( into, kitchen ) (chemin en gras, figure 1). 4.3 G  X  en  X  eration des r ` egles d X  X xtraction
Les r ` egles d X  X xtraction ont pour r  X  ole d X  X den-tifier des instances de propri  X  et  X  es. Ainsi, elles mod  X  elisent le contexte s  X  emantique dans lequel les termes d  X  esignant des instances de concepts appa-raissent. La g  X  en  X  eration de r ` egles d X  X xtraction est automatique. Elle exploite les caract  X  eristiques is-sues des n chemins syntaxiques identiques les plus fr  X  equents. Ces caract  X  eristiques sont de trois types :  X  d  X  ependances syntaxiques ;  X  cat  X  egorie termino-ontologique (s  X  emantique) ;  X  cat  X  egorie morpho-syntaxique.
 Donn  X  ees : corpus, ontologie, skos P aths  X  X  X  ; Variables : Ensemble de termes T pour chaque phrase S du corpus faire fin
Chemins  X  getM ostF requent ( P aths ) ; pour chaque ch de Chemins faire fin Algorithm 1: Acquisition des r ` egles d X  X xtraction
Exemple : l X  X n des chemins r  X  ecurrents entre les deux ensembles de termes des concepts Phe-nomenon et Location qui sont dans l X  X rdre le do-maine et l X  X mage de la propri  X  et  X  e Occured-in est prep ( t P , t O )  X  pobj ( t O , t L ) , avec :  X  les d  X  ependances syntaxiques du chemin  X  prep ( t P , t O )  X  pobj ( t O , t L )  X  les cat  X  egories morpho-syntaxiques  X  t P est un verbe ;  X  t L est un nom ;  X  t O est une pr  X  eposition ;  X  les cat  X  egories termino-ontologiques  X  t P d  X  enote le concept Phenomenon ;  X  t O d  X  enote la propri  X  et  X  e Occured-in ;  X  t L d  X  enote le concept Location ;
Les d  X  ependances syntaxiques sont trans-syntaxiques et termino-ontologiques sont transform  X  ees en contraintes. Cela permet d X  X n-Occured -in ( P henomenon, Location ) avec T sembles de termes issus de l X  X ntologie lexicale : prep ( t P , t O )  X  pobj ( t O , t L )  X  isP rep ( t O )  X  isV erb ( t P )  X  isN oun ( t L )  X  T Location ( t L )  X  T in ( t
La terminologie est repr  X  esent  X  ee en SKOS. Ce formalisme permet de d  X  efinir pour chaque terme sa formulation pr  X  ef  X  er  X  ee ainsi que sa liste de for-mulations synonymes.

Afin d X  X ugmenter la terminologie amorce, nous appliquons les r ` egles acquises pour extraire de nouveaux termes sur le corpus d X  X pprentissage. Chacune des r ` egles engendre trois applications.
Chaque application a comme objectif l X  X xtrac-tion de termes d  X  enotant un ensemble s  X  emantique de termes. R peut alors  X  etre consid  X  er  X  ee comme une fonction d  X  efinie comme suit : La pertinence d X  X n terme t extrait par des r ` egles
R est calcul  X  ee selon la formule suivante : P ertinence ( t ) = ( avec f req sa fr  X  equence, et n le nombre de r ` egles du m  X  eme type ` a partir desquelles il est ex-trait. Cette formule permet de favoriser les termes extraits par plusieurs r ` egles.
Suite ` a l X  X dentification des instances de pro-pri  X  et  X  e, vient la phase de classification des ins-tances de concepts. ` A ce stade, les classes ne sont pas encore connues. La classification de ces instances est fond  X  ee sur le raisonnement per-mis par OWL sur leurs propri  X  et  X  es. De cette fac  X on, les mentions d X  X nstances implicites dans les textes par les instances de concept. Classer les instances dans l X  X ntologie ne suffit pas pour repr  X  esenter de mani ` ere coh  X  erente les connaissances issues des textes. Pour cela, il faut aussi  X  etre en mesure d X  X dentifier de mani ` ere univoque chaque individu. 6.1 Classification des instances
La classification des individus consiste ` a les associer aux concepts qui les d  X  enotent. Nous pr  X  esentons dans cette section comment diff  X  erents m  X  ecanismes d X  X nf  X  erence de OWL peuvent  X  etre ex-ploit  X  es ` a cette fin. 6.1.1 Domaine et Image des propri  X  et  X  es
Lors du raisonnement sur les instances de l X  X n-tologie, chaque instance qui participe ` a une pro-maine ou de l X  X mage de la propri  X  et  X  e 6 de la mani ` ere suivante : Soit P 1 une propri  X  et  X  e avec comme do-maine D 1 et comme image I 2 et soit i 1 et i 2 deux instances de concepts. Alors si P 1 ( i 1 , i 2 ) on d  X  eduit : i 1  X  D 1 et i 2  X  I 2 6.1.2 Condition N  X  ecessaire et Suffisante
OWL permet de d  X  efinir des  X  equivalences entre les concepts et certaines de leurs propri  X  et  X  es, for-mant des conditions n  X  ecessaires et suffisantes pour inf  X  erer le concept auquel appartient une instance ` a partir de ses propri  X  et  X  es. Par exemple l X  X xiome C 1  X  P 2 .C 2  X  P 3 .C 3 d  X  efinit une  X  equivalence entre le concept C 1 et ses deux propri  X  et  X  es P 2 et P 3 ayant pour images dans l X  X rdre C 2 et C 3 . Cet axiome permet l X  X nf  X  erence suivante : Soit i C i 6.2 Identification des instances
Dans les ontologies en OWL, les instances iden-meAs . L X  X nf  X  erence de SameAs exploite les pro-pri  X  et  X  es qui, ` a l X  X nstar des cl  X  es primaires que l X  X n trouve en base de donn  X  ees, permettent d X  X dentifier de mani ` ere unique les individus. Les propri  X  et  X  es repr  X  esentant une contrainte d X  X nicit  X  e peuvent  X  etre d  X  efinies de deux mani ` eres. 6.2.1 Propri  X  et  X  e fonctionnelle et inverse
Une propri  X  et  X  e fonctionnelle associe ` a chaque individu du domaine, un seul individu de l X  X mage.
Cela permet l X  X nf  X  erence suivante : Soit P n une pro-pri  X  et  X  e fonctionnelle, i i , i j et i k trois individus,
P n ( i i , i j )  X  P n ( i i , i k )  X  SameAs ( i j , i k )
La propri  X  et  X  e inverse fonctionnelle a, pour chaque individu de l X  X mage, un seul individu de domaine possible. Cela permet l X  X nf  X  erence sui-vante : Soit P n une propri  X  et  X  e inverse fonctionnelle, i , i j et i k trois individus,
P n ( i j , i i )  X  P n ( i k , i i )  X  SameAs ( i j , i k ) 6.2.2 Contraintes en SWRL
Dans certains cas, plus d X  X ne propri  X  et  X  e est n  X  ecessaire pour repr  X  esenter une contrainte d X  X ni-cit  X  e, par exemple une personne est identifi  X  ee par ses nom, pr  X  enom et date de naissance. Ce type de contrainte doit d  X  efinir les propri  X  et  X  es que deux in-dividus doivent n  X  ecessairement partager pour  X  etre inf  X  er  X  es comme semblables. Ce type de contraintes peut  X  etre d  X  efini ` a l X  X ide de r ` egles SWRL. Par exemple, la r ` egle ci-dessous  X  enonce que les deux individus i x et i y sont un m  X  eme individu s X  X ls poss ` edent des valeurs similaires ` a travers les pro-pri  X  et  X  es P 1 et P 2 ,
P ( i  X  SameAs ( i x , i y )
Dans cette section, nous pr  X  esentons le domaine d X  X pplication, les ressources utilis  X  ees, ainsi que les  X  evaluations d X  X cquisition de termes et d X  X xtraction d X  X nstances de propri  X  et  X  es. Enfin, nous donnons un exemple de cr  X  eation, de classification et d X  X dentifi-cation d X  X nstances de concept ` a partir d X  X ne phrase extraite de notre corpus de test. 149 7.1 Environnement intelligent
Un environnement intelligent est un ensemble d X  X bjets communicants (capteurs, actionneurs et processus de contr  X  ole), dont le comportement g  X  en  X  eral est d  X  ecrit ci-dessous :  X  Un capteur d  X  etecte l X  X ccurrence d X  X n type  X  Un capteur d  X  etecte ou mesure un type de  X  Un actionneur est connect  X  e ` a un appa- X  La d  X  etection d X  X n ph  X  enom ` ene peut conduire ` a  X  Un actionneur, pour  X  etre activ  X  e par un cap-
Afin de mod  X  eliser ce domaine nous avons d  X  efini une ontologie de haut niveau contenant 12 concepts, 15 propri  X  et  X  es entre concepts et 9 entre concepts et types de donn  X  ees. Cette ontologie comporte des instances initiales qui correspondent ` a un environnement physique d X  X n utilisateur, qui n X  X st pas amen  X  e ` a  X  etre modifi  X  e par celui-ci. Cette ontologie est suffisante pour repr  X  esenter notre do-maine, dans la mesure o ` u nous nous int  X  eressons au fonctionnement d X  X n r  X  eseau de capteurs, et elle peut- X  etre reprise pour diff  X  erentes configurations, car seules les instances de d  X  epart changeront (voir (Sadoun et al. , 2012) pour une justification de cette conceptualisation). L X  X nsemble des individus sont identifiables ` a partir de leurs propri  X  et  X  es de lo-calisation et de type. 7.2 Description des ressources
En l X  X bsence de corpus suffisamment grand por-tant sur la sp  X  ecification d X  X xigences dans le do-maine des environnements intelligents, nous avons constitu  X  e un corpus d X  X pprentissage d X  X nviron 5 millions de mots ` a partir de livres  X  electroniques (e-books) de domaines et styles litt  X  eraires diff  X  erents, issus de la Biblioth ` eque num  X  erique Anacleto 7 . La diversit  X  e de ce corpus permet d X  X cqu  X  erir des samment g  X  en  X  erales et transdomaines pour qu X  X n puisse partir d X  X n corpus non sp  X  ecialis  X  e pour acqu  X  erir les r ` egles d X  X xtraction. En contrepartie, logie pour les concepts sp  X  ecifiques au domaine.
Afin de disposer d X  X n corpus d X   X  evaluation, nous avons d  X  evelopp  X  e une plate-forme 8 de collecte de sp  X  ecifications. Les sp  X  ecifications collect  X  ees repr  X  esentent environ 80 phrases (1558 mots). 7.3 R  X  esultats 7.3.1 Acquisition des r ` egles d X  X xtraction
Nous avons acquis en tout 126 r ` egles d X  X xtrac-tion, dont 31 pour l X  X dentification d X  X nstances de propri  X  et  X  es compl ` etes et 95 pour l X  X dentification d X  X nstances de propri  X  et  X  es partielles. Chaque r ` egle les plus fr  X  equents. Nous avons fix  X  e ce param ` etre de fac  X on exp  X  erimentale ` a 4 . N  X  eanmoins, les men-tions de certaines propri  X  et  X  es sont moins fr  X  equentes dans les textes et ce nombre peut s X  X v  X  erer trop bas. En l X  X ugmentant, des chemins non pertinents peuvent engendrer des r ` egles. Afin d X   X  eviter ce bruit  X  eventuel, nous fixons une seconde limite correspondant au nombre de d  X  ependances syn-taxiques composant le chemin. En effet, plus un chemin est long, et donc plus les termes sont dis-tants dans la phrase, moins il y a de possibilit  X  e que
Dans nos exp  X  eriences, le nombre de d  X  ependances maximal d X  X n chemin a  X  et  X  e fix  X  e ` a 4 . 7.3.2  X  Evaluation de l X  X cquisition de termes
La terminologie amorce contient 109 termes, 18 termes pr  X  ef  X  er  X  es et 91 alternatifs. Ces termes sont mod  X  elis  X  es dans l X  X ntologie. Certains termes sont la d  X  enomination d X  X ndividus pr  X  eexistant dans l X  X n-tologie, par exemple les localisations et les diff  X  erents types reconnus par les capteurs.
Le tableau 1 illustre l X  X cquisition des termes sur trois cat  X  egories s  X  emantiques par expansion de la terminologie. Ces cat  X  egories s  X  emantiques repr  X  esentent dans l X  X rdre les ph  X  enom ` enes ` a re-conna  X   X tre, les processus physiques qui corres-pondent aux appareils connect  X  es aux r  X  eseaux de capteurs et les actions possibles 9 . Les r ` egles d X  X x-traction ont  X  et  X  e appliqu  X  ees sur le corpus d X  X ppren-tissage comme cela est d  X  ecrit en section 5. Le premi ` ere colonne exprime les termes amorces is-sus de la terminologie de d  X  epart. La seconde co-lonne le nombre de termes diff  X  erents extraits et la troisi ` eme colonne les termes pertinents.

En raison de la g  X  en  X  eralit  X  e du corpus et la sp  X  ecificit  X  e du domaine, la pr  X  ecision de l X  X xtraction est assez basse. La s  X  election des termes pertinents est r  X  ealis  X  ee manuellement ` a partir de l X  X xamen des n premiers termes retourn  X  es par la formule du cal-cul de pertinence cf. section 5, avec n fix  X  e ` a 25%. 7.3.3  X  Evaluation de l X  X xtraction des triplets
L X  X xtraction des triplets r  X  esulte de l X  X pplication sur le corpus de test des r ` egles acquises. L X  X pplica-tion des r ` egles compl ` etes est prioritaire par rapport aux r ` egles partielles. Cette extraction est effectu  X  ee en ne consid  X  erant que les termes pertinents dans la terminologie.

Les r  X  esultats de l X  X xtraction de triplets candidats sont d  X  ecrits dans le tableau 2. La premi ` ere colonne indique le nombre d X  X nstances ` a reconna  X   X tre. Les colonnes suivantes indiquent le nombre de triplets correctement identifi  X  es, et les triplets identifi  X  es pour les trois propri  X  et  X  es Located-in , Fixed-in et Occured-in qui associent une localisation ` a chacun des concepts Localisation , de Physical-process et Phenomenon . La propri  X  et  X  e Has-type associe un Type aux ph  X  enom ` enes. Les deux derni ` eres lignes repr  X  esentent les instances de concepts Phenome-non et Actuator qui sont class  X  ees et identifi  X  ees lors du raisonnement sur leurs instances de propri  X  et  X  es.
Nous observons que la pr  X  ecision est tr ` es  X  elev  X  ee (0.95). Cela montre la pertinence des r ` egles ac-quises. De plus le rappel obtenu (0.63) est relati-vement  X  elev  X  e compte tenu du fait que l X  X cquisition des r ` egles d X  X xtraction et de la terminologie s X  X st faite ` a partir d X  X n corpus non sp  X  ecifique au do-dans l X  X ntologie, 22 instances de Phenomenon et 17 instances de Actuator ont  X  et  X  e class  X  ees cor-rectement, aucune n X  X   X  et  X  e incorrectement class  X  ee.
Les instances de Phenomenon ont  X  et  X  e identifi  X  ees comme appartenant ` a 10 individus diff  X  erents. Les instances d X  Actuator ont  X  et  X  e identifi  X  ees comme ap-partenant ` a 7 individus diff  X  erents. 7.4 Application tir des triplets extraits des textes. Lors de leur cr  X  eation, les termes repr  X  esentant les domaine et image de la propri  X  et  X  e sont nomm  X  es de la mani ` ere d  X  enote un individu de l X  X ntologie alors il prend le nom de l X  X ndividu. sinon son nom est compos  X  e ` a partir du num  X  ero de la phrase dans laquelle il appara  X   X t et de son num  X  ero de n X ud dans l X  X rbre de d  X  ependances syntaxiques. Par exemple, si le terme appara  X   X t dans la phrase num  X  ero 2 et son num  X  ero de n X ud dans l X  X rbre syntaxique est 3 alors il sera nomm  X  e 2-3. Cela permet de nom-mer les instances de fac  X on unique et de mettre au m  X  eme niveau les instances de concept expli-cites ou implicites dans les textes, issues des r ` egles compl ` etes ou partielles. Ainsi l X  X nstance 2-3 qui appara  X   X t dans les propri  X  et  X  es Occured-in(2-3 , kit-chen) et Has-type(2-3 , movement) est d  X  eduite comme appartenant au concept Phenomenon de deux fac  X ons : ` a partir du domaine des propri  X  et  X  es Occured-in et Has-type d  X  efinies sur le concept
Phenomenon (cf. 6.1.1) et ` a partir de l X  X xiome d X   X  equivalence : Phenomenon  X  Has-type.Type  X 
Occured-in.Location qui d  X  efinit une contrainte n  X  ecessaire et suffisante (cf. 6.1.2) pour reconna  X   X tre une instance du concept Phenomenon . L X  X nstance est ensuite identifi  X  ee par rapport aux autres ins-tances du concept Phenomenon ` a partir de la r ` egle SWRL suivante :
Occured-in ( i P
Has-type ( i P meAs ( i P
Cette r ` egle exprime la contrainte d X  X nicit  X  e (cf. 6.2.2) inh  X  erente aux individus du concept Phe-nomenon . Lors du raisonnement elle s X  X xprime concr ` etement de la mani ` ere suivante : Occured-in (2-3 , kitchen)  X  Occured-in ( i 2 , kit-chen)  X  Has-type (2-3 , movement)  X  Has-type ( i 2 , movement)  X  SameAs (2-3, i 2 )
Nous avons pr  X  esent  X  e une approche de peu-plement d X  X ntologie visant ` a repr  X  esenter de mani ` ere formelle des connaissances issues de sp  X  ecifications d X  X xigences. Cette approche est centr  X  ee sur l X  X dentification de mentions d X  X ns-tances de propri  X  et  X  e dans les textes. Cette identi-fication est faite ` a l X  X ide de r ` egles d X  X xtraction ac-quises ` a partir des chemins syntaxiques r  X  ecurrents entre les termes d  X  enotant les instances de concept et de propri  X  et  X  e. Elles exploitent des connaissances lexicales, syntaxiques et s  X  emantiques. Ces r ` egles permettent d X  X dentifier des instances de propri  X  et  X  es m  X  eme lorsque l X  X ne des instances du domaine ou de l X  X mage est implicite. De plus, ces r ` egles per-mettent de lever l X  X mbigu  X   X t  X  e des termes extraits en capturant le contexte s  X  emantique dans lequel ils apparaissent. Lors du raisonnement au sein de l X  X ntologie, les propri  X  et  X  es permettent de clas-ser les instances de concepts et de les identifier de mani ` ere unique gr  X  ace aux contraintes d X  X ni-cit  X  e mod  X  elis  X  ees sous OWL. L X  X pproche propos  X  ee a  X  et  X  e conc  X ue pour  X  etre ind  X  ependante du domaine et s X  X dapter facilement ` a d X  X utres langues. A partir d X  X ne ontologie mod  X  elis  X  ee, elle ne n  X  ecessite qu X  X n corpus d X  X pprentissage ainsi qu X  X n ensemble de termes de d  X  epart. De plus seul le parseur utilis  X  e et l X  X nsemble de termes de d  X  epart sont d  X  ependants de la langue des textes ` a analyser.
 The field of medicine is heterogeneous because it gathers actors with various backgrounds, such as medical doctors, students, pharmacists, managers, biologists, nurses, imaging experts and of course patients. These actors have different levels of ex-pertise ranging from low (typically, the patients) up to high ( e.g. , medical doctors, pharmacists, medical students). Moreover, actors with different levels of expertise interact, but their mutual under-standing might not always be completely success-ful. This specifically applies to patients and med-ical doctors (AMA, 1999; McCray, 2005; Zeng-Treiler et al., 2007), but we assume that similar situations apply to other actors.

In this study, we propose to perform a compar-ative analysis of written medical corpora, which are differenciated according to their levels of ex-pertise. More specifically, we concentrate on the study of selected verbs used in these corpora and aim to characterize the syntactic and semantic fea-tures of their participants. Most of the partici-pants are arguments (or, in terms of Frame Se-mantics, core frame elements ). They often corre-spond to noun phrases. The description of verbs is based on the Frame Semantics framework (Fill-more, 1982). We assume that verbs are an excel-lent starting point for modeling the contents and semantics of sentences. The study is perfomed with French data. In the following, we briefly present previous work on verbs in specialized lan-guages (section 2) and on Frame Semantics (sec-tion 3). We also describe the material that we use (section 4) and the method developed to process it (section 5). We then give an account of the results (section 6), and conclude with some directions for future work (section 7).
Traditionally, the study of specialized languages focuses on nominal entities (typically, nouns and noun phrases), commonly used for the compila-tion of terminologies, ontologies, thesauri or vo-cabularies. This situation can be explained by the needs raised by specific applications ( i.e. , in-dexing or information retrieval are typically based on nominal entities), but it can also be explained by theoretical and methodological approaches that were designed for processing nominal entities.
Nevertheless, an increasing number of researchers now address the study of verbs and of their role in specialized fields. Specific methods were de-veloped in order to exploit verbs in terminologi-cal descriptions: in banking (Condamines, 1993), computer science (L X  X omme, 1998), environment (L X  X omme, 2012) and law (Lerat, 2002; Pimentel, 2011). The approaches taken by these authors dif-fer, but they all agree on the importance of supply-ing a characterization of the arguments of special-ized verbs. Notice also that TermoStat 1 (Drouin, 2003) can extract verbs from specialized corpora. Indeed, it has been demonstrated that verbs play an important role in Natural Language Processing (NLP) tasks, such as the detection of interactions between proteins or more generally in the extrac-tion of semantic relations (Godbert et al., 2007; Rupp et al., 2010; Thompson et al., 2011; Miwa et al., 2012; Roberts et al., 2008). The study of verbs we propose is based on Frame Semantics (FS) (Fillmore, 1982). This framework is increasingly used for the description of lexi-cal units in different languages, mainly in English (Gildea and Jurafsky, 2002; Atkins et al., 2003; Basili et al., 2008), but it was soon extended to other languages (Pad  X  o and Pitel, 2007; Burchardt et al., 2009; Ohara, 2009; Borin et al., 2010; Ko-eva, 2010). Until recently, French has been ne-glected with regard to this framework. In addition to the description of general language, this frame-work can be adapted to take into account data from specialized languages (Dolbey et al., 2006; Schmidt, 2009; Pimentel, 2011). Other resources include a fine-grained characterization of the se-mantics and syntax of lexical units. For instance, while focussing on verbs (as opposed to FrameNet that takes into account all  X  X rame-bearing units X ), VerbNet (Palmer, 2009) implements a description of verbs and their argument structure within a sim-ilar framework.

FS puts forward the notion of  X  X rames X , which are defined as conceptual scenarios that underlie lexical realizations in language. For instance, in
FrameNet (Ruppenhofer et al., 2006), the lexi-cal database that implements the principles of FS, the frame CURE is described as a situation that comprises specific Frame Elements (FEs), (such as
MEDICATION ), and includes lexical units (LUs) such as cure (noun and verb), alleviate, heal, healer, incurable, nurse, treat . 2 In addition to the description of the frame, FrameNet provides an-notations for LUs that evoke it (Figure 1).

According to our hypothesis, an FS-like model-ing should allow us to describe the syntactic and semantic properties of specialized verbs and, by doing so, uncover linguistic differences observed in corpora of different levels of expertise.
We use two kinds of material: corpora distin-guished by their levels of expertise (section 4.1) and semantic resources (section 4.2), that are used for the semantic annotation of corpora. 4.1 Corpora building and processing
We study four medical corpora dealing with the specific field of cardiology. These corpora are dis-tinguished according to their discoursive specifici-ties and levels of expertise (Pearson, 1998). The first three corpora are collected through the CIS-
MeF portal 3 , which indexes French language med-ical documents and assigns them categories ac-cording to the topic they deal with ( e.g. , cardiol-ogy, intensive care) and to their levels of exper-tise ( i.e. , for medical experts, medical students or patients), the forth corpus is extracted from the Doctissimo forum Hypertension Problemes Car-diaques 4 . The size of corpora in terms of occur-rences of words is indicated in Table 1.  X  C 1 or expert corpus contains expert docu- X  C 2 or student corpus contains expert docu- X  C 3 or patient corpus contains non-expert  X  C 4 or forum corpus contains non-expert doc-These corpora are used for the observation and contrastive analysis of selected verbs. C 1 / C 4 and C 2 / C 3 have comparable sizes. 4.2 Semantic resources The Snomed International terminology (C  X  ot  X  e, 1996) is structured into eleven semantic axes, which we exploit to build the resource that con-tains the following semantic categories of terms:
T : Topography or anatomical locations ( e.g. ,
S : Social status ( e.g. , mari ( husband) , soeur ( sis-
P : Procedures ( e.g. , c  X  esarienne ( caesarean) ,
L : Living organisms, such as bacteries and
J : Professional occupations ( e.g. ,  X  equipe de
F : Functions of the organism ( e.g. , pres-
D : Disorders and pathologies ( e.g. , ob  X  esit  X  e ( obe-
C : Chemical products ( e.g. , m  X  edicament ( medi-
A : Physical agents ( e.g. , proth ` eses ( prosthe-
Terms from these categories are exploited to se-mantically annotate our corpora. The only seman-tic category of Snomed that we ignore in this anal-ysis contains modifiers ( e.g. , aigu ( acute) , droit ( right) , ant  X  erieur ( anterior) ), which are meaning-ful only in combination with other terms. In rela-tion to FS, we expect these categories to be in-dicative of frame elements (FEs), while the in-dividual terms should correspond to lexical units (LUs). For instance, the Snomed category Disor-ders should allow us to discover and group under a 157 single label LUs ( e.g. , hypertension ( hypertension ) , ob  X  esit  X  e ( obesity ) ) related to the FE D ISORDER The objective is first to discover the descriptions of verbs in a way compatible with FS and then to compare them. The description of verbs depends on the recognition and annotation of noun phrases, such as those provided by the Snomed terminol-ogy, which have syntactic dependencies with these verbs. The study is automated as we rely on NLP methods. The proposed method comprises four steps (Figure 2): corpora pre-processing (section 5.1), verb selection (section 5.2), semantic annota-tion (section 5.3), and contrastive analysis of verbs (section 5.4). On the schema, the three coloured boxes show steps that require human knowledge and that are performed manually; all the other steps are carried out automatically. 5.1 Corpora pre-processing The corpora are all collected online and prop-erly formatted. They are then tokenized into sen-tences and words: we expect this may improve POS-tagging. POS-tagging is performed with the French Tree-tagger (Schmid, 1994): its output contains words assigned to parts of speech ( e.g. , verbs, nouns, adjectives) and lemmatized to their canonical forms ( e.g. , singular and masculine ad-jectival forms, infinitive verbal forms). In order to improve the results, we check the output of the POS-tagging with the Flemm tool (Namer, 2000). 5.2 Verb selection Sets of lemmatized verbs are extracted and their frequencies are computed in the four processed corpora. The verb selection process is carried out according to the following principles: 1. Removing forms that do not correspond to 2. Removing verbs which do not convey a med-3. Checking the meaning of the verbs in a med-4. Keeping those verbs with a frequency of 30
After the selection process, we obtain causer ( develop ) , doser ( dose ) and activer ( activate ) among the remaining verbs. Sentences containing the se-lected verbs are extracted from each corpus. 5.3 Semantic annotation
The sets of sentences collected at the previous step are annotated using the Ogmios platform (Hamon and Nazarenko, 2008), which integrates and com-bines several NLP tools. In addition to the syn-tactic annotation, semantic annotation is obtained after the projection of the semantic resource de-scribed in section 4.2: the categories label the par-ticipants (that are likely to correspond to FEs), while the specific terms correspond to LUs. Thus, we assume that semantic categories provided by
Snomed are useful for the description of seman-tic frames in medical corpora and that terms from 158
Step Number 0. Raw list of verbs 6,218 1. Removing errors and foreign words 3,179 2. Removing non-medical verbs 556 3. Checking the verb meaning 47 4. Checking the frequencies 21 this terminology are useful for the automatic de-tection of relevant LUs. In a way, our approach is similar to previous work on automatic labeling of semantic roles (Gildea and Jurafsky, 2002; Pad  X  o and Pitel, 2007), although in our study we focus on specialized domain material, both corpora and resource, and we have no preconception about the semantic roles associated with medical verbs. In-deed, we exploit the entire Snomed International terminology (except the modifiers). 5.4 Contrastive analysis of verbs The semantically annotated sentences are then analyzed manually in order to verify if the se-mantic roles and lexical units are correctly rec-ognized. Wherever necessary, these annotations are enriched manually. This may apply to both missing or unrecognized LUs and FEs. Once the semantic annotation and labeling are completed, verbs from different corpora are analyzed in or-der to study the differences and similarities which may exist between their uses in these corpora. The results are discussed along the following lines: verb selection (section 6.1), semantic an-notation (section 6.2), and contrastive analysis of verbs (section 6.3). 6.1 Verb selection Table 2 indicates the numbers of verbs selected at each step. We can see that an important number of verbs that were removed corresponds to errors, misspellings, and non-medical verbs. The subset of verbs which convey medical meanings corre-sponds to 0.76% (n=47) of the original set. The final subset contains 21 verbs. From this subset, we selected four verbs for a fine-grained analy-sis: observer, d  X  etecter, d  X  evelopper , and activer . These verbs were selected for two reasons: they were found a high number of contexts (respec-tively 270, 74, 193 and 85 contexts in C 1 and C 4 corpora) and these contexts seem to be diversified. 6.2 Semantic annotation
Sentences corresponding to the selected verbs have been automatically annotated with semantic classes that are indicative of FEs. The resulting annotation was checked and enriched manually: few errors are detected ( e.g. , in English-language sentences, or ( o ` u in French ) annotated as C HEM -
ICALS ( gold ) ). The main limitation is due to the incompleteness of annotations ( facteur ( factor ) in-stead of facteur V de Leiden ( Factor V Leiden ) ) and missing LUs ( e.g. , site d X  X nsertion ( insertion site ) as T traumatis  X  es cr  X  aniens ( people with brain injury ) as L
IVING ORGANISMS ), usually not recorded in the terminology. An example of the completed an-notations is presented in Figure 3. We can ob-serve that these annotations are evocative of those in Figure 1. In Figure 3, the verbs are in bold characters, while different FEs appear in different colours: D ISORDERS in red, F UNCTIONS in pur-in green, P HYSICAL AGENTS in pink. The syntac-tic information is also associated with the corre-sponding LUs but not presented in the figure. The LUs mainly correspond to nouns or noun phrases.
Another limitation discovered at this step is due to the erroneous POS-tagging. For instance, among the 32 contexts of the verb activer in C 4 , 15 correspond to its adjectival forms ( e.g. , j etais une 159
Verb C 1 C 4 observer L , J , F , S , A , D L , J , F , A d  X  etecter L , A , J , P , F , D , T activer C , F , P L , P , T d  X  evelopper P , D , L , F L , D , F , T personne tres active ( I have been a very active per-son ) , marche active ( active walking ) ). These are not analyzed in the current study. Hence, the result-ing number of contexts that were analyzed for this verbs is lower than that of the three other verbs. 6.3 Contrastive analysis of verbs The contrastive analysis is performed manually. The most frequent labels for FEs of the four verbs analyzed appear in Table 3. We can observe for instance that L IVING ORGANISM L is usually the most frequent label and appears in both corpora. Typically, it corresponds to human subjects (peo-ple communicating in forum discussions in C 4 , medical staff and patients observed by the medical staff in C 1 ). In C 1 , P ROCEDURES , D ISORDERS and C HEMICALS also occupy an important place. Interestingly, with the verb d  X  etecter , the labels for FEs are identical in both corpora.

Table 4 shows the most frequent patterns of FEs with N 0 (subject) and N 1 (object) functions. We can see that some patterns are common to the two corpora studied (examples (1) to (4)). In the ex-amples presented, the misspellings are genuine. (1) P D with d  X  etecter : j X  X i acheter (2) J D with d  X  etecter : suite a plusieurs (3) D as N 1 with d  X  evelopper : Un syndrome de (4) D D with d  X  etecter : Une pr  X  e  X  eclampsie
On the other hand, other patterns are specific to a given corpus (examples (5) to (8)). (5) T as N 1 with d  X  evelopper in C 4 : Certaines (6) P as N 1 with d  X  evelopper : in the expert (7) F F with activer in C 1 : les formes (8) C F with activer in C 1 : Les Interestingly, the example (5) shows an occurrence of a different meaning of d  X  evelopper from that shown in the previous examples. Notice that we have also extracted non-medical meanings of the verbs (examples (9) and (10)), that cannot be la-beled with the semantic resource we use. (9) Tazzy, tu peux d  X  evelopper ??? ( Tazzy, could (10) Sant  X  e Canada a d  X  evelopp  X  e une nouvelle More generally, the verb d  X  evelopper is used in six patterns common to the two corpora, and eight and five patterns specific to C 1 and C 4 respectively, while the verb d  X  etecter appears in six common patterns and six specific to each of the corpora. No common pattern was identified for the verb ac-tiver : the syntactic and semantic properties of this verb are thus different in the two studied corpora, which may also be due to the small set of available contexts. Another difference between these two corpora is that in C 4 , we can find some contexts in which verbs do not instantiate all the expected FEs: some syntactic positions remain empty.

On the whole, our observations indicate that the studied verbs present several common patterns within C 1 and C 4 . This means that, in this situ-ation, these verbs, although they have a medical meaning, can be correctly understood by patients. When the FEs are partially instantiated, differ from one corpus to the other, or when they show an important difference in terms of frequency, we assume that this may indicate situations in which the understanding may be partial or even unsuc-cessful. In this case, more thorough explanations are needed by patients to fully understand their health condition and required treatment.
We proposed an NLP approach to automatically discover the participants of verbs and label them using an existing medical terminology assuming that the semantic classes of the terminology are in-dicative of frame elements (FEs) within the frame-work of Frame Semantics. The study was per-formed with medical corpora differentiated ac-cording to their levels of expertise: high expertise in
C 1 and low in C 4 . The contrastive analysis of verbs was done on the basis of automatic anno-tations completed manually when necessary. The analysis indicates that some verbs share FEs in the studied corpora, while they usually select different FEs according to corpora.

For future work, we plan to add to this study the analysis of C 2 and C 3 , which we expect may show intermediate patterns or provide a transition between C 1 and C 4 . We also plan to extend this study to other verbs. Up to now, we studied ver-bal arguments in two syntactic positions ( N 0 and
N ), which seems to suffice for the four verbs pre-sented in this paper, but more complex patterns are likely to appear with other verbs. Moreover, auto-matic distinction between core FEs and non-core
FEs (Hadouche et al., 2011), and between the syn-tactic positions of the labeled entities are other di-rections for future work.

Our findings may be helpful in several contexts: improving mutual understanding between medical staff and patients, creating two-fold dictionaries with expert and patient expressions, adapting the content of scientific literature for patients. This last context may also provide an interesting appli-cation and the possibility for the evaluation of the proposed analysis of verbs.
 INFO , pages 1117 X 1121, Brisbane, Australia. 162
Dans le cadre d X  X ne  X tude exploratoi re sur la circulation et la validation sociale des termes, nous souhaiterions revisiter la question de l X  X xpertise. Une analyse contrastive sur un co r-pus de discours de vulgarisation et de f o rums m X dicaux montre une  X conomie sp X cifique de chacun de ces dis cours, rep X rable par des choix lexicaux,  X nonciatifs et argumentatifs d i vers. Les forums se constituent ainsi en un espace de di f-fusion de terminologie m X dicale et en un espace de validation. 
Nous nous int X ressons en tant que sociotermin o-logue (Gaudin, 20 13, Delavigne 2001 a ) aux questions li X es  X  la circulation des unit X s term i-nologiques dans divers discours. Des e n qu X tes men X es sur des terrains vari X s : g X nie g X n X tique, chimie (Gaudin, 1996),  X nergie nucl X aire (Del a-vigne 2001 a ), astronomie (Nicolae, 2013) , ca n-c X rologie (Delavigne, 2013) permettent de remettre en question la r X putation de pr X cision des voc a bulaires scientifiques et techniques, opinion co u tumi X re des sph X res professionnelles et techniques o X  ils sont en usage ; ce pr X jug X  est fr a gile et tomb e vite d X s lors qu X  X n les soumet  X  un examen attentif. De surcro X t, d X s lors que des documents techniques, scientifiques ou m X dicaux franchisent les portes d X  X n service, d X  X n site, d X  X ne entreprise, d X  X ne organisation, que devie n-nent les termes ?
Notre d X  finition du terme est socioterminol o-gique : c X  X st une unit X  lexicale dont la sp X cificit X  est  X  relier  X  son statut dans une communaut X  discursive donn X e. Ce statut se manifeste dans le discours par des marques rep X rables ( X nonc X s d X finitionnels, reformulat ions, connotations autonymiques, th X matisations, etc.). Passage qui renvoie  X  une intertextualit X  et  X  des cultures, le terme ne devient tel que par d X cision du locuteur ou de l X  X nalyste, qui le juge pertinent pour un savoir, un syst X me de connaissances ou une pr a-tique. Cette position  X pist X mologique d X  X n terme comme signe caract X ris X  par une signification socialement norm X e est ais X ment admise a u-jourd X  X ui. 
D X finir le terme par son statut sociolinguistique n X cessite d X  X nvisager autrement les questions d X  X x pertise et de validation terminologique. Ces questions ont d X j X   X t X  pos X es, mais l X  X xploration de nouveaux corpus en renouvelle la probl X m a-tique.
Dans la masse des formes diversifi X es produites vers l X  X xt X rieur de s communaut X s discursives , les termes sont amen X s  X  circuler. D X s lors, co m-ment se joue leur n X gociation hors de leur terreau d X  X rigine ? Quelles variations rencontre -t -on en fonction des genres discursifs ? d es diff X rents supports ?
Dans une communaut X  donn X e, le conse nsus terminologique fonctionne. Cependant, la fr  X -quentation de divers terrains montre une co m-plexit X  qui ne se laisse pas saisir au premier abord : l X  X bservation r X v X le bien des dissensus. 
C X  X st ainsi que la d X couverte des exoplan X tes a oblig X   X  red X finir la notion de plan X te sans qu X  X n accord ne se fasse jour pour borner la signific a-tion du terme ( Nicolae , 201 3 ). N X anmoins, tant que les termes restent cantonn X s  X  leur sph X re organisationnelle , les terminologies jouent leur fonction. C X  X st ce qui permet aux communaut X s de se reconnaitre, d X  X  X re entre -soi : l es identit X s culturelles des acteurs s X  X mpriment dans la mat  X -rialit X  discursive (Delavigne, 2013).

De surcroit, et c X  X st ce qui nous int X resse ici, les choses se troublent singuli X rement lors que les termi nologies doivent sortir des sph X res organ i-sationnelles et circuler hors des circuits pour lesquelles elles sont initialement pr X vues. C X  X st hors des communaut X s d'usagers que les pr o-bl X mes se posent de fa X on cruciale, lorsque les terminol o gies sortent de l eur enclave. Les choses se pa s sent au jointif,  X  la fronti X re, dans l X  X ntre -deux : sans voisin, il n X  X  a pas grand -chose  X  n X gocier. Il est avantageux d X  X xaminer l X  o X  les terminol o gies se dissolvent, se d X sagr X gent :  X  la surface de s X paration. D X s lors, le consensus n'est plus  X  l'ordre du jour, des concurrences d X nom i-natives peuvent apparaitre, des n X gociations di s-cursives se font jour. 
Quand un doute sur un mot, une notion apparait, on se tourne vers l X  X xpert. L X  X xpert appartient  X  une cat X gorie particuli X re de locuteurs auxquels on pose un probl X me technique dont on pense que les r X ponses r X sident dans sa discipline. 
C X  X st un locuteur garant . 
La probl X matique de la garantie m X ne  X  des que s tions de valeur et de l X gitimit X . D X  X  X  l X  X xpert tire -t -il s a l X gitimit X  ? Un d X tour par les dictionnaires ne laisse pas d X  X  X re int X ressant. Il s X  X v X re que l'expert n'y est pas d X crit simplement par celui qui sait, mais aussi par une exp X rience reconnue. Retenons des  X l X ments d X finitoires rep X r X s le mot exp X rience . Autrement dit, au -del X  de la connaissance, du savoir , l X  X xpert sera it aussi celui qui a  X prouv X  une pr a tique ; l X  X xpert est garant de la qualit X  d X  X n co n tenu cognitif, mais aussi exp X rientiel. C X  X st cette exp X rience qui lui conf X re sa l X gitimit X . 2.2 La figu re de l X  X xpert La figure de l X  X xpert a  X t X  analys X e par G X rard 
Petit sur un corpus de discours m X diatiques po r-tant sur la maladie de la vache folle. Il montre la dualit X  du terme qui active une d X finition obje c-tive en r X f X rant  X  une profession et souvent m  X -l X e  X  des traits axiologiques. Il d X crit comment le  X  vocable expert int X gre un paradigme de termes parfois reformulants et cor X f X rentiels : che r-cheur, scientifique, sp X cialiste, chimiste, bot a-niste, biologiste, sociologue,  X pid X miol o giste, m X decin, micro biologiste, v X t X rinaire, virol o-giste, zool o giste, anatomopathologi(st)e . En particulier il entre fr X quemment en co -occurrence avec scie n tifique , autre figure du sp X cialiste.  X  (Petit, 2000). 
Cette convocation de l X  X xpert m X diateur, soci a-lement cautionn X , en fait un  X  gestionnaire di s-cursif  X  entre sa communaut X  d X  X rigine et celui du public pr X sum X . Cette gestion discursive se rep X re dans les discours par diverses marques.
Les r X les se voient ainsi r X partis entre experts et non -experts par un pacte social. Putnam parle  X  ce propos de division linguistique du travail. 
C X  X st vers l X  X xpert que se tourne le non -expert pour savoir si un terme, une d X finition sont justes. Des espaces de validation semblent bien circonscrits : dict ionnaires, ouvrages de r X f  X -rence, appel  X  l X  X xpertise, etc. font  X  loi  X  . C X  X st en outre ce clivage de la communaut X  lingui s-tique qui l X gitime l X  X xistence m X me de la vulg a-risation telle qu X  X n l X  X ntend d X  X rdinaire.

Cependant, en modifiant le corpus, les mod alit X s discursives restent -elles les m X me ? Si l X  X n en retrouve d es traces au travers des domaines diff  X -rents que nous avons fr X quent X s :  X nergie n u-cl X aire, canc X rologie, corpus de vulgarisation, le support en modifie -t -il la nature ? Comment s X  X  joue le r ecours  X  l X  X xpertise ? Pour donner quelques  X l X ments de r X ponse  X  ces question, nous examinerons deux corpus ayant trait  X  l X  X nformation m X dicale. 
Cette contribution se centrera en effet sur un terrain que nous fr X quentons depuis plusieurs ann X  es, la canc X rologie. Depuis les premiers  X tats g X n X raux sur les cancers, en 1998, les ra p-ports  X  l X  X nformation m X dicale ont consid X rabl e-ment  X volu X  et les th X matiques de sant X  ont envahi l X  X space public, dans un contexte l X gisl a-tif autour des droits des pa tients. De v X ritables  X  industries du contenu  X  (Romeyer, 2008) se sont sp X cialis X es dans le domaine de la sant X , proposant une offre d X  X nformation surabondante,  X m a nant tout aussi bien d X  X cteurs institutionnels, d X  X ssociations de malades, d X  X  X ablissements de sant X , des m X dias, de laboratoires pharmace u-tiques, de mutuelles, d X  X ssurances, etc. La nature des informations propos X es pr X sente une forte h X t X rog X n X it X , allant de la mise  X  disposition d X  X nformations m X diales aux informations pr a-tiques, en passant p ar des conseils divers et v a-ri X s.
Cette offre s X  X st av X r X e incompl X te et souvent peu fiable (M X noret, 2007 ; Carretier et al ., 2010). Ce constat a motiv X  l X  X  X ergence d X  X n programme visant  X  mettre  X  la disposition des personnes a tteintes de cancer une information m X dicale valid X e, compr X hensible et r X guli X r e-ment actualis X e, fond X e sur des  X  recommand a-tions  X  destin X es aux professionnels de sant X  2 . Ce programme, d X velopp X  par la F X d X ration nati o-nale des Centres de Lutte contre le C ancer, puis l X  X nstitut national du cancer, agence sanitaire et scientifique, a pour objectif de produire des o u-tils textuels : guides, fiches d X  X nformation, di c-tionnaire..., destin X s  X  compl X ter et  X  renforcer l X  X nformation prodigu X e par les  X quipes m X d i-cale s. Ces outils sont diffus X s dans les  X tabliss e-ments de sant X  concern X s (centres de lutte contre le cancer, h X pitaux publics et priv X s) et aupr X s d X  X ssociations de patients, et disponibles sur la plateforme Cancer Info (www.ecancer.fr).

Le contenu de ce sit e, en tant que site institutio n-nel, offre un exemple de choix pour examiner le fonctionnement d X  X ne vulgarisation m X dicale sp X cifique : une vulgarisation institutionnelle 3 . 
L X  X nsemble des textes y est en effet  X  valid X   X  par un groupe de travai l compos X  de professio n-nels de sant X , de patients, d X  X nciens patients et de proches de personnes malades, ce qui peut con s-tituer un gage de  X  bonne qualit X   X  de la vulgar i-sation. Premi X re part de notre corpus, nous l X  X vons constitu X  en  X  corpus de r X f X rence  X  sur lequ el peuvent s X  X dosser de fa X on contrastive les analyses de notre corpus de forums m X d i caux.
Dans le panel des outils d X  X nformation sollicit X s par les patients, les forums m X dicaux prennent une place grandissante. En contournant le m o-d X l e traditionnel d X  X nformation ascendant, du m X decin vers le patient, ils mettent en sc X ne un patient qui ne recherche pas une information scientifique ou une explication, mais s X  X nterroge sur des aspects pratiques de la maladie (Batta X a, 2012, Delavigne , 20 13)
Nous reprendrons la d X finition que propose Ma r-coccia du forum de discussion, d X fini comme  X  document num X rique dynamique, produit co l-lectivement de mani X re interactive  X  (2004). Cet objet discursif interroge un certain nombre de concepts linguistiques, comme les notions de genre par exemple, ou de vulgarisation, ce que nous allons plus particuli X rement explorer ici . 
C X  X st en outre un type de discours particulier qui soul X ve de redoutables probl X mes d X s lors qu X  X n souhaite le soumettre  X  une analyse aut omatique. 
Un certain nombre de ses caract X ristiques ont  X t X  mises en  X vidence (cf. Henri et Charlier, 2005). 
Notons -en seulement ici certaines : les formes discursives et s X miotiques sp X cifiques qu X  X l co n-voque et qui peuvent  X voquer un  X  sous -genre  X  , son h X t X rog X n X it X  s X miotique avec ses variables d X  X xpression de  X  bas niveau  X  : ponct X mes, e m-phase par la focalisation des capitales ou la graisse typographique, smileys et autres ma r-queurs d X  X nteraction, son hybridation entre  X crit et oral, ses particularit X  morphologiques et sy n-taxiques : n X ographies, accidents dactylogr a-phiques , ellipses, l X  X mportance de certains d X ictiques... (Anis, 1999). Autant d X  X  X  X ments qui viennent singuli X rement compliquer une analyse automatique. 
Notre corpus rassemble un certain nom bre de discussions extraites de forums m X dicaux autour de la th X matique du cancer : Doctissimo, Ligue contre le cancer, Jeunes Solidarit X  Cancer, Atoute.org, Les Impatientes, Anamacap, France 
Lymphome espoir, Cancerdusein.org, sante -medecine.net, Sant X AZ, aufeminin.com, e -sant X .fr, Psychologies.com. 
Nous nous fondons sur l X  X ypoth X se que les stru c-tures sociales (organisationnelles, institutio n-nelles ou autres) et les conditions mat X rielles de la communication contraignent les formes  X no n-ciatives. C X  X st, en suivant Rastier (2011), poser l X  X ypoth X se de l X  X ncidence du global sur le local. 
Notre approche sociolinguistique des faits term i-nologiques s X  X nscrit dans le cadre des analyses d e discours et de la s X mantique de corpus. Les termes sont bien s X r  X  consid X rer non comme unit X s atomis X es, mais au c X ur de pratiques di s-cursives vari X es et situ X es. Notre approche va vers un au -del X  du terme -celui -ci  X tant un point d X  X ntr X e dans les discours -avec une vis X e di s-cursivo -centr X e.

Les analy ses linguistiques des discours de vulg a-risation ont bien d X crit la fa X on dont ces textes mobilisent une intense activit X  de reformulation autour de certaines unit X s terminologiques. Par hypoth X se, ces reformulations sont destin X es  X  aider le destinataire  X  construire du sens. Elles peuvent  X tre de plusieurs types : d X finitionnelles, d X signationnelles, m X taphoriques... Cette pr o-pri X t X  est un moyen dont il est coutumier de se saisir comme moyen de d X pistage des termes, ces traces formelles de cette activit X  ref ormulatrice constituant autant d X  X ndices pour rep X rer des unit X s terminologi ques (Delavigne, 2001 b ) . Des  X  p a trons  X  (Aussenac -Gilles et Condamine s , 2009 ) perme t tent de localiser des  X  structures doubles  X  (Fuchs, 1982), des  X  paradigmes d X f i-nitionnels  X  o u  X  d X signationnels  X  (Mortureux et 
Petit, 1989). A partir d X  X n terme cristallisant auto ur de lui un certain nombre d X   X   X v X n e ments discu r sifs  X  , qui exhibent une  X nonciation en acte, il s X  X git de rep X rer les traces de lev X e du  X  jargon  X  en nous attachant  X  examiner les lieux discursifs o X  il se d X noue. 
Le corpus est exploit X   X  l X  X ide du logiciel de traitement de donn X es textuelles Nooj, d X velopp X  par Max Silberztein 4 . Cet outil nous permet d X  X xplorer les diff X rentes relations autour des termes. Sans reven ir sur les diff X rentes approches outill X es pour analyser le mat X riau langagier, disons seulement que de notre point de vue, Nooj pr X sente l X  X vantage de pouvoir cr X er ses propres patrons de recherche sans l X  X ffet de  X  bo X te noire  X  de certains logiciels. C X  est ainsi que nous avons cr X  X  des grammaires (transducteurs) pour exploiter notre corpus, ainsi qu X  X n di c tionnaire des n X ographies, n X cessaire pour analyser aut o-matiquement le corpus 5 .
Dans les limites de cette pr X sentation, nous nous focaliserons autour de quelques probl X mes te r-minologiques : que deviennent les termes dans ces deux types de corpus ?
Le site Cancer Info d X ploie un contenu centr X  sur les cancers, leurs sympt X mes, les traitements propos X s et leurs effets sur l'organisme 6 . Autour des termes se rep X rent des marques de reformul a-tion, bien d X crites par ailleurs (Mortureux, 1982, 1993 ; Jacobi , 1999) . Nous ne sous y attardons pas ici dans la mesure o X  nous nous servons de ce corpus comme instance de r X f X rence. 
Les forums de s ant X  pr X sentent des objets de discours d X  X ne autre nature. On y d X c X le des  X nonc X s de soutien, des encouragements, des  X changes de recettes (sur les moyens d X  X  X iter les naus X es provoqu X es par certaines chimioth X r a-pies par exemple ), des conseils pratiques, des re n seignements sur les traitements ou encore de simples contacts avec d X  X utres malades ; il s X  X git certes de comprendre sa maladie ou ses trait e-ments, mais aussi de se rassurer, de part a ger... (Romeyer, 2008). On peut noter une forte densit X  de modalisat eurs affectifs dont la fonction semble  X tre de restaurer un lien phatique et con a-tif absent de ces  X nonci a tions asynchrones.
Que deviennent alors les termes dans c es forums m X dicaux ? On pourrait penser dans une pr e-mi X re approche que le discour s m X dical s X  X  di s-sout. Or il s X  X v X re que le forum est un lieu  X maill X  de terminologie m X dicale, reprise, co m-ment X e,  X valu X e, recat X goris X e. Livrons -en un exemple :
Nous conservons la mise en forme et la graphie d X  X rigine dont il faut rendre compte, ainsi que des usages sp X cifiques de la typographie et des ponctu X mes, extr X mement nombreux par rapport au corpus Cancer Info. On d X c X le une forte de n-sit X  terminologique, accompagn X e de traces d X  X ntertextualit X , qui marque la cu lture  X  p X rim  X -dicale  X  des patients  X  l X  X  X vre dans ces discours (Delavigne, 2009). 
Il n X  X  a cependant pas juxtaposition des termes dans les deux corpus. C X  X st ainsi qu X  X n terme comme le verbe br X ler , relatif  X  un effet seco n-daire possible de la radioth X ra pie, est significat i-vement pr X sent du corpus forum (comme il l X  X st par ailleurs dans d X  X utres corpus oraux), mais absent du corpus Cancer Info. 
Autour de ces unit X s terminologiques se focal i-sent  X galement des indices d X  X xplication, de d  X -f inition, de reformulation, en bref, des marqueurs de nature diverse. Examinons un extrait  X  propos du terme thrombose lymphatique superficielle :
On aper X oit un certain nombre de structures d  X -finitoires typiques :  X  est un  X  ,  X  c  X  X st un  X  ,  X  ce sont  X  . L X  X  X onc X  mobilise tout un paradigme d X signationnel avec divers types de reform u-lants :  X  corde  X  ,  X  d X chets non  X vacu X s  X  ,  X  complication assez fr X quents  X  . Le term e se voit m X me sigl X  :  X  TLS  X  . Ce sont autant de traces d X  X ne vulg a risation d X  X n autre type. Nous ne sommes en effet plus l X  dans une communic a-tion desce n dante du m X decin vers le patient, mais dans une autre forme de vulgarisation, de pair  X  pair. 5.3 Les ma rqueurs de reformulation
L X  X xamen d X  X utres extraits du corpus r X v X le des modalit X s diff X rentes de certains marqueurs de reformulation, d X  X  X  l X  X xhibition vulgarisatrice est absente. Un rep X rage de ce s marqueurs permet de mettre au jour des variantes d X  X sage d X  X n corpus  X  l X  X utre. On note ainsi moins de guill e-mets, moins de parenth X ses. Est un , marqueur de reformulation pr X sent dans les deux corpus -sans surprise -, pr X sente des usages qui divergent ne t-tement.

Q uoique moins exhib X e, la vis X e vulgarisatrice r este une caract X ristique tout  X  fait pr X gnante du corpus forum :
Ce sont d X  X utres syst X mes qui sont activ X s de fa X on plus pr X sente : diaphores et autres s  X -quences explicatives, r X seau pr X dicatif comme la f onction.
Cette vulgarisation montre une expertise sp X c i-fique en train de se mettre en place. Cette expe r-tise se d X cline avec des moyens vari X s. Elle laisse place notamment  X  une intertextualit X  marqu X e, explicite, co mme dans l X  X xtrait suivant dans lequel le locuteur fait appel  X  des doc u-ments de r X f X rence :
Ou implicite, c omme celui -ci autour du terme protocole :
On a l X  la convocation inexprim X e d X  X n ailleurs textuel, en l X  X ccurrence un extrait d X  X n guide pour les patients. C X  X st ainsi qu X  X n voit se mettre en place au fil des discussions une validation terminologique particuli X re. Ce peut  X tre par un appel discursif  X  l X  X xpert :
Ou par ce qu X  X n pourrait d X signer c omme une validation  X  interne  X  :
Vulgarisation par des pairs pour des pairs , les formulations sont spontan X ment adapt X es aux besoins des  X nonciateurs, ce dont t X moignent certains co mmentaires  X pilinguistiques :
On a l X  l X  X  X ergence d X  X n patient expert qui s X  X donne  X  une vulgarisation  X  de partage  X  , fac i-litante pour l X  X ppropria tion terminologique. C X  X st ainsi qu X  X n voit se dessiner une  X volution du mod X le de la vulgarisation dans lequel se r X am  X -nage nt les figures de l X  X nterlocution. Les signif i-cations se n X gocient au plus pr X s des pratiques des locuteurs. On passe en somme de la gara n tie issue de l X  X xpertise  X  la garantie provenant de l X  X xp X rience.
D X s que la circulation des termes s X  X  X argit, leur signification est sujette  X  des n X gociations no u-velles. Le r X le de la fronti X re est de r X guler et de filtr er ; lieux de passage, les fronti X res sont aussi ceux o X  se n X gocie le sens. La m X thode d X ploy X e sur deux corpus autour de l X  X nformation m X dicale permet ainsi de mettre en  X vidence un certain nombre de caract X ristiques  X nonciatives autour des termes, varia bles selon les diff X rents corpus. 
Elle montre la fa X on dont un lexique issu de la canc X rologie se diffuse les discours des patients. 
Les modalit X s de la cooccurrence de termes et de leurs paraphrases sont certes vari X es ; n X a n-moins, l X  X nalyse des questions discu r sives de vulgarisation r X v X lent comment les forums se constituent en un espace de diffusion de termin o-logie m X dicale, mais aussi en un espace de val i-dation, avec des logiques de l X gitimation sp X cifiques qui font  X voluer le contr X le social du sens. 
Jacques Anis. (Ed.) . 1999. Internet, communication et langue fran X aise. Herm X s . Paris .

Aussenac -Gilles, Anne Condamines. 2009. Variation syntaxique et contextuelle dans la mise au point de patrons de relations s X mantiques, dans J. -L. Minel (ed.) : Filtrage s X mantique. Hermes/Lavoisier, P a-ris : 115 -149.

Jean -Claude Beacco. 1993. L X  X xplication d X  X rientation encyclop X dique. Les Carnets du C e-discor. Publication du Centre de recherches sur la didacticit X  des discours ordinaires, (1):33 -54. http://cedi scor.revues.org/602 
Jean -Claude Beacco. 2000.  X critures de la science dans les m X dias. Les Carnets du Cediscor. Public a-tion du Centre de recherches sur la didacticit X  des discours ordinaires, (6):15 -24.

C X line Batta X a. 2012. L X  X nalyse de l X  X  X otion dans le s forums de sant X . Actes de la conf X rence conjointe JEP -TALN -RECITAL, RECITAL :267 -280. 
Julien Carretier, Val X rie Delavigne, B X atrice Fervers. 2010. Du langage expert au langage patient : vers une prise en compte des pr X f X rences des patients dans la d X mar che informationnelle entre les profe s-sionnels de sant X  et les patients, Sciences -
Crois X es :6. http://pagesperso -orange.fr/sciences.croisees David Crystal. 2001. Language and the Internet. 
Cambri dge University Press. Fabienne Cusin -Berche &amp; Florence Mourlhon -Val X rie Delavigne. 201 3. Du vag a bondage du jargon. 
Val X rie Delavigne. 2012. Peut -on  X  X raduire X  les mots Val X rie Delavigne. 2001 a . Les mots du nucl X aire. 
Val X rie Delavigne. 2001 b .  X  Rep X rage de termes dans 
Fran X ois Gaudin. 1993. Pour une socioterm inologie :
Fran X ois Gaudin. 1996. Une approche sociolingui s-
France Henri, Bernadette Charlier. 2005. L X  X nalyse 
Daniel Jacobi. 1999. La communication scientifique ;
Michel Marcoccia. 2004. L X  X nalyse conversationnelle Marie M X noret. 2007. Les temps du cancer, Editions 
Sophie Moirand. 2000. Variations discursives dans 
Marie -Fran X oise Mortureux. 1993. Paradigmes d X s i-
Marie -Fran X oise Mortureux. 1982. Paraphrase et 
Florence Mourlhon -Dallies, Florimond Rakotono e-lina, &amp; Sandrine Reboul -Tour X  . 2004. Les discours de l X  X nternet : quels enjeux pour la recherche ? Les Carnets du Cediscor, 8:9 -19. Cristina Nicolae. 2013. Qu'est -ce qu'une plan X te? 
Sens et r X f X rence dans les discours scientifiques et de vulgarisation scientifique. Th X se de doctorat, 
U niversit X  de Rouen. ] The use of terminological systems for the creation of ontologies raises several major issues (Garc  X   X a-Silva et al., 2008). Obviously, ontologies and ter-minologies play a similar normative role. They aim at establishing a common vocabulary and make use of shared representations and concepts to allow the documents interoperability and facili-tate knowledge building. However, ontologies and terminologies have clearly a different formal ap-proach on Semantics. Ontologies are concepts ar-chitectures and are not organized lists of terms . Unlike terms, the concepts are characterized by formal definitions . The formal aspect enables the computerized treatment of the information. To use an ontology to normalize a document is, in that sense, to encode it by bringing a characteristic al-lowing the automated treatment of the informa-tion.

However, the creation of ontologies involves sometimes the use of terminologies, or even more radically, the use of corpus of text. If the ontology is to be integrated within an automated informa-tion treatment system, as for example the informa-tion retrieval (IR), the concepts should match with the terms appearing on the documents to enable the information treatment. The ontology should ensure the coverage of the terminological domain .
The conceptual representation would otherwise be unusable.

The Lerudi (emergency services) Project in-tends to develop an Information System (IS) offer-ing an overview of the Electronic Health Record (EHR) to the health professionals. Additionally, it aims at facilitating the quick reading of the
EHR to allow quick medical decisions under tight time constraints. The field experimentation of that project is the reading of hospital files by an emer-gency regulating physician. Practically, Lerudi is
IR system based on a Termino-Ontological Re-source (TOR) 1 named O NTOL U RGENCES . The
TOR (a) plays the field model role by listing the relevant concepts; and (b) ensures the link be-tween the concepts and their name in the EHR documents. This double function should not only enable an easy annotation and indexation of the patient files, but also facilitate the retrieval of in-formation from the indexed records.

The O NTOL U RGENCES development included 6 phases: (i) the building of the TOR ontological skeleton based on a corpus analysis method; (ii) the use of existing terminological and ontological resources to manually complete the TOR concepts system; (iii) the automatic enhancement and (iv) semi-manual TOR enhancement at the terms level; (v) the TOR enhancement of concepts in relation to the medicines; and finally, (vi) the implementa-tion of validation and quality control procedures.
The first 2 phases of the TOR correspond to the usual ontology construction method, widely tried in our team and did not raise major issues. However, the 3 following phases that were spe-cific to the TOR development were much more problematic. Specifically, the TOR terminological enhancement required external resources: Knowl-edge Organization System (KOS). These external resources are only useable in an architecture sup-porting a complex modeling of the target TOR. In particular, the architecture should accommodate the terms, the concepts and their interrelation, and simultaneously, the KOS used for the enhance-ment. The last stage corresponding to the qual-ity control is also specific to this project and was necessary considering the various participants in-volved in the TOR construction.

Trough the detailed description of the pro-cess guiding this TOR construction and validation within a large team, we aim at showing that: (i) the sustainability of such a resource requires a concise articulation between terms and concepts; and (ii) such a requirement can be met via the implemen-tation of standardized procedures based on a meta-model architecture allowing the modeling of all necessary KOS and other knowledge structures. The rest of the paper is organized as follows: Section 2 briefly presents the advantages of us-ing ontologies for retrieving information. The first two steps of the TOR construction and its speci-ficities are presented in Section 3. Section 4 pro-vides an overview of the UniMoKR model that enables the implementation of the TOR termino-logical and conceptual enhancement procedures; and its uses. Section 5 describes the TOR different enhancement phases 2 . The validation and quality control are detailed in the section 6. Finally, the paper concludes with a summary and a discussion in Section 7.
To begin with, we should ask ourselves what the point in using an ontology for an IR is. Specif-ically, the main advantage of an ontology is to allow an automated reasoning based on the con-ceptual structure and semantic relations between notions. Consequently, in addition to subsump-tion relations ( is-a formal relation ), we modeled the semantic relations between signs, diseases and medical specialties. These relationships enable an interface ( i.e. a cloud of words) to display the medical specialties that characterize a given EHR.
An ontology for IR has also de facto , as any ontology, a structure that depends on the task (Charlet et al., 1996; van Heijst et al., 1997). This structure is not a quality in itself for the IR, but it nevertheless has two advantages: (i) a well-structured ontology is easier to maintain than a poorly structured ontology, (ii) a well-structured ontology enables valid reasonings. This second point is obviously expected from any ontology, but it is clear that it is not always satisfied. Another important property that has to possess an ontol-ogy for IR is the coverage of the terms relevant to express the notions of the target-domain. The fol-lowing two examples will illustrate these points:
Example of the importance of the formal structure
Example of the importance of the terminological
It appears clearly that, the quality of the infor-mation displayed to the final user of the IR system crucially depends on the quality and richness of the TOR. The processes of annotation, indexing and inference rely on the formal structure and the terminological completeness ( i.e. its capacity to cover the terms of the domain). The figure 1 be-low illustrates the different uses of the TOR in the Lerudi project. 3.1 Domain of O NTOL U RGENCES O
NTOL U RGENCES has been built in several steps and by using different resources, and its tar-get knowledge field has been clarified gradually. From the very beginning of the project, we real-ized that the knowledge field that had been orig-inally set for the TOR (that is: the repertoire of concepts that had to be present in the TOR ) had to evolve. We had left with the idea of building an ontology representing only the specific concepts used by the emergency physicians. But it turned out that, from the perspective of information re-trieval in EHRs, such restriction a priori of the tar-get knowledge field was a mistake. Indeed, the information system aims to allow the emergency physician to quickly find medically relevant con-cepts in EHRs. But these concepts can not be re-duced to concepts specific to the medical emer-gency field, they can instead meet any medical specialty .

In the paragraphs below, we present the main phases of the development of O NTOL U RGENCES and the terminological and ontological resources we have used. We do not discuss the problem of the organization of these stages and cycles of development. For this question, we may refer to (Dhombres et al., 2010).Suffice it to say that during the development process of O NTOL U R -
GENCES , we followed the A RCHONTE method de-veloped by B. Bachimont Bachimont et al. (2002). 3.2 The processing of textual data
In the A RCHONTE method, the domain ontology is built on the analysis of documents generated during the activity to be modeled. In our case, we have encountered great difficulties in access-ing a corpus that could perform this function. The emergency services being not computerized, and the paper documents shorter and less numerous than in other services, it was difficult to find docu-ments in sufficient numbers to make up the corpus in question.

Consequently, we used two other kinds of docu-ments: the acts of the Urgences conference of the discipline and the Guides to Good Practice . Be-sides the difficulty we had to preprocess the cor-pus, the main problem was the coverage capac-ity of the corpus compared to the target. Indeed, the corpus of the conference proceedings, that was fully processed, has shown its limits in terms of scope. Conference papers are in many cases con-cerned with the  X  X are bird X , that is with questions that are not representative of the problems that emergency physicians are confronted daily. A spe-cific work has shown this clearly by comparing the terms most frequently detected in the corpus with the actual incidence of the emergency dis-eases (Gayet et al., 2010).

This issue of availability of the corpus should not be underestimated: in the areas where we can base the construction of the ontology on a corpus analyzed by tools of natural language processing (NLP), resorting to existing terminologies oper-ates in the validation process of the work having been done. In the case of interest here, they occur much earlier in the development process. 173 3.3 Reusing the specialty thesaurus For the PMSI 3 coding, the emergency physicians make use of an CIM-10 extract which contains about 1,000 terms. These terms covering an im-portant part of the terminological repertoire used by emergency physicians for coding, it appeared necessary to incorporate them in the ontology. Consequently, a concept was created and defined for each of them.

One of the major limitation of the project is the fact that the CIM-10 terms are suitable for cod-ing, but some of them are difficult to manage in an ontology because they encompass several het-erogeneous concepts. For example, one can find terms such as  X  X ubject waiting to be admitted else-where, in a suitable establishment X  or  X  X ymptoms and signs involving cognitive functions and con-sciousness, other and unspecified X . The concepts associated with such terms, because they articulate in a complex way a multitude of heterogeneous concepts, are difficult to model. 3.4 Reusing the CCAM The french CCAM classification (commune clas-sification of medical acts) has the benefit of having been designed by teams familiar with ontologies. Which means a priori that each concept of this classification has been validated by a formal rep-resentation (Rodrigues et al., 1999). The reuse of the CCAM thus enabled us to incorporate a clas-sification made up in accordance with consistent principles to our TOR.
 The problems rather came from the way the CCAM is organized and designations used for the acts, which are built for specified accounting poli-cies and not at all suitable for their expression in medical documents -our target. Much work has thus consisted in renaming the terms associated with concepts ( cf.  X  3.6). 3.5 Reusing the SNOMED V3.5 The creation of the branch of diseases concepts is always a major part in the constitution of medi-cal ontologies. As the necessary corpus for the design of such a branch were not available or did not cover the whole area, we decided to complete the work by integrating in O NTOL U RGENCES the diagnoses branch of the S NOMED V 3.5 4 . This procedure was mainly carried out by physicians and required more than 100 hours of work: The S
NOMED V 3.5 was notoriously too specific -what could be expected -but appeared also very badly organized -which was quite surprising. From the 25,000 diseases present in the The S NOMED V 3.5, 6 500 have been preserved. 3.6 Additional methodological comments To complete the description of the construction of O
NTOL U RGENCES , a few points of clarification are further needed: 1. O NTOL U RGENCES was developed with the 2. The SKOS 5 language was used for the for-3. The resources used in the construction of on-4. The concepts of the ontology can be distin-The O NTOL U RGENCES ontology provides a con-ceptualization of the emergency field with terms to designate its concepts. This conceptualization can benefit from (i) the terms present in the KOS of Health to increase the detection of concepts in documents processed and from (ii) specific con-cepts about drug molecules in the ATC classifi-cation. To develop this new resource, you must be able to represent the KOS and ontology at the same level of description. Indeed, these resources are available in different formats and languages. 4.1 The UniMoKR metamodel The diversity that exists in the nature, represen-tation, and organization of the knowledge can be explained by different pasts, objectives, and uses. However, these KOS always intend to grasp infor-mation, to share it, and to support the human and computised processing. Thus, it is possible to ex-tract a common model core from this obvious het-erogeneity (i.e. a model common to all knowledge structuring). In the field of knowledge organiza-tion system representation, some norms and stan-dards are in place and facilitate the interoperability (Miles, 2006; Clarke, 2008). Although SKOS and BS 8723 allow terminologies representation, none of them adress the issue of concepts group in a satisfactory manner 6 . We reuse in this project, the UniMoKR model designed in our previous work (Vandenbussche and Charlet, 2009) 7 . This model uses and extends modeling elements from SKOS,
BS 8723 and is already used by research and com-mercial projects (Joubert et al., 2011; Vandenbuss-che et al., 2013).

The Termino-Conceptual part of UniMoKR model describes the relation between a Concept and its related Preferred Term and Simple non pre-ferred Terms (aka synonyms) in each language.
The Group Part enables not only the representa-tion of a whole terminology, but also the repre-sentation of a terminology subset. It allows two different ways to characterize membership: by in-tension (concepts have to meet the restriction re-quirement to be part of a group; all concepts an-swering this request are implicitly members of the group) and by extension (concepts have to ex-plicitly refer to this group via the relationship in-
Group ) Our modeling reified the SKOS original alignement relations and allows alignments repre-sentation generated by various sources as well as the representation of the associated metadata in-formation. Finally, meta-classes intend to guar-anty the UniMoKR model extensibility and to fa-cilitate its re-use and adaptation: some artifacts particular to some terminologies are not taken into account in UniMoKR; however, they need to be represented to avoid the loss of information.
As mentioned above, for the Lerudi information system to be operational in situation, it is nec-essary that the TOR O NTOL U RGENCES covers almost all linguistic forms under which medical concepts relevant for emergency decisions appear in the EHR the system will have to deal with.
Ultimately, the system must also be able to ac-commodate the  X  X hortcuts X  and  X  X mperfections X  of the language in which patient records are writ-ten, which for instance make use of abbreviations or may simply contain spelling errors.

The overall Lerudi system works as follows: the text of the various documents comprised in the
EHR is processed by an algorithm that seeks to es-tablish a correspondence (if necessary, by integrat-ing NLP methods) between the phrases (treated as mere strings) and the system of concepts of the
TOR. If a string has been matched with a concept, the concept will be used to index the document.
Now, medical records are usually written in nat-ural language (or at least in this semi-standardized language suitable for concrete medical activities), 175 for the semantic interpretation process to reach a satisfactory level (or an optimal one: the opti-mum being set by the performance attained by an emergency physicist), it is often necessary to have available all lexical variations (synonyms, short forms, etc.) that may present the textual form of the concept. If a form encountered in the EHR has not been specified in the ontology, the record will not be indexed with the corresponding con-cept. The medical term will not be displayed by the interface. The emergency physicist will then have to put up with an incomplete or incorrect in-formation.

To overcome this problem, two terminologi-cal enhancement processes of the TOR have been performed: (i) an automatic enhancement of the TOR by the adding of terms extracted from var-ious KOS; (ii) a semi-automatic enhancement of the TOR by the adding of noun phrases extracted from the EHRs. 5.1 Enhancement of the TOR through the A first version of the enhanced TOR is realized through the aligment of the emergency domain on-tology with few KOS relevant for the field, includ-ing CIM-10, SNOMED 3.5, MedDRA, ATC. By providing a controlled vocabulary, the KOS sup-port the functions of analysis (annotation) of the EHRs. But, du to the difficulty to validate align-ments, we decide to keep just the alignment to S
NOMED V 3.5. The alignment was performed with the alignment software O N AGUI (Mazuel and Charlet, 2010) 8 and by manually validating all the automatic alignments made.

Finally, during an export phase, the TOR, now optimized for annotation and indexation, is made available in the SKOS format. Once the concepts of the ontology enhanced with lexical forms from the KOS, the representation model of the TOR is converted to SKOS. This operation of conver-sion is performed with the model transformation method described at the section 4.1. 5.2 Enhancement of the TOR through the To improve the terminological completeness of O
NTOL U RGENCES TOR, a complementary semi-automatic enhancement procedure was intro-duced. This procedure incorporates the principles of the bottom-up methodology used by the design-ers of domain ontologies. It includes the follow-ing steps: (i) we first analyze with NLP tools the content of the documents produced by the oper-ating health professionals ( i.e. the EHRs), in or-der to extract (this time by mobilizing statistical methods) the noun phrases likely to be among the most structuring of the considered field of knowl-edge, that is the terms that are specific and essen-tial to the field; (ii) Once these terms are identi-fied, health professionals (emergency physicians):
A) perform a filtering operation to retain only the terms actually belonging to the medical field and likely to be clinically relevant during the process of IR in EHRs and B) validate the relevance of the identified synonymous terms; (iii) these terms are then: A) added as synonyms ( skos:altLabel tag) when they meet medical concepts already present in O NTOL U RGENCES TOR, or B) con-verted into new concepts, when they refer to no-tions that do not yet have a conceptual representa-tion in the TOR (in that specific case these terms correspond to the so-called candidate-terms of the bottom-up methodology. This conceptual conver-sion step requires to produce a formal definition of the concept being considered, which means firstly positioning the concept in the existing ontological hierarchy. 6.1 Why using validation procedures?
After one year of work, it appeared that the imple-mentation of control procedures was necessary to maintain the quality of O NTOL U RGENCES TOR, and that these procedures had to be replayed reg-ularly. Indeed, (i) many stakeholders, physicians as well as modelers, are working together on the ontology, and despite all our efforts, we have not always been able to correctly apply the guidelines for the maintenance of quality and the homogene-ity of the TOR. In addition, (ii) many instructions are binding and a person may apply them one day and forget them another.

In a first step, these procedures do not address the structure of the ontology. The main reason is that at this level of development of the TOR and given the skills of the team, the problems we encountered were first terminological problems.
But it is clear that problems of structuration, also 176 present, call for future treatments ( cf. 7). Our procedures are based on patterns, or anti-patterns when managing mistakes to be avoided. This work falls under the current research area concerned with the control of the quality of ontologies, as can be read on more structural points in (Roussey et al., 2010) or (Rector et al., 2004). 6.2 Which meta-model? The quality control procedures were designed to ensure that the TOR meets the criteria of a spe-cific meta-model. As far as this part is concerned, the meta-model can be expressed by the list of fol-lowing rules:  X  Each concept must carry an annotation  X  ter- X  Each terminological concept ( cf. previ- X  Each terminological concept must have zero  X  Due to the IR algorithms functioning, 6.3 The procedures The procedures are implemented by uploading the ontology to a SESAME store, and via SPARQL requests. Consequently, the quality criteria are verified in the triplestore :  X  Each terminological class must have a pre- X  Each class must have one, and only one, pre- X  Each prefLabel must be associated with a  X  Each altLabel must be associated with a lan- X  Each class must carry a hiddenLabel.  X  Each class must carry only one hiddenLabel  X  Two classes must have the same prefLabel for  X  Two classes must not have the same altLabel  X  Two classes must not have the same hidden- X  Two classes must not have one identical pre- X  Tracking of multiple parent classes. The  X  Additional requests. Most of the additional
Lerudi is a project that applies a specific method-ology to the requirements of the medical emer-gency environment. The goal of this project is to build a TOR capable of retrieving information ef-ficiently. Trough the complete description of the building process and the TOR validation in a large team, we have shown that: (i) concepts and terms must be precisely articulated within such a re-source; (ii) the developed meta-modeling architec-ture must allow the modeling of all necessary KOS and other knowledge structures; (iii) standardized procedures based on this architecture may be im-plemented to enable the modeling.

Finally, the integration of the KOS in the same format and the RDF transformation service (ca-pable of operating pre-treatments) allow to gener-ate a termino-ontological resource with a lexical-ization able to carry out the annotation, inference 177 and indexation actions of the patients files. This project demonstrates the possibility to accommo-date multiple KOS and to provide an efficient re-source based on different request and transforma-tion treatments.
 The meta-thesaurus UMLS is a multi-lingual col-lection of biomedical vocabularies which include concepts associated to Semantic Types (provided by the Semantic Network) and defined with one or more terms synonyms or term variations 1 . The thesaurus is by and large populated with terms from the English language, while other major lan-guages such as Portuguese are lagging behind in terms of volume and quality. This work is a first step to overcome this state of affairs, by inves-tigating how far a small to medium size corpus, combined with an automated approach based on distributional semantics and limited human inter-vention, can provide new terms to be included in the Portuguese section of the thesaurus (UMLS-POR). A good review of pattern-based approaches to semantic relation extraction can be found in (Auger and Barri ` ere, 2008), while (Hamon and
Grabar, 2008) offers a good example of an ap-proach tapping on existing terminology to bring out the synonymy relations between words. Here we focus on simple terms.
The Genomics corpus (GENOMICA) was com-piled at CLUL (Centro de Lingu  X   X stica da Uni-versidade de Lisboa) during 2003 and 2004.
The GENOMICA corpus comprises 611 texts with 1,086,772 tokens and 66,817 words for a word/token ratio of 6.15%. The corpus has been tokenized, tagged for part-of-speeches and lem-matized using a tool we developed at CLUL. The identification of relevant texts for inclusion in the corpus was a challenge since few texts in this area of knowledge are actually produced in Portuguese.
While geneticists mostly write their publications in English, in international conferences and jour-nals, genomic knowledge has to be accessible to
Portuguese speakers, laymen or students. We re-lied on the help of a Portuguese geneticist to locate materials in Portuguese and also to validate the texts as belonging to the area of genetics and ge-nomics. The corpus includes scientific abstracts, papers and books, PhD dissertations, documents from the Portuguese Society for Human Genetics, excerpts from courses in genetics, support materi-als for students taking genetics courses, legal text and Supreme Court rulings on genomics, articles and book for scientific dissemination as well as two newspapers interviews with a Portuguese ge-neticist. A small sample of the corpus consists of transcriptions of spoken data: one faculty class, one conference and one interview (around 11,000 tokens). Four texts are translations to Portuguese, a total of 39,058 tokens. Given the rather modest size of our corpus, and to alleviate the problem of sparse data, we have decided to normalize terms from UMLS and GENOMICA to lower-case. These experiments are about term extraction and concept mapping. Our objective is to extract from the GENOMICA corpus salient terms pertaining to the domain of genomics and map them to con-cepts from the UMLS-POR. Terms extracted and mapped are validated by humans. The experimen-tal flow-chart, including extraction and mapping, is shown in figure 1. The approaches adopted are not novel, but their combination may be.
 3.1 Extracting salient terms from the corpus To extract relevant terms from GENOMICA, that is terms belonging to the field of genomics, we use an approach that computes the salience of terms based on log-odd ratio. The log-odd ratio com-pares the frequency of terms in a specialized cor-pus (GENOMICA) with that in a reference (gen-eral) corpus. High positive values indicates strong salience. We used a random subset of 10,416 files from the CRPC (Reference Corpus of Contempo-rary Portuguese) as reference corpus 2 . Our subset comprises 8,916,910 tokens for 175,220 words, a word/token ratio of 2.0%.

We asked our human expert, a geneticist, to re-view the top 2,000 terms we extracted from GE-NOMICA and labelled them as either  X  X es X  (for a term definitely belonging to genomics),  X  X aybe X  or  X  X o X . The labelling task outcome was 1,403  X  X es X , 115  X  X aybe X  and 482  X  X o X , so a preci-sion of 70% (1,403/2,000). Somewhat surpris-ingly, given that the UMLS-POR comprises a total of 23,350 terms distributed in 16,491 concepts, we find that only a small fraction (21%) of the terms extracted from GENOMICA and labelled  X  X es X  are already present in UMLS-POR, which indi-cates that the thesaurus is in need of an update. We tackle the task of assigning some of the remain-ing extracted terms labelled  X  X es X  not already in
UMLS-POR to an appropriate UMLS concept in the next section. 3.2 Mapping: approach and evaluation
We undertake the task of assigning automati-cally new terms to UMLS-POR using a VSM ap-proach comparing the contexts of occurrences of terms. VSMs are built around word-context ma-trices where rows are words (terms) to compare and columns are words used as context (context-words) for the comparison. Context-words are any words (with the exception of stopwords) neigh-bouring a term. Our approach is typical of VSM and can be summarized as follows: we count oc-currences of context-words around each term and then compute the cosine distance between any pair of terms, assuming that words with the same con-texts are closer in meaning.

Contexts are usually defined as words surround-ing each term. In our case, the context can be more specifically parametrized by size and form. By size we mean the window size, how many words on each side of a term we are considering. By form we refer to either word-form or lemma. Another parameter to consider is the scoring method, how you count the number of occurrences of context-words around a term. We look at parameter tuning in the next section. 3.2.1 Parameter tuning
The first step in building matrices for VSM in-volves tuning parameters. We experimented with different values for three parameters: the window size, the scoring system and the form for context-words. We tested these parameters for twelve terms from five different concepts as found in the ULMS-POR (see table 1).

After building a matrix for all twelve terms summarizing their  X  X ehaviour X  (scores) in GE-NOMICA, we computed the distances between 182
Terms Concepts English procriac  X   X  ao breeds cruzamento C0006159 cross gerac  X   X  ao generation dna C0012854 deoxyribonucleotide adn deoxyribonucleotide nascituro unborn child feto C0015965 fetus fetos fetuses gravidez C0032961 pregnant gestac  X   X  ao gestation arn C0035668 ribonucleic acid rna ribonucleic acid each pair of terms and clustered the terms 3 . Af-ter numerous trials, we found that a window size of two (on each side of the term), counting each occurrence as 1 (without consideration for loca-tion within the window) and the use of word-form give the best result. The clustering obtained with these values for the three parameters set as above is shown in figure 2. The clusters mirror very closely the memberships between terms and con-cepts. The same settings are therefore used for the rest of the experiments.

It is worth mentioning that other works have also found that the immediate context of a word is more important than the distant context for de-termining the meaning of a word. In (Rapp, 2003), a window size of two words was used to achieve 92.5% correct on the 80 TOEFL questions. 3.2.2 Preliminary evaluation and search
Before we assign concepts to terms extracted from GENOMICA, we thought useful to get an evaluation on how effective VSM can be in as-signing concepts to terms which are extracted di-rectly from UMLS-POR, and for which we know to which concept they belong. For example, as we have seen in table 1, the concept C0006159 ( the production of animals or plants by selective pair-ing ) lists three synonyms in the thesaurus: cruza-mento , procriac  X   X  ao and gerac  X   X  ao . Each synonym may appear a certain number of times in GENOM-ICA. The idea is to evaluate the distance computed using VSM for synonyms but also unrelated word-pairs in relation to their frequency in GENOM-ICA. Table 2 presents this evaluation.

Term Synonyms Unrelated Sample frequency pairs size [ 2, 5 ] 86.5 92.8 30 [ 6, 40 ] 73.8 84.5 26 [ 41,  X  ] 55.1 71.8 36
For example, when we compute the average co-sine distance between two synonyms each with frequency (in GENOMICA) between 2 and 5, we find 86.5, while two unrelated words have a dis-tance of 92.8. The results are consistent with the fact that semantic distance is smaller for syn-onyms appearing often in GENOMICA, for which a more reliable statistical model can be built.
Our objective is to compute distance between terms and concepts, so we can assign, as reliably as possible, each term a concept. Therefore, we computed the distance between 26 terms already in UMLS-POR and 329 concepts 4 . After ranking distances from lowest to highest, we found that in 38% of cases, the correct concept can be found among the top 3%. This finding will help our hu-man expert reduce the search space while review-ing new extracted terms. 3.3 Assigning UMLS concepts to extracted This task is complicated by the fact that there is no guarantee that there exists a relevant concept in the thesaurus. In such cases, we should not find any strong association between the term and con-cepts, which in practice translates to short cosine distances as computed by the VSM. Let us also recognize that not all semantic types defined in UMLS are relevant for genomics. According to (Cohen et al., 2007), the relevant semantic types are Gene or Genome , Nucleic Acid , Nucleoside or Nucleotide , Biologically Active Substance , Idea or Concept , Cell Component , Amino Acid , Peptide or Nucleotide , Genomic Function and their sub-hierarchies 5 . If we name the semantic types men-tioned previously UMLS-GENOMICS-POR, then we are left with a set 3,854 terms distributed in 2,907 concepts, compared with the full UMLS-POR (23,350 terms distributed in 16,491 con-cepts).
 Finally, we can set out to assign concepts from UMLS-POR (UMLS-GENOMICS-POR) to ex-tracted terms from GENOMICA which do not cur-rently exist in the thesaurus. In the previous sec-tion we saw that a frequency of at least 41 was a good threshold for building a fairly reliable statis-tical model. To satisfy this constraint, we must reduce our UMLS-POR (UMLS-GENOMICS-POR) search space from 16,491 (2,907) concepts to 330 (96). Our final task is to assign the top (i.e. with the highest log-odd ratio) 141 (135) terms labelled  X  X es X  (and with frequency &gt; 40) by our human expert to one of the 330 (96) con-cepts. Our human assessor reviewed the rank-ing provided by our VSM and found that the cor-rect match was often found substantially below the top 3% ranked terms for UMLS-POR (UMLS-GENOMICS-POR), limiting the usefulness of the approach and requiring a prohibitively high hu-man expertise and revision. However, when we looked solely at the average distance calculated by VSM, we observed that genomic terms are on average 67% closer to concepts from UMLS-POR (UMLS-GENOMICS-POR) than terms out-side the field of genomics. This shows that the ap-proach is good, but yet not good enough for prac-tical purposes.
Given a reference corpus with a decent size and coverage, the extraction of salient terms from a specialized genomic corpus, albeit small, can be achieved with good precision (70%). Mapping those relevant terms to a set of predefined concepts from the UMLS thesaurus turned out to be much more difficult, as human validation required for the proposed ranking would be excessive. Given that the approach gives fairly good results on eas-ier tasks, we believe that a larger corpus and the inclusion of methods developed elsewhere for the treatment of term variations should alleviate hu-man intervention and render the approach more efficient. It should also be interesting to investi-gate in which respect heterogeneity plus scarcity of texts, but also how human intervention have impacted the results. A Portuguese version of
Wordnet is also available 6 , if needed. We are cur-rently working on extracting and mapping com-plex terms.
 converging (Cabr X , 1999). Traditionally, term i-nology and lexicography have been separate r e-search fields with different approaches to compilation and presentation of data. Howev er, modern technology offers unlimited opportun i-ties to meet the needs for several target groups in one database by offering the possibility of choo s-ing between different presentations, in theory, providing means for knowledge transfer across different inf ormation modes. tematic t erminology work ensure s consistency across the entries of a given database ( ISO 704:2009 ) . This improves the quality of the i n-formation tool considerably compared to other types of specializ ed reference works. T hus, t he end -user is presented with consistent information in a written mode representing the terminological information such as definitions, synonyms, equivale nts and sources . I n practice, however, concept clarification usually takes a more grap h-ical starting point , i n particular, when terminol o-gy and knowledge bank s follow the principles of terminological ontologies (concept systems) pr e-viously dis cussed by Madsen and Thomsen ( 2008) . Here t erminological ontologies are d e-fined as domai n -specific ontologies where ce r-tain aspects of terminology theory have been formalized: Characteristics are modeled by fo r-mal feature specifications (attribute -value pairs) and subdivision criteria that correspond to the attributes of the feature specifica tions. In other words , t erminologists may structure their knowledge by means of concept systems , then develop consistent defin i tions, and subsequently process them into article -like entries using a knowledge management tool ( Madsen, 1999 ) .
T his implies tha t the graphical structuring is ce n-tral to terminology method and theory , and end -u sers should have access into the underlying o n-tology or concept system ( graph mode ) as a complementary source of knowledge .

This short paper presents expe r iment s with the primary purpose of exploring Danish end -users X  understanding of concept sy s tems . The prelim i-nary results imply that d o main -specific knowledge in terminological resources can be transferred efficiently to users across di f ferent levels of expertise and by means of diffe r ent i n-formation modes. The paper ou t lines work -in -progress research and will focus on the info r-mation mode variable . The paper is o r ganized as follows: i n section 2 the method is outlined ; i n section 3 the eye -tracking exper i ments are d e-scribed , and i n section 4 the preli m inary results suggested by the experiments are presented fo l-lowed by a concl u sion in section 5 .
Danish professional potential end -users partic i-pat ing in an eye -tracking exper i ment . 2.1 Information mode s 704:2009; Madsen, 1999), which means that synonyms are registered in one entry in the dat a-base, while lexicography is word -oriented, i.e. one dictionary entry co m prises all meanings of an entry -word. With the use of databases, ho w-ever, the possi bilities for presentation do no longer depend on the structure of the data colle c-tion, and thus it is possible to present data from a termino logical ontology with a co n cept -oriented structure in a word -oriented user interface. Te r-minology resources contain lexicographic (wri t-ten) information such as definitions, sources, synonyms and equivalents, but the terminology work offers a complementary g raphical info r-mation mode disp laying the concept position and relations to other concepts (ISO 704:2009).
Therefore, both information modes carry knowledge that can be transferred to end -users. 2.2 Experiment stimuli information across eight block s. Each block re p-resents a taxation term: direct tax; land tax; mi d-dle -bracket tax; personal income tax ; energy tax; excise duty; green tax; motor vehicles tax . The stimulus template is shown in Figure 1.
 tured in three levels filling out half of the screen . 
Articles are equally sized present ing more d e-tailed information (term, definition, equivalent, and comment including sources of both the Da n-ish source language and the English ta rget la n-guage). The eye -tracking stimuli are thus constructed to provide participants with a do u-ble -mode design and each stimulus comprises three primary areas of interest (AOIs): the AOI -question (at the upper part of the screen); the 
AOI -diagram (placed below); the AOI -article (placed on the opposite side of the diagram) . 
Figure 1 shows an example of excise duty and the question translated into English is  X  X hat type of tax or duty is excise duty? 1. Direct tax; 2. General tax; 3. Indirect tax.  X 
I n the exp eriment, the article entry and co n-cept diagram of the stimuli are static images. I n the real system , users should be allowed fun c-tionalities to unfold concept diagrams and art i-cles further. The stimuli of the eye -tracking experiment can be seen as design a rtefacts clos e-ly resembling articles and concept diagrams of an existing knowledge management tool i -T erm ( DANTERMc entre ) . This approach constitutes a usual starting point of human work interaction design (Clemmensen, 2011). In addition, i t is being assume d that users have entered the term i-nology and knowledge bank correctly and found the concept represented in the relevant diagram (graph mode) or article (written mode) necessary for concept clarification .
The experiment begins with a readin g task u s-ing a specialized text in participants X  first la n-guage (Danish) . T hen 48 multiple -choice concept -clarifying questions (trials) resulting from six types of questions about concept clarif i-c a tion pertai n ing to each of the eight chosen d o-main -specific t erms (blocks) are posed. 
Que s tions can be a n swered by consulting info r-mation in either one of the information modes , or the answer lies in both: The six ques tion types i n clude : sub -ordinates (First di agram -based que s tion); sub -division criteria (Second diagram -based que s tion); equiv a lents (First article -based question); comments (Second article -based que s-tion); s u per -ordinate (First diagram -and article -based question ); characteristics (Second di a-gram -and article -based question ). The six que s-tion types are randomly distributed across the eight blocks . T he eight blocks are also ran do m-iz ed, and so is the display -side of the information modes ( di a gram to the right and article to the left side of the scree n or vice versa ). potential end -users of the termino l ogy and knowledge bank in the taxation domain . eye -trackin g laboratory, where a natural user si t-uation is replicated. The approach is guided by the triangulation principle resulting in both qua n-titative and qualitative data (Holmquist et al., 2011) that will contribute to the understanding of profession al end -users X  performance and perce p-tion which contribute to the subs e quent interface design process , in particular , for the development of personas and scenarios (Nielsen, 2002 ) . specific expertise is measured in a combined a s-sessment compri s ing self -assessment and a test revealing their declarative knowledge in the tax a tion domain . In particular, participants are asked to fill out a background questionnaire comprising a declaration of consent, background infor mation (age, gender, education, industry, typical tasks during their professional working day), introduction to concept clarification and a terminology warm -up exercise.
 multiple -choice questions pe r taining to concept clarification in the taxation domain , while they are presented with the double -mode stim uli and their eye -movements are being recorded. A r e-mote SensoMotoric Instrument ( SMI ) eye -tracker, which supports gaze sampling rates of 50 Hz, is used for the recordings of participants X  on -screen eye -movements. The experiment is built in the psychology software E -prime , which facilitates randomization, records user responses, and informs participants whether they answered correctly or not .
 is conducted with the participants. Here they evaluate their performance, preference and needs pertaining to concept clarification , including their use of taxation texts , in their work . In total, the e x periment lasts about one hour. necessary to distinguish between different types of users, i.e. experts, semi -experts and laymen (Gouws, 2009) . Therefore, it has been crucial in the s ampling of participants for the eye -tracking experiment that they represent different levels of expertise ranging from high expert to low non -expert level. In the sample, half of participants are staff me m bers from The Central Customs and Tax Administratio n (SKAT) working in the taxation domain as e.g. legal advisers, econ o-mists, software developers, business analysts, communicators, translators or generalists. The remaining participants are profession al staff member s from e.g. private companies, univers i-ti es and other government organizations . All pa r-ticipants have Danish as their first language, and all questions and concept diagrams are in Da n-ish.

The background questionnaire primarily a s-sesses declarative knowledge skills in the tax a-tion domain, whereas the eye -tracking experiments also require procedural knowledge or logical reasoning skills. Expertise variables should reflect the expertise needed in the expe r-iments . In order to overcome any discre p ancy between the d eclarative nature of the a s sess ment and the procedural nature of the expertise nee d-ed, t he expertise assessment also comprise pa r-ti c ipants X  information seeking skills and their weekly number of electronic searches in search engines, terminological resources such as enc y-clopedia, dictionary or term banks . 3.3 Eye -tracking measures
In eye -tracking resear ch, the recorded eye movements are analyzed by means of detecting events, i.e. measures accounting for scan paths (where do participants look and do they revisit 
AOIs ) and fixation duration (what do partic i-pants look at and do they fixate on AOIs ) (Holmqui st et al., 2011). In particular, the eye -to -mind -hypothesis (Just and Carpe n ter, 1980) uses eye -movements (fixations) to indicate the cogn i-tive effort needed to process and unde r stand stimuli .
Preliminary results reveal a  X  learning effect  X  which r e duc es the response time s of participants across the 48 trials without reducing the relative number of correct answers . Moreover, high rel a-tive a v e r age fixation duration per trial in the 
AOI -diagram and the AOI -article on diagram and article que s tions respectively , suggests that users  X  know  X  where to look for answers a nd can access info r mation in the graph mode .
 the experiments , but need further testing as part of the inferential statistical analyses : to be high on the expertise measure (experts) are quite critical towards the stimuli . Experts have a high success rate, but they are sometimes co n-fused by the simplified concept diagrams and article entries of this experiment and expres s verbally their disagre e ment. In addition, e xperts express a high preference for the detailed and precise articles compared to diagrams, which they might be confused by. O nce they have learned to navigate the experiment, they start appreciating the advantages of diagrams, esp e-cially if they were new to a field, including the double -mode interface d e sign.
 to be low on the exper tise measure (non -experts) have hardly any opinion on the taxation domain .
Non -experts tend to be overwhelmed by the complex taxation domain and spend quite a long time understanding the questions and i n fo r-mation m odes. Ho w ever, if the long response time is disregarded, non -experts perform quite well. The learning effect also applies to non -experts who learn to navigate the information modes across the expe r iment.
 experiments. It seems to lead participants to fuzzy scan paths and random guesses if they do not locate an answer in side the stimuli space . 
This impatience is due to the time pressure that participants feel they are perfor m ing under and the fact that the answer  X  X o not know X  is not available to them. Opposed to the inherent imp a-tience, which tends to shorten the response time, an inherent insecurity tends to prolong the r e-sponse time. The inherent insecur i ty makes pa r-ticipants search for ans wers they already know or have already found. perimental design, it can be concluded that d o-main -specific knowledge is transferred across written and graph modes to both experts and non -e xperts . It should be noted that complete d e scriptive and inferential statistical analyses are in progress. In addition, t he eye -tracking exper i-ments constitute a first step, which need s to be followed by future research on a dynamic system version offering pa r ticipants the possibility of interact ing with the article entries and concept diagrams of the terminology and knowledge bank .
 T he research is funded by the VELUX 
F OUNDATION and constitutes sub -project three of the DanTermBank project which aims at d e-veloping the foundations for the establishment of a terminol o gy and knowledge base in Denmark .
Bodil Nistrup Madsen. 1999. Terminologi 1. Princi p-per og metoder. Copenhagen, Gads Forlag.

Bodil Nistrup Madsen and Hanne Erdman Tho msen. 2008. Terminological Principles Used for Ontol o-gies. In: Proceedings from the International Co n-ference on Terminology and Knowledge Engineering. Managing Ontologies and Lexical Resources, pp. 107 -122.
 DANTERMcentre. DOI= http://www.iterm.dk/ .

ISO 704 : 2009. Terminology work. Principles and methods. International Organization for Standard i-zation.

Kenneth Holmquist; Marcus Nystr X m; Richard A n-dersson; Richard Dewhurst; Halszka Jarodzka; 
Joost Van De Weijer. 2011. Eye tracking. A co m-prehensive guide to methods and measures. Oxford University Press.

Lene Nielsen. 2002. From user to character: an inve s-tigation into user -descriptions in scenarios. In Proc. DIS2002, ACM, 99 -104 .

M. A. Just and P. A. Carpenter. 1980. A the o ry of reading: From eye fixations to comprehen sion , Psychological Review 87(4), 329 -354.

Maria Teresa Cabr X . 1999. Terminology. John Be n-jamins Publishing Company.

R ufus H. Gouws. 2009. Integrated dictionary use of specialised dictionaries for learners. Lex ikos. 19:72 -93.

Serge Verlinde, Patrick Leroyer, and J ean Binon. 2010. Search and You Will Find. From Stand -
Alone Lexicographic Tools to User Driven Task and Problem -oriented Multifunctional Leximats. 
International Journal of Lexicography vol. 23 (1): pp. 1 -17.

Thorkil Clemmensen. 2011. Designing a simple fol d-er structure for a complex domain. Human Tec h-nology, 7(3), 216 -249 .
Pragmatics is a relatively young area of Lingui s-tics. In effect , according to Crystal (1992), at the beginning of the nineties,  X  X o coherent pragmatic theory X  had  X  X een achieved, mainly because of the variety of topics it has to account for X , such as (i) speech acts (Searle, 1975); (ii) deixis, pr e-suppositions and implicatures (Levinson, 1983; 
Grice 1975; 1989); or (iii) pragmatic coherence relations (Hovy and Maier, 1995; Romera, 2004; Asher and Lascarides, 2003; Pr X vot, 2004) . view s of p ragmatics can be found in Yule (1996). Yet , the different theories and a p-proaches to p ragmatics are even now quite fra g-mentary and dis connected. Thus, the way to link the pragmatic categories derived f rom these theories is not obvious . Besides, most of these theories are under development and lack a proper sub-classification of the pragmatic objects and phenomena that they study (Pareja -Lora, 2012b; 2013a) . They are usually based on some exam-ples that s upport their assumptions , but which are insufficient to apply these theories to unr e-stricted texts and dialogues . Furthermore, the sub-classification s and the particularizations of these theories must be frequently re -defined ad hoc for each project (Pareja -Lora et al., 2013b) and are often incomplete and/or biased towards each project assumptions . that account for pragmatic categories are partial, disconnected and/or poorly detailed. Thus, they require b eing fully subcategorized and deve l-oped, as well as being conveniently linked, in order to complement each other and overcome their fragmentation . This was the main aim of the research presented in Pareja -Lora (2012b; 2013a) : the creation of a comprehensiv e concep-tualization that linked these pragmatic categ o-ries , suitable for the pragmatic annotation of texts and dialogues. This was achieved by means of the development of some ontology modules , since they have already been successfully ap-plied for similar purpose s (Chiarcos, 2008; Buitelaar et al. , 2009) . lowed to build and link these ontology modules and other similar ones ( Pareja -Lora, 2012a; 2012c; Pareja -Lora and Aguado de Cea, 2010) , together with a number of recommendations and lessons learned from their development and our previous experiences in this area. They helped us evaluat e, reuse and merge these terminological resources as well as make them interoperate. 
Moreover, t hey also helped us identify their t er-minological gaps and decide how to fill them in systematically . details how we built the ontology module of pragmatic categories, suitable for pragmatic an-notation , by reusing, merging and linking some other terminological and language resources. 
Section 3 present s the ontology -driven recom-mendations for terminological works that we identified as we built this ontology , as well as other related ones. Finally, s ecti on 4 unfolds the conclusions of this research . 
As explained in the Introduction, this research originated from the need to link and to suppl e-ment several terminological and language re-sources dealing with p ragmatics categories. 
Towards this end, we built some interrelate d ontological modules that could give a coherent and interoperable view over them and help in their annotation. For their development, we used ples , the main problems that we had to face when developing one of the ontological modules aforementioned , namely the one including pra g-matic relations and units . First, we present the problems associated to linking and merging the different terminological resources that we r e-used. Second, we discuss how we identified and filled the resulting terminological gaps.
Figueroa et al., 2012). 2.1 Linking and Merging Terminological 
As mentioned above, this section presents how we overcame the problems that we had to face when devel oping our ontological module of pragmatic relations and units . In particular, the part dealing with pragmatic relations had to pr o-vide an ontological hierarchy of object properties as a result , despite object property hierarchies are not very common as yet (this is a fairly new fea-troduces the methodology we followed to build it (Subsection ); h ow the top -level hierarchy of pragmatic unit s was generated (Subsection 2.1.2); and how we linked both hierarchies t o-gether (Subsection 2.1.3) . 2.1.1 Linking and Merging the Resources 
The top-level classes of the ontological modules and the relations that link them were fairly easy to identify and model. First ly, we retrieved a taxonomy of discourse and pragmatic coherence relations, elaborated by Hovy and Mai er (1995). 
This taxonomy survey ed the discourse and pragmatic coherence relations identified in the literature so far. All we had to do was to forma l-ize the pragmatic counterpart of this taxonomy in our ontology module s. This process was quite straightforw ard : i) We included two new object properties in the ii) We linked them both, stating that the latter is iii) Then, we followed a similar process to for-coherence relations differently . For instance, the term interpersonal coherence relation is som e-times used instead (Hovy and Maier, 1995) . So, to account for these other equivalent terms, we added a new rdfs:subPropertyOf Pragma-ticRelation to the ontology, i.e. Inter-personalCoherenceRelation , and added also an owl:equivalentTo statement , that link ed the latter to PragmaticCoherence-Relation . 
PragmaticCoherenceRelation with two labels, namely  X  X ragmatic coherence re-lation X  X en and  X  X nterpersonal coher-ence relation X  X en instead, by means of the rdfs:label annotation property. However, we found it more adequate to model this relationship at the conceptual level, since (a) it allows to d e-velop a sub-ontology with the terminology of each author and/ or theory for the corresponding ontological items (objects or object properties), and link all equivalent terms and/or ontological items also at the conceptual level; (b) any change in the definition of one of these terms that turns it into a new ontological item requires simply to remove the corresponding owl:equivalentTo or owl:equivalentClass statement; or else, in the latter case, substitute it with the definition of a new suitable relation (i.e . obje ct property); and (c) the use of the rdfs:label is fairly la n-guage -dependent . Thus, our approach is more abstract and modular, as well as theory -and la n-guage -independent. The same reasoning was applied in what follows.
 ation object property and its related term were theory -dependent. Including only this equivalent would have entail ed biasing the ontology to some extent. In effect , s ome potential users of the ontology would not find their own way to refer to this concept i n the ontology , and discard (re -)using it. So , just in case, we decided to (a) try and identify in the literature any other equivalent terms referring to this concept ; and (b) model and link them analogously . Eventua l-ly, we found the term participation framework relation (Schiffrin 1987; Redeker, 1990) to be equivalent . So we added the concept Partici-pation-RelationFramework to the ontology and linked it to the other two equivalent concepts by means of two owl:equivalentTo stat e-ments as wel l. pragmatic relations. We used two different te r-minological sources towards this end: a termin o-logical resource in the domain of linguistics, i.e. and a Pragmatics b ook, i.e. Yule (1996). From SIL/GLT we extracted the terms Exophora , 
Homophora , and Deixis (and its sub types ); and from Yule (1996), Hedging (and its subclasses ), 
Mitigation , and AdjacencyPairRelation (and its sub types ). Finally, we modeled them in this ontological module as well (as object prope r-ties, like pragmatic coherence relations ). 2.1.2 Linking and Merging the Resources 
Then , we had to detail the pragmatic units that are inter -related by the pragmatic relations dis-cussed above . So we added a class hierarchy to this ontological module to formalize this type of pragmatic elements. Building this hierarchy overall was not easy, since the literature does not identify clearly what pragmatic units are or how they can be sub-classified (cf. Yule, 1996).
However, we found out that 1. The different pragmatic coherence relations 2. Pragmatic coherence relations link together 3. Some authors refer to pragm atic units as 4. There is yet another and most prominent type 
PragmaticUnit class and the concep ts Prag-maticFunctionalUnit 6 2.1.3 Linking Pragmatic Units by M eans of tion , Pragmateme and SpeechAct as sub-classes of the former concept. We also (a) creat-ed a sub -ontology of PFU s, quite similar to the sub-ontology of pragmatic relations identified in 
Hovy and Maier (1995) ; and (b) included the subclasses o f SpeechAct traditionally identified in the literature, namely Representative , Expressive , Commissive , Directive and Declaration (Searle, 1975; Yule 1996) . 
Finally, we had to link these pragmatic units by means of the pragmatic relations that hold among them. This was achieved by including in the ontology the corresponding rdfs:domain and rdfs:range statements. For example, we co n-strained (a) the domain and the range of a 
PragmaticCoherenceRelation to Macro-proposition ; and (b) the domain and the range of AdjacencyPairRelation to 
SpeechAct . 2.2 Identifying and Filling Termino logical 
Up to this point , we had already developed a couple of ontological taxonomies , whose con-cep ts were not uniformly sub-classified . Thus , we had to discover if some other concepts and/or sub-ontologies were missing and, if so, model them as well . We found out that, for example, the hierarchy of speech acts had not been suff i-ciently detailed for their application to pragmatic annotation yet . So it was built practically from scratch as explain ed below . The remaining sub-ontologies and gaps identified in our ontology were filled following the same approach. 2.2.1 Developing the Speech Act Sub-
In order to build the sub-ontology of speech acts, we followed a top-down approach, since we had at least (1) the top -level sub -classification of speech acts into representatives, expressives, commissives, directives and declarations (Searle, 1975; Yule, 1996); (2) some examples of these sub-classes of speech acts included in Yule (1996) and in SIL/GLT; and (3) a contrastive (English -Spanish) taxonomy of expressive and fore being previously evaluated for its suitability as for a speech act taxonomy. Some of its verbs (e.g. pronounce) did not refer to real speech acts and, besides, we had to identify other possible speech acts not included in this classification (its potential gaps ). Thus, s ome general -purpose language resources were (re -)used towards this end (see below) . speech acts from the entries against the defin i-tions of a general -purpose language resource found out that some of the related definitions were circular , tautological and/or inconsistent. state that Command and Advice are subclasses of Directive . So, to build this part of the hie r-archy, we looked up their corresponding verbs in  X  advise : tions:  X  command : (Faber and Mairal, 1999 ) is  X  X o tell X . This, recu r-sively, led us to look for a relevant definition of  X  X ell X  , and we found out that it entailed a taut o-logical definition :  X  tell : fers to  X  X dvise X . Besides, when we looked up  X  X rder  X  in this dictionary , we found out that it contained another circular definition involving  X  X ell X  :  X  order : consistent, since  X  X rder X  might entail  X  X  position of authority X  of the speaker that is not a real characteristic of  X  X ell X .
 by evaluating these terms with MW LD 10 1. We searched for the top-level performative does not include circular and/or tautological def-initions for them. Th us, we built the sub-classification of speech acts of Searle (1975) and 
Yule (1996) as follows: 2. We compiled a list of the ir related terms in 3. We looked up their definitio n in the M WLD, 2.2.2 An Algorithm to Build Terminological 
To develop our hierarchy, we used an algorithm that we designed following th e approach in Dik (1989) to construct stepwise lexical defini tions (also followed in Mairal -Us X n and Peri X  X n-
Pascual (2010) to define stepwise conceptual decompositions ). the terms we had already selected as follows. 
First, we took the term definitions and split them into their genus and differentiae . Second, we further split their differentiae into their basic components (that is, into their semantic features and/or characteristic type values (ISO/ /DIS 24156-1) ). Third, we grouped together the terms that shared both their genus ( G ) and one differe n-tia basic component ( DBC ). Fourth, we included a concept C in the ontology for G (if no t yet pr e-sent), a nother concept, C X  , for the compositional semantics of G + DBC , and an owl: subClassOf statement that linked C X  to C . 
Fifth, we removed the terms for C and C X  from the list (if present). And sixth, we iterated this process until we had incorporated a concept for tion, we attached its associated differentia basic components to each concept in the ontology , as data properties and property values  X  see General 
Recommendation . 
The hybrid terminological and ontological r e-search works described above (as well as some other related works, not described here for space driven best practices that should be applied on ontology-based terminological works. They can be classified, according to their scope, as (1) general recommendations; and (2) recommenda-tions for terminological resource merge and link . 
They are discussed in a dedicated subsection below. 3.1 General R ecommendations
The main general recommendations for develop-ing ontology-based terminological works are the following: 1. Choose a convenient ontology development 2. Select a n ontology development tool 3. Concept identifiers should be as explicit as 4. Use pervasively th e rdfs:label property 5. Whenever possible, use the rdfs: 6. In order to ensure the completeness of your 7. Taxonomise as much as possible . This crit e-3.2 Recommendations for Terminological 
When merging different terminolo gical resources by means of an ontology, 1. Follow a conceptual, comprehensive, gen-2. Link within the ontology all the terms that 3. Fill in the conceptual and/or terminological 4. The rdfs:label property allows specifying 
In this paper, we have presented how we built a set of ontology modules that include pragmatic categories . These ontology module s have been built on several other terminological and la n-guage resources, in order to overcome the biases, disconnections and lack of details and sub-classificat ion that this area shows altogether. In particular, our ontological modules provide a comprehensive abstract model over the termino l-ogy of p ragmatics and pragmatic categories , which has helped link, merge, comple ment and extend these different resources, a nd make them interoperate . lowed t o build these ontology modules . In pa r-ticular, we have shown the usefulness of object property hierarchies in this type of terminolog i-cal works and how they can be easily created. 
Our methodology includes also a set of reco m-mendations and lessons learned so far when modeling this and other ontology-based termin o-logical resources in the linguistic domain. hierarchies of concepts, which constitute the backbone of conceptual models in terminology works and/or in ontologies. As ontological te r-minology works become more frequent, these recommendations will become more necessary and useful to (a) reuse , link and merge other language or terminological resources; and (b) build them from scratch. They describe also how these hierarchies should be systematically evaluated in order to identify terminological gaps and fill them. Besides, t hey pr ovide some useful hints to name and annotate ont ology concepts with their designating terms and definitions , both in monolingual and multilingual environments . 
Overall, they provide a sound basis for ontol o-gies to be use d as terminology managers , which is one of the possible ways in which ontological and terminological research works might con-verge in the future. 
Nicholas Asher and Alex Lascarides. 2003. Logics of conversation . Cambridge University Press. Ca m-bridge. UK.
 Willem N. Borst. 1997. Construction of Engineering Ontologies . PhD thesis. University of Twente. Enschede. Netherlands.
 Paul Buitelaar, Philipp Cimiano, Peter Haase, and Michael Sintek. 2009. Towards Linguistically 
Grounded Ontologies. In The Semantic Web: R e-search and Applications ( Lecture Notes in Co m-Heidelberg: Springer.
 Chiarcos, Christian. 2008. An Ontology of Linguistic Annotations. In LDV Forum (GLDV -Journal for 
Computational Linguistics and Language Technol-ogy) , 23(1):1 -16.
 David Crystal. 1992. A Dictionary of Linguistics and 
Thierry Declerck and Dagmar Gromann. 2012. T o-wards the Generation of Semantically Enriched Multilingual Components of Ontology Labels . In 
Proceedings of the 3rd Workshop on the Multili n-gual Semantic Web (MSW3) . Boston, USA, N o-vember 2012.
 Simon C. Dik. 1989. The Theory of Functional Grammar. Part I: The Structure of the Clause . Dordrecht: Foris Publications.

Pamela Faber and Ricardo Mairal. 1999. Constructing a lexicon of English verbs . Berlin: Mouton de Gruyter . 
Christiane Fellbaum (ed.). 1998. WordNet: An Ele c-tronic Lexical Database . Cambridge, MA: MIT Press.

Alberto Garc X a -Torres, Antonio Pareja -Lora and D a-niel Pradana-L X pez. 2008. Reutilizaci X n de tesa u-ros: el documentalista frente al reto de la web sem X ntica. In El Profesional de la Informaci X n , 2008, January -February, vol. 17 (1), pp. 8 -21.
Blanca Gil -Urdiciain. 2004. Manual de lenguajes 
Asunci X n G X mez -P X rez, Mariano Fern X ndez -L X pez
Herbert P. Grice. 1975 (1989). Logic and convers a-Thomas R. Gruber. 1993. A Translation Approach to 
Eduard Hovy and Elisabeth Maier. 1995. Parsimon i-International Organization for Standardization. 2013. Marie-Claude L'Homme and Gabriel Bernier -Stephen C. Levinson. 1983. Pragmatics . Cambridge: 
Ricardo Mairal -Us X n and Carlos Peri X  X n -Pascual. Igor Aleksandrovi X  Mel  X  X uk . 2001. Communicative 
Antonio Pareja -Lora. 2012a . Providing Linked Li n-
Antonio Pareja -Lora. 2012b. OntoLingAnnot X  X  O n-Antonio Pareja -Lora. 2012c . OntoLingAnnot's LRO: neering Conference (TKE 2012) . Madrid, June 2012, pp. 49-64. [http://www.oeg -upm.net/ tke2012/pr oceedings, paper 04] Antonio Pareja -Lora . 2013a . The pragmatic level of 
OntoLingAnnot X  X  ontologies and their use in pragmatic annotation for language teaching. In J. 
Ar X s, M.E., B X rcena, and T. Read (eds.) La n-guages for Special Purposes in the Digital Era . Springer [IN PRESS].

Antonio Pareja -Lora , Mar X a Blume and Barbara Lust . 2013b. Transforming the Data Transcription and 
Analysis Tool Metadata and Labels into a Lingui s-tic Linked Open Data Cloud Resource. In Proceed-Linguistics (LDL -2013) . Pisa, September 2013 [IN PRESS].

Antonio Pareja -Lora and Guadalupe Aguado de Cea. 2010. Modelling Discourse -related terminology in OntoLingAnnot X  X  ontologies. In Proceedings of the 
TKE 2010 workshop  X  X stablishing and using on-tologies as a basis for terminological and know l-edge engineering resources X  . Dublin, August 2010.

Laurent Pr X vot. 2004. Structures s X mantiques et pragmatiques pour la mod X lisation de la coh X rence dans des dialogues finalis X s . Th X se de doctorat de l'univers it X  Paul Sabatier. Toulouse. France.
Gisela Redeker. 1990 . Ideational and pragmatic markers of discourse structure. In Journal of Pragmatics , 14: 367-381. Magdalena Romera. 2004. Discourse Functional Units: the Expression of Coherence Relations in Spoken Spanish . Munich: LINCOM.

Deborah Schiffrin. 1987. Discourse markers . Ca m-bridge: Cambridge University Press . John Searle. 1975. Indirect speech acts. In Syntax and 
Semantics , 3: Speech Acts (P. Cole and J. L. Mo r-gan, eds.): 59  X 82. Academic Press. New York. USA.
 Mari Carmen Su X rez-Figueroa, Asunci X n G X mez -P X rez and Mariano Fern X ndez-L X pez. 2012. The NeOn Methodology for Ontology Engineering. In 
M.C. Su X rez-Figueroa et al. (eds.) Ontology Engi-neer ing in a Networked World . Berlin Heidelberg, Springer -Verlag . 
Deborah Tannen. 1982. Analyzing Discourse: Text and Talk . Georgetown: Georgetown University Press.

W3C. 2012. OWL 2 Web Ontology Language  X  Do c-ument Overview (Second Edition) [http://www.w3.org/TR/owl2 -overview/ ]. George Yule. 1996. Pragmatics . Oxford University 
Press. Oxford. UK.
 Preface 5 Invited talks 7 Contents 11 Commitee 15 Session : Representing Multilingual Linguistic Knowledge 17 Session : Term extraction 43 Session : Short papers 77 Session : Acquiring Semantic Relations in Linguistic Resources 111 Session : Medical terminologies 153 Session : Terminologies and ontologies 179 Author table 197 Conference Chair : Guadalupe Aguado de Cea (OEG, Universidad Polit X cnica de Madrid, Spain) Nathalie Aussenac-Gilles (IRIT, CNRS, Toulouse, France) Amparo Alcina Universitat Jaume-I, Castell X n de la Plana, Spain Sophia Ananiadou NaCTeM, Manchester, UK Caroline Barri X re CRIM, Montr X al, Canada Paul Buitelaar DERI, Galway, Ireland Maria Teresa Cabr X  Universitat Pompeu Fabra, Spain Farid Cerbah Dassault Aviation, Paris, France Jean Charlet AP-HP &amp; INSERM, Paris, France Philipp Cimiano University of Bielefeld, Germany Anne Condamines CLLE-ERSS, Toulouse, France B X atrice Daille LINA, Universit X  de Nantes, Nantes, France Val X rie Delavigne Institut National du Cancer, France Pascaline Dury Universit X  Lyon 2, Lyon, France Fidelia Ibekwe-San Juan Universit X  Lyon 3, France Marie-Christine Jaulent INSERM, France Kyo Kageura University of Tokyo, Japan Olivia Kwong City University Hong Kong, Hong Kong, China Marie-Claude L X  X omme (OLST, Universit X  de Montr X al, Canada Elena Montiel-Ponsoda Universidad Polit X  X cnica de Madrid, Spain Adeline Nazarenko LIPN, Universit X  Paris 13 SPC -CNRS, Villeta-Pascale S X billot IRISA, Rennes, France Monique Slodzian CRIM-INALCO, Paris, France Mari Carmen Suarez-Figueroa Sylvie Szulman LIPN, Universit X  Paris 13 SPC -CNRS, Villeta-Annette Ten Teije Free University Amsterdam, Netherlands Koichi Takeuchi Okayama University, Japan Rita Temmerman Erasmushogeschool, Belgium Yannick Toussaint LORIA, Nancy, France  X pela Vintar University of Ljubljana, Ljubljana, Slovenia Pierre Zweigenbaum LIMSI-CNRS &amp; CRIM/ERTIM-INALCO, Paris, Presidents : Adeline Nazarenko (LIPN-Universit X  Paris 13 SPC) Sylvie Szulman (LIPN-Universit X  Paris 13 SPC) Laurent Audibert LIPN, Universit X  Paris 13 SPC Ines Bannour LIPN, Universit X  Paris 13 SPC Sondes Bannour LIPN, Universit X  Paris 13 SPC Davide Buscaldi LIPN, Universit X  Paris 13 SPC Fran X ois L X vy LIPN, Universit X  Paris 13 SPC Jorge J. Garcia Flores LIPN, Universit X  Paris 13 SPC Ehab Hassan LIPN, Universit X  Paris 13 SPC Thibault Mondary LIPN, Universit X  Paris 13 SPC Nada Mimouni LIPN, Universit X  Paris 13 SPC Antoine Rozenknop LIPN, Universit X  Paris 13 SPC Sylvie Salotti LIPN, Universit X  Paris 13 SPC Nadi Tomeh LIPN, Universit X  Paris 13 SPC Jonathan Van Puymbrouk LIPN, Universit X  Paris 13 SPC Haifa Zargayouna LIPN, Universit X  Paris 13 SPC
 Series Editors Jaime G. Carbonell, Carnegie Mellon University, Pittsburgh, PA, USA J X rg Siekmann, University of Saarland, Saarbr X cken, Germany Volume Editors Yuji Matsumoto Nara Institute of Science and Technology, Japan E-mail: matsu@is.naist.jp Richard Sproat University of Illinois at Urbana-Champaign Dept. of Linguistics, Dept. of Electrical Engineering, USA E-mail: rws@xoba.com Kam-Fai Wong The Chinese University of Hong Kong Department of Systems Engineering and Engineering Management Shatin, N.T., Hong Kong E-mail: kfwong@se.cuhk.edu.hk Min Zhang
Institute for Infocomm Research 21 Heng Mui Keng Terrace, Singapore 119613 E-mail: mzhang@i2r.a-star.edu.sg Library of Congress Control Number: 2006937162 CR Subject Classification (1998): I.2.6-7, F.4.2-3, I.2, H.3, I.7, I.5 LNCS Sublibrary: SL 7  X  Artificial Intelligence ISSN 0302-9743 ISBN-10 3-540-49667-X Springer Berlin Heidelberg New York ISBN-13 978-3-540-49667-0 Springer Berlin Heidelberg New York The International Conference on Computer Processing of Oriental Languages (ICCPOL) is a regular conference series of the Chinese and Oriental Languages Computer Society, COCLS (formerly known as the Chinese Language Computer Society, CLCS), which was made in the winter of 2005 in response to the growing international demand in Chinese and oriental languages research and applications. The new vision of the society was also launched at the same time. COLCS was set "to be the international computer society driving the advancement and globalization of the science and technology in Chinese and oriental languages processing." On this front, the society's conference, ICCPOL, and journal, namely, the International Journal on Computer Processing of Oriental Languages (IJCPOL) provide two effective platforms. 
This year marked the 21st meeting of the ICCPOL conference. I was delighted that despite his heavy workload, Kim-Teng Lua kindly accepted my invitation to host ICCPOL 2006 in Singapore. He put together one of the most energetic organizing committees: Minghui Dong, who looked after the local organization including the conference Website, Hui Wang the registration and Min Zhang the publication. Without their dedication and professionalism, ICCPOL 2006 would not have been so successful. 
I am grateful to the Department of Systems Engineering and Engineering Manage-ment at The Chinese University of Hong Kong not only for allowing me to take up the conference chairmanship but, even more importantly, for providing financial aid for me to assess every application in detail. I would also like to thank the program Co-chairs Yuji Matsumoto and Richard Sproats, who jointly worked out an inspiring program. The combination of Asian and American scientists supported our theme of "Beyond the Orient." Many high-quality papers from all around the world were received and unfortunately due to limited space, only a few were accepted for publication in this year's proceedings. The accepted papers truly highlighted "The Research Challenges Ahead" in Chinese and Oriental language processing. As the Co-chairs of the technical program of the 21st International Conference on Computer Processing of Oriental Languages (December 17-19, Singapore) we are delighted and honored to write the introduction to these proceedings. The subtitle of this year's convocation was "Beyond the Orient: The Research Challenges Ahead," the goal being to broaden the scope of ICCPOL beyond its origins successful first step in this direction both in the composition of the Program Committee, which is made up of members from countries around the world and the accepted papers, which include a large number of contributions from outside ICCPOL's traditional home base. 
We received 169 submissions from a variety of countries. We initially accepted only 38 full papers (30 oral presentations and 8 poster presentations determined by the authors' preference), but since this was a fairly small set, we decided to accept another 20 papers as short papers, all of which were presented as posters. Thus, the acceptance Since two papers were withdrawn after the notification, this volume includes 56 papers (36 full papers and 20 short papers) presented at the conference. 
As Technical Co-chairs we can claim but a small amount of credit for the success of the technical program. Our main thanks go to the Program Committee members who worked diligently to give fair reviews of the submitted papers, and most of whom spent additional time coming to a consensus on papers where there was a wide amount of disagreement. 
We also thank the many authors who submitted their work for inclusion in the conference. Needless to say, the conference would not exist were it not for the technical presentations. We are mindful of the fact that there are many computational linguistics conferences and workshops available, and we are therefore happy that so many papers were submitted to ICCPOL 2006. 
We would also like to thank our invited keynote speakers Gerald Penn, Claire Cardie and Hwee-Tou Ng for agreeing to present their work at ICCPOL. 
In this year's conference, we used the START Conference Manager System for most of the paper handling process, that is, paper submission, paper reviews and discussion, the system, who was always quick to answer our queries, and even modified the system to handle the specific needs of our conference. We would also like to thank the committee members of ICCPOL, especially Kam-Fai Wong for his continuous support and timely advice, Minghui Dong for preparing very beautiful Web pages, and Min Zhang and Wai Lam for handling all the final manuscripts that are included in this volume. December 2006 Yuji Matsumoto and Richard Sproat Honorary Conference Co-chairs Shi-Kuo Chang, University of Pittsburgh, USA (Co-founder, COLCS) Benjamin Tsou, City University of Hong Kong, China (President, AFNLP) Jun X  X chi Tsujii, University of Tokyo, Japan (President, ACL) Conference Chair Kam-Fai Wong, The Chinese University of Hong Kong, China Conference Co-chair Jong-Hyeok Lee, POSTECH, Korea Organization Chair Kim-Teng Lua, COLIPS, Singapore Program Co-chairs Yuji Matsumoto, Nara Institute of Science and Technology, Japan Richard Sproat, University of Illinois at Urbana-Champaign, USA General Secretary Minghui Dong, Institute for Infocomm Research, Singapore Publication Co-chairs Min Zhang, Institute for Infocomm Research, Singapore Wai Lam, Chinese University of Hong Kong, China Finance Co-chairs Chris Yang, Chinese University of Hong Kong, China Hui Wang, National University of Singapore, Singapore Galen Andrew, Microsoft Research, USA Masayuki Asahara, Nara Institute of Science and Technology, Japan Hsin-Hsi Chen, National Taiwan University, Taiwan Keh-Jiann Chen, Academia Sinica, Taiwan David Chiang, ISI, USA Lee-Feng Chien, Academia Sinica, Taiwan Key-Sun Choi, KAIST, Korea Susan Converse, University of Pennsylvania, USA Robert Dale, Macquarie University, Australia Guohong Fu, Hong Kong University, China Pascale Fung, Hong Kong University of Science and Technology, China Niyu Ge, IBM T. J. Watson Research Center, USA Julia Hockenmaier, University of Pennsylvania, USA Liang Huang, University of Pennsylvania, USA Kenji Imamura, NTT, Japan Kentaro Inui, Nara Institute of Science and Technology, Japan Martin Jansche, Columbia University, USA Donghong Ji, Institute for Infocomm Research, Singapore Gareth Jones, Dublin City University, Ireland Genichiro Kikui, NTT, Japan Sadao Kurohashi, University of Tokyo, Japan Kui-Lam Kwok, City University of New York, USA Olivia Oi Yee Kwong, City University of Hong Kong, China Gary Geunbae Lee, POSTECH, Korea Gina-Anne Levow, University of Chicago, USA Roger Levy, University of Edinburgh, UK Haizhou Li, Institute for Infocomm Research, Singapore Hang Li, Microsoft Research Asia, China Mu Li, Microsoft Research Asia, China Wenjie Li, Polytechnic University of Hong Kong, China Chin-Yew Lin, ISI, USA Qin Lu, Polytechnic University of Hong Kong, China Bin Ma, Institute for Infocomm Research, Singapore Qing Ma, Ryukoku University, Japan Helen Meng, Chinese University of Hong Kong, China Tatsunori Mori, Yokohama National University, Japan Hwee Tou Ng, National University of Singapore, Singapore Cheng Niu, Microsoft Douglas Oard, University of Maryland, USA Kemal Oflazer, Sabanci University, Turkey Manabu Okumura, Tokyo Institute of Technology, Japan Martha Palmer, University of Colorado, USA Hae-Chang Rim, Korea University, Korea Laurent Romary, LORIA, France Tetsuya Sakai, Toshiba, Japan Rajeev Sangal, International Institute of Information Technology, India Jungyun Seo, Sogang University, Korea Kiyoaki Shirai, Japan Advanced Institute of Science and Technology, Japan Dawei Song, Open University, UK Virach Sornlertlamvanich, Thai Computational Linguistics Lab., Thailand Keh-Yih Su, Behavior Design Corporation, Taiwan Jian Su, Institute for Infocomm Research, Singapore Maosong Sun, Tsinghua University, China Kumiko Tanaka-Ishii, University of Tokyo, Japan Takenobu Tokunaga, Tokyo Institute of Technology, Japan Kiyotaka Uchimoto, NICT, Japan Takehito Utsuro, Tsukuba University, Japan Hui Wang, National University of Singapore, Singapore Patrick Wang, Northeastern University, USA Andi Wu, Microsoft, GrapeCity Inc., USA Fei Xia, University of Washington, USA Yunqing Xia, The Chinese University of Hong Kong, China Bo Xu, Chinese Academy of Sciences, China Jie Xu, National University of Singapore, Singapore and Henan University, China Nianwen (Bert) Xue, University of Colorado, USA Tianshun Yao, Northeastern University, China Zaharin Yusoff, Malaysian Institute of Micro-Electronics, Malaysia Min Zhang, Institute for Infocomm Research, Singapore Guodong Zhou, Institute for Infocomm Research, Singapore Ming Zhou, Microsoft Research Asia, China Hosted by the Chinese and Oriental Languages Computer Society (COLCS) Organized by the Chinese and Oriental Languages Information Processing Society (COLIPS) Supported by Asian Federation of Natural Language Processing (AFNLP) Department of Systems Engineering and Engineering Management (SEEM), The Chinese University of Hong Kong, China Publisher Springer Answering Contextual Questions Based on the Cohesion with Knowledge .................................................. 1 Segmentation of Mixed Chinese/English Document Including Scattered Italic Characters .................................................. 13 Using Pointwise Mutual Information to Identify Implicit Features in Customer Reviews .............................................. 22 Using Semi-supervised Learning for Question Classification ............. 31 Query Similarity Computing Based on System Similarity Measurement ..................................................... 42 An Improved Method for Finding Bilingual Collocation Correspondences from Monolingual Corpora ......................................... 51 A Syntactic Transformation Model for Statistical Machine Translation ....................................................... 63 Word Alignment Between Chinese and Japanese Using Maximum Weight Matching on Bipartite Graph ................................ 75 Improving Machine Transliteration Performance by Using Multiple Transliteration Models ............................................. 85 Clique Percolation Method for Finding Naturally Cohesive and Overlapping Document Clusters ................................ 97 Hybrid Approach to Extracting Information from Web-Tables .......... 109 A Novel Hierarchical Document Clustering Algorithm Based on a kNN Connection Graph ................................................ 120 The Great Importance of Cross-Document Relationships for Multi-document Summarization ................................. 131 The Effects of Computer Assisted Instruction to Train People with Reading Disabilities Recognizing Chinese Characters .............. 139 Discrimination-Based Feature S election for Multinomial Na  X   X ve Bayes Text Classification ................................................ 149 A Comparative Study on Chinese Word Clustering .................... 157 Populating FrameNet with Chinese Verbs Mapping Bilingual Ontological WordNet with FrameNet ................................ 165 Collecting Novel Technical Terms from the Web by Estimating Domain Specificity of a Term .............................................. 173 Building Document Graphs for Multiple News Articles Summarization: An Event-Based Approach ......................................... 181 A Probabilistic Feature Based Maximum Entropy Model for Chinese Named Entity Recognition ......................................... 189 Correcting Bound Document Images Based on Automatic and Robust Curved Text Lines Estimation ...................................... 197 Cluster-Based Patent Retrieval Using International Patent Classification System .............................................. 205 Word Error Correction of Continuous Speech Recognition Using WEB Documents for Spoken Document Indexing ........................... 213 Extracting English-Korean Transliteration Pairs from Web Corpora ..... 222 From Phoneme to Morpheme: Another Verification Using a Corpus ......................................................... 234 Chinese Abbreviation Identification Using Abbreviation-Template Features and Context Information ................................... 245 Word Frequency Approximation for Chinese Using Raw, MM-Segmented and Manually Segmented Corpora ................................... 256 Identification of Maximal-Length Noun Phrases Based on Expanded Chunks and Classified Punctuations in Chinese ....................... 268 A Hybrid Approach to Chinese Abbreviation Expansion ............... 277 Category-Pattern-Based Korean Word-Spacing ....................... 288 An Integrated Approach to Chinese Word Segmentation and Part-of-Speech Tagging ........................................ 299 Kansuke: A Kanji Look-Up System Based on a Few Stroke Prototypes ....................................................... 310 Modelling the Orthographic Neighbourhood for Japanese Kanji ......... 321 Reconstructing the Correct Writing Sequence from a Set of Chinese Character Strokes ................................................. 333 Expansion of Machine Translation Bilingual Dictionaries by Using Existing Dictionaries and Thesauruses ............................... 345 Feature Rich Translation Model for Example-Based Machine Translation ....................................................... 355 Dictionaries for English-Vietnamese Machine Translation .............. 363 Translation Selection Through Machine Learning with Language Resources ........................................................ 370 Acquiring Translational Equivalence from a Japanese-Chinese Parallel Corpus .......................................................... 378 Deep Processing of Korean Floating Quantifier Constructions ........... 387 Compilation of a Dictionary of Japanese Functional Expressions with Hierarchical Organization ..................................... 395 A System to Indicate Honorific Misuse in Spoken Japanese ............. 403 A Chinese Corpus with Word Sense Annotation ....................... 414 Multilingual Machine Translation of Closed Captions for Digital Television with Dynamic Dictionary Adaptation ...................... 422 Acquiring Concept Hierarchies of Adjectives from Corpora ............. 430 Pronunciation Similarity Estimation for Spoken Language Learning ..... 442 A Novel Method for Rapid Speaker Adaptation Using Reference Support Speaker Selection ......................................... 450 Using Latent Semantics for NE Translation ........................... 457 Chinese Chunking with Tri-training Learning ......................... 466 Binarization Approaches to Email Categorization ..................... 474 Investigating Problems of Semi-supervised Learning for Word Sense Disambiguation ................................................... 482 Developing a Dialog System for New Idea Generation Support .......... 490 The Incremental Use of Morphological Information and Lexicalization in Data-Driven Dependency Parsing ................................. 498 Pattern Dictionary Development Based on Non-compositional Language Model for Japanese Compound and Complex Sentences ....... 509 A Case-Based Reasoning Approach to Zero Anaphora Resolution in Chinese Texts .................................................. 520 Building a Collocation Net ......................................... 532 Author Index ................................................... 543 In recent years, contextual question-answering (QA) systems have gained at-tention as a new technology to access information. In this paper, we pro-pose a method to construct a Japanese co ntextual QA system using an existing Japanese non-contextual QA system 1 . Although a contextual question generally contains reference expressions 2 , we expect that the non-contextual QA system will be able to find answers for such a question if reference expressions are ap-propriately completed along with their antecedents.

The completion of a question may be performed in the following steps: (1) detect reference expressions, and then (2) find an antecedent for each reference expression. However, there are ambiguities in these steps, and we may have multiple interpretations, namely, multiple completed question candidates , for one question. In the research area of discourse understanding, there exist many studies of the reference resolution in terms of the cohesion with the context .The centering theory is one of the most widely used methods [1]. This type of reference resolution attempts to find an optimal interpretation so as to maximize the cohesion between a newly introduced sentence and the context. Such a method would definitely work in many cases, but it does not provide the method to resolve the ambiguity in the step (1).

In this paper, we propose another approach. It is the reference resolution in terms of the cohesion with knowledge . It is based on the fact that the QA system can refer to not only the context of the dialogue but also the knowledge base (i.e. a large text document collection). It is noteworthy that  X  X nswering a question X  can be regarded as finding an object, i.e., an answer, whose context in the knowledge base is coherent with the question. Therefore, the cohesion with knowledge may be one of the promising criteria for finding the best interpretation of the question. Here, we hypothesize that the degree of cohesion with knowledge is analogous to the appropriateness of the answer candidate for each completed question candidate. The completed question candidate with the most appropriate answer can be accordingly considered as the best interpretation of the (original) question. 2.1 Contextual Question Answering The contextual QA was introduced as a subtask of the QA track in TREC 2001. However, Voorhees [2] summed up the evaluation as follows:  X  X he first question in a series defined a small enough subset of documents such that the results were dominated by whether the system could answer the particular type of the current question, rather than by the system X  X  ability to track context. X  For this reason, this task was excluded from all subsequent TRECs. On the other hand, a context task has been employed as a subtask of QAC in NTCIR, which is a series of evaluation worksho ps organized by the National Institute of Informatics, Japan. Kato et al. [3] summarized the lessons from the context task of TREC QA as follows: (1) the number of questions in a series is relatively small, and (2) there is no topic shift in a series. They prepared the test sets for NTCIR QAC according to the lessons, that is, (1) a series is relatively long, about seven questions (QAC3), and (2) two types of series are introduced, namely, the gathering type and thebrowsingtype . A question series of the gathering type contains questions that are related to one topic. On the other hand, in a series of the browsing type, the topic varies as the dialogue progresses.
 The approaches of the systems participating in the context tasks in the NT-CIR QAC are mainly based on the cohesion with the context. In general, the approaches are classified into two types. The first type is based on the effort involved in the document/passage retrieval. It expands the query submitted to the IR system with the words/phrases that appeared in the previously asked questions[4]. The second type of approach is based on the completion of ques-tions by resolving reference expressions[5]. One completed question is submitted to a non-contextual QA system. The method that we propose in this paper is similar to the second approach. However, our approach is based on the cohesion with knowledge as well as the cohesion with the context. 2.2 Reference Resolution Detection of reference expressions: The detection of zero pronouns is par-ticularly very important and is studied from various viewpoints. One of the most widely used methods is detection using a case-frame dictionary. A case-frame dictionary is used to find unoccupied cases in a sentence.
 Identification of antecedents: Nariyama [6] proposed a modified version of the centering theory[1] in order to resolve Japanese zero pronouns. It utilizes a  X  salient referent list (SRL)  X  that pools all overt case elements that have appeared up to the sentence in question. If a new case element appears with a case marker identical to that of another case element already existing in the SRL, the new case element takes its place because of recency. In an SRL, the case elements are listed in the following order of salience: Topic &gt; Nominative &gt; Dative &gt; Accusative &gt; Others. A zero pronoun is resolved by selecting the most salient case element in the SRL. Figure 1 shows the overview of the proposed method. The method generates an answer list for each question in a given ser ies by using the following procedure. It should be noted that the non-contextual QA system can perform list-type question answering, as described in Section 3.3. It is the task in which a system is requested to enumerate all correct answers.
 Input. A new question and a list of antecedent candidates. The list is initialized Output. An answer list and an updated antecedent candidate list.
 Procedure 1. Detect reference expressions including zero pronouns in the new question 2. Find antecedent candidates for reference expressions according to a selected 3. Generate all possible completed question candidates using the results of Step 4. Submit completed question candidates to the non-contextual QA system, 5. Provide the most appropriate answer list as the final output. 6. Using the completed question candidate that provides the most appropriate 3.1 Example Before we explain the details of each step, we describe the flow of the proce-dure using the following example series of questions. In this example, we adopt Strategy CRE-C and Measure AM-D as the strategy for completing reference expressions and the appropriateness measure of the answer lists, respectively. Since Question (1a) is the first in the series, it is submitted to a non-contextual QA system without any modification, and obtained the following answer list. The system also assigns semantic categories 3 to each answer in the list according to a thesaurus.
In Step 6 of the procedure, the system updates the list of antecedent candi-dates. When CRE-C is adopted, the list would be the following SRL obtained from the first question and its answer list 5 :
As shown in this example, if the interrogative is the expression to be stored in the SRL, we replace it with the answer list. Each expression in the SRL is also assigned semantic categories.

Next, the system receives the second question (1b). Since the question has a context, the system tries to detect the reference expressions, as described in Section 2.2. First, the system checks whether the question contains demonstra-tives or pronouns, and it finds a demonstrative adjective called  X  X ono. X  Next, the system tries to detect zero pronouns by using the following steps: (i) look up the verb of the question in a case-frame dictionary in order to obtain case frames, and (ii) detect unoccupied cases in the question as zero pronouns. The system also obtains the information of semantic categories for unoccupied cases from the case frames. In the case of Question (1b), the system obtains the fol-lowing case frame for the verb  X  X ru X  (exist) 6 , which satisfies the selectional re-striction in terms of the semantic category, and detects that there are no zero pronouns. The antecedent candidates for the demonstrative adjective are obtained from SRL according to the order in the list. Since there are two candidates at the top of the list, we have the following two completed question candidates:
The system selects the M -best completed question candidates according to the semantic consistency in the reference resolution, as described in Section 3.7, and submits them to the non-contextual QA system in order to obtain an answer list and a value of the appropriateness of the list for each question candidate. In the example, we have the following results: {  X  X ANAMI Hiroshi 7  X , X  X AMADA Kosuke 8  X  } and 0.019 for Question candidate (5a), and {  X  X ANAMI Hiroshi X  } and 0.031 for (5b). Since we hypothesize that the question candidate whose answer list has the highest value of appropriateness is the best interpretation in terms of the cohesion with knowledge, the system outputs the latter answer list as the answer for Question (1b).

After the process of the second question, the system updates the SRL and proceeds to the processing of the next question. 3.2 Non-contextual Japanese QA System The non-contextual Japanese QA system used is a Japanese real-time QA system based on the study by Mori[7]. Mori reported that the MRR (mean reciprocal rank) of the system is 0.516 for the test set of NTCIR-3 QAC2. It treats each morpheme in retrieved documents as a seed of an answer candidate and assigns it a score that represents the appropriateness for an answer. Under the assump-tion that the morpheme is the answer, the score is calculated as the degree of matching between the question and the sentence where the morpheme appears. In his current implementation, the score is a linear combination of the follow-ing sub-scores: the number of shared character bigrams, the number of shared words, the degree of case matching, the degree of matching between dependency structures, and the degree of matching between the NE type of the morpheme and the type of question. 3.3 List-Type QA Processing in Non-contextual Japanese QA We also introduce a list-type QA processing proposed by Ishioroshi et al. [8]. They assume that the distribution of answer scores contains a mixture of two normal distributions,  X  p ( x ;  X  p , X  p )and  X  n ( x ;  X  n , X  n ), i.e., those of the correct answers and incorrect answers, where  X  and  X  are the average and the stan-dard deviation, respectively. Under these assumptions, the correct answers may be separated from the mixture of the distributions by using the EM algorithm. Figure 2 shows an example of the score distribution in the case that the score distribution of the correct answers is separable from that of the wrong answers. 3.4 Detecting of Reference Expressions Our method treats the three types of reference expressions  X  (i) demonstratives and pronouns, (ii) zero pronouns, and (iii) ellipsis of the adnominal modifier  X  X P 1 -no  X  in a noun phrase  X  X P 1 -no NP 2 (NP 2 of NP 1 ). X 
The detection of the reference expressions of type (i) is not difficult because they appear explicitly. With regard to type (ii), we employ an existing method based on a case-frame dictionary as described in Section 2.2. We use  X  X ihon-go goi taikei X  (a Japanese lexicon) as the case-frame dictionary. For type (iii), we adopt Takei X  X  method [9].

If no reference expression is detected in a question, the system assumes that a topic phrase is omitted in the question and introduces a zero topic phrase in order to force the question to have a relation with the context. 3.5 Finding Antecedent Candidates of Reference Expressions Strategy CRE-C: This strategy is based on a modified version of Nariyama X  X  SRL-based centering theory. The list of antecedent candidates is SRL itself. This method is different from Nariyama X  X  method in the following manner: (a) demonstratives and pronouns are resolved before zero pronouns, and (b) a zero topic phrase may refer to all possible case elements in SRL.
 Strategy CRE-H and Strategy CRE-A: For these strategies, the list of antecedent candidates is maintained as described in Section 3.9. The system may select any element from the list for each reference expression. 3.6 Narrowing Down Antecedent Candidates Using Selectional According to the selectional restriction, for each reference expression in the cur-rent question, the system filters out inappropriate candidates in the antecedent candidates that are obtained by one of the strategies described in Section 3.5. With respect to Strategy CRE-C, the system selects the candidate that is at the highest rank in SRL among the appropriate candidates. The selectional re-striction is based on the similarity sim ( r, a ) between the semantic categories of the antecedent candidates and reference expressions, a and r , defined in the thesaurus( X  X ihon-go goi taikei X ). The similarity is calculated by the following equation [10]: where l r and l a are the depths of the categories r and a in the thesaurus re-spectively, and L ra is the depth of the lowest common ancestor of r and a . The symbol  X   X   X  represents the subsumption relation. We determine a threshold value of the similarity Th sim , and filter out each antecedent candidate whose similarity is less than the threshold value. 3.7 Generating Completed Question Candidates and Narrowing By completing each reference expression in the current question with all the possible antecedent candidates, the system generates all the possible candidates of the completed question. However, this process may generate many question candidates, and the non-contextual QA systems may take a very long time to process them. Therefore, we introduce a measure for a completed sentence in terms of the degree of consistency in the reference resolution , and select the M -best question candidates by using the measure. We defined the degree C ( Q )of a question candidate Q in Equation (2).
 where resolv ( Q ) is the set of pairs of a reference expression and its antecedent in the question Q ,and NE ( a )istrueif a is a named entity. 3.8 Finding the Most Appropriate Answer List by Using the The selected question candidates are submitted to the non-contextual QA sys-tem. The completed question candidate with the most appropriate answer may be considered as the best interpretation of the original question. We propose the following two methods as the appropriateness measure.
 Measure AM-D: The appropriateness of an answer candidate list is assumed to be measured by  X  p  X   X  n in Figure 2. Some of the candidates of question completion may not be consistent with the knowledge base. In such cases, the scores of highly ranked answer candidates are not very high and have almost the same distribution as that of the lower ranked candidates. Conversely, if the value  X  Measure AM-M: The appropriateness of an answer candidate list is assumed to be measured by the maximum score of the answer candidates in the list. It is based on the fact that the score of an answer is calculated according to the coherence between the question and the context of the answer. 3.9 Updating the List of Antecedent Candidates By using the completed question candidate that provides the most appropriate list of answer candidates, the system updates the list of antecedent candidates according to a selected strategy for com pleting the reference expressions. Strategy CRE-C: The list is updated in the same manner as the SRL. The difference is the treatment of the interrogatives. An interrogative is replaced with its answer list before the SRL is updated.
 Strategy CRE-H: The list of antecedent candidates is maintained so as to have the following elements: (a) all the case elements in the current completed question, (b) all the phrases in the answer list of the current completed question, and (c) topic phrases, that is, all the case elements in the first question. Here, it should be noted that the system extracts the case elements not from the original current question but from the completed current question. The case elements in the questions before the current question may be retained in the completed current question if the question continues to refer to them.
 Strategy CRE-A: We just adopt all the case elements in all the completed questions thus far. We evaluate the proposed systems in terms of the accuracy of the question answering by using the test set of NTCIR-5 QAC3 [11]. The test set comprises 50 series and 360 questions. In these series, 35 series (253 questions) are of the gathering type and 15 series (107 questions) are of the browsing type. It should be noted that the systems are not allowed to use the series type for answering the questions. The document collection as the knowledge source consists of all (Japanese) articles in the Mainichi Shimbun newspaper and Yomiuri Shimbun newspaper published in 2000 and 2001. The parameters are tuned with the test set of NTCIR-4 QAC2. The threshold value for the selectional restriction Th sim is 0.54, and the number M of completed question candidates to be selected is 20. For measuring the accuracy of the list-type question answering, we use the mean of the modified F measure MMF 1 defined by Kato et al. [11].

We also prepare systems that do not use cohesion with knowledge ( X  X o-CK X  in the following table) as baseline systems. These systems adopt the completed answer candidate that has the largest value of the degree of consistency in the reference resolution, which was described in Section 3.7. As an upper limit of accuracy, we also evaluate the non-contextual QA system with questions whose reference expressions are manually resolved.
 The experimental results are shown in Table 1.
 5.1 Overall Performance of Question Answering With regard to the effect of the introduction of cohesion with knowledge, as shown in Table 1, both AM-D and AM-M, which utilize the cohesion with knowl-edge, outperform the baseline No-CK, which is only based on the degree of con-sistency in the reference resolution. In particular, the appropriateness measure AM-D works well for all strategies for completing the reference expressions.
With regard to the differences between the strategies for completing the refer-ence expressions, Strategy CRE-C exhibit s a better performance than the others. However, the difference between CRE-C and CRE-A is not significant when the measure AM-D is used.

In order to investigate the situation in further detail, let us compare (b) and (c) in Table 1. The systems based on Strategy CRE-C are stable over both types of series. The combination of CRE-C and AM-D is more robust even when topic shifts occur. The systems with the measure AM-M seem to have a better performance in the gathering type series. The reason for this is as follows. Because of the composition of the answer score, the score of a longer question tends to be larger than that of a shorter question. Consequently, the use of the measure promotes the selection of the case frames that have a larger number of case elements. As a result, the system tends to select question candidates that are more cohesive with the context. However, the systems easily fail to track the shift of topic, as shown in Table 1 (c). The strategies CRE-H and CRE-A have good performance for the series of the gathering type, but are not good at the series of the browsing type because they could not track topic shifts properly. 5.2 Failure Analysis A detailed analysis of success and failure is summarized in Table 2. In this table,  X  X uccess X  implies that the generated answer list contains at least one correct answer. The other cases are  X  X ailure. X  Among the success cases, there are many cases where the reference resolution fails but the system successfully finds the answers for the question. This implies that the introduction of expressions of the context into the current question has a positive effect on the performance of question answering even if the accuracy of the reference resolution is insuffi-cient. One of the reasons for this is that these newly introduced expressions may work well in the early stages of question answering, such as document/passage retrieval.

The main reason for the failure lies in the stage of updating the list of an-tecedent candidates in the CRE-C. The failure is caused by, at least, the follow-ing reasons: (1) failure in finding correct answers for some previous questions, (2) failure in finding the appropriate antecedents for the reference expressions in the list of antecedent candidates.

The number of failures in the updating stage in CRE-A is relatively low because the restriction on the list of antecedent candidates is relatively weak and the list may have more candidates than CRE-C. On the other hand, there are many failures in the stage involving the selection of an appropriate question candidate. Since the cohesion with knowledge is taken into account in this stage, the ratio of failures in this stage is closely related to the effectiveness of the cohesion with knowledge. However, we can not jump to the conclusion that the proposed method is not effective because the method based on the cohesion with knowledge works correctly only when the non-contextual QA system can find at least one correct answer in the knowledge base.

In order to estimate the ability of disambiguation based on the cohesion with knowledge more accurately, we investigate the accuracy of answering the questions that satisfy the following conditions: (a) each of them has multi-ple completed question candidates, and at least one candidate is a proper in-terpretation in the given context, and (b) the non-contextual QA system can find at least one correct answer for at least one of the completed answer can-didates with the correct interpretation. The result shown in Table 3 implies that the use of cohesion with knowledge significantly improves the accuracy. By comparing Table 3 with Table 1, we also find that the accuracy may be improved when the non-contextual QA system is able to find appropriate answers.
 In this paper, we introduced the notion of  X  cohesion with knowledge  X  X ndon its basis, proposed a question-answering system to answer contextual questions using a non-contextual QA system. Experimental results showed that the system works effectively under the combination of the strategy based on the SRL-based centering theory, i.e., CRE-C and the appropriateness measure of an answer that is defined in terms of the score distribution of the answer candidates, i.e., AM-D.
According to the failure analysis, the main reason for the failure was that the appropriate antecedents of the reference expressions in the current question do not appear in the list of antecedent candidates. Therefore, further improvement in the non-contextual QA system is required.
 report our further research on th i s spec i al problem . Many methods have been truth i s not absolute because the exper i mental ob j ects are handwr i tten characters and method i s i ntroduced i n [9] that bound i ng box of every connected component i s Engl i sh documents . Bes i des, some compl i cated methods are cons i dered i n order to get Zern i ke moments i s also i ntroduced i n [10] .
 slant angle of pattern are language i ndependent . However how to adopt these for Engl i sh documents, i t i s very prevalent that a word rather than a character i s used to detect i tal i c . But How about m i xed documents? All these problems w i ll be be presented .
 sect i on 3 and conclus i on i s drawn i n sect i on 4 . document and est i mate the slant angle accurately .

In th i s paper, we only d i scuss pr i nted m i xed Ch i nese/Engl i sh documents . There are some features i n pr i nted documents that can i mprove the performance of i tal i c Second, although the slant angle may be d i fferent i n d i fferent documents, the not easy to be done i n handwr i tten documents .
 that only the or i g i nal pattern of scattered character i s adopted . The reason i s that the method and good results are ach i eved . But how to get the shear angle? And how many sheared patterns are needed? These problems haven  X  t been d i scussed thoroughly so far and i t i s very i mportant .

Some researchers shear the pattern by a step of 1 degree, many patterns are ach i eved and compar i son of features w i ll be done and then the slant angle can be done accord i ng to the slant angle . It i s good i f speed i s not cons i dered . But as for researchers assume the slant angle i s a constant angle such as 12 degree . So general way .
 angle . 2.1 Preprocess and eff i c i ent . Otherw i se, the speed of the whole OCR system i s very slow . Therefore very large such as a document page or a large text block, but they can fa i l i n detect i ng Ch i nese/Engl i sh document? If the area i ncludes only one Ch i nese character, the result characters .
 of gett i ng character-pa i r area i n a text l i ne i s as follows .
 F i rst, connected component analys i s i s conducted .

Second, conservat i ve mergence i s done based on the dependency of pos i t i on of connected components . The set of merged components i s def i ned as BC . Th i rd, the w i dth of Ch i nese character i s evaluated . The w i dth i s def i ned as W H .
Fourth, as for BC, a procedure of recurs i ve mergence i s done and the new created set from BC i s def i ned as MC .

The i tal i c determ i nat i on and est i mat i on of slant angle w i ll be done based on the set of MC . 2.2 Ital i c Determ i nat i on the med i an of general slant angles . Although the shear angle can not be j ust the real compar i son of features of patterns between the or i g i nal pattern and the sheared pattern w i ll be done .
 h i stogram . Th i s method i s very s i mple and eff i c i ent . We call i t as PP .
The feature i s extracted accord i ng to the formula as : column pro j ect i on h i stogram of pattern . We presume I0 i s the or i g i nal pattern and I1 i s the sheared pattern . In general, the result of i tal i c determ i nat i on i s g i ven as : i ntroduced and we f i nd i t very useful to i mprove the performance . Th i s method g i ve we i ghts to those connected black runs and b i gger the length of the black run i s, h i gher r i ght part shows the features of the sheared character-pa i r .
 normal character pattern than i n i tal i c character pattern . Accord i ng to the formula (1) and (2), these blank columns can cause error i tal i c determ i nat i on . We assume that the w i dth of pattern i s w o and the number of blank columns i s w b , then the normal i zat i on w i dth i s ob ww w = X  . As a result of fact, the d i fference between (0) fI and most robust .
 2.3 Est i mat i on of Slant Angle degree . Because the slant angle i s constant i n a text l i ne i n general, we can merge all the pattern . est i mat i on of slant angle i s as follows .
 s i m i larly, MAX i s the max i mum and the value i s 18 .
 Second, F(x) i s the value of f(I1) when the shear angle i s x .

F i nally, the follow i ng program w i ll be executed . In the program, the var i ant of i i s the est i mat i on of slant angle .

Program EstimateSlantAngle respect i vely . The collect i ons of C1 and C2 are very large and the number of samples sub-collect i ons, one i s normal and the other i s i tal i c .

The number of normal character samples i n C1 i s 6729 333, where 6729 i s the number of Ch i nese character classes and 333 i s the number of samples i n each class . transform of normal samples, and the shear angle i s 12, 13, 14, 15, 16, 17 and 18 . So the number of samples i n i tal i c collect i on i s 6729 333 7 .
 same as that of C1 .

The th i rd collect i on i s made up of 200 documents wh i ch are from books, j ournals and newspapers . There are many m i xed Ch i nese/Engl i sh i tal i c characters scattered i n the documents . 3.1 Evaluat i on of Ital i c Determ i nat i on The exper i mental results by adopt i ng the methods of PP, WPP, WWPP are shown respect i vely i n table 2-4 .

Based on the above results, we can f i nd that the method of WWPP i s the best . In table 4, th i s method i s tested i n C3 . The result i s good enough to appl i cat i on . 3.2 Evaluat i on of Est i mat i on of Slant Angle The results are shown i n table 5 .
 whether to determ i nate i tal i c or est i mate the slant angle . evaluate our method . So large collect i ons haven  X  t been reported i n other contr i but i ons for scattered Ch i nese i tal i c characters, some further researches are needed to i mprove the accuracy .
 In recent years a large amount of research has been done on the identification of semantic orientation in text. The task is also known as opinion mining, opinion extraction, sentiment analysis, polarity detection and so on. It has focused on classifying reviews as positive or negative and ranking them according to the degree of polarity. The technology has proved to have many potential applica-tions. For example, it could be integrated with search engines to provide quick statistical summary of whether a product was recommended or not in the World Wide Web, thus help consumers in making their purchasing decision and assist manufacturers in performing market analysis.

Our work studies the problem of answering natural language questions on customer reviews of products. The research is based on our previous projects of personalized information retrieval and natural language question answer-ing(QA) system. Some similar works include multi-perspective question answer-ing [1] [2] [3] and opinion question answering [4]. For a QA system, the questions from users could be of two types: objective questions and subjective questions. Subjective questions are those related to opinion, attitude, review, and etc. The identification of semantic orientation from corpora provides a possible way to answer user X  X  subjective questions. According to the observation, those opinion related questions asked by users could be either general or feature-based. The following are such two query instances with opinions: (1) X   X ( How is BMW? ) (2) X   X ( How is the power of BMW? )
For sentence (1), users want to get a general opinion about the automobile  X  X MW X , while for sentence (2), users only want to get those opinions about the power of BMW and don X  X  care other features. So, for our task, we need to first identify product features on which customers express their opinions.

Given a set of product reviews, the task of identifying semantic orientation could be divided into four subtasks [5]: (1)Identify product features expressed in the reviews; (2)Identify opinions regarding product features(including those general opinions); (3)Calculate the semantic orientation of opinions; (4)Rank the reviews based on their strength of semantic orientation.

Most of existing researches focused on subtask (3). In general, there are two approaches to semantic orientation calculation: namely, the rule based approach and the machine learning based approach. The former calculates the seman-tic orientation of each opinion-oriented word/phrase appeared in reviews and uses the average semantic orientation to represent the sentiment orientation of reviews. According to the value of average semantic orientation, it could rank reviews (subtask 4) easily. The latter, which is more widely employed, applies supervised automatic classification tec hnology to classify reviews into bipolar orientation of positive or negative. Utilizing the semantic orientation of opinion-oriented word/phrase, this approach is proved to get an improved accuracy than using usual bag-of-word or n-gram features.

Although important, researches on the identification of product features are relatively few. Hu [6] classified product features that customers have expressed their opinion on as explicit features and implicit features. In their definition, the features which appear explicitly in opinion sentences are explicit features; the fea-tures which do not appear explicit in sentences are implicit features. But they did not propose a method for identifying implicit features. In this paper we address the issue of automatically identifying implicit features from domain dependent product reviews. To the best of our knowledge, no previous work has been con-ducted on exactly this problem. The existing researches on feature identification such as [5] [6] [7]mainly focus on finding features that appear explicitly as nouns or noun phrases in product reviews. The identification of implicit feature is a harder task than identifying explicit one. According to the observation, opinion-oriented word/phrase could usually be a good indicator to the feature which it modifies. So the key issues to the task are to define a feature set related to specific domain and map those indicators to the set of predefined features.

We take a mutual information approach to address the problem. In this pa-per we first present our approaches in the identification of opinion-oriented words/phrases, and map the adjective ones to the corresponding implicit prod-uct features in reviews. By identifying implicit features, we could undoubtedly get more reasonable semantic orientation scores on different product features. 2.1 Explicit Features and Implicit Features Product features include attributes, such as  X   X ( acceleration ) for automo-bile products, and parts, such as  X   X ( brake ). A feature can appear either explicitly or implicitly in product reviews. Features that can be extracted di-rectly from the reviews are Explicit Features , e.g., ( power )in X  opinions without explicit feature words. But we could still deduce the features which their opinions toward from the opinion sentences. Those kinds of fea-tures are Implicit Features . For example, in the sentence of  X   X ( BMW is beautiful ), we could judge that the feature which users talk about is BMW X  X  exterior although the word doesn X  X  appear explicitly.

The identification of explicit features is relatively easy. Using frequent nouns and noun phrases appeared in reviews with some pruning strategies, the experi-ment in [6] could reach the best average precision of 79%. Evaluating each noun phrase by computing mutual information between the phrase and meronymy discriminators associated with the product class, the experiment in [5] achieved 86% in precision and 89% in recall.

In the research of [8], they represented a method to identify implicit features by tagging the mapping of specific feature values to their actual feature. Our work is close to their research in the sense that we also use mapping rules, but it is also different in that we propose an automatic method to generate the mapping rules and use opinion-oriented words. 2.2 Feature Structures in the Domain of Automobile Review Our work in this paper is focused on identifying implicit features in product reviews. Since there are no feature words appeared in reviews, we need firstly to define a set of review features for automatic opinion mining. The definition of features are domain dependent. In our experiment, we consider the domain of automobile review.

According to the investigation of several automobile review websites, we found that people usually evaluate a car from several aspects, such as (Power), product feature sets of automobile according to the 7 items above. This is a very detailed review feature proposal. In fact, many automobile review websites clas-sify automobile review features on a rough level. For example, they combine the features of (Power) and (Handling) as (Performance), com-bine the features of (Exterior) and (Interior) as (Design). From the rough classification schemes, we may have an impression that the feature of (Power) and (Handling) may be hard to divide, and so as the feature of (Exterior) and (Interior). For our task of implicit feature identification, they are also problems. For example, when users speak of an ex-quisite automobile , they may consider both exterior design and interior design. So, we propose that a feature indicator should be mapped to several implicit features. It also seems to accord with our instinct.

Product features which appear explicitly in automobile reviews can also be classified into the feature structure. For example, acceleration belongs to the feature of Power, brake belongs to the feature of Handling. 3.1 The Method of Pointwise Mutual Information Pointwise Mutual Information (PMI) is an ideal measure of word association norms based on information theory [9]. Researchers have applied this measure-ment to many natural language processing problems such as word clustering. PMI compares the probability of observing two items together with the prob-abilities of observing two items independently. So it can be used to estimate whether the two items have a genuine association or just be observed by chance.
If two words word 1 and word 2 have probabilities P ( word 1 )and P ( word 2 ), then their mutual information PMI ( word 1 , word 2 ) is defined as [10]: Usually, word probabilities P ( word 1 ), P ( word 2 ) and joint probabilities P ( word 1 &amp; word 2 ) can be estimated by counting the number of observations of word 1 , word 2 and the co-occurrence of word 1 and word 2 in a corpus normalizing by the size of the corpus. The co-occurrence range of word 1 and word 2 is usually limited in a window of w words.
 The quality of the PMI algorithm largely depends on the size of training data. If there is no co-occurrence of word 1 and word 2 in the corpus, the accuracy of PMI becomes an issue. The PMI-IR algorithm introduced by Turney in [11] used PMI to analyze statistical data returned by the query of Information Re-trieval(IR). So, the corpus used by PMI-IR algorithm is the document collection which is indexed by IR system. The PMI-IR algorithm is proposed originally for recognizing synonyms in TOEFL test. Then Turney [12] used this method in their sentiment classification experiments, where the sentiment orientation  X   X  ( w ) of word/phrase w is estimated as follows.
In equation (2), positive and negative means a set of Reference Words Pair (RWP)[13] with the sentiment orientation of positive or negative respectively. Choosing the RWP of excellent and poor , the equation above can be written as: Where hits ( query ) is the number of hits (the number of documents retrieved) when the query query is given to IR system. And hits ( query 1 ,query 2) is the number of hits  X  query1 NEAR query2  X . 3.2 Feature-Based PMI Algorithm In Turney X  X  research, semantic orientation was calculated by the word association with a positive paradigm word minus a negative paradigm word. For our research, we expand the RWP of two words to a set of features. In our case, we have a set S feature which holds different features.

For each selected word w , we calculate the PMI between w and feature i ( feature i  X  S feature ). Using the PMI method, we get the word association be-tween w and different features, then map w to one or several features according to the probability.

The PMI-IR algorithm use a NEAR operator to simulate the co-occurrence window limit of two words in PMI algorithm. But using NEAR operator has its limitation. The two NEAR words may be distributed on the different sentences or clauses, or even two paragraphs. Thus their semantics may be non-sequential and not represent the correct co-occurrence relationship. For keeping the two words in a semantic sequence, we suggest an improvement of PMI-IR algorithms for our task.

We use conjunction operator to construct queries. Because we have limit word to be adjectives in the polarity lexicon. As features are always nouns/noun phrases, so the query of  X  X ord feature X  would form a normal collocation relation.

Since we calculate the association between w and the feature set S feature ,we can drop p ( word ). Using World Wide Web as the corpus (using the query results returned by search engine), the equation(5) can be simplified as follows: Where  X  is a parameter to prevent the numerator from getting zero when there is no hits returned for the query of  X  X ord feature X  . 4.1 Opinion Words Collection For collecting opinion words, we first download automobile review webpages from the internet, and then extract the opinion words from them manually. Thus we get a small/basic polarity lexicon. U sing these words as seeds, we enlarge our polarity lexicon utilizing synonym and antonym sets in the Chinese Concept Dictionary(CCD), a Chinese version of the online electronic dictionary Word-net. The consideration of utilizing CCD is, in general, words share the same orientation as their synonyms and opposite orientation as their antonyms [6]. For example, the p ositive word ( lovely ) has the synonym set of { of { } . The two sets take the opposite orientation of positive and negative accordingly. In our research, according to the structural charac-teristic of CCD and the expansion results, we only use the result of synonym expansion, and enlarge the polarity lexicon more than 5 times larger.

Most of the opinion words in our lexicon are adjectives. Being good modifiers for product features and carriers of sentiment, in our experiment, only the ad-jectives in polarity lexicon were chosen to identify implicit product features. We map each adjective to the set of product features which it could modified. 4.2 Mapping Opinion Words to Features The IR system used in our experiments is Google. Google provides API to refer queries conveniently and rapidly. Google API returns estimated hit counts rather than actual values. This would add some noise into our model, but still has the necessary stability for our estimation.

For each product feature in the above mentioned feature set S feature ,we choose one or several representative nouns as feature words to calculate the PMI between them and opinion words. Such as, for the feature of (Power), we choose the words of (power), (engine), (horsepower) and etc.
 Table 1 shows PMI scores for some word examples.
 For each opinion word, we have got its PMI scores on each feature i  X  S feature . We need to further map them to one or several most suitable features according to their PMI values.
For this purpose, we first define a function to describe the difference between  X  S (  X 
Then we calculate a series of score dif f for each word w using an algorithm as described in the following pseudo code: Algorithm 1. The calculation of score dif f while S 2 =  X  do end Table 2 is the score dif f of some word examples.

We use value gap to describe the margin between two dif f score sandseta experiential threshold  X  =1 . 0. When the margin is greater than  X  ,wesaythat the two adjacent dif f score have a gap . With each word and its dif f score , we find every gap value from high to low of a set of dif f score , and judge the feature class which the word should belong to according to the sequence.

From Table 2, the biggest gap for (exquisite) should between the feature of Exterior and Handling. So we map the word (exquisite) to feature sets of Craft,Interior,Exterior. For the word of (powerful), the biggest gap should between Power and Interior. So we map the word (powerful) to the feature Power. As for the word (ordinary), we find that there is no gap between every two features. So we consider that the word (ordinary) is a general descriptive adjective.

Based on this method, we map the adjective words in our polarity lexicon to the predefined product feature set. Table 3 gives some experimental example results.
 Feature identification in product reviews is the first step of opinion QA and other opinion mining tasks. In product reviews, the appearance of feature nouns or noun phrases is usually an important clue which aids in mapping to predefined product features. But according to our observation of product review webpages, in many cases, those explicit feature nouns or noun phrases do not appeared in the context. Here comes the importance of the identification of implicit product features. According to the mapping between opinion words and product features, we could judge what features a review is regarding to, without the explicit ap-pearance of feature nouns or noun. In this paper, we describe a method which uses PMI to calculate the association between opinion-oriented adjectives and a set of product review features. We do not conduct a quantitative evaluation of our method, because the relationship between some adjectives, especially those general ones and product features are likely to be highly contextual. But the results of our experiments prove the validity of this method intuitionistically.
Using the method proposed in this paper, we supply our polarity lexicon with the corresponding product feature information. With this resource, we could score a product review from different aspect of features. Future work should include the weighting of explicit features and implicit features when both of the feature types appear in a review.
 Quest ion c l ass i ficat ion i sthetask o f i de n t i f yin gthet y pe o fag iv e n quest ion am on g a predefi n ed set o f quest ion t y pes .T he t y pe o f a quest ion ca n be used as the c l ue t on arr ow d own the search space t o e x tract the a n s w er , a n d used f o rquer y ge n erat ion in a quest ion a n s w er in g ( Q A) s y stem .T heref o re ,i thasa s i g ni fica n t i mpact on the ov era ll perf o rma n ce o fQ A s y stems .T here ha v ebee n Zha n ga n d L ee [ 4 ] a n d Li a n dR o th [13] e x p lo re d iff ere n tt y pes o ffeaturesf o r i mpr ovin g the accurac y. Zha n ga n d L ee c on s i der bag-of-words , bag-of-ngrams aker n e l fu n ct ion ca ll ed tree kernel t o e n ab l e supp o rt v ect o rmach in e (SVM)[3] t o take ad v a n tage o fthes yn tact i c structures o f quest ion s .Li a n dR o th f o cus on se v era l features : words , pos tags , chunks (non ov er l app in g phrases ), named entities , head chunks ( e . g ., the first no u n chu n k in a quest ion) a n d semantically related words (wo rds that o fte no ccur in aspec i fic quest ion t y pe ). T he y a l s o the first on ec l ass i fies i t in t o ac o arse categ o r y; the sec on d determ in es the fi n e categ o r y fr o m the resu l t o f the first c l ass i fier .K adr i a n d W a yn e [10] emp loy err o rc o rrect in gc o des in c o mb in at ion wi th supp o rt v ect o rmach in et oi mpr ov e the resu l ts o fc l ass i ficat ion.

How e v er , ab ov emeth o ds d ono tuseu nl abe ll ed quest ion s w h i ch are a v a il ab l e Fo rsem i-super vi sed l ear nin ga l g o r i thm ,w ec on s i der the T r i-tra inin g o fZh o u measured , that in sta n ce i s used f o r further tra inin gthe o ther c l ass i fier .S uch such as the C o-tra inin ga l g o r i thm prese n ted b y G ol dma n a n dZh o u [11], w h i ch freque n t ly uses 10-f ol dcr o ss v a li dat ion on the l abe ll ed set t o determ in eh ow t o the l ear nin gpr o cess t i me -c on sum in g .A tthebeg innin g ,T r i-tra inin gb oo tstrap -samp l es the l abe ll ed data t o ge n erate d iff ere n ttra inin gsetsf o rthreec l ass i fiers in o rder t o make the three c l ass i fiers d iv erse e no ugh s o that the T r i-tra inin g a l g o r i thm d o es no tdege n erate in t o self-training wi th a s in g l ec l ass i fier .
How e v er , quest ion data i ssparsea n d i mba l a n ced .A c l ass ma yin c l ude a fe w quest ion s , s oi ftheb oo tstrap -samp lin gpr o cedure dup li cates s o me quest ion s tra in ed on these b oo tstrap -samp l ed sets ha v eh i gher err o rratestha n th o se o f s i fiers wi th o ut b oo tstrap -samp lin g .Ano ther pr o p o sa li st o app ly m o re tha non e views ( feature spaces )in the l ear nin gpr o cess .T h i sa llow sthethreec l ass i fiers t o ini t i a lly be tra in ed fr o mthe l abe ll ed set wi th d iff ere n t feature spaces a n dst ill e n ta l g o r i thms a n dt wo vi e w s : bag-of-words a n d bag-of-pos-tags .Two c l ass i fiers fier gets bag -o f -wo rds a n dthe o ther gets p o s -tags .T he th i rd c l ass i fier uses the resu l ts .

T he rema in der o f the paper i s o rga ni zed as f ollow s :S ect ion 2g iv es deta il are g iv e ninS ect ion 3 a n dc on c l us ion sareg iv e ninS ect ion 4 . pr o p o sa l st oi mpr ov e i t .
 12 for every x  X  U do b )T r i-tra inin g wi th mu l t i p l e 13 if h j ( x )= h k ( x )( j, k = i ) l ear nin ga l g o r i thms 2.1 Semi-supervised Tri-training Algorithm fr o masetb y b oo tstrap -samp lin gthe l abe ll ed set L .Fo ra ny c l ass i fier , a n u nl a -be ll ed in sta n ce ca n be l abe ll ed as lon gasthe o ther t wo c l ass i fiers agree on the no t n eeded t o be e x p li c i t ly measured .Fo re x amp l e ,i f h 1 a n d h 2 agree on the will rece iv ea v a li d n e win sta n ce f o rfurthertra inin g ;o ther wi se , h 3 will get a n in sta n ce wi th a noi s yl abe l. Non ethe l ess , as c l a i med in [1 4 ], e v e nin the wo rse case , the in crease in the c l ass i ficat ion noi se rate ca n be c o mpe n sated f o r ,i fthe n umber o f n e wly l abe ll ed in sta n ces i ssuffic i e n t .
 erated vi ab oo tstrap -samp lin gfr o mthe o r i g in a ll abe ll ed tra inin gset in o rder t o thesameasth o se l abe ll ed b yi tse l f , thus ,T r i-tra inin gbec o mes self-training wi th w here Learn i sac l ass i ficat ion a l g o r i thm ; S i i satra inin gsetb oo tstrap -samp l ed fr o m o r i g in a l set L ; e i i stheerr o rrate o f h i in the ( t -1) t h r o u n d .Wi th the assumpt ion that the beg innin gerr o rrate i s l ess tha n0.5, theref o re e i i s ini-t i a lly set t o0.5; e i i stheerr o rrate o f h i in the t t h r o u n d ; L i i stheset o f r o u n da n d in the first r o u n d i t i s est i mated b y[ e i e t ion ra n d o m ly rem ov es | L i |  X  s n umber o f in sta n ces fr o m L i in o rder t o make curre n tr o u n dha v e better perf o rma n ce tha n that o fpre vio us r o u n daspr ov ed err o rrate o ftheh y p o thes i sder iv ed fr o mthec o mb in at ion o f h j a n d h k .B e -cause i t i sd i fficu l tt o est i mate the c l ass i ficat ion err o rrate on the u nl abe ll ed in sta n ces , the a l g o r i thm only est i mates on the l abe ll ed tra inin gset ,wi th the assumpt ion that b o th the l abe ll ed a n du nl abe ll ed in sta n ce sets ha v ethesame d i str i but ion.

T he in terest in gp oin t in T r i-tra inin ga l g o r i thm i sthat in o rder t o e n sure cur -re n tr o u n d o ftra inin gt o ha v e better perf o rma n ce tha n that o fpre vio us r o u n d , the s i ze o feach n e wly l abe ll ed set L i must no tbegreatertha n[ e i l i e i  X  1]. I f i t i sgreatertha n th i s v a l ue , the fu n ct ion Su b sample ( L i ,s )i s used t o ra n d o m ly rem ov eredu n da n t in sta n ces .T he three c l ass i fiers are refi n ed in the tra inin gpr o-cess , a n dthefi n a l h y p o thes i s i spr o duced vi a majority voting .Fo rthesake o f sa vin gspace ,o ther deta il s o fthe T r i-tra inin ga l g o r i thm ca n be see nin[1 4 ]. 2.2 Modified Versions of Tri-training D ue t o the n ature o f quest ion data t y pe ,w h i ch i s v er y sparse a n d i mba l a n ced as g iv e ninT ab l e 1. A s stated in [1 2 ], te x tdatat y pe ,w he n represe n ted in a v ect o r space m o de l, i s v er y sparse .Fo reachd o cume n t , the c o rresp on d in gd o cume n t fe wwo rds in c o mpar i s on wi th a d o cume n t , s o quest ion data i se v e n m o re sparse tha n te x tdata .B ecause o fthe i mba l a n ce , after b oo tstrap -samp lin g , each n e wly created tra inin gsetm i sses a n umber o f quest ion sasc o mpared t o the o r i g in a l l abe ll ed set .I fthem i ssed quest ion sare in ac l ass w h i ch c on ta in safe w quest ion s , the ni tmakesthe ini t i a l err o rrate o feachc l ass i fier in crease w he n be in gtra in ed fr o m these data sets .T he fi n a li mpr ov eme n tafter l ear nin gs o met i mes d o es no t c o mpe n sate f o rth i spr o b l em .Ino rder t o a voi dth i sdra w back ,w epr o p o se t o use on the l abe ll ed tra inin gset . Our e x per i me n ts sh ow ed that ,i ftheperf o rma n ce o f on e o fthethreec l ass i fiers i s far better (o r wo rse ) tha n that o fthe o thers , i s that the i rperf o rma n ces be appr oxi mate .T he m o d i fied v ers ion i sdep i cted in are i de n t i ca l t o th o se o fthe o r i g in a l a l g o r i thm in Fi g .1 a .
Ano ther pr o p o sa li st o use m o re tha non e vi e w, such as t wo o rthree vi e w s l abe ll ed set wi th d iff ere n t feature spaces w h il est ill mak in g sure that the y are d iv erse e no ugh .T he m o d i fied a l g o r i thm seems t o ha v ethesta n dard C o-tra inin g pr o p o sa li sg iv e ninFi g .1 c ,w here view i ( L )i sthe i t h vi e wo f the data set L . Other lin es that are the same as th o se in Fi g .1 aare i g no red . 3.1 Question Data Sets and Feature Selection T he Quest ion An s w er in g T rack in T e x tRetr i e v a l C on fere n ce (T REC )[5,6, 7 ] de -fi n es s ix quest ion c l asses ,n ame ly, abbreviation , description , entity , human , loca-tion a n d numeric .How e v er , f o r a quest ion a n s w er in gs y stem in a no pe n d o ma in, c l asses defi n ed b yT REC ,Li a n dR o th [13] pr o p o sed t o d ivi de quest ion s in t o50 gra in ed c l asses . Our data set 1 i s the same as that used in [13] wi th a t o ta lo fab o ut 6000 quest ion s ( the e x act n umber i s 595 2 ). W ekeep 500 quest ion sfr o m T REC 10 [ 7 ] as the test set w h il ethe o ther 5500 quest ion sared ivi ded in t onon-ov er l app in g l a -t i t ionin g ,inw h i ch a n umber o fc on secut iv e quest ion s ( such as 1000, 2 000, 3000 o r 4 000) fr o mthebeg innin g o fthefi l e i susedasa l abe ll ed set ,w h il e the rest o fthefi l e as 1000, 2 000, 3000 o r4 000) fr o mthefi l et o ge n erate a l abe ll ed set , a n d l et o ther quest ion sf o rm a u nl abe ll ed set .T he d i str i but ion o ftra inin ga n d test in gdata i s in currency a n d religion c l ass .
 each quest ion, n ame ly, bag-of-words a n d bag-of-pos-tags (o r pos-tags f o rsh o rt ). quest ion c on ta in sasma ll n umber o f wo rds ,w h il ead o cume n tca n ha v ea l arge c on s i dered t o be  X  stop-words  X  a n d o m i tted as a d i me n s ion reduct ion step in the pr o cess o fcreat in gfeatures .T h i s i sa ni mp o rta n tstep in i mpr ovin gthe perf o rma n ce o fc l ass i ficat ion as pr ov e nin[1 2 ]. How e v er , these wo rds are v er y r ol e in d o cume n tc l ass i ficat ion w h il eth o se freque n c i es are usua lly equa l t o1 in a quest ion, thus , the y d ono ts i g ni fica n t ly c on tr i bute t o the perf o rma n ce o f c l ass i ficat ion. In o rder t o keep these wo rds w h il est ill reduc in gthed i me n s ion space ,w euseaprepr o cess in gstepf o rbag -o f -wo rds features : a ll v erbs are rest o red t o X  ch il d  X ; wo rds ha vin gtheC D( cardinal number ) part -o f -speech are made the d i me n s ion reduct ion step makes SVM [3] reach the prec i s ion o f8 1. 4 % tra inin g on 5500 quest ion s w h il e the same feature wi th SVM used in [ 4 ] g iv es the prec i s ion o f8 0. 2 % tra inin g on the same data set a n d wi th the same linear ker n e l. Fo r p o s -tags , each word in a quest ion i sc onv erted in t o the f o rm o f POS-word ,w here P O S i s the part -o f -speech tag o fthe word .W ea l s o used the prepr o cess in gstep e x amp l e  X  h ow X  i stra n sf o rmed in t o X  X  R B-h ow X ,  X  X  h o X  i sc onv erted t o X  X P-w h o X ,  X  are  X ,  X  X  s  X ,  X  am  X ,  X  X  ere  X  a n d  X  X  as  X  are c onv erted t o  X  X UX-be  X , etc . 3.2 Experiments with Multiple Classifiers In the first e x per i me n t ,w ede v e lo p o ur pr o grams based on the S parse N et wo rk o f Winnow s (SNoW) l ear nin garch i tecture 2 [ 2 ], w h i ch i mp l eme n ts three l ear nin g a l g o r i thms :P erceptr on, B a y es a n d Winnow. W e used these three l ear nin ga l g o-o fthethesea l g o r i thms , such as the l ear nin grate  X  , thresh ol da n d ini t i a lw e i ght o f P erceptr on a n d Winnow are defau l t v a l ues .T he bag -o f -wo rds feature i s used in th i se x per i me n t .
 T he c o mpar i s on o fe x per i me n ta l resu l ts i s li sted in T ab l e2 ,w here TB a y es , TP erceptr on a n d TWinnow, respect iv e ly, sta n df o r o r i g in a lT r i-tra inin g wi th B a y es ,P erceptr on a n d Winnow a l g o r i thm ;TBPW sta n ds f o rthem o d i fied T r i-tra inin ga l g o r i thm wi th B a y es ,P erceptr on a n d Winnow f ollowin gthea l g o r i thm w ec o mpare the fi n a l resu l t ( Final c ol um n) wi th the resu l tpr o duced b y the set ( Super. c ol um n). Fo r TBPW, w ec o mpared the fi n a l resu l t ( Final c ol um n)
T he resu l ts sh ow that the prec i s ion o fsuper vi sed l ear nin g o f B a y es ,P ercep -tr on a n d Winnow i s no tse n s i t iv et o the s i ze o ftra inin g sets . C on crete ly, w he n the s i ze o f the tra inin g set in creases , the c o rresp on d in gprec i s ion d o es no t in-crease .M a y be , the quest ion data t y pe a n d wo rd features are no tsu i tab l ef o r 95% ( p =0.05).

In the sec on de x per i me n t ,w euset wo a l g o r i thms : the first i s M a xi mum E n-tr o p yMo de l 3 (M E M) [1], the sec on d on e i s S upp o rt V ect o r M ach in e 4 (SVM) [3] thus ,w euset wo SVM c l ass i fiers a n d on e M E M c l ass i fier in the i mp l eme n ta -t ion, wi th the e x pectat ion o fmak in gt wo SVM c l ass i fiers t o ha v eh i gh degree o fdec i s ion on fi n a l h y p o thes i s .Wi th SVM c l ass i fiers ,w e set them t o use linear ker n e l a n d o ther parameters ( e . g ., parameter C ) are defau l t .In th i sd o ma in, o ther ker n e l s o f SVM, such as polynomial , radial basic function o r sigmoid , g iv e p oo rperf o rma n ce .Fo r M E M c l ass i fier ,w eusea ll defau l t v a l ues o f parame -ters ( e . g ., L-BF G S parameter est i mat ion). B ag -o f -wo rds features are used f o r 1 b .A ssh own in the tab l e ,M E M a n d SVM are se n s i t iv et o the s i ze o fthetra inin g sets .T he prec i s ion i s in creased w he n the s i ze o ftra inin g set in creases .Ino ther wo rds ,M E M a n d SVM are m o re su i tab l ef o rth i st y pe o fdata .
Fo r o r i g in a lT r i-tra inin ga l g o r i thm wi th M E M a n d SVM, w ea l s o g iv ethe 1 a ( after be in gtra in ed on ab oo tstrap -samp l ed set )in the c ol um n Initial .T h i s c ol um n sh ow s that the ini t i a l prec i s ion o fac l ass i fier ma y be decreased (o r i ts err o rratema yin crease ).

B ecause the prec i s ion o fma n ua lly part i t ion ed test wi th the s i ze o ftra inin g set o f 1000 i s no t i mpr ov ed ,w ed i d no tcarr yo ut the s i g n test . E x cept f o rthe at the l e v e lo f 95%. 3.3 Experiments with Two Different Algorithms and Two Views the three c l ass i fiers , t wo o fthemare SVM c l ass i fiers a n dtheth i rd on e i sa M E M c l ass i fier .T he first vi e w( feature space )i s bag-of-words thesameasthatused in pre vio us e x per i me n ts , a n d the sec on d vi e wi s pos-tags .W esett wo SVM c l ass i-f o re x amp l e , the first SVM c l ass i fier uses bag -o f -wo rds features , the sec on d SVM features .L et TM E M-wo rd a n d TM E M-p o sbethe o r i g in a lT r i-tra inin ga l g o r i thm wi th M E M us in gbag -o f -wo rds a n dp o s -tags features , respect iv e ly; L et TSVM-wo rd a n d TSVM-p o s respect iv e ly be the o r i g in a lT r i-tra inin g wi th SVM us in g bag -o f -wo rds a n dp o s -tags features ;L et TSSM-2 vi e w sbeam o d i fied T r i-tra inin g me n tareg iv e ninT ab l e4 .In terest in g ly, the test on ra n d o m ly part i t ionin g wi th the tra inin gsets i ze o f4 000 reaches the prec i s ion o f8 1. 4 %w h i ch i sa l s o the pre -c i s ion o f SVM be in gtra in ed on the w h ol etra inin gset o fthes i ze 5500. E x cept f o r o ur o ther tests are s i g ni fica n tatthe l e v e lo f 95%.
 T h i s paper app li ed sem i-super vi sed l ear nin gt o e x p lo re u nl abe ll ed quest ion t o used m o re tha non e l ear nin ga l g o r i thm f o rthethreec l ass i fiers , a n dthesec on d on e vi e w. Our e x per i me n ts in d i cate that the perf o rma n ce i s i mpr ov ed .
Our pr o p o sed a l g o r i thms ha v e the same pr o pert y as that o f sem i-super vi sed nin gare no tthesameaseach o ther .T he reas on i s that the u nl abe ll ed in sta n ces ma y be w r on g ly l abe ll ed dur in gthe l ear nin gpr o cess .Al th o ugh T r i-tra inin ghas the fu n ct ion Su b sample ( L i ,s ) t oli m i tthe n umber o f n e wly l abe ll ed in sta n ces ca llin gthe Su b sample ( L i ,s ) fu n ct ion.
 T he ra n d o mse l ect ion o f the fu n ct ion Su b sample ( L i ,s )i s no tag oo dmeth o d . B ecause , the ra n d o m n ess ma y se l ect ma ny n e wly l abe ll ed quest ion s o f on ec l ass equa ln umbers o f quest ion sf o reachc l ass ,o rse l ect a n umber o f quest ion sf o r each c l ass acc o rd in gt oi ts d i str i but ion in the tra inin g set . t wo vi e w sma y be e x te n ded t o the fourth-training case ,inw h i ch there are f o ur T h i s i sa l s o ap o ss i b l estud yin the future .

In the curre n t i mp l eme n tat ion, w eha v e no tc on s i dered t o se l ect o ther better in future t o ach i e v eh i gher prec i s ion.

Our m o d i fied v ers ion s o fthe T r i-tra inin ga l g o r i thm d ono tha v ea ny c on-stra in ts on data t y pes , theref o re ,on em o re i ssue w h i ch i s wo rth stud yin g in the the f i eld of i nformat i on process i ng .
 s i m i lar un i t .
 ed i t d i stance[2] and based on common words or phrases[3] . Methods based on stat i st i cal grammat i cal analys i s[6] and so on . The i mproved methods based on large-scale corpus such as PMI-IR[7] and var i ous smooth i ng algor i thms[8] i s used to resolve the problem of of paraphrase d i ct i onary[9] or some large-scale Ontology[10][11] to do semant i c s i m i lar i ty computat i on .
 wh i ch could not be observed by people only . But th i s method depends on the tra i n i ng corpus, and i s largely affected by the problem of data sparseness and data no i se . Somet i mes, methods based on semant i c s i m i lar i ty may compute s i m i lar i ty between the But the Ontology are usually bu i lt by hand, wh i ch need to spend a lot of t i me . 3.1 Mathemat i cal Descr i pt i on of Quer i es S i m i lar i ty Computat i on relevant features of quer i es, have not yet been reflected i n any report . Before un i f i ed and def i n i t i ons .  X  : a set of Ch i nese str i ngs or quer i es ; S:  X   X  s subset, that is S  X   X  ; l i st i ng has a correspond i ng semant i c codes;  X   X  S ; S 1 S 2 : two g i ven quer i es, i nclud i ng : S 1 a 1 a 2 ... a i ... a M , i [1 M] the element quant i ty of S 1 i s M; S 2 = b 1 b 2 ... b j ... b N , j [1 N], the element quant i ty of S 2 i s N; The element of S 1 S 2 could be s i ngle character, semant i c words segmented by semant i c  X   X  for example, when the element i s s i ngle character, the query may be expressed as follows : i f i t i s segmented by semant i c d i ct i onary semant i c system), the query may be expressed as {Bo010127, Je090101} . pr i or i ty, we order the character str i ng S 1 and S 2 . And we could get : S 1  X  a 1 , a 2 , ..., a i ...,a M , S 2  X  b 1 , b 2 , ..., b j , ..., b N , s (a i , b i ), abr i dged notates as s i .
 and the correspond i ng element b i of S 2 , wh i ch i s notated as q ( s i ) . wh i ch i s notated as S i m ( S 1 , S 2 ) .
 The common mathemat i cal descr i pt i on of the quer i es s i m i lar i ty i s as follows : elements .

Accord i ng to the pr i mary method of s i m i lar i ty measurement between the s i m i lar systems i n the s i m i lar i ty system theory[1], we should cons i der two aspects when we follows : str i ngs  X  s i m i lar i ty,  X  i [0 1],  X   X  ,  X  [0,1],  X  +  X  =1 . So there i s : S i m ( S 1 S 2 ) =  X   X  Q n +  X   X  Q s =  X   X  3.2 Improvement of L i teral S i m i lar i ty Computat i on b and  X  i 1/K . Accord i ng to formula (2), we could get : l i teralness . For example, accord i ng to formula (4), the s i m i lar i ty between  X   X  and  X   X  i s S i m  X   X  ,  X   X  =0 . 25 . Because formula (4) computes the quer i es would not be rel i able . For i nstance, S i m ( X   X  ,  X   X  )=1 .

Accord i ng to formula (3), set  X  =0 . 6,  X  0 . 4 . Because Ch i nese character str i ng has the feature that the top i c kernel l i es often back of i t, we def i ne  X  i as formula (5) . and the number j of S 2 . So formula (3) could be transformed as follows . S i m ( S 1 S 2 ) 0 . 6 * example, when comput i ng the s i m i lar i ty between str i ng  X   X  and  X   X  by formula correlated to the d i fferent pos i t i ons of s i m i lar elements i n d i fferent quer i es . account, q ( s i ) could be computed by formula (7) : And, formula (3) would be transformed i nto : S i m ( S 1 S 2 ) 0 . 6 * Accord i ng to formula (8), the s i m i lar i ty between query  X   X  and  X   X  i s S i m ( X   X  ,  X   X  )= 0 . 8 . computat i on, they are both based on the l i teral feature . Therefore, i n essence they are the same . 3.3 Quant i ty Computat i on of Mult i -feature S i m i lar Un i ts based on the l i teral feature, semant i c feature and stat i st i c relevant feature .
For semant i c feature, we could set  X  1 0 . 25 . The quer i es are segmented by semant i c two elements could be v i ewed as s i m i lar . q ( s i ) 1 could be computed as follows . Where, n stands for the f i rst d i fferent layer number i n the process i ng of compar i ng the two semant i c codes from root nod, n [1 5] .
 to the l i teral s i m i lar i ty computat i on, wh i ch i s the s i tuat i on of formula (6) . mean that the two elements are s i m i lar . q ( s i ) 3 could be computed as follows . Tab_ Relat i on .
 that i s  X  i 1/K . And K i s the quant i ty of s i m i lar un i t . 3.4 Descr i pt i on of S i m i lar i ty Computat i on Algor i thm of the Quer i es s i m i lar i ty computat i on could be descr i bed as follows .

Algorithm: Similarity_Query compute the similarity between Input: character string S 1 , S 2 Output: Sim, the similarity of the queries: S 1 , S 2
Process: K=K+l, M=M+1, N=N+1 Th i s paper has done two exper i ments, and each exper i ment computes the quer i es  X  each other . Then, make them i nto d i sorder and generate nearly 40,000 pa i rs of synonym test i ng result shows as table 1 .

The second group exper i ment i s an open test i ng . Test i ng set i s made up of unordered quer i es . And the search test i ng of synonyms are do i ng based on the open set . The test pa i r of these words out of order, then compute the i r s i m i lar i ty and choose those words Accord i ng to the s i m i lar i ty, we could d i v i de them i nto synonym, synonym, result shows as table 2 .
 search i ng for synonym i c compound word, the recall of the pa i rs of synonym could showed that, j udg i ng from the angle of the recall, the method based on mult i ple features i s much better than the method j ust based on semant i c or l i teral s i m i lar i ty .
From the data i n table 2, we could see that, the prec i s i on i s 29 . 23 % when us i ng the method based on mult i ple features to recogn i ze the synonym, and 25 . 52 % when based us i ng the method based on mult i ple features i s much more effect i ve than the method features, the rate of the searched relevant words whose s i m i lar i ty i s greater threshold advantage when i t i s used on the quer i es cluster i ng . un i t . The result of the exper i ments shows that, the method based on mult i ple features i s knowledge of semant i cs, system theory and so on . For there are some quest i ons ex i sted proposed algor i thm i n Engl i sh quer i es, runn i ng i n a larger text corpus . Acknowledgments. We thank the rev i ewers for the excellent and profess i onal rev i s i on of our manuscr i pt .
 A collocat i on i s def i ned as an express i on cons i st i ng of two or more words that corre-spond to some convent i onal way of say i ng th i ngs [11] . Collocat i ons are popular word by syntact i c or semant i c features [9] . Normally, there are several translat i ons for each Engl i sh verb raise i s the top frequency word translat i on for a Ch i nese verb . However, when collocated w i th ( proof ), should be translated to produce . mach i ne translat i on [21] . Furthermore, a word may have more than one sense and the sense i n a g i ven context i s determ i ned by i ts collocated words [18] . Thus, the knowl-hab i tually i n the source language and learn the common correspondence i n the target language [16] .

The knowledge of b i l i ngual ccollocat i on correspondence cannot be comp i led manually . Several research works were reported on automat i c acqu i s i t i on of colloca-t i on correspondences . Generally speak i ng, these techn i ques follow two approaches . The f i rst approach i s based on parallel corpora . Collocat i on correspondences are i den-strong d i rect dependency correspondence and more than 80% of them can be mapped corpora has been i nvest i gated [9,21] .
 the percentage of the collocat i ons d i rectly found from the d i ct i onary . Secondly, most Ch i nese-Engl i sh collocat i on correspondences from monol i ngual corpora, ICT , i s and the ob j ect are generated by b i l i ngual d i ct i onary look-up . They are then expanded i nformat i on are used : (1) word translat i on probab i l i ty learned from monol i ngual cor-as the collocat i on correspondence . Exper i ments show that the proposed cand i date expans i on method i mproves the coverage of the collocat i on correspondences and the techn i ques .

The rest of th i s paper i s organ i zed as follows . Sect i on 2 presents related work . Sec-t i on 3 descr i bes the method for cand i date expans i on and Sect i on 4 presents the deta i l outl i nes the evaluat i ons and Sect i on 6 concludes th i s paper . Most prev i ous techn i ques extract collocat i on correspondences from parallel corpus . correspondences have s i m i lar occurrence frequenc i es and word and sentence order as collocat i on correspondence extract i on . It has been shown that at least 100M words are i s not flex i ble .

Based on the observat i on that about 80% b i l i ngual collocat i on correspondences on these, Zhou et al . proposed a method to extract collocat i on correspondences by and Zhou then proposed an i mproved method by us i ng EM algor i thm based on de-recent years .

There are two problems faced by the ex i st i ng collocat i on correspondence i dent i f i -only 83 . 98% . It means that there i s an upper accuracy of collocat i on correspondences the semant i c and contextual i nformat i on, wh i ch are cruc i al . pus and thus i t i s not the focus of th i s study . Th i s study addresses the correspondence noun or a nom i nal phrase, the ob j ect i ve of our research i s to f i nd out the correspond-pendency features . The one w i th the max i mal correspondence probab i l i ty, labeled as E d i ct i onary coverage .

We stud i ed 1000 VO b i l i ngual collocat i on correspondences collected from  X  Eve-ryday English word collocations (with Chinese explanation)  X  [19] and measured the cons i sted of 153,515 entr i es, was prov i ded by Harb i n Inst i tute of Technology, Ch i na . It was observed that the coverage was 97 . 2% for e o and 87 . 5% for e v . Theo-97 . 2%*87 . 5%=85 . 1% . The cand i date coverage should be enhanced i n order to i mprove the accuracy of the collocat i on correspondence i dent i f i cat i on model . Inten-ever, natural languages are too dynam i c render i ng a comprehens i ve to b i l i ngual d i ct i onary costly to establ i sh and too large to operate . Furthermore, from our obser-Sect i on 5) .
 thus we perform ob j ect expans i on us i ng the synonyms i n WordNet . For Trans ( c o ), we s i on, only the synset conta i n i ng more than 3 entr i es that belong to Trans ( c o ) are con-because people use verbs flex i ble and thus verbs always have many senses . Therefore, ferent way based on monol i ngual collocat i on extract i on . To extract monol i ngual col-measures the co-occurrence frequency s i gn i f i cance and the latter measures the cand i dates i s enhanced .
 4.1 Collocat i on Correspondence Ident i f i cat i on Model the outset, i t i s observed that there i s a strong correlat i on between the co-occurrences of word patterns, wh i ch are translat i ons of each other [15] .
 sumpt i on i s extended as follows .

Based on these assumpt i ons, the collocat i on correspondence should have h i gh b i -to max i m i ze the follow i ng equat i on : collocat i on correspondence . 4.2 Est i mat i on of Ch i nese-Engl i sh Translat i on Probab i l i ty that the translat i on operat i ons for c v and c o are i ndependent . translat i on probab i l i ty . dence can be calculated by, We employ a well-developed tool package based on WordNet [13] to calculate the dom i nate synset are calculated and the one w i th the h i ghest value i s adopted . 4.3 Est i mat i on of Engl i sh-Ch i nese Translat i on Probab i l i ty Engl i sh translat i on probab i l i ty est i mat i on . It i s formul i zed as, thesaurus . The algor i thm i s s i m i lar to the one adopted i n [17] . 4.4 Est i mat i on of B i l i ngual Context S i m i lar i ty We rank the context words surround i ng C col and E col accord i ng to the i r co-occurrence words i s calculated by, s i m i lar i ty between C col and E col i s then calculated by, more s i m i lar .

Follow i ng Equat i on 1, 4, 6 and 8, the overall collocat i on correspondence probab i l-t i es i s i dent i f i ed as the f i nal collocat i on correspondence . 5.1 Exper i ment Data Preparat i on Furthermore, a standard test set conta i n i ng 1000 Ch i nese-Engl i sh VO collocat i on corre-spondences was bu i lt based on a b i l i ngual collocat i on d i ct i onary [19] . 5.2 Improvement on the Coverage The f i rst exper i ment evaluated and compared the coverage i mprovement of three as the number of entr i es i n each d i ct i onary are l i sted i n Table 1 . from 109,441 (HowNet) to 128,366 (LDC C-E 2 . 0), c ( col ) roughly rema i ns constant . Also, when the number i ncreases to 153,515 (SunD i ct), the coverage of collocat i on to i mprove coverage . 5.3 Evaluat i ons of Ex i st i ng Techn i ques d i date expans i on method were also evaluated . The basel i ne system ( Model A ) selects correspondence . Dagan proposed another model ( Model B ) wh i ch selected collocat i on cand i dates w i th the max i mal probab i l i ty i n the ob j ect language [2] . Based on dependency pars i ng result, Lv proposed a translat i on model ( Model C ) object , c o ) . These three models assumed that e v Trans ( c v ) and e o Trans ( c o ) . The coverage and Table 2 . Note that NLPW i n parser adopted i n [9] i s not a publ i c resource . In th i s ex-the NLPW i n parser .
 ranked collocat i on correspondence cand i dates i dent i f i ed by the three models, respec-recorded correspondences . Thus, i f the result i ng correspondence cand i date matches regarded successful .
 probab i l i ty . 5.4 Evaluat i ons of ICT nent . ICT est i mates the correspondence probab i l i ty by three components . They are (1) expans i on are also shown i n Table 3 .
 top-3 accuracy wh i ch i s 7 . 9% i mprovement on Model C . Th i s result means that can-d i date expans i on can i mprove both coverage and accuracy of collocat i on translat i on i f that there i s room for further i mprovement . 5.5 D i scuss i ons i mproved by amel i orat i ng the follow i ng problems . (1) The collocat i on cand i dates rank i ng date and the top-3 cand i dates i s obv i ous . Meanwh i le, many correct collocat i on corre-default values should be cons i dered . (2) The no i se f i lter i ng of monol i ngual collocat i on extract i on extract i on accuracy for Ch i nese should be further i mproved . (3) The out of vocabulary (OOV) problem (4) Non-compos i t i onal collocat i on translat i on .

Our model i s based on the correspondence assumpt i on, wh i ch assumes that most word translat i on cand i dates to reduce the i nfluence of OOV problem . For th i s i ncorporates three features i s employed to ref i ne the est i mat i on of collocat i on corre-monol i ngual thesauruses; and (3) b i l i ngual context s i m i lar i ty . Exper i ments on 1000 collocat i on correspondence samples show that ICT outperforms three ex i st i ng tech-tem, ICT , can be used to extract b i l i ngual collocat i on translat i ons from monol i ngual corpora w i th good qual i ty and good eff i c i ency wh i ch outperforms the t i me-correspondence i s valuable for monol i ngual and b i l i ngual word sense d i samb i gua-t i on and mach i ne translat i on .
 the D i rect Grant Scheme pro j ect (2050330) and Strateg i c Grant Scheme pro j ect (4410001), and Hong Kong Polytechn i c Un i vers i ty (A-P203) and a CERG Grant (5087/01E) .
 1.1 Phrase-Based SMT In the field of statistical machine translation (SMT), several phrase-based SMT models [17,12,9] have achieved the state-of-the-art performance. These models have a number of advantages in comparison with the original IBM SMT models [2] such as word choice, idiomatic expression recognition, and local restructuring. These advantages are the result of the moving from words to phrases as the basic unit of translation.

Although phrase-based SMT systems have been successful, they have some potential limitations when it comes to modeling word-order differences between languages. The reason is that the phrase-based systems make little or only indi-rect use of syntactic information. In other words, they are still  X  X on-linguistic X . That is, in phrase-based systems tokens are treated as words, phrases can be any sequence of tokens (and are not necessarily phrases in any syntactic theory), and reordering models are based solely on movement distance [17,9] but not on the phrase content. 1.2 Approaches to Exploiting Syntactic Information for SMT Several previous studies have proposed translation models which incorporate syntax representations of the source and/or target languages. Yamada and Knight [25] proposed a new SMT model that uses syntax information in the target language alone. The model is ba sed on a tree-to-string noisy channel model and the translation task is transformed into a parsing problem. Melamed [14] used synchronous context free grammars (CFGs) for parsing both languages simultaneously. This study showed that syntax-based SMT systems could be built using synchronous parsers.

Charniak et al. [4] proposed an alternative approach to using syntactic infor-mation for SMT. The method employs an existing statistical parsing model as a language model within a SMT system. Experimental results showed improve-ments in accuracy over a baseline syntax-based SMT system.

A third approach to the use of syntactic knowledge is to focus on the pre-processing phase. Xia and McCord [24] proposed a preprocessing method to deal with the word-order problem. During the training of a SMT system, rewrite patterns were learned from bitext by employing a source language parser and a target language parser. Then at testing time, the patterns were used to reorder the source sentences in order to make their word order to that of the target lan-guage. The method achieved improvements over a baseline French-English SMT system. Collins et al. [6] proposed reordering rules for restructuring German clauses. The rules were applied in the preprocessing phase of a German-English phrase-based SMT system. Their experiments showed that this method could also improve translation quality significantly. Our study differs from those of [24] and [6] in several important respects. First, our transformational model is based on statistical decisions, while neither of the previous studies used probability in their reordering method. Second, the transformational model is trained by using bitext and only a source language parser, while Xia and McCord [24] employed parsers of both source and target languages. Third, we consider translation from English to Vietnamese and from English to French.

Reranking [20,18] is a frequently-used postprocessing technique in SMT. How-ever, most of the improvement in translation quality has come from the reranking of non-syntactic features, while the syntactic features have produced very small gains [18]. 1.3 Our Work In our previous work [23], we studied about improving phrase-based SMT using morphological and syntactic transformation in preprocessing phase. We proposed a syntactic transformation model based on the probabilistic context free gram-mar. We considered translation from English to Vietnamese on small corpora. Our various experiments showed improvements in translation quality. However, there were several open questions. Fir st, the corpora are small, which leads to suspicion that the improvements (made by syntactic transformation) over Pharaoh will vanish as the corpora scales up. 1 Second, word-order problem is considered in both preprocessing and decoding phases. It is not clear whether the translation quality is improved if decoding is carried out without reordering. Third, when the syntactic transformation is used, does the SMT system need long phrases to achieve high translation quality? In this paper, we aim to find the answers to these questions.

Our approach is shown in Fig. 1. In the syntactic-transformation phase, first a source sentence is parsed, resulting in a syntactic tree. Next the tree is trans-formed into the target language structure. Then the surface string is extracted from the transformed tree. The resulting source sentence has a word order of the target language. In the decoding phase, the decoder searches for the best target sentence without reordering source phrases. The syntactic analysis and transformation are applied in both training and testing.

For syntactic transformation, we describe a transformational model [23] based on the probabilistic context free grammar. The knowledge of the model is learned from bitext in which the source text has been parsed. In order to demonstrate the effectiveness of the proposed method, we carried out experiments for two language pairs: English-Vietnamese and English-French. We used Pharaoh [10] as a baseline phrase-based SMT system. The experiments showed significant improvements of BLEU score.

The rest of this paper is organized as follows: In section 2, syntactic transfor-mation is presented, including the transformational model, training, and apply-ing the model. Section 3 describes experimental results. One major difficulty in the syntactic transformation task is ambiguity. There can be many different ways to reorder a CFG rule. For example, the rule 2 NP  X  DTJJNN in English can become NP  X  DTNNJJ or NP  X  NNJJDT in Vietnamese. For the phrase  X  X  nice weather X , the first reordering is most appropriate, while for the phrase  X  X his nice weather X , the second one is correct. Lexicalization of CFG rules is one way to deal with this problem. Therefore we propose a transformational model which is based on probabilistic decisions and also exploits lexical information. 2.1 Transformational Model Suppose that S is a given lexicalized tree of the source language (whose nodes are augmented to include a word and a part of speech (POS) label). S contains n applications of lexicalized CFG rules LHS i  X  RHS i ,1  X  i  X  n , (LHS stands for left-hand-side and RHS stands for right-hand-side). We want to transform S into the target language word order by applying transformational rules to the CFG rules. A transformational rule is represented as ( LHS  X  RHS, RS )whichis a pair consisting of an unlexicalized CFG rule and a reordering sequence (RS). For example, the rule ( NP  X  JJNN, 10) implies that the CFG rule NP  X  JJNN in source language can be transformed into the rule NP  X  NNJJ in target language. Since the possible transformational rule for each CFG rule is not unique, there can be many transformed trees. The problem is how to choose the best one. Suppose that T is a possible transformed tree whose CFG rules are annotated as LHS i  X  RHS i which is the result of reordering LHS i  X  RHS i using a transformational rule ( LHS i  X  RHS i ,RS i ). Using the Bayes formula, we have:
The transformed tree T  X  which maximizes the probability P ( T | S ) will be chosen. Since P ( S )isthesameforevery T ,and T is created by applying a sequence Q of n transformational rules to S ,wecanwrite: The probability P ( S | T ) can be decomposed into: where the conditional probability P ( LHS i  X  RHS i | LHS i  X  RHS i )iscom-puted with the unlexicalized form of the CFG rules. Moreover, we constraint:
To compute P ( T ), a lexicalized probabilistic context free grammar (LPCFG) can be used. LPCFGs are sensitive with bot h structural and lexical information. Under a LPCFG, the probability of T is:
Since application of a transformational rule only reorders the right-hand-side symbols of a CFG rule, we can rewrite (2): Suppose that a lexicalized CFG rule has the following form: where F ( h ), H ( h ), R i ( r i ), and L i ( l i ) are all lexicalized non-terminal symbols; F ( h ) is the left-hand-side symbol or parent symbol, h is the pair of head word and its POS label; H is a head child symbol; and R i ( r i )and L i ( l i ) are right and left modifiers of H .Either k or m may be 0, k and m are 0 in unary rules. Since the number of possible lexicalized rules is huge, direct estima-tion of P ( LHS  X  RHS ) is not feasible. Fortunately, some LPCFG models [5,3] can compute the lexicalized rule X  X  probability efficiently by using the rule-markovization technique [5,3,8]. Given the left hand side, the generation process of the right hand side can be decomposed into three steps: Generate the head constituent label, generate the right modifiers, and generate the left modifiers. This is zeroth order markovization (the generation of a modifier does not depend on previous generations). Higher orders can be used if necessary.

The LPCFG which we used in our experiments is Collins X  Grammar Model 1 [5]. We implemented this grammar model with some linguistically-motivated refinements for non-recursive noun phrases, coordination, and punctuation [5,1]. We trained this grammar model on a treebank whose syntactic trees resulted from transforming source language trees. In the next section, we will show how we induced this kind of data. 2.2 Training The required resources and tools include a bilingual corpus, a broad-coverage statistical parser of the source language, and a word alignment program such as GIZA++ [16]. First, the source text is parsed by the statistical parser. Then the source text and the target text are aligned in both directions using GIZA++. Next, for each sentence pair, source syntactic constituents and target phrases (which are sequences of target words) are aligned. From this hierarchical align-ment information, transformational rules and transformed syntactic tree are in-duced. Then the probabilities of transformational rules are computed. Finally, the transformed syntactic trees are used to train the LPCFG.
 Fig. 2 shows an example of inducing transformational rules for English-Vietnamese translation. Source sentence and target sentence are in the middle part of the figure, on the left. The source syntactic tree is in the upper left part of the figure. The source constituents are numbered. Word links are represented by dotted lines. Words and aligned phrases of the target sentence are represented by lines (in the left lower part of the figure) and are also numbered. Word align-ment results, hierarchical alignment resu lts, and induced transformational rules are in the lower right part of the figure. The transformed tree is in the upper right.

To determine the alignment of a source constituent, link scores between its span and all of the target phrases are computed using the following formula [24]: where s is a source phrase, t is a target phrase; links(s,t) is the total number of source words in s and target words in t that are aligned together; words(s) and words(t) are, respectively, the number of words in s and t. A threshold is used to filter bad alignment possibilities. After the link scores have been calculated, the target phrase, with the highest link score, and which does not conflict with the chosen phrases will be selected. Two target phrases do not conflict if they are separate or if they contain each other.

We supposed that there are only one-to-one links between source constituents and target phrases. We used a number of heuristics to deal with ambiguity. For source constituents whose span contains only a word which is aligned to many target words, we choose the best link bas ed on the intersection of directional alignments and on word link score. When applying formula (8) in determining alignment of a source constituent, if there were several target phrases having the highest link score, we used an additional criterion:  X  for every word outside s, there is no link to any word of t  X  for every word outside t, there is no link to any word of s
Given a hierarchical alignment, a transformational rule can be computed for each constituent of the source syntactic tree. For a source constituent X with children X 0 , ..., X n and their aligned target phrases Y, Y 0 , ..., Y n (in which Y i are sorted increasingly according to the index of their first word), the conditions for inducing the transformational rule are as follows:  X  Y i are adjacent to each other.  X  Y contains Y 0 , ..., Y n but not any other target phrases.
 Suppose that f is a function in which f ( j )= i if X i is aligned to Y j .Ifthe conditions are satisfied, a transformational rule ( X  X  X 0 ...X n ,f (0) ...f ( n )) can be inferred. For example, in Fig. 2, the constituent SQ (9) has four children A U X 0 (1), NP 1 (6), ADV P 2 (7), and NP 3 (8). Their aligned target phrases are 3 : Y (9), Y 0 (1), Y 1 (2), Y 2 (3), and Y 3 (8). From the alignment information: 1-3, 6-2, 7-1, and 8-8, the function f is determined: f(0)=2, f(1)=1, f(2)=0, and f(3)=3. Since the target phrases satisfy the previous conditions, the transformation rule ( SQ  X  A U XNPADVPNP ,2103)isi nduced.

For a sentence pair, after transformational rules have been induced, the source syntactic tree will be transformed. The constituents which do not have a trans-formational rule remain unchanged (all constituents of the source syntactic tree in Fig. 2 have a transformational rule). Their corresponding CFG rule applications are marked as untransformed and are not used in training the LPCFG.

The conditional probability for a pair of rules is computed using the maximum likelihood estimate: P ( LHS  X  RHS | LHS  X  RHS )= In training the LPCFG, a larger number of parameter classes have to be es-timated such as head parameter class, modifying nonterminal parameter class, and modifying terminal parameter class. Very useful details for implementing Collins X  Grammar Model 1 were described in [1]. 2.3 Applying After it has been trained, the transformational model is used in the preprocessing phase of a SMT system. Given a source sentence, first it is parsed. Next the resulting syntactic tree is lexicalized by associating each non-terminal node with a word and a part of speech (computed bottom-up, through head child). Then the best sequence of transformational rules is computed by formula (6). Finally, by applying transformational rules on the source tree, the best transformed tree is generated. Finally, the surface string is extracted from the transformed syntactic tree. 3.1 Experimental Settings We carried out some experiments of translation from English to Vietnamese and from English to French. For the first language pair, we used two small corpora: one collected from some computer text books (named Computer) and the other collected from some grammar books (named Conversation). For the second language pair, we used the freely available Europarl corpus [9]. Data sets are described in Table 1. For the quick experimental turn around, we used only a part of the Europarl corpus for training. We created test set by choosing sentences randomly from the common test part [9] of this corpus.

A number of tools were used in our experiments. Vietnamese sentences were segmented using a word-segmentation program [22]. For learning phrase transla-tions and decoding, we used Pharaoh [10], a state-of-the-art phrase-based system which is available for research purpose. For word alignment, we used GIZA++ tool [16]. For learning language models, we used SRILM toolkit [21]. For MT evaluation, we used BLEU measure [19] calculated by the NIST script version 11b. For the parsing task, we used the Charniak X  X  parser [3]. 3.2 Training the Transformational Model On each corpus, the transformational model was trained resulting in a large number of transformational rules and an instance of Collins X  Grammar Model 1. We restricted the maximum number of syntactic trees used for training the transformational model to 40000. Table 2 shows the statistics which resulted from learning transformational rules. On three corpora, the number of transfor-mational rule groups which have been learned is smaller than the corresponding number of CFG rules. The reason is that there were many CFG rules which appear several times, however their hierarchical alignments did not satisfy the condition of inducing transformational rule. Another reason is that there were CFG rules which required nonlocal transformation. 4 3.3 BLEU Scores Table 3 shows BLEU scores of the Pharaoh system (baseline) and the Pharaoh decoder with preprocessing using syntact ic transformation and monotone set-ting. On the Vietnamese corpora, the improvements are 2.5% and 2.4%. On the Europarl corpus, the improvement is smaller, only 1.61%. The difference of those values can be explained in some ways: First, we are considering word or-der problem, so the improvement is higher with language pairs which are more different in word order. According to our knowledge, Vietnamese and English are more different in word order than that of French and English. Second, by using phrases as the basic unit of translation, phrase-based SMT captures local reordering quite well if there is a large amount of training data.

In order to test the statistical significance of our results, we chose the sign test 5 [11]. We selected a significance level of 0.05. The Computer test set was divided into 23 subsets (15 sentences per subset), and the BLEU metric was computed on these subsets individually. The translation system with syntactic transformation was then compared with the baseline system over these subsets. We found that the system with preprocessing had a higher score than the baseline system on 20 subsets, and the baseline system had a higher score on 3 subsets. With the chosen significance level of 0.05 and the number of subsets 23, the critical value is 7. So we can state that the improvement made by the system with syntactic transformation was statistically signifi cant. The same experiments were carried out on the other test sets (see Table 4). All the improvements were statistically significant. 3.4 Maximum Phrase Length Table 5 displays the performance of the baseline SMT system and the syntactic-transformation SMT system with various maximum phrase lengths. 6 Obviously, the translation quality of both systems changes up when the maximum phrase length increases. The second system can achieve a high performance with a short maximum phrase length, while the first system requires a longer maximum phrase length to achieve a similar performance. The improvement of the SMT system with syntactic transformation over the baseline SMT system decreases slightly when the maximum phrase length increases. This experiment gives us two suggestions. First, a maximum phrase length of three or four is enough for the SMT system with syntactic transformation. Second, the baseline SMT system relies on long phrases to solve the word order problem while the other SMT system is based on syntactic transformation to do that. 3.5 Training-Set Size In this section, we report BLEU scores and decoding times corresponding to various sizes of the training set (in terms of sentence pairs). In this experiment, we used Europarl data sets and we chose a maximum phrase length of four. Table 6 shows an improvement in BLEU score of about 2% for all training sets. It means the improvement over Pharaoh does not decrease as the training set scales up. Note that studies which use morphological analysis for SMT have a contrary property of vanishing improvement [7]. Table 7 shows that, for all training sets, the decoding time of the SMT system with syntactic transformation is about 5-6% that of the Pharaoh system. This is an advantage of monotone decoding. Therefore we save time for syntactic analysis and transformation.
 We have demonstrated that solving the word-order problem in the preprocess-ing phase using syntactic transformation can improve phrase-based SMT sig-nificantly. For syntactic transformation, we have described a transformational model based on the probabilistic context free grammar and a technique of in-ducing transformational rules from source-parsed bitext. Our method can be applied to other language pairs, especially when the target language is poor in resources.

By experiments, we have found out the answers for the questions mentioned in Section 1.3. First, by using syntactic transformation in preprocessing and monotone decoding, the translation quality is improved and the decoding time is reduced. Second, the improvement does not vanish as the training-set size increases. Third, in order to achieve the same performance, the maximum phrase length is shorter than that of the baseline system.

In the future, we would like to apply this approach to other language pairs in which the difference in word order is greater than that of English-Vietnamese and English-French. We also would like to extend the transformational model to dealing with non-local transformations.
 d i ct i onar i es or/and term i nology research for human translators .
There i s a vast l i terature on word al i gnment and many approaches have been l i ngu i st i c approaches .

Stat i st i cal approaches also refer to corpus-based approaches, wh i ch al i gn word al i gnment has been i nsp i red by the work on IBM SMT i ntroduced by Brown [2] . IBM models can be i mproved us i ng dependenc i es on word class, smooth i ng techn i ques for the strength of correlat i on between source and target words [4] .

L i ngu i st i c approaches also refer to knowledge-based approaches, wh i ch al i gn languages [5], [6], [7] . Stochast i c i nvers i on transduct i on grammars were also proposed [8] . For Ch i nese and Japanese, Zhang presented a method us i ng Engl i sh maps [10] .

Among the ex i st i ng word al i gnment methods, the unknown word problem, the synonym problem and the global opt i m i zat i on problem are very i mportant factors many unknown words . Most word al i gnment methods deal w i th unknown words stat i st i cally . These methods could hardly avo i d the problem of data sparseness; b) For b i l i ngual lex i cons are useless for word al i gn i ng . We named th i s synonym problem; c) Many w i dely used match i ng methods could only get local opt i mal solut i on . We need i mproved match i ng methods to get global opt i m i zat i on .

In th i s paper, we propose a word al i gnment model between Ch i nese and Japanese part of speech and co-occurrence, and matches words by max i mum we i ght match i ng solve the local opt i m i zat i on problem .

The paper i s structured as follows : i n sect i on 2, we descr i be the deta i ls of our word al i gnment model; i n sect i on 3, the exper i ment des i gn and results are shown, as well as analys i s; conclus i ons are g i ven i n sect i on 4 . The word al i gnment model proposed i n th i s paper represents sentence pa i r by graph .
 2.1 Representat i on the words i n the sentence . Ch i nese sentence and Japanese sentence are represented by set } , , , { Vert i ces correspond to Ch i nese words and Japanese words i n the g i ven sentence . Each edge e hk corresponds to a we i ghted l i nk between Ch i nese word c h and Japanese can be calculated w i th the s i m i lar i ty metr i c descr i bed i n the next subsect i on . 2.2 S i m i lar i ty Measure morpholog i cal s i m i lar i ty, semant i c d i stance, part of speech and co-occurrence . and i t i s calculated as follows : translat i on of Japanese word j ; else returns 0 . When S i mD(c, j ) i s equal to 0, we need semant i c d i stance (S i mS), part of speech (S i mP) and co-occurrence (Asso) . And we follows . a) Morpholog i cal S i m i lar i ty (S i mM) Ch i nese characters . Over half of the Japanese words i nclude kan ji and 28% cons i st of between Ch i nese word and Japanese word . Modern Ch i nese has two ma i n character shares many characters w i th these two Ch i nese character sets .

We assumed that a Ch i nese word and a Japanese word tend to be a translat i on pa i r i f they have common characters . To prove our assumpt i on, we made two level .
 found that there are 15,970 entr i es (37%) wh i ch have common character(s) . Coeff i c i ent (at character level) between c h and each Japanese word j k i n correspond i ng between c h and j k i s larger than 0 X  . Table . 1 shows the data concern i ng correlat i on . as follows :
As a result, the Correlat i on Coeff i c i ent of 0 . 64 strongly supported our assumpt i on of correlat i on between word al i gnment and morpholog i cal s i m i lar i ty . measures the s i m i lar i ty i n terms of morphology, and i t i s calculated as follows : between s i mpl i f i ed Ch i nese and Japanese . b) Semant i c S i m i lar i ty (S i mS) follows : D i ct(c) i s the set of translat i on words ( i n Japanese) of Ch i nese word c . The value of Conrath  X  s measure [11], [12] . c) Part of Speech (S i mP) follows : d) Co-occurrence (Asso) 2.3 Match i ng proposed two match i ng algor i thms wh i ch can select a set of l i nks as word al i gnment result .
 edges E ass i gn i ng vert i ces of set C to vert i ces of set J, such that no two edges i n the match i ng share a common vertex [13] . M(G) denotes match i ng of b i part i te graph G . Def i n i t i on 2. A max i mum we i ght match i ng (MWM) i s a match i ng such that the sum max i mum we i ght match i ng of b i part i te graph G .
The word al i gnment task can be the process of seek i ng the MWM on the b i part i te graph wh i ch represents the i nput sentence pa i r . The algor i thm i s as follows : result .
 However, the MWM match i ng bo i ls down to f i nd i ng 1-to-1 al i gnments, as a match i ng results of real text . We thus extended the MWM by g i v i ng add i t i onal attent i on to the for i nd i cat i ng m-to-n al i gnments .
 follows : M part of f i nal al i gnment result . So does M S (G) .

Then get M A . 2 (G) as follows : the un i on of the three subsets w i ll lead to a large i ncrease of recall w i th a few loss of prec i s i on .

F i nally output M A . 2 (G) as the al i gnment result . 3.1 Exper i ments Des i gn For the evaluat i on of our word al i gnment model, we used a sentence-al i gned Ch i nese-Japanese parallel corpus (for Asso), a b i l i ngual lex i con between Ch i nese and Japanese (for S i mD) and the EDR thesaurus (for S i mS) .

A randomly chosen subset of the corpus i s used as the test set wh i ch conta i ns 100 words . There are 511,326 tokens and 24,304 types i n the corpus . Ch i nese and Japanese sentences are segmented by NEUCSP (ava i lable i n www . nlplab . com) and Chasen (ava i lable i n chasen . a i st-nara . ac .j p) respect i vely .
To evaluate our al i gnment model, we bu i lt seven al i gnment systems (WA_1a, b, c, d, e and WA_2a, b) by our al i gnment model, and bu i lt three more systems (Basel i ne1, Basel i ne2 and GIZA++[1]) by other methods as basel i ne .
 systems to evaluate the effect of our s i m i lar i ty measure methods .
Basel i ne2, WA_2a and WA_2b are bu i lt by i ntroduc i ng all of our s i m i lar i ty measure methods . The d i fference among these three systems l i es i n the match i ng method . Basel i ne2 used Compet i t i ve L i nk i ng match i ng; WA_2a and WA_2b used we compare these three systems to evaluate the performance of our match i ng methods . Also, we compare our best system WA_2b w i th GIZA++ .
 The deta i ls of these exper i ments are shown i n Table . 2 .

For all the exper i ments, we manually create the correct al i gnment, and evaluate the recall, prec i s i on and F-Score . 3.2 Exper i mental Results Table 3 shows the result of Expt . 1 :
From Table 3, we found that WA_1a ach i eved F-Score of 0 . 69, wh i ch i s much morpholog i cal s i m i lar i ty (S i mM) . Many unknown words have been al i gned by S i mM . Because the Ch i nese character set shares many characters w i th the Japanese character set, dur i ng translat i ng Ch i nese unknown word to Japanese, people tend to use common characters between Ch i nese and Japanese, espec i ally for named ent i ty . large i ncrease of recall w i th a few loss of prec i s i on .
 Also, we found that WA_1b ach i eved F-Score of 0 . 58, wh i ch i s h i gher than lex i con can only al i gn a small part of words . Basel i ne1 thus get a low recall of 0 . 38 . Introduc i ng S i mS, the system WA_1b can al i gn many del i cately translated word pa i rs by synonym cha i n .
 WA_1c ach i eved F-Score of 0 . 58 wh i ch shows how much the S i mP w i ll do . WA_1d ach i eved F-Score of 0 . 69 wh i ch shows how much the corpus alone w i ll do . methods .
 Table 4 shows the result of Expt . 2 : From Table 4, we f i nd that WA_2b i mproved the F-score than Basel i ne2 (3% up) . can apply global opt i mal match i ng .
 the method can only f i nd local opt i mal al i gnment wh i ch may i nclude wrong l i nks . For are l i sted .
 l i nks i s 102,348 wh i ch i s the largest . Our match i ng method compares the we i ghts of thus performed better than Basel i ne2 .

From Table 4, we also f i nd that WA_2b i mproved the F-score than WA_2a does Therefore, A . 2 led to a large i ncrease of recall w i th a few loss of prec i s i on . word al i gnment approach i s proved to be effect i ve by these exper i mental results . occurrence, and matches words by the max i mum we i ght match i ng on b i part i te graph . We made a word al i gnment system based on the model, and evaluated the system on GIZA++ .
 enhanc i ng the algor i thm A . 2 by add i ng more restr i cted cond i t i ons to formula 8, 9 and help to mach i ne translat i on . Th i s research was supported by The Document Company Fu ji -Xerox under the VFP Foundat i on of Ch i na (No . 60473140), 985 pro j ect of Northeastern Un i vers i ty (No . 985-2-DB-C03) and Program for New Century Excellent Talents i n Un i vers i ty (No . NCET-05-0287) .
 Machine transliteration has received significant attention as a supporting tool for machine translation (MT) [1,2] and cross-language information retrieval (CLIR) [3,4]. During the last decade , several transliteration models  X  grapheme 1 -based transliteration model (GTM) [5,6,7,8], phoneme 2 -based transliteration model (PTM) [1,9,10], hybrid transliteration model (HTM) [2,11], and correspondence-based transliteration model (CTM) [12,13,14]  X  have been proposed. These models are classified in terms of the information sources for transliteration or the units to be transliterated; GTM , PTM , HTM ,and CTM make use of source graphemes, source phonemes, both source graphemes and source phonemes, and the cor-respondence between source graphemes and phonemes, respectively. Although each transliteration model has shown relatively good performance, it often pro-duced transliterations with errors. The errors are mainly caused by complex transliteration behaviors, meaning that a transliteration process dynamically uses both source graphemes and source phonemes. Sometimes either source graphemes or source phonemes contribute to the transliteration process; while sometimes both contribute. Therefore, it is hard to consider the complex translit-eration behaviors depending on one transliteration model because one model just concentrates on only one of the complex transliteration behaviors. To ad-dress this problem, we combined the different transliteration models with a  X  generating transliterations followed by their validation  X  X trategyasshownin Fig. 1. First, we generate transliteration candidates (or a list of translitera-tions) using GTM, PTM, HTM, and CTM. Then, we validate the candidates using two measures  X  a transliteratio n model-based measure and a web-based measure.

This paper is organized as follows. In section 2, we review previous work based on the four transliteration models. In section 3, we describe the framework of different transliteration models, and i n section 4, we describe the translitera-tion validation. In section 5, we describe our experiments and results. We then conclude in section 6.
 2.1 Grapheme-Based Transliteration Model The grapheme-based transliteration model (GTM) is conceptually a direct or-thographical mapping model from source graphemes to target graphemes. Sev-eral different transliteration methods have been proposed within this framework. Kang &amp; Choi [5] proposed a decision tree-based transliteration method. Deci-sion trees, which transform each source grapheme into target graphemes, are learned and then they are directly applied to machine transliteration. Kang &amp; Kim [6] and Goto et al. [7]proposedamethodbasedonatransliterationnetwork. The transliteration network is composed of nodes and arcs. A node represents a chunk of source graphemes and its corresponding target grapheme. An arc rep-resents a possible link between nodes and it has a weight showing its strength. Li et al. [8] used a joint source-channel model to simultaneously model both the source language and the target language contexts (bigram and trigram) for machine transliteration. Its main advantage is the use of bilingual contexts.
The main drawback of GTM is that it does not consider any phonetic aspect of transliteration. 2.2 Phoneme-Based Transliteration Model Basically, the phoneme-based transliteration model (PTM) is composed of source grapheme-to-source phoneme transformation and source phoneme-to-target grapheme transformation. Knight &amp; Graehl [1] modeled Japanese-to-English transliteration with weighted finite state transducers (WFSTs) by combining several parameters such as romaji-to-phoneme, phoneme-to-English, English word probability models, and so on. Meng et al. [10] proposed an English-to-Chinese transliteration model. It was based on English grapheme-to-phoneme conversion, cross-lingual phonologica l rules and mapping rules between Eng-lish and Chinese phonemes, and Chinese syllable-based and character-based lan-guage models. Jung et al. [9] modeled English-to-Korean transliteration with extended Markov window. First, they transformed an English word into English pronunciation by using a pronunciation dictionary. Then they segmented the English phonemes into chunk of English phonemes, which corresponds to one Korean grapheme by using predefined handcrafted rules. Finally they automat-ically transformed each chunk of English phoneme into Korean graphemes by using extended Markov window.

The main drawback of PTM is error propagation caused by its two-step pro-cedure  X  errors in source grapheme-to-source phoneme transformation make it difficult to generate correct transliterations in the next step. 2.3 Hybrid Transliteration Model and Correspondence-Based There have been attempts to use both source graphemes and source phonemes in machine transliteration. Such research falls into two categories, the correspondence-based transliteration model (CTM) [12,13,14] and the hybrid transliteration model (HTM) [2,11]. The CTM makes use of the correspondence between a source grapheme and a source phoneme when it produces target language graphemes; the HTM just combines GTM and PTM through linear interpolation. The hybrid transliteration model requires the grapheme-based transliteration probability ( Pr ( GT M )) and phoneme-based transliteration prob-ability ( Pr ( PTM )), and then it combines the two probabilities through linear interpolation.

Oh &amp; Choi [12] considered the contexts of a source grapheme and its cor-responding source phoneme for English-to-Korean transliteration. It is based on semi-automatically constructed context-sensitive rewrite rules in a form, A/X/B  X  y , meaning that X is rewritten as target grapheme y in the con-text A and B. Note that X , A ,and B represent correspondence between English grapheme and phoneme like  X  r : | R |  X   X  English grapheme r corresponding to English phoneme | R | . Oh &amp; Choi [13,14] trained a generative model representing transliteration rules by using the correspondence between source grapheme and source phoneme, and machine learning algorithms. The correspondence makes it possible to model machine transliteration in a more sophisticated manner.
Several researchers [2,11] have proposed hybrid model-based transliteration methods. They modeled GT M and PTM with WFSTs or a source-channel model. Then they combined GT M and PTM through linear interpolation. In their PTM , several parameters are considered, such as the source grapheme-to-source phoneme probability, source phoneme-to-target grapheme probability, target language word probability, and so on. In their GT M ,the source grapheme-to-target grapheme probability is mainly considered. Let SW be a source word, P SW be the pronunciation of SW , T SW be a target word corresponding to SW ,and C SW be a correspondence between SW and P SW . P
SW and T SW can be segmented into a series of sub-strings, each of which corre-sponds to a source grapheme. Then, we can write SW = s 1 ,  X  X  X  ,s n = s n 1 , P SW = p t ,and c i = &lt;s i , p i &gt; represent the i t h source grapheme, source phonemes corre-sponding to s i , target graphemes corresponding to s i and p i , and the correspon-dence between s i and p i , respectively. With this definition, GTM, PTM, CTM, and HTM can be represented as Eqs. (1), (2), (3), and (4), respectively.
With the assumption that each transliteration model depends on the size of the contexts, k , Eqs. (1), (2), (3) and (4) can be simplified. To estimate the probabilities in Eqs. (1), (2), (3), and (4), we used the maximum entropy model, which can effectively incorporate heterogeneous information [15]. In the maxi-mum entropy model, event ev is composed of a target event ( te )andahistory event ( he ), and it is represented by a bundle of feature functions ( f i ( he, te )), which represent the existence of certain characteristics in the event ev .Thefea-ture function enables a model based on the maximum entropy model to estimate probability [15]. Therefore, designing the feature functions, which effectively sup-port certain decisions made by the model, is important. Our basic philosophy for the feature function design for each transliteration model is that the con-text information collocated with the unit of interest is important. With this philosophy, we designed the feature functions with all possible combinations of ( s is an exponential log-linear model that gives the conditional probability of event ev = &lt;te,he&gt; , as described in Eq. (5), where  X  i is a parameter to be estimated, and Z ( he ) is the normalizing factor [15].

With Eq. (5) and feature functions, conditional probabilities can be estimated Pr ( te CTM | he CTM ) because we can represent target events ( te CTM ) and history as Pr ( te | he ) with their target events and history events. We used a maximum entropy modeling tool [16] to estimate Eqs. (1), (2), (3), and (4). We validated transliterations by using web-based validation, S web ( s, tc i ), and transliteration model-based validation, S tm ( s, tc i ), like in Eq. (6). Using Eq. (6), we can validate transliterations in a more correct and robust manner because S web ( s, tc i ) reflects real-world usage of the transliterations in web data and S tm ( s, tc i ) ranks the transliterations independent of the web data.
 4.1 Transliteration Model-Based Validation: S tm Our transliteration model-based validation, S tm , uses the rank assigned by each transliteration model. For a given source word ( s ), each transliteration model generates transliterations ( tc i in TC ) and ranks them using the probability described in Eqs. (1), (2), (3), and (4). The underlying assumption in S tm is that the rank of the correct transliterations tends to be higher, on average, than the wrong ones. With this assumption, we represented S tm ( s, tc i )asEq.(7), where Rank g ( tc i ), Rank p ( tc i ), Rank h ( tc i ), and Rank c ( tc i )representtherank of tc i assigned by GTM, PTM, HTM, and CTM, respectively.

S tm ( s, tc i )= 4.2 Web-Based Validation: S web Korean or Japanese web pages are usually composed of rich texts in a mixture of Korean or Japanese (main language) and English (auxiliary language). Let s and t be a source language word and a target language word, respectively. We observed that s and t tend to be near each other in the text of Korean or Japanese web pages when the authors of the web pages describe s as translation of t , or vice versa. We retrieved such web pages for transliteration validation.
There have been several web-based validat ion methods for translation valida-tion [17,18] or transliteration validation [2,19]. They usually rely on the web fre-quency (the number of web pages) derived from  X  Bilingual Keyword Search ( BKS ) X  [2,17,18] or  X  Monolingual Keyword Search ( MKS ) X  [2,19]. BKS re-trieves web pages by using a query composed of two keywords, s and t ; while MKS retrieves web pages by using a query composed of t . Qu &amp; Grefenstette [17] and Wang et al. [18] proposed BKS-based translation validation methods, such as rel-ative web frequency and chi-square (  X  2 ) test. Al-Onaizan &amp; Knight [2] used both MKS and BKS and Grefenstette et al. [19] used only MKS for validating translit-erations. However, web pages retrieved by MKS tend to show whether t is used in target language texts rather than whether t is a translation of s .BKSfrequently retrieveswebpageswhere s and t have little relation to each other because it does not consider distance between s and t in the web pages. To address these prob-lems, we developed a validation method based on  X  Bilingual Phrasal Search ( BPS ) X , where a phrase composed of s and t is used as a query for a search engine. Let  X  X  st ] X  or  X  X  ts ] X ,  X  s And t  X , and  X  t  X , respectively, be queries for BPS, BKS, and MKS. The difference among BPS, BKS, and MKS is shown in Fig. 2. In Fig. 2,  X  X  s t ] X  or  X  X  ts ] X  retrieves web pages where  X  X  st ] X  or  X  X  ts ] X  exists as phrases; while  X  s And t  X  retrieves web pages where s and t simply exist in the same document. Therefore, the number of web pages retrieved by BPS is more reliable for validating translit-erations, because s and t usually have high co-relation in the web pages retrieved by BPS. For example, web pages retrieved by BPS in Fig. 3 usually contain cor-rect Korean and Japanese transliterations and their corresponding English word amylase as translation pairs in parentheses expression. For these reasons, BPS is more suitable for our transliteration validation.

Let TC be a set of transliterations (or transliteration candidates) produced by different transliteration models, tc i be the i t h transliteration candidate in TC , s be the source language word resulting in TC ,and WF ( s, tc i )betheweb frequency for [ stc i ]. Our web-based validation method, S web , can be represented as Eq. (8), which is the relative web frequency derived from BPS.
 Let TC for s = data be { tc 1 = , tc 2 = , tc 3 = } and WF ( s, tc 1 )+ WF ( tc 1 ,s ), WF ( s, tc 2 )+ WF ( tc 2 ,s ), and WF ( s, tc 3 )+ WF ( tc 3 ,s ) be 94,100, 67,800, and 54, respectively. Then, S web for each tc i can be calculated as follows.  X  S web ( s, tc 1 ) = 94,100/161,954 = 0.5811  X  S web ( s, tc 2 ) = 67,800/161,954 = 0.4186  X  S web ( s, tc 3 ) = 54/161,954 = 0.0003. Our experiments were done for English-to-Korean and English-to-Japanese transliteration. The test set for the English-to-Korean transliteration (EKSet) [20] consisted of 7,172 English-Korean pairs  X  the number of training data was about 6,000 and the number of blind test data was about 1,000. The test set for the English-to-Japanese transliteration (EJSet), which consisted of English-katakana pairs from EDICT [21], consisted of 10,417 pairs  X  the number of training data was about 9,000 and the number of blind test data was about 1,000. EJSet con-tained one or more than one correct transliteration for one English word, like &lt; micro ,  X  &gt; ,and &lt; micro ,  X  &gt; ; the average number of Japanese transliterations for an English word was 1.15. EKSet and EJSet covered proper names, technical terms, and general terms. Evaluation was done in terms of the word accuracy ( WA ) in Eq. (9). In the evaluation, we used k -fold cross-validation ( k = 7 for the EKSet and k = 10 for the EJSet). The test set was divided into k subsets. Each one was used for testing, while the remainder was used for training. Then the average WA across all the k trials was computed. Through the cross-validation, we set  X  (0.4 for the EKSet and 0.5 for the EJSet) for HTM in Eq. (4). 5.1 Experimental Results Summaries of our experimental results conducted on EKSet and EJSet are shown in Table 1. In the table, GTM, PTM, HTM, and CTM represent the individual transliteration models used for generating transliterations. S tm , S web ,and S TV represent experimental results validated by Eqs. (7), (8), and (6), respectively. Moreover, we tested S web and S TV according to web search methods (BPS, BKS, and MKS) to show the effect of BPS on transliteration validation. We compared our proposed method with the previous work, GPC [6], GMEM [7], and HWFST [11] 3 . Note that only Top-1 was considered in the previous work because they, except for HWFST, focused only on the Top-1. The Top-n considers whether the correct transliteration is in the Top-n ranked transliterations 4 .
 Compared to individual transliteration models and previous work [6,7,11], S tm , S web (BPS), and S TV (BPS) are more effective, especially in the Top-1 5 . Although S tm by itself showed higher performance than individual transliteration models and previous work [6,7,11], S TV (BPS) (the combination of S tm and S web (BPS)) shows much better performance. The Top-1 of S TV (BPS) has the best perfor-mance 6 . The powerful transliteration validation ability of S TV (BPS) enables our method to achieve the best result in the Top-1. More specifically, S web (BPS) contributes highly to the performance improvement. This indicates that the web data used as the knowledge source for transliteration validation is very useful. Although S tm makes a small contribution to the performance improvement of S
TV (BPS) because S web (BPS) correctly validates transliterations whenever S tm does, the errors of S web (BKS) and S web (MKS) are well compensated for by S tm in S
TV (BKS) and S TV (MKS). For example, Korean and Japanese transliterations for the English words methoxyl and netware were validated by each validation method, as shown in Tables 2 and 3 Note that the value coupled with each translit-eration was assigned by each validation method. In Tables 2 and 3 S tm causes the rank of correct transliterations to be higher in S TV (BKS) and S TV (MKS) than in S web (BKS) and S web (MKS).

When comparing BPS with BKS and MKS, BPS is the most effective web search method for transliteration validation. S web based on MKS has the worst performance because it tends to validate whether tc i is used in a target language rather than whether it is used as a translation of s . Actually, S web basedonBKS is effective because BKS considers both s and tc i while it retrieves web pages. However, the more powerful retrieval ability of BPS protects S web basedonBPS from errors that S web basedonBKScausesasshowninTables2and3.Sohigher performance can be had with S web (BPS) than with S web (BKS)  X  about 12% im-provement in Top-1 of EKSet and about 7% improvement in Top-1 of EJSet 7 .The higher performance of S web (BPS) positively e ects S TV (BPS), thus the perfor-mance of S TV (BPS) is higher than that of S TV (BKS). Both S web and S TV based on BPS outperform those based on BKS or MKS.

The experimental results can be summarized as follows. { S tm , S web (BPS), and S TV (BPS) are more e ective than individual transliter-{ S TV (BPS) shows the best performance. { S web (BPS) mainly contributes the high performance of S TV (BPS). { BPS is the most e ective web search method for transliteration validation We proposed a novel approach for improving machine transliteration perfo rmance by combining multiple transliteration models. We applied a \ generating transliterations followed by their validation " strategy. We generated transliter-ation candidates using four di erent transliteration models and validat ed them using web-based validation and transliteration model-based validation .Experi-ments showed that combining multiple transliteration models was one way f or considering complex transliteration behaviors and that transliteratio n validation was very important for improving machine transliteration performance. O ur two transliteration validation methods were effective. The web-based validation method effectively filtered out wrong transliterations by using web data, which reflects real-world usage of transliterat ions, and the transliteration model-based validation method as a web-independent validation measure complemented the web-based validation method. Moreover, we showed that a web search method significantly affects the performance of the web-based validation method. Exper-iments showed that our  X  Bilingual Phrasal Search ( BPS ) X  is more suitable than  X  Bilingual Keyword Search ( BKS ) X  and  X  Monolingual Keyword Search ( MKS ) X  in transliteration validation.
 Clustering is an important technique that facilitates the navigation, search and analysis of information in large unstructured document collections. It is an un-supervised process to identify inherent groupings of similar documents, where documents exhibit high intra-cluster similarity and low inter-cluster similarity.
Many existing clustering algorithms optimize criterion functions with respect to the employed similarity measures over all the documents assigned to each possible partition of the collection [10,19]. They always impose some explicit and/or implicit constraints as to the number, size, shape or disjoint characteris-tics of target clusters. For example, partitional algorithms like k -means assume cluster number k and do not allow one document belonging to multiple groups. Although fuzzy clustering, such as fuzzy C -means algorithm [2,12], does support overlapping clusters by a membership function and a fuzzifier parameter, they are still confined by cluster number and can find only spherical shape clusters. Some algorithms are model-based, e.g., Naive Bayes or Gaussian Mixture model [1,13]. They assume certain probabilistic distributions of the documents and try to find a model maximizing the likelihood of data. When data cannot fit the pre-sumed distribution, poor cluster quality can result. k -way clustering or bisection algorithms [19] force clusters to be equally sized. Spectral clustering [7,8] has emerged as one of the most effective clustering tools based on max-flow/min-cut theorem [4]. However, they prohibit overlapping clusters.

We define natural document clustering as a problem of finding unknown num-ber of overlapping as well as cohesive document groups with varied sizes and arbitrary distributions of the data. We try to obtain the clustering results with these free characteristics by reducing as many external constraints as feasible and leaving things to the inherent grouping nature among documents. For this pur-pose, we propose a document clustering technique using a novel graph-theoretic algorithm, named Clique Percolation Method (CPM). The idea is to identify adjacent maximal complete subgraphs, which is referred to as Maximal Docu-ment Cliques (MDC), in the document similarity graph using a threshold clique, and then mingle those strongly adjacent MDCs to form naturally overlapping document clusters. Although it does introduce an explicit parameter k ,which is the size of the threshold clique, our algorithm can automatically settle the critical point, at which the natural clustering can be achieved. We show that CPM outperforms representative clustering methods with experiments on the benchmark data.
 The rest of this paper is organized as follows: Section 2 describes the proposed CPM; Section 3 presents the algorithmic implementation of this technique; Sec-tion 4 gives related work; Section 5 presents experimental evaluation results; Finally, we conclude this paper. 2.1 Preliminaries In general, suppose V = { d 1 ,d 2 ,...,d | V | } is a collection of documents. We rep-resent the collection by an undirect graph G =( V, E ), where V is the vertex set and E is the edge set such that each edge { i, j } is a set of two adjacent vertices d ,d j in V . The adjacent matrix M of the graph is defined by M =[ m ij ] | V | i,j =1 , where each entry w ij is the edge weight which is the value of similarity metric (in what follows we use Cosine coefficient) between d i and d j . The graph can also be unweighted where an edge exists indicating the distance of its two vertices smaller than some threshold, in which case w ij is binary.

A clique in G is a subset S  X  V of vertices, such that { i, j } X  E for all distinct { d complete subgraph of G . A clique is said to be maximal if its vertices are not a subset of the vertices of a larger clique, which is referred to as a Maximal Document Clique (MDC) in a document similarity graph. MDC is considered the strictest definition of a cluster [15]. In graph theory, enumerating all maximal cliques (equivalently, all maximal independent sets or all minimal vertex covers) is believed NP-hard [3,18].

Suppose | V | number of documents are given in a measure space with a similar-ity metric w ij . We define a binary relation  X  t between documents on G = { V, E } with respect to parameter t : i  X  t j := w ij  X  t , which is self-reflexive, symmetric and non-transitive. There is an edge { i, j } X  E connecting vertices d i and d j whenever i  X  t j with respect to threshold t . Figure 1 illustrates that given a matrix reflecting the distances between 7 documents and the t value, a series of graphs for the relation i  X  t j are produced with different connectivity densities. Clearly, if each MDC is considered as a cohesive form of cluster, we can discover different number of clusters from these graphs, where t =0 . 5, 2 . 5and3 . 5results in 7, 5 and 3 number of clusters, respectively. They display interesting properties of natural clusters except for excessive intra-cluster cohesiveness.

The series of graphs parameterized by t above can be seen as random graphs with constant set of vertices and a changing set of edges generated with prob-ability p , the probability two vertices can be connected by an edge. Intuitively, tuning the value of t is somehow equivalent to adding or removing some edges according to p in monotonic manner. In order for an appropriate t , we first deter-mine p c , the critical value of p , and then derive t from p c by making use of their interdependency relationship. The critical value p c is defined as the probability, under which a giant k -clique percolation cluster will emerge in the graph, and is known as the percolation threshold for a random network [6]. At this threshold, the percolation transition takes place (see Section 2.2). For clustering, the as-sumption behind is that no cluster can be excessively larger than others, which can be achieved by commanding p&lt;p c . 2.2 Clique Percolation Method in Random Graphs Concepts of k -Clique Percolation. The concept of k -clique percolation is fundamental for Clique Percolation Method (CPM) in random networks, which was studied in [6]. The successful applications of CPM for uncovering commu-nity structure of co-authorship networks, protein networks and word association graphs can be found in [14]. Hereby we briefly present some related notions. Definition 1. k -clique is defined as a complete subgraph of k vertices. Definition 2. k -clique adjacency: Two k -cliques are adjacent if they share k  X  1 vertices, i.e., if they differ only in a single vertex.
 Definition 3. k -clique percolation cluster is a maximal k -clique-connected sub-graph, i.e., it is the union of all k -cliques that are k -clique adjacent. Obviously, a k -clique percolation cluster is unnecessarily a MDC, but it must be equivalent to the union of all MDCs adjacent by at least k  X  1 vertices.
 Definition 4. k -clique adjacency graph is the compressed form of the original graph, where the vertices denote the k -cliques of the original graph and there is an edge between two vertices if the corresponding k -cliques are adjacent. Moving a particle along an edge on a k -clique adjacency graph is equivalent to rolling a k -clique template (threshold clique) from one k -clique on the original graph to an adjacent one. A k -clique template can be placed onto any k -clique of the original graph, and rolled to an adjacent k -clique by relocating one of its vertices and keeping other k  X  1 vertices fixed. Thus, the k -clique percolation clusters are all those subgraphs that can be fully explored by rolling a k -clique template in them [6]. Note that a k -clique percolation cluster consists of all MDCs adjacent by at least k  X  1 vertices. Thus, the cohesiveness of documents in a k -clique percolation cluster as well as the overlap degree between clusters can be tuned by the k value. The goal of CPM is to find all k -clique percolation clusters.
 Percolation Threshold p c . How to estimate the threshold probability p c of k -clique percolation with respect to k ( k  X  2)? The clique percolation theory emphasizes that under such p c (critical point), a giant k -clique percolation cluster that is excessively larger than other clusters will take place [9,6]. Intuitively, the greater the p ( p&gt;p c ) is, the more likely the giant cluster appears, and the larger its size (which includes most of graph nodes), as if using a k -clique can percolate the entire graph.

Consider the heuristic condition of template rolling at the percolation thresh-old: after rolling a k -clique template from a k -clique to an adjacent one by re-locating one of its vertices, the expectation of the number of adjacent k -cliques, where the template can roll further by relocating another of its vertices, be equal to 1. The intuition behind is that a larger expectation value would allow an infi-nite series of bifurcations for the rolling, ensuring that a giant cluster is present in the graph. The expectation value can be estimated as ( k  X  1)( | V | X  k ) p k  X  1 c =1, where ( k  X  1) is the number of template vertices that can be selected for the next relocation, ( | V | X  k ) is the number of potential destinations for this relocation, out of which only the fraction p k  X  1 is acceptable, because each of the new k  X  1 edgesmustexistinordertoreachanew k -clique after relocation. Therefore, the percolation threshold function p c ( k ) with respect to k and | V | is as follows: Generation of Random Graph. According to Eq. (1), we can obtain a series of critical values with regard to the threshold clique sizes provided, which are actually the threshold probabilities of connecting two document vertices by an edge at these critical points. How to generate a random graph with the exactly desirable connectivity is technically very challenging since the degree distribution of each vertex needs to be appropriately modeled. Some work on systematically modeling degree distribution has been done in the field of random networks [9]. In this study, we prefer to simplify our specific problem by using two heuristics.
First, for each vertex, we consider its N-Nearest Neighbors (NNB) instead of using a fixed similarity threshold value, where N is determined by the formula: where the factor | V | X  1 k  X  1 actually scale the size of the original graph down to the level of k -cliques in the graph and p c ( k ) is considered as the average proportion of k -cliques are the nearest neighbors of a given clique in the k -clique adjacency graph. Here we actually use the connectivity of the k -clique adjacency graph to simulate the original graph. The reason we don X  X  use N ( k )= p c ( k )  X  ( | V | X  1) is because the generated graph tends to be over dense since p c ( k )isnot the proportion of NNB nodes, but in fact the probability of two vertices being connected by an edge.

Secondly, we examine the co-relation between p and the similarity threshold t .Given p c , we can estimate the bound(s) of t c so that the graph with the approximated connectivity as that under p c could be generated. Because p -t are monotone, a graph could be produced with edge weights t greater than t c .We derive t c by a simple approximation: where w max and w min are the maximum and minimum values of document similarity in the collection, respectively. Intuitively, we deem that only edge weights somewhat larger than 0 . 5 are considered similar. We also observe p c ( k ) is well below 0 . 5 for the normal size of corpus as k is not too large ( &lt; 20), which can be shown in Fig. 2 and guarantees t c  X  1.

We then apply the two heuristics incrementally during the graph genera-tion process, i.e. by generating connections between NNBs for each vertex at first place and then prune the edges with weights less then t c . The intuition is that denser graphs are penalized more heavily by the combination of the two heuristics. The clustering process is turned out to be a problem of finding all MDCs and then merging those with at least k  X  1 common nodes into clusters. The proposed CPM clustering algorithm includes 5 major steps: 1. Preprocessing: Eliminate words in the stop list, use Porter X  X  stemmer as 2. Given k as parameter, compute Eq. (1) for p c ( k ), Eq. (2) for N ( k )andEq. 3. Create document similarity graph G from A , where each vertex is connected 4. Enumerate all MDCs in G using Algorithm 1; 5. Create a M  X  M adjacent matrix B (where M is the number of MDCs), find Enumerating Maximal Document Cliques. Algorithms for finding maxi-mal cliques (step 4) were studied in [3] and achieved processing time bounded by O ( v 2 )where v is the number of maximal cliques. Their algorithms are dis-tinctive because they can be applied to a graph of comparatively large size. We implement an efficient counterpart of the algorithm using back-tracking method (see Algorithm 1). A MDC is output at each end of back-track. The running time is O ( v ).
 Finding k -Clique Percolation Clusters. When all the MDCs are enumer-ated, a clique-clique adjacent matrix is prepared. It is symmetric where each row and column represents a MDC and the entries are the number of com-mon vertices between two cliques (the diagonal values are the sizes of MDCs). The k -clique percolation clusters are one-to-one correspondent to the connected components in the clique-clique adjacency graph represented by the matrix, which can be obtained using Algorithm 2 (step 5 above). The algorithm first Algorithm 1. Enumerate All MDCs creates a clique-clique adjacent matrix B , in which every off-diagonal entry smaller than k  X  1 and every diagonal element smaller than k are erased (line 2 X 12), and then carrying out a depth-first-search (DFS) to find all the connected components. Traditional hierarchical agglomerative clustering (HAC) are intrinsically graph-based like CPM. HAC treats each data point as a singleton cluster and then successively merges pairs of clusters until all clusters have been merged into a single cluster that contains all documents. Single-link, complete-link and average-link are the most popular HAC algorithms.

In single-link algorithm [16], the similarity between clusters is measured by their most similar members (minimum dissimilarity). Generally, agglomerative process are rather computationally intensive because the minimum of inter-cluster distances must be found at each merging step. For single-link clustering, an efficient implementation of Minimum Spanning Tree (MST) algorithms of a weighted graph is often involved. Therefore, single-link produces clusters that are subgraphs of the MST of the data and are also connected components. It is capable of discovering clusters of varying shapes, but often suffers from the so-called chaining effect. Complete-link [11] measures the similarity between two clusters by their least similar members (maximum dissimilarity). From graph-theoretic perspective, complete-link clusters are non-overlapping cliques and are related to the node colorability of graphs. Complete-link is not vulnerable to chaining effect, but generates excessive compact clusters and is thus very sensi-tive to outliers. Average-link clustering [5] is a compromise between single-link Algorithm 2. Find All k -Clique Percolation Clusters and complete-link: the similarity between one cluster and another is the averaged similarity from any member of one cluster to any member of the other cluster; it is less susceptible to outliers and elongated chains. 5.1 Data Sets We conduct the performance evaluations based on Reuters-21578 1 corpus, which is popular for document classification evaluation purpose. It contains 21,578 doc-uments manually grouped into 135 topic classes. The size of classes is very unbal-anced, ranging from 1 to 3945. Many documents have multiple category labels, and documents in each cluster have a broad scape of contents. In our experi-ments, we select documents that are assi gned to one or more topics, and have the attribute LEWISSPLIT= X  X EST X  with &lt; BODY &gt; and &lt; /BODY &gt; tags. There are 2,745 such original documents, denoted by OC2745, from which we then extract 2,349 documents with unique class labels to form our data set UC2349, and the rest of 396 documents with multiple classes to form MC396. Table 1 shows the statistics of these three resulted data sets. 5.2 Evaluation Metrics We adopt two quality metrics widely used for document clustering [17], i.e., F-measure and Entropy. The F-measure of a class i is defined as F ( i )= 2 PR P + R . The precision and recall of a cluster j with respect to a class i are defined as: P = P recision ( i, j )= N ij N j and R = Recall ( i, j )= N ij N i ,where N ij is the number of members of class i in cluster j , N j is the size of cluster j ,and N i is the size of class i . The overall F-measure of the clustering result is the weighted average of F ( i ): where | i | is the number of documents in class i .

Entropy provides a measure of homogeneity of a cluster. The higher the homo-geneity, the lower the entropy, and vice versa. For every cluster j in the clustering result, we compute p ij , the probability that a member of cluster j belonging to class i . The entropy of each cluster j is calculated using E j =  X  i p ij log( p ij ), where the sum is taken over all classes. The total entropy for a set of clusters is calculated as the sum of entropies of each cluster weighted by its size: where N j is the size of cluster j , m is the number of clusters, and N is the size of document collection. 5.3 Performance Evaluation Experiment 1. Table 2 shows the performance of CPM given the size of thresh-old clique. Obviously CPM produces more clusters than the number of categories in the benchmark. This is because Reuters corpus are manually classified ac-cording to a set of pre-defined keywords ( one for each class roughly). Thus the schema of categorization is rather unifarious. One document may belong to far more groups since the grouping criterion could be diverse. CPM is less limited by external constrains, which favors multifarious categorization schemes, and thus has more clusters. The least number of clusters are found at k =2where CPM is degenerated to find connected components, which actually partitions the collections. With larger k , the cluster number increases as larger k allows for more overlapping clusters.

In terms of both F-measure and Entropy, CPM performance improves rapidly at the first few k augments, but worsens slowly with the further increases. In-terestingly, there are some close optimal values of k on these data sets around 4 X 6. Unlike our expectation, the results on MC396, which contains documents all belonging to multiple classes, show inconsistencies on F-measure and En-tropy. For Entropy, it is reasonable that CPM performs the best on MC396 since CPM favors overlapping clusters. But F-measure gives the worst results on it. F-measure seems very sensitive to outliers and penalizes their recalls heavily. As we found relatively larger proportion of outliers in MC396 clustering results, this may explain the low F-values.

We originally expected that the results on OC2745 would be far and few be-tween UC2349 and MC396, but the worst Entropy results are observed on it. One possible reason is that Entropy favors small cluster number and cluster size. This may also explain the obviously low Entropy values on MC396 other than the advantages on overlapping clusters. Note that when k =2,theperfor-mance is significantly poorer than other choices. This is also because at k =2, CPM algorithm can only find connected components, which are the most relaxed criterion for clustering.
 Experiment 2. In this experiment, we compare CPM with the other two repre-sentative clustering algorithms, k -means and complete-link. Because it is impos-sible to command CPM to produce exact number of clusters with the benchmark, we use k = 4, at which CPM reaches nearly optima based on Table 2. To make comparisons fair under this condition, we examine both k -means and complete-link twice: one uses the same number of clusters as the benchmark, and the other uses the same number of clusters as CPM, which are denoted by KM-B, KM-C, CL-B, and CL-C (suffixes B and C represent Benchmark and CPM, re-spectively). Furthermore, because k -means is well-known to be sensitive to local optima, we repeat the algorithm 50 times with different initial centroids and av-erage the outcomes achieved. The threshold for complete-link distance measure is set according to the computed values of t c by CPM (see Section 3). This is to align with CPM.

Table 3 shows that CPM performs worse than k -means and complete-link if the standard number of clusters as the benchmark are produced. Because CPM generates far more clusters than the standard, this comparison is some-what unfair to CPM. However, when the number of CPM clusters is used, its advantages can be clearly observed. Under this condition, k -means performs the worst among the three. Its poor performance on MC396 is very obvious because k -means can only produce partitioning of the corpus. Complete-link clusters are non-overlapping MDCs. The results show that CPM outperforms complete-link on all three test sets as well. This testifies the advantages of our method over the typical conventional clustering algorithms in terms of unrestraint cluster number.
 We present a novel clustering algorithm CPM by applying clique percolation technique introduced from the area of biological physics. A more generalized framework related to it is the so-called  X  X mall-world network X  describing many kinds of community structures in nature and society, which is extensively studied in random networks [9]. This is the pioneer work for the CPM being applied in document clustering. The preliminary results demonstrate it is feasible and promising for document clustering. We are confident that CPM is interesting and worth of further studies. There are still many issues left to be studied more deeply. So far, the heuristic relationship between p c , N and t c has not been well studied. To generate an appropriate random graph, an alternative is to make use of the degree distribution of graph vertices. For each vertex, some nearest neighbors associated with the precise degree distribution can be considered. This will lead to the further exploration on techniques to analyze complex networks. Furthermore, due to the NP-hardness of MDC enumeration algorithms, the CPM is time-consuming. Improvements on efficiency are required. In the future, we will also compare CPM to some more advanced clustering algorithms. extract i on . Yang [8] conce i ves the database as be i ng constructed w i th attr i bute-value pa i rs, and table mining as a reverse process of table publ i sh i ng from a database . That form and extracts i nformat i on based on that form .
 necessary . As i s well known, HTML does not d i st i ngu i sh between presentat i on and table but also for construct i ng HTML-document layouts . Our prev i ous work  X  s ma i n mean i ngful tables, we set 24 features wh i ch i nteract i n def i n i ng the mean i ngfulness of a table; we bu i lt a separat i on model that ut i l i zed, w i th those features, a mach i ne-learn i ng algor i thm .

If mean i ngful tables are extracted from raw HTML documents, we can extract those tags do not express the web-table  X  s semant i c structure . However, when we conce i ve the semantic structure of a table as a table-schema , then we can declare that the data i n that table i s organ i zed by a table-schema .

The analys i s of the relat i on between a table-schema and table data corresponds reorgan i z i ng the head i n order that the reorgan i zat i on reflects the semant i c form based on the table-schema thus extracted . The relat i on between a table-schema and data corresponds to that of database schema and records, or that of ontology and tr i ples i n the semant i c web .
 recent stud i es undertaken to develop i nformat i on extract i on from web-tables . Sect i on 3 descr i bes the method of extract i ng the head . Sect i on 4 descr i bes Sect i on 5 i llustrates the exper i ments and, f i nally, conclud i ng comments follow i n Sect i on 6 . spec i f i c research and (2) doma i n-i ndependent research . As far as we are aware, most research on table m i n i ng has focused on extract i ng the mean i ngful table, and i nformat i on extract i on from web tables has been treated as a s i de i ssue .
Doma i n-spec i f i c research i s based on wrapper i nduct i on [4], wh i ch performs form, the researchers exper i enced d i ff i culty i n cop i ng w i th the var i ous web-document formats .

We can f i nd, among doma i n-i ndependent approaches, i nformat i on extract i on patterns and extract i on rules . However, Yang  X  s method can hardly cope w i th new tables that conta i n unknown words . We need to repet i t i vely and manually update i ndependence . i nformat i on and avo i ded doma i n-spec i f i c i nformat i on .

The a i m of i nformat i on extract i on from web-tables i s to establ i sh mach i ne-understandable i nformat i on, and th i s can be ach i eved by convert i ng a table to a table-schema and a tr i ple . Because a table head abstracts related data i n the body, the head can be a strong cand i date for the table-schema . The tr i ple can be extracted us i ng th i s table-schema and body elements . Therefore, we should separate the head from the that i s, i nformat i on extract i on .
 Once mean i ngful tables are extracted, we can rather eas i ly extract the i r heads . Accord i ngly, our prev i ous work proposed mean i ngful table-and head extract i on methods . [3] Sect i on 3 . 1 summar i zes these methods br i efly . Sect i on 3 . 2 complements the head extract i on method w i th supplementary heur i st i cs . 3.1 Construct i ng Head Extract i on Model heads were i nst i tuted based on two i mportant factors of web-table ed i t i ng : z Spec i f i c techn i ques for separat i ng the head from the data are used i n order that z The row or column related to a head conta i ns repet i t i on, because the head 
Both of these factors concern (1) rows  X  and columns  X  appearance character i st i cs were formulated 1 .

For the construct i on of a head extract i on model (hereafter, HEM), the mean i ngful ment i oned above, the table head can be a row(s) or a column(s) . Therefore, the rows dec i s i on tree class i f i er, C4 . 5 [7] . 3.2 Supplementary Heur i st i cs of Head Extract i on Model However, table-head extract i on can proceed only when cons i der i ng the semant i cs of a whole table and the structure of a table to wh i ch the table semant i cs are related . For table .
Therefore, th i s sect i on complements the HEM reported i n Sect i ons 3 . 1 w i th heur i st i cs constructed by extract i ng general factors from the results of a comparat i ve HEM . Accord i ngly, the follow i ng heur i st i cs are formulated .
No . Heur i st i cs 2 5 6 extracted . 4.1 Extract i ng Semant i c-Core Element from Extracted Head Generally, a table prov i des a semant i c-core element (hereafter, SCE) i n a head, i nterpret that 25 i s W (the number of w i ns) and 4 i s D (the number of defeats), i n th i s case for Chelsea .
 an SCE, as below .

No . Heur i st i cs 2 4 If a head cell i s a spanned cell, i t has a weak poss i b i l i ty of be i ng an SCE . 4.2 Extract i ng Table-Schemata Us i ng SCE and Table Structures element of i nformat i on and reorgan i zes the head by assum i ng the role of the p i vot, as formulated as below .

No . Heur i st i cs 1 If a c m,* i s a head that i s located i n the m-th row, and i f c m,k i s the SCE, the c 2 
No . Heur i st i cs 2 a) c i , j i s the cell that i s located i n the i -th row and the j -th cell . h i erarchy corresponds to a table-schema .
 4.3 Extract i ng Tr i ple object, attribute and value . The elements in a body of a table correspond to object or cr i ter i a are used to d i st i ngu i sh th i s tr i ple .
 Cr i ter i on 1 ob j ect or value .
 Cr i ter i on 2 
F i gure 7 shows tr i ple extract i on from the Table shown i n F i gures 5 and 6, us i ng i ts table schema .
 To test the performance of table-i nformat i on extract i on, we composed our test-data-su i t by randomly select i ng HTML documents from the Internet and from some of Wang  X  s data [7] . No . of mean i ngful tables 964 969 1,933 
In Sect i on 3 . 1, we extracted the head us i ng HEM based on the dec i s i on tree obta i ned 82 . 2% accuracy i n extract i ng the head .
  X  Row-or Column-based performance class Head 389 2,237 Head 0 . 872 0 . 852 0 . 862  X  Table-based performance heur i st i cs . Table 8 shows the accuracy of such table-schema extract i on . and tr i ples based on def i n i ng the relat i onsh i ps between the table components . Therefore, we suggested a method for extract i ng table-schemata based on table for ontology, and to other areas .
 Acknowledgements. Th i s work was supported by the Reg i onal Research Centers Program(Research Center for Log i st i cs Informat i on Technology), granted by the Korean M i n i stry of Educat i on &amp; Human Resources Development .
 With the dramatic increase in the amount of textual information available in the Web and digital archives, there has been growing interest in massive text infor-mation processing and management. Document clustering is a convenient way and often the first step to help people organize and extract valuable informa-tion effectively and efficiently from various available information resources. As a key research topic in the machine learning research field, document clustering belongs to unsupervised learning. That is, document clustering automatically organize a set of documents into different clusters according to their similarity without an annotated training corpus.
 In literature, there are many works in document clustering [Zamir et al 1998; Steinbach et al 2000] and can be classified into two categories: flat document clustering [Hartigan et al 1979; Kanungo et al 2002], which iteratively orga-nizes a set of documents into flat-distributed clusters, and hierarchical document clustering [Willett 1988; Zhao et al 2002], which iteratively organizes a set of documents into hierarchically connected clusters through a tree structure. This paper will focus on hierarchical document clustering. In general, tree-structured clusters achieved from hierarchical document clustering can be flattened into flat-distributed clusters according to users X  or applications X  demand.

Hierarchical document clustering [Willett 1988; Zhao et al 2002] can be classi-fied into two categories: the top-down approach via splitting and the bottom-up approach via merging.  X  The top-down approach initializes a set of documents as a single big cluster.  X  The bottom-up approach initializes each given document as a singleton clus-
Moreover, hierarchical document clustering can be done via single-link, com-plete link and average link [Schutze 2005]:  X  Single-link hierarchical clustering merges in each step the two clusters with  X  Complete-link hierarchical clustering merges in each step the two clusters  X  Average-link clustering merges in each step the two clusters with the high-
This paper will focus on bottom-up hierarchical document clustering. In com-parison with the top-down approach, the bottom-up approach biases more on local information and less on global information. This can help find smaller but more similar clusters. However, this also makes it suffer from early merging errors since such early merging errors will be carried forward. Another disadvantage is that it is computationally expensive to compute the similarity between any two clusters, especially when there exist tens of thousands of documents. Finally, only two clusters are normally merged in each iterative process.

In order to resolve above problems in bottom-up hierarchical document clus-tering, much research has been done. Wei et al (2005) combined flat document clustering with hierarchical document clustering. They first applied k-means clustering to organize given documents into many small clusters and then used bottom-up hierarchical document clustering to iteratively merge them into a hi-erarchical tree structure. Zhen (2006) merged and split clusters in a dynamic way to resolve the early errors in top-down hierarchical document clustering, which is also applicable to bottom-up hierarchical document clustering. Wu et al (2004) proposed an efficient algorithm to largely reduce computational time and memory requirement. This is done by automatically detecting possible over-laps among different clusters in different stages.

This paper tackles above problems by proposing a novel bottom-up hier-archical document clustering algorithm. This is done via a concept of  X  X NN-connectedness X , which measures the mutual connectedness of clusters in kNNs, andakNNconnectiongraph,whichorgani zes given clusters into several sets of kNN-connected clusters. In such a graph, a connection between any two clus-ters only exists in the kNN-connected clusters of the same set. Moreover, a new kNN-based attraction function is proposed to measure the similarity be-tween two clusters and indicates the potential probability of the two clusters being merged. The attraction function only considers the relative distribution of their nearest neighbors between two clu sters in a vector space while other cri-teria, such as the well-known cluster-bas ed cosine similarity function, measures the absolute distance between two clusters. This makes the attraction function effectively apply to the cases where different clusters may have very different distance variation. In each step, a kNN connection graph, consisting of several sets of kNN-connected clusters, is first constructed from the given clusters using a kNN algorithm and the concept of  X  X NN-connectedness X . For each set of kNN-connected clusters, the attraction degree between any two clusters is calculated and several top connected cluster pairs will be merged. In this way, the iteration number can be largely reduced. As a result, the clustering process can be much speeded and better performance can be achieved through the new kNN-based attraction function.

The rest of this paper is organized as follows. Section 2 describes the kNN connection graph while the bottom-up hierarchical document clustering algo-rithm is presented in Section 3. Finally, we evaluate our algorithm in Section 4 and conclude this paper in Section 5. The bottom-up hierarchical document clustering algorithm proposed in this pa-per is based on a kNN connection graph, which is constructed in each iterative process and consists of two parts: a set of notes, each of which represents a document cluster, and a set of connections, each of which represents the kNN-based attraction degree between the corresponding two clusters. A connection between two clusters only exists when they are  X  X NN-connected X  via a kNN propagation sequence in the kNN connection graph and its attraction degree is computed using the kNNs of the two clusters.

In a kNN connection graph, two clusters c i and c j is defined as  X  X irectly kNN-connected X  if c j  X  kNN ( c i )and c i  X  kNN ( c j ), and defined as  X  X NN-connected X  if there exist a sequence of clusters c k ( c k n ,c j ) are all  X  X irectly kNN-connected X . Here, kNN ( c i ) indicates the k nearest neighbors (including c i itself) for a given cluster c i in the kNN algorithm where a cluster is represented by the average center of all the documents included in the cluster and the cosine similarity function is applied to measure the similarity between any two clusters. Obviously, the  X  X NN-connectedness X  among clusters are symmetric and transitive. Using the concept of  X  X NN-connectedness X , all the clusters in a given kNN connection graph can be organized into several sets of kNN-connected clusters.

Given above definitions, the kNN-based attraction degree between any two clusters c i and c j is computed by the following function, with the denominator calculated over the union set of their kNNs and the numerator calculated over the cross set of their kNNs: The above formula for the attraction function measures the shared clusters be-tween the two sets of kNNs for the two clusters, weighted by their respective similarities with the two clusters. Similar to the kNN algorithm, a cluster is rep-resented by the average center of all the documents included in the cluster and the cosine similarity function is applied to calculate the similarity sim ( c i ,c j ) in the above formula. Obviously, the attraction function-based hierarchical doc-ument clustering belongs to the average-link category. The intuition behind is that, the more shared between the two kNNs of two clusters, the more attractive of the two clusters and the more probable of their merging. Especially, they have the attraction degree of 1 if the two clusters have the same set of kNN, and 0 if they share no nearest neighbors. The most appealing characteristics of this new kNN-based attraction function is that it only considers the relative distri-bution of their nearest neighbors between two clusters in a vector space while other criteria, such as the well-known clu ster-based cosine similarity function, measures the absolute distance between two clusters. This makes the attraction function apply to the cases where different clusters may have very different dis-tance variation. Another advantage is that it not only considers the similarity between the given two clusters but also takes into account the effect of their respective kNNs. Figure 1 shows the overall algorithm of our KNN connection graph-based bottom-up hierarchical document clustering algorithm. Given a set of docu-ments, it first treats each document as a singleton cluster, and then succes-sively merges clusters until all documents have been merged into a single remaining cluster. Normally a hierarchical document clustering algorithm merges two clusters according to their similarity in each iterative process. One advan-tage of our algorithm is that it can merge more clusters in each iterative process. This is done by propagating kNN-connected clusters using the kNN connection graph.

In each iterative process, given a set of clusters, a kNN connection graph is first generated as follows:  X  Determine kNN ( c i ) for each cluster c i in the set of given clusters. In this  X  Determine kNNDC ( c i ) the set of directly kNN-connected clusters for each  X  Determine kNNC ( c i ) the set of kNN-connected clusters for each cluster c i by  X  Compute the attraction degree between each pair of c i and c j in each kNNC
Then, using the kNN connection graph constructed as above, the given clus-ters are merged into a new set of clusters. Figure 2 shows the algorithm for the merging process. Since | kNNC | thenumberofclustersinakNNCmaybequite different for different kNNCs, for each kNNC, | kNNC | /N top ranked pairs are retrieved according to their kNN-based a ttraction degrees without cluster over-lapping 1 and the two clusters in each of the retrieved pairs are merged into a bigger cluster. Here, N controls the merging rate. Obviously, at most 2/N of clus-ters can be merged pairwisely in each iterative process. In this way, the number of the iterative processes can be largely reduced. The iteration stops until there is only one cluster left.

Similar with other bottom-up hierarchical document clustering algorithms, this algorithm also suffers from the computational problem since it needs to cal-culate the similarity between each pair of clusters in determining nearest neigh-bors in the kNN algorithm and the attraction function between two clusters in each set of kNN-connected clusters. Normally, there are tens of thousands of documents to be clustered. This makes our algorithm computationally demand-ing at early stages. In order to resolve t his problem, we can combine it with flat document clustering as described in Wei et al (2005) by first applying a flat docu-ment clustering algorithm (e.g. k-means clustering) to organize given documents into a certain number of basic clusters, such as k . And if the value of k is n/m , where n is the number of the total documents, then the hierarchical document algorithm X  X  complexity will decrease from O ( n 2 log n )to O ( 1 m 2 n 2 log n ).
The major advantage of our algorithm is that the new kNN-based attrac-tion function only considers the relative distribution to their nearest neighbors between two clusters in a vector space wh ile other criteria, such as the cosine similarity, measures the absolute distance between two clusters. This makes our algorithm apply to the cases where different clusters may have very different distance variation. Moreove, the attraction function not only considers the sim-ilarity between the given two clusters but also takes into account the effect of their respective kNNs. Another advantage is that our algorithm can merge many similar clusters in each iterative process. This can largely reduce the iteration number and thus much speed up the overall clustering process. The third ad-vantage is that the final clustering output does not depend on the processing sequence of given documents. This is due to that our algorithm is based on the kNN connection graph, which captures the kNN-connectedness among clus-ters. This makes our approach stable, which only depends on the set of given documents, k(the number of nearest neighbors in kNN) and the number N in controlling the number of top ranked clusters to be merged in each step. In this paper, they are set to 5 and 4 respectively. The kNN connection graph-based hierarchical document clustering is evalu-ated, using precision, recall and F-measu re, against the FudanUniv news corpus. This corpus contains 2816 documents and has been manually classified into 10 classes: education, sports, military, arts, politics, transportation, environment, computer, economics and medicine.

There exists one problem when assessing the agreement between the cluster-ing output and a manually annotated corpus since there is no corresponding class label for each cluster in the clustering output. To resolve the problem, we construct one (a cluster in the clustering output) to one (a class in the annotated corpus) mapping by using a contingency table T, where each entry t ij gives the number of instances that belong to both the i -th estimated cluster and j -th ground truth class. Moreover, to ensure that any two clusters do not share the same class label, we adopt a permutation procedure to find a one-to-one map-ping from the ground truth classes TC to the estimated clustering output EC using the hierarchical cluster output without cluster overlapping. In this way, the one-to-one mapping can be performed, which can be formulated as the function  X  =argmax with the j -th class.

Furthermore, we adopt several evaluation measurements as applied in [Ham-mouda et al 2004] for document clustering:  X  Precision of a class, which measures the ratio of members of a cluster in the  X  Recall of a class, which measures the ratio of members of the associated class  X  F-measure of a class, which integrates the precision and recall:  X  Overall Precison/Recall/F-measure, which, for the clustering result C ,mea-
Finally, all the documents are represented using the vector space model with each feature in a vector of a document representing a word in the document. Since there are no separators between Chinese words, the ICTCLAS Chinese word segmentation system [Zhang et al 2003] is applied to segment a Chinese sentence into a sequence of words. The weight of each word in a document is normalized using the standard tf.idf formula.

Table 1 shows the performance of our algorithm for each class on the Fuda-nUniv news corpus using the kNN-based attraction function. It shows that our approach achieves the F-measure of 75.32. It also compares the attraction func-tion with the general cluster-based (cosine) similarity function. It shows that the kNN-based attraction function performs better than the general cluster-based cosine similarity function.

Table 2 shows the effect of applying k-means clustering at the initial stage by first clustering given documents into 500 classes and then applying the proposed algorithm in this paper. It shows that that our algorithm achieves the F-measure of 72.75 using the kNN-based attraction function. It also shows that the kNN-based attraction function much outperforms the general similarity function by 2.93 in F-measure.

Comparing Table 1 and Table 2, we can find that, although initial k-means clustering can largely speedup the whole clustering process, it much reduces the performance. It also shows that the kNN-based attraction function is much more robust to the initial merging errors caused by k-means clustering. It is also interesting to note that, with initial k-means clustering, the kNN-based attrac-tion function slightly increases the recall and significantly decrease the precision due to its aggressive merging strategy while the cluster-based similarity func-tion much decrease both the recall and the precision. Finally, Table 3 compares our algorithm with the famous k-means clustering algorithm [Kunungo et al 2002]. For comparison, the cluster number in the k-means clustering algorithm is set to the true cluster number of the FudanUniv news corpus. It shows that k-means clustering achieves the F-measure of 68.20. Comparing Table 1 and Ta-ble 3, we can find that our kNN connection graph-based hierarchical clustering significantly outperforms k-means clustering by about 7 in F-measure. This paper proposes a novel bottom-up hierarchical document clustering algo-rithm. Through a KNN connect graph, this algorithm can simultaneously merge several clusters through capturing the kNN-connectedness among clusters. In this way, it can largely reduce the iteration number and much speed up the clustering process. Moreover, a novel kNN-based attraction function is proposed to determine the merging of two clusters. It only considers the relative distribu-tion to their nearest neighbors between tw o clusters in a vector space while other criteria, such as the cosine similarity, measures the absolute distance between two clusters. This makes our algorithm apply to the cases where different clus-ters may have very different distance variation. Moreover, the attraction function not only considers the similarity between the given two clusters but also takes into account the effect of their respective kNNs. Evaluation on a news document corpus shows that our algorithm performs better than the widely used k-means clustering algorithm. It also shows that the new kNN-based attraction function can better measure the similarity between two clusters and is more robust to the early merging errors than the general cluster-based similarity function.
In the future work, we will explore our algorithm and the kNN-based attrac-tion function in more corpora with a large number of documents, e.g. the Reuters corpus.
 Automated mult i -document summar i zat i on has drawn much attent i on i n recent years . Mult i -document summary i s usually used to prov i de conc i se top i c descr i pt i on about a cluster of documents and fac i l i tate the users to browse the document cluster . For ex-NewsBlaster (http : //www1 . cs . columb i a . edu/nlp/newsblaster/), have been developed to group news art i cles i nto news top i cs, and then produce a short summary for each look at the short summary .

A part i cular challenge for mult i -document summar i zat i on i s that the document set summar i zat i on methods to analyze the i nformat i on stored i n d i fferent documents and summary i s expected to preserve the globally i mportant i nformat i on conta i ned i n the documents as much as poss i ble, and at the same t i me keep the i nformat i on as novel as poss i ble . In recent years, mult i -document summar i zat i on has been w i dely explored i n top i c sess i ons i n ACL, COLING, and SIGIR have advanced the technology and methods have been w i dely explored by [3, 5, 6, 8, 13, 14] .

In recent years, graph-based methods [4, 10, 11] have been proposed for to PageRank [2] and HITS [7] . However, all the methods have not d i fferent i ated d i f-t i onsh i p as a separate  X  X odal i ty X  and comput i ng sentence i nformat i on r i chness based on each  X  X odal i ty X  . Also, the approach appl i es a d i vers i ty penalty process to capture the novelty of a sentence . Exper i ments on DUC 2002 and DUC 2004 data are per-formed and we f i nd that the cross-document relat i onsh i ps between sentences are very i mportant for mult i -document summar i zat i on . The system based only on the cross-document relat i onsh i ps can always perform better than or at least as well as the sys-tems based on both the cross-document relat i onsh i ps and the w i th i n-document rela-t i onsh i ps between sentences .
 summar i zat i on approaches are presented i n Sect i on 2 . The exper i ments and results are g i ven i n Sect i on 3 . Lastly, we conclude our paper i n Sect i on 4 . The proposed approach i s an extens i on of prev i ous graph-based summar i zat i on meth-ment set respect i vely; (2) the i nformat i on r i chness of the sentences i s computed based tences w i th h i gh aff i n i ty rank scores are chosen to produce the summary . 2.1 Aff i n i ty Graph Bu i ld i ng term t i n the correspond i ng sentence and isf t i s the i nverse sentence frequency of term i  X  j . Then M i s normal i zed to M t i vely normal i zed to ra int M 2.2 Informat i on R i chness Computat i on be formulated i n a recurs i ve form as follows : i s ach i eved when the d i fference between the i nformat i on r i chness scores computed at th i s study) .

S i m i larly, the i nformat i on r i chness score for a sentence s i can be deduced based on G l i near comb i nat i on of InfoRich intra (s i ) and InfoRich inter (s i ) as follows : f i nal i nformat i on r i chness of sentences from the cross-document relat i onsh i ps and the equally i mportant .

Note that all prev i ous graph based summar i zat i on methods have InfoRich(s i ) = In-2.3 D i vers i ty Penalty Impos i t i on aff i n i ty rank scores of sentences as follows : h i ghest aff i n i ty rank scores are chosen to produce the summary accord i ng to the sum-mary length l i m i t . 3.1 Exper i mental Setup 2001, DUC 2002 and DUC 2004 ( i. e . task 2 i n DUC 2001, task 2 i n DUC 2002 and task 2004 data as test sets i n our exper i ments . In DUC 2002, 59 TREC document sets (D088 i s excluded from the or i g i nal 60 document sets by NIST) of approx i mately 10 documents each were prov i ded and gener i c abstracts of each document set w i th lengths of approx i -lengths of 665 bytes or less i s requ i red to be created . Note that the TDT top i c would not removed and Porter  X  s stemmer [12] was used for word stemm i ng .
 count i ng overlapp i ng un i ts such as the n-gram, word sequences and word pa i rs between the cand i date summary and the reference summary . ROUGE toolk i t reports separate scores for 1, 2, 3 and 4-gram, and also for longest common subsequence co-occurrences . Among these d i fferent scores, un i gram-based ROUGE score (ROUGE-1) has been shown to agree w i th human j udgement most (L i n and Hovy, 2003) . We show three ROUGE metr i cs i n the exper i mental results : ROUGE-1 (un i gram-based), ROUGE-2 (b i gram-based), and ROUGE-W (based on we i ghted longest common subsequence,  X -b X  opt i on i n ROUGE toolk i t and we also use the  X -m X  opt i on for word stemm i ng . 3.2 Exper i mental Results DUC 2002 and task 2 of DUC 2004 respect i vely . The top three systems are the sys-tems w i th h i ghest ROUGE scores, chosen from those perform i ng systems i n the tasks of DUC 2002 and DUC 2004 respect i vely . The lead basel i ne and coverage basel i ne are two basel i nes employed i n the mult i -document summar i zat i on tasks of DUC . The system computes the i nformat i on r i chness of a sentence based only on the 2) Intra-L i nk : The system compute the i nformat i on r i chness of a sentence based only the w i th i n-document relat i onsh i ps between sentences respect i vely, and then comb i nes i nformat i on r i chness of a sentence based on the whole aff i n i ty graph w i thout d i fferen-prev i ous graph based summar i zat i on methods, i. e . InfoRich(s i ) = InfoRich all (s i ) .
Tables 1 and 2 show the compar i son results on DUC 2002 and DUC 2004, respec-t i vely . The factor  X  i s tuned on DUC 2001 and set to 8 . We can see from the tables much outperform the top perform i ng systems and basel i ne systems . Among the four sh i ps between sentences ( i. e .  X  X nter-L i nk X ) performs best on both DUC 2002 and above observat i ons demonstrate the great i mportance of the cross-document relat i on-sh i ps between sentences for mult i -document summar i zat i on .

F i gures 1 to 4 show the compar i son results of the four aff i n i ty graph based systems under d i fferent values of the penalty degree factor  X  . Here, the ROUGE-W compar i -son results are om i tted due to page l i m i t . Seen from the f i gures, the systems cons i der-system based only on the w i th i n-document relat i onsh i ps between sentences ( i. e .  X  X n-cross-document relat i onsh i ps between sentences, the system based only on the cross-document relat i onsh i ps betweens sentences for mult i -document summar i zat i on .
In order to i nvest i gate how the relat i ve contr i but i ons from the cross-document rela-summar i zat i on performance, F i gures 5-6 show the performances of the  X  X n i on-L i nk X  system under d i fferent values of the we i ght i ng parameter  X  . The ROUGE-1 and the systems decrease w i th the i ncrease of  X  on both DUC 2002 and DUC 2004, wh i ch demonstrates that the less relat i ve contr i but i ons are g i ven to the cross-document rela-t i onsh i ps between sentences, the worse the system performance i s . The cross-document relat i onsh i ps between sentences are much more i mportant than the w i th i n-document relat i onsh i ps between sentences .

The exper i mental results demonstrate the great i mportance of the cross-document expla i ned by the essence of mult i -document summar i zat i on . The a i m of mult i -document summar i zat i on i s to extract i mportant i nformat i on from the whole docu-ment set, i n other words, the i nformat i on i n the summary should be globally i mportant recommendat i ons of ne i ghbors i n other documents are more i mportant than the votes or recommendat i ons of ne i ghbors i n the same document . great i mportance of the cross-document relat i onsh i ps between sentences . The system can ach i eve best performance even based only on the cross-document relat i onsh i ps between sentences . Though the exper i ments were performed on Engl i sh data sets, i t i s character i st i cs of the spec i f i c language .

We w i ll further i nvest i gate the i mportance of the cross-document relat i onsh i ps be-tween sentences i n the task of top i c-focused mult i -document summar i zat i on i n future work .
 Common i nstruct i ons used to recogn i ze Ch i nese characters could be class i f i ed as two read i ng the full text of an art i cle . Whenever a student understands the content of the art i cle, s/he recogn i zes the overall mean i ng of each sentence and then come to under-stand each character . A teacher makes students understand Ch i nese characters ma i nly ter-central i zed i nstruct i on deploys the method to class i fy Ch i nese characters w i th the memor i ze characters and to learn the words . Through Ch i nese Stem-Der i v i ng Instruc-or a component that i s der i ved from the character-central i zed i nstruct i on . However, to learn through CSDI, a student needs to recogn i ze few bas i c Ch i nese characters [1] .
The CSDI has also been proved very effect i ve for students w i th read i ng d i sab i l i t i es to recogn i ze Ch i nese characters [1] . Prev i ous stud i es on CSDI have also showed that short-term memory through the CSDI [2] .
 posed computer-ass i sted appl i cat i on of the Computer-Ass i sted Ch i nese Stem-Der i v i ng Instruct i on (C-CSDI) was as good as i nstruct i ng students by the Ch i nese Stem-Der i ved son-t i me can be reduced by adopt i ng the computer-ass i sted i nstruct i on [4] . Although a through the T-CSDI, one teacher would not be able to take care of many students w i th through a computer i zed i nstruct i onal tool i n the classroom [5] . In th i s study, we pro-pose a teach i ng strategy : the Teacher Involved Computer-Ass i sted Ch i nese d i scuss the human i nteract i on des i gn, and prov i de suggest i ons and teach i ng methods for further stud i es . 2.1 People w i th Read i ng D i sab i l i t i es and Ch i nese Stem-Der i v i ng Instruct i on understand the art i cle because they are busy i n recogn i z i ng unfam i l i ar words and may not be able to understand the mean i ng and context of the art i cle well [7] . The Ch i nese one of the character-central i zed i nstruct i ons, can be used to help people w i th read i ng d i sab i l i t i es recogn i ze Ch i nese characters .
 ample,  X   X  i s composed of  X   X  and  X   X  . The Ch i nese character  X   X  i s a stem rad i cal or a component, can be used to dr i ve students to recogn i ze unfam i l i ar characters v i sual form, art i culat i on, and word mean i ngs of Ch i nese characters, wh i ch can be de-r i ved from one stem character comb i ned w i th rad i cal characters . 2.2 Computer-Ass i sted Instruct i on Computer-Ass i sted Instruct i on (CAI) i s used as an i nstruct i onal tool adopted to i m-over, learn i ng mot i vat i on can be ma i nta i ned by the CAI effect i veness, espec i ally for elementary students [1] . CAI can be a means of i nstruct i on for teachers to apply an effect i ve teach i ng strategy by us i ng computers, and us i ng t i me and other resources w i sely [5] .
 However, the software affects des i gners, tra i ners (or teachers) and users (or students) i n the software-develop i ng process) [8] . appl i cat i on works to i nstruct students when the teacher i s i nvolved [3] . In Ta i wan, although students start learn i ng Ch i nese characters at the f i rst grade, CSDI cannot be used for i nstruct i ng students at th i s grade level . The students must reach a normally only recogn i ze a few characters and therefore would not be qual i f i ed to be the cat i on must teach students for at least one semester i n order to d i agnose the student w i th students from pr i mary school w i th learn i ng d i sab i l i t i es as the part i c i pants .
The s i ngle sub j ect research shows the effect i veness of an i nstruct i on can be proved through three successful cases [4] . Therefore, to understand the effect i veness of three analys i s . Th i s work adopts s i ngle sub j ect research method and selects three part i c i pants from a the follow i ng : Table 1 shows the background i nformat i on of the three student sub j ects i n th i s study . Th i s study adopts three i nstruct i on approaches : T-CSDI, C-CSDI, and TC-CSDI, to number one and number two i nd i cate a student must be i nstructed through T-CSDI, number three and number four through C-CSDI, and others through TC-CSDI . A teacher teaches each part i c i pant one sess i on a day . Age 
Grade The Wechsler Intell i gence Scale for Ch i ldren-III Full score 96 79 105 The Grade Ch i nese Characters Read i ng Raw Test /Percent i le 
Rank of computer i nstruct i on from the p i lot study . The flow theory shows whenever a person pants and the phase of character game i n order to re i nforce the memory of i nstructed Ch i nese characters .

F i g . 2 shows the proposed des i gn of the TC-CSDI . The dotted arrow  X  X ss i st X  means a teacher does not i nterrupt the teach i ng process through computer and only ass i sts users and rad i cals, and answer i ng students  X  quest i ons when the teacher adopts TC-CSDI i n i n TC-CSDI .
 Table 2 shows the selected sets of Ch i nese characters . The learn i ng effect i veness of each teach i ng strategy, T-CSDI, C-CSDI, and TC-CSDI, Table 3 presents the average result of ten-t i me i nstruct i on w i th T-CSDI, C-CSDI, and TC-CSDI .
 than C-CSDI, wh i ch i n turn i s better than T-CSDI . Whereas, a prev i ous study shows exper i mental results of prev i ous study and th i s study are d i fferent .
Table 3 shows that part i c i pant B performs worst among three part i c i pants on i ntel-l i gence quot i ents and percent i le of the result of the graded Ch i nese Characters Read i ng t i mes of standard dev i at i on of C-CSDI) from adopt i ng C-CSDI to adopt i ng TC-CSDI . C-CSDI) from adopt i ng C-CSDI to adopt i ng TC-CSDI . Hence, the r i s i ng trend seems whenever a teacher ass i sts the poorer reader, the i mprovement i s greater than a better reader .
 analys i s shows the results of adopt i ng TC-CSDI are w i th i n top 20% and the results of adopt i ng T-CSDI grows up stably . The var i at i on of the results of C-CSDI i s large . On The f i fth test through C-CSDI shows a d i fferent result from the ones of other two i n-by computer games he j ust had played and the C-CSDI was the f i rst adopted i nstruct i on at that t i me .
 TC-CSDI are shown 100% s i nce the f i fth test . The var i at i on of the results of C-CSDI i s large . The results of T-CSDI descend from 100% to almost 40% at the s i xth test, be-cause the part i c i pant i s eager to be i nstructed by C-CSDI or TC-CSDI at that t i me . In average, the teacher helps part i c i pant B seven t i mes dur i ng the process of TC-CSDI . T-CSDI, C-CSDI and TC-CSDI are s i m i lar . However, the results of TC-CSDI are of TC-CSDI .
 CSDI than us i ng T-CSDI and C-CSDI . d i fference i n effect i veness of three i nstruct i on methods .

Th i s study proposes a novel i nstruct i on, Teacher-Involved Computer-Ass i sted Ch i -However, the result of adopt i ng TC-CSDI to teach part i c i pant B i s better than to teach the process of i nstruct i ng students w i th TC-CSDI .
 teacher . However, we cannot draw a general conclus i on that i s stat i st i cally sound due to the small sample s i ze that we have . Therefore, we recommend that follow i ng : 1) enlarge the sample s i ze to test and val i date the effect i veness of TC-CSDI . study . 3) Further study would be needed to determ i ne i f TC-CSDI could be feas i ble for people whose nat i ve language i s not Ch i nese .
 some effect i ve content-based text process i ng techn i ques for handl i ng these huge superv i sed learn i ng algor i thms have demonstrated remarkable performance for text Entropy, na X ve Bayes models .

A common problem of text class i f i cat i on i s h i gh-d i mens i onal sparse feature space of document representat i on wh i ch i s formed us i ng bag-of-words model . To solve the problem, there are two common effect i ve ways, namely feature select i on and feature extract i on . The term feature select i on refers to algor i thms that output a subset of the words i n the vocabulary as features, some feature select i on methods such as document also prov i de a better class i f i cat i on accuracy due to f i n i te sample s i ze effects[7] . classes, to obta i n correct rank among all compet i ng classes, one way i s to seek some effect i ve techn i ques that enlarge separat i on between the correct class and other system .
 shown i n sect i on 6 . At last, we address conclus i ons and future work i n sect i on 7 . In recent years Na X ve Bayes(NB) approaches have been appl i ed for text class i f i cat i on, wh i ch make  X  X a X ve Bayes assumpt i on X , called mult i -var i ate Bernoull i model and NB model usually performs better than the mult i -var i ate Bernoull i NB model . In th i s mult i nom i al NB model br i efly here s i nce full deta i ls have been presented i n paper [5] . so the most l i kely class c* for a document d i could be computed as P(w t |c j ) thus represents the probab i l i ty that a randomly drawn word from a randomly est i mates for these parameters from a set of labeled tra i n i ng data .
Document frequency threshold i ng(DF) and mutual i nformat i on(MI) are two i s done by us i ng DF and MI measures, respect i vely, compar i ng w i th our d i vergence-based measure . Deta i ls about DF and MI measures are d i scussed i n the yang  X  s paper[6] . Here we only br i efly i ntroduce them as follows .
 performance . In DF-based feature select i on procedure, a feature would be d i scarded i f i ts document frequency was less than a predeterm i ned threshold .

Suppose that let t be a term and c be a class, A be the number of t i mes t and c co-between t and c could be est i mated us i ng 
In compar i son exper i ments, to perform a global feature select i on, we use MI max measure as follows : In general, a text class i f i cat i on system uses d i rect rank order i ng method to ass i gn a correct class to i nput text i n wh i ch all compet i ng classes are ordered decreas i ngly . ach i eved i f the correct rank order i ng can be obta i ned . Unfortunately, the metr i c for Bayes models, and the parameters of the class i f i cat i on model are est i mated from a g i ven tra i n i ng data[8] .
 enlarge the i nterclass d i stance, and reduce the i ntra-class var i ance so that a max i mum based feature select i on approaches wh i ch we focus on i n th i s paper . In th i s sect i on, we present the concept of d i vergence wh i ch i s a measure of d i ss i m i lar i ty between two classes, and could be used to determ i ne feature rank i ng and p for class C i i s g i ven by[8] V th i s paper, could be calculated by[9] where tr( A ) i s the trace of matr i x A . W i thout any loss of general i ty, we def i ne the problem of feature select i on procedure feature subset . So the best subset X *  X  ) of s i ze d could be formed by 
Now we see that an exhaust i ve search i ng procedure i s requ i red to produce the costly and completely computat i ons for each poss i ble subset, poss i bly turn i ng feature subset of any s i ze w i thout the need for an exhaust i ve search[10] .
In the paper[11], Schne i der proposed a feature select i on score for text class i f i cat i on based on the KL-d i vergence between the d i str i but i on of words i n tra i n i ng documents an approx i mat i on est i mat i on method was adopted under two assumpt i ons : the number of occurrences of a feature i s the same i n all documents that conta i n the feature, and cond i t i ons are always not sat i sf i ed well .
 wh i ch w i ll lead to a large d i vergence are more i mportant ones, s i nce they carry more the total d i vergence may be d i scarded i n the procedure of feature select i on .
To i mplement the feature select i on, we adapt  X  X ax i m i n X  algor i thm wh i ch was i ntroduced i n our algor i thm shown i n table 1 .
 6.1 Compar i son Exper i ment on Newsgroups The f i rst data set i s the Newsgroups set wh i ch conta i ns approx i mately 20,000 newsgroup documents . It i s part i t i oned evenly across 20 d i fferent newsgroups . In the data preprocess i ng procedure, we d i scard the sub j ect l i ne and remove the words that there are 62264 words left . In the exper i ment, we use f i ve tr i als w i th 20% of the data accuracy across tr i als .
F i gure 1 shows results on the Newsgroup data set, and shows for all feature select i on methods NB class i f i cat i on model do best at the max i mum vocabulary s i zes . Our d i vergence-based method outperforms greatly MI measure, and has sl i ght better performance than DF measure . As d i scussed i n the paper[6], a weakness of MI Maybe rare terms w i ll have a h i gher score than common terms . In yang  X  s compar i son comp.os.ms-windows. misc, comp.sys.ibm.pc.hardware, comp.sys. mac.hardware, and between all poss i ble class pa i rs . In our approach, an i mportant feature w i ll be selected have good contr i but i ons to i mprove the class i f i cat i on performance . 6.2 Compar i son Exper i ment on Reuters-21578 The second compar i son exper i mental results are based on the Reuters-21578 w i dely i nclud i ng acq, corn, crude, earn, grain, interest, money-fx, ship, trade, and wheat are removed, and we do not use stemm i ng . The result vocabulary has 23444 words . Results on Reuter-21578 are shown as macro-averaged prec i s i on-recall breakeven F i gure 2 shows that our d i vergence measure outperforms DF and MI measures . Through data analys i s, we f i nd the phenomenon of confus i on class i n Reuter-21578 i s ModApte spl i t, for example, corn, grain, wheat categor i es ev i dently belong to and MI measures . From F i gure 2 we could see a surpr i sed phenomena that DF and MI measures do best at the max i mum vocabulary s i zes; however, d i vergence measure that i n Reuter categor i zat i on tasks, for several of the categor i es h i gh accuracy can be of the category[2][5] . Our results are cons i stent w i th such results . study further Gauss i an M i xture Model(GMM) for text class i f i cat i on . research was supported i n part by the Nat i onal Natural Sc i ence Foundat i on of Ch i na(No . 60473140), 985 pro j ect of Northeastern Un i vers i ty(No . 985-2-DB-C03) and by Program for New Century Excellent Talents i n Un i vers i ty(No . NCET-05-0287) . The ma i n problem i n construct i ng language models i s data sparseness, for many events solve such problem to some extent and fac i l i tate syntact i c pars i ng as well .
There has been an amount of prev i ous work us i ng local d i str i but i onal i nformat i on co-occurrence stat i st i cs of common words, for suff i c i ently frequent words, to well-founded i nformat i on theoret i c model to i nduce large numbers of plaus i ble semant i c and syntact i c clusters .

In th i s paper, we ma i nly focus on evaluat i ng performance of four methods employed on the n-gram class model proposed by Brown [6] , where we employed a b i gram model . funct i on words w i th respect to a g i ven content word to character i ze the content word  X  s word  X  s local context . The last one i s word cluster (WC), i n wh i ch each word  X  s context clusters before and after i t .
 methods, and demonstrates the i mprovement of performance when apply i ng word sect i on comes to a conclus i on . Four cluster i ng methods of Ch i nese words, MMI, FW, HFW and WC, are exam i ned i n th i s paper respect i vely . 2.1 MMI maps a word i w to i ts cluster i c , to reduce the perplex i ty of a n-gram language model . = " can be calculated us i ng (1) . cluster wh i ch the prev i ous word belongs to, then (1) can be rewr i tten as (2) . Further, (2) can be reduced as (3) . cluster i ng . For a b i gram language model, an exchange algor i thm was proposed to i mplement i t, wh i ch works as F i g1 .
 Repeat Unt i l the stopp i ng cond i t i on i s sat i sf i ed .

Suppose all words w i ll be part i t i oned i nto G clusters, the most frequent G-1 words, executed a pre-spec i f i ed number of i terat i ons or no more words are moved . 2.2 FW Table1 .
 calculated accord i ng to the former occurrence frequency i nformat i on i n certa i n w i ndow that e i ther s i de of the content word we cons i der . Take a toy example on the follow i ng sentence NBA  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  . Suppose the w i ndow s i ze i s 4, the occurrence of funct i on word w i th respect to content word can be calculated as Table2 . To sum up, the occurrence frequency of a g i ven funct i on word can be calculated by w i ndow . For the corpus of People's Daily i n January of 1998, the occurrence frequency of i n relat i on to i s i llustrated as F i g2 . of funct i on word w i th respect to content word can be calculated as F i g3 . for each content word should be normal i zed and a part i t i on-based cluster i ng method i s appl i ed to cluster i ng words . 2.3 HFW NEXT&gt; .
 d i str i but i on of two words  X  context, see (4) . Here, V denotes all the words that appear i n local context of word w, p 1 denotes the d i vergence i s adopted as the measurement of the d i stance (5) . Suppose the prev i ous word i s i ndependent of the next one, the d i stance of two words can be def i ned as (6) . 2.4 WC There are two drawbacks for HFW . F i rst, the data i s too sparse to est i mate the context d i str i but i on adequately for any but the most frequent words . Second, the two pos i t i ons of one word  X  s local context are not dependent . Both problems can be solved by means the words g i ven i n the clusters as formula (7) . We wr i te () cw for the cluster word w i s i n, or for brev i ty 1 c for 1 () cw , and so on . and 2 p , Kullback-Le i bler d i stance can be s i mpl i f i ed as (8) . word cluster, we ut i l i zed an i terat i ve algor i thm to obta i n word clusters .
In pract i ce we have a prel i m i nary cluster i ng no very rare words w i ll be i ncluded, and some common words w i ll also not be ass i gned ow i ng to that they are amb i guous or they qual i ty may be depended on the select i on of i n i t i al cand i date . 3.1 Performance Measures and semant i c prec i s i on .
 The automat i c approach to evaluate a g i ven cluster based on POS works as follows :
For each word, lookup i n the corpus and f i nd all the tags that i t has been ass i gned to; words i n the cluster that possess the most common tag over the total number of words i n the cluster and return i t as a percentage .

A Ch i nese thesaurus d i ct i onary  X  X  X  X  X  X  X  (TongY i C i C i L i n) i s employed to the rat i o of the number of the most common semant i c delegate to the total number of words i n the cluster . 3.2 Exper i mental Sett i ngs segmented and POS tagged i n advance . The total number of words i s 1093083 . In FW, d i stance of two words were measured by Manhattan d i stance . The most 1000 frequent words method . In order to evaluate cluster i ng results fa i rly, the same number of words and clusters was pre-spec i f i ed . For each cluster i ng method, 4096 words were clustered w i th i n 120 clusters . 3.3 Exper i mental Results Table3 shows that POS prec i s i on for each method surpasses 50% and semant i c prec i s i on i s above 30% . For MMI, the semant i c prec i s i on even reaches 50% and POS may lead to some wrong cluster i ng result .

Table4 shows some clusters from the MMI  X  s cluster i ng result at random, wh i ch can prec i sely make human name, locat i on name, and other proper nouns, t i me words, and  X  X  X   X  X   X  X  X  X   X  X  X  X   X  X  X  X   X  X  X  X   X  X  X   X  X   X  X  X   X  X  X   X  X   X  X  ... For i nstance, one cluster { , , , , , , , , , ,...} means bund i n f i nance . 3.4 Apply i ng Word Clusters to Ch i nese Syntact i c Pars i ng We have appl i ed al i gnment-based method [11][12] , wh i ch i nduces const i tuent by means i nduce Ch i nese syntact i c structures and got sat i sfactory results . The al i gnment of two i s a l i st (&lt; &gt;,&lt; &gt;,&lt; &gt;) . Nevertheless, and , both clusters, the al i gnments of all sentence pa i rs could be ad j usted and the performance of syntact i c pars i ng may be i mproved .

Table5 g i ves the exper i mental result of al i gnment-based Ch i nese syntact i c pars i ng and that of i ntroduc i ng word cluster i nto al i gnment . FW, MMI, HFW, and WC i nd i cate that us i ng correspond i ng cluster i ng result to ad j ust the al i gnment wh i le NC means no i mproved by means of word clusters, one reason for no drast i c i mprovement i n not tagged by the same standard, and another i s that the word clusters i n our Ch i nese part i t i on words .
 Word cluster i ng i s a means of construct i ng small but effect i ve language models, wh i ch i n some cases could i mprove N-gram language models and syntact i c i nduct i on of cluster i ng, evaluated four d i fferent methods and found that MMI and FB could i dent i fy results and syntact i c pars i ng of Ch i nese and extract translat i on template by means of b i l i ngual word clusters .
 w i th respect to the types of event evoked by the ma i n verb . FrameNet, WordNet are FrameNet covers a w i de range of event types represented i n frames w i th i nd i cat i on of Frame Element (FE), i. e . the Semant i c Roles for the event, Phrase Type (PT) and the i r however, the amount i s relat i vely small compared to many lex i cograph i cal resources between them . We a i m at comb i n i ng the two resources together i n order to prov i de a poss i ble semant i c relat i ons between words . appropr i ate range of poss i ble of semant i c roles . Sentence level knowledge i s based on the FrameNet frame . In order to draw the appropr i ate frame, we would need a lex i cal-semant i c verb class i f i cat i on based on the event type evoked by the verb . Word level knowledge i s from WordNet . 2.1 FrameNet FrameNet[2] i s based on the theory of Frame Semant i cs . A frame represents a sentences i n each frame show the poss i ble semant i c roles for a g i ven target word . We knowledge base . Currently, there are around 3000 verbs attached to 320 d i fferent frames . 2.2 Ch i nese-Engl i sh B i l i ngual Ontolog i cal WordNet The B i l i ngual Ontolog i cal WordNet (BOW) was constructed by the Academ i a of the SUMO ontology . There are more than 12610 verb synsets i n WN 1 . 6 and each synset represents a lex i cal concept shared by the synonyms i ncluded . 2.3 FrameNet WordNet Mapp i ngs Sh i &amp; M i halcea  X  s [7] FrameNet WordNet Verb Mapp i ng (FnWnVerbMap) has mapped the verb covered by FrameNet w i th WordNet . Around 3,600 verb senses from WordNet have been mapped to over 300 frames . The i r work has suggested the work . 2.4 SUMO The Suggested Upper Merged Ontology [12] i s created at Teknowledge Corporat i on . for general purpose terms and acts as the bas i s for construct i ng more spec i f i c doma i n ontolog i es . N i les &amp; Pease [9] mapped WordNet synsets and SUMO concepts . We attempt to i dent i fy the SUMO conceptual i zat i on of a FrameNet frame by a stat i st i cal expand i ng the verb coverage .
 S i nce BOW i s base on WordNet 1 . 6, the equ i valent synsets i n WordNet 2 . 0 must f i rst be establ i shed i n order to comb i ne w i th the FrameNet WordNet Mapp i ng (FnWnVerbMap) wh i ch i s based on WordNet 2 . 0 .
 The FnWnVerbMap i s based on the verb-senses i ncluded i n the lex i cal coverage of of WordNet, many synsets are not mapped w i th any frame . In order to expand the verb coverage of frame, we suggest an automat i c ass i gnment of synsets to the i r correspond i ng frame v i a WordNet synset relat i ons and SUMO conceptual i zat i on . 3.1 Expand i ng FrameNet Coverage w i th Synsets Based on the above mapp i ng, each FrameNet frame corresponds to some synset(s) . W i th these frame-mapped synsets, we may act i vate more synsets wh i ch are related to hypernym, troponym(hyponym), enta i lment and antonym, cause and etc . These related synsets are semant i cally related to the frame br i dged by the mapp i ng . Others, however, may be i nappropr i ate to the frame depend i ng on the scenar i o evoked by the verb .
 In order to appropr i ately i dent i fy su i table synsets, we subsume SUMO concepts to FrameNet frames and take th i s i nformat i on as select i on cr i ter i a among the cand i dates i n order to f i lter out i ncorrect synsets . 3.2 Populat i ng Frames w i th Synsets by SUMO WordNet synsets have been mapped w i th SUMO concepts . Upon the FnWnVerbMap, we can retr i eve a group of SUMO concepts l i nked to the frame v i a the mapped synsets . A var i ety of SUMO concepts w i ll be found because SUMO i s not a concept taxonomy . As some of the SUMO concepts have a h i gher frequency of occurrence, we ass i gn the relevant frames to the i r correspond i ng SUMO concepts based on the degree of prom i nence of the SUMO concepts as determ i ned us i ng a stat i st i cal d i str i but i on approach .
 d i str i but i on approach . 1 . Core SUMO concepts w i th a pos i t i ve standard score greater or equal to 1 i n the 2 . Per i pheral SUMO concepts w i th a pos i t i ve standard score between 0 and 1 i n the 3 . Irrelevant SUMO concepts have an occurrence w i th a negat i ve standard score i n 
Synset cand i dates wh i ch correspond w i th the frame  X  s Core or Per i pheral SUMO concepts are des i gnates as the correspond i ng synsets for the frame . In other words, the both WordNet synsets i nter-relat i ons and SUMO concept mapp i ng between Frames and Synsets . The pseudo-code of the algor i thm used to ass i gn FrameNet semant i c frames to WordNet verb synsets i s descr i bed i n F i gure 4 and i llustrated i n F i gure 5 . 3.3 Evaluat i on on Recall and Prec i s i on The three frames are : STATEMENT , core FE, the Semant i c Labels, i ncludes Speaker, Message and Med i um . GIVING , core FE i ncludes Donor, Rec i p i ent and Theme ATTACK , core FE i ncludes Assa i lant and V i ct i m 
The mapp i ng result i s shown i n Table 1 . We manually ver i f i ed the synset i n the 3 test cases as the golden standard . The result of recru i t i ng appropr i ate synsets based on were recru i ted out of the 91 cand i date synsets (Prec i s i on : 91 . 21%) .
The methodology has a very h i gh accuracy on f i lter i ng out i ncorrect synsets among the cand i dates . In STATEMENT frame, all 58 i ncorrect synsets were f i ltered out; In the frame f i ltered out 26 of the 27 i ncorrect synsets (96 . 29%) .
 By employ i ng the algor i thm, we have successfully expanded the verb coverage of FrameNet frames w i th WordNet synsets . Tak i ng the STATEMENT frame by way of synsets wh i ch means i t y i elds more than 95 new Ch i nese verbs for the verb coverage of the frame .
 However, the recall percentage of these 3 test cases i s not very h i gh . In the STATEMENT frame, only 95 correct synsets out of the 139 correct synsets were recru i ted (68 . 34%), 31 out of 40 correct synsets were recru i ted (77 . 5%) i n the GIVING room for i mprovement i n ass i gn i ng more synsets to FrameNet frames .
The h i gh prec i s i on shows the pos i t i ve potent i al of us i ng SUMO concept as select i on cr i ter i a for mapp i ng WordNet synsets w i th FrameNet frames . The relat i vely conceptual upper ontology and i ts concepts denote world mean i ng; FrameNet frames d i str i but i on of the grammat i cal real i zat i on of the text (Word Mean i ng -Verb) may be groups of SUMO concepts : Impacting (17), Motion(1), Regulatory Process(1) &amp; Shooting (2). It i s noted that the SUMO concept Impacting has a mean i ng s i m i lar to the ATTACK scenar i o but was not determ i ned as the Frame core or per i pheral SUMO . have a mean i ng s i m i lar to the scenar i o construed by the frame .

Hence, i t shows that mapp i ng between concepts from d i fferent knowledge base search i ng among all concepts of the mapp i ng source . The pro j ect of ass i gn i ng BOW Ch i nese synsets to FrameNet i s currently under development . The h i gh accuracy of the Frame X  X ynsets Mapp i ng y i elds a useful correspond i ng FrameNet frame, prov i d i ng a comprehens i ve knowledge base for knowledge, Engl i sh verb lex i cal data from WN 2 . 0 and Ch i nese verb lex i cal data FrameNet, representat i ve Ch i nese example sentence should be i ncluded i n the knowledge base mak i ng i t a useful tool for deal i ng semant i c pars i ng .
Takehito Utsuro 1 , Mitsuhiro Kida 2 , Masatsugu Tonoike 3 , and Satoshi Sato 4 Lexicons of technical terms are one of the most important language resources both for human use and for computational research areas such as information retrieval and natural language processi ng. Among various research issues regard-ing technical terms, full-/semi-automatic compilation of technical term lexicon is one of the central issues. In various resea rch fields, novel technologies are in-vented every year, and related research areas around such novel technologies keep growing. Along with such invention of technologies, novel technical terms are created year by year. Considering such a situation, it requires a huge cost for manually compiling lexicons of technical terms for hundreds of thousands of technical domains. Therefore, it is inevitable to invent a technique of full-/semi-automatic compilation of technical term lexicons for various technical domains.
The whole task of compiling a technical term lexicon can be roughly decom-posed into two sub-processes: (1) collecting candidates of technical terms of a technical domain, and, (2) judging whether each candidate is actually a techni-cal term of the target technical domain. The technique of the first sub-process is closely related to research on automatic term recognition, and has been rela-tively well studied so far (e.g., [5]). On the other hand, the technique of the second sub-process has not been studied well so far. Exceptional cases are works such as [1,2], where their techniques are mainly based on the tendency of technical terms appearing in technical documents of limited domains rather than in documents of daily use such as newspaper and magazine articles. Although the underlying idea of those previous works is very interesting, those works are quite limited in that they require existence of certain amount of technical domain corpus. It is not prac-tical for manually collecting technical domain corpus for hundreds of thousands of technical domains. Therefore, as for the second sub-process here, it is very impor-tant to invent a technique for automatically classifying the domain of a technical term.

Based on this observation, among several key issues regarding the second sub-process above, this paper mainly focuses on the issue of estimating the domain specificity of a term. In this paper, supposing that a target technical term and a technical domain are given, we propose a technique of automatically estimating the specificity of the target term with respect to the target domain. Here, the domain specificity of the term is judged among the following three levels: i) the term mostly appears in the target domain, ii) the term generally appears in the target domain as well as in other domains, iii) the term generally does not appear in the target domain.

The key idea of the proposed technique is as follows. In the proposed tech-nique, we assume that sample technical terms of the target domain are available. Using such sample terms with search engine queries, we first collect a corpus of the target domain from the Web. In a similar way, we also collect sample pages that include the target term from the Web. Then, the similarities of the con-tents of the documents are measured between the corpus of the target domain and each of the sample pages that include the target term. Finally, the domain specificity of the target term is estimated according to the distribution of the domain of those sample pages.

Figure 1 illustrates rough idea of this technique. Among the three example (Japanese) terms, the first term ( impedance characteristic )mostlyappearsin the documents of the  X  X lectric engineering X  domain on the Web. In the case of the second term ( electromagnetism ), about half of sample pages collected from the Web can be regarded as in the  X  X lectric engineering X  domain, while the rest are not. On the other hand, in the case of the last term ( response characteristic ), only a few of the sample pages can be regarded as in the  X  X lectric engineering X  domain. In our technique, such difference of the distribution can be easily identified, and the domain specificities of those three terms are estimated.
As experimental evaluation, we first evaluate the proposed technique of es-timating domain specificity of a term using manually constructed development and evaluation term sets, where we achieved mostly 90% precision/recall (de-tails are presented in [6]). Furthermor e, in this paper, we present the result of applying this technique of estimating domain specificity of a term to the task of discovering novel technical terms that are not included in any of existing lex-icons of technical terms of the domain. Candidates of technical terms are first collected from the Web corpus of the target domain. Then, about 70  X  80 % of those candidates are excluded by roughly judging the domain of their constituent words. Finally, out of randomly selected 1,000 candidates of technical terms per a domain, we discovered about 100  X  200 novel technical terms that are not included in any of existing lexicons of the domain, where we achieved about 75% precision and 80% recall. In this section, we first describe the proposed technique of estimating domain specificity of a term using the Web. 2.1 Outline Here, we estimate the domain specificity of a term t with respect to a domain C , supposing that the term t and the domain C are given. Generally speaking, the coarsest-grained classification of domain s pecificity of a term is binary classifica-tion, namely, the class of terms that are used in a certain technical domain, vs. the class of terms that are not used in a certain technical domain. In this paper, we further classify the degree g ( t, C ) of the domain specificity into the following three levels: g ( t, C )= (When we simply classify domain specificity of a term into two classes with the coarsest-grained binary classification above, we regard those with domain specificity  X + X  or  X   X   X  as those that are used in the domain, and those with domain specificity  X   X   X  as those that are not used in the domain.)
The input and output of the process of domain specificity estimation of a term t with respect to the domain C are given below:
The process of domain specificity estimation of a term is illustrated in Figure 2, where the whole process can be decomposed into two sub-processes: (a) that of constructing the corpus D C of the domain C , and (b) that of estimating the specificity of a term t with respect to the domain C . In the process of domain specificity estimation, the domain of documents including the target term t is es-timated, and the domain specificity of t is judged according to the distribution of the domains of the documents including t . The details of those two sub-processes are described in the followings. 2.2 Constructing the Corpus of the Domain When constructing the corpus D C of the domain C using the set T C of sample terms of the domain C ,first,foreachterm s in the set T C , we collect into a set D s the top 100 pages obtained from search engine queries that include the term s . 1 The search engine queries here are designed so that documents that describe the technical term s are ranked high. When constructing a corpus of the Japanese language, the search engine  X  X oo X  2 is used. The specific queries that are used in this search engine are phrases with topic-marking postpositional particles such as  X  s -toha, X   X  s -toiu, X   X  s -wa, X  and an adnominal phrase  X  s -no, X  and  X  s . X  Then, union of the sets D s for each s is constructed and denoted as D ( T C ):
Finally, in order to exclude noise texts from the set D ( T C ), the documents in the set D ( T C ) are ranked according to the number of sample terms (of the set T
C ) that are included in each document. Through a preliminary experiment, we decided here that it is enough to keep top 500 documents, and regard them as the corpus D C of the domain C . 3 2.3 Domain Specificity Estimation of Technical Terms Given the corpus D C of the domain C , domain specificity of a term t with respect to a domain C is estimated through the following three steps: Step 1. Collecting documents that include the term t from the Web, and con-Step 2. For each document in the set D t , estimating its domain by measuring Step 3. Estimating the domain specificity g ( t, C )of t using the document set Details of those three steps are given below: Collecting Web Documents Including the Target Term. For each target term t , documents that include t are collected from the Web. According to a procedure that is similar to that of constructing the corpus of the domain C described in section 2.2, the top 100 pages obtained with search engine queries are collected into a set D t .
 Domain Estimation of Documents. For each document in the set D t ,its domain is estimated by measuring similarity against the corpus D C of the domain C . Then, given a certain lower bound L of document similarity, documents with large enough similarity values are extracted from D t into the set D t ( C, L )[6]. Domain Specificity Estimation of a Term. The domain specificity of the term t with respect to the domain C is estimated using the document sets D t and D t ( C, L ). Here, this is done by simply calculating the following ratio r L of the numbers of the documents within the two sets: Then, by introducing the two thresholds a (  X  )and a (+) for the ratio r L ,the specificity g ( t, C )of t is estimated with the following three levels: In experimental evaluation of section 4, as in the case of the lower bound L of the document similarity, the two thresholds a (  X  )and a (+) are also determined using the development term set mentioned above. This section illustrates how to apply the technique of domain specificity estima-tion of technical terms to the task of discovering novel technical terms that are not included in any of existing lexicons of technical terms of the domain. First, as shown in Figure 3, from the corpus D C of the domain C , candidates of technical terms are collected. In the case of the Japanese language, as candidates of novel technical terms, we collect compound nouns with frequency counts five or more, consisting of more than one noun. Here, we collect compound nouns which are not included in any of existing lexicons of technical terms of the domain. Then, after excluding terms which do not share constituent nouns against the sample terms of the given set T C , the domain specificity of the remaining terms are automatically estimated. Finally, we regard terms with domain specificity  X + X  or  X   X   X  as those that are used in the domain, and collect them into the set T web,C . We evaluate the proposed method with five sample domains, namely,  X  X lectric engineering X ,  X  X ptics X ,  X  X erospace engineering X ,  X  X ucleonics X  ,and  X  X stronomy X  . For each domain C of those five domains, the set T C of sample (Japanese) terms is constructed by randomly selecting 100 terms 4 from an existing (Japanese) lexicon of technical terms for human use. We evaluate the results of discovering novel technical terms that are not included in any of existing lexicons of technical terms of the domain. First, Table 1 compares the numbers of candidates of novel technical terms collected from the Web, with those after excluding terms which do not share constituent nouns against the sample terms of the given set T C .As shown in the table, about 70  X  80 % of the candidates are excluded, while the rate of technical terms within the remaining candidates increased. This result clearly shows the effectiveness of the constituent noun filtering technique in reducing the computational time of discovering fixed number of novel technical terms. Then, per a domain, we randomly select 1,000 of those remaining candidates, and esti-mate their domain specificity by the proposed method. After manually judging the domain specificity of those 1,000 terms, we measure the precision/recall of the proposed method as in Table 2, where we achieved about 75% precision and 80% recall. Here, however, as candidates of technical terms, we simply collect compound nouns, where sometimes their term unit is not correct since the tech-nical term candidate could be with a certain prefix or suffix. Considering this fact, Table 2 also gives the term unit correct rate for those with domain speci-ficity  X + X  or  X   X   X . Finally, taking this term unit correct rate into account, we can conclude that, out of the 1,000 candidates, we discovered about 100  X  200 novel technical terms that are not included in any of existing lexicons of the domain. This result clearly supports the effectiveness of the proposed technique for the purpose of full-/semi-automatic compilation of technical term lexicons. This paper proposed a method of domain specificity estimation of technical terms using the Web. We then applied this technique of estimating domain specificity of a term to the task of discovering novel technical terms that are not included in any of existing lexicons of technical terms of the domain.
 should be descr i bed i n the summary . Ex i st i ng Graph-based rank i ng algor i thms are used i nformat i on by assess i ng we i ghts of l i nks between nodes .

In th i s paper, we propose to extract event i nformat i on and der i ve i ntra-event relat i ons between event elements i n news art i cles w i thout deep natural language when creat i ng summar i es . We close w i th the d i scuss i on of future work . mathemat i cal structure of a text or a group of texts [8] . At the same t i me, graph-based structure, rather than rely i ng only on local node-spec i f i c i nformat i on . To rank ent i re sentences for sentence extract i on, most of prev i ous works add a node to how to represent sentence and how to def i ne connect i ons between sentences . The Unfortunately, dependency analys i s requ i res syntax process i ng techn i ques .
Event-based summar i zat i on has been i nvest i gated i n recent research . As i ntroduced above, [5] and [6] both extracted events i nformat i on by dependency structure of sentences and then formed a graph for summar i zat i on . In contrast, F i latova and structure analys i s of sentences [7] . They evaluated sentences only by t i mes of approach cla i med to out-perform convent i onal tf* i df approach on summar i zat i on and event def i n i t i on i s too str i ct to capture adequate i nformat i on from texts .
Our work d i ffers from these prev i ous stud i es i n two key respects . F i rst, we propose a novel approach to extract sem i -structured events w i th shallow natural language process i ng . Second, we bu i ld event-centr i c document graphs to make conceptual 3.1 Extract i on of Event Events descr i bed i n texts l i nk ma j or elements of events (people, compan i es, locat i ons, frequently occurr i ng nouns, k i nd of named ent i t i es that can not be marked by general named ent i ty taggers . A verb or an act i on noun i s deemed as an event term only when i t appears at least once between two name ent i t i es . Event terms roughly relate to the act i ons of events . Thus, we extract events based on named ent i t i es and co-occurrence of event elements w i thout syntact i c analys i s .

Events are extracted from documents by us i ng follow i ng steps : 1 . Mark texts w i th named ent i t i es and POS tags . 4 . Scan documents aga i n to extract events as event terms w i th ad j acent named 
Th i s approach complements the advantages of stat i st i cal techn i ques and captures f i ve extracted events . The event  X  X ue X  represents the structure of Sub j ect-Verb-Ob j ect  X  X oftware X  i s not as proper as  X  X he Internet software market X  . However, graph-based rank i ng algor i thm calculates the we i ghts of nodes and roughly gets r i d of un i mportant event elements and extra elements added by m i stake . 3.2 Bu i ld i ng Document Graph To form the document graph, we take these events by choos i ng event elements (event terms and name ent i t i es) as nodes . The edges between event elements are establ i shed by co-occurrence i n a same event . A p i ece of a graph bu i lt by our system for cluster d30026 (DUC 2004) i s shown i n F i gure 2 .
The document graph i s we i ghted but und i rected . D i fferent from prev i ous work on i ntra-event relevance [7] [9], the relat i onsh i p between event elements i s measured not only by count i ng how many t i mes they co-occur i n events, but also by tak i ng same pa i r of named ent i t i es are mostly because of compl i cate sentence structure, such event i s i nd i cated as (, ) (, )1/ ad j acent event terms between the same named ent i ty (pa i r) . The we i ght of connect i on w i th i n graph i s calculated as R( , ) R( , ) ( , ) enlarges a part of document graph i n F i gure 2 to show the we i ght of each edge .
S i nce these events are commonly related w i th one another semant i cally, under the same or related top i c, we can der i ve i ntra-event relevance between two event terms or two named ent i t i es from document graph . Where NE( ) terms 
For the conven i ence to observe organ i zat i on of document and to i nvest i gate certa i n des i gn to form document graph on event and sentence level . To determ i ne the based on a measure of event elements overlap and the other i s to use the cross and neglect other words, thus the second approach i s better to make use of event sum all the we i ghts of connect i ons between event elements and s i m i larly, relat i ons of sentence by we i ghts of connect i ons between events .
 3.3 Node Scor i ng w i th PageRank for Summar i zat i on To score the s i gn i f i cance of nodes i n a document graph, our system uses the followed by several i terat i ons unt i l convergence .
 The formula for calculat i ng Pagerank of a certa i n node n i s g i ven as follows : where L i s the set of nodes l i nk i ng i nto node n d i s a dampen i ng factor, set to 0 . 85 exper i mentally they cover the most of concepts, remov i ng all dupl i cate sentences .

W i th rank i ng algor i thm for graph, process of extract i ve summar i zat i on can be fully i nformat i on fus i on, sentence compress i on and sentence generat i on i n the future . We test our event-based graph i cal approach by the task of mult i -document summar i zat i on i n DUC 2001(task 2) and DUC 2004(task 2) . The documents are pre-processed w i th GATE to recogn i ze named ent i t i es, verbs and nouns .
In order to evaluate the qual i ty of the generated summar i es, we use the automat i c correlated w i th human j udgments .

In our f i rst exper i ment our approach i s evaluated on 200-words summar i es of DUC 2001 . We determ i ne the sal i ent concept by document graph on event element level . We compare the ROUGE scores of add i ng frequent nouns or not to the set of named commun i ty [11] . ROUGE scores are reported for each document set rather than average score because ROUGE scores depend on each part i cular document set created accord i ng to document graph w i th frequent nouns rece i ves h i gher ROUGE ROUGE score i s ga i ned on average . The advantage of graph-based approach over i mproper h i gh i df scores from rare words that are unrelated to the top i c .
Next, we compare two methods to measure the strength of relat i onsh i p between event elements, one i s proposed i n prev i ous work by t i mes of co-occurrence i n events, evaluate th i s ad j ustment on d i fferent strateg i es on der i v i ng event relevance by graph-based rank i ng algor i thm i n [9], and prove that i mprovement i s sl i ght but constant .
As d i scussed before, document graph can be constructed by choos i ng d i fferent observe and i nvest i gate documents more conven i ently .
 In th i s paper, we propose a new approach to present documents by event-based graph Document graph makes use of the assoc i at i ons of event elements based on co-occurrence to avo i d complex natural language process i ng techn i ques . Graph-based summar i zat i on .

The graph constructed i n th i s way allow further complex process i ng, such as graph-based document representat i on and rank i ng algor i thms i s that they exclus i vely approach can be adapted to other languages . In fact, we have recently attempted to summar i zat i on (F i gure 7) .
 Acknowledgments. The work presented i n th i s paper i s supported part i ally by Nat i onal Natural Sc i ence Foundat i on of Ch i na (reference number : NSFC 60573186), part i ally by Research Grants Counc i l on Hong Kong (reference number CERG PolyU5181/03E) and part i ally by the CUHK strateg i c grant (# 4410001) . Quest i on Answer, Mach i ne Translat i on and so on . Lots of works have been done on approaches and mach i ne learn i ng based approaches . [Gr i shman, et al . 1995] [Krupka, rece i ved more and more attent i ons . [Sek i ne, et al . 1998] used Dec i s i on Tree for NER . [B i kel, et al . 1997] modeled NER task us i ng H i dden Markov Model(HMM) . Max i mum Entropy Model[Borthw i ck, et al . 1999][M i kheev, et al . 1998] have been also proposed to solve the problem of NER . For Ch i nese, Role-tagg i ng method was proposed i n [ZHANG Hua-p i ng, LIU Qun, 2004] . [Lv Ya-j uan, Zhao T i e-j un, 2001] used dynam i c programm i ng . been shown that d i fferent models have d i fferent strength on d i fferent NE types . Comb i nat i on of several sub-models to cope w i th d i fferent k i nd of NE respect i vely i s an effect i ve way to i mprove the performance of NER . For example, [Chen, et al . models are used on Locat i on and Organ i zat i on recogn i t i on .

No matter wh i ch model i s used, almost all NER models recogn i ze NE by m i n i ng [Borthw i ck, 1999] [Youzheng Wu, Jun Zhao, et al 2005] . How to make use of performance of NER .
 Th i s paper proposes a probab i l i st i c feature based max i mum entropy approach to funct i ons, i t i s one of the several d i fferences between th i s model and the most of the prev i ous ME based model . We also explore several new features i n our model, wh i ch works, we use sub-models to model Ch i nese Person Names, Fore i gn Names, locat i on name and organ i zat i on name respect i vely, but we br i ng some new techn i ques i n these sub-models . Exper i mental results show our ME model comb i n i ng above new i n MSRA NER task of SIGHAN06 contest name, locat i on name and organ i zat i on name .

Our work i s under the framework of Max i mum Entropy (ME), but d i ffers from standard ME model .

We cons i der a random process wh i ch produces an output y , a member of a f i n i te model that accurately represents the behav i or of the random process . Such a model i s g i ven by (1) . the requ i rement that 1 ) | ( =  X  CPN denotes the Ch i nese person name, SN denotes Surname, a typ i cal feature i s :
But i n Ch i nese, f i rstly, most of words used as surname are also used as normal words . The probab i l i t i es are d i fferent for them to be used as surname . Furthermore, a surname i s not always followed by a g i ven name, both cases are not b i nary . To model these phenomena, we g i ve probab i l i ty values to features, i nstead of b i nary values .
For example, a feature funct i on can be set value as follows :
Or and (5), both and are Ch i nese surname, but they have d i fferent probab i l i t i es as surnames . That i s to say, we can get more i nformat i on from i nstances . a person name . So the conf i dence funct i on i s 
Th i s funct i on i s i ncluded i n ME frame as a feature . Ch i nese word segmentat i on always ex i sts . We propose some patterns for model roles of Ch i nese characters i n person names .
We have seven patterns to collect cand i date Ch i nese person name; f i rst two Ch i nese person name brought by word segmenters . (1) We use BCD pattern to model a Ch i nese person name wh i ch i s composed of three Ch i nese characters . For example :  X  (B) (C) (D)  X  . Where B i s the surname of the Ch i nese person name . C i s the f i rst g i ven name . D i s the last g i ven name . (2) We use BD pattern to model a Ch i nese person name wh i ch i s composed of two Ch i nese characters . For example  X  (B) (D)  X  . (3) BCH pattern i s used to model an amb i guous case where the last g i ven name and i ts next Ch i nese character can be a Ch i nese word . For example :  X  (B) (C) comb i nes w i th as a word before NER . (4) UCD pattern models an amb i guous case where the surname and i ts prev i ous character are comb i ned i nto a word . For example :  X  (U) (C) (D)  X  . Where i s a name, wh i le i s a Ch i nese word . (5) In BE pattern, the f i rst g i ven name and the last g i ven name can be a word . For example :  X  (B) (E)  X  . Here, i s a person name, wh i le i s a Ch i nese word . example :  X  U (D)  X  . Where i s a person name, wh i le i s a normal word . (7) Somet i mes, a Ch i nese personal name may be composed of only two g i ven names, for example  X  (C) (D)  X , where i s a person name, and we use CD to denote th i s pattern .
 character l i sts wh i ch belong to the class  X  X , C, D, U, H, E X  respect i vely . We collect cand i date Ch i nese person names based on the feature patterns . Moreover, contextual words of the cand i date person name are also used for model .
For Ch i nese person name recogn i t i on, a problem here i s how we can know whether a person name i s composed of two or three Ch i nese characters . We used another technology to help boundary detect i on . We use co-occurrence count of a cand i date personal name . For example :  X   X  
In th i s sentence, we collected a cand i date Ch i nese person name  X   X , then we should make a dec i s i on whether the last character  X   X  belongs to th i s personal name or not . To do i t, we j ust count follow i ng two co-occurrences and compare them . Here, we have So,  X   X  i s not i ncluded i n the personal name,  X   X  i s a correct cho i ce .
We i mplement Ch i nese person name recogn i t i on and fore i gn person name i nclude Ch i nese surnames i n i t, wh i ch are i mportant cues for our model to recogn i ze Ch i nese person names . In th i s case, a p i ece of the fore i gn person name may often be recogn i zed as a Ch i nese person name .

In follow i ng example, the tagger nr means a person name .  X  /nr /nr  X   X ,  X   X  and  X   X  are Ch i nese surname, when we recogn i zed the Ch i nese name f i rstly, we w i ll choose  X   X ,  X   X  and  X   X  as three cand i date Ch i nese person names, but , and are three fore i gn names, so, we des i gn a flex i ble method to i dent i fy fore i gn personal name i n th i s case . For a amb i guous Ch i nese character l i ke  X   X  or  X   X  wh i ch can be both i n a context to f i nd some other Ch i nese characters wh i ch belong to Ch i nese person name or fore i gn person names . Accord i ng to the results of collect i on, the model w i ll choose Ch i nese person model or fore i gn person model to i dent i fy the cand i date person name . i mproved .
 Locat i on names often end w i th the some spec i f i c words l i ke  X  /prov i nce X  . method and the conf i dence funct i on are also used for locat i on name recogn i t i on . The conf i dence funct i on for locat i on name i s as equat i on (8) .
Feature of person name and locat i on name are selected as follows : CPN denotes the Ch i nese person name; FPN denotes the fore i gn person name; LN denotes the locat i on name; model to dec i de whether i t i s an organ i zat i on .
 organ i zat i on .
 organ i zat i on name, W keyword i s a keyword of organ i zat i on l i ke  X  /company X  .
The probab i l i ty of a cand i date as an organ i zat i on name i s def i ned as i n (9) . constant), we then tag BIO i nformat i on for i t us i ng ME model . BIO i s usually used i n denotes the beg i n word (or character) of an organ i zat i on name, I i s a m i ddle word (or character), L i s the last word (or character) and O i s not i n an organ i zat i on name . We Output probab i l i ty i s used to ass i gn tags .
 features to recogn i ze B, I, L and O as follows .
 We i mplement several exper i ments to test our model .
 outperform normal b i nary feature funct i on based ME model . We use 7M corpus of one-month People  X  s Da i ly (January, 1998) .
 Table1 . The label of R, P and F show the recall, prec i s i on and F-measure respect i vely .
From the Table1, We can f i nd the performance of probab i l i ty feature i s better than that i n b i nary case, the F-measure i mproved 7 . 2% .
 Therefore, we use the probab i l i ty feature funct i on i n follow i ng exper i ment result . data . Data of June i s used as test data . The exper i mental results are shown i n Table2 . are shown i n Table . 3 f i lter i ng i s very effect i ve We took part i n the SIGHAN (2006) ent i ty recogn i t i on open track contest for M i crosoft Research As i a Research (MSAR) corpus, and ach i eved the h i ghest F-measure .
 feature funct i ons . We also explore several new features i n our model, wh i ch i ncludes sub-models to model Ch i nese Person Names, Fore i gn Names, locat i on name and organ i zat i on name respect i vely . Exper i mental results show our ME model comb i n i ng above new elements br i ngs s i gn i f i cant i mprovements .
 The research work was supported by the Ch i na Nat i onal Pro j ect 863 (the 863 Program) under the grant No . 2001AA114210 and MOE funded pro j ect (MZ115-022) :  X  X ools for Ch i nese and M i nor i ty Language Process i ng X  .
 When i mag i ng bound documents, such as books, or i g i nally stra i ght text l i nes often greatly, because current OCR systems oftern requ i re text l i nes be stra i ght . Therefore, i t i s a necessary process for character recogn i t i on system i n both scanner and camera i mages to make text l i nes stra i ght .

In the l i terature, several methods have been proposed to deal w i th th i sk i nd of d i s-tort i on . However, there has not been a good way to automat i cally and robustly rect i fy bound document i mages so far . [1] [2] dev i sed spec i al camera equ i pments to acqu i re document i mages . The depth of an i mage can be measured us i ng these tools and 3D shape model be reconstructed . Although these methods are useful i n some g i ven ap-use i nda i ly l i fe . [3][4] proposed effect i ve shape from shad i ng method to handle scanned document i mages . However, i t i s hard to deal w i th l i ght i ng i nformat i on for camera i m-ages . Some approaches are based on one document i mage [5,6,7,8,9,10] . In these pa-pers, curved text l i nes are usually fitted to represent the d i stort i on i nthe i mage . And then the i mage i s restored based on some predefined model . Th i s one-i mage approach i s more appl i cable, s i nce no spec i al dev i ces, camera parameters or extra i mages are mat i cally and accurately .

In [5], one p i xel w i dth text l i ne features are extracted first . Then search i ng i s done from r i ght to left based on the predefined step and angle . Next, two typ i cal d i rectr i xes are chosen to model the bound document as a general cyl i nder surface . F i nally, the rec-method i s used to trace curved text l i nes . After text l i nes are detected, a source and target mesh are constructed correspond i ngly . Lastly, the i mages are corrected by a two-pass i mage warp i ng algor i thm . Zhang [7] also presented a method based on regress i on of curved text l i nes . Th i s method i s espec i ally for scanned i mages . Shade area and clean area are detected first . Then pro j ect i on and connected components analys i s are used to regress i on method . However, these ex i st i ng methods e i ther need human i nteract i on or are not robust i n compl i cate camera captured document i mages .

In th i s paper, we propose a new one-i mage based bound document i mage correct i on method . Our key i dea i sto i ntroduce an automat i c and robust correct i on . In order to do the correct i on prec i sely, an graph model i s proposed to locate the text l i nes . The whole us i ng BST method to compensate nonun i form l i ght i ng or shade . Next, feature po i nts are extracted from the m i n i fied i mage and locally opt i m i zed text curves are detected texture warp i ng .

Th i s paper i sorgan i zed as follows . In sect i on 2, the deta i l of our correct i on method i s presented . Sect i on 3 analyze the exper i ment results . And conclus i on i s presented i n sect i on 4 . 2.1 Text L i ne Feature Extract i on and Preprocess i ng A vers i on of Seeger  X  s Background Surface Threshold i ng method(BST) [11] i s i mple-mented . Text areas are labeled first . Then, cont i nuous background i ntens i t i es are est i -mated and threshold i ng i s performed at the background plus some offset . Th i sb i na-r i zat i on method can handle l i ght var i at i on i n the document i mage, and produce a less no i sy i mage . After that, we propose to explo i t the same observat i on as [5] : at a certa i n d i stance, text l i nes can be seen as l i ne segments . Our method i s based on th i s i dea that r i zed i mage i s down sampled . Some geometr i c rules are then used to filter out non-text blocks . Afterwards, text l i nes  X  m i ddle pos i t i on i n each column are reco rded to form the feature i mage of text l i nes .

In add i t i on, the obta i ned bound document i mages often i nclude some parts of the ne i ghbor i ng page, as the example i mage shown i nF i g . 1 . Some documents also have mult i ple columns . To deal w i th these, a s i mple pro j ect i on profile based method i sap-pl i ed to analyze document layout i n the feature i mage . Attached ne i ghbor i ng parts are screened out, and d i fferent columns are separated before be i ng sent to the follow i ng process . A curved document i mage and i ts correspond i ng feature i mage after layout analys i s are shown i nF i g . 1 .
 2.2 Graph Model of Text L i ne Features Graph based method i sw i dely used i nthel i ne or curve detect i on problems [12] . We can segments, and segments on the same text l i ne do not overlap w i th each other . In v i ew of these, an or i ented graph G = { V, E } i s adopted to represent the feature i mage . Here V i s the set of nodes represent the segments  X  vertexes, E  X  V  X  V i s the set of d i rected edges that represent l i ne segments . Define the or i entat i on of all the edges e  X  E i s from cluster them i nto d i fferent groups, and edges i n each group represent a curved text l i ne . Some defin i t i ons are necessary for further process i ng .
 Defin i t i on 1. Two e d ge s e 1 and e 2 are connectible only if there is a path between all the four vertexes on these two edges.
 v i s no poss i ble path through the four vertexes(The reason i sthed i rect i on of an edge i s from left to r i ght) . So the two edges i n (a) can be connected, those i n (b) cannot be connected .

If two edges could be connected, the i r connect i on coeffic i ent i s defined as c = d +  X  (  X  1 +  X  2 ) where d i s the Eucl i dean d i stance between two vertexes to be connected,  X  ,  X  2 are the dev i at i on angles between two d i rected edges, as shown i nF i g . 3,  X  i sa j agged penalty parameter, i t controls the preference between d i stance and smoothness, ab i gger  X  means smooth i ng connect i on i s preferable .
 Defin i t i on 2. Two connectible edges e i and e j can be merged only if where c ij is the connection coefficient between edge e i and e j , and c i . ,c j . are the con-nection coefficients of edge e i ,e j to other edges respectively, T is a preset threshold to prevent over-connection.
 If two edges could be merged, i t means they can be clustered i nto one group . The whole procedure of edge cluster i ng i sasfollows .
 Algor i thm of cluster i ng : 1 : compute the connect i on coeffic i ent set of the edges, S = { connection coefficients } 2 : c ij = m in { S } 3 : wh i le ( c ij i s less than T ) 4 : cluster e i and e j i nto one group 5 : merge e i and e j , update S 6 : c ij = m in { S } 7 : end 8 : output the cluster i ng result 2.3 Computat i on T i me Reduct i on For a document i mage cons i sts of many text l i nes, the number of l i ne segments i n calculate the connect i on coeffic i ent set . Wh i le, a curved document i mage has the prop-erty that : vert i cally, edges on the same text l i ne only span across a narrow str i p . Thus, comput i ng the connect i on coeffic i ent of two edges far away i n the vert i cal d i rect i on i s unnecessary . Moreover, separat i ng the i mage i nto hor i zontal str i ps only i nfluences boundary are collected to cluster w i th the prev i ous merg i ng result . Suppose the number Commonly, the computat i on t i me i s reduced several t i mes . 2.4 Curve Opt i m i zat i on and Image Restorat i on After the above processes, l i ne segments are clustered i nto d i fferent groups, i. e . feature po i nts are clustered i nto d i fferent groups . Po i nts i n each group represent one curved d x + e i i s used to fit the least square regress i on curve . Choos i ng th i s model i sto smooth the small fluctuat i ons and s i mpl i fy later processes . There are st i ll two k i nds of false detect i ons must be handled . One i s the short curves formed by po i nts i n figures or i s adopted to delete the po i nts group wh i ch i s shorter than 1 / 3 of the i mage w i dth . The other case i s the occas i onal cross i ngs between text l i nes . The local cons i stency between curvature of text l i nes i sut i l i zed to screen them . In a local reg i on, the average slope local reg i on . Then the curve slope dev i ates from  X  larger than a preset threshold  X  i s deleted .
 A set of po i nts are sampled on each curve accord i ng to the length of the curve . ( x i ,y i ) i sapo i nt on curve i . After the correspond i ng po i nts i n the source i mage and the dest i nat i on i mage are located, the techn i que of texture warp i ng are used to restore the document i mage . 2.5 Exper i ment Result and Analys i s Exper i ments were made on 100 bound book document i mages taken from ord i nary consumer d i g i tal cameras . The resolut i on of i mages ranges from 2 mega p i xels to 4 mega p i xels . F i g . 4 shows some of the restorat i on results . Same parameters  X  =10 ,T = 2 0 ,n =10 are used for all the i mages .
 OCR performance before and after rect i ficat i on i s compared based on ABBYY F i neReader 8 . 0 for Engl i sh documents and our OCR software for Ch i nese documents . Images are b i nar i zed us i ng BST method first and the val i d content reg i on after layout analys i s are sent i nto OCR software . The word accuracy measure i s used as the evalu-at i on metr i c : accuracy = n  X  e n ,where n i s the number of characters i n the document, and e i s the number of falsely recogn i zed characters . Here, only s i ngle character recog-n i t i on result i s cons i dered . The compar i son of recogn i t i on accuracy before and after for Ch i nese document can not handle curved text l i nes well, the accuracy rate before documents .

We also compared our rect i ficat i on results w i th the s i m i lar funct i on i n OCR soft-ware ABBYY F i neReader 8 . 0 . Two metr i cs are used to evaluate the performance . F i rst, d i storted document i mages, the ma i n problem for OCR systems i s that curved text l i nes make the layout analys i s and l i ne separat i on processes hard . Preprocess i ng methods i n most OCR systems depend on stra i ght text l i nes . Even i f some OCR systems that could l i nes are often falsely placed on the same l i ne or texts on the same l i ne are placed as portant task for rect i ficat i on algor i thms . We propose the PCRL metr i cus i ng the layout rebu i ld i ng funct i on i n OCR software, the number of l i nes i n the document and the l i nes correctly rebu i ld i ng by OCR software are both counted .
 The compar i son between our method and ABBYY  X  s i sshown i nTable2 . In the exper i -ment, formula l i nes are not counted i n .

The ABBYY F i ne Reader 8 . 0 i seffic i ent for camera i mages, i ts preprocess i ng method does not depend on stra i ght text l i nes . So i ts s i ngle character recogn i t i on ac-to the i mage resolut i on, so the i mages are analyzed together . For s i ngle column docu-ment, our method outperforms ABBYY  X  s method both i n the ob j ect i ve metr i candthe perceptual results . For the two column document, ABBYY  X  s method i s better, i t can perform rect i ficat i on i n the whole i mage scope . 2.6 Conclus i on and Future Research In th i s paper, we have proposed a correct i on method for bound document i mage . It follows the one i mage approach . That i s only one camera or scanned i mage i s needed . It text-l i ne features are first extracted from the b i nar i zed i mage, and curves of separate text l i nes are detected automat i cally us i ng a graph model approach . Next, the techn i que of texture warp i ng i sut i l i zed to restore the i mage . Exper i ments on OCR accuracy before and after restorat i on and compar i son w i th ABBYY  X  s method show the val i d i ty of the method .

In the future research, we i ntend to exam i ne some global models to deal w i th mult i -column documents, as well as short text l i nes at the top or bottom of a document i mage . newspapers or web documents . Patents are generally very long and verbose, and the i r patents, or 100-200 g i gabytes of text, wh i ch are made up of hundred f i elds of textual or non-textual i nformat i on (Larkey, 1998) . One can take advantage of i ts structure for model i ng are reported (Kurland and Lee, 2004; L i u and Croft, 2004) . e i ther to smooth document language model or as an i ndependent top i c model wh i ch i s i nterpolated w i th document model for f i nal scores of retr i eved documents . They report smooth i ng document model w i th cluster model does i ncrease the retr i eval document and cluster model .

Our work extends Kang et al . (2006)  X  s top i c-or i ented model w i th cons i derat i on of the character i st i cs of manually clustered documents . We show and d i scuss the cluster exper i mental results and our conclus i on . 2.1 Internat i onal Patent Class i f i cat i on System (WIPO) . The 5 levels of IPC are : sect i on, class, subclass, ma i n group, and subgroup . A patent appl i cat i on i s hand-ass i gned to one or more appropr i ate IPC codes by human classes, or sect i ons . We cons i der IPC at each level as a cluster of documents, although for our exper i ments we only use IPC Code at level 5 . 2.2 Stat i st i cs of IPC Clusters cluster membersh i ps of patent documents are collected from the NTCIR-4 patent document collect i on .

The s i ze and the document membersh i ps of IPC clusters are very d i fferent from number of clusters a document can belong .
 s i ze var i es from 1 to 85355 documents, and the average number of documents per the l i terature wr i tten by Fall et al . (2003) .

F i gure 2 shows how many IPC codes a document belongs and frequenc i es of such documents . A patent document has at least 1 IPC code and at most 91 IPC codes . On dev i at i on i s about 3 .
 that of Z i pf  X  s : few occur very often, wh i le others occur rarely . 3.1 Automat i c Cluster i ng Many researchers have tr i ed d i fferent cluster i ng methods to create automat i c clusters k-means (L i u and Croft, 2004), or d i fferent methods that uses common class i f i cat i on algor i thms such as k-Nearest Ne i ghbors algor i thms (Kurland and Lee, 2004) .
There has not been any research on the character i st i cs of automat i cally bu i lt expect the propert i es for some of the methods .

For k-Means algor i thm, i f we assume a document has an equal chance to be Each document has probab i l i ty of 1/k and there are n i ndependent tr i als where n i s the var i ance of n * 1/k * (n X 1)/k, or (n 2 -n)/k 2 . bu i lt for every document, there are n numbers of overlapp i ng clusters . (KL) d i vergence . However, such measures solely depend on the presence of terms of the documents i n cons i derat i on . 3.2 Manual Cluster i ng performance accord i ng to Kurland and Lee (2004) . For the i r exper i ments, wh i le small g i ves super i or retr i eval performance .
 take advantages of manual cluster i ng and automat i c cluster i ng : an accurate relevance measure and the control over cluster s i zes . Another s i mple but crude method i s to use only clusters of wanted s i zes and i gnore the rest .
 coeff i c i ent for s i m i lar i ty score . 4.1 Cluster-Less Language Model We set cluster-less language model as our basel i ne to compare cluster-less model w i th cluster-based model . We take the performance of Kang et al . X  s Jel i nek-Mercer Smoothed un i gram max i mum l i kel i hood language model : smooth i ng parameter for Jel i nek-Mercer smooth i ng, and q i s a query term . P ml when  X  =0 . 2 . 4.2 Interpolat i on Model or i ented . Each model uses the cluster model generated by language model i ng for d i fferent purposes . Smooth i ng-or i ented model uses cluster model for smooth i ng document language model before smooth i ng w i th collect i on model, wh i le top i c-or i ented model i nterpolates document model and cluster model after smooth i ng each model w i th collect i on model .

Our cluster-based models are extended vers i on of the top i c-or i ented model wh i ch performed better than smooth i ng-or i ented model .

Top i c-or i ented model, wh i ch we w i ll refer to as i nterpolat i on model i s : and that document D belongs .
 extends the i nterpolat i on model and have the same parameter sett i ngs . 4.3 Cluster S i ze-L i m i t Model As Kurland and Lee (2004) have done, we try to prevent too many i rrelevant documents clusters hav i ng s i ze larger than the parameter are i gnored ent i rely . Relevant documents i n large clusters may not be benef i ted . Nonetheless, for exper i mental purposes, i t should be suff i c i ent enough to show how cluster s i ze affects the retr i eval performance . 4.4 Cluster Expans i on Model parameters : the number of top clusters from wh i ch clusters are expanded, and the number of clusters to expand from each top cluster .
 Exper i mental Setup. For our test collect i on, we use NTCIR-4 patent wh i ch conta i ns top i cs are used for our exper i ments .
 sect i ons are extracted to represent documents .
 relevant document set .

For i ndex and query terms, character b i grams of Japanese, numbers, and Engl i sh words are used . 5.1 S i ze-L i m i t Model brought i n w i th i ncreased s i ze-l i m i t . 5.2 Cluster Expans i on Model We expected Cluster Expans i on Model to perform well . However, i t performed even worse than Cluster-less Model . The poor performance can be expla i ned i n several number of expanded clusters seemed reasonable from the authors  X  po i nt of v i ew, the range def i ned by exper i menter may not cover the opt i mum parameters . Also, the rat i o of document model and cluster model of the Interpolat i on Model was f i xed throughout the exper i ments, but reduc i ng the number of clusters decreased the port i on of cluster model i n the f i nal score of the Interpolat i on Model . At d i fferent number of clusters, One needs to search the opt i mal rat i o exhaust i vely .

The effect of chang i ng the number of clustered used and the number of expanded cluster, however, i s well demonstrated . We have proposed new models for cluster-based patent retr i eval us i ng Internat i onal s i ze plays an i mportant role i n cluster-based retr i eval, and we were able to show that i t appl i es to manual cluster as well . W i th such knowledge, we proposed models more Expans i on Model can also apply to automat i cally clustered documents and we plan carry out such exper i mentat i on, soon .
 Acknowledgments. Th i s work was supported by the Korea Sc i ence and Eng i neer i ng Foundat i on (KOSEF) through the Advanced Informat i on Technology Research Center (AITrc), and also part i ally by the BK 21 Pro j ect i n 2006 .
 Recently, environments have been prepared in which a large number of audio and multimedia archives such as video tapes, digital libraries and so on can be easily used. Especially, a rapidly increasing number of spoken documents, such as broadcast radio and television programs, are archived, many of which can be accessed through the Internet. The needs for retrieving such speech information have been more growing, by the day, while it is definitely true that an effec-tive retrieval technique is lacking at present. Moreover, and the development of technology for retrieving such speech information is becoming more and more important.

In the TREC Spoken Document Retrieval track[1], a number of studies were presented on the subject of English and Mandarin broadcast news documents. A standard approach to spoken document retrieval (SDR) is to automatically transcribe spoken documents into word sequences, which can be directly matched against queries.

Robinson et al. [2] have proposed a retrieval method that is robust for word recognition errors in case of transcribing spoken documents. This method uses parallel text-formed documents whose contents are similar to contents of the spoken documents. Hauptmann et al. [3] and Jourlin et al. [4] have investigated SDR performance using various indexes. Each index has various recognition per-formances when spoken documents were transcribed. In this approach, however, a serious problem arises when both the queries and the documents include out-of-vocabulary (OOV) keywords , where matching against OOV keywords always fails, because the OOV keyword can not be transcribed as a word. Many previous studies handled an OOV problem in spoken document retrieval. In most of them, spoken documents were not transcribed into word sequences, but into phoneme / syllable sequences using phoneme / syllable recognizers[5,6,7,8]. Wechsler et al.[5] used a phoneme recognizer to transcribe spoken documents. K. Ng et al.[6] worked on the use of sub-word unit representation based on phonemes in spo-ken document retrieval. H. M. Wang[7] al so investigated syllable-based indexing for retrieval of spoken documents in Mandarin Chinese using a syllable lattice. C. Ng et al. [8] reported, however, that better retrieval performance in terms of average precision was obtained using word-based indexing rather than using phoneme/syllable based indexing. Furthermore, word based retrieval must be faster than that based on phoneme/syllable.

For spoken document retrieval, a mis-recognition problem is also fatal as far as word based indexing is used. It is well known that a voting scheme such as ROVER (Recognizer Output Voting Error Reduction) for combining mul-tiple speech recognizers X  outputs can achieve word error reduction[9]. We [10] have proposed a spoken document retrieval method which is robust for mis-recognition and OOV words. In [10], we prepared two types of indexes, the one was a word-based index, the other was a syllable-based index. Our final goal in this study is to make a refined word-based index of spoken documents. So, this paper describes word error correction of outputs of large vocabulary continuous speech recognition (LVCSR) system by using WEB documents for spoken doc-ument indexing. Especially, we focus on a correction of mis-recognized proper nouns that are likely to be keywords for retrieving documents. Considering a case of automatically indexing news speeches from TV and radio, we can find the text-formed documents, whose contents are similar to the news speeches, on the Internet. So, the proposed method may be very effective. In the result of ex-periment of error correction, our technique significantly improved the recognition rate of proper nouns on a TV news dictation. 2.1 Overview of the Process Figure 1 shows an overview of the error correction process we proposed. In this framework, a document contains some sentences is processed as a processing unit instead of a sentence unit.
 First, word or word sequence candidates to be corrected are detected by two LVCSR systems. At the same time, trans cribed words or word sequences that are probably correct are also detected. Those words are used to retrieve WEB documents as a query. Next, after retrieving WEB documents using the words, correct word candidates which may be substituted for mis-recognized words that are extracted from the WEB documents. Then, each incorrect word candidate is matched against a correct word candidate using a syllable-based DP matching technique. If a incorrect word candidate is identical to a correct word candidate, the incorrect word candidate is replaced by the correct one. 2.2 Detection of Incorrect Word Candidates To detect incorrect words or word sequences that are mis-recognized, we uses two LVCSR systems, the one named  X  X POJUS X  which has been developed in Toyohashi University of Technology[11], as well as the one named  X  X ulius X  which is provided by IPA Japanese dictation free software project [12]. Both decoders are composed of two decoding passes, where the first pass uses the word bigram, and the second pass uses the word trigram. Our previous study reported that the agreement between the outputs with different decoders can achieve quite reliable confidence on broadcast news speech and reading speech of newspaper sentences [13]. So, we suppose that agreement portions, in which words are out-putted in common from two LVCSR syste ms, are correctly recognized words, then those are used to retrieve WEB documents as a query. Whereas, suppose that the other potions, i.e. words from a decoder are not identical to the ones from the other decoder, are incorrectly recognized words, and are candidates should be corrected as shown in Figure 2. We denote the one of the portions as  X  X ncorrect word candidate X  in this paper. 2.3 Extraction of Correct Word Candidates The flow among a broken line in Figure 1 shows extracting candidates of correct words. We denote the word set extracted by this process as  X  X orrect word can-didates X . The incorrect word candidate described above may be replaced by the correct word candidate.
 Correct word candidates which consist of only proper nouns are extracted from WEB documents retrieved by the Internet search engine  X  X oogle X . A query for  X  X oogle X  consists of words which are proper nouns and outputted in common from two LVCSR systems when a spoken document is transcribed. The query is automatically composed. I f 10 documents can not be retrieved from  X  X oogle X , a word randomly selected is removed from the query. Then, the new query is used again in the search engine. Those words included in the query have high confidence because about 95% of proper nouns which are transcribed in common by two LVCSR systems are correct.

Finally, the correct word candidates are converted into syllable sequences. 2.4 Substitution Process In the substitution process, first, incorrect word candidates are matched against correct word candidates using a syllable-based DP matching to investigate ad-equacy for the substitution. In other words, a word-spotting is performed to check that whether a correct word candidate is in a portion of incorrect word sequences or not. The substitution process is made story by story.

Figure 3 shows an overview of the syllable based DP matching. Let us denote a syllable sequence, which is detected as a incorrect word candidates, as X = { x of a correct word candidate as Y = { y 1 ,y 2 ,  X  X  X  ,y J } ( J is the number of syllable in Y , y j  X  Y ). As shown in Figure 3, we apply the syllable based DP matching on the syllable sequence X against the syllable sequence Y . In this DP matching, the likelihood G ( i, j ) of arriving at the point ( i, j ) in a DP lattice matrix is maximized: where P ( r, h ) is the probability of mis-recognizing syllable r as h . P ( r,  X  )means the probability of insertion error of syllable r ,and P (  X , h ) is the probability of deletion error of syllable h . Syllable sequences are obtained using a syllable recognizer for a development data des cribed below in section 3.1. By compar-ing those sequences with the references, the syllable recognition error confusion matrix is calculated as bellow: where C ( r, h ) is the number to misrecognize syllable r as h . When the DP score G ( i, J ) is larger than a given threshold, we judge that the incorrect word candidate is identical to the correct word candidate. The threshold is decided based on an experiment in our previous work[10]. 3.1 Data Set The speech data used in this work is Japanese NHK (Japan Broadcasting Corp.) broadcast TV news from June 1st to July 14th, 1996. The data is divided into 2 portions. The one (June 1st) is an evaluation set for error correction, the other (from June 2nd to 14th July) is an development set for calculating the confusion probability P ( r, h ) described in section 2.4. The evaluation speech data is partitioned into separate news stories 1 , and includes 18 stories (175 sentences, 435 proper nouns) in total, half of which are utterances with noise such as background music. It is very difficult to transcribe certain portions of those spoken documents, because utterances with dialogue speech and field reports are also included in this data. 3.2 Recognition Performance of Each Decoder We uses two LVCSR systems described in Section 2.2. As the language models for both recognizers, the word bigram and trigram language models for 20K vocabu-lary size (coverage of 96.7%) are trained using 5 years Japanese NHK broadcast news scripts (1992  X  1996, approximately 120,000 sentences). The acoustic mod-els are based on a Gaussian mixture HMM. We used esyllable-based HMMs.
 Speaker-independent acoustic models were trained by using read speech (about 20,000 sentences uttered by 180 male speakers; JNAS[15]). The feature para-meters consist of 12 dimensional mel frequency cepstrum coefficients (MFCC), delta 12 dimensions, delta delta 12 dimensions, delta powers, and delta delta powers 2
The sampling frequency is 16 kHz, and t he frame is shifted by 10 ms at every frame.

Table 1 shows word-based recognition rates of each decoder for the target spoken documents.  X  X roper nouns X  denotes the correct rates of proper nouns only. 3.3 Experiment on Error Detection We evaluate a performance of detecting incorrect word candidates by using two LVC S R s y s t e m s .
Table 2 shows the performance of error correction. The recognition rate of proper nouns which are recognized by two LVCSR systems in common is 94.5% (273 / 289 = 0.945%). The number of incorrect word candidates which include proper nouns required error correction is 130 among all automatically detected incorrect word candidates, that is 755 3 . As there is the case of including a few mis-recognized proper nouns in a incorrect word candidate, the total number of automatically detected proper nouns should be corrected is 146, whereas the total number of manually detected ones is 162 words. Our proposed error de-tection approach achieves detection rate of 90.1% (146 / 162 = 0.901) of proper nouns incorrectly recognized.

We investigated whether WEB documents including the correct word candi-dates are truly retrieved or not, when a query used to retrieve the WEB docu-ments is made from the common words from two LVCSR systems. In the error correction experiment described below, the query for the search engine is au-tomatically made. However , manually selecting keywords from the outputs of LVCSR systems as a query are used for WEB retrieval in this section. This is to simply inspect how WEB documents including correct word candidates are retrieved by the transcribed words. The result claims that the search engine can retrieve WEB documents which include 139 correct proper nouns corresponding to the incorrect word candidates that are not correctly transcribed. In short, there is a fair possibility of correcting the mis-recognized proper nouns of 95.2% (139 / 146 = 0.952) using the WEB documents. 3.4 Experiment on Error Correction The Error correction performance is ev aluated throughout the series of the process. Table 3 shows the result of error correction experiment. The number of correct proper nouns which are included in the WEB documents retrieved by the  X  X oogle X  search engine, where a query is automatically composed, is 76 words compared with 146 proper nouns that should be corrected. Proper nouns of 52.1% (75 / 146 = 0.521) can be automatically retrieved from the WEB doc-uments. In the case of using the queries manually composed, proper nouns of 95.2% can be covered descrived above on section 3.3. This shows that a new retrieval method of WEB documents are required such as how to make a query, using new search engine and so on.

The number of proper nouns, which are correctly replaced as incorrect word candidates using the syllable-based DP matching, is 29 words. The rate that incorrect word candidates are replaced with correct proper nouns is 19.9% (29 / 146 = 0.199). In 29 words, 12 words can not be correctly transcribed by the both the LVCSR systems. In other words, 17 words of the remainder are correctly recognized by either LVCSR system.

Considering the case that words, which are outputted from both the LVCSR systems, are registered into a word-b ased index, and it is not necessary to agree between those words from both the LVCSR systems, 75 words in 146 proper nouns can be corrected. As the all proper nouns from both LVCSR systems are entered into a index, there is a risk of degrading retrieval performance of spoken documents. However, our previous works [14] clearly claimed that redundant words in a index did not fatally injure the retrieval performance.

Finally, the proposed error correction technique achieves improvement on the recognition rate of the proper nouns in each LVCSR system to 80.0% from 76.6% (Julius) and 70.8% (SPOJUS), respectively. This paper proposes the error correction technique of outputs of the LVCSR systems for spoken documents indexing. The technique consists of three conpo-nents, the first process is to detect portions of incorrect word or word sequences by using the agreement of outputs between two LVCSR systems. The next is to retrieve WEB documents which are related to the incorrect words, then, ex-tract correct words which should be replaced as the incorrect words from the documents. The final process is to substitute the incorrect words with the cor-rect words by the syllable-based word-spotting technique. In the result of error correction experiment, the proposed technique significantly improves the recog-nition rate of the proper nouns in each LVCSR system.

In the future work, we are goint to perform the retrieval experiment of spoken documents using a index which contains corrected words by this techniques. In addition, we would like to develop more refined method that make a query for retrieving WEB documents.
 T ra n s li terat ion,  X  phonetic translation  X  X  r  X  translation by sound  X , i susedt o tra n s -l ate pr o per n ames a n dtech ni ca l terms espec i a lly fr o m L at in l a n guages t onon-L at in l a n guages , such as fr o mE n g li sh t oKo rea n, J apa n ese ,o rCh in ese .Fo r T ra n s li terat ion sare on e o fthema in s o urces o ftheOut -o f -Vo cabu l ar y pr o b -l em [1]. Transliteration pair acquisition e x tracts tra n s li terat ion pa i rs fr o mb ilin-mach in etra n s l at ion a n dcr o ss -l a n guage in f o rmat ion retr i e v a l[1].Mo st o fthe pre vio us wo rk on transliteration pair acquisition [ 2 ,3, 4 ,5,6, 7 , 8 ] has f o cused on because i td o es no tc on s i der the rea l-usage o ftra n s li terat ion s in te x ts .
A sthe W eb bec o mes on e o fthema in k nowl edge s o urces f o r n atura ll a n guage i c on sus in gthe W eb [9,10,11,1 2 ,13]. T he y e x tracted tra n s l at ion l e xi c on sfr o m i dated the e x tracted tra n s l at ion l e xi c on sb y us in g W eb freque n c y( the n umber o f W eb pages retr i e v ed b yW eb search e n g in es ). How e v er , the yonlyv a li dated w hether a target term ( t )w as li ke ly t o be a c o u n terpart o fas o urce term ( s )  X  th i sca n be regarded as on e -w a yv a li dat ion fr o m s t o t  X  rather tha n b o th w hether t i s li ke ly t o be a c o u n terpart o f s a n d w hether s i s li ke ly t o be a
To address these pr o b l ems ,w epr o p o se a nov e l tra n s li terat ion pa i racqu i s i t ion m o de l that e x tracts tra n s li terat ion pa i rs fr o mthe W eb (W eb search resu l ts ) a n d m o de l s .
 T h i spaper i s o rga ni zed as f ollow s .In sect ion 2 ,w ed i scuss re l ated wo rk . In sect ion s 3 a n d4 ,w esh ow e x tract ion o ftra n s li terat ion pa i rca n d i dates a n d sect ion 6. T he pre vio us wo rk e x tracted tra n s li terat ion pa i rs fr o mb ilin gua l te x ts such as 1) m onolin gua l c o mpar i s on appr o ach a n d2 ) b ilin gua l c o mpar i s on appr o ach  X  f o rmed a wo rd in on e l a n guage in t o ph on et i ca lly equ iv a l e n t on e in the o ther t w ee n t wo wo rds w r i tte nin the same l a n guage  X  X n e i s o r i g in a lwo rd a n dthe o ther i sge n erated b y mach in etra n s li terat ion. T he b ilin gua l c o mpar i s on ap -l a n guages [3, 6, 8 ]. Ge n era lly, the bas i cframe wo rk o famach in etra n s li terat ion in the b ilin gua l c o mpar i s on appr o ach i ss i m il ar t o each o ther  X  the t wo m o d -pr o aches d ono tsh ow s i g ni fica n td iff ere n ce in perf o rma n ce i fthet wo m o de l s in a target term i s no tused in target l a n guage te x ts .
T here ha v ebee n se v era lW eb -based tra n s l at ion v a li dat ion m o de l s [10, 11, 1 2 , 13]. Qu &amp; Grefe n stette [10] used W eb freque n c y( the n umber o f W eb pages ) t o b o th r o ma nji a n dka nji (o rb o th s o urce term a n d target term ) as a quer y f o r W eb search e n g in es .T he n the y used the n umber o ftheretr i e v ed W eb pages t o in gch i-square (  X  2 ) test in Eq .(1).Fo rs o urce term s a n d target term t , the y descr i bed in Eq .(1)[11,1 2 ,13]. Each parameter in Eq .(1)w as represe n ted b y the n umber o f W eb pages retr i e v ed b y a W eb search e n g in easf ollow s .  X  a : the n umber o f W eb pages c on ta inin gb o th s a n d t ;  X  b : the n umber o f W eb pages c on ta inin g s but no t t ;  X  c : the n umber o f W eb pages c on ta inin g t but no t s  X  d : the n umber o f W eb pages c on ta inin g n e i ther s no r t  X  N = a + b + c + d B ecause W eb search e n g in es ca n usua lly accept Bool ea n quer i es , the f o ur param -eters ( a , b , c , a n d d )in Eq .(1) are o bta in ed b y retr i e vin g W eb pages wi th the f ollowin gf o ur Bool ea n quer i es , X  s t  X ,  X  s  X  t  X ,  X   X  s t  X , a n d  X   X  s  X  t  X , each v a li dat in g w hether target term ( t )i sm o st li ke ly t o be a c o u n terpart o fs o urce term ( s ). How e v er ,w e n eed t ov a li date w hether t i s li ke ly t o be the c o u n terpart o r join t v a li dat ion bet w ee n s a n d t . W eused W eb search resu l ts as a l a n guage res o urce f o re x tract in gtra n s li terat ion pa i rs .W e used a g iv e n E n g li sh term as a quer y f o ra W eb search e n g in ea n d the n e x tracted tra n s li terat ion sc o rresp on d in gt o the E n g li sh term fr o mthe W eb search resu l t .W eb search resu l ts o utput b yW eb search e n g in es usua lly c on ta in aser i es o fs ni ppets 2 o f W eb d o cume n ts retr i e v ed b y the W eb search e n g in es .Ko-rea nW eb pages are usua lly c o mp o sed o fr i ch te x ts in am ix ture o f Ko rea n( ma in descr i pt ion o rtra n s l at ion o f Ko rea n terms in Ko rea n te x ts .Fo re x amp l e , the W eb search resu l tf o rtheE n g li sh term synapse are sh own in Fi g .1.T he W eb search resu l tc on ta in stheu n der lin ed E n g li sh term synapse a n dtheu n der lin ed Ko rea n tra n s li terat ion  X  s i-n aep -seu  X , w h i ch i sthec o u n terpart o f synapse .T he tra n s li terat ion a n d i ts c o rresp on d in gE n g li sh term in the W eb search resu l t in-d i cate that W eb search resu l ts as a s o urce o ftra n s li terat ion pa i rs ca n be v er y usefu l.
Our ca n d i date e x tract ion i s based on atra n s li terat ion b o u n dar y detect ion erat ion sc o rresp on d in gt o a n E n g li sh term .No te that the ca n d i dates are a set in gs o fa n E n g li sh term .L et e = eg 1 , ... ,eg n be a n E n g li sh term c o mp o sed o f n E n g li sh l etters , K S be a set o f Ko rea n s yll ab l es , a n d eg 0 a n d eg n +1 be dumm y l etters represe n t in gthestarta n de n d o f wo rds , respect iv e ly. T he n, ca n d i dates o fthebeg innin ga n de n d in gb o u n dar i es are ge n erated wi th Eq .( 2 ). are s ni ppets in W eb search resu l ts . O n ce the a l g o r i thm rec o g ni zes the b o u n d -a voi dredu n da n ttra n s li terat ion ca n d i dates .T he l e n gth c on stra in tmea n sthata w here n i sthe l e n gth o fa n E n g li sh term ( the n umber o fE n g li sh l etters ) a n d m i sthe l e n gth o fa Ko rea n tra n s li terat ion ca n d i date ( the n umber o f graphemes in atra n s li terat ion ca n d i date ). T he l a n guage c on stra in tmea n sthata ll characters l etters ,n umbers , a n d o ther spec i a l s y mb ol s ).
  X  Ca n d i date E x tract ion X  f o rtheE n g li sh term synapse i ssh own in Fi g . 2 . fi n ds the ca n d i dates represe n ted b y the square brackets ,w here the o pe n bracket represe n ts the beg innin gb o u n dar y a n dthec lo se bracket represe n ts the e n d in g seu  X ) a n d ( synapse , X  s in-g y e on g -se  X ). v a li dat ion m o de l based on the t wo m o de l s i s represe n ted in Eq .(3).In th i s sect ion, w e descr i be the t wo m o de l s in m o re deta il.
 4.1 Phonetic Similarity Model: S PSM L et e = eg 1 ,  X  X  X  ,eg m be a n E n g li sh term c o mp o sed o f m E n g li sh l etters (o r graphemes ) a n d k = kg 1 ,  X  X  X  ,kg l be a Ko rea n term c o mp o sed o f l Ko rea n graphemes .T he ph on et i cs i m il ar i t y m o de l ph on et i ca lly c o mpares e wi th k wi th the assumpt ion that k will be ph on et i ca lly s i m il ar t o e i f k o r i g in ated fr o m e i sest i mated wi th kc i ,w h i ch i sachu n k o f Ko rea n graphemes c o rresp on d in gt o eg i .So Pr ( kg l 1 kc i depe n ds on eg i  X  2 , 4.2 Joint Web Validation Model: S Joint L et e be a n E n g li sh term , K be a set o f Ko rea n tra n s li terat ion ca n d i dates e x-tracted fr o m W eb search resu l ts f o rthequer y e , k i be the i t h Ko rea n tra n s li t -erat ion ca n d i date in K , E i be a set o fE n g li sh ca n d i dates e x tracted fr o m W eb assumpt ion u n der lin ed in the join t W eb v a li dat ion m o de li sthat e will be the at ion pa i r .Wi th th i s assumpt ion, w eapp li ed f o r w ard a n dback w ard v a li dat ion t o e a n d k i , as sh owninFi g .3.Fo r w ard v a li dat ion determ in es  X  h ow li ke ly k i am on ge l eme n ts o f K i st o be a c o u n terpart o f e  X  X  X  t v a li dates ca n d i dates o f parts o f k i , E i , are e x tracted fr o mthe W eb search resu l ts f o rthequer y k i in a s i m il ar w a y descr i bed in S ect ion 3. All E n g li sh terms in the W eb search resu l ts f o rthequer y k i are e x tracted a n dthe n E i i sc on structed wi th the e x tracted E n-a n dEq .( 2 ). T he join t W eb v a li dat ion m o de l ca n be represe n ted wi th Eq .(6), respect iv e ly.

W e o bser v ed that e a n d k te n dt o be c lo se t o gether in te x ts o f W eb pages i fthe y are c o u n terparts o feach o ther , such as  X  s i-n aep -seu  X ( synapse )inFi g .1. To retr i e v esuch W eb pages ,w e used  X  Bilingual Phrasal Search ( BPS ) X , w here phrases c o mp o sed o f e a n d k are used as quer i es f o ra W eb search e n-t ion m o de l s ,w h i ch are based on W eb freque n c y der iv ed fr o m  X  Bilingual Key-word Search ( BKS ) X  [10, 11, 1 2 ,13]. L et e a n d k be a E n g li sh term a n da Ko rea n term , a n d  X  X  ek ] X  o r  X  X  ke ] X  a n d  X  e And k  X  X  r  X  k And e  X  be quer i es f o r BPS a n d BKS, respect iv e ly. T he n, the d iff ere n ce bet w ee nBPS a n d BKS ca n be represe n ted as Fi g . 4 . X  X  ek ] X  o r  X  X  ke ] X  retr i e v es W eb pages w here  X  X  ek ] X  o r  X  X  ke ] X  e xi sts as phrases ;w h il e  X  e And k  X  retr i e v es W eb pages i f e a n d k s i mp ly e xi st in thesamed o cume n t .No te that BKS freque n t ly retr i e v es W eb pages , w here e a n d k ha v e li tt l ec o-re l at ion, because BKS d o es no tc on s i der d i sta n ce bet w ee n e a n d k .W eb freque n c y based on such W eb pages makes i td i fficu l tt o b y app lyin gthec on stra in tthat e a n d k sh o u l d be a phrase in the retr i e v ed W eb pages .T heref o re , the n umber o f W eb pages retr i e v ed b yBPSi sm o re re li ab l ef o r tra n s li terat ion pa i r v a li dat ion.

L et W B PS ( e, k ) be the sum o fthe W eb freque n c i es retr i e v ed b y X  X  ek ] X  a n d  X  X  k represe n tf o r w ard v a li dat ion a n dback w ard v a li dat ion, respect iv e ly. No te that S B PS ( k i ,e )=0i f E i d o es no tc on ta in e .
 5.1 Experimental Setup the n umber o ftra inin gdata w as ab o ut 6,000 a n dthe n umber o fb lin d test data w as ab o ut 1,000. T he test set c ov ered pr o per n ames , tech ni ca l terms , a n dge n era l terms .W etra in ed Eqs .( 2 ) a n d ( 4 )wi th the tra inin gdata .In the e v a l uat ion, Each on e w as used f o r test in g ,w h il etherema in der w as used f o rtra inin g .T he n, set  X  in Eq .( 2 ) t o 0.001 a n dthe n umber o f W eb pages o utput b y a W eb search e n g in e ( used f o r  X  Ca n d i date e x tract ion X ) t o 100 thr o ugh cr o ss -v a li dat ion. W ee x per i me n ted wi th the c on tr i but ion o feach v a li dat ion m o de l( S BK S , S  X  2 , S
B PS , S PSM , S Joint , a n d S TV ) a lon g wi th the e ff ects o feach W eb search meth o d (BPS a n d BKS) on tra n s li terat ion pa i r v a li dat ion. In th i se x per i me n t ,w ef o-S  X  2 [11, 1 2 ,13], w ere c o mpared t oo ur S B PS t o sh ow the e ff ect iv e n ess o f BPS a n d S B PS . S BK S in Eq .( 8 )i ss i m il ar t o S B PS in Eq .( 7 ); but S BK S uses W BK S (W eb freque n c y der iv ed fr o m BKS).
 W em o de l ed  X  2 ( e, k )in Eq .(9)in the same ma nn er as in Eq .(1)[11,1 2 ,13], but i f E i d o es no tc on ta in e .
 W ec o mpare the three W eb -based v a li dat ion m o de l s ( S BK S , S  X  2 , a n d S B PS ) u n der three c on d i t ion s .Fi rst ,w e tested them w he n the f o r w ard v a li dat ion w as used a lon eas in pre vio us wo rk [10,11,1 2 ,13] ( S BK S , S  X  2 , a n d S B PS in T ab l e 1). S ec on d ,w e tested them wi th o ur join t W eb v a li dat ion m o de l( S Joint ( S BK S ), S Joint ( S  X  2 ), a n d S Joint ( S B PS )inT ab l e 1). Fin a lly, w e tested them b y app lyin g S
TV ( S TV ( S BK S ), S TV ( S  X  2 ), a n d S TV ( S B PS )inT ab l e 1). W ee v a l uated the tra n s li terat ion pa i rs in the b lin d test data . 5.2 Results perf o rmed b o th S BK S ( ab o ut 53% in To p -1) a n d S  X  2 ( ab o ut 15 7 %inTo p -1) 3 . T he noi se caused b yBKS made i td i fficu l tf o r S BK S a n d S  X  2 t o c o rrect ly v a li-t ion pa i rs b y us in gre li ab l e W eb pages der iv ed fr o m BPS. S  X  2 a n d S BK S usua lly made err o rs w he n tra n s li terat ion ca n d i date k i appears much m o re freque n t ly tha n the c o rrect tra n s li terat ion in the W eb ; k i had m o re cha n ces t o appear wi th pared t o the case w he nw e used the f o r w ard v a li dat ion a lon e 4 .Mo re spec i fica lly, the perf o rma n ce o f S BK S a n d S  X  2 w as much m o re i mpr ov ed tha n that o f S B PS b y the join t W eb -v a li dat ion m o de l. T he ma in reas on o ftheperf o rma n ce i m -pr ov eme n t w as that w ec o u l d no tfi n dE n g li sh term e in the W eb search resu l ts w as a w r on gpa i r .

M a ny err o rs o f S Joint ( S  X  2 ) a n d S Joint ( S B PS )in the To p -1 w ere caused b y sub -E n g li sh term synapse w ere ra n ked b y each m o de l as f ollow s .No te that the u n der -( X  e  X  X  sap o stp o s i t ion) i s in c o rrect .  X  S  X  2 : X  s i-n aep -seu -e  X (3 rd ),  X  X i-naep-seu X  (15 th )  X  S BK S : X  s i-n aep -seu -e  X (1 st ),  X  X i-naep-seu X  ( 2 n d )  X  S B PS :  X  X i-naep-seu X  (1 st ),  X  s i-n aep -seu -e  X (15 th )  X  S Joint ( S  X  2 ):  X  s i-n aep -seu -e  X (1 st ),  X  X i-naep-seu X  (3 rd )  X  S Joint ( S BK S ):  X  s i-n aep -seu -e  X (1 st ),  X  X i-naep-seu X  ( 2 n d )  X  S Joint ( S B PS ):  X  X i-naep-seu X  (1 st ),  X  s i-n aep -seu -e  X (3 rd ) S in b o th f o r w ard a n dback w ard v a li dat ion. Fin a lly, S TV sh ow ed the best perf o r -ma n ce 5 .A ctua lly, b o th S PSM a n d S Joint b y themse lv es perf o rmed w e ll but a m o re p ow erfu l tra n s li terat ion pa i r v a li dat ion c o u l dbehadb y c o mb inin gthem . S i ca lly d i ss i m il ar t o E n g li sh terms a n dc o u l d no tbed i scarded b y S Joint . 5.3 Error Analysis W ea n a ly zed err o rs caused b y abse n ce o fc o rrect tra n s li terat ion pa i rs in the e x tracted tra n s li terat ion pa i rca n d i dates  X  the err o rs o ccup i ed ab o ut 10% o f test data ( 724 am on g7 ,1 72 ). W edefi n ed t wo err o rt y pes  X  retrieval error a n d extraction error  X  caus in gtheerr o rs .T he retr i e v a l err o r o ccurred w he n there are no tthec o rrect Ko rea n tra n s li terat ion s in the W eb pages retr i e v ed b y a W eb search e n g in e ( e . g . ceiling , chemoreceptor , a n d gibbsite ) X  X  t o ccup i ed ab o ut 6% o ftestdata ( 4 0 2am on g7 ,1 72 ). How e v er ,w efi n dthatm o st o ftheretr i e v a l err o rca n be addressed b yin creas in g snippet size ( the n umber o f W eb pages retr i e v ed b y a W eb search e n g in e ) 6 .T he e x tract ion err o r o ccurred w he n the e v e n th o ugh the c o rrect on ee xi sted in the W eb search resu l t  X  X  t o ccup i ed ab o ut 4 %o ftestdata (3 22 am on g7 ,1 72 ).  X  in Eq .( 2 ) ma inly caused the err o r  X  as  X  in creases , the n umber o fthee x tract ion err o ra l s oin creases 7 .W eca n decrease the e x tract ion err o r i f w eass i g n  X  t olow er v a l ues . W epr o p o sed a nov e l appr o ach t o tra n s li terat ion pa i racqu i s i t ion fr o mthe W eb . Our s y stem retr i e v ed W eb d o cume n ts wi th E n g li sh terms as quer i es f o ra W eb search e n g in ea n dthe n e x tracted ca n d i dates o ftra n s li terat ion pa i rs in the W eb search resu l ts .T he no ur s y stem v a li dated the ca n d i dates b y the c o mb in-ach i e v ed h i gher perf o rma n ce ( ab o ut 8 1% pa i r accurac y) tha n each in d ivi dua l only the f o r w ard v a li dat ion. Fin a lly, BPS ( Bilingual Phrasal Search ) ca n g iv em o re re li ab l e resu l ts tha nBKS( Bilingual Keyword Search ) [10, 11, 1 2 ,13].
 B ecause e x per i me n ts dem on strated that o ur join t -v a li dat ion m o de l based on bet w ee no ther l a n guage pa i rs , such as E n g li sh a n d J apa n ese . Zellig S. Harris, the most influential teacher of Chomsky, wrote  X  X rom Phoneme to Morpheme X  in 1955 [1]. Harris studied language from a computational view-point, and the spirit of his approach was taken up by Chomsky and is seen in Chomsky X  X  well known linguistic theories [2]. In his paper, Harris suggests that morpheme boundaries can be detected by observing the successor count s of phoneme sequences. A successor of an utterance of a given length n is the phoneme that follows the utterance. A successor count is the number of differ-ent successors, and it is obtained by going through many utterances that start with the given utterance. For example, given a short sentence  X  X e is clever X  /hiyzklev are collected and the successor counts are measured. Then utterances coming after /hi/ (such as  X  X it it X ,  X  X e is good X ) are collected and successor counts are measured. This is repeated for /hiy/, /hiyz/, /hiyzk/, and so on. Harris describes what is observed when this successor count shift is monitored: The idea behind Harris X  X  hypothesis is illustrated in Figure 1. (This figure is further explained later in this paper).

When Harris was doing this research, computers were not used as personal tools, nor were there huge bodies of electronic data available. Therefore, he tested his hypothesis by asking people to give as many utterances as possible that began with a certain utterance fragment. This method of asking people directly is the most accurate possible, but limits the scale of any experiment; Harris does not state how many people or how many phrases he considered when evaluating his hypothesis. We now have easy access to relatively powerful computers and large corpora to use as our tool and database, though, so we can test Harris X  X  hypothesis using these means. In this paper, we first mathematically reformulate Harris X  X  hypothesis from an information theoretic viewpoint and then discuss a large-scale evaluation of the reformulated hypothesis.

Harris X  X  hypothesis is interesting because it fills the gap of  X  X ouble articula-tion X   X  first described by Martinet [3]  X  meaning that language is segmented into two different units: phonemes without meaning and morphemes with mean-ing. Precisely, Harris X  X  hyp othesis can generally suggest that this gap is filled, but can be regarded more generally: how a meaningful unit is generated given a sequence. Language can be considered to have multiple layers  X  phoneme, mor-pheme, word, collocation and so on  X  with each layer, except that of a phoneme, formed of a sequence of meaningful units. Harris X  X  hypothesis suggests that each layer is formed as chunks of a smaller layer. For example, if given a morphomeme sequence, we should be able to scan through the successor counts of morpheme sequences and find that the peaks are correlated with word boundaries. Indeed, studies in NLP have found that Chinese, which is written as ideogram (mor-pheme) sequences, can be segmented into words based on an hypothesis similar to that of Harris with a precision as high as about 90% [4] [5]. Another example is that for a given word sequence, scanning through the successor counts of the word sequence should reveal peaks correlated with collocation boundaries. This has been tested for collocation extraction [6], and has been applied in an appli-cation tool [7]. Thus, Harris X  X  hypothesis can be generalized as a law governing language, and for segmenting out a meaningful unit at various levels. To date, however, the hypothesis has not been tested at this most basic level of phoneme to morpheme or phoneme to word using any of the corpora now widely available. Therefore, we have chosen to scientifically verify the extent to which the Harris hypothesis holds. The formulation presented here is exactly the same as the one which appeared in [8].

The successor count can be considered a measure of the complexity of the suc-cessor. This can be modeled through information theory. Given a set of elements  X  and a set of n-gram sequences  X  n formed using  X  ,the branching entropy of an element occurring after a given n -gram sequence X n = x n is defined as where P ( x )= P ( X = x ), P ( x | x n )= P ( X = x | X n = x n ), and P ( X = x ) indicates the probability of x occurring. The last term in this formula,  X  log P ( x | x n ), indicates the information of a token of x coming after x n , and thus the branching after x n . H ( X | X n = x n ), the local entropy value for a given x n , indicates the average information of branching for a specific n-gram sequence x n . For simplicity, we denote H ( X | X n = x n )as h ( x n ) for the rest of this paper.
Harris X  X hypothesissaysthatforanygiven x 1 ,..., x i , x i +1 ,where x i forms the prefix of x i +1 , h ( x i ) repeatedly falls and rises for i = 1,. . . , and the peak points are the boundaries of an element larger than the elements of  X  . These falls can be explained in relation to another universal nature of language, H ( X | X n ), defined as This H ( X | X n ) is the average of h ( x n ); i.e., the average uncertainty of a successor for any subsequence of length n . For language data, it is known that the larger n becomes, the smaller H ( X | X n ) will be. For instance, in the case of the word  X  X atural X , it is easier to guess which character comes next given x 6 = X  X atura X  than x 1 = X  X  X . This fact means it is more effective and more natural to consider all increasing points rather than just peak points. Therefore, the hypothesis to be tested in this paper is That is, we consider places at n where
This hypothesis (X) differs from Harris X  X  hypothesis in two ways:  X  Harris uses the successor variety count, but we use branching entropy.  X  Harris uses the maximum points, but we use increasing points.
 3.1 Procedure To verify (X), we need test data and training data. As we want to apply (X) to two cases,  X  X rom phoneme to morpheme X  and  X  X rom phoneme to word X , a gold standard  X  X ata indicating the supposedly true segmentation boundaries X  for word boundaries and one for morpheme boundaries are required for the test data.

The test data was first segmented at punctuations. These text segments are called fragments in this paper. Both the test data and the training data were then transcribed into phoneme sequences. Further details of the input data are explained in  X  4.1 and  X  5.1.

From this transcribed training data, we measured the branching entropy for all phoneme subsequences in the test data. As a subsequence becomes longer, the entropy approaches zero. Therefore, we set a maximum length value maxlen and considered only subsequences shorter than maxlen . For all of these subsequences, we obtained branching entropies from the training data.

Each fragment was then processed as follows to obtain the boundary, where x m,n indicates a subsequence of a given sequence x from offset m until n . 1. Set m =0, n = m +1. 2. Calculate h ( x m,n ). 3. Compare the result with h ( x m,n  X  1 ). If h ( x m,n )  X  h ( x m,n  X  1 ) &gt; threshold , 4. If n&gt;m + maxlen ,then m = m +1, n = m + 1. Otherwise, set n = n +1 The threshold is an arbitrary threshold that we varied during our evaluation. This procedure is illustrated in Figure 1 for the case where m is fixed to a certain offset and n is successively increased in step 4 of the procedure. This algorithm is the simplest algorithm to test (X), where we just scan through all phoneme subsequences of length less than maxlen .

So far, we have considered only regular (forward) order processing: the branch-ing entropy is calculated for successive elements of x n . We can also consider the reverse order, which involqves calculating h for the previous element of x n .Inthe case of the previous element, the question is whether the head of x n forms the beginning of a context boundary. The above algorithm can then be applied in re-verse. Even though language linearity suggests asymmetry between regular and reverse orders, we also apply the thus defined process in the reverse (backward) order, as reverse order processing was also suggested by Harris [1]. 3.2 Precision and Recall The output was evaluated in comparison with the gold standard. We calculated the precision and recall to test both the  X  X rom phoneme to word X  and  X  X rom phoneme to morpheme X  cases. Precision and recall were defined as N correct is the number of correct boundaries in the result, N test is the number of boundaries in the test result, and, N true is the number of boundaries in the gold standard.
 For example, if the boundary in the gold standard is  X  X bc | def | ghi | jk X , with a | indicating a boundary, there are three boundaries. Suppose that we obtained a result of  X  X b | cd | ef | ghi | jk X , and two of the four reported boundaries are correct. The precision is then 50% (= 2 4 ) and the recall is 67% (= 2 3 ).
Note that as word boundaries are all included among the morpheme bound-aries, the precision is always better for morpheme results than for word results because the numerator is larger for morphemes. 4.1 Data We used 100 MB of data from the Wall Street Journal corpus as training data and 1 MB as test data. English text was transformed into phoneme sequences by using the CMU Pronouncing Dictionary [9]. Some examples of text and its tran-scription by CMU pronunciation description are shown in Table 1. A phoneme is described as a sequence of capital letters possibly followed by a number. There are 39 phonemes used in the CMU dictionary, which contains various words and their variants. If a word in a fragment (see  X  3.1) was not included in the CMU dictionary, we eliminated the whole fragment. Such eliminated fragments amounted to 23.4% of the test data, so the actual amount of test data was 766 Kbytes. Note that after the English was transformed into phoneme sequences, NO SPACES to indicate the word boundaries remained. Since punctuation was eliminated when we used fragments, the input test data uniformly consisted of phoneme sequences. The constant maxlen was set to 10.

Word boundaries in the original English text were considered the gold stan-dard. However, it was difficult to obtain a complete set of morphemes in English and the only publicly available means to do so was the PC-KIMMO analysis tool [10]. However, this software works only for English text. As the CMU dictionary does not provide phoneme-to-text alignment, it would be difficult to automati-cally create a gold standard for 766 K of phoneme sequence data. Therefore, we went through the following procedure to obtain a very small gold standard for morphemes. 1. Randomly obtain 50 fragments containing only words registered in PC-2. Process each fragment with PC-KIMMO and get the morpheme boundaries. 3. By looking at this result from PC-KIMMO, manually put morpheme bound-4.2 Small Examples Before going through our full-scale experiment, here we show that hypothesis (X) holds for a small example.

Figure 2 and Figure 3 each show an actual graph of the entropy shift for the input phrase  X  X H EH1 S CH ER0 AH1 V G UH1 D F EY1 TH X  ( gesture of good faith ). The former shows the entropy shift for the forward case, and the latter shows the entropy shift for the backward case. Note that for the backward case, the branching entropy was calculated for phonemes before the x n .

In Figure 2, there are two lines, one for the branching entropy after the sub-strings starting from JH. The leftmost line plots h (JH), h (JH EH1) ... h (JH EH1 S CH ER0 AH1 V). There are two increasing points, indicating that the phrase was segmented between ER0 and AH1 and after V. The second line plots h (V) ... h (V G UH1 D F EY1 TH). The increasing locations are between D and F, and after TH.

Figure 3 shows the entropy shift for the backward case. There are two lines, indicating the branching entropy before the substring ending with suffix TH. The rightmost line plots h (TH), h (TH EY1) ... h (TH EY1 F D UH1 G V) running from back to front. We can see increasing points (as seen from back to front) between F and D and between G and V. As before, the leftmost line starts from V and runs from back to front, indicating boundaries between AH1 and ER0, and just before JH.

If we consider all the increasing points in the two forward and backward lines and take the union set of them, we obtain the correct segmentation as follows with | indicating the boundaries: which is the 100% correct word segmentation in terms of both recall and preci-sion. For this example, no morpheme boundaries were detected.

In fact, as there are 13 phonemes in this input, there should be 13 lines starting from each phoneme for all substrings. For readability, however, we only show two lines each for the forward and backward cases and set the maximum length of a line to 7, in experiments the length should be 10 because we took 10-grams out of the learning data. If we consider all the increasing points in all 13 lines and take the union set, then we again obtain 100% precision and recall. We find it amazing that all 13 lines indicate only correct word boundaries. 4.3 Larger-Scale Performance Precision and recall were plotted by changing the threshold from 0.0 to 2.4, with an interval of 0.2 (Figure 4). Two lines are shown: one for the  X  X rom phoneme to word X  case, and the other for the  X  X rom phoneme to morpheme X  case.

For the word result, we obtained an F-score of 86.1% (precision=81.2%, re-call=91.5%) at a threshold value of 1.6. Note how high this is: the input is a plain phoneme sequence without any spaces, yet the word boundaries were de-tected at 85%. At the highest threshol d of 2.4, the precision was 89.2% with recall of 70%.

The 18.8% (=100-81.2) of boundaries which were not word boundaries in-cluded morpheme boundaries. The F-score for morpheme boundaries at a thresh-old of 1.6 was 80.4% (precision=90.5%, recall=72.3%). Thus, almost half were correct in-word morphemes (90.5-81.2=9.3% which is the half of 18.8%). As mentioned in  X  3.2, for a given threshold, the precision of the morpheme result is always higher than that for the word result; therefore, this performance decrease was due to the recall decrease.

This was partly due to limitations of the vocabulary in the corpus. For ex-ample, the word  X  X bsorb X  contains two morphemes  X  X b X  and  X  X orb X , but when the corpus only has  X  X bsorb X  as the word with the prefix  X  X b X , the boundary after  X  X b X  is missed. However, we consider this phenomenon to also be due to the nature of English in that the English language puts less emphasis on the morpheme as a unit compared with other languages such as Chinese, where morphemes appear more explicitly in the language system. Therefore, we next tested (X) on the Chinese language. 5.1 Data The training data for Chinese was 200 MB of unsegmented text taken from the Contemporary Chinese Corpus of the Center of Chinese Linguistics at Peking University. The test data was the 7.8 Mbyte manually segmented People X  X  Daily corpus of Peking University [11]. Chinese is a suitable test language for testing (X) because the phonetic standard pinyin forms a good approximation of a phoneme sequence. Thus, Chinese text is converted into pinyin by using NJstar [12], a Chinese word processor software application, and the tonal number is eliminated. Pinyin can then be automatically separated into what corresponds to phonemes. For example, the Chinese pinyin  X  X i ran yu yan X  (natural language) is decomposed into phonemes as  X  X  i r a n y u y a n X . Some special cases in Chinese pinyin had to be carefully processed during this decomposition. For example, the pinyin  X  X  X  following  X  X  X ,  X  X  X , or  X  X  X  is actually pronounced as  X  X  X , so we decomposed  X  X u X ,  X  X u X , and  X  X u X  into  X  X  v X ,  X  X  v X , and  X  X  v X , respectively. In the case of  X  X i X ,  X  X in X ,  X  X ing X , and  X  X u X , the  X  X  X  and  X  X  X  are not pronounced, so  X  X  X  and  X  X  X  were eliminated. The constant maxlen was set to 12.

As the test data was manually segmented into words, they were considered the gold standard for words. As for the morphemes, ideogram boundaries were considered the gold standard. 5.2 Results Our results are shown in Figure 5. The precision and recall were plotted by changing the threshold of the boundary detection condition from 0.0 to 2.4, with an interval of 0.2. Again, we show two lines, one corresponding to the word results and the other to the morpheme results.
For morphemes, we obtained an F-score of 79.4% (precision=76.7%, recall=82.4%) at a threshold value of 1.2, which was similar to the English morpheme case. Therefore, from phoneme to morpheme, (X) seems to hold with an 80% F-score for both English and Chinese.

For words, however, we obtained an F-score of 66.9% (precision=54.7%, re-call=86.1%) at a threshold value of 1.6. Comparing the two graphs, the word result can be obtained by shifting the morpheme result towards the left. This means the precision decreased, while the recall was similar for each threshold. At a threshold of 1.6, 42.8% (=100 -55.3) of the places indicated were not word boundaries and 20% (precision is 83.5, 83.5-54.7 = 19.8) of these were correct morpheme boundaries. This is a drastic difference from the English case, which warrants discussion. We account for the above difference from English as follows. From phoneme to morpheme, the performance was the same for Chinese and English: recall and precision both being about 80%. However, there was a drastic difference for the word boundaries. For English, precision and recall was more than 80%, but precision decreased to 57% for Chinese, with half of the remaining 43% containing morpheme boundaries.

This, in fact, shows an important role morphemes play in Chinese. Chi-nese language is formed with the unit being ideograms and the pinyin for each ideogram forms a syllable. In this language, morphemes are far more explicit than in English. Every word is formed as a true combination of ideograms. Therefore,  X  X rom phoneme to morpheme X  holds only for  X  X orpheme X  and word boundaries are formed at a higher level of  X  X rom morpheme to word X . In our previous work where we applied (X) to the case  X  X rom morpheme to word X  [4], the F-score was 83%, with precision of 88% and recall of 79%. In contrast, English had a higher result for the  X  X rom phoneme to word X  case. This suggests that morphemes are less explicit as units, so word units should be formed directly  X  X rom phoneme X .
How correct was Harris? Harris stated that the cuts accord very well with the word boundaries and quite well with the morpheme boundaries of that utterance . For morphemes, we obtained F-scores of about 80% for both English and Chi-nese, indicating that the morpheme boundaries were detected quite well and that (X) is valid in this case. With regard to words, it depended on the language. For English, the F-score was about 85%, with recall being above 90%, which indicates the word boundaries were detected very well . For Chinese, though, (X) does not hold and words seem to be formed  X  X rom morphemes X . Therefore, Harris was probably unaware that his hypothesis not only applies for double articulation, but also applies more generally: that the hypothesis (X) might be a law which can be used to segment out a larger meaningful unit from a smaller unit chain. We scientifically verified the validity of Harris X  X   X  X rom phoneme to morpheme X  hypothesis using a large-scale corpus. Harris X  X  hypothesis is that morpheme/word boundaries can be detected from changes in the complexity of phoneme se-quences. After re-formulating Harris X  X  hyp othesis from an information theoretic viewpoint, we did a large-scale experiment on sequences in English and Chinese. Harris stated that morpheme boundaries could be detected  X  X uite well X  under his hypothesis, and we confirmed this with an F-score of about 80%. However, Harris also stated that word boundaries could be detected  X  X ery well X ; although we confirmed this for English, with an F-score of 85%, we could not confirm it for Chinese. This result suggests that Chinese words are constructed from morphemes rather than from phonemes.
 As a spec i al form of unknown words, Ch i nese abbrev i at i ons are frequently used w i th-(NLP), espec i ally for agglut i nat i ve languages such as Ch i nese, i n that the problem i s exacerbated by the lack of word boundar i es . How to i dent i fy abbrev i at i ons 1 becomes a common problem w i th i n Ch i nese word segmentat i on, Ch i nese co-reference resolu-however, there l i es the above-ment i oned more bas i c problem : How to retr i eve abbre-v i at i ons w i th i n agglut i nat i ve Ch i nese texts? 
To a large extent, the success of i dent i fy i ng Engl i sh abbrev i at i ons 2 goes to two as-Chang (2004), who presented a h i dden Markov model (HMM) based approach for st i ll of necess i ty i n the task .
 word classes and therefore create abbrev i at i on-templates to meet our a i m of i dent i fy-i ng new abbrev i at i ons from ex i st i ng ones .
 pans i on . The extended tasks, however, are not the ma i n focus of th i s paper .
The rest of th i s paper i s organ i zed as follows . In next sect i on, the system arch i tec-and i mprovement solut i ons are respect i vely presented . The rema i nder of th i s paper i s exper i ment results and conclus i on . descr i bed i n Gao (2003), we def i ne Ch i nese words i n th i s paper as one of the follow-cally der i ved words, (3) facto i ds, and (4) named ent i t i es, because these four types of essed i n d i fferent ways i n our system .
In th i s paper, we made the follow i ng assumpt i ons about abbrev i at i ons : abbrev i at i on conta i n i ng more than f i ve characters i s very rare (less than 0 . 1%) . (2) An abbrev i at i on conta i ns no lex i con words (otherw i se i t w i ll be segmented dur-shows that th i s k i nd of abbrev i at i on i s few (about 4%) .
 replaced w i th tags (e . g . ,  X 12 : 30  X  w i ll be replaced by  X  X _TIME  X . ) . In our study, word segmentat i on i s employed for two reasons : F i rst, based on abbrev i a-cand i dates w i ll be collected from unknown character sequences, and word segmenta-t i on i s requ i red for the retr i eve of unknown sequences . Second, word segmentat i on i s 
After unknown sequences are generated dur i ng word segmentat i on, search i ng ab-stra i nts, i t w i ll be collected as an abbrev i at i on cand i date .
An abbrev i at i on cand i date may also relate to (be a sub-sequence or super-sequence i n our system . S i nce a range of weak ev i dence must be comb i ned i n order to make a v i at i on d i samb i guat i on w i ll be presented i n next sect i on .
The system overv i ew i s i llustrated i n F i g . 1 . As we can see,  X   X  i s marked model, and eventually only  X   X  (abbrev i ated from  X  X lymp i c Games  X  ) i s class i f i ed as  X  X bbrev i at i on  X . We use the SVM (support vector mach i nes) model to d i samb i guate abbrev i at i on can-eled by us i ng a class-based language model) and context i nformat i on analys i s . 3.1 Abbrev i at i on Format i on Analys i s The abbrev i at i on format i on analys i s i s based on the assumpt i on that a Ch i nese abbre-v i at i on i s generated as follows : F i rst, a person chooses a sequence of concepts to set up the concept structure of the abbrev i at i on; then the person attempts to express each common concept structure . E . g . ,  X  X hangha i -F i lm-Stud i o  X  and  X  X e i -+ category/ i ndustry + ent i ty-postf i x  X . Therefore, although new abbrev i at i ons are con-class  X  or s i mply  X  X haracter class  X  ) .
 Word Class of Abbrev i at i ons study . We use a stat i st i cal language model to est i mate the probab i l i ty word sequence i s to model i t as a product of b i gram probab i l i t i es the probab i l i ty us i ng classes we arr i ve at the follow i ng probab i l i ty model : perform a max i mum-l i kel i hood est i mat i on : our system .
 i n the format i on of abbrev i at i ons .  X  result i ng word classes . As i llustrated, most of these entr i es are  X  X ocat i on name  X . Abbrev i at i on Format i on Features Three features are used for abbrev i at i on format i on analys i s : Abbrev i at i on format i on score : For an abbrev i at i on cand i date abbrev i at i on format i on score w i ll be est i mated by us i ng : Where (Katz, 1987) . abbrev i at i on produced tends to be too short, because shorter i tems tend to get h i gher probab i l i ty based on the language model .
 Numer i c i nformat i on : Although facto i ds have been normal i zed dur i ng preprocess-date as well as the i r character pos i t i on :  X  X EGINNING  X  ,  X  X IDDLE  X  , or  X  X ND  X . 3.2 Context Informat i on Analys i s In our study, two features of context i nformat i on are adopted to i mprove abbrev i at i on i dent i f i cat i on : cand i date . Note that the contextual words are amb i guous when both s i des of th i s abbre-i n many cases such no i se w i ll occur only once on the document level . der i ved abbrev i at i ons are neglected . Second, exper i ments showed that some sub-we w i ll prov i de solut i ons to the three problems respect i vely . 4.1 Morpholog i cal Analys i s As descr i bed i n Gao (2003), the morpholog i cally der i ved words are generated us i ng 4 part i cle( i. e . express i ons that are verb+comp) :  X  X alk  X  +  X  X ut  X  -&gt;  X  X alk out  X  ; (3) redupl i cat i on :  X  X appy  X  -&gt;  X  X app i ly  X  ; and (4) merg i ng : solve those four morpholog i cal patterns : For morpholog i cal pattern of aff i xat i on, head gested i n Gao (2003) . On the other hand, morpholog i cal pattern of merg i ng i s a spe-and a new feature i s employed :  X  X RUE  X  i f both AC and BC are proved be i ng lex i con words and  X  X ALSE  X  otherw i se . abbrev i at i on merged from two lex i con words, namely AC and BC . 4.2 Substr i ng Analys i s abbrev i at i ons . For i nstance, the substr i ng  X   X  of the abbrev i at i on  X   X  i s shar-i ng the same conceptual structure w i th another factual abbrev i at i on  X   X :  X  X ate-between a factual abbrev i at i on and i ts substr i ngs .
 and  X  X  i ght-m i n i mum super-sequence  X  (RMS) : In the consecut i ve sequence of ABCD, ABC i s the LMS for BC and BCD i s the RMS for BC . For the overall occurrences of for the overall occurrences of a substr i ng, e i ther the i r LMSs or RMSs tend to keep the denotat i on . Therefore, i t i s poss i ble to develop a rule comb i n i ng LMSs and RMSs for d i scr i m i nat i ng real abbrev i at i ons from the i r substr i ngs : Super-sequence d i vers i ty feature : Formally, th i s feature i s scored by us i ng Div(A) , d i vers i ty : count(x) returns the overall occurrence-number of x on the document level . 4.3 Named-Ent i ty Ident i f i cat i on employed i n our system to i dent i fy CNs and FNs i ns i de the unknown sequences . Ch i nese person names : As descr i bed i n Gao (2003), Ch i nese PN cons i sts of a fam i ly G character were also stored i n a g i ven name l i st .
 Transl i terated fore i gn names : As descr i bed i n Sproat (1996), FNs are usually transl i t-Fortunately, there are only a few hundred Ch i nese characters that are part i cularly com-
It should be emphas i zed that there i s no feature d i mens i on i ncrease dur i ng i ntegrat-i ng named-ent i ty i dent i f i cat i on : i t i s employed only for cand i date prun i ng . The selected data conta i ns 20,063 sentences from 4,769 documents, wh i ch are d i v i ded 4,941 . The abbrev i at i on tokens w i th i n the corpus have been manually annotated . The or i g i nal corpus i s already segmented, and i n order to get unsegmented raw corpus we removed from the lex i con .

The SVM model i s employed for abbrev i at i on d i samb i guat i on . The commonly SVM model calculates separat i ng hyperplanes that max i m i ze the marg i n between two rators, kernel funct i ons can be used to calculate scalar products i n h i gher d i mens i onal spaces .

To evaluate the performance of our system, we use F-measure . Based on the prec i -s i on P and the recall R, the F-measure i s def i ned as follows : 5.1 Compar i son of Kernel Funct i ons i nterest i ng to note that the l i near kernel outperforms the Gauss i an RBF kernel as well as the polynom i al kernel, w i th the f i nal F-measure of 73 . 7% . The reason m i ght be the 
In order to deal w i th data sparseness problem, we d i scard those features occurr i ng only once i n the tra i n i ng data .
 5.2 Improvement Evaluat i on We conducted i ncrementally the follow i ng four exper i ments : w i th context i nformat i on analys i s, wh i ch i s selected as our basel i ne performance; (2) Integrat i ng the feature of morpholog i cal analys i s (MA) i nto (1); (3) Integrat i ng the feature of substr i ng analys i s (SA) w i th (2); (4) Integrat i ng NE i dent i f i cat i on (NEI) w i th (3) .
 Both MA and SA w i ll br i ng new features i nto our SVM class i f i er, wh i le NEI i s used Table 2 . As can be not i ced, our basel i ne model reaches the F-measure of 64 . 0 % . the i mprovement .
 nately some abbrev i at i ons occur only once throughout i ts context so that they can not employed by SA .

In our basel i ne model, the set of abbrev i at i on cand i dates i s large, and merely about 1/19 of them are real abbrev i at i ons . Thereby, the cand i date prun i ng performed by NEI Corpus i s a news corpus, wh i ch tends to use a large number of Ch i nese person names and transl i terated fore i gn names .
 In th i s paper, we proposed a superv i sed learn i ng approach for automat i c abbrev i at i on exper i mental result upon the People  X  s Da i ly Corpus .

Moreover, add i t i onal exper i ments further demonstrate the i mprovement after i nte-i ntegrat i ng NEI .

In our future work, we w i ll focus on f i ne-tun i ng our abbrev i at i on format i on model examples and negat i ve examples, so that we can i mprove the qual i ty of word classes and therefore generate more d i scr i m i nat i ng abbrev i at i on-templates . We would l i ke to thank Galen Andrew for helpful suggest i ons on i mplement i ng word paper .
 Word frequencies play important roles in many NLP-related applications, for example, TF in information retrieval. The estimation of word frequencies is easy for English whereas difficult for Chinese, because unlike English, there isn X  X  spacing to explicitly delimit words in Chinese text. We therefore can not obtain word frequencies by simply counting word token occurrences in raw corpora. Generally speaking, we need a  X  X erfect X  (or, correct) manually word-segmented Chinese corpus to estimate word frequencies [5]. However, we face two fundamen-tal difficulties. The first is that there exists serious inconsistency within/among manually segmented corpora, even wh en the same segmentation standard is adopted for annotation. Due to the characteristics of Chinese word-formation [2,1], it is very tough to construct a  X  X ully X  correct manually segmented corpus, although the definition of  X  X ord X  [14,13,11] seems very clear from the linguistic perspective. For example, a constituent,  X   X , we can either consider it as a compounding word, pork , or consider it as a phrase consisting of two single-character words  X   X (pig) and  X   X (meat). Thus, the word frequency of  X   X  could be pretty high if it is treated in the corpus in the former way, and could also be zero if treated in the latter way. The second difficulty is, according to Zipf X  X  law, in order to obtain a statistically reliable word frequency estimation, even for a medium-sized Chinese wordlist, a balanced corpus with several hun-dred million characters, rather than several million characters, is required. But constructing such a huge manually segmented corpus is almost impossible,  X  it is both labor-intensive and time-consuming.

Since a  X  X erfect X  manually segmented corpus is not feasible, (although the  X  X mperfect X  manually segmented corpus is obviously useful for word frequency estimation), we have to in addition consider the possibility of making use of the following three types of corpora for the task here:
The first type is  X  X erfect X  automatically segmented corpus : Use a  X  X erfect X  word segmenter to segment the corpus automatically, leading to a  X  X erfect X  automat-ically segmented corpus. Then word fre quencies can be easily estimated based on the corpus. Clearly, it would be ideal if a very powerful word segmenter is available [7]. Unfortunately, the state-of-the-art Chinese word segmenters are not satisfactory in performance. In the First International Chinese Word Segmen-tation Bakeoff in 2003 [8] organized by SIGHAN, the highest F-scores for word segmentation in the open test on four small-scale corpora were 95.9%, 95.6%, 90.4% and 91.2%, respectively. In the Second SIGHAN International Chinese Word Segmentation Bakeoff [3], the situation remains unchanged in nature, de-spite the minor increase in performance of word segmentation. A side-effect of such systems is that they try to solve segmentation ambiguities and recognize unknown words in context, producing a lot of unexpected inconsistencies in seg-mentation, which are obviously not favored by the task here.

The second type is MM-segmented corpus : Use  X  X aximal matching X (MM), the most basic method for Chinese word segmentation, to segment the corpus au-tomatically, then obtain the approximated word frequencies from the resulting corpus. [7] first used MM to handle large-scale texts. According to the direc-tion of sentence scanning, MM can be further sub-categorized as forward MM (FMM) and backward MM (BMM). Experiments in [4] showed that MM is both effective and efficient (fast and easy to implement). [10] distinguished four cases in which FMM and BMM were both considered, and it provides a very strong evidence for supporting MM-based schemes to be reasonable estimations of word frequencies. Another advantage of MM-based schemes is their high consistency in word segmentation. The weak point of MM is that segmentation errors in-evitably exist and when out-of-vocabulary words exist, the performance of MM will drop severely.
The third type is raw corpus : Use the frequency of a string of characters as an approximation (notice that we use the term  X  X pproximation X  here) of the word frequency of a constituent [9], which can be derived directly from any raw corpus. Obviously, its value is always larger than the value of word frequency for any word given a corpus. This scheme may over-estimate word frequencies seriously for some words (in particular for mono-syllabic words), but it has two good properties: the first one is that it is free from any kind of word segmentation errors; the second one is that this kind of corpus can be easily obtained and the size can be arbitrarily large.

According to the analysis above, for the task of word frequency estimation, a  X  X erfect X  word-segmented corpus is ideal but, it doesn X  X  exist, either manu-ally or automatically -what we have are a variety of imperfect ones as well as raw corpora. Each type of corpora has its own advantages and drawbacks, so neither of them alone can fit the task of word frequency estimation. We have to consider a trade-off strategy which tries to utilize all the imperfect word-segmented corpora available so far, ranging from manually segmented corpora, MM-segmented corpora to raw corpora, and combine them to do sort of word frequency approximation, instead of word frequency estimation.

The remainder of this paper is organized as follows: Section 2 introduces the data set we used throughout the paper; Section 3 proposes the construct process of our trade-off scheme; Section 4 presents experiments to show the performance of the proposed scheme. And Section 5 concludes our work. In this section we introduce the corpora we used in our experiments throughout the paper.

First, two manually word-segmented corpora: The first one is the HUAYU corpus consisting of 1,763,762 characters, constructed by Tsinghua University and Beijing Language and Culture University. The second one is the BEIDA corpus consisting of 15,839,323 characters, constructed by Peking University. So the manually word-segmented corpora have totally 17,603,085 characters.

Second, the golden-standard corpus: We use a manually word-segmented corpus constructed by the National Institute of Applied Linguistics, denoted YUWEI, which contains 25,000,309 words with 51,311,659 characters. As the YUWEI corpus is sort of a noted authority and relatively large in size, we take it as golden-standard for our tests. An original wordlist is obtained from this corpus and the corresponding word frequencies can be obtained. We delete the words with frequency less than 4 from the original wordlist to form our final wordlist, which is denoted YWL and contains 99,660 entries.

Third, a raw corpus: We use a very large raw corpus, denoted RC, which contains 447,079,112 characters. Taking YWL as the wordlist, we obtain the frequency of a string of characters for each word from RC.
 Fourth, MM-segmented corpora: In terms of YWL, we segment the raw corpus RC with FMM-segmenter and BMM-segmenter separately, resulting in two MM-segmented corpora.We denote them RC FMM and RC BMM respectively.

Thus in total, we have two moderate size manually-segmented corpora (HUAYU and BEIDA), one very large raw corpus (RC), two MM-segmented corpora (RC FMM and RC BMM), and a golden-standard corpus (YUWEI). In this section we propose our trade-off scheme. In order to properly combine the five corpora which are of different size and different types, the combining process is organized in three steps. Firstly we combine the raw corpus and the two MM-segmented corpora. Secondly we combine the two manually segmented corpora. At last we combine the above two results and obtain the final approximation scheme. In the following, we introduce this step by step. 3.1 Combining Raw and MM-Segmented Corpora From each of the three corpora: the raw corpus and the two MM-segmented corpora, we can obtain word frequency for each word w i ( i =1 , 2 ,..., 99660), respectively. We use the following symbols to clarify further descriptions:
The work of [12] indicates that in the framework of MM, the average of f
FMM ( w i )and f B MM ( w i ) gives the best approximation of word frequencies for 1 to 4 character words. f B MM ( w i ) is the best for 5 characters words, and f
RAW ( w i ) the best for words with word length 6 or above. We simply follow this conclusion here.

Using F RF B ( w i ) to represent the result of word frequency approximation by jointly considering RC, RC FMM and RC BMM, we have: For words with 1-4 characters: For words with 5 characters: For words with 6 or more than 6 characters:
This word frequency approximation scheme is called RFB. 3.2 Combining Manually Segmented Corpora Having two manually segmented corpora HUAYU and BEIDA, we can obtain the word frequency for each word w i in YWL. We denote the word frequency of ( w i ) derived from these two corpora f HUA ( w i )and f BE I ( w i ) respectively. We simply take the sum of the two values as the result of word frequency approximation in terms of these two manually segmented corpora, denoted F H B ( w i ):
This word frequency approximation scheme is called HB. 3.3 Combining F RF B ( w i )and F HB ( w i ) With two parts of combining results F RF B ( w i )and F H B ( w i ), we come up with two problems:
The first one is, these two results come from the corpora with different sizes which are extremely unbalanced: one (HUAYU+BEIDA) is 17,603,085 charac-ters while the other (RC) is 447,079,112 characters. So we can not directly com-bine the results from these two corpora. We thus need to introduce a parameter  X  to balance the corpus size.

It is na  X   X ve that we just take the ratio value of the two corpora size as the value of  X  . Later on, we will adjust  X  through experiments to receive the most appropriate value. At this stage, we just take the size ratio as the  X  value, so  X  =25.4.

We use C 0 to denote the total number of characters of the manually segmented corpora (HUAYU+BEIDA), and let C 1 denote the total number of characters of raw corpus (RC).

We expect to integrate the manually segmented corpora (HUAYU+BEIDA) and the raw corpus (RC) into a  X  X ew X  corpus of size 2 C 0 .ThusthesizeofRC will be reduced to C 1 /  X  . Accordingly the word frequency F RF B ( w i ) should be changed to F RF B ( w i ):
In order to keep the whole corpus size to be 2 C 0 after integration, the final manually segmented corpus size, denoted C 0 , should be: Thus the word frequency F H B ( w i ) will in turn become F H B ( w i ):
The second problem concerns an observation in Chinese, i.e., the smaller the word length is, the less reliable the approximation obtained from the raw corpus, and thus the larger the weight of the approximation result from F H B ( w i ) should be. Here, we use a factor  X  as the weighting parameter. Experimentally, we set  X  as follows:
Taking the above two problems into consideration, and based on Equation 5 and Equation 7, the final word frequency approximation in terms of RC, RC FMM and RC BMM can be represented as:
Correspondingly the final word frequency estimated by HUAYU and BEIDA should be: Thus we get our final trade-off strategy, denoted F RF B + H B ( w i ): Note that for 4 or more than 4 characters words,  X  =0, thus in these cases Equation 10 reduces to Equation 11:
This word frequency approximation scheme is called RFB+HB. In order to evaluate the performance of our trade-off scheme (RFB+HB), we conducted experiments from different perspectives. We compare this scheme with the other two schemes: the first one is the scheme using raw corpus and MM-segmented corpora (RFB), the second one is the scheme using only manually-segmented corpora (HB). Following experiments focus on these three schemes. 4.1 Perspective 1: The Spearman Coefficient of Rank Correlation In terms of word frequencies derived from YUWEI, we can obtain a rank se-quence for the 99,660 entries of YWL, denoted R YW , which is in descending order of word frequencies. Similarly, we can also obtain a rank sequence for all these entries in terms of each of F H B ( w i ), F RF B ( w i )and F RF B + H B ( w i ), denoted R
H B , R RF B and R RF B + H B respectively. Every word w i in YWL has its own rank numbers in R H B , R RF B and R RF B + H B .Weassigntheseranknumbersto w ,with R YW as a fixed index, resulting in three new rank sequences, denoted R Then we calculate the closeness between R YW and each of R H B ( w i ), R
RF B ( w i )and R RF B + H B ( w i ), with R YW as the standard rank sequence. We use the Spearman coefficient of rank correlation (SCRC) to measure the closeness between a pair of rank sequences over YWL, as given by: where d i is the difference between two rank numbers of w i with respect to R YW and R , N is the length of YWL, and R is R H B , R RF B or R RF B + H B .Table1 shows the values of SCRC( R YW ,R ), under  X  =25 . 4.

The SCRC value of the proposed scheme is the biggest among the three, indicating that the rank sequence R RF B + H B is the closest to R YW compared to the rank sequences R H B and R RF B .

We also conduct an experiment to determine the most adequate value for  X 
Fig.1 shows that  X  =6 . 5 receives the highest SCRC value. So in the later experiments, we fix  X  =6 . 5.

To further observe the performance of the proposed scheme, we continue to carry out some experiments on subsets of YWL. Table 2 and Table 3 give the SCRC values for the top part of YWL with word frequencies  X  10 and  X  200 respectively.

In these two cases, the proposed scheme also outperforms the other two schemes.

The improvements of the proposed scheme compared to the other schemes over YWL under word frequencies  X  4,  X  10, and  X  200, are summarized in Table 4.
 4.2 Perspective 2: Rank Sequence Deviation Now we look at the performance of the proposed scheme in more detail, par-ticularly its relationship with word length. We therefore define the rank se-quence deviation  X  ( R YW ,R ) with respect to two rank sequences R YW and R ,  X 
R for short, as i (  X  scheme compared to the other two schemes respectively, as listed in Table 5.
From Table 5 we can see, the proposed scheme receives the best results for 1 to 3 character words in YWL but for 4+ character words, it turns to be worse. In order to further investigate the performance of our scheme, we divide the YWL words into three parts, i.e., high, medium and low frequency words. Fig.2 shows the coverage rate of top N frequent words to YUWEI.

Based on the coverage rate curve shown in Fig.2, we get the point HM to divide high and medium frequency words and the point ML to divide medium and low frequency words. Then we have:
High frequency words: Top 8,076 frequent words (1  X  HM), with word fre-quency &gt; 281; Medium frequency words: the words from 8 , 077 th to 60 , 224 th (HM  X 
ML) with word frequency &gt; 12; Low frequency words: the remained words (ML  X  99,660), with word frequency &gt; 3.
 Then we do experiments on them respectively. The results are given in Table 6, Table 7 and Table 8. We can see that in most cases, our scheme received the best results. But for the low frequency words, especially one character words and 4+ character words, the results turn to be worse. 4.3 Perspective 3: The Coverage Rate We select the top 50,000 frequent words from R H B , R RF B and R H B + RF B ,then calculate the coverage rates of them over YUWEI. Table 9 gives the results.
In Table 9 we can see that the coverage rate of the proposed scheme increases 3.0% and 1.9% compared to HB and RFB respectively.
 4.4 Sample Analysis Now we choose R H B and R H B + RF B to make further comparison. Comparing against R H B , there are totally 57,024 words in R H B + RF B whose ranks are better adjusted (i.e., these ranks are closer to the standard sequence R YW ( w i )than their ranks in R H B ), which we call positive samples; 42,619 words whose ranks are worse adjusted (i.e. these ranks are farther apart from the standard sequence R
YW ( w i ) than their ranks in R H B ), which we call negative samples; 17 words have the same ranks in R H B and R RF B + H B . Table 10 and Table 11 show the distribution of positive samples and negative samples over different frequency regions (high, medium, and low), respectively.

Here we give some positive examples which are reasonably adjusted, such as  X  (biologic technology) X ,  X  (knowledge economy) X ,  X  (information thruway) X  and  X  (greenhouse effect) X . These words have high frequency nowadays. When using our scheme, the ranks of this kind of words are properly adjusted ahead. We also give some negative examples, such as  X  (Premier Zhou) X ,  X  (central red army) X  and  X  (Xidan Market) X . These words are in rare use today, but our scheme made wrong de-cisions by adjusting them to higher rank positions, due to the fact that these words were frequently used historically, as reflected in RC, a very large raw cor-pus covering the linguistic phenomena of that time span more intensive than HUAYU, BEIDA, as well as YUWEI. In this paper we propose a trade-off scheme which jointly uses the raw corpus, MM-segmented corpora and manually seg mented corpora to make approxima-tion for word frequencies in Chinese. The experiments indicate that this new scheme can benefit the word frequency estimation, though in some cases it seems not very satisfactory, as indicated by  X   X   X  in Table 8. Besides, the experiments presented here are also very preliminary mainly due to the limited resources available. How to obtain a more accurate word frequency estimation for Chinese is still a big challenge.
 groups of words wh i ch i s often used as a pre-process i ng of full pars i ng problem [1] . bas i c components for convey i ng the mean i ng of sentences and appropr i ate translat i on pars i ng problem, help analyze general structure of complex sentences, and prov i de i mportant clues for detect i ng ma i n pred i cates i n Ch i nese sentences .
Usually, var i ous const i tuents are i ncluded i n MNP and there ex i sts long dependency mach i ne learn i ng (ML) methods . Here, two sal i ent features such as expanded chunks and class i f i ed punctuat i ons are developed to i mprove performance . and future work w i ll be g i ven i n Sect i on 6 . stat i st i cs-based and ML-based approach . Among them, the ML-based systems have relat i vely h i gh performance and have become ma i n stream [7 and 9] . Usually, features, such as POS and semant i c codes are employed . Also the 2-phase ML method proves to be effect i ve, wh i ch f i rst detects BPs and then detects MNPs i n [7] .
 features should be selected st i ll rema i n as cr i t i cal i ssues . As noted before, expanded chunks and class i f i ed punctuat i ons are developed as effect i ve features for detect i ng MNPs . The reasons are expla i ned as follows . 3.1 Expanded Chunks We put forward the def i n i t i on of expanded chunks such as word groups wh i ch can i nclude BPs, co-occurrence patterns, words surrounded by quotat i on marks or brackets, and words l i sted by sl i ght pause marks . In other words, the expanded i dent i fy i ng MNPs .
 The usage of correctly i dent i f i ed expanded chunks w i ll i mprove the eff i c i ency of MNP i dent i f i cat i on, but there ex i sts a r i sk when i nappropr i ate expanded chunks are and recall of our rule-based system i s 97% and 65%, respect i vely . 3.1.1 Base Phrases def i ned, although there are many researchers work i ng on i t . In th i s paper, we followed base phrase i n Ch i nese . Hence, we def i ned 10 types of BPs i n Ch i nese . Among these BPs, SBAR, PP, CONJP, INTJ, LST, and LP cannot prov i de useful clues i n MNP i dent i f i cat i on, hence we w i ll not take these BP i nformat i on i nto cons i derat i on . NP, proper NP, temporal NP, and quant i f i er NP and VP i s sub-class i f i ed i nto common VP, copula VP,  X  X ou( )  X  VP, and ad j ect i ve VP . 3.1.2 Co-occurrence Patterns 3 Ex 1 : [ /NN /NN ] [( /P /NR /NN /NN /LC ) /DEC could be cons i dered as an MNP . 3.1.3 Words Surrounded by Quotat i on Marks or Brackets Ex 2 : ( /NR /NR ) /AD /VC /VA /DEV /VV [( /PU /VV /CD /M /NN /PU /P /VV /AS /NN /LC /AD From the example, we can see that by pre-chunk i ng the words surrounded by quotat i on marks, w i der context i nformat i on can be referenced by subst i tut i ng the patterns by an MNP tag . Here,  X   X  can reference the word  X   X  wh i ch i s far from the head word  X   X  because the expanded chunks are handled as one un i t . 3.1.4 Words L i sted by Sl i ght Pause Marks Ex 3 : ( /NR /NR ) ( /NT ) /LC /VV /VV /AS [ /VV ( /NN /PU /NN /PU /NN /PU /NN /PU /NN ) /ETC /NN /DEC /CD /M /NN /NN ] /PU In th i s example, we pre-chunk the words l i sted by sl i ght pause marks wh i ch play as a l i ngu i st i c un i t when detect i ng MNP . 3.2 Class i f i ed Punctuat i on shown i n Table 1 .

Here, i ns i de and outs i de the MNP i nd i cate frequenc i es when punctuat i on i s located i n an MNP or out of an MNP, respect i vely . The rate of outs i de MNP shows a d i st i nct Treebank, we propose to class i fy Ch i nese punctuat i ons i nto f i ve groups as descr i bed i n Table 2 .
 As noted before, our system i s a 2-phase hybr i d approach . In the f i rst phase, expanded such as POS, expanded chunk, and class i f i ed punctuat i on .
To construct the MNP i dent i f i cat i on model, we employ IOBES tags wh i ch were class i f i cat i on problem w i th IOBES tags . The deta i led descr i pt i on i s as follows . 28,000 MNPs w i th the average length of 3 to 4 words . We employ a ML approach of Support Vector Mach i nes (SVM) [13] and all of the exper i ments performed 10-fold 
The exper i ments w i ll focus on how effect i vely the 2-phase hybr i d approach works d i scovered through exper i ments . Here, the w i ndow s i ze i s based on tokens wh i ch are punctuat i ons are marked as  X  X U  X  i n the tra i n i ng corpus . 5.1 Us i ng BPs and Expanded Chunks how useful the expanded chunks are . Table 4 shows the result of when pre-chunked BPs and expanded chunks are adopted as pre-process i ng i n the f i rst phase . The exper i ment result d i splays that expanded chunks are more useful than BPs . However, rule-based system has 97% of prec i s i on and 65% of recall . Low recall h i nders more i mprovement i n MNP i dent i f i cat i on .
 5.2 Us i ng Class i f i ed Punctuat i ons wh i ch i mprove the performance by 2 . 57% when the w i ndow s i ze i s 9 . 5.3 Us i ng Both Expanded Chunks and Class i f i ed Punctuat i ons exper i ments are conducted w i th a f i xed w i ndow s i ze of 9 . When expanded chunks and than the basel i ne model . Furthermore, the IOBES tag result i s d i splayed i n Table 7 .
From Table 7, we can see that class i f i cat i on performance of MNP_B and MNP_S the i dent i f i cat i on of such tags .
 5.4 Error Analys i s that noun could be tagged as one of the cand i date tags such as MNP_B, MNP_E, MNP_I, and MNP_S, wh i ch further compl i cate the class i f i cat i on problem . The components of MNPs . Here are two examples .
 Ex 4 : [ /NR /NN /NN /NN ] [ /NN /NN /NN ] Ex 5 : [ /AD /AD /VV /P /NN /LC /DEG /CD /M /NN ] /VC...... In example 4,  X   X  i s a top i c and  X   X  i s a sub j ect and both of them are MNPs . However, the boundary between them cannot be detected by the features wh i ch we use currently . The f i fth example conta i ns adverbs such as  X   X  and  X   X  and these words do not appear frequently i n an MNP . Our system cannot detect  X   X  and  X   X  as members of MNP .

Hence, these problems should reference more l i ngu i st i c knowledge such as valency i nformat i on and so on, and they rema i n as future works . 5.5 Comparat i ve Exper i ments To more fa i rly compare the performance of our proposed method, the comparat i ve result i s shown as Table 9 . Our proposed system has the best performance .
We conduct a comparat i ve exper i ment w i th the same method and the corpus used i n [7] wh i ch shows a large i mprovement w i th our proposed system . Comparat i ve both corpora; hence, we expect that i t can also prov i de useful i nformat i on . The proposed method shows a prom i s i ng result of the 2-phase hybr i d approach, wh i ch punctuat i ons .
 cons i der i ng expanded chunks as one un i t . However, i t also has latent problem of error 97% of prec i s i on and 65% of recall . Therefore, a more eff i c i ent method i s needed to proved to be a powerful feature . We have grouped the Ch i nese punctuat i ons i nto f i ve Treebank .
 as one of the cand i date tags such as MNP_B, MNP_E, MNP_I, and MNP_S . As a any i nflect i ons or funct i onal words between them . These problems should be resolved by referenc i ng more compl i cated l i ngu i st i c knowledge such as valency i nformat i on and so on . Also, more elaborate post-process i ng i s necessary to guarantee that the result can be used i n other appl i cat i ons wh i ch adopt MNP Cond i t i onal Random F i eld (CRF) i n our system . Further research should be carr i ed out to resolve these problems .
 Acknowledgments. Th i s work was supported by the Korea Sc i ence and Eng i neer i ng Foundat i on (KOSEF) through the Advanced Informat i on Technology Research Center (AITrc), and also part i ally by the BK 21 Pro j ect i n 2006 .
 Abbrev i at i ons (also referred to as short-forms of words or phrases) are w i dely used i n role i n i mprov i ng i nformat i on extract i on and retr i eval systems [1][2][3][4] .
Over the past years, great progress has been ach i eved i n Engl i sh abbrev i at i on resolu-ods [3] and mach i ne learn i ng methods [4][5][6] . Yu et al (2002) presented a solut i on for t i on databases, namely GenBank Locusl i nk, SWISSPROT, LRABR of the UMLS Spe-c i a i l i st Lex i ocn and B i oBACUS . Toole (2000) descr i bed another rule-based method for per i ments on some 10,000 documents i n the f i eld of av i at i on showed that the i r method i mproved the accuracy of expans i on by about 10% i n compass i on w i th prev i ous rule or d i ct i onary based methods . Gaudan et al (2005) appl i ed support vector mach i nes (SVMs) ach i eved a prec i s i on of 98 . 9% and a recall of 98 . 2% . Yu et al (2003) also demonstrated max i mum entropy approach to abbrev i at i on expans i on i n med i cal text and an accuracy of 89% was reported on a sample of 10,000 rheumatology notes [6] .
 model for Ch i nese abbrev i at i on generat i on and expans i on [7] . Although the i r method for non-reduced abbrev i at i ons i n Ch i nese . Lee (2005) proposed a rule-based approach performance, part i cularly the recall of expans i on i s h i ghly dependent on the coverage of rules for expans i on .
 morpholog i cal h i nts l i ke cap i tal i zat i on for abbrev i at i on resolut i on .
In th i s paper, we propose a hybr i d approach to Ch i nese abbrev i at i on expans i on . In assumpt i on, a mapp i ng table of short-words and long-words and a d i ct i onary of short-form/full-from pa i rs are appl i ed to generate the respect i ve expans i on . Then, a h i dden Markov model (HMM) based d i samb i guat i on i s employed to rank theses cand i dates and select a proper expans i on for each abbrev i at i on . To i mprove expans i on accuracy, further appl i ed to double-check the expanded results and rev i se some error expans i ons most Ch i nese abbrev i at i ons can be correctly expanded by our system . from the PKU corpus [10] . Sect i on 5 g i ves the exper i mental results . F i nally, we draw our conclus i ons i n Sect i on 6 . 2.1 Methods for Creat i ng Ch i nese Abbrev i at i ons In general, Ch i nese abbrev i at i ons or short-forms are created us i ng three ma j or methods, namely reduct i on, el i m i nat i on and general i zat i on [8][9] .
Reduct i on i s the most popular method for creat i ng Ch i nese abbrev i at i ons [9], wh i ch produces abbrev i at i ons by select i ng one or more key morphemes from each const i tuent word of the or i g i nal full-forms . For example, ( The University of Hong Kong ) i s abbrev i ated to by select i ng the second morpheme of the f i rst word ( Hong Kong ) and the f i rst morpheme of the second word ( university ), respect i vely .
 example, ( Tsinghua University ) i s convent i onally abbrev i ated to nature of the full-form .

In the way of generat i on, abbrev i at i ons are created by general i z i ng parallel parts of the i r correspond i ng full-forms . For example, ( three preventions ) i s an abbrev i at i on for the comb i ned phrase fire prevention, among the three parts of the or i g i nal express i on, so i t i s be i ng general i zed . 2.2 Types of Abbrev i at i ons i n Ch i nese abbrev i at i ons .

G i ven a full-form spond i ng abbrev i at i on types of Ch i nese abbrev i at i ons can be formally def i ned as follows :
If m n = and (namely for 1 = i to n , abbrev i ated w i th reduct i on .

If m n &lt; and ) 1 ( n j F s or i g i nal full-form after abbrev i at i on .

If m n &lt; and ) 1 ( n j F s some add i t i onal morphemes or words are usually needed to abbrev i ate an express i on w i th general i zat i on .

For conven i ence, the latter two types of abbrev i at i ons are called by a j o i nt name i n th i s paper, namely non-reduced abbrev i at i ons . Furthermore, the component words of a short-form are des i gnated as short-words wh i le the const i tuent words i n a full-form d i st i ngu i sh reduced abbrev i at i ons from non-reduced abbrev i at i ons . 3.1 Overv i ew or i g i nal full-form i n three ma i n steps : know exactly how a g i ven abbrev i at i on i s created before expans i on . To ensure every could be created e i ther by the method of reduct i on or by the method of non-reduct i on d i ct i onary of short-form/full-form pa i rs . Markov model d i samb i guat i on i s developed i n th i s paper to perform th i s task . of the second step . Actually, HMM-based d i samb i guat i on can only handle local by HMM-based d i samb i guat i on and rev i se the error expans i on i f any . 3.2 Generat i on of Expans i on Cand i dates 3.2.1 Generat i ng Expans i on Cand i dates w i th a Mapp i ng Table each short-word i n i t are known, i ts expans i on cand i dates can be thus determ i ned by Table 1 i s des i gned to map each short-word i n reduced abbrev i at i ons to a set of long-the abbrev i at i ons have been expanded to the i r respect i ve full-forms . follows : F i rstly, the reduced abbrev i at i on i s segmented i nto a sequence of short-words us i ng a mapp i ng table between short-words and long-words and a d i ct i onary of 
Short-words Long-words English translation ... ... ... normal Ch i nese words . Then, each segmented short-word i s mapped to a set of long-words by consult i ng the mapp i ng table . In our current system, all the generated long-words and the i r match i ng short-words are stored i n a latt i ce structure . Obv i ously, any comb i nat i on of the relevant long-words forms an expans i on cand i date . 3.2.2 Generat i ng Expans i ons w i th a D i ct i onary of Abbrev i at i ons phemes or words w i ll be added to i ts short-form . Therefore, a one-to-one mapp i ng re-and the component words w i th i n i ts full-form . Due to th i s reason, the above mapp i ng strong capac i ty of expans i on generat i on .
 full-forms . Th i s d i ct i onary can be manually comp i led or automat i cally comp i led from an abbrev i at i on-expanded corpus . Table 2 presents some pa i rs of non-reduced abbre-v i at i ons and the i r full-forms used i n our system .
 Short-forms Full-forms English translation ... ... ... 3.3 D i samb i guat i on of Abbrev i at i ons w i th HMMs G i ven an abbrev i at i on expans i on cand i dates . Where, ) 1 ( m i s and ) 1 ( n i f Markov models (HMMs), the task of abbrev i at i on d i samb i guat i on i s to f i nd a proper expans i on F  X  that max i m i zes the follow i ng score of two parts : the abbrev i at i on model ) | ( F S P and the full-form model ) ( F P . s i on cand i dates are generated w i th the above d i ct i onary of short-form/full-form pa i rs, the probab i l i ty can be est i mated d i rectly from an abbrev i at i on-expanded corpus us i ng v i at i on and the relevant full-form i s generated w i th the above mapp i ng table, then the abbrev i at i on model can be approx i mately calculated w i th the follow i ng formula the n-gram language model i s appl i ed to approx i mate the full-form model as follows :
Where ) | ( ber of contextual words . It should be noted that the left context L and the r i ght con-text R around the abbrev i at i on are also taken i nto account i n the full-form model for employ a b i gram LM i n th i s study, namely 2 = N . Let t i ve words on the left and r i ght of the abbrev i at i on, we have ) | ( ) | ( 3.4 Rev i s i ng Error Expans i ons Us i ng L i ngu i st i c Knowledge whether the expans i on y i elded by the above procedure i s correct for a g i ven abbrev i a-t i on i n a spec i f i c context : (1) The hypothes i s of one sense per d i scourse .

The hypothes i s of one sense per d i scourse i s f i rst i ntroduced i n the study of word results y i elded by the above stat i st i cal expans i on procedure and rev i se some error ex-pans i ons i f any .
 namely ( Iran ) and ( Iraq ) . But i n a text l i ke / / / / distribution and suggested increasing the exportation of Iraqi oil ), the short-form i s more l i kely to be abbrev i ated from ( Iraq ), rather than ( Iran ) . (2) The format for def i n i ng abbrev i at i ons .

The format i on for def i n i ng abbrev i at i on has been w i dely used i n the study of Eng-i n the format  X  &lt;full-form&gt; &lt; short-form&gt;  X  or  X  &lt;full-form&gt; &lt; short-form&gt; , as i llustrated i n the text  X ... f i nd the full-form of a g i ven abbrev i at i on d i rectly from the relevant art i cle . mapp i ng table and HMMs, we bu i lt an abbrev i at i on-expanded corpus from the Pek i ng quence of short-words and a sequence of full-words, respect i vely . It should be noted the full-form i n the short-form after abbrev i at i on . For example, the short form ( standing committee ) i s usually segmented to one word i n lex i cal word segmentat i on . But, i t i s segmented to / / i n short-form segmentat i on because i ts full-form, i. e . words can be eas i ly i dent i f i ed .
 news text from the People  X  s Da i ly, wh i ch has been manually segmented and tagged cons i der these expl i c i tly labeled abbrev i at i ons i n our current evaluat i on . total of more than one m i ll i on words i n January or February of the PKU corpus, there d i str i buted i n d i fferent sentences . As shown i n Table 3, more than 14% of sentences general i zat i on .

Corpus No . words No . sentences Reduced
For tra i n i ng (January) 1 . 12M 47,288 
For test i ng (February) 1 . 15M 48,095 To measure the expans i on performance, we calculate recall and precision i n our the total number of abbrev i at i ons expanded automat i cally by the system . Overall P (%) 75 . 1 85 . 5 86.3 ment, two basel i ne methods, namely the un i gram language model and the b i gram language model, are i ntroduced for compar i son . It should be noted that both the tracted from the tra i n i ng corpus only . As can be seen from Table 4, our system can ach i eve a recall of 83 . 8% and a prec i s i on of 86 . 3% on average for d i fferent types Ch i nese text . The results i n Table 4 also show that the proposed method performs better than the un i gram LM and b i gram LM, wh i ch i nd i cates that us i ng HMMs and performance .
The mapp i ng table and the ab-brev i at i on d i ct i onary are from tra i n i ng data only 
The mapp i ng table and the ab-brev i at i on d i ct i onary are from both tra i n i ng and test data respect i ve overall recall and prec i s i on are i mproved from 83 . 8% and 86 . 3% to 89 . 1% and 88 . 6% i f all the abbrev i at i ons be i ng tested are covered by the mapp i ng table and i dent i f i cat i on rema i ns unresolved at present . In th i s paper, we presented a hybr i d approach to Ch i nese abbrev i at i on expans i on us-assumed that th i s short-form may be created e i ther by reduct i on or by non-reduct i on tween short words and long words and a d i ct i onary of non-reduced short-form/full-HMM-based d i samb i guat i on i s employed to rank these cand i dates and select a error expans i ons i f any . We evaluated our approach w i th an abbrev i at i on-expanded ach i eved a recall of 83 . 8% and a prec i s i on of 86 . 3% on average for d i fferent abbre-d i samb i guat i on .
 Hong Kong Research Grants Counc i l Compet i t i ve Earmarked Research Grant 7271/03H .
 Korean, l i ke Western European languages, i ncludes orthograph i c-words [8], wh i ch /knowledge-or stochast i c-based approaches have been used i n i nvest i gat i ng automat i c word-spac i ng systems . Among those, the stochast i c approach has unreg i stered words . However, data sparseness i s a challenge to the stochast i c method . Korean word-spac i ng have adopted the syllable-N-gram-based method, though, i n the ep i stemolog i cal v i ew, i t i s rather unnatural compared to the word-N-gram-based orthograph i cal character i st i cs .
 detect the opt i mal word-spac i ng po i nts, based on the morpheme un i gram and i ts d i scusses the exper i ments, and Sect i on 5 offers conclud i ng comments . In general, h i gher-order N-gram models are not used when there i s not suff i c i ent data [4] . On extract i ng enough word N-grams and apply i ng the N-gram model, wh i ch uses us i ng the cont i guousness between words . However, th i s method i s not easy i n Korean due to the data sparseness produced by the typolog i cal character i st i cs of Korean : the predom i nance of agglut i nat i ve morphology (Suff i xes can be attached success i vely to morphemes for i nflect i on s i multaneously encode several mean i ngs, because Korean parts of speech . 2.1 Syllable-N-Gram-Based Approaches Many Korean stochast i c word-spac i ng stud i es have used syllable-N-gram-based models, because the syllable-N-gram approach eas i ly augments the order of the N-language . The model i n [2] used syllable-b i gram i nformat i on extracted from the raw syllable tr i gram-based approach . Word-spac i ng problems were treated us i ng a h i dden Markov model . Us i ng the tr i gram model, wh i ch cons i dered the current tag and representat i ve syllable-N-gram models can resolve Korean word-spac i ng problems to dependency .

However, a mean i ngful performance can only be obta i ned by augment i ng the N-gram order unt i l a tr i gram model value i s reached, wh i ch has i mportant consequences for memory s i ze (see Table 1) . Moreover, us i ng syllable-N-grams to pred i ct  X  X ords X  i s less ep i stemolog i cally acceptable than us i ng word-N-grams, because word-spac i ng can be conce i ved as detect i ng word boundar i es and not syllable boundar i es, even i f i n some cases we f i nd frequent syllable patterns due to phonotact i c restr i ct i ons and local syntact i c dependence . 2.2 Word-N-Gram-Based Approaches Korean stochast i c word-spac i ng model except [1], whereas the word recogn i t i on approach has been w i dely used i n rule-/knowledge-based approaches [6] . The word-spac i ng model i n [1] was constructed us i ng a word-un i gram model wh i le m i n i m i z i ng the N-gram order, to avo i d a drast i c augmentat i on of the parameters count, i n sp i te of the drawback of us i ng the word-N-gram-based approach . The probab i l i ty of the words another . Although the Korean language shows a free-word-order predom i nance, local spac i ng probab i l i ty, were i ncluded i n the model to compensate for the lack of context probab i l i ty of the syllable b i gram  X  k,  X  and  X  k+1,1 .
Apply i ng (1), [1] obta i ned a word-un i t prec i s i on of 93 . 39% us i ng 29 . 2 MB (word un i gram + syllable b i gram = 25 . 1 + 4 . 1 MB, respect i vely), wh i ch i s a smaller memory prec i s i on of 93 . 06% (see Table 1) . 2.3 Resolv i ng Data Sparseness and Memory Increase i n Word-Un i gram Model Although the word-un i gram-based model i n [1] obta i ned a h i gh performance w i th though that memory s i ze i s smaller than that of the syllable tr i gram model . A comb i ned model compensated for the data sparseness of the word-un i gram-based model us i ng the Korean Morpholog i cal Analyzer (KMA) i mplemented at Pusan Nat i onal Un i vers i ty i n Korea . [1] Th i s dynam i cally expanded the number of cand i date words among the poss i ble morphemes, us i ng a longest-rad i x search strategy, and correct i on of 98 . 39% and 97 . 51%, respect i vely, be i ng observed . Data-sparseness-words . However, smooth i ng us i ng the KMA techn i que could not prevent an i ncrease i n the d i ct i onary memory s i ze by 33 . 5 MB (word un i gram + syllable b i gram + KMA = 25 . 1 + 4 . 1 MB + 4 . 3 MB, respect i vely) (see Table 1) .
 memory s i ze i s based on the follow i ng def i n i t i on and assumpt i on . Def i n i t i on 1. Morphemes are the i mmed i ate const i tuents of a word . morpheme(s)  X  probab i l i ty i n the word .
 because the system can more product i vely propose cand i date words comb i n i ng morphemes, and (b) we can reduce the d i ct i onary memory s i ze because the number of sect i on d i scusses how to pred i ct a word us i ng the factors that control the comb i nat i on i s to resolve both data sparseness and increased memory size while maintaining an epistemological approach, that is, word recognition for word-spacing . 3.1 The Correlat i on Between the Observed Word and Its Category Pattern In contrast to the syllable-N-gram approach, the morpheme-N-gram order i s not easy to augment, because morphemes, although f i n i te i n number, are much more numerous that the occurrences of morphemes are independent of each other. The probab i l i ty of the i nd i v i dual morphemes i n the word, w k , as i n (2) . word performs under certa i n constra i nts . Each morpheme, as part of a word, refers to ordered accord i ng to the i r categor i es i n a part i cular pattern i n a word . only a s i ngle morpheme appears .
 or w i th other morpheme(s) .
 part i cular power for const i tut i ng a word .
 probab i l i ty based on the morpheme un i gram and i ts power we i ght i n appear i ng i n the category power we i ght parameters w i th i n the category pattern . 3.2 Bas i c Concepts Involved i n the Category-Pattern-Based Model Construct i on categor i es used i n the model construct i on are l i sted i n Table 2 . grouped i n a general i zed category or not cons i dered i n model construct i on accord i ng to the i r l i ngu i st i c character i st i cs and heur i st i cs, as follows . z All i nflect i onal end i ngs, except pre-end i ngs, are grouped i nto one category, z Verbs and ad j ect i val verbs that mostly show the same morphosyntact i c shown i n (3) .
Accord i ng to Assumpt i on 2, the morphemes const i tute a word under a part i cular category pattern, cp j , w i th a correlat i on between those morphemes i n the word . Each morpheme appears i n a category pattern w i th a part i cular power . The est i mated word category pattern should be i n d i rect proport i on to the probab i l i ty of the word . 3.3 Parameter F i tt i ng Us i ng S i mulated Anneal i ng f i t the parameter wcp j,i , real-word lists according to each cp j (CPWL) were extracted CPWL were extracted . Each CPWL was sorted by word-frequency rank, and sample data sets for tra i n i ng (SCPWLs) were constructed by select i ng one word every 10 words i n the CPWL was less than 1,000 words (about 82 CPWLs i n all), they were all algorithm was adopted i n order to f i t the parameters . However, the hill-climbing algorithm was less econom i cal than simulated annealing because i t always chose the algor i thm tolerated the choos i ng of the worst value and was thus protected from any effect of local maxima .
 m i n i m i ze the mean of the error between the observed probab i l i ty of a word, as i n (1), and the word probab i l i ty est i mated from the morpheme probab i l i ty, wh i ch was we i ghted w i th a category we i ght, wcp j , (1...n) , i n the category pattern cp j . 3.4 Word Pred i ct i on w i th Morpheme Un i gram and Category Power word-spac i ng model i n (1) can be rewr i tten as a category-pattern-based word-spac i ng model, as i n (7) . Remark 2. F i gure 1 i llustrates the mean of the error and the standard dev i at i on of each wh i ch the two results follow the same flow i ng, decreas i ng curve, we can understand average frequency of d i fferent words . For example, the average frequency of d i fferent words (total number of words/total number of d i fferent words) was 1069 . 33 occurrences for the cp16 &lt;VX+ETM&gt;, wh i ch had the h i ghest standard dev i at i on, and 13 . 48 occurrences for the cp1 &lt;N + PP&gt; i n SCPWLs . study . [1] value becomes . Therefore, to obta i n an opt i mum word-spac i ng system, we need to i mprove the F-measure of correctly spaced words, wh i ch i mprovement comb i nes both recall and prec i s i on .

In Table 5, Model A represents the word-un i gram-based model us i ng SWD and odds() . Model B represents the word-un i gram-based model w i th the morpheme-system shows a comparably h i gher performance w i th SWD for i nternal data than for performance us i ng external data w i th a tra i ned category we i ght, wcp j,i . Th i s study resolved data sparseness by produc i ng abundant cand i date words based on morpheme un i grams and category we i ghts, and reduced the d i ct i onary memory s i ze Furthermore, th i s approach st i ll ma i nta i ns an ep i stemolog i cal approach because word recogn i t i on proceeds through category patterns composed of the i mmed i ate pattern, and thus the system was robust toward unseen words; however, the oppos i te effect somet i mes occurred because the we i ght was not tra i ned for each morpheme . morphemes produc i ng those except i ons as a part i cular category .
 Acknowledgements. Th i s work was supported by the Nat i onal Research Program under Grant M10400000332-06J0000-33210 .
 In the last decades, enormous efforts have been made to Ch i nese word segmentat i on and part-of-speech tagg i ng . However, the state-of-the-art system performance for Internat i onal Ch i nese Word Segmentat i on Bakeoff i n 2003 organ i zed by SIGHAN [1], the h i ghest F-measures for word segmentat i on i n the open test on four small-SIGHAN Internat i onal Ch i nese Word Segmentat i on Bakeoff [2], the s i tuat i on segmentat i on .

It has been observed that, Ch i nese word segmentat i on ( i nclud i ng segmentat i on amb i gu i ty resolut i on and unknown word process i ng) i n some cases needs the ass i stance of h i gher level l i ngu i st i c process i ng to some extent, at least at the morph-mature, the i dea of comb i n i ng word segmentat i on w i th part-of-speech tagg i ng comes i ntegrated, lead i ng to an i mproved performance i n both word segmentat i on and part-of-speech tagg i ng [5, 6] .
 i nput sentence i n terms of word frequenc i es and sentence length; (2) perform part-of-speech tagg i ng for each of the N-best word sequences, ach i ev i ng the N-best tag sequences accord i ngly; and (3) use a we i ghted score that m i xes the factors of (1) and (2) to choose the  X  X est  X  solut i on from the N-best word sequences, along w i th i ts  X  X est  X  g i ven sentence; (2) cont i nue to expand each word sequence i nto all of i ts poss i ble tag speech tagg i ng for the sentence; and (3) f i nd the opt i mal path over such a space us i ng dynam i c programm i ng i n the framework of H i dden Markov Model (HMM), y i eld i ng worth stress i ng that i n th i s approach, step (1) and (2) are done s i multaneously . [8] demonstrated the d i st i nct i on between these two strateg i es by the follow i ng example : (1)
There are two poss i ble segmentat i ons : a . | | | | b . | | | | 
The strategy of pseudo-i ntegrat i on would prefer segmentat i on (1a) because the probab i l i ty of word sequence  X  +  X  i s greater than that of  X  +  X  i n terms of word frequenc i es, whereas the strategy of true-i ntegrat i on would prefer segmentat i on (1b) because the probab i l i ty of tag sequence  X  X  PREP TIME-N TIME-N  X  i s l i kely to be greater than that of  X  X  ADV TIME-CLASSIFIER TIME-N  X . some segmentat i on amb i gu i t i es . For example : (2) 
Two poss i ble segmentat i ons can be found for i t :
The probab i l i ty of the tag sequence  X ...+N+V+N+V+...  X  for segmentat i on (2b) i s accord i ng to part-of-speech N-grams i n Ch i nese . So the strategy of [6, 8] may fa i l to y i eld a full parse tree .
 computat i onal model, the more powerful the d i samb i guat i on capab i l i ty of th i s model, Ch i nese texts i s i mposs i ble i n the near future .
 feas i b i l i ty . [8] reported an exper i ment on 729 sentences w i th 10,734 Ch i nese characters, i n the word-segmented and part-of-speech tagged corpus i n the news doma i n, cons i st i ng of about 70 thousand Ch i nese characters . For the sake of descr i pt i on, we use FMM-Seg poss i ble-segmentat i ons  X  , POSB i gram-Tagg i ng to stand for  X  X art-of-speech tagg i ng w i th a B i gram model  X  ,  X  X  || y  X  to stand for that x and y are performed s i multaneously, effect i ve throughout th i s paper) . In the i r exper i mental results, compared w i th  X  X MM-Seg  X  POSB i gram-Tagg i ng  X  ,  X  X P || POSB i gram-Tagg i ng  X  has a 1 . 31% i mprovement i n word segmentat i on, a 1 . 47% i mprovement i n part-of-speech tagg i ng, and a 2 . 69% i mprovement i n total performance for the comb i ned processes, show i ng the reasonableness of i ntegrat i on .
 methods d i scussed i n [8], as well as other related key factors (for example, to do word that scheme and try to i mprove i t further . Exper i ments are carr i ed out on PDA9801-06, a manually word-segmented and part-of-speech-tagged news corpus of the People  X  s Da i ly from January to June 1998, constructed by Inst i tute of Computat i onal learn parameters for the related models, and the rema i n i ng 20% (w i th 1,460,487 words) as the test set . We def i ne three sets of metr i cs for measur i ng the performance of the proposed schemes : R-WS and F-WS respect i vely . R-PT and F-PT respect i vely .
 (3) The prec i s i on, recall and F-measure of word segmentat i on and part-of-speech tagg i ng comb i ned, denoted P-WSPT, R-WSPT and F-WSPT respect i vely . Obv i ously, the follow i ng equat i ons hold :
In all the exper i ments throughout th i s paper, the follow i ng bas i c strategy i s employed for almost all the proposed schemes : i n the process of word segmentat i on, all poss i ble word sequences w i ll be generated by AP for an i nput sentence; and, i n the word sequences w i ll be generated . In cases where we need to f i nd the best path from polynom i al t i me complex i ty . 2.1 Segmentat i on Us i ng Word N-Grams scheme i s too s i mple . So we explore some more soph i st i cated schemes here . (1) Segmentat i on us i ng  X  X he forward max i mal match i ng  X  (denoted FMM-Seg) f i nd the best word sequence (denoted AP&amp;WordUn i gram-Seg) f i nd the best word sequence (denoted AP&amp;WordB i gram-Seg) :
It i s expected that word b i grams w i ll be more powerful than word un i grams . AP&amp;WordB i gram-Seg+WordUn i gram-Smooth i ng) :
The word b i gram model w i ll encounter a ser i ous data sparseness problem . A word un i gram back-off would be a natural cho i ce for smooth i ng . (5) Segmentat i on us i ng  X  X ll-poss i ble-segmentat i ons  X  , and us i ng word b i grams, along w i th part-of-speech b i gram smooth i ng (denoted AP&amp;WordB i gram-Seg+ POSB i gram-Smooth i ng) : tagg i ng based on the b i gram model .
 Exper i mental results for the above f i ve word segmentat i on schemes are g i ven i n Table 1 .

We regard FMM-Seg as the basel i ne for word segmentat i on . As can be seen i n Table 1, all WordUn i gram-based and WordB i gram-based schemes s i gn i f i cantly outperform FMM-Seg . The performance of AP&amp;WordB i gram-Seg w i thout smooth i ng i s even necessary for word b i grams . The two k i nds of smooth i ng, AP&amp;WordB i gram-Seg WordUn i gram-Smooth i ng and AP&amp;WordB i gram-Seg POSB i gram-Smooth i ng, show AP&amp;WordB i gram-Seg POSB i gram-Smooth i ng i s the best among the f i ve schemes . 2.2 Integrat i on Schemes i n the Convent i onal Framework In the exper i ments below, the follow i ng b i gram part-of-speech tagg i ng model i s used (denoted POSB i gram-Tagg i ng) :
We try s i x i ntegrat i on schemes under the convent i onal framework : (1) FMM-Seg  X  POSB i gram-Tagg i ng 
Th i s s i mplest pseudo-i ntegrat i on w i ll serve as the basel i ne . (2) AP&amp;WordUn i gram-Seg  X  POSB i gram-Tagg i ng Th i s pseudo-i ntegrat i on i s s i m i lar to the work of [5] .
 (3) AP || POSB i gram-Tagg i ng 
Th i s i s same as the best scheme g i ven i n [8] . (4) AP&amp;WordB i gram-Seg  X  POSB i gram-Tagg i ng (5) AP&amp;WordB i gram-Seg WordUn i gram-Smooth i ng  X  POSB i gram-Tagg i ng (6) AP&amp;WordB i gram-Seg POSB i gram-Smooth i ng  X  POSB i gram-Tagg i ng segmentat i on . It thus deserves spec i al attent i on .
 Exper i mental results are summar i zed i n Table 2 .

AP&amp;WordB i gram-Seg POSB i gram-Smooth i ng  X  POSB i gram-Tagg i ng (scheme 6) outperforms all the other schemes . Although i t appears to be a sequent i al i ntegrat i on the smooth i ng here has i ncorporated part-of-speech tagg i ng i nto the process of word segmentat i on . Compared w i th the basel i ne (scheme 1), scheme 6 has a 3 . 59% i mprovement i n F-WSPT . Note also that AP-Seg || POSB i gram-Tagg i ng (scheme 3, a i ntegrat i ons) by 3 . 22% and 0 . 27% i n F-WSPT respect i vely . 2.3 D i v i de-and-Conquer Integrat i on speech b i grams, as long as the data sparseness problem i s overcome . Formulae 3 and 4 prov i de two alternat i ve ways for smooth i ng . Here, we propose a further alternat i ve . d i st i ngu i sh f i ve cases i n terms of f ( w i -1 ) and f ( w i ) : (1) Both f ( w i -1 ) and f ( w i ) are h i gh suff i c i ent :
We need to set a frequency threshold u (2) Otherw i se (2 . 1) f ( w i -1 w i ) i s qu i te h i gh work i n th i s case .

We need to set another threshold b (2 . 2) Otherw i se (2 . 2 . 1) f ( w i -1 ) i s h i gh, but f ( w i ) i s low (accord i ng to u The contr i but i on of w i w i ll become unrel i able i f we cont i nue to use formula 7 .
Thus we replace w i w i th i ts part-of-speech, i. e . , back off from a pure word b i gram trans i t i on probab i l i ty more rel i able : (2 . 2 . 2) f ( w i -1 ) i s low, but f ( w i ) i s h i gh (accord i ng to u a b i gram cons i st i ng of a part-of-speech and a word : (2 . 2 . 3) Both f ( w i -1 ) and f ( w i ) are low (accord i ng to u  X  ) 
We should completely back off from a word b i gram to a part-of-speech b i gram, because the data sparseness problem i s very ser i ous :
In essence, cases 2 . 2 . 1, 2 . 2 . 2 and 2 . 2 . 3 reflect some degree of smooth i ng . We name th i s strategy  X  X  i v i de-and-conquer i ntegrat i on  X  , as summar i zed i n Table 3 . Otherw i se ( n i s the length of the i nput sentence) . Some words i n the sentence may not be ass i gned second round . We denote th i s strategy AP&amp;D i v i deConquer-Seg-Tag  X  POSB i gram-Seg-Tag, i s sort of true-i ntegrat i on i tself, though  X   X   X  appears i n the notat i on . Results of the d i v i de-and-conquer i ntegrat i on approach are l i sted i n Table 4 . The d i v i de-and-conquer i ntegrat i on (scheme 7) shows a small i mprovement i n F-WSPT (0 . 07%) as compared to the best scheme i n Table 2 (scheme 6), i. e . , the best scheme i n the convent i onal framework .
 exper i ment, as l i sted i n Table 5 .

As can be seen from Table 5, scheme 7 s i gn i f i cantly outperforms scheme 6 i n both t i me complex i ty and space complex i ty : i t only uses 86 . 11% of the t i me and 46 . 17% of the space that scheme 6 used to process the test set .

It i s worth not i ng that, exper i mentally, the strategy of  X  X ll-poss i ble-segmentat i ons  X  before part-of-speech tagg i ng .

To deepen our understand i ng of scheme 7, we randomly d i v i de the tra i n i ng set i nto scheme w i th these sub-corpora i ncrementally, and observe i ts F-WSPT i n the test set accord i ngly . F i gure 1 demonstrates that the F-WSPT of scheme 7 i s not overly sens i t i ve to the s i ze of the tra i n i ng corpus . are tested and compared based on a large-scale test corpus . A novel true-i ntegrat i on i mprov i ng the performance of Ch i nese word segmentat i on and part-of-speech tagg i ng .
 Acknowledgments. The research i s supported by the Nat i onal Natural Sc i ence Foundat i on of Ch i na under grant number 60573187 and 60321002, and the Ts i nghua-ALVIS Pro j ect co-sponsored by the Nat i onal Natural Sc i ence Foundat i on of Ch i na under grant number 60520130299 and EU FP6 .
 Many people who are not natives of Japanese or Chinese find it very difficult to learn Japanese or Chinese characters. This is partially because their abil-ity to look up those characters is often quite limited. Roughly, there are three conventional ways to look up kanjis in kanji dictionaries:  X  by how they are read,  X  by their traditional Chinese radica ls, recorded in a shape index, and  X  by the number of strokes.
 All of these approaches follow the conventions of the ideographic system, which are not easily understood by beginners. Beginners do not know how to read kanjis, nor are they used to viewing kanji shapes and determining the special parts used to consult dictionaries. Moreover, looking up kanjis by stroke count can seem arbitrary and confusing to beginners, as what appear to be multiple strokes are often conventionally counted as one stroke (because one stroke is considered what can be continuously drawn with a brush in calligraphy).

An overview of previous work on kanji look-up for non-natives can be found in [1]. Most methods, however, are designed on the basis of the traditional conven-tions. For example, the SKIP code [2] is based on the traditional stroke counting method. One exception is a method allowing kanjis to be looked up by esti-mated readings according to the user X  X  knowledge, even though such estimated readings could be incorrect [3]. This approach assumes that users have a certain basic knowledge of kanjis, enabling them to guess how an unfamiliar one should be read.

There are various shape-based entry systems for Chinese (well summarized in [5]), but these systems again assume that the user is well acquainted with kanjis. Methods based on decomposed kanji parts require the user to memorize 10 to 30 character shapes, as in methods such as the Cangjie method and the four-corner method. Stroke-based methods require that strokes be entered in the proper order, for example, by the five-stroke method. Still, this last method is probably the most similar to our contribution in that it only uses five stroke prototypes.

Given this situation, we have developed an alternative stroke-based kanji look-up method that does not rely on ideographic conventions. Our idea is to use three stroke prototypes which form the basis of kanjis. The three prototypes are ver-tical, horizontal, and other (diagonal or bent strokes). For example, in the case of the character , the code is  X 4-3-2 X , meaning four horizontal strokes, three vertical strokes, and two other strokes (which are diagonals). The stroke pro-totypes were selected through an analysis of kanjis written by several complete beginners. The codes are entered into our web-based kanji look-up system, called Kansuke, and the characters corresponding to a given code will be displayed.
We first look at how arbitrary the kanji system is, and then go on to explain how a non-native may look up kanjis. Each kanji has a correct ordering of strokes for writing it down. All Japanese natives are educated in kanji writing from age 6 to age 17, learning the correct stroke order for writing each kanji. The order comes from calligraphy, and there are some general rules: from top to bottom, and from left to right.

There are also related rules for counting the number of strokes, where some connected vertexes are combined and regarded as one stroke. For example, Figure 1 shows how to correctly write three Japanese kanjis. The left-most col-umn shows the three complete characters, with the stroke orders shown on the right. The stroke count is five for all three characters. In the first row, the top-most and right-most lines are considered one stroke, while the left-most and bottom lines are counted as two different strokes. This is different from the second character, however, where the two left-most strokes are connected to horizontal strokes.

As seen from these two examples of relatively simple kanjis, the stroke order and stroke count are arbitrary. An extreme case is shown in the third row. Even though this character does not appear very complex, even many Japanese natives do not know how to write it in the correct order.

When a person does not know how to read a kanji, a kanji dictionary is consulted by examining the radicals (the individual parts of a character) of which it consists. This is also done by Japanese, using the bushu methodology. For those who are not familiar with the radicals, there is a multi-radical method [1], where people visually scan through parts consisting of 249 radical prototypes and choose those contained in the target kanji. When even this is difficult, the last option is to look up a kanji by its stroke count; e.g., by entering  X  X ive X  for the characters in Figure 2. However the conventions that determine stroke count are largely arbitrary. Consequently, non-natives must leap a large hurdle to become initiated in looking up Japanese kanjis.

One way to overcome this problem is to provide a universal way to describe kanjis. For this purpose, the way non-native beginners write kanjis will give us hints. 3.1 A Beginner X  X  View of Kanjis Figure 2 shows the efforts of an anonymous beginner. The left-most column shows the printed kanjis, with the written stroke sequence to the right.
We can see how variant a subject can be in writing these kanjis. If we tested more subjects, this variety would become even greater. By viewing such results produced by several beginners, however , we found the subjects had some common ideas:  X  The feature of a stroke being straight or bent is preserved.  X  The gradients of strokes are mostly preser ved. Especially, horizontals, verti- X  Sharply curved bent lines are considered one stroke. For example, when we From these observations, we can see how f ew features the non-natives shared. Under such circumstances, predictive/ambiguous entry is one possibility for de-signing an entry method, where users enter a code sequence formed only of letters with a few codes, which is expanded into the corresponding actual data by a machine. We designed such prototypes based on the above common obser-vations. 3.2 Kansuke Code Non-natives are generally not used to browsing kanjis, and they can scan a list of kanjis only slowly. Therefore, the code should allow precise classification of a huge number of kanjis through their assigned codes.

As the notion of traditional strokes is quite arbitrary, we first redefined this notion based on the small analysis explained in the previous section: a stroke is any part of the kanji that does not contain a sharp angle. Kansuke code was then designed as three numbers representing the quantity of horizontal, vertical, and other strokes (re-defined), so that it can be used to search for a kanji. Strokes classified as  X  X ther X  can be diagonal, bent, or complex. Examples of Kansuke code can be seen in Figure 3. For example, in the case of , there are four horizontal strokes, three vertical strokes, and two other strokes, so the Kansuke code is  X 4-3-2 X . Likewise, the code for the second character will be  X 1-1-2 X . A slightly tricky case is shown in the third row, where the stroke in the middle is classified as a vertical stroke but can also be classified as an  X  X ther X  stroke. For such ambiguous cases, multiple codes are attached, as explained in more detail in  X  4.3.

We compared the use of these codes with other conventional methods. First, the average number of candidates per entry for the traditional stroke-count method, the system of kanji indexing by patterns (SKIP), and Kansuke are compared in Table 3.2. These results were obtained for 2965 kanjis, all within the JIS first standard. For the traditional stroke count (for example, a count of five for all kanjis in Figure 1), the average number of candidates was 100, mean-ing beginners often have to go through a painfully long list to find the kanjis they seek.

As an alternative, SKIP is a method applied to a kanji dictionary especially designed for beginners [2]. In this method, a kanji is typically classified into one of four initial patterns describing the division of the kanji into two portions, and a code is constructed using this initial pattern and the stroke count of each portion. Thus, has a SKIP code of  X 1-4-3 X , indicating a vertical division (1), and the stroke counts for the left-hand (4) and right-hand sides (3). The SKIP method produced only 9.7 candidates on average, but the maximum number of candidates was as high as 67.

In contrast, Kansuke produced only four candidates on average, with a maxi-mum of 28 candidates. Thus, our code classifies kanjis far better than the other conventional methods. This superior classification is due to the fairly uniform distribution of strokes as regards the code type, as shown in Table 2. The num-bers of all strokes taken from 2965 kanjis were quite evenly distributed among our three prototypes (horizontal/vertical/other) with about 10,000 strokes in each category. 4.1 Interface Based on the Kansuke Code, the Kansuke system was designed for quick and efficient kanji look-up.

The Kansuke system consists of an interface and a database where the inter-face interactively filters candidate characters obtained from the database. The wholesystemiswrittenasanHTML/CSSwebpageandcanrunonanyIn-ternet browser. Our database of kanjis is enclosed in text format, and accessed through JavaScript functions. This allows dynamic queries without connection to a server. The system is accessible on the web [4]. We are now preparing a free-download version for local machine use.

The Kansuke system can be used by entering Kansuke code for the target character. Kansuke code can be constructed for the whole character, for ex-ample,  X 4-3-2 X  for . An alternative, perhaps a better way for the novice is to look up the target via components included in the target character. We ex-plain this here by looking up an example character . The user first chooses a component that is included in . Suppose that the user chose (at the top on the right-hand side of ). As  X  X  Kansuke code is  X 1-2-0 X  (one hor-izontal, two vertical strokes), the user enters the code. The system display then looks like panel 1 of Figure 4, with corresponding kanjis shown on the right side of the page, while corresponding components with the code  X 1-2-0 X  appear in the lower-left part. When the user selects the target component from the left side of the page, the system shows all characters that contain (Figure 4-2). Note that the appears at the top of the right side of the page. As there are still many kanjis that appeared as candidates, suppose that the user chose to filter further by (the left part of ). As is formed of three other strokes, the code  X 0-0-3 X  is entered. The system shifts to the figure showing corresponding components on the left page and all kanjis that consists of any of those components in addition to are shown on the right-hand side (Figure 4-3). When the component is selected, the kanji candidates are filtered so that the kanjis that consist of and are shown (Figure 4-4). The user may now choose his target, the sixth from the top on the right side of the page, and the dictionary entry for the kanji is shown (Figure 4-5).

The list of candidates on the right side is sorted according to their stroke counts, which provides a good approximation of each candidate X  X  complexity. Thus, simpler kanjis, often targeted by beginners, appear at the beginning of the list and can be retrieved more quickly. On the other hand, components shown on the left side are placed according to a manual grouping of similar components, so that similar components will be placed close to each other.

Consequently, kanji lookup is enhanced not only by Kansuke code, but also through the use of components. We will next show how this is designed within the Kansuke system and explain how characters of ambiguous code are processed in the system. 4.2 Component Trees Given a kanji, classifying all of its strokes to get the code is time-consuming and the probability of making a counting mistake is high. The Kansuke code of the character , for example, is  X 13-8-3 X , for a total of 24 strokes. As almost all complex kanjis are composites of smaller kanji parts, one way to make the search easier is to search by components .

A component is a group of strokes that is visually isolated in the kanji. If we take, for instance, ( toki , meaning time), some of its components are and . Someone more knowledgeable of kanjis, however, might prefer to search by , ,and , which are the traditional Chinese radicals. Another example is the kanji ( okina , meaning venerable man), which can be seen by a skilled reader as the combination of and , whereas a beginner could only distinguish , , and two instances of .

Taking all such possibilities into account, every kanji in the database is repre-sented as a component tree . Within this component tree, a node denotes a part of the kanji and its children nodes denote its components. Having the root node denote the whole kanji, the tree thus denotes a recursive decomposition into smaller components. Figure 5 shows an example, in this case component trees for the kanjis and discussed above.

In the Kansuke system, any component corresponding to the node of the tree can be encoded by the Kansuke code and used to search the target kanji, as explained in the last section. Such flexibility is realized by this tree structure and is not provided by other kanji search methods based on a component-type look-up system, such as multi-radical methods [1].

The tree structure is also advantageous for maintaining the database, because the computer can dynamically calculate the Kansuke code of a node by summing up the codes of the children nodes. If we change a value at a node, the change is automatically reflected in the upper nodes. This function is especially powerful, because some of the components require the attachment of multiple Kansuke codes as mentioned; we explain this in more detail next.
 4.3 Multiple Codes for Ambiguous Kanjis As the number of stroke prototypes is very small, some stroke types could be difficult to uniquely classify into a type. For example, ( kokoro , heart), is represented in Kansuke code as  X 0-0-4 X , but some users might code it as  X 1-1-3 X .
The easiest solution would be to display all kanjis corresponding to codes with small code differences from the user X  X  entry. In our case, however, this can result in too many candidates, due to the small number of prototypes. For example, even for the smallest difference of one, an average of almost 24 additional kanjis would be added ((three stroke types)  X  (the increase or decrease in the total number of strokes; i.e., 2)  X  4.1 (from Table 3.2)).

We therefore added all the probable codes to each kanji, and hard-coded them in our database. For example, is represented by the codes  X 0-0-4 X  and  X 1-1-3 X . The user can enter either to retrieve this kanji. With this solution, the average number of candidates per entry increases, but only slightly.

In fact, as most kanjis are just a group of components, we only had to con-sider the ambiguity for the component, and the rest of the calculation is done dynamically via the tree explained in the previous section. For now, of about 250 components, 50 have such multiple code definitions. 5.1 Settings To evaluate the effectiveness of the Kansuke system, we compared our method with three other methods: the SKIP code method, the traditional look-up method using only the number of strokes (denoted as  X  X troke count X ), and a multi-radical method (denoted as  X  X adicals X ). There are other conventional look-up methods, but since most require some knowledge of kanji pronunciation they cannot be used by beginners.

A total of 16 subjects participated in the evaluation. Eight were native speak-ers of Japanese or Chinese, and the other eight had no knowledge of the Japanese language or kanjis. We refer to these groups as native speakers and non-native speakers. Note that the non-native speakers were familiar with none of the four methods, whereas the native speakers were familiar with the stroke count, radi-cals, and part of the look-up process used in SKIP.

As it took about 10 minutes to test a method on a subject, only three methods were tested on each subject. Three sets of five kanjis each (sets A, B, and C) were constructed randomly by computer (Table 3), under the condition that the n th kanji of each set had close to the same stroke count. The subjects looked up each set using one of the four systems (stroke count, SKIP, radicals, or Kansuke) in different combinations and in different orders.
 The methods and sets assigned to each subject were chosen according to the Latin square, as shown in Table 4. The order of methods was decided randomly at the run time for each subject. Such experimental conditions ensured that the differences between the three sets (A, B, and C) and the ordering of the methods did not influence the results. Moreover, the interface used to look up kanjis was exactly the same for all methods, except for the GUI buttons and labels that had to be changed to match the type of input data required by each method. Every subject connected to our test website to participate in the experiment. For each method, each subject went through the following stages: 1. Read the instructions and the explanation of the system (one of the four 2. Look up one kanji as a preliminary test under the same environment used 3. Perform the real test for four kanjis using the assigned method. The target Each subject was asked afterwards to make qualitative comments on each system. 5.2 Results The evaluation results are shown in Table 5. The average time needed to look up one kanji and the standard deviation are shown in the second column in seconds. The average number of trials is shown in the third column, except for the SKIP method where this information was not available. The last column shows the percentage of kanjis that the subjects failed to find.

The results in Table 5 show that the performance of non-native speakers was best when using the Kansuke system. Kansuke enabled the shortest look-up time because the average number of trials was low. In addition, the low standard deviation shows how reliably a kanji could be found. Apart from time consid-erations, Kansuke also had the lowest percentage of failures. This is important since a method must work for all kanjis to be effective. When using SKIP, many non-native speakers could not find the target because they failed to select the correct initial pattern. The radical method produced the worst results, with an average time of more than two minutes and a failure rate close to 30%. This poor performance was due to the inability of non-native speakers to properly identify the radicals  X  the arbitrarily defined components  X  of some kanjis.
 In contrast, the native speakers were much faster with the stroke count and SKIP methods. With the traditional method, they only required an average of about 37 seconds to look up a kanji, whereas the non-native speakers required more than 2 minutes. These results reflect the time needed to scan through the displayed kanji candidates as well as the time needed to count the number of strokes. As the native speakers were accustomed to reading kanjis, their scanning speed was much faster, and even the display of many kanjis did not seriously slow them down. The SKIP method also produced a good look-up time, but some failures occurred due to incorrect selection of the initial pattern. Curi-ously, the average number of trials with Kansuke was higher than for non-native speakers. We attribute this low performance of the Kansuke system for native speakers to the fact that for these subjects the Kansuke system was the only completely new method, which required some degree of learning overhead. In addition, as native speakers are used to the traditional way of counting strokes, this knowledge probably prevented these subjects from properly counting the Kansuke strokes.

We also had the subjects complete a questionnaire regarding the pros and cons of the different methods. Their opinions are summarized here:  X  The way of counting strokes with Kansuke is easier for beginners.  X  Both beginners and natives liked the decomposition of kanjis in the Kansuke  X  Even with multiple codes attached, some strokes could not be uniquely classi- X  The traditional method displays too many candidates.  X  The stroke counts of the traditional method and SKIP are ambiguous.  X  The native speakers were used to the traditional method.  X  The SKIP codes were liked by those who were used to counting the number  X  The SKIP codes were ambiguous when selecting the initial pattern.  X  The radicals method is not suitable for some kanjis with no obvious radicals; No method proved to be totally free of ambiguity, making this a problem with all methods. For our method, we may alleviate this problem by attaching more multiple codes as explained in  X  4.3. Overall, though, both beginners and native speakers showed great interest in our system and enjoyed the experiment. We have developed a method for looking up Japanese kanjis using only a small number of stroke prototypes. Our method differs from previous methods in that the stroke classification does not require the user to have any preliminary knowl-edge of kanjis. Given a kanji, the user constructs a Kansuke code, which is based on the counts of three stroke prototypes: horizontal, vertical, and other strokes. For example, the code for is  X 4-3-2 X , because there are four horizontal strokes, three vertical strokes and two other strokes. After this code is entered into a web-based Kansuke interface we have developed, a list of the corresponding can-didates is displayed. Even though the number of prototypes is very small, the average number of candidates for a given code is about 4, whereas that for the traditional method is about 100. Further refinement enables a kanji to be looked up using different codes. This feature is enhanced by a tree-structure decompo-sition of each kanji.
 We conducted a user evaluation with two groups of subjects: Japanese or Chinese native speakers and non-native speakers. The non-native speakers could look up kanjis more quickly and reliably with our Kansuke system, and with fewer failures, than with other existing methods. This demonstrates that our method can support complete beginners who want to access information in East Asian languages.
 In everyday reading tasks, humans distinguish effortlessly between written words. This is despite languages often seeming ill-suited to error-free word recog-nition, through a combination of inter-character similarity (i.e. the existence of graphically-similar character pairs such as [ shi ] and [ tsuchi ] ) and inter-word similarity (i.e. the existence of orthographically-similar word pairs such as bottle and battle ). While native speakers of a language tend to be oblivious to such similarities, language learners are often forced to consciousl y adapt their mental model of a language in order to cope with the effects of similarity. Additionally, native speakers of a language may perceive the same character pair significantly differently to language learners, and there may be radical differences between language learners at different levels of proficiency or from different language backgrounds.

This paper is focused on the similarity and confusability of Japanese kanji characters. This research is novel in that it analyses the effects of kanji con-fusability across the full spectrum of Japanese proficiency, from complete kanji novices to native speakers of the language. Also, unlike conventional psycholin-guistic research on kanji confusability, it draws on large-scale data to construct and validate computational models of similarity and confusability. This data set was collected for the purposes of this research via a web experiment, and con-sists of a selection of both control pairs aimed at targeted phenomena, and also random-selected character pairs. The research builds on psycholinguistic studies of the visual recognition of both Chinese hanzi and Japanese kanji.

The paper is structured as follows. We begin by discussing the background to this research (Section 2), then follow with a description of our web experiment and its basic results (Section 3). We construct some simple models of the sim-ilarity data (Section 4), then evaluate the models using the experimental data (Section 5). Finally, we lay out our plans for future work (Section 6). 2.1 Types of Similarity This paper chiefly concerns itself with orthographic similarity of individual Japanese characters, that is, graphical similarity in the way the characters are written. Note that within the scope of this paper, we do not concern ourselves directly with the question of orthographic similarity of multi-character words. That is, we focus exclusively on inter-character similarity and confusability.
Other than simple orthography, character similarity can also be quantised semantically or phonetically ; identically-pronounced characters are termed ho-mophones . In Chinese and Japanese, radicals 1 are often semantic or phonetic cues of varying reliability in determining character similarity. When radicals are shared between kanji, more than orthographic similarity may be shared. If a radical is reliably semantic, then two kanji sharing it are likely to be semanti-cally related in some way (e.g. kanji containing the radical ,suchas [ mune ]  X  X hest X  and [ ude ]  X  X rm X  are reliably body parts). If reliably phonetic (e.g. , as in [ d  X  o ]  X  X opper X  and [ d  X  o ]  X  X ody X ), the two kanji will share a Chinese or on reading, and will hence be homophones. It may thus not be impossible for skilled readers to give purely orthographic similarity judgements in the presence of shared radicals, since evidence shows these cues are crucial to reading, as to be discussed in Section 2.2. 2.2 Lexical Processing of Kanji and Hanzi In considering potential effects to control for, and types of similarity effects, we draw on the many psycholinguistic studies of kanji or hanzi recognition, with the basic assumption that human reading processes for the two scripts are fundamentally similar. It is beyond the scope of this paper to give these studies full treatment, but we refer the interested reader to some pertinent results.
There is much support for a form of hierarchical activation model for the recognition of kanji (Taft and Zhu 1997, Taft, Zhu, and Peng 1999). In such a model, firstly strokes are visually activated, which in turn activate the radicals they form, which then activate entire kanji. Evidence for such a model includes experiments which showed stroke count effects (summarized by Taft and Zhu (1997)), and numerous radical effects, in cluding radical frequency with seman-tic radical interference (Feldman and Siok 1997, Feldman and Siok 1999), and homophony effects which only occurred with shared phonetic radicals (Saito, Inoue, and Nomura 1995, Saito, Masuda, and Kawakami 1998). There is also evidence that structure may be important for orthographic similarity in general, and for radical-based effects (Taft and Zhu 1997, Yeh and Li 2002). 3.1 Experiment Outline A short web-based experiment was run to obtain a set of gold-standard or-thographic similarity judgements. Participants were first asked to state their first-language background, and level of kanji knowledge, pegged to one of the levels of either the Japanese Kanji Aptitude Test ( ) 2 or the Japanese Language Proficiency Test ( ) . 3 Participants were then exposed to pairs of kanji, in a manner shown in Figure 1, and asked to rate each pair on a five point graded similarity scale. The number of similarity grades cho-sen represents a trade-off between rater agreement, which is highest with only two grades, and discrimination, which is highest with a large number of grades. Although participants included both first and second language readers of Chinese, only Japanese kanji were included in the stimulus. Chinese hanzi and Japanese hiragana and katakana were not used for stimulus, in order to avoid potential confounding effects of character variants and of differing scripts. The pairs were also shuffled for each participant, with the ordering of kanji within a pair also random, in order to reduce any effects caused by participants shifting their judgements part-way through the experiment.

Each participant was exposed to a common set of control pairs, to be dis-cussed in Section 3.2 below. Further, a remaining 100 random kanji pairs were shown where both kanji were within the user X  X  specified level of kanji knowledge (where possible), and 100 were shown where one or both kanji were outside the user X  X  level of knowledge. This was in order to determine any effects caused by knowing a kanji X  X  meaning, its frequency, its readings, or any other potentially confounding properties.

Web-based experiments are known to provide access to large numbers of participants and a high degree of volu ntariness, at the cost of self-selection (Reips 2002). Although participants of all language backgrounds and all lev-els of kanji knowledge were solicited, the nature of the experiment and the lists advertised to biased participants to be mainly of an English, Chinese or Japanese first-language background. 3.2 Control Pairs There are many possible influences on orthographic similarity judgements which we hoped to detect in order to determine whether the data could be taken at face value. A sample pair and a description of each control effect is given in Figure 2. Since the number of potential effects considered was quite large, the aim was not statistical significance for the presence or absence of any effect, but rather guidance in similarity modelling should any individual effect seem strong. All frequency and co-occurrence counts were taken from 1990 X 1999 Nikkei Shinbun corpus data. 3.3 Results The experiment had 236 participants, with a dropout rate of 24%. The partic-ipants who did not complete the experiment, and those who gave no positive responses, were filtered from the data set. The remaining 179 participants are spread across 20 different first languages. Mapping the responses from  X  X ery different X  as 0 to  X  X ery similar X  as 4, the mean response over the whole data set was 1.06, with an average standard deviation for each stimulus across raters of 0.98.

To measure the inter-rater agreement, we consider the mean rank-correlation across all pairs of raters. Although the kappa statistic is often used (Eugenio and Glass 2004), it underestimates agreement over data with graded responses. The mean rank correlation for all participants over the control set was strong at 0.60. However, it is still lower than that for many tasks, suggesting that many raters lack strong intuitions about what makes one kanji similar to another.
Since many of the first language backgrounds had too few raters to do signifi-cant analysis on, they were reduced to larger groupings of backgrounds, with the assumption that all alphabetic backgrounds were equivalent. Firstly, we group first-language speakers of Chinese (CFL ) and Japanese (JFL). Secondly, we di-vide the remaining participants from alphabetic backgrounds into second lan-guage learners of Japanese (JSL), second language learners of Chinese (CSL), and the remainder (non-CJK). Participants who studied both languages were put into their dominant language based on their comments, or into the JSL group in borderline cases. 4
Figure 3 shows mean responses and agreem ent data within these participant groups. This grouping of raters is validated by the marked difference in mean responses across these groups. The non-CJK group shows high mean responses, which are then halved for second language learners, and lowered still for first language speakers. Agreement is higher for the first-language groups (JFL and CFL) than the second-language groups (JSL and CSL), which in turn have higher agreement than the non-speakers. Both of these results together suggest that with increasing experience, participants were more discerning about what they found to be similar, and more consistent in their judgements. 3.4 Evaluating Similarity Models Normally, with high levels of agreement, we would distill a gold standard data-set of similarity judgements, and evaluate any model of kanji similarity against our gold-standard judgements. Since agreement for the experiment was not suf-ficiently high, we instead evaluate a given model against all rater responses in a given rater group, measuring the mean rank-correlation between the model and all individual raters in that group.

We also have reference points to determine good levels of agreement, by mea-suring the performance of the mean rating and the median rater response this way. The mean rating for a stimulus pair is simply the average response across all raters to that pair. The median rat er response is the response of the best performing rater within each stimulus set (i.e. the most  X  X greeable X  rater for each ability level), calculated using the above measure. 4.1 Pixel Difference Model In Section 2.2, we briefly discussed evidence for stroke level processing in visual character recognition. Indeed, confusability data for Japanese learners taken from the logs of the FOKS (Forgiving Online Kanji Search) error-correcting dictionary interface suggests that stroke-level similarity is a source of error for Japanese learners. The example [ ki, moto ]  X  X asis X  and [ bo, haka ]  X  X rave / tomb X  was taken from FOKS dictionary error logs (Bilac, Baldwin, and Tanaka 2003), and is one of the pairs in our  X  X troke overlap X  control subgroup (Figure 2).
This example shows that learners mistake very similar looking kanji, even when there are no shared radicals, if there are sufficient similar looking strokes between the two kanji. Ideally, with a sufficiently rich data set for kanji strokes, we could model the stroke similarity directly. As an approximation, we instead attempt to measure the amount that strokes overlap by rendering both kanji to an image, and then determine the pixel difference d pixel between the two rendered kanji. We can easily move from this distance metric to a similarity measure, as below: This calculation is potentially sensitive both to the size of the rendered images, and the font used for rendering. For our purposes, we considered an image size of 100  X  100 pixels to be sufficiently detailed, and used this in all experiments described here. To attempt to attain reasonable font independence, the same calculation was done using 5 commonly available fonts, then averaged between them. The fonts used were: Kochi Gothic (medium gothic), Kochi Mincho (thin mincho), Mikachan (handwriting), MS Gothic (thick gothic), and MS Mincho (thin mincho). The graphics program Inkscape 5 was used to render them non-interactively.

This method of calculating similarity is brittle. Suppose two characters share a significant number of similar strokes. If the font renders the characters in such a way that the similar strokes are unaligned or overly scaled, then they will count as differences rather than similarities in the calculation. Further robustness could be added by using more sophisticated algorithms for scale and translation invariant image similarity.

Consider the minimum pixel difference of a pair over all possible offsets. This defines a translation invariant similarity measure. Since the current method cal-culates only one alignment, it is an underestimate of the true translation invari-ant similarity. Since characters are rendered in equal-sized square blocks, and radical frequency is position-dependent, the best alignment usually features a low offset between images. The current approximation is thus a close estimate on average, and is considerably less expensive to compute.

Pixel difference is also likely to underestimate the perceptual salience that repeated stroke units (i.e. radicals) have, and thus underestimate radical-level similarity, except where identical radicals are aligned. Nevertheless, we expect it to correlate well with human responses where stroke-level similarity is present. Pairs scored as highly similar by this method should thus also be rated as highly similar by human judgements. 4.2 Bag of Radicals Model Just as the pixel model aimed to capture similarity effects at the stroke-level, we now try to capture them at the radical level. Fortunately, at the radical-level there is an existing data-set which indexes kanji by all of their contained radicals, the radkfile . 6 It was designed to aid dictionary look-up, and serves as a simple method of determining all the unique radicals used by a kanji. Two examples of kanji decomposed into their component kanji are given in Figure 4.

Using all the potential radicals as dimensions, we can map each kanji onto a vector space of radicals, giving it a boolean entry in each dimension determining whether or not the kanji contains that radical. On this vector space, we can calculate the cosine similarity between the two kanji vectors, to achieve a simple similarity measure based on radical composition: Comparing high-similarity examples from the different methods (Figure 5), we can immediately see some drawbacks to this model. Example (g) shows that the number of each radical present is discarded, hence and are considered identical with this method. Example (h) shows that position is also discarded, yet there is evidence that radical effects are position specific (Taft and Zhu 1997, 1999). This model also ignores similarity due to stroke data, yet the existence of high-similarity examples such as (d) and (e) which do not share radicals indicates that stroke overlap can also be a significant contributor to similarity. Larger structure such as layout may also be important for similarity (Yeh and Li 2002), and it too is discarded here.

Nonetheless, radicals are clearly significant in the perception of kanji. If the presence or absence of shared radicals is the main way that individuals perceive similarity, then this model should agree well with human judgements, whether or not they make use of the additional semantic or phonetic information these radicals can encode. The pixel and radical models were evaluated against human judgements in vari-ous participant groups, as shown in Figure 6, and can be compared to the mean rating and median raters. The pixel based similarity method exhibits weak rank correlation across the board, but increasing in correlation with increasing kanji knowledge. The radical model however shows strong rank correlation for all groups but the non-CJK, and better improvements in the other groups.

These results match our predictions with the pixel-based approach, in that it performs reasonably, but remains only an approximation. The radical method X  X  results however are of a comparable level of agreement within the CFL and JFL groups to the median rater, a very strong result. It suggests that native speak-ers, when asked to assess the similarity of two characters, make their judgements primarily based either on the radicals which are shared between the two charac-ters, or on some other measure which correlates well to identification of shared radicals. Intuitively, this makes sense. Native speakers have a great knowledge of the radicals, their meaning and their semantic or phonetic reliability. They also have the most experience in decomposing kanji into radicals for learning, writing and dictionary lookup.

The radical model still has poor correlation with the non-CJK group, but this is not an issue for applications, since similarity applications primarily target either native speakers or learners, who either already have or will pick up the skill of decomposing characters into radicals. To attempt to determine when such a skill gets picked up, Figure 7 shows agreement when raters are instead grouped by the number of kanji they claimed to know, based on their proficiency level. Aside from the [600 , 1000) band, there are consistent increases in agreement with the radical method as more kanji are learned, suggesting that the change is gradual, rather than sudden. Indeed, learners may start by focusing on strokes, only to shift towards using radicals more as their knowledge of radicals improves.
If we compare the histograms of the responses in Figure 8, we can see stark differences between human responses and the two models. The radical model considers the majority of stimuli to be completely dissimilar. Once it reaches stimulus pairs with at least one shared radical, its responses are highly quan-tised. The pixel model in comparison always finds some similarities and some differences, and exhibits a normal style bell curve. Human responses lie some-where in between the pixel and radical models, featuring a much smaller number of stimuli which are completely dissimilar, and a shorter tail of high similarity than found with the pixel model. 6.1 Similarity Several potential improvements could be made to our similarity modelling. In particular, a translation invariant version of pixel similarity could be easily con-structed and tested. On the other hand, the data-set created by Apel and Quint (2004) provides rich stroke data, which would allow a holistic model combin-ing strokes, radicals and layout into a unified similarity metric. This should be superior to both the pixel model, which only approximates stroke-level similar-ity, and the radical model, which discards position and stroke information. The data-set created here allows fast and simple evaluation of any new similarity models, which should help foster further experimentation.

Kanji similarity metrics have many potential uses. A similarity or distance metric defines an orthographic space across kanji, which we can in turn use in novel ways. Our interest lies in dictionary lookup, and indeed a user could browse across this space from some seed point to quickly and intuitively arrive at a target kanji whose pronunciation is unknown. Particularly dense regions of this space will yield easily confusable pairs or clusters of high similarity. Presenting these to learners during study or testing could help these learners to differentiate between similar characters, but also to better structure their mental lexicon. Depending on the level of similarity the application is concerned with, the high amount of quantisation of responses may be a disadvantage for thresholding to only high-similarity responses. This remains one advantage of the pixel model over the radical model. 6.2 Confusability From a similarity metric, the next step would be to determine confusability prob-abilities across pairs of kanji. Since confusability need not be symmetric, there may be other effects such as frequency which also play a role. Several studies of character perception at the word-level have found evidence of asymmetric in-terference effects for low frequency words with high frequency neighbours (van Heuven, Dijkstra, and Grainger 1998).

Similarity provides a means to bootstrap collection of confusability data, use-ful since authentic confusability data is difficult to find or construct. The avail-able data mainly comes from controlled experiments in artificial environments, for example in explorations of illusory conjunctions (Fang and Wu 1989). Hand analysed logs for the FOKS dictionary detected a few accidentally corrected or-thographic confusability examples, suggesting genuine occurrence of these errors (Bilac, Baldwin, and Tanaka 2004). The FOKS system also provides a method of turning a basic confusability model into a source of genuine confusability data. By adding the confusability model to the FOKS error model, any errors successfully corrected using the model will indicate genuine confusion pairs. We thus can create an informed confusability model, which bootstraps a cycle of confusability data collection and model validation. 6.3 Perception There remain many open questions in orthographic similarity effects. Since the control pairs were not numerous enough to statistically determine similarity ef-fects from the various effect types, further experimentation in this area is needed. In particular, it is unconfirmed as to whether semantic or phonetic similarity contributed to the similarity judgements analysed here. It could be tested by comparing pairs that share the same number of radicals, where the shared radi-cals for one pair were reliable semantic or phonetic cues, but the shared radicals for the other pair were not. We have discussed positional specificity of shared radicals as shown by Taft and Zhu (1997, 1999); the same specificity may also occur in radical-based similarity effects, and should be further investigated, as should stroke level effects. We carried out an experiment seeking graphical similarity judgements, intending to form a similarity dataset to use for modelling. Since agreement between raters was moderate, we instead used the human judgements directly as our dataset, evaluating models against it. Two models were proposed, a stroke-based model and a radical based model. The stroke-based model was approximated using pixel differencing, rather than created directly. The pixel model showed medium agreement, but the radical model showed agreement as strong as the best indi-vidual rater for native speakers.

Although the radical-based model X  X  performance may be adequate for some applications, there is much promise for a holistic model taking into account stroke, radical and positional effects. The data-set created here provides a means for quick and effective evaluation of new similarity models for kanji, thus allow-ing much experimentation. As well as seeding new dictionary lookup methods, the similarity models considered provide a basis for confusability models, which are themselves useful for error-correcting lookup, and in turn generating con-fusion data. Such confusion data, along with the similarity judgments collected here, will provide important evidence for understanding the perceptual process for kanji. Each Ch i nese character i s a logograph composed of strokes i n a part i cular sequence . A generally accepted (standard) stroke sequence for each Ch i nese character i s def i ned heur i st i c rules to general i ze the standard stroke sequence [1], for example,  X  X he upper However, the rules are not always accurate s i nce there are many var i at i ons among the automat i cally determ i ne the standard stroke sequence g i ven a stat i c Ch i nese character handwr i t i ng educat i on appl i cat i ons .

In handwr i t i ng recogn i t i on researches, the stroke sequence conta i ns i mportant (HMM) to i dent i fy the character w i th known stroke sequence . Chen et al. [5] characters as code sequences of stroke segments def i ned by the i r shapes, l i ne work made use of the stroke sequence for perform i ng onl i ne handwr i t i ng recogn i t i on . hand movement show i ng the stroke sequence i s recorded . The i nput strokes are then extracted and al i gned w i th the correspond i ng i nstances on the stroke sequence . They are then parsed to the onl i ne character recogn i zer . The authors i n [9] proposed to trace sequence of extracted strokes from characters on an i mage .
 correct stroke sequence [10][11][12] . In [10], the threshold values of each stroke stroke . The system proposed i n [11] and [12] compared the i nput handwr i t i ng w i th the template that i s the standard handwr i t i ng by a sk i lled teacher .
A few researchers have targeted on the problem of recover i ng the stroke sequence characters w i th many strokes so that the standard stroke sequence may not necessary cont i nuous strokes . Lee et al. [16] proposed a method s i m i lar to the one proposed by Sh i momura [13], but w i th the appl i cat i on i n Hangul (Korean character) recogn i t i on .
In th i s paper, we propose a method to est i mate the standard sequence g i ven a set of backward for every pa i r of strokes and use the class i f i cat i on result to def i ne the state stroke sequence i f there i s more than one cand i date from the shortest path algor i thm . handwr i t i ng Ch i nese character i mage . In th i s paper, we focus on the method of stroke been correctly extracted from the i mage . The analys i s of our proposed algor i thm w i ll thus not be affected by any errors from the stroke extract i on . The extracted strokes are of our proposed study .
 seen that there are altogether 4  X  3 = 12 poss i ble trans i t i ons among these four states . character w i th onl i ne strokes but unknown sequence . To complete the route, each trans i t i on costs and i t corresponds to the standard stroke sequence . determ i ne the stroke sequence .
 value (-1 for forward or 1 for backward) hence i t may result i n more than one shortest the best est i mated sequence .
 Block diagram 2.1 State Trans i t i on Cost Computat i on select i ng some po i nts on each stroke and measur i ng some offsets between every Pos i t i onal Features. To compute the pos i t i onal features, we cons i der three po i nts for F i g . 3 shows these three po i nts for two example strokes s i and s j .
We apply several measures to compute the geometr i c d i fference between each pa i r i llustrat i on of these offsets .
 (a) Vert i cal Offset (b) Hor i zontal Offset backward order . (c) Rad i al Offset expressed i n polar coord i nates . It i s def i ned as R = r i  X  r j where . 2 2 forward order .
 before/after s j i n the standard stroke sequence . We have collected some Ch i nese ground truth stroke sequence i nformat i on i s ava i lable . Th i s means that g i ven any two arb i trary strokes from a character, we know from the ground truth whether they are i n We randomly choose one-tenth of the collected data for tra i n i ng the class i f i er, and the other n i ne-tenth for test i ng .
 corresponds to a part i cular offset between var i ous reference po i nts on the strokes ( i. e . ( s , s j ) . We compute the offsets for the same reference po i nts on the two strokes, e . g . , percentage of stroke pa i rs whose forward/backward orders are correctly i dent i f i ed i n step i s to comb i ne these good features to form a s i ngle class i f i er .
We comb i ne the component feature values by a l i near method to f i nd the we i ght and each feature vector conta i ns m elements . We form an n  X  m feature matr i x A and  X  X ackward X  respect i vely . The we i ghts of the features form the dec i s i on boundary and marg i n vector . F i nally, the class i f i cat i on score i s g i ven by  X  class i f i cat i on rate of 93 . 46% . 2.2 Determ i nat i on of the Shortest Path(s) pa i r of strokes form a matr i x and are used as the state trans i t i on costs . Next, we would l i ke to f i nd the path w i th the m i n i mum cost such that all states are v i s i ted once . Th i s path f i nd i ng problem i s equ i valent to the Travel i ng Salesman Problem (TSP) . and i mplementat i ons [18] . Trad i t i onal TSP can be solved by branch-and-bound algor i thms and l i near programm i ng . However, the complex i ty of TSP i s large because i t i s a NP-hard problem . As a result, people developed methods to i mprove the performance of TSP, such as Markov Cha i n and genet i c algor i thm (GA) . In th i s paper, we have adapted the genet i c algor i thm [19] that treats the TSP propos i t i on as a i t may result i n more than one shortest path (stroke sequence) . 2.3 F i nal Dec i s i on energy i s def i ned as the total d i stance moved by the hand dur i ng both pen-up and pen-assumed that the hand moves i n a stra i ght l i ne toward the beg i nn i ng po i nt of the next stroke .

It can be expected that many erroneous cases would have been resulted i f only more clues about the relat i ve stroke order . On the other hand, the shortest path stroke sequences by the shortest path algor i thm, i t i s more l i kely that these cand i date stroke sequences can be ranked properly accord i ng to the total handwr i t i ng energy . As the standard stroke sequence i s generally des i gned for sav i ng handwr i t i ng energy, the sequence w i th the m i n i mal handwr i t i ng energy i s chosen as the f i nal solut i on . product i on errors such that there are no m i ss i ng, extra, concatenated or broken strokes . The captured stroke sequences of each sample character may not be the same as the standard stroke sequence but we have manually i nspected each of them to obta i n the ground truth standard stroke sequence . The number of strokes of these characters var i es from 2 (very s i mple characters) to 14 (qu i te compl i cated characters) . These characters such as  X   X  and  X   X  are non-curs i ve but some conta i n compl i cated structure cons i sted of a few smaller rad i cals .

In the test i ng, we have analyzed altogether 9804 stroke pa i rs from the sample characters w i th more strokes . The second method (Method II) i s proposed by Lau et stroke, wh i ch i s one of the features used i n the i r method, i s not appl i cable .
We have compared our proposed method w i th ex i st i ng methods I [ 13 ] and II [14] i n terms of percentage of fully recovered stroke sequence and the rank d i stance . The percentage of fully recovered stroke sequence means the percentage of characters rank d i stance i s a measure of sequence s i m i lar i ty between the exper i mental result and est i mated sequence and the standard sequence are .
 percentage of fully recovered stroke sequence and rank d i stance versus the number of strokes . F i g . 5(a) shows that our proposed method can correctly est i mate stroke our proposed method i s relat i vely low as compared w i th the ex i st i ng methods .
The Ch i nese character  X   X  i s one challeng i ng case w i th wh i ch our proposed two methods . F i g . 6 shows the standard stroke sequence from the Ch i nese d i ct i onary . F i g . 7 i llustrates the stroke-by-stroke sequences deduced by our method as well as the strokes are near each other as shown i n F i g . 7(b) . The result of Method II looks better . wrong sub-sequence i n the rad i cal  X   X  i ns i de the character  X   X  . Our proposed method i s more robust than the other methods that only cons i der the stroke prox i m i ty 
There are some cases that we st i ll cannot handle . F i g . 8 shows two examples w i th wr i tten after the part  X   X  . It should be noted that for some other characters w i th the same rad i cal  X   X , the vert i cal stroke  X   X  would not be wr i tten last . For example, the standard stroke sequence for the character  X   X  should be  X   X  . Although our As future work, we can break down the character i nto smaller rad i cals, and then determ i ne the stroke sequence by f i rst cons i der i ng stroke subsequence for each rad i cal and then cons i der the sequence of the rad i cals .
 Samples Stroke sequence We proposed a method to determ i ne the standard stroke sequence g i ven a set of pos i t i onal features . Cand i date sequences are found as the path w i th the m i n i mum cost us i ng the solut i on from the Travel i ng Salesman Problem . The best stroke sequence i s deduced accord i ng to the total handwr i t i ng energy i f there i s more than one cand i date proposed method performs w i th h i gher accuracy than ex i st i ng methods .
The determ i nat i on of standard stroke sequence g i ven a set of strokes i s very useful rad i cal  X  rad i cal  X   X  . The character  X  characters i n the wrong stroke sequence . Our algor i thm enables the computer to check student and teacher as i t could gu i de the student how to wr i te the correct sequence and handwr i t i ng homework .
 the ent i re stroke sequence of characters w i th many strokes ( i. e . more than 9 strokes) . Some fa i led cases have been i nvest i gated . These cases are due to the complex/spec i al structure of some Ch i nese characters such that the stroke sequence cannot be general i zed solely i n terms of pos i t i onal features and handwr i t i ng energy . As future work, we w i ll explore more features to make the approach more general . our proposed algor i thm i n other or i ental character sets such as Japanese H i ragana/Katakana and Korean Hangul s i nce these character sets are also logographs that look s i m i lar to Ch i nese characters .
 Acknowledgments. The work descr i bed i n th i s paper was supported by a grant from C i ty Un i vers i ty of Hong Kong (Pro j ect No . 9360092) .
 Bilingual dictionaries have such a great influence on the performance of machine translation (MT) systems and cross-lingual information retrieval applications, etc., that the expansion of these dictionaries is one of the principal objectives in MT system development. Above all, adding multi-word lexical entries is a promising area for dictionary improvement. There are many recurrent multi-word expressions, the proper translation of which is often different from the simple combination of each individually tr anslated component of the expression. In this paper, such a bilingually fixed expression is defined as a  X  X ulti-word expression (MWE) X . Since it is hard to say at this stage that MWEs are regis-tered comprehensively enough in the dictionaries, it is necessary to continue to research and collect them.

Generally speaking, a variance can be seen in the degree to which the proper translation for a nominal MWE matches with the sequence of its individually translated components. This study classifies two-word MWEs into three types as described in Section 2 below, and focuses especially on the two classes of MWEs which match comparatively better in the proper translation and the individual translation, such as  X  X alt shaker X .

There are two methods of acquiring expressions to be registered in bilingual dictionaries: one is using raw data in corpora, etc. (Kaji 2005; Shibata et al. 2005; Utsuro et al. 2005, etc.) and the other is using language resources created by people. (Ogawa et al. 2004; Fujita et al. 2005, etc.) The proposed method in this paper falls into the latter category since bilingual dictionaries and thesauruses constructed by people are used as resources.

The basic concept of this study is as follows: in the MWE  X  X alt shaker X  and its proper translation  X  shio-ire  X  ,  X  X haker X  corresponds to  X  ire  X  and this correspon-dence is highly likely to be accurate in the case of the MWE  X  X pice shaker X , since  X  X pice X  is a word semantically similar to  X  X alt. X  Thus, the MWE  X  X pice shaker X  and its possible translation  X  supaisu-ire  X  are expected to be associatively ac-quired as a new lexical entry. Based on this assumption, this paper proposes a method of creating a new MWE and its possible translation previously unregis-tered in bilingual dictionaries by replacing one of the components of a registered MWE with its semantically similar words.

Some of the new multi-word lexical entries acquired with this method are almost certain to improve the translation quality, but others are not. In order to select appropriate entries, this method prioritizes the pairs of newly acquired MWEs and their possible translations by the criteria described in Section 5 below and outputs the pairs in order of highest priority. In this paper, from the standpoint of how the MWE X  X  proper translation matches with the individual translation, two-word English MWEs are classified into the following three types.
 Compounds with First Matched and Second Unmatched components: When the translation of the first component of an MWE matches with the first component of its proper translation while the translation of its second compo-nent does not match with the second component of the proper translation, it is classified as  X  X irst Matched and Second Unmatched X .

For example, in the bilingual dictionary of the MT system used in this ex-periment, the proper translation  X  shio-ire  X  is given for the MWE  X  X alt shaker X . When the MT system translates them individually,  X  shio  X  is given for  X  X alt X  and  X  sheeka  X  for  X  X haker X . Therefore, the first component X  X  translation  X  shio  X  of  X  X alt shaker X  matches with the first component of its proper translation  X  shio-ire  X  while the second component X  X  translation  X  sheeka  X  X oesnotmatchwiththe second component of the proper translation.
 Compounds with First Unmatched and Second Matched components: When the translation of the first component of an MWE does not match with the first component of its proper translation while the translation of its second component matches with the second component of the proper translation, this MWE is classified as  X  X irst Unmatched and Second Matched X .

For example, the proper translation  X  kaiin-kaisha  X  X sgivenfortheMWE  X  X ember company X . When the MT system translates them individually  X  men-baa  X  is given for  X  X ember X  and  X  kaisha  X  for  X  X ompany X . Therefore, the first component X  X  translation  X  menbaa  X  for  X  X ember company X  does not match with the first component of its proper translation  X  kaiin-kaisha  X  while the second component X  X  translation  X  kaisha  X  for  X  X ompany X  X  matches with the second com-ponent of the proper translation.
 Both Unmatched: When the translation of the first component of an MWE does not match with the first component of its proper translation and also the translation of its second component does not match with the second component of the proper translation, this MWE is classified as  X  X oth Unmatched X . For example, the proper translation  X  seikuu-ken  X  X sgivenfortheMWE X  X irsupe-riority. X  Neither of the components X  translations  X  kuuki  X  for  X  X ir X  or  X  yuuetsu  X  for  X  X uperiority X  individually matches with the proper translation  X  seikuu-ken  X  for  X  X ir superiority X  at all.

This study proposes a method of dealing with the MWE types  X  X irst Matched and Second Unmatched X  and  X  X irst Unmatched and Second Matched X  out of the three types described above. This paper, however, refers only to the process of the MWE type  X  X irst Matched and Second Unmatched. X  For the  X  X irst Matched and Second Unmatched X  MWEs focused on in this pa-per, the translation of the second component does not match with the sec-ond component of its proper translation. It can be assumed that the same thing happens with replacing the first component of the original MWE with a semantically similar words. In other words, it is highly possible that the ap-propriate possible translation of a new MWE is acquired not by just joining each translation of its first and second components together, but by replacing the first component of the proper translation of the original MWE with the translation of a word semantically similar to the first component of the original MWE.

For example, as described in Section 1 above, since  X  ire  X  corresponds to  X  X haker X  of  X  X alt shaker X , the translation for  X  X haker X  must be  X  ire  X , not  X  sheeka  X  X venin the case of the MWE  X  X pice shaker, X  which is created by replacing  X  X alt X  in  X  X alt shaker X  with a word semantically similar to  X  X alt X  ( X  X pice X , etc.) Based on this assumption, this paper proposes a method of acquiring a new MWE and its possible previously unregistered translation in bilingual dictionar-ies. The method proposed consists of three major steps as follows: 1. Extract the  X  X irst Matched and Second Unmatched X  MWEs from bilingual 2. Extract words semantically similar to the first component of the original 3. Prioritize the pairs of the new MWEs and their possible translations. The most exhaustive method of acquiring new MWEs is to create all the possible combinations of the words fitting the rules of English syntax as candidates to be registered in a dictionary. It is, however, not desirable to take this method, since it will create a large number of incompatible combinations or compositional expressions, which are unnecessary to register in the dictionary.

In the method proposed, based on the assumption described in Section 3 above, new MWEs are created based on existing bilingual dictionaries and the-sauruses. More specifically, synonyms for the first component of the original MWE are extracted from a thesaurus. Replacing the first component with its synonym creates the new MWEs.
 WordNet Version 2.0(Miller 1998) 1 is used as a thesaurus. For the given word X ,itssynonym Y is extracted from WordNet only when it satisfies the following condition; Similarity SIM ( X , Y ) (Kurohashi et al. 1996) 2 calculated in equa-tion (1) below, is more than or equal to a certain threshold. The threshold is tentatively set at 0.7.

The values d X and d Y in the equation (1) are the depths from the root node to X and Y in WordNet, respectively, and d C is the depth from the root node to thecommonnodeofboth X and Y . From here on after, asemantically similar word Y refers to a word which has a similarity SIM ( X , Y )ismorethanor equal to the threshold.

A node in WordNet is represented as a word sense not as a word. Therefore when a word has more than one sense, it appears in WordNet more than once. There may exist more than one path from the root node to a given node. In order to extract words semantically similar to a word, all the possible paths for each word sense are followed.

By joining together the translation by an MT system of a word seman-tically similar to the first component of the original MWE, and the second component of the proper translation which corresponds to the second compo-nent of the original MWE, a possible translation for the new MWE is created. As shown in Table 1 below, in the case of the original MWE  X  X alt shaker X ,  X  X pice X  is extracted from WordNet as a w ord semantically similar to the com-ponent  X  X alt X  more than or equal to the threshold. As a possible translation for  X  X pice shaker X ,  X  supaisu-ire  X  is acquired by joining  X  supaisu  X  X sthetrans-lation for  X  X pice X  from the MT system and  X  ire  X  corresponding to  X  X haker X  in  X  X alt shaker X .
 It is important to select appropriate entries for an MT dictionary from the pairs of new MWEs and their possible translations, which are acquired with the method described in Section 4 above. This study proposes a method of prioritizing each pair and outputting the pairs in order of highest priority from the standpoint of the contribution to the better translation quality. 5.1 Prioritizing by Similarity in an English Thesaurus Replacing the first component of the original MWE with its semantically similar word creates a new MWE. Regarding this, it is assumed that the higher word sense similarity between the first com ponent of the original MWE and its se-mantically similar word, the higher the probability of the new MWE as a correct English noun phrase.

Based on this assumption, the similarity SIM ( W E , SimW E ) for the first com-ponent of the original MWE, WE, and its semantically similar word, SimW E ,is calculated in equation (1) described in Section 4 above. The similarity score is employed as priority score Score SeedP air ( NewPair ) for the pair of new MWE and its possible translation, NewPair , created from the pair of the original MWE and its proper translation, SeedP air .
 5.2 Prioritizing by Similarity in a Japanese Thesaurus This section describes countermeasures for the two significant problems found from the observation of the new MWEs created and prioritized through the methods described in Section 4 and Subsection 5.1 above. The problems are: inappropriate selection of the word senses in WordNet and the inappropriate translation by the MT system. It is possi ble to acquire an appropriate possible translation for the new MWE, only when the selection of the word sense in WordNet and its translation by the MT system are both appropriate.

As described in Section 4 above, all the paths for each word sense are fol-lowed in order to extract semantically similar words from WordNet. As a result of this process, when the first component of the original MWE is replaced with a semantically similar words, not only appropriate MWEs but also inappropriate ones may be created. This is because some of the words extracted from WordNet cannot co-occur with the second component of the original MWE. For example, in the case of the original MWE  X  X op vocal X  ( X  poppu-kashu  X ) registered in the bilingual dictionary,  X  tansansui  X  can be selected, as well as  X  ryuukouka  X ,as a word sense for the first component  X  X op X . When the path is followed with the word sense of drink  X  tansansui  X  in WordNet, the semantically similar word  X  X oda X  is acquired. However, the new MWE  X  X oda vocal X  ( X  sooda-kashu  X ),re-placing  X  X op X  with  X  X oda X , is an incorrect selection.

Also, the inappropriate translation by the MT system of a word semanti-cally similar to the first component of the original MWE causes problems. For example,  X  X tate X  is extracted from WordNet as a word semantically similar to the first component  X  X overnment X  of the original MWE  X  X overnment authority X  ( X  seifu-toukyoku  X ). The appropriate translations for  X  X tate X  as the first com-ponent of the new MWE are  X  kokka  X ,  X  shuu  X , etc. The MT system used in this experiment, however, gives the translation  X  joutai  X   X  X ondition X  so that the inappropriate possible translation  X  joutai-toukyoku  X  X screated.

When either or both of the problems described above occur, it is highly pos-sible that the similarity is low between the translation for the first component of the original MWE and the translation for the word semantically similar to the first component of the original MWE. Therefore, as a countermeasure for these two problems, it is useful to introduce a new criterion to lower the priority when the similarity of the translations is low. For such a criterion, the similarity in a Japanese thesaurus is used between the translation for the first component of the original MWE, W J , and the translation for its semantically similar com-ponent , SimW J . The similarity in the Japanese thesaurus is also calculated in equation (1) in Section 4 above. The EDR Electronic Dictionary 3 is used here for a Japanese thesaurus. As an appropriate example, in Table 2, the similarity be-tween the translation  X  shio  X  for the first component of the original MWE  X  X alt X  and the translation  X  supaisu  X  for its semantically similar word  X  X pice X  is 0.667, which is comparatively high. On the other hand, as an inappropriate example, the similarity between the translation  X  poppu  X  for the first component of the original MWE  X  X op X  and the translation  X  sooda  X  for the semantically similar word  X  X oda X  is 0.222, which is very low. Consider also the similarity between the translation  X  seifu  X  for the first component of the original MWE  X  X overnment X  and the translation  X  jotai  X   X  X ondition X  for the semantically similar word  X  X tate X  for  X  X overnment X : 0.0. The priority score for the pair of new MWE and its possible translation, NewPair , acquired from the pair of original MWE and its proper transla-tion, SeedP air , is determined by both the similarity score in the English the-saurus SIM ( W E , SimW E ) and the similarity score in the Japanese thesaurus SIM ( W J , SimW J ). Thus, the adjusted score Score SeedP air ( NewPair )calcu-lated in equation (3) below is given to each pair NewPair .
 5.3 Prioritizing Considered with a Number of Original MWEs Original MWEs in bilingual dictionaries have been judged by their developers as having a positive effect on translation quality. Consequently, a new MWE cre-ated from a larger number of original MWEs is regarded as more plausible than one created from a smaller number of original MWEs; i.e. the former contributes more to the improvement of translation quality than the latter. Since this paper focuses on two-word MWEs of the type  X  X irst Matched and Second Unmatched X , the original MWEs, from which a single new MWE is created, have the same second component in common, such as  X  X alt shaker X  and  X  X epper shaker X . While  X  X pice X  is extracted from WordNet as a word semantically similar to both  X  X alt X  and  X  X epper X ,  X  X arbonate ( tansan X  X n ) X  is extracted as a word semantically simi-lar to only  X  X alt X . In this case, when calculating priority of newly created MWEs,  X  X pice shaker X  takes precedence over  X  X arbonate shaker X .

Based on this idea, when a single pair of a new MWE and its possible trans-lation, NewPair , is created from the pairs of its original MWEs and their proper translations, SeedP air 1 , ..., SeedP air n ,, the final priority for the new pair AggScore ( NewPair ) is given as the sum of each priority for the new pair
For example, as shown in Table 2 below, the priority for new pair (spice shaker, supaisu-ire ) is calculated to be 1.204 by adding priorities 0.556 and 0.648 respectively for the original pairs, when created from original pairs (salt shaker, shio-ire ) and (pepper shaker, koshou-ire ), while the priority for new pair (carbonate shaker, tansan X  X n-ire ) is 0.762 when created from original pair (salt shaker, shio-ire ).
 6.1 Experimental Procedure In this experiment, a version of our English-Japanese MT system 4 was used. Two-word lexical entries were extracted from part of the bilingual dictionary in the MT system. A total of 25,351 entries were extracted for the experiment. They were classified into four types,  X  First Matched and Second Unmatched X ,  X  X irst Unmatched and Second Matched X ,  X  X oth Unmatched X  as described in Section 2, and  X  X oth Matched X . There were some cases in which the combined translations of the first and second components of a new MWE completely match with the proper translation of an MWE registered in the bilingual dictionaries; therefore, they were classified as  X  X oth Matched X . The total percentage of the entries in types  X  X irst Matched and Second Unmatched X  and  X  X irst Unmatched and Second Matched X , which are focused on in this study, is 31.1%. The pairs of new MWEs and their possible translations were created from original MWEs in types  X  X irst Matched and Second Unmatched X  and  X  X irst Unmatched and Second Matched X , and output in order of highest priority.

The newly acquired pairs were evaluated as follows: First, English native speakers evaluated the new MWEs to judge whether or not they were appropriate as English noun phrases. Second, as for the new MWEs judged as appropriate, Japanese native speakers compared their possible translations acquired through our proposed method with their individual translations generated by the MT system, and then evaluated them as  X  X ood X ,  X  X ad X  or  X  X ame X .  X  X ood X  signi-fies that the new possible translation is better than the individual translation and adding this lexical entry is expected to improve the translation quality.  X  X ad X  signifies that the new possible translation is worse than the individual translation and adding this lexical entry would deteriorate the translation qual-ity.  X  X ame X  signifies that both or neither of the possible translation and the individual translation are appropriate and adding this lexical entry would not affect the translation quality. In Table 5 below, the new MWEs evaluated as inappropriate English noun phrases by the English native speakers are grouped with the ones with  X  X ame X  under  X  X afe X . This tallying up was conducted be-cause such inappropriate MWEs rarely appear in actual text and there would be little possibility of their hampering the translation quality. 6.2 Experimental Results and Discussion A total of 759,704 pairs of new MWEs and their possible translations were acquired from the 2,148 pairs of original MWEs and their proper translations in type  X  X irst Matched and Second Unmatched X  with the method described in Section 4 and 5 above. The top 500 prioritized pairs were evaluated.

In order to verify how much each criterion proposed in Section 5 above con-tributes to the improvement of prioritization performance, the results of the following four prioritizing methods are compared: (a) Prioritize by the similarity in an English thesaurus (baseline method). (b) Prioritize using the similarity in both English and Japanese thesauruses. (c) Prioritize considering the similarity in an English thesaurus and the number (d) Prioritize considering the similarity in both English and Japanese the-The performances of the prioritizing methods are shown in Table 3 above. The following points are found in the table, and it can be concluded that the effectiveness of the proposed method is verified: 1. In the baseline method (a), the percentage of  X  X ood X  minus that of  X  X ad X , 2. In the method employing the similarity in a Japanese thesaurus as well as 3. The method considering the number of original MWEs (c)is almost as effec-4. In the proposed method (d), compared to the baseline method (a), although A group of the  X  X ood X  pairs of new MWEs and their possible translations acquired by the proposed method include such pairs as  X  X ockey equipment X  and  X  hokkee-yougu  X  and  X  X ar plant X  and  X  jidousha-koujou  X , which are not listed even in the large-scale bilingual dictionary  X  eijiro  X  5 . This paper gives a method of expanding bilingual dictionaries in MT systems by creating a new MWE and its possible translation which had previously been unregistered in bilingual dictionaries, by replacing one of the components of a registered MWE with a semantically similar words, and then selecting appropri-ate lexical entries from the pairs of new MWEs and their possible translations according to a prioritizing method. In the proposed method, the pairs of new MWEs and their possible translations are prioritized by referring to more than one thesaurus and considering the number of original MWEs from which a single new MWE is created. As a result, the pairs which are effective for improving translation quality if registered in bilingual dictionaries are acquired with an improvement rate of 55.0% for the top 500 prioritized pairs. This accuracy rate exceeds the one marked with the baseline method.
 Nowadays, much attent i on has been g i ven to data-dr i ven (or corpus-based) mach i ne translat i on [2] . Th i s paper focuses on EBMT approach .
 sentence are retr i eved and comb i ned to produce a translat i on based on some heur i st i c cr i ter i on/measures . Most EBMT systems select the best example scored by the from the max i mum entropy (ME) framework [8] i s i ntroduced i nto EBMT i n order to examples .
 bas i cally prefers larger translat i on examples, because the larger the translat i on i s, the d i mens i onal feature space to i nclude general features of d i fferent aspects . conclus i on i s drawn i n Sect i on 6 . The process of EBMT could be descr i bed as : G i ven an i nput sentence, match correspond i ng translat i on fragments, then, recomb i ne them to g i ve the target text .
The overall arch i tecture of our approach i s summar i zed i n F i gure 1 . The f i rst step learn i ng approach i s used to explore what features they have as opt i mal translat i ons, and therefore an opt i m i zed translat i on model i s tra i ned .

In th i s paper, the translat i on model i s constructed under the max i mum entropy translat i on probab i l i ty i s g i ven by :
Th i s approach has been suggested by [13, 14] for a natural language understand i ng task and successfully appl i ed to stat i st i cal mach i ne translat i on by [15] .
We obta i n the follow i ng dec i s i on rule : Hence, the t i me-consum i ng renormal i zat i on i n Eq . 1 i s not needed i n search . To tra i n the model parameters use the GIS (General i zed Iterat i ve Scal i ng) algor i thm [16] . It should be noted that, as was already shown by [16], by apply i ng su i table transformat i ons, the GIS algor i thm i s able to handle any type of real-valued features . In pract i ce, we use YASMET 1 wr i tten by Franz J . Och for perform i ng tra i n i ng . essent i al factors mot i vate our def i n i t i on for features : z The construct i on of feature space should take i nto account l i ngu i st i c aspects as z The features should be selected from source language, target language and 
The feature space wh i ch i s emp i r i cally def i ned accord i ng to the above mot i vat i ons i s shown i n Table 1 . by some mathemat i cal techn i ques to represent spec i f i c mean i ngs i n deeper levels . The follow i ng l i sts the equat i ons for the key features adopted i n exper i ments :  X  Sentence length rat i o to match each other than two sentences whose length rat i o i s far d i fferent .  X  Word al i gnment rate 
The more words are al i gned, the more accurate translat i on i s . word number of source sentence  X  Cover i ng rate Th i s feature measures how much the i nput sentence i s covered by examples .  X  Average frequency of examples 
Th i s feature measures how often the examples are used . w i th spec i f i c problems of the basel i ne EBMT system . 4.1 Exper i mental Sett i ng (BTEC) wh i ch i s prov i ded i n the IWSLT2004 . The exper i ment cons i sts of two parts : close test and open test . In the close test, example base i s bu i lt us i ng the development examples by us i ng the al i gnment method ment i oned i n [18] .

In the exper i ment, the performance of translat i on i s evaluated i n terms of BLEU score [19] . The basel i ne system i s based on the work presented i n [18] . It works i n three steps . F i rst, the i nput sentence i s decomposed i nto fragments accord i ng to the example base, and the opt i mal decompos i t i on i s searched by an evaluat i on funct i on based on some heur i st i c measures . Second, the opt i mal translat i on i s searched for translat i on generat i on i n wh i ch examples  X  orders are ad j usted accord i ng to N-Gram model .

The translat i on model proposed i n th i s paper totally uses 21 features i n the them are features at phrase (example) level such as average frequency of examples, development set of BTEC . A comprehens i ve test and compar i son of the other w i ll be reported elsewhere . 4.2 Results translat i on model i ng prov i des an effect i ve way to comb i ne all these features . Table 3 evaluates the effect of each k i nd of feature adopted i n our translat i on model . at word level are om i tted; In the th i rd row, features at phrase level are om i tted; And i n the last row, features at sentence level are om i tted .

From the results, we see that all three k i nds of features are very helpful i n worse performance . In the close test, features at phrase level are the most i mportant, i t features at word level are the most i mportant, th i s i s because some examples can not be found i n example base, so, features at word level play a more i mportant role .
For more concrete analys i s, we randomly selected 60 proposed translat i ons and checked them by hand . The hand check determ i ned that 28 outputs are correct and the and the numbers of each error are l i sted i n Table 4 .  X  Data sparseness 
Data sparseness i s the error caused by lack of translat i on examples . In such a case, d i ct i onary .
  X  Al i gnment error 
Al i gnment error i s the error caused by i ncorrect al i gnment results  X  Word order 
Word order refers to the case where the word order i s ungrammat i cal .  X  Select i on error 
Select i on error i s the error caused by unsu i table translat i on examples .  X  Others above error types .

Among them, data sparseness i s the most outstand i ng problem . Therefore, we can bel i eve that the system w i ll ach i eve a h i gher performance i f we obta i n more corpora . In order to adequately i ncorporate d i fferent k i nds of i nformat i on that can be explored features of d i fferent aspects .

In the exper i ments, the proposed model shows s i gn i f i cantly better result . The result demonstrated the val i d i ty of the proposed model .
 i ntroduc i ng some features to reflect the word orders of translat i ons . In 2003 the Engl i sh-V i etnamese Mach i ne Translat i on (EVMT) has been found i n Ho pro j ect has been developed i n three phases . The f i rst phase establ i shes framework and f i eld from Engl i sh to V i etnamese as stand-alone system . The second phase w i ll lasts EVMT i s phrasal transfer model [see 5], wh i ch cons i st of analyz i ng, transferr i ng and generat i ng steps .

Phrasal transfer model for EVMT allows system to transfer a source lex i cal entry the problem of EVMT . However, i t makes d i ct i onary more complex and slows down d i ct i onary, wh i ch i s requ i red for generat i ng grammat i cal V i etnamese sentence .
Next sect i on descr i bes word -to  X  phrase transfer model for EVMT . These d i ct i onar i es are d i scussed i n th i rd sect i on before some results has been reported . Word-to-Phrase Transfer model [5] was developed from transfer system of mach i ne syntact i c structure changes i n target text (see f i gure 1) .

Important components of the model are Target language Phrase pattern and Target language Phrase structures that affect lex i cons and structure of target text . target sentence after lex i con mapp i ng . F i gure 2 represents workflow of transfer stage .
The structure and lex i con after analys i s stage has transfer to lex i con and structure i n dest i nat i on text . However, i n case of lex i cal gap, a source word should be translated to analys i s and generat i on phases depend on grammar rules of each language . Th i s sect i on descr i bes three d i ct i onar i es used i n EVMT . 3.1 Engl i sh D i ct i onary Englex supports root words i n follow i ng format : \lf `better \lx AJ-AV \alt Suffix \fea comp \gl1 `good \gl2 &lt;text&gt; &lt;body&gt; &lt;entry&gt; &lt;form&gt; &lt;orth&gt;A&lt;/orth&gt; &lt;orth&gt;a&lt;/orth&gt; &lt;/form&gt; &lt;gramGrp&gt; &lt;pos&gt;n&lt;/pos&gt; &lt;gen&gt;m&lt;/gen&gt; &lt;/gramGrp&gt; &lt;sense&gt; &lt;def&gt;the first letter of the Language1 
Alphabet&lt;/def&gt; &lt;/sense&gt; &lt;/entry&gt; &lt;/body&gt; &lt;/text&gt; 
The advantages of th i s approach are s i mpl i c i ty and therefore, speed . Unfortunately, root lex i cons to surface words, some morpholog i cal parsers l i ke PC-KIMMO are appl i ed .

Add i t i onally, Engl i sh d i ct i onary appl i es XML format to TEI XML template [11] as shown i n F i g . 4 . 3.2 B i l i ngual Engl i sh-V i etnamese D i ct i onary d i ct i onar i es has been developed by HoNgocDuc i n [11] .
 Sample of entr i es i s shown i n f i gure 5 .
 are needed to change the paper-based d i ct i onary to XML mach i ne readable format l i ke f i gure 6 .
 The spec i al feature i ntroduced here i s &lt;schema &gt; wh i ch def i nes pattern of phrase . In th i s example  X  X P X  mean  X  X oun phrase X  and number_of_schema =3 means th i s phrase has noun-verb structure . There are some templates for noun phrases such as noun-noun, noun  X  X d j ect i ve, noun-verb, noun  X  adverb and so on [2] . &lt;text&gt; &lt;body&gt; &lt;entry&gt; &lt;form&gt; &lt;orth&gt; Abuser &lt;/orth&gt; &lt;orth&gt; abuser &lt;/orth&gt; &lt;/form&gt; &lt;gramGrp&gt; &lt;pos&gt;n&lt;/pos&gt; &lt;gen&gt;&lt;/gen&gt; &lt;/gramGrp&gt; &lt;vietnamese_sense sensenumber =1 &gt; &lt;def&gt; ng  X  X  i l  X  m d  X  ng &lt;/def&gt; &lt;gramGrp&gt; &lt;pos&gt;np&lt;/pos&gt; &lt;schema type =  X  X P X  number_of_schema = 3/&gt; &lt;/gramGrp&gt; &lt;/vietnamese_sense &gt; &lt;vietnamese_sense sensenumber =2 &gt; &lt;def&gt; ng  X  X  i l  X  ng m  X  &lt;/def&gt; &lt;gramGrp&gt; &lt;pos&gt;np&lt;/pos&gt; &lt;schema type =  X  X P X  number_of_schema = 3/&gt; &lt;/gramGrp&gt; &lt;/vietnamese_sense &gt; &lt;vietnamese_sense sensenumber =3 &gt; &lt;def&gt; ng  X  X  i s  X  nh  X  c &lt;/def&gt; &lt;gramGrp&gt; &lt;pos&gt;np&lt;/pos&gt; &lt;schema type =  X  X P X  number_of_schema = 3/&gt; &lt;/gramGrp&gt; &lt;/vietnamese_sense &gt; &lt;/entry&gt; &lt;/body&gt; &lt;/text&gt; 3.3 V i etnamese D i ct i onary (f i gure 7) . @ X c c  X  m -d . C  X  m g i  X c kh X ng  X  a th X ch  X  X  i v  X  i a i. C X   X c c  X  m . G X y  X c c  X  m . &lt;entry&gt; &lt;form&gt; &lt;orth&gt;  X  c c  X  m &lt;/orth&gt; &lt;orth&gt;  X c c  X  m &lt;/orth&gt; &lt;/form&gt; &lt;gramGrp&gt; &lt;pos&gt;n&lt;/pos&gt; &lt;/gramGrp&gt; &lt;sense sensenumber =1 &gt; &lt;def&gt; C  X  m gi X c kh X ng  X  a th X ch  X   X  i v  X  i ai &lt;/def&gt; &lt;/sense&gt; &lt;sense sensenumber =2 &gt; &lt;def&gt; C X   X c c  X  m &lt;/def&gt; &lt;/sense&gt; &lt;sense sensenumber =3 &gt; &lt;def&gt; G X y  X c c  X  m &lt;/def&gt; &lt;/sense&gt; &lt;/entry&gt; 
Th i s d i ct i onary works well w i th V i etnamese grammar [2] . To the moment of th i s wr i t i ng, there are more than 20,000 entr i es have been added to sentences only 189 were successful transferred . Among these transferred text, only 112 sentences were acceptable . There are some reasons of the unsuccess : f i rst, analyzed structure too complex, and too amb i guous; second, generat i on i s s i mple and need more soph i st i cated to adapt V i etnamese grammar .
 correspond i ng to a source language word, one that conveys the correct sense of a source word and makes more fluent target language sentences . Translat i on select i on i s s i gn i f i cantly accord i ng to results of translat i on select i on . d i fferent languages thus requ i r i ng more complex knowledge than other problems corpora used for them are not eas i ly ava i lable .

Most of recent researches on translat i on select i on are based on stat i st i cal methods, wh i ch ut i l i ze var i ous text resources such as a parallel corpus, a non-parallel corpus, a language resources through mach i ne learn i ng . A mono-b i l i ngual d i ct i onary, WordNet, and a target language monol i ngual corpus are ut i l i zed to extract features and a small-s i zed b i l i ngual corpus i s used to tra i n mach i ne learn i ng programs .
Mach i ne learn i ng has not been preferred for translat i on select i on because i t i s hard target language corpus, and then some of them are general i zed i nto numer i cal scores spec i f i c word . i nformat i on . Along w i th them, much research has been devoted to extract translat i on i nformat i on for translat i on .

Dagan and Ita i have proposed a new method for sense d i samb i guat i on and translat i on select i on that uses word co-occurrence i n a target language corpus [3] . Based on th i s method, some latest approaches have explo i ted word co-occurrence that i s extracted from target and source monol i ngual corpora [4, 5, 6] . They extract contextual words, they are expected to have s i m i lar mean i ng and to server to reduce data sparseness of word co-occurrence . Those target language based monol i ngual corpus and s i mple mapp i ng i nformat i on between a source word and i ts target words . However, they are apt to select an i ncorrect translat i on because of amb i gu i ty of target word senses for i nd i v i dual source words as shown i n Lee et al . [7] .

To overcome the d i ff i culty of knowledge acqu i s i t i on, some stud i es have attempted types of words or appl i cable only when extra knowledge sources i nclud i ng a corpus . Lee et al . have proposed a hybr i d method that comb i nes sense d i samb i guat i on extract knowledge for sense d i samb i guat i on and word select i on, and i ntroduce selects better translat i on than prev i ous methods based on monol i ngual corpora even i t conducts translat i on between languages i n d i fference l i ngu i st i cs fam i l i es . In th i s paper, I propose a translat i on select i on method that ref i nes the work of Lee and select i on .
 the i r f i nd i ng of  X  X ord-to sense and sense-to-word  X  relat i onsh i p between a source word target language monol i ngual corpus . Scores for four features for sense d i samb i guat i on sense d i samb i guat i on ( sCOOCp, sCASEp and sRCASEp ) and s i x feature for word select i on ( tCOOCp, tCASEp, sRCASEp, stCOOCp, stCASEp , and stRCASEp ) are computed us i ng word co-occurrence i n a target language corpus . 3.1 Features for Sense D i samb i guat i on from a Mono-b i l i ngual D i ct i onary and the k -th sense of s i i s k case .
 means an entry word i s an uncountable noun or l i ke  X  X 6  X  that means a verb entry word cond i t i on . WordNet i s used to test compat i b i l i ty of typ i cal words . sentences, and simEX calculate s i m i lar i ty between context words and words i n i nput sentence, and k examples respect i vely . k based on WordNet . 3.2 Features for Sense D i samb i guat i on from a Target Language Corpus sCOOCp, sCASEp and sRCASEp are features for sense d i samb i guat i on that set have the same sense or s i m i lar usage, so they can be replaced w i th each other i n a translated sentence . sCOOCp, sCASEp and sRCASEp represent how l i kely target sentence, wh i ch are calculated us i ng the equat i on (2) .
A syntact i c relat i on between words i s changed i n the process of translat i on i n some  X  X nswer  X  but, i n i ts translat i on  X  ( jilmum-e dapha-da ) X ,  X  ( jilmun )  X  that i s a translat i on of  X  X uest i on  X  i s not an ob j ect of a verb translat i on  X  ( dapha-computed by us i ng all words i n a i nput sentence as context words i n the equat i on (2), . 3.3 Features for Word Select i on from a D i ct i onary and a Corpus tPOS, tFREQ, tCOOCp, tCASEp, sRCASEp, stCOOCp, stCASEp , and stRCASEp are features for word select i on and they are extracted or calculated based on a d i ct i onary and a target language corpus . A feature tPOS i s a tagged POS of a target word that i s i nteger value of target word frequency i n a target language corpus . tCASEp i s calculated us i ng a s i m i lar way of calculat i ng sCASEp above . tCASEp represents how frequently a target word i n a sense d i v i s i on co-occurs i n a corpus w i th and tRCASEp are calculated by i gnor i ng syntact i c dependency l i ke sCOOCp and cons i der i ng syntact i c relat i ons i n a target language l i ke sRCASEp respect i vely . stCOOCp, stCASEp , and stRCASEp are obta i ned by normal i z i ng tCOOCp, tCASEp tCASEp becomes 1 when ( ) k i Ts has only one element . To make the max i mum d i v i ded w i th i ts max i mum value . mach i ne learn i ng for translat i on select i on i s employed not to depend on features that numer i cal values as expla i ned i n the prev i ous sect i on .
 two features -dPOS and dORD . And then, values for seven features are l i sted  X  simDEF, simEX, TYP, SYN, sCOOC, sCASE and sRCASE . The next value represents the f i rst twelve values for  X   X  and  X   X  are equal because they are i ncluded i n the tFREQ, tCOOC, tCASE, tRCASE, stCOOC, stCASE and stRCASE ) follow after sense appropr i ateness . The last value represents whether a g i ven target word i s a translat i on word i n an al i gned b i l i ngual sentence, wh i ch also has a b i nary value of 0 or 1 . tagged POS, dPOS, dORD, simDEF, simEX, TYP, SYN, sCOOC, sCASE and sRCASE ), the second one for word select i on w i th sense appropr i ateness and the e i ght and they are completely i gnored i n tra i n i ng .
 s i multaneous comb i n i ng of features for sense d i samb i guat i on and word select i on, and translat i on, a word that has a smaller sORD value i s selected as translat i on 1 . The proposed method was automat i cally evaluated w i th the same env i ronment and the same evaluat i on method of Lee et al . [7] . Tra i n i ng data was extracted from test data of Lee et al . From 3,462 content words i n 1,304 example sentences of a Korean-to-Two mach i ne learn i ng algor i thms of C4 . 5 and T i lburg Memory Based Learner select i on system of Lee et al . was evaluated a tagged POS , dPOS, sORD, simEX, SYN and sRCASE . In word select i on, the comb i nat i on of a tagged POS, tPOS, tFREQ and stRCASE showed the h i ghest of translat i on select i on .

In the proposed method, features for sense d i samb i guat i on and words select i on are no i sy tra i n i ng data .
 th i s work, the average number of translat i ons per noun word i s 12 . 3, thus a result of prev i ous researches although the proposed method used automat i cally extracted knowledge w i th exclud i ng word-dependent features i n mach i ne learn i ng and word i n an al i gned target language sentence even though i t i s also a good translat i on of a g i ven source word . of the proposed method w i th a small-s i zed b i l i ngual corpus shows better performance than that of prev i ous methods, add i t i onal exper i ment should be conducted to analyze method by i ntroduc i ng other knowledge resources l i ke a target language thesaurus . Acknowledgments. Th i s paper was supported by Research Fund, Kumoh Nat i onal Inst i tute of Technology .
 Acqu i r i ng translat i onal equ i valence from parallel corpus i s needed i n corpus-based ma-ch i ne translat i on, whether i n stat i st i cal mach i ne translat i on or i n example-based ma-depends on the qual i ty of the i n i t i al word al i gnment [1] . There are numerous researches on word al i gnment [1,2,3,4,5], wh i ch can be class i fied i nto two types, stat i st i cal mod-els and heur i st i c models, accord i ng to the used models . Stat i st i cal models are based on types of the two languages and therefore are language dependent . It i s easy for heur i st i c models to i ntegrate l i ngu i st i c knowledge for i mprov i ng performance g i ven a language pa i rortask[6,7] .

Although the research on general model (language i ndependent) for word al i gnment or task also have spec i al s i gn i ficance . So far reported researches on word al i gnment, whether us i ng stat i st i cal models or heur i st i c models, i nvolved many language pa i rs, such as Engl i sh-French, Engl i sh-Japanese, Ch i nese-Engl i sh, and Ch i nese-Korean . To our knowledge, there i s no report on Japanese-Ch i nese language pa i r . Word al i gnment However, no quant i tat i ve result has been reported .

Th i s paper presents our work on word al i gnment for a Japanese-Ch i nese parallel corpus . We propose a new word al i gnment method by comb i n i ng stat i st i cal model and heur i st i c model . For stat i st i cal model, we use GIZA++ software [2] and exam i ne i ts performance on the Japanese-Ch i nese language pa i r . For heur i st i c model, we extend the method i ntroduced by Ker [4] and i mplement a lex i cal-knowledge based al i gner . We conduct a compar i son between GIZA++ and the lex i cal-knowledge based al i gner . The exper i mental results show that each al i gner has respect i ve advantages . We, therefore, between Japanese and Ch i nese parallel texts . The Japanese-Ch i nese parallel corpus we used i s developed at NICT (Nat i onal Inst i -tute of Informat i on and Commun i cat i ons Technology) of Japan [8] . Hereafter, we call i t NICT Japanese-Ch i nese Corpus . The corpus cons i sts of or i g i nal Japanese sentences from Ma i n i ch i Newspaper and the i rCh i nese translat i ons . The corpus i s already sen-tence al i gned . In Japanese s i de, morpholog i cal and syntact i c structures are annotated s i de, word segmentat i on and part-of-speech are annotated follow i ng the spec i ficat i on of Pek i ng Un i vers i ty [10] . The deta i l of the corpus i sl i sted i n Table 1 . Th i s sect i on descr i bes the i mplementat i on of a lex i cal-knowledge based al i gner . The al i gner cons i sts of two components . The first one i stoestabl i sh rel i able al i gnments and the second one i stoextendal i gnments based on the al i gnments establ i shed i nthe first component . For a g i ven Japanese sentence J and i ts Ch i nese translat i on C ,let W
J and W C denote the i r word sequences . ( j, c ) denotes a word al i gnment between the word j i n W J and the word c i n W C . j i denotes a spec i fied word at the pos i t i on i i n W
J and c l k denotes a spec i fied word sequence w i th length l i n W C that starts at the pos i t i on k . 3.1 Component for Establ i sh i ng Rel i able Al i gnment In th i s component, we cons i der one-to-many al i gnment, the case of a Japanese word j be i ng al i gned w i th a sequence of Ch i nese words c l k (1  X  k  X | W C | , 1  X  l  X  L ) . We s e t L =4 i nth i s paper . Hereafter, we use  X  c to express any word sequence w i th i n the length of 4 . One-to-one al i gnment i saspec i al case when l =1 . Actually, the case only the case of one-to-many i s descr i bed here .

In measur i ng the degree of s i m i lar i ty between two str i ngs, D i ce coeffic i ent defined i n formula (1) i sused . Basedonth i s measure, we define the score of ( j,  X  c ) . The h i gher the score i s, the more l i kely j i sal i gned w i th  X  c .
 Three k i nds of lex i cal resources we explo i t are descr i bed below . Orthography. About a half of Japanese words conta i nkan ji ,theCh i nese characters We then comb i ne the three scores as descr i bed above i n the follow i ng way, where The Algor i thm of establ i sh i ng rel i able al i gnment i s descr i bed as follows . Algor i thm 1. Al i gn j i n W J w i th  X  c i n W C us i ng lex i cal-knowledge . Input W J and W C Output Rel i able al i gnment A rel Step 3 Loop .
 3.2 Component for Extend i ng Al i gnment It i s observed that words w i th i n one syntact i c structure are often to be translated i nto words that belong to the same syntact i c structure i n the target sentence [4] . For exam-ple, when j 1 and j 2 belong to the same syntact i c structure and j 1 has been al i gned w i th al i gnments that are the nearest to  X  j on the left and r i ght and the two al i gnments that are algor i thm we only cons i der one to one al i gnment .

F i rst, add ( Null 0 ,Null 0 ) and ( Null | W J | +1 ,Null | W C | +1 ) to A rel as the leftmost and the r i ghtmost rel i able al i gnments .

Second, search four al i gnments from A rel as follows .
We i llustrate th i s search i ng i nF i g . 1 . Therearefiverel i able al i gnments, expressed i n words 4 and 6 are the nearest words to  X  j =5 ,theCh i nese words 3 and 6 are the black ball .
 can be measured us i ng the follow i ng quant i tat i ve var i ables .
 the last word of  X  c lef t of  X  j , respect i vely .
 Th i rd, est i mate the score of (  X  j,  X  c ) by regard i ng to a lef t of  X  j as follows [7] . d i slocate from the rel i able al i gnment a lef t of  X  j . The smaller the sum of them i s, the larger the score of the al i gnment i s . The i tem e |  X i  X   X  k | also i mposes penalty on the are pos i t i ve or negat i ve at the same t i me . In th i s case, |  X i  X   X k | = ||  X i | X  X   X k || X  from a rel i able al i gnment i n the same d i rect i on or not i s regarded more i mportant .
In the same way, we can calculate the score of (  X  j,  X  c ) by regard i ng to a rig h t of  X  j , a F i nally, select the score w i th the largest value .

Score dis expresses the score calculated by us i ng d i slocat i on i nformat i on . The algo-r i thm for extend i ng al i gnment i s descr i bed as follows .
 Input W J and W C , from wh i ch j and  X  c al i gned i n the first component have been Output Extended al i gnment A aug .
 Step 3 Loop .
 output as al i gnment results . an i ntermed i ary . We then use the bu i lt d i ct i onary i nthewordal i gnment and ver i fy the d i ct i onary . Two used mach i ne-readable d i ct i onar i es are as follows . EDR Japanese-Engl i sh D i ct i onary [11] LDC Engl i sh-Ch i nese D i ct i onary [12] format i on : the number of Engl i sh translat i ons i n common [13], the part of speech, and Japanese kan ji i nformat i on [14] . We then took the results that were ranked w i th i ntop5 . As a result, we obta i ned a Japanese-Ch i nese d i ct i onary that conta i ns 144,002 Japanese entr i es . We evaluated the performance of the i mplemented lex i cal-knowledge based al i gner and GIZA++ tool on the NICT corpus . We randomly selected 1,127 sentence pa i rs from the corpus and manually annotated them w i th word al i gnments . There are totally 7,332 reference al i gnments . The results were evaluated i n terms of three measures, Prec i s i on, Recall and F-measure . In the lex i cal-knowledge based al i gner, the thresholds are set as  X  lex =0 . 85 ,  X  lex =0 . 4 and  X  dis =0 . 8 [4][7] . In the appl i cat i on of GIZA++, two d i rect i ons are tested : the Japanese i s used as source language and the Ch i nese as target language, and v i ce versa . For tra i n i ng data, we used the whole corpus, 38,383 sentence l i sted i n Table 2 .

Och used post-process i ng step that comb i nes the al i gnment results of GIZA++ i n l i z i ng three groups of al i gnment results wh i ch are produced by the lex i cal-knowledge based al i gner, J  X  X  X  C of GIZA++, and C  X  X  X  J of GIZA++, respect i vely . Then a ma j or i ty dec i s i on i s used do dec i de the final result . If an al i gnment appears at the two or three result groups, the al i gnment i s accepted . Otherw i se, i t i s abandoned . In th i sway,we evaluat i on result on the same date i s also l i sted i n Table 2 .
 The lex i cal-knowledge based al i gner obta i ned a h i gher prec i s i on (73 . 4%) and GIZA++ obta i ned a h i gher recall rate (64 . 8%) . The former could correctly al i gn the less frequently occurr i ng words by us i ng lex i cal knowledge, wh i le the latter could not could correctly al i gn the often occurr i ng words, for some of wh i ch the former could not because of the defic i ency of the current b i l i ngual d i ct i onary .
Compared w i th the lex i cal-knowledge based al i gner, the mult i -al i gner ach i eved an i mprovement of 12 . 2% i n prec i s i on, 5 . 9% i n recall rate and 8 . 3% i n F-measure . Com-pared w i th J  X  X  X  C of GIZA++, the mult i -al i gner ach i eved an i mprovement of 33 . 3% i n to our task of ass i st i ng manual annotat i on of word al i gnment .

Two results obta i ned by the lex i cal-knowledge based al i gner are shown i nF i gure 2and3 . The upper are Ch i nese sentences and the lower are Japanese Sentences . In F i gure 2, the one-to-many al i gnment example i s one Japanese word `` D X  X  X  X  X   X   X   X  (classmate) be i ng al i gned w i th two Ch i nese word ``  X  X   X   X  (same class) and ``  X   X   X   X  (classmate) . In F i gure 3, the many-to-one al i gnment example i s two Japanese words `` (culture asset) .
 Th i s paper presents the i mplementat i on of a lex i cal-knowledge based al i gner, wh i ch con-s i sts of two components . The first components obta i nrel i able al i gnments by us i ng three the correspondence between the trad i t i onal and the s i mpl i fied Ch i nese characters . The second component extends al i gnments by us i ng d i slocat i on i nformat i on . The perfor-mance of the i mplemented al i gned i s evaluated and the compar i son w i th GIZA++ i s con-ducted on the NICT Japanese-Ch i nese parallel corpus . Furthermore, a mult i -al i gner i s developed based on the lex i cal-knowledge based al i gner and GIZA++ . The exper i men-ass i st i ng manual annotat i on of word al i gnment .

In the future research, we w i ll i mprove the lex i cal-knowledge based approach to i n-i n GIZA++, proposed by Och .
 One of the most salient features in languages like Korean is the complex behavior of numeral classifiers (Num-CL) linked to an NP they classify. Among several types of Num-CL constructions, the most complicated type includes the one where the Num-CL floats away from its antecedent: 2 (1) pemin-i cengmal sey myeng-i/*-ul te iss-ta There also exist constraints on which arguments can  X  X aunch X  floating quantifiers (FQ). Literature (cf. [1]) has proposed that the antecedent of the FQ needs to have the identical case marking as in (1). However, issues become more compli-cated with raising and causative constructions where the two do not agree in the case value: As given in the raising (2a) and causative (2b), the Num-CL sey myeng  X  X hree CL X  can have a different case marking from its antecedent, functioning as the matrix object. In a sense, it is linked to the original grammatical function of the raised object and the causee, respectively.

Central issues in deep-parsing numeral classifier constructions thus concern how to generate such FQ constructions and link the FQ with its remote an-tecedent together with appropriate semantics. This paper shows that a typed feature structure grammar, HPSG, together with Minimal Recursion Semantics (MRS), is well-suited in providing the syntax and semantics of these construc-tions for computational implementations. 3 We have inspected the Sejong Treebank Corpus to figure out the distributional frequency of Korean numeral classifiers in real texts. From the corpus of total 378,689 words (33,953 sentences), we ident ified 694 occurrences of numeral clas-sifier expressions. Of these 694 examples, we identified 36 FQ examples, some of which are given in the following: (3) a. ... salam-i cengmal han salam-to epsessta. The FQ type is relatively rare partly because the Sejong Corpus we inspected consists mainly of written texts. However, the statistics clearly show that these FQ constructions are legitimate constructions and should be taken into consid-eration if we want to build a robust grammar for Korean numeral classifiers. 4 3.1 Forming a Numeral-Classifier Sequence and Its Semantics The starting point of our analysis is forming the well-formed Num-CL expres-sions. Syntactically, numeral classifiers are a subclass of nouns (for Japanese see [5]). However, unlike common nouns, they cannot stand alone and must combine with a numeral or a limited set of determiners: 5 Semantically, there are tight sortal constraints between the classifiers and the nouns (or NPs) they modify. For example, pen can classify only events, tay machinery, and kwuen just books. Such sortal constraints block classifiers like tay from modifying thin entities like books as in *chayk twu tay  X  X ook two-CL X . Reflecting these syntactic and semantic properties, we can assign the following lexical information to numerals ( num-det ) and classifiers ( cl-n ) within the feature structure system of HPSG and MRS. 6 (5) The feature structure in (5a) represents that there exists an individual x whose CARG (constant argument) value is  X 3 X . The feature NUM is assigned to the numerals as well as to determiners like yele  X  X everal X  and myech  X  X ome X  which combine with classifiers. Meanwhile, (5b) indicates that syntactically a classifier selects a NUM element through the SPR, whereas semantically it belongs to the ontological category person rel . The feature CLTYPE differentiates classi-fiers from common nouns. An independent grammar rule then ensures that only [NUM +] elements can combine with the [CLTYPE +] expression, ruling out unwanted forms such as *ku myeng  X  X he CL X . 3.2 Syntax and Semantics of the Floating Quantifier Constructions As noted earlier, the Num-CL can float away from the NP it classifies. There exist several supporting phenomena indicating that the FQ modifies the following verbal expression. One phenomenon is the substitution by the proverb kule- X  X o so X . As noted in (6), unlike the NI type, only in the NC type, an FQ and the following main verb can be together substituted by the proverb kulay-ss-ta : This means that the FQ in the NC type is a VP modifier, though it is linked to a preceding NP.

Coordination data also support a VP modifier analysis: (7) [namhaksayng-kwa] kuliko [yehaksayng-i] [sey myeng-i] oassta The FQ  X  X hree-CL X  cannot refer to only the second conjunct  X  X irl students X : its antecedent must be the total number of boys and girls together. This means the FQ refers to the whole NP constituent as its reference. This implies that an analysis in which the FQ forms a constituent with the preceding NP then cannot ensure the reading such that the number of boys and girls is in total three.

Given this VP-modifier treatment, the following question then is how to link an FQ with its appropriate antecedent. There exist several constraints in iden-tifying the antecedents. When the floating quantifier is case-marked, it seems to be linked to an argument with the same case marking. However, further com-plication arises from examples in which either the antecedent NP or the FQ are marked not with a case marker, but a marker like a TOP: The data suggest that a surface case marking cannot be a sole indicator for the linking relation, and that we need to refer to grammatical functions. What we can observe is that, regardless of the location, the NOM-marked FQ is linked to the subject whereas the ACC-marked FQ is linked to the object. This observation is reflected in the following lexical information: 7 (9) Asgivenin(9),theNOM-marked num-cl-mw modifies a verbal element whose SUBJ has the same index value, whereas the ACC-marked num-cl-mw modifies a verbal element which has at least one unsaturated COMPS element whose INDEX value is identical with its own INDEX value. What this means is that the NOM or ACC marked num-cl-mw is semantically linked to the SUBJ or COMPS element through the INDEX value. Our system yields the following parsing results for (8b): 8 (10) As seen from the parsed syntactic structure, the FQ sey kay-lul  X  X hree CL-ACC X  (NP-ACC) modifies the verbal expression mek-ess-ta  X  X at-PST-DECL X . How-ever, as noted from the output MRS, this modifying FQ is linked with its an-tecedent sakwa-lul  X  X pple-ACC X  through the relation part-of rel .Leavingaside the irrelevant semantic relations, let X  X  see card rel and apple rel .Asnoted,the ARG0 value (x14) of part-of rel is identified with that of card rel whereas its ARG1 value (x4) is identified with the ARG0 value of the apple rel .Wethuscan have the interpretation that there are three individuals x14s which belongs to the set x4. Further complication in parsing FQ constructions comes from raising, causatives, and topicalization where the FQ and its antecedent have different case values. In such examples, the two need not have an identical case value. For example, as given in (11b), the ACC-marked raised object can function as the antecedent of either the NOM-marked or ACC-marked FQ: In the present analysis in which the case-marked FQ is linked to either the SUBJ or a COMPS element as given in (12), we can expect these variations. Let us consider the lexical entry for the raising verb mitessta  X  X elieved X : (12) (12a) represents the lexical entry for mitessta  X  X elieved X  in (11a) selecting a sentential complement. Meanwhile, (12b) represents the raising verb  X  X hought X  in (11b) in which the subject of the embedded clause is raised as the object. That is, yecatul-ul  X  X omen-ACC X  functions as its object even though it originally (semantically) functions as the subject of the embedded clause.

Equipped with these, our grammar generates the following parsing results for (11a): (13) Syntactically, as noted from the parsed structure, the ACC-marked FQ sey myeng-ul  X  X hree CL-ACC X  (NP-ACC) modifies the VP chakhata-ko mitessta  X  X onest-COMP believed X . 9 Meanwhile, semantically, the ACC-marked FQ is linked to the ACC-marked object yecatul-ul  X  X oman-ACC X . This is because in our grammar the antecedent of the ACC-marked FQ must be an unsaturated complement of the VP it modifies. As noted from the semantic relations part-of rel , card rel and woman rel in the parsed MRS, this linking relation is attested. That is, the ARG0 value (x9) of woman rel is identified with the ARG1 value of part-of rel whereas the ARG0 value of card rel is identical with the ARG0 value of part-of rel . Thus, the semantic output correctly indicates that the individuals denoted by the FQ is a subset of the individuals denoted by the antecedent. For the raising example (11b), our grammar correctly produces two structures. Let X  X  see (14) first. As seen from the parsed syntactic structure here, the FQ sey myeng-i  X  X hree CL-NOM X  (NP-NOM) modifies the complex VP chakhata-ko mitessta  X  X onest-COMP believed X . However, in terms of semantics, the FQ is linked to the subject of the VP that it modifies. 10 This linking relation is once again attested by the MRS structure here. As noted here, the two semantic arguments of part-of rel ,ARG0andARG1,haveidenticalvalueswiththeARG0 value of card rel (x14) and man rel (x4), respectively. (14) (15) Meanwhile, as given in the second parsing result (15), the FQ sey myeng-i  X  X hree CL-NOM X  modifies the simple VP chakhata-ko  X  X onest-COMP X  only. Since the VP that the FQ modifies has only its SUBJ unsaturated, the SUBJ is the only possible antecedent. The output MRS reflects this raising property: The ARG0 value of part-of rel identified with that of card rel whereas its ARG1 value is identified with the ARG0 value of woman rel . Our system thus correctly links the NOM-marked FQ with the ACC-marked antecedent even though they have different case values. One of the complicated issues in building a robust parsing system is whether to cover empirical as well as psychological (intuition-based) data. Even though examples like the case mismatches in FQ occur not often in the corpus data we inquired, we need to deal with such legitimate constructions if we want to develop a system aiming for reflecting the fundamental properties of the language in question.

The grammar we have built within the typed-feature structure system and well-defined constraints, eventually aiming at working with real-world data, has been implemented in the HPSG for Korean. We have shown that the grammar can parse the appropriate syntactic and semantic aspects of the FQ construc-tions. The test results provide a promising indication that the grammar, built upon the typed feature structure system, is efficient enough to build semantic representations for the simple as well as complex FQ constructions.
 Some languages have functional expressions , which consist of more than one word and behave like a single functional word. In English,  X  X n spite of X  is a typical example, which behaves like a single preposition. In natural language processing (NLP), correct detection of functional expressions is crucial because they determine sentence structures and meanings. Implementation of a detector of functional expressions requires a dictionary of functional expressions, which provides lexical knowledge of every functional expression.

The Japanese language has many functional expressions. They are classified into three types according to the classification of functional words: particle, aux-iliary verb, and conjunction. The particle type is sub-divided into five sub-types: case-marking particle, conjunctive particle, adnominal particle, focus particle, and topic-marking particle. A remarkable characteristic of Japanese functional expressions is that each functional expression has many different surface forms; they include derivations , expression variants produced by particle alternation and insertion, conjugation forms produced by the final conjugation component, and spelling variants .

Compilation of a dictionary of Japanese functional expressions for natural language processing requires two lists. The first is a list of headwords of the dictionary; the second is the complete list of surface forms of entries in the first list.

Although there are several lists of Japanese functional expressions such as [2] and [3], compilation of the first list is not straightforward because there is no con-crete agreement on the selection guideline of headwords of Japanese functional expressions. For example, [2] and [3] follow different selection guidelines: both of  X   X  X  X  X  X  X  X  (ni-taishi-te) X  and  X   X  X  X  X  X  X  X  (ni-taisuru) X  are headwords in [2]; only the former is a headword and the latter is its derivation in [3]. We need a way of resolving this type of contradiction to merge different lists of headwords.
The second list is required because NLP systems have to process functional expressions in surface forms that appear in actual texts. Because native speakers easily identify functional expressions in surface forms, there is no explicit list that enumerates all surface forms in diction aries for human use. We need a systematic way of generating the complete list of surface forms for machine use.

This paper proposes a methodology for compilation of a dictionary of Japa-nese functional expressions with hierarchical organization. We design a hierarchy with nine abstraction levels. By using this hierarchy, we can merge different lists of headwords, which are compiled according to different guidelines. This hierarchy also provides a way of systematic generation of all different surface forms. 2.1 Various Surface Forms of Japanese Functional Expressions Several different language phenomena are related to the production of various surface forms of Japanese functional ex pressions. We classify these surface-form variants into four categories: derivations , expression variants , conjugation forms , and spelling variants .

In case two forms that have different grammatical functions are closely related to each other, we classify them into derivations . For example,  X   X  X  X  X  X  X  X  (ni-taisuru) X  and  X   X  X  X  X  X  X  X  (ni-taishi-te) X  are closely related to each other because  X   X  X  X  X  X  X  (taisuru) X  and  X   X  X  X  X  X  X  (taishi-te) X  are different conjugation forms of the same verb. They have different grammatical functions: the former behaves like an adnominal particle and the latter behaves like a case-marking particle. Therefore we classify them into derivations. This view comes from the fact that several case-marking particles can be used as adnominal particles with slightly different forms.

In case two forms have slightly different morpheme sequences with the same grammatical function and meaning except s tyle (formal or informal), we classify them into expression variants . Language phenomena that are related to produc-tion of expression variants are: 1. Alternation of functional words (particles and auxiliary verbs) 2. Phonetic phenomena 3. Insertion of a focus particle
The third category of surface-form variants is conjugation forms .Incasethe last component of a functional expression is a conjugation word, the functional expression may have conjugation forms in addition to the base form. For exam-ple, a functional expression  X   X  X  X  X  X  X  X  (koto-ni-suru ) X  has conjugation forms such as  X   X  X  X  X  X  (koto-ni-shi ) X  and  X   X  X  X  X  X  X  X  (koto-ni-sure ), X  because the last component  X   X  X  X  (suru) X  is a conjugation word.

Some conjugation forms have two different forms: the normal conjugation form and the desu / masu (polite) conjugation form. For example, a variant  X   X  X  X  X  X  X  X  X  (koto-ni-shi-masu ) X  is the desu / masu conjugation form of  X   X  X  X   X  X  X  X  (koto-ni-suru ), X  where  X   X  X  X  X  (shi-masu) X  is the desu / masu form of  X   X   X  (suru). X 
The last category of surface-form variants is spelling variants . In Japanese, most words have kanji spelling in addition to hiragana spelling. For example, both of  X   X  X  X  X  X  X  X  (ni-ataQ-te) X  (hiragana spelling) and  X   X  X  X  X  X  X  X  (ni-ataQ-te) X  (kanji spelling) are used in practice. 2.2 Hierarchy with Nine Abstraction Levels In order to organize functional expressions with various surface forms described in the previous subsection, we design a hierarchy with nine abstraction levels. Figure 1 shows a part of the hierarchy. In this hierarchy, the root node (in L 0 ) is a dummy node that governs all entries in the dictionary. A node in L 1 is an entry (headword) in the dictionary; the most generalized form of a func-tional expression. A leaf node (in L 9 ) corresponds to a surface form (completely-instantiated form) of a functional expression. An intermediate node corresponds to a partially-abstracted (partially-instantiated) form of a functional expression. Table 1 overviews the nine abstraction levels of the hierarchy. From L 3 to L 9 correspond to the phenomena described in the previous subsection. First, we have defined the following order according to the significance of categories of surface-form variants: Then, we have defined the order L 4  X  L 6 and L 7  X  L 8 in order to make a simple hierarchy.
 In addition to these seven levels, we define the following levels.
 L 2 Meaning categories L 1 Headword Because the hierarchy covers from the most generalized form (in L 1 ) of a func-tional expression to the completely-instantiated forms (in L 9 ) of it, any form of a functional expression can be inserted in some position in the hierarchy.
From this hierarchy, multiple lists of headwords can be generated. Our list of headwords is nodes in L 1 . In case you follow the guideline that each headword has the unique meaning, which roughly corresponds to the guideline used by the book [3], nodes in L 2 become headwords. In case you follow the guideline that each headword has the unique grammatical function, nodes in L 3 become headwords.

We design an ID system in which the structure of hierarchy can be encoded; an ID consists of nine parts, each of which corresponds to one of nine levels of the hierarchy (in Fig. 2). We assign a unique ID to each surface form. Because an ID represents the position of the hierarchy, we easily obtain the relation between two surface forms by comparing their IDs. Table 2 shows three surface forms of functional expressions. By comparing IDs of (1) and (2), we obtain that the leftmost difference is  X  X  X  and  X  X  X  at the ninth character; it corresponds to L 5 so they are phonetic variants of the same functional expression. In contrast, the first 4 digits are different between (1) and (3); from this, we obtain that they are completely different functional expressions.
 3.1 Compilation Procedure We have compiled a dictionary of Japanese functional expressions, which has the hierarchy described in the previous section. The compilation process is in-cremental generation of the hierarchy, because we have neither the complete list of headwords nor the list of all possible surface forms in advance.

The compilation procedure of an incremental step is: 1. Pick up a functional expression from [3]. 2. Create a node that corresponds to the given expression and insert it at the 3. Create the lower subtree under the node.

Most of headwords in [3] correspond to nodes in L 2 . Some exceptions corre-spond to nodes in L 4 or L 5 . In order to insert such nodes into the hierarchy, we create the additional upper nodes if necessary.

In step 3, we create the lower subtree under the inserted node, which means enumeration of all possible surface forms of the functional expression. Most of surface forms can be generated automatically by applying generation templates to the inserted node. We manually remove overgenerated (incorrect) forms from the generated subtree. Several exceptional forms are not included in the gener-ated subtree. We manually add such exceptional forms into the subtree.

We have already inserted 412 functional expressions, which are all functional expressions described in [3], into the hierarchy. The number of nodes in each level is shown in Table 1. The number of nodes in L 1 (headwords) is 292, and the number of leaf nodes (surface forms) is 13,958. 3.2 Description of Functional Expressions When we create a leaf node in the hierarchy, we assign the following eight prop-erties to the node. 1. ID (described in Sect. 2.2) 2. Meaning category 3. Readability 4. Style 5. Negative expressions 6. Idiomatic expressions that include the functional expression 7. Example sentences 8. Reference
In practice, we specify the above properties at the appropriate intermediate nodes in the hierarchy, not at leaf nodes. For example, we specify meaning cat-egories at nodes in L 2 ; we specify styles at nodes in L 8 . A standard inheritance mechanism automatically fills all slots in the leaf nodes. This way of specification clarifies the relation between properties and forms of functional expressions; e.g., the style property is independent of spelling variants. There is no large electronic dictionary o f Japanese functional expressions that is available in public.

Shudo et al. have collected 2,500 func tional expressions i n Japanese (1,000 of particle type and 1,500 of auxiliary-verb type) and classified them according to meaning [5,6]. In the list, the selection of headwords is not consistent, i.e., headwords of different abstraction levels exist; they correspond to the nodes at L , L 4 ,and L 5 in our dictionary. This list has no explicit organization structure except alphabetic order.
Hyodo et al. have proposed a dictionary of Japanese functional expressions with two layers [1]. This dictionary has 375 entries in the first layer: from these entries, 13,882 surface forms (in the second layer) are generated automatically. This dictionary does not provide precise classification between two surface forms, such as phonetic variants and spelling variants, which our dictionary provides. We have proposed a methodology for compilation of a dictionary of Japanese functional expressions with hierarchical organization. By using this methodology, we have compiled the dictionary with 292 headwords and 13,958 surface forms. It covers all functional expressions described in [3]. The compilation process of integrating additional functional expressions, which are described in [2], not in [3], is planned in the next step.

Our dictionary can be used for various NLP tasks including parsing, gen-eration, and paraphrasing of Japanese sentences. For example, the use of our dictionary will improve the coverage of the detection method of functional ex-pressions [7]. Experimental evaluation of application of this dictionary to actual NLP tasks is future work.
 Politeness plays an important role in conversations, this is especially so in Japan. The correct use of honorific expressions i s indispensable for maintaining an ap-propriate social distance between individuals. Recently, however, misuse of hon-orific Japanese expressions h as increased. One of the origins for this misuse may be a lack of education regarding honorific conversations. As normative honorific expressions take a long time to learn, a computer-aided education system for learning honorific expressions would be useful. We have developed a computa-tional system to indicate the misuse of honorifics in word form and in perfor-mance of expressions in Japanese speech sentences The proposed system was verified using test data prepared by the authors and also by third-party lin-guistic researchers. The tests showed that the system was able to discriminate between correct and incorrect honorific sentences in all but a few cases. 2.1 Types of Honorific Expressions The honorific expressions commonly used in spoken Japanese sentences can be divided into three types.

Subject honorific expressions ( X  X onkeigo X  in Japanese) are used to show respect toward a person, who is usually the subject of a predicate in the sentence, by elevating the status of the person. For example,  X  X sharu X  is a subject honorific expression which means  X  X peak. X 
Object honorific expressions ( X  X enjougo X  in Japanese) are used to show respect toward a person, who is usually the object of a predicate in the sentence, by humbling the speaker or the subject of the predicate. Furthermore, object honorific expressions can be subdivided into expressions that elevate the status of the object and expressions that do not elevate the status of the object. We call these  X  X bject honorific-a X  and  X  X bject honorific-b. X  For example,  X  X tadaku X  is an object honorific expression-a which means  X  X iven, X  and  X  X ousu X  is an object honorific expression-b which means  X  X peak. X 
Polite expressions ( X  X eineigo X  in Japanese) are used to show politeness, but not necessarily respect, toward a person, who is usually the listener. Polite expressions include an auxiliary verb at the end of a sentence. For example,  X  X oudesu X  is a polite expression which means  X  X  hear that. X  2.2 Categories of Honorific Misuse The misuse of honorific expressions can be divided into two kinds: misuse in word form and misuse in performance.
 Misuse in word form In a misuse in word form, the word form is bad; i.e., differs from the normative honorific forms. We constructed a list of expressions whose word form is bad in terms of honorifics by using traditional Japanese textbooks and articles written by Japanese linguistic researchers. For example,  X  X tadakareru X  is a misuse in word form of  X  X tadaku. X  Misuse in performance In a misuse in performance, the word form is normative, but its honorific feature is inconsistent with the social relationships among the speakers, listeners, and individuals being referred to in the sentence. We took into account the relative social positions and in-gro up/out-group relationships among people to represent the social relationships because many textbooks of Japanese honorifics stated that these factors are important for choosing adequate honorific expressions in performance. Social distance (e.g., familiarity) among people may also affect the choice of honorific expressions in performance (Brown et al., 1987). However, this factor does not strongly relate to honorific norms because we do not need to follow honorific norms strictly when all the people involved are familiar with each other. We, therefore, ignored this factor in the proposed system and assumed all people involved in a sentence were  X  X ot so familiar each other. X  3.1 Restrictions The system deals with sentences that satisfy the following restrictions: 1. Only one predicate, with one subject and one object, is included. 2. Two to four people are involved; a speaker (denoted as  X  X  X ) and a listener 3. Symbols indicating people ( X  X  X ,  X  X  X ,  X  X  X , or  X  X  X ) must be shown in the
These restrictions require that sentences be simple so that the system does not need any high-precision parsing program because no parsing program is cur-rently able to accurately identify the subject and object person for each predicate in complicated sentences. At this time, the following restrictions for the sub-ject and object are assumed under the restrictions enumerated above: { subject, object } = { S,L } ,or { L,S } for two people, { subject, object } = { S,A } , { L,A } , { A,S } , or { A,L } for three people, { subject, object } = { A,B } for four people ( { subject, object } = { B,A } is equivalent to { subject, object } = { A,B } in the system). 3.2 Honorific Features of Sentences As we explained in Section 2.1, honorific e xpressions can be divided into subject honorific, object honorific (object honorific-a and object honorific-b), and polite expressions. These are concerned with the predicate, the honorific titles of the subject/object of the predicate, and the end of the sentence. The honorific fea-tures of the sentences that follow the restrictions stated in Section 3.1 can thus be represented by s , o , e ,and p , whose value assignments are defined in Table 1. These are individually referred to as  X  X onorific elements, X  and a set of them are referred to as  X  X onorific pattern. X  3.3 System Input and Output Figure 1 shows an example of system input and output. The social relationship (i.e., [the number of people involved in the sentence]  X  [relative social position among the people]  X  [in-group/out-group relationship among the people]) among people named  X  X amada, X   X  X ato, X  and  X  X akahashi X  (the fourth person X  X  name was assigned as  X  X imura, X  and these names were replaced by  X  X , X   X  X , X   X  X , X  and  X  X  X  in the system) are shown in the upper right portion of the figure. In this example,  X  X amada (S) X  and  X  X akahashi (A) X  belong to the same organization, named  X  X , X  so they are in the  X  X n-group. X  The social position of  X  X akahashi X  is higher than that of  X  X amada. X   X  X ato X  belongs to another organization, named  X  X , X  so  X  X ato X  is in the  X  X ut-group X  from  X  X amada X  and  X  X akahashi. X  The social relationship among people involved in the sentence can be changed by using the buttons  X  , X   X  , X   X  X , X   X  X , X   X  X , X  and  X  X , X  where  X   X  X nd X   X  are for changing the relative social position, and  X  X , X   X  X , X   X  X , X  or  X  X  X  are for changing the organization of each person.

The system checks the honorific validity of the input sentence and classifies any misuse as word form or performance and points out in the sentence.

The sample sentence shown in the upper portion of Figure 1 is  X  X akahashi X  (a person X  X  name)  X  X a X  (a postpositional particle that indicates the subjective case)  X  X ato X  (a person X  X  name)  X  X ama X  (a honorific title that means  X  X r./Ms. X )  X  X i X  (a postpositional particle which mea ns  X  X o X )  X  X osetumei X  (object honorific-a form of a verb that means  X  X xplain X )  X  X i X  (a conjugation of a verb,  X  X uru, X  that means  X  X o X )  X  X asu X  (a polite auxiliary verb the end of a sentence). The speech intention is  X  X akahashi explains to Sato. X  No misuse in word form was found in this sentence, but a misuse in performance was indicated because the existence of the honorific title for the subject ( X  X an X ) is inconsistent with their social relationship. No misuses in word form and in performance were found in this sentence, So, the system output  X  X nput sentence is correct. X 
The sample sentence shown in the lower portion of Figure 1 is  X  X akahashi X  (a person X  X  name)  X  X an X  (a honorific title that means  X  X r./Ms. X )  X  X a X  (a post-positional particle that indicates the subjective case)  X  X ato X  (a person X  X  name)  X  X ama X  (a honorific title that means  X  X r./Ms. X )  X  X i X  (a postpositional particle which means  X  X o X )  X  X osetumei X  (object honorific-a form of a verb that means  X  X x-plain X )  X  X i X  (a conjugation of a verb,  X  X uru, X  that means  X  X o X )  X  X asu X  (a polite auxiliary verb the end of a sentence). The speech intention is the same men-tioned above. No misuse in word form was found in this sentence, but a misuse in performance was indicated because the existence of the honorific title for the subject ( X  X an X ) is inconsistent with their social relationship. 3.4 Process Flow Figure 2 shows a process flow chart of the system. The process consists of the following steps.
 (Step 1) Replace the persons X  names:  X  X amada, X   X  X ato, X   X  X akahashi, X  and  X  X imura X  with  X  X , X   X  X , X   X  X , X  and  X  X , X  respectively. (Step 2) Obtain a row of morphemes from the input sentence by using the Japanese morphological analysis program. (Step 3) Check the row of morphemes for misuse in word form by using the list of words in bad form. If any misuses are found, proceed to Step 4; otherwise, proceed to Step 5. (Step 4) Output  X  X isuse in word form was found, X  along with all the portions of the input sentence corresponding to the partial rows that have misuse in word form. Then quit the process. (Step 5) Identify the subject and the object of the predicate by using templates prepared by us. (Step 6) Check the honorific type of each partial row of morphemes by using the honorific dictionary (Table 2). Then determine the values of honorific elements s , o , e ,and p by using Table 1. (Step 7) Check the consistency of the honorific elements with the social rela-tionships by using the consistency table, which defines the consistency between them (details of the consistency table are explained in Section 3.5). (Step 8) If all of the honorific elements are defined as consistent with the social relationships in the consistency table, output  X  X he input sentence is correct. X  Otherwise, output  X  X isuse in performance was found, X  the portions of the in-put sentence corresponding to honorific elements whose values are not consistent with the social relationship, and the kind of the inconsistency.

In the example shown in Figure 1, the sentence  X  X akahashi san ga Sato sama ni gosetumei simasu X  has been replaced by  X  X  san ga L sama ni gosetumei simasu X  in Step 1. Then, a row of morphemes,  X  X  X   X  X an X   X  X a X   X  X  X   X  X ama X   X  X i X   X  X o X   X  X etumei X   X  X uru X   X  X asu, X  has been obtained in Step 2. No misuse in word form was identified in the row of morphemes, so Step 4 was skipped. In Step 5, the subject and the object were identified as  X  X  X  and  X  X , X  respectively. Then, in Step 7, the values of honorific elements were assigned as s =1, o =1, e =1, and p = 2 because there are honorific titles  X  X an X  and  X  X ama X  for persons A and L, the honorific type of the auxiliary verb at the end of the sentence is polite ( X  X asu X ), and the honorific type of the predicate ( X  X o X  ver b  X  X uru X ) is object honorific-a. Finally, the existence of the honorific title of person A ( X  X an X ) is judged as misuse in performance because s = 1 is not defined as consistent with the social relationship among S, A, and L in Table 3. 3.5 Consistency Table Table 3 shows a part of the consistency table for three people (S, L, and A). The consistency table defines the consistency between the honorific patterns of the sentences and the social relationships among the speakers, listeners, and indi-viduals being referred to in the sentence, where the social relationship was rep-resented by the combinations of [the number of persons involved in the sentence]  X  [relative social position among persons]  X  [in-group/out-group relationship among persons]. The conditions concerning the subject and the object of the predicate are also described with the social relationship. The symbols  X  X , X   X  X , X   X  X , X  and  X  X  X  indicate persons involved in the sentence, as described above. Sym-bols shown in same/other  X () X  means that the persons indicated by the symbols are in-group/out-group. Additionally, ( X&gt;Y ) means that the social position of the person corresponding to X is higher than that of the person corresponding to Y ,where X and Y are in-group. The symbols  X   X   X  X nd X   X   X  mean logical  X  X ND X  and  X  X R, X  respectively. In the example shown in Figure 1, s = 1 (there is an honorific title for the subject) was judged to be inconsistent with the so-cial relationship, (A &gt; S)(L) and { su b j :A } , because such correspondence was not defined in Table 3.
 The decision list (Rivest, 1987) was used to obtain Table 3 using a training set. The training set was comprised of 819 training data. Each training data is a pair of a social relationship among the people and a sentence that is considered to be honorifically normative on the social relationship. Three sentences were prepared for each condition of possible combinations of [the number of persons involved in the sentence]  X  [relative social position among persons]  X  [in-group/out-group relationship among persons]  X  [subject/object] (273 variations in total). That X  X  why there were 819 (=3  X  273) training data as mentioned above. The sentences in the training set were written by us, following as closely as possible the honorific norms stated or suggested in traditional Japanese textbooks and articles written by Japanese linguistic researchers. The speech intention was set to  X  X peak. X 
The procedure to obtain Table 3 is as follows: (Step 1) Make a table for training. The table contains [the number of persons involved in the sentence], [relative social position among persons], [in-group/out-group relationship among persons], [subject/object], and a honorific pattern, of each training data in the training set. (Step 2) Make a candidate list for consistency table. The candidate list is comprised of features. Each feature is one of the combinations of [the number of persons involved in the sentence]  X  [relative social position among persons]  X  [in-group/out-group relationship among persons]  X  [subject/object]  X  [values of honorific elements], joined by logical  X   X  . X  The candidate list covers all the possible combination of them. (Step 3) Maintain all the features in the candidate list which are consistent with the training table, the other features are deleted in the candidate list. (Step 4) Delete features which are included in other features in the candidate list. (Step 5) Join the remaining features in the candidate list by using logical  X   X  . X 
Each portion in Table 3 can be summarized to simpler conditions. For exam-ple, the conditions corresponding to s = 0 can be summarized to: ( su b j ,S)(L)  X  (S= su b j )  X  (S &gt;su b j )  X  X  su b j :S } . The proposed system was verified by using test set-1 and test set-2. Both test sets were consisted of test data whose format was the same as that of the training data. Validity Check using Test Set-1 We prepared test set-1 to contain correct and incorrect test sets. No test data included in the correct test set contained any misuses. All test data included in the incorrect test set contained some misuses. Both test sets covered all the conditions for each possible combination of [the number of people involved in the sentence]  X  [relative social position among the people]  X  [in-group/out-group relationship among the people]  X  [subject/object] (273 variations in total). The speech intention was set to  X  X peak. X  The training set used to construct the consistency table was used as the correct test set. Both correct and incorrect test sets contained 819 test sets. The experim ental results showed that the system accurately judged all of the test data in the correct test set to be  X  X orrect, X  and that it accurately indicated the misuses in all the test data from the incorrect test set.
 Validity Check using Test Set-2 Test set-2 was prepared by third-party linguistic researchers. Five kinds of speech intentions,  X  X peak, X   X  X hone, X   X  X xplain, X   X  X isit, X  and  X  X how, X  were assumed when preparing the test set. Other variat ions concerning social relationship  X  [subject/object] were the same as those in test set-1. The total number of test sets included in both correct and incorrect test sets was 4,095. The exper-imental results showed that the system judged 99.4% of the correct test set to be  X  X orrect X  but judged the rest of the data (0.6%) to be  X  X isuse. X  The system accurately indicated 97.3% of misuses in the incorrect test set, but judged the rest of them (2.7%) to be  X  X orrect. X  Mo st cases of incorrect responses, i.e. the 0.6% in the former and 2.7% in the latter, were due to differences in honorific norm between the third-party linguistic researchers and us. Because the system was constructed to follow the honorific norms stated or suggested in traditional Japanese textbooks and articles written by Japanese linguistic researchers, the system may tend to regard some sentences as honorifi-cally incorrect even though they are actually permissible in Japanese society. To reveal what kind and how many of the norms used in the system are perfectly acceptable honorific expressions for everyday use by non-experts (people who are not linguistic experts), we performed the following experiment. 5.1 Procedure Forty subjects who are over 30, twenty males and females, participated in the experiments. We used an age requirement because we expected that people over 30 would have considerable experience in use of honorific expressions. Variation in speech intention  X  social relationship  X  [subject/object] were the same as those in test set-2 described in Section 4 (4,095 variations in total). The subjects were required to write at least one sentence for each variation under restrictions 1 to 3 described in Section 3.1. We obtained 54,600 sentences through this experiment. We prepared a test set that consisted of 54,600 data sets by using these sentences so that the data format was the same as that of the training data. 5.2 Experimental Results and Discussion The experimental results showed that the system judged 70% of the test data sets to be  X  X orrect. X  All were considered to be valid. However, the remaining 30% of the test data sets were judged as  X  X isuse. X  Among the test sets judged as  X  X isuse X , 20% of the test sets were judged to be  X  X isuse in word form X  and the remaining 80% of the test sets were judged to be  X  X isuse in performance. X  Typical data of the misuse in performance are as follows. (1)[ s =1 ] is inconsistent with the situation where S and su b j are in the in-group and S &gt; (or =) su b j . Example:  X  X  san ga anata ni hanasitandesho X : (S &gt; A &gt; L). (2)[ s =1 ] is inconsistent with the situation where S and su b j are in the in-group, and L and them (i.e., S and su b j ) are in the out-group. Example:  X  X  san ga watasi ni hanasimasita. X : (A &gt; S)(L). (3)[ o =1 ] is inconsistent with the situation where S and o b j are in the in-group and S &gt; (or =) o b j . Example:  X  X  san ni hanasitanda X : (S &gt; A &gt; L). (4)[ e =1 ] is inconsistent with the situation where S and L are in the in-group and S &gt; L. Example:  X  X  kun ga anata ni ittandesune X : (S &gt; L &gt; A). (5)[ e =0 ] is inconsistent with the situation where S and L are in the out-group. Example:  X  X  san ga B san ni hanasita X : (S)(L)(A)(B). (6)[ p =0 ] is inconsistent with the situation where S and L are in the in-group and su b j&gt; S. Example:  X  X  san ga B kun ni hanasimasita X : (L=A &gt; S &gt; B).
For educational purposes, cases (1), (3), (4), and (6) are not recommended to be indicated as serious misuse because these cases are considered to be per-missible (Kokugoken 1992). However, cases (2) and (5) should be indicated as serious misuse because these cases are considered to be inappropriate for edu-cation (Kokugoken 1992). Such differences in educational importance should be reflected in a graphical user interface of the system (e.g., by changing the display scheme for an serious misuse). We developed a computational system to indicate the misuse of honorifics in word form and in performance of expressions in Japanese speech sentences. The misuse in word form was checked by constructing the list of expressions whose word form is bad in terms of honorifics. The misuse in performance was checked by constructing a consistency table that defines consistency between the honorific features of sentences and the social relationship among the people involved in the sentences. The social relationship was represented by combinations of [the number of people involved in the sentence]  X  [relative social position among the people]  X  [in-group/out-group relationship among the people]. The proposed system was verified using test data prepared by the authors and also by third-party linguistic researchers. The results showed that the system was able to discriminate between the correct and the incorrect honorific sentences in all but a few cases. Furthermore, differences in the educational importance among the norms used in the system were revealed based on the experiments using sentences written by people who are not linguistic experts.
 There is a strong need for a large-scale Chinese corpus annotated with word senses both for word sense disambiguation (WSD) and linguistic research. Although much meet the requirements of practical NLP programs such as machine translation and information retrieval. Although plenty of unsupervised learning algorithms have been put forward, SENSEVAL ([1]) evaluation results show that supervised leaning approaches, in general, are much better than unsupervised ones. Undoubtedly high accuracy WSD needs large-scale word sense tagged corpus as training material ([2]). It was argued that no fundamental progress in WSD could be made until large-scale lexical resources were built ([3]). The absence of a large-scale sense tagged corpus remains one of the most critical bottlenecks for successful WSD programs. In English a word sense annotated corpus SEMCOR (Semantic Concordances) ([4]) has been built, which was later trained and tested by many WSD systems and stimulated large amounts of WSD work. In the field of Chinese corpus construction, plenty of attention has been paid to POS tagging and syntactic structures bracketing, for instance the Penn Chinese Treebank ([5]) and Sinica Corpus ([6]), but very limited work has been done with semantic knowledge annotation. The semantic dependency knowledge has been annotated in a Chinese corpus ([7]), which is different from word sense tagging (WST) orientated towards WSD. [8] introduced the Sinica sense-based lexical knowledge base, but as everyone knows, Chinese pervasive in Taiwan is not the same as mandarin Chinese. SENSEVAL-3 ([1]) provides a Chinese word sense annotated corpus, which contains 20 words and 15 sentences per meaning for most words, but obviously the data is too limited to achieve wide coverage, high accuracy WSD systems. 
This paper is devoted to building a large-scale Chinese corpus annotated with word senses, and the ambitious goal of the work is to build a comprehensive resource for Chinese lexical semantics. The resulting lexical knowledge base will contain three major components: 1) a corpus annotated with word senses; 2) a lexicon containing sense distinction and description; 3) the linking between the lexicon and the Chinese Concept Dictionary (CCD) ([9]). This paper is so organized as follows. The corpus, the lexicon, CCD as well as the interactive model are presented in detail in section 2 as 4 subsections. Section 3 is devoted to discuss the adapted strategy to improve consistency. Section 4 is the evaluation of the corpus. Finally in section 5 conclusions are drawn and future works are presented. 2.1 The Corpus At the present stage the sense tagged corpus mainly comes from three months X  texts balance. The input data for WST is POS tagged using Peking University X  X  POS tagger. The high precision of Chinese POS tagging lays a sound foundation for researches on sense annotating. The emphasis of WST therefore falls on the ambiguous words with the same POS. All the ambiguous words in the corpus will be analyzed and annotated, which is different from most of the previous works that focus on limited specific words. 2.2 The Lex i con Defining sense has long been one of the most heated-discussed topics in lexical semantics. The representation of word senses in the lexicon should be valid and efficient for WSD algorithms in the corpus. 
Human beings make use of the context, communication surrounding and sometimes even world knowledge to disambiguate word senses. For machine understanding, the last resort for WSD is the context. In this paper the feature-based formalism is adopted to describe word senses. The features, which appear in the form  X  X ttribute =Value X , can incorporate extensive distributional information about a word sense. The many different features together constitute the representation of a sense, but the language definitions of meaning serve only as references for human readers. An example of feature-based description of meaning is shown in figure 1 as for some senses of verb  X  /kai1 X . Thus , for example, can be defined in a set of features: 
The granularity of sense distinction has long been a thorny issue for WSD specified will not be regarded as the distinguishing factors when discriminating word senses, such as the goal, the cause of the action of a verb. That is, finer sense distinctions without clear distributional indicators are ignored. As a result, the senses defining in our lexicon are somewhat coarse-grained compared with the senses in conventional dictionaries, and the inter-annotator agreement is more easily reached. [10] argued that the standard fine-grained division of senses for use by human reader may not be appropriate for the computational WSD task, and that the level of sense-discrimination that NLP needs corresponds roughly to homographs. The sense distinctions in our lexicon lie in between fine-grained senses and homographs. 
With the feature-based description as the base, the computer can accordingly do the unification in WSD algorithms, and also the human annotator can correctly identify the meaning through reading the sentence context. What X  X  more, using feature-based formalisms the syntax / semantics interface can be easily realized ([11]). 2.3 The Interact i ve Construct i on To achieve the ambitious goal of constructing a comprehensive resource for Chinese lexical semantics, the lexicon containing se nse descriptions and the corpus annotated with senses are built interactively, simultaneously and dynamically. On one hand, the introspection. The annotator can add or delete a sense entry, and can also edit a sense description according to the real word uses in the corpus. The strategy adapted here conforms to the spirit of lexicon construction nowadays, that is, to commit to corpus evidence for semantic and syntactic generalization just as Berkeley FrameNet project did ([12]). On the other hand, using the sense information specified in the lexicon the human annotators assign semantic tags to all the instances of the word in a corpus. The knowledge base of lexical semantics built here can be viewed either as a corpus in which word senses have been tagged, or as a lexicon in which example sentences can be found for any sense entry. The lexicon and the corpus can also be split as separate lexical resource to study when needed. SEMCOR provides an important dynamic model ([4]) for sense-tagged programs, where the tagging process is used as a vehicle for improving WordNet coverage. Prior to SEMCOR WordNet has already existed and the tagging process served only as a way to improve the coverage. However, the Chinese lexicon containing sense descriptions oriented towards computer understanding has not yet been built, so the lexicon and the corpus are built in the same procedure. To some extent we can say that the interactive model adapted here is more fundamental than SEMCOR. 
A software tool is developed in Java to be used as the word sense tagging interface (figure 1). The interface embodies the spirit of interactive construction properly. In the upper section there displays the context in the corpus, with the target ambiguous word highlighted. The range of the context can shift as required. The word senses with feature-based description from the lexicon are displayed in the bottom section. Through reading the context, the human annotator decides to add or delete a sense entry, and can also edit a sense X  X  feature description. The annotator clicks on the appropriate entry to assign a sense tag to the word occurrence. A sample sentence can corresponding sense entry. 2.4 L i nk i ng Senses to CCD Synsets The feature-based description of word meaning as discussed in 2.2 describes mainly the syntagmatic information but cannot include the paradigmatic relations. A lexical knowledge base is well defined only when both the syntagmatic and paradigmatic information are included. WordNet has been widely experimented in WSD researches. We are trying to establish the linking between the sense entries in the lexicon and the synsets in WordNet. CCD is a WordNet-like Chinese lexicon ([9]), which carries the main relations defined in WordNet and is a bilingual concept lexicon with the parallel Chinese-English concepts to be simultaneously displayed. The offset number of the corresponding synset in CCD is used to convey the linking, which expresses the structural information of the tree. Through the linking to CCD, the paradigmatic relations (such as hypernym / hyponym, meronym / holonym) between word senses in the lexicon can be reasonably acquired. After the linking has been established, the many existing WSD approaches based on WordNet can be trained and tested on the Chinese sense tagged corpus. 
The linking is now done manually. In the word sense tagging interface (figure 1) the different synsets of the word in CCD, along with the hypernyms of each sense (expressed by the first word in a synset), are displayed in the right section. A synset selection window (named Set synsets) contains the offset numbers of the synsets. The annotator clicks on the appropriate box(es) to assign a synset or synsets to the currently selected sense in the lexicon. word senses definition CCD synset offset subcategory valence subject The lexical semantic resource is manually constructed. Consistency is always an important concern for hand-annotated corpus, and is even critical for the sense tagged corpus due to the subtle meanings to handle. 
Actually the motivation of feature-based description of word meaning is intended to lexicon clearly describe the distributional context of word senses, which provide the annotator with clear-cut distinctions between different senses. In the tagging process the guidelines for sense distinction can be set up based on the features, and thus the unified principle may be followed when distinguishing different words X  senses. 
Another observation is that the consistency is easier to keep when the annotator manages many different instances of the same word than handle many different words in a specific time frame, because the former method enables the annotator to establish an integrative knowledge of a specific word and its sense distinction. The word sense tagging interface as shown in figure 2 provides the tool (the window named Find/Replace), which allows the annotator to search for a specific word to finish tagging all its occurrences in the same period of time rather than move sequentially through the text as SEMCOR did ([4]). 
Checking is of course a necessary procedure to keep the consistency. The annotators are also checkers, who check other annotator X  X  work. A text generally is first tagged by one annotator and then verified by two checkers. There are five annotators together in the program, of which three are majored in linguistics and two are majored in computational linguistics. A heated discussion inevitably happens when there exist different views. After discussion, the disagreement between annotators will be greatly reduced. Checking all the instances of a word in a specific process of tagging. A software tool is designed to gather all the occurrences of a word in the corpus into a checking file with the sense KWIC (Key Word in Context) format in sense tags order. Figure 2 illustrates some example sentences containing different senses of verb  X  /kai1 X . The checking file enables the checker to have a closer examination of how the senses are used and distributed, and to form a general view of how the sense distinctions are made. The inconsistency thus can be reached quickly and correctly. 4.1 Inter-annotator Agreement The agreement rate between human annotators on word sense annotation is an important concern both for the evaluation of WSD algorithms and word sense tagged corpus. Suppose that there are n occurrences of ambiguous target words in the corpus. Let m be the number of tokens that are assigned identical sense by two human annotators (the annotator and the checker in this program). Then a simple measure to quantify the agreement rate between two human annotators is p , where / pmn = . Table 3 summarizes the inter-annotator agreement in the sense tagged corpus. Combining nouns and verbs the agreement rate achieves 84.8%, which is comparable to the agreement figures reported in the literatures. [13] mentioned that for the SENSEVAL-3 lexical sample task there was a 67.3% agreement between the first two taggings. Table 3 also shows that the inter-annotator agreement for nouns is obviously higher than verbs. 4.2 The State of the Art The project is now going on. Up to now, 813 nouns and 132 verbs have been analyzed and described in the lexicon with the feature-based formalism. In three-month People X  X  Daily texts together 60,895 word occurrences have been sense tagged. By now this is almost the biggest scale sense tagged corpus for mandarin Chinese. ambiguous words 20 945 word occurrences (including training and test data) This paper describes the construction of a sense-tagged Chinese corpus. The goal is to create a valuable resource both for word sense disambiguation and researches on Chinese lexical semantics. Actually some researches on Chinese word senses have been carried out based on the corpus, and some supervised learning approaches, such as SVM, ME, and Bayes algorithms have been trained and tested on the corpus. A small part of the sense-tagged corpus has been published in the website www.icl.pku.edu.cn. Later we will move to other kinds of texts on account of corpus balance and data sparseness. To analyze more ambiguous words and to describe more senses in the lexicon, and to annotate more word instances in the corpus are of course the next urgent task. To train algorithms and to develop software tools to realize semi-automatically sense tagging are also next undertakings. Acknowledgments. This research is funded by National Basic Research Program of China (No. 2004CB318102). Closed Captions (CCs), which are hidden text in the video signal for deaf and hard of hearing and late deafened people, display the dialogue, narration and sound effects of a TV program. While closed captioning was originally developed for the hearing impaired, it can also be a great benefit for both foreign residents in Korea who do not understand Korean and Korean language learners.

All the terrestrial broadcasting stations in Korea are in a state of transition from analog to Digital Television (DTV) and to High-Definition TV (HDTV). As part of this transition, CCs must also be converted for service from ana-log to digital. Since the first broadcasting with analogue CCs by KBS (Korean Broadcasting System) in 1999, the rate of closed captioning TV program has been reached 31.7% (2005). But, closed captioning for DTV in Korea is not in progress yet and expecting 2007 or later for the DTV closed captioning service.
Closed captioning by Machine Translation (MT) system could be one of the most cost-effective choices for the multilingual closed captioning. There have been several approaches to translate analogue CC automatically with MT systems including ALTo (Simon Fraser Univ., Canada)[1], [2], KANT (CMU, USA)[3], and CaptionEye (ETRI, Korea)[4]. But, these systems resulted in rather hardly understandable translations so that they failed to reach practi-cal systems. One of the major reasons for this is related to translating Named Entities (NEs) of proper names which are very popular in news and drama. NEs convey important information of news articles and drama scripts. So, correct translation of NEs is quite indispensa ble for effective comprehension of news and drama. But, it is impossible to enlist all NEs in the dictionary because we could encounter lots of new NEs of persons, locations and organizations every-day. In most case, we cannot predict the appearance of new entities. And that, the translation of the NEs cannot be achieved by simple combinations of each translation of the words in the NE expression. So, it is quite necessary to col-lect multilingual NEs and their certified multilingual translation equivalences automatically.
 Another problem is related to translating domain-specific terminologies. Most MT systems support multiple domain dictionaries for better translation results. But they rely on the user for stacking multiple domain dictionaries and do not change the dictionaries while the translation is being performed. This can be a crucial weak point when we translate the TV news captions, in which lots of incidents belonging to different domains are reported.

In this paper, we propose a Dynamic Dictionary Adaptation methods for mul-tilingual machine translation of CCs for DTV. To cope with frequent appearance of unregistered NEs and the articles of multiple domains as in TV news pro-gram, we adopted live Web resources of multilingual NEs and their translingual equivalences from Web sites of daily news, providing multilingual daily news in Chinese, English, Japanese and Korean. We also devised a Dynamic Domain Identifier (DDI) for news caption based on decision tree induction in order to activate and stack the multiple domain dictionaries dynamically [5].

In Sect. 2, we survey the related works on previous CC MT systems. In Sect. 3 we introduce Dynamic Dictionary Adaptation including NE Alignment for the translingual equivalences, Dynami c Domain Identifier, and Program Iden-tification. In Sect. 4, we evaluate our system to show the overall performance enhancement. Concluding remarks will be found in Sect. 5. There have been several approaches for Closed Caption (CC) MT systems for EIA-608 analog CC. Simon Fraser Universities in Canada developed a fully auto-matic large-scale multilingual CC MT system, ALTo . In order to handle proper names, they used pattern matching and caching names in a name memory, where previously recognized names are stored, to recognize proper names [1], [2]. With 63 patterns, they reported a recall of 72.7% and a precision of 95.0%. Carnegie Mellon University (CMU) also briefly reported a real-time translation system of business news captions (analogue) from English to German based on their existing multi-engine MT systems [3]. Unknown words including hu-man/company/place name are identified by Preprocessor. They only use in-house knowledge for recognizing proper names.
 In Korea, CaptionEye systems had been newly developed by Electronics and Telecommunication Research Institute (ETRI) [4] from 1999 to 2000. Caption-Eye system is multilingual MT systems among Korean-to-English, English-to-Korean, Korean-to-Japanese and Japanese-to-Korean language pairs. Caption-Eye is essentially a kind of pattern-based system. The system did not pay many attentions to NEs of proper names. So, the system lacks of special module for handling proper names, although the target domain contains news captions. Simple Finite State Automata (FSA) based pattern matching for proper name recognition and gazetteers during morphological analysis are the only resources for handling NEs. In this section, we present three components of Dynamic Dictionary Adapta-tion. The first one is an Automatic Named Entity Alignment for gathering mul-tilingual NE translingual equivalences from the Web pages of daily news. The second is Dynamic Domain Identification for automatic dictionary stacking for news captions which has multiple domain articles. The last one is a Program Identification for activating program specific dictionary like dramas. By using Electronic Program Guide (EPG) information of TV programs, we could also exploit the very specific knowledge of a definite TV program when to trans-late the CCs of the program. With this dynamic adaptability of MT systems, we could make the translation quality higher. 3.1 Named Entity Alignment for Multilingual Named Entity Named Entities (NEs) of proper names are very popular in news and drama. NEs convey important information of news articles and drama scripts. So, correct translation of NEs is quite indispensabl e for effective comprehension of news and drama. But, it is impossible to enlist all NEs in the dictionary because we could encounter lots of new NEs of persons, locations and organizations everyday. And that, the translation of the NEs, cannot be achieved by simple combinations of each translation of the words in the NE ex pression. Incorrect translations of NEs make TY program viewers hard to understand the news. So, correct translation of NEs is quite indispensable for effective comprehension of news CCs. The same problems concerning to NEs can be found when to translate CCs of TV dramas.
We try to raise the translation quality higher to the commercial level by solving the translation problems of multilingual NEs with very practical and integrated techniques of machine translation and information extraction. Our solution is obtaining live translingual equivalences of NEs from Web sites of daily news, providing multilingual daily news in Chinese, English, Japanese and Korean. Most of the significant news articles can be found on the Web sites before we watch the same news on TV. So, we devised an intelligent Web crawler for extracting multilingual NEs from the Web sites and aligning their translingual equivalences from the non-parallel, content-aligned multilingual news articles. The aligned translingual NEs are lively updated in order to be used by the multilingual CC MT systems from Korean into Chinese/English/Japanese when the similar news on TV is translated.
 The method for finding translingual equivalences between Korean and English NEs is basically based on the Feature Cost Minimization Method proposed by Huang et al. (2003) [6]. They proposed to extract NE translingual equiva-lences between Chinese and English based on the minimization of linearly com-bined multi-feature cost minimization. The costs include transliteration cost and translation cost, based on IBM model 1, and NE tagging cost by an NE identi-fier. They required NE Recognition on both the source side and the target side. They reported the NE translingual equivalence with 81translation score by 0.06 of NIST8 score (from 7.68 to 7.74). We adopt only two features: transliteration cost and translation cost. It is because that Korean and English NE Recognizers we developed are based on the SVM framework. SVM only output hard deci-sion values for 2-class problem, zero or one. So, NE tagging cost used in [6] is meaningless in our model.
 In case of the aligning between Korean and English NEs, the alignment cost, C (K ne ,E ne ) for a translingual NE pair of English and Korean (K ne ,E ne )is their linear combination of the transliteration score, C translit (K ne ,E ne )and The transliteration score, C translit (K ne ,E ne ) and translation cost C translat (K ne ,E ne ) are adopted from [6] with modifications for Korean-to-English transliteration. Korean character (syllable) is almost independently transliter-ated into an English letter string through  X  X orean Syllable-to-Romanization X  Table. Considering that mappings from Korean character to their English strings are mostly in a determinate way, ie., P(e i | k i )  X  1. Given a Hangeul (Korean Alphabet) sequence (k =  X  X  1 ...k l (k) X ) and a set of English letter sequence E = { e 1 , ...,e n likely transliteration A* that maximize the transliteration likelihood as follows:
Word translation probability P(e | k) can be estimated using the alignment models for statistical machine translation. Let K ne denote a Korean NE and it is composed of i Korean words, k 1 ,k 2 , ... ,k i .LetE ne denote an English NE and it is composed of j English words, e 1 ,e 2 , ... ,e j . The translation probability of a Korean and English NE pair P translat (K ne ,E ne ) is computed as follows, which is known as the IBM model-1: 3.2 Dynamic Domain Identifier for Automatic Domain Dictionary The base MT systems support 28 domain dictionaries for better translation re-sults. But it relies on the user for stacking multiple domain dictionaries and does not allow the user to change the dictionaries while the translation is being per-formed. This can be a crucial weak point when we translate the news captions, because a news program reports lots of incidents belonging to different domains. So, we devised a Dynamic Domain Identifier (DDI) for news caption trans-lation based on decision tree induction (C5.0) [5] in order to activate and stack the multiple domain dictionaries dynamically.

We identify the domain of an article with its lead sentence. Lead sentence is the first statement by the anchor, which highlights the topic of succeeding article. The DDI detects the shift of a new article by semi-structural analysis, analyzes the lead sentence and identifies the domain of subsequent article. The shift of a new article can be recognized at the very beginning of the news or just after the end of an article. The DDI activates top-one to top-three domain dictionaries with priorities. By using the proper stacking of domain-specific dictionaries, transfer ambiguities can be considerably resolved, and the quality of MT can be raised. We evaluate the DDI with lead sentences of MBC 9 Newsdesk caption corpus in January, 2005, which amounts to 1,060 sentences of 814 articles. The accuracy of first-ranked domain shows 65.7%. In the case of top-two, the accuracy increased up to 78.2%. The accuracy reaches 93% of precision with top one to three candidates. We use top-three domains for activating domain dictionaries. 3.3 Program Identifier for Program Specific Knowledge Program System Information Protocol (PSIP) in ATSC DTV transport stream provides the program information which enables the DTV viewer to surf the world of DTV programs and easily choose a program. PSIP also provides meta-data that may be used by the DTV receiver to display information about cap-tioning (e.g. notifying the presence of CC) [7]. The program information is very crucial for activating program-specific dictionaries and program-specific compo-nents (e.g. domain identification for news caption) in CC translation system. TV Drama, especially soap opera, is a serialized program in which the same characters appear and the storyline follow the day-to-day lives of the characters. So, the NEs of the persons, locations and organizations) found in the current drama will surely be presented next time. We accumulate the program-specific NEs and provide their multilingual translations by live updating through smart update server. This program-specific translation knowledge is activated when the program name of current channel is identified. We evaluate our multilingual CC translation system in two different ways: a subjective evaluation to measure the overall performance enhancement of mul-tilingual CC translation system, and an objective one to quantify the effects of three components (NEA, DDI, and PI) we adopted to the base MT systems.
 The evaluation news corpus is part of the MBC 9 Newsdesk caption corpus in 2005. The corpus is unseen while the system has been developed. Table 1 shows the statistics of the evaluation news corpus. 4.1 Subjective Evaluation: MOS Test As a subjective evaluation, we used Mean Opinion Score (MOS) test, which is one of the voice quality evaluation methods for Text-to-Speech (TTS) systems. The translation results were mapped to a full five point MOS scale, ranging from 1 to 5 based on the clearness of the translated sentence [8].

The empirical results of MOS evaluation are shown in Table 2. The columns except for those in the bottom line are the number of sentences rated 1 to 5. The average score is calculated by dividing the total score by the total number of sentences. With the integration of three components, we obtained less impressive enhancement of 0.1 for Korean-to-Japanese by (4.6-4.5). That X  X  because of the similarity between two languages and the initial higher translation quality of the base Japanese-to-Korean MT system. But, the proposed Dynamic Dictionary Adulteration approach turn out more effective in Korean-to-Chinese and Korean-to-English as high as 0.5 and 0.5, respectively. The average enhancement is 0.37, which means almost a half level up to the next higher MOS scale. 4.2 Objective Evaluation: BLEU and NIST11 Score In order to quantify the effects of the effects of three components (NE Align-ment, Dynamic Domain Identification, and Program Identification); BLEU score by IBM, and NIST mteval V11 score (NIST11 score) by NIST [9]. Table 3 demon-strates the enhancement of CC translation with the integrated Dynamic Dictio-nary Adaptation components of NEA, DDI, and PI in case of Korean-to-English CC translation.

With the encouraging effects of NE Alignment, PI, and DDI, we could raise the overall translation accuracy. For ne ws CCs, as we expected, the translingual NE recognition is the most effective. Those enhancements are meaningful to translation quality.
 In this paper, we present a preliminary multilingual CC translation system for Dital TV. In order to raise the translation quality at the practical level, we pro-posed Dynamic Dictionary Adaptation methods including multilingual Named Entity Aligning, Dynamic Domain Identification, and Program Identification. With the proposed integrated Dynamic Dictionary Adaptation approaches, we obtained average enhancement of 0.37 i n MOS (Mean Opinion Score) for Korean-to-Chinese (2.9 to 3.4), Korean-English (3.1 to 3.6) and Korean-Japanese (4.5 to 4.6) in a news domain.The enhancement 0.37 means almost a third level up to the next higher MOS scale. The proposed methods is language independent. So, it is applicable to any language pairs.
 Since many inconsistencies are likely to exist in the thesauri compiled by human lexicographers, such thesauri should be revised or reexamined by comparing them to objective data extracted from corpora. There are two main approaches to extracting thesauri or an ontology from huge corpora automatically. One is categorization into semantic classes by calculating the distributions of words in the corpus using extraction of semantic relationships among words, such as hypernym-hyponym relationships or part-whole relationships, from corpora using syntactic patterns ([1], [2], [7], [12]). A proper semantic class and its suitable label (or conceptual name) for a semantic class are important components in the organization of a thesaurus and ontology, as is the overall structure of concepts from the top to the bottom levels. We have been developing a means of automatically organizing the concepts of adjectives Japanese words. From corpora, we extract the concepts of adjectives  X  i.e., abstract nouns categorizing adjectives  X  semiautomatically, and then, we classify the concepts of adjectives by using Kohonen X  X  self-organizing map and introducing a directional similarity measure to calculate an inclusion relationship. As a result, we obtain the similarity and hierarchical relationships of concepts of adjectives on the map. 
In this paper, we explain how the self-organizing map is constructed, focusing especially on the hierarchical relationships among the concepts of adjectives. 
In Section 2 we explain the method to extract class names of adjectives from corpora. In Section 3, we explain the encoding of our input data for Kohonen X  X  SOM and how hierarchies of the concepts of adjectives are constructed. In Section 4, we surface and qualitative characteristics. We compare our created hierarchies with those of the EDR lexicon, a huge handcrafted Japanese lexicon, by using Scheffe X  X  paired comparison method [15]. We conclude with Section 5. Consider the Japanese syntactic structure,  X  X oun1 wa Noun2 ga Adj, X  where  X  X oun1 wa  X  refers to a topic and  X  X oun2 ga  X  refers to a subject. According to Takahashi [17], gentle), X   X  seishitsu (nature) X  (Noun2) is a superordinate concept of  X  otonashii (gentle) X  (Adj), and the adjective  X  otonashii (gentle) X  includes the meaning of an abstract noun  X  seishitsu (nature). X  In this sentence, an abstract noun  X  seishitsu (nature) X  can be omitted without changing the meaning of the sentence; i.e., the meanings of  X  Yagi wa otonashii (A goat is gentle), X  and  X  Yagi wa seishitsu ga otonashii (The nature of a goat is gentle) X  are the same. Our method is first to extract from the corpora all nouns preceded by the Japanese expression  X  toiu,  X  which is typical Japanese expression, which introduces some information about the referent of the noun, such as an apposition. Therefore, we can elucidate the content of nouns found in this pattern by means of their modifiers. We then use syntactic patterns such as (1)  X  X oun1 wa Noun2 ga Adj X  and (2)  X  X dj + Noun2_ no + Noun1 X  to determine instance-category relationships among the extracted data. Noun1 is a concrete noun adnominal noun usage. From the data, we manually select examples in which Noun2 can be omitted without changing the meaning of the original sentence or phrase. If Noun2 can be omitted, Noun2 may be an abstract concept of the modifying adjective. We have collected 365 abstract nouns from two years worth of articles from the Mainichi Shinbun, a Japanese newspaper, and extracted adjectives co-occurring with abstract nouns in the manner described above from 100 novels, 100 essays, and 42 years worth of newspaper articles. A self-organizing map (SOM) can be visualized as a two-dimensional array of nodes on which a high-dimensional input vector can be mapped in an orderly manner through a learning process. After learning, a meaningful nonlinear coordinate system for different input features is created over a neural network. Such a learning process is competitive and unsupervised, and is called a self-organizing process [9]. In our previous work, similarity relationships between concepts were computed using feature vectors calculated from the Euclidian distance. In our current work, hierarchical relationships are computed as well as similarity relationships by introducing a directional similarity measure to the SOM. We used the complementary similarity measure (CSM) as the directional similarity measure to calculate an inclusion relationship for our data. The details of this are given in Section 3.1.2. An example of an extracted map is shown in Fig. 1. All concepts are distributed from the top concept  X  X oto (thing) X  to hyponyms on the map. The coordinates on the vertical and horizontal axes indicate the location of each concept based on the similarity between concepts calculated by the SOM using the CSM. The height of each concept above the plane in Fig. 1 corresponds to a CSM value of an inclusion relationship with  X  X oto (thing). X  In the next section, we explain the details regarding our input data for the SOM, the CSM, and the construction of hierarchies of all concepts using CSM values. 3.1 Self-Organ i z i ng Map The basic method of encoding the SOM follows Ma et al. [13]. Input Data As explained in Section 2, we extracted abstract nouns categorizing adjectives and made a list like the one shown in Table 1 as learning data. 
There were 365 abstract noun types, 10,525 adjective types, and 35,173 adjective tokens. The maximum number of co-occurring adjectives for a given abstract noun was 1,594. Encod i ng The semantic map of nouns was constructed by first defining each noun as a set of its modifiers (adjectives). From Table 1, for example, we can define  X  kimochi (feeling) X  (proud), X  and  X  kanashii (sad), X ...}. Suppose there is a set of nouns, w i ( i = 1, ... ,  X  ), is the number of adjectives of w i . One method of encoding nouns so that they can be treated with an SOM is to use random coding, which is an ordinary method used to construct SOMs (see [9] for details). Through several preliminary computer experiments, however, we found that this method is not suitable for our task. Because in random coding each co-occurring word is represented by a random vector with fixed number of dimension (e.g., 100 dimensions), and each noun is represented by sum of vectors of all co-occurring words, when the number of co-occurring words with nouns become large (e.g., 10,525 adjectives co-occurred with nouns in our experiment), it is very difficult to encode nouns properly. Therefore, we used a new method as described below. Suppose we have a correlation matrix (Table 2) where d i j ( w and  X   X  n  X  is an n -dimensional space.). T is the number of learning steps. 
Note that the individual d i j of vector V ( w i ) only reflects the relationships between a pair of words when they are considered independently. relationship between words. This similarity measure was developed to enable recognition of degraded machine-printed text [5]. Yamamoto and Umemura [18] used the CSM to calculate a hypernym-hyponym relationship between a word pair. a indicates the number of times the two labels appear together; b indicates the number occurs but  X  X abel 1 X  does not; and d is the number of times neither label occurs. In our adjectives co-occurring with both abstract nouns, b and c indicate the number of adjectives co-occurring with either abstract noun ( X  X abel 1 X  and  X  X abel 2 X , respectively), and d indicates the number of adjectives co-occurring with neither abstract noun. Depending on the direction of the calculation, in other words, between substituted into b and c is reversed. That is, CSM calculates a similarity between words asymmetrically. 
For a comparison with the CSM, we used another directional similarity measure, the overlap co-efficient (Ovlp). According to [11],  X  X he overlap coefficient has the flavor of a measure of inclusion. It has a value of 1.0 if every dimension with a nonzero value for the first vector is also nonzero for the second vector or vice versa. X  abstract nouns w i and w j in our data. 
The learning step of an SOM consists of an ordering phase and a final phase. We used 30,000 learning steps in the ordering phase and 100,000 in the final phase. The map shown in Fig. 1 was the SOM of a 45 x 45 array where a neighborhood with a hexagonal topology was used. The initial radius of the neighborhood was set at 45 in the ordering phase and the last radius was set at 7 in the final phase. 3.2 Construct i on of H i erarchy Composed from Concepts When Us i ng CSM Using the CSM, we constructed a hierarchy composed of concepts of adjectives through the following process [19]. The hierarchical construction of concepts is a procedure that is independent from the process of distributing concepts on the two-dimensional map made by the SOM. After we obtained hierarchies, we plotted them on the map made by the SOM. Values of CSM were normalized, and we made a list of CSM values that we obtained. Examples of calculation results obtained using CSM are as follows. For example, in Table 3,  X  Inshou (impression) X  and  X  Kanji (feeling) X  is a hypernym-hyponym relationship. 1) For a hypernym A and a hyponym B, the initial hierarchy is A-B. 2) First, detect hyponyms deeper than B; i.e., hyponyms in the initial hierarchy A-B. 3) Second, find superordinate nouns shallower than A, a hypernym in the initial In steps 2) and 3), the superordinate/subordinate relationship between two nouns must be retained. If the relationship is brok en, we cannot connect the two nouns. 4.1 Compar i son of CSM w i th Other Methods : Ovlp, and CSM Us i ng Frequency In this section, we compare hierarchies constructed automatically using several methods. We also determine which hierarchie s provide a suitable point of comparison with the EDR concept hierarchy. The methods we compared were as follows. 1) CSM without frequency information (CSM) 2) Overlap coefficient without frequency information (Ovlp) 3) CSM with frequency information (Freq) We set the normalized threshold value at 0.3 and 0.2 for CSM and Ovlp (CSM0.3, CSM0.2, Ovlp0.3, Ovlp0.2), and at 0.2 and 0.1 for Freq (Freq0.2, Freq0.1). Higher threshold values create hierarchies with a smaller number of nouns and lower threshold values lead to overly large and incoherent hierarchies. The thresholds that we set were found to enable the construction of hierarchies with a number of nouns that seemed suitable for our purposes. Among the hierarchies we created were some in which the same adjective was held by all concepts in a path from the bottom node to the top node as an instance of the concept. This type of hierarchy is considered  X  X  hierarchy of an adjective X  in this paper (see Section 4.2., Rules). 
First, we counted the number of hierarchies of adjectives among the automatically extracted hierarchies to find the most plausible hierarchies. Then, we calculated the number of target adjectives covered by the automatically constructed hierarchies and calculated what percentage of the 365 abstract nouns appeared as elements in the hierarchies. Features of the hierarchies created using the different methods are shown in Table 4; that is, the percentage of hierarchies of adjectives among all automatically extracted hierarchies, the number of target adjectives covered by the automatically created hierarchies, and the percentage of the 365 abstract nouns that appeared in the hierarchies as elements. The top three scores for each category are circled in Table 4 (a bold circle indicates an es pecially good score). Poor scores are marked with an  X *. X  We can see that CSM0.2 and Ovlp0.3 were suitable threshold values in terms of the three features we considered. CSM0.3 created the highest number of hierarchies, but these contained the lowest number of abstract nouns. In other words, CSM0.3 exhibited high precision but low recall. Ovlp0.2 created fewer hierarchies, but the low but recall was high). This means that Ovlp0.2 created overly large hierarchies. Freq0.1 exhibited a similar tendency, creating fewer hierarchies of adjectives that covered many abstract nouns. Freq0.2 created fewer hierarchies of adjectives, and the hierarchies typically covered a small number of abstract nouns. CSM0.3 exhibited a similar tendency. The above results indicate that CSM0.2 and Ovlp0.3 seem to create the most appropriate hierarchies, so these were the methods and threshold values we used in our evaluation. From the methods using frequency information, we used Freq0.2 in our evaluation because it created a greater number of hierarchies of adjectives than that created by Freq0.1. 4.2 Compar i son of Created H i erarch i es w i th Ex i st i ng Handcrafted Thesaurus The EDR lexicon is a large Japanese lexicon for computers that was constructed by a great number people under a number of supervisors. We performed an experiment to compare automatically generated hierarchies to EDR hierarchies. We used a method of psychological scaling, Scheffe X  X  method of paired comparison [15], for this experiment. Exper i mental Data Based on the results in Section 4.1, we used the following as experimental data. For 30 adjectives, we obtained 30 hierarchies generated by CSM0.2, Ovlp0.3, and Freq0.2. We refer to these identical hierarchies generated by all three methods as  X  X OMMON X  hierarchies. We also obtained corresponding hierarchies from the EDR lexicon. For each adjective, we compared a pair of hierarchies that contained those adjectives consisting of a generated hierarchy and an EDR hierarchy (COMMON-EDR). We had participants in the experiment judge the plausibility of the paired hierarchies for the adjectives shown in Table 5. Part i c i pants The 20 participants consisted of linguists, lexicographers, and people familiar with natural language processing (NLP). Exper i mental Procedure We showed the participants a pair of hierarchies for a certain adjective (a set consisting of an automatically generated hierarchy and an EDR hierarchy) and had them judge how valid each hierarchy was. They scored each hierarchy on a five point scale (-2, -1, 0, 1, 2) as illustrated below. The test was conducted as follows, where a letter indicates the name of a concept. For example, A-B-C means that concepts A, B, and C are connected in a hierarchical relationship. In this case, the top concept is A and the bottom concept is C. 
A score of  X 0 (equally valid) X  was given when both hierarchies appeared either valid or invalid. We provided two rules as criteria for judging the validity of a hierarchy. Rule 1) If concepts are connected in a hierarchical relationship, an instance will 
According to Lexicology [4], a system of hyponymic relationships constitutes a taxonomy or, in other words, a branching lexical hierarchy based on inclusiveness. For example,  X  X reature X  includes  X  X ird X  and  X  X orse X ,  X  X ird X  includes  X  X uthatch X  and  X  X obin X , and  X  X uthatch X  includes  X  X hite-breasted nuthatch X  and  X  X ed-breasted nuthatch X  (p. 472). Conversely, an instance in a hyponymic concept is also an instance in a superordinate concept. For example,  X  X reasted nuthatch X  is an instance in  X  X uthatch, X   X  X ird, X  and  X  X reature. X  In our data,  X  X n instance X  corresponds to  X  X djective X X  for which a hierarchy was created automatically. Rule 2) If concepts are connected in a hierarchical relationship, a hyponymic concept 
According to Cruse [3], X will be said to be a hyponym of Y (and, by the same token, Y is a superordinate of X), if A is f(X) entails, but is not entailed by, A is f(Y). ) For example,  X  X his is a DOG X  unilaterally entails  X  X his is an ANIMAL X  (pp. 88-89). From our comparison of COMMON and EDR paired hierarchies (COMMON-EDR), we calculated a test statistic (T-value): where N is the number of samples, 1 x and 2 x indicate values averaged over the 20 participants for methods 1 and 2, respectively, and 1 2 s and 2 2 s indicate the unbiased variance over the participants for methods 1 and 2, respectively. Here, method 1 is COMMON and method 2 is EDR. 
We investigated the difference between the performance of automatic generation and that of EDR at significance levels of 1%, 5%, and 25%. A significant difference means that an assessment of relative merit between two methods differs significantly. Conversely, no significant difference means that an assessment of relative merit between two methods does not differ significantly. At significance levels of 1% and 5%, we investigated whether a significant difference exists between automatic generation and EDR, and at significance levels of 5% and 25%, we investigated whether a significant difference exists between automatic generation and EDR. The T-value indicates the extent to which either the automatic generation or EDR was significantly better. A positive T-value indicates that automatic generation was superior to the EDR lexicon while a negative value indicated the opposite. Exper i mental Results The T-values from our comparison of the COMMON and EDR hierarchy pairs are shown in Fig. 2. These T-values were obtained through Scheffe X  X  paired comparison method. (The numbers in the figure are the adjective ID numbers from Table 5 for each pair of hierarchies used in the experiment.) A) Result 1 significance level: 5%, significant difference ( T &gt; 1.68 ) At the 5% significance level, we found a significant difference for the following adjectives (T-values in parentheses). (1) Adjective IDs for positive T-values: Total adjective IDs: 2/30 (2) Adjective IDs for negative T-values: Total adjective IDs: 17/30 B) Result 2 significance level: 1%, significant difference: T &gt; 2.4 At the 1% level, the result was the same as result 1. C) Result 3 significance level: 5%, no significant difference: T &lt; 1.68 At the 5% significance level, there was no significant difference for the following adjectives. (1) Adjective IDs for positive T-values: Total adjective IDs: 5/30 (2) Adjective IDs for negative T-values: Total adjective IDs: 6/30 D) Result 4 significance level: 25%, no significant difference: T &lt; 0.68 At the 25% significance level, there was no significant difference for the following adjectives. (1) Adjective IDs for positive T-values: Total adjective IDs: 2/30 (2) Adjective IDs for negative T-values: Total adjective IDs: 3/30 D i scuss i on Comparing COMMON with EDR, the assessed validity of the EDR hierarchies was better for 17 pairs, whereas that of the COMMON hierarchies was better for 2 pairs (a COMMON. An example is shown below. Even in this raw comparison, our method generated results as good as those obtained from the EDR for 43% {(11+2)/30} of the hierarchies. As in other experiments, though we compared hierarchies generated only by CSM0.2 or Ovlp0.3 with those in EDR, EDR obviously differs more significantly than CSM0.2 and Ovlp0.3. Automat i cally generated h i erarchy EDR 
We asked examinees to write comments on the comparison between the EDR dictionary and our hierarchy. Many of their reasons for judging the EDR hierarchies as better were related to feeling a sense of incongruity because several nodes were missing from the automatically extracted hierarchies. When a lower concept in our hierarchy was directly linked to an upper concept, the examinees tended to consider the EDR better. Due to the sparseness of data in corpora, some necessary abstract nouns may not have been extracted. This can be overcome by using bigger corpora. Judging from these situations and the results of our raw comparison with EDR hierarchies, we believe our method is applicable to the automatic extraction of a thesaurus-like structure from huge corpora. We have described the construction and evaluation of concept hierarchies of adjectives from corpora. First, using topological features of constructed hierarchies, we determined suitable methods and threshold values. We then conducted an evaluation to identify whether EDR hierarchies or our automatically generated hierarchies were objectively judged to be the best formed or most valid and to compare them from the viewpoint of content. From the topological features, we found that CSM0.2, Ovlp0.3, and Freq0.2 created better hierarchies, in that order. The evaluation results indicated that our automatic extraction was applicable to 43% {(11+2)/30} of the hierarchies. We will continue to develop and improve our methods of extracting the information needed to construct better hierarchies of adjectives. The study of speech recognition using hidden Markov models (HMMs) [1] has been well understood for several decades. General speech recognition systems have been used to find the matched words of a speaker's utterance using the acostic score of the speech signal and the language model score of the word sequence. In the case of learning a foreign language and native language, for a student and a person with impaired hearing, respectively, it is important to find not only how well the meaning Therefore, it is not easy to estimate the pronunciation distance between two utterances and to determine the similarity using automatic speech recognition systems. In this paper, we propose to use a distance measure of the acoustic space and speaker space to automatically compute the pronunciation similarity. 
The pronunciation of a speech segment can be characterized by acoustic features used in the study of speech synthesis [4]. However, extracting those information from speech signals automatically is another difficult task. A rather simple approach using the distance between cepstral vector sequences is known to represent the characteristics of speech signals. This approach is motivated by utterance verification [5][6] and speaker verification [7], which deal with selecting the correct utterance and speaker, respectively, using confidence measures [8]. The likelihood ratio between the probabilistic distributions using correctly hypothesized model parameters, and incorrectly hypothesized model parameters, determines whether to accept or reject the utterances using a critical decision threshold. However, these methods are different from the proposed method in two aspects. First, the issue of this paper is to deal with of the utterance model. Second, it analyzes the utterances not only in view of word matching, but also personal pronunciation characteristics. Computer-assisted language learning (CALL) has been studied in the research area of foreign language and pronunciation learning [9]. However, the main interest of those areas is learning the vocabulary or grammar. 
This paper proposes a new approach to estimate the distance of two speaker X  X  pronunciations and calculate a probabilistic score of this distance which can be considered as a confidence measure. To obtain the optimal confidence measure threshold, a support vector machine (SVM) is used. For the acoustic feature vector space, mel-frequency cepstral coefficients (MFCC) are used as feature vectors. In estimating the distance of two utterances, the dynamic time warping (DTW) method [10] and the distance between speakers using the codebook is used. The performance of the proposed method is evaluated using the seven fundamental Korean vowels from six speakers. 
In section 2, the theory of the pronunciation similarity, the decision threshold finding procedure for a specific phone set, and a classification method are explained. Section 3 presents the experimental results. Some conclusions are discussed in section 4. different, even though the same word is spoken. In addition, as one speaker repeatedly utters the same word, speech is not always the same because the acoustic signal is generated through a very dynamic process inside the human body. Even if speech is different for each person, multiple utterances of the same word have very similar patterns which are typically analyzed with statistical methods such as HMMs. Therefore, the majority of automatic speech recognition systems have statistical patterns of speech in the form of parameters in the HMMs. 2.1 Est i mate the D i stance of Pronunc i at i on S i m i lar i ty Inspired by utterance and speaker verification, the distance, D , of pronunciation dissimilarity can be defined as follows; the speech distance is subtracted by speaker distance since even though the speech distance for the same word is correctly estimated, some speakers are very different to each other by the underline characteristics of the personal vocal tract. If the reference speech is not sufficient to have enough statistics for HMMs, another method such as DTW may be needed to directly estimate the distance of acoustic feature vectors. Therefore, equation (1) can be changed to the following; 
DTW is the utterance distance calculated using the DTW method and CB is the speaker distance estimated using codebooks. This paper uses two kinds of distance measure; the first is the utterance distan ce using a DTW method, the second is the codebook distance. If a small number of codewords are kept, the codebook can be trained using a k -means algorithm. To compensate for the unit difference, the above equation can be modified as; where 1 w and 2 w are the compensating parameters or weights and  X  is a decision threshold. This threshold determines the correctness of the pronunciation. In the next sector, we explain how to find these parameters. 2.2 Est i mat i ng Threshold Us i ng a Support Vector Mach i ne The baseline of the pronunciation similarity is started with just a single phone pronunciation. To estimate the decision threshold of each phone for a reference speaker, the codebook statistic is first trained from the reference speaker X  X  data. Then, for each speaker, the utterance distances and the codebook distance are computed using all training utterances. The codebook distance is calculated by averaging all codeword distances from the input data. The SVM [11][12] is used to find the optimal binary decision threshold. The SVM uses a linear kernel for the separating hyperplane. To describe this hyperplane, generally, the following form is used; where x is the input vector (i.e., DTW x = equation (4) is rewritten as the following equation; where  X  and  X  , which come from w and b (i.e., 1 2 w w  X  =  X  and This equation can be used to easily show two dimensional space graphs. Fig. 1 shows the separating hyperplane for the distance between one vowel,  X  X a X , of the reference speaker and seven vowels from other training speakers. In this figure, the utterance and speaker distances between the same phonemes (i.e.,  X  X a X  vs.  X  X a X ) are closer than estimated values of the parameters are  X  =  X  0.87 and =  X  26.91. This hyperplane maximally separates the same vowels and the different vowels. The confidence measure is estimated using the distance from the hyperplane. 2.3 Conf i dence Measure When a score of the pronunciation similarity between two utterances is requested, a probabilistic score can be provided using the confidence measure computed as in the previous section. Equation (5) which may be similar to the likelihood ratio test of utterance verification [6] provides a theoretical formulation to address the confidence measure problem. After the parameters are determined using the training procedure, the distance can be transformed to a confidence measure, y , using a sigmoid function; between utterances i and j . To evaluate pronunciation similarity, automatic speech and speaker distance estimation and SVM classification for the decision threshold were implemented. The experiments were evaluated with seven fundamental Korean vowels from six male speakers. The utterances were recorded in 16 KHz sampling rate with a condenser microphone. Each vowel was pronounced ten times and data was extracted using speech detection algorithms. Data was then converted to a vector sequence of 13 dimensional MFCC features, which were processed using energy normalization and cepstral mean normalization algorithms. The utterance and codebook distance were calculated with other speakers for cross validation. To estimate the decision parameters, three speakers were trained. Namely, if one speaker was selected as a reference, two other speakers were used to generate the decision parameters. The decision threshold parameters were different in each speaker and in each of the seven vowels. After the training process, test evaluation with the rest speakers was performed. 3.1 Utterance and Codebook D i stance Table 1 shows the utterance distance and speaker distance between the reference speaker X  X  phone  X  X a X  and training speaker X  X  phones, which include  X  X a X ,  X  X h X , and the rest six vowels. After calculating the distances between phone pairs, equation (3) uses the average utterance and speaker distance, to show the compensated distance. The average is calculated from the same labeled vowel pairs. From the experimental The weight, 2 w , like 1/4 is applied to show the compensated distance and estimate the decision threshold. Through the training process, two distances are obtained as input and the threshold is estimated. each vowel to estimate the conpensated distance from  X  X a X  with 1/4 weight of . CB 3.2 Seven Vowels Test To test the correct acceptance between phonemes, the number of vowel pairs is counted whenever both vowels are classed as matching vowels. To test correct rejection, it is also counted when both vowels are different. Each vowel of the one hundred pairs were compared. The following experiments were evaluated with three reference and three testing speakers, and one vowel of the three reference speakers had a total of nine hundred distance pairs. paired to six vowels of the testing speaker. The number of mismatched vowel pairs totaled five thousand four hundred. Table 2 shows the correct acceptance and correct rejection for pronunciation of seven Korean vowels. Using automatically selected threshold parameters, each pair was estimated and averaged. Since codebook size affects codebook distance, tables 3 and 4 show the results when using different codebook sizes. 3.3 Pronunc i at i on S i m i lar i ty Score The confidence measure driven from equation (6) is used as pronunciation similarity score for comparing a reference speaker to the testing speaker. Fig. 2 shows twenty pairs of the accumulated count probability graph where the compensated distance driven from equation (5) was converted as the similarity score, ) , ( j i y . The count of the matched vowel (i.e.,  X  X w X  vs.  X  X w X ) and mismatched vowel (i.e.,  X  X w X  vs.  X  X w X ) the scores of 0.5 or more show pronunciation similarity. This paper proposed a new approach to estimate pronunciation similarity using the distance between vector sequences of cepstral features for each speaker. This approach not only deals with speech word matching, but also characters of speaker pronunciation. In the results of the pronunciation similarity experiments on the seven fundamental Korean vowel data, it has been observed that the total average of the correctness showed the accuracy of 83%. Future work will consider speech features such as pitch, formant, duration, and accent using an automatic detection algorithm. In addition, speaker normalization methods such as vocal track length normalization may be incorporated since these methods can reduce inter speaker variation. This work was supported by a grant (no. R01-2006-000-11162-0) from the Basic Research Program of the Korea Science &amp; Engineering Foundation. Typical speaker adaptation method includes the MAP family [1] and the linear trans-formation family (e.g., MLLR) [2]. These two families require significant amounts of adaptation date from the new speaker in order to perform better than a speaker inde-pendent (SI) system. Recently, a family of clustering and selection based speaker adaptation schemes (e.g., CAT, SST) has received much attention [3, 4]. This ap-proach utilizes the correlations among different reference speakers and performs ef-fectively in rapid speaker adaptation even if only one adaptation sentence has been used. Speaker selection training (SST) is a typical example of selection based speaker adaptation [3]. It trains a speaker dependent (SD)model for each of reference speakers and assumes that the adapted model for the test speaker must be a linear combination of the selected reference models. How to make a trade off between good coverage and small variance among the cohorts selected is still a very trick problems relied on the experiments. Dynamic instead of fixed number of close speaker selection seems to be a good alternative. Take support vector machine (SVM) as solution, we can choose an optimal set of reference models with a very limited amount of adaptation data. How-ever, such selection based SVM is influenced by some critical factors, such as the kernel and its parameters, the complexity of the question and the noise data near the optimal hyperplane, etc. 
We can see clearly that the reference speakers are selected according to the support vectors (SV). In particular, it was observed that SVM solely trained on the SV set extracted by another machines with a test performance not worse than after training full dataset [5]. These support vectors lie cl ose to the decision boundary between the two classes and carry all relevant information about the classification problem. This led to assume that it might be feasible to generate SV only without training SVM. 
In this paper, agreeing with the assumption above, we proposed a heuristic method the speaker X  X  model. In the next section, we discuss the basic idea of SVM based speaker selection. Our proposed method, reference support speaker selection (RSSS) is explained in section 3. Experimental results using our algorithm are shown in Section 4. Section 5 summarizes the paper and gives a conclusion. SVM is a promising machine learning technique developed from the theory of Struc-Fig.1 shows a typical two-class problem in which the examples are perfectly separa-ble using a linear decision region. H1 and H2 define two hyperplanes. The closest in-class and out-of-class examples lying on these two hyperplanes are called the support vectors. For a separable data set, the system places a hyperplane in a high dimensional space so that the hyperplane has maximum margin. SVM used basically for binary (positive and negative) classification. nonlinear map which transforms x from the input space into the feature space, a kernel-based transformation: where,  X  denotes inner product. Different kernel functions form different SVM algo-rithms, here are three kernel functions are often used [8]. 
A kernel-based decision function allows dot product to be computed in a higher dimensional space without explicitly mapping the data into these spaces. It has the form: 
The training set instances that lie closest to the hyper-plane are selected as the sup-port vectors that always have nonzero i  X  coefficients. Therefore, the SV set can fully describe the classification characteristics of the entire training set. 
Regarding the reference speakers and the test speaker as two classes, we can reference speakers distribute around the test equality. Then the reference speakers corresponding to these support vectors can be selected as a subset, called speakers support vector. Figure.2 illustrates the principle of this method. 
From above we can conclude the support vectors, which chosen as the representa-tive points of the two classes and  X  X upport X  the optimal hyperplane, are determined through maximizing the margin of the two classes. Hence, if we can find SV set di-rectly without using SVM, the training time can be reduced greatly without much loss of classification precision. 3.1 Ma i n Procedure of Proposed Plan support vectors (called reference speaker su pport vectors). the two data-points of opposite class that are in distance closest to each other are the most probable support given one point in positive examples, the point, which is closed in distance in the negative examples, is the most probable su pport vectors and hence select them as candidate speaker support vectors. 
In the feature space, the distance between two points F z z where ) , ( j i ij x x K k = is kernel function. 
For implementing reason, we extract the mean vectors of the GMM of M reference speaker to form supervectors ) , 2 , 1 ( M m S the test utterance and extract the mean vectors of the Gaussians to form the supervec-tor t S  X  of test speaker, every dimension of t S  X  has the order are the same as
Considering t S  X  as positive example and ) , 2 , 1 ( M m S the distance between them can be calculated using equation 3. For verifying the speaker selection, a confidence measure of each speaker model can then be derived. The algorithm is described as follows. The algorithm is shown in Figure.3. 
Input: given training set: Output: the selected candidate support vector. 
Procedure : for (m=1; M; m++) calculate tm d (using formula 3); find out the N( N can selected from 3 to 5 )lest distances as measurement, then for every ) , 2 , 1 ( M m S then we can selected the candidates SV through experiment threshold T  X  . 3.2 HMM Model Est i mat i on In this part, a single-pass re-estimation procedure, conditioned on the speaker-independent model, is adopted. In the adaptation procedure, there has no inherent structure X  X  limitation of transformation-based adaptation schemes such as MLLR .A speaker adapted acoustic model is calculated from the HMM statistics of the selected speakers using a statistical calculation method. 
The process of re-estimation would update the value of each parameter. The poste-advance . 
The one-pass re-estimation formula may be expressed follows: where speaker at time t , m  X  ~ is the estimated mean vector of the m  X  X h mixture component of the target speaker. The variance matrix and the mixture weight of the m  X  X h mixture component can also be estimated in a similar way. 4.1 Exper i ment Setup The database we used in these experiments is based on mandarin Chinese corpus pro-vided by the 863 plan (China High-Tech Development Plan). About 60 hours of speech data from Chinese male speakers are used to train a gender-dependent SI model. We use 39 dimensional features consisting of 12 cepstral coefficients and log energy feature left-to-right HMM model with eight continuous density mixtures is trained. Then triphone-based HMM models are used in this continuous speech recognition. 
The speakers ready for selection consist of 150 male speakers, with 250 utterances each. Typically one utterance, both in training and test set, lasts 3~5 seconds. Test set consists of 20 male speakers from the same accent with training set, 20 utterances each. 10 of them are used for selecting and adaptation. The other 10 are used for test-ing. It should be noted that we focus on very rapid adaptation of large-vocabulary system in this paper. All the adaptation methods in experiments are performed with only one adaptation sentence. 
In order to evaluate the performance of the reference support vector, we perform different kernel methods and compare the results. 4.2 Exper i mental Results Table 1 shows average recognition rates of RSSS (with 100 reference speakers) with different kernels. CSV represents the number of selected speakers. We take the SVM based speaker selection(SSVS) as comparison. 
From Table 1 we can see that linear kernel based selection obtains the best recogni-tion accuracy. As we known, although the polynomial kernel and RBF kernel can handle the case when the relation between class labels and attributes is nonlinear, but they use more parameters during training which influences the complexity of model selection. Then the support vectors obtained by linear kernel may bring a better per-formance in speaker selection. 
From Figure.4, we observed an interesting result that heuristic procedure reduced the number of support vector, but the performance dose not decrease, we can also conclude from Figure.4 that as the number of reference speakers grows, the proposed method can select more accurate support vectors which are acoustically close to the test. So the performance will be improved. It is make out dynamically choosing the optimal cohort speakers for each target speaker is one of the key concerns in order to keep the balance between good coverage of phone context and acoustic similarity to the target speaker. Generally, performance of selection based speaker adaptation is very sensitive to the choice of initial models, the number of selected speakers is always fixed. A novel selection method based on SVM is proposed in this paper. It realizes dynamic speaker selection by finding the support vector and its corresponding speaker in the subset of reference speakers. Our experiments have shown the proposed scheme can improve the performance and robustness of adaptation, even few adaptation sentences is avail-able. Further work will be focus on how to measure the relative contribution of each phone model of each specific speaker selected. This research is partially supported by NSFC (National Natural Science Foundation of China) under Grant No.60475007, Key Project of Chinese Ministry of Education under Grant No.02029, and the Foundation of Chinese Ministry of Education for Century Spanning Talent. The problem of translating named enti ties (NE) across languages occupies an important place in machine translation. Such entities are in principle unlimited, making the exhaustive construction of translation dictionaries tedious if not impossible, and as such require special treatment and algorithms. The problem is compounded when we consider long names that span multiple words: some parts of which may be directly transliterated with only phonetic and orthographic knowledge, whilst others require some semantics.

In this paper we investigate an algorithm that uses contextual-semantic in-formation to assist in the discovery of pairs of NE translations in two languages. The approach is generic enough to work for any sort of phrase translation, we will concentrate on features particularly salient to Named Entities, with partic-ular attention to those found in abundant web news corpora. Some quantitative results are presented to illustrate the interaction of various factors such as cor-pus effects, feature sets and extraction, and tunable thresholds, and what impact these have on the performance of our algorithm. There is a large body of prior work that deals with the transliteration of Named Entities, in which phonemic and orthographic knowledge is chiefly used. Recent works such as [1], [2] and [3] tend to focus primarily on the use of such knowledge; less work has been done on exploiting semantic knowledge for transliteration.
Notable exceptions to this include [4], [5] and [6]. In [4], a simple context vector model was used in conjunction with a phonetic transliteration model, but while sophisticated statisical modeling for vector elements was applied, the method is still similar to a standard vector approach that employs a cosine-similarity mea-sure. In [5], web news corpora was used to discover word translations (including NEs), by utilizing patterns in their temporal distribution across relevant news sources. The Canonical Correlation Analysis (CCA) technique studied in [6] is similar to LSA in that it extracts factors salient to translation.

Latent semantic analysis (LSA) is a purely statistical technique first pioneered by researchers in IR. It has been found to be robust in many different tasks and settings, and some recent work suggest that it may have some capacity to emulate human-like abilities at finding word associations. What we are proposing is a particular application of LSA to NE translation. It is almost identical to the cross-lingual latent semantic indexing (C L-LSI) algorithm first proposed in [7] and later further refined and studied in [8], [9] and other researchers in the field of Cross-Lingual Information Retrieval (CLIR). The basic idea of CS-LSI is to project feature vectors from either language into a joint space: this is accomplished by simply augmenting two document-term matrices together and performing a single SVD, side-stepping the need for a translation lexicon. Our algorithm improves on this by introducing an explicit step to map  X  X rue X  pairs directly from one language X  X  feature space to the other. This is possible since NEs tend to exhibit far less polysemy and have fewer paraphrases compared to common phrases, so that a nearly one-to-one correspondence exists across languages. Experiments in [8] demonstrate that CL-LSI works exceedingly well, with highly competitive performance against CLIR systems that use explicit translation, predicating some sort of implicit translation being performed.
These works collectively suggest that not only is the semantic knowledge that is embedded within contextual co-occurrence patterns relevant for NE transla-tion, but an LSA approach can potentially work well. To our knowledge, our proposed method is one of the first to refine the use of LSA for NE translation, and is particularly powerful because it can potentially scale to diverse feature sets numbering in the hundreds of thousands on modest computing platforms. Our algorithm is designed to function in the context of a larger bootstrapping framework that fuses information from multiple knowledge sources. This frame-work, illustrated in Figure 1, first uses some rudimentary method for Named Entity Recognition (NER) to produce a list of candidate NEs in each language. Modules which each score pairs of candidates according to some criterion crite-ria (be it phonetic, orthographic or contextual similarity), can be added to the framework at will. These scores then combined; top ranking pairs are added to a  X  X orrect X  list that is used to further train and improve each module.
 Our proposed algorithm has a two stage-training process as illustrated in Figure 2. First, we construct the semantic spaces for each language based on the list of  X  X rue X  candidates. An explicit mapping function between either space is found. The next step is to produce a joint semantic space, and from this we can generate two linear projectors that will project context vectors from the feature space of either language onto the joint semantic space.

Let us denote the set W = { w 1 ,w 2 , ...w l } containing l  X  X rue X  pairs, where each pair w k  X  W refers to a particular named entity. We have two sets of features F = { f 1 ,f 2 , ...f n } and G = { g 1 ,g 2 , ...g m } ,where F contains n features from the first language, and similarly G for the second language. The pair-feature matrices T 1 and T 2 , corresponding to feature sets F and G respectively, are constructed by counting the number of collocations between NE w i and feature f j (or g j ). The contribution of each feature is weighted using a TF-IDF score, so that each element in T 1 and T 2 is where N is the total number of features generated, n ( f k )isthenumberoftimes feature f k was generated over the entire corpus. The log 2 N n ( f g ) term is analogous in function to the inverse document frequency (IDF) commonly used in infor-mation retrieval; however we must note that in our formulation, we are treating the contexts of transliteration pairs as documents and features as terms. Each NE-feature matrix is decomposed using Singular Value Decomposition (SVD), such that U sand V s are orthonormal matrices. Each row vector in T 1 corre-sponds to the feature vector in the language; each column vector corresponds to the context vector for a particular word. Since U 1 is orthonormal, defining a linear operator P 1 ( x )= U  X  1 1 S  X  1 1 x projects the feature vector into a new space in which each the projected vectors are now spaced apart. Vector components that correspond to smaller singular values may be dropped altogether from the computation; this has been shown to empirically improve results on indexing tasks, by smoothing out noise in the data and ameliorating  X  X verfitting X .
Given the vectors x k and y k that correspond to the kth translation pair, we would like to find some invertible mapping P : x k  X  y k such that the Euclidean distance between y k and y k is minimized. This mapping associates the vectors within the space induced by the features of one language, to the space induced by the other language, and it is  X  X ptimal in some sense X  with respect to correct translation pairs. If we take certain assumptions about the underlying probability distribution for components of context vectors (as in [10]), then the best projector is achieved by the linear operator where U  X  X  V  X  X  and S  X  X  are the results from the earlier singular vector decompo-sition, and T  X  1 is the matrix pseudo inverse of T 1 .

Let us denote T J =[ T 1 | T 2 ], applying the SVD gives T J = U J S J V T J .The linear operator defined by P 1 , 2  X  J ( x )= S  X  1 J U T J x projects every context vector from T J to an orthonormal set, i.e. P 1 , 2  X  J ( T )= V J . Two linear projectors can be constructed that will simultaneously ma p a context vector from either language into the joint feature space and associat e the cross-lingual LSA vectors for every seed pair. The projector from the first language to the joint space is given by
This computation can be optimized by carefully ordering matrix multiplica-tions; matrices U  X  1 J and U 2 should be done first. The cosine similarity score between the joint space vectors is then computed and used as the module score. Our choice of terminology (NE-features vs document-terms) reflects an under-standing that the LSA approach can be extended beyond just simple word-occurrences and indeed to other levels of linguistic knowledge beyond semantics; features can be designed to capture syntactical, phonological and even morpho-logical levels of knowledge, so long as they and the NEs exhibit some manifes-tation of the  X  X istributional hypothesis X .

AlistofsomefeaturesusedinourexperimentsisshowninTable1.Note that character unigrams, bigrams and trigrams are labelled as C1, C2 and C3 respectively. Complex feature spaces can be created by merging any combination of feature classes. The number of features in from each class can be limited by imposing threshold on feature X  X  count IDF value.

The features listed here generally span higher levels of linguistic knowledge, notably semantics, topic of discourse and common usage (pragmatics). The ob-vious features based on word or n-gram character co-occurrences have common applications in topic-detection (TDT) tasks. The more unconventional features such as position within document, or temporal occurrence have the potential to capture some artifacts of writing style and the  X  X erd-reporting X  behaviour seen with news-worthy events. These may be particularly salient to NEs in the way they distribute in news corpora. The same can be said for section-headings or article level co-occurrence with other NEs as well. Effect of Explicit Association. The HKNews parallel corpus contains 44621 pairs of written Chinese and English news articles aligned at the sentence level. A list of 283 transliteration pairs were manually created from the corpus by exam-ining the most frequently occurring bigram words and characters. Topical bias within the corpus resulted in several prominent semantic groups, such as place or street names (e.g. Tsim Sha Tsui), country names (e.g. US), organizations (e.g. United Nations), and named persons (e.g. Jiang Zemin).

This corpus was divided into three subsets: small, medium and large. These subsets are respectively one thirds, two-thirds and all of the corpus. Each sentence-level (or document-level) alignment is given a unique tag; these tags are used in-lieu of features. The projector from each language to the joint space is com-puted in two different ways: with and without explicit association of the seed pairs. Leaving out the association step is mathematically equivalent to replacing the long sequence of multiplications in Eq. 4 with the identity matrix.
A ten-fold cross validation was used. In each trial, the transliteration pairs were randomly divided into a seed/training and held-out/testing parts in varying proportions (100, 150, 200 seeds). The number of latent factors to retain was chosen per trial to maximize MRR. Averaged results are shown in Table 2.
As expected, we observe that either having a larger corpus or more seeds improves performance on the held-out test set. Furthermore, performing an ex-plicit association consistently improves the performance over standard CL-LSI. Strangely, better results are not obtained sentence ids versus document ids. This could be explained by the fact that the algorithm effectively ignores the fact that there the ids themselves indicate alignment, in addition to where the boundaries of salient documents or sentences lie. H ence our algorithm has to do some extra work in automatically figuring out the alignment between documents (or sen-tences). Furthermore, the feature space of sentence ids could either be highly complex, so that our assumptions about the underlying joint distribution be-tween features and NEs (Gaussian-ness) is no longer valid. This, in conjunction with having many numerous low-count features, gives us very poor feature space for LSA-based training methods. Another interesting anomaly observed here is that the results for medium sized corpora seem to perform a little poorer com-pared to the large and small sized corpora. More investigation is needed to look into this issue.
 Number of Latent Factors to Retain. Empirical results have shown that dropping the latent factors with small eigenvalues can provide some sort of  X  X moothing effect X  that reduces noise in the context vectors. The distribution of the  X  X est X  number of retained factors from our first experiment was analyzed to determine its dependence on the number of initial pairs and corpus size. Results are shown in Table 3 for the  X  X est X  number of retained factors averaged over 15 randomized seed-lists. It appears that as rule of thumb, discarding roughly a fifth of the latent factors should give reasonable results, but more empirical study is needed in order to figure out how many latent factors need to be discarded. Combinations of Feature Spaces. The second experiment investigates the impact of using different features. The entire corpus was used in a ten-fold eval-uation. In each trial a random set of 150 seeds were selected for training, and the remainder were held out for testing. The average MRR for selected pairs of features is shown in Table 4.

The results indicate that the LSA approach to feature combination is sub-optimal. As yet, it is not possible to simply glob large feature spaces together and expect the algorithm to automatically sift out the best subset. This is evident when we observe that the document level features and temporal features perform best on their own, and that the overall result is worse when they are combined.
The experiments with the temporal features show that the best feature com-bination occurs when we use same sized time windows for both language corpus. This issue of scale is not consistent for the sentence and document level features. For instance the written Chinese sC1 works best with English sW+dW , but written Chinese dC2 and its combinations work better with English dW . This suggests that perhaps more study must be done on feature combination in order for it to be effective.
 Bootstrapping. In the final experiment we demonstrate that our algorithm is viable in the bootstrapping setting. For this demonstration our best set of features along with optimal count and idf thresholds were chosen. All trials were run over our full set of news corpus. For each trial, a random list of 40, 60 or 100 initial seeds were chosen, new transliteration pairs were generated iteratively. During each iteration, the best scoring candidate pairs whose words are not in the original list are added to the list. The resulting list is then used to train the next iteration. The number of candidates to add is increased by one for every iteration. The micro-averaged recall and precision for 20 random trials for each configuration during each iteration shown in Table 5. Note that the  X  row simply records the net change in precision and recall after 10 bootstrapping iterations. The results show that with as little as 40 seed pairs, we can expect to obtain on average more correct pairs than wrong pairs with each iteration of the bootstrapping procedure.
 In this paper we have proposed an application of LSA towards Named Entity Translation. Experiments demonstrate its potential to combine information from large and diverse feature sets towards identifying NE pairs across languages. Furthermore, imposing an explicit pairwise mapping across pairs is helpful for Named Entity Translation. At the same time, less than stellar results suggest that it is unlikely that this approach by itself will yield spectacular results; rather it may be necessary to combine other measures and sources of information (as in Fig. 1) to get a practical system. More experimentation needs to be done to isolate variations due to the choice of seed pairs, possibly to semantic sub-clusters of translation pairs (e.g. place, person, org). Finally, some useful quantitative parameters were determined for our corpus, and we demonstrated the feasibility of this approach within a bootstrapping framework.

Future work include more in-depth experiments to compare this against anal-ogous approaches based on support vector machines or artificial neural networks. In addition, we would like to extend the general approach to the case of multiple languages, and extract transliteration tuples from comparable corpora.
 We would like to thank the reviewers for invaluable feedback on the initial draft; which have given us valuable insight for follow-up work, even though not all of their concerns could be adequately addressed with the final revision of this paper.
 Chunking identifies the non-recursive cores of various types of phrases in text, possibly as a precursor to full parsing or information extraction. Steven P. Abney was the first person to introduce chunks for parsing[1]. Ramshaw and Marcus[2] first represented base noun phrase recognition as a machine learning problem. In 2000, CoNLL-2000 in-troduced a shared task to tag many kinds of phrases besides noun phrases in English[3]. Much work has been done on Chinese chunking[4,5,6,7], of which supervised learn-ing approaches are the most successful. However, to achieve good performance, the amount of manually labeled training data required by supervised learning methods is quite large.

Semi-supervised learning has recently become an active research area. It requires only a small amount of labeled training data and improves performance using unlabeled data. In this paper, we investigate the use of a semi-supervised learning approach, tri-training learning[8], on Chinese chunking. By considering that Chinese chunking is a sequence labeling problem, we propose a novel approach of selecting training samples for tri-training learning. The experimental results show that the proposed approach can improve the performance significantly using a large pool of unlabeled data.
The rest of this paper is as follows: Section 2 describes the definition of Chinese chunking. Section 3 describes the algorithm of tri-training for Chinese chunking. Sec-tion 4 introduces the related works. Section 5 explains the experimental results. Finally, in section 6 we draw the conclusions.
 We defined Chinese chunks based on the Upenn Chinese Treebank V4.0 (CTB4) 1 as Chen et al. [9] did. And we use a tool 2 to generate the Chinese chunk dataset from CTB4. 2.1 Data Representat i on To represent the chunks clearly, we represent the data with an IOB-based model as the CoNLL-2000 shared task did, in which every word is to be tagged with a chunk type label extended with I (inside a chunk), O (outside a chunk), and B (inside a chunk, but also the first word of the chunk).

With data representation, the problem of Chinese chunking can be regarded as a sequence labeling task. That is to say, given a sequence of tokens (words pairing with Part-of-Speech tags), x = { x 1 ,x 2 , ..., x n } , we need to generate a sequence of chunk tags, y = { y 1 ,y 2 , ..., y n } . In the following sections, we call an original sentence ( x )as a data sequence, and a labeled sentence ( y ) as a labeled sequence. 2.2 Features In this paper, we regard Chinese chunking as a sequence labeling problem. The ob-servations are based on features that are able to represent the difference between two events. We utilize both lexical and Part-Of-S peech(POS) informatio n as the features.
We use the lexical and POS information within a fixed window. The features are listed as follows:  X  W i ( i =  X  2 ,  X  1 , 0 , 1 , 2)  X  W i W i +1 ( i =  X  2 ,  X  1 , 0 , 1)  X  P i ( i =  X  2 ,  X  1 , 0 , 1 , 2)  X  P i P i +1 ( i =  X  2 ,  X  1 , 0 , 1) Where W refers to a Chinese word while W 0 denotes the current word and W i ( W  X  i ) denotes the word i positions to the right (left) of the current word, and P refers to a POS tag while P 0 denotes the current POS and P i ( P  X  i ) denotes the POS i positions to the right (left) of the current POS. Tri-training was proposed by Zhou and Li [8], which was motivated from co-training. It designs three classifiers learning from unlabeled examples via an unlabeled example is labeled for a classifier if the other two classifiers agree on the labeling under cer-tain conditions. This method can release t he requirement of co-traini ng with sufficient and redundant views. Additionally, tri-training learning considers the agreements of the classifiers while selecting new samples. We just need the labeled tags instead of the confident scores made by the classifiers. Thus we can use any classifier, which can be used in chunking, in tri-training learning. 3.1 Algor i thm of Tr i -tra i n i ng The algorithm of tri-training for chunking is presented in Table 1, and consists of three different classifiers. At each iteration, the unlabeled sentences are labeled by the current classifiers. Next, a subset of the sentences newly labeled is selected to be added to the training data. The general control flow of the algorithm is similar to the algorithm described by [8]. However, there are some differences in our algorithm. Firstly, we design a new measure to compute the agreement between two classifiers. Secondly, we propose a novel selection method based on the agreement measure.
 3.2 Select Tra i n i ng Samples In selecting new training samples procedure, there are two steps: 1) compute the scores for all sentences; 2) select some sentences as new training samples according to the scores. We design a simple agreement measure to compute the scores for all sentences. Then based on the agreement measure, we propose a novel sample selection method, which is called Two Agree One Disagree method, to select newly labeled sentence as training samples.
 Agreement Measure. Here, we describe how to compute the agreement between two classifiers for every sentence.

Suppose we have a data sequence x = { x 1 , .., x n } . Then use two classifiers to tag the Now, we compute the agreement A g between y a and y b as follows: where n is the number of tokens in x , f is a binary function to tell whether y ia and y ib are the same label. A g denotes the agreement between two labeled sequences or the agreement between two classifiers. The larger A g is, the higher the agreement is.
 Select i on Methods. After computing the agreement, we should select new samples from the set of newly labeled sentences for next iteration. Suppose have three classi-fiers A, B, and C, we will select new samples for classifier A. As [10] suggested, we should select the sentences, which have high training utility. Thus we prefer to choose a sentence, which is correctly labeled by B or C and is not parsed correctly by the target classifier A, to be a new training sample.

We adopt two selecting principles: 1) I f the higher agreement scores between the classifiers B and C at a sentence, the sentence is more likely correctly labeled. 2) If the classifier A disagree with the other classifiers (B and C) at a sentence, the sentence is not parsed correctly by A. To investigate how the selection criteria affect the learning process, we consider two selection methods.

We propose Two Agree method by applying the first principle. A newly labeled sentence is selected by:  X  Two Agree Method ( S 2 A ): Firstly, we tag U A using B and C. Then we compute the
By applying both two principles, we propose Two Agree One Disagree Method. A newly labeled sentence is selected by:  X  Two Agree One Disagree Method ( S 2 A 1 D ): Firstly, we tag U A using A, B and C.
Each selection method has a control parameter m , that determines the number of newly labeled sentences to add at each iteration. It also serves as an indirect control of the number of errors added to the training set[10]. In our experiments, we set m as 30% after tuning parameters. Semi-supervised learning is halfway betw een supervised and unsupervised learning. In addition to labeled data, the algorithm requires unlabeled data. The interest in semi-supervised learning increased recently for natural language processing[11,12,13].
There are some researchers who applied semi-supervised learning on chunking and parsing. Ando and Zhang[14] proposed a semi-supervised learning method that em-ploys the structural learning for English chunking. Steedman et al.[15] learned the sta-tistical parsers from small datasets using bootstrapping.

A prominent semi-supervised learning algorithm is co-training, which was first in-troduced in Blum and Mitchell[16] as a bootstrapping method. It has the similar control flow as tri-training. In this paper, we also investigate the application of co-training to Chinese chunking. The co-training algorithm used was presented in [11]. We do not use the confident scores. Instead, we select new samples, which have higher agreement score A g of two classifiers. 5.1 Exper i mental Sett i ng The UPENN Chinese Treebank-4(CTB4) consists of 838 files. In the experiments, we used the first 728 files (FID from chtb 001.fid to chtb 899.fid) as training data, and the other 110 files as testing data. In the t raining data, we used sizes of labeled sen-tences: 500 sentences. The other sente nces (9,378) were used as unlabeled data for semi-supervised learning methods.
 In the experiments, we used Support Vector Machines (SVMs), Conditional Random Fields (CRFs), and Memory-based Learning (MBL)[17] as the classifiers in tri-training. The first two models have achieved good performance in chunking[18][19]. Although the MBL model does not perform well as the first two models do, we hope that it can help to improve the performance of the first two models. And we used SVMs and CRFs as the classifiers in co-training. We used YamCha (V0.33) 3 to implement the SVMs model, CRF++ (V0.42) 4 to the CRFs model, and TiMBL 5 to the MBL model.

And we used all the default parameter settings of the package in our experiments. In learning procedures, we trained the models in 5 0 iteration rounds and selected 50 (max-imum) newly labeled sentences as new training samples in each iteration. We evaluated the results as CONLL-2000 share-task did. In this paper, we report the results with F 1 score. 5.2 Exper i mental 1 : Select i on Methods of Tr i -tra i n i ng In this experiment, we investigated the p erformance of different selection methods: Two Agree method ( S 2 A ) and Two Agree One Disagree method ( S 2 A 1 D ). Figure 1 shows the experimental results, where CRF S1 refers to the CRFs model with Two Agree method, CRF S2 refers to the CRFs model with Two Agree One Disagree method, and the other strings have similar meanings.

From the figure, we found that Two Agree One Disagree method achieved better per-formance than Two Agree method. All three classifiers with Two Agree One Disagree method provided better results. And the CRFs model provided the best results among three models. These results indicated that Two Agree One Disagree method can se-lect new samples more efficiently via applying the second selecting principle. We also found that the MBL model can help to improve the performance of the CRFs and SVMs models, although it did not perform well as the CRFs and SVMs models did. With Two Agree One Disagree method, tri-training boosts the performance of the MBL model from 83.32% to 86.43% (3.11% higher), the SVMs model from 85.92% to 87.06% (1.11% higher), and the CRFs model from 86.68% to 87.57% (0.89% higher). 5.3 Exper i mental 2 : Tr i -tra i n i ng vs. Co-tra i n i ng In this experiment, we compared the performance between tri-training and co-training. Figure 2 shows the comparative results, where tri-training used Two Agree One Dis-agree method. We found that tri-training outperformed co-training. CRFs with tri-training achieved the best results with 87.57%, while CRFs with co-training got 87.16%. 5.4 D i scuss i on Our experimental results showed that tri-training learning approaches can exploit unla-beled data to improve the performance of Chinese chunking.

To test whether the improvements obtained by tri-training are significant, we per-formed one-tail paired t-test to compare the accuracy of tri-training against co-training and baseline systems. Here baseline systems refer to the models trained with seed data. For each token, if a classifier gives the correct tag, its score is 1, otherwise its score is 0. The p-values of one-tail paired t-test are s hown in Table 2. From the table, we found that tri-training gave higher accuracy than the baseline at the level of significance 0.001 for both the CRFs and SVMs models. For the CRFs model, tri-training provided higher accuracy than co-training at the level of significance 0.01. In addition, for the CRFs model, co-training gave higher accuracy than the baseline at the level of significance 0.01.

Tri-training showed statistically significant improvement over the baseline, but the gap between them was not so big. We looked into the detail over all types of chunks. Table 3 shows the results of all types of chunks generated by the CRFs model. Please note that the results of CLP and LST are 0 . 00 because there are not these two types of tags in 500 original labeled sentences. The results showed that the improvement was not observed uniformly on all types of chunks. Comparing tri-training with baseline systems, there were five types unchanged accuracy or changed very small, one type degraded, and six types changed from 0.2% to 3.35%. These indicated that we should try to improve the performance over all types to get better results.
 This paper presented an experimental study in which three classifiers were trained on labeled and unlabeled data using tri-training. We proposed a novel approach of select-ing training samples for tri-training learning by considering the agreements of three classifiers. The experimental results showed that the proposed approach can improve the performance significantly.

In this paper, we trained the models based on word-based sentences with POS tags attached. However in real applications, t he input is character-based sentences. Thus in future we will investigate th e performance of tri-training learning method on character-based chunking.
 Email categorization becomes very popular today in personal information management. It seeks to assign a label to each incoming email and thus manage emails by folders. Text classification methods have already been applied to email categorization and anti-spam filtering with mixed success [3,9,11]. Nevertheless, as they intend to deal with all classes in one model, two problems are inevitable. One, n-way classification methods surfer from the feature unevenness problem, which is referred to as the fact that features learned from training samples distribute unevenly in classes. For example, observation on Enron email corpus [8] reveals that number of messages in folders ranges from 5 to 1,398. Features extracted from those folders might vary significantly. Two, number of classes in text classification is an important classification methods perform less effective when considering more classes. 
Many efforts have been contributed to tackle the two problems by designing novel classification methods or enhancing the ones in existence. We believe that quality of text classification can be improved via researching on multi-class basis on the one hand. But on the other hand, there exist other ways that are worth trying, the binarization approaches for instance. 
The binarization approaches are investigated in this paper. The n-way classifier is first decomposed to multiple pairwise binary classifiers using strategies such as one-against-rest, one-against-one and some-against-rest. Then the binary classifiers are assembled using strategies such as round robin and elimination to achieve the objective of n-way classification. When elimination assembling strategy is adopted, quality of binary classifier used in elimination influences final prediction significantly. So some coupling strategies are adopted to help finding the optimal binary classifier. We implement the binarization approaches based on support vector machines (SVM) method. Substantial experiments are conducted on Enron email corpus [8] to justify our research efforts. The remaining sections of this paper are organized as follows. In Section 2 and Section 3, three binarization techniques and two assembling techniques are presented, respectively. In Section 4, experiments on email categorization are presented as well as comparisons and discussions. We describe related works in Section 5 and conclude this paper in Section 6. The binarization techniques seek to decompose the n-way classification problem to multiple pairwise binary classification problems using binarization strategies such as one-against-rest, one-against-one and some-against-rest [10]. 2.1 One-Aga i nst-Rest One-against-rest (OAR) is a straightforward binarization strategy referred as unordered learning technique. It transforms an n-way classification problem into N two-class problems where N is total number of classes involved. OAR uses samples in class i as positive samples and ones in other classes j(j=1, 2, ..., N; j  X  i) as negative samples. In most cases, N binary classifiers are not enough to make a correct prediction. Then the k -fold cross-validation method is used to produce N*k base binary classifiers. Theoretically, bigger k will produce better results. 
Compared to n-way classification, OAR is more accurate because two-class problem can be modeled more accurately than N classes using mathematical theory. Notably, more training samples can be used to produce accurate binary classifiers. Disadvantage of this method is that the feature unevenness problem still remains there as negative samples are usually more than that of positive samples. So the feature unevenness problem can not be well addressed in OAR. 2.2 One-Aga i nst-One One-against-one (OAO) is another binarization technique designed to resolve the feature unevenness problem. For each possible pair of classes, a binary classifier is trained on samples from the two involved classes. Compared to OAR, OAO transforms the n-way problem into ) 2 , ( N C two-class problems. 
As classes compete against each other, the feature unevenness problem can therefore be limited to a lower degree in OAO. Quality of email categorization can be thus improved. The drawback of OAO is computational complexity in training. Besides, when training samples in some classes are very few, some extra techniques, say k -fold validation, are required to train the binary classifiers. 2.3 Some-Aga i nst-Rest The feature unevenness problem remains serious if samples distribute very unevenly in folders. To make the binary decision process more flexible, the some-against-rest (SAR) binarization technique is proposed. The some-against-rest technique is a typical divide-and-conquer solution. Classes are first split into two groups, referred to as super classes. Training samples with class labels in the first group are considered as positives and samples from classes in the second group as negatives. Then binary classifiers are built with the re-labeled samples. 
Advantage of SAR is that, not only is computational complexity saved, but equivalent quality of email categorization can be achieved provided that the grouping procedure is effective. The feature unevenness problem can be resolved more effectively than any other binarization approaches because pairwise coupling [6] is complexity of SAR is higher than the other binarization techniques. 2.4 Compar i son on a Real Case The n-way classification and binarization techniques for a real classification problem are illustrated in Fig.1. For the 6-class problem shown in Fig.1(a), OAR shown in Fig.1(b). As a comparison, OAO learns ) 2 , 6 ( C =6*5/2=15 binary classifiers, one for each pair of classes. Fig.1(c) shows the binary classifier &lt;+, ^&gt;. Obviously, in OAO case, the base classifier uses fewer examples than that in OAR case and thus has more freedom for fitting a decision boundary between the two classes. However, computational complexity in OAO training is increased from 6 classifiers to 15 ones. 
The SAR approach uses documents in two groups of classes as binary *), (-, #, ~)&gt;. In the SAR approaches, training samples can be arranged appropriately in terms of training sample number and content similarity. Assembling process aims to produce a final multi-class prediction based on predictions from the base binary classifiers generated in the binarization process. Two assembling strategies are discussed in this paper, i.e. round robin and elimination. 3.1 Round Rob i n Assembl i ng Round robin (RR) assembling technique is designed as a voting technique for OAO binarization technique [5], in which all the N*(N-1)/2 binary classifications are executed on given test samples. The label holding maximum votes from the base binary classifiers is output as final prediction. This concept can be revised and adopted in other binarization techniques. 
RR is originally adopted to produce a vote between two base classes. Now we extend the concept to consider two groups of classes, namely two supper classes. For example, in OAR, we consider the  X  X est X  classes as one super class. Now RR becomes applicable to OAR, and the OAR binary classifiers can be used to elect an optimal winner. For SAR, RR is made applicable by considering the  X  X ome X  classes as one super class and the  X  X est X  classes as another super class. 
Another key issue in RR assembling is weight of the binary classifier generated by the binarization techniques. Intuitively, binary classifiers hold different prediction confidence. The confidence is in turn considered as weights of the binary classifiers when votes are counted for each class. The process to calculate weight of each binary classifier is referred to as pairwise coupling [6]. Typical metrics used in pairwise coupling are discriminativeness and evenness. Discriminativeness represents the degree that content of two classes is discriminative. Evenness represents the degree confidence is product of the two degree values. 
The ultimate goal of RR assembling is to produce a final prediction based on votes counted for each class. In the three binarization techniques, votes are counted in different ways. In OAR, we assign 3  X   X  to the  X  X ne X  class and 0 to each of the  X  X est X  the  X  X est X  classes is assigned ) /(N-1 3  X   X  , where  X  is weight of this binary classifier.  X   X  to the  X  X ome X  classes and 0 to the  X  X est X  classes if the  X  X ome X  group wins the competition; otherwise 0 is assigned to the  X  X ome X  classes and ) ( 3 K N /  X   X   X  to the  X  X est X  classes, where K is number of classes in the  X  X ome X  group. 3.2 El i m i nat i on Assembl i ng Elimination (EL) is another effective assembling technique in binarization approaches. It is often ignored by researchers because the elimination procedure is somehow error-prone. Intuitively, final prediction would be incorrect if the correct class is unfortunately eliminated in an earlier elimination round. However, we argue that such an incorrect elimination can be avoided when prediction confidence is incorporated in pairwise coupling [6]. 
The elimination process for OAO goes like this. A competition list is initialized to contain all classes and maintained after each run. In each run, two classes are applied to classify the test sample. When a prediction is made, the loser class is eliminated from the list and the winner is put back. Now size of the competition list is decreased by one. The elimination process is repeated until the competition list contains one class, which is considered as the optimal prediction that the test sample most likely belongs to. For OAR, the binary classifier with biggest weight is selected to classify the test sample. If the  X  X ne X  class wins, this class will be output as prediction. Otherwise, further eliminations will be executed within the  X  X est X  classes using OAR binarization technique until the final prediction is obtained. Elimination process in SAR is similar. If the  X  X ome X  super class wins, classes in the  X  X ome X  group remain in the competition list and classes in the  X  X est X  group are eliminated. Then the SAR binarization technique is adopted for the  X  X ome X  group and produces further eliminations until the final prediction is obtained. 
Advantage of the EL assembling technique is the least computational complexity in searching for the final prediction. However, EL assembling technique is error-prone. When the binary classifier is selected inappropriately, the overall prediction will be put in danger. So prediction confidence is calculated to help finding the optimal binary classifiers. When the optimal classifiers are employed in elimination, some prediction errors can be avoided. 3.3 Compar i son We compare the two assembling techniques in two aspects, i.e. classification quality and computational complexity. Intuitively, the RR assembling is more reasonable than the EL because less errors occur in the former assembling technique. When prediction confidence is considered as weight for each voting binary classifier, quality of RR assembling can be further improved. 
Elimination is advantageous in terms of computational complexity in predicting the optimal class. For example, with EL assembling, less than 1  X  N rounds of binary classification are required. But with RR assembling, k N * rounds are required in Obviously, RR is much more computationally complex than EL. 4.1 Exper i ment Setup Substantial experiments are conducted in this paper to evaluate the binarization techniques and assembling techniques in performing the task of email categorization. The training sets and test sets are extracted from the Enron email corpus [8] using the same data preparation method in [1]. We use the first 80% emails as training samples and the rest for test emails as test samples. Evaluation criteria are precision (p), recall (r) and F-1 measure (f) [12]. To produce overall results for each user, the micro-average method [12] is adopted. We use SVM light [7] as base binary classifier in our binarization implementations. approaches. Three binarization approaches are implemented, i.e. OAR, OAO and SAR, with two binarization and assembling techniques, i.e. RR and EL. 4.2 Results We first run the baseline SVM n-way classification method. Then we run our approaches using various binarization and assembling techniques. Micro-averaged F-1 measure values achieved by the binarization approaches on seven users are presented in Table 1. 4.3 D i scuss i on I : N-Way vs. B i nar i zat i on The first comparison is made between the baseline n-way SVM method and the binarization approaches. Table 1 reveals that all the binarization approaches outperform SVM on all users. It thus proves that classification quality can be improved in the binarization approaches. 
F-1 measure gain in SAR is around 0.18 over the baseline on average. The best performance is achieved on user beck , in which SAR-RR outperforms the baseline by 0.206 on F-1 measure. This can be addressed by observing training samples created by user beck . We find that user beck manages 101 folders and emails are distributed in folders very unevenly. It is thus not strange that SVM produces worst F-1 measure on user beck . Fortunately, overall classification quality equivalent to the binary classifiers can be achieved in the binarization approaches. Moreover, advantage of the binarization approaches gets more distinct when number of folders becomes bigger. 
In contrast, F-1 measure improvement on user williams is very little, i.e. 0.004, over the baseline. We look into email collection maintained by user williams and find feature unevenness problem is extremely serious as 50.5% training samples come from folder schedule_crawler , 37% from bill_williams_iii and the remaining 12.5% from the other 14 folders. Classification problem in this case actually degrades to binary classification between folder schedule_crawler and bill_williams_iii . This is why F-1 measure on user williams is rather high and the binarization approaches do not improve much. 4.4 D i scuss i on II : B i nar i zat i on Approaches Another general trend revealed in Table 1 is that SAR produces best quality among the three binarization approaches and OAR performs worst. This proves that the feature unevenness problem is resolved most effectively in SAR. However improvement varies on the seven users. This is because training environment is different for these users in terms of folder discriminativeness and evenness. 
An interesting finding is that RR and EL in OAO produce satisfactory quality close to each other while RR outperforms El by 0.024 on average in OAR. This can be explained that elimination is most error-prone in OAR. When prediction confidence is incorporated in eliminatio n assembling, optimal binary classifiers can be correctly selected in OAO. This is why RR outperforms EL by only 0.003 on average. 4.5 D i scuss i on III : Round Rob i n vs. El i m i nat i on According to Table 1, all binarization approaches using RR assembling outperform those using EL. This reveals that RR is more reasonable than EL since fewer errors occur in RR. Although prediction confidence can be calculated very accurately, quality of binarization approaches using EL never goes beyond those using RR. 
Another finding is that RR outperforms EL by around 0.05 in SAR while by 0.024 in OAR and by 0.003 in OAO. It can be inferred that EL is more error-prone in OAR and SAR. This can be explained by methodological difference amongst the three binarization techniques. In SAR, fewest errors are made because the optimal binary classifier used in elimination is easily found. As comparison, OAO constructs binary classifiers found in OAO are not as effective as those found in SAR in resolving the feature unevenness problem. This proves that SAR is most feasible in adopting the elimination assembling technique while OAO is least. Decomposition and combination process in our binarization approaches are similar to error correcting output coding (ECOC) method discussed in [2]. Technically, ECOC is a general framework for the binarization methods to n-way classification problem and the binarization techniques discussed in this paper are special coding schemes for ECOC. However, we attempt to compare the binarization methods in email categorization, in which feature unevenness problem is serious. 
The round robin assembling technique is described and evaluated in [5]. Our work is different. We focus on comparing round robin against other two binarization approaches on addressing the feature unevenness problem. Moreover, this is the first large-scale investigation on various binarization approaches to email categorization. Classification quality is decreased by feature unevenness problem in email categorization. We propose to resolve this problem with binarization classification approaches. In this paper, three binarization techniques, i.e. one-against-rest, one-against-one and some-against-rest are implemented using two assembling techniques, i.e. round robin and elimination. Experiments show that the binarization approaches outperform the baseline n-way SVM classifier, in which some-against-rest improves elimination assembling in helping making optimal predictions, with the price of higher computational complexity. Word sense disambiguation involves the association of a given word in a text or discourse with a particular sense among numerous potential senses of that word. For this task, many supervised machine learning algorithms have been used for the WSD task, including Na  X   X ve Bayes, decision trees, an exemplar-based, support vector machines, maximum entropy, etc (see, e.g., [3]). However, supervised methods require large labeled data for training, which are expensive to obtain. Therefore, many researchers have recently concentrated their efforts on how to use unlabeled data to boost the performance of supervised learning for WSD, such as in [5,11,10,7]. The process of using both labeled and unlabeled data for training is called semi-supervised learning or bootstrapping .

In this paper, we focus on an approach that iteratively enlarges labeled data with new labeled examples which are obtained from unlabeled data. A common method for the extension of labeled data is to use the classifier trained on the current labeled dataset to detect labels for unlabeled examples. Among those new labeled examples, some highly accurate ones are selected and added to the current labeled dataset. This process is iteratively repeated until there is no unlabeled example left, or until the number of iterations reaches a pre-defined number. Two well-known methods of this approach are self-training [9] and co-training [1]. A general algorithm of this approach is sketched in Fig. 1 (it can be considered as the general self-training algorithm). We address two problems occurring in this algorithm as follows.

P 1 : The first problem is how to determine a subset of new labeled examples at each extension of labeled data. To obtain  X  X ood X  new labeled examples, we consider two criteria: the correctness of labeling and the number of new labeled examples. It is clear that adding a large number of misclassified examples into the labeled dataset will probably result in decreasing the accuracy of the classi-fier, and increasing confidence 1 of labeling may receive a small number of new labeled examples. Suppose that we have a new example which is assigned a label with a detection probability, previous studies normally use a threshold for this probability to decide whether a new labeled example will be selected or not, such as in [9,1]. However, choosing a high threshold will create difficulty in ex-tending labeled data, and does not always result in correct classification. On the contrary, choosing a lower threshold may result in more misclassified examples. In this paper, we will increase confidence of new labeled examples by using one more supervised classifier for the detection process.

P 2 : The second problem is that of how to generate the final classifier when the process of extending labeled data is completed. According to the framework in Fig. 1, this process will be stopped when the number of iterations reaches a pre-specified value, or until the unlabeled dataset becomes empty. Normally, the classifier, which is built on the labeled data obtained at the last iteration, is chosen as the final classifier. Some studies use a development dataset to find the most appropriate value for the number of iterations, such as in [7,5]. Others used an upper bound of error rate of training data as the condition for stopping this process, such as in [2]. However, the last classifier may be built based on new training data with some misclassified examples (related to problem P 1 ), so some advantages and some disadvantages are concurrently brought to the last classifier in comparison with the initial supervised classifier (i.e. the classifier which trained on the original labeled da taset). In our knowledge, this observa-tion, which has not been observed in previous studies yet, may lead us to a solution of combining the advantages of both the initial and the last classifiers under classifier combination strategies.

The solutions for these two problems consequently generate variants of boot-strapping algorithms. They were evaluated through experiments on the four words interest , line , hard , server ; and on English lexical sample of Senseval-3. Therefore, next section presents prop osed solutions and new bootstrapping algorithms. Section 3 presents experiments with data preparation, results and discussion. Finally, conclusions are presented in section 4. 2.1 Extending Labeled Data This section presents problem P 1 . An usual approach to this task is using a supervised learning algorithm to train a classifier based on the labeled dataset, and then using this classifier to detect labels for examples in a subset U of the current unlabeled dataset U . Suppose h is the supervised classifier and l ( h, e )is the label of example e detected by h with probability P ( h, e ). If P ( h, e )isgreater than a threshold  X  ,thenexample e with new label l ( h, e ) will be considered to be added to L . As mentioned previously, using a classifier with threshold  X  for determining new labeled examples may cause a tradeoff problem between the extensibility and the accuracy of label detection. Furthermore, increasing  X  does not always result in an increase of accuracy in new labeled examples. To tackle this problem, we propose a solution that uses one more classifier, h 2 ,as follows.

Suppose that at each extension, the maximum number of new labeled ex-amples which are added to the current labeled dataset is N .Wefirstlyuse h and h to detect labels for examples in U , and then select the examples e such that P ( h, e )  X   X  . Among such new labeled examples, the examples which have agreements of labelling of h and h are selected. If there exist more than N such examples, we will prefer the examples with high confidence of detection of h . In addition, if there are not enough agreements of labelling between h and h , we use h to select more new labeled examples, and also prefer the examples e with high P ( h, e ). By this solution we can increase the confidence of new labeled examples, and also maintain the capability of extending labeled data.
 Concerning this task, we also build a procedure of retaining class distribution. It is necessary because if a classifier is built based on the training data with a bias in some classes (i.e. some classes dominate others), then this bias will be increased at each extension of the labeled data. This problem is also considered in previous studies such as [1,8]. For a set of labeled examples, we divide it into the subsets such that the examples in each subset have the same label. We call them class-based subsets. So that, building a procedure of retaining class distribution for a set of new labeled examples means to resize its class-based subsets to keep class distribution of the original labeled dataset. 2.2 Generating Final Classifier There is a fact that when extending labeled data, the feature space will be ex-tended concurrently with adding some examples which are misclabeled. There-fore, the quality of labelling of the last classifier (trained on the last labeled dataset) will depend on each particular test example. In comparison with the initial supervised classifier, the final classifier may be better in detecting some test examples if these examples contain many new features covering by the new labeled examples. In the contrary case, if test examples can be well labeled by the initial supervised classifier, it is not necessary and risk to use the last clas-sifier to label these examples. A natural way to utilize advantages of both these classifiers is to combine them when making decision of labelling. It then becomes a classifier combination problem. Based on OWA combination rules, which were also used for WSD, as presented in [4], we found that the median rule and max rule are intuitively applicable for this objective. These combination rules are recalled as follows.

For each example e , suppose P i ( h 1 ,e ) ,i =1 ,...,m is the probability distrib-ution on class set { c 1 ,...,c m } of using classifier h 1 to label e . A similar definition is applied to P i ( h 2 ,e ), for classifier h 2 and i =1 ,...,m .

Combining h 1 and h 2 using median and max rules, we obtain new probability distributions as follows:
Then the class c k will be assigned to example e when using median rule (or max rule) iff: 2.3 A New Bootstrapping Algorithm By the solutions as mentioned above, we generate a new bootstrapping algorithm as shown in Fig. 2. In this algorithm, Resize ( L 1 ,N ) is the procedure for retaining class distribution, which returns new sizes for all class-based subsets of L 1 and satisfy that sum of all new sizes is less than or equal N ; L is the labeled data; U is the unlabeled data; A and A are two supervised learning algorithms, in which A is the primary one;  X  is a threshold; K is the maximum number of iteration; N is the maximum number of added labeled examples at each iteration; C = { c 1 ,...,c m } is the set of classes; suppose S is a set of labeled examples, define C i ( S )= { e | e  X  S, l ( e )= c i } ,where l ( e ) is the label of example e ,and i =1 ,...,m .

In fact, the new algorithm is an extension of the general self-training by pro-viding solutions for problems P 1 and P 2 . For experiment, we consider four vari-ants of the bootstrapping algorithm as follows: A 1 is the general self-training algorithm; A 2 is the self-training algorithm with the solution for problem P 2 (we separate this by A 2 a and A 2 b respect to median and max rules for the combi-nation step); A 3 is the self-training algorithm with the solution for problem P 1 ; A 4 is the self-training algorithm with the solutions for problem P 1 and P 2 (i.e. the new algorithm; we separate this by A 4 a and A 4 b respect to median and max rules for the combination step). 3.1 Data The first experiment was carried out on the datasets of the four words, namely interest , line , serve ,and hard , which were obtained from Pedersen X  X  homepage 3 . All examples in these datasets were tagged with the right senses. The sizes of these data are 2369, 4143, 4378, and 4342 for interest , line , serve ,and hard , respectively. These datasets are large enough for dividing into labeled and un-labeled data sets. Furthermore, because we knew the tagged senses of examples in unlabeled dataset, we could evaluate the correctness of the new labeled ex-amples (for problem P 1 ). We randomly extract 100 examples for labeled data, 300 examples for test data, and the remaining examples are treated as unlabeled examples.

The second test was carried out on English lexical sample from Senseval-3 data 4 . Unlabeled data in this experiment was collected from the British National Corpus (BNC) with about 3000 examples for each ambiguous word. Note that because the English lexical sample was also retrieved from BNC, so for a fair test, we removed all contexts from unlabeled dataset which also appear in the training or test datasets. 3.2 Feature Selection Two of the most important kinds of information for determining the senses of an ambiguous word are the topic of the context and the relational information rep-resenting the structural relations between the target word and the surrounding words in a local context. A bag of unordered words in the context can represent the topic of the context, while collocation can represent grammatical informa-tion as well as the order relations between the target word and neighboring words. We also use more information about local context represented by words assigned with their positions, and their p art-of-speech assigned with positions. These kinds of features are investigated in many WSD studies such as [6,5]. Par-ticulary, all features used in our experiment fall in the kinds: a set of content words that include nouns, verbs, and adjectives, in a context window size 50; a set of collocations of words (we selected a set of collocations as presented in [3]); a set of words assigned with their positions in a window size 5; and a set of part-of-speech tags of these word also assigned with their positions, also in window size 5. 3.3 Parameters and Results For the new algorithm, we chose naive Bayes (NB) as the primary supervised learning algorithm and support vector machines (SVM) as the second algorithm. That because SVM is a discriminative learning algorithm meanwhile NB is a generative one, so this difference will make SVM as an independent and confident classifier to verify the correctness of NB classifier X  X  detection.

For the remaining parameters, we fix the maximum size of unlabeled dataset used at each iteration | U | = 800 and the maximum size of new labeled examples which are added to the labeled dataset N = 300. The number of iteration K runs from 1 to 15 when testing on datasets of the four words, and then the best was used for testing on Senseval-3.
In the first experiment, we investigated the problem P 1 through various values of  X  , and the effectiveness of using one classifier (NB) and two classifiers (NB and SVM). The threshold  X  was tried the values { 0.5, 0.6, 0.7, 0.8, and 0.9 } .The result in Fig. 3 shows that using one more classifier much decrease error rate (from about 25% to 10%). In addition, this results also suggests the choice of  X  =0 . 6, which may ensure both the correctness and capability of the extension.
Table 1 shows a test on English lexical sample of Senseval-3 5 whereweran the bootstrapping algorithms 10 times and got the average. The algorithm A 3 is better than the algorithm A 1 (71 . 03% in comparison with 70 . 6%) shows the effectiveness of problem P 1 . In addition, it should be noted that the algorithm with solution for problem P 2 still improves much the accuracy of self-training in the case not using solution for problem P 1 (the algorithm A 2 ). It can be ex-plained that: permitting a high error rate of new labeled examples will expand more feature space which results in recognizing more new examples, and the proposed combination strategies keeps i ts advantage and decrease its disadvan-tage. Overall, combining the solutions for both problems P 1 and P 2 give the best result, and it increases about 1 . 9% of accuracy in comparison with supervised learning using NB. In this paper, we have shown two problems of semi-supervised learning, particu-larly for self-training algorithm. They include the problem of extending labeled data and that of generating the final classifier. These problems have been inves-tigated in WSD problem, and corresponding solutions were given. For the first problem, we used one more classifier to decrease error rate of new labeled exam-ples. And for the second problem, we used two strategies of classifier combination including median and max rules to utilize both advantages of the last classifier (built based on the extended labeled data) and the initial supervised classifier. With those solutions, a new bootstrapping algorithm with several variants was generated. The experiments show that the proposed solutions are effective for improving semi-supervised learning. In addition, it also showed that unlabeled data significantly improve supervised learning in word sense disambiguation. Many practical dialog systems, such as online air travel planning [1] and tele-phone weather report announcing systems [2], are premised on the idea that users can make their requests clear enough and form their requests as lingual expressions. Under these conditions, a system can estimate the users X  intentions using methods like pattern-matching, because the users X  aims are definite and the input is restricted. Therefore, the system can give correct answers from prepared databases. In terms of answers, such a system needs fixed utterance patterns. This type of dialog system works well for specialized tasks, but is unsuitable for multiple tasks or open-ended input.

On the other hand, Eliza [3] is a well-known chat system for open-ended con-versations. Eliza does not restrict the topic of conversation or the type of input. Eliza is a psychotherapeutic counseling system; therefore, it converts user utter-ances into interrogative sentences or make s nonsense responses, e.g.,  X  X eally? X ,  X  X  see. X , and  X  X hat happened? X , to avoid answering to the user clearly and to promote introspection. Consequently, Eliza gives no new information to users.
We propose a new dialog system that lies between these two types of dialog system. Our system converses about vario us topics and gives information related to the user X  X  utterances, and it is useful for a new idea generation. When a user has an obscure desire to get information about his or her interest, but no concrete goal, he or she may generate some ideas through conversing with our system. Our system has three dialog modes. The information retrieval utterance mode uses a corpus as the source for the system X  X  utterances. The proper answer to a user X  X  utterance is selected from a corpus. To select the next system utterance, surface cohesion and lower semantic coherence are taken into account. In the study we report here, our system was made to converse about movies. Although the range of topics of our trial system is limited due to the limited corpus, the user can enter utterances freely. Below is a sample dialog in which our dialog system (S) talks about movies with auser(U). 1 We suppose that the user X  X  and the system X  X  utterances are made alternately. Our dialog system has three modes of operation.  X  Accurate Understanding Utterance Mode (AUU mode)  X  Information Retrieval Utterance Mode (IRU mode)  X  Nonsense Response Mode (NR mode) In IRU mode, the system selects a sentence from a corpus to respond to the user X  X  utterance. The user X  X  utterance is only analyzed morphologically, because we could not prepare enough and reliable knowledge, such as case frames, to analyze a sentence syntactically for an open-ended conversation.
We gathered sentences from web pages using the Japanese web search engine goo 3 with the keywords  X   X   X  EGA X  and constructed a movie corpus.  X   X  EGA X  means  X  X ovie X  in Japanese. The keyword  X   X  EGA X  was used to exclude irrelevant pages.  X  is the expression which is to be the main theme of the dialog, that is to say, either a film title or the name of a person such as a director, an actor, or a writer of some film. Pages searched on the Internet included weblogs, diaries, and online shopping sites. Only a part of the page X  X  content may be related to the main theme  X  or, even worse, only a few sentences may be relevant. Thus, the content of the gathered pages was hand-checked to remove sentences unrelated to  X  . Each sentence in our corpus was organized according to  X  ,and S (  X  ) denotes the set of sentences gathered by the keywords  X   X   X  EGA X .

Case elements are often abbreviated in Japanese. These invisible case ele-ments are called zero pronouns. Anaphoric relations with zero or non-zero pro-nouns must be resolved in the original documents for our system to be able to use corpus sentences as candidates for the system X  X  utterances. In our trial sys-tem, such anaphoric relationships were manually resolved in advance and each zero or non-zero pronoun in a corpus sentence was replaced with its correct antecedent.

In IRU mode, our system selects a proper response to a user X  X  utterance from the corpus sentences through the three kinds of filtering and one ranking.We explain these filterings and the ranking respectively. 4.1 Filtering by Dialog Main Theme We suppose that a dialog in our system will not be very long, and that the main theme of the dialog will be set at the beginning of the dialog and will not change. In our system, once a dialog starts, the system seeks the noun phrase that the first user X  X  utterance most centrally concerns as the main theme of the dialog. We describe the centralness ranking later. Sentences from web pages whose main themes are the same as the dialog X  X  main theme tend to be more appropriate for the system X  X  utterances than ones on other web pages. Therefore, if the main theme of a dialog is  X  , our system restricts the candidates for its utterances to S (  X  ). If S (  X  ) is empty, the IRU mode fails to generate a sentence. 4.2 Filtering by Surface Cohesion Our system selects utterance candidates in a way that the transition of the cen-tral concerns of the dialog will be natural. One theory that deals with such a transition in a discourse is the Centering Theory, and Walker, et al. applied this theory to Japanese[4]. When we create a transition model of central con-cerns according to the Centering Theor y, the zero pronoun relationship must be resolved. Although anaphoric relationships have already been resolved in the corpus sentences, they must be resolved in the user X  X  utterance, as well. Precise zero pronoun resolution requires a case frame dictionary, but constructing one entails a lot of effort. Moreover, there is the problem of word sense disambigua-tion using case frames[5,6]. Therefore, we suppose the following simple rule and our system selects candidates from S (  X  ) according to this rule. (1) The centralness of discourse entities in a sentence is ranked as follows: (2) If there is neither a noun phrase with a postposition  X  X A X  nor one with a (3) We call what the system X  X  utterance should most centrally concern the focus
The anaphoric relationships have already been resolved in the corpus sen-tences. Therefore, the sentences extracted in step (3) most centrally concern f .When f is set to the highest ranking entity in the centralness ranking of the previous user X  X  utterance and therefore the postpositonal phrase  X  f HA X  is abbreviated in the extracted sentences in step (3), we can determine that the extracted sentences have surface cohesion according to the Centering Theory. When f is set to the dialog X  X  main theme, the local theme becomes the dialog X  X  main theme, and this transition also seems natural. 4.3 Filtering by Predicate Coherence Candidates are selected by the predicate coherence with the user X  X  utterance. We show how to select proper candidates by predicate coherence. Verb Coherence. Auser X  X utteranceisassumedtohaveapredicateverb.
 The predicate verb of the next utterance is restricted. If the system X  X  utterance uses an unnatural predicate, the conversation will not be natural. Therefore, the system selects candidates to provide predicate verb coherence. In our system, verb coherence is calculated using mutual information given by where  X  p ( v ) is the probability estimation for v and  X  p ( v i ,v j ) is the probability estimation for v i and v j to appear within two continuous sentences. To estimate the above probabilities, we gathered 5,875,515 sentences from web pages about movies. When the predicate verb of the user X  X  utterance is v , candidates are selected on the condition that a sentence whose predicate is v i are satisfied by MI V ( v, v i )  X   X  ,where  X  is a threshold. In our trial system, the threshold is set experientially as  X  =0 . 5.
 Adjective Cohesion. Discussing movies, a sentence whose main predicate is an adjective evaluates a quality of something such as movie a title or an actor. When a user makes an evaluation, he or she expects that the system will agree with his or her opinion. 5 The 1000 most frequent adjectives in the 5,875,515 sentences described in the above section were thus sorted into positive, nega-tive, or other categories. When the user X  X  utterance has a positive or negative adjective as its main predicate, the system X  X  utterance should have same type as the predicate of user X  X  utterance. When the predicate of the user X  X  utterance is neither positive nor negative, the system selects any sentences with predicate adjectives regardless of their views. In our trial system, every adjective is sorted on manual. A study of collecting evaluat ing expressions from a corpus [7] may help us determine the adjective type automatically. 4.4 Ranking by Coherence of Nouns At last, candidates are ranked according to noun coherence. The contents of the system X  X  utterance should be related to the one of the user X  X . To evaluate this relationship, we focus on noun coherence between the user X  X  utterance and each candidate for the system X  X  utterance. We suppose that N is the set of all nouns in the user X  X  utterance except for the current focus, and N is a set of all nouns in each candidate for the system utterance except for the current focus 6 . According to N and N , we evaluate the noun coherence with mutual information of ( n, n )  X  N  X  N as follows: where  X  p ( n )and X  p ( n ) are the probability estimations of n and n and  X  p ( n, n )is the joint probability estimation of n and n . Here, n and n are assumed to co-occur when a sentence having both n and n exists in the learning data. The three highest pairs of MI N (  X  )s are selected and the sum is calculated as the cohesion of the user X  X  utterance and each candidate. The noun cohesion is restricted to the top three pairs to reduce the influence on length of each candidate sentence.
Finally, the candidate sentence that has the highest noun cohesion with the user X  X  utterance is selected from the corpus for the next system X  X  utterance. In addition, this sentence is removed from the candidates in the corpus to prevent the system from selecting the same sentence in the same dialog. Below we discuss a few dialog examples. The following examples continue the dialog about  X  X harlie and the Chocolate Factory. X  (Example 1) This example is a success. It is a part of a series of utterances, and this user X  X  utterance has a zero pronoun whose antecedent is  X  Charlie and the Chocolate Factory , X  and the antecedent of the noun phrase with  X  X A X  in the system X  X  utterance, which is abbriviated as a zero pronoun, is the same as the user X  X . In regard to the predicate, both  X  X OI igoodj X  in the user X  X  utterance and  X  X UB-ARASH  X  I (splendid) X  in the system X  X  utterance are positive adjectives. In regard to the noun phrases, we show the top three pairs of noun phrases in N  X  N as follows. The left-hand side of each pair is a noun phrase and the right-hand side is mutual information.
 (Example 2) This example is also a success. Focus cohesion is as good as in Example 1. The predicate adjective of the user is the same word  X  X AME DA X  (bad). 7 Strictly speaking,  X  X AME DA X  in the user X  X  and the system X  X  utterances are different. The subjective case of  X  X AME DA X  in the user X  X  utterance is the latter half of it, and the one in the system X  X  utterance is the conversation partner (the user). To solve this problem, we need to resolve modification relationships. We proposed a dialog system that makes a natural conversation about movies with users. The system selects a sentence from a corpus when the conditions of focus consistency and predicate cohesion are satisfied and noun cohesion is the highest. We had some persons use our system, and got the subjective evaluation that its responses seems very natural. We will evaluate the system quantitively in our future work.

The system considers only the local consistency at present. Therefore, the system may compliment a movie once but later denigrate it. In the future, we hope to achieve throughout a dialog. We also plan to review predicate cohesion. Our criterion is not necessarily the best for selecting the proper sentence. We must compare criterions from the other points of view and choose the best one. An important issue in empirically minded research on natural language parsing is to what extent our models and algorithms are tailored to properties of specific languages or language groups. This issue is especially pertinent for data-driven approaches, where one of the claimed advantages is portability to new languages. The results so far mainly come from studies where a parser originally developed for English, such as the Collins parser [1], is applied to a new language, which often leads to a significant decrease in the measured accuracy [2,3,4,5,6]. How-ever, it is often quite difficult to tease apart the influence of different features of the parsing methodology in the observed degradation of performance.

One topic that is prominent in the literature is the issue of lexicalization, i.e., to what extent the accuracy can be improved by incorporating features that refer to individual lexical items, as opposed to class-based features such as part-of-speech. Whereas the best performing parsers for English all make use of lexical information, the real benefits of lexicalization for English as well as other languages remains controversial [4,7,8].

Another aspect, which so far has received less attention, is the proper treat-ment of morphology in syntactic parsing, which becomes crucial when dealing with languages where the most important clues to syntactic functions are often found in the morphology rather than in word order patterns. Thus, for a lan-guage like Turkish, it has been shown that parsing accuracy can be improved by taking morphologically defined units rather than word forms as the basic units of syntactic structure [9].

In this paper, we study the role of lexicalization, morphological structure and morphological feature representations in data-driven dependency parsing of Turkish. More precisely, we compare representations based on the notion of inflectional groups proposed by Eryi  X  git and Oflazer [9] to a more traditional rep-resentation based on word forms. We experiment with different ways of repre-senting morphological features in the input to the parser, and compare lexicalized and unlexicalized models to see how they interact with different representations of morphological structure and morphological features.

The parsing methodology is based on a deterministic parsing algorithm in combination with treebank-induced classifiers for predicting the next parsing action, an approach previously used for the analysis of Japanese [10], English [11,12], Swedish [13] and Czech [14]. Our study complements that of Eryi  X  git and Oflazer [9], which considers dependency parsing of Turkish in a probabilistic framework.

The rest of the paper is structured as follows. Section 2 describes some ty-pologically prominent features of the Turkish language; section 3 introduces the framework of data-driven dependency parsing; and section 4 explains the exper-imental setup. In sections 5 X 7, we present experimental results on the impact of a morphologically defined tokenization (section 5), of rich inflectional features (section 6) and of lexicalization (section 7). Section 8 presents the results ob-tained with an optimized parsing model, in comparison to the state of the art, and section 9 contains our conclusions. Turkish is a flexible constituent order language. Even though in written texts, the constituent order of sentences generally conforms to the SOV or OSV struc-tures, [15] the constituents may freely change their position depending on the requirements of the discourse context. From the point of view of dependency structure, Turkish is predominantly (but not exclusively) head final.

Turkish has a very rich agglutinative morphological structure. Nouns can give rise to hundreds of different forms and verbs to many more. Furthermore, Turkish words may be formed through productive derivations, and it is not uncommon to find up to five derivations from a simple root. Previous work on Turkish, [16,17,9] has represented the morphological structure of Turkish words by split-ting them into inflectional groups (IG). The root and derived forms of a word are represented by different IGs separated from each other by derivational bound-aries. Each IG is then annotated with its own part-of-speech and any inflectional features. Figure 1 shows the IGs in a simple sentence:  X  X   X  u  X  c  X  uk odaday X m X  ( I X  X  in the small room ). The word  X  X daday X m X  is formed from two IGs; a verb is derived from an inflected noun  X  X dada X  ( in the room ).

Dependency relations in a sentence always hold between the final IG of the de-pendent word and some IG of the head word [17,9], so it is not sufficient to just identify the words involved in a dependency relation, but the exact IGs. In the example, the adjective  X  X   X  u  X  c  X  uk X  ( small ) should be connected to the first IG of the second word. It is the word  X  X da X  ( room ) which is modified by the adjective, not the derived verb form  X  X daday X m X  ( I X  X  in the room ). So both the correct head word and the correct IG in the head word should be determined by the parser. A prominent approach to data-driven dependency parsing in recent years is based on the combination of three techniques: 1. Deterministic parsing algorithms for building dependency graphs [10,18] 2. History-based models for predi cting the next parser action [19] 3. Discriminative classifiers to map histories to parser actions [10,20] A system of this kind employs no grammar but relies completely on inductive learning from treebank data for the analysis of new sentences and on determin-istic parsing for disambiguation. This combination of methods guarantees that the parser is robust, never failing to produce an analysis for an input sentence, and efficient, typically deriving this analysis in time that is linear or quadratic in the length of the sentence.

For the experiments in this paper we use the arc-standard variant of Nivre X  X  parsing algorithm [18,21,22], a linear-time algorithm that derives a labeled de-pendency graph in one left-to-right pass over the input, using a stack to store partially processed tokens in a way similar to a shift-reduce parser.

The features of the history-based model can be defined in terms of different linguistic attributes of the input tokens, in particular the token on top of the stack, which we call the top token , and the first token of the remaining input, called the next token . The top token and the next token are referred to collec-tively as the target tokens , since they are the tokens considered as candidates for a dependency relation by the parsing algorithm. In addition to the target to-kens, features can be based on neighboring tokens, both on the stack and in the remaining input, as well as dependents or heads of these tokens in the partially built dependency graph. The linguistic attributes available for a given token are the following: Lexical form (stem) ( LEX ), Part-of-speech category ( POS ), Inflectional features ( INF ), Dependency type ( DEP ).

To predict parser actions from histories, represented as feature vectors, we use support vector machines (SVM), which combine the maximum margin strategy introduced by Vapnik [23] with the use of kernel functions to map the original feature space to a higher-dimensional space. This type of classifier has been used successfully in deterministic parsing by Kudo and Matsumoto [10], Yamada and Matsumoto [11], and Sagae and Lavie [24], among others. To be more specific, we use the LIBSVM library for SVM learning [25], with a polynomial kernel of degree 2, with binarization of symbolic features, and with the one-versus-one strategy for multi-class classification. The Turkish Treebank [16], created by METU and Sabanc X  University, has been used in the experiments. This treebank comprises 5635 sentences with gold stan-dard morphological annotations and labeled dependencies between IGs. In the treebank, 7.2% of the sentences contain at least one dependency relation that is non-projective, not counting punctuation that is not connected to a head. 1 Each dependency link in the treebank starts from the final IG of the dependent word and ends in some IG of the head word.

Since the parsing algorithm can only construct projective dependency struc-tures, we only use projective sentences for training but evaluate our models on the entire treebank. 2 More precisely, we use ten-fold cross-validation, where we randomly divide the treebank data into ten equal parts and in each iteration test the parser on one part, using the projective sentences of the remaining nine partsastrainingdata.

The evaluation metrics used are the unlabeled ( AS U ) and labeled ( AS L )at-tachment score, i.e., the proportion of tokens that are attached to the correct head (with the correct label for AS L ). A correct attachment implies that a de-pendent is not only attached to the correct head word but also to the correct IG within the head word. Where relevant, we also report the (unlabeled) word-to-word score ( WW U ), which only measures whether a dependent is connected to (some IG in) the correct head word. Non-final IGs of a word are assumed to link to the next IG, but these links, referred to as InnerWord links, are not considered dependency relations and are excluded in evaluation scores. Results are reported as mean scores of the ten-fold cross-validation, with standard error, complemented if necessary by the mean difference between two models.

We use the following set of features in all the experiments described below:  X  POS of the target tokens  X  POS of the token immediately below the top token in the stack  X  POS of the token immediately after the next token in the remaining input  X  POS of the token immediately after the top token in the original input string  X  DEP of the leftmost dependent of the top token  X  DEP of the rightmost dependent of the top token  X  DEP of the leftmost dependent of the next token This is an unlexicalized feature model, involving only POS and DEP features, but we can get a lexicalized version by adding LEX features for the two target tokens. The value of each LEX feature is the stem of the relevant word or IG, rather than the full form. The reasoning behind this choice, which was corroborated in pre-liminary experiments, is that since the morphological information carried by the suffixes is also represented in the inflectional features, using the stem instead of the word form should not cause any loss of information and avoid data sparseness to a certain extent. The basic model, with and without lexicalization, is used as the starting point for our experiments. Additional features are explained in the respective subsections. An overview of the results can be found in table 1. In this set of experiments, we compare the use of IGs, as opposed to full word forms, as the basic tokens in parsing, which was found to improve parsing ac-curacy in the study of Eryi  X  git and Oflazer[9]. More precisely, we compare three different models:  X  A word-based model, where the smallest units in parsing are words repre- X  An IG-based model, where the smallest units are IGs and and where Inner- X  An IG-based model, where InnerWord relations are processed deterministi-For these models, we use a reduced version of the inflectional features in the treebank, very similar to the reduced tagset used in the parser of Eryi  X  git and Oflazer [9]. For each IG, we use the part-of-speech of each IG and in addition include the case and possessive marker features if the IG is a nominal 3 or an adjective with present/past/future participle 4 . Using this approach, the POS feature of the word  X  X daday X m X  becomes +Noun+Pnon+Loc+Verb .

When lexicalizing the IG-based models, we use the stem for the first IG of a word but a null value ( X   X ) for the remaining IGs of the same word. This representation, which is illustrated in figure 2 for the example given in figure 1, also facilitates the deterministic processing of InnerWord relations in the third model, since any top token can be directly linked to a next token with LEX = X   X , provided that the two tokens are adjacent.

In order to calculate the accuracy for the word-based models, we assume that the dependent is connected to the first IG of the head word. This assumption is based on the observation that in the t reebank, 85.6% of the dependency links land on the first (and possibly the only) IG of the head word, while 14.4% of the dependency links land on an IG other than the first one.

The parsing accuracy obtained with the three models, with and without lex-icalization, is shown in table 1. The results are compatible with the findings of Eryi  X  git and Oflazer [9], despite a different parsing methodology, in that the IG-based models generally give higher parsing accuracy than the word-based model, with an increase of three percentage points for the best models.

However, the results also show that, for the unlexicalized model, it is nec-essary to process InnerWord relations deterministically in order to get the full benefit of IG-based parsing, since the classifiers cannot correctly predict these relations without lexical information. For the lexicalized model, adding deter-ministic InnerWord processing has no impact at all on parsing accuracy, but it reduces training and parsing time by reducing the number of training instances for the SVM classifiers. Instead of taking a subset of the inflectional features and using them together with the main part-of-speech in the POS field, we now explore their use as separate features for the target tokens. From now on, our POS tag set therefore consists only of the main part-of-speech tags found in the treebank.
As shown in earlier examples, the inflectional information available for a given token normally consists of a complex combination of atomic features such as +A3sg , +Pnon and +Loc . Thus, when adding inflectional features to the model, we can either add a single feature for each complex combination, or a single feature for each atomic component. As seen in table 1, both methods improve parsing accuracy by more than one percentage point across all metrics, but splitting features into their atomic components gives a slight advantage over the single feature approach. (The difference is quantitatively small but very the lexicalized models.)
Previous research has shown that using case and possessive features for nom-inals and possessive features for adjectives with participles improves parsing ac-curacy [9]. In order to get a more fine-grained picture of the influence of different inflectional features, we have tested six different sets, where each set includes the previous one and adds some more features. The following list describes each set: 1. No inflectional features at all 2. Case and possessive inflectional features for nominals and adjectives with 3. Set 2 + person/number agreement infl ectional features for nominals and 4. Set 3 + all inflectional features for nominals 5. Set 4 + all inflectional features for verbs 6. Set 5 + all inflectional features The results, shown in figure 3, indicate that the parser does not suffer from sparse data even if we use the full set of inflectional features provided by the treebank. They also confirm the previous finding about the impact of case and possessive features. Besides these, the number/person agreement features avail-able for nominals and verbs are also important inflectional features that give a significant increase in accuracy.
 Throughout the previous sections, we hav e seen that lexicalized models consis-tently give higher parsing accuracy than unlexicalized models. In order to get a more fine-grained view of the role of lexicalization, we have first studied the effect of lexicalizing IGs from individual part-of-speech categories and then from different combinations of them (see figure 4). 5 The results show that only the individual lexicalization of nouns and conjunctions provides a statistically sig-nificant improvement in AS L and AS U , compared to the totally unlexicalized model. Lexicalization of verbs also gives a noticeable increase in the labeled accu-racy even though it is not statistically significant. A further investigation on the minor parts-of-speech of nouns 6 shows that only nouns with the minor part-of-speech  X  X oun X  has this positive effect, whereas the lexicalization of proper nouns does not improve accuracy. It can be seen from the chart of combinations that whereas lexicalization certainly improves parsing accuracy for Turkish, only the lexicalization of conjunctions and nouns has a substantial effect on the success.
Although the effect of lexicalization has been discussed in several studies re-cently [7,4,8], it is usually investigated as an all-or-nothing affair. The results for Turkish clearly show that the effect of lexicalization is not uniform across syn-tactic categories, and that a more fine-grained analysis is necessary to determine in what respects lexicalization may have a positive or negative influence. For the models especially suffering from sparse data, it may even be a better choice to use some kind of limited lexicalization instead of full lexicalization. The results from the previous section suggests that the same is true for morphological information. After combining the results of the previous three sections, we performed a final optimization of the feature model. We found that using minor parts-of-speech instead of main parts-of-speech as the values of POS features and adding one more LEX feature for the token after the next token gave a best overall perfor-parser on two different subsets of the treebank. The first subset, which is used by Eryi  X  git and Oflazer [9] in order to evaluate their parser (giving AS U =73.5  X  1 . 0 and WW U =81.2  X  1 . 0 ), consists of the sentences only containing projective depen-dencies with the heads residing on the right side of the dependents. We obtained by using ten-fold cross-validation. Using the optimized model but omitting all which shows that the improvement in accuracy cannot be attributed to lexical-ization alone. The second subset is the Turkish dataset of the CoNLL-X Shared Task on Multi-lingual Dependency Parsing [26]. We obtained an AS U =75.82 and AS L =65.68 which are the best reported accuracies on this dataset.

When we make a detailed analysis on individual dependency types, we see that the parser cannot find labeled dependencies for the types that have fewer than 100 occurrences in the treebank, with the single exception of RELATIVIZER , which is generally the enclitic  X  X i X , written separately from the word it attaches to. Since this dependency type always occurs with the same particle, there is no sparse data problem. If we exclude the low-frequency types, we can divide the results into three main groups. The first group consists of determiners, particles and noun phrases which have an AS U score over 79% and which are found within the closest distances. The second group mainly contains subjects, objects and different kinds of adjuncts, with a score in the range 55 X 79% and a distance of 1.8 X 4.6 IGs to their head. This is the group where inflectional features are most important for finding the correct dependency. The third group contains distant dependencies with a much lower accuracy. These are generally relations like sentence modifier, vocative, and apposition, which are hard to find for the parser because they cannot be differentiated from other nominals used as subjects, objects or normal modifiers. Another construction that is hard to parse correctly is coordination, which may require a special treatment. Turkish is a language characterized by flexible constituent order and a very rich, agglutinative morphology. In this paper, we have shown that the accu-racy achieved in parsing Turkish with a deterministic data-driven parser can be improved substantially by using inflectional groups as tokens, and by making extensive use of inflectional and lexical information in predicting the next parser action. Combining these techniques leads to the highest reported accuracy for parsing the Turkish Treebank.

However, besides showing that morpholexical information may improve pars-ing accuracy for languages with rich morphology and flexible word order, the experiments also reveal that the impact of both morphological and lexical in-formation is not uniform across different linguistic categories. We believe that a more fine-grained analysis of the kind initiated in this paper may also throw light upon the apparently contradictory results reported in the literature, especially concerning the value of lexicalization for different languages.
 This work is partially supported by a research grant from T  X  UB  X  ITAK (The Sci-entific Technical Research Council of Turkey).
 A wide variety of MT methods are being studied [1, 2, 3], including pattern-based MT difficult to obtain high-quality translations for disparate language groups such as English and Japanese. Statistical MT have been attracting some interest recently [9, 10, 11], but it is not easy to improve the quality of translations. Most practical systems still employ the transfer method , which is based on compositional semantics . A problem from the semantics and is thus liable to lose the meaning of the source text.
Better translation quality can be expected from pattern-based MT where the syntactic structure and semantics are handled together. However, this method requires immense pattern dictionaries which are difficult to develop, and so far this method has only been employed in hybrid systems [12, 13] where small-scale pattern dictionaries for specific fields are used to supplement a conventional transfer method.
Example-based MT has been expected to resolve this problem. This method obtains translations by substituting semantically similar elements in structurally matching translation examples, hence there is no need to prepare a pattern dictionary. However, the substitutable elements depend on translation examples. This made it impossible to judge them at real time. This problem could be addressed by manually tagging each example beforehand, but the resulting method would be just another pattern-based MT.
This problem [14] has been partially resolved by a highly comprehensive valency pattern dictionary called Goi Taikei (A-Japanese-Lexicon) [15]. This dictionary contains 17,000 pattern pairs for the semantic analysis in the Japanese-to-English MT system ALT-J/E [16]. High quality translations with the accuracy of more than 90% has been performed for simple Japanese sentences, but there are still cases where a suitable translated sentence structure cannot necessarily be obtained. A valency pattern expresses the semantic relationship between independent words. The meaning of subordinate words (particles, auxiliary verbs, etc.) is dealt with separately, hence the original meaning is sometimes lost. Addressing this problem requires a mechanism that deals with the meaning of subordinate words within the sentence structure as a whole.
In order to realize such a mechanism, we propose a language model that focuses on the non-compositional expressions, and a method for creating patterns based on this model. This method obtains pattern pairs from parallel corpus by the semi-automatic generalization of compositional constituents. 2.1 Compos i t i onal Const i tuents and Non-compos i t i onal Const i tuents In the framework of expressions that come to mind during the process where a speaker is forming a concept, there are two types of cons tituents to consider. One is those that constituents. And the other is those that do not cause the overall meaning to be lost. The former are referred to as N-constituents (Non-compositional constituents) , and the latter are referred to as C-constituents (Compositional-constituents).
 Definition 1: C-constituents and N-constituents C-constituent is defined as a constituent which is interchangeable with other constituents without changing the meaning of an expression structure. All other constituents are N-constituents .
 Definition 2: C-expressions and N-expression C-expression (Compositional expression) is defined as an expression consisting of C-constituents, and N-expression (Non-compositional expression) is defined as an expression comprising one or more N-constituents.
 constituent can constitute one expression . 
Before applying these definitions to actual linguistic expressions, the meaning of an expression structure is needed to be defined. Although a great deal of research has been made concerning the meaning of linguistic expressions, any statement is nothing more than a symbol as far as processing by a computer is concerned, and hence we just need to express meanings in a system of symbols that is free from semantic inconsistencies. In this study, considering applications to Japanese-to-English MT, the meaning of expression structures is defined in terms of an English expression. 
In Figure 1, the source sentence is a Japanese expression expressing a relationship between two events. The meaning of the expression structure is Immediately after performing one action, somebody performed the other action. This meaning is defined by using the English expression. For the constituent such as she , college) and small local company , there is a domain of substitutable constituents that doesn't change the meaning of the expression structure , therefore these are C-constituents . 2.2 Character i st i cs of C-Constituents From the above definitions, it can be pointed out that a C-constituent possesses the following four important characteristics. From these characteristics, it is possible to obtain important guidelines for pattern-forming. (1) Language pair dependence of C-constituent Since one linguistic expression is used to define the meaning of another, the number and scope of C-constituents depends on the language pair. For languages that belong to the same group, the scope of C-constituents is large, while for disparate language groups it is expected to be smaller, as reflected in the different levels of difficulty of translating between the languages. 2 Finite choice for alternative constituents Although C-constituents can be substituted, that does not mean they can be substituted with anything at all. The range that can be substituted is limited both grammatically and semantically, thus this must be indicated in the pattern as the "domain" of the constituent. 3 C-constituent dependent on constituent selection The scope of constituents is determined arbitrarily. Hence whether a constituent is compositional or non-compositional depends on how the constituent is chosen. Accordingly, to obtain general-purpose patterns, it is better to increase the number of C-constituents. 4 Simultaneity of a C-constituent and an N-expression A so-called C-constituent is only compositional when seen in the context of the entire expression, and itself may actually be a N-expression. 2.3 Language Model According to definition 1, a linguistic expression consists of C-constituents and N-constituents. According to characteristic 3 , if we select a C-constituent from an expression with a meaningful range (e.g., word, phrase or clause , a C-constituent may itself also be an N-expression according to characteristic 4 . Consequently a linguistic expression can generally be expressed with the language model shown in Fig. 2. As this figure shows, when C-constituents are repeatedly extracted from N-expressions, the end result is an N-expressions that contains no C-constituents. Although the resulting N-expression may just be a single word, it could also be an idiomatic phrase that has no substitutable constituents. Thus, in this language model, linguistic expressions can be articulated into one or more N-expressions and zero or more N-constituents.
 2.4 Patterns for N-Express i ons An important aspect of the language model is that the N-expressions that appear at each stage of the articulation are meaningful expression units. In this element decomposition process, loss of the original meaning can be avoided by using a semantic dictionary for N-expressions at each stage. For example, if linguistic expressions are classified into sentences, clauses and phrases, and semantic dictionaries are constructed for N-expressions at each of these levels, then this would constitute the bulk of a mechanism for assimilating the meaning of entire sentences.

It is thought that patterns are a suitable framework for expressing the syntactic structure of N-expressions, because: (a) a N-constituent cannot be substituted with another constituent, thus a literal (b) the order in which C-and N-constituents appear is often fixed, thus there is Therefore, in this study we will use a pattern-forming approach for meaningful N-expressions. According to our language model, three kind of expression patterns compound and complex sentence patterns, simple sentence patterns and phrase patterns will be almost sufficient to cover Japanese expressions.
 In this study, complex and compound sentences were targeted because the Goi Taikei 15 can gives good translations for most of simple sentences. But, complex and compound sentences are very difficult to obtain good translation results by the conventional MT systems. The number of predicates was limited to 2 or 3 because it is thought that complex and compound sentences with four or more predicates can often be interpreted by breaking them down into sentences with three predicates or fewer. 3.1 The Pr i nc i ples of Pattern-Form i ng The Japanese-English parallel corpus is a typical example where the meaning of Japanese expressions is defined with English expressions. And when translation example is considered, the following two types of C-constituents can occur: (1) cases where there is a constituent in the English expression that corresponds to a (2) cases where a constituent in the Japanese expression has no corresponding SP pairs were therefore produced by extracting components corresponding to these two cases from parallel corpus, and generalizing the results. 3.2 SP Generat i on Procedure First, a parallel corpus was created by collecting together a sentence pair of 1 million basic Japanese sentences. From this corpus, 150,000 translation examples for compound and complex sentences with two or three predicates were extracted. Then, correspondence relationships between the constituents were extracted and converted SP-dictionary. For C-constituents that can be semi-automatically recognized as such, the generalization is also performed semi-automatically. 3.3 Examples of SPs An example of a SP is shown in Table 1. The meanings of the variables , functions, etc. used in this table are shown below.
 Word-level SPs : N1, N3, N4: Noun variables. V2, V5: Verb variables. Here, attached bracket represents semantic attribute numbers specifying semantic constraints on a variable. #1[...]: Omissible constituents. /: Place of a constituent that need not appear. .tekita : Function for specifying a predicate suffix. AJ(V2): Adjectival form of the value of verb variable V2. N1 poss: Value of N1 transformed into possessive case.
 Phrase-level SPs : NP1: Noun phrase variable.
 Clause-level SPs : CL1: Clause variable. so+that (..., ...): A sentence generation function for so that sentence structure. subj(CL): Function that extracts the subject from the value of a clause variable. 3.4 The Number of D i fferent SPs Table 2 shows the number of SPs in the resulting SP-dictionary and the number of constituents replaced by variables at each level of generalization.

In Table 2, compared to the number of SPs of word-level and phrase-level SPs, the number of clause level SPs was particularly small. This indicates that most of the clauses in the parallel corpus are N-constituents which are impossible to generalize. The proportion of generalized C-constituents were 62% at the word level and 22% at the phrase level , but just 4.3% at the clause level .

For N-constituents, a semantically suitable translated result cannot be obtained when the constituent is extracted, translated and incorporated into the original sentence. Looking at the parallel corpus, most of the English translations of Japanese compound and complex sentences are simple sentences whose structures are very diverse. Regarding the results of Table 2, in the case of Japanese-to-English MT, high-quality translations cannot be achieved by conventional MT method based on compositional semantics . 4.1 Exper i mental Cond i t i ons A pattern parser that compares input sentences against the SP-dictionary was used to evaluate the coverage of the SP-dictionary. The experiments were conducted by cross-validation manner and ten thousand input sentences were used. These were randomly selected from the example sentences used for creating the SPs. Since the input sentences will always match the SPs from which they were created, matches of this type were ignored and the evaluation was restricted matches to other SPs.
An input sentence many times matches to more than one SP and not all of them are necessarily correct. Therefore, the coverage was evaluated according to the following four parameters: 4.2 Saturat i on of Matched Pattern Ratio Fig. 3 shows the relationship between the number of SPs and the matched pattern ratio . As you can see, there is a pronounced tendency for the matched pattern ratio to become saturated. When the SPs on the horizontal axis are rearranged in order of their frequency of appearance, the rate of saturation becomes about 5 times faster. 
According to the previous study [17], the number of valency patterns required to more or less completely cover all simple sentences was estimated to be somewhere in the tens of thousands. We can say that the number of required SPs for complex and compound sentences is also expected to converge somewhere in the tens of thousands or thereabouts. 4.3 Matched Pattern Ratio and Precision Table 3 shows the evaluation results. It was shown that 91.8% of the input sentences are covered syntactically by the whole dictionary. However, there were also many cases of matches to semantically inappropriate SPs, and the semantic coverage decreased to 70% when these were eliminated. The number of clause-level SPs was just one tenth the number of word-level SPs , but had comparatively high coverage.
 4.4 Semantic Coverage Since semantic ambiguity is small in the order of word-level, phrase-level and clause-level SPs, it is probably better to select and use the most semantically appropriate SP based on this sequence. Fig. 4 shows the ratio of SPs that are used when they are selected based on this sequence.

As Fig. 4 shows, about 3/4 of the meanings of Japanese compound and complex sentences are covered by the SP-dictionary. When MT is performed using the complex and compound sentences, while phrase-level and clause-level SPs will be applied to the other half. An Non-compositional language model was proposed and, based on this model, a sentence pattern dictionary was developed for Japanese compound and complex sentences. This dictionary contains 123,000 word-level , 80,000 phrase-level and 12,000 clause-level sentence pattern pairs (215,000 in total).

According to the results, the compositional constituents that could be generalized were 62% for independent words, 22% for phrases, whereas only 4.3% for clauses. This result shows that in Japanese-to-English MT hardly any Japanese compound and complex sentences can be translated into English as shown in a parallel corpus when they are translated by separating them into multiple simple sentences and then recombined. 
Also, in evaluation tests of a SP-dictionary, the syntactic coverage was found to be SP-dictionary is very promising for Japanese to English MT.
 This study was performed with the support of the Core Research for Evolutional Science and Technology (CREST) program of the Japan Science and Technology Agency (JST). Our sincere thanks go out to everyone concerned and to all the research group members.
 Anaphoric relations include using noun phrases, pronouns, or zero anaphors to stand for previously mentioned nominal referents [1,2]. Therefore, anaphora resolution has become an indispensable part in message understanding as well as knowledge acquisition.

In the past literature, different strategies to identify antecedents of an anaphor have been presented by using syntactic, semantic and pragmatic clues [3,4,5]. In addition, a corpus-based approach is proposed by Dagan and Itai [6]. However, a large corpus is needed for acquiring sufficient co-occurring patterns and for dealing with data sparseness. Recently, outer resources like WordNet are applied for enhancing the semantic verification between anaphors and antecedents [7,8]. Nevertheless, using WordNet alone for acquiring semantic information is not sufficient for solving unknown words. To tackle this problem, a richer resource from the Web was exploited [9]. Anaphoric information is mined from Google search results at the expense of less precision.

Contrast to rich studies in English texts, efficient Chinese anaphora resolu-tion has not been widely addressed. Recently, Wang et al. [10] presented an event-based partial parser and an efficient event-based reasoning mechanism to tackle anaphora and ellipsis appearing in primary school X  X  mathematical problem description sentences. On the other hand, Wang et al. [11] presented a simple rule-based approach to handle nominal and pronominal anaphora in financial texts. However, any rule-based approach is essentially lacks of portability to other domains. Yeh and Chen [12] resolved zero anaphora by using centering theory and constraint rules. Though they had 65% F-score, yet they tackle the inter-sentential zero anaphors only. C ontrast to the shallow parsing used by Yeh and Chen [12], Converse [13] used full parsing results from Penn Chinese Treebank for pronominal and zero anaphora resolution. Since less features were used at resolving intra-sentential zero a naphora, the overal l resolution is not promising.

Xu [2] reported that zero anaphora is the most common phenomenon than pronominal and nominal anaphora in Chinese written texts. The zero anaphors can be in single sentence or in consecutive sentences. In this paper, the presented resolution is aimed to tackle both inter and intra sentential zero anaphora by exploiting more semantic information with the help of outer knowledge resources. The animate relation between nouns and verbs is acquired from the resources to improve the presented anaphora resolution. The kernel resolution module is embedded with a case-based reasoning (CBR) mechanism for its benefits in new knowledge acquisition [14]. Essentially such incremental learning approach is able to achieve optimal performance. In our design, the discovery of similar cases allows us to identify reference patterns and select possible antecedents. The experimental results on 1047 instances show that the presented approach can yield promising zero anaphora resolution in terms of 82% recall and 77% precision.

The remaining part of this paper is organized as follows. Section 2 intro-duces the Chinese sentence structures and zero anaphora. Section 3 describes the overall procedure of the proposed resolution in details. Section 4 gives the experimental results and analysis from different aspects. Section 5 is the final conclusions. In the survey of Xu [2], there are five basic types of simple sentence structures in Chinese. A simple sentence is defined to be a sentence bounded by punctua-tion marks like  X  , , , , ,  X  [15,16]. Several consecutive simple sentences form a complex sentence. In the following examples, we describe the sentence structures and the zero anaphors denoted by  X   X . The intra-sentential cases are in Examples 2 and 3; inter-sentential cases are in other examples of complex sentences.
 Example 1. SVO sentence: It can be  X  subject + transitive-verb + object  X  and  X  subject + intransitive-verb X . Zero anaphors may occur in the position of subject or object .
 Example 2. Serial verb sentence: A serial verb construction contains several verbs in a simple sentence, expressing simultaneous or consecutive actions. All verbs in the sentence have the same grammatical subject.
 Example 3. Pivotal sentence: A sentence is called a pivotal sentence if its pred-icate consists of two verb phrases and the object of the first verb is functioned as the subject of the second verb.
 Example 4. Individual noun phrase sentence: The individual noun phrase is likely to be the antecedent of the succeeding zero anaphor.
 Example 5. Prepositional phrase sentences: When a prepositional phrase forms a simple sentence, a zero anaphor is likely to occur in front of the preposition. Figure 1 illustrates the proposed resolution procedure including text preprocess-ing, zero anaphor (ZA) detection and antecedent (ANT) identification process, a case-based reasoning module (CBRM), a case base, and two lexicon resources. Each of these modules is described in the following subsections. 3.1 Text Preprocessing The corpus we used in both training and testing phases is the well-known Acad-emia Sinica Balancing Corpus (ASBC) [17]. Every text in the corpus is segmented into sentences, and every word is tagged with its part-of-speech. There are 46 kinds of POS tags used in ASBC Corpus. Using POS features, we construct a finite state machine chunker to chunk each base noun phrase as antecedent candidates.
 In the design of the finite state machine, each state indicates a particular POS of a word. The arcs between states mean a word input from the sentence sequentially. If a word sequence can be recognized from the initial state and ends in a final state, it is accepted as a base noun phrase with no recursion, otherwise rejected.

In addition, unlike western languages, Chinese grammar has little inflection information. Chinese verbs appear in the same form no matter whether they are used as nouns, adjectives, or adverbs [18]. The following examples show the usage for each of four cases by the word  X   X  ( X  X elax X ).
 Example 6. Verb case: Example 7. Noun case: Example 8. Adjective case: Example 9. Adverb case:
Therefore, verbs as described in Example 7 are treated as nouns while perform-ing chunking phase. Moreover, verbs in adjective cases are regarded as modifiers of noun phrases. 3.2 ZA Detection and ANT Identification Zero anaphora resolution involves zero anaphor detection and antecedent iden-tification. In ZA detection phase, verbs are examined to assign corresponding subjects and objects. If there is any omission, the ZA detection will submit the sentence to CBRM to decide whether there is a ZA or not. If yes, the ANT iden-tification phase will be performed by using resolution template returned from CBRM.

In ZA detection phase, it must be noted that there are three conditions should be ignored while detecting ZA around verbs [19]. The conditions are described in the following examples. For instance, the object of verb  X   X  (finish) in Example 10 is shifted to the position after the word  X   X (Ba).
 Example 10.  X   X  (Ba) sentence: Example 11.  X   X  (Bei) sentence: Example 12. In adverb case: when the verb functions as a part of adverb as described in section 3.1, it would not be the related verb to a ZA. 3.3 Case-Based Reasoning Module Case-based reasoning has been successful in the applications like legal case re-trieval. It is an incremental and sustained learning since a new experience is retained each time a problem has been solved. In the application of the pre-sented ZA resolution, a set of similar sentences measured by a given similarity function will be retrieved from the pre-constructed case base to detect whether an input sentence has a ZA or not. The most similar example will be reused if it is a ZA example. If necessary, this retrieved example will be revised to be a final version. Then the new version, its context and resolution will be retained in the case base. Figure 2 illustrates the procedure of the presented CBRM which is in charge of the communication between the ZA resolution and the outer resources. The module functions are summarized as follows: 1. Retrieve the most similar example w.r.t. the input sentence. 2. Reuse the resolution method of the most similar example on the basis of 3. Revise the resolution process manually if there is any error. 4. Retain the refined example into case base.
 Lexicon Resources. As mentioned above, two outer resources are used during ZA resolution to acquire informative features like animacy attribute of nouns and verbs. The resources are CKIP lexicon [20] and The Academia Sinica Bilin-gual WordNet (SinicaBOW), both of them were released from The Association for Computational Linguistics and Chinese Language Processing. CKIP lexicon contains 80,000 entries annotated with syntactic categories and semantic roles. For example,  X   X  ( X  X riter X ),  X   X ( X  X ommander X ), X   X ( X  X ustomer X ) are regarded as animate nouns. SinicaBOW is a Mandarin-English bilingual data-base based on the frame of English WordNet and language usage in Taiwan. There are four kinds of verbs, as shown in Table 1, as animate verbs, namely, { cognition } , { communication } , { emotion } ,and { social } . Case Representation and the Case Base. In this paper, each case in the case base is represented in Tables 2 and 3 at different phases. At training phase, the case base contains 2738 examples which were collected from ASBC corpus and annotated with ZA marker (denoted as  X   X ) by human experts. Table 2 shows an input case at representation level and its ZA template which contains six features and sentence pattern described in Table 4. Table 3 shows one case stored in the case base which contains both ZA template and ANT template used as ZA resolution method. Equation (1) is the presented similarity func-tion used to compute the similarity between the input case and the stored case examples. The similarity computation concerns the similarity of ZA template features and sentence pattern. By discovering examples analogous to the form of a problem, we can immediately use examples to solve the problem. There-fore, the ANT template with the highest similarity value will be retrieved from the case base and be used to identify the antecedent with respect to a given input sentence. For example, an omission occurs before the verb  X   X (an-nounce) in Table 2. We extract the example as shown in Table 3 to infer the corresponding antecedent. Due to the number of matched features in the ANT template, we can decide that  X   X  (chairman) should be the antecedent of in Table 2.
 S : input sentence S
C :casesentence  X  , X  : weighting factors, where  X  +  X  =1 MATCH ( S I ,S C ): number of matched features in case S I and S C PATTERN ( S I ,S C ): the value of PATTERN in case S I and S C The presented resolution is justified with narrative report texts selected from the ASBC corpus. Table 5 lists the statistical data of both training and testing corpus. Table 6 shows the performance results in terms of precision and recall at various matching thresholds. It is observed that the optimal performance (in terms of F-score) is achieved when  X  and  X  value are 0.7 and 0.3 respectively.
In order to verify feature impact, a baseline model is built in such a way that only grammatical role feature is used in ANT identification. Figure 3 shows that the highest F-score is obtained when all the ZA template features indi-cated as  X  X LL X  are concerned and the baseline yields the worst performance at comparison. It is also noticed that the resolution performance can be enhanced significantly after applying animate feature (denoted as  X  X  X ) and sentence pat-tern mapping (denoted as  X  X  X ). On the other hand, we also verify the sensitivity of the training case size in our presented CBR approach to ZA resolution. It is found from Figure 4 that the feasible performance results can be obtained when training corpus size is close to the testing one. If the training case size is half of the testing case size, the performance may drop 20%.

While analyzing the ZA resolution, we classified three main sources of errors as follows: (1) Preprocessing error: there are 6% ZA errors attributed to base noun phrase chunker. (2) Animacy ambiguity: there are some verbs that can be both animate and inanimate such as  X   X  (make), making the ZA resolution a wrong animate feature selection. (3) Inappropriate template: there exists some exception sentence structure given by the authors, making the ZA identification unresolvable. There are 14% errors attributed to the inapplicability of the stored template method. In this paper, we presented a case-based reasoning approach to Chinese zero anaphora resolution. Compared to other rule-based resolution methods , the presented approach turns out to be promising to deal with both intra-sentential and inter-sentential zero anaphora. In addition, we introduced two new features, namely animate relation acquired from outer resources and sentence patterns, in both ZA detection and ANT identification. Experimental results show that they can contribute 9% improvement to overall resolution performance. The drawback with this approach is the construction of the case base in advance. However, our experimental analysis shows that feasi ble performance results can be obtained when training corpus size is close to the testing one.
With the growing interest in natural language processing and its various ap-plications, anaphora resolution is worth considering for further message under-standing and the consistency of discourses. Our future work will be directed into the following studies: (1) Modifying similarity function: Semantic classes of nouns may be employed to enhance the effectiveness of comparing sentence pattern similarity. (2) Extending the set of anaphor being processed: This analysis aims at identi-fying instances (such as definite and pronominal anaphor) that could be useful in anaphora resolution. (3) Exploiting web resource: The web resource can be utilized to identify sentence patterns and other useful features such as gender and number of entities. In any natural language, there always exist many highly associated relationships between words, e.g. "strong tea" and "powerful computer". Although  X  X trong X  and  X  X owerful X  have similar syntax and semantics, there exist contexts where one is much more appropriate than the other (Halliday 1966). For example, we always say  X  X trong tea X  instead of  X  X trong computer X  and  X  X owerful computer X  instead of  X  X owerful tea X . Psychological experiments in Meyer et al (1975) also indicated that human X  X  reaction to a highly associated word pair was stronger and faster than that to a poorly associated one. Lexicographers use the terms  X  X ollocation X  and  X  X o-occurrence X  to narrower sense between grammatically bound words, e.g.  X  X trong X  and  X  X ea X , which occur in a particular grammatical order, and  X  X o-occurrence X  for the more general phenomenon of relationships between words, e.g.  X  X octor X  and  X  X urse X , which are likely to be used in the same context (Manning et al 1999). This paper will concentrate on  X  X ollocation X  rather than  X  X o-occurrence X  although there is much overlap between these two terms. 
Collocations are important for a number of applications such as natural language generation, computational lexicography, parsing, proper noun discovery, corpus linguistic research, machine translation, information retrieval/extraction, etc. As an example, Hindle et al (1993) showed how collocation statistics can be used to improve the performance of a parser where lexical preferences are crucial to resolving the ambiguity of prepositional phrase attachment. 
Currently, there are two categories of approaches used to discover collocations and co-occurrences: statistics-based approaches and parsing-based approaches. On the one hand, the statistics-based approaches are widely used to extract co-occurrences, where two words are likely to co-occur in the same context, from a large raw corpus, by Kupiec et al 1995; Zhao et al 1999), mean and variance (Smadja 1993), t-test (Church et al 1989; Church et al 1993), chi-square test (Church et al 1991; Snedecor et al 1989), likelihood ratio (Dunning 1993) and mutual information (Rosenfeld 1994; Denniz 1998; Zhou et al 1998; Zhou et al 1999). On the other hand, the parsing-based approaches rely on linguistic analysis and extract collocations, which differentiate between different types of linguistic relations, from the parsed trees of a large corpus. Normally such methods are combined with the frequency-based method to reject the ones whose frequencies are below a predefined threshold (Yang 1999). 
Generally, both the statistics and parsing-based approaches are only effective on frequently occurring words and not effective on less frequently occurring words due to the data sparseness problem. Moreover, the extracted collocations or co-occurrences are always stored in a dictionary, which only contains a limited number of entries with limited information for each one. Finally, the collocation dictionary normally does not differentiate the strength degree among various collocations. 
This paper combines the parsing-based approach and the statistics-based approach, and proposes a novel structure of collocation net. Through the collocation net, the data sparseness problem is resolved by prov iding a clustering mechanism and the collocation relationship between any two words can be easily determined and measured from the collocation net. Here, the collocation relationship is calculated using novel estimated pair-wise mutual information (EPMI) and estimated average mutual information (EAMI). Moreover, all the information extracted from the linguistic analysis is kept in the collocation net. Compared with the traditional collocation dictionary, the collocation net provides much more powerful facility since it can determine and measure the collocation relationship between any two words quantitatively. 
The layout of this paper is as follows: Section 2 describes the novel structure of collocation net. Section 3 describes estimated pair-wise mutual information (EPMI) and estimated average mutual information (EAMI) to determine and measure the collocation relationship between any two words while Section 4 presents a method for automatically building a collocation net given a large law corpus. Experimentation is given in Section 5. Finally, some conclusions are drawn in Section 6. The collocation net is a kind of two-level structure, which stores rich information about the collocation candidates and others extracted from the linguistic analysis of a large raw corpus. The first level consists of word and feature bigrams 1 while the second level consists of classes that are clustered from the word and feature bigrams contains only one word and feature bigram while each second level class contains one or more word and feature bigrams clustered from first level atomic classes. 
Meanwhile, each class in both levels of the collocation net is represented by its related collocation candidate distribution, extracted from the linguistic analysis. In and a collocation relation type, which represents the collocation relationship between the left side and the right side. Both the left and right sides can be either a word and feature bigram or a class of word and feature bigrams. For example, a collocation feature bigram; hi C is the i -th class in the h -th level and k CR is a relation type. 
Briefly, the collocation net is defined as follows:  X  wf stores possible word and feature bigrams  X  CR stores possible collocation relation types  X  1 L and 2 L are the first and second levels in the collocation net, respectively;  X  measurements to calculate the collocation relationship between the two word and wise mutual information (EPMI) and estimated average mutual information (EAMI). Moreover, we also extend the EPMI and EAMI to determine and measure the collocation relationship between any two words. In this way, we can not only determine the most possible collocation relationship between any two words but also measure the strength of the collocation relationship between them. 3.1 EAMI : Est i mated Average Mutual Informat i on Traditionally in information theory, average mutual information (AMI) measures the co-occurrence relationship between two words as follows: measure the average mutual information between the two word and feature bigrams wf and j wf given the collocation relation type k CR as follows: Here, we use  X * X  to indicate all the possibilities on the corresponding part. The problem with the above equation is that it only works on frequently occurring word and feature bigrams and is not reliable on less-frequently occurring word and feature bigrams (e.g. frequency &lt; 100). In order to resolve this problem, we propose a modified version of AMI, called estimated average mutual information (EAMI), to measure the collocation relationship of a collocation candidate when one or two word and feature bigrams do not occur frequently. This is done by finding two optimal classes in the collocation net and mapping the less-frequently occurring word and feature bigrams to them through the word-clustering mechanism provided in the collocation net as follows: to itself when the word and feature bigram occurs frequently or mapped to any class in 2 L when the word and feature bigram does not occur frequently. 3.2 EPMI : Est i mated Pa i r-W i se Mutual Informat i on Similarly in information theory, pair-wise mutual information (PMI) measures the change of information between two words as follows: to measure the PMI of the collocation candidate as follows: Similar to AMI, the problem with the above equation is that it only works on frequently occurring word and feature bigrams. In order to resolve this problem, we also propose a modified version of PMI, called estimated pair-wise mutual information (EPMI), to calculate the information change of a collocation candidate when one or two word and feature bigrams does not occur frequently. This is done by using the two optimal classes found in calculating EAMI as follows: Equation (10) measures the pair-wise mutual information using the collocation candidate between the two optimal classes and takes the class transitions hm i C C  X  1 quantitative measurement for a collocation candidate but also as a selection criteria to determine the two optimal classes in calculating EPMI since EAMI takes the joint probability into consideration, while EPMI is used to measure the strength degree of a collocation candidate. For example, parse tree re-ranking can be performed by considering EPMI of the included collocation candidates in parse trees. 3.3 Collocat i on Relat i onsh i p Between Any Two Words Given any two words i w and j w , the EPMI and EAMI between them are defined as the EPMI and EAMI of the optimal collocation candidate related with the two words. Here, the optimal collocation candidate is determined by maximizing the EPMI among all the related collocation candidates over all possible word and feature bigrams and all the possible collocation relation types: Here in Equation (12), the collocation candidate j k i wf CR wf  X   X  is determined through maximizing ) , ( j i w w EPMI in Equation (11). built iteratively as follows: 2) Build the collocation net based on the extracted collocation candidates: Given 3) Examine whether the collocation net is to be re-built. For example, whether the The experimentation has been done on the Reuters corpus, which contains 21578 news documents of 2.7 million words in the XML format. In this paper, the Collins X  parser is applied and all the collocations are extracted between the head and one modifier of a phrase. In our experimentation, only six most frequently occurring collocation relation types are considered. Table 1 shows them with their occurrence frequencies in the Reuters corpus. Collocation Relation Type Remark Freq VERB-SUB The right noun is the subject of the left verb 37547 VERB-OBJ The right noun is the object of the left verb. 59124 VERB-PREP The right preposition modifies the left verb 80493 NOUN-PREP The right preposition modifies the left noun 19808 NOUN-NOUN The right noun modifies the left noun 109795 NOUN-ADJ The right adjective modifies the left noun 139712 
To demonstrate the performance of the collocation net, the N-best collocations are extracted from the collocation net. This can be easily done through computing the EAMI and EPMI of all the collocation candidates extracted from the corpus, as described in Section 3. Then all the collocation candidates whose EPMIs are larger than a threshold (e.g. 0) are kept as collocations and sorted according to their EPMIs. As a result, 31965 collocations are extracted from the Reuters corpus. Table 2 gives some of examples. It shows that our method can not only extract the collocations that the corpus. Another advantage is that our method can determine the collocation relationship between any two words and measure its strength degree. In this way, our method can even extract collocations that never occur in the corpus. Table 3 gives some of them. For example, the collocation candidate NOUN(abatement)_NOUN-ADJ_ADJ(eligible) can be measured as a collocation with EAMI of 1.01517e-05 and EPMI of 1.174579 although this collocation candidate doesn X  X  exist in the corpus. The main reason is that the collocation net provides a word-clustering mechanism to resolve the problem of data sparseness. This is done by using the word-clustering mechanism in the collocation net as shown in Section 3. Table 4 shows an example class  X  X inance/tax X  in the second level of the collocation net. Left Side Collocation NOUN(accountan) NOUN-ADJ ADJ(associate) 3.22 8.68e-06 NOUN(business) NOUN-NOUN NOUN(customer) 1.22 9.66e-05 NOUN(abatement) NOUN-ADJ ADJ(eligible) 1.17 1.02e-05 VERB(transfer) VERB-SUB NOUN(business) 1.06 5.18e-05 NOUN(asbestos) NOUN(abatement) NOUN(abba) NOUN(market) NOUN(share) NOUN(stock) NOUN(tax) NOUN(currency) NOUN(contract) NOUN(income) 
NOUN(trade) ...... 
In order to further evaluate the usefulness of the collocation net, we have used it in parse tree re-ranking using the standard PARSEVAL metrics. Here, Collins X  parser is sentence are considered in re-ranking. This is done by building a collocation net on the golden parse trees in the training data and adjusting the probability of each parse tree candidate using the collocation net to achieve parse tree re-ranking. For each parse tree candidate, e.g. ij T with the original probability parse tree candidate can be adjusted by considering the contribution of its included collocation candidates: change of information when the collocation candidate i CC is collocated. Then, all the parse tree candidates for a sentence can be re-ranked according to their adjusted probabilities as calculated in Formula (2). 
Table 5 shows the effect of parse tree re-ranking using the collocation net. It shows that the use of the collocation net can increase the F-measure by 1.6 in F-measure. This paper proposes a novel structure of two-level collocation net and a method capable of automatically building the co llocation net given a large raw corpus. Through the collection net, the collocation relationship between any two words can be calculated quantitatively using novel estimated average mutual information (EAMI) as the selection criterion and estimated pair-wise mutual information (EPMI) as the strength degree. Obviously, the two-level collocation net can be easily extended to more levels through cascading such a two-level structure. 
Future works include systematic evaluation of the collocation net on a much larger corpus, its application to other languages su ch as Chinese and in a general-purpose parser for adaptation to a new domain/application, and development of a more-level collocation net. 
 Series Editors Randy Goebel, University of Alberta, Edmonton, Canada J X rg Siekmann, University of Saarland, Saarbr X cken, Germany Wolfgang Wahlster, DFKI and University of Saarland, Saarbr X cken, Germany Volume Editors Wenjie Li The Hong Kong Polytechnic University Hung Hom, Kowloon, Hong Kong E-mail: cswjli@comp.polyu.edu.hk Diego Moll X -Aliod Macquarie University Sydney NSW 2109, Australia E-mail: diego@ics.mq.edu.au Library of Congress Control Number: Applied for CR Subject Classification (1998): I.2.6-7, F.4.2-3, I.2, H.3, I.7, I.5 LNCS Sublibrary: SL 7  X  Artificial Intelligence ISSN 0302-9743 ISBN-10 3-642-00830-5 Springer Berlin Heidelberg New York ISBN-13 978-3-642-00830-6 Springer Berlin Heidelberg New York The International Conference on the C omputer Processing of Oriental Lan-guages (ICCPOL) series is hosted by the Chinese and Oriental Languages Society (COLCS), an international society founded in 1975. Recent ICCPOL events have been held in Hong Kong (1997), Tokushima, Japan (1999), Seoul, Korea (2001), Shenyang, China (2003) and Singapore (2006).
 on the Computer Processing of Oriental Languages (ICCPOL 2009) held in Hong Kong, March 26-27, 2009. We received 63 s ubmissions and all the papers went through a blind review process by mem bers of the Program Committee. After careful discussion, 25 of them were sel ected for oral presentation and 15 for poster presentation. The accepted paper s covered a variety of topics in natural language processing and its applications, including word segmentation, phrase and term extraction, chunking and parsing, semantic labelling, opinion mining, ontology construction, machine translation, information extraction, document summarization and so on.
 of submitted papers for their support. We wish to extend our appreciation to the Program Committee members and additional external reviewers for their tremendous effort and excellent review s. We gratefully acknowledge the Orga-nizing Committee and Publication Committee members for their generous con-tribution to the success of t he conference. We also than k the Asian Federation of Natural Language Processing (AFNLP), the Department of Computing, The Hong Kong Polytechnic University, Hong Kong, the Department of Systems En-gineering and Engineering Management, The Chinese University of Hong Kong, Hong Kong, and the Centre for Language Technology, Macquarie University, Australia for their valuable support.
 January 2009 Wenjie Li ICCPOL 2009 was organized by the The Hong Kong Polytechnic University, The Chinese University of Hong Kong and City University of Hong Kong Bonnie Dorr University of Maryland, USA (2008 President, Jong Hyeok Lee POSTECH, Ko rea (President, COLCS) Elizabeth D. Liddy Syracuse University, USA (Chair, ACM SIGIR) Jun-Ichi Tsujii University of Tokyo Japan (President, AFNLP) Qin Lu The Hong Kong Polytechnic University, Robert Dale Macquarie University, Australia Wenjie Li The Hong Kong Polytechnic University, Diego Molla Macquarie University, Australia Grace Ngai The Hong Kong Polytechnic University, Gaoying Cui The Hong Kong Polytechnic University, You Ouyang The Hong Kong Polytechnic University, Chunyu Kit City University of Hong Kong, Hong Kong Ruifeng Xu City University of Hong Kong, Hong Kong Tim Baldwin University of Melbourne, Australia Sivaji Bandyopadhyay Jadavpur University, India Hsin-Hsi Chen National Taiwan University, Taiwan Keh-Jiann Chen Academia Sinica, Taiwan Key-Sun Choi Korea Advanced Institute of Science and Guohong Fu Heilongjiang University, China Choochart Haruechaiyasak National Electronics and Computer Technology Xuanjing Huang Fudan University, China Kentaro Inui Nara Institute of Science and Technology, Seung-Shik Kang Kookmin University, Korea Chunyu Kit City University of Hong Kong, Hong Kong Olivia Oi Yee Kwong City University of Hong Kong, Hong Kong Sobha L. Anna University -KBC, India Wai Lam The Chinese University of Hong Kong, Jong-Hyeok Lee Pohang University of Science and Technology, Korea Sujian Li Peking University, China Haizhou Li Institute for Infocomm Research, Singapore Wenjie Li The Hong Kong Polytechnic University, Ting Liu Harbin Institute of Technology, China Qun Liu Chinese Academy of Sciences, China Qing Ma Ryukoku University, Japan Diego Molla Macquarie University, Australia Masaaki Nagata NTT Communication Science Laboratories, Manabu Okumura Tokyo Institute of Technology, Japan Jong Cheol Park Korea Advan ced Institute of Science and Sudeshna Sarkar Indian Institute of Technology Kharagpur, Yohei Seki Toyohashi University of Technology, Japan Jian Su Institute for Infocomm Research, Singapore Zhifang Sui Peking University, China Le Sun Chinese Academy of Sciences, China Bin Sun Peking University, China Thanaruk Theeramunkong Sirindhorn International Institute of Takehito Utsuro University of Tsukuba, Japan Vasudev Varma International Institute of Information Yunqing Xia Tsinghua University, China Ruifeng Xu City University of Hong Kong Min Zhang Institute for Infocomm Research, Singapore Tiejun Zhao Harbin Institute of Technology, China Hai Zhao City University of Hong Kong, Hong Kong Guodong Zhou Suzhou University, China Jingbo Zhu Northeastern University, China Chinese and Oriental Languages Computer Society (COLCS) Asian Federation of Natural Language Processing (AFNLP) Department of Computing, The Hong Kong Polytechnic University Department of Systems Engineering and Engineering Management, The Chinese University of Hong Kong Centre for Language Technology, Macquarie University, Australia A Density-Based Re-ranking Technique for Active Learning for Data Annotations ..................................................... 1 CRF Models for Tamil Part of Speech Tagging and Chunking .......... 11 A Probabilistic Graphical Model for Recognizing NP Chunks in Texts ........................................................... 23 Processing of Korean Natural Language Queries Using Local Grammars ...................................................... 34 Improving the Performance of a NER System by Post-processing, Context Patterns and Voting ...................................... 45 Research on Automatic Chinese Multi-word Term Extraction Based on Term Comp onent ................................................ 57 A Novel Method of Automobiles X  Chinese Nickname Recognition ....... 68 Fast Semantic Role Labeling for Chinese Based on Semantic Chunking ....................................................... 79 Validity of an Automatic Evaluation of Machine Translation Using a Word-Alignment-Based Classifier .................................. 91 Lexicalized Syntactic Reorderin g Framework for Word Alignment and Machine Translation .............................................. 103 Found in Translation: Conveying Subjectivity of a Lexicon of One Language into Another Using a Bilingual Dictionary and a Link Analysis Algorithm ............................................... 112 Transliteration Based Text Input Methods for Telugu ................. 122 Harvesting Regional Transliteration Variants with Guided Search ...... 133 A Simple and Efficient Model Pruning Method for Conditional Random Fields .......................................................... 145 Query-Oriented Summarization Based on Neighborhood Graph Model .......................................................... 156 An Extractive Text Summarizer Based on Significant Words ........... 168 Using Proximity in Query Focused Multi-document Extractive Summarization .................................................. 179 Learning Similarity Functions in Graph-Based Document Summarization .................................................. 189 Extracting Domain-Dependent Semantic Orientations of Latent Variables for Sentiment Classification ............................... 201 Mining Cross-Lingual/Cross-Cultural Differences in Concerns and Opinions in Blogs ................................................ 213 Partially Supervised Phrase-Level Sentiment Classification ............ 225 A Novel Composite Kernel Approach to Chinese Entity Relation Extraction ...................................................... 236 Automatic Acquisition of Attributes for Ontology Construction ........ 248 Speech Synthesis for Error Training Models in CALL ................. 260 Probabilistic Methods for a Japanese Syllable Cipher ................. 270 Dialogue Strategies to Overcome Speech Recognition Errors in Form-Filling Dialogue ............................................ 282 Research on Domain Term Extraction Based on Conditional Random Fields .......................................................... 290 PKUNEI  X  A Knowledge X  X ased Approach for Chinese Product Named Entity Semantic Identification ..................................... 297 Experiment Research on Feature Selection and Learning Method in Keyphrase Extraction ............................................ 305 Korean-Chinese Machine Translation Using Three-Stage Verb Pattern Matching ....................................................... 313 Domain Adaptation for English X  X orean MT System: From Patent Domain to IT Web News Domain .................................. 321 Constructing Parallel Corpus from Movie Subtitles ................... 329 Meta-evaluation of Machine Translation Using Parallel Legal Texts ..... 337 Flattened Syntactical Phrase-Based Translation Model for SMT ....... 345 An Integrated Approach for Concept Learning and Relation Extraction ...................................................... 354 Text Editing for Lecture S peech Archiving on the Web ............... 362 Document Clustering Description Extraction and Its Application ....... 370 Event-Based Summarization Usin g Critical Temporal Event Term Chain .......................................................... 378 Acquiring Verb Subcategorization Frames in Bengali from Corpora ..... 386 An Investigation of an Interontologia: Comparison of the Thousand-Character Text and Roget X  X  Thesaurus ............................. 394 Author Index .................................................. 403 In machine learning approaches to natural language processing (NLP), supervised learning methods generally set their parameters using labeled training data. However, creating a large labeled training corpus is expensive and time-consuming, and is often a bottleneck to build a supervised classifier for a new application or domain. For example, building a large-scale sense-tagged training corpus for supervised word sense disambiguation (WSD) tasks is a crucial issue, because validations of sense definitions and sense-tagged data annotation must be done by human experts [1]. 
Among the techniques to solve the knowledge bottleneck problem, active learning is a widely used framework in which the learner has the ability to automatically select the most informative unlabeled examples for human annotation [2][3]. The ability of the active learner can be referred to as selective sampling . Uncertainty sampling [4] is unlabeled examples for human annotation from the learner X  X  viewpoint. In recent years, uncertainty sampling has been widely studied in natural language processing applications such as word sense disambiguation [5][6], text classification (TC) [4][7], statistical syntactic parsing [8], and named entity recognition [9]. 
The motivation behind uncertainty sampling is to find some unlabeled examples near decision boundaries, and use them to clarify the position of decision boundaries [7]. Experimental results show that some selected unlabeled examples (i.e. near decision boundaries) have high uncertainty, but can not provide much help to the learner, namely outliers. Uncertainty sampling often fails by selecting such outliers [10]. There are some attempts done in previous studies [7][9][10][11]. Cohn et al. [11] and Roy and McCallum [10] proposed a method that directly optimizes expected future error on future test examples. However, in real-world applications, their methods are almost intractable due to too high computational cost for selecting the approach to selecting examples based on informativeness, representativeness and applications. Zhu et al. [7] proposed a sampling by uncertainty and density technique in which a new uncertainty measure called density*entropy is adopted. But because the density*entropy uncertainty measure is based on posterior probabilities produced by a probabilistic classifier thus may not work for active learning with non-probabilistic classifiers such as support vector machines (SVMs). 
To solve this outlier problem, this paper presents a new density-based re-ranking approach based on an assumption that an unlabeled example with high density degree is less likely to be an outlier. It is noteworthy that our proposed re-ranking technique and is easy to implement with very low additional computational cost. Experimental results of active learning for WSD and TC tasks on six evaluation data sets show that our proposed re-ranking method outperforms traditional uncertainty sampling. Active learning is a two-stage process in which a small number of labeled samples and a large number of unlabeled examples are first collected in the initialization stage, adopted. The general active learning process can be summarized as follows: Procedure : General Active Learning Process Input : initial small training set L , and pool of unlabeled data set U Use L to train the initial classifier C 
Repeat z Use the current classifier C to label all unlabeled examples in U z Using uncertainty sampling technique to select the most uncertain unlabeled z Augment L with this new labeled example, and remove it from U z Use L to retrain the current classifier C Until the predefined stopping criterion SC is met. 
In this study, we are interested in uncertainty sampling [4] for pool-based active learning, in which an unlabeled example x with maximum uncertainty is selected for human annotation at each learning cycle. The maximum uncertainty implies that the current classifier (i.e. the learner) has the least confidence on its classification of this unlabeled example. The key is how to meas ure uncertainty of each unlabeled example x . The well-known entropy is a popular uncertainty measurement widely used in previous studies on active learning [5][8][12]. The uncertainty measurement function based on the entropy can be expressed as below: Where P(y|x) is the a posteriori probability. We denote the output class y ..., y k }. H(.) is the uncertainty measurement function based on the entropy estimation of the classifier X  X  posterior distribution. figure as follows. 
In uncertainty sampling scheme, these examples near decision boundaries are viewed as the cases with the maximum uncertainty. Fig. 2 shows two unlabeled examples A and B with maximum uncertainty at the i th learning cycle. Roughly speaking, there are three unlabeled examples near or similar to B , but, none for A . We outlier. Adding B to the training set will thus help the learner more than A . 
The motivation of our study is that we prefer not only the most informative example in terms of uncertainty measure, but also the most representative example in terms of density measure. The density measure can be evaluated based on how many examples there are similar or near to it. An example with high density degree is less likely to be an outlier. In recent years, N -best re-ranking techniques have been successfully applied in NLP community, such as machine translation [13], syntactic parsing [14], and summarization [15]. Based on above motivation, we propose a density-based re-ranking technique for active learning as follows. To our best knowledge, there has been no attempt to use re-ranking technique for active learning. In most real-world applications, the scale of unlabeled corpus would be very large. To estimate the density degree of an unlabeled example x , we adopt a K-Nearest-Neighbor-based density (KNN-density) measure [7]. Given a set of K (i.e. =20 used in unlabeled example x , the KNN-density DS(.) of example x is defined as: The traditional cosine measure is adopted to estimate the similarity between two examples, that is where w i and w j are the feature vectors of the examples i and j . 
In our density-based re-ranking scheme, a baseline learner 1 based on uncertainty sampling is used to generate N -best output from the unlabeled pool U at each learning iteration. The re-ranking stage aims to select the unlabeled example with maximum density degree from these N candidates. The procedure of active learning with density-based re-ranking can be summarized as follows: Procedure : Active Learning with Density-based Re-ranking Input : initial small training set L , and pool of unlabeled data set U Use L to train the initial classifier C 
Repeat z Use the current classifier C to label all unlabeled examples in U z Using uncertainty sampling technique to select N most uncertain unlabeled z Select the unlabeled example with maximum density (i.e. estimated by Equation z Augment L with this new labeled example, and remove it from U z Use L to retrain the current classifier C Until the predefined stopping criterion SC is met. 4.1 Deficiency Measure To compare density-based re-ranking technique with traditional uncertainty sampling method, deficiency is a statistic developed to compare performance of active learning methods globally across the learning curve, which has been used in previous studies [16][7]. The deficiency measure can be defined as: learning method, and AL is an active learning variant of the learning algorithm of REF. n refers to the evaluation stopping points (i.e. the number of learned examples). Smaller deficiency value (i.e. &lt;1.0) indicates that AL method is better than REF following comparison experiments, the REF method (i.e. the baseline method) refers to as entropy-based uncertainty sampling. 4.2 Evaluation Data Sets In the following sections, we will construct some comparison experiments of active learning for word sense disambiguation and text classification tasks, using six publicly available real-world data sets as follows. z Word sense disambiguation task: Three publicly available real-world data sets z Text classification task: Three publicly available natural data sets are used in this 4.3 Experimental Settings We utilize a maximum entropy (ME) model [21] to design the basic classifier for WSD and TC tasks. The advantage of the ME model is its ability to freely incorporate features from diverse sources into a single, well-grounded statistical model. A publicly available ME toolkit 3 was used in our experiments. 
To build the ME-based classifier for WSD, three knowledge sources are used to capture contextual information: unordered single words in topical context , POS of neighboring words with position information , and local collocations , which are the same as the knowledge sources used in [22]. In the design of text classifier, the maximum entropy model is also utilized, and no feature selection technique is used. 
In the following comparison experiments, the algorithm starts with a initial training set of 10 labeled examples, and selects the most informative example at each learning iteration. A 10 by 10-fold cross-validation was performed. All results reported are the average of 10 trials in each active learning process. 4.4 Experimental Results Fig. 4 and Table 1 show results of various active learning methods for WSD and TC based re-ranking technique, comparing to uncertainty sampling scheme 4 [4]. Fig. 4 depicts that re-ranking method constantly outperforms uncertainty sampling on these because the KNN-density criterion can effectively avoid selecting the outliers that often cause uncertainty sampling to fail. In other words, density-based re-ranking technique prefers to choose an unlabeled example with maximum density which is less likely to be an outlier. 
Seen from results on 10 subsets of OntoNotes shown in Table 1, re-ranking method in most cases only achieves slightly better performance than uncertainty sampling. The anomaly lies on revenue subset of OntoNotes, for which a worse deficiency than baseline uncertainty sampling is achieved by re-ranking method. From Table 1 we can see that those words in OntoNotes have very skewed sense distributions. In general, a supervised WSD classifier is designed to optimize overall accuracy without taking into account the class imbalance distribution in a real-world data set. The result is that Data sets Uncertainty sampling Density-based re-ranking OntoNotes the classifier induced from imbalanced data tends to overfit the predominant class and already achieves 95.8% accuracy using the initial small training data, and only makes 0.6% accuracy improvement during active lear ning process. In such anomaly situation the density-based re-ranking method seems to have possibly caused negative effects on active learning performance. Our density-based re-ranking technique can be applied for committee-based sampling [3] in which a committee of classifiers (always more than two classifiers) is generated to select the next unlabeled example by the principle of maximal disagreement among these classifiers. In this case, the baseline learner is a committee of classifiers. From top-m candidates outputted by this baseline learner, the density-based re-ranking technique selects one with maximum density degree for human annotation. 
From experimental results of batch mode active learning, we found there is a redundancy problem that some selected ex amples are identical or similar. Such situation would reduce representative ability of these selected examples. We plan to study the redundancy problem when applying density-based re-ranking technique for batch mode active learning in our future work. 
Furthermore, we believe that a misclassified unlabeled example may convey more information than a correctly cl assified unlabeled example that is closer to the decision boundary. We think it is worth stduying how to consider misclassification information and density criterion together for the selection of informative and representative unlabeled examples during active learning process. 
Zhu and Hovy [12] studied the class imbalance issue for active learning, and found that using resampling techniques such as over-sampling or under-sampling for class imbalance problem can possibly improve the active learning performance. It is worth studying in the future work how to combine the best of density-based re-ranking and resampling technique for active learning. In this paper, we addressed the outlier problem of uncertainty sampling, and proposed a density-based re-ranking method, in which a KNN-density measure is considered to select the most representative unlabeled example from N most uncertain candidates Experimental results on six evaluation data sets show that our proposed density-based re-ranking technique can improve active learning over uncertainty sampling. In future work, we will focus on how to make use of misclassified information to select the most useful examples for human annotation, and applying our proposed techniques for batch mode active learning with probabilistic or non-probabilistic classifiers. Acknowledgments. This work was supported in part by the National 863 High-tech Project (2006AA01Z154), the Program for New Century Excellent Talents in University (NCET-05-0287), Microsoft Research Asia Theme Project (FY08-RES-THEME-227) and the National Science Foundation of China (60873091). based on both its root words and morpheme parts. Identifying POS-tags in a given text is an important task for any Natural Language Application. POS tagging has been based methods. The statistical models are the HMMs [1], MEMMs [2] and CRFs [3]. These taggers work well when large amount of tagged data is used to estimate the parameters of the tagger. However, for morphologically rich languages like Tamil words in the corpus to improve the performance of the tagger. Chunking or shallow word groups like noun phrase, verb phrase etc. It is considered as an intermediate step towards full parsing. This paper presents the exploitation of CRFs for POS tagging and Chunking of Tamil languages with the help of morphological information that can be obtained from morphological analyzer. In Tamil languages, the availability of the problem. In addition, a different approach and set of features are required to tackle the partial free word order nature of the language. This paper is organized as follows. The next section explains the related work in POS tagging and chunking of natural work. In Section 4, we explain how morphological features are vital in identifying the the language specific models for POS tagging and chunking. The methodology for evaluation of the model by comparing a baseline CRF model and our morpheme feature based model. Finally, we conclude and address future directions of this work. POS tagging is essentially a sequence labelling problem. Two main machine learning approaches have been used for sequence la beling. The first approach is based on k-order generative probabilistic models of paired input sequences, for example HMM [4] or multilevel Markov Models [5]. The second approach views the sequence labeling problem as a sequence of a classification problem, one for each of the labels in the sequence. CRFs bring together the best of generative and classification models. Like classification models, they can accommodate many statistically correlated features of the inputs, and they are trained discriminatively. But like generative models they can trade off decisions at different sequence positions to obtain globally optimal labeling. Lafferty [6] showed that CRFs overwhelms related classification models as well as HMMs on synthetic data and on POS-tagging task. Among the text chunking techniques, Fei Sha [3] proposed a Conditional Random Field based approach (how the work differs). There are also other approaches based on Maximum entropy [7], memory-based etc. Pattabhi et al [8] presents a transformation based learning (TBL) approach for text chunking for three of the Indian languages namely Hindi, Bengali and Telugu. 
Many techniques have been used to POS tag English and other European language corpora. The first technique to be developed was rule-based technique used by Greene and Rubin in 1970 to tag the Brown corpus. Their tagger (called TAGGIT) used context-frame rules to select the appropriate tag for each word. It achieved an accuracy of 77%. Interest in rule-based taggers has re-emerged with Eric Brill's tagger that achieved an accuracy of 96%. The accuracy of this tagger was later improved to 97.5% [9]. In the 1980s more research effort went into taggers that used hidden Markov models to select the appropriate tag. Such taggers include CLAWS; it was developed at Lancaster University and achieved an accuracy of 97% [10]. 
In Brill's tagger, rules are selected that have the maximum net improvement rate that is based on statistics. More recently, taggers that use artificial intelligence techniques have been developed. One such tagger [11] uses machine learning, and is a form of supervised learning based on similarity-based reasoning. The accuracy rates of this tagger for English reached 97%. Also, neural networks have been used in developing Part-of-Speech taggers. The tagger developed at the University of Memphis [12] is an example of such a tagger. It achieved an accuracy of 91.6%. Another neural network ta gger developed for Portuguese achieved an accuracy of 96% [13]. 
The sequential classification approach can handle many correlated features, as demonstrated in work on maximum-entropy [14] and a variety of other linear classifiers, including winnow [15], AdaBoost [16], and support-vector machines [17]. Furthermore, they are trained to minimize some function related to labeling error, CRF++ tool [18] and morphological analyzer for both POS tagging and chunking for Tamil. Conditional random fields (CRFs) are undirected graphical models developed for distribution over the hidden variables x given observations z . This model is different from generative models such as Hidden Markov Models or Markov Random Fields, which apply Bayes rule to infer hidden states [20]. CRFs can handle arbitrary using high-dimensional feature vectors.
 data, denoted z . The nodes x i , along with the connectivity structure represented by the hidden states x . Let C be the set of cliques (fully connected subsets) in the graph of a CRF. Then, a CRF factorizes the conditional distribution into a product of clique observed data and the hidden nodes in the clique c, respectively. Clique potentials are functions that map variable configurations to non-negative numbers. Intuitively, a potential captures the  X  X ompatibility X  among the variables in the clique: the larger the potential value, the more likely the configuration. 
Using clique potentials, the conditional distribution over hidden states is written as where The computation of this partition function can be exponential in the size of x . Hence, exact inference is possible for a limited class of CRF models only. Potentials  X  are described by log-linear combinations of feature functions where T c w is called as a weight vector. 
Using feature functions, we rewrite the conditional distribution (1) as 4.1 POS Tagging The main parts of speech in Tamil are: derivatives (that is, nouns derived from verbs, nouns derived from other nouns, and nouns derived from particles) and primitives (nouns not so derived). These nouns could be further sub-categorized by number and case. The noun can be classified as Noun, Pronouns, Relatives, Demonstratives and Interrogatives Nominal noun, pronominal noun, verbal noun. and gender. The Verb tag can be sub-categorized into Imperative, Finite Verb, Negative finite verb and permissive. Further sub-categorization of the Verb class is possible using number, person and gender. 3. Other class: This includes: Adjective, Adverbs, Prepositions, Conjunctions, Interrogative, Demonstrative Particles, Intensifier and Interjections. The tag set has five main tags namely Noun, Verb, Adjective, Adverb, Particles The subcategories of the five main categories contains 131 tags. 
The verbs have been sub-categorized by  X  X ype X  (perfect, imperfect, imperative), person, number and gender, and the tag name reflects this sub categorization. For example, the word mtd; tUfpwhd; [avan varukiRAn]  X  he [Pronoun ] comes X , which is a perfect verb in the second person masculine plural form, has the tag VPSg3M. An indicative imperfect second person feminine singular verb such as mts; vOjpf;nfhz;bUf;fpwhs;  X  X he [singular, feminine] are writing X  would be tagged VISg3FI. 
Similarly, personal pronouns are tagged for number, person and gender. As well as personal pronouns, there are relative and demonstrative pronouns, which are also classed by number and gender. Nouns are classed by number (compare gwit , gwitfs ; , meaning  X  X  bird X , and  X  X ooks X ). Foreign and proper nouns receive separate tags. The category of others includes prepositions, adverbs and conjunctions, all of which appear interjections, exceptions and negative particles. Dates, numbers, punctuations and abbreviations are also tagged separately. Dates are classified as either a Tamil date such as rpj;jpiu [ciththirai] which is the first month in the Tamil calendar. 
The approach that was used in this work for POS tagging is as follows: CRF model (CRF++) is used to perform the initial tagging and then a set of transformation rules is applied to correct the errors produced by CRFs. The baseline CRF model contains the basic word features, whereas the modified CRF model includes morphological suffix and prefix information. The modified CRF model is then trained using these updated features. To measure the performance of CRFs against other ML approaches we carried out various experiments using Brant X  X  TnT [3] for HMMs, Maxent for MEMMs. Interestingly HMMs performed as high as CRFs with basic features. We preferred CRFs over HMMs as addition of features like the root words were much easier in CRFs. 4.2 Chunking Phrase chunking is a process that separates and segments sentences into its sub chunk labels. The CRF models are trained on the feature templates for predicting the chunking boundary. Finally the chunk labels and the chunk boundary names are merged to obtain the appropriate chunk tag. It is basically HMM+ CRF model for chunking. 
Table 1 shows the Phrase chunking for an example sentence in Tamil. The input to the phrase chunker consists of the words in a sentence annotated automatically with part-of-speech (POS) tags. The chunker's task is to label each word with a label indicating whether the word is outside a chunk (O), starts a chunk (B), or continues a chunk (I). For example, the tokens in table 1 would be labelled BIIBBIIIIIBIO . 5.1 POS Tagging Baseline CRF Model : A template with the following features as mentioned an example in CRF++ tools with a window size 5. This CRF model is considered as a baseline model for POS tagging. The features are word(-2) ,word(-1), word(0), word(1), word(2) word (-2) word(-1) word(0) word(1) word(2) Morpheme featured based CRF Model: Root of a word that can be identified through morphological analyzer provides information about main POS type classification and the further classification can be done by separate CRF model for each main class. The Root types are Numbers; Numbers in words, Date Time, Enpavar 1 , Enpathu 1 , En 1 , Noun, Pronoun, Interrogative Noun, Adjectival Noun, Non Tamil Noun, Verb, Finite Verb, Negative Finite Verb, Adjective, Demonstrative Adjective, Interrogative Adjective Postposition, Conjunction, Particle, Intensifier, Interjection and Adverb. The CRF model is designed with the following features Lost Morpheme part(word(0), previous to the last morpheme part(word(0), Root type(word(0)), Root type(word(-1)) and Root type (word(+1) ) Root type(word(0)) / Root type (word(-1)) Root type( word(+1)) / Root type ( word(0)) Last morpheme part (word(0)) / Lost Morpheme part(word(-1)) Lost Morpheme part(word(+1))/ last morpheme part (word(0)) (Three states) last morpheme pa t(word(-1)) / Lost Morpheme part(word(0)) / Root type(word(0)) 5.2 Chunking Baseline CRF Model : The features of base line CRF model are 1) State features word(-2), word(-1), word(0), word(1), word(2), POS(-2), POS(-1), POS(0), POS(1),
POS(2) word(-1))/ word(0), word(0) / word(1), POS(-2) / POS(-1), POS(-1) / POS(0), POS(0) / POS(1), POS(1) / POS(2),POS(-2))/ POS(-1)/ POS(0), POS(-1)/ POS(0)/ POS(1) and POS(0)/ POS(1) /POS(2)
The window size for baseline CRF is 5 with labeled -2, -1, 0, 1, 2. Word denotes string of characters. POS denotes the Part-of-speech of the word in the corresponding position mentioned in the parameter. Morpheme featured based CRF Model: In partially free word order languages, the view, the features to be considered in designing a CRF model are its POS type, Last end morpheme component ( E l ) and previous to last end morpheme component ( E l-1 ). The size of the window is 5 words. The centre word of the window is considered as designed language specific CRF model for chunking are 6.1 POS Tagging The Baseline CRF model for POS tagging is trained with 39,000 sentences (a mixtures different type of sentences with an average of 13 words per sentence. This corpus is semi automatically POS tagged and manually verified). The modified feature CRF model is also trained with the same corpus of 39000 POS tagged sentences that are used in the baseline CRF model. The tagging of word whose main category is known by root information of the word using morpheme components of word which obtained from morphological analyzer. 6.2 Chunking The baseline CRF model for chunking is trained with the corpus (39000 sentences) that is tagged by modified CRF models for POS tagging. The modified CRF model for chunking are trained on the same corpus that tagged by POS tagging and tested with the same datasets used in the generic CRF model for chunking. The performance comparison between both models is explained in the next section. Evaluation in Information Retrieval makes frequent use of the notation precision and recall. The same metrics can be used for both POS tagging and chunking. system got right 
Recall is defined as the proportion of the target items that the system selected To combine precision and recall into a single measure of over all performance, The F measure is defined as precision and recall. A value of  X  is often chosen for equal weighting of precision and recall. With this  X  value the F measure simplifies to 
The standard evaluation metrics for a chunker are precision P (fraction of output chunks that exactly match the reference chunks), recall R (fraction of reference chunks returned by the chunker), and their harmonic mean, the F measure. The relationships between F score and labeling error or log-likelihood are not direct, so we report both F score and the other metrics for the models we tested. 7.1 POS Tagging Evaluation The modified feature CRF model is evaluated and compared with the performance of the baseline CRF model. These two models are tested with three test sets. The test sets sets are 18345, 19834 and 18907 words respectively. Table 2 shows the performance of the POS tagging with baseline CRF model. Some of the untagged words are proper names. Three experiments are conducted with test sets. It shows precision, recall and F-Score values for corresponding test sets. The table 3 shows the Performance of the POS tagging with Modified CRF model. The result shows the total POS tagging of 3 test set as used in the previous experiment. The effect of comparison between both CRF model shows that F-score value of modified CRF model for POS tagging increased significantly and Recall metric value of the modified CRF model is predominantly increased as compared with baseline CRF model. 
The modified CRF model improves the performance of Tamil POS tagging significantly as compared with baseline. The reason behind this is the modified model depends on the root type and morpheme ends of the word. The POS type of a word in Tamil is strongly depends on the morpheme ends and its root type. The baseline CRF model depends on the words and its sequence but the language is free word order nature. The sequence is not so much important for predicting the POS type of word. 7.2 Chunking Evaluation The modified feature CRF model is evaluated and compared with the performance of the tagging evaluation. The sizes of test sets are 6342, 6834 and 6521 chunks respectively. 
Table 4 shows the performance of the chunking with baseline CRF model. Three experiments are conducted with separate test set with size of 6342, 6834 and 6521 chunks respectively. It shows precision, recall and F-Score values for corresponding test sets. Table 5 shows the performance for Tamil chunking with precision, recall and F-corpuses which are already used in baseline model. 
The baseline CRF model is designed with features of the words, their POS and depends on the morpheme ends and its root type. Tamil language is free word order nature and morphologically rich. Our CRF model is designed with these language properties in mind. This CRF model with modified feature improves the performance of Tamil chunking significantly as compared with baseline. The graph in figure shows the performance comparison of Modified feature CRF model with baseline CRF model for Tamil Chunking for three difference set of test data. The models designed for POS tag and chunking for Tamil using CRF give high performance as compared baseline CRF POS tagging and chunking. The morphological information is very important feature for designing CRF model of languages with free word order or partially free word order characteristic. This work can also be improved by adding error correction module like Transformation based Learning. Also this model can be improved by adding the phrase boundary detection before Chunking process. This work can be extended for semantic tagging like named entity recognition and semantic role labeling. apr o cedure f o r i de n t i f yin g no u n phrases (NP chu n ks )in ase n te n ce .T hese NP in o ther chu n ks .Al th o ugh the c on cept o f NP chu n k in g w as first in tr o duced b y[1] f o rbu il d in g a parse tree o fase n te n ce , these NP chu n ks pr ovi de usefu lin f o rma -t ion f o r NLP taskssuchassema n t i cr ol e l abe lin g o r wo rd se n se d i samb i guat ion. Fo re x amp l e , a v erb  X  ssema n t i cargume n ts ca n be der iv ed fr o m NP phrases a n d be e x tracted fr o m NP phrases .

T he meth o ds f o rc l ass i f yin g NP chu n ks in se n te n ces ha v ebee n de v e lo ped b y best reca ll i s 9 4 . 60% on the C oNLL-2 000 data .
 In th i spaper ,w ed i scuss a n e w meth o df o r i de n t i f yin g NP chu n ks in te x ts . assumpt ion s .Mo re ov er , the c o rresp on d in gmathemat i ca l represe n tat ion o fthe a NP chu n k .

In e x per i me n ts on the P e nn T reeba n k WSJ data ,o ur meth o dach i e v es a v er y a n dthea v erage o ff -measure i s 9 8 . 2 %. Mo re ov er ,in e x per i me n ts on the C oNLL-2 000 data ,o ur meth o ds d o es better tha n a ny o fthec o mpet in gtech ni ques : the 95 . 8 3%.

T he rest o f the paper i s structured in the f ollowin g w a y. T he sec on d sect ion prese n ts the pr o p o sed meth o d .T he th i rd sect ion dem on strates the emp i r i ca l resu l ts .T he f o urth sect ion re vi e w sre l ated researches .T he fifth sect ion g iv es a c on c l us ion. 2.1 An Example T ab l e 1 sh ow sthata nin put se n te n ce wi th i t P O S tags .T he n, b yo ur meth o d , these categ o r i es . 2.2 Describing the Task L et U be a l a n guage , V be vo cabu l ar yo f U , a n d T be POS tags o f V .L et s w here C 1 in d i cates the curre n ts y mb ol i s in a nNP chu n k , C 2 in d i cates the curre n ts y mb ol i s no t in a nNP chu n k , a n d C 3 starts a n e wNP chu n k .T he tasks ca n be stated as f ollow s :
G iv e n S =( s 1 , ... ,s N ), w e n eed 2 . t o determ in e NP chu n ks based on ( c 1 , ... c N ). 2.3 Building Probabilistic Graphical Models s . t . s ) ca n be represe n ted as Fi g 1.

B ased the assumpt ion w eha v emade , sh ow sth i sm o de l. F r o mth i sm o de l, aset o f N +1 c li ques 1 i s o bta in ed : Mo re ov er , aset o f2 N  X  3 separat o rs 2 i s rece iv ed : pr o duct o fc li ques d ivi ded b y the pr o duct o f separat o rs .

H e n ce : Differences between Other Graphical Models. W hat i sthed iff ere n tbe -t w ee no ur m o de l a n d o ther graph i ca l m o de l s ?InFi g 3, w esh ow a HMM[ 7 ], a sec on d o rder HMM[ 4 ], a M E MM[ 7 ], aCR F[ 8 ], a n dthem o de lo fus .Fo rs i m -p li c i t y, w e j ust sh ow the s i part .T hese m o de l sreprese n ta join tpr o bab ili t y on C oNLL-2 000 shared task data set ,o ur m o de l ca n ach i e v e the better perf o r -ma n ce tha nHMM s .How e v er , there are no c o mpar i s on sbet w ee no ur m o de lwi th MEMMs o r CRFs .
 Making a Decision and Error Estimations. Fo r S =( s 1 , ... ,s N ), s i  X  S , based on (3), w edefi n e M ( s i ,c i ): W ese l ect C k f o r s i i f : W eest i mate a n err o rf o rass i g nin g C k t o s i b y: A cc o rd in g (5), (6) i sm ini ma l.
 Term Estimations of the Equation (3). Fo r S =( s 1 , ... ,s N ), f o re v er y s that w i a n d t i are in depe n de n tc on d i t ion ed on c i . 3.1 Experiment Setup Corpora and Training and Testing Set Distributions. W eha v eusedt wo c o rp o ra in o ur e x per i me n ts . O n ec o rpus i stheC oNLL-2 000 shared task data set [10]. Fo rtheC oNLL-2 000 task data , first ,w eusethe o r i g in a l tra inin gseta n d the test in gset ( de v e lo ped b y the C oNLL-2 000 task data set creat o rs ) t o test o ur m o de l. T he n, w em ix the o r i g in a l tra inin gseta n dtest in gsett o gether a n d Nin e parts are used as tra inin gdataa n d on epart i sastest in gdata .T h i s i s d on e repeated ly f o rte n t i mes . Each t i me ,w ese l ect on e o fthete n partsasthe test in gpart .T he tra inin gsetc on ta in sab o ut 2 3 4 , 000 t o ke n s w h il ethetest in g set c on ta in sab o ut 2 6 , 000 t o ke n s .Fo rthe WSJ c o rpus ,w ese l ect the sect ion s fr o m 0 2 00 t o 0999 as o ur tra inin gseta n d the sect ion sfr o m 1000 t o 2 099 as o ur the test in gsetc on ta in s 1000 fi l es .T hese 1000 test in gfi l es are d ivi ded in t o10 Evaluation Metrics. T he e v a l uat ion meth o ds w eha v eusedareprec i s ion, re - X  be the t o ta ln umber o f s i w h i ch tru ly be lon gs t o the c l ass C k .T he prec i s ion i s P re =  X   X  , the reca ll i s R ec =  X   X  , a n dthef -measure i s F me = 1  X  3.2 Procedures and Results Fo re v er y se n te n ce in the tra inin gc o rpus ,w ee x tract no u n phrases ,nono u n phrases , a n dbreakp oin t phrases .W ef o rm a set o fthreec l asses .T he c l ass c 1 c c on ta in sa ll break p oin t phrases .
 o f P O S tags t i )in c i d ivi ded b y the t o ta ln umber o f wo rds w (o r : the t o ta l n umber o f wo rd phrases w i w i +1 (o r : the n umber o f P O S tag phrases t i t i +1 )in c d ivi ded b y the n umber o f wo rds w i (o r : the n umber o f P O S tags t i )in c i .W e (o r : the n umber o f P O S tag phrases t i t i  X  1 )in c i d ivi ded b y the n umber o f wo rds w i ( the n umber o f P O S tags t i )in c i .
 Making a Decision on the Testing Set. Fo ra wo rd wi th i ts c o rresp on d in g P O S tag in each se n te n ce in the test in gset ,w eass i g n ac l ass based on (5). In o ur as f ollow s .W ese l ect C k f o r s i ,w he n: W etestprec i s ion a n d reca ll u n der d iff ere n t  X  .
 Results from the CoNLL-2000 Shared Task Data Set. O n C oNLL-2 000 shared task data ,w eha v etested o ur m o de l based on (3), ( 4 ), a n d (5). T he resu l t i sdem on strated in Fi g4 .

In the Fi gure 4 , the X a xi sreprese n ts the te n test in g sets descr i bed in S ec -f -measure acc o rd in g ly. C o rresp on d in gt o each n umber in the X a xi s , there are f -measure , a n dthe low er v a l ue represe n ts a prec i s ion. By c oll ect in gtheresu l ts f o reachtest , the a v erage prec i s ion i s 95 . 15%, the sta n dard de vi at ion f o rthe the f -measure i s 0 . 00 27 .

W e further test o ur m o de l b y e x c l ud in gthe l e xi c on ( the w i ) a n dus in g only the P O S tags ( the t i ). T he test resu l t i ssh owninFi g 5. C o mpar in gtheresu l ts w eha v eg o t in the Fi g4 , the a v erage prec i s ion i s reduced ab o ut 3% fr o m 95 . 15% t o9 2 . 27 %. T he a v erage reca ll i s reduced ab o ut 2 . 4 % fr o m 96 . 05% t o93 . 7 6%. An dthef -measure i s reduced ab o ut 3 . 4 % fr o m 95 . 59% t o9 2 . 7 6%.
F urther ,w etest o ur m o de l b y e x c l ud in gthe P O S tags a n dus in g only the l e xi c on. T he test resu l t i sdep i cted in Fi g 6. C o mpar in gtheresu l ts t o that o f a v erage reca ll i s reduced ab o ut 2 . 8 % fr o m 96 . 05% t o93 . 35%. An dthef -measure i s reduced ab o ut 6% fr o m 95 . 59% t o 8 9 . 7 5%.

By c o mpar in gthethreeresu l ts on the C oNLL-2 000 shared task data ,w eha v e no t i ced that i fthem o de li sbu il t only on the l e xi ca lin f o rmat ion, i thasthe low est perf o rma n ce o ff -measure 8 9 . 7 5%. T he m o de l X  sperf o rma n ce i mpr ov ed 3% in f -measure i f i t i sc on structed b yP O S tags .T he m o de l ach i e v es the best perf o rma n ce o f 95 . 59% in f -measure i f w earec on s i der in gb o th l e xi c on sa n d P O S tags .

Ano ther e x per i me n t w eha v ed on e on the C oNLL-2 000 shared task data set m o de l based on the equat ion (10). W etest o ur m o de lwi th in the ra n ge o f  X  =0 . 01 the prec i s ion in creases a n d the reca ll decreases .W he n  X  equa l s 5, the prec i s ion = the reca ll = 95 . 6%. By o bser vin gtheresu l t ,w eha v e no t i ced i f w e w a n tt o w eca n se l ect  X &lt; 5.
 Results from the WSJ Data Set. T he sec on ddataset w eha v ee x per i me n ted i s WSJ data o f P e nn T reeba n k .T he ma in reas on f o rust o use th i sdataset i sthat w e w a n tt o see w hether the perf o rma n ce o f o ur m o de l ca n be i mpr ov ed w he ni t i sbu il t on m o re data .W ebu il d o ur m o de lon atra inin gset w h i ch i sse v e n t i mes l arger tha n the C oNLL-2 000 shared task tra inin gdataset (S ect ion 3.1). T he fr o m 96 . 05% t o9 8 . 65%. T he a v erage f -measure i s in creased 2 . 7 % fr o m 95 . 59% t o9 8 . 2 %. O v er the past se v era ly ears , mach in e -l ear nin gtech ni ques f o rrec o g ni z in g NP chu n ks in te x ts ha v ebee n de v e lo ped b y se v era l researchers .I tstartsfr o m [ 2 ]. A start p oin t o fa NP chu n k i sreprese n ted b y a no pe n square bracket w h il ea n e n dp oin t i sreprese n ted b y a n c lo sed square bracket .T he meth o dca l cu l ates the tra in ed on the B r own C o rpus .T he test in gresu l t i ssh own b y aset o ffiftee n se n te n ces wi th NP chu n ka nno tat ion s .

Ramsha w a n d M arcus [3] sh ow that NP chu n k in gca n be regarded as a tagg in g task .In the i rmeth o d , three tags ( c l asses ) are defi n ed .T he y are I tag , O tag , a n d B tag .T he first tag in d i cates that the curre n t wo rd i s in a NP chu n k .T he sec on dtag in d i cates that the curre n t wo rd i s no t in a NP chu n k .T he l ast tag sh ow sabreakp oin t o ft wo c on secut iv e NP chu n ks .Ino rder t ol abe lon e o f these three tags t o each wo rd in ase n te n ce , the y des i g n aset o fru l es based on lin gu i st i ch y p o theses a n dusetra n sf o rmat ion based l ear nin gtech ni ques t o tra in the tra inin gset in o rder t oo bta in aset o f o rdered ru l es . E x per i me n ts are c on ducted on the WSJ c o rpus , sect ion s 0 2 -2 1 are used f o rtra inin ga n d sect ion the f -measure i s 93 . 3%.

Molin aeta l. [ 4 ] emp loy the sec on d -o rder HMM t oi de n t i f yNP chu n ks in te x ts b y creat in ga no utput tag seque n ce C =( c 1 , ... ,c N )wi th the best represe n tat ion f o ra nin put seque n ce S =( s 1 , ... ,s N ). In o rder t o determ in epr o bab ili t i es o f are c on ducted on the C oNLL-2 000 task data .T he prec i s ion i s 93 . 5 2 %, the reca ll i s 93 . 4 3%, a n dthef -measure i s 93 . 48 %.

T he m o st rece n t attempt f o r i de n t i f yin g NP chu n ks uses a supp o rt v ect o r mach in e (SVM) [5]. In th i smeth o d , based on the meth o d olo g y pr ovi ded b y[3], aset o fh i gh d i me n s ion a l feature v ect o rs are c on structed fr o mthetra inin gset t o represe n tc l asses .An e w a n du n k nown v ect o r x ca n be j udged b y a lin ear c o mb in at ion fu n ct ion. A cc o rd in gt o the auth o rs ,SVM has ach i e v ed the best perf o rma n ce am on ga ll the a l g o r i thms .T he paper sh ow sthat SVM ca n ach i e v e the prec i s ion 9 4 . 15%, reca ll 9 4 . 2 9%, a n df -measure 9 4 . 22 %on the C oNLL-2 000 task data set .

Our meth o dad o pt Ramsha w X  s i dea [3] o fass i g nin gd iff ere n tcateg o r i es t o wo rds in ase n te n ce based on w hether these wo rds are in s i de a NP chu n k ,o uts i de a NP chu n k ,o rstarta n e wNP chu n k .Mo re ov er , the in f o rmat ion c oll ected f o r the best descr i pt ion o fa n e win put se n te n ce .How e v er ,o ur m o de l ass i g n sa categ o r y f o ra wo rd o fthe in put se n te n ce based on the in f o rmat ion o fthe wo rd , the pre vio us wo rd , a n dthe n e x t wo rd w eha v emetbef o re ,w h i ch a huma no fte n d o es th i s in the same w a y. e x amp l e , a v erb  X  ssema n t i cargume n ts ca n be der iv ed fr o m NP chu n ks a n dfea -be e x tracted fr o m NP chu n ks .Wi th th i sm o t iv at ion, w ed i scuss a pr o bab ili s -m o de li sc on structed u n der t wo c on d i t ion a lin depe n de n ce assumpt ion s ,i tca n accurate ly i de n t i f yNP chu n ks in se n te n ces .
 When we type something in existing search engines, it takes the form of keywords. That is, for most of commercial search engines, queries are treated as a bag of keywords. It works pretty well for web pages; however the situation is changing with advent of Semantic Web. For example, semantic web applications already enable the users to search like this with a formal query language [1]:  X  X ind who regularly posts on this forum more than once in a week and whose posts are read by more than 20 subscribers. X  It is difficult to describe this with a set of mere keywords.
 standing of query is important. In recent TREC QA task, one can see various strategies for query processing. In some systems, classical NLP methods of parsing and seman-tic analysis are used to process queries [2], while others ignore sentence structures and treat a query as a bag of words, and then process it with a classification method [3]. Most systems are somewhere in the middle, combining hand-coded rules, statistical classifiers and partial parsers [4].
 approaches are reported in literature. Zhou et al. [5] used keywords as queries, and built any possible structure among keywords, and then let users select one through a visual interface. Tablan [6] used Controlled Natural Language (CNL) to process formal queries in a natural language form. Kiefer et al. [7] used a form and a visual interface to guide users to compose formal queries. Kaufmann [8] made a usability study on various query methods and concluded that casual users prefer a natural language interface to a menu-guided or a graphical query interface. However he also noted that NL interfaces are generally more difficult to build and less accurate.
 sider two Korean sentences:  X   X   X   X   X   X   X   X   X   X   X   X  ? (What is the company that takes over Daewoo Electronics?) X ,  X   X   X   X   X   X   X   X   X   X   X   X  ? (What is the com-pany that taken over by Daewoo Electronics?) X . In Korean, they have identical nouns and verbs with identical order, but have opposite sentence structures (Object-verb-subject and Subject-verb-object) and opposite meanings. When treating a query as a bag of words, this kind of information is lost. It has to be recaptured somehow by a different method.
 onymous phrases can be written in different syntactic forms. For example, consider syn-onymous Korean questions with three different syntactic structures:  X   X   X   X   X   X   X   X   X   X   X   X  ? (Where did he assigned to?) X ,  X   X   X   X   X   X   X   X   X   X  X  X   X  X  X  . (I would like to know the location assigned to him.) X , X   X   X   X   X   X   X   X   X   X   X   X  ? (The place he will work this time is?) X . For traditional parsing, post-processings like semantic anal-ysis, classification or rule heuristic are needed to conclude that the sentences above are asking the same thing.
 curring phrases or sentences. A phrase which is recognized by a local grammar not only reveals its syntactic structure, but also its class and possible synonymous phrases. By a well-prepared local grammar, a computer system can directly reveal the intention of a user X  X  natural language message. In this paper, local grammars are adopted to capture patterns of query sentences. The system can process a large number of sentences from finite query types. It is designed to be used as a natural language query interface for a large scale IT ontology.
 its graph building process. In Section 3, the parsing system and its processes are de-scribed. In Section 4, the experiment and the results are shown, and finally Section 5 has conclusions. 2.1 A Brief Background on the Local Grammar Approach and Its Tools The notion of  X  X ocal grammar X  has been devised by Gross [9], where it is for cap-ture semi-frozen phrases or sequences with frozen behavior inside a specific domain (eg.  X  X loudy with sunny periods X , weather forecast domain). The idea is that once lo-cal grammars for recurring elements are complete (eg. date expression, personal title expressions, etc), then they can be reused in the description of larger linguistic con-structions. It is a bottom-up approach towards a description of a language.
 form is possible, but with minimal change in its meaning. In [10] he gives examples like below.
 According to Gross, these phrases are synonymous, and they can easily be captured in a local grammar.
 Unitex [12] and recently Outilex [13] are the tools that enable linguists to describe local grammars and lexicon grammars. They not only support the grammar building in a visualized way, but also support various capabilities needed to test language resources, such as morphological analysis, tagging, disambiguation and applying of grammars over corpus.
 tion extraction [14], named entity localization [15], grammatical structure identification [16]. All of these experiments result in recall and precision rates equaling the state-of-the-art.
 Unitex, local grammars are expressed in graphs, and such graphs are called local gram-mar graphs (LGG). 2.2 Building Local Grammar Graphs for IT Domain In this section, the building processes of LGGs are briefly described. It starts with gath-ering of sentences from target domain. Some policy for the modeling is decided, then in a bottom-up fashion, sentences are clustered into groups. Each group is then modeled into local grammar graphs.
 gathered from web forums. Figure 1 shows some of such sentences.
 subclasses with policy described below. 1. Exclusion of Yes or No questions. 2. For questions with interrogative words, types of interrogative words are limited to 3. Questions with hidden interrogative words are treated in the same way with the 4. Most of the sentences are written in form of active voice. Thus, the basic sentence 5. Synonymous expressions among predicate-noun based phrases and simple-verb be combined with general do-verb( X   X   X   X ), or become-verb( X   X   X   X ) to form a new verb. Such nouns are called predicate nouns . Examples of predicate-noun based verbs are,  X   X   X   X   X  (become-separation) X ,  X   X   X   X   X  (become-union) X ,  X   X   X   X   X  (do-development) X . In a given domain, one can also find simple verbs with similar mean-ing. For above examples,  X   X   X  X  X  (divide) X , X   X   X   X   X  (being-merged) X  and  X   X   X   X  (make) X  are similar simple verbs. Policy #5 states that finding and describing such syn-onymous phrases for given domain are important.
 shows the classes and number of graphs per each class.
 a surface form or a lexical code, or a sub-graph. For Korean, the surface form can be either an eojeol 1 , or a morpheme. A lexical code is a syntactic/semantic code of lexi-con as described in the lexical resource (eg. Noun+animal+living-human). Frequently reappearing linguistic elements are usually captured in a separate graph, and used as a sub-graph. Box  X  X onderNI X  in the figure is a sub-graph that holds asking/wondering expressions. The symbol  X # X  means that both sides connected by a  X # X  are morphemes and should be connected to form one eojeol.
 the graph and the generated text, there are code units like  X  X +Prod X  or  X  X +Comp X . Such code represents a slot that can be filled with lexicon with given attributes. In the system, it is called as a  X  X ariable X . 2 It usually holds the needed arguments for query patterns. All variable types used in the graphs are listed in Table 1. 3.1 Generating All Possible Sentences The system first traverses grammar graphs to generate all possible sentences that can be recognized by each grammar. There are two reasons for this. First, it makes match method simpler. By generating all sentences, minimal match unit is changed from a morpheme to an eojeol. Thus morphological analysis of the query sentence is not needed at the query time. Result of a morphological analysis can have more than one output for a given eojeol, thus it reveals some ambiguity. Though the ambiguity at this level can be decided effectively, it is better to avoid it.
 an agglutinative language, Korean can have near free word-order. Even if the modeling of a query is good for the situation, word-order changes or a newly inserted adverb can ruin the perfect match. Modeling all possible word-order change in the graph is not feasible, so partial matching has to be done. Processing partial match is generally a difficult problem related to alignment [18]. In the system, the issue of partial matching is converted into an easier problem with some assumptions described in the next section. 3.2 Building the Eojeol Sequence Table In this stage, all generated sentences are checked word by word to keep records of eojeol level bi-gram sequences. The method is described in the Figure 5. in a particular pattern, there would be exactly one sentence in the generated pattern sentences. 3 When the query is not covered completely, the best pattern is chosen that covers most of the query. This is a bi-gram model of eojeols to find the most similar sentence in the pre-generated bag of sentences. By changing the alignment problem into a bi-gram model, some information of word location is lost. However, it is endurable and gives some additional advantages. Inserted adverbs or changing of word-orders which are not modeled in the LGGs can now be treated as partial matches.
 sition, the bi-gram table is a 43,642 by 43,642 table, where each row and column has an observed eojeol and each cell holds graph IDs where a sequence of row eojeol to column eojeol has been observed. 3.3 Query Processing This section describes how the system uses the sequence table to process user X  X  query. The code is outlined in Figure 6.
 with a variable eojeol. If such decomposition is possible, the eojeol is decomposed into variable-eojeol form and its variable value is saved for later processing. The system then process eojeol by eojeol to decide the query pattern. It records simple voting counts from each eojeol sequence. If an eojeol sequence ( E 1 , E 2 ) has been observed in two patterns P a and P b , this sequence votes for both P a and P b . All votes for each pattern are accumulated. Each candidate pattern is checked whether or not it has recognized all the needed arguments for the pattern. If some arguments are missing, the pattern gets no final vote count. When a pattern gets all needed arguments, it gets one additional vote count. name and recognized arguments as the output. If a tie happens, the system calls the tie-breaker. The tie-breaker calculates each eojeol X  X  IDF value by regarding each graph pattern as a document. With this value, the tie-breaker chooses a pattern with higher score.
 The system was implemented in Perl, and tested on a MacOS system with 3.0Ghz intel zeon quadcore CPU. From 163 graphs, 167,298,259 sentences have been generated. It took 30 hours to obtain to 43,642x43,642 eojeol sequence table. With the sequence table, user queries are processed in real time. The experiments were made with two things in mind. How well will our model process sentences for each given pattern? And how much is the coverage of our model in possible queries in this domain? tions for each pattern. In the survey form, description of each pattern was written in a plain language with a small number of examples. For each pattern, they were asked to write down several questions with similar meaning. For the second experiment, the questions about IT company and people were sampled from Korean web forum. 4 To test the coverage of our patterns, we only gathered questions posted in the year of 2008 (all graphs were built in 2007). We did not made any other restrictions on selecting of questions. Before doing query process, all entity names and general nouns that can be placed in the variable slots are enlisted in the system X  X  lexical resource. plicated sentences (7 sentences), 477 sentences were left. Out of 477 sentences, the system correctly processed 426 sentences, thus accuracy is 89.3%.
 actually one of 38 subclasses. The examples of this type are Yes/No question, or ques-tions out of pattern X  X  scope (Pattern 8.c is about a person X  X  physical issue like height, weight and blood type. But one of the tester wrote something like  X  X oes he have a hot temper? X ). Unknown synonymous phrase are the cases where the match fails because of an unlisted expression with similar meaning. For example, the patterns for company  X  X xpanding X  should enlist all synonymous verbs, but we could find some missing ex-pressions like  X   X   X   X   X  (move in) X  or  X   X   X   X   X  (targeting) X . Unknown structure is similar in effect, but this counts cases with different sentence structure. For example, a noun phrase  X   X   X   X   X   X  (merged company) X  can be expressed in verb phrase form,  X   X   X   X   X   X   X   X   X  ? (did they merged with the company?) X , but it was not listed in our pattern. Unknown structures can appear not only as a phrase, but also as a full sen-tence. Unbreakable tie cases are where tie breakings are not possible. When two pattern covers exactly the same sequence of words, no tie-breaking is possible. The result shows 12 such cases and it means that some patterns are quite similar or nearly overlapping. If out of pattern cases are removed from the set and unbreakable tie cases are counted as correct, the result can go up to 92.9% of accuracy.
 tem only processed 32% of them correctly. Most of errors were from unknown type of questions, such as recruiting, stock market related questions, or questions about com-paring of two companies. The number of unknown structure case is also high, where companies X  merging, takeover and profit related questions are more complex than the described patterns. Note that we did not restrict the type of questions in this experiment. Any questions about IT people and IT company written in a sentence were sampled. This paper presented a query processing system based on hand-crafted local grammars. By analyzing large number of user questions of the domain, 38 subclasses are modeled with 163 local grammar graphs. Each pattern graph essentially holds all synonymous expressions of the given meaning. A bi-gram model is adopted to handle partial match problem caused by Korean word-order change and free insertion of adverbs. The sys-tem showed a good accuracy over given question types, and meaningful coverage over unrestricted queries of the domain. At the best of our knowledge, this is the first system that uses local grammars to understand Korean natural language queries.
 proach can recognize not only syntactic structures of a sentence but also the class of the sentence in the pre-defined domain, thus effectively extracts the intention of the sen-tence directly. We will continue our research to produce better models. Future works are including: translating query patterns into actual SPARQL queries and evaluate cov-erages over the ontology, processing sentences as a combination of more than one LGG patterns, and expanding of query patterns to cover more issues of the domain. trackable and can be used as a query while sentences are not. However, local grammar-based applications like our research show that if you can restrict the domain, variations of sentences (including verbs and verb phrases) are also trackable, and can be used to understand user X  X  natural language message.
 Acknowledgements. This work was supported in part by MIC &amp; IITA through IT Leading R&amp;D Support Project and by the Korean Ministry of Education under the BK21-IT Program.
 Named Entity Recognition (NER) is an important tool in almost all Natural Language Processing (NLP) application areas including machine translation, question answer-ing, information retrieval, information extraction, automatic summarization etc. The current trend in NER is to use the machine-learning approach, which is more attrac-system is much cheaper than that of a rule-based one. The representative machine-learning approaches used in NER are Hidden Markov Model (HMM) (BBN X  X  IdentiFinder [1]), Maximum Entropy (ME) (New York University X  X  MENE [2]) and Conditional Random Fields (CRFs) [3]. Support Vector Machines (SVMs) based NER system was proposed by Yamada et al. [4] for Japanese. The process of stacking and voting method for combining strong classifiers like boosting, SVM and Trans-formation Based Learning (TBL), on NER task can be found in [5]. Florian et al. [6] tested different methods for combining the results of four systems and found that robust risk minimization worked best. Munro et al. [7] employed both voting and bagging for combining classifiers. The work reported in this paper differs from that of conducted a number of experiments to post-process the outputs of the classifiers with the lexical context patterns, which are generated in a semi-automatic way from an unlabeled corpus of 10 million wordforms, and used several heuristics to improve the performance of each classifier before applying weighted voting. 
Named Entity (NE) identification and classification in Indian languages in general and in Bengali in particular is difficult and challenging as. A pattern directed shallow parsing approach for NER in Bengali is reported in [8]. A HMM based NER system for Bengali has been reported in [9], where additional con-textual information has been considered during emission probabilities and NE suf-Bengali NER can be found in [10] and [11] with the CRF, and SVM approach, re-spectively. Other than Bengali, the works on Hindi can be found in [12] with CRF, in [13] with a language independent method, in [14] with a hybrid feature set based ME approach and in [15] using MEMM. As part of the IJCNLP-08 NER shared task, various works of NER in Indian languages using various approaches can be found in the proceedings of the IJCNLP-08 workshop on Named Entity Recognition on South and South East Asian Languages (NERSSEAL 1 ). We have used a Bengali news corpus [16], developed from the web-archive of a widely read Bengali newspaper for NER. A portion of this corpus containing 200K wordforms has been manually annotated with the four NE tags namely, Person, Loca-tion, Organization and Miscellaneous . We have also used the annotated corpus of 122K wordforms, collected from the IJCNLP-08 workshop on Named Entity Recog-nition on South and South East Asian Languages (NERSSEAL) data. This data was agriculture and scientific domains. We consider only those tags that represent person, location, organization and miscellaneous names (NEA [Abbreviation], NEN [num-ber], NEM [Measurement] and NETI [Time] ar e considered to belo ng to the miscella-further divided into the following forms: 
B-XXX: Beginning of a multiword NE, I-XXX: Internal of a multiword NE con-XXX  X  PER/LOC/ORG/MISC. For example, the name sachin ramesh tendulkar is tagged as sachin /B-PER ramesh /I-PER tendulkar /E-PER. The single word NE is tagged as, PER: Person name, LOC: Location name, ORG: Organization name and MISC: Miscellaneous name. In the final output, sixteen NE tags are directly mapped to the four NE tags. Incorporating diverse features in an HMM-based NE tagger is difficult and compli-cates the smoothing typically used in such taggers. In contrast, CRF or SVM based method can deal with the diverse and morphologically complex features of the Indian languages. 
In this paper, we have used CRF and SVM frameworks in order to identify NEs from a Bengali text and to classify them into Person, Location, Organization and Miscellaneous . Lexical context patterns, second best tag of CRF and class decomposi-tion technique of SVM have been used in order to improve the performance of each of the classifiers. Finally, we have combined these post-processed models with the help of three weighted voting techniques. We have used the C ++ based CRF++ package (http://crfpp.sourceforge.net) for NER. Support Vector Machines (SVMs) have advantages over conventional statistical learn-ing algorithms, such as Decision Tree, HMM, ME in terms of its high generalization performance independent of feature vector dimension and its ability of learning with all combinations of given features without increasing computational complexity by intro-ducing the kernel function . We have used YamCha ( http://chasen-org/~taku/ software/yamcha) toolkit, an SVM based tool for detecting classes in documents and formulating the NER task as a sequential labeling problem. Here, we conducted several experiments with the different degrees of the polynomial kernel function. We have used TinySVM-0.07 (http://cl.aist-nara.ac.jp/~taku-ku/software/TinySVM) classifier that seems to be the best optimized among publicly available SVM toolkits. 3.1 Named Entity Features Experiments have been carried out in order to find out the most suitable features for NER in Bengali. Following are the details of the set of features that have been applied to the NER task:  X  Context words: Preceding and following words of a particular word. This is based on the observation that the surrounding word s are very effective in the identification of NEs.  X 
NEs. A fixed length (say, n ) word suffix/prefix of the current and/or the surrounding word(s) can be treated as feature(s). If the length of the corresponding word is less than or equal to n-1 then the feature values are not defined and denoted by ND. The feature value is also not defined (ND) if the token itself is a punctuation symbol or contains any special symbol or digit. Another way to use the suffix information is to modify the feature as binary valued. Variable length suffixes of a word can be matched with predefined lists of useful suffixes (e.g., -babu , -da , -di etc. for persons highly inflective Indian languages like Bengali. The underlying reason is that NEs contain some common prefixes and/or suffixes.  X  Named Entity Information (dynamic feature): NE tag(s) of the previous word(s). 
The NE tag(s) of the previous word(s) is (a re) very effective in deciding the NE tag of the current word.  X  tence or not. Though Bengali is a relatively free order language, the first position of the sentence often contains the NE. Bengali grammar follows the SOV (Subject-
Object-Verb) structure and maximum of the NE(s) generally do appear in the sub-ject position. Also, we have used the newspaper corpus and NEs are generally found in the starting positions of the sentences in the news documents.  X  Length of the word (binary valued): Whether the length of the word is less than three or not. This is based on the observation that very short words are rarely NEs.  X  consider the infrequent words in the training corpus. The intuition of using this fea-ture is that frequently occurring words are rarely NEs.  X  Digit features: Several digit features have been considered depending upon the pres-ence and/or the number of digit(s) in a token (e.g., ContainsDigit, FourDigit, 
TwoDigit), combination of digits and punctuation symbols (e.g., ContainsDigi-tAndComma, ConatainsDigitAndPeriod), combination of digits and symbols (e.g., ContainsDigitAndSlash, ContainsDigitAndHyphen, ContainsDigitAndPercentage). 
These binary valued features are helpful in recognizing miscellaneous NEs such as time expressions, monetary expressions, date expressions, percentages, numbers etc.  X  Position of the word (binary valued): Po sition of the word in a sentence is a good feature is used to check whether the word is the last word in the sentence.  X  Part of Speech (POS) Information: We ha ve used a CRF-based POS tagger [17] that was developed with 26 POS tags, defined for the Indian languages.  X  Gazetteer Lists: Gazetteer lists, developed from the Bengali news corpus [16], have been used as the features in each of the classifiers. If the current token is in a par-ticular list, then the corresponding feature is set to 1 for the current and/or the sur-used in the present work along with the number of entries in each gazetteer: (1). Organization clue word (e.g., kong, limited etc): 94, Person prefixes (e.g., sriman, (2). Common word (521 entries): Most of the Indian languages NEs appear in the (3). Lexicon (128,000 entries): We have developed a lexicon [18] from the Bengali For English NER, a context pattern induction method through successive learners has been reported in [19]. Here, we have developed a method for generating lexical con-text patterns from a portion of the unlabeled Bengali news corpus [16]. These are used classifiers. For this purpose, we have used an unlabeled corpus of 10 million word-forms of the Bengali news corpus [16]. Given a small seed examples and an unlabeled corpus, the algorithm can generate the lexical context patterns in a bootstrapping example for other NE classes and error example for non-NEs. Out of 200K wordforms, 150K wordforms along with the IJCNLP-08 shared task data have been used for training the models and the rest 50K wordforms have been used as the development data. The system has been tested with a gold standard test set of 35K wordforms. Statistics of the training, development and test sets are given in Table 1. 
A number of experiments have been carried out taking the different combinations of the available words, context, orthographic word level features and gazetteers to # of sentences 21,340 3,367 2,501 #of wordforms 272,000 50,000 35,000 #of NEs 22,488 3,665 3,178 #Average length of NE 1.5138 1.6341 1.6202 Bengali. These are defined as the baseline models. Results of each model along with the best combination of the feature are shown in Table 2. 
Results demonstrate that the SVM based baseline system performs best with the f-score value of 76.3% followed by CRF (f-score=75.71%). During all the experiments, we have observed that word context, prefixes, suffixes, POS information, dynamic formance of the NER system. We have conducted various experiments with the sev-eral degrees of the polynomial kernel function and observed the highest performance with degree 2. The use of gazetteers increases the performance by 4.11%, and 4.45% in the f-score value in the CRF, and SVM, respectively. 5.1 Use of Context Patterns as Features The high frequency patterns of the Accept Pattern set (discussed in section 4) are used as the features of the classifiers. A feature  X  X ontextInformation X  is defined by observ-current word in the following way: Results of the models that include this feature are presented in Table 3. The use of this feature along with the previously discussed features increase the overall f-score values to 82.69%, and 83.39% in the CRF, and SVM based systems, respectively. Thus, these are the improvement of 2.87%, and 2.64% in the f-score values with the use of context features. Model Recall (in %) Precision (in %) F-Score (in %) 5.2 Second Best Tag for CRF If the best tag given by CRF is  X  X NE X  (other than NE) and the confidence of the second best tag is greater than a particular threshold value then the second best tag is choice of the threshold value. Evaluation results of the system have shown the im-second tag decreases the precision values by 2.44%. This resulted in the overall per-formance improvement in terms of f-score value (by 2.29% ). 5.3 Class Decomposition Technique for SVM SVM predicts the class depending upon the labeled word examples only. If target classes are equally distributed, the pairwise method can reduce the training cost. Here, we have a very unlabeled class distribution with a large number of samples belonging one-vs-rest strategy. One solution to this unbalanced class distribution is to decom-pose the  X  X NE X  class into several subclasses effectively. Here, we have decomposed the  X  X NE X  class according to the POS inform ation of the word. That is, given a POS tagset POS , we produce new | POS | classes,  X  X NE-C X  X C  X  POS . So, we have 26 sub-classes which correspond to non-NE regions such as  X  X NE-NN X  (common noun),  X  X NE-VFM X  (verb finite main) etc. Results have shown the recall, decomposition technique is included in the SVM. 5.4 Voting Voting scheme becomes effective in order to improve the overall performance of any system. Here, we have combined the models using three different weighted voting techniques. In the literature [20][21], it has been shown that the voted system per-forms better than any single machine learning based system. In our experiments, in order to obtain higher performance, we have applied weighted voting to the three systems. But before applying weighted votin g, we need to decide the weights to be given to the individual system. We can obtain the best weights if we could obtain the accuracy for the  X  X rue X  test data. However, it is impossible to estimate them. Thus, we have used following weighting methods in our experiments: 5.4.1 Uniform Weights (Majority voting) We have assigned the same voting weight to all the systems. The combined system selects the classifications, which are proposed by the majority of the models based system is selected. 5.4.2 Uniform Weights (Majority voting) We have assigned the same voting weight to all the systems. The combined system selects the classifications, which are proposed by the majority of the models based system is selected. 5.4.3 Cross Validation Precision Values The training data is divided into N portions. We employ the training by using N-1 iteration, we have evaluated the individual system following the similar methodology, i.e., by including the various gazetteers, context features, second best tag and the class decomposition technique. At the end, we get N precision values for each of the system. Final voting weight for a system is given by the average of these N precision values. Here, we have considered the value of N to be 10. We have defined two different types of weights depending on the cross validation precision as follows: (a). Total Precision: In the first method, we have assigned the overall average preci-(b). Tag Precision: In the second method, we have assigned the average precision Experimental results of the voted system are presented in Table 4. Evaluation results show that the system achieves the highest performance for the voting scheme  X  X ag Precision X , which considers the individual tag precision value as the weight of the corresponding system. Voting shows an overall improvement of 5.23% over the CRF based system and 4.08% over the SVM based system in terms of f-score values. Voting Scheme Recall (in %) Precision (in %) F-Score (in %) 5.5 Results on the Test Set Three models have been tested with a gold standard test set of 35K wordforms. Ap-models have shown the f-score values of 74.56%, and 75.67% in the CRF and SVM models, respectively. Results have demonstrated the improvement in f-scores by 7.68%, and 9.52% in the CRF, and SVM models, respectively, by including the gazet-teers, context features, second best tag of CRF and class decomposition technique of SVM. 
These two systems are then combined together into a final system by using three weighted voting techniques. Experimental results are presented in Table 5. The sys-tem has shown the highest recall, precision, and f-score values of 91.33%, 88.19%, and 89.73% , respectively. Clearly, this is the improvement in performance compared to any individual system. Voting Scheme Recall (in %) Precision (in %) F-Score (in %) 5.6 Comparison with Other NER Systems The most recent existing Bengali NER systems, i.e., HMM based system [9], CRF based system [10] and SVM based system [11] have been trained and tested with the same datasets. Some works in the area of Bengali NER have been reported in the IJCNLP-08 NER shared task. But, comparisons with those works are out of scope because of the following reasons: Evaluation results are presented in Table 6. Results show the effectiveness of the proposed NER model that outperforms the other existing Bengali NER systems by the impressive margins. The system outperforms the HMM based system with 13.45% f-sore, CRF based system with 8.44% f-score and SVM based system with 7.77% f-score. Thus, it has been established that purely statistical approaches cannot always the mixed domains). So, post-processing the output of the statistical framework with reasonably good performance. Results also suggest that combination of several classi-fiers is more effective than any single classifier. In this paper, we have developed a NER system for Bengali by combining the CRF and SVM classifiers using weighted voting approach. Performance of each baseline classifier has been improved by the lexical context patterns generated from an unla-beled corpus of 10 million wordforms, second best tag of CRF and class decomposi-tion technique of SVM. The proposed approach shows the effectiveness with the reasonably high recall, precision and f-score values. We have shown that the reported system outperforms the other existing Bengali NER systems. This approach can be applicable for NER in other languages, especially for the Indian languages. Future works include investigating how the proposed approach has its effects on each of the NE classes. Automatic term extraction (ATE) has gained the interest of many researchers and has applications in many kinds of NLP tasks, such as Information Retrieval, Information Extraction, and Automatic Domain Ontology Construction in the last few years. There tive-noun or noun-noun sequences to improve precision by ranking their candidates [1]. These methods are limited by the experience of the specialists who manually select the grammatical patterns. 
Many researches on ATE focus on methods that are based on statistics. The etc. are widely used. Mutual Information and log-likelihood measure the unithood from the strength of inner unity, and Left/Right Entropy measures the unithood from the strength of marginal variety. 
Some studies rely on a combination of linguistic knowledge and statistical associa-tion measures. Term extraction is modeled as compound extraction and is formulated as a classification problem in [6]. The mutual information, relative frequency and Part-of-speech tags are used as features for compound extraction. 
The C-value measure is used to get more accurate terms, especially those nested terms in [7]. This empirical approach doesn X  X  take domain speciality of term into account, therefore a comparable numbers of frequent constituents, such as  X   X  X  X  X  X   X  (Reference),  X   X  X  X  X  X  X   X (Address) are wrongly recognized. Many re-searches focus on the statistics between a compound term and its component single-terms recently [8] [9] [10] [11], but their me thods seldom consider the fact that terms are prominent key concepts in a subject field. 
Based on the domain-specific prominence, a comparative approach for candidate term ranking by comparing their ranks in a thematic corpus and a background corpus is taken in [12]. 
This paper presents an automatic Chinese multi-word term extraction method based on the unithood and the termhood measure. We test the hypothesis that both the uni-thood and the termhood can measure whether a candidate is a true term. The unithood the lexical unit is used to refer to a specific concept in a specific domain. Taking term component into account to estimate the termhood, we propose two measures of a can-didate term to be a true term: the first measure is based on domain speciality of term, and the second one is based on the similarity between a candidate and a template that contains structured information of terms. Experiments on I.T. domain and Medicine domain show that our method is effective and portable in different domains. 
The rest of the paper is organized as follows. Section 2 briefly introduces the idea of our method. Section 3 describes our term extraction algorithm. Section 4 presents the experiments and analysis. Section 5 is the conclusion. 2.1 Term Component The economical law of term generating suggests that a majority of multi-word terms consist of the much smaller number of the single-word terms [13]. An economical term system tends to improve the ability of combination of the single-word terms. We call those single-words which are part of multi-word terms  X  X erm Component X . Take multi-word term  X   X  X  X  X   X  X  X  X   X   X (Computer Control System) as an example, all of the words  X   X  X  X  X   X (Computer),  X   X  X  X   X (Control) and  X   X   X   X (System) are term components. From linguistic perspective, a multi-word term can be represented by features of term components. 2.2 The Brief Idea of Termhood Measure Based on Term Component A term component makes up multi-word terms more contributively when it is domain specific. So, we can make the judgment about how likely it is that a term candidate will be a true term by measuring the term components. When a new multi-word is gener-domain-specific and have strong ability to make up multi-word terms. The more term components are as part of a candidate, the more likely the candidate to be a true term.  X  The contribution of domain-specific term components to a true term When a term component appears frequently in a domain D , and seldom appears in other domains, the term component is prominently domain-specific. Meanwhile, the candidates that are made up by those term components are domain-specific. For ex-ample, the term component  X   X  X  X  X   X (Register) appears frequently in IT domain, so the candidates, such as  X   X  X  X   X  X  X  X   X (General Register),  X   X  X  X   X  X  X  X   X (Flag Regis-ter) have strong domain speciality. By comparing ranks of term components in a the-matic corpus and a general balanced corpus , we measure how domain-specific are the term components. Furthermore, it is a measure of the termhood of a candidate term.  X 
The contribution of term components th at contain structu re information to a true term the term component tends to appear on the same position of many multi-word terms with a fixed length, a candidate is very likely to be a true term when the term compo-nent appears on the same position of the candidate. For example, the term component  X   X  X  X   X (Digital) often appears on the first position of many terms, such as  X   X  X  X   X  X  X   X  X  X   X (Digital Logic Circuits) and  X   X  X  X   X  X  X   X   X   X (Digital Electronics). Then for a candidate  X   X  X  X   X  X  X   X  X  X   X (Digital Signal Processing), the first word of the candi-date is  X   X  X  X   X (Digital), the candidate is very likely to be a true term. So, there is an-other measure of the termhood of a candidate by using the structure information of term components. 
The above two measures of the termhood consider the relation between term com-ponents and multi-word terms, and effectively estimate the domain speciality of candidates. 2.3 The Architecture of the ATE Process The automatic term extraction based on term component contains four major modules, namely Corpus Preprocessing, Unithood Measuring, Term Component Weight Measur-ing and Termhood Measuring. The preprocessing module includes word segmentation and n-grams frequency count [14]. The unithood measuring module calculates the uni-thood of n-grams, and obtain candidate word strings whose unithood are above selected threshold. The term component weight measuring module extracts term components from terminology dictionary, and calculates the weight of domain speciality and struc-ture template of term components. The termhood measuring module calculates the termhood of all candidate terms based on the term components. By a combination of unithood and termhood of candidates, score of all the candidates are measured. 
The architecture of the ATE process is shown in Figure 1. 3.1 The Unithood Measurement The unithood of n-gram is measured on the strength of inner unity and marginal vari-ety [15]. After word segmentation and n-gram frequency counting, the unithood of all n-grams is calculated. The n-grams whose unithood are above a threshold will be selected as candidates. and there are n-1 ways to split. On the measure of inner unity, we calculate the mutual minimum as the mutual information of the n-gram, as the Formula 1 shows. On the measure of marginal variety, we calculate the left entropy and right entropy of the n-gram, and the minimum will be the measure of marginal variety, as Formula 2 and Formula 3 show. The unithood of n-gram is measured by Formula 5, which combines the strength of inner unity and marginal variety. e of xy , | xy | is the frequency of xy in corpus C . 3.2 The Termhood Measurement Based on Term Component The measure of termhood of candidate is based on the domain speciality and structure information of term component. So, we first extract all the term components from the terminological dictionary. Then we calculate the weight of domain speciality and structure template of all the term components. 3.2.1 The Domain Speciality Measure on Termhood The domain speciality of multi-word terms can be measured by the domain speciality of term component. We rank the term components in a thematic corpus and a general balanced corpus by their frequency in th e corpus. A term component is more domain-specific when it ranks much higher in the thematic corpus than in the general bal-anced corpus [12]. 
A term candidate is very likely to be a true term when the components of the can-didate have prominent domain speciality. This paper applies the domain speciality measure ( DS ) as shown in Formula 6 and Formula 7. candidate contains. 3.2.2 The Structure Information Template Measure on Termhood  X  The structure information template of term component The multi-word terms are made up with term components; a term component tends to appear on a fixed position of a multi-word term. This paper makes a statistical analysis ( S(C i ) ) for every term component. Table 1 lists several template and some instances.  X  The method of template matching A structure information template of term component has four parts: the term compo-nent, the number of words that are on the left of the component (as Left ), the number of words that are on the right of the component (as Right ), and the weight of the tem-plate (frequency of the template in the terminological dictionary). We can measure the weight of a candidate term by template matching, but the templates which are ex-information of term components. So, we use a template matching method that is based on the similarity between a candidate and the templates to overcome data sparseness. For a candidate term CT = C 1 ...C i ...C n , we sign the number of words on the left of C as Left X  , and the number of words on the right of C i as Right X  . Formula 8 defines a variable called Edit dist . The similarity [16] is defined by Formula 8. 
Term component Template Weight Example  X  X  X  X  (Drive) W+  X  X  X  X  (Drive) 68  X  X   X  X  X  X  (Floppy Drive)  X   X  X  X  X  (Register) W+W+  X  X  X  X   X  X  X  (Digital)  X  X  X  Where T is a template, Sim(T) is the similarity between CT = C 1 ...C i ...C n and T .  X  The measure of termhood of candidate term The algorithm of measuring termhood of a candidate based the structure template ( ST ) is shown as below: Input  X  Candidate term ( CT ); Length of the CT ( Len ) Output  X  ST(CT) Method  X  ST(CT) = 1; This section presents the evaluation of the algorithm. To evaluate the performance of our method, experiments on IT domain and Medicine domain are conducted. 4.1 Data Sets In I.T. domain, we use the  X  X ncyclopedi a of Computer Science and Technology X  (ECST) which contains about 2,709K Chinese text to extract multi-word terms. To extract term components of I.T. domain, we use a terminological dictionary which contains about 141,760 terms from the Institute of Computational Linguistics, Peking University. When measuring the domain speciality of term component, the  X  X inhua Net X  corpus which contains about 80M Chinese text is used as the thematic corpus, and a similar-sized I.T. journal corpus is used as the general balanced corpus. To evaluate the portability of our method, we conducted an experiment on the Medicine domain. The corpus we use to ex tract terms is the  X  X irculatory System Diseases X  section of the book  X  X nternal Medicine X  (IM) that contains about 405K Chinese text. The term components of Medicine domain is extracted from  X  X ESH X  and  X  X odern Medicine X  from the Encyclopedia of China Publishing House. We also use the  X  X inhua Net X  corpus as the thema tic corpus, and use a similar-sized  X  X odern Medicine X  corpus for measure of domain speciality of term components. 
We conduct comparing experiments to compare the performance of our method with the method of C-value [7] and the method only using unithood. 
The performance is evaluated in terms of precision, recall and F-measure. To evaluate the precision, we select a sample of 100 terms from N terms, where N is the total number of extracted terms. Then we manually check the 100 terms. Recall is hard to evaluate in ATE, we measure the r ecall of I.T. domain by measuring the recall the recall of Medicine domain is measured by the recall of the 475 terms that appear on the  X  X ndex X  section of the  X  X M X . 4.2 Experiments on DS and ST Method In order to evaluate the two measures on termhood: the domain speciality measure experiments on I.T. and Medicine domain. We build a hybrid method taking both two methods above. Formula 10 shows the hybrid approach: the structure template measure. The result of three approaches is shown in Table 2 and Table 3. 
Table 2 and Table 3 show that, with the increment of number of ranked terms, we get better recall, though the precision drops, the F-measure improves much. When we select the 30,000 top ranked terms, we get a best performance using the hybrid method with an F-measure of 54.3% in I.T. domain. By using the ST method, we get the best performance with an F-measure of 48.0% in Medicine domain when we se-lect the 3,000 top ranked terms. 4.3 Comparing Experiments on Unithood, C-Value and the Hybrid In order to compare the performance of hybrid method, unithood method and the C-value method, two sets of experiments are conducted in I.T. domain and Medicine Domain. The unithood method is the algorithm described in section 3. The C-value [7] is measured as Formula 11. Where a is the candidate string, | a | is the number of words in string a , f(a) is the fre-that contain c , b i is the candidate extracted terms that contain a, c(a) is the number of those candidate terms. 
Table 4 and Table 5 show that the hybrid approach improves performance of uni-thood and c-value notably. The improvement in F-measure is about 10%, and in pre-cision is above 25%. The primary improvement is in precision, due to the measure of domain speciality and structure information template of term component. 1  X  X  X  X  2  X  X  X  X  (Signal-3  X  X  X  (Clause) 8  X  X  X  (Click) 13  X   X  (Lattice) 18  X   X  (Frequency 4  X  X  X  5  X  X  X  (Default) 10  X  X  X  (Track) 15  X   X  (Adnex) 20  X  X  X  X  Template Instance Template Instance 1 W+  X  X  X  2  X  X  X  3 W+W+  X   X  4 W+  X  X  X  5  X  X  X  4.4 Term Components and St ructure Information Templates In addition to extracting terms from corpus, we also build the resource of term compo-nents and structure templates, which may be useful in further ATE methods. Table 6, 7 list some of the top term components and templates. 
We extract 20,000 term components with 63,000 structure information templates from the I.T. terminological dictionary, and 12,000 term components with 40,000 structure information templates from the Medicine terminological dictionary. This paper proposes an automatic Chinese multi-word term extraction method based on the unithood and the termhood measure. Term components that are extracted from terminological dictionary are taken into account to estimate the termhood. We pro-pose two measures of a candidate term to be a true term: the first measure is based on domain speciality of term component, and the second one is based on the similarity between a candidate and a template that contains structured information of terms. By a combination of unithood and termhood of candidates, score of all the candidates are measured. 
Experiments on I.T. domain and Medicine domain show that our method performs better than the unithood method and the C-value method both in precision and recall. The result of experiments on two domains presents that our method is portable in different domains. Acknowledgments. This work is supported by NSFC Project 60503071, 60873156, 863 High Technology Project of China 2006AA01Z144 and 973 Natural Basic Research Program of China 2004CB318102. In recent years, the development of t echnologies makes the communication more convenient and freer. The Internet allows people to publish their own opinions online People always use abbreviations, aliases or some other short and lovely names to replace the original names of some people, some objects, etc. These new names are widely accepted online. We call them nicknames as a whole. The definition of nick-name is in Section 3. 
Nowadays, more and more people can afford automobiles, so they may need in-formation from others. Thus, the public bulletin board systems (BBS) become one of the most important ways for people who already have or who want to purchase a new automobile to communicate. The famous forums in China such as The Home of automobiles 1 , The Pacific Automobile 2 and so on are often divided into some separate sub forums by different kinds of automobiles. Then, if people want to ac-quire information of a specific automobile, they can go to the sub forum of each specific type. 
However, the free style of writing leads a lot of people to call their automobiles by their nicknames. For instance, they call  X   X  X  X  X  X   X (Ma Zi Da Six) as  X   X  X   X  (Ma Six), they call  X   X  X  X  X   X  (Bei Dou Xing) as  X   X  X  X  X   X  (small xing xing), and they call  X   X  X  X  X   X (Fu Ke Si) as  X  X KS X  etc. Thus, we can get the original text, and we can know which automobile the original text describes through the name of the sub fo-using the general methods of named entity recognition. 
Named entities (NE) are broadly distributed in texts, and named entity recogni-tion (NER) plays a significant role in data mining (DM), information extraction (IE), natural language processing (NLP) and many other applications. Previous studies on NER are mainly focused on the recognition of location (LOC), organiza-tion (ORG), name of person (PER), time (TIM), and numeral expressions (NUM) in news domain, or other named entity recognition (NER) such as product named entity recognition in business domain and term acronym named entity recognition in biology domain, etc. 
Admittedly, the named entity recognition is one of the most effective ways to ex-tract information from texts. However, the general NER may not extract the key information effectively for automobiles X  nickname recognition problem. Not only automobiles, other products or some other things have their nicknames. Thus, we decided to do the automobiles X  nickname recognition in Chinese text, and establish a database of automobiles X  nicknames. The database can be used in data mining, infor-mation extraction, search engine, and so on. Finally, we will establish a search engine about the comment and introduction of automobiles. And our algorithm can be ex-tended to other domains which have the same nickname recognition problem. The goal of automobile X  X  nickname recogni tion is to extract the nicknames of the specific automobile from the text. 
Eckhard et al.[1] recognized Danish named entities including product names us-ing constraint grammar, but this method was dependent on the specific grammar highly. 
Zhang et al. [2] used Hidden Markov Model, the semantic role as linguistic feature and the pattern rules as the combinative points to do Chinese named entity recognition and Zhang et al. [3] used role model to do the Chinese named entity recognition. These methods perform excellently on general NER, but not proper for nickname recognition problem. 
Chang et al .[4] presented an algorithm using a statistical learning method to match short/long forms. Sun et al. [5] presented an algorithm for Chinese abbreviations acronym recognition problem. These methods are not focus on the traditional NER, but also not proper for our problem. Wentian Company et al.[7] established a search engine of automobile X  X  Chinese comments, however, it only accepts the original automobiles X  names as keywords. from the passage. 
A nickname is a name of an entity which is used to replace the original name. A nickname is sometimes considered desirabl e, symbolizing a form of acceptance, but stage name, and also from a title (for example, City of Fountains), although there may be overlap in these concepts. 3 The automobiles X  nicknames always mean that people have a close emotional bond to their autom obiles or just abbreviations because the original names would be too long to type. 
In our research, we defined an automobile X  X  nickname by three parts, and the three parts constitute five different patterns. 
The three parts are: prefix, main, postfix, which have been described in the follow-ing list Table 1. 
The five patterns are: prefix + main, prefix + main + postfix, prefix + postfix, main + postfix, main, which have been described in the following list Table 2. Parts Description Examples Prefix(Pre) Main(M) Postfix(Post) According to Table 1, we define the patterns of the automobiles X  nicknames as below: Patterns Examples 
Main 
Prefix + Main 
Prefix+Main+Postfix
Prefix + Postfix  X 
Main + Postfix  X 
To avoid ambiguity, we stipulate some additional conditions: 1. The type of a specific automobile is a part of the nickname. For instance,  X  X Q 6 X  will be tagged as  X  X Q(M) 6(post) X , but will not be tagged as  X  X Q(M) 6(O) X . 2. We have learned from about 4,000 ni cknames, and find that people accept the nickname, but they do not accept words which constitute by two or more characters. For instance,  X   X   X  X  X   X  (Small blue lion, which refers to the car Peugeot) will be tagged as  X   X  (pre)  X  (pre)  X  (M) X  [Small (pre) blue(pre) lion (M)];  X   X  X  X  X  X  X  X   X  (Silver gray little lion zi) will be tagged as  X   X  (O)  X  (O)  X  (pre)  X  (M)  X  (post) X  [Silver(O) gray(O) small(pre) lion(M) zi(post)]. Even though X   X  X  X   X (Silver gray) is a word describes the color of the automobile, but it is not formed by a single Chinese character. Here is a passage which comes from the famous Chinese Automobiles X  BBS,  X  X he Home of Automobile X . The passage is from the sub forum  X   X  X  X   X . The content of this passage is that:  X   X  X  X  X  X  X  X  X   X  X  X   X  X  X  X  X  X  X  X  X  X  X  X   X  X  X  X  X  X  X  X  X   X  (I want to show my small grey lion zi. I bought my Little Six one month ago.) Because every-one knows the brand of this automobile is a lion, so the main characters are X   X   X , X   X   X ,  X   X   X . (The Chinese name of the automobile and the lion). So the passage after labeled is that:  X   X  (O)  X  (O)  X  (O)  X  (O)  X  (Pre)  X  (Pre)  X  (M)  X  (Post)  X  X  X  (O)  X  (O)  X  (Pre)  X  (Post)  X  (O)  X  (O)  X  (O)  X   X  (O)  X  (O)  X  (O)  X  (O)  X  (O)  X  (O)  X  (O) X . [I(O) want(O) to(O) show(O) my(O) small(pre) grey(pre) lion(M) zi(post). I(O) bought(O) my(O) Little(pre) Six(post) one(O) month(O) ago(O).] Consider the definition of nickname we defined above, our motivation is making a classification of the characters according to their linguistic features. We designed our algorithm based on Hidden Markov Model, and using Viterbi algorithm to find the best solution. 
We defined the state sequence and the observation sequence of HMM in Section 4.1 and 4.2, describe Viterbi algorithm in Section 4.3, and the whole process in Section 4.4. 4.1 The State Sequence states. Similar to the parts of nickname, the six states are: prefix, main, postfix, other, before, after, which are listed in Table 3 below. We will assign a corresponding state to each character automatically, and then perform NER. States Description Examples Prefix(Pre) Main(M) Postfix(Post) 
Before(B) The neighboring character or word in front of the nicknames  X 
After(A) The neighboring character or word following the nicknames  X  X M X ,  X 
Others(O) The characters do not appear in the nicknames 4.2 The Observation Sequence Since our definition is very different from a POS or other tag set, we decided not to the characters sequence. We have defined the observation values as follows. 
Because different brands of automobiles have different names and different informa-tion, the main characters type is not a fixed set. There may be overlap between the main longs to both Main Characters type and Other Characters type, its type is Main Characters. 
Main Characters 
Single Adjective 
Characters indicate 
Other Characters 4.3 Use Viterbi Algorithm to Find the Proper State Sequence A state set can be viewed as a token tag collection. However, from the point of view of the named entity recognition, a POS tag is always defined by the part-of-speech of a word, while a state in our research is always defined based on linguistic features. A token may represents one or more states, such as  X   X  X  X  X  X  X  X  X  X  X   X (Ma Zi Da Six is an excellent automobile) and  X   X  X  X   X  X  X  X  X  X  X  X  X  X  X   X (My Ma Zi Da have drove six thousands kilometers). The  X   X   X  (six) in the former means the type of the automo- X  X ostfix X  state while it plays the  X  X ther X  state in the latter. 
We have prepared the state set and state co rpus. Let us consider the question, given a token sequence, how can we tag a proper state sequence automatically? We have quences. The algorithm and its calculation are given below: 
After tokenization of original sentences, suppose that T is the token sequence, S is the state sequence for T, and S# is the best choice with the maximum probability. That is, According to the Bayes' Theorem, we can get For a specific token sequence, P(T) is a constant. Thus, based on E1 and E2, we can get E3: Consider the Hidden Markov Model, we may consider S as the state sequence and T as the observation sequence. The statement sequence is hidden behind the observation sequence, and the S is hidden behind T. Thus, we can use Hidden Markov Model to solve this problem. Because of that T is the observation sequence, which is a real sequence we can ob-serve, so P(T) = 1 . error, we decided to use the log probability to replace the original one. That is, Finally, we can use Viterbi algorithm to solve E6 problem and finish the state tagging. 
Next, we will use the sentence  X   X   X  2.0  X  X  X   X (The purple Xuan 2.0 is excellent) which used nickname  X   X   X  2.0 X  (purple Xuan 2.0) to replace the original name  X   X  X   X  the most probable token sequence will be  X  X olor(  X  ) /main(  X  ) /num(2.0) /other(  X  ) another state, the impossible transfer has been eliminated. 
Consider that some state values and observation values do not have any bond, we use P(t i |s i )=- X  to demonstrate them. During the Viterbi Algorithm, we will not choose these states to calculate the next move if we know they are impossible. This im-provement may save a vast amount of time. 
Figure 1 shows how the Viterbi Algorithm works, on one simple sentence. The rows we haven X  X  draw on the figure) means a transition from one state to another state. This is the state transition matrix. 
After the Viterbi algorithm, we could get the optimal tagged sequence which has a maximum value. In this example, the best state sequence is  X   X   X /O  X   X   X  /O  X   X   X /Pre,  X   X   X /M,  X   X   X /O. (Mine/O Purple/Pre Xuan/M, is excellent/O). 4.4 The Work Flow of Chinese Automobile X  X  Nickname Recognition The steps are listed below: 1. Prepare the data. Separate train set and data set. 2. Train the model. 3. Tokenization sentences. 4. Tag token sequences using Viterbi algorithm. Get the optimal state sequence which has the maximum possibility. (It also can get the top 2-3 results as required and to increase the recall rates of right tokens). 5. Nickname recognized after maximum matching with the specific nickname pat-terns, the nickname patterns are shown in Table 2. 5.1 Dataset Even though the nickname is popular and being used and accepted widely, but we have not found any published dataset which contains information in automobiles X  Thus, we have to make the dataset by ourselves. We have chosen the forum named  X  X he home of Automobile X  and  X  X he Pacific Automobile X , collected two thousands passages as passage dataset and two thousands sentences as sentences dataset. We assure that each passage in passages dataset has at least one nickname and each sentence in sentences dataset has one nickname. Then we tagged the parts of nicknames artificially. the sentences have less noise than the passages, because of that the passages include a lot of paragraphs and some paragraphs may not contain any nicknames. We have used other sentences in sentences dataset as the sentences testing set, used the whole 2000 passages as the passages testing set. 5.2 Evaluation Metric We have used precision (P), recall (R) and the F-measure (F) to evaluate the result of and recall. 
That is, In the formulation above,  X  is the relative weight of precision and recall. In our expe-riments, we supposed that precision and recall have equally weight. So we set  X  = 1. 5.3 Experiments the result of precision, recall and F-measure we listed in Table 5 and Table 6 are the average value. 
The result reveals that our algorithm performances a little better on sentences than on the passages, because passages usually have more noises. Such as  X   X  X  X  X  X  X  X  X  X   X  X  X  X  X  X   X  (My Gang Gang had gone to be repaired just now. In Chinese,  X  X ang gang X  means, just now.) The first  X   X  X  X   X  means just now, it is not a nickname while the second X   X  X  X   X  refers to the car  X   X   X   X  (Jin Gang). Training Set 500 95.1% 93.2% 0.9414 Test Set by Sentences 1500 93.7% 91.3% 0.9248 Test Set by Passages 2000 92.2% 89.5% 0.9082 Training Set 1000 98.1% 96.2% 0.9714 Test Set by Sentences 1000 96.7% 92.3% 0.9445 Test Set by Passages 2000 95.2% 91.5% 0.9331 
The result also proved that using 1000 sentences as the training data got better per-formance than 500 sentences as the training data. The experiments we made have proved that our methods of Chinese automobiles X  nickname recognition have got a good result. Our algorithm can be used to establish a nickname database. The nickname database can be used to improve other projects on automobiles X  domain such as the named entity recognition, information extraction, data mining, search engine, etc. 
We think that this algorithm could also expand to other domains, not only in auto-mobiles domain. Not only automobiles have nicknames, many objects all have their own nicknames. We can fine tuning our program and the form of nicknames to make our method can be used on nickname recognition in other domains. 
In our future work, we will try to use our theory, model and program on other do-mains. Finally, we hope that our research on nickname recognition could get a general model. This work is supported by the National Grand Fundamental Research 973 Program of China under Grant No.2007CB310900 and Chun-Tsung scholarship, which was es-tablished by Nobel Laureate Professor Tsung-Dao Lee. 
Thanks for the technical support from Su pertool company, especially from Ming-hui Wu, Xuan Zhao, Songtao Chi, Yuqian Kong, Pei Liu, Chaoxu Zhang, Jian Gao, Hao Xu, Bingzheng Wei. We are grateful for the help you give us. Thanks for your nice attitude and thanks for the many hours of discussions. Semantic Role labeling (SRL) was first defined by Gildea etc. [1]. The SRL task is to identify and classify the semantic roles of each predicate in a sentence. The semantic mantic relationship with the related predicate. Typical tags include Agent, Patient, Source, etc and adjuncts such as Temporal, Manner, Extent, etc. Since the arguments can provide useful semantic information, the SRL is crucial to many natural language processing tasks, such as Question and An swering [2] and Machine Translation [3]. With the efforts of many researchers [4, 5], different machine learning methods and linguistics resources are applied to this task, which has made SRL progress fast. 
Compared to that on English, however, the research on Chinese SRL is still in its infancy stage. Previous work on Chinese SRL [6, 7, 8] mainly focused on how to transplant the machine learning methods, which has been successful on English. Sun [6] did the preliminary work on Chinese SRL without any large semantically anno-tated corpus of Chinese. This paper made the first attempt on Chinese SRL and produced promising results. After the PropBank [9, 10] was built, Xue [7] and Xue [8] have produced more complete and systematic research on Chinese SRL. 
The architectures of complete SRL systems for Chinese and English are quite simi-lar. They are mostly the  X  X arsing  X  semantic role identification  X  semantic role classi-fication X  pipeline. The fundamental theory of this architecture is the  X  X inking theory X , in which the mapping relationship between the semantic constituents and the syntactic projected to a syntactic node in the pars ing tree. Accordingly, the whole process the nodes which are recognized as semantic roles are classified into different catego-ries. Unlike the previous  X  X arsing based X  architecture, in this paper, a  X  X emantic chunking X  method is proposed. We use similar technologies as chunking to identify and classify the semantic roles. With this method, the parsing trees are no longer necessary for SRL, which will greatly reduce the time expense, since parsing is quite a time-consuming task. Also, it can improve the performance of Chinese SRL. 
Hacioglu [11] have done the similar work on English with semantic chunking. But their work focused on transplant the widely-used syntactic chunking technology to semantic role labeling. However, only features that are useful to syntactic chunking are not enough for semantic chunking, because the characteristics of the two research objects are different. The results of the experiments had reassured this problem. Be-sides, semantic chunking has more practical use on Chinese, since both parsers and chunkers on Chinese are not easy to find and the performances are not satisfactory. In chunking to build a robust semantic role labeler without parsers. Besides, we will make comparisons of the time consumption between semantic chunking based meth-ods with previous parser based ones. 
The rest of the paper is organized as follows. In section 2, the semantically anno-tated corpus -Chinese Propbank is discussed. The semantic chunking, which is the main part of this paper is described in s ection 3. We then apply this method on hand-crafted word segmentation and POS tagging in section 4. The results of the experi-ments can be found in section 5. Section 6 is the conclusion and future work. The Chinese PropBank has labeled the predicate-argument structures of sentences from the Chinese TreeBank [9]. It is constituted of two parts. One is the labeled data, which indicates the positions of the predicates and its arguments in the Chinese Tree-bank. The other is a dictionary which lists the frames of all the labeled predicates. The following is an example from the PropBank. 
In this sentence,  X  X  X  X  (the Harbor of Ningbo) is the proto-agent of the verb  X  X  X  (utilize), which is labeled as arg0 in PropBank.  X   X  (foreign investments) is the describe the method of the investment utilization, so it is labeled as argM-ADV.  X  X rgM X  shows that this constituent is an argument of modality.  X  X DV X  is a functional tag, which suggests that this constituent is adverbial. the constituents which are the arguments of the predicate and to classify them. There are two types of labels in the PropBank. One is the numbered label which includes arg0, arg1...arg4. The other is argM with functional tags like ADV in the example. Other tags include TMP, which represents the temporal information and EXT, which verb which is quite useful for other NLP tasks. The PropBank is built on the hand-crafted annotations of the parsing tree from the TreeBank, because the arguments are chunks of words which correspond to the nodes build a classifier to filter the nodes in the parsing trees. However, since the purpose of SRL is just to determine which chunks of words are verb arguments, we can also view SRL as a sequence labeling problem, which leads to the idea of semantic chunking. Semantic chunking is named because of its similarity with syntactic chunking. It is used to identify the  X  X emantic chunks X  in a sentence. Some people use the term to express the idea of syntactic chunking exploiting semantic information [12]. How-semantic chunks. In other words, the semantic chunks are verb-dependent. With dif-ferent target verbs, same sentences will have different semantic chunks. The process of semantic chunking is: firstly, we make the same number of copies of the sentence as the number of predicates in it, and for each copy, there is only one predicate which will be the target. Secondly, we use the chunking method to identify the semantic chunks of the target predicate. Lastly, we assign different tags to the semantic chunks we have identified in the second step. Take the following sentence as an example. (1) China construct five new insurance companies There are two predicates in this sentence: the verb  X   X  X  X   X (construct) and the adjective  X   X   X (new). So firstly, the sentence is copied twice, either of which has only one target predicate, like (1a) and (1b). Then for the sentence (1a) and (1b), the semantic chunks are identified and classified respectively. In (1a), there are two semantic chunks of the verb  X   X  X  X   X (construct). That is  X   X  X   X , the arg0 and  X   X  X  X  X   X  X  X  X   X (five insurance companies), the arg1. However in (1b), there is only one chunk. That is  X   X   X  X  X  X   X (insurance companies), and (1b), have no relationship with each other in the last two stages of semantic chunking. 
Through the first step of semantic chunking, we solve the problem that arguments of different verbs in the same sentence might overlap, like  X   X  X  X  X   X  X  X  X   X  (five insurance companies) in (1a) and  X   X   X  X  X  X   X  (insurance companies) in (1b). 
We call the second step the process of semantic chunk identification, and the third step, the process of semantic chunk classification. The semantic chunk identification and classification are done separately instead of simultaneously. 3.1 Semantic Chunk Identification From the view of functionality, semantic chunk identification (SCI) is equal to the integration of parsing and semantic role identification. The purpose of SCI is to iden-tify which part of the sentence is the potential argument of the target predicate in this sentence. 
The semantic chunk identification is also a sequence labeling problem. The first sequence we shall choose. There are some widely-used representation methods, like IOB1, IOB2, IOE1, IOE2 and start/end. For this paper, we used all these representa-tion strategies to find out which one will work better. 
IOB1, IOB2, IOE1, IOE2 belongs to the inside/outside representation. IOB1 was first introduced by Ramshaw [13], and the other three are by Sang [14].  X  X  X  indicates that the current word is inside a chunk and  X  X  X  indicates that it X  X  outside the chunk. start or end of the chunk. 
IOB1  X  X  X  is used when the current word is a start of a chunk which is immediately chunk. IOE1  X  X  X  is used when the current word is an end of a chunk which is imme-diately followed by another. IOE2  X  X  X  is used any time when the current word is an end of a chunk. However, start/end is quite a different representation, and it was first used for Japanese Named Entity Recognition by Uchimoto 3 [15]. There are five signs in start/end representation, which are: B, I, E, O and S. The uses of these signs are: 
B the current word is the start of a chunk. I the current word is in the middle of a chunk. E the current word is an end of a chunk. O the current word is outside a chunk. S the current word is a chunk with only 1 word. 
From the definitions, we can find that the start/end is the most complex one. 3.2 Semantic Chunk Classification Semantic chunk classification is quite like semantic role classification. In this stage, the semantic chunks which have been identified previously should be classi-argM+(functional tags) are assigned to the semantic chunks. However, because we didn X  X  get the parsing tree from the previous stages, many features that are related its place. In this section, we present a SRL system based on hand-crafted word-segmentation and POS tagging. We removed the parsing information from the Chinese TreeBank and only left the word segmentation and POS information. Since the word segmentation and POS tagging are quite challenging tasks in Chi-nese, we omit these to focus on the SRL task and make the comparisons with other related systems fair. 4.1 Data We use Chinese PropBank 1.0 (LDC number: LDC2005T234) in our experiments. PropBank 1.0 includes the annotations for files chtb_001.fid to chtb_931.fid, or the first 250K words of the Chinese TreeBank 5.1. For the experiments, the data of Prop-Bank is divided into three parts. 648 files (from chtb_081 to chtb_899.fid) are used as chtb_080.fid. The test set includes 72 files, which are chtb_001 to chtb_041, and chtb_900 to chtb_931. We use the same data setting with that in [8]. 4.2 Classifier In our SRL system, we use the CRF++ 5 toolkit as the classifier, since the CRFs have proven effective on the sequence labeling problem [17]. 
The CRF++ toolkit uses the LBFGS training algorithm which converges much guished two kinds of feature templates: the unigram template and the bigram tem-bigram template can generate more feature functions which will improve the perform-ance but increase the time and space expense. In our system, we take advantage of both to achieve a balance. 4.3 Features for Semantic Chunk Identification sented. With these templates, we can extract thousands of features. Unigram Feature Templates: TargetPredicate The target predicate in the sentence 
Distance0 The distance between the current word and the target Word-1(0, 1) The previous (current, following) word POS-1(0, 1) The previous (current, following) word X  X  POS 
TargetOrNot-1(0, 1) Whether or not the previous (current, following) word is 
Begin0, Middle0, End0 The current word and the target predicate divide a Begin-1, Middle-1, End-1
Begin1, Middle1, End1 The simplification form of the three parts generated by 
PunctsInMiddle0 The number of punctuations (comma, period, semicolon, 
VerbsInMiddle0 The number of verbs in the middle part between the Bigram Feature Templates:
Word-1/Word0/Word1 The combination of Word-1, Word0 and Word1. Slash  X / X  
Verb-1/Verb0/Verb1 Verb-1 (0,1) feature will be the previous(current, VerbPOS-1/ VerbPOS0/VerbPOS1 VerbVector-1/ VerbVector0/ VerbVector1 Rhythm-1/Rhythm0/ Rhythm1 SemanticCategory0/ TargetPredicate
Output-1/Output0 The combination of the previous output and the current Distance0/ TargetPredicate VerbsInMiddle0/ TargetPredicate Word0/POS0/ TargetPredicate VerbVector0/ TargetPredicate Word0/Distance0/ 
POS0/TargetOrNot0 4.4 Features for Semantic Chunk Classification The features used for semantic chunk classification are listed below. They are parser based methods. Since the semantic chunking method doesn X  X  depend on parsing, the parsing-dependent features like  X  X ath X  cannot be used. We must find some new ones. Bigram Feature Templates: FirstWord The first word of a chunk.
 FirstWordPOS The POS of FirstWord LastWord The last word of a chunk LastWordPOS The POS of LastWord PreviousWord The word just before a chunk.
 PreviousWordPOS The POS of PreviousWord.
 FollowingWord The word just after a chunk.
 FollowingWordPOS The POS of FollowingWord.
 Length The length of a chunk Distance The distance between the chunk and the target predicate BeforeorAfter Whether the chunk is before or after the target predicate
VerbSemcat The semantic category of the target predicate, which is LastWordSemcat The semantic category of LastWord TargetPredicate The target predicate
MiddleWords The words in the middle of a chunk. Start and end are not MiddlePOS The POS of the MiddleWords
SimpleMiddle0 It is the same with SimpleMiddle0 in the process of semantic Frames The target predicate may have several frames in PropBank. AllFrames A string consisting of all the frames of the target predicate. Output-1/Output The combination of previous and current output Unigram Feature Templates: FirstWord/LastWord PreviousWord/Prev iousWordPOS The results of semantic chunking based SRL on hand-crafted word segmentation and POS tagging are presented in Table 3. SCI and SCC represent respectively semantic chunk identification and classification. We have used five different representations in our experiments. From the results, we find that start/end performs the best among all the representations. For the in/out representation, results on IOB2/IOE2 are better than those on IOB1/IOE1. Besides, the performance of semantic chunk classification parser-based methods. The highest precision is 94.25%, compared to 94.1% in [8]. 
Specifically on the labeled precision of semantic roles of different lengths, we will find that the performance decreases with the growth of length, namely, the number of the words contained in the semantic roles. Table 4 shows the amount, labeled recall, precision and f-value of arguments that have 1 to 9, and 10 or more words. From the table, we can see that the performances decrease sharply if the lengths of the semantic roles are below 8. However, this trend ends when the lengths are 9 and above. 
More intuitive impressions can be acquired in Figure 2, which reveals the more de-however, from 9 to 20 and above, the F-values are quite stable at around 60%. 
To prove that our method is effective, we make a comparison between our system same data setting as ours. The results are presented in Table 5. 
In this table, we can find that the recall and f-value of our system are higher while the precision is lower. This probably can be explained with the use of parser. Because of the parsing errors, a semantic role sometimes can X  X  correspond to a constituent in a parsing tree, so subsequently; it will never be recognized correctly, which will lower parsing tree, the rich information provided by the parsing tree will be of great help to identify it and assign the right tag, which would be the reason for the higher precision. 
As we mentioned before, the greatest difference between the semantic chunking based methods and parsing based methods is the elimination of the parsing stage, which is a greatly time-consuming task. The comparison of the time consumption on shown in table 6. We build a simple parsing based SRL system, in which we use the Collins parser. The Collins parser is trained with the model1 presented by Collins [19] on the training set. We also used the head rules for Chinese used by Sun [6]. There are 922 sentences in the test set, and the average length of the sentences in the test set is 26.8 words. The experiment running environment is a server with Pentium Dual-Core 2.8GHz CPU and 2 gigabytes memory. 
In table 6, we can find the semantic chunking based method has indeed decreased the time expense sharply, from hours to seconds. It X  X  easy to understand because the time complexity of the algorithm used in Collins parser is O(n 5 ), which is far behind that of the decoding algorithm of CRFs, whose time complexity is only O(n), and n represents the length of the sentence. And the time gap will increase greatly with the growth of the sentence length. Not only the Collins parser, but other parsers will also parsing process increase very fast in amount with the growth of the sentence length. In this paper, we have built a completely new SRL system on the Chinese PropBank corpus based on semantic chunking. Instead of classifying the constituents in a pars-labeled precision, with the novel method we have presented in this paper, we can greatly reduce the time expense, most of which is consumed during the parsing stage in traditional strategies. Less time consumption is useful especially for large corpora, like the millions of pages from the web. 
Previous Research [4, 5] has shown that the great limitation of semantic role label-more prominent. The performance of Chinese SRL on automatic parsing is much worse than that of English while the performances of the two languages through hand-crafted parsing are comparable [8]. The semantic chunking based method has Chinese that lacks efficient parsers. In the future, we will explore more features which are related with the SRL task. Other NLP tasks like the named entity recognition, syntactic chunking and more lan-guage resources can provide helpful information. Besides the introduction of more linguistic knowledge, it X  X  interesting to consider how to integrate the semantic chunk-ing based and parsing based methods to build a SRL system which has rich syntactic information and consumes less time. Acknowledgments. This work is supported by National Natural Science Foundation of China under Grant No. 60303003 and National Social Science Foundation of China under Grant No. 06BYY048. The importance of automatic evaluation cannot be overstated. It is necessary to examine translation quality in developing a machine translation system. An automatic evaluation method performed more efficiently than a manual evaluation method. Owing to the advent of automatic evaluation method, we can easily check translation quality. 
Even though there have been proposed various evaluation methods, we can classify automatic evaluation methods roughly into two types. One compares machine-generated translations (MTs) with human-generated translations (HTs), i.e., BLEU [5], NIST [6], and METEOR [7]. Under this method, multiple reference translations would yield better evaluation results. The other method evaluates quality of MTs without reference HTs. Comparing with the former method, the latter method is a cheaper method, because there is no need to prepare reference HTs when evaluating MTs. This method evaluates MT quality by analyzing linguistic properties of MTs, i.e., classification-based approaches [1, 2, 3, 4]. This method consists of the following constructing a classifier using machine learning algorithms based on the features, and (iii) classifying test MT data either into MT-like or HT-like translation. Even though this method requires HTs in constructing a classifier, no more reference HTs are re-quired when evaluating MTs. Regarding (multiple) reference HTs, this method is cheaper and plausible for evaluation of system improvement. 
In this paper, we adopted the classification-based evaluation method for English-to-Japanese MT systems. Our classifier was constructed by machine learning algo-rithms, i.e., Support Vector Machines (SVMs). This classifier examines the degree of literal translation of MTs and HTs, and determines MTs as MT-like or HT-like trans-decrease suggests that a classification method can evaluate the difference of transla-tion quality between a pre-improved MT system and the improved system, as noted by a previous study [1]. This is because  X  X ood X  MTs are indistinguishable from HTs. A  X  X ood X  MT sentence should be incorrectly classified into an HT sentence. The previous study [1], however, has not yet assessed their classification method from this viewpoint. 
We examined whether the classification accuracy of our proposed method would method presented statistically significant decrease in the classification accuracy de-pending on the fluency of translation in four degrees. This dependent relation of clas-sification accuracy with the translation quality suggests that our proposed method should provide valid evaluation results for system improvement. Hence, this method is expected to assist system development. 
We further examined the property of our evaluation criteria by comparing our method with a previous automatic evaluation criterion, i.e., METEOR. Through this comparison, we found that our evaluation criterion is stricter than the METEOR-based method. report the experimental design and the results. In section 4 we summarize this paper. In this section we will introduce previous studies using machine learning algorithms to classify MTs depending on translation quality. 2.1 Classification-Based Evaluation Methods Some are expensive methods [8, 9, 10], because these methods need manual evalua-tion results (manually labeled training examples) to train classifiers. The others [1, 2, 3, 4] need no manual evaluation results. Due to the cost problem, we decided to em-ploy the latter method in this paper. 
The latter method treated evaluation of tran slation quality as a classification prob-lem. This method evaluates translation quality on the basis of the classification results of MTs either into HT-like or MT-like translation. Classification is carried out by examining the linguistic properties of MTs.  X  X ood X  translations share a number of properties with HTs, while  X  X ad X  translations are dissimilar to HTs and exhibit trans-lation properties specific to MTs. 
A study [1] constructed classifiers for Spanish-to-English MTs with decision trees and thus they obtained three types of classi fiers: (i) a perplexity-feature-based classi-features. These classifiers exhibited the following accuracy: 74.7% accuracy for the classifier (i), 76.5% for the classifier (ii), and 82.9% for the classifier (iii). Another study [2] adopted SVMs in constructing a classifier for Chinese-to-English MTs. The classification features were combined with (i) n-gram precision of MTs compared with HTs, (ii) the sentence length of MTs and HTs, and (iii) the word error rate of MTs. This classifier achieved 64.4% accuracy. 
Another study [3] also employed SVMs, and constructed a classifier for English-to-French MTs. Classification features involved linguistic properties of translation such as subcategorization properties and semantic properties, e.g., finiteness and ar-gument structures. This classifier marked 77.6% accuracy. The other study [4] constructed a classifier for English-to-Japanese MTs using SVMs. Classification features consisted of word-alignment properties between Eng-lish sentences and translations (MTs or HTs), i.e., aligned pairs and unaligned words. Their classifier showed 99.4% accuracy. 2.2 Property of Cla ssification Accuracy Under a classification-based evaluation method, classification accuracy should de-crease for better MTs, because the translation is similar to HTs as a previous study [1] suggested. Since a classifier incorrectly classifies  X  X ood X  MTs into HTs, classification accuracy would decrease. By contrast, classification accuracy should increase for  X  X ad X  MTs. Given this property of classifi cation accuracy, we consider that the ade-quacy of classification-based evaluation should be assessed by checking not only the independent accuracy but also the translation-quality-dependent accuracy. 
The previous studies [1, 2, 3, 4], however, did not assess their method from view-point of the dependent accuracy. Then, we examined our classification method by checking whether the classification accuracy depends on translation quality, too. In this section we will introduce our classifier that employed word-alignment distribu-tion as classification features. 3.1 Constructing a Classifier We constructed a classifier with SVMs, well-known learning algorithms that have high generalization performance [13]. SVMs have an advantage that the algorithms do not depend on the number of classification features. We trained an SVM classifier by taking HTs as positive training examples and MTs as negative examples. 
As our classifier is trained with parallel corpora, our method requires neither multiple reference HTs nor manually labeled training examples. Due to these properties, our method should be an inexpensive but effective automatic evaluation metric. 3.2 Classification Features Literal translation often makes translation unnatural. This unnaturalness should be observed in any type of translation such as English-to-French translation, English-to-Japanese translation. Thus, a human translator used various translation techniques in order to exclude unnatural literal translation if necessary. By contrast, MT systems often provide literal translation even if non-literal translation is required. From view-translation makes MTs distinct from HTs. 
Let us illustrate such unnatural literal translation here. For instance, a human trans-lator does not provide literal translation for an English nominal modifier  X  X ome X  in the case of Japanese translation (1c). If the nominal modifier  X  X ome X  is literally translated into Japanese, a Japanese nominal modifier  X  X kuraka-no (some) X  comes up, as seen in (1b). Actually, the sentence (1b) was obtained with a state-of-the-art MT system. (1) a. Source Sentence: Some students came. English nominal modifier with a Japanese existential verb  X  X -ta (existed), X  as in (1c). Both the translations (1b) and (1c) are perfectly grammatical Japanese sentences, but some unnaturalness remains in (1b) due to the literal translation of  X  X ome. X  
To identify the unnatural literal transla tion, we decided to examine the word-alignment distribution between source sentences and translations, i.e., MTs or HTs. Literal translations maintain lexical properties such as parts of speech, whereas non-literal translations usually lack the parallel lexical properties. Hence, literal translation should be more easily aligned than non-literal translation. This hypothesis seems correct, as we observed MTs and HTs exhibited different word-alignment distribu-tions in section 4.3. 
The distinction of alignment distribution is illustrated in the example (2). Sentence natural translation in (2c), i.e., HT. (2) a. Source Sentence: Today, the sun is shining. An alignment tool we used in the experiment provides alignment distributions for the example sentences (2) as in Table 1. Here,  X  X lign(A, B) X  means that an English word  X  X  X  and a Japanese word  X  X  X  compose an aligned pair,  X  X on-align_eng(C) X  means that an English word  X  X  X  remains unaligned, and  X  X on-align_jpn(D) X  means that a Japanese word  X  X  X  remains un-aligned. 
From the alignment distribution in (2b) and (2c), we see that the ratio of alignment and non-alignment varies between MTs and HTs. MTs involve more aligned pairs, while HTs involve more non-aligned words. Thus, we consider that the distribution of aligned pairs might reflect MT-likeness, whereas that of non-aligned words could exhibit HT-likeness. Given this property, a word-alignment-based classification should reveal the literal translation properties of MTs. 
As alignment distribution exhibits lexical properties of translation, we can add some other features indicating syntactic and discourse properties. However, the ex-perimental results in the next section rev ealed that alignment distribution-based clas-sification marked sufficiently high classification accuracy. Then, we will employ only alignment distribution as classification features in this paper. In this section we will describe our experiment design, and report experimental results. 4.1 Design In this experiment, we assessed the validity of our evaluation method by examining amined the classification accuracy for translation fluency. In addition, we also investigated whether the classification accuracy was influenced by an alignment tool or an MT system. A previous study [4] used an alignment tool developed by Sharp [12]. This tool carries out alignment of (sets of) words using bilingual dictionary/thesaurus and dependency analysis. constructed two classifiers: one used a popular word alignment tool GIZA++ [14], and the other employed Sharp X  X  tool, and compared the classification accuracy of these classifiers. automatic evaluation method, i.e., METEOR. Since METEOR is an improved method of regarded it as an adequate candidate for this comparison. 4.2 Method We constructed a classifier using a parallel corpus consisting of Reuters X  news articles in English and their Japanese translations [15]. We chose this corpus, because the goal of our classifier is to evaluate English-to-Japanese machine translations. As far as we know, there is no publicly available eval uation data for English-to-Japanese machine translation. 
Since some pairs of a source sentence and its translation appear ed repeatedly in this corpus, the repeated translations were eliminated. Japanese translation of this corpus was taken as HTs. MTs were translation of English source sentences with three state-of-the-art translation systems commercially available in Japan. Conse-quently, we obtained three sets of translation data (henceforth MTa, MTb and MTc). Each set consisted of 25,800 sentences (12,900 HT and 12,900 MT sentences). Given the general circumstances of MT systems in Japan, these systems should basically adopt a rule-based translation method. However, some parts of the system might im-plement an example-based translation method. 
From these sets of translation data, we derived word-alignment distributions be-tween the source sentences and MTs/HTs, using word-alignment tools, i.e., GIZA++ tion data, i.e., MTa data, MTb data and MTc data. (henceforth, we refer to translation data with alignment distribution as MT data or as HT data). GIZA++ was trained with 12,900 HT and 12,900 MT sentences, and each parameter was assigned a default value. The alignment results consisted of the aligned pairs and non-aligned words. Each alignment instance was taken as a classification feature. 
Machine learning was carried out with the TinySVM software [16]. The first order as default settings. 
We randomly chose 500 sentences from MTa to examine the classification accu-racy of our method for different qualities of translation. These sentences were assessed by three human evaluators (not the authors) whose native language was Japanese. The evaluators assessed both the adequacy and fluency of the sentences, and scored them on a scale from 1 to 4. The evaluation scores were determined with a median value of the scores among the evaluators. Adequacy is defined as to what extent a translated sentence conveys the meaning of the original sentence. Fluency is defined as the well-formedness of a translated sentence, which can be evaluated inde-pendently of adequacy. The evaluation results are shown in Table2. From the results, we found that our MT data involved more problems with respect to the fluency, because more than half property, we decided to examine the fluency of translation. 4.3 Translation Data Properties We examined the word-alignment distributions in MT data and HT data (12,900 sen-tences for each data). As Table 3 shows, all the MT data involved more aligned words in MTs than HTs. This tendency is observed in both GIZA++-based alignment and alignment distributions between HT and MTs (p&lt;0.05, Chi-square test). We further hypothesis that MT and HT be distinguishable based on word-alignment distribution. 4.4 Classifica tion Accuracy Supposing that machine translation system might affect the classification accuracy, we accuracy using both alignment tools, GIZA++ and Sharp X  X . These classifiers were con-fore using GIZA++, we segmented Japanese sentences into words. 
The classification accuracy for each MT data, i.e., MTa, MTb and MTc data, was examined in five-fold cross validation tests. Table 4 shows the mean classification accuracy for each test. Fairly high classification accuracy was observed in all the MT data and alignment tools. Hence we have concluded that our classification method is robust for MT systems and alignment tools. 4.5 Comparison with a Classifier Based on Syntactic Features We evaluated our classification method by comparing with a baseline. We set a classifier of the parse tree. Therefore, we decided to compare our alignment-distribution-based classifier with a classifier based on syntactic properties, i.e., dependency relations. 
Although this comparison is not so rigorous, we think the comparison can suggest that our method is valid. HTs and MTs were parsed with a Japanese dependency parser CaboCha [17]. The dependency relation between a modifier and a modified phrase was used as a classification feature. The classification accuracy of each MT data was 83.1% (MTa), 82.0% (MTb), and 83.3% (MTc). We set this classification accuracy as the baseline. Our method outperformed the baseline in all cases, exhibit-ing approximately 20% superiority. Based on these results, we concluded that our word-alignment-based classifiers more accurately distinguish MTs and HTs than dependency-relation-based classifiers. 4.6 Validity of Our Evaluation Method We examined whether classification accuracy decreases for fluent translation. The clas-sults (500 sentences) as in Table 2. Our method judges a correctly classified translation as less fluent, whereas an incorrectly classified translation as fluent. In this experiment, we checked the classification results of the following classifiers: (i) a classifier based on structed with both aligned pairs and non-aligned words. 
Decrease in classification accuracy was not observed in the classifiers (ii) and (iii). Due trast, classification accuracy decreased in the classifier (i). Table 5 and 6 show the classifi-i.e., the ratio of correctly classified translation (worse translation). 
We applied Fisher X  X  exact test to evaluate the statistical significance of the distribution of classification accuracy of these classifiers. Statistically significant difference was found there is no great difference between these two results. Thus, the classification accuracy of our evaluation method is expected to decrease between a pre-improved MT system and a post-improved MT system in the course of system development. Therefore, we have con-cluded that our method is valid for evaluating system improvement. By comparing the evaluation results between the GIZA++-based method and the Sharp X  X  tool-based method, we found that both methods marked the similar classification results for the influent translation, i.e., fluency degree 1. These methods, however, differed with respect to the evaluation results on the fluency degrees 4. Here, the GIZA++-based method regarded 11.1% of MTs as  X  X ood X  translation, while the Sharp X  X  tool-based method classified more MTs (44.4%) into  X  X ood X  translation. Hence, if one has to detect fluent translation as much as possible, the Sharp X  X  tool-based method fits the purpose. 
We also found that the classification accuracy seemed independent of the fluency of translation when a classifier was constructed based on non-aligned words as classi-fication features. Non-aligned words represent the distinction between MTs and HTs, based on non-aligned words, i.e., (ii) and (iii), should have provided more  X  X ncorrect X  MTs should differ from HTs with respect to non-aligned words. 
Following this idea, we investigated translation properties of fluent MTs and those of the relevant HTs. This investigation showed that HTs, unlike MTs, conveyed addi-information should be taken as unaligned words, because relevant information does not appear in a source sentence. Due to these non-aligned Japanese words for addi-tional information, even fluent MTs could be judged as MT-like translation under our method. This additional information would result in an adverse effect on the classifi-cation based on non-aligned words. 
Let us illustrate the explication of contextually salient meaning in HTs. The MT (3b) was evaluated as the highest fluency degree, and the classifiers using non-alignment words judged it as MT. The relevant HT (3c) involves additional informa-translator added some contextually salient information, and thus the translation in-volves additional expressions such as  X  X arket player X  and  X  X tatistics. X  Since these additional expressions are regarded as non-aligned words, a classifier would distin-guish the MT sentence (3b) from the HT sentence (3c). (3) a. Source Sentence: Most were expecting an improvement. 4.7 Property of Our Evaluation Method We used the word-matching component of METEOR. The component is applicable to Japanese. Reference translation was made up of Japanese translation sentences of Reuters X  news articles. Before using METEOR, we segmented Japanese sentences into words. 
Our evaluation method is a binary classifier, which judges MTs as  X  X ood X  (HT-like) or  X  X ad X  (MT-like) translation. On the other hand, METEOR determines the translation the comparison of MTs with the relevant reference HTs. METEOR represents transla-tion quality with scores ranging from 0.0 to 1.0. In order to compare METEOR with our method, we set a threshold on METEOR score, and we classified MTs into two groups: if a translation marked a higher score than the threshold, it would be judged as  X  X ood X  translation, and vice versa. If we set a proper threshold between  X  X ood X  and  X  X ad X  trans-lations, the METEOR-based classification should properly classify MTs into the four threshold for METEOR that the scores would yield as many statistically significant degrees as possible. In order to determine the threshold, we examined the classification results by setting a threshold in every 0.1 from 0.1 to 0.9. 
The distribution was significantly different (p&lt;0.05) when the threshold was 0.4, 0.5, and 0.6. We further performed the residual analysis. This analysis showed that the difference was significant in three degrees (fluency degrees 2, 3, and 4) when the threshold was 0.4, and that the difference was significant in two degrees when the threshold was 0.5 and 0.6. From these results, we decided to compare our classification method with the ME-TEOR-based classification with the threshold 0.4. The classification result under the METEOR-based method is shown in Table 7. Table 7 shows the number of sentences whose METEOR scores were lower/higher than the threshold 0.4. The ratio indicates the rate of lower score sentences, and it parallels with the classification accuracy of our method. The result of the residual analysis for the classification is shown in Table 8. By comparing the evaluation results between our method (Table 5 and 6) and the METEOR-based method (Table 7), we found that our classification differed from the METEOR-based classification with respect to the evaluation results on the fluency degrees 1 and 4. While the METEOR-based method correctly classified 68.5% of MTs as  X  X ad X  translation in the degree 1, our method regarded most MTs (99.5%; 95.7%) as  X  X ad X  translation in the degree 1. Hence, if one must never overlook influ-ent translation, our method fits the purpose. 
In the case of fluency degree 4, the METEOR-based method regarded more MTs (66.7%) as  X  X ood X  translation than our method (11.1%; 44.4%). Hence, if one is sure to evaluate fluent translation as fluent, the METEOR-based method is preferable. From this classification property, we found that our method and the METEOR-based method fit the different purposes for the evaluation. We proposed an automatic method of evaluating MTs, which employed neither refer-ence translations for evaluation of new translations nor manually labeled training data (HT-like) or  X  X ad X  (MT-like) translations. The classification is determined based on the word-alignment distributions between source sentences and translations. As we confirmed, the alignment distribution was different between HTs and MTs, and that the classification results depended on the translation quality. In addition, we examined our evaluation property, and found that HT-likeness was a strict evaluation criterion. Given these findings, we conclude that our method is an inexpensive and effective automatic evaluation metric for translation improvement. 
This paper leaves several problems unsolved. First, we must examine the classification properties based on non-aligned words. As we saw, additional information in HTs might words in more detail. Second, our method should be improved by comparing with the previously proposed evaluation method, or with the other classifiers employing the general linguistic features. Thirdly, we will examine our method in other languages, e.g., Chinese. Last but not least, we will examine our method for sentence level evaluation. Researchers have long believed that syntactic analyses of languages will improve natural language processing tasks, such as semantic understanding, word alignment and machine translation. In cross-lingual applications, much work has explicitly in-troduced grammars/models to describe/captu re languages X  structural divergences. [1] is one of the pioneering researches in ordering corresponding grammatical con-stituents of two languages. Wu devises binary Inversion Transduction Grammar (ITG) rules to accommodate similar (in order) an d different (reverse order) word orienta-tions in synchronous bilingual parsing. The constraint imposed by Wu X  X  straight and terms of machine translation (see [2]). On the other hand, [3], given source-language (SL) production rules of arbitrary length, utilizes EM algorithm to distinguish statisti-cally more probable reordered grammatical sequences in target-language (TL) end from others. Recently, ever since Chiang X  X  hierarchical phrase-based machine transla-tion model [4], successfully integrating bilingual grammar-like rewrite rules into MT, more and more researchers have devoted themselves to syntax-based MT system: [5], [6], and [7]. 
Syntactic reordering plays a vital role for modeling languages X  preferences in word order in above grammatically motivated systems and has been proved to be quite characteristic differences of SL and TL word orders. These rules are aimed to reorder SL sentences such that new sequences of words better match their TL counterparts. Although better translation quality is obtained, two issues are worth mentioning: there might be exceptions to reordering rules with coarse-grained grammatical labels and paper, we propose a framework which automatically acquires lexicalized reordering rules based on a parallel corpus. 
The reminder of this paper is organized as follows. Section 2 discusses the reorder-ing framework in detail. Section 3 shows the data sets used and experimental results. At last, Section 4 concludes this paper. In this section, we begin with an example to illustrate how lexicalized reordering rules can have positive influence on word aligning and machine translating quality. There-after, we elaborate on the proposed automatic reordering framework. 2.1 An Example Consider an English sentence  X  X fter the m eeting, Mr. Chang went straight home X  and its Mandarin Chinese translation  X   X  X   X   X   X   X  X  X   X  X  X   X   X   X . Figure 1 shows the parse tree of the English sentence (we can ignore the  X   X   X  between English words for now), and correspondence links between the English words and their Mandarin counterparts. 
In Figure 1, there are three crossings among the word alignment links, indicating three instances of reversing of some syntactic constituents during translation process. noun phrase. The example may lead us to conclude that we should invert all preposi-tional phrases when encountering IN and NP constituents. However, as indicated by when the lexical item of IN is  X  X n X , and syntactic information alone is not sufficient to make such a reordering decision, especially, presented with a coarse-grained gram-matical label set. Instead, we further need lexical cues of the IN syntactic constituent (i.e.,  X  X fter X  and  X  X n X ). Accompanied with th e lexicalized information, we have higher chance to recognize that, contrast to Englis h, in Chinese, temporal subordinating conjunctions always appear after the noun phrase. Similarly, for the second crossing, we need to examine the corresponding word of the first NNP of a proper noun since a title word (e.g.,  X  X resident X ,  X  X rofessor X ,  X  X r. X  and so on) has different ordering preference in Chinese, compared with the first NNP, proper noun (i.e., John), in Fig-ure 2. [9] and [10] also point out the importance of lexical items in determining word orders from one language to another. 
Encouragingly, it is straightforward to incorporate lexical information into syn-chronous context-free grammar rules such as ITG rules. Table 1 shows the lexicalized syntactic reordering rules that apply for the sentence pair in Figure 1. We follow Wu X  X  notation in [1] by using pointed bracket to depict the inverted order of the correspond-ing syntactic constituents in two languages and the English words enclosed in paren-theses are lexical cues for the constituents. Intuitively, by learning the reordering rules Figure 3, where the horizontal dashed lines imply the subtrees had been inversely reordered. Note that such reordering rules capture languages X  divergences, thus poten-tially conducive to word alignment and translation. 2.2 Reordering Model represent anchor points for reordering two consecutive constituents to fit the word orientations of Chinese. Take Figure 1 and Figure 3 for instance. In Figure 1, the first dot whose associated grammar rule is PP  X  IN NP represents the Chinese choice in ordering corresponding IN and NP syntactic constituents, containing Chinese transla-NP  X  NNP NNP denotes the orientation choice of NNP and NNP counterparts in Chinese; The seventh one whose associated rule is VP  X  VBD ADVP represents the reordering anchor of corresponding Chinese VBD and ADVP constituents. Figure 3, on the other hand, shows the reordered tree by choosing to reverse neighboring phrases of the first, fifth and seventh anchor point in Figure 1. 
Mathematically, given a SL parse tree  X  , reordering models search for  X  * , satisfying reshuffled  X  * more in tune with grammar in target language. In this paper, by using tree-points denoted by  X   X   X . Then, the problem of searching for most probable transformed tree ( points in s . In other words, provided with the representation s and the system parameter set  X  ={  X  j }, our model looks for the most likely reordering label sequence y * where z( s ) is a normalization factor and  X  j is the weight of the syntactic feature func-conditional random fields (CRFs) to find the best label sequence, upon which most probable reordered tree are based. Input: source-language sentence e and its parse tree  X  Output: string representation s INSERT  X   X   X  between words in e // as reordering anchor points FOR each word w IN e IF w is  X   X   X  Lw=the word immediate to the left of the  X  in e Rw=the word immediate to the right of the  X  in e LHS=the immediate descendent of node P along the path from P to Lw in  X  OUTPUT s
Table 2 summarizes the content derived from abovementioned transformation algo-word and successive word of the current one respectively, and P , LHS , Lw , RHS and transformation algorithm. [1] suggests binary-branching ITG rules prune seemingly unlikely and arbitrary word permutations but yet, at the same time, accommodate most ported to be beneficial to machine translation in terms of quality and speed. Therefore, in this paper we focus on reorderings of binary trees which can be obtained by binary syntactic parsers (e.g., Berkeley Parser) or by following the binarizing process in [11]. If, during training, probabilistic CRFs always observe inversion of the grammar rule PP the rule S  X  NP VP, CRFs model will tag the first dot as I (for inversion) and tag the sixth mined, the SL parse tree in Figure 1 will be successfully reordered into one in Figure 3, which abides by grammatical ordering preferences in the target language (e.g., Chinese). 
Our framework leverages CRFs to train the weights of the feature functions related transformation procedure, and, at runtime, brings SL parse trees closer to TL word order by applying lexicalized reordering grammar rules or pure grammatical rules. Which type to choose is informed by highly-tuned feature weights in CRFs training. 2.3 Training Process of CRFs To gain insights on how to order the corresponding SL syntactic constituents in the target language, SL sentences are aligned to TL sentences at word level and are monolingually parsed by some existing parser. Furthermore, based on word alignment results, firstly, the minimum/maximum word position on target language end that SL part-of-speech tags can cover is determined, i.e. TL spans of SL POS tags, and then the TL spans are itera-tively obtained in bottom-up fashion. In the end, parse trees contain not only monolingual grammatical labels but also bilingual information concerning the TL span of each SL tree node, on which we work to differentiate dissimilar word ordering preferences in the two languages from similar ones. The training process is outlined as below. parser Par , and a CRFs implementation crf APPLY WA on the corpus C to obtain word alignment PARSE source-language end of C by use of Par FOR each sentence pair ( e , f ) IN C
DENOTE  X  as the parse tree of e ,  X ( pos ) as the part-of-speech nodes in  X  and 
FOR each node n IN  X ( pos )
FOR each node n IN  X ( nonT ) span( n )=from min(aligned positions on TL side of n  X  X  children) to max(aligned 
APPLY tree-to-string transformation algorithm on e and  X  to obtain their string FOR each dot d IN s After collecting all the string representations with ordering information, we train crf to determine the weights  X  associated with chosen syntactic feature functions. 
Take sentence pair in Figure 1 for example. The TL-end span of each label in the parse tree and the string representation with orientation information are shown in Figure 4 and in Table 3 respectively. String representations with orientation informa-flect the contribution of lexical or syntactic items in determining TL word orders of a specific SL context (e.g., PP  X  &lt;IN (After) NP (the)&gt; ). We start with the data sets and settings we used in experiments. Afterwards, we evaluate the impact our reordering framework has on performance of bilingual word alignment and machine translation. 3.1 Data Sets and Experimental Settings We used the first 200,000 sentence pairs of the news portion of Hong Kong Parallel Text as our parallel corpus C . A MT testing data set, composed of 1035 English sen-tences of average 28 words randomly chosen from Hong Kong news 4 (excluding reference translation set, that is, one reference translation per English sentence. More-over, the English sentences in both training and testing sets were syntactically parsed by Berkeley parser 5 beforehand. 
We employed CRF++ 6 as the implementation of probabilistic conditional random fields to construct the proposed syntactic reordering framework. During CRFs X  pa-rameters training (Section 2.3), we deployed GIZA++ as the word aligner. Besides, to make CRF++ more accurately learn ordering choices of two languages in syntactic content words (nouns, verbs and adjectives) on English end was lower than 0.8 or the length of the English sentence was shorter than 20. In other words, CRF++ was dedi-cated to search for significant lexicalized or non-lexicalized reordering rules from highly-aligned and potentially long-range distorted sentence pairs. After filtering, approximately 23,000 parallel sentences of C were retained to tune CRF++. 
At runtime translation, on the other hand, our framework exploited Pharaoh ([12]) as the phrase-based MT decoder. The language model Pharaoh needs was trained on the Chinese part of the whole Hong Kong news, 739,919 sentences in total, using SRI language modeling toolkit, while phrase translation table was built upon C after word aligned using GIZA++. 3.2 Evaluation We are interested in examining whether our methodology captures meaningful syn-tactic relationships between the source and target languages, thus boosting the accu-racy in word alignment and decoding. We experimented different ways of introducing source sentence reordering to the phrase-based machine translation system (i.e., Phar-aoh). First, we performed word alignment on the original and reordered source sen-tences to derive two sorts of phrase translation table used in MT decoder. Then Therefore, there are four sets of transla tion results where the source sentences in the using these four data sets was measured by BLEU scores ([13]) and summarized in a contingency matrix in Table 4. 
As suggested by Table 4, when using the reordered sentences to perform word alignment and decoding, the translation quality improved by more than 0.7 BLEU point. If we left the training data unchanged and simply reordered the test sentences, we get a significant improvement of 1.3 BLEU points over translating the original test sentences. One can find that test sentence reordering resulted in greater improvement (6% relative) over training sentence reordering (3% relative). There might be two sented by [14]: it is, sometimes, difficult to propagate improvements in word align-ment to translation. Additionally, GIZA++, a word aligner modeling distortion in languages, is much more capable of capturing distortion of words than Pharaoh, a decoder exhibiting global reordering problems. As a result, there were about 3% im-provement gap between these two different settings of data sets. 
Encouragingly, if both the training and test sentences were pre-reordered, our method outperformed baseline by more than 2 BLEU points. Overall, it is safe to say that our automated reordering framework improves translation quality for disparate language pair such as English and Chinese. This paper has introduced a syntactic reordering framework which automatically learns reordering rules from a parallel corpus using conditional random fields. In experiments, these reordering rules, if necessary, accompanied with lexical informa-word aligners and MT systems alike. 
As for future work, we would like to examine whether integrating more syntactic framework further boosts the performance. We also like to inspect the performance of our methodology in other distantly-related language pairs such as English and Arabic. search e ffo rt has bee n spe n t on aut o mat in g such tasks us in g v ar io us n atura l l a n guage pr o cess in g appr o aches .Mo st pre vio us researches , fr o mc on struct in g
W h il ea n umber o f l a n guages such as J apa n ese , Ch in ese , a n dGerma n are j ect ivi t y a n a ly s i st ool s in o ther l a n guages .

Mo t iv ated b y the l atter appr o ach , th i s paper prese n ts a meth o df o raut o-successfu l resu l ts in i ts rece n tapp li cat ion st oNLP tasks [11, 8 ]. Espec i a lly, [ 8 ] c on structed a graph o f Wo rd N et s yn sets us in gg lo sses t o create edges am on gthe the Wo rd N et s yn sets in the graph acc o rd in gt o the e vi de n ces c oll ected fr o m i ts n e i ghb o rs .T he appr o ach has sh own t o d i scer n the OR P s o fthes yn sets m o re ac -A dapt in gas i m il ar frame wo rk ,w eha v ecreatedab i part i te graph o f l e xi c on e n-tr i es ,wi th e n tr i es o f on e l a n guage f o rm in gac l uster a n dthe o ther l a n guage o fth i spaper .

Our wo rk f o cuses on creat in gasub j ect ivi t yl e xi c on in Ko rea n ut ili z in gsub -stud y. How e v er ,o ur wo rk d o es no tre ly on a ny l a n guage  X  spec i fic in f o rmat ion but only requ i res a b ilin gua l d i ct ion ar y bet w ee n the s o urce a n dthetarget l a n-guages , mak in g i teas ily app li cab l et oo ther l a n guage pa i rs . thesaurus [3,13,9,5,1 2 ,5, 7 , 8 ]o rra w c o rpus [1, 2 ,1 7 ].

T here a l s o has bee n e ffo rts t o ut ili ze the l a n guage res o urces created in E n-dates because d i ct ion ar i es li st the se n ses in o rder o fthec o mm on usages he n ce the first se n se be in gthem o st pr o bab l e on e ,o ur wo rk f o cuses on h ow t o reduce the amb i gu i t y err o rs w h il est ill ma in ta inin gag oo d n umber o ftra n s l at ion s . [ 8 ] uses a graph represe n tat ion o f Wo rd N et s yn sets a n dara n d o m  X  X  a l km o de l ta in sthec l ues f o rtheedgesfr o mg lo sses o f Wo rd N et e n tr i es ,o ur wo rk creates n ar y such that a f o re i g nwo rd be in gthed i rect tra n s l at ion o fas o urce wo rd creates a n edge bet w ee n the t wo wo rds . m o de l.

S ub j ect ivi t yl e xi c on s v ar yinw hat in f o rmat ion ( subjective / objective , posi-wo rd , se n se ) a n dh ow the i rstre n gths are measured ( weak / strong , pr o bab ili t y but a ny OR P s w h o se stre n gths ca n be n umer i ca lly tra n sf o rmed in t o sc o res t o be used wi th in o ur lin ka n a ly s i sm o de l. 3.1 Translating Subjectivity Lexicon there are ma ny mu l t i X  X o rds that are no t li sted in the b ilin gua l d i ct ion ar y, a n d wi th them [1 2 ]. [1 2 ] re li es on aheur i st i cmeth o dthattra n s l ates only the first se n se , s in ce used se n ses are li sted bef o re the l ess freque n t ly used on es .S uch a scheme wo u l d pr o bab ly resu l t in a l e xi c on wi th better qua li t yin the se n se o fc onv e yin gsub -app li cat ion usages .
 n umber o fe n tr i es .W e assume that f o reachE n g li sh wo rd a n d i ts P O S, o ur the usage freque n c y, a n deachse n se a l s o c on ta inin ga n umber o ftra n s l at ion ca n d i dates ,w h o se ra n k i sa l s oo rdered in re v erse o f i ts usage freque n c y. First Word (FW). T h i s appr o ach ass i g n sthese n t i me n tsc o res o ftheE n g li sh wo rd t oonly the first wo rd o f the first se n se .T h i stra n s l at ion scheme fi l ters u n certa in ca n d i dates , the s i ze o ftheresu l t in g l e xi c on be in gthesma ll est . First Sense (FS). T he appr o ach take nin FS i ss i m il ar t o the on eused in [1 2 ]. wo rd ,i mp lyin gthatd iff ere n ttra n s l at ion wo rds wi th the same se n se are equa lly li ke ly t o be tra n s l ated .
 All Senses (AS). AS ass i g n sthese n t i me n tsc o res o ftheE n g li sh wo rd t o a ll the wo rds in i ts tra n s l at ion ca n d i dates .T h i sschemepr o duces the ma xi mum n umber o f Ko rea nwo rds , a llowin gu n re li ab l e wo rds in the l e xi c on.
 Sense Rank (SR). Ko rea nwo rds are ass i g n ed d iff ere n tsc o res b y the i rse n se the resu l t in g l e xi c on s in the e v a l uat ion pr o cess . 3.2 Refining the Lexicon with a Link Analysis Algorithm Si m il ar ly t o[ 8 ], o ur appr o ach uses a graph bu il tfr o mthe wo rds wi th OR P s as v ert i ces , a n dthere l at ion sam on gthe wo rds as edges c onn ect in gthe v ert i ces . the s yn sets ,wi th the h y p o thes i sthatg lo ss o fas yn set will usua lly c on ta in terms t ion ar y s o that no des c onn ected b y edges are d i rect tra n s l at ion s o feach o ther . T hese t y pes o fedgesarem o re su i ted f o rbu il d in gamuchm o re sema n t i ca lly t i ght graph structure tha n the on eus in gs yn set g lo sses .

N atura lly, edges o fd i rect tra n s l at ion sc onn ect E n g li sh wo rds t oKo rea nwo rds only, a n d Ko rea nwo rds only t o E n g li sh wo rds .T h i st y pe o fgraph i sca ll ed a b i part i te graph ,w here v ert i ces are part i t ion ed in t o t wo d i s join tsets wi th no edges c onn ect in ga ny t wo v ert i ces in the same set .

HITS i sa lin ka n a ly s i sa l g o r i thm that rates v ert i ces o f a graph b y determ in-structure [ 22 ].

C on s i der in gthehub n ess o fa n E n g li sh v erte x as i ts se n t i me n tsc o re , a n da auth o r i tat iv e n ess o fa Ko rea nv erte x as the v erte xwi th c onn ected n ess t o E n-l ear nin gthese n t i me n tsc o res o fa Ko rea nl e xi c on. D amp in gfact o r in P ageRa n k in equat ion 1).

C o mb inin gthe i deas resu l ts in equat ion 1 w here TC ( w )i stheset o ftra n s l at ion ca n d i dates o fa wo rd w ,  X  a n d  X  are damp in gfact o rs f o r Ko rea n a n dE n g li sh v ert i ces .
 n ess o f Ko rea nv ert i ces , a n d vi ce v ersa f o r  X  .
 ma xi m i ze the qua li t yo ftheE n g li sh l e xi c on as w e ll, us in g the equat ion 2 . o ut on o ur t wo phase lin ka n a ly s i sframe wo rk : first , ru nnin g HITS wi th Ko rea n n ess o f Ko rea nwo rds , a n dsec on d ly, ru nnin g HITS aga in wi th E n g li sh wo rds o fE n g li sh wo rds .T he lin ka n a ly s i sm o de lin each phase sh o u l dtaked iff ere n t v a l ues f o r  X  a n d  X  t o ad j ust the v ar i ab ili t yo f v ert i ces acc o rd in g ly. 4.1 Setup in Op inionFin der ( OF )[10] 1 a n d S e n t iWo rd N et 1.0.1 ( SentiWN )[ 7 ] 2 .
OF i saset o fE n g li sh wo rds a n dse n t i me n ta nno tat ion sc oll ected fr o ma n um -gathered . Each wo rd in OF has a P O S tag a n dcateg o r i es o f Positive / Negative Strong wo rds wi th 1 . 0.

SentiWN i saset o f Wo rd N et s yn sets wi th aut o mat i ca lly ass i g n ed p o s i t iv e , s yn set i s treated separate ly wi th the se n t i me n tsc o res o fthes yn set as i ts own, i g no r in gthes ynony m in f o rmat ion pr ovi ded b yWo rd N et s yn sets .
W eusea n onlin eb ilin gua l d i ct ion ar y pr ovi ded b y ap o rta lw ebs i te 3 .Fo r o ur 1 42 , 7 91 tra n s l ated wo rds in Ko rea n.
 ( FW , FS , AS , a n d SR ).

T he parameters  X  a n d  X  in equat ion s 1 a n d2are o pt i m i zed on ahe l d  X  X  ut data us in g v a l ues fr o m 0 . 1 t o0 . 9wi th a step o f 0 . 1. 4.2 Evaluation Method W ef ollow ed the e v a l uat ion scheme in [ 8 ], w h i ch uses a Mi cr o X  X N Op c o rpus [ 24 ] 4 as a g ol dsta n dard a n dthe p X  X ormalized Kendall  X  distance (  X  p )[ 2 5] as the e v a l uat ion measure .

Mi cr o X  X N Op i s a subset o f Wo rd N et that are tagged wi th OR P sb y the n umber o fE n g li sh ma jo r in g MS c stude n ts .Divi ded in t o three sect ion s ( Common , n egat iv esc o res .Fo r o ur research ,w euse Group1 as a he l d  X  X  ut data a n d Group2 4 96 s yn sets in Group1 a n d4 99 s yn sets in Group2 o f Mi cr o X  X N Op w as tra n s l ated Ko rea nwo rds no tappear in g in a ny o fthe l e xi c on s in o ur e x per i me n ts w ere sta n dard .

T he p X  X ormalized Kendall  X  distance i sameasure o fh ow much t wo ra n ked pa i rs o f i tems are tested , such that the agreeme n ts o fthe i rpart i a lo rders are d i sta n ce i sdefi n ed as n u i sthe n umber o fpa i rs o rdered in the g ol dsta n dard but t i ed in the pred i ct ion, a n dZ i sthe n umber o fpa i rs o rdered in the g ol dsta n dard .

T he measure f o rapred i cted li st w h o se i tems are ra n ked in the same o rder as in the g ol dsta n dard , the n  X  p equa l s 1. I fa li st d o es no t o rder i tems but rather retur n sa n u no rdered li st , the n the measure bec o mes 0 . 5. w ehade x pected : heur i st i cs that tra n s l ate only re li ab l e wo rds te n dt o ha v e e v er , the qua li t yo fthe l e xi c on i s better tha n the on es tra n s l ated fr o m Sen-tiWN because w he n tra n s l ated , sc o res are a v eraged s o that the wo rds now de v e lo ped res o urces w h il e SentiWN i screated in c o mp l ete ly aut o mat i c fash ion.

T he pr o p o sed frame wo rk wi th t wo lin ka n a ly s i sm o de l shasac o mpe n sat in g e ff ect in each phase that the l e xi c on s mutua lly c o mp l eme n teach o ther in tur n t o ra n ge fr o ms li ght ly n egat iv e (+1 . 2 9%) t o e x cept ion a l(  X  4 1 . 3%). target l a n guages has bee n emp i r i ca lly sh own t o create g oo dqua li t yl e xi c on s .
Unli ke pre vio us wo rk ,w eha v ee x p lo red the p o ss i b ili t yo f regard in ga l a n-tempted t o dra w c o mpe n sat ion in teract ion sus in g a graph structure as a med i um . a n de x te n d in gt o d iff ere n t l a n guage pa i rs .
 T h i s wo rk w as supp o rted in part b yMK E &amp;IITA thr o ugh IT L ead in gR &amp;D S upp o rt P r oj ect a n da l s oin part b y the BK 2 1P r oj ect in 2 00 8 . Transliteration is the process of mapping text written in one language in to another by means of a pre-defined mapping. It is useful when a user knows a language but does method to input data in a given language. Hence, transliteration can be understood as the process of entering data in one language using the script of the another language. In general, the mapping between the alphabet of one language and the other in a trans-literation scheme will be as close as possible to the pronunciation of the word. English form of chats, mails, blogs and other forms of individual online writing. This kind of transliterated text is often referred by the words formed by a combination of English and the language in which transliteration is performed, like -Arabish (Arabic + Eng-lish), Hinglish (Hindi + English) etc. Depending on various factors like mapping, language pair etc, a word in one language can have more than one possible translitera-proper nouns and other named entities. In this paper, we deal with the problem of back transliteration from Telugu to English. However, we discuss only English to Telugu transliteration thr oughout this work. 
Telugu is the third most spoken language in India and one of the fifteen most spoken lugu has 56 alphabets (18 vowels and 38 consonants), two of which are not in use now. which is widely accepted and used. Many tools and applications have been designed for Indian language text input. But, an evaluation of the existing methods has not been per-formed in a structured manner yet, to standardize on an efficient input method. 
Most of the Indian language users on the internet are those who are familiar with typing using an English keyboard. Hence, instead of introducing them to a new key-board designed for Indian languages, it is easier to let them type their language words using Roman script. In this paper, we deal with text input as a transliteration problem. engine. Amongst the different methods tried out, an edit distance based approach worked most efficiently for the problem. We have performed these experiments on roman script based languages too. Further, the technique will also help transliterating text offline. There is a huge amount of data in the form of Romanized Telugu or any other language, used in various sites on the internet. Lyrics sites and discussion boards are the best examples for this kind of a scenario. Our technique can be applied in converting this text to Unicode. We describe our design and subsequent experi-ments conducted to verify the efficiency of the system in this paper. 
The rest of this paper is organized as follows -Section 2 describes the related work. Section 3 explains some of the approaches we have tried out in the process of sign methodology. Section 5 presents our experiments and results. Section 6 presents our conclusions and outlines the future work. Many applications concerning text input for Indian languages have been designed in the recent past. Keyboard layouts like Inscript [7] and Keylekh [3] have been devel-oped for Indian languages. There are also online applications like Quillpad 1 and Google's Indic transliteration 2 , which facilitate typing in Telugu through Roman script, without any previous learning on the part of the user. There are also mapping schemes to map Roman character sequences to Telugu characters. Some of them include ITRANS 3 , RTS (Rice Transliteration Scheme) 4 and Wx 5 . Softkey-boards are also designed for Indian languages by Google (gadget on iGoogle), Guruji.com 6 and Telugulipi.net 7 among several others. However, our discussion in this work is limited to the Roman script based typing interfaces for Telugu language. 
Edit distances have been used for a variety of matching tasks. They have been used widely for the purpose of detecting spelling variations in named entities. Freeman et.al. [1] used an extension of Levenshtein edit distance algorithm for Cross-linguistic name mapping task in English to Arabic transliteration. Cohen et.al. [12] have per-formed a comparison of different string distance metrics for name matching tasks. They have also implemented a toolkit of string matching methods for general purpose application. Mauricia et.a. [11] proposed a method to build Indo-European languages tree by using Levenshtein distance 8 . Makin et.al. [6] applied string matching tech-niques in the context of Cross Lingual Information Retrieval among Indian languages, to identify cognates between two Indian languages. Nayan et.al. [2] used Levenshtein distance, Soundex 9 and Editex [1] for the purpose of Named Entity Recognition in Indian languages. Pingali et.al. [5] also have mentioned about the usage of Fuzzy string matching techniques for Word normalization in Indian languages. 
However, though all the above approaches have used edit-distance and other string matching based approaches widely, this work differs from them in the application scenario. While all of them have used those methods for name-matching tasks in mono-lingual as well as cross-lingual retrieval, we used those methods to develop an input method for Telugu language. Hence, we have used them to build an input method for Telugu through transliteration of the Roman script. We have considered text input as a transliteration problem in this work. Hence, we have worked on evolving approaches which consider text input from this point of view. Since we had access to a 1,600,000 strong word list for Telugu, collected from a crawl data index of an Indian language search engine, we have made attempts to ex-ploit it. Further, lack of a parallel corpus for this kind of data came in the way of us-ing any Machine Learning based techniques. Hence, we have come up with several approaches to text input, which involved the usage of this word list. The word list in Telugu was first stored in Roman script, by following a pre-defined mapping. This mapping was formed by performing an Ease of remembrance experiment on the exist-ing mapping schemes for Indian languages. Ease of remembrance This experiment was performed based on a similar experiment by Goonetilleke et.al. [8], which was performed to estimate the ease of remembrance of a particular roman-ized mapping compared to other. They have performed it for Sinhalese. We have asked a group of 9 users to romanize the alphabets of Telugu language. Users were given a 2-column table. First column contai ns the Telugu alphabet . Second column is vacant and the user was asked to fill it th e spelling of the Telugu alphabet according to their intuition. It is followed by the following procedure: We performed the experiment with three popular mapping schemes for Indian lan-guages ( RTS, Wx and ITRANS). The results are tabulated below: Mapping Scheme Average edit distance per word 
Since RTS had a better ease of remembrance as proved by this experiment, we have used a near RTS notation to convert our word list to Roman notation. After con-proaches for forming a data input method. Though we have finally concluded that Levenshtein distance based approach works the best of all, we are presenting the dif-ferent approaches we have tried in the process of forming an efficient text input method for Telugu. The experimented approaches are explained below. 3.1 Generating Combinations In this method, the input word is split in to individual phonemes. For example: bharat first column contains the phoneme and the second column contains possible alterna-tives to that phoneme. For example, the phoneme bha can have its variations as ba,baa,bha,bhaa (  X  ,  X  X  ,  X  ,  X  X  ) . The second column is filled using the data obtained by performing the  X  X ase of remembrance X  experiment. This approach is similar to that performed by Sandeva et.al [9], who also tried a word list based approach for building a Sinhalese language prediction system. Finally, using this mapping table, all possible combinations of words that can be formed from the input word are generated. Valid words from the generated combinations are returned as predictions. However, as the length of the word increases, lakhs of combinations need to be generated, which slows give only valid words as output, which again involves the usage of the large word list. improvement over this idea was implemented, which involves converting the existing word list in to a mapping table. 3.2 Building a Mapping Table from the Word List In this approach, we have re-arranged the Romanized word list to form a key-value data structure. The roman-mapping was case sensitive. Hence, we have formed a new data structure in which the key is a lower-cased word and its value had all the possible case-variated words to the key word, which existed in the dictionary. For example, against an entry kavali , the entries will be: {kAvAlI;kAvAli;kAvali;kavAli;khavAlI} ( better than the previous one, it had one drawback. It works only if all the vowels and consonants entered were correctly spelt by the user, which is not always the case. For example, the word Burma (another name for the country -Myanmar) is actually pro-nounced barma and hence written so in Telugu. But, since this approach does not save those variations in its data structure, it fails to transliterate in this case. This ob-viously resulted in a less efficient system since this is a common problem in text input for Indian languages. 3.3 Fuzzy String Matching Based Approaches Since the above mentioned approaches have not proved to be efficient enough for the task, we have tried out some experiments with string similarity metrics and other fuzzy string matching approaches. First, we have experimented with Jaro-Winkler distance , which has been explained in detail in [4]. Our procedure is based on the as-word. The approach scaled well and was fast enough. However, as mentioned by purpose was to get the spelling variations of first names and last names. But our data-set is a generalized data set, which has both named entities as well as non-named enti-ties, since this is the problem of a general purpose text input. Hence, this approach did not give a good efficiency to provide a better text input method. Hence, we have tried to experiment with normalization methods like Soundex. Originally designed to get the spelling variations of names, Soundex achieves its pur-pose by grouping the letters in to respective classes. The Soundex algorithm is ex-plained to some extent in Freeman et.al. [1]. In essence, it assigns a code to the given word based on its grouping. Similar names will possess the same Soundex code to a large extent. We have customized the Soundex grouping of letters as per the needs of a transliteration approach. We have altered the grouping of classes to some extent and extended the "Alphabet+3 digit code" of Soundex to "Alphabet+N digit code" where N is dependent on the length of the input word, since there will be so many words that have a similar 3 digit code as the input word. However, this did not improve the effi-ciency much. 
Hence, we have tried using edit-distance as a method to devise an efficient text in-put method for Indian languages. We have used Levenshtein distance for this purpose word to be converted to the target word. Typically, insertions/deletions/substitutions are the three common factors involved in typing one language using the script of the other language. This approach has worked well and was proved to be very efficient. Our approach is explained below. Levenshtein distance between two strings is defined as the minimum number of op-above mentioned operations. Hence, Levenshtein distance is a suitable metric to ex-periment with, in the development of a text input method. The following are the steps involved in our approach: 
Step 1-Pre-processing the input word: The preprocessing step involves applying a few heuristics on the input word to make it as close as possible to the internal map-ping. For example, say a user enters the input word as "raamaayanam" (  X  X  X  X  X  X  X  X  X  ) , which is one of the Indian epics. The word list actually saves such a word as "rAmA-yaNaM" according to its mapping. Hence, one of the rules applied in the preprocess-words only after converting the words in to lower case, the two words match. This is a simple example. Such rules are written to do some pre-processing on the input word to convert it in to the required form. 
Step 2-Prediction: As mentioned before, based on the intuition that the first letter with that alphabet in the word list. The words which satisfy the requirements are re-character should also match. However, this is not a mandatory requirement. The re-quirements also include calculation of three Levenshtein distances. They are: 1. Levenshtein distance between the two words (i) 2. Levenshtein distance between the consonant sets of the two words (j) 3. Levenshtein distance between the vowel sets of the two words (k) Here, the terms consonant set and vowel set refers to the concatenation of all conso-nants and all vowels in the word respectively. Step 3-Conversio n to Unicode: Finally, we have to convert th e Romanized predic tions back to Unicode notation. This is done by reversing the process followed to convert the word list in to a Roman nota-tion, which was performed in the initial stage. We have conducted several experiments by varying the Levenshtein distance thresh-olds of the three distances mentioned above. Our experiments and the results are ex-plained in the following section. We have performed some experiments by varying the threshold levels of the three Levenshtein distances we have considered in designing the system. We have tested offering good predictions. The datasets are explained below. since it is a general dataset which encompassed the other two categories. 
The results are summarized in the following tables. The first column indicates the experiment number, the second, third and fourth columns indicate the different Levenshtein distances explained in the previous section and the last column shows the efficiency. Efficiency is calculated as the number of words for which the system gave correct predictions for a given input word. General Observations: 1. The system has not been very efficient with an exclusively named entity dataset compared to the generalized dataset. 2. The person list had the largest accuracy for a particular experiment, more than the generalized dataset. 3. The accuracy with the general dataset has been considerably good. The second column in the table word Levenshtein distance (i) refers to the Leven-shtein distance between the two words being compared. As expected, the system per-formed efficiently with an increasing value of i . But, since there should be some limit Expt No. Expt No. Expt No. distance (j) refers to the difference between consonant sets of the two words. We have be a lot of words within the threshold i, which may not be related to the source word experiments because, it is very unlikely that more than two words which differ in more than two consonants will be representing the same source word. The fourth column refers to vowel set Levenshtein distance(k), which represents the difference in vowels of the two words. This has been varied as either 1 or 2 in most of the experi-ments we have conducted. This factor has been totally ignored in the last two experi-ments, to verify its influence on the overall system efficiency. 
As it can be seen from the results, the system performed very well with the gener-Telugu. This approach is language independent and hence can be applied to any other Indian language, owing to the similarity between Indian languages. The success of the system with the Levenshtein distance approach can be attributed the relationship be-tween the nature of Levenshtein distance and the process of typing Telugu in English, as mentioned in the previous section. 
The failure of this system with Named Entities can be owing to the fact that the based on how the words are spelt since the dictionary is converted to a Roman map-ping based on the same factor. But, the system worked better with Indian named enti-same reason; since Indian words are spelt in English in the same way as they are pro-ciently with named entities compared to generalized test data, since our purpose is not named entity transliteration but a general purpose text input system. We did not work on specifically transliterating named entities in this approach. Hence, in this perspec-tive, the results on the named entity datasets are encouraging enough. However, the input method for Indian languages. Possible alternatives to improve efficiency with named entities can be trying with a different metric, using a combination of metrics or using some heuristics in the existing method. 
The variation of accuracy of the system with word Levenshtein distance threshold has been plotted in the graph below: 
As it can be noticed from the graph, though the system did not perform well with an exclusively Named Entities based dataset, it performed considerably well with the general purpose dataset, which is a better representation of a general purpose data input needs of the user. Hence, we can claim that Levenshtein distance based translit-eration method can provide an efficient data input system for Telugu as well as other Indian languages. lugu, which can be easily extended to other non-Roman script based languages. Since it does not involve any learning on the part of the system, it does not put heavy weight on the system. In this approach, we have exploited the availability of a large word list for the language to design a new text input method. Having tried out different ap-proaches in using this word list, we have finally concluded that a Levenshtein distance based approach is most efficient of all. This is because of the relation between Leven-only valid words as suggestions and it gives the results instantaneously. 
We have to work on further improving the approach so as to make it work better with named entities too, since it will also help in creating gazetteers of named entities in Telugu. We have to test the system on larger datasets before making a full fledged application which uses this system. We plan to explore other distance metrics and/or using a combination of distance metrics to achieve a good working system. The sys-tem offers too many suggestions for words of smaller length. We have to look at al-ternative methods to deal with this problem. A proper ranking function needs to be designed to rank the results in an optimal order. becomes  X   X  X  X  X  /Ji-Cheng-Che/ 1  X  in Taiwan, and  X   X  X  X  /De-Shi/ X  in Singapore. It work has been reported on the same problem in transliteration. Transliteration is a only maintains the pronunciations, but sometimes also carries forward certain mean-ing that can be appreciated by native people [3 ]. It is common that an English word is usually transliterated into different Chinese variants in different Chinese-speaking communities or regions. For example, the movie Shrek becomes  X   X  X  X  X  /Shi-Rui-Ke/ X  in Taiwan and  X   X  X  X  X  /Shi-Lai-Ke/ X  in China. In many natural language proc-essing applications, we have to deal with such regional transliteration variants. A big transliteration lexicon would help us address this problem. However, new words emerge every day and it is not trivial to compile such a lexicon composed of the Eng-lish words and all their regional variants. One of the possible solutions is to automati-cally construct a transliteration lexicon of regional variants from the Web, which is a live source of new words.

Many studies on transliteration are focused on the generative transliteration models [4][5][6] which re-write a foreign word into the native alphabets. The generative modeling technique is found useful in handling out-of-vocabulary (OOV) words, such as personal names, place names and technical terms, in tasks such as machine generative models hinges on a manually validated bilingual lexicon, which is not easily available. 
The Web is one of the largest text databases for OOV named entities. It is also here is that these transliterations are not well-organized in a structured manner and at the same time they are artifacts created by many individuals across the globe, which transliterations from the Web for a single language pair, such as English-Japanese [9], English-Korean [6] and English-Chinese ( E-C ) [10]. However, to our best knowledge, there are not many studies on automatically constructing transliteration lexicons of regional variants or regional lexicons in short. In a regional lexicon, an entry called a tuple includes an English word and its regional variants, such as Shrek ,  X   X  X  X  X  /Shi-Rui-Ke/ X  (Taiwan) and  X   X  X  X  X  /Shi-Lai-Ke/ X  (China). 
The Web domain names, such as  X .cn X  and  X .tw X , provide us with a natural label-ing of geographical division, also referred to as language regions. In theory, one can extract transliterations from one regional Web, and then simply use English words as the pivot to construct a lexicon of transliteration variants. The performance achieved by this approach can be considered as a baseline. In addition to this basic approach, in domain knowledge encoded in a transliteration model to guide the Web search. This paper is organized as follows. In Section 2, we briefly introduce related work. In Section 3, we present how to apply knowledge to query formulation so as to guide the search. In Section 4, we propose a cross-training strategy, which exploits regional corpora, for phonetic similarity model training and transliteration extraction. In Sec-tion 5, we conduct experiments to validate the effectiveness of the proposed algo-rithms. In Section 6, we discuss some observations from the databases themselves. Finally, we conclude in Section 7. Learning regional transliteration lexicons involves both transliteration modeling and extraction. The former allows us to establish a similarity metric between bilingual words to ensure the quality of extraction, while the latter studies the effective retrieval techniques to provide the quantity of extraction. The grapheme-based [5], phoneme-techniques. In this paper, we focus on the retrieval techniques and adopt the phonetic similarity model (PSM) [10] to validate each extracted transliteration pair. PSM is also called transliteration model in the rest of this paper. Next we briefly introduce the prior work related to transliteration extraction. There have been many attempts to extract translations or transliterations from the Web, for example, by exploring query logs [9] and parallel [11] or comparable corpora [12]. However, most of them are for terms of a single language pair. On the other hand, Cheng et al. [1] proposed a transitive translation method by exploiting forward-backward collocation information and using English as a pivot language to construct a regional translation lexicon. However, this method relies completely on collocation information which is more suitable for the extraction of translation variants than that for transliteration. 
Some recent studies use information retrieval (IR) techniques for transliteration ex-traction. Sproat et al. [12] exploited both phonetic similarity and temporal distribution advocated a method to explore English appositive words in the proximity of Chinese proper nouns in mixed code text, also known as parenthetical translation. Phonetic similarity, temporal correlation and parenthetical translation are shown to be informa-tive clues for finding transliterations. Searching for transliterations from the Web, this end effectively, we propose a method for guiding the search of target translitera-tions in this paper. Constraint-based learning algorithms [14] , which impose constraints or introduce domain knowledge into the selection of candidates, have been successfully used in information extraction. The idea can also be applied to harvest regional transliteration variants to obtain quality candidates. In Web-based information retrieval, many documents are indexed by search engines; however, search queries are usually too short to reflect users X  intentions. It remains a challenge to construct adequate queries for effective search. We propose to introduce constraints that incorporate knowledge to formulate the queries. Now the problem is what kind of knowledge is useful and how to include them. Transliteration modeling represented by the phonetic similarity models encoding knowledge and deducing mapping rules of basic pronunciation units from the transliteration samples in the training pool is an excellent candidate. 
Taking an English word,  X  X bby X , as input, the generative model outputs Chinese transliteration candidates,  X   X  X  X  X   X ,  X   X  X  X   X ,  X   X  X  X  X   X ,  X   X  X  X   X  and  X   X  X  X  X  , X  re-ferred to as quasi transliterations as opposed to validated real-world transliterations, by adopting the joint source channel model [5] (http://hlt.i2r.a-star.edu.sg/transliteration/). Then, the queries are formed by augmenting the English word with each of the candi-dates to favor the desired returns. We expect that such a constraint-based exploration style, if we consider the baseline as using the English words as the queries and the E-C word pairs as the expanded queries, we expect to leverage the search of genuine trans-dated as search constraints. It is common that in Chinese/Japanese/Korean (CJK) predominant webpages, English words are used in parentheses near to the CJK transliterations to serve as appositive in a sentence as in Fig. 1. We adopt the strategy, known as recognition followed by vali-dation [10], to use such translation clue and the phonetic similarity to extract Chinese proximity of English and Chinese collocatio n. Then, we validate each candidate by using phonetic similarity clue in a hypothesis test. 
The learning of PSM transliteration model takes place as we extract translitera-works in an unsupervised manner with minimum human intervention. Now let X  X  ex-query returns from China and query returns from Taiwan. The transliterations which share the common English words form transliteration tuples. 4.1 Cross-Training Cross-training [15] was proposed for learning classifiers for partially overlapped or corre-lated classes. It considers each data set as a view to the problem to let classifiers help each 
Given a). An initial PSM trained on a small set of seed pairs; b). A snippet database; 
Learning transliterations c). Validate the transliteration candidates in the snippet database using PSM; d). Collect the automatically validated transliterations; e). Re-estimate the PSM from the validated transliterations with EM algorithm; f). Repeat Steps c), d) and e) until no new transliterations are found. other. Cross-training was used successfully in creating an integrated directory from multi-ple Web directories, such as Yahoo! Directory (http://dir.yahoo.com/) and Open Directory (http://www.dmoz.org/). It was also studied in the classification of Web pages [16]. information available in different regional data sets to boost the learning. Specifically, we expect the PSM learning from China corpus will help boost the PSM learning of Taiwan corpus, and vice versa, as shown in Fig. 3. 
We propose a cross-training algorithm that employs two learners (see the upper and training allows the two learners to exchange their correlated information over the problem in a collaborative learning process. It can be seen as an extension of the un-supervised learning, as summarized in Fig. 4. To construct a regional lexicon from the We b, we need to obtain a collection of snip-crawling. We explore two different crawling techniques: the autonomous exploration constraint-based exploration (CBE), which exploits domain knowledge encoded in the transliteration model to guide the crawling. 
In the AE procedure, we start with a small set of transliteration pairs as seed pairs to bootstrap the initial phonetic similarity model (PSM). The PSM is used to measure the phonetic similarity between an E-C pair of words. We also use the English words The validated transliterations are further submitted as queries for the new Web search. In this way, we submit queries and validate the extraction returns with the PSM. This process allows us to discover new transliteration pairs, such as  X  X ucas- X  X  X  X  /Lu-Ka-Si/ X  in Fig. 1 as the return of the search query  X  X obert X . 
Note that the retrieved snippets may contain both true and false transliteration pairs. The PSM model acts as a filter to sc reen out the false pairs. As AE procedure the seed query after a while. Therefore, the AE crawler is seen as an autonomous agent. To retrieve transliteration variants, we can launch AE crawlers at different regional Web divisions independently. The returns that share the common English word form tuples of transliteration variants. 
The independent AE-based crawling results from different crawlers do not guaran-tee anchoring at the same set of English words. To increase the possibility of desired expand the query so as to search towards th e genuine transliterations. In this way, we hints for CBE-based crawling. The idea is that a hint resulting from a reasonably good transliteration model [5] is better than no hint at all and helps find the target. We ex-pand the common set of English words with generated regional transliterations to database that share common English words. 
Note that there are differences between AE and CBE. For example, AE procedure CBE procedure, we can now move on with learning the PSM model and construction of regional lexicons. 
With the AE procedure, we first create a snippet database D AE consisting of two nese amounting to 22.5 GB in size; and CN AE from .cn domain, which has 450,150 snippets. Most of them are Chinese words mixed with English ones, as seen in Fig. 1. 
With the CBE procedure, we select top-1,000 boy X  X  and girl X  X  names of year 2006 from American Social Security Online 2 . There are 1,942 unique names in total. Then, we generate top-20 Chinese candidates for each English name. In this way, we obtain regional Web through a search engine. We obtain 38,840 returning files in traditional Chinese from .tw domain also called TW CBE(20) and 2.50 GB in size; 38,840 returning files in simplified Chinese from .cn domain also called CN CBE(20) and 4.15 GB in size. Each file has at most 100 snippets in this corpus. This database is called D CBE(20) .
To establish the ground truth for performance evaluation, we only select first 500 500(3) , with top-3 candidates, from CBE-collected data. For ease of reference, we summarize the statistics of the corpora in Table 1. Note that D AE-500 is a condensed TW CBE-500(3) from .tw domain and CN CBE-500(3) from .cn domain. The labeling sug-gests 6,176 and 76,850 valid pairs, with 3,118 and 22,330 distinct pairs, for D AE-500
We report the performance in F-measure, which is described in Eq. (1), where precision is defined as the ratio of extracted number of DQTPs (distinct quali-total extracted pairs and recall is defined as the ratio of extracted number of DQTPs over that of total DQTPs. Eq. (1) measures the performance of learning regional vari-ants on a pair basis similar to that in single language pair, i.e., the DQTPs from two different data sets with the same English word are pooled together. 5.1 Unsupervised Learning on AE Database As discussed at the beginning of Section 4, first, we derive an initial PSM using ran-domly selected 100 seed DQTPs for a learner and simulate the Web-based learning pairs to the DQTP pool; (iii) re-estimate the PSM by using the updated DQTP pool. We evaluate performance on the D AE-500 database in section 5.1 to section 5.3. 
To construct a regional lexicon, we run the extraction over each data set independ-ently in unsupervised learning and then use English as the anchor to establish the transliteration tuples. We run multiple iterations of unsupervised learning. At 6th this paper. 5.2 Cross-Training on AE Database Mandarin Chinese has been used prevalently in Taiwan and China and people in the two regions do not follow the same rules for E-C transliteration due to different local accents and cultural preference. Note that the phonetic features of the Chinese-speaking communities are similar. We expect the phonetic features from multiple regional sets will help each other X  X  learning as in Fig. 5. 
With cross-training, we run the same procedure as in section 5.1 for each of the re-gional data sets in parallel. Then, we can re-estimate the PSM of each learner with the measure performance is reported in Fig. 5 for unsupervised learning (UL) and cross-training (CT). The results show that the collaborative cross-training attains higher F-measure (0.510) than independent unsupervised learning (0.475). This suggests that cross-training benefits from le arners exchanging information with the other. 5.3 Cross-Training on CBE Database As discussed at the beginning of this section, the independent AE-based crawling result in an inefficient process when compiling the tuples of regional variants. On the other hand, the CBE-based crawling can be considered as a kind of focused crawling shown in Fig. 6 and report the F-measure for top-1 and top-3 results. At the 6 th itera-tion, the F-measure with top-1 candidate, denoted as UL(1), and top-3 candidates, denoted as UL(3), are 0.540 and 0.551, respectively, which outperform that on AE database. 
We further conduct an experiment with cross-training and report the performance measure by CT(3) is 0.573, which represents the best performance. 
It is observed that the experiments on CBE database show better results than those on AE database. Cross-training also consistently outperforms unsupervised learning in all cases. The performance of harvesting regional transliteration variants is inferior to that on single language pairs. This is easy to understand because the regional trans-language pairs that share the common English words. 5.4 Learning from the Web We have conducted several experiments on a local database. It would be interesting to see how the proposed algorithm works directly on the Web. We conduct an experi-ment using the same 100 seed pairs as described in section 5.1 to train an initial PSM on D CBE(20) , which is collected using CBE procedure with top-20 candidates. The ground truth is not available for the whole corpus, therefore, we only report the esti-mated precision by randomly sampling 500 tuples. There are 12,033 distinct tuples (43,369 distinct pairs) extracted, with the estimated precision of 0.834 on a tuple basis or 0.894 on a pair basis. The expected correct tuples (pairs) are 10,036 (38,768). Note learning process. Some selected entries of the regional transliteration lexicon are shown in Table 2. The statistics over regional transliteration variants may help understand the underly-
In the second row of Table 3, we report the average number of Chinese translitera-tions for an English word (#Trans). It shows that Taiwan databases that adopt tradi-tional Chinese characters have higher numbers of transliteration variants per English word than China databases that adopt simplified Chinese characters. It suggests that transliterations are less regular in Taiwan. From Table 3, we can expect that on aver-age we will obtain 1.247 regional variants for D AE and 1.451 for D CBE . It implies that we can obtain more transliterations by constraint-based exploration. 
The desire to achieve semantic transliteration [3] increases number of translitera-tion variants. Chinese has more than 4,000 frequently-used characters representing only about 400 syllables. That means, for each syllable, there could have multiple character choices. For example, an English name  X  X ophia X  is a girl X  X  name. To carry its feminine association forward to Chinese,  X   X  X  X  X  /Su-Fei-Ya/ X  is used in Taiwan and  X   X  X  X   X  /Suo-Fei-Ya/ X  is used in China. What is in common is that both choose characters of feminine connotation, except that the Taiwanese transliteration uses a Chinese surname  X   X  /Su/ X  to start the word, while the Chinese one does not. 
Despite many differences, the regional variants share many commonalities. Most of the Chinese characters carry meanings. For those that have positive or neutral connota-the top-5 characters in the four data sets. It is interesting to find that the top-3 charac-ters, i.e.,  X   X  /Si/, X   X   X  /Ke/ X  and  X   X  /Er/, X  are common across the four databases. We also find that some characters are used much more often than others. The top-20 char-acters account for 20.234%, 24.516%, 24.012% and 29.097% of the usage in the four data sets. Note that for comparison, all transliterations in Table 4 have been converted 24.881% of entries in CN AE-500 are the same as their counterparts in TW AE-500 . We also 
The correlation at character and transliteration level provides consistent informa-tion that allows the cross-training to boost performance. Although the rendering of Chinese characters are different, the underlying sound equivalence warrants the pho-netic exploration for the cross-training algorithm. This paper studies two techniques for crawling Web corpora, autonomous exploration (AE) and constraint-based exploration (CBE), which exploits transliteration knowl-edge to guide the query search, and two lear ning strategies, unsupervised learning and cross-training. Experiments show that the cross-training effectively boosts the learn-ing by exchanging correlated information across two regional learners. We find that the CBE-based crawling followed by cross-training technique is a practical method for harvesting regional transliteration variants. 
We have studied transliteration variants between Taiwan and China for simplifica-tion. The same technique is applicable to deriving regional variants for Chinese translit-erations of Hong Kong, Singapore and Malaysia as well. It would be an interesting work to apply the technique to other language pairs, such as North and South Korean, as well. The research in this paper was partially supported by National Science Council, Tai-wan, under the contract NSC 97-2218-E-033-002. CR F s are a structure l ear nin gt ool first in tr o duced in [1]. CR F s o fte no utper -f o rm ma xi mum e n tr o p yM ark ov m o de l(M E MM) [ 2 ], a no ther p o pu l ar structure l ear nin gmeth o d .T he ma in reas on i sthat , am on gd i rected graph i ca l m o de l s , CR F sd ono tsu ff er fr o mthe l abe l b i as pr o b l em as much as M E MM a n d o ther c on d i t ion a lM ark ov m o de l sd o[1].So far , CR F sha v ebee n successfu lin ag oo d n umber o fapp li cat ion s , espec i a lly in n atura ll a n guage pr o cess in g [3].
A sa ny o ther ge n era l-purp o se mach in e l ear nin gt ool, feature e n g in eer in g i s features fr o maut o c on structed ca n d i date set i sa no pe n pr o b l em s in ce [1]. How-th i spaper .B ecause o frap i dpr o gress o fm o der n c o mputer ma n ufacture tech -ce n t research m ov e , but i t i s no ta lw a y sc onv e ni e n tt o carr y am o de lwi th s o ma ny features .In th i sstud y, w e will c on s i der t o pru n ea n e xi st in gCR F sm o de l f o rst o rage reduct ion a n ddec o d in g speedup . Our purp o se i st o reduce the g iv e n w etr y t oin d i cate th o se m o st n ecessar y part in the m o de l.

T he m o st d iff ere n ce bet w ee no ur i dea a n dpre vio us wo rk , e i ther f o r w ard o r back w ard feature pru nin g ,i s that structura l fact o r i s involv ed in o ur c on s i der -at ion. T hus a s i mp l ecr i ter ion i spr o p o sed t o ra n kfeaturegr o ups rather tha n features that pre vio us wo rk usua lly f o cused on.

T he rema in der o f the paper i s o rga ni zed as f ollow s .S ect ion 2pr o p o ses a cr i te -c on c l udes the paper a n dd i scusses future wo rk . 2.1 CRFs G iv e n a nin put (o bser v at ion) x  X  X a n d parameter v ect o r  X  =  X  1 , ... , X  M , CR F s graph ,w h i ch represe n ts the in terdepe n de n c yo f y a n d x . c  X  C ( y , x ). Z a ll o utput v a l ues , Y .
 Alo g -lin ear c o mb in at ion o f w e i ghted features , v ect o r o bta in ed fr o mthec o rresp on d in gc li que c .I thasbee n pr ov ed that the f o rm in equat ion ( 2 )i sasuffic i e n ta n d n ecessar y c on d i t ion t o guara n tee the e x p (  X F ( y, x )), w here F ( y , x )= c f c ( y , x )i stheCR F X  sg lo ba l feature v ect o r f o r x a n d y .
 Z w eca no bta in the f ollowin gd i scr i m in a n tfu n ct ion f o rCR F s : 2.2 Pruning via Ranking Feature Groups In equat ion s (1) a n d ( 2 ),  X  c ( y , x ;  X  )i s o fte n re w r i tte n as t wo parts , w here In ab ov eequat ion s , f k ( y, x )) i sastatefeaturefu n ct ion that uses only the l abe l perf o rmed on them .In pract i ce , state features o fte n c ov ers the m o st part o fa ll on es in ag iv e n m o de l. T hus , the pru nin gm o st ly a i ms at state features . b in ar y f o rm , tra inin g i sc o mp l eted .

Two n atura lw a y sarec on s i dered f o rthem o de l pru nin g . O n e i sbased on the c on d i t ion H that determ in es the feature .F eature c o u n tcut -off acc o rd in g t oi ts o ccurre n ce in the tra inin gdata i ssuchameth o d .T he o ther i sbased spect iv efeature i s .I t seems that w eca n ra n ka ll features s i mp ly acc o rd in g acter i st i cs are add i t ion a lly involv ed f o rthef o rmer .Fo re x amp l e ,M ark ovi a n a n ddec o d in g will in e vi tab ly l ead t o adramat i c decrease o fperf o rma n ce in m o st cases .

H a vin gseque n ce l abe lin gtaskasa n e x amp l e ,w ema y regard the dec o d in g ov er the g iv e n structure defi n ed b y CR F s appr oxi mate ly as t wo-stage pr o ce -T he sec on d stage will fi n daser i es o f l abe l s wi th the ma xi ma ljoin tpr o bab ili t y thr o ugh search in gapath ov er the matr ix c on structed b y these b o u n dar y pr o bab ili t i es .

W e will f o cus on the first stage s in ce i ts o utput c on s i sts o f the bas i s o fthe search in the sec on d stage .A s w eca nno t determ in ethee x act l abe l f o rac li que w. r . ts o me H , afeaturegr o up 1 .H ere feature gr o up pru nin g rather tha n feature pru nin gmea n sthata ll features act iv ated acc o rd in gt o the predefi n ed c on d i-t ion H ov er x will be d i scarded in dec o d in g .W he n t wo gr o ups o ffeatures , f H 1 a n d f H 2 , are act iv ated f o rac li que c ,o ur quest ion will be ,w h i ch on e will be features , w here  X  H ( y )i sthec o rresp on d in g w e i ght f o rfeature f H ( y ), a n da v g (  X  H )= N  X  y  X  Y must o ccur in the tra inin gdata .W e hereafter will keep th o se gr o ups cr i ter ion f o rmu l a ( 7 )in the reduced m o de l. 3.1 Settings pru nin gmeth o dthr o ugh l ear nin ga n ddec o d in g in o rder -1 lin ear -cha in CR F s . seque n ce l abe lin gtasks in Ch in ese ,wo rd segme n tat ion (WS) a n d n amed e n t i t y rec o g ni t ion (N ER ), are e v a l uated .Two data sets o f wo rd segme n tat ion, AS a n d MS R A, are fr o msharedtask B ake off-2 3 , a n dt wo data sets o f n amed e n t i t y rec o g ni t ion, C i t yU a n d MS R A, are fr o m B ake off-3 4 , as summar i zed in T ab l e 1 wi th c o rpus s i ze in n umber o f characters ( t o ke n s ). T he perf o rma n ce o fb o th WS a n d N ER i smeasured in terms o fthe F-measure F = 2 RP/ ( R + P ), w here R a n d P are the reca ll a n dprec i s ion o fsegme n tat ion o r N ER .

E xi st in g wo rk sh ow sthatb o th WS a n d N ER f o rCh in ese ca n be e ff ect iv e ly f o rmu l ated as character tagg in gtask [9,10,11,1 2 ]. A cc o rd in gt o these resu l ts , espec i a lly fr o mthe l atter ,w euseaset o fcarefu lly se l ected l abe l set a n dc o rre -sp on d in g feature sets t o tra in m o de l f o rtheset wo tasks , respect iv e ly. W e will sh ow that the m o de l pru nin g i sst ill e ff ect iv ee v e nin these m o de l sthatca n t ion in a wo rd i skeptus in gf o r wo rd segme n tat ion task as in [11,1 2 ]. W eha v e sh ow that 6-tag set ca n br in g state -o f -the -art perf o rma n ce s in ce o ur pre vio us wo rk in [10,11]. I ts s ix tags are B , B 2 , B 3 , M , E a n d S .Fo r N ER ,w e n eed t o te ll apart three t y pes o f N Es ,n ame ly, person , location a n d organization n ames . C o rresp on d in g ly, the s ix tags are a l s o adapted f o r characters in these N Es but non e N E characters , a l t o gether w eha v e 19 tags f o r N ER .T he f ollowin ge x amp l e ill ustrates h ow characters in wo rds o f v ar io us l e n gths are tagged in aseque n ce f o r wo rd segme n tat ion l ear nin g . An dth i s i sa n e x amp l ef o r N E tagg in g . f o rb o th tasks .A sf o r N ER , fi v eu n super vi sed segme n tat ion features ge n erated in tr o duced as in [1 2 ].
A perf o rma n ce c o mpar i s on o f o ur tra in ed m o de l(wi th o ut a ny pru nin g ) a n d o ther best e xi st in gresu l ts i sg iv e ninT ab l e2 .T h i sc o mpar i s on sh ow sthat w e perf o rma n ce . 3.2 Pruning Results No te that a ll these m o de l sc on ta in m illion s o ffeaturegr o ups .

A cc o rd in gt o the ra n k in gmetr i c in ( 7 ), w erem ov ethem o de l step b y step a n d o bser v eh ow the perf o rma n ce cha n ges . Our e x per i me n ta l resu l ts sh ow that a ny perf o rma n ce lo ss i s no te n c o u n tered u n t il pru nin grate i s l arger tha n 65% f o rt wo WS tasks a n d 90% f o rt wo N ER tasks .T hese resu l ts are sh own in Fi gure 1( a ). T h i s in d i cates that these m o de l sareh i gh ly redu n da n t .
In Fi gure 1( b ), w e keep fe w feature gr o ups wi th t o psc o res a n d o bser v eh ow the 1/50 features ca n g iv eab ov e 9 7 % perf o rma n ce in a ll tasks .T he v a l ue 9 7 % a n d F-sc o re rate in Fi gure 1( b ) are c o mputed in th i s w a y: d ivi de F-sc o re wi th 1/50 o rs o me o ther am o u n t o ffeaturesb yF-sc o re wi th fu ll features .

A sac o mpar i s on, w ec o mpare the pr o p o sed meth o d wi th feature c o u n tcut -off 5 .W epru n ethem o de l acc o rd in gt o the pr o p o sed ra n k in gmetr i c wi th the same rate as cut -off thresh ol ds are set t o 2 ,3, 4 , a n d 5, respect iv e ly. T he pru nin grates o feachcut -off thresh ol ds are g iv e ninT ab l e 3. T he perf o rma n ce c o mpar i s on bet w ee no ur meth o da n dcut -off meth o dare ill ustrated in Fi gures 2 .W efi n d that the s i mp l ecut -off acc o rd in gt o the o ccurre n ce t i mes o ffeaturesma y cause ser io us perf o rma n ce lo ss ,w h il e o ur pru nin gmeth o d only cause li tt l ef o rthesame pru nin grate .

T he e x per i me n ta l resu l ts ha v esh own that the pr o p o sed meth o d i se ff ect iv e a n dCR F sm o de l sthat w ead o pt at l east are h i gh ly redu n da n t .W ed on X  tg iv e the resu l ts ab o ut dec o d in gspeedupafterm o de l pru nin g , because dec o d in g speed am o de l sure ly he l ps speedup dec o d in gs in ce the search space f o rdec o d in g i s n arr ow ed . scheme ,no tas in g l e feature but a feature gr o up i sp i cked up f o rpru nin g .A st o [ 4 ,1 4 ].

Bo th m o de l pru nin ga n dfeaturese l ect ion n eed a ra n k in gmetr i ct o e v a l uate pr io r .Ino rder t o make the ga in c o mputat ion stractab l e , the li ke li h oo d i sap -pr oxi mated b y apseud oli ke li h oo d .In feature se l ect ion, th o se feature ca n d i dates wi th h i ghest ga in are added in t o the o pt i ma l subset . Rece n t ly, b oo st in gtech -ni ques are pa i dm o re a n dm o re atte n t ion a n dapp li ed t o CR F stra inin g speedup parameter est i mat ion f o rCR F s .In the i rf o rmu l at ion, t o ch oo se a g oo dfeature , a w e i ghted l east -square -err o r (WLS E ) pr o b l em sh o u l dbes olv ed , w here w i a n d z i are t wo parameters that ca n be c o mputed as in Lo g i t Boo st ametr i csc o re .

CR F s l ear nin g i s no t o fte n a n eas y c o mputat ion a ljo b in ma ny cases as w e n eed ma l feature subset f o rCR F s i se v e n harder task tha n CR F stra inin g i tse l f .Fo re x-in c o mputat ion. T hus ,w eca n regard pru nin ga n e xi st in gm o de lwi th m illion s o f ma no bser v at ion, w h i ch i ssure ly atractab l ec o mputat ion a l task .
Our resu l ts sh ow sthatafe w features c on tr i bute a great dea l t o the perf o r -ma n ce a n de xi st in gCR F sm o de l sthat w eha v ee x am in ed in th i s wo rk at l east perf o rma n ce . W epr o p o se a p o ster io rpru nin gmeth o df o rCR F sm o de l. CR F s Mo de l sf o rarea l t wo seque n ce l abe lin gtasks ,n ame ly, seque n ce segme n tat ion a n d n amed e n t i t y
C o mpared t o the e xi st in g , the pr o p o sed pru nin gmeth o d i seffic i e n t because only a lo ca l metr i c o f each feature gr o up n eeds t o be c o mputed bef o re a s o rt in g o perat ion i sperf o rmed .Fo rh i gher perf o rma n ce a n dam o re c o mpact s y stem ,i t o fc o mputat ion a l res o urces ,w eha v et ol ea v eth i sas on e o f o ur future wo rk . CR F s l ear nin g rather tha n g lo ba l stat i st i ca lin f o rmat ion as m o st o thers .
T h o ugh w echeckthee ff ect iv e n ess o f the pr o p o sed ra n kmetr i c only f o rCR F s , l ear nin g schemes .

Ano ther i ssue ab o ut future wo rk i s that there are ma ny o ther l ear nin gtech -ni ques that n atura lly pr o duce sparse s ol ut ion ssuchass o me l az y-update a l g o-r i thms .Ty p i ca lly, the structured a v eraged perceptr on o f [15] ge n era lly yi e l ds m o de l s wi th v er y fe w act iv efeatures ( usua lly bet w ee n1% a n d 10% act iv eac -c o rd in gt o the i rrep o rt ), s in ce the parameters are updated only in the case that atra inin gerr o r o ccurs .T hus ,on ec o u l ds i mp ly tra in the perceptr on a n dd i scard a ll features wi th zer ow e i ght ,o bta inin gam o de lwi th i de n t i ca l beha vio ra n dfar fe w er features .How e v er , th i s i sbe yon d w hat w e in te n dt o stud y ab o ut CR F s be a l s ol eft as future wo rk .
 Query-oriented summarization is promoted by the Document Understanding Conferences (DUC). It aims to produce a brief and well-organized summary for a cluster of relevant documents according to a given query that describes a user X  X  them concentrate on evaluating the relevance of the sentence to the query by and/or developing evaluation methods that can effectively calculate the combinational effects of the features. The most significant sentences are often picked out to produce a summary. We call this sort of models feature-based models. While feature-based models evaluate the sentences relying on the individual sentences themselves, in the recent past, there is growing interest in graph-based models where the links among the sentences are also taken into account an d thereby the information embodied in the entire graph can be utilized. In other words, the graph-based models emphasize and feature-based models which are normally link-free . 
Existing graph-based approaches to query-oriented summarization basically model the documents as a weighted text graph by taking the sentences measured by their relevance to the query as the nodes and the connection between the sentences measured by sentence similarity or association as the edges. The graph-based ranking algorithms such as PageRank are then applied to identify the most significant sentences. These approaches are inspired by the idea behind web graph models which have been successfully used by current search engines. In web graph models, the web is considered as a directed graph, na mely web graph, where the web pages are regarded as the nodes and the hyperlinks among the web pages as the edges. Intuitively, graph-based summarization models work well because it takes into account global information recursively comput ed from the entire text graph rather than only relying on the local context of a single sentence. However, we argue that we need a critical investigation on the difference between the web graph and the text graph before we directly adapt the PageRank algorithm from web graph models to text graph models. We think the text graph is in essence different from the web graph in the following two aspects. links on the web graph are certain. They are determined by the hyperlinks and can be changed when the hyperlinks are inserted, modified or removed. Even though the two web pages may not be directly linked, it is still possible for one to reach the other by traveling through some intermediate pages and the links among them. Traditional similarity of a sentence pair is significant enough and they are no longer changed once the graph has been constructed. Accordingly, any two sentences must be evaluated for potential connections in the text graph. This makes the indirect paths between two sentences makes less (or even no) sense than the indirect paths between two web pages. Say, for example, importance would be repeatedly propagated if there are a direct link and many indirect links between the sentences. Propagation to neighbors only would be more reasonable. Second, the weights of the links are interpreted differently. The (normalized) edge weight in the web graph can be naturally interpreted as the transition probability between the two web pages. In contrast, the edge weight in many different ways to define the similarity or the distance. 
We highly support the idea of combining both the link-aware and link-free oriented summarization in this paper. Based on the difference between the text graph and the web graph discussed previously, we suggest evaluating sentence significance based on the neighborhood graph. The significance of a sentence is evaluated according to its own value and the propaga tion effects from its near neighbors. Compared with the iterative PageRank algorithm, this neighborhood based one-time propagation algorithm is of higher computing efficiency and meanwhile it also allows both the link information and the individual sentence features well used. Based on the neighborhood graph model, we pay special attentions to how to deign the distance function and study different approaches to co nvert the existing similarity measures to the distance measures. More important, in order to better incorporate query influence in the graph, we design a query-sensitive similarity measure to replace the commonly used query-unaware similarity measure. 
The remainder of this paper is organized as follows. Section 2 reviews existing graph-based summarization models and the corresponding ranking algorithms. Section 3 introduces the proposed sentence ranking algorithm and the query-sensitive Section 5 concludes the paper. Graph-based ranking algorithms such as Google X  X  PageRank [1] have been success-fully used in the analysis of the link structure of the WWW. Now they are springing up in the community of document summarization. The major concerns in graph-based summarization researches include how to model the documents using text graph and how to transform existing page ranking algorithms to their variations that could accommodate various summarization requirements. 
Erkan and Radev [2] [3] represented the documents as a weighted undirected graph by taking sentences as vertices and cosine similarity between sentences as the edge weight function. An algorithm called LexRank, adapted from PageRank, was applied select summary sentences. Meanwhile, Mihalcea and Tarau [8] [9] presented their PageRank variation, called TextRank, in the same year. 
Likewise, the use of PageRank family was also very popular in event-based summarization approaches [4] [5] [13] [15]. In contrast to conventional sentence-based approaches, newly emerged event-based approaches took event terms, such as verbs and action nouns and their associated named entities as graph nodes, and connected nodes according to their co-occurrence information or semantic dependency relations. They were able to provide finer text representation and thus could be in favor of sentence compression which was targeted to include more informative contents in a fixed-length summary. Nevertheless, these advantages lied on appropriately defining and selecting event terms. 
All above-mentioned representative work was concerned with generic summarization. Later on, graph-based ranking algorithms were introduced in query-oriented summarization too when this new challenge became a hot research topic recently. For example, a topic-sensitive version of PageRank was proposed in [10]. The same idea was followed by Wan et al. [14] and Lin et al. [7]. Given three sets of chronologically ordered documents, Lin et al [7] proposed to construct timestamp graph for the update summary task by incrementally adding the sentences to the graph. Different from generic summarization, query-oriented summarization is necessarily driven by queries. Although the query effects can be modeled in a text terms of sentence relevance to a given query in all current published work. We argue nodes. Sentence similarity should be a measure not only replying on the content of the two sentences involved, but also conditioned on query requirement. 
Measuring similarities between two text units, such as documents, sentences, or terms, has been one of the most fundamental issues in information retrieval. While a large amount of related work was found in literature, little of them considered user. Tombros and Rijsbergen [11] [12] pioneered in development of query-sensitive similarity functions. They combined the traditional cosine similarity between the pair of documents with the collective similarity of the two documents and the query documents and the query. Later, Zhou and Dai [16] proposed a query-sensitive similarity measure for content-based image retrieval based on Euclidean distance which was widely used in image processing. The task of query-oriented summarization defined in DUC is to generate a summary documents D . The query usually consists of one or more interrogative and/or narrative sentences. The generated summary is restricted to a given length in words. So far, extractive summarization has been the predominant technique in query-oriented summarization, in which the most critical process involved is sentence ranking. We will introduction our sentence ranking strategy in the following sections. 3.1 Neighborhood Graph Based Sentence Ranking Given a query q and a cluster of relevant documents D consisting of a set of sentences { s , s 2 , ..., s n } and each sentence s i is associated with a weight w( s i ), 
We start with the following definitions and then discuss weight propagation among them. Definition 1 (Eps-Neighborhood of a sentence) : The Eps-Neighborhood of a sentence s (indicated by Definition 2 (Neighborhood Graph for a sentence set) : The neighborhood graph for a set of sentences D (indicated by  X  G ), is a undirected graph that satisfies (1) each node and its length varies in the proportion of the distance between the two sentences that the edge connects. The Eps-Neighborh ood is a symmetric relation according to Definition 1. So ) (
In a neighborhood graph, as seen in Figure 1, sentences are connected to their neighbors within the defined distance. Connections bring along influence on one another. The influence in turn changes the weight of each individual sentence to some The degree of the influence between them in the two directions may not equal when s i and s j are of different weights. Definition 3 (Influence Damping Factor) : Given ) ( influence on each other in terms of distance is a symmetric distance damping function defined by where  X  is a parameter to smooth the distance. We can imagine that the weight w( s j ) spread out from s j to s i and its influence is damped with ) , , (  X  influence becomes ) ( ) , , (
As a result, the entire weight of s i (normally called sentence significance SIG ( s i )) is adjusted by adding the cumulative influence from all the sentences in its Eps-Neighborhood to the original weight of itself. The following equation is deduced. 
We obtain the distance function required in equation (2) from the similarity  X  ( s i , s j )). where 3.2 Query-Sensitive Similarity Existing similarity measures produce static and constant scores. However, we believe that similarity between the two sentences s i and s j should be adjusted when the query q is involved. This observation is extremely important for studying query-oriented summarization models. 
Intuitively, the query-sensitive similarity measure should consist of two parts, i.e. the query-independent part and the query-dependent part. The query-independent part concerns the dedication of query-unaware similarity, while the query-dependent part further highlights the contribution of the terms in common not only in s i and s j but also in q . Formally, the query-sensitive similarity can be formulated as )) | , ( ), | , ( ( ) | , ( q s s sim q s s sim f q s s sim
Let } ,..., , { word vectors. the following weight coefficient function where 0&lt; not cope with, i.e.,  X   X  = . In this case 
Then, we define the query-sensitive similarity function as When we move from query-unaware similarity equation (6). Proposition 1. The query-sensitive similarity defined by equation (7) ranges from () as the cosine similarity between the two corresponding vectors. Proof: Let 
Then, There are three parameters in Proposition 1, i.e.  X  , meaningful and can be appropriately determ ined according to the practical application requirements.  X  is the contribution degree of the original query-unaware similarity lower and upper bounds of the contribution from the query-dependent part. The advantage of making the proposed query-sensitive similarity bounded is that we can mentioned parameters with appropriate valu es. Note that, these values are usually pre-assigned according to th e specified applications. 
Now, we integrate the query-sensitive similarity into the neighborhood graph based sentence ranking algorithm introduced before. Equation (2) and (3) are re-formulated as Experiments are conducted on the DUC 2005 50 document clusters. Each cluster of documents is accompanied with a query description representing a user X  X  information need. Stop-words in both documents and queries are removed 1 and the remaining words are stemmed by Porter Stemmer 2 . According to the task definition, system-incrementally add into a summary the highest ranked sentence of concern if it doesn X  X  significantly repeat the information already included in the summary until the word officially adopted for DUC automatic evaluations since 2005, as the evaluation metric. 
In the following experiments, the sentences and the queries are represented as the similarity. The sentence-level inverse sentence frequency (ISF) rather than document-level IDF is used when dealing with sentence-level text processing. 4.1 Experiments on Ranking Strategy We first evaluate the proposed neighborhood-based weight propagation ranking strategy (denoted by NM). For comparison purpose, we also present the results of another two widely used ranking strategies. One is to simply rank the sentences based on their relevance to the query (denoted by QR). The other is the PageRank deduced 0.85 according to the previous literature. Ta ble 1 below shows the results of average recall scores of ROUGE-1, ROUGE-2 and ROUGE -SU4 along with their 95% confidence intervals within sq uare brackets. In these experiments, the distance parameter is set to 1. As shown in Table 1, both NM and PR are able to produce much better results than QR. In particular, NM outperforms QR by 5.53% of the improvement in ROUGE-1, 15.96% in ROUGE-2, and 9.85% in ROUGE-SU4. The contribution of link-aware weight is remarkable. Besides, it is also observed that NM outperforms PR by 2.54% of the increase in ROUGE-1, 6.21% in ROUGE-2 and 3.37% on ROUGE-SU4. The improvement is very encouraging to us. 4.2 Experiments on Query-Sensitive Similarity Table 2 below compares the results of NM with query-unaware (i.e. NM) and query-sensitive similarity (called QsS-NM) measures. The three parameters in the query-sensitive similarity measure are set to  X  =0.8, We are happy to see that the improvements from the query sensitive similarity in NM indeed exist. The performance raised is consistent though not quite significant. results and the similarity measure itself only functions as a part of significance evaluation, the further improvement based on NM may become much hard-earned. We can conclude that the query-sensitive similarity is a direction worth further extensive study. More important, it can be applied in many applications other than query-oriented summarization. 4.3 Experiments on Distance Measure The distance measure required in Influence Damping Factor needs to be calculated from the similarity measure. The cosine similarity has been recognized as a remarkably versatile and popular similarity measure in text processing. However, there is no canonical best conversion between the similarity and distance functions. In the following table, we examine three different approaches. They are (1) our proposed Sine of the angle between the two vectors (i.e. SIN) which can be regarded as a distance (normally used in pattern recognition and data mining applications) and the cosine similarity (normally used in information retrieval and text mining applications). 
The parameter settings here are the same as those in Section 4.1 and 4.2. We can see in Table 3 that SIN is favorable in both NM and QsS-NM although the difference between the three distance functions is not significant enough. This observation is interesting and motivates us to further investigate. 4.4 Comparison with DUC Systems Thirty-one systems have been submitted to DUC for evaluation in 2005. Table 4 compares neighborhood models with them. To provide a global picture, we present the following representative ROUGE results of (1) the worst-performed human summary (i.e. H), which reflects the margin between the machine-generated summaries and the human summaries; (2) the top five and worst participating systems according to ROUGE-2; (3) the average ROUG E scores (i.e. AVG); and (4) the NIST baseline which simply selects the first sentences in the documents. We can then easily locate the positions of the proposed models among them. 
It clearly shows in Table 4 that both QsS-NM and NM outperform the first-ranked system (i.e. S15). QsS-NM is above S15 by 7.86% in ROUGE-2 and 3.12% in ROUGE-SU4. Even NM can also have 6.21% increase in ROUGE-2 and 2.58% in ROUGE-SU4. These are definitely exciting achievements since the best system (i.e. S15) is only 1.12% above the second-best system (i.e. S17) on ROUGE-2 and 1.46% on ROUGE-SU4. Graph based models and algorithms have been an interesting and promising research topic in text mining. In this work, we develop a novel sentence ranking algorithm based on neighborhood graph models for query-oriented summarization. The main contributions are (1) examining the problems in the existing graph-based summarization methods, and then proposing a new ranking algorithm based on neighborhood weight propagation; (2) designing a query-sensitive similarity measure that incorporates the query information into graph edges. The ROUGE evaluations on DUC 2005 dataset show that the proposed method is workable which can outperform the best participating system in DUC competitions. The research work presented in this paper was partially supported by the grants from NSF of Hubei, China (Project No: 2008CDB343), NSF of China (Project No: 60703008), RGC of HKSAR (Project No: PolyU5217/07E) and the Hong Kong Polytechnic University (Project No: A-PA6L). Wi th the rap i dde v e lo pme n t o f research a n dtech nolo g yinv ar io us fie l ds , a n d c ov er in gthesamet o p i c .W he n apers on w a n ts t o get a c o mprehe n s iv ere vi e w ab o ut a t o p i c , he has t o read as ma ny as p o ss i b l ere l ated d o cume n ts in o rder no tt oo m i ta ny v a l uab l ep oin ts .W he n read in gad o cume n t , he ge n era lly reads paragraph b y paragraph in o rder t o capture the ma in p oin ts o ftheauth o rs .Allo f o ther NLP tasks .Fo re x amp l e , search e n g in es ca n ach i e v eah i gher retr i e v a l summar i zat ion a n dse v era lo ther NLP tasks ( e . g ., in f o rmat ion e x tract ion a n d quest ion a n s w er in g ) ca n rec i pr o ca lly b oo st each o ther  X  sperf o rma n ce .
Wi th te x t as the defau l tmater i a l med i um , summar i zat ion i sdefi n ed as a reduct iv etra n sf o rmat ion o fs o urce te x tt o summar y te x tthr o ugh c on te n tc on-the s o urce [ 2 0]. So summar i zat ion f o cuses on gett in gthema in mea nin g o fad o c -represe n ts on ep i ece o f the message (o rk nowl edge ) tra n sm i tted in w r i t in g o r B ased on th i s assumpt ion, the i ssue o f summar i z in gate x tbec o mes the i ssue o f fi n d in gtheset o fte x tsegme n ts wi th the l argest qua n t i t yo f in f o rmat ion u n der summar i zat ion appr o ach us in ga n e wwo rd s i g ni fica n ce measure based on wo rd in f o rmat ion.

T he rest o f the paper i s o rga ni zed as f ollow s .S ect ion 2prese n ts a re vi e wo f re l ated wo rk on summar i zat ion, pr ovi d in greaders wi th a c o mprehe n s iv esce -i t i es f o r future wo rk . Research on summar i zat ion ca n be traced back t oL uh n[13]inw h i ch wo rd fre -summar y creat ion. In the past ha l fce n tur y, wi th the in creas in g n eed f o raut o-mat i cd o cume n t summar i zat ion, m o re a n dm o re researchers ha v epa i d atte n t ion t oi t , resu l t in g in a l arge b o d yo f li teratures .Do cume n t summar i zat ion ca n be d o es no tappear in the s o urce .In add i t ion, as y stem i s referred t o as ge n er i c summar i zat ion w he ni ts purp o se i st o capture the ke y mea nin g o f in put s o urces et a l. [1] a n d S parck Jon es [ 2 1] g iv eadeta il ed sur v e yo fthee xi st in g research in th i sarea .

H ere ,w ef o cus on re vi e win gth o se appr o aches w h i ch share a c o mm on frame -t o pr o duce a summar y. No rma lly, the f o rmu l aass i g n sasc o re t o each se n te n ce w hereas wi th regard t o ke ywo rds , a ll k in ds o f measures are ut ili zed t o sc o re a ca n d i date , thereb y mak in gke ywo rds p l a y aspec i a l r ol e in such sc o r in gs .T he paper .

To sum up , age n er i c summar y a i ms t o d i st ill the ov era ll mea nin g o fad o cu -vi ta l r ol e in such se n te n ce se l ect ion. f o rmu l a .T he i mp o rta n ce o feachc o mp on e n t wo rd o fase n te n ce i smeasured d o cume n tt o be summar i zed , the n ra n keachse n te n ce us in gthec o mb in at ion o f d o cume n ts i sc on ducted t o sh ow that wo rd in f o rmat ion i sapr o m i s in gmeasure f o r summar i zat ion. 3.1 Measure for Word Significance In NLP tasks , a l a n guage i sreprese n ted b y ahugege n era l c o rpus in that l a n-guage .Allo fthe wo rds fr o mthatc o rpus f o rm a harm onio us s y stem w here each pr o bab ili t y( a l s o ca ll ed re l at iv efreque n c y) in the ge n era l c o rpus . In o ur appr o ach , f o ra wo rd  X  s in f o rmat ion 2 used in NLP tasks ,w epr o p o se a n e w f o rmu l at o redefi n e i ts qua n t i ficat ion us in g ( 2 ). W here p d ( w ) a n d p G ( w ) de no te wo rd w  X  sre l at iv efreque n c yin ad o cume n tt o be summar i zed a n d in the ge n era l c o rpus G , respect iv e ly, a n d  X   X  log 2 p G ( w ) X  represe n ts a wo rd  X  s in f o rmat ion in the ge n era l c o rpus .

In o rder t o create a n appr o pr i ate summar y, o ur o b j ect iv e i st o e x tract the the ge n era l c o rpus a lon e , the ni thasa ni de n t i ca l sc o re no matter in w h i ch d o cume n t o re v e ninw h i ch ge n re o fd o cume n ts .I tmea n sthata wo rd p l a y sa t o re in f o rce the in f o rmat ion metr i cfr o m the perspect iv e o fa wo rd  X  sr ol e in a etc . E v e ni f i t i sc o mp o sed o faset o fd o cume n ts , the n umber o f wo rd t o ke n s i tc on ta in s i smuchsma ll er tha n that o fthege n era l c o rpus .A cc o rd in g ly, the that on a l a n guage .
 tr i but ion s p a n d q i sdefi n ed as (3), w h i ch measures the a v erage n umber o fb i ts n eeded t oi de n t i f y a n e v e n tfr o maset o fp o ss i b ili t i es .
 p a n d q must d i str i bute ov er the same pr o bab ili t y space .Ano ther measure tak in g i s w  X  sfreque n c yini ts d o ma in d o cume n t , N the t o ta ln umber o fd o cume n ts in ac oll ect ion, a n d d(w) the n umber o fd o cume n ts c on ta inin g w . me n ts in ac oll ect ion. I tseemsthatthere i s no c o rresp on de n ce bet w ee n a wo rd  X  s metr i cs in the f ollowin ge v a l uat ion s . 3.2 Sentence Ranking Formula the n umber o f wo rds in se n te n ce s , I ( w )i ssett o1w he n w i sake ywo rd ,o ther wi se 0, in f o ( w )i sequ iv a l e n tt oin f o( w )in( 2 )w he n w i sake ywo rd ,o ther wi se 0. p o s i t ion will be added t o the f o rmu l a , resu l ted in (6), based on the o bser v at ion st yl e o fa n e w srep o rt .

Fo raquer y-based summar i zer , quer i es p l a y ad i rect iv er ol e in summar y pr o-o fc o mm on u ni grams bet w ee n s a n d Q ov er the t o ta ln umber o fu ni grams in s t o the ma xi m i zed mutua lin f o rmat ion [3] sc o re c o mputed fr o ma wo rd pa i r li st in w h i ch each pa i r i sc o mp o sed o f w a n deachc o mp on e n t wo rd o fquer y Q .A MI sc o re i sca l cu l ated in ad o cume n tt o be summar i zed wi th a wo rd win d ow o f s i ze 10 as the c o-o ccurre n ce sc o pe .
 3.3 The Preprocessing and Sentence Selection D ur in gac o mp l ete summar y creat ion pr o cess , bes i des the ab ov eme n t ion ed ke y them ;( c ) E x c l ude st o p wo rds ( fu n ct ion wo rds such as  X  the  X ,  X  but  X , etc ) fr o m tha n athresh ol d v a l ue .
 4.1 Evaluation Tasks and Measures s in g l ed o cume n t summar i zat ion a n dquer y-based mu l t i-d o cume n t summar i za -t ion. Do cume n t Un dersta n d in gC on fere n ce (DU C ) 3 i sa i med at pr ovi d in gate x t a n de n ab lin gm o re researchers t o part i c i pate in a n dt o make further pr o gress in th i sarea .Ino rder t o make the perf o rma n ce c o mpar i s on wi th o ther e xi st in g g iv e ninDU C .T he ge n era l c o rpus used in o ur appr o ach i sthe BN C [ 2 ].
RO U GE ( Reca ll-Or i e n ted Un derstud y f o rG i st in gE v a l uat ion)[1 2 ], i sapack -age wi de ly used in rece n td o cume n t summar i zat ion e v a l uat ion ssuchas DU C . RO U GE sc o res ha v ebee n sh own t o c o rre l ate v er yw e ll wi th ma n ua l e v a l ua -a summar y b y c o u n t in gthe n umber o f ov er l app in gu ni ts bet w ee ni ta n d i dea l summar i es created b y huma n s .I t in c l udes f o ur d iff ere n tmeasures ( RO U GE -N, RO U GE -L, RO U GE -W, a n dRO U GE -S) t o retur n a reca ll sc o re on N-gram , the b y c o mpar in g the summar y t o be e v a l uated wi th the i dea l summar i es .S e v era l measures o fRO U GE are app li ed as e v a l uat ion measures in o ur e x per i me n ts . 4.2 Generic Single Document Summarization T he ge n er i cs in g l ed o cume n t summar i zer based on (6) i sc o mpared wi th the on e ta inin g ,o rbe in gc on ta in ed .B ased on th i s assumpt ion, three graphs are bu il tt o c on duct d o cume n t summar i zat ion a n dke ywo rd e x tract ion. T he s y stem i sre -p o rted t oo utperf o rm se n te n ce ra n k appr o ach [15] a n d mutua l ra n k appr o ach T he c o mpar i s on e x per i me n ts are carr i ed o ut f ollowin gthetask 1o f DU C 0 2 . P r ovi ded 56 7E n g li sh n e w sart i c l es , task 1 a i ms t o create a ge n er i c summar y f o reachd o cume n t wi th a l e n gth o f appr oxi mate ly 100 wo rds .B es i des the same data ,w ead o pt the same RO U GE measures as w e ll as a set o f i de n t i ca l ru nnin g parameters f o rRO U GE in the c o mpar i s on because the perf o rma n ce o fRO U GE measures i sse n s i t iv et o s o me ru nnin g parameters .Fo re x amp l e , the  X -l X  o pt ion mea n sthat only the first n wo rds in as y stem -ge n erated summar y are used f o r t oov erest i mate the RO U GE sc o res .T he o ffic i a l RO U GE resu l t o fthetask 1o f d i st in gu i shab l ee no ugh f o rRO U GE sc o res . 4.3 Query-Based Multi-document Summarization P e o p l earebec o m in gm o re a n dm o re in terested in cust o m i zed in f o rmat ion w he n pared t o age n er i cs in g l ed o cume n t summar i zer , t wo m o re m o du l es are added t o th i sk in d o f summar i zer ,n ame ly, se n te n ce c o mpress ion a n dquer y e x pa n s ion. Sentence Compression. In ge n era l, the l e n gth o fapr o duced summar yi s li m i ted t o afi x ed wo rd n umber in DU Ctasks , s olon gse n te n ces wi th e x tra n e o us te x tu ni ts will reduce a summar y X  sc on te n tc ov erage . Our se n te n ce c o mpress ion are further c l ass i fied in t o three sets : X  X  X  f o rth o se at the beg innin g , X  X  X  f o r th o se in the m i dd l e , a n d  X  E  X  f o rth o se at the e n d . Each part c o rresp on ds t o will be rem ov ed fr o mthese n te n ce in quest ion. Ty p i ca l e x tra n e o us wo rd u ni ts  X  X  here  X  u n der certa in c i rcumsta n ces , te x tu ni ts beg innin g wi th  X  k nown as  X  tr i mm in g will cause perf o rma n ce low ered in stead o f i mpr ov ed . Query Expansion. In DU C , each quer y used t o descr i be a quest ion on certa in fr o m i t i s v er yli m i ted .Fo r the purp o se o f o bta inin gm o re k nowl edge fr o ma quer y, w etr y t o e x te n d i tus in ga n e xi st in g on t olo g yn amed Wo rd N et [9] as f ollow s :( a )S e l ect no u n sfr o ma P O S tagged quer y as the e x pa n d in g seeds a n d acqu i re the defi ni t ion a n dthes ynony ms o feach no u n seed fr o mthe Wo rd N et ; ( b )To ke ni ze a n d l emmat i ze the o utput fr o mstep ( a ); ( c ) O nly non-st o p no u n wo rds will be used t o e n r i ch the quer y.
 Experiments. T he pr o p o sed appr o ach t o quer y-based mu l t i-d o cume n tsum -2 50-wo rd summar yo fthed o cume n ts that a n s w ers the quest ion( s )in the t o p i c i dea l summar i es created b y huma n s .T he ru nnin g parameters o ffic i a lly spec i fied in DU C 0 7f o rRO U GE ha v ebee n ad o pted .

Av ar i et yo fe x per i me n ts are c on ducted t o check the c on tr i but ion o feach te n ce ra n k in gf o rmu l as .T he resu l ts are prese n ted in T ab l e 3w here Q mea n s a no r i g in a l quer y stateme n ta n d Q represe n ts a n e x pa n ded on e .A s ill ustrated in T ab l e 3, b y c o mpar in gtheperf o rma n ces o fra n k in gf o rmu l as o f  X 1. X ,  X  2 . X , a n d  X 3. X , o rth o se o f  X  4 . X ,  X 5. X , a n d  X 6. X , w eca n see that the pr o p o sed wo rd s i g ni fica n ce measure wo rks better tha n the trad i t ion a lwo rd in f o rmat ion metr i c w h i ch measures a wo rd  X  s i mp o rta n ce only us in gstat i st i ca lin f o rmat ion fr o ma  X 9. X  X  s o utput c o mpared wi th that o f  X  8 . X . In add i t ion, i t seems that e i ther ab -se n ce o fthet wo k in ds o fc o rre l at ion s ,no matter at w h i ch l e v e l, will cause the perf o rma n ce dr o pbased on the o bser v at ion o fperf o rma n ces  X  X  ff o rmu l as  X 6. X ,
T here are 30 gr o ups part i c i pat in g in the DU C 0 7c o mpet i t ion. A cc o rd in gt o a nAN O VA test carr i ed o ut b y the DU C o rga ni zers , the t o ps ix h i ghest sc o red s y stems are s i g ni fica n t ly better tha n the rema inin g24s y stems a n dt wo base lin e s y stems [19]. Our best resu l tra n ks 5 th am on gthet o ps y stems as prese n ted in T ab l e4 . O n eth in gt o me n t ion i s that the t o p 5 h i ghest sc o red s y stems ra n ked b y RO U GE -2sc o re ha v etheh i ghest RO U GE -SU 4sc o res as w e ll. T he s y stem ra n ked sec on d o utperf o rms o ur appr o ach only b y0. 4 % a n dthet o p on es y stem d o es b y0. 8 5% in RO U GE -2sc o re .Si m il ar phe no me n aare o bser v ed in RO U GE -SU 4sc o re .I tca n be sa i dthat o ur appr o ach has ach i e v ed a state -o f -the -art perf o rma n ce in quer y-based mu l t i-d o cume n t summar i zat ion s . k nowl edge fr o mas in g l ed o cume n t o rac oll ect ion o fte x ts . Espec i a lly wi th the rap i dde v e lo pme n t o fthe In ter n et , the n eed f o raut o mat i cd o cume n t summa -creat in gasummar y. B ased on th i s assumpt ion, w epr o p o se a n e wwo rd s i g ni fi -summar i zat ion. T he e x per i me n ts , b o th on the task 1o f DU C 0 2 w h i ch f o cuses g o a li st o pr o duce a quer y-based summar y f o raset o fre l ated d o cume n ts , ill us -trate that o ur pr o p o sed appr o aches ach i e v eastate -o f -the -art perf o rma n ce .
P art i cu l ar ly no te wo rth y ab o ut o ur appr o ach are the f ollowin gp oin ts :( a )W e redefi n ea n e w qua n t i ficat ion measure o f wo rd i mp o rta n ce in NLP tasks , a n d successfu lly app ly i tt o d o cume n t summar i zat ion; ( b )W eha v epr o p o sed a nov e l fr o mthem i cr o perspect iv e -at wo rd l e v e l, a n dfr o mthemacr o perspect iv e -perf o rma n ce i mpr ov eme n t in the pr o p o sed summar i zat ion appr o ach .
Fo rthefuture wo rk , t wo p o ss i b ili t i es are c on s i dered . O n e i st o app ly the summar y creat ion.
 T he research descr i bed in th i spaper w as supp o rted b y C i t yUniv ers i t yo f Hon g Kon gthr o ugh the S trateg i c Research Gra n t7 001 87 9.
 Multi-document summarization aims at giving readers a quick view to their particular interests through generating a brief, well-organized, fluent summary from multiple documents. Also, most current multi-document summarization tasks provide a spe-help systems produce a summary. A variety of methods have been developed, which can be either extractive or abstractive summarization. Extractive summarization in-volves assigning scores to some units (e.g. sentences, keywords) of the documents and extracting the sentences with highest scores, while abstractive summarization usually needs robust techniques of language generation such as sentence compression and generation. Here we mainly focus on extractive summarization, which assigned scores to sentences according to the user query and the document content. 
For extractive summarization task, selection of appropriate features highly influ-ences the system performance. Many current summarization systems have devoted summarization system which adopts various features, such as sentence length cut-off feature, fixed-phrase feature, paragraph feature and so on, and then concluded that the system performed better than any other system using only a single feature. Nenkova and Vanderwende [2] examined the impact of word frequency on summarization. It is remarkable that the system simply based on frequency features could perform unex-pectedly well in MSE (Multilingual Su mmarization Evaluation) 2005. The well known summarization system MEAD has explored an effective feature based on clus-documents. In addition, the features based on sentence position, sentence length ([4], [5]) are also widely used in various summari zation systems. Ouyang et al [5] has used in their system the feature based on semantic relations and named entity feature. 
So far, most summarization systems focus on exploring n -gram features based on consecutive terms. However, with n increasing, the summarization method will be some useful discontinuous term sequences are also ignored. In fact, there are so many phrases and fixed collocations with term distance spanning more than 2 terms, which are usually important for summarization systems. To solve this problem, we need between them. For example, in sentence  X  X  X  X  like to go to china, if your parents per-mitted, with you. X , the relationship between  X  X o to china X  and  X  X ith you X  would be ignored, if using n-gram(n&lt;5). The same problem also exists in the field of informa-tion retrieval. To leverage the full information of the relative position between terms, term proximity [6] and query expansion [7] [8] have been used in the information retrieval field for a long time, both improving the result of search engine. Because the query guides how to select sentences under the framework of extractive summariza-tion, we can borrow term proximity and query expansion in our query focused sum-marization system. 
First, we introduce the term proximity feature, based on which we further define weighted proximity to measure the effect of spanning distance on terms. At the same time, to remedy the sparseness of term sequences, we utilize WordNet[9] to conduct concept expansion. 
The remainder of this paper is organized as follows. Section 2 will introduce the framework of our summarization system which is based on exploring various fea-tures. At the same time, the commonly used features are also presented. Section 3 details the proximity feature. In order to reasonably measure the proximity between terms and remedy the sparseness of term pairs, we further present the weighting method of proximity and query expansion. Section 4 illustrates experiments and evaluations. Section 5 finally concludes the paper. 2.1 Background The task of DUC query focused summarization requires creating from a set of rele-vant documents (most set contain 25 documents) a brief, well-organized and fluent summary to the information seeking need which is indicated by the queries in the topic description. Here is an example of the topic description. 
For each topic, four human summarizers are asked to provide a summary with lim-ited length (no more than 250 words) from the 25 related documents for the automatic evaluation. 2.2 Feature Based Scoring System As described in section 1, most summarization systems adopt an extractive summari-zation framework, under which various features are explored. We mainly aim at veri-assigned a score which cumulating the impact s of the features used. The impact of each feature is represented by its weight, which are tuned manually. The formula is as follows. Where s means a sentence, f i means the feature value while w i indicates the weight of the feature f i set experimentally. With the extractive framework, we can easily verify whether each feature can work on the summary performance, through adding a new feature into the summarization system. At the same time, by tuning the feature weight, we can know to what extent a feature improves the performance. 
In order to verify our proposed features, we generate a summary using the MEAD 1 system whose basic features include centroid, position and length. In addition, we introduce two query focused features -a word matching feature and a name entity matching feature -to reflect the guidance of the query in our summarization system. (1) Centroid feature 
Where ) ( j t tfidf is the tf-idf score of j t in the whole data set. (2) Sentence Position Feature where n is the total number of the sentences and s is the i th sentence in the document. (3) Sentence length feature is used to cut off too long or too short sentences. Usually we set a threshold value K , when the length is greater than K, the sentence is reserved. Otherwise, the sentence is excluded for extracting. (4) Word Matching (WM) Feature (5) Name Entity Matching (NEM) Feature named entity is usually defined as a word, or a sequence of words that can be classi-fied as a person name, organization, location, date, time, percentage or quantity. Here only four classes (person, organiza tion, location, date) are involved. In the feature based extractive summarization systems, each sentence is scored to judge whether it should be extracted. The sc ore should present the importance of the sentence itself in a cluster of documents, wh ich are well represented by centroid fea-ture. The guidance of the topic description is also important in query focused summa-rization systems. Here we propose the proximity feature to improve the action of the query on summarization performance. Each sentence is assigned a normalized prox-term pairs. We will introduce the proximity of term pairs and its improvement as the following subsections. 3.1 Proximity We use proximity to represent the relative position of two terms. With proximity, we can comprehensively describe the meaning of a sentence, further to strengthen the one topic, the proximity value is computed . First we locate the instance of each term pair in the query sentence, every two terms within a maximal distance of ten (or hav-ing a maximal of nine terms between the keyword pair) form a term pair instance ( t i , t )., And we compute a term pair instance ( tpi ) proximity as follows: t 3.2 Query Expansion WordNet is a large lexical database organized by sets of cognitive synonyms (syn-available, it is widely used for concept expansion. The raw proximity computation will be improved by concept expansion based on which seriously affected the raw proximity value. And another is that there are always many expressions for one concept, such as  X  X .S.A. X  and  X  X merica X  both stand for the concept of United States of America. To solve this problem, we expand the query by expanding each component term to a cluster of terms with similar meaning. For effi-part of speech of the term. After query expansion, the term pair instance evolved to be a concept pair instance, and the formula is as follows. c c in the documents. 3.3 Weighted Proximity Because each term in a document may have different importance, we propose that different term pairs even with the same spanning distance should have different de-documents. Usually the importance of a term can be represented by the inverted document frequency (idf). And the formulation is further improved as follows. Where IDF ti and IDF tj respectively mean the inverted document frequency of term t i and term t j . 4.1 Experiment Design We set up our experiments on the data set of DUC 2006[10]. The baseline system we use is the mead system with the limited features: position, sentence length and cen-troid. All documents are pre-processed by removing stop words and stemming remains. According to the task definition, system generated summaries are strictly limited to the 250 English words in length. After sentence scoring, we conduct a reranking process, called Maximal Marginal Relevance (MMR)[11]. That is, when we sentences already in the summary. If the sentence is similar to any sentence already in the summary, the sentence will not be considered into the summary. The process is repeated until the length of the summary reach the upper limit. 
To verify the effect of our proposed feature in summarization system, we design imity feature and its improvement on the summarization performance by tuning the feature weights. The second experiment designs whether the proximity feature works with WM and NEM features added. Because WM and NEM features are two impor-tant features to reflect the guidance of the query in generating summaries. 4.2 Evaluation Metrics ROUGE (Recall Oriented Understudy for Gisting Evaluation) toolkit has been work-ing well for evaluating summarization systems. It adopts a variety of n-gram matching approaches to evaluate totally h human summaries. When computing ROGUE score for a summarization system, human summaries must be created in advance. In our experiments, for each topic set, there are four human summaries for evaluation. ROUGE-1, ROUGE-2 and ROUGE-SU4, the DUC automatic evaluation criteria [12], are used to compare our systems built upon the proposed approaches. ROUGE-2 evaluates a system summary by matching its bigrams against the model summaries. ROUGE-SU4 matches uni-grams and skip-bigrams of a summary against model summaries. A skip-bigram is a pair of words in their sentence order, allowing for gaps within a limited size. 4.3 Experiment 1: Weight Tuning summarization performance and at the same time illustrates the changed performance with weight changing. The baseline system we used is MEAD with three basic fea-experience. Then we set the weight of the proximity feature to be 2, 5, 10 or 20 respec-tively. Table.1 gives the results: the average ROUGE-1, ROUGE-2 and ROUGE-SU4 value and corresponding 95% confidential intervals of all systems are all proposed. 
Method Feature 
Baseline 0.37870(0.36630, + Prox + Prox + QE + WProx + QE 
In the table, the  X + X  symbol means that a feature is added to the baseline system. For example,  X + Prox X  means that the system includes the proximity feature except the three basic features. The figures are boldfaced for the highest performance of each system with weight tuning. From the results we can see that the effect of the proximity feature, producing a significant raise. While appropriately applying query expansion and weighted proximity, the improvement continues. Also, we can conclude the best achieve their best, as Fig. 2 shows. 4.4 Experiment 2: Query Focused Features In this experiment our goal is not to conclude which feature can improve the query-focused summarization more. We mean to present that the proximity feature can further improve the summarization performance with other query focused features available. Word match (WM) feature and name entity match (NEM) feature are two important query relevant features, which work in query focused summarization sys-tems. These experiments are still implemented based on the baseline system. With WM and NEM features added respectively or simultaneously, the performance is further improved when the improved proximity feature is used. Table 2 gives the results and the boldfaced figures clearly demonstrate that the proximity feature is effective. + WM 0.38964(0.37733, 0.40351) 0.08128(0.07456, 0.08832) 0.13771(0.13008, 0.14460) + WM + WProx + QE + WM + NEM 0.39175(0.37811, 0.40613) 0.08101(0.07373, 0.08878) 0.13750 (0.12996, 0.14511) + WM + NEM + WProx + QE In this paper, we propose an effective feature which can represent the relation of key terms for the query focused multi-document summarization systems. First, we intro-duce the feature of term proximity commonly used in the field of information re-trieval, which can comprehensively represen t the meaning of a query according to the relative position of terms. To resolve the problem of data sparseness and to represent the proximity in the semantic level, concept expansion is conducted based on Word-Net. By leveraging the term importance, the proximity feature is further improved and weighted according to the inverse term frequency of terms. The experimental results show that the proximity feature and its improvement can contribute to improving the summarization performance. 
In this work, concept expansion is conducted simply by expanding one term to its synset. We will explore more complex semantic relations to conduct further concept expansion for improving the proximity feature. In addition, now we manually tune the feature weight and only a few features are used with the proximity feature. Our future work will adopt machine learning methods to tune the feature weights and employ the proximity feature more appropriately. We would like to thank the anonymous reviewers for their detailed comments. This work is supported by NSFC programs (No: 60603093 and 60875042), and 973 National Basic Research Program of China (2004CB318102). A text summarization task requires creating short and concise summaries from long summarization tasks have been defined and studied in the literature. The goal of document summarization, in particular, is to compress (sets of) long documents to short summaries which are anticipated to pr eserve as much information as possible. That is to say, summaries should contain the most important concepts conveyed in original documents. 
Within the general framework of extractive summarization, the concepts in original documents are first ranked by their importance. The highest ranked ones are then picked up to form the summary. Judging the importance of the concepts is a crucial but complicated problem which needs to consider multiple aspects according to the variety and examined. Most factors, usually indicated by features, are based on statistical and syntactic analysis of the original text. In recent years, graph-based ranking algorithms, such as PageRank or HITS which have been very successful in ranking web pages, are introduced to the text ranking area. The graph-based ranking approaches consider the intrinsic structure of the texts instead of treating texts as simple aggregations of terms. Thus it is able to capture and express richer information in determining important concepts. 
The two basic elements in a graph model are the nodes and the edges. In text rank-ing applications, the nodes can be of various granularities of text fragments like terms, sentences, documents and etc. Here we focus on sentence ranking and thereby take sentences as nodes. Then the remaining issue regarding to graph construction is how to define the edges that establish the connection between the nodes. Unlike in a web text graphs are not provided beforehand. A typical way adopted in existing graph-based summarization models is to calculate the similarity between the sentences and similarity calculation plays an important role in graph construction and it directly affects the ranking results. However, the problem of how to measure the similarity between the sentences is neglected in most existing graph-based summarization ap-proaches, which simply apply the cosine similarity. In this study, we explore machine learning of similarity functions which allows synthetical and efficient scaling of sen-tence linkage in a document graph. 
The main evaluation forum providing benchmarks for researchers working on document summarization to exchange their ideas and experiences is the Document Understanding Conferences (DUC). Since nowadays researchers pay more and more attention to the tasks which model the real-world problems, the DUC defines a query-focused multi-document summarization task in 2005 which is motivated by modeling complex question answering. The task requires creating a brief, well-organized and fluent summary from a set of relevant documents according to a given query that describes a user X  X  information need. We choose this task as the platform to study how to develop a good similarity function to measure the relations between the sentences. To this end, we first define several similarity measures from different points of views on the relations between the sentences, including one query-dependent and three query-independent measures. Then we conduct a preliminary study to investigate the feasibility of learning similarity functions. The primary objective is to examine if we can make use of the given human-generated  X  X olden X  summaries to learn a reliable similarity function from a set of pre-defined similarity measures. 
The reminder of the paper is organized as follows. Section 2 briefly introduces the related work. Sections 3 then details the proposed supervised similarity learning ap-proach and its application in document summarization. It is followed by Section 4 which presents experiments and evaluations. Finally, Section 5 concludes the paper. Document summarization has attracted much attention since the initial work by Luhn [1]. Summarization approaches can be roughly classified into two categories: abstrac-tive summarization and extractive summari zation [2]. Abstractive summarization writes a concise abstract that truly summarizes documents. However, due to the limi-tation of current nature language understanding and generation techniques, abstractive approaches are still confined in specified domains [3, 4]. In contrast, extractive sum-marization, which selects a number of indicative text fragments from the original documents to form a summary instead of rewriting an abstract, are much easier to implement. In a particular summarization system, the selected text fragments can be phrases [5], sentences [1, 6, 8], or paragraphs [7] etc. Currently, many successful systems adopt the sentences considering the tradeoff between content richness and grammar correctness. They extract the most salient sentences from original docu-ments to compose a summary. For these systems, the sentence ranking algorithm that determines which sentences are more important than others is crucial. 
In fact, measuring the importance of the sentences is a complicated problem. . Ex-isting studies examined many factors such as TFIDF, key phrase, position and etc. A traditional feature-based ranking algorithm uses multiple features to reflect these factors and calculate the importance of sentences based on the combination effect of them. Machine learning approaches are us ually introduced to improve the feature combination process [6, 8]. A common probl em with these summarization approaches relations between sentences. Motivated by incorporating the inter-sentence informa-tion, currently there is a trend to introduce graph-based models to the sentence rank-ing in order to obtain more accurate estimation of sentence importance. TextRank [9] and LexRank [10], which are inspired by PageRank, are good examples. Erkan and Radev [10] used weighted undirected graphs to represent documents by taking sen-tences as vertices and the cosine similarity between sentences as the edge weight function. PageRank-like iterative ranking algorithms were then applied to calculate sentence importance scores. In contrast, following the idea of HITS, Zha [11] built a sentences for ranking sentences and terms simultaneously. In [12], Mihalcea and Tarau presented a comparative study of different graph-based ranking methods. Three algorithms, namely HITS, Positional Power Function and PageRank were examined. They found out that when the same similarity function was used to construct the The above works are conducted on generic summarization. To query-focused summa-rization, it has been well acknowledged that query information should be taken into account when ranking sentences. OtterBacher et al [13] proposed a query-sensitive version of TextRank in a query-based random walking model. Their idea was fol-lowed by Wan et al. [14] who further differentiated between inter-document and intra-document relationships. Addressing the issue of measuring similarity with respect to a specified context, Tombros and Rijsbergen [15] pioneered the development of the ferred by matrix notation transformation to a general graph-based model which meas-ures sentence relations with a query-sensitive similarity function. Therefore, we think that the key issue of the graph models is indeed the similarity functions which meas-ure the relations between the nodes. Thus we regard designing similarity measures as the basic problem in this study. In this section, we start with giving several similarity measures which measure the rela-tions between the sentences from different aspects. Then we propose a learning-based strategy which can combine these measures (called features in machine learning) in order to obtain more accurate estimations. Finally, we introduce a graph-based sentence ranking algorithm. 3.1 Similarity Measures Most graph-based approaches use the cosine similarity to scale the relations between two sentences. The cosine similarity mainly reflects the extent of overlap between the word vectors of the two sentences. However, the cosine similarity measure may not sufficiently reflect the relations in the context of query-focused summarization. In our belief, sentence relations should be determined by multiple factors. Here are some examples selected from the DUC 2006 data set. 
We can see that  X  X ndians X  is a common word in (A) and (B), but meanwhile (A) also overlaps (C) with the word  X  X chool X . Given that the query of this topic is to dis-cuss conditions on American Indian reservations or among Native American commu-more related to (B) than to (C). The following is another example. 
The same word in sentences (D) and (E) is  X  X apanese X  while the same word in sen-tences (D) and (F) is  X  X xpect X . Obviously, sentences (D) and (E) are much more re-lated to each other with or even without the given query. This rests with the difference of the content overlapped. A named entity such as  X  X apanese X  is surely more informa-tive than a common word such as  X  X xpect X . 
A common problem is exhibited in the above illustrative examples. The sentence pairs with equal number of overlapping words have different relations between them. Such situations, however, can hardly be identified by the cosine similarity alone. Indeed, the cosine similarity comes up with the wrong decisions in the two given queries, name entities and many other factors. In this work, we examine a number of potential factors and conclude four different similarity measures for scaling the sen-tence relations. 
The first similarity measure we use is the traditional cosine similarity. For the cal-frequency and ISF is the inversed sentence frequency in the documents. The similarity is then calculated by the cosine value of the angle between the two sentence vectors. is the inversed sentence frequency of w in the documents. 
Name entities, such as persons, locations and organizations, contain rich semantic information. Therefore, we consider the entity overlapping as well and derive the entity-based similarity measure as where e indicates a named entity in a sentence. 
The position of a sentence is a common-used indicator to judge whether a sentence is important. For example, traditional approaches often select the first sentences in the documents. This is a reasonable idea since usually many writers would like to give a brief introductions or reviews at the very beginning of the documents in order to give readers a general impression of what the doc uments are going to tell. For the problem of relation scaling, we make a basic assumption that the adjacent sentences are more related than the separated sentences because the adjacent sentences are probably talk-ing about the same thing. Based on this assumption, the position similarity measure is defined according to the relative distance of the two sentences, where pos ( s ) is the position of a sentence s in a document. 
To emphasize the role of query, we also define a query-sensitive similarity, which is considered more suitable for query-focu sed summarization. In the calculation, the modified cosine similarity only counts the words which appear in the query. where Q is the query. 3.2 Supervised Similarity Function Learning The similarity measures represent different points of views to measure the relations view alone is not sufficient to scale the relation indeed. A simple and straightforward features to obtain a composite measure, e.g. by using a linear combination function. 
The composite similarity function covers the effect of multiple factors, therefore it can be expected to be more accurate and reliable. The feature weights  X  i in the linear combination can be either assigned manually or tuned by conducting exhaustive ex-periments. However, manually assigning feature weights relies on human knowledge. The performance is not predictable. Repeating experiments may find the optimum weights, but it is time-consuming when the number of features increases. As a better solution, we develop a supervised learning approach which can combine the features automatically by utilizing human generated  X  X olden X  summaries. This approach makes it potentially possible to automatically search for the best similarity function. 
The training data used to learn the composite similarity function is generated auto-matically with reference to human generated  X  X olden X  summaries. This idea comes from the assumption that the human summaries should contain the most important contents in original documents. Based on this assumption, the similarity between the two sentences in training data is assigned as where H is a collection of human summaries. Noti ce that unlike the training data used in this way is somewhat  X  X seudo X  standard in the sense that it provides useful information though may not be completely accurate. But it is practical. In fact, it is almost impossi-ble even for human to assign an accurate similarity value to each pair of sentences. 
Given the document sets D whose corresponding human summaries H are given, the training data is constructed as } , | ) ), , ( {( vectors to a real similarity value. In this paper we adopt the linear v -SVR model [16] which finds the optimum function f 0 from a set of candidate functions where L ( x ) is the loss function, C and v are the weights to balance the factors. and s ' j in the new document sets D ' can be predicted as: 3.3 Graph-Based Sentence Ranking Algorithm Once the similarity measure is defined, the document graph can be easily built based on sentence relations, and then a graph-based ranking algorithm derived from the famous PageRank [17] is applied to rank and select the summary sentences. The de-tails of the algorithm are given below. 
We denote the document set to be summarized as D = { s i , i =1, 2, ..., N } where s i indicates a sentence in D and N is the total sentence number in D . The ranking algo-rithm will assign an important score score ( s i ) for each sentence s i . Let the initial sen-the formula given below. tween the two sentences s i and s j . The final scores are obtained when the calculation is converged. 
The graph-based ranking algorithm works on a relation graph where the sentences in the documents are connected. It allows the importance of each sentence to be calcu-eRank, the convergence of the algorithm is ensured by the random walk factor d . Through the interaction between the sentences and the iterative process, we could formation of documents. 4.1 Evaluation Data Set The experiments are conducted on the query-focused summarization data sets from DUC 2005 and 2006. Each data set includes 50 document sets, and each document set contains a query and 25-50 relevant news documents. For each document set, several human summarizers are asked to provide a 250-word summary for automatic evalua-tion. The following table gives some information of the data sets. 
The DUCs evaluate machine generated summaries with several manual and auto-matic evaluation metrics [18]. In this paper, we use ROUGE [19] to compare our systems with human summarizers and top performing DUC systems. ROUGE (Recall Oriented Understudy for Gisting Evaluation) is a state-of-art automatic summarization evaluation method based upon N -gram comparison. For example, ROUGE-2 evalu-ates a system summary by matching its Bi-grams against the model summaries. where S is the summary to be evaluated, H j ( j= 1, 2, ..., h ) is h model human summa-is the number of times that the Bi-gram t i occurs in the j -th model human summary H j
Though ROUGE is based on simple N -gram matching, and argued by quite a num-ROUGE-2 has a Spearman correlation of 0.95 and a Pearson correlation of 0.97 com-pared with human evaluation, while ROUGE-SU4 has correlations of 0.94 and 0.96 respectively [18]. In the following experiments, we provide three commonly refer-enced ROUGE scores for the purpose of comparison, including ROUGE-1, ROUGE-2 and ROUGE-SU4. 4.2 Experiment Set-Up tences. The sentences are then ranked by the proposed graph-based ranking approach. Top sentences are selected into the summary until the word (actually the sentence) the MMR method [20] to alleviate the problem of information redundancy. Each time the candidate sentence currently under concerned is compared with the sentences that are already picked up into the summary. If the sentence is not considered significantly similar to any sentence already in the summa ry, it is then selected into the summary. The similarity between sentences is calculated by the cosine similarity measure. 
All documents are pre-processed by removing stop words and stemming. Named entities including person and organization names, location and time are automatically tagged by GATE 1 . The regression models are implemented by LIBSVM [21]. Two-fold cross-validation is applied to the summarization systems with the supervised vided into two 25-document groups. Each time, we use one group to generate training data and test on the other. The scores are averaged as the final overall scores. 4.3 Experimental Result In the experiments we first compare the efficiency of different similarity functions in document graph construction. Table 2 and Table 3 report the average ROUGE-1, ROUGE-2 and ROUGE-SU4 scores of six different approaches which all follow the graph-based algorithm introduced in section 3.3 but use different similarity functions, including four individual sentence similarity measures (labeled as  X  Cosine  X , similarities with manually assigned weights (labeled as  X  Linear  X ) 2 and the supervised similarity learning function (labeled as  X  Learning  X ). query-sensitive similarity performs better compared to the original cosine similarity for a query-focused summarization task. This illuminates the importance of designing corresponding similarity measures for specified tasks. However, the entity-based and position-based measures do not perform well in both years. We attribute the reason to considers the sentence pairs in the same documents. Consequently, they both can only reflect a small portion of the sentence relations in the whole document collection. Ta-ble 4 shows the percentages of the sentence pairs whose similarity values do not equal zero among all the pairs. It is clear that only a few sentence pairs have none-zero simi-larity values under the entity-based measure or the position-based measure. This unavoidably affects their efficiency on measuring the relevance between sentences. 
Although none of the single similarity functions can ideally measure the relevance between sentences, on the other hand, the linear combination of these similarities achieves better results. It improves the performance remarkably compared to the sin-gle measures. This proves that the combination similarity function which synthesizes relevance. The results also show that the supervised similarity learning algorithm can further boost the performance by optimizing the combination function. However, to ures in our system. When the combination process only involves a few numbers of measures, manual search can also find reasonably good feature weights. In future work, we intend to design more appropriate measures to see if we can benefit more from the advantages of supervised learning. We then compare our summarization system to current state-of-the-art systems. We select the top three DUC systems in 2005 and 2006 respectively. Tables 5 and 6 report the average ROUGE-1, ROUGE-2 and ROUGE-SU4 scores of a DUC human summarizer (labeled as  X  H  X , or  X  A  X ), a lead-based baseline provided by the DUC (labeled as  X  Baseline  X ), top-three performing systems (labeled as  X  24  X  or  X  15  X  etc.), average of all DUC participating systems in each year (labeled as  X  Average  X ) and our learning-based approach (labeled as  X  Learning  X ). It is shown that our summarization system performs better than all the submitted systems in the DUC 2005, and ranks the second in the DUC 2006. These results suggest that the proposed approach is very competent. 
Finally we examine the stability of the learning algorithm by conducting a com-parison on using different training data to learn the similarity function. Table 7 below presents the results when training and testing supervised similarity function on differ-ent data sets. In the results, the performances are very close when the similarity func-that the learning algorithm is stable. In the study of this paper, we investigated the problem of how to scale sentence rela-tions in graph-based summarization. We defined several similarity measures from different points of view of the relations and proposed a supervised learning algorithm to effectively combine the features. We then apply the learned similarity functions to a graph-based ranking approach. Results show that the proposed method is efficient and stable. The resulting system can perform as well as state-of-the-art systems when evaluated by ROUGE on the data sets from the DUC 2005 and the DUC 2006. Acknowledgments. The work described in this paper was supported by Hong Kong RGC Projects (No. PolyU5217/07E and No. PolyU5230/08E). An increasing number of internet users are expressing not only factual data but also their subjective thoughts and opi nions on various topics in weblogs. A lot of recent re-search tries to access the author X  X  opinions and sentiments hidden in the weblogs. There are many applications that can be explo ited when we can access weblogs X  opinions or sentiments. For instance, information extraction and question-answering systems could flag statements and queries regarding opinions rather than facts [1]. Also, it has proven useful for companies, recommender systems and editorial sites to create summaries of people X  X  experiences and opinions that consist of subjective expressions extracted from reviews (as is commonly done in movie ads) or even just a review X  X  polarity  X  positive ( X  X humbs up X ) or negative ( X  X humbs down X ) [2].

Most previous work [3, 4, 5, 6] investigated semantic orientations of words or phrases to analyze sentiments of sentences or docum ents. However, semantic orientations of words or phrases are investigated without reflecting properties of the domain in which documents are included. Weblogs deal with various topics, and they contain both facts and opinions, and the same word might possess different semantic orientations in dif-ferent domains. Therefore, we have to assign different semantic orientations of words or phrases according to the domain of the weblogs.

In this paper, we present an unsupervised learning model to classify sentiments of weblogs. Therefore our model does not requi re sentiment tagged corpus to be trained. Our model, which is based on an aspect model, assigns semantic orientations of latent factors depending on the domain of the weblogs. Our model uncovers semantic orien-tations of latent factors, which are domain dependent, and use them for sentiment clas-sification. The experiments on three domains (movie, automobiles and digital cameras) show that our model is effective at accessing a domain dependent semantic orientation and classifying weblogs X  sentiments.

The paper is organized as follows. Section 2 introduces related work. In Section 3, we formulate the sentiment classification problem of the weblogs and show the detail of our approach. The experimental results are presented in Section 4. Finally, we conclude the paper and discuss future work in Section 5. In the case of opinion analysis, there has been a lot of previous research such as the development of linguistic resource, subjectivity detection and sentiment classification.
In developing linguistic resources, some previous works have focused on learning adjectives or adjectival phrases [3, 4, 5]. R iloff et al. [7] developed a system that can dis-tinguish subjective sentences from objective sentences using lists of subjective nouns learned by bootstrapping algorithms. Wilson et al. [8] presented a two-step contex-tual polarity classification to analyze phrase X  X evel sentiment. Kamps et al. [9] proposed measures that determine the semantic orientation of adjectives using WordNet X  X  syn-onymy relation. Takamura et al. [10] used lat ent variables to find semantic orientations of phrases. Their approach is similar to our model in that it uses latent variables. How-ever our approach is different from Takamura et al. in that our approach finds semantic orientations of latent variables and uses them to classify the sentiments of the weblogs.
Turney [5] classified the sentiment of a document using the average semantic orienta-tion of phrases that was assigned using the PMI method. Pang et al. [11] employed three standard machine learning techniques (Naive Bayes, Maximum Entropy, and SVM) to classify the sentiment of a document. Pang and Lee [2] presented cut-based classifi-cation. In their approach, sentences in a doc ument are labeled as either subjective or objective, and a standard machine learning cla ssifier that classifies document-level sen-timent is applied to subjective sentences. Whitelaw et al. [12] presented a method for sentiment classification which is based on analysis of appraisal groups.

Applications using sentiment analysis of weblogs or news have been proposed [13, 14]. Mei et al. [15] proposed a probabilistic m odel to capture the mixture of topics and sentiments simultaneously. Liu et al. [16] investigated ways to use sentiment informa-tion from blogs for predicting product sales performance. In order to mine sentiment information from blogs, they presented a S-PLSA model based on PLSA. They used sentiment information captured by S-PLSA as features of an auto regression model. Our approach, on the other hand, presents a g eneral framework to classify sentiments of the weblogs using semantic orientations of latent variables.
 In this section, we propose a novel probabilis tic approach to classify the sentiments of weblogs, which is an unsupervised learning model.

The weblogs contain various opinions of the author about many fields. This property of the weblog causes a word to have differ ent semantic orientations according to the domain in which it is used. For example, the adjective  X  X npredictable X  may have a neg-ative orientation in an automotive review in a phrase such as  X  X npredictable steering X , but it could have a positive orientation in a movie review in a phrase such as  X  X npre-dictable plot X  [5]. This problem results from the use of domain-independent semantic orientations of words or phrases without considering the properties of the domain in which they are used. In general, the seman tic orientation of a word is affected by the surrounding context as well as the domain. For example, words following a negation word can have the opposite semantic orientation. However, in this study, we deal with the problem that results from the property of the weblogs, exemplified in the above case. In order to analyze complicated proper ties of weblogs, we regard them as results generated by the mixture model of the latent factors. We expect that those latent factors would correspond to the author X  X  sentiments presented in the weblogs. We use semantic orientations of latent factors, instead of words or phrases, to classify the sentiments of weblogs.

To this end, we present a Sentiment Aspect Model (SentiAM) based on the aspect model. Aspect model [17] is a latent variable model for co-occurrence data which as-sociates an unobserved class variable z  X  Z = { z 1 ,  X  X  X  , z K } with each observation. Sen-tiAM adds a semantic orientation factor to the aspect model. We regard the semantic orientation factor as having a dependency relationship with the latent variable. Figure 1 (a) shows a graphical representation of the statistical dependencies of SentiAM. If our model can capture domain-dependent semantic orientations of latent variables, it will yield better sentiment classification accura cy compared to the model that uses domain-independent semantic orientations of words or phrases. 3.1 Sentiment Aspect Model In this section, we formally present SentiAM to classify sentiments of weblogs.
Let D = { d 1 , d 2 ,  X  X  X  , d N } be a set of weblogs, with words from a vocabulary W = { w 1 , w 2 , let S = { P , N , F } be a set of semantic orientations. P , N and F represent positive, neg-ative and neutral orientations respectively. Let Z = { z 1 , z 2 ,  X  X  X  , z K } be a set of latent variables.

Suppose that the weblog d , the semantic orientation s and the word w are condition-ally independent given the latent variable z (corresponding graphical model represen-tation is depicted in Figure 1 (b)). Then, we can view the problem of classifying the sentiment of the weblog d as follows: Because probability P ( d ) does not have any effect on the decision of the weblog X  X  sentiment, we can transform the equation ( 1 ) by using Bayes rule to the following: In equation ( 1 ) , P ( z | d ) represents how much a latent variable z occupies in the senti-ment of a weblog d , intuitively, and P ( s | z ) represents a semantic orientation of a latent variable z .

For the sentiment classification of the weblogs, the parameters of SentiAM are learnt are calculated based on the aspect model. Next step finds the probabilities of semantic orientations P ( s | z ) of latent variables. 3.2 Generative Probabilities According to Figure 1 (b), assuming that the weblog d and the word w are conditionally independent given the latent variable z , the generative process of weblog-word pair ( d , w ) is defined by the mixture: We use Expectation-Maximization [18] algorithm to estimate the parameters, P ( d | z ) , P ( w | z ) and P ( z ) , which maximizes the below likelihood function.
 EM algorithm involves an iterative process with alternating steps, expectation step and maximization step, in order to learn the parameters which maximize complete likelihood.

In the expectation step (E-Step), the posterior probabilities are computed for the latent variables: In the maximization step (M-Step), the parameters are updated: The parameters, P ( d | z ) and P ( z ) , learnt by the EM algorithm are used to classify sen-timents of the weblogs, and the parameter P ( w | z ) is used to find the probabilities of semantic orientations of latent variables P ( s | z ) . 3.3 Finding Domain-Dependent Semantic Orientations of Latent Variables We use a lexicon resource tagged with positive and negative semantic orientations in order to find domain-dependent semantic orientations of the latent variables. In our approach, SentiWordNet [19] is used as a polarity tagged lexicon. SentiWord-Net is a lexical resource in which each synset of WORDNET (version 2.0) is associated to three numerical scores Ob j ( s ) , Pos ( s ) and Neg ( s ) . And it describes how Objective, Positive and Negative the terms contained in the synset are. For each word in Senti-WordNet, positive, negative and neutral scores are defined as probabilities ( Ob j + Pos + Neg = 1). Therefore we can readily find probabilities of semantic orientations of words, and we use them to find semantic orientations of latent variables.
 The semantic orientations of the latent v ariables are calculated as the following: Given a word, P IND (  X | w ) represents probability of domain-independent semantic orien-tation defined in SentiWordNet. SentiWordNet defines semantic orientation of a word according to its sense. Therefore, semantic or ientation of a word could become different if its sense is changed. We use the first sense of the word to simplify the experiment, because WSD (Word Sense Disambiguation) is another issue and out of the scope of this paper.

In equation ( 9 ) and ( 10 ) , the probability of semantic orientation of latent variable z is the expectation of the probabilities of semantic orientations of all words which are generated by the latent variable z . The expectation makes th e semantic orientation of the latent variable to be domain-dependent.

In the aspect model, different latent factors correspond with different topics, and words generated with high probability by a particular latent factor are related with top-ics captured by the latent factor. Due to these properties, different meanings (dependent on the context) of a polysemous word are identified by different latent factors. Similarly, different latent variables of SentiAM correspond with different opinions and sentiments contained in weblogs, and words generated with high probability by a particular latent factor have similar semantic orientations. For example,  X  X npredictable X  is defined as negative semantic orientation with 0.625 probability by SentiWordNet. On the other hand,  X  X npredictable X  is used with positive semantic orientation in the movie domain. Therefore a latent variable which generat es  X  X npredictable X  with high probability also generates many other words which have positive semantic orientation. Finally, the use of the expectation of semantic orientations of words makes the latent variable to assign the semantic orientation domain-dependently. Through experimenting on several dif-ferent domains, we can verify the effect of the expectation of the semantic orientations. 3.4 Features Selection In reality, SentiAM gives poor results when we use all words in the weblogs to train parameters of SentiAM. Weblogs contain not only the writers X  opinions but also factual descriptions, and this damages the effect iveness of the modeling. This is because the opinionated words and non-opi nionated words may co-occur with each other, thus they will not be separated by the EM algorithm. This inhibits latent variables from properly modeling weblogs X  opinions. We consider feature selection to solve the problem. In-stead of considering all words present in the weblogs, we attempt to seek out appraisal words to describe weblogs.

To this end, we choose subjective words in SentiWordNet ( Pos + Neg &gt; 0 . 0) as candi-date words. To investigate the influence of feature selection on sentiment classification accuracy, we make four feature types using candidate words.
 Ty pe A consists of adjectives only. Ty pe B is comprised of adjectives and verbs. Ty pe C consists of adjectives and adverbs. Ty pe D is comprised of all feature candi-dates, ie. nouns, verbs, adjectives and adverbs. We conducted several experiments on three different domains. We tried to verify that latent variables are able to model domain-dependent semantic orientation correctly in each domain and that the use of the latent varia bles are effective for sentiment classifi-cation of the weblogs. 4.1 Data Sets We used reviews in the domains of Movie, Automobiles and Digital Cameras. As a movie review dataset, the polarity dataset of Pang and Lee [2] was used. It contains 1000 positive and 1000 negative reviews all written before 2002 with a cap of 20 re-views per author per category. For an automobile review dataset, Epinion X  X  1 result set from  X  X ars &amp; Motorsports X  section for a keyword automobile was used. For a digi-tal cameras dataset, Epinion X  X  result set from  X  X igital Cameras X  section for keywords Canon, Nikon and Pentax was used. Among the reviews, short reviews with less than 50 words were excluded. Epinions review system displays authors X  rating with five stars. We regard reviews with more than 3 stars (  X  4 ) as positive reviews and reviews with less than 3 stars (  X  2 ) as negative reviews. The number of positive reviews obtained from epinions significantly outnumbers the number of negative reviews. Therefore, we randomly chose positive reviews to match the number of negative reviews. In general, the use of a different number of positive and negative reviews as test dataset can lead to a biased result. Once the two groups are selected in equal numbers, the reviews are parsed for html-tags removal and POS tagging by Stanford Tagger. No stemming and stopword lists were used. All feature words between a negation word ( X  X ot X ,  X  X sn X  X  X , etc.) and the first punctuation mark following the negation word is marked with NOT tag [11]. Table 1 shows information of the dataset which we used for experiment. 4.2 Baseline For the baseline of the experiment, we classified weblogs X  sentiments using semantic orientations of subjective words within SentiWordNet. For each word, SentiWordNet defines its semantic orientation as a score (probability). We regard a review as a positive review when the sum of positive scores of subjective words within a review is greater than that of negative scores, otherwise we regard it as a negative review.

The baseline precisions of the movie domain, the automobile domain and the digi-tal camera domain are 0.44, 0.63 and 0.62, respectively. We can see that the precision was worse than that of random selection in movie domain. This result shows the diffi-culty of sentiment classification in the movi e domain. This result is because there are many words that have a different semantic or ientation compared to domain-independent semantic orientation defined at SentiWordNet such as  X  X npredictable X  in the movie do-main. This result shows the problem when using domain-independent semantic orien-tations of words or phrases. Therefore domain-dependent semantic orientation of latent variables can remedy this problem. 4.3 Semantic Orientation of Latent Variables We wanted to verify that latent variables capture domain-dependent semantic orienta-different semantic orientations depending on the domain where they are used. In Sen-tiWordNet,  X  X npredictable X  has a negative semantic orientation with its 0 . 625 score. For each domain, we verified semantic orient ations of latent variables which gener-ate  X  X npredictable X  with high probability. Tab le 2 shows semantic orientations of two latent variables which generate  X  X npredic table X  with the highest probability. The re-sults of movie and automobile datasets correspond to our expectation. Even though the domain-independent semantic orientation o f the word  X  X npredictable X  is negative, the two latent variables that generate the word the most have positive semantic orientations in the movie domain.

We expected that latent variables generating the word  X  X npredictable X  with high probability would have a negative semantic orientation in the digital cameras dataset. To the contrary of our expectation, positive semantic orientation is higher or equal to negative semantic orientation. We think this is due to the small size we had for Sen-As shown in the Table 1, the size of the digital cameras dataset is noticeably smaller than the other two. In fact the word,  X  X npredictable X , occurs only once in 400 reviews in the digital cameras dataset upon a closer inspection. This might lead to improper parameter training of SentiAM.

We found words in the movie domain with semantic orientations different to domain-independent ones. Table 3 lists some of them with their semantic orientations and con-texts.

The experiment shows that SentiAM assigns dom ain-dependent semantic orientation to latent variables correctly. The experiment presented in the next section shows that these latent variables also outperform other types of features such as words and phrases in the sentiment classification of the weblogs.
 4.4 Sentiment Classification In this section, we investigate the influence of various feature sets (Type A, Type B, Type C, and Type D) and the number of latent variables on accuracy of weblogs X  senti-ment classification.

We used the same number of positive reviews as negative reviews. Therefore, a ran-dom guessing sentiment of the reviews will have 50% accuracy.

Table 4 shows the best performance we achieved for each domain and the feature set used. In the movie domain, we obtained 24% higher precision than random guessing and 30% higher precision than the baseline. In the automobile domain, we also obtained 15% and 2% higher precision than random guessing and the baseline. In the digital camera domain, we obtained 12% better precision than random guessing but the same precision with the baseline.

We think that the small improvement of accu racy in automobiles a nd digital cameras compared to that in the movie domain is due to the small size of the two domains X  test corpora. As we saw in section 4.3, the bigger the size of the dataset, the better SentiAM assigns domain-dependent semantic orientations of latent variables, thus resulting in higher accuracy.

Feature Type A which consists of adject ives only, resulted in the best accuracy in all domains. Many previous works [5, 20, 3 ] related to sentiment analysis dealt with adjectives heavily. Adjectives are less ambiguous with respect to sentiment than other word classes. Therefore, they can convey th e author X  X  opinion more correctly than any other parts of speech.

Figure 2 (a) shows the influence of varying feature types on sentiment classification of the weblogs in the movie domain. The two other domains yield similar results and the following analysis on the movie domain also applies to them.

Type A and Type B produced results with hi gher accuracy than the others. This is because adjectives and verbs reflect the a uthors X  opinions less am biguously with respect to sentiment than other word classes. We obtained the worst accuracy when using Type D which includes nouns. Nouns are likely to be non-opinionated or less-opinionated. Thus including them in the feature set might generate noise when latent variables sepa-rate the different opinions and sentiments of the weblogs. We have not yet investigated the influence of nouns on sentiment classification. We leave this issue for future work.
Figure 2 (b) shows the accuracy of sentimen t classification according to different numbers of latent variables, z = 3 , 5 , 10 , 15. We obtained the best accuracy using five latent variables, the worst accuracy using fift een latent variables. Too many latent vari-ables overfits the data set and does not produce either positive or negative polarity. On the other hand, too few latent variables might not be able to capture semantic orientation with enough accuracy and thus reduces the accu racy of sentiment classifier as well. In this paper, we have presented a novel probabilistic approach to classify sentiments of weblogs. Our model, SentiAM, is an unsupervised learning model based on the as-pect model. SentiAM assigns domain-dependent semantic orientations to latent vari-ables using resources in which semantic orientations of words or phrases are tagged domain-independently. Experimental results confirm that semantic orientations of la-tent variables are effective at classifying the sentiments of weblogs.

We have presented a general framework to capture domain-dependent semantic ori-entations of latent variables. Though we used SentiWordNet as a polarity tagged lexicon resource, any resource can be applied. Therefore, semantic orientations of lexicons ex-ploited by previous work can be also used by our framework.

We used semantic orientation of the first sense of a word in SentiWordNet. Knowl-edge about word sense can improve accuracy of sentiment classification. We could use WSD or any means to improve word sense detection, thus improving the accuracy of sentiment classification.
 We use semantic orientations of latent variables to classify sentiments of weblogs. Our framework could, of course, also be applied to analyzing semantic orientations of more fine units such as sentences and words . We leave this issue for our future work. Acknowledgement. This work was supported in part by MKE &amp; IITA through IT Lead-ing R&amp;D Support Project and also in part by the BK 21 Project in 2008.
 are se v era l c o mmerc i a l a n d non-c o mmerc i a l ser vi ces such as Technorati , Blog-Pulse , kizasi.jp , a n d blogWatcher .Wi th respect t o mu l t ilin gua l b lo gser vi ces , Globe of Blogs , Best Blogs in Asia Directory , a n d Blogwise ca n be li sted . T REC 2 00 7 Blo gtrackas on e o f i ts task [ 2 ]. In th i spaper ,w etakea n appr o ach o f c oll ect in gb lo g feeds rather tha n b lo gp o sts , ma inly because w e regard the f o rmer as a l arger in f o rmat ion u ni t in the b lo g o sphere a n dprefer i tasthe in f o rmat ion k nowl edge base o f w e ll k nown facts a n dre l at iv e ly n eutra lo p inion s wi th rather ra w, user ge n erated med i a li ke b lo gs ,w h i ch in c l ude l ess w e ll k nown facts a n d much m o re rad i ca lo p inion s .W e regard Wi k i ped i aasa l arge sca l e on t olo g i ca l k nowl edge base f o rc on ceptua lly in de xin gtheb lo g o sphere .Fin a lly, w e use such w e ll k nown facts a n dmuchm o re rad i ca lo p inion sthatarec lo se ly re l ated t o a g iv e n t o p i c .

In add i t ion t o pr o p o s in gthe ov era ll frame wo rk o fcr o ss -lin gua l a n dcr o ss -o p inion s . O v er vi e wo f the pr o p o sed frame wo rk i ssh own in Fi gure 1. Fi rst , mu l t ilin gua l quer i es f o rretr i e vin gb lo g feeds on at o p i c (in th i scase  X  X  ha lin g  X ) are created fr o m Wikipedia e n tr i es .N e x t , fr o mthec oll ected b lo g feeds , terms that are char -terms in t wo l a n guages , as w e ll as a m onolin gua l measure f o rtermsre l ated t o o p inion s o fb lo gp o sts in t wo l a n guages . b o th J apa n ese a n dE n g li sh b lo gp o sts f o reach o fth o se t o p i cke ywo rds .Fo ra d iff ere n ces are t o s o me e x te n tre l ated t o d iff ere n ces in o p inion s . 4.1 Blog Feed Retrieval th i spaper , E n g li sh a n d J apa n ese ).

Fi rst ,ino rder t o c oll ect ca n d i dates o fb lo g feeds f o rag iv e n quer y, in th i s paper ,w eusee xi st in g W eb search e n g in e API s ,w h i ch retur n ara n ked li st o f b lo gp o sts , g iv e n at o p i cke ywo rd .W eusethesearche n g in e  X  X  ah oo !  X  X PI 2 f o r
N e x t ,w eemp loy the f ollowin gpr o cedure f o rtheb lo gd i st ill at ion: i) G iv e n at o p i cke ywo rd , ara n ked li st o fb lo gp o sts are retur n ed b y a W eb ii) A li st o fb lo g feeds i sge n erated fr o mtheretur n ed ra n ked li st o fb lo gp o sts iii) Re -ra n kthe li st o fb lo g feeds acc o rd in gt o the n umber o fh i ts o fthet o p i c 4.2 Blog Post Retrieval o f the target page .T he n, b lo gp o sts w h i ch c on ta in the t o p i c n ame o rat l east on e o fthee x tracted re l ated terms are aut o mat i ca lly se l ected .

Fo reacht o p i c ,T ab l e2sh ow sthe n umbers o f terms that are c lo se ly re l ated t o the t o p i ca n daree x tracted fr o meach Wi k i ped i ae n tr y. T he n, acc o rd in gt o the e x tracted re l ated terms are aut o mat i ca lly se l ected .T ab l e2a l s o sh ow sthe a n dthet o ta ln umbers o f wo rds / m o rphemes c on ta in ed in th o se p o sts . 4.3 Extracting Characteristic Terms the b lo gp o sts retr i e v ed acc o rd in gt o the pr o cedure descr i bed in the pre vio us f o rE n g li sh , seque n ces o f on e wo rd , t wo wo rds , a n dthree wo rds are e x tracted as ca n d i date terms .T he n, th o se ca n d i date terms are ra n ked acc o rd in gt o the f ollowin gt wo measures , s o that terms that are character i st i c only in on e l a n guage o r in b o th l a n guages are se l ected : a )To ta l freque n c yo feachterm in the w h ol ese l ected b lo gp o sts .T h i smeasure
Certa in n umber o ftermsd ono tha v etra n s l at ion in t o the o ther l a n guage ,o r appear in the w h ol ese l ected b lo gp o sts in the o ther l a n guage .H ere , a l th o ugh s o me o fsuchterms wi th o ut tra n s l at ion o r wi th zer o freque n c y tra n s l at ion are e x treme ly character i st i c only f o r on e l a n guage , m o st o f o ther terms are noi ses that sh o u l dbe i g no red here .A m on gsuchterms , th o se w h i ch are e x treme ly a n d j udge w hether the y are character i st i cterms o r noi ses .
T ab l e 3 sh ow se x cerpts o fma n ua lly se l ected character i st i cterms , as w e ll as R
E ( Y E ,Y J ). H ere , m o st o f them are character i st i c only in on e l a n guage ,w h il e s o me are character i st i c in b o th l a n guages .A s w esh ow in sect ion 4 .5, th o se est i mat in gd iff ere n ces in c on cer n sa n d o p inion s in E n g li sh a n d J apa n ese . 4.4 Ranking Blog Feeds/Posts Fin a lly, w era n ktheb lo g feeds / p o sts in terms o fthet o p i c n ame a n dthere l ated terms e x tracted fr o mthe Wi k i ped i ae n tr y( as descr i bed in sect ion 4 . 2 ) 7 .H ere , only the b lo gp o sts that are retr i e v ed in sect ion 4 . 2 are ra n ked , a n d only the b lo g feeds that c on ta in such b lo gp o sts are ra n ked . Ra n k in gcr i ter i aareg iv e n be low:  X  Blo gp o sts are ra n ked acc o rd in gt o the f ollowin gsc o re ,w h i ch i sdefi n ed as a
T ab l es 4 a n d 5li st e x cerpts o ft o p 10 ra n ked b lo gp o sts , a lon g wi th t o ta l represe n tc l ear o p inion s on for o r against the i ssue o ftheg iv e n t o p i c 8 . 4.5 A Map of Characteristi c Terms for Visually Mining a n d o p inion s in b lo gp o sts ,w edes i g n amap o f character i st i ctermse x tracted fr o m J ap n ese / E n g li sh b lo gp o stsassh own in Fi gures 2 a n d 3. In th i smap , a (  X  R J ( X J ,X E ) ,P J ( X J )) o rat (  X  ( ma xi mum rate in the map ) ,P J ( X J )) (w he n P ( X E )=0).Si m il ar ly, a n E n g li sh term X E wi th the J apa n ese tra n s l at ion X J i sp lo tted at the c oo rd in ate ( R E ( X E ,X J ) ,P E ( X E )) o rat ( ma xi mum rate in the map ,P E ( X E )) (w he n P ( X J )=0).In the map , se v era l terms w h i ch ha v ere l-t y p i ca lonlyinon e l a n guage . O n the o ther ha n d , terms that are character i st i c in in b o th l a n guages .
 T he f ollowin gs r o ugh ly summar i ze the fi n d in gs f o rs o me o fthef o ur t o p i cs . Fo rthet o p i c  X  X  ha lin g  X , a l m o st a ll the terms w h i ch are character i st i c in E n-Ch in ese ill ega lo rga n tra n sp l a n t . O n the o ther ha n d , ma ny terms w h i ch are d i seased k i d n e y.

B ased on th o se o bserbat ion resu l ts ,w eca n argue that ma jo rc on tr i but ion o f th i spaper i sthat w e successfu lly inv e n taframe wo rk o fm inin gcr o ss -lin gua l such as th o se pre vio us ly stud i ed in [3, 4 ]. T he n, i t will bec o me f o rust o eas ily respect t o the i ssue o ftheg iv e n t o p i c . l a n guages , th i spapersh ow ed the e ff ect iv e n ess o f the pr o p o sed frame wo rk wi th in c on cer n sa n d o p inion s .T here e xi st se v era lwo rks on stud yin gcr o ss -lin gua l a l s o stud i ed m inin gc o mparat iv ed iff ere n ces o fc on cer n s in n e w sstreamsfr o m c l es acr o ss 9l a n guages .T h o se pre vio us wo rks ma inly f o cus on n e w sstreamsa n d Sentiment analysis is the process of extracting opinions from written documents and determining whether they are positive or ne gative expressions. In recent years, the In-ternet usage has increased and many people have used it to publish their opinions about various topics, such as movies or the qua lity of various goods. The amount of published opinion has increased rapidly, so automatic sentiment extraction is desirable.
Much previous works on sentiment analysi s has focused on document-level senti-ment classification. Pang and Lee [1] [2] use a machine learning technique with mini-mum cuts algorithm and Turney [3] extracts polarity of phrases using the pair-wise mu-tual information(PMI) between the phrases and seed words. However, document-level sentiment classification is too coarse for m any sentiment tasks such as opinion search and opinion tracking, reputation survey and opinion-oriented information extraction. Document-level sentiment c lassification incorrectly assumes that subject of all senti-ment expression is same with the subject of a document. Therefore, these applications need phrase-level sentiment analysis.

Recently, many researchers have focused on th e phrase-level sentiment analysis. Na-sukawa [4] constructs a sentiment lexicon, and patterns manually with polarity and POS of a word. Zhongchao [5] also manually defines sentiment patterns and learn a polarity scores using document frequencies of each pattern in positive and negative documents. Wilson [6] uses these previous sentiment resources and a polarity-tagged corpus and tried to identify contextual polarity in phrase-level sentiment analysis. Breck [7] also uses polarity-tagged corpus to identify opinion phrases in a sentence.

Polarity-tagged corpus contains sentences whose opinion expressions are tagged with positive and negative labels. However such corpus is hard to construct manually and not readily available in various domains. Our experiment result shows that it is hard to achieve high recall with small amount of a polarity-tagged corpus in a supervised approach.
 However, we can get a sufficient sentiment s entences such as movie reviews on the Internet. We use these sentences to train our phrase-level sentiment classification sys-tem instead of using a polarity-tagged corpus . Sentiment sentences are marked by users as positive or negative. We construct a positive phrase corpus and a negative phrase cor-pus from the sentiment sentences. Those phrases are used to construct polarity-tagged corpus automatically. Our system does not re quire a manual polarity-tagged corpus. We call this approach a partially supervised appr oach, because our system learns sentiment tags with automatically constructed polarity-tagged corpus. We views the problem of sentiment labeling at phrase-level as a sequential tagging. Therefore our approach uses Hidden Markov Model (HMM) and Conditional Random Fields(CRF) which is used frequently in tagging problem.

This paper presents a new partially supervised approach to phrase-level sentiment classification. Beginning with a large sent ence-level sentiment r esource, we calculate sentiment orientation of each phrases, the n we get a positive phrase set and a negative phrase set. With these subjectivity clues, we automatically construct polarity-tagged corpus by marking subjectivity clue as positive in positive sentences and negative in negative sentences. Our partially supervised approach learns from the automatically constructed polarity-tagged corpus. Experime nt at results show that partially supervised approach is a feasible approach in the phrase-level sentiment classification. 2.1 Sentiment Resources There are several approaches to automatic sentiment resource construction such as the conjunction method [8], the PMI method [9] and the gloss use method [10]. Turney [3] uses only phrases that contain adjectives or adverbs. Those methods construct useful sentiment resources, but they have some limitations.

Those methods can not extract the sentiment of phrases which are dependent on spe-cific domains. There are also many phrases in corpora which are not correctly spelled, such as movie reviews or goods reviews on the Internet. They do not work well on jar-gons or idioms, which are difficult to find in the dictionary or to analyze using a parser or a tagger. Such approaches also use ric h English sentiment resources which are not available in other languages. Therefore we propose an automatic sentiment resource construction approach which works well in such environments. In this paper we con-struct sentiment resources using positive or negative sentences. Those sentences have polarity scores between 1 and 10. A value of 10 is the most positive sentimental score and 1 is the most negative sentimental score. We can use the average score of a word if the size of each score set is the same.
 However, the size of each score set is not the same in most of cases. Therefor we nor-malize each score.
 s is a score set between 1 and 10. S is a set of all scores and W is a set of all words. the polarity of each phrase using this appro ach. This approach can be easily applied to all language and domains. 2.2 Features of Phrases Unigrams and bigrams are good features in sentiment document classification [1], in-dicating that unigram and bigram are appropriate features for identifying the sentiment of phrases. We also used trigram. The experimental data used in this paper is in Korean which is an agglutinative language. We applied Korean segmentation to the training and test data set, which segments auxiliary words and compound nouns. We get fol-low positive and negative unigrams, bigrams and trigram by using Section 2.1 approach (Table 1, 2).  X  X iscount-card X  was the most negative unigram in Korean movie review, because people said that even  X  X iscount-card X  was wasteful for the movie.  X  X iscount-card X  has domain specific polarity. And there are some named entity word such as  X  X parrow X ,  X  X epp X ,  X  X t-dae X , and  X  X n Emergency Action Number X . Negative bigram  X  X o rul X  is a part of negative trigram  X  X in-gup-jo-chi ho rul X . Table 1 and Table 2 show that unigram, bigram and trigram appropriate for sentimen t phrase feature and Section 2.1 works well for extracting semantic orientation of word. 2.3 Automatic Construction of Tagged Sentiment Resource The Sentiment resource construction appro ach presented in Section 2.1 is not error prone. However this method is good enough for a utomatically constructing a polarity-tagged corpus. We calculate semantic orientation of phrase using sentence-level re-sources. While constructing semantic resources, we identified semantic orientation scores of phrases between 1 and 4 as negative, between 6 and 10 as positive and others as neutral. After constructing the semantic resources, we labeled the sentiment of each phrase in the sentence-level sentiment resource. We tagged subjectivity phrases as pos-itive in the positive sentence set, and tagged them as negative in the negative sentence set. Other phrases were tagged as neutral. We shows this procedure by example. jjang-jjang( X  X ood-good X ) is a positive word (Table 1). Although it is a positive word, negative sentence can have the word. Following sentence is a negative sentence. -jjang-jjang ha-nun nom-dul da alba. ( X  X ll people who say good-good to the movie are employee of the movie company X )
We labeled subjectivity word in this sentence by polarity of the sentence. Polar-ity of the sentence is negative, therefore w e labeled subjectivity word  X  X jang-jjang X  as negative. -jjang-jjang/ Negative ha-nun/Neutral nom-dul/Neutral da/Neutral alba/Neutral ./ Neutral
Following sentence is a positive sentence. -scenario ga jjang-jjang ha da( X  X cenario is good-good X )
We labeled subjectivity as, -scenario/Neutral ga/Neutral jjang-jjang/ Positive ha/Neutral da/Neutral Positive or negative sentiment phrases can represent opposite senses by their context. We followed the sense of the context rather than the sense of the sentiment phrase itself. We confirmed that this assumption is correct in the experiment. We used these automatically constructed tagged sentiment resource in the learning of HMM and CRF. 2.4 Opinion Tagging with Conditional Random Fields Similar to our approach Breck [7] use CRF to identify sources of opinion phrases. They defined the problem of opinion source identification as one of sequential tagging. Given a sequence of tokens, x = x 1 x 2 ... x n , we need to generate a sequence of tags, y = y 1 y 2 ... y n . The tag is a polarity label which can be positive or negative or neutral. There are three kinds of labels that are positive, negative or neutral. A detailed descrip-tion of CRFs can be found in Lafferty [11]. For our sequence tagging problem, we create a linear-chain CRF based on an undirected graph G =( V , E ) ,where V is the set of ran-dom variables Y = { Y i | 1  X  i  X  n } , one for each of n tokens in an input sentence. And tor function,  X  k is a weight assigned for each feature function, and K and K are the number of features defined for edges and nodes respectively. Following Lafferty [11], the conditional probability of a sequence of labels y given a sequence of tokens x is where Z x is a normalization constant for each x , and given training data D ,asetof sentences paired with their correct positive, n egative, neutral tag sequences, the param-For inference, given a sentence x in the test data, the tagging sequence y is given by argmax y P ( y | x ) . We used word features between the -4 and 4 window in the CRF model. 2.5 Opinion Tagging with Hidden Markov Model We use the HMM which is usually used in the tagging problem. There are three states in our HMM model: positive, negative and neutral. Observations of our HMM model are word. HMM model predicts state of each observations. We get initial probabil-ity, emission probability and transition probability from the automatically constructed polarity-tagged corpus. We use the Viterbi algorithm to encode the test data using those probabilities. 2.6 Opinion Tagging with Hidden Markov Model and Conditional Random We automatically constructed a tagged sentiment resource that is only partially correct. As a result, it is difficult to expect excelle nt precision performance with such an incom-plete resource. Instead of using tagged sentim ent resource directly to label sentiment phrases in the training data, we can refin e the data using HMM or CRF. We select the HMM to refine the tagged sentiment resource.
It is important to revise the polarity of the tagged sentiment resource which is refined by HMM, because HMM is trained by automati cally constructed tagged sentiment re-source. The system revise the labeling erro r of the tagged sentiment resource, because we know the polarity of the sentence.

For example, HMM sometimes marks negative sentence  X  X e fully grasped inversion story of the movie X  as  X  X e/Neutral fully/Neutral grasped/Neutral inversion/ Positive story/ Positive of/Neutral the/Neutral movie/Neutral X . The system revises the result to  X  X e/Neutral fully/Neutral grasped/Neutral inversion/ Negative story/ Negative of/ Neutral the/Neutral movie/Neutral X  by using the sentence polarity. Then CRF learns the tag with the neighborhood words.

The system considers all subjectivity tags in the positive sentence set as positive tags and we also consider all subjectivity tags in negative sentence set as negatives. We can expect better precision performance than when using the automatically constructed sen-timent resource directly. CRF model is train ed by the tagged sentiment resource refined by HMM. We refer to this combination of HMM and CRF as the HMM+CRF model. 3.1 Training Data Training data are composed of movie reviews from naver movie review 1 that are scored at the sentence level. Training data are sc ored from 1 to 10. We used the scores which are in the Pos (7-10), Neg (1-4) ranges. The number of points in each score set is 20,000, so the total number of training data is 160,000. The data contains some sentences that have doubtful scores, because sometimes people set movie reviews wrong. We use the Pos and Neg sets when we construct the sentiment resource. We only use Neg (1 , 2 , 3) and Pos (8 , 9 , 10) scores when we construct tagged sentiment resource to get more explicitly expressed resources. 3.2 Test Data The test data set was also extracted from nav er movie review. These data are comprised of more recent review sentences than the tra ining data set. We asked two annotators to classify and label the data set with scores of 1 , 2 , 3 (900 negative sentences) and 8 , 9 , 10 (900 positive sentences) scores. They tagge d each sentimental phrase in the sentence as positive, negative or neutral. We want t o evaluate consistency and agreement be-tween human evaluators. Polarity tag boundary is not exactly same between annotators. Therefore we use a CRF model trained by the sentiment tag sequence assigned by each human to evaluate consistency and agreement. The two humans assigned consistent tags to test data (Table 3, Table 4). Agreemen t between Human1 and Human2 was reason-able enough to use them as test data, becau se precision and recall are high enough to believe that there are shared sentimental c ommon sense between the humans (Table 3, Table 4). The CRF model that was trained by sentiment tag sequences of Human2 is better than Human1 (Table 3, Table 4). So we selected the test data of Human2 as our experiment test data. 3.3 Evaluation As with other information extraction tas ks, we use precision, recall and f-measure to C and P are the sets of correct and predicted expression spans, respectively. F 1 is the harmonic mean of precision and recall, 2  X  P  X  R P + R . Evaluation is done on the sentimental phrase in the sentence. It was tagged as positive or negative in the sentence. Our method often identifies expressions that are close to, but not precisely the same as, the manually identified expressions. For example, for the expression  X  X oundly criticized X  our method might only identify  X  X riticized X . We therefor e introduced softened variants of precision expression c  X  X ssigns X  to expression p in a sense defined by a . We report results ac-cording to two predicates: exact and overla p . exact ( c , p ) is true only when c and p in exact ( c , p ) are the same spans -this yields the usual notions of precision and recall. A softer notion is produced by the predicate, which is true when the spans of c and p overlap [7]. 3.4 Baseline Sentiword resource baseline marks a phrase as positive when it belongs to an automati-cally constructed positive phrase set in Section 2.1 and marks a phrase as negative when it belongs to a negative phrase set.
 We run the 10-fold cross validation test using only tagged test data (1800 sentences). Supervised CRF (S-CRF) and Supervised HMM (S-HMM) are used in the test. We used that result as our baselines as well, we comp ared supervised approaches and our par-tially supervised approaches. Features of supervised CRF are the same as the partially supervised CRF. 3.5 Results Bigram model performs better than the trigram or the unigram model. Trigram and unigram models outperform the recall of big ram in the sentiment resources, because unigram and trigram can determine the polarity bigram can not determine. Unigram improves performance more than trigram whe n it was used with bigram (Table 5). These models show better performance on the overlap evaluation than the exact evaluation (Table 5, 6).

Partially supervised HMM, CRF and HMM+CRF outperforms the performance of the models that use only sentiment resources, e specially the precision. Supervised HMM perform better than other model in exact evaluation. Its f-measure is 52.58% in ex-act evaluation. f-measure difference between supervised HMM and partially supervised HMM is 4.97% in exact evaluation. But, partially supervised HMM shows better over-all performance than other models in overlap evaluation. Its f-measure is 0.41% higher than supervised HMM in overlap evaluation.

Precision of partially supervised CRF is hi gh in exact evaluation, but recall of this model is not so good. Training data of the partially supervised CRF is not completely correct. The data was generated automatica lly by sentiment resources constructed by our approach. The sentiment resources which generate the training data of CRF have a 66.72% recall in overlap evaluation. This pe rformance affects the recall of CRF. 45.89% precision of training data also affects the precision of CRF. But we improved the preci-sion by using the polarity of a sentence.
 Partially supervised HMM improved recall, but its precision is not high. We can use HMM+CRF to overcome weak precision of partially superived HMM and weak recall of supervised CRF.

Table 7 shows the performance of identifying source of subjectivity phrases. Super-vised HMM performs well in the exact evaluation. 4.1 Subjectivity Labeling Problem The most important part of identifying sentiment of phrases is subjectivity tagging. Breck [7] identified subjectivity phrases us ing the various features and CRF as a su-pervised learning in the MPQA. It is difficult to compare directly with the evaluation result of the experiment, because we do not use the same dataset(MPQA) and the lan-guage is also different. In spite of these difference, we know that from their results, their f-measure of identifying subjectivity phrase is 73 . 05% in overlap evaluations [7]. It shows that it is not an easy problem to identify subjectivity phrases even if we use various features and supervised learning.

Many subjectivity errors come from the negative sentimental phrase. There are data sparseness problems in identifying the neg ative sentiment of phrases, because there are many ironic, cynical, and metaphoric and simile expressions in the negative expres-sions. These affect the overall performance in identifying the sentiment of phrases. 4.2 Necessary Characteristics of Training Data We used the partially supervised approach to overcome the problem of insufficient polarity-tagged corpus. Our approach used tagged sentiment of phrases automatically generated by sentiment resources. These se ntiment resources are automatically ex-tracted from sentence-level s entiment resource. Our approach also needs sentence-level sentiment training data. Such data sets are more plentiful than tagged sentimental phrase data sets. However in these data sets, there are more polarity annotations at the docu-ment level than at the sentence level. We need to select sentiment s entences in sentences when we use those data sets. In this case, this process unavoidably carries some error in selecting sentiment sentences. We compared the sentiment phrase (positive, n egative or neutral) tagging performance between various models (Table 6). We also compared the subjectivity phrase (sentimen-tal or neutral) tagging performance (Table 7) . One interesting result is the difference in performance in identifying sentiment (Table 6) and subjectivity (Table 7). Subjectivity includes positive and negative sentiment. Th erefore, it is simpler to label subjectivity phrases than to label sentiment phrases. In s pite of the fact that identifying subjectiv-ity phrases is a simpler task than identifying the sentiment of phrases, precisions in identifying subjectivity and sentiment are within 10% in both the exact and the overlap evaluations. This suggests that errors between positive and negative labels are minor. In other words, the overall performance is more heavily affected by the performance of subjectivity classification than by the performance of sentiment classification. The difficulty observed in identifying subjectivity phrases implies some ambiguity, even be-tween human decisions (Table 4). So the most important part of identifying sentiment of phrases is subjectivity tagging. Many subjectivity errors occurred when identifying negative sentimental phrases.

Our model solved the phrase-level sentiment classification problem by using partially supervised tagging approaches. That approach only used the sentence-level sentiment resource. Its precision is 76.01% and its f-meausre is 66.86%. Its f-measure is higher than the supervised approaches in the overlap evaluation. We found that the sentiment phrase tagging problem can be solved by a partially supervised approach.
 Acknowledgement. This work was supported in part by MKE &amp; IITA through IT Lead-ing R&amp;D Support Project and also in part by the BK 21 Project in 2008.
 Relation extraction is the task of finding predefined semantic relations between pairs of entities from the text. The research on relation extraction has been promoted by the Message Understanding Conferences (MUCs) and Automatic Content Extraction (ACE) program. According to the ACE program, an entity is an object or a set of between entities. For example, the sentence  X  X ill Gates is the chairman and chief software architect of Microsoft Corporation X  conveys the ACE-style relation  X  X RG-AFFILIATION X  between the two entities  X  X ill Gates (PER) X  and  X  X icrosoft Corpo-ration (ORG) X . The problem of relation extraction is typically cast as a classification problem. When learning a classification model, one important issue that has to be addressed is how to scale the similarity between the two relation instances. Existing approaches include feature-based and kernel-based approaches. Feature-based approaches trans-varying from entity semantic information to lexical and syntactic features of the rela-tion context. Kernel-based approaches, on the other hand, explore the structured rep-resentation such as parse tree or dependency tree, and compute the similarity between the structured contexts. In fact, feature-based approaches can be deemed as a specific case of kernel-based approaches by using the dot-product as a kernel function. Therefore, although feature-based approaches are easier to implement and achieve much success, kernel-based approaches still attract much interest from researchers due to their ability of exploring complex structured features through various kinds of representation of objects. 
In contrast to the significant achievements concerning English and other Western languages, research progress in Chinese relation extraction is quite limited. This may be attributed to the different characteristic of Chinese language, e.g. no word bounda-ries and lack of morphological variations, etc. Especially, the widespread tree-kernel based approaches in English relation extraction do not adapt to Chinese well since the quality of the available Chinese syntactic analysis tools are not as reliable as English tools. Motivated by this, we investigate in this paper how to design useful kernels for Chinese entity relation extraction, which will not be restrained by the unsatisfied Chinese parsing results. We propose to use a composite kernel which consists of two individual kernels. The first kernel is a linear kernel formulating the non-content type features and entity subtype features. The second kernel explores the content information in the contexts of the relation instances. Instead of tree-based kernels, we introduce a novel string-based kernel which can avoid the errors propagated from the incorrect parsing results. With these two kernels, we examine several combination functions to search for a near  X  X ptimum X  composite kernel. Experiments on the ACE 2005 data set show that the polynomial combination is more effective in capturing the information of the entities and their contexts. 
The reminder of the paper is organized as follows. Section 2 briefly introduces the related work. Section 3 details our proposed approach. Section 4 presents experiments and evaluations and Section 5 concludes the paper. Lots of approaches have been proposed in the literature of relation extraction, includ-ing rule-based, feature-based and kernel-based approaches. 
Rule-based approaches employ a number of linguistic rules to capture various rela-tion patterns for identifying the relations. For example, Miller et al. [1] addressed the task of relation extraction from the statistical parsing viewpoint. The result essentially depended on the quality of the entire full parse trees. 
Feature-based approaches utilize a set of carefully selected features obtained from different levels of text analysis, from part-of-speech (POS) tagging to full parsing and dependency parsing. For example, Kambhatla [2] employed Maximum Entropy mod-for relation extraction. Zhao and Grishman [3] explored a large set of features that are relation extraction using SVM. They conducted exhaustive experiments to investigate the incorporation and the individual contribu tion of diverse features and reported that chunking information contributes to most of the performance improvement from the syntactic aspect. Jiang and Zhai [5] systematically explored features of different levels of complexity in three subspaces and evaluated the effectiveness of different sufficient to achieve state-of-art performance, while over-inclusion of complex fea-tures might hurt the performance. 
Kernel-based approaches design kernel functions on the structured representation (sequences or trees) of the relation instances to capture the similarity between the two relation instances. For example, Zelenko et al. [6] proposed extracting relations by computing kernel functions based on two parsing trees. Culotta and Sorensen [7] used augmented dependency tree kernel, which is context-sensitive. The above two work was further advanced by Bunescu and Mooney [8] who argued that the information to between them in the dependency graph. Zhang et al. [9] explored the use of the syn-extraction and these features can be we ll captured by the convolution tree kernel. Zhou et al. [10] proposed a tree kernel based on context-sensitive structured parse trees and combined the tree kernel with a feature-based kernel. Evaluation on the ACE RDC corpora showed that feature-based and tree-kernel based approaches com-plemented each other. They thus reached the conclusion that a composite kernel which could integrate both flat and structured features was preferred. 
The previous approaches mainly focused on English relation extraction task. Al-though Chinese processing is as important as English and other Western language processing, only a little work has been published on Chinese relation extraction. Zhang et al. [11] exploited a feature-based approach by modeling the position struc-ture between two entities and using them along with entity type features and context duced several novel wordlist features and some correction mechanisms based on relation hierarchy and co-reference informa tion. Their system achieved very good performance. There were also some kernel-based approaches proposed for Chinese limited to the PERSON-AFFLIATION relation type only. Huang et al. [14] examined the performance of the tree kernel and shortest path dependency kernel for Chinese relation extraction. It outperformed the traditional feature-based approaches on non-nested relations, but unfortunately the overall performance was not satisfying. composite kernel consists of an entity kernel and a string semantic similarity kernel. 3.1 Entity Kernel instance. Structure information is an important factor for judging the relation. For example, many entity pairs with a relation type  X  X mployment X  have the nested posi-tion structure, such as  X   X  X  X  X  X  X  X   X  (Russian president) and  X   X  X  X  X   X  (Russia). 
In this paper we defined six position features which depict both the internal and ex-ternal structures of a relation instance. Given a named entity mention nem , let nem . start and nem . end denote the start and end positions of nem in a sentence respec-nem 1 . end &lt; nem k . start and nem k . end &lt; nem 2. start . 
The internal position features are defined by the relevant position between the two entities of the relation instance, including nested, adjacent and separated. On the other hand, the external features represent whether the two entities of the instance are em-bedded in some other entities, including none-embedded, one-embedded and both-embedded. The definition of the internal and external features is expressed in Table 1 and Table 2 respectively. Making use of symmetrical expression, for any entity pair nem 1 and nem 2 , we assume nem 1  X  nem 2 or nem 1 precedes nem 2 to simplify the expres-sions. By combining 3 internal and 3 external features, a total of 9 different position structures can be denoted, which are illustrated in Appendix A. 
A position-based kernel is then defined by the 6 position features as where P 1 and P 2 indicate the position structures of two relation instances respectively, P.f i is the i -th position feature of the position structure P . The function C (.,.) returns 1 if the two feature values are identical and 0 otherwise. 
Another kind of features is the entity type attributes features which are defined as the tion type will has a large probability to be the  X  X ser-Owner-Inventor-Manufacturer X  relation. In this paper, we use binary features to depict the type and subtype information of an entity. 
A type-based kernel is then defined by these type features as tity E . 
Finally, the entity kernel is defined by combining the position-based and type-based kernels together as: structure and the two entities of the relation instance R respectively. 
In the above equation, K p (.,.) returns the number of position feature values in whole entity kernel can be viewed as a reflection of the non-content characteristic of the two relation instances. 3.2 String Semantic Similarity Kernel In our belief, besides the non-content information, the content information in contexts mentioned in the introduction, the tree-based kernel which was widely used in English relation extraction did not work well in Chinese relation extraction due to the con-which does not rely on any complicated text structure like parsing trees. In [15], Islam and Inkpen proposed a string-based similarity which simply represented the texts as kernel to capture the content information. than the words, considering Chinese word-b ased models may be heavily affected by the two characters by Point-wise Mutual Information (PMI) which is a popular method for calculating corpus-based similarities between the words. ( w 1 , w 2 ) stands for the frequency of the characters w 1 and w 2 appearing together in a sentence and M is total number of the tokens in the corpus. After the similarities of all the character pairs are calculated, they are divided by the maximum one to obtain the normalized values between 0 and 1. 
Based on the PMI similarity between the two characters, we use the following way to calculate the semantic similarity between the two text strings. Consider a text string T characters of the two strings by putting )) , ( ( iteratively select a set of indicative elements from the matrix. Each round we find out the maximum-valued element  X  . Then we remove all the matrix elements of i -th row and j -th column from M most similar character pairs between the two text strings without repeating characters. The elements in  X  are then summed up and the sum is regarded as the similarity between the two strings. Finally we multiply the similarity by the reciprocal harmonic mean of m and n to obtain a balanced similarity value between 0 and 1. 
In conclusion, the similarity between T 1 and T 2 is calculated as: 
The string semantic similarity kernel is defined as: the window size is set to 4. 3.3 Composite Kernel Since the entity kernel and the string kernel depict the non-content and content infor-can well complement each other. Therefore a combination of them should yield more accurate estimation of the relevance between the two relation instances. In this paper, we try several different functions to combine the two kernels: 1) Linear Combination Here the sub-kernels K (.,.) are normalized first and a is the coefficient to balance the two kernels. a is set to 0.5 experimentally. 2) Semi-Polynomial Combination 3) Polynomial Combination Here the definition of ) , (  X   X  K is the same as above. 
Essentially, the combination functions imply different views of the structure be-tween the kernels. Generally, if a view is more coincident with actual situation, it can lead to the better kernels. 4.1 Experiment Setting The experiments are set up on the data set of the ACE 2005 Chinese Relation Detec-tion and Characterization task provided by Linguistic Data Consortium (LDC). The 633 documents are manually annotated with 9278 instances of relations. 6 relation SVM classifiers and the remaining to evaluate. Precision (P), Recall (R) and F-measure (F) are adopted to evaluate the performance. 
In the implement, we use SVMlight [17] as the binary classification tool and chose "NONE" class when there is no any predefined relation between the two entities. We internal type and none-embedded external type. The reason of this consideration is that in the actual data the ratios of positive to negative class of the other position types are so imbalanced that the learning method is very inefficient on them. So we would instances belong to the chosen types so the harm to the recall caused by ignoring the other position types is within the acceptable range. Table 3 gives the detailed statistics, and the chosen types are indicated in bold. 4.2 Experimental Results semantic similarity kernel ( K S ); (3) the linear composite kernel combining both precisions, recalls and F-measures. We can see that the performance is low when the string similarity kernel is used alone. In contrast, the performance is much higher when only the entity kernel is used. It seems to indicate that the non-content information is much more important than the content information. In fact, most relation types have their conventional structures and entity types. Therefore, the non-content information is essential and decisive in exploring the relations be-the entity kernel. On the other hand, the content information may play the role as a complement of the non-content information. The result that the performance is improved by integrating the two kernels supports this idea, though the improve-ment is not very significant. However, at least we can expect that a combination of the two kernels which explore the relation from different points of views can lead to the better kernels. 
In the second set of experiments, we intend to further study the effect of kernel combination by comparing the performance of several different composite kernels introduced in Section 3.3. An additional quadratic entity kernel ( KL 2 ) is also included. Table 5 reports the performance of all the kernels. From the experiments, we observe that: 1) The best composite kernel achieves 6% improvement over the entity kernel. 2) Among our composite kernels, the polynomial combination yields the best 
In the third set of experiments, we compare our kernel-based approach to the ap-proach introduced in [11]. They propose a sophisticated feature-based approach which includes both entity-based features and context-based features. Notice that their en-tity-based and context-based features are not defined exactly the same as ours. Results are reported in Table 6. The performance of our proposed kernel-based approach is remarkably better than the performance of their proposed feature-based approach. In this paper, we propose a composite kernel for Chinese relation extraction. Two kernels concerning the roles of both the co ntent and non-content information in simi-larity calculation are defined and combined to obtain the composite kernel. Several combination functions are compared in experiments to find the good composite kernel. Experiments on the ACE 2005 data set show that the proposed approach is effective. PolyU5211/05E) and China NSF (60603027). An ontology can be seen as an organized structure of concepts according to their describes C . For instance, the concept {software} should be associated with a manu-facturer and a release date, among other things. As attributes themselves are also concepts, an attrib ute of a concept C can also be defined by a relation between C and or attributes, which are extracted from syntactic structures [1-5]. Most current ontolo-gies are constructed manually by domain experts, some with assistive tools. However, manual construction is time-consuming and timely update is always difficult [6-7]. 
Automatic ontology construction can take either a top-down approach or a bottom-up approach. In the top-down approach, some upper-level ontology is given and algo-rithms are developed to expand it from the most general concepts downwards to reach the leaf nodes where instances of concepts can be attached as leaves [8]. On the other hand, in the bottom-up approach, some doma in corpus is used to extract concepts, attributes and relations without prior ontological knowledge. Most corpus-based on-work is to identify relations among the concept terms [9-10]. In a truly corpus-based pointed out that instances of concepts are no rmally not considered a part of ontology. If they are appended in an ontology, they should appear only as leaf nodes. There are methods to identify relations between con cepts assuming that concept terms are concept instances appear even more than their corresponding concepts. For example, [[Internet Explorer]] and [[Microsoft Excel]] are instances of the concept {software}, and in a general corpus, such instances may occur more. The association between instances and their corresponding concepts must be identified. Otherwise the con-structed ontology would be a confusing network of links without distinction of con-cepts and their instances. However, for ontology construction using the corpus based approach, there is a natural gap between the ontology as a concept-level structure and the corpus as an instance-rich resource. For example, there are more sentences like  X  X okia N73 has two cameras and a powerful CPU X  rather than  X  X  mobile phone can work to acquire concepts and their associat ed attributes using a bottom-up approach. 
Wikipedia (Wiki for short), the world X  X  largest online encyclopedia, serves as the corpus with definitions and descriptive information. Its tags and constructs defined through XML also supply semantic rich information for text mining and information extraction. This paper proposes a simple method to identify the set of instances of the same concept through the {{Infobox}} structure supplied in Wiki for instances with identified through a simple identification scheme which identifies the most appropri-ate set of attributes of a concept by ranking the attributes linking different instances to the same concept. As each acquired attribut e may have different semantic classifica-tions, the algorithm also identifies the most appropriate semantic type with the help of category information in Wiki articles. These categories roughly give a topic of given attribute according to its valu es indicating the attribute X  X  semantic nature. For exam-ple,  X  X ategory: people X  will be given to the {name} attribute of concept {writer}, denoted as {name} writer , so it would not be qualified as a software or a server in com-puter science domain. Simple rules are applied to the identification algorithm to im-prove the performance. The rest of the paper is organized as follows. Section 2 presents some related works. Section 3 describes the proposed model and the algorithm for acquiring concept-level attributes and identifying attribute types. Section 4 shows the evaluation details along with some analyses. Section 5 concludes the paper with future directions. Many efforts have been made to acquire attributes for ontology construction. For top-relations between their general concepts are usually defined by linguists or domain experts. The suggested upper merged ontology (SUMO) is such kind of ontology, which was built by a working group with collaborators from the fields of engineering, philosophy, and information science [7]. Efforts are also made to use automatic meth-ods to roughly classify concept attributes. The work in [15] proposed a classification of concept attributes and developed two su pervised classifiers to identify concept attributes from candidates extracted from th e Web. The classifiers used four types of information: morphological information, clustering information, a question model and an attribute-usage model which is based on text patterns such as  X  X hat is | are the A of  X  X elated agent X , and  X  X on-attribute X . 
For bottom-up ontology construction, most of the efforts focused on automatic methods to acquire attributes from a give n corpus. Therefore, the methods need to bridge the gap between instance-rich resour ce and concept-level target, an ontology. Clustering is one of the natural ideas to solve this problem. Attribute-based and value-based (instances of attributes) clustering we re compared in [1] to evaluate whether attributes can lead to better lexical descriptions of concepts. Simple text patterns were used to automatically extract both value-based and attribute-based descriptions of concepts for clustering purposes. The conclusion is that even though there are less describe concepts as well as or even better than attribute values do. On the other hand, the best descriptions of concepts include attributes with more general attribute values. Both clustering and pattern-based fuzzy inferences were used in [9] to acquire con-cepts, attributes, associations and new instances for a Chinese domain ontology. An unsupervised neural network algorithm was used for clustering concepts and defining taxonomic relationships. Then the attributes and associations were extracted based on These episodes indicate subject-verb-object or parallel or descriptive relations. How-ever, this clustering and shallow semantic analyzing method does not perform well in the area with rapid changing terms and con cepts or with complex semantics. Pattern based methods are also tried by many researchers on semi-structured texts or more efficiency resources such as tables and query logs [11-12]. The work of [11] proposed a pattern based method which acquired attribute-value pairs for given objects from tables in HTML documents. The proposed method acquired attributes for objects from 620 given class-object pairs collected from definition sentences in Japanese Wiki. The approach in [12] exploited both web documents and query logs to acquire tributes adopting on these instances, so the instances linked attributes to classes. However, the precision is considerable only on small-scale manually-assembled classes. These approaches for attribute acq uisition are complicated in method or lim-ited in effectiveness and coverage. 
Wiki is an open-content and collaboratively, it has expanded from around 1 million articles (November, 2006) to more than 2 millions till now [13]. Wiki is an informa-tion-rich resource with internal and external hyperlinks and relevant classification declared by contributors manually, which makes it a good resource for concept, at-tribute, and relation extraction. However, Wiki is also an instance-concept mixed corpus, which means some Wiki pages describe concept instances, while others describe concepts. For example, a page on the subject {Company} is a concept page. In this paper, several notations are defined to identify different kinds of objects. The notation {C} is for concept C . Yet, a page on the subject of [[Microsoft]] is actually an instance page. The notation [[I]] indicates an instance or a value of an attribute. As {location} is an attribute of th e concept {company}. The notation [AT] labels attrib-help of instance description information supplied by Wiki infoboxes among the ways Wiki links instances to concepts. An {{Infobox}} in Wiki is a consistently-formatted table which is mostly present in articles with a common subject ( concepts ) to provide summary information and help that subject. In fact, an {{Infobox}} is a generalization of a taxobox (from taxonomy) which summarizes information for a subject or subjects [14]. 
Fig. 1 shows the syntax of {{Infobox}} in a Wiki article page and an example of with {{Infobox}} and Category List. The right part gives an example of {{Infobox}} contents and categories. It shows an instance article page with the name [[Atlas Shrugged]], which is the name of a novel and the related information according to predefined {{Infobox}} syntax. The first line of an {{Infobox}} provides {book} as the concept which the instance [[Atlas Shrugged]] belongs to. The subsequent list defined using  X = X  are attributes of {book} which are associated with the instance [[Atlas Shrugged]]. The terms or phrases on the right part are values of the corre-sponding attribute on the left hand side. For instance, in the entry author=[[Ayn Rand]] , author can be considered an attribute of {book}, denoted by {author} book , and [[Ayn Rand]] is an instance of {author} book . 
Consequently, {{Infobox}} can be used for two purposes. Firstly, it can be used to extract attributes for a given concept. Second ly, it can be used to estimate appropriate semantic type for each attribute. It should be pointed out that even though each same concept may use a different list of attr ibutes. Therefore, there is the issue of how to identify a common set of attributes which are considered most appropriate for a concept. In Wiki, an additional list of category information is contained in a page in total i number of categories the page editor considers to be related to this page. They category information is used to estimate the semantic type of the attributes. 3.1 Concept Attributes Acquisition As {{Infobox}} structures in instance pages denote corresponding concepts, the first step is to collect all the instance pages through the identification of {{Infobox}} structures in Wiki. As shown in Fig. 1, the format of Wiki and {{Infobox}} are rela-tively fixed. Thus the corresponding information can be acquired by patterns. Each attribute A of different instances( I s ) belonging to one concept C is collected into one set S A (C) with the help of two 2-level hash tables, one for concept and instance map-ping named H I (C) and the other for instance and corresponding attributes named H A (C) . Then the attribute identification pro cess can be achieved by the Attribute Acquisition Algorithm (AAA) to obtain S A (C) as shown bellow which only selects a threshold value. 
Attribute Acquisition Algorithm (AAA) { for each C in H I (C) { Count_of_Ins = scalar(H I (C)); for each I in H I (C) { for each A in H A (I) { H A (C)-&gt;{A}++; }endfor }endfor }endfor for each C in H A (C) { Freq= H A (C)-&gt;{A}; if(Freq&gt;=THRE*Count_of_Ins) {put A into S A (C)}endif }endfor }endalgorithm AAA 
AAA can be divided into two parts. The first part is the counting of appearances for each attribute for C . The second part is the selection of qualified attributes accord-ing to the majority threshold value, named THRE , which is an experimentally deter-mined algorithm parameter. In algorithm AAA, all the appearance count stored in a 2-level hash H A (C) with first level of keys as concepts and the second level as attrib-concept will be selected as qualified concept attributes. All the qualified attributes are stored into the set S A (C) . 3.2 Attribute Type Identification The conclusion of [1] has mentioned that concepts can be described better if both attributes and attribute values are supplied. Generally speaking, in ontology construc-domain or concepts in ontologies of differ ent domains. One way to identify a specific the location of an accident whereas for organizations such as {UKschool}, an associ-ated attribute {site} UKschool can be the website address of this organization. This exam-ple shows that if the domain of the attribute value range (called the attribute type ) is identified, it can further qualify the semantic type of the attribute. 
Attribute values can be stated in different formats such as words, sentences and ta-bles. Identifying attribute type is not straig ht forward because they are quite arbitrary in its presentation form. This work proposes to use the frequency of Wiki article names as instances to indicate the implicit inclination of attribute types. This is based on two considerations. First, the attribute value descriptions contain useful informa-tion but are usually not well-formed. So, the use of NLP tools such as parsers are not names using pre-defined form so it is easy to acquire these Wiki articles. In fact, these article names can be considered as named entities marked in attribute value descrip-tions. Consequently, these Wiki article names are taken as key words of attribute value descriptions for attribute type an alysis. For example, attribute {developer} software of [[REXX]](a programming language) has a value  X  X ike Cowlishaw &amp; [[IBM]] X  homonymic Wiki article names. Then, the category information in matched Wiki {developer} software of [[ICQ]] takes [[AOL]](American online) and {developer} software of [[Internet Explorer]] takes [[Microsoft]] as values. Then a 2-level hash mapping Wiki article names to corresponding categor ies [13] can be used to obtain categories of Wiki pages named [[Mike Cowlishaw]], [[IBM]], [[AOL]], and [[Microsoft]], such as  X  X ategory:Computer Programmers X ,  X  X ategory:Software companies of the United States X  and  X  X ategory:Companies listed on NASDAQ X . The most frequently used categories will be selected as the attribute type of a given attribute. Two 2-level hash tables are used in Type Identification Algorithm ( TIA) . One is H V (A) mapping attributes to key words in their values. The other is the category hash mapping article names to corresponding gram list, referred to as H CAT (V) . 
Type Identification Algorithm (TIA) { for each A in H V (A) { MAX=0; for each V in H CAT (V) { for each CAT in H CAT (V) { Freq = H CAT (A)-&gt;{CAT}++; If (Freq&gt;= MAX) {MAX=Freq;}endif }endfor }endfor }endfor for each A in H CAT (A) { F = H CAT (A)-&gt;{CAT}; if(F==MAX) {put CAT into S CAT (A);}endif }endfor }endalgorithm TIA 
The structure of algorithm TIA also contains two parts similar to AAA. The first part is to collect candidates of an attribute type. The second is to select the most ap-propriate semantic type if an attribute. In algorithm TIA, A is an attribute, V is a key cords the number of instances linking to the same category. MAX records the maximum vote number for each attribute. The first part of algorithm TIA is to collect the categories with key word values directly linking to them and record the categories Freq will be stored into the result set, namely S CAT (A) . 
As Wiki is an open-content and collaboratively edited encyclopedia, some categories are not formal enough to be cons idered as attribute types. For example, semantic type induced from this value should be DATE rather than a specific date value. In the experiment, TIA has also integrated some pre-processing and post-processing steps to handle these special cases which will be discussed in Section 4.2. The experiments are implemented using the English Wiki with the cut off date of November 30 th , 2006 containing about 1.1 million article and category pages. After applying {{Infobox}} extraction patterns, 111,408 {{Infobox}} have been extracted for 110,572 article names, which are considered as instance pages of Wiki. The dif-{{infobox}} structures. For example, [[Arnold Schwarzenegger]] is an instance of {Actor} and also an instance of {Governor}, which contains different attributes. In the algorithm, an instance is allowed to be associated with more than one concept term. There are 1,558 concepts relevant to the 110,572 instances. For example, con-cept {company} has 2,585 instances, such as [[Microsoft]] and [[Bank of China]]. 4.1 Evaluation for Algorithm AAA There are two measures in the evaluation of AAA on the extracted attributes. The first measure is to examine how many of the extracted attributes are in fact commonly the attribute precision, p . The commonly used measure of recall for acquired really the scope of this work to judge their appropriateness as a whole because there is no uniformed agreement on the completeness of attributes which is a subjective issue and depends on the level of details needed. On the other hand, different threshold number of concepts with respect to the total number of concept pages (total of 1,558) in Wiki. So, the second measure is to exam ine how many concepts are identified with attributes. In other words, the measure is the recall of concepts, r . 
Experiments are conducted to see how different values of THRE , the algorithm pa-rameter, can affect the performance in terms of attribute precision and concept recall. The value of THRE ranges from 0.0 to 1.0 by taking 0.1 as increment for each value. the attribute precision and concept recall to find a good threshold value which gives a as the commonly used f -factor as follows: where r i and p i are concept recall and attribute precision of the i th data set. 
For actually evaluation, a sampling method is used to estimate both the precision the total 11 sets of data, a total of 440 concept-attribute pairs are evaluated manually. 
Fig. 2 shows the evaluation result. It can be seen that the attribute precision ranges from 52.5% to 92.5% with an optimal point at around 0.9. On the other hand, the recall of concepts decreases from 100% when THRE is 0.0 to 80.0% when THRE is 1.0. For example, if the threshold is set as 1, 311 concepts out of 1,558 would have no f-factor is reached when the threshold is set to 0.9. At this point, the concept recall is 90.7% and attribute precision reaches 91.6% and f-factor as 92.5%. To explain more on the effect of different THRE , take the concept node {company} as an example. There are a total of 2,585 instances of {company}. When THRE is set to 1.0, it means pany}, there is no qualified attributes under threshold 1.0. When THRE is reduced to ments, the threshold value around 0.9 is a good choice. 4.2 Evaluation for Algorithm TIA To evaluate TIA for attribute type identification, the optimal threshold value THRE types and how many concepts/attributes can be covered by TIA, referred to as the recall of concepts/attributes. 
Initial evaluation of TIA uses 100 evenly distributed samples of &lt; concepts, attributes, and attribute recall is also too low. Further analysis is then made to find the reason to such data sparseness. Also, the formats of attribute values are not uniform making it difficult to extract them even if they are present. Besides, some of the categories present in the Wiki and  X  X ategory: Italian football clubs X . So, the issue is to remove the instance information. resolve the problem in some degree. 
The preprocessing is to eliminate the reference to named entities such as countries, cities, etc.. An n-gram collection (n=1, 2 here) is applied to attribute values and cate-gories. Unigrams and bigrams of these category strings are extracted as substitutes of categories and those with highest product of frequency and length will be considered as attribute type. That is to use the components of a category instead of the category itself. As a result, [football clubs] will be extracted as category. The post-processing handles the errors caused by numeric values and hyperlinks. types. For example, the attribute type of year 1200 should be [year] rather than  X  X ate-gory:1200 X  containing the actual attribute value of year 1200. There are also cases instance {Brazil}is not defined as a Wiki article name. Some attributes are listed instances of {architect} in the Wiki version used here. According to the analysis, two simple induction rules are applied as post-processing. They are listed as follows: R2: If an attribute has no attribute values, its attribute type is labeled by the name of 
Table 1 shows the evaluation result of the original TIA ( TIA ), TIA+Preprocessing attribute types are correctly identified. After adding the pre-process part, coverages of concepts and attributes reach 100% and 67%, respectively. However, the precision is still no more than 30%, which means only category information is not enough. By applying two simple induction rules, the precision of TIA reaches 80% and the recall of attributes also near 80%. As the performance is improved quite significantly after applying n-grams and induction rules, further analysis is made to look for reasonable explanations. The Table 2 shows the contributions of original TIA, the n-gram pre-processing and post-processing rules in identifying attribute types. 
Table 2 shows that pre-processing by using n-grams contributes to an additional 17.1% attribute type identification and post-processing contribute to 42.5% of the attribute types including numbers, dates, links and attribute names. This explains why the performance of pre-processing and post-processing can significantly im-prove the performance so that many more attributes can be correctly categorized. However there is still about one quarter of the attribute types which cannot be identi-fied by the algorithm, which means that category information is not sufficient for attribute type identification. The unvalued attributes also limit the precision of algorithm. Other information in Wiki may be used in the future to improve the per-formance of TIA. association between instances and concepts, {{Infobox}} tables in Wiki are used to acquire essential attributes for concepts an d category information are used to identify attribute types to complement the descriptions of concepts. A simple substring han-dling and two simple induction rules can effectively improve the precision and cover-age of attribute type identification. The f -factor of concept attribute acquisition reached 91.6%. The precision of attribute type identification reached 80%. The ad-vantage of the proposed method is that it only uses information supplied by Wiki. There is no need for complex algorithm or training. Although the coverage of infobox cannot cover all pages, the method is completely automatic and the acquired instances and concept attributes are enough for bootstrapping ontology construction and can be directly used for automatic ontology construction and extension. 
More information from Wiki can be used to improve the precision of concept at-tributes acquisition and attribute type identification. For example, the absolute num-ber of instances for one concept can be cons idered as an influen ce factor for concept attribute identification. Also, the context of the pages containing infoboxes can be type identification. Other work can also be explored to apply the proposed method in a selected domain for domain ontology construction. Acknowledgments. This project is partially supported by CERG grants (PolyU 5190/04E and PolyU 5225/05E), B-Q941 (Acqui sition of New Domain Specific Con-cepts and Ontology Update), and G-YE66 (An Intelligent Risk Assessment System for International Maritime Safety and Security Law in HK and China). With the rapid development of computing technology such as speech processing, multimedia and internet, computer-assisted language learning (CALL) systems are getting more comprehensive [1]. A computer assisted pronunciation teaching (CAPT) system, as the basic component of a CALL system, can give feedbacks to a learner X  X  mis-pronunciation in order to improve his pronunciation. In addition to an embedded automatic speech recognizer, a CAPT is able to score the pronunciation quality of the learner. A simple CAPT system, which gives one evaluation score based on different speech features, can only tell how good a learner performs, but not how he can improve. More complicated error model based CAPT systems can give suggestions on how to improve learner X  X  pronunciations [2][3][4]. 
A more comprehensive speech recognizer in CAPT needs acoustic models for both the correctly pronounced phones and the incorrectly pronounced ones. Consider a phone, the learner is most likely to utter it either as [fleg] or [flag]. Having both two error phone models and the correct model, the recognizer can tell not only how wrong the actual pronunciation is as a score, but also which actual error model is most likely such as  X  X ut your tongue a little downward X  to improve learner X  X  pronunciation from [fleg] to  X  X lag X  ([fl X g]). Obviously, a comprehensive CAPT system like this is much better than those which can only give a simple score without specific suggestions. Yet a comprehensive CAPT needs a large amount of incorrectly pronounced speech data to train the error models through a large amount of recording followed by error type clustering using either a manual or an automatic classification method. Without enough training data, error models can face data sparseness problem. Collecting incorrectly pronounced speech data is a labor intensive and costly work. And incorrectly pro-nounced speech data is much more difficult to obtain than the correct ones [5]. 
Instead of exhaustive manual recording an d classification to minimize the effect of data sparseness problem, this paper proposes an alternative method to produce incor-rect speech data through a formant speech synthesis. Analysis of a small set of samples of correct/incorrect phone pairs suggests the formant relationships between these pairs. According to the obtained formant relationships, the formant frequencies in the vocal tract filter of the correct phone are mapped into that of the incorrect ones. In this way, all the incorrect speech data are obtained fr om their correct counterparts. The charac-teristics of the speakers and other environmental variations of original correct speech are unchanged by keeping the original formant bandwidths and the LPC residual. 
A small experimental system is built for demonstration and evaluation of the pro-posed method. Experiment shows that the synthetic incorrect speech trained model performs as good as its prerecorded counterpart and with enough generated synthetic data, it can actually out-perform the pre-recorded counterpart. 
The rest of the paper is organized as follows. The basic design idea and methodology are explained in Section 2. Experiments and results are detailed in section 3. Section 4 is the conclusion. 2.1 Basic Design Idea of the Formant Synthesizer Fig . 1 shows a general speech synthesizer. The synthesis process can be modeled as a linear combination of a vocal tract filter as a linear time-invariant system and the glottal synthesizer for error phone production in this work uses a vocal tract filter specified by the first four formant frequencies F 1 to F 4 and the first four bandwidths B 1 to B 4 . 
To excite any vocal tract, a sound source is needed. In this paper, the linear predic-tion (LPC) residual is used as the sound source. By extracting the LPC residual from the prerecorded speech material, the pitch and voice of the speaker remain the same. 
The following is a brief description of linear prediction analysis. A more detailed treatment can be found elsewhere in literature [9]. The sampled speech waveform () sn can be approximated by another sequence  X  () sn through linear prediction of the past p samples of () sn : minimizing the mean squared differences. The LPC residual is: 
Formants are the resonances of the vocal tract. they are manifested in the spectral domain by energy maxima at the resonant frequencies. The frequencies at which the determined by the positions of the articulator s (tongue, lips, jaw, etc). The relationship or ratio of adjacent formants is called formant structure. 
Former researches demonstrate that the quality (correct or incorrect) of a phone is mainly determined by formant frequency structure whereas sound source, formant bandwidths, absolute values of formant frequencies and other speech parameters are mostly depend on different speaker, speaker X  X  different state and emotion, and different environmental conditions [6]. A reasonable inference is that by modifying only formant structure, a correct phone can be synthesized into an incorrect phone, yet other speech characteristics can be kept unchanged. Consequently, the phone model trained by the speaker and other conditions as its correct counterpart. 
Table 1 shows the first four formant frequency values of two different phones [a:] and [e], read by two different speakers, A and B. The ratios of the corresponding for-mant frequencies between different phones are also listed. Table 1 shows that the formant structures by both speakers are very similar for the same phonemes even though they have very different absolute formant values. In fact, the ratios of the two phonemes from the two speakers are almost the same. This suggests that the formant frequency ratio of different phones from a small set of speech data can be applied to generate a larger set of speech data. 
T his invariant of formant ratio is meaningful to build the incorrect speech synthe-sizer. Scaling formant frequencies by the ratio of the incorrect to the correct, a correct phone can be synthesized into an incorrect one. The principle in this work is synthesis large amount of incorrect speech data from correct speech data using the ratios obtained from a much smaller set of sample data. The characteristics of speakers, emotions and other variations are kept unchanged as the original correct speech data. 2.2 Auto-synthesis Program Procedures Some preparations need to be done before auto-synthesis. Firstly, the common types of pronunciation errors are identified thr ough expert analysis and summarization. This conceptual error models can be established. Based on the analysis result, correct and between the first four formant frequencies of correct and incorrect speech data. As a result, the modification ratio of each formant can be determined. 
After the preparation, the prerecorded correct speech data and the corresponding formant ratios can be input to the auto-synthes izer to generate the incorrect speech data one by one as shown in the block diagram in Fig.2. 
The proposed auto-synthesis has three steps. Step One is the LPC analysis. The LPC coefficients of the prerecorded correct speech data are computed by the autocorrelation method. The LPC residual of the prerecorded correct speech data is extracted by in-verse filtering. And the first four formant frequencies of the same data are decided by solving the LPC equation [7]. Here the prediction order p=18. 
Step Two is the formant frequency modification. The formant frequencies of the modified formant frequencies for incorrect synthetic speech data. A new vocal tract filter for the incorrect synthetic speech is built using the modified formant frequency values. 
Step Three is the synthesis. The LPC residual is used to excite the vocal tract filter, and then the new synthesized incorrectly pronounced speech data are obtained. 
The proposed method has two main advantages. Firstly, it can maintain the char-acteristics of the speakers used in the training data by keeping the original LPC residual and formant bandwidth. Multiplying the formant frequencies only by a ratio can also keep the difference of formant location caused by different vocal tract sizes. Secondly, the relationship between formant frequencies and the type of incorrectly pronounced error concluded by a small set of speech data can be used to modify other speech data which is not from this small set. The incorrectly pronounced speech data synthesized in this experiment is used in a Chinese Putonghua CAPT system rEcho [4] to teach native Cantonese speakers to speak Mandarin. Three typical syllables/characters confusing to native Cantonese of incorrectly pronounced syllables for each choice are showed in Table 2. 
The corresponding syllables/characters are constructed into some carefully chosen sentences for the purpose of pronunciation error detection. Table 3 shows the 3 sentences containing these phones (1)  X   X  X  X [ X ] X  X  X  X  (li3 da4 ge1 mai4 mian4 bao1) X  with [k  X  ] in it, (2)  X   X  X  X [ X  X  X  X  X  (a1 li4 he1 ka1 fei1) X  with [x  X   X  ] and (3)  X   X  X  X  X  X  X  X  X  X  X  X  pronunciations showed in Table 2 can then be constructed. 
The aim of the experiments is to examine the performance of the synthesized in-correct speech data when applied to the CAPT system. Two speech training databases are used for comparison the synthesized da ta training acoustic models to the prere-corded data training ones. The prerecorded speech database is from a group of 20 male and 20 female speakers who are native mandarin speakers and can imitate Cantonese. pronounce the incorrect phones, they are inst ructed to follow the error models and types summarized by expert. The synthetic speech database is obtained by modifying the correct speech data in the prerecorded databa se. In other words, the prerecorded data-pronunciations whereas the synthetic database contains only the 6 sentences with in-correct phonemes. There are 80 samples for each of the sentences in both databases. showed in Table 3 where K 1 ~K 4 are the modification ratio of F 1 ~F 4 , respectively. 3.1 Evaluation by Open Test In this section, the performances of the CAPT using both the prerecorded and the synthesized speech data are evaluated by prerecorded testing data. For each sentence, the training set of prerecorded and synthesized model comprises 60 sentences, which are randomly selected from the prerecorded and synthesized database. The test sets for the two models are the same, comprising 20 prerecorded sentences which are not contained in the training set. Fig. 3 shows the evaluation result expressed by character sentences of the 1st sentence  X   X  X  X  X  X  X  X  X  X  (li3 da4 ge1 mai4 mian4 bao1) X . [x  X  ], [x
From Fig.3, it can be seen that with equal amount of training sentences, the synthetic sentence training models perform worse than their prerecorded counterparts as a whole. difficult to synthesize with fricative consonants in them. Therefore, the 2nd and the 3rd sentences have lower cer than the 1st sentence. 
It is understandable that the CAPT performs better using prerecorded real data for training compared to the system using equal amount of synthesized data. However, synthetic phones are artificially synthesized with a ratio. By deliberately changing the ratio, more synthesized sentences can be generated to enlarge the training size whereas using prerecorded data does not have that liberty. 
To further investigate the size of the synthesized data to the performance of CAPT, another set of experiment is conducted using the sentence  X   X  X  X  X  X  X  X  (a1 li4 he1 ka1 fei1) X  with different sizes of synthesized data. Results are listed in Fig.4. Fig. 4 shows phone [x  X  ]. Yet when the synthesized data size is increased to 640, the cer is reduced to synthetic cer is decreased from 33% with 80 training sentences to 10% with 640 sen-tences, which is even better than the prerecorded counterpart of 20%. Generally speaking, the more synthesized data are used, the better the cer values are. In fact, cer decreases quite rapidly as the number of synthetic training sentences increases. The results also indicate that in a system with limited real speech data for training, synthe-sized speech data is not only useful, it can even perform better than system using real data provided that there are sufficient synthesized data used. ratio in both directions. 3.2 Test by Native Cantonese Speakers In this section, the prerecorded and synthesi zed data are used to train the CAPT system, Cantonese speakers and cannot speak Chinese Putonghua well. This is very much like the real working condition of CAPT. 
Fig.5 shows the cer of the 3rd sentence  X   X  X  X  X  X  X  X  X  X  X  (a1 mei3 you3 xi2 ti2 da2 Cantonese speakers. They mispronounced it into [si] and [  X  i]. So Fig.5 has cer of [si] and [  X  i] only. 
Fig.5 shows that with the same number of training speech samples (80), the synthetic speech trained model performs worse than the prerecorded model by 33% in cer . As decrease rapidly. The 640 synthetic sentences trained model has a 0 cer for both [si] and [ i], which is better than the system using the prerecorded data. These experiments shows that a CAPT system using synthesized data for error model training can sub-stitute prerecorded speech trained model provid ed that the training size is reasonably large. The test conducted in this experiment is more convincing than the test in Section 3.1. The test speech data in Section 3.1 are recorded by people who can speak Chinese Putonghua(Manderin) very well, their incorrect pronunciations are produced based on instructions. But the test speech data in this subsection is recorded by native Cantonese speakers who cannot pronounce the word correctly. This paper presented a novel work to use synthesized data for error model training in the CAPT system. Results show that with reasonably large number of synthesized speech data based on correct and incorrect speech data, a CAPT system can give comparably performances as its prerecord ed counterpart. This gives light to the availability of more comprehensive CAPT systems which requires a large amount of training data, especially incorrectly prono unced data. More experiments can be con-ducted on the appropriate size of the synthetic data. Other phonetic features may also be investigated to see their effect in synthesis. This project is partially funded by the Hong Kong Polytechnic University (Grant No.: A-PF84). In th i spaper ,w euse n atura ll a n guage pr o cess in gtech ni ques t o attack a J apa n ese ke y.

I ftherec i p i e n ts o fthec i pherte x t message ha v e the subst i tut ion ke y, the y part yw h oin tercepts the message ma y be ab l et o guess the o r i g in a l p l a in te x tb y a n a ly z in gtherepet i t ion patter n s in the c i pherte x t .

F r o ma n atura ll a n guage perspect iv e ,w eca nvi e w th i scr y pta n a ly s i staskas ak in d o fu n super vi sed tagg in gpr o b l em [1]. W emusttageachc i pherte x tt o ke n make se n se , s ow euse l a n guage m o de lin g (LM) tech ni ques t o ra n kpr o p o sed de -qu i ck ly.

In th i spaper ,w e attack a part i cu l ar J apa n ese s yll ab l e -subst i tut ion c i pher fr o mthe W arr in g S tates P er io d , sa i dt o be emp loy ed b y Ge n era lU esug iK e n-sh in. 1 T he c i pher emp loy sthecheckerb o ard -st yl eke y sh own in Fi gures 1 a n d2 . T he ke yi sfi ll ed o ut acc o rd in gt o the i-ro-ha a l phabet ,w h i ch uses each J apa n ese s yll ab l ee x act ly on ce .To e n c o de a message ,w e loo kupeachs yll ab l e in the ke y, a n dthesec on dd i g i t i sther ow in de x.

No te that the ke y c on ta in s ka but no t ga .In th i ss y stem , p l a in te x t ga i s as ko , a n d gu i se n c i phered the same as ku , a n ds o f o rth .No te that th i smea n sthe rec i p i e n tma y ge n erate se v era l read in gs fr o mthec i pher a n dmustusec o mm on tr y t o rephrase the o r i g in a l message in o rder t o a voi du nin te n ded amb i gu i t y. ters ca n be used , s o that the c i pherte x thasthe o ut w ard appeara n ce o f w r i tte n l a n guage .Al s o, the ke y ca n be scramb l ed per io d i ca lly t oin crease secur i t y.
T he g o a lo fcr y pta n a ly s i s i st o take a nin tercepted n umber seque n ce a n dguess the n umber seque n ce in Fi gure 3. In the past , researchers ha v ee x p lo red ma ny strateg i es f o r attack in gdec i pherme n t m o de l f o r P( e )on separate p l a in te x tdata .W ethe n ad j ust the P( c | e ) parameter pr o bab ili t y ca n be w r i tte n: the guessed ke y. Kni ght et a l. [5] pr o p o se the E M a l g o r i thm [6] as a meth o dt o guess the best pr o bab ili st i c v a l ues f o rtheke y. O n ce the ke yi ssett l ed on, the Vi terb i a l g o r i thm ca n search f o r the best dec i pherme n t :
Kni ght et a l. [5] a l s o de v e lo pa nov e l tech ni que f o r i mpr ovin gdec i pherme n t us in gthe Vi terb i a l g o r i thm t oin stead search f o r :
Fin a lly, the y pr ovi de a n e v a l uat ion metr i c (n umber o f guessed p l a in te x tt o-ke n sthatmatchthe o r i g in a l message ), a n dthe y rep o rt resu l ts on E n g li sh l etter subst i tut ion dec i pherme n t .
 T he nov e l c on tr i but ion s o fth i s paper are :  X  W e attack a m o re d i fficu l tc i pher s y stem .T he U esug i c i pher has m o re charac - X  W e attack c i pher l e n gths that are no ts olv ed b ylow-o rder l a n guage m o de l s .  X  W efi n dthath i gher -o rder n-gram m o de l s parad oxi ca lly ge n erate wo rse dec i- X  W estud y the i mpact o fra n d o mrestartsf o rE M(no temp loy ed b y[5] f o r roomaji v ers ion 2 o ftheb oo k Tale of Genji ( c .10 2 1AD).W epr o cess the r oo ma ji in t o three parts :  X  LM tra inin gdata (900,01 2s yll ab l es ).  X  LM sm oo th in gdata (1 4 19 s yll ab l es ).  X  Pl a in te x t messages (v ar io us s i zes ).

W ee n c i pher the p l a in te x t messages b y first rep l ac in g ga wi th ka , a n ds o Our c i pherte x ts c on ta in 4 6 u ni que t y pes .T hree o fthece ll s in the 7  X  7 i-r o-ha tab l eareu n used in the c i pher s y stem .

Our e v a l uat ion metr i cf o rdec i pherme n t i sthesameas [5] X  X  ec o u n tthe n umber o fmatchesbet w ee n the guessed dec i pherme n tseque n ce a n dthe o r i g in a l message .No te that a successfu l dec i pherme n tmust no t only re -create the c o rrect sh o u l dbedec o ded as ka o r ga , etc .

T he m o st c o mm on s yll ab l e in o ur data i s to .T he base lin e o frep l ac in ge v er y c i pherte x tt o ke nwi th to yi e l ds a n accurac yo f4 %. tab l e P( c | e ) that ma xi m i ze P( c ):
P( c | e )i sa 65 x 4 6 tab l e .W ebeg in wi th u ni f o rm pr o bab ili t i es , s o that each o fthe 65 p l a in te x t characters maps t o a ny o fthe4 6 c i pherte x t n umbers wi th pr o bab ili t y1/ 4 6. 5.1 Language Models case ,J apa n ese ), a n dth i s i scaptured in the P( e )l a n guage m o de l. In o ur e x per -n &gt; 3. W esm oo th b yin terp ol at in g wi th low er -o rder n-grams .W eest i mate on e sm oo th in g  X  per n-gram o rder .

W ereprese n t LM sas w e i ghted fi ni te -state accept o rs (WFSA s ). Fo reach LM, w emeasure i ts mem o r y s i ze (n umber o f WFSA tra n s i t ion s ) a n d i ts perp l e x-i t yon he l d -o ut data .W e measure perp l e xi t yon the p l a in te x t messages , th o ugh o fc o urse , these messages are no tused in the c on struct ion o fa ny LM s . Fi gure 4 sh ow sthere l at ion sh i pbet w ee n mem o r y requ i reme n ts a n d LM e n tr o p y t i cu l ar mem o r y s i ze a mach in ema y ha v e ,w eca n se l ect the LM o rder that g iv es the best perp l e xi t y.
W euseE M t o search f o rthebesttab l e P( c | e ). E Mi sa ni terat iv ea l g o r i thm i terat ion cha n ge in P( c ) fa ll sbe low asetthresh ol d .W eterm in ate ear ly i f2 00 i terat ion s are reached .

Fi gure 5 sh ow sdec i pherme n tresu l ts w he nw eusea 3-gram LM tra in ed on v ar io us am o u n ts o fdata .W eca n see that m o re LM data (i. e ., m o re k nowl edge ab o ut the J apa n ese l a n guage )l eads t o better dec i pherme n t .Fi gure 5 a l s o c on-firms that ma xi m i z in gEquat ion 3: i s better tha n ma xi m i z in gEquat ion 2 :
Fi gure 6 sh ow sthec i pherte x t ,o r i g in a l message , a n d best dec i pherme n t o b -ta in ed fr o mus in gthe 3-gram LM. 5.2 Random Restarts me n tstr in gs a n d accurac i es .Fi gure 7 a n d8sh ow stheresu l t o ftheu ni f o rm start in gc on d i t ion a lon g wi th 2 9 ra n d o mrestartsus in gthe2 -gram a n d 3-gram LM s , respect iv e ly. Each p oin t in the scatter -p lo treprese n ts the resu l ts o fE M at the e n d o fa n E M ru n, a n dthe y-a xi sg iv es the accurac yo ftheresu l t in gde -c i pherme n t .W e o bser v eage n era l tre n d  X  X  he nw e lo cate a m o de lwi th a g oo d g oo d , because i tmea n sthatE Mi sma xi m i z in gs o meth in gthat i s o fe x tr in s i c P( c ), a n d i tretur n sa non se n se dec i pherme n t .In the o ther c l uster , E M fi n ds per i e n cemuchthesamephe no me non. W he non ehasme n ta lly c o mm i tted t o the hard t o escape dur in g subseque n ta n a ly s i s .

Fi gure 9 sh ow sthe l ear n t subst i tut ion tab l efr o mthebest 3-gram dec i pher -me n t wi th c o rrect e n tr i es marked wi th .

W he nw eusera n d o mrestartsf o rdec i pherme n t ,w eca nno ts i mp ly p i ck the w ep i ck the str in gthatresu l ts fr o mthem o de lwi th the best P( c ). A ssee nin Fi gure 8 , the best P( c ) d o es no tguara n tee the best accurac y. How e v er ,w eare restarts .B ecause o fth i s , the rest o fthee x per i me n ta l ru n s in th i s paper emp loy 30 ra n d o mrestarts . 5.3 Objective Function W e in tu i t iv e ly fee l that m o re k nowl edge o f J apa n ese will l ead t o better dec i-pherme n ts .T h i s i struef o r lon gc i phers li ke the on e w eha v esee n s o far .W hat ab o ut sh o rter c i phers ?How d o the d iff ere n t n-gram LM sperf o rm w he n used eg y. T ab l e 1 sh ow s best dec i pherme n terr o r rates ass o c i ated wi th d iff ere n t LM o rders ( tra in ed on the same data ) f o rthet wo c i phers (lon ger a n dsh o rter v er -s ion s ). W e o bser v ethat w eca n get better accurac i es wi th h i gher n-gram LM sf o r w he nw e in crease the n-gram o rder o fthe LM, w efi n dthatdec i pherme n tsur -ma ny m o re actua lJ apa n ese wo rds .W h il ethete x t i s non se n se , fr o m a certa in d i sta n ce i tappearsm o re J apa n ese -li ke tha n the 2 -gram dec i pherme n t .W he nw e take measureme n ts ,w efi n d that the 5-gram LM str on g ly prefers the (non se n se ) but no tt o such a degree .

T he c on seque n ce o fthe 5-gram m o de l be in gs ow r on g ly o p inion ated i sthat o rder t o acc o mm o date the LM X  sdes i re t o pr o duce certa in str in gs .T he l ear n t subst i tut ion tab l e i smuchm o re non-determ ini st i ctha n thetruesubst i tut ion tab l e ( ke y).
 W e remed y th i s . Reca ll fr o mEquat ion 1 that E M X  s o b j ect iv efu n ct ion i s :
H ere , the P( e ) fact o rcarr i es t oo much w e i ght , s oino rder t o reduce the  X  X o te  X  that the LM c on tr i butes t o E M X  s o b j ect iv efu n ct ion, w ecreatea n e wo b j ect iv e fu n ct ion:
No te that th i s i s substa n t i a lly d iff ere n tfr o m the pr o p o sa lo f [5] t o stretch o ut the subst i tut ion pr o bab ili t i es after dec i pherme n thasfi ni shed .In stead ,w e actua lly m o d i f y the o b j ect iv efu n ct ion E M uses dur in gdec i pherme n t i tse l f .
T ab l e2sh ow sthe i mpr ov eme n ts w egetfr o mthe n e wo b j ect iv efu n ct ion at v ar io us n-gram o rders f o rthet wo c i phers .T he resu l ts are much m o re in acc o rd wi th w hat w ebe li e v esh o u l dhappe n, a n dgetsust ow here better n-gram m o de l s o f [5].

To sum up o ur e x per i me n ts on the U esug i c i pher ,Fi gure 10 sh ow sa ni ce c o rre l at ion bet w ee nLM e n tr o p y a n de n d -t o-e n ddec i pherme n t accurac y. resu l ts .Fi gures 11-13 g iv etheresu l ts .

T ab l e 3 sh ow s i mpr ov eme n ts ach i e v ed on a 9 8 -l etter E n g li sh c i pher w he n us in gthe n e wo b j ect iv efu n ct ion in tr o duced in S ect ion 5.3. Fo r the graphs in Fi gures 1 2a n d 13, w ea l s oi mpr ov e LM sm oo th in ga n dthereb yo bta in further i mpr ov eme n ts in accurac y( d own t o a n err o r -rate o f 0.0 2f o rthe7 -gram m o de l tra in ed on 7m illion l etters ). W eha v estud i ed a part i cu l ar J apa n ese s yll ab l ec i pher .In d oin gs o, w eha v e made se v era lnov e li mpr ov eme n ts ov er pre vio us pr o bab ili st i cmeth o ds , a n d w e rep o rt i mpr ov ed resu l ts .F urther i mpr ov eme n ts in LM sma yl ead t o accurate dec i pherme n t o fsh o rter te x ts , a n dfurthera l g o r i thms ma yl ead t o accurate de -o ther l a n guages .
 u n der S R IIn ter n at ion a l X  spr i me C on tract N umber NB C HD0 4 005 8 . A dialogue system is an efficient interface for human-computer interaction that helps users achieve their goals using natural language. Recent research on dialogue systems has focused on spoken dialogue systems that combine a text-based dialogue system with automatic speech recognition (ASR) system. ASR is classified as either isolated speech recognition or continuous speech rec ognition, based on the recognition unit. A dialogue system requires spontaneous speech recognition technology, which is a type of continuous speech recognition. However spontaneous speech recognition systems have not exceeded an accuracy of 70% in a laboratory environment [1]. One reason feedback because of being independently studied. However, more recently, a design that integrated the two systems was tried, but was limited to a small working area [2]. 
We propose a postprocessing method that uses the domain knowledge after ASR process to overcome the performance problems of ASR system in a spoken dialogue system. The language processing analyzes the speech recognition result, or sentence, as a speech-act, action, named entity, etc. The speech-act is used as the most impor-tant information for recognizing a user X  X  intention, and for Dialogue management. In the proposed method, we use information from the legacy database to infer the current speech-act. The validity of the user X  X  analyzed intention can then be verified using the inferred speech act. The proposed system has to generate system responses dynami-cally for erroneous ASR results. 2.1 Effects of Speech Reco gnition Errors on Speech-Act speech recognition errors. Specifically, an erroneous speech-act may result in misun-derstanding the user X  X  goal. Speech-acts are the most important factor when determin-ing a task goal. Thus, speech-act analysis should work robustly throughout the entire dialogue system. 
The following examples by Cavazza [3] show how to process a dialogue between a system and user, where speech-acts are mis-recognized. Fig. 1 shows the system gen-erating an incorrect response, because the system recognizes the user X  X  speech-act  X  X eject X  as  X  X ccept. X  2.2 Researches for Correction of Sp eech Recognition Erro rs in a Dialogue Studies on methods for correcting speech recognition errors have mostly focused on the language model or acoustic model in ASR system. Recently, researchers have been making efforts to use dialogue knowledge in these studies. Gorrell [4] divided the system X  X  answers into twelve categories according to the patterns of speech rec-ognition errors, and then chose the most correct answer using Latent Semantic Analy-sis (LSA). 
This paper proposes dialogue strategies that are adoptable in the field of form-filling dialogues . Form-filling dialogues use a form to allow the system to share knowledge with a user during a dialogue. The user X  X  goal is achieved through insert-ing, deleting, modifying, or searching the form. Most task-oriented dialogues have a bias toward form-filling [5]. Litman and Allen [6] introduced a plan theory into a dialogue model. A plan-based dialogue model uses discourse and domain knowledge, recognized by a plan tree that links the user X  X  intentions with the recipes. The system can process a mixed initiative dialogue using an obligation stack, which arranges the system X  X  responses and belief space [7]. Oh [8] applied a plan-based model in Korean. However the performance of those models was not guaranteed in the spoken dialogue system, because they assumed that there were no errors in the input data from ASR and Natural Language Understanding (NLU) systems. Therefore, it is necessary to devise methods to overcome the errors from ASR and NLU systems in the application of a spoken dialogue system. A traditional spoken dialogue system consists of ASR system, NLU system, and dia-logue manager. ASR system transforms a user X  X  speech into text. The NLU system mine the user X  X  intention, and then delivers them to the dialogue manager [9]. The performance of the NLU system cannot be trusted when ASR results are incorrect. Therefore, a scheme is needed to determine whether or not the results of ASR and NLU systems are correct, as well as a system that can manage the dialogue dynami-system. The error detection module verifies the NLU output. When the output of this tion sub-dialogue strategy is tried if the module is invalid. 
The next section will explain this negotiation sub-dialogue strategy in detail. each command corresponds with an action and a speech-act. When the system fails to analyze the user X  X  command, it tries to infer the user X  X  command based on the above four commands. For inference, the system manages a sub-dialogue for negotiation using a user X  X  database and knowledge. Confirmation strategies are divided into implicit confirmation and explicit confirmation. Explicit confirmation is a method where the system directly asks whether the results of ASR are correct or not, whereas implicit confirmation is a method where the system confirms the accuracy by watch-ing the user X  X  reaction to the system X  X  utterance. 
We propose strategies for generating a negotiation sub-dialogue for speech recogni-deals with an unknown user action and unknown speech-act for a given utterance. The second applies to a pattern where the speech-act is determined to be a  X  wh-question,  X  but the target is unknown. Each strategy has its own inference and confirmation-generation rules. These will be explained in the next sub-section. 4.1 Pattern with Unknown Speech-Act and Unknown Action The system may not be able to determine the speech-act of the user X  X  current utter-ance when ASR fails to rec ognize the predicate related to both the speech-act and ther, because it does not know the user X  X  inte ntion. However, in the proposed method, base and some recognized words. 
We use the domain object to represent the state of a task. For example, as illustrated in Fig. 3, the recognized words are  X  today, X   X  one hour  X  and  X  Hong Gildong X  each of which is used to fill a corresponding property of the domain object. The domain ob-database search, the system continues the dialogue using the three proposed strategies. 
Strategy #1 guesses at the insertion of data for the user X  X  intention, because the sys-tem found nothing in its search using the given properties, which are composed of recognized words. In strategy #2, the system infers the modification or deletion of data, except for that inserted, to determine the user X  X  intention, because only one tuple is searched. The system uses explicit or implicit confirmation to determine the correct intention from among all the candidates. In strategy #3, because more than one tuple is searched, the system should ask the user for additional knowledge, and should add the given knowledge into the previous properties. With the updated properties, the system searches the database again and continues the dialogue using strategies #1, #2, and #3 recursively. Fig. 4 shows the processes of the three strategies, where both the speech-act and action are unknown. The next paragraph will explain details of strat-egy #2 using an example. 
Fig. 5 shows the candidate list of user utterances for the recognized words. The system guesses at the user X  X  utterance from among seven utterances based on the recognized properties. To respond to every possible intention of the user, the system uses explicit and implicit confirmation. If only one tuple was discovered after search-involve a modification or deletion of the tuple. To deal with this situation, the system seeks an answer to user system utterance #2 in Fig. 6, which is confirmed explicitly. correct. In other cases, the user X  X  utterance may be a question concerned with proper-ties (utterance #3 ~ #6). In addition, utterance #7 is a case where the user X  X  knowledge conflicts with the system X  X  knowledge. To solve this conflict, the system generates system utterance #1, which is a kind of implicit confirmation as shown in Fig. 6. Ut-terance #1 implicitly informs the user that they have the wrong information. Conse-quently, the system can satisfy every candidate intention. 4.2 Pattern with  X  X h-question X  and Unknown Target A  X  X h-question X  is a kind of speech-act. However, it is different from other speech-acts in that it requires an additional property: the target of the question. The target is what the user wants to know through the  X  X h-question X  utterance. If the system can-information. 
Let us explain how to process this pattern where the speech-act is a  X  X h-question X  cases. with recognized properties. Although the system does not recognize the word for the target , the system can give an answer that says that  X  X here is no information. X  There-fore, this is implicit confirmation, because the system implicitly informs the user that he/she has the wrong information. In strategy #5, the system infers that the user wants to know a specific property. Although the system searches the tuple containing what the user wants to know, the system cannot gi ve the user a correct answer, because the enough properties to include the correct an swer, using recognized words. Strategy #6 is similar to strategy #3, as the system acquires additional properties though more questions. The system updates the domain object using additional properties, then searches the user X  X  database again. Based on the results of the DB search, the system continues the dialogue recursively using the strategies. The proposed dialogue strategies were adopted in the field of form-filling dialogues. We experimented with strategy #2 using a plan-based dialogue system dealing with form-filling dialogue. 
For the purpose of our experiments, we ex panded the plan-based dialogue system used by Oh [12]. Our system constructed a discourse structure using recipes . So, additional recipes were built for the proposed dialogue strategies.  X  case2_confirm X  is the upper recipe of  X  X urface_unknown_case2, X  and initiates a negotiation sub-and the user X  X  intention is determined based on the user X  X  response. Fig. 7 shows how to expand the proposed method into a general plan-based model. Our proposed method has high portability, with only a minimum effort needed to modify the confirmation representation. 5.1 Results We used the ASR system of Ahn and Jung [10] for the spoken dialogue recognition. In the experiments, we used a confidence level of 0.8 for the ASR results. Therefore, when the confidence value was higher than 0.8, the system regarded the recognition recognition had failed, and the system entered the negotiation sub-dialogue. In our experiments, 200 dialogues were used. We measured the task completion rate when the confidence value of a word was lower than 0.8. 
As shown in Table 1, the performance of the base system not using the proposed strategies was 85%. However, the performance of our system using the proposed strategies was 89%, 4% higher than the base system. Consequently, our system cor-rected 27% of the overall errors. We proposed dialogue strategies to overcome speech recognition errors. When the system cannot determine the speech-act of an utterance due to speech recognition errors, it confirms the user X  X  intention by gathering additional knowledge with nego-tiation sub-dialogue strategies. Users can achieve their goal through this sub-dialogue even if speech recognition errors occur. 
The proposed system corrected 27% of the incomplete tasks. When a system fails to understand a user X  X  command, most dialogue systems retry from the beginning of the dialogue. This is inefficient, and the performance of a dialogue system may be undervalued. However, our system attempts to use a negotiation sub-dialogue to The task completion rate of the proposed system was improved by 4%, but the user-sensed improvement would be higher than 4% because rather than start over from the beginning of the dialogue, the system continues with an efficient sub-dialogue. 
In the future, we will try to devise a robust model that covers various dialogue phenomena and study aspects of cognitive science besides linguistic approaches. We will also continue to work on resolving error propagation problems from ASR system. Terms are the most fundamental units used to represent and encap sulate a concept in any specific domain [1]. Automatic domain term extraction (ADTE) has important automatic domain ontology construction, and is widely used in information retrieval, information extraction, data mining, machine translation and other information proc-essing fields. Algorithms for automatic term extraction compute at least two indexes. One called domain index, and the other is unit index [2]. Kage introduced Unithood and Termhood[3]. Unithood refers to the degree of strength or stability of syntagmatic combination or collocation. Termhood refers to the degree that a linguistic unit related to domain-specific concepts [4]. 
Generally, there are two kinds of measures for estimating the unithood. One is the internal measure [5,6,7], the other kind of measure is the contextual measure [8,9]. In candidate. The most commonly used measurement for termhood is the TF/IDF, it cal-culates the termhood by combining word frequency with a document and word occur-rence within a set of documents. The research on terms extraction is mainly based on corpus, ADTE is further classi-first step extracts term candidates which have high unithood and the second step veri-fies them as terminology measured by their termhood. Nakagawa limited the term candidates in compound nouns and single-nouns, and his works only focused on the relationship between single-nouns and compound nouns, his method cannot deal with non-compound terms [10]. Eide Frank focused on domain-specific key phrase extrac-tion, he considered only two attributes for discriminating between key phrases and first appearance in the whole document [11]. Frank mainly focused on key phrases of a document, the second feature may not help much in extracting terms. Chan pro-posed a statistical model for finding domain specific words (DSW). He defined Inter-Domain Entropy (IDE) as acquiring normalized relative frequencies of occurrence of terms in various domains. Terms whose IDE are above a threshold are unlikely to be associated with any certain domain. Then the top-k% candidates can be determined as the domain specific words of the domain. 
At present, in Chinese ADTE, Computational Linguistics Institute of Peking Uni-versity in conjunction with the Chinese standards Research Center did a project, they developed an information technology and technology building standards, extracted related term candidates from the corpus, and then they filtered it using context infor-mation and chapter structure. Jing-Shin Chang used bootstrapping method to extract domain vocabulary automatically [12]. Sung-chen Lin extracted domain terms auto-matically using topic words [13], he thought that terms always appear at the same time with topic words, and these terms with high frequency are domain terms. The most important factors for CRF model are that, suitable features can be chosen natural language phenomenon. In this paper, in order to make the method be domain independent, we use as less domain knowledge as possible. We adopted several common features, which are the words themselves, the POS of the word, semantic information, left information entropy, right information entropy, mutual information and TF/IDF. Compared with our previous works, we have added the semantic infor-mation as a new feature in this paper. 
Among the seven features, the words themselves provide some domain knowledge, general terms are single-nouns and compound nouns, and also include some verbs. Semantic information is tagged based on HowNet. Left and right entropies are impor-tant measures for judging if a word is the borderline of a term, so it can be said to be a term X  X  unithood. Left and right entropies can measure the uncertainty of a word X  X  col-lection with its adjacent words. For example, if the left word of a word w varies, we can treat w as the left border of a term. The formula below can be used to estimate the left information entropy of a word. which appears to the left of the word w  X  w la is the string composed of the word w and w is a . In the same way, we can get the right information entropy formula  X 
In CRF model, the token is the largest sequential component. It is composed of different meaning according to different tasks. In this paper a token refers to a word, so we can X  X  calculate the term candidates X  mutual information and TF/IDF. We adopted the words X  information entropy, mutual information and TF/IDF. The for-mula of mutual information of words w 1 and w 2 is as show below  X  In formula (2)  X  p ( w 1 , w 2 ) is a coherent-probability of words w 1 and w 2 . We supposed that the TF/IDF of the words composing terms helps to estimate the TF/IDF of terms, so we treat the words X  TF/IDF as a feature of CRF model. The TF/IDF formula is: 
Doc_count( w ) is the number of the file in which a word w appears, and count( w )is range of 0 and 1. The larger the TF/IDF is, the more domain knowledge w carries  X 
As far as we know, CRF model treats the features X  value as a tag. But the value of left and right information entropy, mutual information and TF/IDF are float numbers, so they can X  X  be used directly as features. We used K-Means clustering to classify these values. In this paper the features ha ve been divided into 10 grades according to the best results can not be determined in this paper because experiments were only done for 10 grades. In this paper, the tool for POS tagging was designed by HIT [14], and the tag sets can be found in  X  X IT Chinese tree tagging sets X . We use the symbols B, I, and O to label other words. 4.1 Experimental Sets The Experiment was divided into three groups. We used POS, left and right informa-tion entropy and TF/IDF as features in group1, added mutual information into the model in group 2, and added semantic information in group 3. Different templates were used in group 1, group 2 and group 3. The scale of the corpus was measured by the number of words. 4.2 Evaluation Index Generally speaking, there are four indexes to evaluate in domain term extraction. The four indexes are precision, recall, F-measure, and coverage, (1) Precision for term extraction (2) Recall for term extraction (3) Coverage for term extraction (4) F-term measure 
To compute the term coverage we need a domain terminology. But now there is only one IT terminology which is supplied by the Institute of Computational Linguis-tics, Peking University and we don X  X  have a military terminology, so we can X  X  calcu-late the coverage in our experiment. We just used precision, recall, and F-measure, to evaluate the results. 4.3 Experiments Results and Analysis Our training data came from the military news of Sohu network. And we did experi-magazine  X  X odern Military X  2007, volumes 1 to 8. Due to the large number of terms, only calculated the relative precision and recall rate of terms extracted only from Vol.3 of  X  X odern Military X  2007 magazine. Figure 1 shows the results using different features. In group 1, we used five features, i.e. word itself, POS, left and right infor-mation entropy and TF/IDF; in group 2, we added mutual information into the model; in group 3, we also added the semantic information as feature. 
In figure 1, the vertical unit represents 10,000 words. It can be seen in figure 1, that the F-measure improved with a certain degree after we added the mutual information as feature in training the CRF model. But it was not significantly improved when the rate achieves 73.24 %, recall rate is 69.57%, and F-measure is 71.36%. In group 3, we also added the semantic information as feature, the F-measure is improved with the training data scale. When the training data achieved 60,000 words, the best results in F-measure is 76.46%. 
To further verify the correctness of our method, we made detailed analysis of the terms we extracted. Table 1 illustrates some terms that appeared in corpus with high frequency, while table 2 illustrates some terms that only appeared once in the corpus. In table 1, it can be seen that most of terms with high frequency are single words. When we added POS and words features in training CRF template, the features re-that appeared with lower frequency. 
There are no standard test data and evaluation for term extraction in Chinese, so we can not compare our results to other experiments. But the following are some of other evaluations done by others like, JianZhou Liu from Huazhong Normal University automatic extracted terms from open corpus, and the precision of the first 10,000 terms was 75.4% [15]. In the literature [16], Wenliang Chen extracted domain terms different results in different domains. The highest precision rate he achieved was only relied on domain corpus the highest precision would be 43.40%. Tingting He used Proton string decomposition method to extract terms. The precision of terms which were composed by more than three words was 72.54% [17]. This paper presented a new method to extract domain terms. According to the ex-perimental results, our method based on CRF model performed well. We got good results using only six features. If we had added other features into the model, the re-sults may have been even better. For example, we could get the best grade number and best templates for the model through experiments. Of course, what is more impor-tant is that we can add some domain features such as keywords feature, dictionary feature etc. There are several methods to make our proposal achieve optimum per-formance. In future works, we will experiment in various domains, and improve on our proposal. Acknowledgement. This work is supported by the national natural science foundation of China (No. 60736044) and the National High-Tech Development 863 Program of China (No. 2006AA010108).
 named entity recognition of the product names known as the task of product NER. Whereas some realistic business IE applications call for more semantic information product name actually stands for in a knowledge base of products. For instance, if we want to search articles about certain product, we need to identify all the product names in the article text representing this product. There is still a gap between prod-uct NER and the further requirements of semantic information extraction. Thus, we would like to introduce a new task called Named Entity Semantic Identification to solve the problem. Compared with the traditional NER, the task of Named Entity Semantic Identification is to find out the exact entity which the text that recognized as named entity really stands for. Figure 1 is an example which demonstrates the task of named entity semantic identification: the term  X  X oyBook S52 X  was recog-nized as a product named entity and finally identified as the No.86091 Entity in the knowledge base. 
This paper describes the definition of product named entity semantic identification and an approach for the product named entity semantic identification, in which a class-based language model and a query-based identification mechanism were em-phase, we use a query-based identification mechanism to find out which entities in the knowledge-base match the product named entities in the given text. Experimental results show that this system performs well in the domain of IT products. 2.1 Consideration PKU NEI (Peking University Named Entity Iden tification) is an approach that solved the Product Named Entity Semantic Identification task using an automated generated knowledge-base. And a series of experiments shows its high reliability and high accuracy. 
The pipeline of task of Product Named Entity Identification could be intuitively di-vided into 2 phases: Named Entity Recognition Phase and the Semantic Identification Phase. Generally, at the Named Entity Recognition Phase, we first do a product NER Identification Phase, based on the result of the NER Phase, we identify corresponding entities to the entities detected by the NER phase. Different from the concerns of traditional NER, in the Named Entity Recognition Phase of Product Named Entity Semantic Identification, we do not care much about the exact words in the named phase. Instead, the recall-measure of the NER means much more, and it is the bottle-neck of the whole system X  X  recall performance, because in the process describe above, the recall could not be promoted in the second Semantic Identification Phase. 
The Semantic Identification Phase has different concerns from the first phase. It requires a knowledge base. The key problem is how to deal the ambiguity problem and how to identify the entity accurately in a short time. 2.2 Named Entity Recognition Phase 2.2.1 Method PKU NEI adopts an algorithm to do the NER task based on a role-model [9] X  X  variant. As we discussed in Section 2.1, the task of this phase is different from general NER tasks, so we designed a new process to deal with the problem with new observation set, new role set and new procedure. 
Similar as the H. Zhang et al. [9] X  X  algorithm, our algorithm uses the result of the segmentation and POS-tagging task as features. Moreover, as we can use knowledge base, we classify the tokens with more categories besides their POS tag: potential company/organization/brand name and potential model names. 
We tag the token as potential company/organization/brand name (CN) by detecting if the token matches the company/organization/brand name in the knowledge base. We tag tokens as potential model names (MN) when they composed only in non-Chinese characters or digits. 
We tag tokens as potential limitation attributes (LA) when they in a manually maintained list contains product catalog names such as  X   X  X  X  X  X  X  X   X ,  X   X  X  X   X ,  X   X  X  X   X   X  or some attributes product usually have, such as  X   X  X  X   X   X   X  X  X   X . It depends on the domains. These words usually play a different role from those words that have same POS-tag. And we tag the tokens by usual POS-tagging, if these tokens could not be tagged as CN, MN or LA. We adopt the POS tagging system called PKU-POS set which is defined by Peking University. Tokens are tagged like  X  X r X   X  X  X   X  X  X ... And we define a set OB as: Besides, taking account of the task X  X  feature, we defined a set of roles for product NER phase as following. So we have set ROLES defined as: 
To simplify the problem, we consider the problem as a HMM task. Given a sequence of text W 1 W 2 ...W n , we could generate a observation sequence of HMM by tagging them in set OB. After that we got the observation sequence of HMM : o o 2 ...o n . And the state set is just the same as the role set ROLES. After we trained the parameters using maximum-likehood method, we have a HMM built. After we calculate the most likely sequence of hidden states, we can using the templates demonstrated in Table 2 for matching the sequence, and find out the potential NEs. 2.3 Semantic Identification Phase 2.3.1 Overview The PKU NEI uses a query based method to identify the entity names to the entry in knowledge base. 
The knowledge base contains the following information for entities: entity X  X  stan-dard name and its variants, entity X  X  catalog, entity X  X  manufactory, the segmentation the entity X  X  name into tokens and a weight of each part. The performance of this phase is highly depended on the quality of the knowledge base. In the section 3.3.2, we would emphases the method to build the knowledge base automatically. The method of determining named entities X  entries (like finding out Enity_ID) in knowledge base could be summarized as 2 parts: query and score. Task of query calls for finding out all the entities contains at least one token in the knowledge base. Task of score is that mined by the matching-degree. The most matching one is the result entity. 
Besides taking account of the matched tokens X  weight in knowledge base, context information about the catalog and manufactory is also important. In section 3.3.3, we will describe our method. 2.3.2 Construction of Knowledge Base in IT Hardware Domain Based on the structured information in some hardware related website, we designed a method to construct a knowledge base with 82839 entities in the area of IT hardware without much participating of human. To design the method to construct the knowl-edge base, we would take several concerns into consideration. First, the method should be automatic. Even with in a specific domain, there are still thousands kinds of product could be found. Starting to construct a knowledge base from scratch manually is not possible. Fortunately, there are usually data source we could use for helping us to construct a knowledge base. Second, we need high quality of estimating the weight-value of each token of entity names, because the semantic identification uses the weight-values to identify the entities. 
There are a lot of hardware websites on the internet which provide the news about the hardware products and benchmarks between products. The hardware website usually maintains a structured information bases to help the visitors to find the related information about a specific product (Fig.2 is typical sample). In the webpage, we can retrieve the information about the entities X  names, aliases, manufactories and the re-website like Fig.2 and we exploit this information to construct the knowledge base. 
After tokenizing each product X  X  name and its aliases, the remaining problem is to determine the weight of each token. Let t is a token appeared in product e X  X  name, V(t | e) is the weight of t in entity e. product entity e. document frequency of entity, so we let N is the total number of articles, and we use the C(e) / N to estimate P(e), where C(e) is the count of e X  X  related articles(Fig.2 (2)). We storage each token t in each entity e X  X  weight V(t | e) in the knowledge base. Ta-ble.3 is an example of the generated knowledge base X  X  entry. Standard Name  X  X  X  Compaq Presario V2000 Aliases: hp compaq presario v2000 Manufactory  X  X  X  hp 
Tokens &amp; Weights: (  X  X  X  , 0.057) (compaq, 0.161) (presario, 0.266) (v2000, 2.3.3 Query and Score With the knowledge base like Table.3, we could identify the potential entities in NER phase to the knowledge base by querying like what search engine usually does. (1) provides a way to evaluate the impor tance of a token for certain entity. The matching-degree in PKU NEI is the sum of V(t | e) for all matching token t. In addi-tion, we considered the context information in this phase. If an entity X  X  manufactory is appeared near the recognized entity name (by NER Phase), there would be a good chance that the entity is the product of this manufactory. So we would add a factor to the matching-degree. With the consideration of all these factors, we suppose the entity in the knowledge base with the highest matching-degree is the result we want. 
For identifying the entity effectively, we use the tokens in knowledge base X  X  en-tries to create an inverted index table. And for the potential entity names W i W i+1 ...W j using the inverted table, and we get the a series sets of entities R k , R k+1 ...R l , where R i is the set of entity which contains the token W i , Let R = U the set R, calculate the  X 1.0/(d + 4) X  is the factor of context manufactory information. It would be a en-hancement when the manufactory informatio n is located near the recognized named entity. 
And e with M e = max(M e X  ), is the entity we want. We have tested our approach in hardware product domains. For testing the algorithm of NER phase, we selected 1000 paragraphs containing IT hardware product entities in many catalog from http://www.intozgc.com/, and we tag these paragraphs manu-ally. We selected randomly for training set in size of 100 and 500 paragraphs. And the rest of paragraphs are used for testing. Compared with [4], our method has a higher F-Measure. Different corpus and test data might be an important cause to consider. Our usage of knowledge might be another cause, like the work of tagging CN and LA in NER phase provide a more precise observation description. Another reason may be the easier standard of judging the result. If NER result could provide enough informa-tion for the process of semantic identification phase, we suppose that it is OK. 100-size 0.84 0.79 0.819 500-size 0.93 0.85 0.890 think efficient training data is crucial. 
For determining the knowledge base using by the second phase, we spidered the website of http://product.zol.com.cn/ and calculate the weight for the entities. Finally we got an entity knowledge base containing 82839 entities covering many catalogs. Table 5 is illustrating the overall performance in our experiments. 
Because the Semantic Identification Phase accepts the NER phase X  X  potential NE un-conditionally, so the Semantic Identification Phase got a recall 1.00. The performance of precision of this phase got a good result when the input potential NE is correct. In this research, our main contributions are: 1. We proposed the concept of Named Entity Semantic Identification for helping 2. In NER Phase of PKU NEI, we proposed a refinement of a role-model-variant 3. In Semantic Identification Phase of PKU NEI, we provide a systematic way 
The weakness of role-model was responsible for the failure of some cases. The per-search should put more attention on the refinement of the NER phases and try to drive this approach for practical system. This research is sponsored by the National Grand Fundamental Research 973 Program of China under Grant No.2007CB310900, National Innovation Program for Under-graduates and the Supertool Inc. Usually the main idea of the documents is expressed by one or more words, named keyphrases. With keyphrases, the readers can rapidly catch the important content of the documents. Moreover, keyphrases can be used as the index, through which users can find the documents they want. For example, when a search engine searches the documents with keyphrases as indexes, it can give the results more precisely. However, most of the keyphrases are indexed manually, which cost a huge amount of labor and time. Thus, automatically indexing keyphrases is a significant work. 
Keyphrase extraction and keyphrase assignment are two general approaches of automatically keyphrase indexing. Keyphrase assignment, in despite of its better performance, is difficult to accomplish. As most of keyphrases appear in the documents and these keyphrases capture the main topic, the keyphrases extraction techniques are commonly used. Our research focuses on keyphrase extraction from Chinese news in People Daily. Here we treat the keyphrase extraction proce ss as a supervised learning task. First, we negative examples. The positive phrases mean that they can serve as the keyphrases, while the negative ones are those which cannot serve keyphrases. At the same times, we extract some shallow features from the samples and used learning method to build the model. When we run the model on the test documents, the examples marked positive are considered as keyphrases. 
The remainder of this paper is organized as follows. Section 2 discusses related work. Section 3 describes our system architecture. Various shallow features are introduced in Section 4. Section 5 illustrates the experiment results and Section 6 presents our conclusion and future work. Turney (1999) implemented a system for keyphrase extraction called GenEx, with parameterized heuristic rules and a genetic algorithm [1]. He used some frequency features and some position features [2]. Frank et al. (1999) implemented another system named Kea with the Bayesian approach [3]. He used only two features: TF X IDF and first occurrence. A lot of experiments have been done which proved the performances of Kea and GenEx are statistically equivalent. These two systems are simple and robust. However, limited by their performance, it X  X  still unpractical to use these original systems to replace manual work. Based on these two works above, many new systems have been brought forward. Mo Chen (2005) presented a practical system of automatic keyphrase extraction for Web pages [4]. They used the structure information in a Web page and the query log data associated with a Web page collected by a search engine server. They added some features about structure extracted from html documents. Since they treated a Web page as a special text and promoted the performance in the area of Web page processing, it X  X  not a general model of text processing. 
Olena Medelyan (2006) improved the Kea system by using the thesaurus and adding semantic information on terms and phrases [5]. The new system, named Kea++, can sometimes double the performance of Kea. Basing on Kea, Kea++ adds However, the performance of this system highly depends on the quality of the thesaurus, which is not public available. 
In this paper, we try to survey more shallow features to describe the characteristic of keyphrases more precisely and more comprehensively. We hope the new system is a general model of text processing and don X  X  rely on the thesaurus. We experiment on several popular learning models, such as decision tree, Na X ve Bayes, SVM and CRF, to compare the performance of these systems. documents indexed with keyphrases. All the word sequences with length less than 6 are extracted from the documents and serve as the input of next modules. Because many of the word sequences are meaningless and inappropriate for being keyphrases, will be introduced below. The indexed keyphrase can be seen as positive samples, and the other candidate keyphrases are labeled as negative samples. Each candidate keyphrase is represented by a feature vector x i . Then whether a candidate is a being a keyphrase. Then, all the candidate phrases composed of the training set {&lt; x i , y comes, we also use the filter rules to get all the candidate keyphrases, and generate a corresponding result whether a candidate phrase belongs to keyphrases. This process is illustrated in Figure 1. 
The acquisition of candidate keyphrases is the foundation of our work. We hope to obtain all the phrases as the candidate keyphrases which are included in the collection of word sequence. We assume that a word sequence with higher frequency and specific syntactic structures is preferred to be a phrase. 
Filter Rules 1: S = w 1 w 2 ...w n is a Chinese word sequences. If freq (S) &lt; 2, then S { x | x is a keyphrase}.

We filter the word sequences which occur less than two times and get the set of keyphrase candidates because most of keyphrases appear in article more than once. Filter Rules 2: S = w 1 w 2 ...w n is a Chinese word sequences. PS = p 1 p 2 ...p n is the POS sequences of S. If PS { x | x is a POS sequence of training data}, then S { x | x is a keyphrase}.

With the training data, we got most of the meaning combination of POS. Using these POS sequences, we can filter the meaningless word sequences. 
When obtaining the shallow statistical features of the phrase set, we select several ones. The details will be discussed in flowing section. Then we choose several supervised leaning method to build the model. After scoring the candidate phrases, the phrases with high score are tagged as keyphrases. We use cross-validation to evaluate the performance of our system. Documents Candidate Documents Through feature designing, we use several features which were pr oposed in the former papers and add some new features. Some of these features are statistic trend of importance of these features .In this paper, we discuss the following features. 4.1 Frequency Features These features reflect some frequency aspects of candidate keyphrases. We use Nagao string frequency statistics algorithm to calculate these features. TF X IDF 
In this equation, freq ( P , D ) is the number of times P occurs in D . size ( D ) is the number of words in D . df ( P ) is the number of documents containing P in the global corpus. N is the size of the global corpus. This feature was used by Nevill.et.al in Kea frequency of that phrase in general use. We use term frequency to represent the use of phrases in particular document and document frequency to represent the general usage. Because some leaning algorithms are linear, we add TF and IDF as other 2 features independently. ISF 
ISF stands for Inverse Sentence Frequency. In this equation, sf( P ) is the number of IDF, a phrase X  X  sentence frequency indicates how common it is. A phrases which is less appears often contain especial and importa nt meaning. We present this features to estimate the importance of sentence difference in keyphrase extraction. 4.2 Position Feature First Occurrence 
In the equation, numbefore ( P , D ) is the number of word that precede the phrase X  X  the position of the phrase X  X  first appearance. It was also used by Nevill.et al. in Kea [6]. This feature represents how much of the document precedes first appearance of the phrase. 4.3 Features Combining the Frequency and the Position Number in Title Title is the most concise abstract of a document. This feature was used by Mo Chen in Kex [4]. Rate in First Paragraph paragraph of D . freq( P , D ) is the number of times P occurs in D . Authors often present their topic in the earliest paragraph. Thus, phrases which appear in the first paragraph may be the keyphrases. We define this feature to estimate the importance of the phrases in this special paragraph. Rate in First Sentence sentences of every paragraph in D . First sentence always include the critical point of the whole paragraph. We propose this feature to estimate the importance of topic sentence and the phrases in this particular sentence. 4.4 Phrases Length This feature was used by Olena Medelyan in Kea++ [5]. Length indicates how much This feature is independent with the context of article, so we classify it specially. 4.5 Documents Features Word Number This is the number of words in document. Keyphrase Number This is an approximate number of keyphrases . We can estimate this feature using the feature Word Number discussed before. Keyphrase Rate This feature is calculated as Keyphrase Number divided by Word Number . It indicates how we estimate the number of keyphrases. Higher keyphrase rate maybe also show higher possibility of positive examples. 5.1 Sample Filter In our document collection, there are 392 documents with 2,118 keyphrases. 780 keyphrases appear somewhere in the body of the corresponding documents. The first step is extracting phrases from documents. We extracted all of the word strings whose positive examples. Secondly, we calculate the term frequency and filter out the word lots of useless phrases both in the training data and the test data. Then we got 17807 negative examples and 683 positive examples. The ratio of positive examples ascends POS Rules to filter out the negative examples. The result, however, is not very ideal. While we filter a number of negative examples, we also filter nearly half of the positive examples at the same time. Due to the lack of keyphrases (not more than 2118), we add some manual work on POS filter. 5.2 Feature Evaluation We evaluate the features to choose the important ones. Using information gain attribute evaluation (in Weka 3.5 1 attribute selection module) and ranker search method, we got the importance order of the features. Table 1 shows the importance of each feature. 
Using TF X IDF and First Occurrence, Kea learns with Na X ve Bayes method. Kea++ adds Length and another feature which is related to the thesaurus. Because the lack of Table 2, the data shows the performance of these systems in original document collection (in English) [5]. 
In the third row and the fourth row of Table 2, the data shows the experiments on our document collection (in Chinese). From the result of Kea (in the first row of Table 2 and the third row of Table 2), we can draw a conclusion that the performance of extraction techniques is less dependent on languages. According to the rank of features in Table 1, we establish a system which is temporarily called  X  X ur system X . In our system, we chose some appropriate features (TF  X IDF, First Occurrence, TF, ISF, IDF, Number in Title, Length and Keyphrase Number) and learn the training data with Na X ve Bayes method. We could easily find out that appropriate shallow features bring the improvement of the performance. The performance of our system is better than Kea X  X  and our system maybe enhance the performance of Kea more than Kea++. 5.3 Leaning Model To solve this learning problem, we use C4.5, Na X ve Bayes, SVM, CRF algorithm to Number in Title, Length and Keyphrase Number as the features. The results are showed in Table 3. C4.5 is a kind of decision tree. We use J48 (in Weka 3.5) as an accomplishment of C4.5. This model can be generated rapidly and get the capability of processing continual data. But this model will excessively depend on one main feature and ignore the others. 
In the experiment of SVM, we use SMO algorithm (in Weka 3.5). SVM is a great optimization model. A main disadvantage of SVMs is the complexity of the decision rule, which requires the evaluation of the kernel function for each support vector. This linear model does not fit the task very well. 
CRF is the most popular algorithm these years. It X  X  powerful in sequence labeling tasks, such as Named Entity Reorganization, Information Extraction and Text Chunking. CRF model could overcome the data sparseness and solve the  X  X abel Bias X  problem. Unfortunately, this model could not suit the tasks which are not sequential. 
Na X ve Bayes is a simple model, but it gets a high quality in classifying problem, especially when the features are independent with each other. From the result above, we could draw the conclusion that Na X ve Bayes method can suit to the task of keyphrase extraction. This paper discusses the feature selection and learning methods through a lot of ex-periments of keyphrase extraction. With th e appropriate features and learning algo-rithm, we can improve the performance of keyphrase extraction system, even better than thesaurus based system. However, there X  X  a long way to reach the ideal result. In the future work, we will consider adding some semantic knowledge as heuristics, such as thesaurus and topic word, to improve the system in some specific area. This work is supported by NSFC programs (No: 60603093 and 60875042), and 973 National Basic Research Program of China (2004CB318102). Recently some Korean global companies pr omote the localization policy in Chinese market. To break the language barrier we developed a Korean-Chinese machine trans-lation system focusing on the technical domain. We adopt pattern-based approach, which simplifies traditional three-stage rule-based MT system to 2 stages -analysis and verb pattern matching part. Verb pattern plays the most important role in our system. Syntactic transformation and verb sense disambiguation almost depends on nese verb pattern. Intuitively the size of verb pattern is the bigger the better, so man-ual construction is the main drawback of pattern-based approach, but in comparison with the manual rule design, it is much more reasonable for the robust MT system. Furthermore pattern addition guarantees performance improvement progressively how to effectively construct verb patterns which implies which patterns contribute to the system mostly. The second is how to increase the hit rate of verb pattern utilizing the existing patterns, when transferring to other domain. Our pattern-based system originates from Korean-Chinese TV news caption MT system. For the couple of years X  development in news domain, we constructed over 300,000 verb patterns, which leading to progressive improvement. But we found, at the certain point, the addition of pattern dose not take much effect any more, which means the newly added patterns actually have no impact any more for news system. But, we found in techni-proper patterns from the numerous candidates. It is also hard to estimate how many patterns we need in new domain and how to reuse the existing patterns. In section 2, we briefly introduce system overview, and in section 3, describe 3-stage verb pattern matching approach and the process of pattern construction. In section 4, shows how the hit rate of matching changes with the increase of pattern size and, using the three-stage pattern matching. Fig. 1 shows the structure of our Korean-Chinese MT system. There are two main parts in our architecture. One is analysis part the other is patterns matching one. The analysis part consists of Korean morphological analysis module, and parsing module. Korean input is separated into several simple sentences. After parsing, the simple sentences are converted into the proper format, which includes noun chunking, verb restoration of some duel POS noun, attachment of semantic features. 
In our previous system, we use only one stage verb pattern matching approach, if it hits the pattern, then it generates Chinese acco rding to the Chinese pattern linked with Korean pattern. If it fails, Chinese generator arranges the word order by rules. Unfor-tunately in this case most of long inputs leads to the Chinese outputs are in disorder. In our new system we extend previous standalone matching process to three-stage structure. If it fails in the basic verb pattern matching stage, then it tries default verb matching in the second stage. Default verb pattern is automatically extracted from pattern in third stage. Default ordering pattern is automatically extracted from default nese verb, but generalized with symbol KVERB and CVERB, so if it hits the pattern, then generates Chinese verb through partial verb pattern matching in stage one or two. Some of Korean adverbial postpositions have semantic ambiguities. We use disam-biguation database of adverbial postposition to resolve this kind of problem occurred rules as in the previous system did. We strive to improve the pattern hit rate and avoid the default generation by rules. 3.1 Basic Verb Pattern In our system verb patterns consists of two parts. Korean pattern is linked with Chinese  X   X  X  X  (ESEO: adverbial case marker) X  to express syntactic relation. We use about 200 semantic features to express the Korean pattern. In the following verb pattern exam-ple, it uses semantic features like  X  X oncrete place, symbol X  to disambiguate the multi-sense Korean verb  X   X  X  (DE-DA). The Korean verb  X   X  X  (DE-DA) has over 20 senses according to the change of the follo wing arguments. In this case, using seman-tic feature is the effective solution for disambiguation. Verb Pattern: A=concrete place!  X  B=symbol!  X   X  (display)!  X  &gt;  X  A  X   X   X  :v B 
We can see semantic code or word is a necessary feature in the technical domain, and we call patterns using such features are basic verb patterns. We X  X e constructed over 300,000 basic verb patterns for about 20,000 Korean verbs. It is about 15 pat-terns per one verb, and we can see the improvement with the increase of patterns. In section 4, we show the evaluation result. Using the existing basic patterns, we X  X e manually tested 400 sentences of technical documents. We focused on the results of 50 sentences, which was under the 40 point (the perfect translation is 100 point.). The result was that, among theses 50 low-point outputs, 46% error was caused by the The problem is there are tremendous patterns are waiting for be constructed. For more effective construction of patterns and utilization of existing patterns, we analyzed the 150,000 existing patterns for about 20,000 Korean verbs. We focused on the diver-gence of the Chinese verbs, which linked with Korean verbs. As a result, we X  X e constructed 16, 000 patterns for about 8,000 verbs. We found, these verbs are mono-sense verbs. From the lexical disambiguation point of view, it means some patterns are duplicated. The root of the following Ko rean frequent verbs originates from Chi-nese verbs, and most of these verbs have the unique Chinese translation. Over 50% of frequent Korean verbs are in this category. We define them as A-class verbs, and should pay attention to the duplicated construction. About 4,000 verbs have two senses in average, and in the most of cases, they are biased to a unique Chinese trans-lations. Some are Chinese-root verbs and others are pure Korean verbs. We define gence between Korean and Chinese instead of the lexical transformation. About 8,000 verbs have 5 senses in average, and most of them are pure Korean verbs. We define them as C-class verbs. The Korean verb  X   X  X   X  has more than 20 senses with the change of semantic feature of argument, like  X  X loat, fly, rise, become stale, be distant, move, scoop up... X . For this class verb, we should focus on the disambiguation of lexical divergence. Through the above analysis, the current basic verb pattern using semantic features is reasonable for some verbs, especially for C-class verb. For A, B struction criterion. 3.2 Default Verb Pattern For A, B class verb, we constructed patterns by length of them. The length represents with the number of arguments in patterns. We only constructed the unique structure of  X   X  X  X  X  X  X  (develop) X , we X  X e constructed over 80 verb patterns with the same struc-ture of  X  X =SEM1!  X  (SBJ-MARKER) B=SEM2!  X  (OBJ-MARKER)  X  X  X  X  !  X  &gt; A  X  X  :v B X  
We extract one default pattern from these 80 existing patterns for the above struc-ture, using default argument marker  X * X . We sort the existing patterns by Korean verb frequency of Chinese verbs and translation of Korean postpositions. We automatically extract 100,000 default patterns, from the existing basic patterns, which will cover the most of A, B class verb patterns. The followings are the example of default patterns. Default pattern: A=*!  X  (SBJ-MAKER) B=*!  X  X  X  X  (VIA) C=*!  X  X  X  X  (FROM) D=*!  X  (OBJ-MARKER)  X  X  X  X  !  X  (buy) &gt; A  X   X  B  X  C  X  X  :v D. 
In the pattern matching module, we add the default pattern matching function. Af-ter failure of the basic pattern matching, it tries the default matching. In the section 4, it shows the pattern hit rate and translation performance comparison between two versions. 3.3 Default Ordering Pattern works, but still it fails in some cases. In the following example, we extract a default verb pattern from the basic pattern. Basic verb pattern: A=*!  X  (SBJ-MAKER) B=  X  X  X  (system)!  X  X  X  (IN) C=  X  X  X  (range)!  X  X  X  X  (according to)  X  X  X  X  !  X  (be decided) &gt; A  X  B  X   X  C  X   X  X  :v Default Verb Pattern: A=*!  X  (SBJ-MAKER) B=*!  X  X  X  (IN) C=*!  X  X  X  X  (accord-ing to)  X  X  X  X  !  X  (decide) &gt; A  X  B  X   X  C  X   X  X  : 
If the Korean input sentence is  X   X  X  X  X  X  X  X  X  X   X  X  X   X  X  X   X  X  X  X  X  X   X  X  X  X  X  X  X   X  X  X  X   X  X  X  X  X  X  (The transmitting frequency is decided by the band of wireless above Korean input, if the verb be changed from  X   X  X  X  X  X  X  (be decided) X  to  X   X  X  X  X  X  (be decided) X , which is the synonym of original verb, then unfortunately it  X   X  X  X  X  X  (be decided) X . For this case we try another approach to utilize the existing patterns without the new manual construction. We automatically squeezed 100,000 default verb patterns to the 10,000 default ordering patterns. We sort them by the structure of patterns, generalizing the conc rete Korean verbs with abstract symbol KVERB, and Chinese verbs with abstract symbol CVERB. The main role of default ordering patterns is arranging the Chinese word in right order. Default ordering pattern: A=*!  X  (SBJ-MAKER) B=*!  X  X  X  (IN) C=*!  X  X  X  X  (ac-cording to) KVERB &gt; A  X  B  X   X  C  X  CVERB:v 
For default ordering matching, we add another function, which is called after de-fault verb pattern failure. We can see from the above example, default ordering pat-not disambiguated. For verb generation, we use the partly matched arguments to se-lect the most highly scored patterns and take the Chinese verb in that pattern. For post position generation, we make reference to the default postposition data, which de-scribed in section 3.4. We strive to utilize the existing verb patterns as possible as we can, on the basis of without downgrading of translation performance. The relevant evaluation shows in section 4. 3.4 Default Adverbial Postposition Generation fault ordering pattern get hits. 
The Korean postposition like  X   X  X  (to, also) X  has two senses. We extracted the disambiguation data from the basic verb patterns. It uses semantic code of the argu-ment and the collocation of verb as features. The following is the format of data. [  X  X  ] *!  X  X  @Chn1-Frq1,Chn2-Frq2... [  X  X  ] $  X  X  X  (place)!  X  X  _  X  X  (go)@  X  (to) 10 [  X  X  ] $  X  X  (value)!  X  X  _  X  X  X  X  X  (record)@  X  (also) 4 3.5 The Process of Pattern Construction matching log data in volume, and use the Korean pattern generator to automatically section 3.1.We define the verb class with the following criterion through the analysis verb is under 50%, then it is C-class verb, or it is A, B class. The A-class verb pattern like  X  X =*!  X  (SBJ-MARKER) B=*!  X  (OBJ-MARKER)  X  X  X  X  X  X  (develop) X  has only two arguments without adverbial arguments, so there is a strong probability that it is hit by existing verb patterns or default patterns. For about 15,000 verbs in A, B class we X  X e manually constructed only 20,000 unique patterns from the 200,000 can-didates. For the high frequent 5,000 C-class verb, we X  X e constructed the whole candi-dates about 30,000 patterns, which have over two arguments. Default verb patterns and default adverbial postposition data are au tomatically extracted from the basic verb patterns, and default ordering patterns are automatically extracted from the default verb patterns. If we add new basic verb patterns, then it will automatically enlarge the default verb and ordering patterns. Fig.3 shows the hit rate and the number of matching failure with the addition of new basic patterns for 2,000 sentences of technical domain. We use the existing 150,000 basic patterns as a base system. With the increase of 50,000 basic patterns, the hit rate only increases 0.6%. 
The next two bunches of patterns are the newly constructed patterns, which are fol-lowed the criterion of three verb classes. As a result, the third bunch of 25,000 basic leads to the increase of 11.3% hit rate. Comparing with the previous addition of 50,000 patterns, it has a significant improvement. 
Fig. 4 shows the hit rate change with the addition of default verb pattern and de-fault ordering pattern. Using 100,000 default verb patterns, the hit rate increases 2.9%, and with the 10,000 default ordering patterns, the hit rate increases 0.9%. Using three-stage verb pattern matching, the hit rate totally increases 3.8%, comparing with the basic one-stage system. 
Fig. 5 shows the translation performance using the new verb pattern approach. We use 400 technical sentences with 4 references as the evaluation set to check the BLEU score. The result is similar with the one showed in Fig. 5. Using default verb patterns, BLUE score increases 5.1% and using default ordering patterns, BLEU score increase 0.1%. Using three-stage verb pattern matching, the score increases 5.16%, comparing with the basic one-stage system. This paper described three-stage verb pattern matching approach and the relevant process of pattern construction in our Korean-Chinese machine translation system in technical domain. With this approach, we made promising improvement of pattern matching performance. In the next phase, we will try the recombination technique to automatically construct default ordering patterns, and adopt this approach to Korean-English system to see the feasibility. Generally, we can get satisfactory performance of machine translation system by narrowing translation target domain. Recently, some MT systems achieved great performance in specific domain and could be successful in commercial market. So, it is very important to restrict target domain of MT system for its practicality. 
These facts being given, changing target domain of an existing MT system is also another important issue. There have been studies on domain adaptation in MT com-munities [1,2,3]. This paper presents a me thod to adapt an existing MT system for changing its translation target domain. Th e process of domain adaptation includes adaptation of each module of MT system and customization of translation knowledge, such as dictionary, transfer patterns, analysis rules. In this paper, we give an explana-tion of the steps for customizing the existing patent specific MT system for newly targeted IT web news domain. 
We already developed an English-Korean MT system for patent. Also, using the patent MT system, the online translation service for English patent documents has been provided at IPAC (International Patent Assistance Center) under MOCIE (Min-istry of Commerce, Industry and Energy) in Korea. Now, as another challenge, based on the patent specific English-Korean MT system, we are developing an English-Korean MT system for IT web news. So, our focus is how to tune an existing MT system for new target domain. 
The English-Korean MT system for IT web news consists of analysis module in-cluding tagging and syntactic analysis, transfer module and generation module. The English-Korean MT system for IT web news employs dictionary, parsing rules, trans-fer patterns, lexical rules and so on as main knowledge. Syntactic analyzer (or Parser) parses English sentences using parsing rules which is linked to transfer patterns. After syntactic analysis, English-Korean transfer is performed to convert English syntactic structure to Korean syntactic structure using the transfer patterns and to select Korean target words of English source words. Finally, Korean generator generates Korean sentences. 
The detailed explanation about the domain adaptation method for each MT system describes experimental results and discussions. In the section 4, we sum up the dis-cussion and show the future research direction. This section gives the detailed description about how to adapt each module of the MT system and knowledge like dictionary and patterns. 2.1 POS Tagging Module POS tagging module carried out the adaptation based on early mentioned analysis results for the characteristics of IT web news domain from the perspective of POS tagging. Considering that the customization from the patent domain to the IT web tagging module focused to release its rest riction. The domain adaptation approaches by tagging module are described below: 2.2 Syntactic Analysis Module The main process for the adaptation of syntactic analysis module includes the followings: z The process for coordinate NP structure: To deal with various types of coordi-z The improvement in recognition accuracy for time-related NP structures and z Tuning for interrogative sentences: There were many interrogative sentences in z Tuning for sentences including varied expressions like inversion, ellipsis: We z The use of IT web news domain specific patterns: We applied patent-specific z Base NP chunking z Improvement in performance of base NP chunking: We devised the mechanism 2.3 Transfer Module As mentioned above, many semantically ambiguous words are used in the IT web news domain. This means that a target wo rd selection accordi ng to context would play important role for improving transl ation accuracy. At the same time, it means target word selection. So we introduced a target word selection function for frequently used nouns and verbs. 
In the case of nouns, we defined co-occurring words as the words which is located at the position to modify the ambiguous head word within noun phrase. First, frequent noun phrases were extracted from the English IT web news corpus. Second, for the noun phrases extracted from the English corpus, corresponding Korean noun phrases were extracted from the Korean compar able corpus. Third, English co-occurring words to be used to resolve target word selection ambiguities were extracted from the English noun phrase aligned to Korean noun phrase. In the case of verbs, we used the words located at argument positions of verbs as clues for selecting proper target word for ambiguous verbs. Similarly, we extracted frequent verb frames from the English IT web news corpus and the comparable Korean IT web news corpus. After aligning argument position of English verb frames [4, 5]. 2.4 Translation Knowledge Knowledge adaptation includes the construction of unknown  X  X ords/compound words X  and corresponding target words, th e adaptation of target words of existing words for the IT web news domain and the construction of patterns used frequently in the IT web news domain. 
We extracted unknown words and unknown compound words from the IT web news corpus and then added to dictionary attachi ng their target words. Table 1 shows the num-ber of unknown words and unknown compound words registered newly in this process. 
We also performed the domain adaptation for target words of nouns, verbs and ad-jectives of dictionary. For this, using th e English-Korean comparable corpus, we refined target words of source words semi-automatically and added IT web news domain specific contents of example words. In Fig. 1, word  X  X ircumference X  has con-tents for IT web news domain and contents for patent domain. In Fig. 1, we can see the feature name, such as  X  X TYPE X ,  X  X POS X ,  X  X SEM X ,  X  X ROOT X , etc, and their feature values. For example,  X  X ROOT X  means the Korean target word feature.  X  X  : NEWS  X  X  :PAT 
Transfer patterns and lexical rules which are employed for the structural transfer were extracted from the IT web news corpus automatically, and then corresponding Korean translation parts were added by hands. We present the example of transfer patterns and lexical rules in Fig. 2 and Fig. 3 respectively. Transfer pattern and lexical rule have same format. They consist of a source part and a target part. A source part contains the condition on which the pattern can be matched. { for! dear life } -&gt; {  X  X  X  _  X  X  X  _  X  X  X  X  ! }
Table 2 presents the number of transfer patterns and lexical rules which were addi-tionally introduced for IT web news domain. 3.1 Human Evaluation To evaluate proposed domain adaptation method, we conducted a human evaluation and a automatic evaluation. First, a detailed description about the human evaluation is the followings: z Test set: The test set for the experiment consists of 400 sentences extracted from z Test set configuration: z Evaluation criterion: z Evaluation: After instruction about the evaluation criterion, 5 professional trans-
For comparison, we evaluated two MT systems (the existing patent specific Eng-lish-Korean MT system and the customized English-Korean MT system for IT web news domain). The results are shown in Table 5. 
After applying the domain customization process, there was an remarkable im-provement in translation accuracy. The number of the sentences that were rated equal to or higher than 3 points was 296. It means that about 74.0% of all translations were understandable. 3.2 Automatic Evaluation We also conducted automatic evaluation. The test set for the automatic evaluation con-sists of 1,000 sentences. For each sentence, a set of 5 references was produced by hands in advance. Table 6 presents the Bleu scores for two English-Korean MT systems. In Table 6, BLEU-MA column represents the Bleu score measured at morpheme level. 3.3 Evaluation for Each MT System Module We evaluated performance of each module of two MT systems. The results are shown in Table 7. In table 7, ERR means error reduction rate. The improvements in each mod-ERR of other modules. The reason is caused by the customization of knowledge, such table 7, the tagging accuracy was measured in the word level and the syntactic analysis accuracy and the target word selection accur acy were measured in the sentence level. Category 
MT system for patent 98.40% 55.3% 49.8% Customized MT system For IT web news 99.07% 77.9% 85.8% In this paper, we presented the domain adaptation method from the patent domain to the IT web news domain for an English-Korean MT system. This method includes the customization of each MT system module as well as knowledge. Of course, the pro-posed domain adaptation method can be generalized and be applied to other domain and to other MT systems. We have demonstrated the effectiveness of the proposed method by conducting human evaluation an d automatic evaluation. Because some processes of the proposed method require human hands, our next research topic is about how to customize MT system automatically. Text alignment is an important task in Natural Language Processing (NLP). It can be used to support many other NLP tasks. Lots of researches have been done on bilingual alignment [1,2,3,13], and some specific kinds of corpora have gained more and more focus. One of the typical examples is the movie subtitles, it is free available and has rich semantic. [4] showed that a Machine Translation(MT) system gave a slightly better result when training on subtitles compared to Europarl (Europarl Parallel Cor-particular for statistical MT. each text piece consists of one or two short sentences shown on screen with an average perceive and understand a given subtitle so the internal understanding complexity is small. The linguistic subtitle structure is closer to oral language with great variability. As a result the language of subtitling covers a broad variety of any conceivable topic, even with exaggerated modern youth language. Still, [5] pointed out widespread un-known words in subtitles. They comprise proper names of people and products, rare-word forms and foreign words. One must notice the fact that different language versions of subtitles for a same movie are not necessarily written by the same person. Still, the amount of compression and re-phrasing is different for various languages, and it also depends on cultural differences and subtitle traditions. The Fig. 1 shows a short example of German subtitles and their Chinese correspondences in SubRip format. Sentence alignment is an almost obligatory first step for making use of German-Chinese subtitles. It consists of finding a mapping between source and target language crossing dependencies. Most works on automatic alignment of film subtitles are still in its infancy. [6] handled the alignments on time with a variation by empirically fixing the global time-shifting at 500 milliseconds. [11] showed that the alignment approach based on time overlap combined w ith cognate recognition is clearly superior to pure length-based alignment. The results of [11] are of 82.5% correct alignments for Dutch-English and 78.1% correct alignments for Dutch-German. The approach of equally synchronized to the original movie. A method named Dynamic Time Warping (DTW) for aligning the French-English subtitles was obtained from Internet [10]. The approach of [10] requires a special bilingual dictionary to compute subtitle correspon-dences, and [10] reports 94% correct alignments when turning recall down to 66%. However, the corresponding lexical resources in [10] cannot be easily accessed. 
In this paper, we propose a new language-independent approach considered both for our subtitle translation. Section 2 dedicates to the method presented by us for sub-title alignments, and our improvement by using lexical anchors. The evaluations of result with comparison to traditional length-based approach are discussed in Section 3. Before alignment can be applied, the subtitle corpus needs to undergo a few prepro-cessing steps. Each German subtitle file has been tokenized and corresponding Chi-responded word. We intend to combine stemming, which has been demonstrated to improve statistical word alignment [9]. For German, a light stemmer (removing in-flections only for noun and adjectives) presents some advantages. Despite its in flex-ional complexities, German has a quite simple suffix structure, so that, if one ignores the almost intractable problems of compound words, separable verb prefixes, and prefixed and infixed "ge", an algorithmic stemmer can be made quite short and effec-tive. The umlaut in German is a regular feature of plural formation, so its removal is a sch X n, beautiful; schon, already). In future work, we would like to improve the stem-mer especially for the irregular morphological variations used by verbs and nouns. We expect that the effect of combining these morphological operations will reduce the sparse data problem and speed up the computation of correspondence pairs. 2.1 Dynamic Time Warping A considerable number of papers [1-3] have examined the aligning sentences in paral-lel texts between various languages. These works define a distance based on length or lexical content, which involves the use of dynamic programming. Since the time in-different translations should be shown at roughly the same time. However, this case does not occur very often. Every subtitle file is built independently from others even for the original video track. This result in growing time gaps between corresponding beans. The time span is never identical at the millisecond level. 
In order to handle this problem, we apply dynamic programming to calculate the best path between two subtitle files. This algorithm uses the interval of the start time from two subtitles to evaluate how likely an alignment between them. Two subtitles are not considered as an aligned pair if th eir start times are far away from each other. To make it easily find the most probable subtitle alignment, the possible alignments in German and Chinese subtitles are asynchronous. The cost of all possible alignments, which are measured by time differences, ha s been considered from the beginning to the end of the subtitle file. 
Let  X   X   X , X   X  be the lowest cost alignment between subtitle  X ,...,1 and  X ,...,1 where  X  is the index of Chinese and  X  is the index of German subtitle. Previously the  X   X  0,0  X  is set to 0. Then one can define and recursively calculate  X   X   X , X   X  as follows: quency match types. 
This leaves determining the cost function,  X  X  X  X   X   X , X   X  as follow: For each step, once the align mode of  X  and  X  is determined by (1),  X   X   X , X   X  must be set to  X   X  X  X   X   X , X   X  of the selected mode. That is to say, the matrix  X  is built dynamically in the procedure. 2.2 Lexical Cues Extension richer information available in the text. To obtain an improvement in alignment accu-racy, the lexical content must be considered . Intuitively, the lexical content could be used to find reliable anchor points. Previous work [2,9,11] focused on using bilingual ures [11] such as the longest common subsequences to decide the most relevant can-didate pairs. Here, we apply the approach based on measures of association on rough-ly parallel texts, which has been processed by dynamic programming in Section 3, to derive the bilingual dictionary automatically. Then we find the anchor points by 2:1, 1:2 and 2:1 matches. Though the validity of the co-occurrence clue is obvious for parallel corpora, it also holds for comparab le corpora and unrelated corpora [12]. The simple form as follow: where N is the number of all roughly alignments except 1:0 and 0:1 mode. For a selected count of pairs that have G in German subtitles, but lost C in the correspondence Chinese subtitle.  X   X  X  is the count of pairs in which Chinese subtitle have C, but the aligned Ger-man subtitle misses G.  X   X  X  counts the number of pairs that have neither G nor C . 
For each Chinese word and word in current subtitles, we use (1) to calculate their score. Using the confidence level of  X  X 0.05 and the critical value  X   X   X 3.845 , that all entries belonging to the words are not found in the German stop list and Chi-nese stop list. Since the size of each subtitle file is limited, the word C may have sev-eral correspondent words G with the same score. For them, the pairs which score the highest are remained. Table 1 shows the result for 6 Chinese words and their corres-pondent German translations in 685 results totally.  X  X  X  Bier bier  X  X  X  X  X  Pentagon pentagon  X  X  gluckwunsch gluckwunsch  X  X  X  X  Welt erklart global kommunismus sowjetsist 
As the Table 1 illustrated, in many cases our program predicts the expected word with other typical associates mixed. Since a number of reliable word correspondences have been found, we can use them to predict anchor points by some simple heuristics. We count a score that indicates the number of words to match between the Chinese subtitle C and German subtitle G as follows bilingual dictionary. Since our dictionary may provide several candidate German translations for a Chinese word, and  X  X   X   X   X   X  can be a word or a collection. Therefore a Kronecker  X   X   X , X   X  is given to check the matches. Assuming that an alignment should mainly consist of match translations, we can use a threshold  X  for this score to decide whether an alignment is likely to be correct or not, thus, to be an anchor point. We examine the pure-length based approach [13] and the DTW based on time-delay with its lexical extension that we proposed in this paper. The evaluation has been conducted on a corpus extracted from randomly selected 10 movies. For each movie, we take out randomly around 400 Chinese and their German corresponding subtitles, that the sentence pairs are at initial of the movie and consecutive within each movie. All of t sentence pairs hese pairs are manually aligned. When conducting some pre-vious evaluations, most of them limited their test to few dozens for each movie, which obviously facilitates the task. We separated 1,000 sentence pairs from all this manual-ly aligned pairs as a training set. We then used relative frequencies of each occurring alignment mode in training set to estimate the parameter  X  . For efficiency reasons we round them into integers as shown in Table 2. 
Therefore it will cause the algorithm to give 1:1 match for a priority, which is most common. These parameters will be shared in the three approaches we evaluated. For the length-based approach, the number of German characters generated by each Chi-nese character is also calculated in our training set, the mean  X 2.667  X  , with a stan-dard deviation  X 1.040  X  . 
The result of time-based dynamic programming and its lexical extension are listed in Table 3. The threshold  X  as mentioned above was set to 0.05 previously. To be able recall and precision. However, in some cases the count of exact matches is somewhat arbitrary. We count partially correct alignments, which have some overlap with Len 30.8% 30.1% 30.4% 38.2% 37.3% 37.7% Len+ Lexi 37.5% 50.6% 43.1% 45.8% 61.8% 52.6% Time 73.4% 67.8% 70.5% 83.0% 76.7% 79.7% 
Time+ Lexi 66.4% 72.4% 69.3% 76.8% 83.8% 80.1% accordingly to the recall and precision. 
The pure length-based approach showed their weakness being compared to other approaches on sentence alignment of subtitles. The possible reason could be the inac-the length of potential aligned sentences in German and Chinese. Since the linguistic structure of subtitle is closer to the oral language with great variability, this leads the translations may drop some elements for cu ltural reasons. The correlation coefficient of German and Chinese sentence length in subtitles is 0.813, which indicates the sen-poor performance of length based approach. 
The score showed that the dynamic programming with lexical extension yields bet-ter precision, which could be expected due to the anchor points, since the lexical ex-reliable word alignments limit the search space for local minima. While we only al-low the 1:1 alignments to be anchors, the original 1:2, 2:1 and 2:2 alignments will be divided into several pairs, which results more retrieved pairs than actual ones and aligned the subset of the corpus for further research. The developed alignment method on the total Chinese and German subtitle corpus retrieved only 1:1 and 1:2 align-ments, for which there is a correct rate of 88.4%. A sentence alignment method for movie subtitles is proposed in this paper. The pro-posed approach is based on the time-shift information in subtitles and it uses dynamic programming to minimize the global delay. The statistical lexical cues are also intro-duced to find word correspondence. As we have shown in the evaluation in Section 3, this additional technique yields better performance, it enhances about 7% of precision. Future work will be based on IBM Word Alignment Model to retrieve translation system. The results of this paper may boost the research towards a practical MT sys-tem between German and Chinese. Along with the development of MT technology, various evaluation approaches have been proposed to examine different facets of the quality of MT output, for instances, assessing fluency, accuracy and informativeness on a 5-point scale [1], testing under-standability via comprehension exercises [2], checking grammatical test points in the form of a test suite [3], and many others. 
In recent years, MT evaluation has undergone a paradigm shift from human judg-ment to automatic measurement. As defined in Papineni et al. [4], an MT output is evaluation into a matter of assessing text similarity between MT output and human translation. Since, a number of evaluation metrics have been proposed along this line of exploration, including BLEU [4], NIST [5], TER [6], and METEOR [7], to provide a fast, cost-effective and objective way to compare the performance of different MT systems on the same basis. They have been widely adopted by MTers as de facto standards for MT evaluation for various purposes, in particular, system comparison and development. 
In practice, however, these metrics are not intended to directly measure the transla-tion quality of a given MT system. Rather, it measures how similar a piece of MT output is to a human translation. However, theoretical support is still to be found for perimental evidences that we claim that these two variables tend to correlate with each other. It is therefore a question to as k whether such correlation remains constant across different languages and text genera. This kind of uncertainty inevitably leads to the works on meta-evaluation for MT, i.e., the evaluation of MT evaluation metrics. This has been one of the major themes in a number of recent MT evaluation campaigns, including HTRDP [8], TC-STAR [9], NIST open MT evaluation [10] and the shared task of MT evaluation in ACL workshop on statistical MT [11]. Different text genera are covered, such as news, business, dialog, travel, technology, etc. In most cases, the evaluation metrics correlate with human judgments reliably, but still there are discordant cases. Culy and Riehemann [12] show that the two evaluation metrics BLEU and NIST perform and some MT outputs even outscore professional human translations. Callison-Burch et al. [13] raise an example in 2005 NIST MT evaluation that the system ranked at the top in the human evaluation section is ranked only the sixth by BLEU. Babych et al. [14] comment that these evaluation metrics cannot give a  X  X niversal X  prediction of human perception towards translation quality, and their predictive power may be  X  X ocal X  to particular languages or text types. For a new language pair and text genre, human evaluation of the performance of a given metric is necessary in order to prove its reliability. 
In this paper we present our recent work on the evaluation of a number of MT evaluation metrics using legal texts in Chinese and English. Our previous work [15] on evaluating the use of online MT in legal domain has shown the potential of MT evaluation metrics in helping lay users to select appropriate MT systems for their evaluation data is described in detail, including the retrieval, preprocessing and trans-lation of legal texts, and their human evaluation, followed by a recognized evaluation protocol. Section 3 reports the automatic evaluation on these texts with different popular evaluation metrics, and the correlation between metric scores and human judgments. Section 4 concludes this paper, highlighting the value of this kind of meta-evaluation, particularly from the user perspective. The basic resource required for our evaluation is a set of parallel texts aligned at sen-parallel corpus, namely BLIS [16], for our work. It is the most authoritative and com-prehensive legal text collection in Hong Kong with high quality translation and com-complex structures of long sentences and the use of special terminologies, however, pose a challenge not only for MT systems but also for the evaluation metrics in use, a huge challenge to differentiate between the subtle quality differences of their human translations and MT outputs. 
Two phases of work were involved in turning the BLIS corpus into the evaluation translation by different MT systems and the second to evaluate the translation quality of these MT outputs by human evaluators. 2.1 Retrieval, Preprocessing and Translation of Parallel Sentences To retrieve a representative sub-corpus from BLIS, all its documents were randomly picked out with a constraint of 20 sentences in length. This is to ensure that the evalu-Chinese-English bilingual documents were selected in this way. 
The necessary preprocessing was then performed on these selected texts, includ- X  X  X  X  X   X  X  X  X   X   X   X  ) that were not to be handled by MT. The sentence alignment for the selected texts, which had been automatically performed by machine, was manually double-checked in order to ensure its correctness. Finally, the dataset con-sisted of 269 pairs of unique parallel sentences. This data size was proved adequate to provide reliable evaluation results, as shown in Estrella et al. [17]. Each of these sentences was translated into the other language by four commercial off-the-shelf MT systems, comprising both transfer-based and statistical systems. A program was developed for submitting the source texts sentence by sentence to each MT system and then collecting their translation outputs in an automatic manner. Two sets of MT outputs, i.e., both the English-to-Chinese (EC) and Chinese-to-English (CE) transla-tions, were subsequently retrieved. 2.2 Evaluation by Human Evaluators The human evaluation of these MT outputs was carried out following the protocol of NIST open MT evaluation [18]. It focused on two essential features of translation quality, i.e., adequacy and fluency. The former refers to the degree to which the trans-lation conveys the information of its source text or reference translation, whereas the latter to the degree to which the translation is grammatical, readable and understand-able. Totally 70 evaluators participated in this evaluation, including 48 undergradu-ates and 22 postgraduates. All of them are Hong Kong students with Chinese as their mother tongue and English as a second language. About half of them major in lan-guage-related subjects such as linguistics, translation, and English communication, and the other half in various other subjects. 
For each sentence of MT output, the evaluators were first asked to judge its fluency and, afterwards, its adequacy by comparing it with a reference translation. A 5-point scale (Table 1) was used for the rating of both. As defined in the guideline of NIST X  X  MT assessment [18], the fluency refers to a sentence that is  X  X ell-formed grammati-cally, contains correct spellings, adheres to common use of terms, titles and names, is gree to which information present in the original (i.e., the reference translation) is also communicated in the translation X . Fluency : 
How do you judge the fluency of this translation? 5 Perfect 5 All 4 Good 4 Most 3 Non-native 3 Much 2 Disfluent 2 Little 1 Incomprehensible 1 None 
The evaluators were encouraged to follow their intuitive reactions to make each judgment and not to ponder their decisions. Fo r each sentence, they were instructed to spend, on average, no more than 10 seconds on fluency and 30 seconds on adequacy. The evaluation was conducted via an online web interface. Each evaluator was ran-domly assigned roughly the same number of questions, with a restriction that candi-date translations from the same source document were not evaluated by the same evaluator. In other words, no evaluator would work on different translations of the same source document by different MT systems. There were 35 sets of translations in total, and each set was handled by two evaluators. The MT outputs assessed by human evaluators were then evaluated again with differ-ent automatic evaluation metrics. The reliability of an evaluation metric is determined by how consistent its evaluation results agree with human judgment on the same piece of MT output. Five evaluation metrics were examined in this evaluation, including BLEU [4], NIST [5], METEOR [7], TER [6], and ATEC [19] (see Appendix). The first three were used in 2008 NIST open MT evaluation [10] and the last was developed by us. then evaluated with these metrics using a single reference translation. For EC transla-tion, only BLEU and NIST metrics were applied, as they were the only two applicable to Chinese texts. The evaluation was conducted at both the character and word levels, with the aid of a word segmentation program from Zhao et al. [20]. For CE transla-tion, all five metrics were applied. 
To determine the closeness of evaluation metric scores to human judgment of translation adequacy and fluency, the Pear son correlation coefficient was used. For CE translation, the correlation was measured at both the sentence and system levels. The former is measured in terms of the difference between human ratings and metric translation sentences. For EC translation, the correlation measurement was carried out at the system level only, as BLEU and NIST are not designed to work at the sentence level. Adequacy system .978 .990 .864 .991 Adequacy sentence --.436 .304 .305 
Table 2 reports the correlation results for each evaluation metric. For EC transla-tion, it shows that both BLEU and NIST can attain a higher correlation with the aid of word segmentation than without. Both of them perform better at the word level than at character level. Especially for NIST, wo rking on words improves its correlation on fluency from .519 to .849, and achieves a very strong correlation of .991 on adequacy, nearly equivalent to human judgment. BL EU also presents a mild correlation im-more stable in providing consistent evaluation results at both the character and word levels. 
For CE translation, all the other evaluation metrics outperform better than BLEU at the system level, demonstrating a higher correlation on both fluency and adequacy. Among these metrics, our ATEC is the best at modeling human judgment of transla-tion quality, showing correlations of .98 and .96 on fluency and adequacy, respec-correlations. METEOR is the best, but its correlations around .40 are still far below a compare the performance of various MT systems in a quite credible manner given MT system. 
Finally, an interesting observation is that at system level, all the evaluation metrics show a higher correlation on adequacy than on fluency in EC translation. However, a man judgments better on fluency rather than on adequacy. It is an interesting point to start further study of the behaviors of evaluation metrics on different languages. 
To have a better understanding on how evaluation metrics would affect the results of MT evaluation, we have repeated our previous evaluation of online MT systems on translating legal documents from various languages into English [15]. This time the evaluation metric ATEC is used to compare with BLEU that was used in [15]. ATEC is shown above to have achieved the highest correlation on English translation. Table 3 presents the resulted rankings of the online MT systems according to BLEU and ATEC scores for various language pairs. About half of the ranks, as highlighted in grey color, are changed due to the different scores by the two metrics. This shows that the choice of evaluation metric can have a significant impact on system compari-son. Hence, a proper selection of evaluation metrics is critical to this end. It needs to evaluation metric. We have presented our recent work on the evaluation of a number of popular scoring metrics for automatic MT evaluation using parallel texts from the legal domain. As so many evaluation metrics are available for measuring different features of MT outputs and each of them has its own strengths and weaknesses on various text types as well as language pairs, this evaluation has explored the basic methodology to guide us to opt for a proper choice of evaluation metric for our particular situation. It also serves as a complement to other related evaluations on other text genera and languages, in the hope of a more comprehensive understanding of the evaluation metrics in use. Our evaluation shows that the pioneering evaluation metrics BLEU and its variant NIST demonstrate their high reliability in evaluating EC translation, especially with the aid of Chinese word segmentation. For CE translation, however, our own metric ATEC system rankings of online MT systems on different language pairs. The high correla-tions obtained by BLEU, ATEC and other evaluation metrics, however, are retainable only at the system level. The correlations at the sentence level are still rather low for all evaluation metrics. This suggests an inadequacy of our understanding of the elements Our future work will concentrate on further exploration in this direction. The evaluation data generated in our work reported above, including the system translations, human judgments and scores of evaluation metrics, has been made publicly available at http://144.214.20.216:8080/ctbwong/ LegalMetaEval/ for research purpose. ATEC evaluates MT outputs in terms of two essential features of translation quality, namely, word choice and word position. The word choice of a translation is measured reference translation ( r ), which is defined as follows in (1): 
To account for the variances of word position between the candidate and the refer-ence, we add a penalty to the formula to sum up the word position differences of each matched unigram as follows in (2), where w is a weighting factor empirically set to 4. 
The ATEC score for each candidate sentence is computed by combining the uni-gram F-measure and the penalty of word position difference in the following formula. The final score for a system is obtained by averaging the sentence scores. There have been several popular models for SMT in recent years, since the state-of-art word-based model [1] again received attentions last century. Among all these models, phrase-based models are most popular, due to theirs simplicity and powerful mechanism for well modeling local reorderings and multi-word translations. While due to the fact that phrase-based models cannot handle long-distance reorderings, discontinuous phrases and linguistic information, such as syntactical structure, syntax-based models are then proposed. There are two alternative ways to make use of syntactical information: one is to use additional non-linguistic structures or grammars model syntactical structure(s) of languages [6,7,8,9,10,12,13]. 
Although good models and advancements has been proposed already, there are still several fundamental issues for SMT to be investigated, such as how to properly transfer non-isomorphic trees between languages , and reorderings models based on structures and so on. 
In this paper, we propose a translation model based on flattened syntactical phrases, which are designed to reduce the complicity and sparseness of original tree-to-tree models by ignoring the inner structures of aligned trees or forests, and somehow to relieve the negative influence of the errors companied with syntactical analysis. In additional, for flattened syntactical phrase,  X  X   X   X  , non-terminals indexed by  X  make translation more flexible, and we find that some invalid translations can be firstly filtered in accordance to the syntactical context of  X   X  . For phrase-based models, a French (source) sentence  X  is always segmented into various different phrase combinations, where each phrase has several English combination, the translations of phrases are then permutated to be candidate English translations; finally, by using certain ranking methods, one or more most higher-score English sentence(s)  X  would be selected as the best translation(s) of  X  from all English candidate sentences generated by model. 
The noisy-channel model [2] is firstly introduced for modeling the translation by assuming that the inputted  X  is encoded from  X  , and the aim of translation is to find a decoder to encode  X  into  X  . By Bayesian rule, this procedure can be formalized as: where we call  X   X   X   X  and  X  X  X  X  X  X  as language model and translation model respectively. One more flexible model is then proposed by Och and Ney [3], a log -linear model using Maximum Entropy framework: models mentioned above and any relevant knowledge can be easily taken into account as feature functions. It has been a standard framework for SMT. 
Much efforts have been carried out to improve reordering models for phrase-based models, including the basic distance-based distortion model [14,15], flat reordering model [16], lexicalized reordering model [ 17,18], phrase reordering model [19], and syntactical reordering models [20,21] for global reorderings. 
There already exist several syntax-based tree-to-tree transla tion models sharing similar idea in mapping between syntactical trees of both languages, such as Cowan X  X  tree adjoining grammar (TAG) [7], and dependency grammars [9,22,23]. It seems to be hard and complicate that these models leave syntactical structures unchanged for modeling. A tree sequence alignment-based model proposed in [24], captures both non-syntactical phrases and discontinuous phrases with linguistic features. 
To avoid the weaknesses of syntactical information, synchronous grammar based translation models are proposed by defining a non-syntactical structures to denote the real structures of languages, such as Inversion Transduction Grammar (ITG) [4], where syntactical trees are restructured as binary trees, and then the reordering problem turns to be ordered or reversed; for hierarchical phrase-based model [5], hierarchical structures are only determined by word alignments, but to some extent, they always could capture syntactical information in a flat form. possible mappings for translation by word alignments, such as: where the words with same subscript in both languages are connected, and nodes with brackets indicate their structures, a tree-to-t ree model can extract syntactical rules as: which require full structure matching in source side and then the possible translations rooted by NP , but be rooted by the nodes equaling to NP in syntactical form. 
And for hierarchical phrase-based model, a possible rule is shown: this rule seems to be clearly and flat, any content matching this hierarchical noticed: first, how to determine the boundaries of  X   X  and  X   X  because different boundaries may lead to different translations, and they share the same probabilities in hierarchical phrase-based model; second, even the boundaries of  X   X  and  X   X  are determined, the selection of its translation rules can only be determined by the probabilities estimated in training data, regardless of the syntactical structure it really contains. These sometimes occur simultaneous, for example, Chinese strings [  X  [  X  X  X   X  X  X  ]] and [[  X  X  X  X  ]  X  X  X  ] , where symbols [ and ] are used to indicate the real structures and translations are completely different. 
As above mentioned, our flattened syntactical phrases are then proposed to handle them. By the definition in section 4, above translation pair can be rewrite as: target side, syntactical constraints on va riable nodes are given, according to its syntactical context, such as  X   X  |  X  X 1 X  X  X  X  X  , which means whatever the translations of  X   X  are, 1 X  should always form a syntactical structure  X  X  with the previous node  X  X  .
There are also lexical phrases similar to that in phrase-based models, except for syntactical context, such as: Our synchronous grammar is defined as a serious of synchronous rewrite rules, which can be formalized as: (lexicons) and non-terminals (variable nodes) in each right-side. 
The rule extraction method in our model use word alignment and bilingual syntactical structures to determine the boundaries of rules. Given training data  X   X  X  X  ,  X  similar definitions to  X   X   X  X  X  X  ,  X   X  and  X   X   X  X  X  X  , then we call  X  X   X   X   X  aligned; 3.  X  X   X   X  X   X  sides of  X   X   X 
More formally, the set of rules is the ones derived from flattened syntactical phrase or composed with other rules, which are satisfied with: 1. 1. If  X  X  X  X   X  2. 2. If  X   X  X  X , X  X  X  X  is a rule of  X  X , X , X , X  X  , and  X  X  X  X   X   X  X , X , X , X  X  ,  X  is index for connecting terminals and non-terminals in both sides. 
Large amount of rules would be extracted by above definitions. To avoid some of them which contain too many nodes or complicated structures slowing down training and decoding procedures, we filter the extracted rules by: the number of non-terminals and terminals in source side is limited to  X  (=4), due to the statistics that the percentage of nodes with 2-4 children is more than 95%; the depth of source right-non-terminals contains; only  X  (=2) non-terminals are allowed in single rule; fully non-terminals rules are the special ones with only 1 depth allowed. instead of the traditional noisy-channel model: where Pr  X   X , X  |  X   X  is the probability of derivation  X  , which is proportional to that of  X 
A derivation always contains many ordered rules, so we could decompose above feature functions on rules as following:  X  X  X  X  is each single rule of  X  , then if we treat these features as the weight of rules: then the translation model can be final written as: where  X   X  X  X   X   X   X  is defined as a feature of n-gram language model with weight  X   X  X  X  . 5.1 Features The features used in our model can be divided into two parts, one is the default feature set as in phrase-based models, and the rests are the syntactical features:  X   X  X  X  X Pr X  ,  X  X  X  X Pr X  ,  X  X  X  X  X  X  X  X  X  ,  X  X  X  X  X  X  X  X  X  , similar to the traditional translation  X  Rule Penalties: Rule number penalty, allows to model the prefer rule number in  X  Traditional features, the 5-gram language model and word penalty. Our decoder is based on a redefined algorithm based on Cocke-Kasami-Younger (CKY) algorithm with beam search, for mapping derivations of languages: derivation  X  that yields  X   X  making : In this section, experiments on Chinese-to-English translation are applied, and three models are used: Helix , the implementation of our model; Pharaoh, the state-of-art phrase-based system; and the traditional tree-to-tree model using STSG. Two 5-gram language models trained respectively on training set and English Gigaword corpus data sets are listed in Table 1 in detail. 7.1 Baseline Systems For baseline system one, Pharaoh, the state-of-art phrase-based model, all the parameters remain default as those in Pharaoh. The length for source phrases is STSG applied similar limitations of syntactical structures mentioned in 4.1. 7.2 Helix removing non-terminals with one single child, subcategorizing some node annotations annotations, are implemented firstly. According to the rule extraction method, we 1.22M are lexicon rules and 93.3K fully non-terminal rules. Following is the number of each rule type used in developing and testing data sets: 7.3 Evaluation The evaluation metric for these experiments is a case insensitive BLEU-4. The rate training method proposed by, and then are used in corresponding translation models. The final performance of the three systems shows in table 3. 
Our system achieves 1.87% and 4.76% relative improvements over two baseline systems on testing set, which shows that our model can better modeling translation, and compared to STSG, our model is more competitive in handling syntactical structures by flattening syntactical structures into syntactical phrases. 
For some complicate and long-range synt actical structures, our model performs much better than STSG, but there are still some syntactical structures not modeling well for both systems, such as the most simple sequence or structure of similar words separated by Chinese punctuation  X   X   X , because we could not count on the models to words, not to say Pharaoh. In this paper, we proposed a flattened syntactical phrase-based translation model, and experimental results indicate our model outper form over the state-of-art phrase-based model, Pharaoh and traditional tree-to-tree model using STSG. Our method managed to make traditional tree-to-tree model more flexible by turn syntactical structures into flattened syntactical phrases only containing the key nodes as the root, leaf nodes and hierarchical phrases, but also can incor porate syntactical information for guidance. 
The conflicts between word alignment and sy ntactical structures in rule extraction cause many losses of rules compared to no n-tree-to-tree models, a possible way is by using a loosely aligning model or cross-validation on them to allow more possible translation rule to be extracted. This work was supported in part by the National 863 High-tech Project (2006AA01Z154), the Program for New Century Excellent Talents in University (NCET-05-0287), MSRA research grant(FY08-RES-THEME-227), and National Science Foundation of China(60873091). Ontology is a kind of concept models that could describe system at the level of semantics and knowledge. It aims to access knowledge of a domain in a general way and provides a common understanding for co ncepts in the domain so as to realize knowledge sharing and reuse among different application programs and organizations to spend a great deal money, material, manpower and time. Automatic or semi-automatic means have been proposed for the last 30 years and many achievements have been made. 
Researches on ontology automatic construction are substantially as follows: term terms [4], relations between terms extraction [5], concept hierarchies clustering [6]. tasks of concept learning. Named-Entity Recognition and Information Extraction are about concepts and is a fundamental way to organize the knowledge. Lexico-syntactic automatically[7][8]. 
In the current research, concept learning and hierarchical relations extraction are carried out separately, however there are interrelated to each other. On the one hand, the attribute values of concepts are very helpful to judge if they have a  X  X s-a X  relationship. On the other hand  X  we say a concept is the hypernym of some concepts because it has the common attribute values of these concepts. Thus the two tasks can be integrated into one to guide each other.

The remainder of this paper is organized as follows: in section 2, we introduce our basic idea. While in section 3, we present the core algorithm. And then, in section 4, we describe the experiments and do several result analyses. Finally, we conclude and provide avenues for further research. 2.1 Inference of Attributes Valu es through Hierarchical Relations concept B. 2.2 Inference of Hierarchical Rela tions through Attributes Values If the attribute value set of concept A is the subset of that of concept B, concept A is very likely to be a hypernym of concept B. 2.3 Integration of the Two Approaches In the integration of the concept learning an d hierarchical relations extraction, we can use Hypothesis (1) and hierarchical relations extracted to guide attribute values extraction. Also, we can use Hypothesis (2) an d attribute values to guide hierarchical relations extraction. However, as the attribute values extraction and hierarchical relations extraction are completed automati cally, the result is not accurate.  X  X ad X  prevent this, two strategies are introduced to make up. Strategy 1: Bootstrap based on WWW 
Strategy 2: To introduce the weight of premise into the inference 3.1 Main Framework The inputs of the method of integrated approach of concept learning and hierarchical relations extraction based on WWW are concepts, a seed collection of the attributes values and a seed collection of hierarchical relations. The collection of candidates for attributes values and hierarchical relations is gained through the attributes values extraction, hierarchical relations extr action and the interaction between them. 3.2 Extract Hierarchical Relati ons Guided by Attribute Values In this section  X  we present a Bootstrap system (Fig 2), which develops the key component of the hierarchical relations extr action. This system presents a technique to generate patterns from HTML documents (Section 3.2.1). More specially, this system presents a new strategy to evaluate the quality of patterns (Section 3.2.2). Finally, we use the patterns to extract hierarchical relations. 3.2.1 Generate Patterns the left, middle and right context of both concept A and concept B. The weight is the associating weight with the relationship, which can be used to evaluate it. 3.2.2 Evaluate Patterns The evaluation of the candidate pattern is the key part of our system. We evaluate the that the collection of attributes values of concept A and B has a include-relationship. 
For the first strategy  X  the similarity between two patterns P 1 and P 2 is defined as: 
Sim P P
For the second strategy  X  the collection of attributes values of concept A and B has a include-relationship is defined as: 
Include A B
Integrated formula is as below: 
The whole process will end when no new candidate relations are generated. 3.3 Extract Attribute Values Guided by Hierarch ical Relations In this section, we present Snowball systems (Figure 3), which develops the function expand seed set through coordinate construction (3.3.1); then we present a method to expand seed set through hierarchical relations extracted by 3.2 (3.3.2). 3.3.1 Expand Attribute Value Collection through Coordinate Construction HTML document is too large scale to be treat ed as the basic unit. So we can not only Then the task of attribute values extraction will be divided into two steps as follow: Step 1: Select the relevant sentences including attribute values; Step 2: Extract the attribute values from those sentences. 
We choose the sentences following the tw o conditions. The first one is that the sentence contains a coordinate construction. The second one is that the sentence contains at least one attribute value seed. One reason why the coordinate construction is used is that it is the simplest construction. Another reason is that the phrases in the coordinate construction have the same characteristics. adequate relevant sentences and eventually obtain accurate comprehensive aggregation of attribute values, we conclude the strategy of choosing sentences and distilling value emerges. As below graph, the frame is the part of choosing sentences and distilling attributes interactively. 
If a phrase is in a coordinate construction with another one with high weight, it should also have a high weight too. construction with the target phrase. Complete algorithm of 3.3.1 is as below: 3.3.2 Expand and Evaluate Attribute Value Collection through Hierarchical which we denote by HRSet . Thus we can expand the attribute value collection via the an accurate one. So the attribute values generated through it will be given a weight. 
If we have extracted that co ncept A has the attribute value Attr(B)[i] , we increase the weight of this attribute value by the product of weight of Attr(B)[i] and Hyponym(A, B) ; else, we create a new attribute value Attr(B)[i] for concept A, and set the product of weight of Attr(B)[i] and Hyponym(A, B) as the weight of it. 4.1 Experiment Settings We use Attribute values extraction based on WWW as our baseline 1 and DIPRE as our baseline2. DIPRE is a method presented by Alex Mayers. This paper choose ontology in modern medicine domain which bases on Medical Subject Headings (MeSH) compiled by U.S. National Library of Medicine. There are 20,000 classes in the ontology and each class has attributes: Name, Definition, Code, Symptom, Laboratory examination, Pathogeny and so on. We choose 100 classes as our test set.We retrieve from the WWW related to an ontology concept via Google API. Only the first 100 documents are used for the attribute values extraction. 4.2 Results 4.2.1 Baseline 1: Attribute Values Extraction Based on WWW The attribute value extraction task is carried out separately. 4.2.2 Baseline 2: DIPRE The hierarchical relations extraction task is carried out separately. 4.2.3 Integrated Approach The two tasks are done together in an integrated approach. The result shows that our approach improves the F-value by 2.32% from the Baseline 1. The result shows that our approach improves the Precision by 12.38% from the Baseline 2. 
We get the best result when  X  given the value 0.75. This paper proposes an integrated appr oach. The candidate attribute values of concepts are used to evaluate the extracted hierarchical relations. The hierarchical relations are used to expand the candidate attribute values of concepts. We introduce the weight into the iteration to ensure the accuracy of results. Our experiments show performance improvements in both tasks. Acknowledgments. This work is supported by NSFC Project 60503071, 60873156, 863 High Technology Project of China 2006AA01Z144 and 973 Natural Basic Research Program of China 2004CB318102.
 The enormous amount of information on the web have been forming an in-frastructure for the so-called knowledge society. Most of such the information is expressed in text at the present time. Hereafter, it is expected that not only texts but also audio, video or other mixed media will be utilized as the knowledge resources. Above all, speech is a kind of language media, and produced daily. In fact, the amount of spoken documents being produced is overwhelmingly more than that of written documents. If the speech such as discourse, lecture, mono-logue, conversation and discussion are accumulated on the web, the amount of sharable information in the knowledge society would get much larger.
 ered. Though a style of uploading speech data would be very simple and would have high feasibility, its significance from a viewpoint of reusability is not so large. If the transcribed text is also archived in addition to the speech data, its effectiveness would be increased in terms of accessibility. On the other hand, because of high redundancy of spontaneous speech, the transcribed text in it-self is not readable on an Internet browser, and therefore not suitable as a web document.
 documents for the purpose of building a speech archiving system. The tech-nique edits automatically transcribed texts and improves its readability on the browser. The readable texts can be generated by applying language technologies such as paraphrasing, segmentation and structuring to the transcribed texts. An edit experiment was conducted by using lecture speech data, and the result has shown the feasibility of the technique. Furthermore, a prototype system of spo-ken document archiving was implemented in order to confirm its effectiveness. archiving on the web. Section 3 describes an edit technique of spoken docu-ments. Section 4 reports evaluation of the technique. Section 5 describes our speech archiving system. We have aimed at constructing an environment where spoken documents are archived as web documents. In general, a web document is described by the markup language such as HTML, and created in consideration of the readability on a browser. In that sense, a spoken document is not suitable for being read with an Internet browser. Therefore, a transcribed text needs to be converted into an HTML text.
 have constructed a system named ViaScribe in order to improve accessibility of speech data [1]. ViaScribe targets lecture environments and creates a learning material into which audio, video, slides and transcribed texts are integrated. The learning material is uploaded to the Web, then the participants can access it with a Internet browser. ViaScribe inserts linefeeds into the transcribed text at places where pauses occur in order to improve the readability. Although Bain et al. target English speech, redundancy in spoken documents is larger in Japanese speech. Therefore only the linefeed insertion cannot achieve enough readability. Shibata et al. have proposed a method of automatically generating summary slides from a text [2]. They have clearly specified the relations between sentences or clauses by itemization. However, their research differ from ours in that the units such as words or phrases are not targets. Furthermore, treating the written language, they do not consider the features of spoken language.
 automatic summarization [3, 4, 5, 6] and conversion from spoken language to written language [7]. However, they are all conversions between plain texts. In contrast, we propose a method for converting it into a web document, which can be utilized as a fundamental technology in the archiving system. The conversion of spoken documents into web documents is executed by editing spoken sentences so that they become easy to read. The conversion is realized by performing paraphrasing of expressions, segmentation into short sentences and structuring of a sentence in sequence. 3.1 Paraphrasing In spoken language, there are a lot of expressions which are not proper as written language expressions. Such expressions can be converted into readable expres-sions by paraphrasing them into other expressions.
 tion, according to the kind of target expressions. In deletion , redundant expres-sions particular to spoken language are removed. There exists not only fillers (uh, er etc.) and hesitations but also other redundant expressions. (1)  X  X  X  X   X  X  X  X  X  X  X   X  X  X  X  X  X  X  (the thing which is called normalization is  X   X  X  X  X  X  X  X  (the thing which is called) X  is redundant, and the readability is improved by removing it. In replacement , expressions which are not proper as written language are replaced with other expressions. For example, in the case of (2)  X  X  X  X  X  X  X  X  X   X  X  X  X  X  X  X  X  (I read Japanese books, etc.)  X   X  X  X  X  (etc.) X  is an expression particular to spoken language. By replacing  X   X   X  X  X  (etc.) X  with  X   X  X  (etc.) X , it becomes a proper expression. Furthermore, in honorific expressions, polite expressions, and so on, there are several cases where replacement is needed. In insertion , expressions which do not appear in spoken language are added.
 created paraphrasing rules by using the actual lecture speech data [8]. Fig. 1 shows examples of the rules and their applications. Here, in insertion, since considering the meaning is required, it is difficult to create the rules by only surface information. Therefore, the target of this paper is limited to deletion and replacement. 3.2 Segmentation In sentence segmentation, one sentence is divided into two or more sentences. In the case of a long sentence, the structure of the sentence is simplified and thus it becomes easy to read. In segmentation, a sentence is divided at any of word boundaries, and the final bunsetsu 1 of each divided sentence is transformed into a proper expression as the sentence-end bunsetsu. Fig. 2 shows an example of sentence segmentation. (3)  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  In this example, by inserting a sentence boundary between  X   X  X  X  X  X  X  X  (Though I think that) X  and  X   X  X  X  X  X  X  X  X  X  X  (I X  X  adopted, so) X  and transforming  X   X  X  X   X  X  X  X   X  into  X   X  X  X  X  X  X  (I think that) X , the following sentence is generated: (4)  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  For analysis about the place, we divided sentences in the transcribed lecture speech text by hand. About 89% of 299 sentence divisions are performed just after a conjunctive particle  X  X eredomo X ,  X  X a X ,  X  X ode X ,  X  X hi X ,  X  X o X ,  X  X ara X , or renyou-tyuushi bunsetsu. Here, renyou-tyuushi bunsetsu is a bunsetsu of which the morphological information is one of the following two cases: adverbial form (e.g.  X   X  X  X  , X   X   X  X  X   X ) and adverbial form + conjunctive particle  X  X e X  (e.g.  X   X  X  X   X  , X   X   X  X  X  X   X ) .
 correspond to a predicate, they often become the ends of semantically meaning-ful units. In our research, these are considered to be the candidates of sentence boundaries. However, a sentence can not necessarily be divided at all the candi-dates of division points. In some cases, the meaning of the original sentence may be lost by dividing at the candidate. Therefore, we established the condition of sentence segmentation based on dependency structure. That is, a sentence is segmented at a division point if no bunsetsu, except for a bunsetsu located just before the point, depends on some bunsetsu located after the point. 3 . 3 S t r u c t u r i n g In our research, we structure a sentence to make it easy to read. Concretely speaking, parallel structures are detected, and displayed by being itemized. The parallel structures in a Japanese sentence can be detected based on the depen-dency structure. We use JUMAN [10] as morphological analysis and KNP [9] as dependency parsing.
 (5)  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  (Now, I recognize the In this sentence, noun phrase  X   X  X  X  X  X  X  X  X  X  X  X  X  X  X  (the actions taken by our country) X  and  X   X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  (the pains of other country peoples caused by them) X  have a parallel relation with each other. 3 . 4 E x a m p l e o f S C C C C e n t e n c e o C C C C n v e r s i o n As an example of text edit by our technique, we describe the edit process of the following sentence: (6)  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  First, in paraphrasing processing, paraphrasing rules are applied to the under-lined parts. (7)  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X   X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  Second, in segmentation processing, the sentence is divided into three sentences by slashes right after the underlined bunsetsus. (8)  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  //  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  EU  X  Last, in structuring processing, our technique identifies the parallel phrases by detecting bunsetsus which have parallel relation with each other, based on the dependency structure of the second sentence, and then, the parallel phrases are itemized and displayed. (9)  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  EU  X  X  X   X  X  X  X  X  X  X  X  X  X  X  X  X  X   X  X  X  X  X  X  X  Fig. 4 shows the conversion of the transcribed text into the edited text. We can see that it becomes easy to read by the conversion. To evaluate our method, we conducted an experiment on text conversion. As the experimental data, we used the transcribed data of five Japanese lectures in the simultaneous interpretation database [8]. We created the correct data of text conversion by editing each sentence in the above-mentioned data by hand. Evaluation is conducted based on each precision and recall for paraphrasing, segmentation and structuring. In addition, the precision is defined as the ratio of the correctly edited places to the places edited by our method, and the recall is defined as the ratio of the correctly edited places to the manually edited places in the correct data.
 paraphrased. The precision and recall were 72.5% and 66.5%, respectively. As the result of error analysis, we found out that redundant expressions were not fully deleted. This is because the paraphrasing rules for sentence-end expressions which contain redundant expressions are not fully created.
 recall of segmentation were 50.5% and 59.3%, respectively. Our segmentation technique does not take into account the length of a sentence. Therefore, even if an original sentence is short or each sentence generated by dividing the original one becomes too short, our method segments the original one. In that case, the readability is not necessarily improved by our segmentation.
 and recall were 32.0% and 25.9%, respectively, both were low. A possible reason is the problem with the accuracy of morphological and dependency analysis.
 Especially the dependency accuracy on a sentence including parallel structures was not high, and then it materially affected the performance of structuring. We implemented a speech archiving system named RISA according to our tech-nique which has been explained in section 3. The system converts recorded spo-ken documents into web content for the purpose of their circulation on the Inter-net. The content consists of five components: text, sound, video, keyword, and search interface (as shown in Fig. 6). The texts are created by executing linefeed insertion and topic segmentation after text editing. They are saved as HTML files. We used the speech synthesis software HitVoice [11] for generating sound content, and TVML (TV program Making Language) [12], which is provided by NHK Science &amp; Technical Research Laboratories, for video content generation. The users can directly access to the subcontents because text, sound and video are aligned in a fine-grained fashion. The important words in the text are pre-sented as keywords. The search component outputs the corresponding topic to the query input. The set of keywords enables broad viewing of the content of a spoken document. The links of each topic enable topic-by-topic browsing and are suitable for selective reuse. This paper has proposed a technique for converting speech transcripts into Web texts for lecture contents archiving. The technique recognizes speech and edits the transcribed text. Paraphrasing, segmentation, and structuring enable re-moval of unsuitable phrases and redundant expressions in written language. The experiment using lecture speech has shown the effectiveness of the technique. The speech archiving system was implemented to confirm its feasibility.
 Acknowledgements. This research was partially supported by the Grant-in-Aid for Scientific Research (B) of JSPS and by The Asahi Glass Foundation.
 Clustering description is one of the key issues in document clustering application. The solve the problem of document clustering description and to meet the users X  special needs. Document clustering description is to label the clustered results of document information requirements or not. Therefore, labeling a clustered set of documents is an important and challenging work in document clustering applications. 
To resolve the problem of the weak readability of the traditional document cluster-ing results, a method of automatic labeling document clusters based on machine learn-ing is put forward. The authors introduce clustering description extraction in applica-five models respectively, and compare their performance. 
The rest of this paper is organized as follows. The next section reviews some related work on extraction of document clustering description. In section 3, a detailed description of the proposed approach is presented. Subsequently in section 4, the authors introduce clustering description extraction in application to topic digital library building firstly. The proposed approach is then evaluated. The paper is con-cluded with the conclusion and future wok in section 5. Clustering description algorithms can be divided into two categories which are man-ual description methods and automatic description methods. Glenisson &amp; Gl X nzel [1], Lai &amp; Wu [2] extracted clustering description manually. Automatic methods mainly extract important words from the clustering set of documents. Different clustering algorithms employ different ways to calculate the importance of words [3]. 
Cutting &amp; Karger used term frequency as the weight of word and select words with high weight as clustering description [4] [5]. Muller and Dorre used the top N words as clustering description [6]. Anton &amp; Crof t [7] used key phrase as classification de-scription. In their opinion, phrase-based cluster representations are superior compared maximal-length phrases with high frequency and phrases with the highest TF*IDF value in the clustering set to be clustering description respectively. Glover &amp; Pennock [10] used statistical model based on parent, child and self features. 
Tseng &amp; Lin [3] used WordNet as an external resource to extract category words to be clustering description symbols. Pucktada &amp; Jamie [11] used statistical information of descriptive words in cluster, super cluster and document set comprehensively to score the descriptive ability of descriptive words and get label of every cluster. Dawid [12] proposed that DCF ( Description Comes First can be applied to solve the prob-lem of weak readability and interpretability. To solve the problem of the weak readability of document clustering result, the re-quirement of clustering description is analyzed. Some methods are used to extract the clustering description. In this paper, clustering description problem is transformed into classification problem, which need to cl assify or rank candidate category descrip-tion words. Then, these words can be separated into clustering description words and non-description words. Three machine learning methods i.e. support vector machine model (SVM), multiple linear regression model (MLR) and Logistical regression model (Logit), are used to solve clustering description problem. 3.1 Basic Requirements of Clustering Description The basic requirements of clustering description should include the following contents: (1) Conciseness. Conciseness means that clustering description should be as short as possible, but sufficient enough to convey the topic of the cluster [12]. The simplest measurement of conciseness of descriptive words is the length. It can be measured by forward that the conciseness problem of clustering description can be solved by combing minimum description length principle and maximum description accuracy principle. (2) Comprehensibility. Comprehensibility indicates ability to map clustering de-parency by Dawid [12], while Krishna [14] calls it the productiveness of labels. (3) Accuracy. Accuracy means that clustering description should reflect the topic of the corresponding cluster. Different from ke yword extraction, object of clustering description is not individual document but multiple documents in the cluster with the same subject. Tseng &amp; Lin [3] use correlatio n coefficience and its variation to meas-ure the correlation degree between descriptive words and cluster. (4) Distinctiveness. Distinctiveness means that descriptive words must frequently appear in clusters it represents and rarely appear in other clusters. Hanan &amp; Mohamed [15] propose a modified TF*IDF method to compute the distinctiveness of descriptive words. Pucktada &amp; Jamie [11] also propose a similar method to measure the distinctiveness of descriptive. 
Some global features (e.g. frequency of descriptive words in current cluster, a modified TF*IDF and average value of first position of descriptive words appeared in current cluster) are used to measure accuracy and distinctiveness of descriptive words. 3.2 Two Benchmark Models To compare the performance of clustering description algorithms based on machine learning, two benchmark models, i.e. BaseLine1 (denoted as BL1) and BaseLine2 (denoted as BL2) are proposed respectively. 
The first benchmark model uses center vector (which is also called Concept Vec-tor) of clusters as descriptive words. In this paper, top 5 words with the high weights in concept vector will be selected. 
The second model is based on a heuristic method. According to the requirements of clustering description, the importance of candidate descriptive words is evaluated with several features which are Document Frequency (DF), Inverse Cluster Frequency (ICF) and length of words (LEN). 3.3 Machine Learning Models In this paper, three statistical machine learning models, i.e. multiple linear regression model (denoted as MLR), Logistical regression model (denoted as Logit), and support vector machine model (denoted as SVM) will be applied in clustering description extraction respectively. Vapnik proposed support vector model in 1995 to solve the all based on SVM light [17]. 3.4 Feature Selection of Clustering Description of clustering description extraction are shown in table 1. First_Para
Word denotes descriptive word itself. LEN is length of descriptive word and POS is the part-of-speech of it. DF*ICF is document frequency multiplies inverse cluster frequency of descriptive word. ) IDF(t * TF ij is average value TF*IDF (before cluster-cluster. Position_global denotes global position feature, namely document ratio of descriptive word appearing in title, abstract, heading, and first paragraph and last paragraph in current cluster. 
Table 1 shows 12 features and their normalized computation methods used in the model of BL1  X  BL2  X  MLR  X  Logit and SVM respectively 3.5 Training and Testing of Model 3.5.1 The Construction of Model Training Set In this section, training sets are built for MLR  X  Logit and SVM respectively. For verify the three models with 10-fold cross-validation. In this paper, training and test-ing of SVM are all based on SVM light [17]. Table 2 shows a sample of training data for SVM model. 3.5.2 Testing the Model For BaseLine1, we use top 5 words in concep t vector as clustering description words. For BaseLine2, we use top 5 words as clustering description words. We use training set of MLR, Logit and SVM model to train them and get training models respectively. Then, we calculate the weight of every descriptive word in test cluster using the train-ing model and top 5 words to be cluster description words. construction firstly. Then, we analyze descriptive results of BL1  X  BL2  X  MLR  X  Logit and SVM respectively, and compare their performance. 4.1 Application of Clustering Description Extraction We use document clustering and clustering description extraction to build topic digital library. The hierarchical structure is generated after clustering documents in each the hierarchical structure for user navigation after manual adjustments. Figure 1 shows the hierarchical structure and navigation result of the realty domain. The on-is open and the website address is http://topic.cnki.net. 4.2 Data Sets and Evaluation Method of Clustering Description In this study, we collect documents from database of  X  X nformation Center for Social Sciences of RUC X , which is available at http://art.zlzx.org. We randomly choose 2,000 academic documents from the database. Each document includes the title, ab-stract, keywords, full-text, heading of paragraph or sections, boundary information of paragraphs or sections, references, etc. Then we annotate clustered results ( the num-ber of cluster is 20), and set the category label and its corresponding cluster to be the corresponding cluster. We adopt 10-fold cross-validation to evaluate clustering de-scription models. Precision, recall and F1 value, which are classic evaluation methods in the field of information retrieval, will be applied to evaluate the performance of the method of clustering description. 4.3 Experiment Result and Analysis 4.3.1 Performance of Models Table 3 shows precision, recall and F1 value of clustering results of these five models. 
As shown in Table 3, performance of statistical machine learning is superior to benchmark model in term of precision. Moreover, SVM model is better than MLR model which is superior to Logistic model. This indicates that SVM model can per-form effective classification, and extract effective clustering description in the task of clustering description. BaseLine1 is superior to BaseLine2 which selects cluster de-scriptive words combining DF , I CF and LEN. all the models is around 0.5 which can be further improved. 
For F1 value, SVM model performs the best, and Baseline1 is the worst one among the five models. It indicates that center vect or is inferior to statistical machine learn-ing method when applied in clustering description. Statistical machine learning method can make use of all kinds of features of clustering description. However, traditional methods can only assemble several features together, so its performance is worse than statistical machine learning method. lem. Machine learning models are applied to clustering description and the experi-mental results show that the performance of SVM model is the best. There are two problems to solve in document clustering description extraction: (1) There are few existing human labeled clustering description corpus. At the same time, manually labeling clustering description corpus is costly and time-consuming. (2) Models we discussed in this paper can not obtain satisfying precision and recall, so it X  X  necessary to find more effective clustering description features to enhance the quality of cluster-ing description. Therefore, we X  X l study how to select more effective clustering de-scription features in future. Acknowledgments. This research was partially supported by National Key Project of Scientific and Technical Supporting Programs (No.2006BAH03B02), Project of the Education Ministry's Humanities and Social Science funded by Ministry of Education of China (No.08JC870007). Event has been regarded as an effective concept representation in recently emerged event-based summarization. With regard to the definition of events, in conjunction with the common agreement that event contains a series of happenings, we formulate events as  X  X Who] did [What] to [Whom] [When] and [Where] X  at sentence level. In this paper, we focus on  X  X id [What] X  and define verbs and action nouns in source documents as event terms that characterize or partially characterize event occurrences. 
Notice that in addition to the quality and quantity of the informative contents con-veyed by the extracted sentences, the relations among the extracted sentences, such as temporal relations in news articles, and structure of the final summary text should also be a matter of concern. Sentence relation in source and summary text, if appropriately defined and identified, is a good means to reflect text cohesion, the way of getting the source and extracted text to  X  X ang together X  as a whole and the indicator of text unity. semantic relations existing between not only pairs of words but also over a succession of a number of nearby related words spanning a topical unit of the text. These sequences of related words are called lexical chains and tend to delineate portions of the text that have a strong unity of meaning. Lexical chains have been investigated for extractive text summarization in the past. having a correspondence to the structure of the text. Normally, nouns or noun com-pounds are used to denote the things and compute lexical chains (i.e. lexical chains are normally noun chains). In this paper, we assume that the source text describes a series of events via the set of sentences and take both informative content and structure of the source text into consideration. We look for the critical temporal chain of event terms and use it to represent the source text and to generate the final summary. We concen-trate on verb chains, other than noun chains, aiming to improve event-based summari-zation and lexical cohesion of the generated summary. Here, event terms and event term chain represent informative content and text cohesion, respectively. 
To compute the critical temporal chain, event terms are connected to construct a tem-instances [1]. The DFS-based (Depth-First Search) algorithm is applied in searching the sentences. Sentences with the highest significance scores are extracted to form the final summary. 
The remainder of this paper is organized as follows. Section 2 reviews related work. Section 3 introduces the proposed event-based summarization approach using critical temporal event term chain. Section 4 then presents experiments and discus-sions. Finally, Section 5 concludes the paper and suggests the future work. Daniel et al. [2] pilot the study of event-based summarization. Filatova and Hatzivas-siloglou [3] then define the concept of atomic events as a feature that can be automatically extracted. Atomic events are defined as the relationships between the important named entities. Allan et al. [4] present a list of events within the topic in the order those events are reported and produce a revised up-to-date summary at regular time intervals. Lim et al. [5] group source documents on time slots by the time infor-mation given by newspaper articles or publication dates. They build the local or representative for the time slot. 
The concept of lexical chain is originally proposed to represent the discourse struc-ture of a document by Morris and Hirst [6]. Barzilay and Elhadad [7] first introduce lexical chain in single document summarization. The lexical chains are computed using relatedness of nouns determined in terms of the distance between their occur-rences and the shape of the path connecting them in the WordNet thesaurus. Follow-important concepts from the source document and make lexical chains a computation-ally feasible candidate as an intermediate representation. Reeve et al. [9] apply lexical chain based summarization approach to biom edical text. The concepts were not de-rived from WordNet but domain-specific semantic resources, UMLS Metathesaurus and semantic network.
At present, the applications of temporal information in summarization are mostly based on time information in the source document or publication date. The lexical chains mentioned above are all based on nouns, derived from WordNet or domain specific knowledge base. In this paper, we derive temporal information from the tem-poral relations among event terms and regard the temporal event term chains as the immediate representation of the source document. 3.1 Temporal Event Term Graph Construction In this paper, we introduce VerbOcean, a broad-coverage repository of semantic verb relations, into event-based summarization. Different from other thesaurus like Word-paper, only the happens-before temporal relation is explored. When two events hap-pen, one may happen before the other. This is defined as the happens-before temporal relation in VerbOcean. 
The happens-before temporal relations on a set of event terms can be naturally represented by a graph, called temporal event term graph. We formally define the temporal event term graph connected by temporal relation as G= ( V, E ), where V is a set of event term vertices and E is a set of edges temporally connecting event terms. Fig. 1 above shows a sample of temporal event term graph built from a DUC 2001 document set. 
The temporal event term graph is a directed graph because the happens-before rela-tion in VerbOcean clearly exhibits the cons picuous anti-symmetric property. For ex-ample, we may have some  X  X uestion X  with something and then decide to  X  X riticize X  it for some reason. The event represented by the term  X  X uestion X  happens-before the one represented by the term  X  X riticize X . That is why a directed edge from  X  X uestion X  to  X  X riticize X  appears in Fig. 1. 3.2 Critical Temporal Event Term Chain Identification connected graph. For example, there are eight sub-graphs or components in the graph illustrated in Fig. 1. Among them, a maximal weakly connected component, which contains the maximal number of the event terms, can be found. We assume that the event terms in the maximal weakly connected component reflect the main topic of original documents since such a component normally involves much more connected (i.e. relevant) event terms than any other components on the graph. Referring back to Fig. 1, the maximal weakly connected component contains 118 event terms, while the second largest weakly connected component contains only 8. Note that, for illustra-tion purpose, only a partial maximal weakly connected component is shown in Fig. 1. 
Some maximal weakly connected sub-graphs are cyclic. In such a situation, the edge whose terminal term vertex has the maximal in-degree is removed in order to Anyway, the terminal term can still be reached from other term vertices. The connec-tion remains. 
From the directed acyclic graph, we extract all the source vertices and sink verti-ces. The source vertex is defined as a vertex with successors but no predecessors, i.e. some edges being incident out of it but no edge being incident on it. On the contrary, edges being incident on it but no edge being incident out of it. 
All directed paths in the directed acyclic graph of the maximal weakly con-nected component from each source vertex to each sink vertex are computed with the DFS-based algorithm. The longest path is defined as the critical temporal event term chain and the critical temporal event term chain in Fig. 1 is  X  X e-gard  X  ignore  X  refuse  X  resist  X  counter  X  reverse  X  shift  X  move  X  add X . 
Afterwards, we evaluate the sentences that contain the event terms in the chain and judge which ones should be extracted into the final summary based upon the critical temporal event term chain computation. 3.3 Sentence Selection To apply the critical temporal event term ch ain in summarization, we need to identify the most significant sentences that best describe the event terms in the chain. Consid-ering terms are the basic constitution of sentences, term significance is computed first. Since this paper studies event-based summarization, we only consider event terms and compute the significances of the event terms in the maximal weakly connected com-ponent of a temporal event term graph. 
Two parameters can be used in the calculation of event term significance. One is event term in a temporal event term graph. The degree of a term vertex in the directed graph is the sum of the in-degrees and out-degrees of that term. one sentence containing this term in source documents. We only extract one sentence for each event term to represent the event to avoid repeating the same or quite similar information in the summary. For sentence selection, the sentence significance is com-puted according to the event terms contained in it. 
Based on event term occurrences, the significance of a sentence can be calculated by the following Equation (1). where i TFS and m TFS are the sum of the term occurrences of the i th sentence and the maximum of all the sentences that contain event terms in the critical temporal chain, respectively. 
Alternatively, we can use degrees of event terms to calculate the significance of a sentence. where tively. 
It should be emphasized here that in Equations (1) and (2) the event terms under concern must be the ones in the maximal weakly connected component of the tempo-ral event term graph. We evaluate the proposed summarization a pproach based on the critical temporal event term chain on the DUC 2001 corpus. The corpus contains 30 English document sets. Among them, 10 sets are observed to contain descriptions of event sequences. They are the main concern of this paper. All final summaries are generated in 200 words length. 
The final summary shown in Fig. 2 is generated from the temporal event term graph sentence significance based on the vertex degree. The corresponding source document set is about the topic  X  X hether to exclude illegal aliens from the decennial census and indeed talks about the resident census history, the exclusion announcement, the reasons of the agreement and rejection, and the vote result. More important, the critical tempo-ral event term chain contributes to the cohesion of the final summary. The summary has apparently shows temporal characteristic in sentence sequences, from resident census history, exclusion announcement and reasons for agreement and rejection to Senate vote result at the end. 
To evaluate the quality of generated summaries, an automatic evaluation tool, called ROUGE [10] is used. The tool presents three ROUGE values including uni-gram-based ROUGE-1, bigram-based ROUGE-2 and ROUGE-W which is based on longest common subsequence weighted by the length. 
Table 1 above shows the average ROUGE scores of event term occurrence and tf*idf approaches on the ten selected document sets. The ROUGE-2 score of the event term occurrence approach is comparatively about 39.2% better than the tf*idf ap-proach. Our approach also shows more advantageous than the tf*idf approach on both ROUGE-1 and ROUGE-W. We highlight the ROUGE-2 scores here because we take semantic relevance between the events into account but tf*idf does not. 
Google X  X  PageRank [11] is one of the most popular ranking algorithms. It is a kind of graph-based ranking algorithm deciding on the importance of a node within a graph by taking into account the global informa tion recursively computed from the entire graph. After constructing the temporal event term graph, we can also use the PageR-calculations of sentence significance using PageRank and vertex degree are both based on the links among the vertices in the graph, we compare the ROUGE scores of the critical temporal event term chain based summarization using the vertex degree in sentences selection to those of PageRank-based approach. Table 2 above compares the average ROUGE scores of the two approaches. The ROUGE-1 score of the event term degree approach is about 2.5% above the ROUGE-1 score of the PageRank algorithm while the ROUGE-2 scores of them are quite simi-lar. This is mainly because both the degree and the PageRank approaches take the semantic relevance between event terms into consideration in the calculation of sig-nificances of event terms and sentences. 
Table 3 finally compares the average ROUGE scores of the ten document sets se-lected for the experiments above and the other twenty document sets in the DUC 2001 corpus. The ROUGE-1 average scores of the twenty document sets are much worse occurrence and 17.6% lower using event term degree. The similar conclusions can be drawn on ROUGE-2 and ROUGE-W. This suggests that the proposed event-based approaches indeed can handle those documents describing events or event sequences much better. But they may not suit event irrelevant topics. In this paper, we investigate whether temporal relation between events helps improve performance of event-based summarization. By constructing the temporal event term graph based on the semantic relations derived from the knowledge base VerbOcean and computing weakly connected components, we find that the maximal weakly con-nected component can often denote the main topic of the source document. 
Searching in the maximal weakly connected component with DFS-based algo-chain are supposed to be critical for generating the final summary. In experiments, the significance of these terms is measured by either term occurrence in source docu-ments or the degree in the constructed graph. The ROUGE results are promising. Term occurrence significantly outperforms tf*idf and term degree is also comparative to well-known PageRank. 
In the future, we will introduce the other types of VerbOcean verb semantic rela-tions into event-based summarization. Besides the critical temporal chain in the event semantic computation. We also plan to combine the surface statistical features and the semantic features during the selection of representative sentences in order to generate better summaries. The work described in this paper was supported partially by a grant from the Research Grants Council of the Hong Kong Special Administrative Region, China (Project No. CERG PolyU 5217/07E) and Research Grant from Hubei Provincial Department of Education (No. 500064). Subcategorization refers to certain kinds of relations between words and phrases in a sentence. A subcategorization frame is a statement of what types of syntactic argu-ments a verb (or adjective) takes, such as objects, infinitives, that-clauses, participial clauses, and subcategorized prepositional phrases [1]. Several large, manually devel-oped subcategorized lexicons are available for English, e.g. the COMLEX Syntax [2] with explicitly stated syntactic and semantic information based on Levin X  X  verb classi-fication [5]. It is a hierarchical domain-independent, broad-coverage verb lexicon with mappings to other lexical resources. 
The collection of different subcategori zation frames for other parts-of-speech various types of subcategorization frames co mpared to other parts of speech phrases. There are several reasons for the importance of subcategorization frames for the development of a parser in that language. There are many languages that have no ex-isting parser. So the information acquired fr om the subcategorization frames can im-prove the parsing process. Apart from parsing and dictionary preparation, we can use the acquired subcategorization frames for Qu estion-Answering system to retrieve pre-dictable components of a sentence. The acquired subcategorization frames can also be used for phrase alignment in a parallel sentence level corpus to be utilized for training a statistical machine translation system. 
The description of a system developed for automatically acquiring six verb sub-categorization frames and their frequencies from a large corpus using rules is men-tioned in [7]. Development of a mechanism for resolving verb class ambiguities using subcategrization is reported in [6]. All of these works deal with English. A cross-lingual work on learning verb-argument structure for Czech language is described in [8]. In [9], the method consists of different subcategorization issues that may be con-sidered for the purpose of machine aided translation system for Indian languages. 
The present work deals with the acquisition of verb subcategorization frames of a specific verb from a Bengali newspaper corp us. The subcategorization of verbs is an essential issue in parsing for the free phrase order languages such as Bengali that has no existing parser. In this paper, we have developed a system for acquiring subcatego-rization frames for a specific Bengali verb that occurs most frequently in the Bengali news corpus. Using a Bengali X  X nglish bilingual lexicon [10], the English verb mean-ings with its synonyms have been identified for the Bengali verb. All possible acquired frames for each of the English synonyms for the Bengali verb have been acquired from the VerbNet and these frames have been mapped to the Bengali sen-tences that contain the verb tagged as main verb and auxiliary. It has been experimen-tally shown that the accuracy of the frame acquisition process can be significantly improved by considering the occurrences of various argument phrases and relevant POS tags before and after the verb in a sentence. Evaluation results with a test set of 1500 sentences show the effectiveness of the proposed model with precision and re-call values of 85.21% and 83.94% respectively. 
The rest of the paper is organized as follows. Section 2 describes the framework for the acquisition of subcategory frames for a specific Bengali verb. Evaluation re-sults of the system are discussed in section 3. Finally section 4 concludes the paper. We developed several modules for the acquisition of subcategorization frames from the Bengali newspaper corpus. The module s consist of POS tagged corpus prepara-tion, Verb identification and selection, English verb determination, VerbNet frames acquisition and Bengali Verb Subcategorization Frame Acquisition. 2.1 POS Tagged Corpus Preparation In this work, we have used a Bengali news corpus [11] developed from the web-archive of a widely read Bengali newspaper. A portion of the Bengali news corpus containing 1500 sentences have been POS tagged using a Maximum Entropy based POS tagger [12]. The POS tagger was developed with a tagset of 26 POS tags (http://shiva.iiit.ac.in/SPSAL2007/iiit_tagset_guidelines.pdf), defined for the Indian languages. The POS tagger demonstrated an accuracy of 88.2%. We have also devel-oped a rule-based chunker to chunk the POS tagged data for identifying phrase level information during the acquisition process. 2.2 Verb Identifi cation and Selection We have partially analyzed the tagged and chunked data to identify the words that are tagged as main verb (VM). The identified main verbs have been enlisted and passed through a stemming process to identify their root forms. Bengali, like any other Indian languages, is morphologically very rich. Different suffixes may be attached to a verb depending on the different features such as Tense, Aspect, and Person. The stemmer uses a suffix list to identify the stem form. Another table stores the stem form and the corresponding root form for each verb. 
The specific root verb that occurs most frequently in any inflected form (as main verb with VM POS tag) is taken up for fine-grained analysis of verb subcategorization each subcategorization frame of the verb. We have chosen the verb ( dekha ) (see) from the corpus that has the largest numb er of occurrences in the corpus. 
The specific root verb, selected in the way as described above, has been considered as the target verb in our work. The sentences containing the target verb including their verb appears as an auxiliary verb (VAUX). These sentences are kept separate for acquisition. 
I them mangoes eating have seen 2.3 English Verb Determination The verb subcategorization frames for the equivalent English verbs (in the same sense) are the initial set of verb subcategorization frames that are considered as valid for the Bengali verb. This initial set of verb subcategorization frames are validated in the POS tagged corpus. The process describe d in section 2.2 already identifies the root form of the target verb. To determine equivalent English verbs, we have used the available Bengali-English bilingual dictionary that has been preprocessed for our task. Various syntactical representations of a word entry in the lexicon are analyzed to identify its synonyms and meanings with respect to verb only. The example of an en-try in the bilingual lexicon for our target verb ( dekha ) is given as follows. [ d  X  kh  X  ] v to see, to notice; to look; a. seen, noticed adv. in imitation of. The above lexicon entry for the Bengali word shows that it can have three different tries that appear with the verb POS. Different synonyms for the verb having the same sense are separated using  X , X  and different senses are separated using  X ; X  in the lexi-con. The synonyms including different senses of the target verb have been extracted from the lexicon. This yields a resulting set called Synonymous Verb Set (SVS). For example, the English synonyms ( see , notice ) and synonym with other sense ( look ) are selected for Bengali verb ( dekha ). Now, the task is to acquire all the existing possible frames for each member of the SVS from the VerbNet. 2.4 VerbNet Frames Acquisition VerbNet associates the semantics of a verb with its syntactic frames, and combines traditional lexical semantic information such as thematic roles and semantic predi-cates, with syntactic frames and selectional restrictions. Verb entries in the same VerbNet class share common syntactic frames, and thus they are believed to have the same syntactic behavior. The VerbNet files contain the verbs with their possible sub-category frames and membership information is stored in XML file format. 
The XML files of VerbNet have been preprocessed to build up a general list that contains all members (verbs) and their possible subcategorization frames (primary as well as secondary) information. This prep rocessed list is searched to acquire the sub-categorization frames for each member of the SVS of the Bengali verb ( dekha ) (iden-frames obtained for the members of its SVS. It has also been observed that the mem-bers of the SVS also occur in separate classes of the VerbNet depending on their senses. The acquired frames (primary and secondary) for each member of the SVS of the verb ( dekha ) have been enlisted based on their occurrences in the VerbNet classes as shown in Table 1. Stimulus is used by verbs of perception for events or objects that elicit some response from an experiencer. This role usually imposes no restrictions. Theme is used for participants in a locatio n or undergoing a change of location. The lus, are defined as the secondary frames. 2.5 Bengali Verb Subcategorization Frame Acquisition The acquired VerbNet frames have been mapped to the Bengali verb subcategoriza-nature with other phrases in Bengali sentences. The NNPC (Compound proper noun), NNP (Proper noun), NNC (Compound common noun) and NN (Common noun) tags verb. 
In simple sentences the occurrence of the NNPC, NNP, NNC or NN tags preceded by the PRP (Pronoun) NNP, NNC, NN or NNPC tags and followed by the verb gives similar frame syntax for  X  X asic Transitive X  frame of the VerbNet. ( ami )(PRP) ( kakatua )(NNP) ( dekhi )(VM) 
The syntax of  X  X HAT-S X  frame for a Bengali sentence has been acquired by identifying the sentential complement part of the verb ( dekha ). The target verb followed by a NP chunk that consists of another main verb and WQ tag (question word) helps to identify the  X  X HAT-S X  kind of frames. In order to acquire the frame of  X  X P-ING-OC X , we have created the list of possible Bengali inflections that can appear for the English  X -ING X  inflection. These inflec-tions usually occur in sentences made up of compound verbs with conjunctive partici-these inflections followed by the target verb then it gives a similar description of the VerbNet frame  X  X P-ING-OC X . ( ami )(PRP) (NP) (( tader ) ( haste )) ( dekhechi ) 
The presence of JJ (Adjective) generally does not play any role in the acquisition process of verb subcategorization frames. Some frames like  X  X ttribute Object Posses-sor-Attribute Factoring Alternation X ,  X  X OW-S X ,  X  X heme-PP X  and  X  X timulus-PP X  did not have any instance in our corpus. A close linguistic analysis shows that these frames can also be acquired from the Bengali sentences. The set of acquired subcategorization frames or the frame lexicon can be evaluated against a gold standard corpus obtained either through manual analysis of corpus data, or from subcategorization frame entries in a large dictionary or from the output of the parser made for that language. As there is no parser available for the Bengali and also no existing dictionary for Bengali that contains subcategorization frames, manual analysis from corpus data is the only method for evaluation. The sentences retrieved from the chunker (with an accuracy of 89.4%) have been evaluated manually to ex-tract the sentences that are fully correct. Th ese sentences have been considered as our gold standard data for evaluation of subcategorization frames. 
A detailed statistics of the verb ( dekha ) is presented in Table 2. Stemming process has correctly identified 276 occurrences of the verb ( dekha ) from its 284 occurrences in the corpus with an accuracy of 97.18%. During the Bengali verb subcategorization frame acquisition process, it has been observed that the simple sentences contain most simple Bengali sentence to identify the verb subcategorization frames is easier in the absence of a parser than analyzing complex and compound sentences. 
The verb subcategorization frames acquisition process is evaluated using type preci-sion, type recall and F-measure. The results have been shown in Table 3. The evaluation Total no. of occurrences of the verb ( dekha ) (before stemming) in the corpus 284 No. of sentences where ( dekha ) occurs as a verb 276 No. of sentences where ( dekha ) occurs as a main verb (VM) 251 No. of sentences where ( dekha ) occurs as an auxiliary verb(VAUX) 25 No. of simple sentences where ( dekha ) occurs as a verb 139 No. of simple sentences where ( dekha ) occurs as a main verb (VM) 125 
No. of simple sentences where ( dekha ) occurs as an auxiliary verb (VAUX) 14 process has been carried out in two stages. In Stage-1, we have retrieved whatever result we acquired from the corpus and have identified the frames keeping the phrases and rences in the sentence. This chart is shown in Figure 1. Based on these values, various rules have been applied. Results show that the recall value is not changed so much but there is an appreciable change in the precis ion values after considering the occurrences of different chunks and tags to select the arguments of the verb. 
Number of different types of frames acquired is shown in Table 4. The result shows a satisfactory performance of the system. The acquisition of subcategorization frames for more number of verbs and clustering them will help us to build a verb lexicon for Bengali language. There is no restriction for domain dependency in this system. For the free word order languages like Ben-gali, verb morphological information, synonymous sets and their possible subcatego-rization frames are all important informatio n to develop a full-fledged parser for Ben-gali. This system can be used for solving alignment problems in Machine Translation for Bengali as well as to identify possible argument selection for Q &amp; A system. With the development of the Internet, the volume of information is now incomparable can be created through human cognitive ability to classify things. However, human cognitive ability is not each individual's specific ability. Everything in the world has its understand things more easily by associating them to related categories. In this way, we simplify information processing and understand perceived things better by classifying them systematically according to their characteristics. 
The lexical classification systems covered in this study have been developed into various types according to the use of words or information. There are examples of mation technology: information retrieval, knowledge management, information sys-tem design, ontology building, machine translation, and dictionary compilation. There are also implemented lexical classification systems related to the vocabulary re-sources of natural languages such as Roget's Thesaurus[1], WordNet[2], Lexical FreeNet[3], Kadokawa Thesaurus[4], and EDR[5]. Cases of ontology building include KR Ontology[6], CYC Ontology[7], Mikrokosmos Ontology[8], SENSUS Ontolo-gy[9] and HowNet[10], and there are business applications of ontology such as Enter-prise Ontology[11], UMLS[12], UNSPSC[13], RosettaNet[14], ISO 2788[15] and ANSI Z39.19[16]. 
Lexical classification is concept classification by nature. Lexical classification sys-tems mentioned above suggest that there are various concept classification systems today. It is said that people have the same cognition, memory, causal analysis, catego-As mentioned above, in the current situation that various concept classification sys-concept classification systems and intelligent information systems. In response to the demand, research is being made on ontology mapping, merge and integration, and semantic integration[18, 19]. The main research method is utilizing shared ontologies or finding mappings in ontological features. 
However, if there is a general concept classification system ( interontologia ) as a reference system, it will become more systematic and easier to integrate concept present study examined the relevancy of lexical categorization between the Thousand-Character Text [20, 21], which is a representative Eastern classic, and Roget's Thesau-rus[1], which is a famous Western classified lexicon. Through this study, we analyze similarity between the two in categorization and classification. The Thousand-Character Text(  X  X  X  X  ) was written by Zhou Xingsi(  X  X  X  X  ) by order of Emperor Wu(  X  X  X  ) in the Liang(  X  ) Dynasty of China in around the 6th century, tive classical Chinese textbook used widely to teach children. The oldest Thousand-Character Text annotated with pronunciation and meaning in Korean is the version of Han Seok-Bong(  X  X  X  X  ) published in 1583. There is also a record on an earlier ver-sion published in Gwangju, Korea in 1575. The Thousand-Character Text is old four-character verse composed of a total of 250 four-character phrases or 125 couplets and its materials are Chinese history, culture, etc. [20, 21]. Roget's Thesaurus [1] was first published in 1852 by English surgeon Peter Mark Roget. This is the first synonym/antonym dictionary. Roget's Thesaurus is not in meaningless alphabetical order. The thesaurus classifies lexical knowledge systemati-cally. The top hierarchy is composed of 6 classes, under which are divisions. Each division is again subdivided into sections. In each hierarchy is unique entry informa-pressed in the form of "Vocabulary &amp;c. (Entry word) Entry number. X  
There is a study on lexical classification systems in Korean representative classics such as the Thousand-Character Text, Yuhap(  X  X  X  ) and Hunmongjahoi(  X  X  X  X  X  X  ) [20]. In the study, they argue that the Thousand-Character Text is structured well and has a clear system. In addition, they emphasize the accuracy of its classification sys-tem that does not allow even a repetition of the same character among the 1000 characters. Largely according to semantic pa ragraphs, they classify concepts in the Thousand-Character Text as follows: astronomy, nature, royal task, moral training, loyalty and filial piety, virtuous conducts, five moral disciplines, humanity and jus-tice, palace, meritorious retainers, feudal lo rds, topography, agriculture, mathematics, in presenting Chinese characters by semantic paragraph, the Thousand-Character Text arranges basic Chinese characters appropriately and is outstanding in terms of lexical system and the perception of basic Chinese characters. This study has conducted analysis on concept relevancy between the Thousand-Character Text and Roget's Thesaurus through six steps as follows. Step 1. Index the Thousand-Character Text and Roget's Thesaurus, and build da-tabases: Sort out Chinese characters from the Thousand-Character Text, and words from Roget's Thesaurus. Then build the master databases of the Thousand-Character Text and Roget's Thesaurus. sand-Character Text and build a database: Determine English words representing mean-ing of Chinese characters in Thousand-Character Text, referring to Chinese-English dictionary Kingsoft2008[22], and Classical Chinese Character Frequency List[23]. 
Step 3. Choose English words with the closest meaning to Chinese characters in the Thousand-Character Text: Determine one English word representing meaning of each Chinese character in the Thousand-Character Text through a drop-down list. 
Step 4. Map Chinese characters in the Thousand-Character Text to Roget's Thesau-rus categories: Choose Roget's Thesaurus categories of the English words representing Chinese characters in the Thousand-Character Text through a drop-down list. Step 5. Analyze the results of mapping Chinese characters in the Thousand-Character Text to Roget's Thesaurus: Verify Chinese characters in the Thousand-Character Text classified into the Roget's Thesaurus categories. 
Step 6. Map Chinese characters in the Thousand-Character Text to Roget's Thesau-rus sections: Classify Chinese characters in the Thousand-Character Text into Roget's Thesaurus sections, and make a correlation analysis. Fig. 1 is the distribution chart of Chinese characters in the Thousand-Character Text mapped to Roget's Thesaurus categories. Among the 1,044 categories of Roget's Thesaurus, 424 categories have one or more corresponding Chinese characters while 620 categories do not have any corresponding Chinese characters. Thus, the overall mapping rate is about 41%. Although the mapping rate is somewhat low, Fig. 1 shows that the mapping is distributed over the range of all categories. 
We have analyzed the mapping on the section level of Roget's Thesaurus, which are higher categories in the hierarchy of the thesaurus. Fig. 2 shows the result of sec-tion level analysis. Among a total of 39 sections in Roget's Thesaurus, only one does not have any corresponding Chinese character, but 38 sections have one or more. The overall mapping rate is as high as 97%. Table 1 compares the mapping on the catego-ry level with the one on the section level of Roget's Thesaurus. 
Based on the results of mapping presented above, we examine the correspondence to Chinese characters in the Thousand-Char acter Text on the section level. Except Chinese characters in the Thousand-Character Text. The mapping rate is relatively Chinese characters implying non-material and metaphysical meanings show high correspondence. 
No Class Sections Roget's Categories Chinese Characters Mapping Rate Rank 02 1 relation 18 9 0.50 6 05 1 number 23 9 0.39 23 07 1 change 13 6 0.46 11 09 2 space in general 13 8 0.62 2 16 4 17 4 19 4 reasoning processes 4 1 0.25 30 25 4 31 5 32 5 33 5 
Table 2 shows the correspondence between the Thousand-Character Text and Ro-get's Thesaurus on the section level. We can see that the number of Chinese charac-ters changes as the number of Roget's Thesaurus categories does. We have obtained characters in the Thousand-Character Text and Roget X  X  Thesaurus categories on the section level. 
In the correlation analysis, the number of Roget categories is defined as X , the For Equation (1), the values of variables are as follows. If these values are substituted for the variables in Equation (1), we obtain r xy = 0.94, showing quite a high correlation between the Thousand-Character Text and Roget's Thesaurus on the section level. The present study have examined concept relevancy between the Thousand-Character Text and Roget's Thesaurus. The correlation analysis suggests that the experimental data have very high relevancy. From the result of our experiment, we may say that there is an interontologia behind Roget's Thesaurus and the Thousand-Character Text. Tasks for future research include: (1) complementing omitted parts in mapping of Chinese characters in the Thousand-Character Text to Roget's Thesaurus categories with the 1800 commonly used Chinese characters in Korea and comparing the results; and (2) analyzing difference between comparison of the Thousand-Character Text and Roget's Thesaurus on the category and section levels. From these studies, we expect to have a set of Chinese characters for a refined lexical knowledge classification sys-tem. Lastly, based on the character set, we will develop a new lexical category system applicable to knowledge classification. Acknowledgments. This work was supported by the Korea Research Foundation Grant funded by the Korean Government. (KRF-2008-313-H00009) 
 Simone Diniz Junqueira Barbosa Phoebe Chen Alfredo Cuzzocrea Xiaoyong Du Joaquim Filipe Orhun Kara Tai-hoon Kim Igor Kotenko Dominik  X  Sle  X zak Xiaokang Yang Volume Editors Ming Zhou Microsoft Research Asia Beijing, China E-mail: mingzhou@microsoft.com Guodong Zhou Soochow University Suzhou, China E-mail: gdzhou@suda.edu.cn Dongyan Zhao Peking University Beijing, China E-mail: zhaody@pku.edu.cn Qun Liu Institute of Computing Technology Chinese Academy of Sciences Beijing, China E-mail: liuqun@ict.ac.cn Lei Zou Peking University Beijing, China E-mail: zoulei@pku.edu.cn ISSN 1865-0929 e-ISSN 1865-0937 ISBN 978-3-642-34455-8 e-ISBN 978-3-642-34456-5 DOI 10.1007/978-3-642-34456-5 Springer Heidelberg Dordrecht London New York Library of Congress Control Number: 2012949755
CR Subject Classification (1998): I.2.7, H.3.1, H.1.2, H.3.3, H.3.5, H.5.2, I.2.1 NLP&amp;CC (CCF Conference on Natural Language Processing &amp; Chinese Com-puting) is the annual conference of the CCF TCCI (Technical Committee of Chinese Information). As a leading co nference in the field of NLP and Chinese computing of CCF, NLP&amp;CC is the premier forum for NLP researchers and prac-titioners from academia, industry, and government in China and Asia Pacific area to share their ideas, research results and experiences, which will highly promote the research and technical innovation in these fields domestically and interna-tionally. The papers contained in these proceedings addres s challenging issues in Web mining, search and ads, social networks, multi-lingual access, question answering as well as the fundamentals and applications in Chinese computing. process, 27 English papers and 16 Chines e papers were selected for presentation as full papers. The acceptance rate is 28%. Furthermore, this year X  X  NLP&amp;CC also included nine posters. The Chines e full papers together with posters are published by ACTA Scientiarum Naturalium Universitatis Pekinensis, and are not included in this volume. This volume contains the 27 English full papers presented at NLP&amp;CC 2012.
 who chose NLP&amp;CC 2012 as a venue for their publications. We are very grateful to the Program Committee members and Organizing Committee members, who put a tremendous amount of effort into soliciting and selecting research papers with a balance of high quality and new ideas and new applications.
 October 2012 Ming Zhou NLP&amp;CC 2012 was organized by the Technical Committee of Chinese Informa-tion of CCF, Peking University, and Microsoft Research Asia.
 Conference Co-chairs JianGuo Xiao Peking University, China Bo Zhang Tsinghua University, China Program Co-chairs Ming Zhou Microsoft Research Asia, China Guodong Zhou Soochow University, China Area Chairs Fundamentals on CIT Houfeng Wang Peking University, China Applications on CIT Zhi Tang Peking University, China Web Mining Jun Zhao Harbin Institute of Technology, China Search and Ads Shaoping Ma Tsinghua University, China NLP for Social Networks Juanzi Li Tsinghua University, China Machine Translation Qun Liu Chinese Academy of Sciences, China Question Answering Xuanjing Huang Fudan University, China Demo Co-chairs Haifeng Wang Baidu corporation, China Yi Guan Harbin Institute of Technology, China Organization Chair Dongyan Zhao Peking University, China Publication Co-chairs Qun Liu Chinese Academy of Sciences, China Lei Zou Peking University, China ADL/Tutorial Chair Chengqing Zong Chinese Academy of Sciences, China Evaluation Chair Xiaojun Wan Peking University, China Financial Chair Zhengyu Zhu China Computer Federation, China Sponsor Chair Dongyan Zhao Peking University, China Website Chair Aixia Jia Peking University, China Yuki Arase Microsoft Research Asia Jiajun Chen Nanjing University Hsin-Hsi Chen National Taiwan University Xiaoqing Ding Tsinghua University Lei Duan Microsoft Bing Yang Feng Sheffield University Jianfeng Gao Microsoft Research Redmond Tingting He Central China Normal University Hongxu Hou Inner Mongolia University Yunhua Hu Microsoft Research Asia Degen Huang Dalian University of Technology Xuanqing Huang Fudan University Long Jiang Alibaba Corp.
 Wenbin Jiang Chinese Academy of Sciences Sadao Hurohash Kyoto University Wai Lam The Chinese University of Hong Kong Mu Li Microsoft Research Asia Baoli Li Henan University of Technology Juanzi Li Tsing University Wenjie Li The Hong Kong Polytechnic University Henry Li Microsoft Research Asia Ning Li University of Science and Technology Beijing QingshengLi AnyangNormalUniversity Shoushan Li Soochow University Dekag Lin Google Hongfei Lin Dalian University of Technology Bingquan Liu Harbin Institute of Technology Changsong Liu Tsinghua University Qun Liu Chinese Academy of Sciences Ting Liu Harbin Institute of Technology Xiaohua Liu Microsoft Research Asia Yang Liu Tsinghua University Yajuan Lv Chinese Academy of Sciences Xueqiang Lv TRS Qing Ma RyuKoku University Yanjun Ma Baidu Jun Ma Shandong University Shaoping Ma Tsinghua University Yuji Matsumoto Nara Institute of Science and Technology Massaki Nagata NIT Jianyun Nie University of Montreal Cheng Niu Microsoft Bing Tao Qin Microsoft Research Asia Liyun Ru Sohu Xiaodong Shi Xiameng University Shumin Shi Beijing Institute of Technology Shuming Shi Microsoft Research Asia Rou Song Beijing Language and Culture University Jinsong Su Xiameng University Zhi Tang Peking University Jie Tang Tsinghua University Junichi Tsujii Microsoft Research Asia Xiaojun Wan Peking University Bin Wang Chinese Academy of Sciences Haifeng Wang Baidu Houfeng Wang Peking University Mingwen Wang Jiangxi Normal University Xiaojie Wang Beijing University of Posts and Wenjun Wang Tianjin University Bo Wang Tianjin University Furu Wei Microsoft Research Asia Yunqing Xia Tsinghua University Jianguo Xiao Peking University Deyi Xiong Singapore I2R Jinan Xu Beijing Jiaotong University Jun Xu Microsoft Research Asia Endong Xun Beijing Language and Culture University Muyun Yang Harbin Institute of Technology Zhengtao Yu Kunming Univers ity of Science and Technology Chengxiang Zhai University of Illinois at Urbana-Champaign Jiajun Zhang Chinese Academy of Sciences Ruiqiang Zhang Yahoo Min Zhang Tsinghua University Min Zhang Singapore I2R Yangsen Zhang Beijing Information Science and Technology Dongdong Zhang Microsoft Research Asia Yujie Zhang Beijing Jiaotong University Shiqi Zhao Baidu Dongyan Zhao Peking University Jun Zhao Chinese Academy of Sciences Tiejun Zhao Harbin Institute of Technology Hai Zhao Shanghai Jiaotong University Guodong Zhou Soochow University Ming Zhou Microsoft Research Asia Jingbo Zhu Northeastern University Qiaoming Zhu Soochow University Qing Zhu Renming University Chengqing Zong Chinese Academy of Sciences Ngodrup Tibet University Turgun Ibrahim Xinjing University Nasun-Urt Inner Mongolia University Tse ring rgyal Qinghai Normal University Organized by China Computer Federation, China Hosted by Peking University Microsoft Research Asia In Cooperation with: State Key Laboratory ACTA Scientiarum Springer of Digital Publishing, Naturalium Universitatis Beijing, China Pekinensis Sina Weibo Tencent Weibo Mingbo Education Technology Personalized Paper Recommendati onBasedonUserHistorical Behavior ........................................................ 1 Integration of Text Information and Graphic Composite for PDF Document Analysis ............................................... 13 Automatic Generation of Chinese Character Based on Human Vision and Prior Knowledge of Calligraphy ................................ 23 A Spoken Dialogue System Based on FST and DBN .................. 34 Sentiment Analysis Based on Chinese Thinking Modes ................ 46 Ontology-Based Event Modeling for Semantic Understanding of Chinese News Story ............................................ 58 Dependency Forest for Sentiment Analysis .......................... 69 Collation of Transliterating Tibetan Characters ...................... 78 Topic Structure Identification of PClause Sequence Based on Generalized Topic Theory ................................ 85 Chinese Semantic Role Labeling with Dependency-Driven Constituent Parse Tree Structure ............................................. 97 Contextual-and-Semantic-Information-Based Domain-Adaptive Chinese Word Segmentation .............................................. 110 Exploration of N-gram Features for the Domain Adaptation of Chinese Word Segmentation .............................................. 121 Fusion of Long Distance Dependency Features for Chinese Named Entity Recognition Based on Markov Logic Networks ................. 132 Learning Latent Topic Information for Language Model Adaptation .... 143 Compact WFSA Based Language Model and Its Application in Statistical Machine Translation .................................. 154 A Comparative Study on Discontinuous Phrase Translation ........... 164 Handling Unknown Words in Statistical Machine Translation from a New Perspective ........................................... 176 Dependency Network Based Real-Time Query Expansion ............. 188 Divided Pretreatment to Targets and Intentions for Query Recommendation ................................................ 199 Exploiting Lexical Semantic Resou rce for Tree Kernel-Based Chinese Relation Extraction .............................................. 213 Research on Tree Kernel-Based Personal Relation Extraction .......... 225 Adaptive Topic Tracking Based on Dirichlet Process Mixture Model .... 237 Answer Generating Methods for Community Question and Answering Portals ......................................................... 249 Summarizing Definition from Wikipedia Articles ..................... 260 Chinese Named Entity Recognition and Disambiguation Based on Wikipedia .............................................. 272 The Recommendation Click Graph: Properties and Applications ....... 284 A Linked Data Generation Met hod for Academic Conference Websites ........................................................ 296 Author Index .................................................. 309
Yuan Wang 1 , Jie Liu 1 , , XingLiang Dong 1 , Tianbi Liu 2 , and YaLou Huang 1 , 2 With the rapid development of the Internet, researchers tend to share and search for papers in Digital Libraries (DLs). Most latest papers first appear on the Inter-net for researchers to search for and to read, which means DLs are stepping into a golden age. Nowadays there are some famous platform providing researchers rapidly sharing academic achievements, such as arXiv.org, sponsored by Cor-nell University and Science Paper Online(www.paper.edu.cn),sponsored by the Ministry of Education of China. However, the number of papers on the Inter-net grows exponentially, bringing the problems of information overload, which makes it difficult for researchers to find useful information efficiently. Faced up with these problems, recommendation technique is one of the most effective means. So far, Elsevier, PubMed and SpringLink have offered recommendation service for their users. These sites offe r paper recommendation that meets users X  personal interests by sending them emails or through RSS subscription. But all the recommendation requires users to state their interests explicitly, either to provide information about their interested categories initiatively.

In this paper, we proposed a persona lized recommendation model based on researchers X  expressions of interest thr ough analysis of their historical behav-ior in which users do not need to specify their preference explicitly. In a paper sharing system, the users are usually researchers from different areas, and they have specific preference fo r certain areas. Therefor e, we hypothesize that the users X  interests can be excavated from their behaviors on the site that are ac-cumulated spontaneously when they browse the pages, which does not need extra provision. By collecting and analyzing users X  behavioral information bear-ing users X  interests, we built a perso nalized recommendation model and choose candidate papers for recommendation. The experiment shows that our recom-mendation model based on users X  behaviors improves the accuracy of paper recommendation.

For newly registered and inactive users whose behavioral information is scarce and easy to be noisy, we cannot get thorough knowledge about their preference, so it X  X  hard to provide service for them well. Meanwhile, such as user A and user B share same preference, user B a nd user C have close preference, but A and C share a little same content or papers. So it is hard for us to find some correlation between A and C ,which ignorin g potential association between the two . To solve this problem, we further optimize our model, which will be showed in detail in Section 3. In the experiment section, we discussed the optimization of our model.

The paper is organized as follows. Section 1 introduces related work. Section 2 discusses how to build personalized recommendation model based on users X  behaviors. Section 3 makes an analysis on relationship of user preference trans-mission and we go further into how to optimize the model for new and inactive users. Section 4 verifies the accuracy of our personalized r ecommendation model to recommend through exper iments. The last section b riefly comes to some con-clusions and proposes future work. Personalized Recommendation is an active service technique, in which servers collect and analyze user information to learn about their behaviors and interests to build a model, and provide services th at meet their personal needs based on the personalized interest model. Nowadays, many personalization systems have been built to achieve personalized service in different ways, among which infor-mation filtering is a relatively successful one. There are two mainly approaches in filtering: collaborative filtering and content-based filtering.

Collaborative filtering approach[1] is to filter information based on the similarity of users. AT&amp;T Lab built PHOAKS[2] and REFERRAL Web[3] rec-ommendation system in 1997. Kurt[4] introduced personalized recommendation based on collaborative filtering approach to CiteSeer search engine in 2000. Being able to filter some complex concepts such as information quality and taste, which are hard to express, this approach is mainly used in commercial recommendation syst ems like Amazon 1 ,eBay 2 and Douban 3 . However, because of large resource sets and the sparseness of rating data, collaborative filtering fails to solve the problems of cold start a nd others. Recently researches focus on creating virtual users to augment grading for items[5], explain new products with fuzzy natural language processing[6], or cluster users and apply collaborative filtering to clustered groups[7].

Content-based filtering approach has a simple and effective structure, which is mainly used in text recommendation system[8] and hybrid collabora-tive filtering recommendation[9]. The earliest recommendation system was based on content-based filtering including Web Watcher[10], LIRA[11], Leticia[12] and et al. All of them recommended resources by evaluating the similarity between resource content and user interest.

Personalized recommendation for scientific papers draws the atten-tion of providing service for researc hers. McNee[13] realized recommendation by building paper reference graph with collaborative filtering. Torres[14] com-bined collaborative filteri ng with content-based filter ing. Since Torres accepted recommendation results from other syst ems before filtering, it was difficult to implement such input, thus preventing it from being applied to practical applica-tions. Yang [15]proposed a sort-oriented collaborative filtering approach, which extracted users X  behavioral preference from users X  web log and coped with the cold start problem in collaborative filtering. But noises we mentioned above in web log reduced the credibility of web data and affected the results of recom-mendation.

Notice that scientific papers consist of text and text can imply rich informa-tion. Taking into account the issues of sparse data and cold start in collaborative filtering, we believe that content-base d filtering is more effective. Up to now, recommendation by extracting text reflecting users X  preference from the log and building preference model for users has proved to be effective. Based on statisti-cal principles, Chu and Park[16]built users X  personalized model with metadata, which means treat papers users read as a unit. Kim[17] et al. designed user fre-quency model according to terms X  weigh t through users X  web log to recommend basedoncontent.

The methods mentioned above mainly based on inter-citation by the historical papers[13][18], or based on use of user X  X  browse log [16][17][14]. While modeling based on references between papers didn X  X  take each researcher X  X  interest into all-sided consideration. When considering web log, they treat user as a center, but overlook the noise problem inside. The method we propose in this paper utilizes users X  structured behavioral information on the scientific paper sharing site with content-based filtering to provide pers onalized recommendati on for registered users. To implement user-centered personali zed recommendation, we first need to track users X  behavior to collect sufficient in formation and figure out what can reflect users X  features. The selection of behaviors has a great influence on modeling user preference. By analyzing the struct ure of the site and its records as well as existing data, we chose the following be haviors to represent users X  preference: publishing a paper, marking a paper as favorite, rating a paper, making a com-ment, and tagging a paper. All above are users X  active operations on papers. For each user, we extract the title, abstract and keywords of papers that they operate on to form a text file as their configuration file. The personalize recom-mendation model we propose in this paper is to use users X  profile, according to which the paper sets are filtered by content and then recommendation sets are formed.
 We define recommendation task as a triple relationship: ( D i ,D x ,U ), in which U refers to the current user, D x is the document set the user is viewing, and D i is the document set to be recommended to the user. We adopt probability model to calculate the probability that we recommend D i , given the current user U and the document set D x being viewed. We define the similarity between our recommended resource and user as: To make it easier to calculate P ( d i | u k ,d x ), we suppose that users and documents draw from independent identical distribution. Then, Given that the current user is viewing the current paper, P ( u k ,d x ) is constant. Therefore the similarity between user and paper is proportion to the numerator: P ( d i ), in the condition of current user u k and the paper d x being viewed now. In Equation (3), P ( u k | d i ) denotes users X  preference. Without P ( u k | d i ), it will become statistical language model which acts as a baseline. We define the above three as similarity between the user and the paper, similarity between papers and the priori probability of a document respectively. We discuss details about them in the following part. 3.1 Priori Probability of Paper The priori probability of paper here means to evaluate the probability that a doc-ument will be selected. It is evaluated by users X  historical behaviors to grading papers throughout the global website. We make a reasonable assumption that a document is more valuable when global users operate more on it. First, we de-fine users X  behavior set: A= { down, keep, visit, tag, score, comment, collect,  X  X  X } which refers to downloading, marking as favorite, viewing, tagging, scoring, com-menting and so on. D is the set of all documents in the corpus. Then we have: In the equation, a iterates all behaviors in A, and C ( d i | a ) is the number of oper-ations users had on document d i . We assume that all behaviors are independent because of the randomness of users X  behaviors. The normalized probability of behaviors is used as an overall evaluation for documents.

Considering that records for user-registered users are sparse, or that bad-quality documents might have few users X  behavior records, the value of Eigen function C ( d i | a ) is 0. To avoid this situation, we adopted a technology named absolute discount smoothing[19]. It is to subtract a constant value from events in the model, and distribute the subtracted values evenly into events that do not appear. In this paper, the value of Eigen function is term frequency and we do not have to add up the probabilities to 1 when discounting. We assign a small value (0.1 in this paper) to those whose Eigen function values are 0 to achieve absolute discount smoothing, so that they can get a lower score.
 3.2 Similarity between Papers We mention P ( d x | d i ) as the similarity between paper d x and paper d i .Wecan easily apply statistical language model on it. The title, abstract, keywords can give a graphic description of the document, while the domain of area of it is decided by users who was submitters. So we use the title, abstract, keywords and domain of area as documents X  feature, and calculate the similarity through word segmentation: where w refers to any word in the document d x . tf ( w,d i ) is the frequency in which w appears in d i . tf ( d i ) means the frequency of all words in d i . tf ( w,D )is the frequency that w appears in all documents, and tf ( D ) is the total frequency that all words appear in all documents. a is a parameter used for smoothing (we use 0.1 here). 3.3 Similarity between User and Paper The similarity between the user and the document is represented P ( u k | d i ), where users X  preference information becomes fully integrated into personalized recommendation model. A ccording to VSM (Vector Space Model), we discom-pose user information and document information into terms, to calculate the similarity based on probability statistics language model. The representation of document is the same as discussed in 3.2 . Users are represented, as mentioned above, through the characteristic of pref erence model built according to users X  behaviors. We can get the similarity between the two: where W k refers to the entry set in user k  X  X  Eigen space. Since the one-dimensional characteristic of user and document is independent, we have: Under our assumption, the final equation is: where P ( u k | w ) refers to the ratio of w appears in user u k and in all users. The measurement of P ( w | d i ) is the same as what is mentioned in 3.2. In practical applications, the amount of terms in documents and the number of users are quite huge-larger than a hundred thousand. Despite the enormous total amount, the term vectors for each user are usually rather sparse. For one user: Firstly, a user has a specific area of interest, and he does not care about other areas. Therefore, terms in other areas are meaningless for the user, making the user-word matrix global sparse, local dense. Secondly, if a user has just registered or has little information, almost all values of his terms are 0. The above two points both will lead to data sparse. With sparse data, content-based recommendation will not get a good performance. For example, we suppose three users:A,B and C, where B focuses on interdisciplinary. When A and B have high relevance to each other, while B and C share same interests. But because A and C have a few common prefe rence, we directly consider A and C don X  X  have any relevance, which ignores potential associations between them [20]paper21. When recommendation papers for A, it will male recommendation results confined to a certain field without C X  X  field blind to A. This problem is also recommendation technical difficult problem. Under this circumstance, we need to get relevant information from other users as global information to make up complement users with less information. To cope with the problem of insufficient information, we have to increase the density of the third part of our model.[20] predicted the missing items by diffusion and iterative optimization method to smooth the original matrix, so as to increase the density of matrix. Here we redefine user-word matrix based on random walk. In Equation (9), P ( u k | w ) needs to be calculated from the whole domain. We built a UW matrix, i.e., user-word matrix as follows: In Matrix (11), C u i w j refers to the frequency in which w j appears in u i . Normalize the matrix by column, and we can get In Matrix (12), P u i w j is the percentage of the number of occurrence of w j in u i to that of w j in all user configuration files, i.e., the value of P ( u k | w j ). We normalize C ( U, W )byrow,andgetanewmatrix: where P w i u j is a ratio of the number of occurrence of w j in u i to the number of that of all words in u i .

In order to reduce the number of 0 in the matrix, we randomly walk on UW matrix which means multiply P ( W, U )and P ( U, W )togetanew C n ( U, W ), it X  X  defined as follows: where C 1 ( U, W )= C ( U, W ) , as the number of iteration increases, the matrix will become denser and denser. But on the other side, the deviation with the original term frequency matrix becomes larger, and comes to a constant number in the end. Therefore, the number of tighten is determined by the equation (15): If the change exceeds a give n threshold, it stops. Mai ntain each tighten matrix and we mix the primitive matrix with it. As equation (16) shows, a is influence factor, which measures the original matrix and iterative matrix how effect the description in the user preferences. In this section, we optimize the origina l user preference modeling based on ran-dom walk model, where the potential a ssociation between users is taken into account. After optimization, the content for new or inactive users can be com-plemented, so that we can provide bette r content-based recommendation. Mean-while we take advantage of delivery relationship among users X  content, which improve the performance in predicting use rs X  potential preference. Beyond that, we filter more interesting things to recommend to users. In this section, we will examine the performance of our content-based method personalized recommendation model based on users X  behaviors. Here we carry on three groups of experiments. The first experiment compared the recommendation results of considering users X  preference and without considering. Beyond that, we analysis the optimal model based on random walk with different iteration times. Finally, combining preference information after optimal iteration and original preference, we get a fusion model to find the better solution for select better technique papers to recommend. The results show that the optimized model has a good performance for recommendation as well as good robustness. 5.1 Dataset Our dataset is provided by Science Paper Online (www.paper.edu.cn), which is a well-known scientific paper sharing system. On this platform, researchers can fast share their papers, do some reviewing, tagging, etc. Especially, the section of  X  X he First Publications  X  shares the first published scientific research from users. Users in the website can publish, keep, download, visit, tag, score and make comments about papers. In the experiments , we choose five actions to represent users X  preference: publishing, keeping, tagging, commenting and scoring. The data we use include users X  behavioral information and first publish of papers from October 1, 2010 to March 1, 2011.

According to the practical situation of the website, we got test data as ( U, D x ,D i ,L ) after processing the original data. U refers to user id. D x is the paper the user is currently reading, while D i is the paper to be recommended and L refers to a label. In this paper, we assign L the value 1 when users are interested in the recommended paper with clicking and the value 0 when they are not interested. There are 638 data samples, 339 labeled with 1 and 299 with 0. Involved are 26 users and 93 papers. We divide them into 108 groups.
In our personalized recommendation model, papers with a higher probability mean more confidence to recommend, while papers with a lower probability are not recommended to users.
 5.2 Quantitative Evaluation In recommendation, it is necessary as IR to evaluation the top recommendation results. We utilize the MAP and NDCG in information retrieval to evaluate our recommendation model. MAP is short for Mean Average Precision: where N d is the total number of papers currently viewed , avgP k is the average accuracy of recommended papers when paper k is viewed. It is defined as: where M is size of recommended paper set, p ( j ) is the accuracy of first j recom-mended papers, l ( j ) is label information, which is 1 if the recommended paper is relevant and 0 if not. C ( d i ) is the total number of related papers to the viewed one d i . MAP reflects the accuracy of recommenda tion and evaluates the global effectiveness of personalized model.

The second criterion is NDCG (Normalized Discounted Cumulative Gain), which is sort-oriented. NDCG is applied to evaluate the accuracy of top results in recommendation set.
 Given a sorted paper sequence, the NDCG of the n th paper NDCG @ n is: where r ( i ) refers to the relevant grade of i th paper and Z n is a normalized parameter, which assures and the values NDCG @ n of top results add up to 1. If the number of result set is less than r , the value of NDCG @ n is re-calculated. In this paper, we experiment with the evaluation from NDCG @1 to NDCG @6. 5.3 Experiment Results Three groups of experiments have been designed. The first experiment compared the recommendation results of considering users X  preference and without consid-ering. In this group experiment, we use o riginal user-word matrix to represent users X  preference. The fo llowing is the comparison:
The result of comparison shows that the MAP of personalized recommen-dation model is improved from 86% to 91%, increased by nearly five percent . Figure 1(b) shows the recommendation accuracy evaluated using NDCG @1 to NDCG @6, and the average improvement of NDCG is 10.2%. It verifies the effectiveness of our recommendation model with users X  preference based on their behavior .
For the optimized part of the model, we test different setting of parameter n of dense matrix with experiment, and results are shown in Fig. 2(different graphic symbol means different iteration times). Fig. 2 reveal that the perfor-mance of the model declines as the numbe r of interations increases. When the number of iterations are 0, 1, 2, 3, the degrees of decrease are similar to each other. The more iterations, the sharper MAP declines. This is because that the original information is lost as the number of iterations increases. Users X  person-alized information will be lost when we random walk on the user-word matrix.So we adopt the method of weighting to evaluate the model. Having compare orig-inal user preference to fusion model with w eighted iteration information, as per equation(16). The generalized cross-va lidation leads to a good selection of reg-ularization parameters a (In equation (16), here is set 0.35). we get Fig. 3(a) and Fig. 3(b)(represent as ori + n ,where ori means original model, n means the iteration times): Figure 3(a) shows that when we take the original information into account and iterate the user-wor d matrix once, the effect is better than that without iteration information. When iterating more than twice (such as twice and third ), fusion model get increasement compared with original model, approximately 0.1%. But iterating more than three times result in performance degradation(see in Fig. 3(a) column 5). It X  X  concluded that fusion model with one time iteration has got a best performance under our algorithm.

This section conducted experiments to evaluate the personalized recommen-dation model based on users X  behavior and got relatively good results, which verified the effectiveness of the model. It can be concluded from the experi-ment results that analyzing users X  behaviors is useful to recommendation model, because it makes the recommendation more personalized and can satisfying to users X  different needs. In this paper we proposed a personali zed recommendation model based on users X  historical behavior, which can effectivel y represent researchers X  interests. With users X  preference profile ex tracted from historical beh avior, this paper generates recommendation with the help of content from user model and paper infor-mation. Try to avoid recommending one-sided due to modeling only based on single user himself and ignore the relationship between them, we introduce ran-dom walk model in original model to helping correlation transformation between users. Therefore new users a nd inactive users both benefit from it. Experimental results verified the effectiveness of our model in the field of technique papers recommendation. But as the amount of data increases, it is unnecessary to con-duct global recommendation for users within specific areas. Clustering before analysis can help to reduce the recommendation set. In the future we will think about clustering based on content and filte ring preference-devi ating information to improve the performance further.
 Acknowledgments. Thanks to the anonymous revi ewers, especially regarding writing and experiments. This research is supported by the National Natural Science Foundation of Ch ina under Grant No. 61105049.
 indicated the significance and necessity of large scale processing of electronic docu-ments. Different from scanned documents, the born-digital documents are generated by document processing software such as Microsoft Word, PowerPoint and LaTex. In formation for textual content. However, current digitalization and OCRed format like Portable Document Format (PDF) documents contain no logical structure at any high ting and font features embedded within the document, identifying logical structure of the document has attracted much attention both in academic and practical fields. 
As the premise of robustness of logical layout understanding, various researches on layout analysis of PDF format documents were launched [1-2]. ICDAR (International Conference on Document Analysis and Recognition) has already held two competi-tions on Book Structure Extraction Competition focusing on structure recognition and novels are relatively easy to handle for PDF converters [1]. In applications of convert-ing PDF to re-flowable formats like ePub or CEBX (Common e-Document of Blend-ing XML), reliable layout analysis is highly desired to enrich the reading experience challenging for reliable layout analysis of PDF converters, including graph recogni-tion integrating with text segmentation [1], tables and equation identification, etc.. Current page segmentation methods participating in ICDAR Page Segmentation Competitions perform better in separating text than non-text regions [5]. However, it is claimed that the leading text segmentation algorithms still have limitations for con-temporary consumer magazine [2]. Illustrative graphic segmentation receives little pose, the graphics in documents need to be segmented and identified accurately. Image-based document analysis and understanding has been discussed for decades. camera documents. Image-based document layout analysis segments the document image into homogenous geometric regions by using features like proximity, texture or whitespace. Most of the research has been done on connect components (CCs) of page images. The  X  X ocstrum X  method [7] exploited the k nearest-neighbor pairs be-tween connect component centers by features like distance and angle. Kise [8] pointed simplify the task of page segmentation by combining connected components appro-mon [9] proposed a bottom-up method based on Kruskal algorithm to classify the text bounded connected components, and described the page structure by dividing the Delaunay triangles into text area and fragment regions. Ferilli [11] used the distance between connect component borders for bottom-up grouping method. Recently, Koo thod suffers from non-text objects. 
The methodologies in page segmentation can be extended for digital-born docu-or converted in PDF format. These documents represent characters and images in also called as figure or illustration in certain context, is a powerful way of illustrating and presenting the key ideas or findings. It has various categories such as photograph from conventional cameras or microscopes, drawing, 2D or 3D plot, diagram and flow chart. In PDF, figures and tables usually need to be recognized through grouping page primitives such as lines, curves, images and even text elements. Current digital library metadata has little improvement in graphic identification covering all kinds of documents, which is based on the proximity of page elements. Shao [14] focused the dard Adobe FrameMaker template, which is not the common case in most of PDF books or magazines. The Xed system [15] is proposed to c onvert PDF to XML, and it claimed that traditional document analysis will drastically improve PDF X  X  content extraction. graph object integrating with text elements. A hybrid method is proposed and its ap-plication on PDF documents is presented. The preprocessing step and the graph based method are presented in Section 3 and 4. Its application on PDF sources is presented at section 5. The conclusion is given in Section 6. born-digital documents like true PDFs, other than PDF files embedded with scanned document page image. The PDF documents are described by low-level structural objects like text elements, lines, curves and im ages etc., and associated style attributes composite object is constituted by basic objects with certain predefined similarity. A graphic object includes picture, geometric gr aphic or graphic character element. From another perspective, text elements can be categorize as body text from the article, text belonging to graph or picture (or image), text inside table, and text for footer, header or other decorations, etc..
The complexity of graphic recognition has become a significant step to be handled text primitives corresponding to figures, a hybrid method incorporating both low-level structural elements analysis and vision based image analysis is proposed. Firstly, the bounding boxes of the parsed text elements embedded by PDF are exported and then converted from the metric of logic units to pixels. As Fig.1 has demonstrated, by im-posing the bounding boxes of text elements on the original document page image, the super-pixel representation of a page image has provided the layout analysis with great convenience. The text regions in a page image can be regarded as an array of bound-ing boxes in two dimensions. As is given in Fig.1 (a), a two-column page in Chinese from an electronic book is plotted with all the bounding boxes of the parsed text ele-ments. It contains a composite graph combining a line graph component with decoration, etc. Fig.1 (b) is an example page from an English e-book crawled from the web. These cases are challenging to segment the graphic components when either the graphic has been embedded with text or has touched text elements. For PDF docu-ments, in fact, the graphic of line drawing in Fig.1 (a) is not parsed as a whole object graphic in Fig.1 (b). To use the layer information provided by low-level structure elements embedded in original input image before passing the cleaned image to graphic region analysis. As is shown in Fig.2, all the text elements in pages are covered with white pixels and the non-text page images only contains graphic parts, lines and decorations, etc.. based analysis proposed in section 4.1, and the non-text graphic layer is processed by texture features and morphological analysis given in section 4.2. 4.1 Graph Based Analysis As perceptual grouping works in human vision, graph-based method [17] developed segmentation purposes. In [12], the connect component (CCs) are used as graph this paper, page element or primitive corres ponding to a vertex are constructed in the graph. All the text elements can be connected by establishing a neighbourhood system. neighbourhood representation of 2D image. (, ) (, ) vv E  X  constructed. In this application, the elements in V are the centroids of the bounding boxes extracted from PDF parser. wvv is a linear combination of the selected feature functions. The undirected graph constructed can be called as page grap h in the field of document analysis [9]. 
Two feature functions are defined based on the Euclidean distance function fvv and an angle dissimilarity function ( , ) Ai j fvv :  X  X  X   X  . The weight for each edge ( , ) component CV  X  and the inter-component difference 12 (, ) Dif C C between two are formulated as: the vertices connecting them are considered to be in one composite component. If two region comparison pred icate is defined as: where the minimum internal difference 12 (, ) MInt C C is defined as: To identify an evidence for a partition, the difference between two components must be greater than the internal difference. The extreme case is that the size of component is 1, and () 0 Int C = . Therefore, a threshold function  X  can solve this problem: where C is the size of C . 
In the graph based method, the edge causing the grouping of two components is ex-actly the minimum weight edge between the components. That implies the edges for constructing minimum spanning tree of each component [17]. The computational complexity is reduced by path-compression. weight is minimized: Minimal spanning tree requires that the sum of the edge weights is minimal among all graph is built by the Kruskal algorithm. 4.2 Texture Entropy and Morphological Analysis The co-occurrence matrix is generally define d over an image. It reflects the distribu-tion of co-occurring values at a given offs et. Mathematically, it is formulated as: the offset. The texture entropy En is defined as: where i and j are the row and column, ij p is the probability matrix. can be identified on the specific connected component. 5.1 Delaunay Triangulation and Text Elements Clustering To construct the neighborhood system of text elements, Delaunay triangulation is elements are clustered into the right graphic regions. 
As can be seen, the super-pixel representation and Delaunay triangulation of Fig.1 (a) and (b) are illustrated in Fig.3 (a) and (b) respectively. The clustering of text ele-ments is based on the algorithm proposed in Section 4.1 according to predefined thre-shold of feature dissimilarity, which is a combination of font similarity and Euclidean The text elements belonging to the body text area, title, foots and notes, graphic com-elements and graphic object. 5.2 Segmentation of Graphics component with all the pictorials texts. 
The proposed hybrid segmentation algorithm was tested on two Chinese e-books, construction of evaluation set is very time-consuming, preliminary work has been already initiated, which will be further carried out in evaluation of both low-level and high-level page segmentation. However, we manually counted the integration of text graphics. PDF documents. By utilizing inherent advantages of born-digital documents embed-can benefit the layout analysis. Delaunay tessellation is applied on the centroids of the posed hybrid method uses graph based con cept to group the text elements according grating with texts is accomplished by text clustering and connected components seg-mentation. The experimental results on document pages of PDF books and magazines have shown satisfactory performance. Acknowledgements. This work was supported by the National Basic Research Pro-gram of China (973 Program) (No. 2010CB735908). aspects. Chinese script, which has evolved along with change of society and culture, transmitted information in daily life and provided aesthetic perception in art. Its geo-tive civilization feature. Moreover, a calligraphist X  X  unique handwriting skills infer his own emotion and experience. This paper employs visual aesthetics and prior know-ledge of calligraphy to generate Chinese character. 
Two types of organizations paid most attention on Chinese glyph: font compa-nies[8-11] and Chinese character research groups. Font companies must supply suffi-commercial requirements as well as satisfy the Chinese government standard, such as GB2312, GBK and GB13000.1 etc. These companies employ a large amount of glyph designers and workers to manufacture elaborate Chinese font. The production process of font is time-consuming and costly. During the whole process, firstly, basic strokes of originality are created by font designer to decide stroke style of the new font. Se-condly, radicals are composed of the created strokes. Finally, radicals construct Chi-nese characters. The three steps are logical. In fact, the production process is dynamic and iterative, in other words, the design of stroke, the composition and the decompo-sition of radicals or characters are iterative so that the number of characters increases gradually and dynamically. 
In order to speed up font manufacture and realize calligraphic manifestation using information technology, innovative models[1-6, 12] and systems[7, 13-15] were pro-posed to automatically generate Chinese character. In general, methodologies broadly composing new characters with samples[4, 5, 17, 18]. Being different from previous is to provide an effective and efficient way to build a prototype for a new style font. customized Chinese character for digital entertainment in cyberspace. 
The remainder of this paper is organized as follows. In next section, previous work graphy is modeled in Section 3, which reveals unique individual handwriting pattern. model of calligraphic prior knowledge for a novel algorithm to automatically generate Section 6 concludes this paper. imitation and samples based character generation. The methodology of handwriting drawn. As a topological feature, skeleton of glyph was extracted by Yu et al.[19], and hand, contour of glyph is a critical geometric feature. Xu et al.[21] extracted contour dynamic models to represent deformation of brush. Whereas, in order to reduce com-along stroke trajectories. The other important methodology is to construct new characters using samples. This partly inherits the idea of font production process. Lai et al.[17, 18] utilized the tween strokes and shape of stroke, etc. 
The strategy of handwriting imitation emphasizes the reconstruction of handwrit-ing progress according to extracted geometri c and topological feat ures, such as skele-training process in which the unique individual handwriting pattern of calligraphist is formed. The sampled based character composition exploits correlation between Chinese character. A character generation algorithm based on calligraphic prior knowledge and visual aesthetics is proposed in the next two sections. cooperation between mind and hand. It is always a very long period for a learner and shown in Fig. 1. The five layers are Character layer (C layer), Radical layer (R layer), character, which belongs to a layout pattern. Characters  X   X   X  and  X   X   X  belong to Sin-belongs to Left-Right layout pattern. The character  X   X   X  can be decomposed to a top unit which can not be decomposed to smaller radical. However, each radical may have different forms in DFR layer, such as radical  X   X   X  has at least two different can be decomposed to one or more strokes in S layer, and each stroke may have dif-ferent forms in DFS layer. The second part of prior knowledge of calligraphy is con-cerned with calligraphist X  X  unique handwriting skills, on which more details are given in next two sub-sections. 3.1 Modeling Stroke and DFS hence the stroke style and layout pattern are stable. As shown in Fig. 2, six characters are written by a calligraphist Yan Zhenqing. The main horizontal lines in each charac-press down the brush hardly at the end of horizontal line. The stable stroke style pro-forms of the stroke ij dfs : can be composed from stroke using (2): cal layout k rl , and dfr , where m indicates the radical m r . 3.2 Modeling Radical and DFR As mentioned in the above sub-section, the layout pattern of a calligraphist X  X  charac-ter is stable. As shown in Fig. 3, the same character  X   X   X  has two kinds of layout pat-terns: partially surrounding pattern and top-bottom pattern. In Fig. 3(a)~(c), the radi-cal  X   X   X  is partially surrounded by radical  X   X   X , and in Fig. 3(d), the top radical  X   X   X  is just above the bottom radical  X   X   X . The core is not concerned with which pattern is layer using (3): character layout p cl , and the character p c can be composed with the chosen differ-ent forms of radicals using 
In brief, the structure of Chinese character is modeled using the proposed five lay-made well used to compose Chinese character. An automatic generation algorithm of Chinese character based on visual aesthetics is proposed in next section. Chinese character can be composed based on prior knowledge of calligraphy using (1) solutions. In this section, numerical solutions for (2) and (3) are given using a bottom-top nonlinear and non-Gaussian algorithm based on Marr X  X  vision assumptions[22]. 4.1 Modeling Stroke and DFS The spatial arrangement of objects in an image was investigated by Marr[22] accord-jects; 3. Local density of the objects; 4. Local orientation of the objects; 5. Local dis-tances associated with the spatial arrangement of similar objects; 6. Local orientation section. defined as: where strokes. 
The orientation of stroke ij  X  and the orientation of radical mk  X   X  are proposed cor- X   X  is equal to the the stroke which has the biggest area among all strokes. The orientation of layout k rl and p cl can be represented respectively by: Moreover, the distance between strokes in ra dical or radicals in character are defined as: where between strokes and radicals are detected using Area dfr dfr  X  X   X  . Formula (6)~(9) match Marr X  X  point 5. Marr X  X  vision assumption. A nonlinear Bayesian algorithm is proposed in next sub-section to implement (2) and (3). 4.2 Nonlinear Bayesian Algorithm to Generate Character In order to generate Chinese character satisfying the proposed visual aesthetics, three transformation operators are defined to adjust strokes and radicals described in Fig. 1. The three operators include scaling S , rotation R , and translation T : The core of character generation is to figure out proper parameters sx , sy ,  X  , tx , ty , on which depending strokes and radicals are adjusted so that the group of coeffi-cients on visual aesthetics: ij  X  , mk  X   X  , ij i j  X  X   X  X   X  ligraphy. Obviously, this is an optimal problem. 
Firstly, consider the condition that charact er is composed with two radicals, as de-problem. The state vector x ,  X  x and the measurement z are defined as: The state equation and the observation equation are: where solution. Instead, Particle Filter[24], Sequential Importance Sampling (SIS)[25] algo-rithm, Markov Chain Monte Carlo (MCMC)[26] method, and Sampling Importance Resampling (SIR)[27] filter etc are used to approximate the optimal solution. 
The above automatic generation algorithm of Chinese character is concluded as follows: Step 1 : Input prior information of target character and absent radicals. Step 2 : If all radicals to compose the charact er exist, then turn to Step 4. 
Step 3 : Compose the absent radicals using the Bayesian dynamic models, similar to (11)~(14). tep 4 : Compose the target character with necessary radicals using the Bayesian dy-namic models. 
Step 5 : Output the target character. In order to evaluate the proposed algorithm, Yan Zhenqing X  X  130 calligraphy charac-vector graphics and decomposed to radical s and strokes for DFR layer and DFS layer, as illustrated in Fig. 1. The two processes of conversion and decomposition are both strokes of the character  X   X   X . 
The 14 automatically generated Chinese characters are mixed with Yan Zhenqing X  X  14 calligraphy characters to quantify the visual acceptance of characters. And 17 per-person who picked it. Intuitively, the picked characters are sparse. In fact, the picked and 0.55 respectively. In other words, the proposed algorithm in this paper could gen-erate Chinese characters with almost the same visual acceptance of Yan X  X  calligraphy. numbered including three calligraphy characters (No.1~3) and two generated charac-essential reason to pick out No.1 character is that the form of No.1 character in mod-ern age is  X   X   X , of which the right radical is different with Yan X  X . Hence, the lack of bottom of the No.2 character isn X  X  sealed so well that a hook is exposed. This pheno-menon is caused by Yan X  X  handwriting style, which directly lead to the No.4 and No.5 character and the No.5 character have the same layout pattern  X  X op-bottom X , and even the same bottom radical  X   X   X . It is a potential reason resulting in that the two charac-tains Yan Zhenqing X  X  handwriting style. visual acceptance relative to Yan Zhenqing X  X  calligraphy. The lack of prior know-ledge of calligraphy results in contrary visual aesthetics. And the handwriting style of algorithm. provides raw elements for each layer. Marr X  X  vision assumption is deeply analyzed to propose the visual aesthetics for the proposed automatic generation algorithm of Chi-nese character. The whole generation process can be described as a Bayesian dynamic model, in which, the state equation control the update of parameters to adjust strokes, radicals and their layout, and the proposed visual aesthetics is employed by the mea-surement equation. Experimental results show the automatically generated characters have almost the same visual acceptance compared to Yan Zhenqing X  X  calligraphy. One reason affecting visual acceptance is the extent of mastering prior knowledge of calligraphy. Spoken dialogue systems abstract a great deal of concern from its appearance in the 1990s. In the past two decades, spoken dialogue systems developed rapidly. However, due to the difficulties of natural language understanding and dialogue management, most spoken dialogue systems still stay in the laboratory stage.
As the prior part of the dialogue management, natural language understanding plays an important role in spoken dialogue system. In previous studies [1,2], the user utterance is usually mapped to the system state vector, and the dialogue strategy depends only on the current system state. Natural language parsing is relatively simple keyword matching or form filling based on syntax network. These natural language understanding methods have a poor ability in describing language, and these mapping methods will lose a wealth of language information in user utterances.

As the center of spoken dialogue system, dialogue management has been a hot academic research area for many years. Early spoken dialogue systems are entirely designed by experts. The systems usually employ a finite state ma-chine model [3] including a large number of artificial designed rules. These arti-ficial rules are written in a very good ex pression and they describe the specific applications correctly, but the preparation and improvement of the rules will become increasingly difficult when the dialogue task becomes complex. In re-cent years, data-driven based dialogue management model gradually attracts the researchers X  attention. This kind of dialogue management model obtain knowl-edge through the annotated dialogue corpus automatically. Dialogue manage-ment mathematical model based on Markov decision process (MDP) is proposed in [4], and the reinforcement learning ( RL) is proposed to learn the model pa-rameters in the same study. Studies in [5] propose a partially observable MDP (POMDP) dialogue management model. These models have become hot topics of the dialogue management approaches since they are proposed.

However, there remain many problems when these models are used in the actual applications. Firstly, the system uses a reinforcement learning mechanism. Interaction with the actual users is ver y time-consuming, and is hard to achieve the goal. Many systems get the optimal parameters from the interaction with the virtual users, so there are some difference between the optimal strategy and the real dialogue. Secondly, the dialogue models obtained by this method lack of priori knowledge of dialogue, so they are difficult to be controlled.
In this paper we propose the finite state transducer (FST) model to solve the natural language understanding problem. The FST model maps the user utterances to user actions, and extracts the attribute-value information in user utterances. For dialogue management problem, we propose dynamic Bayesian network (DBN) model. It generates the dialogue strategy not only considering the system state, but also depending on the user actions.

The paper is organized as follows: the action-group FST model and the de-tails of using action-group FST model to d etect the user actions are presented in section 2. Section 3 describes the dynami c bayesian network and dialogue man-agement based on DBN model. Section 4 presents our spoken dialogue system and the performance of the two modules. Finally, conclusions are presented in section 5. 2.1 User Action-Group FST Model Each utterance corresponds to a single action. We annotate the utterances by action labels, and mark out the information contained in the utterances. Then we collect the utterances which have the same action label together to generate an action-group. We use the  X  X eterminize X  and  X  X inimize X  operation described in the openFST tools [6] to unite and optimize the action-group FST. Fig.1 shows the building process of an action-group FST for a particular action-group.
It is unrealistic to establish a multiple action-group FST by many action-groups which are built through dividing all of the daily utterances into different action groups. In this paper, the proposed action-group FST is only used in the fields identified by the dialogue scenes. The utterances used for building the action-group FST are collected in these particular fields.

These utterances are labeled in two stages, the first stage only marks which action-group the utterances belong to, while the second stage needs to mark out the information contained in every utterance. We use attribute-value pairs [7] to describe the information contained in utterances, such as  X  X urrency=dollar X . The purpose of the second-level label is to parse the user utterances automati-cally according to the string sequence in the FST model that matches the user utterance properly. If the action-group is labeled according to domain knowledge, the model will be too sparse, and the portability will be poor. However, it is un-able to accomplish the dialogue management tasks if the action-group is labeled based on sentence structure. Therefore, we design the action-group according to [7]. Table 1 lists the major action-groups that we used in our system.
 2.2 User Action Detection Based on Action-Group FST We hope to find the most similar word sequence as the user utterance in the action-group FSTs. Then we consider the user utterance belongs to the action-group where the word sequence is in. In this paper, the edit distance is used to measure the degree of similarity between the two word sequences. The decoder X  X  work is to find the optimal word sequences X  path in action-group FST that can make the edit distant minimum.
 The most common method used to calculate the edit distance is the Wagner-Fischer algorithm [8]. This is a string to string edit distance calculation method, and its core idea is dynamic programming. We improve the Wagner-Fischer algorithm to compute the edit distance between the string and the network. The algorithm is also based on the idea of dynamic programming. It calculates the edit distance between user word string and each node in the FST respectively. The pseudo-code for this dynamic programming process is shown in Table 2.
In table 2, minEDcost ( n i ,W ) represents the accumulated minimum edit dis-tance from state 0 to state i and cost ( n i  X  1 ,n i ) represents the increased edit distance from state i  X  1 to state i .

When the user utterance matches a string sequence which has the smallest edit distance with the utterance from an action-group FST, we assume the user utterance belongs to the action-group. At the same time we can automatically ex-tract the attribute-value information in user utterance according to the matched string. 3.1 DBN Dynamic Bayesian network (DBN) is a probabilistic graphical model that can deal with the data with timing characteristics. It is formed by adding time information in the conventional Bayesian network. In the DBN, the topology of the network in each time slice is the sa me. Time slices are connected through a number of arcs which represent the relat ionship between the random variables in different time slices. Fig.2 shows the topology of a DBN model.

Bayesian network learning consists of two categories: parameter learning and structure learning. Parameter learning is to estimate the conditional probability distribution of each node by the training data when the network structure is known. The structure learning, also known as model selection, is to establish the network topology through training data. Structure learning problem is very complex, so we manually define the network structure and mainly consider the problem of parameter learning in this paper. Under the conditions of a given net-work structure, parameter learning can be divided into two categories according to the observability of nodes.

If all nodes can be observed, the model parameters can be obtained directly by a large number of data samples. Howeve r, in most cases the data we collected is not enough, then the Dirichlet distribution can be used as the prior distribution of parameters. We update the model through the  X  X vidence X  until we get the ultimate posterior probability parameters. We use the notation B h to represent the Bayesian network topology, and X = x 1 ,...,x n to represent a set of n discrete random variables in the network, where each x i has r i possible values x ,x 2 i ,...,x r i i . Then the joint probability distribution of X can be shown as the following equation: sponding with node x i ,and  X  s =(  X  1 ,..., X  n ) is the parameters of the network model. We assume the training data set D = { d 1 ,...,d N } has a total of N samples, and each sample includes all the observable nodes. Then the Bayesian network parameter learning problem can be summarized as follows: calculate the posterior probability distribution P (  X  s | D,B h ) when given the data set D .We get the following equation:  X  ijr i ) and assume that parameter vectors  X  ij are independent with each other, then the posterior probability of the model parameters is shown as the following equation: The prior distribution of Parameters  X  i is Dirichlet distribution, and then we get the follow equation: where N ijk is the number of occurrences of x i = x k i , parents ( x i )= parents j ( x i ) in the data set D ,and  X  ijk is the number of occurrences of the events in priori, and  X  ij =  X  ijk , N ij = k N ijk . Finally, according to equation 3 and equation 4, we obtain the following equation[9]: When there are hidden nodes in the network, the parameters of the model can be estimated via the EM algorithm. In the E-step the expectations of all nodes are calculated, while in the M-step these expectations can be seen as observed, and the system learns new parameters that maximize the likelihood based on this. After a number of iterations, the system will converge to a local minimum. 3.2 Dialogue Management Based on DBN In this paper, we do not use the complex state space in the spoken dialogue system. We just put the user action into the dialogue management model to affect the dialogue action [10]. Similarly, we assume that the dialogue process has the Markov property, and then get the following equation: where S t 1 , A t  X  1 1 ,and U t 1 represent the system state, the system action, and the user action that from the initial moment to time t , respectively. s t , a t ,and u t represent the system state, the syste m action, and the user action at time t .The DBN model will take the system input at every time into account, so we add the user action into every time slice in order to make the model depend on the user action. The system action at time t depends on the system state s t and the current user action u t . Meanwhile, the current user action u t as a system input also directly affect s the system state s t . We retain the transfer connection arc between the states in adjacent time sli ces. The DBN model structure described above is shown in Fig.3.

According to the above description, t he DBN model consists of four parts: the system state space S , system dialogue action space A ,theDBNnetwork topology B and the user action space U . We collect a series of dialogue corpus as  X  X vidence X  to learn the model parameters and infer the optimal system action.
If the system state vector s t is observable, we can update s t according to the attribute-value information that extracted by the action-group FST model. Then we can estimate the model paramet ers directly from the data according to the parameter learning method describe d in section 3.1. To inference the system action, we directly calculate the joint conditional probability distribution of the system action under the conditions of the system state vector s and user action vector u , and take the maximum probability of action as the current system action. This idea is described by the following equation: If the system state vector is unobservable, we have to use the EM algorithm to estimate model parameters from the  X  X  vidence X . The current system action is calculated by the following equation: The previous content describes the modeling method of natural language under-standing module and the dialogue management module. This section presents the overall design of the spoken dialogue system, and provides several experi-ments to verify the performance of each module. Finally, the experimental results are analyzed. 4.1 Overall Structure of the Spoken Dialogue Systems A typical spoken dialogue system not only includes the natural language under-standing (NLU) module and the dialogue management (DM) module, but also contains the automatic speech recognit ion (ASR) module, the text to speech (TTS) module, and the natural language generation (NLG) module. For the spoken dialogue system in particular scene, it also contains a database or net-work database for inquiry. Our spoken dialogue system structure is shown in Fig.4.

The implementation of the ASR module and the TTS module in Fig.4 uses the open source HTK tools [11] while the NLG module uses template filling [3] method. We use mysql to bulid the database and use query to build communi-cation between the database and DM module. The NLU module and dialogue strategy in the shaded blocks are the focus of our studies. This system receives the user voice x , and uses the ASR module to convert it to the text sequence w . Then the action-group FST model in the NLU module maps the word se-quence w to the user action u and attribute-value information k . The dialogue management module has two functions: to maintain the system state and to give the dialogue strategy. We use form filling method to represent the system state [1]. The dialogue management module uses the attribute-value information k to update the system state s , and then uses the DBN model to calculate the dialogue strategy based on the system state s and the user action u .Ifauserre-quests for information, the dialogue management module needs to interact with the database to query information q . Then the dialogue management module sends the system action a and the information q to the NLG module where the information will be integrated into the response text m . Finally, the synthetical voice y generated by the TTS module is passed to the user. 4.2 Experimental Results Experiments on Action-Group FST Model. In this section, we will present the action-group FST model built by openFST toolkit. The annotated data is divided into the training and testing set. The training set consists of 100 dialogues, with a total of 671 rounds data, while the attribute-value pairs in user words are 846. The testing set consists of 20 dialogues, with a total of 138 rounds data, while the attribute-value pairs in the user words are 185. In the annotating process, words that cannot be listed have an alternative, such as the amount of money in the currency e xchange scene is replaced by  X  X mount X . We use the classification accuracy of us er action and the precision, recall and F-measure of the attribute-value pairs in user utterance to evaluate the action-group FST model.

The experiments are divided into two g roups. We test the effect of the action-group FST model and the traditional keyword matching respectively. The rules of keyword matching method are studied from the training set only, and they are used to identify user action and extract user information. The results are shown in Table 3.

The experimental results prove that the action-group FST model outperforms the keyword matching method. The keyword matching method has better preci-sion, but its recall is worse than that of the action-group FST model. The overall effect of the action-group FST model has a slight improvement compared with the manual keyword matching method. However, the keyword matching method needs experts to spend much time to write the rules, and this work will become very difficult when the amount of data increases. The action-group FST model will perform better if the amount of data increases, and it has good portability. In summary, the action-group FST model has more advantages.
 Experiments on DBN Model. This section presents the DBN model built by the BNT toolkit [12]. The annotated data is also divided into the training and testing set, and the data size is the same as described above.

In the training phase, 671 rounds of dialogue data are sent to the DBN model by sequence to estimate the model parameters. The priori parameters of the initial model are obtained by the Dirichlet distribution. In the testing phase, we send the user utterances to the DBN model by their sequence in the dialog. The difference between the system output action and real data is recoded. When the system generates a wrong action, we send the user utterance by sequence just as there is not any error. Finally, we use the precision of the system output to evaluate the performance of the DBN model.

In this section, we test the DBN model and the MDP model separately, and the experiments on the DBN model are divided into three groups. We assume all nodes in the DBN model are observable in the first experiment. Therefore, there are three variables in the training data. They are the user action, the system state and the system action. The testing data include only the user action and the system state, and the DBN model infers the most suitable system output action according to these two variables . In the second experiment, we set the system state in the previous time slice as evidence both for training and testing. Therefore, the DBN model infers the most suitable system output action based on three variables. We hide the system st ate in the third experiment. Therefore, there are only the user action and the system action in the training data. We use EM algorithm to estimate the parameters of the DBN model and the DBN model infers the most suitable system output action based only on the user action. Table 4 shows the results of these experiments. The last experimental result in Table4isobtainedbytheMDPmodel[4]onthesamedatasetdescribedabove.
 In the training phase, the award value of each step is set to  X  1. If the dialogue ends with satisfaction, then the system rewards 20 for the whole dialogue. There are about 10 rounds in each dialogue, so the whole dialogue gets a positive award sum if the dialogue ends with satisfaction, otherwise it gets a negative award sum. The final jump probability is obtained according to the normalized award value.

From the results shown in Table 4, we can see that the DBN model in the first experiment outperforms the model in the second experiment. This proves that the simple DBN model performs better than the complex model in short dialogue. In the third experiment, the performance degrades significantly when the system state is hidden. This means the system state is very important for DBN model. In related work [7], more complex method is used to describe and maintain the system state, which is consistent with our experimental results. In addition, the performance of the MDP model is poor. This is probably because the MDP model is to get the global award maximum while the DBN model is to seek a single round of local optimal. In the case of limited training data, it is difficult to get the optimal solution for the MDP model. This conclusion is consistent with the related work in [4]. 4.3 The Overall Performance of Our Spoken Dialogue System Natural language understanding and dialogue management, as two of the most important modules in spoken dialogue systems, have good performance sep-arately through the action-group FST model and the DBN model. However, a conclusion concerning the performance of the whole system cannot be drawn eas-ily through these results. A subjective experiment on our overall system proves that the system can basically meet user X  X  needs. A survey of the user of our sys-tem is shown in Table 5. The results show that the performance of the system is related to the users, and researchers X  test results are much better than those of the other users. The main reasons for the errors are that the user utterances is beyond the training data, and ordinary users are more likely to say words outside the training set. Fig.5 is an instance of the spoken dialogue system interaction with human in a currency exchange scene. This paper focuses on the natural language understanding and dialogue manage-ment in spoken dialogue system. To cope with these tow problems, we propose the action-group FST model and the DBN model. The advantages of these two models include: (1) the model can work on a small-scale data set, while the model will perform better on a large-scale data set; (2) these two models have good portability because there is little artificial work in model designing.
Although the action-group FST model and DBN model show good perfor-mance in some specific fields of spoken dialogue systems, there remains much to improve. If the concept of word similarity is added into the action-group FST model, the edit distance calculatio n results of the synonyms will be a deci-mal. Then the action-group FST model can perform better when matching user utterances. In addition, the form filling method used for representing the sys-tem state cannot meet the system X  X  need s when the system becomes complex. Furthermore, the DBN structure design and parameter learning will be more complex when the system state becomes a hi gh-dimensional vector. These issues will be addressed in future work.
 computers to recognize the human emotions, which is more widely used in industry and review data, which could generate summaries about aspects for consumers. Another example is the word-of-mouth on social media, such as Blog and Micro-Blog, and min-ing the tendency of a group of peoples X  opinions is badly needed. There has been some work by researchers. Such as Wang et al [3] presented a graph-based hashtag sentiment classification approach to analyze the sentiment tendency in Twitter. 
In China, researchers concentrate on phrase level and sentence level sentiment classification recently, such as COAE (Chinese Opinion Analysis Evaluation) [4]. Less work has been done on passage level comparing with the former two levels, for the structures and patterns of emotion expressions are more complex. Turney [5] pro-posed a lexicon based method to identify the passage X  X  emotion by using the average good result, its flexibility and human annotation accuracy are still a potential problem. Xu et al [7] proposed a method based on semantic resources, and Condition Random Field (CRF) is applied to label emotion sentence by sentence; after the emotion chain formed, the emotion of the passage is determined. 
In this paper, we analyze relationships between thinking modes and language, and three Chinese thinking modes are taken into consideration to assist sentiment analysis and emotion classification. Then a Chinese sentiment expression model (CSE) is pro-applied when the CSE model could not classify the implicit emotions accurately. Chinese thinking modes and the Chinese sentiment expression model (CSE). In sec-tion 4, implicit emotion mining method based on Latent Semantic Analysis (LSA) is work and point out some directions for future research in Section 6. act of thinking or the resulting ideas or ar rangements of ideas. Different civilizations cradle different thinking mode, and thinking modes determine the expression way of languages. From the concept of thinking, we can find that thinking and language are inseparable. Language is the expression way of thinking, and it carries the abstraction of the reality. When we express our thinking, to a large degree, the features of think-and Chinese thinking modes respectively. 2.1  X  X piral Graphic Mode X  and  X  X traight Line Mode X   X  X piral graphic mode X  is one of Chinese thinking modes, and it commonly reflects in mally introduces the topic in an indirect way like a spiral graphic, so the topic of the Roman philosophy, so they focus on deduction and thinking in a straight line way. On passage organization aspect, they tend to state their views directly and frankly. Take these sentences below as examples: (1) Chinese:  X   X  X  X  X  X  X  X  X  X  X  X  X  X  X  X   X  English:  X  X e was shocked by what he saw. X  (2) Chinese:  X   X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X   X  English:  X  X  got a perfect answer after deeply thinking. X  Form the above mentioned examples, we can find that the key emotion words  X  X hocked X  and  X  X erfect X  appear in the front part of the sentence, but the Chinese emo-tion word  X   X  X  X   X  and  X   X  X  X   X  locate at the end part of the sentences. 2.2  X  X oncreteness X  and  X  X bstractness X  The second Chinese thinking mode is called  X  X oncreteness X , for Chinese is a hierog-description are used in Chinese to illustrate abstract things, and some character com-ness X , which is another western thinking mode. They tend to implement general vocabularies and their variants to express abstract feelings or opinions, such as  X -ion X ,  X -ance X  and  X -ness X , but concrete things are seldom applied to explain. (3) Chinese:  X   X  X  X  X  X  X  X   X  English:  X  X isintegration. X  (4) Chinese:  X   X  X  X  X  X  X  X  X  X  X  X   X  English:  X  X here there is a will, there is a way. X  idiom, while only the variant of disintegrate is used to explain these meaning in Eng-lish. And in example (4), we could get an overview that verb has advantages in Chi-nese, but noun is more frequent used flexibly in English. 2.3  X  X catter View X  and  X  X ocus View X   X  X catter view X  [10] is the third Chinese thinking modes. From the angle of ontology, Chi-nese tend to emphasize unified whole, which means think from more to one. When Chi-nese express their feelings or comments, they are usually inclined to use various words to example, we can frequently find that more than one verb is used in one Chinese sentence, ing thinking, which is called  X  X ocus View X .  X  X ocus view X  can be reflected by the only one verb, which normally is the core of one sentence.  X  X ocus view X  is widely adopted in Eng-lish. Here are two examples about  X  X catter view X  and  X  X ocus view X  below: (5)Chinese:  X   X  X  X  X  X  X  X  X  X  X  X  X  X  X  X   X  English:  X  X e walked into the classroom with a textbook in hands. X  (6)Chinese:  X   X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X   X  English:  X  X he boy and the girl were playmates in their childhood. X  From the examples (5), we can clearly find that more than one word are implemented core of the sentence. In example (6),  X  X catter View X  is embodied in Chinese expres-sion, which is reflected by more words used to express the same meaning. 
To sum up the above mentioned Western and Chinese thinking modes [11], we graphic mode X  and  X  X traight line mode X  are reflections on passage expression model. thinking modes provide us a new angle to analyze text sentiment orientation. 3.1 External Resource divided into 4, 6, 8, 10 and 20categories etc. While in DUTIR Emotion Ontology, the widely in sentiment analysis and emotion classification. 
To recognize the affective lexicon, we calculate mutual information between the method, the formula is defined as follows: Where x is a data sequence and y a label sequence, y|v is the set of components of y 
While in Chinese Opinion Analysis Evaluation (COAE) [4], emotion is classified into 4 categories, which are  X  X appy X ,  X  X ngry X ,  X  X ad X  and  X  X ear X , and opinion evalua-tion consists of positive and negative. So we made some adjustments on DUTIR Emotion Ontology to complement the standard of COAE. 3.2 Quantification of Similarities between Thinking Modes Although there are differences between Western and Chinese thinking modes, there decrease the its intensity; if an adversativ e occurs, the emotion of the sentence is de-termined by the part after the adversative. 
Sentence is the foundation of constructing paragraph and passage, and it is the mini-level is chosen as minimum research object level. Modifier window strategy is adopted to analyze the emotion of a sentence and its score. In this strategy, negative words, ad-verb of degree and adversative are detected if they exist in the modifier window. 3.2.1 Quantification of  X  X piral Graphic Mode X  important they are to determine the emotion of the sentence. For example: (7)Chinese:  X   X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X   X  English:  X  X very aspect about the hotel is ok except the disappointing service. X  Although the former part presents a positive opinion, the word  X   X  X  X   X  (disappointing) emotion-determining words mostly locate the end part of Chinese sentences. And this conclusion is still useful to paragraphs and passages [14]. 
In order to simulate the  X  X piral grap hic mode X , equation (2) is applied: Where A can be a passage, paragraph or sentence, a can be a paragraph, sentence the emotion unit a i . 
From the equation (2) we can find that if the closer a i locates the tail, the larger the score(a i ) will be. Take example (7), the word  X   X  X  X   X  (disappointing) locates near the tail of the sentence, it can get a larger score than the word  X   X   X  (good) after compu-ting by equation (2), so different emotion words regain new emotion weights, so this sentence tends to have a negative tendency. 3.2.2 Quantification of  X  X oncreteness X  analysis. In all nations, adjective tends to have a better performance to express feeling barrassed. In order to simulate this thinking and language characteristic, different part of speech word and sentence structure are adopt ed different tactics. The details can be seen as follows in equation (3). priority to the verb, and which meets the Chinese thinking explicitly. 3.2.3 Quantification of  X  X catter View X  press the same sentiment or emotion. And a great number of nouns or adjectives are used as predicate and this has been discussed above. In order to simulate the Chinese thinking mode  X  X catter view X , view-window is adopted, which means that in specific position different sizes of windows are applied to reflect this characteristic. Based on our experiment, when the size of window is fixed at 6, we can get the best result. If the size of view-window is too small, it cannot capture the words which significantly which will also mislead the analysis. 3.3 Chinese Sentiment Expression Model After analyzing the features of Western and Chinese thinking modes and their specific quantification methods, in this part, we will introduce a Chinese sentiment expression model, which is the combination of Chinese thinking modes. When we try to get the sentiment or emotion tendency of the text content, all of the Chinese thinking modes are taken into consideration.  X  X catter view X  and  X  X piral graphic mode X  are conclusions on Chinese expression, feature of sentence structure, and  X  X piral graphic mode X  is a characteristic of passage organization. While in English expression, only one verb is contained in one sentence, so the emotion of this sentence is easy to determine after analyzing the key verb. But variants to express emotions. In order to enrich the vocabulary, DUTIR Emotion Ontology [8] is adopted. In four group, which are  X  X appy X ,  X  X ngry X ,  X  X ad X , and  X  X ear X , and opinion-bearing words are classified into  X  X ositive X  and  X  X egative X . For this coarse granularity partition can classification error rate and improve the accuracy. 22773 emotion words and 6 groups are chosen manually to testify the influence of  X  X catter view X  on sentiment analysis. the emotion groups can maintain that every passage has an explicit topic. emotion classification would be misled by negative words and degree words. For example, the negative words can change the emotion tendency, and the degree words can enlarge or lessen the emotion intensity. So in this article, degree words, negative words and adversative are all taken into consideration. 
After integrating the two Chinese thinking modes,  X  X catter view X  and  X  X piral graphic mode X , the Chinese sentiment expression model is proposed. DUTIR Ontology [8] and classification principle are applied to implement the characteristic of  X  X catter view X , and  X  X piral graphic mode X  is simulated based on the position-influenced strategy. The above mentioned Chinese sentiment expressions are extracted based on the expli-cit Chinese thinking mode  X  X catter view X  and  X  X piral graphic mode X . But the Chinese thinking mode  X  X oncreteness X  mostly appears in an indirect way. If the emotion of the sentence cannot be analyzed by using explicit features, the thinking mode  X  X oncrete-ness X  is helpful. Chinese express their emotions or feelings influenced by  X  X oncrete-ness X  thinking mode, and they tend to make use of familiar and similar object. So it is mine the emotion of the implicit emotions, and it has been widely applied to calculate the similarity between word and word, word and passage, or passage and passage. In section 4.1, we discuss what kind of texts is labeled as implicit emotion text. Then we introduce the method to determine the emotion of implicit emotion text. 4.1 The Criterion of Implicit Emotion Sample implicit emotion articles mostly have low scores comparing with the explicit ones, so the threshold is needed to classify the explicit emotion sample and the implicit ones. 
In this paper, a group of samples are chosen to determine the threshold, and we call one explicit emotion word, and its intensity is 9 in DUTIR Emotion Ontology [8]. The sentiment expression. The scores are implemented to determine the threshold. 4.2 Implicit Emotion Classification Based on LSA LSA (Latent Semantic Analysis) is proposed by Dumais et al in the year 1988, which is used in statistic method to analyze mass texts, and get the latent semantic relation-ships among words. The main object of LSA is to map the higher dimension Vector Space Model (VSM) to lower dimension latent semantic space. indirect way, so we need to further analysis, and the samples which scores are larger than the threshold are chosen as seed samples [15]. Then LSA is implemented to mine the latent semantic relationships between the implicit samples and the seed samples. After the above steps, we can obtain the emotion tendency of the implicit samples. Equation (4) is used to compute the score of the implicit sample, and then identify the emotion of the sample  X  5.1 Experiments of Chinese Thinking Modes in Different Domains analysis, three group experiments have been done on Evaluation data set provided by negative subset. The reviews in the data set cover sentence level, paragraph level, and passage level, and this can avoid the particularity of sentence structure. The baseline is the method proposed by Turney [5], and three thinking modes are integrated to see method and method proposed by Turney are shown in figure 1, and the columns in it from 1 to 6 respectively represent the subsets: elec-neg, elec-pos, hotel-neg, hotel-pos, stock-neg and stock-pos. 
From the results in figure 1, we can get a conclusion that no matter positive or neg-Chinese thinking modes can assist the sentiment analysis indeed, and they have inde-analyze the three Chinese thinking modes in different domains. 
The first group experiments are adopted on electronics negative and positive re-views. The reviews in elec-neg data set are relatively longer and rich in verbs. From figure 2, we can find that after adding each Chinese thinking mode the precision has cause too much nouns are in the text context and less emotion words are used to ex-press the only one emotion. 
The second group experiments are implemented on negative and positive hotel re-into consideration, the results are much higher than before. It verifies that  X  X oncrete-decided by the specific data set to get the best result. The experiment results of hotel reviews are shown in figure 3 as follows. reason that a great number of specialized wo rds exist and part of them does not appear in DUTIR emotion ontology [8]. That is also the limitation of lexicon based method. modes. Due to most stock reviews are passage-level passages, the length of the review is relatively long than the former two electronic and hotel data sets. And it can expli-citly reflect the Chinese thinking modes  X  spiral graphic mode and its advantage. 5.2 Experiment of Chinese sentiment Expression Model and LSA In this experiment part, the experiment data set is ChnSentiCorp [17], and it is about relabeled. Then we get 1828 positive passages and 2163 negative passages. 
For comparison, we implement two baseline methods: one is lexicon based method proposed by Turney [5], which decides the passage polarity only by its emotion resource proposed by Xu [7], which takes semantic resource into consideration, such as negation and transition. The result is listed in table 1. applied in experiments have a higher probability to be misclassified by them, and that samples have also been done. Later we will illustrate them. 
Semantic method focuses on the sentence level, and we can see that there is a pre-cision increase of our proposed method -CSE from the comparing results. In order to identify the opinion tendency of the whole passage, the semantic method proposed by Xu [7] has to count the numbers of positive sentences and negative ones, and the larg-CSE model has a better performance in binary opinion classification than Xu [7]. Implicit emotion sample classification is a difficult problem in sentiment analysis. emotion classification based on LSA is necessary. Some statistic jobs have been done by us, and the details are listed in table 2. 
In order to solve the implicit emotion problem, LSA is implemented. Each implicit emotion sample is used as a query to index the relevant seed sample in latent semantic sample and seed sample, the similarity threshold value is given 0.8 empirically. Fig-ChnSentiCorp [17]. Figure 6 is macro-average precision of the former two data sets. From figure 5 to 6, we can find that after secondary classification by implementing higher than 90%. explicit emotion sample from Chinese thinking modes,  X  X piral graphic mode X  and also make sure the credibility of emotion cl assification. LSA is adopted in secondary tent semantic space based on the  X  X oncreteness X  thinking mode. classified. There are 8 misclassified seed samples in positive data set and 15 of that in negative data set; second, due to the variety of implicit emotion samples, some of the implicit emotion samples cannot index the samples with higher similarities. The contribution of this paper is to propose the Chinese sentiment expression model, which focuses on the thinking modes. Three Chinese thinking modes,  X  X piral graphic effectively improve the accuracy of emotion classification. Thinking mode  X  X oncrete-ness X  mostly exists in implicit emotion expressions. In order to solve this, Latent Se-mantic Analysis (LSA) is applied to implement  X  X oncreteness X  when CSE model could not classify the implicit emotions accurately. Two traditional sentiment analysis methods are used to verify the effectiveness of proposed method, CSE and LSA. Ex-perimental results show that the performance of sentiment analysis included the Chi-nese Thinking mode factors and LSA mining is better than that not included. In the past decade, event has emerged as a promising research field in Natural Language Processing (NLP), Information Retrieval (IR) and Information Ex-traction (IE). In online news services do main, event-based techniques which can extract entities, such as time, person, location, organization etc. from news sto-ries and find relationship among them to r epresent structural events, have been paid wildly attention. Based on the identification and extraction of these valu-able event facts, more convenient and intelligent services can be implemented to facilitate online news browsing in event and semantic level.

However, the definition and representation of event are different in various re-search areas. In the literature, there are a number of event models nowadays. For example, event templates used in Message Understanding Conference (MUC) [1], a structural event representation in Automatic Content Extraction (ACE) [2], a generic event model E [3] [4] in event-centric multimedia data management, and ontology-based event models, such as ABC [5], PROTON [6] and Event-Model-F [7] in knowledge management. But these models are not suitable for semantic understanding of news events. In order to support semantic applications, for example, event information extraction and semantic relation navigation, we pro-pose a News Ontology Event Model (NOEM) to describe entities and relations among them in news events.

As we know, 5W1H (including What, Who, When, Where, Why, How), a concept in news style is regarded as basics in information gathering. The rule of the 5W1H originally states that a news s tory should be considered as complete if it answers a checklist of 5W1H. The factual answers to these six questions, each of which comprises an interrogative word: what, who, when, where, why and how, are considered to be elaborate enough for people to understand the whole story [8].

In NOEM, in order to address the whole list of 5W1H, we define concepts of entities (time, person, location, organization etc.), events and relationships to capture temporal, spatial, information, ex periential, structural and causal aspect of events. The comparison of NOEM with existed event models shows that it has a better knowledge representation ability, feasibility and applicability. By automatically extracting structural 5W1H semantic information of events and populating these information to NOEM, an event knowledge base can be built to support event and semantic level applications in news domain.

The rest of the paper is organized as follows. We first review related work in Sec. 2 by discussing event definitions and event modelings. The proposed ontology event model NOEM is introduced in Sec. 3. In Sec. 4, we evaluate the representative ability of NOEM by comparing it with existing event models and demonstrate its feasibility and applicability by means of case study. In Sec. 5, we conclude this paper. 2.1 Event Definitions The notion of an event has been widely used in many research fields related to in natural language processing, although with significant variance in what ex-actly an event is. A general definition of event is  X  X omething that happens at a given place and time X , according to WordNet [9]. Cognitive psychologists look events as  X  X appenings in the outside world X , and they believe people observe and understand the world through event because it is a suitable unit in accor-dance with aspect of human cognition. Linguists have worked on the underlying semantic structure of events, for example, Chung and Timberlake (1985) stated that  X  X n event can be defined in terms of three components: a predicate; an in-terval of time on which the predicate occurs and a situation or set of conditions under which the predicate occurs. X  In [10], Timberlake further supplements it as  X  X vents occur in places, under certain conditions, and one can identify some participants as agents and some as patients. X 
In recent years, some empirical research have been developed on the cognitive linguistics theoretical basis. TimeML [11] is a rich specification language for event and temporal expressions in natural language text. Event is described as  X  X  cover term for situations that happen or occur. Events can be punctual or last for a period of time X . In event-based summarization, Filatova et.al. [13] define a concept of atomic event . Atomic events link major constituent parts (participants, locations, times) of events through verbs or action nouns labeling the event itself. In IE community, an event represents a relationship between participants, times, and places. The MUC extracts prespecified event information and relates the event information to particular organization, person, or artifact entities involved in the event. The ACE describes event as  X  X n event involving zero or more ACE entities, val ues and time expressions X .

The event extraction task in ACE req uires that certain specified types of events that are mentioned in the source language data be detected and that selected information about these event s be recognized and merged into a unified representation for each detected event. According to the requirements of seman-tic understanding of news and the characteristics of news story, we define event as  X  X n event is a specific occurrence which involves in some participants X . It has three components: a predicate; core participants, i.e., agents and patients; auxil-iary participants, i.e., time and place of the event. These participants are usually named entities which correspond to the what, who, whom, when, where elements of an event. The relationships among entities and events are also concerned. By analyzing the connections between the predicates, we can get the why and how elements which are cause and effect of the event. 2.2 Event Modeling Event modeling involves event definition, event information representing and storing. There have been several event models in different application domains. Probabilistic Event Model. In [12], a news event probabilistic model is pro-posed for Retrospective news Event De tection (RED) task in Topic Detection and Tracking (TDT). In the model, news articles are represented by four kinds of information: who (persons), when (time), where (locations) and what (key-words). Because news reports are always a roused by news events, a news event is modeled by mixture of three unigram models for persons, locations and keywords and one Gaussian Mixture Model (GMM) model for timestamps.
 Atomic Event Model. In event-based summarizati on, Filatova et.al. [13] de-note atomic events as triple patterns &lt;n m ,t i ,n n &gt; . The triples consist of an event term t i and two named entities n m ,n n . This event model was adopted by a number of work in event-based summarization [14] [15].
 Structural Event Model. In IE domain, event model is a structural template or frameset in MUC and ACE respectivel y. In MUC, the event extraction task is a slots filling task for given event templates. That is, extracting pre-specified event information and relating the event information to particular organization, person, or artifact entities involved in the event. In ACE, the event is a complex event structure involving zero or more ACE entities, values and time expressions. Generic Event Model. Jain and Westermann propose a generic event model E for event-centric multimedia data management applications [3]. The model is able to capture temporal aspect, spatial as pect, information aspect, experiential aspect, structural aspect and causal aspect of events [4].
 Ontology Event Model. Ontology is an explicit and formal specification of a shared conceptualization [16]. It is an important strategy in describing semantic models. ABC ontology [5] developed in Harmony Project 1 is able to describe event-related concepts such as event, situ ation, action, agent, and their relation-ships. PROTON, a base upper-level ontology developed from KIMO Ontology in Knowledge and Information Management (KIM) [6] platform, has the ability to describe events which cover event annotation types in ACE. In [7], a formal model of events Event-Model-F is proposed. The model can represent arbitrary occurrences in the real world and formally describe the different relations and interpretations of events. It actually blends the six aspects defined for the event model E and interrogatives of the Eventory system [17] to provide comprehensive support to represent time and space, objects and persons, as well as mereological, causal, and correlative relationships between events.

We mainly concern about ontology-b ased event models in this paper. On the basis of analyzing existing event models, we build NOEM for seman-tic modeling news event in this work. The main advantages of ontology-based modeling lie in two aspects: 1) It is able to provide common comprehension of domain knowledge, determine commonly recognized terminologies, and imple-ment properties, restrictions and axioms in a formulated way at different levels within certain domains. 2) Implicit knowledge can be acquired from known facts (events, entities and relations) by using inference engine of ontology. 3.1 The Design of NOEM Our goal of designing NOEM is to provide a basic vocabulary for semantic annotation of event 5W1Hs in news stories. So classes and properties are carefully selected to guarantee NOEM X  X  compactedness as well as to supply abundant semantics. In accordance with Jain X  X  generic event model, our model also tries to capture temporal, spatial, information, ex periential, structural and causal aspect of events. The proposed event model is able to cover information of events in three levels.  X  Event information: Based on existing event models, we select general  X  Event relations: Events (and the activities underlying them) may be re- X  Event media: Events can be described in various media, e.g. audio, video, 3.2 Main Concepts and Properties in NOEM In this section, we discuss main concepts, properties of NOEM and how they are used to represent 5W1H semantic eleme nts of an event in detail. The designed News Ontology Event Model is shown in Fig. 1, for the sake of clarity, only main concepts and properties are included.
  X  Happening is the superclass of all types of eventuality. It has four sub- X  Event is a concept denotes dynamic  X  X appening X . An event always has a  X  Situation describes static status preceding o r following an event. Properties  X  Constellation is a set of happenings with some relations among them, e.g.,  X  Agent is a concept to represent who and whom . It is the superclass of  X  X roup X   X  Time apparently represents when . Its subclasses  X  X ogicalTime X ,  X  X hysical- X  Place represents where . Its subclasses  X  X ogicalPlace X ,  X  X hysicalPlace X  and  X  X el- X  Document is the media aspect of an event. It has an URI (Universal Re- X  Topic is a concept in document level. It is related to category of a news story, Janez Brank et. al. [18] classified ontology evaluation methods into four cate-gories: (1) Comparing the ontology to a  X  X olden standard X ; (2) Using an ontology in an application and evaluating the results; (3) Comparing with a source of data about the domain to be covered by the ontology; (4) Evaluation is done by hu-mans who try to assess how well the ontology meets a set of predefined criteria, standards, requirements.

In this section, we evaluate the representative ability, the feasibility and ap-plicability of NOEM using a combination of above methods. 4.1 Comparison with Existing Event Models When designing the NOEM, we analyzed ex isting event-based systems and event models with respect to the functional requirements. These models are motivated from different domains such as the Eventory system for journalism, the structural representation of event in MUC and ACE , the event model E for event-based multimedia applications and Ontology-based models, such as the Event Ontology as part of a music ontology framework, ABC, PROTON for knowledge manage-ment and Event-Model-F for distributed event-based systems. All models are domain-dependant and they are too simple or too complicated for news event understanding task in this paper.

By analyzing the representative ability of existing Event models, we obtain six factors: action, entity, time, space, rela tions (here we concer n structural, causal and correlation among events) and event media. An overview of the analysis results and comparison to the features of NOEM along the representative ability is listed in Table 3. It shows that NOEM has a better representative ability than structural event models, i.e., probabilistic, atomic and MUC/ACE models. In addition, NOEM X  X  representative ability is as good as PROTON and Event-Model-F, two classic ontologies. At the same time, NOEM has a more compact design with only a few classes and relations, and is suitable to modeling Chinese News. 4.2 Manually Evaluation To evaluate NOEM in a practical environment, four postgraduate students are invited to manually analyze 6000 online news stories from XinHua and Peo-ple news agency. These news stories cover 22 topics of CNML such as politics, economy, military, information technology, sports and so on. With the help of headline of each news item, one topic sentence that contains the key event is identified. Then 5W1H elements of the key event are labeled from the head-line and the topic sentence according to the NOEM definition. The annotation result shows that 85 percent of onlin e news story can be described by NOEM appropriately.
 4.3 A Case Study Here we take a story from Xinhua news agency September 9, 2005 as an example to extract and describe the key event elements. The snippet of the news is shown in the left part of Fig. 2. We first use a machine learning method in our previous work [19] to identify the topic sentence about the key event of this story. And then we use a verb-driven method, along with Name Entity identification and semantic role labeling method proposed i n work [20] to get the key event X  X  5W1H information from the story.
From the key event of the news,  X  X hinese President Hu Jintao arrived in the Canadian capital of Ottawa on Thursday for a state visit. X , we get an ACE  X  Movement  X  event and its 5W1H elements (right part of Fig. 2). The extracted 5W1H semantic event informat ion, their types and semantic relations are denoted in RDF (Resource Description Framework) 2 triples. For the exam-ple event, we get triples &lt; Chinese President Hu Jintao, arrive, Canadian capital of Ottawa &gt; , &lt; arrive, isTypeof, Movement/Transport &gt; , &lt; Chinese President Hu Jintao, isTypeof, Person &gt; , &lt; 2005-09-08T00:00:00, isTypeof, Time &gt; ( Thursday is normalized as 2005-09-08) and &lt; Ottawa, isTypeof, Place &gt; . 4.4 Population of NOEM The proposed NOEM is built on Prot  X  eg  X  e 3 . By using a predefined template, an OWL (Web Ontology Language) 4 file is automatically generated in which triples are mapped to the concepts and relations according to NOEM. By this means, the event 5W1H elements can be populated into Prot  X  eg  X  e as instances.
A snippet of automatic generated OWL file of the example story is listed below. &lt;Event rdf:ID="NewsEvent_588"&gt; &lt;/Event&gt;
Besides the key event, &lt; Chinese President Hu Jintao, deliver, a written state-ment &gt; is also identified as a subevent and associated to the key event by rela-tionship  X  X ollows X . The automatically mapping and populating the 5W elements and relations into Ontology shows the feasibility and applicability of NOEM. In this paper, NOEM, an event Ontol ogy which describes concepts of 5W1H event semantic elements and relationships of events is proposed. NOEM is able to capture temporal, spatial, information, experiential, structural and causal aspect of an item of news. By taking advantage of logical reasoning ability of the NOEM ontology, the output of why and how elements together with relationships among who , what , whom , when and where of events can be used to build a multidimensional news event network. This will largely facilitate online news browsing in event and semantic level.

Our future work is to build a news events knowledge base and a semantic retrieval engine on NOEM. This will strongly support semantic information re-trieval on event level and other event level semantic applications.
 Acknowledgment. This work is supported by the National High-Tech Project of China (Grant No. 2012AA011101).
 Dependency grammars have received a lo t of attention in sentiment analysis (SA). One important advantage of dependency grammars is that they can di-rectly capture syntactic re lations between words, which are key to resolving most parsing ambiguities. As a result, employing dependency trees produces substan-tial improvements in sentiment analysis [12,6,10].

However, most dependency-based systems suffer from a major drawback: they only use 1-best dependency trees for featu re extraction, which adversely affects the performance due to parsing errors(93% [8] and 88% [3] accuracies for English and Chinese on standard corpora respectively). To make things worse, sentiment corpora usually consist of noisy texts from web, which will lead to a much lower parsing quality. As we will show, the tree-based systems still commits to using features extracted from noisy 1-best trees. Due to parsing error propagation, many useful features are left out of the feature set.
 To alleviate this problem, an obvious solution is to offer more alternatives. Recent studies have shown that many tas ks in natural language processing can benefit from widening the annotation pipeline: using packed forests [13] or de-pendency forests [20] instead of 1-best tr ees for statistical machine translation, packed forests for semantic role labeling [22], forest reranking for parsing [2], and word lattices reranking for Chinese word segmentation [4].

Along the same direction, we propose an approach that applies dependency forest, which encodes exponentially ma ny dependency trees compactly, for tree-based sentiment classification systems. In this paper, we develop a new algo-rithm for extracting features from dependency forest. Experiments show that our forest-based system obtains 5.4 poi nt absolute improvement in accuracy over a bag-of-words system, and 1.3 point improvement over a tree-based system on a widely used sentiment dataset [18]. Our research builds on previous work in the field of sentiment classification and forest-based algorithms. For sentiment classification, the design of lexical and syntactic features is a fundamental step. There has been an increasing amount of work on feature-based algorithms for this problem. Pang and Lee [18] and Dave et al. [9] represent a document as a bag-of-words; Matsumoto et al. [12] extract frequently occurring connected subtrees from dependency parsing; Joshi and Penstein-Ros X  X  [6] use a transformation of dependency relation triples; Liu and Seneff [10] extract adverb-adjective-noun relations from dependency parser output while Wu et al. [21] extract features from phrase dependency parser.
Previous research has convincingly demonstrated forests ability to offer more alternatives, which is useful to solving parsing error propagation problem, and has led to improvements in various NLP tasks, including statistical machine translation [13,20], semantic role labeling [22], and parsing [2]. In this section, we present the baseline sys tem that extracts features from 1-best dependency trees.

Figure 1(a) shows a dependency tree of the English sentence the film is sick, slick fun . The dependency tree expresses relation between words by head-dependents relationships of nodes. The arrow points from the dependent to its head. For example, in Figure 1(a), fun is the head of slick .

Inspired by the previous work [12], we ex tract connected subtrees from depen-dency trees. A connected subtree is a more general form obtained by removing zero or more nodes from the original dependency tree. Figure 1(b) shows some examples. The top left subtree is obtained by removing the node slick ,andthe bottom left subtree is obtained by further removing the node the .

Recent studies show that this kind of part ial subtrees could capture the syntac-tic information between words quite well [12,14,5]. For example, in Figure 1(b), to express the relation between the words film and fun , a subtree t does not only show the co-occurrence of film and fun , but also make sure that they are syntactically connected by the word is .

Since the number of dependency features tends to be very large, we only remain the features that occu r frequently in the corpus. As the tree-based system relies on 1-bes t trees, the quality of features might be affected by parsing errors and therefore ultimately results in classification mistakes. We propose to encode multiple dependency trees in a compact rep-resentation called dependency forest, which provides an elegant solution to the problem of parsing error propagation.

Figure 2(a) and 2(b) show two dependency trees for the example English sentence in Figure 1. The word sick can either be an adjective as an attribute of the film, or be a modificatory word like slick for the word fun. The two dependency trees can be represented as a single dependency forest by sharing common nodes and edges, as shown in Figure 2(c).

Each node in a dependency forest is a word. We assign a span to each node to distinguish among nodes. For example, the span of the word sick is (3,4) because it is the fourth word in the sentence. Since the seventh word fun dominates the word slick , the span is (5,7). Note that the position of fun itself is taken into consideration.

The nodes in a dependency forest are connected by hyperedges . While a edge in a dependency tree points from a dependent to its head, a hyperedge groups all dependents of their common head. For example, the hyperedge e 2 : denotes that both film 0 , 2 and fun 3 , 7 are dependents of the head is 0 , 7 .
Formally, a dependency forest is a pair V,E ,where V is a set of nodes, and E is a set of hyperedges. For a given sentence w 1: n = w 1 ...w n ,eachnode v  X  V is represented as w w i +1 ...w j . Each hyperedge e V is the head of the hyperedge and tails ( e )  X  V are its dependents.

We followed the work [20] to construct dependency forest from k-best parsing results, and transform a dependency forest into a hypergraph. In tree-based systems, we can extract d ependency subtrees by simply enumer-ating all nodes in the tree and combining the subtrees of their dependents with the heads. However, this algorithm fails to work in the forest scenario because there are usually exponentially many subtrees of a node.

To solve this problem, we develop a n ew bottom-up algorithm to extract dependency subtrees. Our approach often extracts a large amount of dependency subtrees as each node has many hyperedges. To maintain a reasonable feature set size, we discard any subtrees that dont satisfy two constraints: 1. appear in at least f distinct sentences in the dataset [12]; 2. the fractional count should not be lower than a pruning threshold p ; Here the fractional count of a subtree is calculated like in the previous work [13,20]. Given a tree fragment t , we use the inside-outside algorithm to compute its posterior probability: where root ( t ) is the root of the subtree, e is an edge, leaves ( t )isasetofleavesof the subtree,  X  (  X  )and  X  (  X  ) are outside and inside probabilities, respectively. For example, the subtree rooted at fun 3 , 7 in Figure 2(c) has the following posterior probability: Now the fractional count of the subtree t is where TOP is the root node of the forest. As a partial connected subtree might be non-constituent, we approximate the fractional count by taking that of the minimal constituent tree fragment t hat contains the co nnected subtree.
We observed that if a subtree appears in at least f distinct sentences, then each edge in the subtree should also occurs at least f times. According to this observation, we can first enumerate all edges that appear in at least f distinct sentences, then we can check whether the edge in this set when dealing with a head and a dependent. Take the edge sick, fun in Figure 2(c) for example, if the occurrence of the edge is lower than f , then any subtree that contain this edge could not be possible appear in at least f times.

Algorithm 1 shows the bottom-up algorithm for extracting subtrees from a dependency forest. This algorithm maintains all available subtrees for each node (line 12). The subtrees of a head can be constructed from those of its dependents. For instance, in Figure 2(c), as the subtree rooted at fun 3 , 7 is we can obtain another subtree for the node is 0 , 7 by attaching the subtree of its dependent to the node ( EnumerateSubtrees in line 8) Note that we only keep the subtrees that appear in at least f distinct sentences (line 6), and have a fractional count not lower than p (line 10). We carried out experiments on the movie re view dataset [18], which consists of 1000 positive reviews and 1000 negative reviews. To obtain dependency trees, Algorithm 1. Algorithm of extracting subtrees from a dependency forest. All subtrees should appear in at least f distinct sentences and have a fractional count not lower than p .
 we parsed the document using the Stanford Parser [7] to output constituency trees and then passed the Stanford constituency trees through the Stanford constituency-to-dependency converter [11].

We consider two baselines: 1 Unigrams : using unigrams that occur at least 4 times [18] 2 Unigrams+Subtree 1  X  best : Unigrams features plus partial connected sub-We construct dependency forest from 100-best parsing list. 1 We set the pa-rameter f = 10 for both dependency trees and forests, and pruning threshold p =0 . 01 for dependency forests. 2 All experiments are carried out using the Max-Ent toolkit 3 with default parameter settings. All results reported are based on 10-fold cross validation.

Table 1 shows the result on the movie review dataset. The first column Fea-tures indicates where the features are extracted from: 1-best dependency trees, 100-best list or dependency forests. The second column denotes the averaged number of extracted features, while the last column is the averaged time of sub-tree extraction. We compare our method to previously published results on the same dataset (rows 9-11), showing that our approach is very competitive. We find that using subtrees from 100-best list or forests achieve significant improve-ment over 1-best trees, validating our belief that offering more alternatives could produce substantial improvements. Using 100-best list produce only double sub-trees in over 100 times longer than using 1-best trees, indicating that a k-best list has too few variations and too many redundancies [2]. When incorporating unigrams features, forest-based system obtains significant improvement of 5.4 point in accuracy over the bag-of-words sy stem, and 1.3 point improvement over the tree-based system. An interesting finding is that combining subtrees from 100-best list and unigrams features doesnt achieve any improvement over 1-best tree. We conjecture tha t: (1) as most syntactic information is already captured by 1-best trees, using 100-best list can introduce little new information, (2) more noisy information would be introduced when extracting features from 100-best list, because there would be some low-quality parsing trees in the 100-best list (e.g. the trees at the foot of 100-best list). In contrast, we can extract new sub-trees from dependency forests, which could not be extracted from any single tree in 100-best list (e.g. a subtree that consists of two parts from two different dependency trees). On the other hand, with the help of fractional count pruning, we would discard most low-quality subtrees. In this paper, we have proposed to extrac t features represented as partial con-nected subtrees from dependency forests, and reduced the complexity of the extraction algorithm by discard subtrees that have low fractional count and occurrence. We show that using dependenc y forest leads to significant improve-ments over that of using 1-best trees on a widely used movie review corpus. In this work, we still select features manually. As convolution kernels could exploit a huge amount of features without an explicit feature representation [14,5,1,16,15,19], we will combine dependency forest and convolution kernels in the future.
 Acknowledgments. The authors were supported by 863 State Key Project No. 2011AA01A207. We thank the anonymous reviewers for their insightful comments.
 alphabet used specially to transliterate foreigner scripts especially the Sanskrit. The native Tibetan has 30 consonants and four vowels. The 30 consonants are  X  ,  X  , exactly and vice versa. This transliterating alphabet has 34 consonants and 16 vowels. 
The transliterating Tibetan is different from the native Tibetan in many ways. One difference is that the transliterating Tibetan has two kinds of collation. The first kind the collation of these characters follows the sorting rules of the transliterating Tibetan racters and the native Tibetan syllables, and the collation follows the sorting rules of the native Tibetan dictionary [2]. Huang et al. have evaluated each Tibetan letter or symbol with the Unicode collation element and proposed a general structure for collation of Tibetan syllables [4-5]. As a matter of fact, they just collate both the transliterating characters and the native Tibe-than 6600 transliterating Tibetan characters [6-7]. By far, it is still an open problem to Tibetan dictionary. To realize the second kind collation accurately , it is necessary to distinguish the trans-literating characters from the native Tibetan syllables correctly. 
It is easy to distinguish a transliterated sentence from a native Tibetan sentence. As shown in Fig. 2, there is an inter-syllable separator  X   X   X  between every two syllables of a native Tibetan sentence while there is no such separator in a transliterated sentence. Some transliterated characters or phrases ar e used as a common syllable in a native Tibetan orthography. Generally, a pre-composed character is a transliterating charac-ter if it meets one of the following conditions. or  X  X  .  X  ,  X  , or  X  . 4) A pre-composed character has two consonants, but the first consonant is none of  X  , characters are  X   X   X  ,  X   X  ,  X   X   X  , and  X   X   X  . 5) A pre-composed character has three consonants, but the first one is none of  X  ,  X  ,  X   X   X  ,  X   X  , and  X   X  . 6) A pre-composed character has more than three consonants. Examples of such characters are  X   X   X  ,  X   X   X  , and  X   X   X   X  . 7) A horizontal combination of several consonants, but there is no prefix consonant  X  X  X  X  . 8) A horizontal combination of a conson ant and a pre-composed character, but the consonant is neither the prefix consonant nor the suffix consonant. Examples of such combinations are  X  X   X  ,  X  X  ,  X  X   X  ,  X  X  , and  X  X  . 9) A horizontal combination of several pre-composed characters, but the last one is and then its collation is decided by those syll able series. Therefore, it is necessary to describe the syllable of transliterating characters. 3.1 The Collation Rules of the Trans literating Tibetan Dictionary consonant, and vowel and there are no concepts of prefix consonant, suffix consonant, and superscript consonant. Therefore, the phrases  X  X   X  ,  X  X  X  , and  X  X  X n belong to the chap-nants and two vowels. For example, the syllable  X  X  has two vowels. The first one is  X   X  first vowel either. corresponding syllable series  X  X  X  X  X n X  X  . 
The collation of the single transliterating syllable is as follows. 1) The syllables with  X  as the basic consonant are sorted as respectively). 2) The syllables with  X  as the basic consonant are sorted as 1). 3) The syllables with  X  as the basic consonant are sorted as 1). 3.2 The General structure of All Transliterating Characters As mentioned above, a transliterating syllable is a pre-composition of a basic conso-nant with no more than two foot consonants and no more than two vowels. So, it has a general structure as shown in Fig. 3. the native Tibetan dictionary and 2) collated with the rules of the transliterating cha-racter dictionary. 4.1 Collated with the Rules of the Tran sliterating Charac ter Dictionary the following five steps as shown in Fig. 4. Step 1: Decompose each transliterating character into syllable series first. 
Step 2: Expand each syllable further into th e letter series according to the sort order Step 3: Replace each letter in the letter series with the corresponding collation element. Step 4: Compress the collation element series. 
Step 5: Compare the two compressed collation element series and we have got the collation result of two transliterating characters. However, this paper just focuses on the first three steps. ly, compare the two letter series as we compare two English strings and we have got this kind collation. 4.2 Collated with the Rules of Native Tibetan Syllable Dictionary consonant are the postscript consonant and the post-postscript consonant respectively. ample, only the letters  X  ,  X  ,  X  ,  X  , and  X  can appear in the prescript position. 
A few native Tibetan syllables have two foot consonants. For example, the syllable  X  P. R. China on Tibetan Character Set, have two foot consonants. 
When a transliterating character is collated with the rules of the native Tibetan syl-lable dictionary, a generalized structure should be constructed so that it can represent Fig. 5 is such general structure. 
When a transliterating character compares with a native Tibetan character or firstly should be decomposed into a serial of transliterating syllables as shown in the middle column of Table 2; and then each syllabl e is decomposed into a letter series by following the sort order shown in the right part of Fig. 5. If there is no letter in a cer-tain position, a space  X   X   X  is used instead; Finally, compare the two letter series shown in the right column of Table 2. not so popularly; however, there are more than six thousands of them. Therefore, it is necessary to study the collation of these tr ansliterating characters. The paper proposes rating dictionaries. Based on the proposed structures, all transliterating characters can be collated successfully and effectively with the rules of two different dictionaries. Acknowledgment. This work is partially supported by NSFC under Grant No.60963016 and Key laboratory of Tibetan Information Processing, Ministry of Education of the People X  X  Republic of China. The authors also thank the anonymous reviewers for their invaluable comments and suggestions. The study of discourse structure plays a crucial role in language engineering, includ-ing but not limited to summarization, information extraction, essay analysis and scor-chine translation[1]. A common practice adop ted by present studies is to decompose the text into small units such as sentences, phrases and words, which are selected as teristics of the discourse structure are rarely exploited. 
Chinese discourses are characterized with a high frequency of anaphora, especially distribution rules of zero anaphora having been found are hard to formalize and hence and nouns, with quite little research on the resolution of zero anaphora[3]. With regard to the characteristics of Chinese discourses, Generalized Topic Theory[4] sets punctuation clauses (PClauses hereafter), which have clear boundaries, clauses are explicitly described. Within this framework, a stack model of the dynamic generation of topic clauses is devised, providing theoretical basis and formal approach for Chinese discourse analysis. constructed and a scheme for constructing a candidate topic clause(CTC) set has been the correct topic clause. Expe riments yield an accuracy rate of 73.36% for open test, which has great significance for discourse related Chinese processing. On the basis of sequences. 
In the rest of the paper, section 2 briefly introduces the Generalized Topic Theory and its concepts relevant to this study; section 3 describes the scheme for identifying the topic structure of a PClause sequence; section 4 presents the corpus, baseline and result and the analysis on it; and the last section provides a summary and future work. 2.1 PClause Sequence The basic unit of Chinese discourse is PClause, which is a string of words separated by punctuation marks of comma, semicolon, period, exclamation mark or question mark or direct quotation marks. [6] E.g.1. (Fortress Besieged by Ch'ien Chung-shu)  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X   X  X  X  X  X  X  X  X  giggle, were much better than their diplomats.) 1 quence can be represented as below: c 1 .  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  (These policemen knew no German,) c 2 .  X  X  X   X  X  X  X  X  X  X  (but were trying to flirt,) c 3 .  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  (made the Jews women giggle,) c 4 .  X  X  X   X  X  X  X  X  X  X  X  X  X  X  X  (were much better than their diplomats.) 
Some PClauses can stand alone as a sentence, having a complete structure of topic--comment. For example, in c 1 , the topic is  X   X  X  X  X  X  X  X  ( these policemen) X , the comment is  X   X  X  X  X  X  X  X  ( knew no German) X . But some PClauses, which may miss sentence components, cannot stand alone. c 2 , for example, is just a comment. Its topic  X   X  X  X  X  X  X  X  ( these policemen) X  is in c 2.2 Topic Structure The topic structure of Chinese is the syntactic structure of PClause sequence, which is composed of a generalized topic and one or more comments. A comment itself can be ture. Such a structure can be represented by indented new-line representation[6]. For instance, E.g.1 can be represented as below. 
In the above graph, what is quoted by the  X  X ] X  X arks is comment, the left of which is the topic. And what is quoted by the  X  X } X  X arks is the topic structure. The left part of c 1 is at x =0, and the other PClauses are indented to the right edge of its topic. 
In the indented new-line representation, if the x value of the topic of a topic clause is 0, then it is the outmost topic of the topic structure. 2.3 Topic Clause If all the missing topic information, including the outmost topic, of each PClause is filled up, then the complete structure is termed as a topic clause. clause are: c .  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  t c .  X  X  X  X  X  X  X  X  X  X  t c .  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  t c .  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  t
Below is the generating process of the above topic clauses. this topic in t 1 and concatenate the rest with c 2 , and we will have t 2 ; concatenate the rest with c 3 , and we will have t 3 ; this topic in t 3 and concatenate the rest with c 4 , and we will have t 4 . 
If we regard the beginning and the end of a topic clause respectively as the bottom of topic clause can be formalized by a stack model. with c . Hence a topic clause can be defined as following part of the paper, i will be referred to as PClause depth. following strategies: using stack model to generate CTCs, using Edit Distance to calculate for our work to devise the topic clause identification scheme of PClause sequence. 3.1 Identification Objective For a PClause sequence, if its topic clause sequence can be identified, then the topic paper is to identify the topic clause sequence for a given PClause sequence, viz., giv-tive is to identify the topic clause of each PClause t 1 ,...,t n . 3.2 Identification Process The process of identifying the topic clauses of a PClause sequence can be stored and represented as a tree. E.g.2 is the first four PClauses (in word-segmented form) from an article about  X  X aja kenojei X . The identification process of this PClause sequence is shown in Fig.1. 
E.g.2. raja kenojei (from China Encyclopedia) c 1 .  X  X  X   X   X  X  X  X   X  X   X  X  X   X   X   X   X  c 2 .  X   X  X   X  c 3 .  X   X   X  c 4 .  X   X  X  X   X  ( c 1 . raja kenojei is rajiformes rajidae raja de one species, c 2 . snout medium-sized, c 3 . tip projecting. c 4 . tail slim and long.) The topic clause of c 1 is t 1 , same as c 1 , which is the root node in Fig.1. generated as the CTCs of c 2 . [1].  X   X  X   X  [2].  X  X  X   X   X  X   X  [3].  X  X  X   X   X   X  X   X  [4].  X  X  X   X   X  X  X  X   X   X  X   X  [5].  X  X  X   X   X  X  X  X   X  X   X   X   X  X   X  [6].  X  X  X   X   X  X  X  X   X  X   X  X  X   X   X  X   X  [7].  X  X  X   X   X  X  X  X   X  X   X  X  X   X   X   X  X   X  [8].  X  X  X   X   X  X  X  X   X  X   X  X  X   X   X   X   X  X   X  [9].  X  X  X   X   X  X  X  X   X  X   X  X  X   X   X   X   X   X  X   X  ([1].snout medium-sized, [2].raja kenojei snout medium-sized, [3].raja kenojei is snout medium-sized, [4].raja kenojei is rajiformes snout medium-sized, [5].raja kenojei is rajiformes rajidae snout medium-sized, [6].raja kenojei is rajiformes rajidae raja snout medium-sized, [7].raja kenojei is rajiformes rajidae raja de snout medium-sized, [8].raja kenojei is rajiformes rajidae raja de one snout medium-sized, [9].raja kenojei is rajiformes rajidae raja de one species nout medium-sized,) 
They form the nodes on the second level in the tree shown in Fig.1. (For conveni-ence, only 3 nodes are displayed.) 
Similarly, the CTCs of c 3 can be generated by using c 3 and the CTC of c 2 (although t still uncertain yet, it must be a node on the second level in the tree). For instance, given a CTC on the second level in the tree, such as  X   X  X  X   X   X  X  X  X   X   X  X  shown in Fig.1 A part; given  X   X  X  X   X   X   X  X  (raja kenojei is snout medium-part. They form the nodes on the third level in the tree shown in Fig.1. By the same token, a tree of CTCs can be generated for the text  X  X  X  (raja kenojei).
Adopting proper strategies to calculate the value of each node in the CTC tree, we can then calculate the path value of each leaf node to the root node. The path with the largest path value can be found and the nodes on it from the root node to the leaf node are the topic clause sequence to be found. 
In this way, the task of identifying the topic clause of a PClause sequence is con-verted to searching the maximum path in the tree. 3.3 Recognition Algorithm Input: PClause Sequence c 1 ,...,c n Output: topic clause sequence t 1 ,...,t n 1. Generate CTC tree of a PClause Sequence constructed correspondingly, which has n levels. On each level, there will be a num-the sequence number of the node in level k , and n 2 is the sequence number of the fa-ther node in level k-1. Level 1: [&lt;1 ,ct 1 , 0,1,0&gt;], ct 1 =c 1 . k level will be generated by three steps. CTC, the definition of which please refer to section 3.4. Step 2: Pruning  X  to delete the nodes of low score by some strategies. 
Two pruning strategies are adopted. a. single-node-based pruning: For all the CTC nodes constructed from a node on value, then the shorter one is given priority. b. level-based pruning: If the number of nodes on level k-1 is greater than 50, then the top 50 nodes with largest path value are kept and the rest are deleted. 
Step 3: Sorting  X  After the above steps, the nodes on level k are sorted in descend-ing order by v value. The sorted nodes on this level are then numbered starting from one. 2. Generating CTC Sequence t &lt;k-1,ct r ,v r ,r,n r &gt;  X  tcs(k-1) , r=n j . 
So that t 1 =c 1 . 3.4 CTCs Scoring Function In the previous work[5], a Topic Clause Corpus (Tcorpus) is used and two approaches mum topic clause from the CTCs. corpus, whose similarity is marked as sim_CT(d) . For any two strings x and y , given that their similarity is sim(x,y) . sim_CT(d) is defined as From the CTC generating process it can be seen that the topic clause of a PClause is related to the PClause itself and the topic clause of precedent PClause which are the context of the topic clause. Therefore, when identifying the topic clause of a PClause, the context in which the topic clause is generated must be taken into account. 
Given a CTC d , the PClause d_c from which d is generated, and the topic clause of the PClause preceding d_c is d_t pre , the context similarity of d is defined as the information of topic clause t . ) 
Experiments have been done which separately adopted sim_CT(d) and ctxSim_CT(d) 11.3% higher (see section 4). For E.g.3: 
E.g.3. d_tc pre  X  A  X  X   X   X  H  X  H C  X  d_c  X   X  X  X   X   X   X   X   X  (for baiting.) t 1  X  A  X  X   X   X  H  X  X  X   X   X   X   X   X  st 1  X  A C  X  X   X  H  X  (usually A C is equipped with H,) t 2  X  A  X  X   X   X  H  X  H C  X  X  X   X   X   X   X   X  similar parts of them (the shaded words) has nothing to do with d_c . The correct topic clause is t 2 , which is identified by ctxSim_CT . The topic clause in Tcorpus most similar to t 2 in E.g.3 and its context are as below: t_tc pre  X  A  X  X  X  B C  X  C  X  (A has some B C is equipped with C,) t_c  X   X   X   X   X   X   X  (for baiting.) t  X  A  X  X  X  B C  X  C  X   X   X   X   X   X  and that d_t pre and t_t pre also have some components in common (similar components other words, ctxSim_CT is better than sim_CT in CTC evaluation. 4.1 Corpus modern Chinese general-purpose word segmentation system [7] developed by Beijing Language University is used for word segmentation and generalization. To ensure the accuracy rate for word segmentation, the original GPWS vocabulary bank, ambiguous word bank and the semantic tag bank are extended. 15 texts are used for test in the experiment. When identifying the topic structure of one text, the topic clauses in the rest 201 texts constitute the training corpus Tcorpus. topic structure consists of 15.59 PClauses. In the 717 PClauses, 452 share the compo-nent of fish names, a proportion of 63.04%. 4.2 Baseline obtained by simply concatenating the fish name (the title of the text) to the beginning of the clause. Therefore, the baseline is defined as baseline=number of PClauses whose topic is the text title/total number of PClauses (4) texts about fish, the number of PClauses whose topic clause is the PClause conca-tenated with the text title is 5786. Therefore, the baseline is 5786/9 508= 0.6085. 4.3 Evaluation Criteria fied is hitN, then the identification accuracy rate is hitN/N. 5.1 Experiment Result The result of open test on 15 texts is shown in Fig.2. If sim_CT(d) is used as the scor-ing function, the accuracy rate is 64.99%, 4.14 percent points higher than the baseline. accuracy rate reaches 76.25%, 15.44 percent points higher than the baseline. 5.2 Analysis (1) The reason for the low accuracy rate for texts about barbinae 
Most PClauses have only one topic clause, so that in the experiment there is only one correct answer for each PClause. However, some PClauses may have more than one CTCs that can be regarded as the correct answer. For example: 
E.g.4 . c .  X   X   X  X  X  X   X  ( have a spindle-shaped body, ) ctc 1 .  X   X  X   X   X   X  X  X  X   X  ( this subfamily have a spindle-shaped body, ) are 23 such  X  X istakes X  have taken place. Ta king this into consideration, the number 78.13%. Other texts also have similar issues. (2) On some levels in the CTC tree, there may be nodes with the same CTC string. In extreme cases, all the nodes in one level have the same CTC string. 
For example, on the 3 rd level in Fig.1, the node  X   X  X  X   X   X ( raja kenojei tip pro-jecting) X  appears three times. In some texts for testing, there are cases that in the CTC tree, some levels may have nodes that contain identical CTC information. If the CTC is not the correct one, and the topic information for the subsequent PClauses is absent, a chain of errors will be caused. 
Therefore, to ensure the heterogeneity of the nodes on each level is an issue to be considered in future work. A plausible appr oach could be that when constructing the nodes on each level in the CTC tree, same brother nodes, if any, will be merged into one node while keeping the total number of nodes on each level unchanged. The CTC tree will thus be transformed into a CTC graph, which preserves more path informa-tion with the space complexity unchanged. (3) The relation between accuracy rate and the PClause position(Sequence number of the PClause in the text). 
From the PClause identification process, esp. the construction process of CTC tree, clause identification. (4) The relation between the accuracy rate and the PClause depth Fig. 4 shows that the PClause depth may contribute to the decline of accuracy rate. There are as many as 139 PClauses with depth of 2, the accuracy rate for their topic clause identification being as low as 53.96%. This paper briefly describes Generalized Topic Theory, on the basis of which it pro-poses a research scheme for identifying the topic structure of PClause sequence. Cor-identified with satisfying experiment results. 
However, there are some aspects where this work needs to be improved. First, the scientific methods should be found to calculate the values reasonably. Second, it is a question how to keep heterogeneity of the nodes on each level in the tree. In addition, the achievement of topic clause identification in encyclopedia texts about fish can be extended to other encyclopedia corpora. Further efforts should be made to probe into the application of this experiment scheme to more fields. traction [1], question answering [2], co-r eference resolution [3] and document catego-rization [4]. Given a sentence and a predicate (either a verb or a noun) in a sentence, semantic arguments (roles) of the predicate. According to predicate type, SRL can be divided into SRL for verbal predicates (v erbal SRL) and SRL for nominal predicates (nominal SRL). 
Usually, there are two kinds of methods for SRL. One is feature-based methods, kernel-based methods, which represent a predicate-argument structure as a parse tree and directly measure the similarity between two predicate-argument parse trees in-stead of the feature vector representations. Although feature-based methods have been consistently performing much better than kernel-based methods and represent the state-of-the-art in SRL, tree kernel-based methods have the potential in better captur-on the constituent parse tree (CPT) structure. 
Although some feature-based methods [9-10] have attempted to explore structured information in the dependency parse tree (DPT) structure, few tree kernel-based me-thods directly employ DPT due to its sparseness in that DPT only captures the depen-dency relationship between two words. While both DPT and CPT are widely used to represent the linguistic structure of a sentence, however, there still exist some impor-tant differences between them. For example, DPT mainly concerns with the depen-sentence as done in CPT. Therefore, these two kinds of syntactic parse tree structures may behave quite differently in capturing different aspects of syntactic phenomena. 
In this paper, we explore a tree kernel-based method for Chinese nominal SRL us-tree (D-CPT). This is done by transforming DPT to a new CPT-style structure, using dependency relation types instead of phrase labels in the traditional CPT structure. In this way, our tree kernel-based method can benefit from the advantages of both DPT and CPT since D-CPT not only keeps the dependency relationship information in DPT but also retains the basic structure of CPT. Evaluation of Chinese nominal SRL on Chinese NomBank shows that our tree kernel-based method achieves comparable performance with the state-of-the-art feature-based methods. 
The rest of this paper is organized as follows: Section 2 briefly reviews the related draws the conclusion. on feature-based methods for SRL, please refer to Xue [11] and Li et al [12].  X  Tree Kernel-Based Methods for SRL by Collins and Duffy [13]. Motivated by this work, more and more tree kernel-based methods are proposed and explored in SRL since then [7,8,14]. further separates the PAF structure into a path portion and a constituent structure por-over these two portions. Zhang et al [8] proposes a grammar-driven convolution tree tween those non-identical substructures with similar syntactic properties. To our knowledge, there are no reported studies on tree kernel-based methods for SRL from the DPT structure perspective. However, there are a few related studies in other NLP tasks, such as semantic relation extraction between named entities [15] and co-reference resolution [16], which employ DPT in tree kernel-based methods and achieve comparable performance to the ones on CPT. For example, Nguyen et al [15] explore three schemes to extract structured information from DPT: dependency words (DW) tree, grammatical relation (GR) tree, and grammatical relation and words (GRW) tree.  X  SRL on Chinese With recent release of Chinese PropBank and Chinese NomBank for verbal and no-matically explore Chinese verbal and nominal SRLs using feature-based methods, given golden predicates. Among them, Xue and Palmer [17] study Chinese verbal SRL on Chinese PropBank and achieve the performance of 91.3 and 61.3 in F1-measure on golden and automatic CPT structures, respectively. Xue [18] extends their study on Chinese nominal SRL and attempts to improve the performance of nominal data for nominal SRL. Xue [11] further improves the performance on both verbal and nominal SRLs with a better constituent parser and more features. 
Since then, Li et al [12] improve Chinese nominal SRL by integrating various fea-tures derived from Chinese verbal SRL via a feature-based method on CPT, and achieve the state-of-art performance of 72.67 in F1-measure on Chinese NormBank. Li et al [19] further present a feature-base d SRL for verbal predicates of Chinese from the views of both CPT and DPT. To our knowledge, there are no reported studies on tree kernel-based methods for Chinese SRL from either CPT or DPT perspectives. dependency parsing, which produces different parse tree structures. In particular, the words as nodes and corresponding dependency types as edges. An edge from a word word in a dependency tree has exactly one parent except the root. 
Fig. 1 shows an example of the DPT structure for sentence (  X  X   X  X  X  X   X  X   X   X   X   X   X   X  X  X  /The Import &amp; Export Bank of China and the enterprise strengthen notated. Specifically, the nominal predicate  X   X  X  X  /cooperation  X  with  X   X   X  /strengthen  X  as the support verb has a argument,  X   X  X   X  X  X  X   X  X   X   X   X  /the Import &amp; Export Bank and the enterprise  X , as Arg0. In addition, W, R and G denote the word itself, its dependency relation with the head argument, and its part-of-speech structure. Then, we explore different ways to extract necessary structured information kernel for computing the similarity between two parse trees and its combination with improvement. 3.1 D-CPT Just as described in the introduction, both DPT and CPT have their own advantages. D-CPT not only keeps the dependency relationship information in DPT but also re-tains the basic structure of CPT. This is done by transforming the DPT structure to a new CPT-style structure, using dependency types instead of phrase labels in the tradi-D-CPT structure from the DPT structure: part-of-speech G as its left-most child while only keeping its contained dependency ponding to Fig. 1. 2. For each terminal node, create a new node by moving its contained word W as its (only) child while only keeping its contained part-of-speech. Fig. 2(b) illustrates an example of the resulted parse tree, corresponding to Fig. 2(a). 3.2 Extraction Schemes more structured information would be provided at the risk of more noisy information. 
In our study, we examine three schemes for this purpose, considering the specific DPT, these schemes can directly encodes the argument structure of lexical units popu-lated at their nodes through corresponding dependency relations. 1) Shortest path tree (SPT) 
This extraction scheme only includes the nodes occurring in the shortest path con-governing node. Fig. 3(a) shows an example of SPT for nominal predicate  X   X  X  X  /cooperation X  and argument candidate  X   X   X  /enterprise X . 2) SV-SPT (Chinese) NomBank adopts the same predicate-specific approach in representing nominal predicate-specific phenomena, such as support verbs, which cover much useful information in determining the semantic relationship between the nominal indicate the support verb of the nominal predicate. Fig. 1 includes an example support verb  X   X   X  /strengthen X , in helping introduce the arguments of the nominal predicate  X   X  X  X  /cooperation X . Normally, a verb is marked as a support verb only when it shares some arguments with the nominal predicate. Statistics on NomBank and Chinese NomBank shows that about 20% and 22% of arguments are introduced via a support predicate and its arguments in the D-CPT struct ure, e.g. the one as shown in Fig. 2(b), new structure as SV-SPT. 3) H-SV-SPT 
It is well proven that the head argument of the argument candidate plays a critical role in verbal SRL. In our study, we also consider the head argument information in formation to SV-SPT. We call the new structure as H-SV-SPT. 3.3 Kernels Given a parse tree structure, this paper employs the well-known convolution tree ker-nel [13] to compute the similarity between two parse trees. In principle, the convolu-feature space. 
Besides, in order to capture the complementary nature between feature-based me-thods and tree kernel-based methods, we combine them via a composite kernel, which structure and a feature-based linear kernel KL as follow: Table 1 shows a list of features in the feature-based linear kernel extracted from the CoNLL-2009 shared tasks, which aim at performing and evaluating SRL using a de-pendency-based representation for both syntactic and semantic dependencies on Eng-lish and other languages. 4.1 Experimental Setting Following the experimental setting in Xue [11] and Li et al [12], 648 files (chtb_081 to 899.fid) are selected as the training data, 72 files (chtb_001 to 040.fid and chtb_900 to 931.fid) are held out as the test data, and 40 files (chtb_041 to 080.fid) as the de-velopment data, with 8642, 1124, and 731 propositions, respectively. 
To save training time, we use a simple pruning strategy to filter out the dependency effectively reduces the number of instances for semantic role labeling by approx-imately 2-3 folds at the risk of 2% loss of semantic arguments. After pruning, we first do argument identification for those remaining candidates, and then classify the posi-tive ones into their corresponding semantic roles. We use the SVM-light toolkit with the convolution tree kernel function SVMlight X  TK as the classifier. In particular, the training parameters C (SVM) and  X  (tree kernel) are fine-tuned to 4.0 and 0.5 respectively. For the composite kernel, the coefficient  X  strategy to implement multi-class classification, which builds multiple classifiers so as to separate one class from all others. The final decision of an instance in the multiple binary classifications is determined by the class which has the maximal SVM output. 
To have a fair comparison of our system with the state-of-the-art ones, we use the widely-used segment-based evaluation algorithm, proposed by Johansson and Nugues 0.05, respectively. 4.2 Experimental Results on Golden Parse Trees Table 2 shows the performance of our tree-kernel-based method using different ex-traction schemes on the D-CPT structure. Here, the golden CPT structure is converted 2009 shared task. 
Table 2 shows that: 1) SPT achieves the performance of 65.98 in F1-measure with a much lower recall 2) SV-SPT achieves the performance of 69.89 in F1-mesure. This means that SV-3) H-SV-SPT further slightly improves the performance by 0.43 (&gt;) in F1-measure, 
Table 3 illustrates the performance comparison with different kernel setups on gol-den parse trees. It shows that: 1) Our tree kernel on the new D-CPT structure using the extraction scheme of H-4.3 Experimental Results on Automatic Parse Trees testing process. In this subsection, we evaluate the performance using automatic parse adopted by the CoNLL-2009 shared task. Table 4 and Table 5 present the perfor-mance on automatic parse trees. 
Table 4 and Table 5 show that: 4.4 Comparison with Other Tree Kernel -Based Methods on DPT Structure Nguyen et al [15] propose a dependency words (DW) tree, a grammatical relation (GR) tree, and a grammatical relation and words (GRW) tree, extracted from the DPT their work, the DW tree is simply constituted by keeping the words in the DPT struc-pendency relations. The GRW tree is formed by combining the DW and GR trees, where the latter is inserted as a father node of the former. Table 6 compares our D-CPT structure with the DW, GR and GRW trees on Table 6 shows that even SPT, extracted from the D-CPT using the simplest scheme, pendency information of the DPT structure but also retains the CPT structure. 4.5 Comparison with Other Systems Finally, Table 7 compares our proposed method with the state-of-the-art ones on Chi-nese NomBank, Xue [11] and Li et al [12]. Both of them are feature-based ones with various features derived from the CPT structure via extensive feature engineering. 
Table 7 shows that our tree kernel-based method achieves comparable performance with the state-of-the-art feature-based ones on either golden parse trees or auto parse schemes to extract necessary information from D-CPT. It will be easy to incorporate other useful information, such as competitive information from other argument candidates. 4.6 Experimentation on the CoNLL-2009 Chinese Corpus To further illustrate the effectiveness of the novel DR-CPT structure for better repre-sentation of dependency relations in tree kernel-based methods, we also do the expe-rimentation on the CoNLL-2009 Chinese corpus. 
Since most predicates in the CoNLL-2009 Chinese corpus are verbal and do not Furthermore, we only select those simple features widely used in CoNLL-2008 and CoNLL-2009 shared tasks in the composite Kernel. 
Predicate disambiguation is a sub-task of the CoNLL-2009 shared task. In order to better compare the results of SRL-only, we simply employ the predicate disambigua-tion module as proposed by Bjorkelund et al [20], who obtained the best F1 score on the Chinese corpus. Table 8 compares the performance of different kernel setups on the CoNLL-2009 Chinese corpus. It shows that: 1) Our tree-kernel method achieves comparable performance with Meza-Ruiz and 2) Our composite kernel (without global re-ranking) achieves comparable perfor-This paper systematically explores a tree kernel-based method on a novel D-CPT structure, which employs dependency types instead of phrase labels in the traditional transforms the DPT structure into a CPT-style structure. Generally, D-CPT takes the advantages of both DPT and CPT by not only keeping the dependency relationship information in DPT but also retaining the basic structure of CPT. Furthermore, sever-for nominal SRL and verbal SRL (CoNLL-2009 corpus). Evaluation shows the effec-tiveness of D-CPT both on Chinese NomBank and CoNLL-2009 corpus. 
To our knowledge, this is the first research on tree kernel-based SRL on effectively exploring dependency relationship information, which achieves comparable perfor-mance with the state-of-the-art feature-based ones. In future, we will explore more necessary structured information in the novel D-CPT structure. Besides, we will explore this structure to similar tasks, such as seman-tic relation extraction between named entities and co-reference resolution. Acknowledgements. This research is supported by Project BK2011282 under the Natural Science Foundation of Jiangsu Province, Project 10KJB520016 and key Project 11KJA520003 under the Natural Science Foundation of Jiangsu Provincial Department of Education. CWS (Chinese Word Segmentation) is a fundamental task in Chinese Language processing. In recent years, widespread atte ntion has been paid to CWS. Researchers methods. Meanwhile, the Chinese word segmentation evaluations organized by SIGHAN (Special Internet Group of the Association for Computational Linguistics) with uniform training and testing data to compare their different methods of CWS in the same test platform. In previous SIGHAN Bakeoff, most of the systems with high-performance are based on machine learning methods to implement sequence labeling [1-2]. Among those methods, the character-based labeling machine learning methods [3-5] has got more and more attention and become the mainstream technology of CWS. However, Refs. [6-8] employed another sequence tagging based machine learn-ing methods, namely, a word-based segmentation strategy, which is also based on the character-base sequence annotation. containing lots of new words, has been generated, which has brought many challenges to the CWS. Although many methods have shown impressive results in some segmen-tation evaluation tasks, they are limited to corpus on specific area. Their accuracy will vast majority of the texts, which need to be segmented, do not have feature tags, such CWS system can contribute the maximum value [9]. Therefore, SIGHAN-CIPS has including computer, medical, financial and literary. The CWS systems need to be adaptive to different domains by training on only one domain corpus, namely, the so-called cross-domain CWS. One important thing the Cross-domain CWS should take into account is that there are many common-used words and terminologies in a usually regarded as OOVs in other areas . Different from common OOVs, most of those territoriality OOVs belong to a specific area, and usually appear several times in the context of their respective areas. No matter how large the vocabulary of the seg-cross-domain segmentation method based on a joint decoding approach which com-bined the character-based and word-based CRF models, made good use of the chapter evaluation of SIGHAN Bakeoff 2010, some other excellent cross-domain word seg-mentation systems emerged. Among those systems, Ref. [10] introduced a multi-layer CWS system based on CRFs, integrating the outputs of the multi-layer CWS system and the conditional probability of all possible tags as the features by SVM-hmm. This system achieved the best performance in the opening tests, while it is a little bit com-plicated. In Ref. [11], the hidden Markov model HMM (Hidden Markov Models) was used to revise substrings whose marginal probability was low, and achieved high performance in both closed and open tasks, but its recall of OOV was not outstanding. Ref. [12] proposed a new CWS approach using the cluster of Self-Organizing Map networks and the entropy of N-gram as features, training on a large scale of unlabeled corpus, and it obtained an excellent performance. However, most of the participating systems are dealing with the OOVs, which have their own distinct territorial characte-ristics, as the general ones instead of the cross-domain ones on the basis of ensuring the overall performance of the CWS. However, most of the participating systems are dealing with the OOVs, which have their own distinct territorial characteristics, as the performance of the CWS. tion of a candidate word and can affect the cost factor of the candidate words. Those candidate words are selected by the character-based CRFs. At the same time, we utilize the information of the synonym in the system dictionary instead of the information of the OOVs in the candidate words, because of the similarity of syntax taking full advantage of the contextual information and the synonym information word-lattice and select the best path as the final segmentation results. Cross-Domain CWS algorithm. Section 4 shows the experimental results. Finally, some conclusions are given in Section 5. Conditional random fields (CRFs), a statistical model for sequence labeling, was first mainly use to achieve global optimum sequence labeling. It is good enough to avoid label bias problem by using a global normalization. 2.1 Character-Based and Word-Based CRFs In previous labeling task of character-based CRFs, the number of the characters in the word-lattice can well represent this phenomenon. A word-lattice can not only express all possible segmentation paths, but also reflect the different attributes of all possible lexical analysis. Our paper adopt the word-lattice based CRFs that combines the character-based CRFs and the word-based CRFs, and specifically, we put the candidate words se-date words in the word-lattice using word-based CRFs model. When training the word-lattice based CRFs model, the maximum likelihood estimation is used in order to avoid overloading. In the end, Viterbi al gorithm is utilized in the decoding process which is similar with Ref. [6]. 2.2 Feature Templates The character-based CRFs in our method adopt a 6-tag set in Ref. [15] and its feature subscripts -1, 0 and 1 stand for the previous, current and next character, respectively. Four categories of character sets are predef ined as: Numbers, Letters, Punctuation and Chinese characters. Furthermore, the Accessor Variety in Ref. [16] (AV) is applied as global feature. 
Two kinds of features are selected for the word-based CRFs, like Ref. [6]: unigram features and bigram features. The unigram ones only consider the attributes informa-tion of current word, and bigram ones are also called compound features, which util-ize contextual information of multiple words. Theoretically, the current word X  X  context sliding window can be infinitely large, but due to efficiency factors, we define where W stands for the morphology of the word, T stands for the part-of-speech of the words, and subscript 0 and subscript 1, respectively, stand for the former and the latter of two adjacent words. words selected by the N-Best paths of the character-based CRFs into the word-lattice, dictionary and the OOVs can be treated equally by the character-based CRFs, which because too many incorrect candidate words will be added into the word-lattice if we chose more than 3-Best paths, which not only put bad impact on the performance of paths, the segmentation system does not work well on recalling the OOVs. can not get from the system dictionary, then it will be treated as one of four different spectively, conferred as a noun, strings, numbers, punctuation. Additionally, the cost dictionary. 
Taking the characteristics of the territorial OOVs into account, we apply the domain OOVs. 3.1 Contextual Information The territorial OOVs may repeatedly emerge in the specific domain, but it is hard to words. This approach is mainly based on the following assumptions: Assumption 1: The occurrence of a word will increase the possibility of emerging of the word in the same chapter. contexts, then it is probably a word, in that case, the Contextual Variable is proposed ( Cost ), the frequency of being a candidate word ( Frequency ), the frequency of being the node in the final segmentation path ( rNum ). 
The acquisition of the contextual information is throughout the entire segmentation algorithm, and the specific process is as follows: 
Firstly, put all the candidate words w included by 3-Best paths into the set S ( w 1 , w , ..., w n ). Secondly, search for each word w in set S from the system dictionary, if exists, then the information in the dictionary, such as the POS, the cost and so on, of then the Frequency in the table of the word increases by 1, and if not neither, then we above. At last, repeat these steps until the last word w n in set S has been searched. 
It can be seen from the above process that the higher the frequency of the candidate word is, the more likely it tends to be a word. Considering that the Frequency and the where cost 0 (w) stands for the original cost of the words. 3.2 Semantic Information tions are limited, so we utilize the synonym relations, one kind of semantic informa-sentence environment. When building the word-lattice, we propose the synonym in-formation to obtain the property and cost of the candidate words selected by the cha-racter-based CRFs via selecting the 3-Best paths, because the property and the cost of OOVs can not be found in the system dictionary, but can be substituted by the infor-mation of their synonyms. 
To illustrate, the word fragment "  X  X  X  ", an Out-of-vocabulary, is in the word-lattice, but not in the system dictionary. So we can not get the information of the can-example, the information of the word "  X  X  X  ", a synonym of the candidate word "  X  X  X  ", can take the place of the information of "  X  X  X  ". 
The semantic resources we used in this paper is synonym forest (extended version), synonym forest with five-level coding, for each word information, there is a eight bit semantic encoding, which can represent each single word in the synonym forest. ters, the second level with lowercase letters, the third level with two bytes of decimal cimal coding, the end with the sign of "=", "#" and "@". The specific coding rules are shown in Table 1: 
Except for the synonym and the classification information, the synonym forest also includes some self-governed words, which do not have any synonyms. In order to enhance the search efficiency, we delete those self-governed words. Because the clos-er the distance of two synonym sets are, the more similar their meanings are, we fol-low the principle of proximity when search for the synonym of the candidate words. 
The search process is as follows: first, find the synonym set of the candidate word, and then look up each synonym of that syno nym set into the system dictionary to find if the synonym exists. If there it is, then we will replace the candidate word with the synonym and the information of it, and if not, then the fifth level of the synonym sets will be searched, and if not neither, then the fourth level. If the fourth level does not than search further. There are two reasons, one is the efficiency factor, the other one is that if the set of the word is too far away, the meaning of the words in two different sets will be much different, so we would rather giving it up than using it and bringing a negative impact. 3.3 Word Segmentation Process With the contextual information and synonyms information added, the cross-domain word segmentation process is as follows: Step1. Put all the candidate words in 3-Best paths selected by the character-based CRFs model into the word-lattice. lattice, which is divided into four cases to deal with:  X  If the candidate words are in the system dictionary, then assign the properties and cost of the words in the system words are not in the system dictionary, but in the dictionary of contextual information, siged to the candidate words, and a weight value, calculated by Eq. (1), will be added to the cost of the candidate words.  X  If the candidate words is not in the system dic-tionary, neither in the contextual information dictionary, then we will search the syn-cases are not suitable for the candidate words, then the candidate words will be classi-fied according to the classification mentioned above. 
Step3. To find the optimal path, the least costly path of word segmentation, in the word-lattice using the Viterbi algorithm acco rding to Eq. (4), and the values of Tran-all feature functions are binary ones, the cost of the word is equal to the sum of all the weight of the unigram features about the word, and the transition cost is equal to the sum of all bigram features about the two parts of speech. and factor is the amplification coefficient. which can contribute to the follow-up Step3 to select the best path. 4.1 Data Set system dictionary and synonym forest is used in our method, without using any other manually annotated corpus resources. Thus, the experiment results are evaluated by P (Precise), R (Recall) and F-value. The system dictionary we used is extracted from the People X  X  Daily from January to June, in 2000, containing 85000 words, with the POS being the Peking University POS system. The word-based CRFs model is trained by the corpus with POS tag provided by the ev aluation, which is from the People's Daily of January, in 1998). 4.2 Experimental Results described above, we have conducted four groups of experiments. Experiment 1 is the base experiment that does not include these two types of information. Experiment 2 is the +CV experiment with only contextual information added. Experiment 3 is the +CiLin experiment that add only synonyms information. Experiment 4 is the experi-ment with both two types of information added. 
Table 2~5 give the segmentation results of four groups of experiments, respective-be clearly seen from Table 2 to Table 5 that the performance in F-value and R oov im-the improvement is more considerable when adding both of the two information si-multaneously, with R oov increasing by 1.6 to 5.6 percentage. 
The following sentence fragments can help us analyze the impact of contextual in-formation on the CWS:  X   X  X  X  X  X  X  X  X  X  X  X  X  X   X  X  X  X  X  (Shizuka Kamei)  X  X  X  (3  X  19  X  )  X  X  X  X   X  ......  X   X  X  X  X  X  X   X  X  X  X  X  X  X  X  X  X  X  X  X  X   X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  ......  X   X   X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  ......  X   X  X  X  X  X  X  X  X  X  X  X  X  X  X   X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  ......  X  X  X  X   X  X   X   X  X  X  X  X  X  X  X  X  X  X  X   X  X  X  X  X  X  X  X  X  X  ...... X . 
In the above five sentence fragments, the word  X   X  X  X  X  X   X (name) appears five tion system, only three times that the word  X   X  X  X  X  X   X  is segmented correctly, while it is cut correctly all five times after adding the contextual information. Therefore, the contextual information is very helpful to iden tify such candidate words that repeat in a occurrence in the previous paragraph. systems of the SIGHAN Bakeoff 2010 evaluation in F-value and R oov . The experi-better than the best results of the SIGHAN Bakeoff 2010 evaluation in the three areas of the computer, medicine and finance. by the character-based CRFs, including the morphology of the word, the part-of-approximate the cost of the candidate word in the entire path, we replace the property based CRFs, and add all the words included by the 3-best paths into the word-lattice. word-lattice to recall more OOVs. At last, the word-based CRFs are utilized to select the least costly path from the word-lattice as the final segmentation results. 
Our method not only take full advantage of character-based CRFs model to gener-words. Our method is evaluated by the simplified Chinese domain-adaptive testing data from SIGHAN Bakeoff 2010. The experimental results show that the F-value and the recall of OOVs of the testing data in Computer, Medicine and Finance domain are higher than the best performance of SIGHAN Bakeoff 2010 participants, with the recall of OOVs of 84.3%, 79.0% and 86.2%, respectively. Acknowledgements. This work has been supported by the National Natural Science Foundation of China (No.61173100, No.61173101), Fundamental Research Funds for the Central Universities (DUT10RW202). The authors wish to thank Jiang Zhenchao, the design and editing of the manuscript. Chinese Word Segmentation (CWS) methods can be roughly classified into three types: dictionary-based methods, rule-based methods and statistical methods. Due to the large number of new words appearing constantly and the complicated phenomena in Chinese language, the expansion of dictionaries and rules encounter a bottleneck. The first two methods are therefore difficult to deal with the changes in language usages. Since the statistical method can easily learn new words from corpora, Chinese Word Segmenta-tion systems based on such methods can achieve high performance. For this reason, the statistical method is strongly dependent on annotated corpora. Theoretically, the larger amount of the training data with higher quality annotation will bring about the better performance for Chinese Word Segmentation systems. 
Texts to be processed may come from different domains of the real world, such as news domain, patent domain, and medical domain, etc. When switching from one domain to another, both vocabulary and its frequency distribution in texts usually vary because the ways of constructing words from characters are different. This fact brings great challenges to Chinese Word Segmentation. The accuracy of a developed system other different domains. 
A solution is to develop domain-specific system for each domain by using corres-ponding annotated data. In application, the target domain of a text to be processed is work. At present, a large amount of annotated data with high quality is not available for each domain and therefore it is not practical to develop domain-specific system in this way. 
In order to solve the problem of the domain adaptation for Chinese Word Segmen-tation, many methods have been proposed, such as data weighting algorithm and semi-supervised learning algorithm. (Meishan Zhang et al., 2012) propose an approach that can incorporate different external dictio naries into the statistical model to realize domain adaptation for Chinese Word Segmen tation. However, these methods have resources readily available. 
This paper proposes an approach in which n-gram features from large raw corpus are explored to realize domain adaptation for Ch inese Word Segmentation. Compared with annotated corpus and domain-specific dictionary , a raw corpus is more easily to obtain. Our experimental results show that the proposed approach can effectively improve the ability of the Chinese Word Segmentation system for domain adaptation. menting sequential data, which is first proposed by (John Lafferty et al.,2011) on the basis of the maximum entropy models and hidden Markov models. CRF are undirected graphical models in which the parameters are estimated by maximizing the joint near-chain CRF is most widely used in machine learning tasks. 2.1 Conditional Random Fields sequences. The most probable label sequence for an input X can be efficiently determined using the Viterbi algorithm. For the sequence labeling task like Chinese Word Segmentation, CRF and ME per-formed better than HMM. In addition to the advantages of the discriminative models, CRF optimizes parameters and decodes globally by taking state transition probabilities into account and consequently can avoid label bias problem. CRF is one of the most effective machine learning models for sequence labeling task. We use CRF++ (version 0.55) 1 in this paper. 2.2 Tag Set and Feature Template We apply CRF to Chinese Word Segmentation by regarding it as a sequence labeling task. This is implemented by labeling the position of each Chinese character in word best performance among the tag sets for Chinese Word Segmentation. Therefore we also use the same 6-tag set, whose definition is descripted in Table 1 in detail. 
Following the work of (Low et al., 2005), we adopt feature templates and a context of five characters for feature generation. The feature templates used in our model are characters. Four types are defined in Table 3.We call the information as basic features. In this paper, n-gram refers to a sequence of n consecutive Chinese characters. A word can be considered as a stable sequence of characters. In a large enough corpus, words with some meanings will occur here and there repeatedly. This implies corresponding peated in a large number of texts are more likely to be words. This is the basis on which shortage of large annotated corpus, an approach proposed in this paper can be applied for automatic domain adaptation by exploiting the n-gram features from the raw corpus of the target domain. 
In this work, we examined two kinds of n-gram features: n-gram frequency feature and AV feature. Our experimental results show that these simple statistical features are indeed effective in improving the ability of CWS system for domain adaptation. 3.1 N-gram Frequency Feature We define n-gram frequency as the number of occurrences of n-gram in a corpus. The reason for considering this information is that the higher the frequency of n-gram, the greater the possibility of it being a word. 
N-gram frequencies are extracted from raw corpus of target domain for (2=&lt;n&lt;=5) efficiency in computing, n-grams whose frequency values are less than five are filtered out. In order to alleviate the sparse data problem, we group all the frequency values into three sets: high-frequency (H), middle-frequency (M), and low-frequency (L) (Yiou Wang et al, 2011). The grouping way are defined as follows: if the frequency value of a n-gram is represented as H; if it is between top 5% and 20%, it is represented as M, otherwise it is represented as L. In this way, n-gram frequency lists are produced. We regard the n-gram as words in the following processing. character. From a candidate word, a feature is generated in the form of  X  X -B X , where A is the position of the current character in the candidate word and B is the frequecy of this candidate word. Then, the feature gene rated from each candidate word is conca-tenated with each other by  X  X  X  as one n-gram frequency feature. Note that the conca-tenating order follows the position orders of the current character in candidate words, i.e. B  X  B 2  X  B 3  X  M  X  E, for standardization. 
Fig.1 shows the generation process of 2-gram frequency feature for the sentence  X  (numeral), displayed in a square frame, is generated as follows. First, candidate words  X   X  X   X  (number) and  X   X  X  X   X  (data) are retrieved from the 2-gram frequency list. From the candidate word  X   X  X   X  a feature  X  X -M X  is generated because its frequency is  X  X  X  and  X   X   X  is the last character of the word. After that, another feature from the candidate word  X   X  X  X   X  is generated as  X  X -H X  because the frequency is  X  X  X  and  X   X   X  is the first character of the word. Finally, the 2-gram frequency feature is represented as  X  X -H|E-M X . 3.2 N-gram AV Feature AV (Accessor Variety) is a statistical standard used in (Feng et al., 2004) to determine whether a character sequence is a word when extracting words from Chinese raw texts. (Hai Zhao and Chuyu Kit, 2007; Hai Zhao and Chuyu Kit, 2008) has explored an approach to extract AV global features from raw corpus for CRF learning. Following features for CRF learning and on avoiding data sparseness at the same time. Different from the n-gram frequency  X  the n-gram AV has a selection for frequency. The main idea of the AV is that if a character sequence appear in a variety of context, then the sequence is likely to be a word. The AV of a sequence s is defined as: cessor and successor of s . following the grouping way described in 2.1, the AV feature are grouped into three sets, high-frequency (H), middle-frequency (M), and low-frequency (L) and thus n-gram AV lists are produced. In generation of n-gram AV feature, candidate words containing current character will be retrieved out from the n-gram AV lists and then features of the candidate words are concatenated as the final n-gram AV feature for CRF training and decoding. In order to verify the proposed approach, we specify patent domain for CWS to adapt to. The raw corpus of the patent domain is taken from the Chinese part of the NTCIR-9 1 Chinese-English parallel patent description sentences. Such formed Chinese patent corpus consists of 1 million sentences. There are two phases in the domain adaptation implementation, construction of n-grim statistical information base and generating of n-gram features. In the first phase, n-gram frequency features and n-gram AV features are extracted from the corpus and n-gram st atistical information base including n-gram frequency lists and n-gram AV lists is produced.In the second phase, n-gram features are generated for the sentences. 4.1 Construction of N-gram Statistical Information Base According to the definitions of n-gram frequency feature and the n-gram AV feature described in section 2, we extracted character sequences of n-gram (2=&lt;n&lt;=5) from the overview of the construction of n-gram statistical information base is shown in Fig.2. 4.2 Generation of N-gram Features sentences. The method of generating n-gram features has been described in 2.1 and 2.2, respectively. The framework of domain adaptation of CWS system to the patent domain is shown in Fig.3. The part surrounded by the dotted line is the core component for the CWS system to adapt to the patent domain. Chinese Word Segmentation, we evaluate the segmentation results of the new CWS system incorporated with the n-gram features of the patent domain and then compare the results with those of the baseline CWS system that uses only basic features. 5.1 Data We used the Penn Chinese Treebank (CTB) as annotated corpus and defined data sets as follows: chapter 1-270, chapter 400-931 and chapter 1001-1151 for training data set; chapter 270-300 for test data sets. The proportion of unknown words is 3.47%. Since the data of the corpus is mainly from newswire, the domain of CTB may be regarded as news domain. 
The unlabeled data of the patent domain is the Chinese patent corpus. The n-gram statistical information base is built from this raw corpus. Chinese patent corpus and manually annotated word segmentations following the specification of Penn Chinese Treebank Project. As a result, 10636 Chinese words are unknown words in this patent test data is 22.4%. 5.2 Results and Analyses We used recall (R), precision (P), and F 1 as evaluation metrics and also measured the recall on OOV (R OOV ) tokens and in-vocabulary (R IV ) tokens. 
Table 4 shows the performances of the baseline system on the test data of CTB and the patent domain. The baseline system is developed on the training data of CTB by using the basic features. The proportions of unknown words in test data of CTB and the patent domain are respectively 3.47% and 22.4%. 
Table 4 shows that the baseline system performed very well on CTB test data. This can be explained that the test data and the training data are from the same domain, i.e. the news domain. When test data changed to the patent domain, F 1 value of the baseline system greatly decreased to 83.89% from 97.62%. For a more detailed investigation, R results demonstrated the serious impacts brought to the performance of Chinese Word very important for Chinese Word Segmentation. 
The performances of the CWS systems developed by using the proposed approach are shown in Table 5, where  X  X  X  refers to using n-gram frequency feature and  X  X  X  refers to using n-gram AV feature. The test data is the 300 sentences of the patent domain. For Table 5. The results show that both n-gram frequency feature and n-gram AV feature contributed to the improvement in performance from the view of each metric, and that the combination of (a) and (b) achieved further improvements. 
Through the observation of Table 4 and Table 5, we may conclude as follows.  X  The impact of interdisciplinary on Chinese Word Segmentation is very obvious,  X  The n-gram features including n-gram frequency feature and n-gram AV feature  X  In terms of F 1 measure, the improvement contributed by n-gram AV feature is  X  N-gram features can effectively increase the recall of OOV in CWS system. It is further observed that some scientific and technical terms were successfully recalled after n-gram features of the raw corpus have been added into the system. For instance, one manually annotated sentence  X   X  /  X  X  X  X  /  X  X  X  /  X  X  X  X  /  X  /  X  X  X  /  X  X  X  ... X  system obtained the correct result. For another manually annotated sentence:  X   X  /  X  /  X  X  X  /  X  X  X  /  X  X  X  /  X  /  X  X  X  /  X  X  X  X   X  (...without piezoelectric element or wedge transducer...), the baseline system segmented the word  X   X  X  X  X   X  into two parts,  X   X   X  and  X   X  X  X   X  , while the new system avoided this erroneous segmentation. The word  X   X  X  X  X   X  and  X   X  X  X  X   X  are both physics terms which seldom occur in the news domain. The original intention of the paper is to explore n-gram features of the target domain corpus for a CWS system to be able to recognize the new words and the CWS system on the patent domain performed as we expected. 
Through the above experimental results and analyses, we observed that n-gram features are effective for the CWS system to adapt to the patent domain from the news domain. This paper proposes an approach in which n-gram features from large raw corpus are explored to realize domain adaptation for Chinese Word Segmentation. The n-gram Chinese Word Segmentation. This approach can be easily implemented because the with an annotated corpus and a domain-specific dictionary, a raw corpus of the target domain is easily obtained with low cost. model for domain adaptation of Chinese Word Segmentation. Na m ed Entity Recognition (NER) is one of the key techniques in the fields of Infor m ation Extraction, Question Answering, Parsing, Metadata T agging in Se-m antic Web, etc. NER task is to identify the three categories (entity, ti m eand digital), seven subclasses (person, organization, place, ti m e, date, currency and percentage) of na m ed entities in the text [1-2]. Because of word flexibility of per-son, organization and place, their recognition is very difficult, while, co m position of ti m e, date, currency and percentage tends to have m ore obvious rules. T here-fore, NER generally refers to the recognition of person, organization and place. As early as in 1995, MUC-6 established the special evaluation of NER, greatly pro m oted the develop m ent of English NER technology. MUC-6 and MUC-7 also established a m ulti-language entity recognition evaluation task ME T (Multilin-gual Entity T ask), including Japanese, Spanish, Chinese and other languages. BAKEOFF-3 and BAKEOFF-4, Held in 2006 and 2008, established a special evaluation task for Chinese NER. In 2003 and 2004,  X  X hinese infor m ation pro-cessing and intelligent hu m an-m achine interface technology evaluation X  task, held by  X 863 Progra m  X , established Chinese Na m ed Entity Recognition evalua-tion task. T hese evaluation tasks had played a very i m portant role in pro m oting the develop m ent of Chinese Na m ed Entity Recognition. Co m pared to English NER, Chinese NER is m ore difficult [2]. T he m ain differences between Chinese NER and English NER lie in: (1) Unlike English, Chinese lacks the capitalization infor m ationwhichcanplayveryi m portant roles in identifying na m ed entities. (2) T here is no space between words in Chinese, so we have to seg m ent the text before NER. Consequently, the errors in word seg m entation will affect the re-sult of NER. In this paper, we propose a Chinese NER m odel based on Markov Logic Networks with e m phasizes on (1) Co m bining local feature, short distance dependency feature and long distance dependency feature into a unified statisti-cal relational learning m odel; (2) Integrating probabilistic learning and relational learning for Chinese NER by MLNs. In order to deduce the co m plexity of the m odel and the searching space, we divide the recognition process into two steps: (1) word seg m entation and POS tagging; (2) na m ed entity recognition based on the first step. Proposed m ethod is tested on the open do m ain and restricted do m ain corpus. T he Precision, Recall and F1 in open do m ain and restricted do-m ain are respectively (78.39%, 85.89%, 81.97%) and (85.39%, 88.89%, 87.10%). Experi m ental result shows that proposed m odel is better than Condition Ran-do m Fields when only use local and short distance dependency features, and long distance dependency features can effectively i m prove the recognition effect. Early approaches to Na m ed Entity Recognition involve a lot of hu m an effort, require researchers to write a series of co m plex regular expressions to m atch candidate entity, and need to develop a large dictionary of co mm on entities. Moreover, these approaches could only be engineered to suit a specific do m ain [1]. T hese li m itations m otivate the develop m ent of m achine learning syste m sin natural language processing. Bikel, et al first proposed a na m ed entity recogni-tion m ethod based on Hidden Markov Mod els (HMM) [3]. While, because HMM is a generative m odel, theres only one feature variable with each state. However, we m ay want to incorporate m ore output features in our m odel to i m prove the accuracy of the tagger. For exa m ple, for a given token, we do not only want to consider the identity of the word. We would also want to take m ore output features into account such as the previous token, the next token, whether the token includes any digits or sy m bols etc. T o m aintain tractability of co m puta-tion, HMM have to assu m e that observation features are independent of each other. In real life, m ost observations do have co m plex dependencies, and assu m -ing independence between the features can severely i m pair the perfor m ance of the m odel. T o handle the li m itation of HMM, Liao, et al proposed m any m eth-ods for NER based on linear-chain Conditional Rando m Fields (CRF) [4]. Even though linear chain CRF has m any advantages over so m eofthe m ore traditional m odels, it also has weaknesses. In a linear chain CRF, we assu m e that the only dependencies are between the labels of adjacent words. T hus, linear chain CRF is not able to use infor m ation fro m longer distance dependencies to assist label. While in real life, there are m any long distance dependencies in a docu m ent. For exa m ple, if a word is tagged as a category, when the subsequent sections in the docu m ent again or repeatedly involved this word, it always appears in the sa m eorsi m ilar for m and its labeling is often the sa m e. J Liu, et al, integrated long distance dependencies, proposed a m ethod for bio m edical na m ed entities recognition based on skip-chain CRF [5]. Skip-chain CRF can handle relatively si m ple long distance dependencies, while, in real life, there are m any co m plex long distance dependencies, skip-chain CRF is also unable to handle the m . tional learning m ethods to increase the accuracy of English NER, while, related researches for Chinese NER is still rare. Statistical Relational Learning (SR-L) is a co m bination of probabilistic learning and relational learning [6]. T he strength of probabilistic m odels is that they can handle uncertainty in learning and reasoning. Meanwhile, first order logi c or relational datab ases can effectively represent a wide range of knowledge. SRL techniques atte m pt to co m bine the strength of the two approaches. T his co m bined strength of probabilistic learn-ing and relational learning gives SRLs m ore power in learning and inferences. Recently, there have been so m e studies in the application of SRL techniques to infor m ation extraction. Bunescu and Mooney have used Relational Markov Networks to identify protein na m es in bio m edical text [7]. Do m ingos and Poon have applied Markov Logic Networks for the seg m entation and entity resolution of bibliographic citations [8].
 distance dependencies for Chinese NER. T o the best of our knowledge, this is the first research work that, integrated long distance dependencies, applies Markov logic Networks to Chinese NER. T his paper is organized as follows. In section 3 we will review the related definitions of MLNs. In section 4 we will introduce our m ethod for Chinese NER, followed by the experi m ent in section 5. T he conclusions are given in section 6. A m ong m any statistical relational learning m ethods, Markov Logic Networks (MLNs) is a powerful, direct approach. It is a first-order knowledge base with a weight attached to each for m ula which can be viewed as te m plates for features of Markov networks. It is defined as follows [9-10]: in first-order logic and w i is a real nu m ber. T ogether with a finite set of constants as follows: cate appearing in L . T he value of the node is 1 if the ground ato m is true and 0otherwise. in L . T he value of this feature is 1 if the ground for m ula is true and 0 otherwise. T he weight of the feature is w i associated with F i in L .
 clauses. T ogether with a set of constants, it defines a Markov network with one node per ground ato m and one feature per ground clause. T he weight of a feature is the weight of the first-order clause that originated it. T he probability of a state x in such a network is given by the log-linear m odel: Where Z is nor m alization constant, w i is the weight of the i-th for m ula, and n ( x )isthenu m ber of satisfied groundings. 4.1 Feature Selection and heir First Order Logic Representation Local Features. Word itself, part of speech, word context and so m e specific dictionaries can be taken in to consideration when select local features. However, for the reason that dictionary and so m e others external r esources are the co m -m on entity resources by m anual sorting, they have very s m all contribution to verity the effectiveness of proposed approach. T herefore, in order to better verify the validity of the m ethod, we only select the inherent features of the docu m ent. Basic features we selected as follows: 1) Independent Feature: Represent infor m ation in the words of candidate en-tity. It includes word itself and its POS tag and it ai m s to inspect the internal infor m ation in candidate entity. For exa m ple, if a candidate entity includes word  X  (Corporation) X , the probability that it is labeled as organization will be increased. First-order logic for m ula that represents this feature as follows: its POS tag infor m ation.  X + X  m eans separate weight is learned for different grounding for m ulas. 2) Local Context Feature: Represent infor m ation between current word and its adjacent words. For exa m ple,  X  (Well-known scholar in the field of m achine learning,  X  X hou Zhihua X  Professor) X , suppose the word we want to tag is  X  (Zhou Zhihua) X . It is very difficult to directly label the word, however, if we consider its previous adjacent word  X  (scholar) X  and its next adjacent word  X  (professor) X , the probability that the word  X  (Zhou Zhihua) X  tagged as person will be increased. First order logic for m ula that represents this feature as follows: Neighbour ( x, y ) denotes that x is the previous adjacent word of y . T he for m ulas denote that candidate entities X  X djacent words have i m pact on its label. Short Distance Dependency Features. Local features are the infor m ation that can be extracted directly fro m the corpus, while it does not take into ac-count the dependencies between the labels of candidate entities. However, there is a wealth of relationship between the labels in a docu m ent. Such as, the label of an entity X  X  previous word always is non-entity label, etc. For exa m ple, In the sen-tence  X  200 (200 person in charge fro m Peking University, T singhua University and other insti-tutions to the scene to accept the stude nts and parents consulting.) X ,  X  (Peking University) X  and  X  ( T singhua University) X  are organization-s, labels of their previous words ( X   X ( X  X ro m  X ) and  X   X ) are all non-entity label. T herefore, dependencies of the labels of candidate entities should be used effectively. CRF is able to handle these short distance dependent features, and achieves fairly good result in the entity recognition task [12]. So we follow the idea of CRF, assu m e that the label of current word only relies on the label of its previous adjacent word and its next adjacent word. First order logic for m ula that represents this feature as follows: T he two for m ulas denote that the label of current word depend on the label of its previous adjacent word and its next adjacent word.
 Long Distance Dependency Features. If a candidate entity is tagged as a category in a docu m ent, when the entity again or repeatedly involved in the subsequent sections of the docu m ent, it usually appears in the sa m eorsi m ilar for m .Andso m e of the candidate words the m selves has a m biguity, if there is no prior knowledge, it is very difficu lt to recognize. For instance: gy, Yunnan Nor m al University and other provincial key universities graduate enroll m ent plan announced yesterday) X  University, Kun m ing University of Science and T echnology are still the two uni-versities whose enroll m ent is the largest in our province) X  (In our province, there are 17 universities and research institutions, including , that including Yunnan University, Kun m ing University of Science and T ech-nology and the Observatory, that recruit graduate students ) X  with the original order. In sentence 1),  X  (Yunnan University) X  and  X  (Kun m ing University of Science and T echnology) X  can be judged as organization. T he two entities are repeated in 2), we can obtain the correc-t category by the result of 1).  X  (YunDa) X  and  X  (KunGong) X  are the abbreviation of  X  (Yunnan University) X  and  X  (Kun m ing University of Science and T echnology ) X , and it is very difficult to label the m only based on the local features as well as short distance features. While, once taking advantage of long distance features, they can be labeled straightforward by the correct category by the result of 1) and 2). T herefore, long distance fea-tures are very useful in the identification of candidate entities. T his paper takes two types of long distance dependencies into consideration: 1) Ho m o m orphis m Repetition. Donate that if a candidate entity appears in different locations of the sa m edocu m ent, the label of these entities should be labeled as the sa m e. Regular expression is used to m atch ho m o m orphis m repetition to get the entity repeti-tion infor m ation. 2) Abbreviation Repetition. Donate that if an entity appears in a docu m ent, and in the follow-up portion of the docu m ent, the abbreviation of the entity appears, the two candidate entities should be labeled as the sa m e category. However, identification of the abbreviation of a na m e entity also is a difficult proble m in NER task. In order to ensure the syste m  X  X  recognition accu-racy, accuracy of extracting abbreviation repetition should be guaranteed, while recall rate of abbreviation repetition should be relatively relaxed. Abbreviation repetition can be identified by m atching candidate word with the key words of entity X  X  full na m e. In both cases, first order logic for m ulas can be unifor m ly represented as follows: 4.2 Weight Learning Weight Learning in MLNs is to esti m ate the weights of for m ulas utilizing the training data [8-10]. We adopt Discri m inative Weight Learning (DWL) to learn for m ulas X  X eights. T he prerequisite of DWL is that it m ust be known priori that which predicates will be evidence and which ones will be queried. For the proble m of Chinese NER, this is known. DWL divides grounding ato m s in the do m ain into two sets: a set of evidence ato m s X and a set of query ato m s Y .Inour approach, Y is all the grounding ato m sof Label ( y, + l ); others all belong to X . T he conditional likelihood (CLL) of Y given X is: Where Z x is the partition function given X , F y is the set of all MLNs clauses with at least one grounding involving a query ato m , n i ( x, y )isthenu m ber of true groundings of the i  X  th clause involving query ato m s, G y is the set of ground clauses in M L,C involving query ato m s, and g j ( x, y )=1ifthe j  X  th ground clause is true in the data and 0 otherwise. By taking partial derivation of log-likelihood function of the for m ula above, we can obtain: T he ti m eco m plexity of calculating E w [ n i ( x, y )] accurately is enor m ous. Its ap-proxi m ate value can be calculated by calculating n i ( x, y  X  w ), y  X  w represents predi-cates in its Markov Blanket. T herefore, it translates into counting n i ( x, y  X  w )in the m axi m u m posteriori hypothesis state y  X  w ( x ). T hen, we can obtain the weight of the for m ulas. 4.3 Inference Inference in Markov Logic Networks includes m axi m u m likelihood inference, cal-culating m arginal probabilities and calculating conditional probability [8-11]. T his paper only needs the m axi m u m possible explanation (MPE) which involves only the m axi m u m likelihood inference. T he following is a brief introduction to the m ethod of m axi m u m likelihood inference we used in Markov Logic Network. T he m axi m u m likelihood inference process can be stated as: given evidence set of X , seek the m ost probable state of the world Y . T hat is: According to Markov logic network X  X  joint probability distribution, the equation above can be transfor m ed into: T herefore, the MPE proble m in Markov logic reduces to finding the truth assign-m ent that m axi m izes the su m of weights of satisfied clauses. T he m ost co mm only used approxi m ate solver is MaxWalkSA T , a weighted variant of the WalkSA T local-search satisfiability solver, which can solve hard proble m s with hundreds of thousands of variables in m inutes. While, One proble m with MaxWalkSA T is that they require propositionalizing the do m ain (i.e., grounding all ato m sand clauses in all possible ways), which consu m es m e m ory exponential in the arity of the clauses. By taking advantage of the sparseness of relational do m ains, where m ost ato m sarefalseand m ost clauses are trivially satisfied, MPE inference can be conducted by LazySA T algorith m which only ground ato m sandclausesthat is needed and can save m e m ory exponentially. T herefore, we adopt LazeSA T for inference in proposed m ethod. T o objectively evaluate the effect of proposed m ethod, we organize two set of experi m ents in open and restricted do m ain. Open do m ain experi m ent is based on People X  X  Daily X  X  open corpus in January 1998. For the reason that there are m ore repetitions of attractions, places and other entities in the fields of touris m which could reflect the effect of proposed m ethod better, restricted do m ain experi m ents are based on the corpus in touris m field of Yunnan by m anual collection. 5.1 Data In open do m ain experi m ent, effectiveness of proposed m ethodistestedonPeo-ple X  X  Daily X  X  open corpus in January 1998, in which the average repetition of entities in each docu m ent is about three ti m es. we select three types of entities (person, place and organization), and then process the corpus in specific way: First, each word in the corpus is divided into a separate row, then, tag the label behind the corresponding word and its POS tag, which each entity tag is labeled in the for m of beginning, inter m ediate and end label, each non-entity is labeled by non-entity label. Exa m ple of corpus after pre-processing is as follows: Original corpus: Processed corpus Experi m ent in restricted do m ain is based on artificially collected 2000 docu-m ents in the field of Yunnan touris m . Firstly, pre-process the corpus utiliz-ing word seg m entation and POS tagging tools. T hen, m anually tag the corpus into eight categories: Attraction (jd), Nu m ber (Nu m bers in Chinese, e.g.  X  tival(jr), T i m e( T i m e in Chinese, e.g.  X  ( T wenty-first century) X )(t). Exa m ple of the corpus after pre-processing is as follows: T he first colu m nistheseg m entation result of original text, the second colu m n is corresponding POS tag, and the third colu m n is corresponding entity label tagged by m anual where  X  X  X  indicates a non-entity label. T he average repetition of entities in each docu m ent is about fifteen ti m es. Detailed statistics of data collections are shown in T able 1.
 5.2 Experimental nalysis T hree co m parative experi m ents are organized for each of the two experi m ents: the first experi m ent is based on Conditional Rando m Fields; the second experi-m ent is based on Markov Logic Networks which only use local features and short distance features; the third experi m ent is also based on Markov Logic Networks with co m prehensive utilizing local, short distance dependency and long distance dependency features. Closed and open co m parative experi m ents both are orga-nized for each of the three types of experi m ents. Evaluation of the two sets of experi m ents is based on the following three indicators: T able 2 gives detailed statistics of the co m parative experi m ental result. posed m ethodinopendo m ain and restricted do m ain are respectively (78.39%, 85.89%, 81.97%) and (85.39%, 88.89%, 87.10%). T he reason why the accuracy of the experi m ental result is not very pro m inent is that, in order to verify the validity of the proposed m ethod, we only select so m e inherent basic features in the docu m ent regardless of any excessive rules and m ore contexts (e.g. 2 or m ore gra mm odel). Experi m ental result shows that using only local and short distance dependences features, experi m ent result of MLNs is better than CRF. When long distances dependency features are integrated into MLNs, experi m ents result both in open do m ain and restricted do m ain are all i m proved and m ore obvious in restricted do m ain. Increase of precision, Recall and F1 in open do-m ain and restricted do m ain are respectively (5.11%, 4.27%, 4.66%) and (4.11%, 6.27%, 5.16%). T hisisbecauseentitieshave m ore repetitions in restricted do-m ain and then, long distances dependence features contribute m ore to i m prove the experi m ental result in restricted do m ain.
 Acknowledge m ent. This paper is supported by National Nature Science Foun-dation (No.60863011, 61175068), and the Key Project of Yunnan Nature Science Foundation (No. 2008CC023), and the National Innovation Fund for Technology based Firms (No.11C26215305905), and the Open Fund of Software Engineer-ing Key Laboratory of Yunnan Province (No.2011SE14), and the Ministry of Education of Returned Overseas Students to Start Research and Fund Projects. Over the past few years, selecting training data which are similar to the transla-tion task from the large corpus has become an important approach to improve the performance of language model (LM) in statistical machine translation (SMT) [1-5]. This would empirically provide more accurate lexical probabilities, and thus better match the translation task at hand[5].

The major challenge for data selection is how to measure the similarity be-tween the queried sentence and the LM training corpus. To solve this problem, many researchers proposed various kinds of similarity measures to select similar sentences for LM adaptation, such as TF-IDF[1-3, 6], centroid similarity[4], cross-entropy difference[5], cross-lingual information retrieval[7], and cross-lingual sim-ilarity (CLS)[8]. Unfortunately, they all take the similarity measure without considering the topic information and the distribution of words in the whole LM training corpus. These information have been successfully used for LM adapta-tion in SMT[9, 10] and been proved very useful. This approach infers the topic posterior distribution of the source text, and then applies the inferred distri-bution to the target language LM via marginal adaptation. However, it focus on modify the LM itself, which is different from data selection method for LM adaptation.

To address this problem, we propose a more principled latent topic based data selection model for LM adaptation in SMT. To the best of our knowledge, this is the first extensive and empirical study of learning the latent topic informa-tion for data selection to adapt LM. We employ the topic model (e.g., Latent Dirichlet Allocation) to discover the latent topics in the whole content of LM training corpus. Then we calculate the topic-similarity between the first pass translation hypotheses 1 and the sentences in the LM training corpus based on the latent topic information. Moreover, we propose a cross-lingual projecting method, which projects the source input sentences in the translation task to the target language representation, and th en we combine it with the topic model. Therefore, when given the source input sentence, we can select the topic-similar sentences directly without the first pass translation hypotheses. TF-IDF and latent topic information are based on different knowledge, we assume they are complementary to each other, and the p erformance can be further improved by combining them, as we will show in the experiments.

The remainder of this paper is organized as follows. The next section intro-duces some related work of LM adaptation. Section 2 describes our proposed latent topic based data se lection model for LM adapt ation. Section 3 presents large-scale experiments and analyses, and followed by conclusions and future work in section 4. A variety of latent topic models have b een used for LM adaptation in speech recognition (SR)[11-19], which show the latent topic information are very useful for LM adaptation. The previous works have primarily focused on customizing a fixed n-gram LM for each lecture by combining n-gram statistics from general conversational speech, other lectures, tex tbooks, and other resources related to the target lecture[11-14]. Moreover, they focus on in-domain adaptation using large amounts of matched training data[19]. However, most, if not all, of the data available to train an LM in SMT are cr oss-topic and cross-style. Therefore, these previous latent topic based LM adapting methods in SR are not suitable for SMT, and we will illustrate a novel latent topic based data selection model for LM adaptation in this paper.

To the best of our knowledge, none of the existing studies have addressed data selection for LM adaptation in SMT by learning the latent topics. In the next section, we explore a new approach to discover the latent topic information into the similar data selection for LM adaptation. For the first pass translation hypotheses or the source input sentences in the translation task, we estimate the bias LM, from the corresponding similar LM training sentences. Since this size of sel ected sentences is small, the corresponding bias LM is specific and more effective, giving high probabilities to those phrases that occur in the selected sentences.

The generic LM P g ( w i | h ) and the bias LM P b ( w i | h ) is combined using linear interpolation as adapted LM P a ( w i | h ) [2,7], which is shown to improve perfor-mance over the individual models: where the interpolation factor  X  can be simply estimated using the Powell Search algorithm[20] via cross-validation, and the bias LM is of the same order and smoothing algorithm as the generic LM.

The resulting adapted LM is then used in place of the generic LM in the translation process, would empirically provides more accurate lexical probabili-ties, and thus better matches the translation task at hand. Our work focuses on latent topic based data selection model, and the quality of this model is crucial to the performance of adapted LM. 3.1 Latent Topic Based Data Selection Model Before introducing our proposed method, we first briefly describe the LDA model[21]. LDA models the generation of document content as two indepen-dent stochastic processes by introducing latent topic space. For an arbitrary word w in document d , (1) a topic z is first sampled from the multinomial dis-tribution  X  d , which is generated from the Dirichlet prior parameterized by  X  ;(2) andthentheword w is generated from multinomial distribution  X  z ,whichis generated from the Dirichlet prior parameterized by  X  . The two Dirichlet priors for documents-topic distribution  X  d and topic-word distribution  X  z reduce the probability of overfitting training documents and enhance the ability of inferring topic distribution for new documents.

In latent topic based data selection model (LT), the first pass translation hypotheses and the sentences in the LM training corpus can be considered as documents. In this paper, we employ state-of-the-art topic model -LDA to discover the latent topics information and the distribution of words in them. We consider the first pass translation hypotheses as a question sentence s ,and assume that s and the LM training sentence S are represented by a distribution over topics. | s | represents the length of s , and we obtain the topic distribution of s by merging the topic distributions of words: Then, we assume that s and S have the same prior probability, K represents the number of topics, N represents the numbers of s , so the score function can be written as: 3.2 Parameter Estimation After introducing our proposed LT method, we will describe how to estimate the parameter used in the model. In LT, we introduce the new parameters, which lead to the inference not be done exactly . Expectation-Maximum (EM) algorithm is a possible choice for estimating the par ameters of models wit h latent variables. However, EM suffers from the possibility of running into local maxima and the high computational burden. Therefore, we employ an alternative approach -Gibbs sampling[22], which is gaining po pularity in recent work on latent topic analysis.

After training the model, we can get the following parameter estimations as: where n sz and n zw are the number of times of sentence s and word w which are assigned to the topic z ,and V represents the number of unique words.

Next, we concentrate on how to select proper topic number to obtain our model with best performance and enough iteration to prevent the overfitting problem. We calculate the perplexity on LM training corpus C to estimate the performance of our model, which is a sequence of tuples ( s, w )  X  C : where, the probability P ( w | s ) is calculated as follow: 3.3 Combining Latent Topic with TF-IDF for Data Selection Since the LT model and TF-IDF use different strategies for data selection, we assume that this two models are compleme ntary to each other, it is interesting to explore how to combine their strength. In this section, we propose an approach to linearly combine the LT model with the TF-IDF model for data selection. In this paper, we choose TF-IDF as the foundation of our solution since TF-IDF has gained significant performance for LM adaptation in SMT[1-3, 6]. Formally, we have where, the relative importance of LT and TF-IDF is adjusted through the inter-polation parameter  X  . 3.4 Latent Topic Based Cross-Lingual Data Selection Model Inspired by the work of CLS[8], we assume the following processing. The source sentence u and the target sentence v lie in two different vector space, we need to find a projection of u in the target vocabulary vector space before similarity can be evaluated. We estimate the bilingual word co-occurrence matrix  X  from an unsupervised, automatic word alignment induced over the SMT parallel training corpus. We use the GIZA++ toolkit to estimate the parameters of IBM Model 4, and combine the forward and backward viterbi alignments. Then, the projection of the source sentence u in the target vector space can be calculated by the vector-matrix product, as show: The target term in  X  v will be emphasized that most frequently co-occur with the source term in u . X  v can be interpreted as a  X  X a g of words X  translation of u .Next, we extend  X  v into latent topic based cross-lingual data selection model (CLLT) for LM adaptation. We consider  X  v as the first pass translation hypotheses  X  s ,so CLLT can be written as follows: We use CLS to calculate the source sentence u to each target sentence S .How-ever, due to the lack of optimization measu res for sparse vector representation, the similarity is not accurate. In our model, we add the optimization measures (TF-IDF), called CLS s , which improves the performance, as we will show in the experiment. What is more, we apply this criterion for the first time to the task of cross-lingual data selection for LM adaptation in SMT. This model can be written as follow: Lastly, we combine CLLT and CLSs into a cross-lingual data selection framework by the linear interpolation parameter, as follows: where, the relative importance of CLLT and CLS s is adjusted through the in-terpolation parameter  X  . We measure the utility of the proposed LM adaptation approach and the tra-ditional approaches in two ways: (a) comparing the reference translations based perplexity of adapted LMs with the generic LM, and (b) comparing SMT per-formance of adapted LMs with the generic LM. 4.1 Corpus We conduct experiments on two Chinese-to-English translation tasks: IWSLT-07 (dialogue domain) and NIST-06 (news domain).

IWSLT-07. The bilingual corpus comes from BTEC and CJK corpus, which contains 3.82K sentence pairs. The LM training corpus is from the English side of the parallel data (BTEC, CJK and CWMT2008), which consists of 1.34M sentences. IWSLT-07 test set consists of 489 sentences with 4 English reference translations each, and development s et is the IWSLT-05 test set with 506 sen-tences.

NIST-06. The bilingual corpus comes from LDC 2 , which consists of 3.4M sentence pairs. The LM training corpus is from the English side of the English Gigaword corpus 3 , which consists of 11.3M sentences. NIST-06 MT Evaluation test set consists of 1664 sentences with 4 English reference translations each, and development set is NIST-05 MT Evaluation test set with 1084 sentences. 4.2 Iteration and Topic Number Selection Fig. 1(a) shows the influence of iteration number of Gibbs sampling on the topic model generalization ability. Empirically, we set the topic number as 96 on IWSLT-07 and 168 on NIST-06, respectively, and change the iteration number in the experiments. Note that the lower per plexity value indicates better gener-alization ability on the holdout LM training corpus. We see that the perplexity values decreases when the iteration times are below 1000 on IWSLT-06 and 1400 on NIST-06, respectively. Fig. 1(b) shows t he perplexity values for different set-tings of the topic number. We see that the perplexity decreases when the number of topics starts to increase . However, after a certain po int, the perplexity val-ues start to increase. Based on the above experiments, we train our latent topic model using (a) 96 topics and 1000 iterations on IWSLT-07 and (b) 168 topics and 1400 iterations on NIST-06, respectively. 4.3 Perplexity Analysis We randomly divide the development set into five subsets and conduct 5-fold cross-validation experiments. In each trial, we tune the parameter  X  in Equation (1) with four of five subsets and then apply it to one remaining subset. The experiments reported below are those averaged over the five trials.
For both IWSLT-07 and NIST-06, we estimate the generic 4-gram LM with the entire LM training corpus as a baseline. Then, we apply the proposed method and other traditional methods to select the top-N similar sentences which are similar to the test set, train the bias 4-gram LMs (with the same n-gram cutoffs tuned as above) with these selected sentences, and interpolate with the generic 4-gram LM as the adapted LMs. All the LMs are estimated using the SRILM toolkit[23]. Perplexity is a metric of LM performance, the lower values indi-cates the better performance. So we estima te the perplexity of English reference translation accord ing to adapted LMs.

Fig. 2 shows the LM perplexity experiment results. We can see that the En-glish reference translation based perp lexity of adapted LMs decreases consis-tently when the size of selected top-N sen tences increases, and also increases consistently after a certain size in all a pproaches. So proper size of similar sen-tences with the translation task make the LM perform well, but if too much noisy data take into the selected senten ces, the performance become worse. Sim-ilar observations have been done by the previous work[1, 5]. The experiment results indicate that adapted LMs are significantly better predictors of the cor-responding translation task at hand than the generic baseline LM. 4.4 Translation Experiments To show the detailed performance of sel ected training data for LM adaptation in SMT, we carry out the later translation experiments with the lowest perplex-ity situation according to the above per plexity experiment, top 8K sentences on IWSLT-07 and top 16K sentences on NIST-06. We conduct translation experi-ments by HPB SMT[24] system, as to demonstrate the utility of LM adaptation in improving SMT performance by the BLEU[25] score, and use minimum er-ror rate training[26] to tune the feature weights for maximum BLEU on the development set.
Fig. 3 illustrates the impact results of parameters  X  and  X  to SMT performance on two development sets. TF-IDF, CLSs, LT and CLLT are used for reference. We see that the combined model LT TF-IDF and CLLT CLS s perform better than each other alone when they are bet ween 0 and 0.6, the best performance gains when they are 0.3 on IWSLT-07 and 0.4 on NIST-06, and we use these parameters on two test sets.

Table 1 shows the main SMT performance of LM adaptation. The improve-ments are statistically significant at the 95% confidence interval, and we see some clear trends: (1) Our proposed CLS s performs better than CLS (row 4 vs. row 3), be-cause of the added smoothing measure which makes similarity computation more accurate. (2) Our proposed LT and CLLT do not outperform the baseline method TF-IDF (row 5 and row 6 vs. row 2). This demonstrates that the knowledge extracted from LT is not as effective as that extra cted from TF-IDF. However, LT models word-topic information and word-distribution information from the whole LM training corpus. The knowledge extracted from LT is much noisier than that of TF-IDF. We suspect the above reason leads to the poor performance of LT and CLLT. (3) Our proposed LT TF-IDF significantly outperforms LT and TF-IDF (row 7 vs. row 2 and row 5), and CLLT CLSs significantly outperforms CLLT and CLSs (row 8 vs. row 4 and row 6). This demonstrates that the latent word-topic and word-distribution information extracted from LT is complementary to the knowledge extracted from TF-IDF on data selection for LM adaptation. (4) Our proposed CLLT CLSs outperforms LT TF-IDF (row 8 vs. row 7), and CLSs outperforms TF-IDF (row 4 vs. row 2). This demonstrates that the first pass translation hypotheses have lots of noisy data[27, 28], which mislead the selected similar senten ces[9, 16, 27, 28], and take noisy data into the selected sentences. However, cross-lingual data se lection model can avoid this problem, and makes the sentence selection depend on the source input translation task directly. In this paper, we propose a novel latent topic based data selection model for LM adaptation in SMT. Furthermore, we expand it into cross-lingual data se-lection by a cross-lingual projection. Compared to the traditional approaches, our approach is more effective because it takes the distribution of words and the latent topic information into the similarity measure. Large-scale experiments conducted on LM perplexity and SMT performance demonstrate that our ap-proach significantly outperforms the traditional methods.

There are some extensions of this work in the future. We will utilize our approach to mine large-scale corpora by distribute infrastructure system, and investigate the use of our approach for other domains, such as speech translation systems.
 N -gram language model is one of the important components in modern statistical machine translation systems. It helps to generate the reasonable translations which are corresponding to grammar and common usage of natural language. Using higher order LM models and more training data can significantly improve the translation performances [1]. However, decoding a single sentence can trigger hundreds of thousands of queries to the LM. Therefore, the LM must be fast for the actual SMT systems. Previous proposed techniques [2,3,4] employed the LM with trade-offs among time, space and a ccuracy. In this paper, we try to deal with this case by a compact WFSA based LM.

The weighted finite-state automaton has been introduced successfully in many natural language processing applications, as many of them have been possible to break down, both conceptually and literally, into cascades of simpler probabilistic finite-state transition [5]. We consider the LM query process as a sequence of state transitions in WFSA, which draws a uniform framework [6] for LM without almost any redundant operations and speeds up the queries substantially. Our WFSA based LM is organized with a trie structure which makes the LM stored in a compact way. We also introduce a simple LM cache by hash table to further speed up the queries. The results show that our method can finally improve the query speed by about 75%. The current research efforts to speed up the query typically follow the context features of query process in n-gram LM [7,8,9]. Pauls [7] presented several lan-guage model implementations that were highly compact and fast to query. They introduced a language model that took a word-and-context encoding for the suf-fix of the original query to accelerate the scrolling queries. They also exploited the scrolling nature of queries in the n -grams encoded tries with last-rest in-stead of the reverse direction, although they found the speed improvement from switching to a first-rest encoding was modest. This has also been exploited by Li [8], who proposed equivalent language model states to explore the back-off property of n -gram language model. Heafield [9] presented a KenLM that im-plemented two data structures, the PROBING data structure and TRIE data structure, for efficient language model queries.

Mathias [10] and Kolak [11] captured the nature language model with a WFSA in speech translation. They used the concept of WFSA to represent the knowl-edge in a uniform, and broke the complex problems into a cascade of simple WFSA. Nasr [12] introduced the WFSA to construct a new kind of LM by sev-eral local models and a general model usi ng a greedy finite state partial parser. Chiang [6] investigated Bayesian inference for WFSA and demonstrated the genericity of this framework which improved performance over EM. Most of cur-rent studies reduced the WFSA by a standard operation as minimization. Our WFSA based LM is designed with trie structure, which has already stored the LM in a compact way.

The rest of this paper is organized as follo ws: Sect. 3 introduces the motivation for our approach with the basic concept of LM and WFSA; Sect. 4 describes the system implementation, including the data structure of WFSA based LM and query method. Sect. 5 reports and analy zes the experimental results; and the conclusions are given in Sect. 6. 3.1 N-gram Language Model Generally, statistical language model are used to assign probabilities to string of words or tokens. Let w L 1 denote a string of L tokens over a fixed vocabulary. The n-gram language model assigns a probability to w L 1 according to where the approximation r eflects a Markov assumption that only the most recent n  X  1 tokens will be considered wh en predicting the next word.
The smoothing techniques [13,14] are usually introduced with back-off to avoid the sparse data problem in modeling the LM. The context-dependent back-off is used as follows where  X  (  X  ) are pre-computed and stored probabilities, and  X  (  X  )areback-off weights of the history. The LM file contains the parameter  X  (  X  ) for each listed n -gram, and the parameters  X  (  X  )and  X  (  X  ) for each listed m -gram, 1  X  m&lt;n ; for unlisted m -grams,  X  (  X  ) = 1.0 by definition. 3.2 Query Problems in N -gram LM The state-of-art language model toolkit SRILM [15] uses trie to organize the n -grams. Querying in trie can be usually composed by two types: forward query and back-off query. However, both of the two query types will be a waste of time as: 1) if the forward query reaches the leaf node of trie , it has to do the forward query from the beginning of trie when the next word comes in. 2) if the n -gram is not involved in the LM, it has to do the back-off query still from the beginning of trie .Thusmanynodesin trie are traversed with out any use.

A sample of traditional LM query is shown on the left side in Fig.1. The arrows in trie represent the query tracks. Although we only need the probability of want to eat apples when calculating the 4-gram LM of  X  X  want to eat apples X , the nodes such as  X  X ant X ,  X  X ant to X  and  X  X ant to eat X  are traversed completely useless. Moreover, because of the fragment  X  X  want to drink does not involved in LM, it has to query the sub fragment  X  X ant to drink X  according to (2). Thus the nodes  X  X ant X  and  X  X ant to X  still traversed uselessly.

We do not consider the query in LM as a random procedure but a continuous process. Still take the case in Fig.1 as an example. If we use a roll-back pointer to guide the query directly to an equivalent state [8], the next query can simply proceed by only one search. This gives us the instinct to use a WFSA to solve this problem. Figure 1 at the right side shows the query method in a WFSA based LM. We can see the WFSA will also speed up the back-off queries by the introduction of roll-back pointer. Thus both of the two query problems in LM can be successfully resolved with the concept of WFSA. 4.1 WFSA A WFSA is conceived as an abstract automaton with a finite number of states. The state it is in at any given time is called the current state. It can change from one state to another when initiated by a triggering event or condition which is called the transition, and carry out weights for each transition.

Normally, a typical WFSA can be assigned by a 5-turple M =( Q,  X , I, F,  X  ), where Q is a set of states, I  X  Q is a set of initial states, F  X  Q is a set of final states,  X  is the alphabet which represents the input and output labels, and  X   X  Q  X  (  X   X   X  ) is the transition relation. A transition is labeled with  X  if it can be traversed without input symbol. The label w i in a input string L is accepted by the automation M is defined to be The state is traversed according to the input labels until it reaches one of the final states. For each transition step, there will be an output which represents the weighted element. 4.2 WFSA Based LM Structure We use trie as the basic structure of our compact WFSA language model. The nodes in trie are the set of finite state Q ,andtherootof trie is the initial state I. Each node of trie except the root is the set of final state F . The input label  X  is the alphabet of input sentences, and the weights are the probabilities for n -gram and back-off. The transition relation  X  is composed by forward transition T f and roll-back transition T b . The forward transition traverses along the path of trie which is corresponding to forward query and the roll-back transition traverses with the roll-back pointer which points to the equivalent position in trie .It should be noted that the roll-back transition triggers spontaneously without any input when it reaches to the leaves of trie or carries out back-off queries, which represents the  X  transition in WFSA.
Figure 2 shows an example of 4-gram LM based on the compact WFSA. Nodes in the trie are based on the sorted arrays [16] with probability, back-off, and an index (the solid line in Fig.2) into higher order of n -gram LM. Different with the previous work, the nodes of our WFSA based LM store a roll-back pointer (the dash line in Fig.2) for the m -order (3  X  m  X  n ). It is just because of these roll-back pointers that make our LM act as WFSA, which will be illustrated later in the next section. The 2-order omits roll-back pointer to 1-order as it can be easily queried by the vocabulary identifier. The nodes of w 5 at the 3-order and w 6 at the 4-order are pointing to the corresponding equivalents which is not shown in Fig.2. Notice the roll-back pointer in w 6 at the 4-order can safely point to the one at the 2-order, for the back-off probability is restricted to 1.0 if the direct back-off is not involved as described in Sect 2.1. 4.3 Query with WFSA Based LM For a given input of word sequence w L 1 , the query process of LM can be seen as a series of state transitions based on WFSA. Take the 4-gram language model in Fig.2 as an example. The transition of query state is triggered by each input word w i (1  X  i  X  6) in w 6 1 , and each state is corresponding to the node in trie . The state transition process is shown in Fig.3. The state s j i is represented by thenodeinLM,and i and j are the beginning and ending identifier of query fragments respectively.
 It can be found that each transition triggered with the input of word (or null ). The forward transition T f is triggered for each of the input words. If the state is not involved in LM, the query process will continue traversing to the current state and trigger a roll-back transition T b , until it successfully makes a forward transition. The roll-back transition T b is just corresponding to the  X  transition in WFSA.

The example of LM query process in Fig.3 is processed as follows: After ini-tializing the query process, the current query state firstly traverses to the initial state s 0 0 . Then the state traverses forward to s 1 1 , for the fragment w 1 exists in the language model. The same situation happens when w 2 and w 3 input, and the current state has reached s 3 1 . Then the state transfers to s 4 1 and rolls back to s 4 2 spontaneously according to the roll-back pointer as it has reached the leaf node of trie .Whenword w 5 inputs, the state continues rolling back to s 4 3 ,as the state s 5 2 does not exist, until it traverses forward to the state s 5 3 . At last, the current state gets to the final state s 6 3 and quits the query process after the last word w 6 inputs.
 The pseudo code of query WFSA based LM is shown as follows: 4.4 Hash Cache To make further speed up of queries, we add a simple hash cache to our WFSA based LM to cache the repetitive queries when decoding a sentence in SMT system. Our cache uses an array of key-value pairs with the size fixed to 2 b for some integer b (we used 24). We choose the current state and input word as the key of our hash table. We use a b -bit hash function to compute the address in an array where we will place an output state and the carried out probability for each state transition.

For each query of the cache, we check the address of a key given by the b -bits hash. If the key located in the cache array matches the query key, then we get the output state and probability which are stored in cache. Otherwise, we fetch the probability from LM with WFSA and place the new key and value in the cache. The cache will be cleared for each of the translated sentences.

Both of the forward and back-off query speed can be improved by the cache as there are many repetitive LM querie s in SMT. Moreover, when calculating the probability for an n -gram which is not involved in the LM, the back-off must perform multiple queries to fetch the necessary back-off information, although the WFSA based LM query has already imp roved the query speed substantially. Our cache retains the fully results of these calculations and thus saves additional computations in back-off queries. 5.1 Setup We used the state-of-art hierarchical phrase-based translation system [17] as our baseline, and test the LM query efficiency on two Chinese-to-English translation tasks: IWSLT-07 (dialogue domain) and NIST-06 (news domain). The test sets of the two domains are IWSLT-07 test sets and NIST-06 test sets, which contain 489 sentences and 1664 sentences separately. We obtained the translation models (TM) following the same constraints as in Chiang [18]. We trained the 4-gram and 5-gram language models using SRILM and then converted them to our WFSA structure before decoding. The training corpora of the experiments are listed in Table 1.
 5.2 Results Storage Space Experiments. We tested our implementation of WFSA based LM ( WFSA ) on the two tasks. Notice that there are no additional nodes in trie structure. For each node, there are only 5 additional bytes comparing with the SRILM . The bytes are composed by a 4 bytes integer and a 1 bytes character, which represent the roll-back pointer. Thus, the size of WFSA based LM is corresponding to the node number of trie , which makes it compact. The results are shown in Table 2.

In Table 2, the storage sizes of WFSA based LM increase about 35% than the SRILM in the two domains. It is because o f the extra bytes that keep the roll-back pointer in the trie node. However, the increment is acceptable as it is linearly dependent with the nodes of trie .
 Query Speed Experiments. We used the 4-gram and 5-gram LM in IWSLT-07 and NIST-06 to test the query efficiency. We measured the time 5 required to perform each query in actual translation using the SRILM as our baseline. Then we measured the time of using WFSA based LM ( WFSA ) and introduced our cache to WFSA ( WFSA+cache ) to speed up the query. Times were averaged over 3 runs on the same machine. The results are shown in Table 3.

As expected, the use of WFSA speeds up the LM query substantially. The query speed in both domains has been improved by 57.1% and 59.5% in 4-gram LM, and 67.4% and 68.4% in 5-gram LM separately. The improvement of query speed in 5-gram LM is much better than 4-gram by about 10%. It suggests that our implementation of WFSA is suitable for higher n -grams. Although the WFSA has already improved the speed, it can be found our WFSA+cache queries more effectively. The query speed can be further improved by the intro-duction of cache in all of the translation tasks. The final implementation of our WFSA+cache can speed up the query by about 75%. Analysis. We measured the probability of repetitive queries and back-off queries in 4-gram LM that occurred during decoding, which had a close relationship with our WFSA based LM. The results on the two tasks are shown in Table 4.

It can be seen that back-off queries are w idely existed in statistical machine translation and as many as about 60% in 4-gram queries are proceeding for back-off query. Notice that the probabilities of back-off queries are similar in both tasks, which are corresponding to the increment of query efficiency. It suggests that our model can effectively a ccelerate the back-off queries.

Although decoding a single sentence can trigger a huge number of LM queries, it can be found most of these queries are rep etitive. Therefore, keeping the results of LM queries in a cache can be effective at r educing overall queries. This has been confirmed by our experimental results in the query speed experiments above. We have presented a new method for faster LM implementation based on com-pact WFSA. We consider the LM query process as a series of state transitions with WFSA according to the context cha racter of LM. This method improves the query speed effectively with a compact LM in size for SMT system. We have also described a simple caching techniqu e which leads to performance improve-ments in over all decoding time. Our WFSA based language model can not only be used in the faster query of LM in SMT, but also be suitable in other nature language processing, such as speech recognition and information retrieval, etc. On one hand, many research works[1,2,4,7,10,11] reported discontinuous phrase translation significantly improves the transl ation quality in statistical machine transla-tion direction (e.g. Chinese-to-English) with only one language pair. Thus, it remains an interesting question whether the discontinuous rules always have a big contribution experiments. rules (e.g. source-side discontinuity or target-side discontinuity, Figure 1 for example) contribute to the translation quality quite differently and they reported the finding that source-side discontinuity plays a much more important role than target-side disconti-nuity in Chinese-to-English translation. This finding is reported at least by two related translation system that aims at better translating non-continuous phrases. [4] extended a traditional phrase-based system for discontinuous phrase translation. They made also reported the same finding using a hierarchical phrase-based system Joshua[6]. It experiments. discontinuity is allowed. Therefore, if we know some kinds of discontinuous rules are useless, we can remove large amount of such redundant rules without performance loss. Correspondingly, we propose a role-based rule filtering strategy. Our work described in this paper includes three parts: the first one and also the most discontinuity; the second one is investigating the contribution of discontinuity transla-tion in different language pairs; and the third one is designing a role-based rule filter-ing strategy. 
Relationship between source discontinuity and target discontinuity [11] tried to translation. [4] also discovered this phenomenon in their extended phrase-based mod-el for non-continuous phrase translation. They meanwhile pointed out the phenome-explained. only one translation direction for one language pair. It is worth to study whether it is much useful in other language pairs. filtering method which considers the possible 66 rule patterns. In order to deeply investigate the discontinuous rules, we first classify them into dif-ferent kinds. Generally, the discontinuous rules are classified into source-side discon-tinuity and target-side discontinuity. We can see that these two classes are overlapped. hierarchical rules[2]. side discontinuities can be obtained by combining two contiguous rules. For example, the rule classification, we define the strict both-side discontinuity with word alignments: Definition: Given a rule , X  X   X   X  which is both-side discontinuity with word u, v , ' u , ' v are terminal strings, we consider them as terminal element. both-side discontinuity. For example, for the rule we consider this rule as a strict both-side discontinuous rule. 4.1 Experimental Set-Up based system Joshua[6]. We conduct our experiments on four language pairs: Chinese-English, Spanish-English, French-English, and German-English. For Chi-NIST2006 test data and the test sets include NIST2005 test data (test-1) and NIST2008 test data (test-2). For the direction English-to-Chinese, we split NIST2008 test data into two parts: the first 800 sentences are used as tuning set and the rest 1059 language model is built on the target part in both directions. 1.48M sentence pairs for French-English and German-English, and 1.47M for Span-ish-English. We train the 5-gram language models on the large monolingual data: 227M words for German, 218M words for French, 94M for Spanish and 549M for English. The tuning set is Devset2009-a consisting of 1025 sentences, the first test set data of WMT09 including 2525 sentences. 
The word alignment is obtained with GIZA++ and case-insensitive BLEU-4 is em-ployed to evaluate the performance of translation results. 
To have a detailed comparison, we design six groups of experiments according to plus source-only discontinuous rules (+SDR); 3) CR plus target-only discontinuous rules (+TDR); 4) CR plus both-side discontinuous rules (+BDR); 5) CR plus both-strict both-side discontinuity. 
Tables 2-5 report all the translation results in four language pairs. The bold figures in tables just show which one, of source discontinuity and target discontinuity, leads to bigger gains. 4.2 Contribution of Discon tinuous Rules in Different Language Pairs From Table 2, we can easily see that translation with all discontinuous rules (in ALL Chinese-to-English translation. However, when coming to European language pairs, BLEU point on German-to-English test-1 set. Furthermore, translation with disconti-nuous rules sometimes even decreases the translation quality (such as that on Spanish-to-English translation). We speculate that the systematic divergency between Chinese and English is much larger than that between these European language pairs; thus the complicated translation rules such as discontinuous rules are very helpful in Chinese-pairs. In sum, the experimental results show that discontinuous phrase translation does not always contributes much to translation quality in any language pair. 4.3 Source Discontinuity vs. Target Discontinuity If we focus on the bold figures in Tables 2-5, we can easily see that source discontinu-Chinese-to-English translation while the target discontinuity wins in the inverse Eng-lish-to-Chinese translation. In other three language pairs (Tables 3-5), it also seems if source discontinuity is more helpful in one translation direction, the target discontinu-ity is more useful in the inverse translation direction (test-2 set in F  X  E and tuning set because the discontinuous rules do not contri bute much in these three language pairs essentially. 4.4 Contribution of Both-side Discontinuity Both-side discontinuity does not affect th e contribution difference between source and target discontinuity since it has the same effect on both of them. We can see from the tables that both-side discontinuity always improves translation quality (+BDR lines in tables). To figure out how much of the improvement does the strict both-side discon-tinuity contribute, we re-train and re-test without the strict both-side discontinuities (-SBDR lines in tables). The performance decreases for the lack of SBDR in all cases. excluding SBDR occupy a quite large part of all rules but the contribution is relatively small; therefore, lots of such rules of the kind are incline to be filtered. inspiration. For example, since we have known the target discontinuity is more effec-tive in English-to-Chinese translation, we can specially handle the target discontinuity forts in Chinese-to-English translation. translation, we can use the +TDR configuration as the best setting which obtains near-ly the best performance but only contains less than 30% of all the rules. In the rest of this paper, we will introduce a role-based rule filtering strategy. 5.1 Rule Classification by Role tremely large. Even though we only keep those which are relevant to the test set, the translation rules and then throw the rules of some classes away with specific strategy. [12] classified the rules based on their counts and removed the rules which occur less than a minimum count (such as 3). [9] divided the rules into two kinds: one must be well-formed dependency tree in the target part, the other is the rest. They only retain the rules of the first kind. [5] proposed a pattern-based rule classification. A rule pat-tern is composed by source format (terminal elements, non-terminal elements, and patterns which do not reduce translation quality. 
In this paper, we propose a role-based rule classification. The role here means the model can be divided into two functions: one is for phrase translation; and the other is for phrase reordering. For hierarchical phra se-based models and syntax-based models, the phrase translation rules can be further classified into continuous phrase rules and discontinuous rules. Additionally, the composed rules [3] are sometimes useful. Based rules (RR); (3) Source discontinuous rules (SDR); (4) Target discontinuous rules (TDR); (5) Strict both-side discontinuous rules (SBDR); (6) One non-terminal com-posed rules (CR1NT); (7) Two non-terminal composed rules (CR2NT) 
The lexical phrase rules are just the rules without non-terminals. The rules, such as the meaning of the three kinds of discontinuous rules in previous sections. One non-reordering and discontinuity translation, i.e. the rule ,' XuXuX  X  which can be concatenated by lexical phrase rule ,' Xuu  X  and the general glue rule. Two non-terminal composed rules are similar to CR1NT. It is worth noting that RR and SBDR role-based rule classification to form a partition of the rules, we consider the rules of this kind as SBDR. essential and which ones can be removed safely. The premise of our filtering strategy is that some kinds of rules are useless to translation quality in certain language pairs. 5.2 Filtering Strategy Because we have known the contribution of different kinds of discontinuous rules, we can design different rule filtering strategies according to the different language pairs. as our basis for filtering since it has nearly the same performance with that using all rules and meanwhile excludes more than 70% rules. However, in Chinese-to-English translation, the ALL configuration cannot be substituted. Thus, in this case, we adopt subsection, the Spanish-to-English and Chinese-to-English translations (test-1 data set used as the test set) are used as inst ances to introduce the filtering strategies. 
Spanish-to-English: Since the CR configuration in Table 2-5 includes LPR, and CR1NT, CR2NT, and RR, and then study whether they are necessary 4 . To have a gives the patterns which occupy the most part but are proved to be useless in Arabic-to-English translation[5]. 
The experimental results are shown in Table 7 and Table 8. An interesting thing we can see from Table 7 that the role-based rule filter can further discard a large number of rules (Line 3 in Table 7) and keep approximately the same performance with that using (LPR) and target discontinuous rules (TDR) leads to degradation in translation quality. that done by our strategy. Chinese-to-English: As we have no idea which kind rules could be excluded in Chinese-to-English translation (see Table 2), we just apply the LOO strategy to filter for comparison. The statistics are reported in Table 9-10. Different from Spanish-to-English translation, the rules of most roles are indispensable. Even for the composed rules with one non-terminal (CR1NT), [5] argued these rules are redundant for translation. Fortunately, we can still discard a lot of useless rules (CR2NT) and reduce the rule set by 20%. reason why the rules of pattern (d) cannot be taken away is that about half of the dis-carded rules are strict both-side discontinuous rules that we have proved to be useful. filtered by other strategies (such as count-based). To sum up, our role-based strategy course, our strategy is effective since a large number of rules can be discarded if the translation quality is not degraded without these rules. 
Another important thing we should notice is that, compared to 66 possible patterns roles and it is much simpler for experimentation. Furthermore, this classification me-thod is also valid for other translation models, such as string-to-tree model [3] in which and composed rules (similar to our CR1NT and CR2NT). Therefore, our rule filtering strategy is more robust. complicated rules needed for translation between them. In this paper, we present an empirical study on the contribution of different kinds of of discontinuity classification is proposed. Then, we analyze the symmetry of linguis-source discontinuity is more effective in one translation direction, the target disconti-that discontinuity translation in some main European language pairs is not as useful as in Chinese-English language pair. 
Having known the contribution of discontinuity translation, we finally propose a role-based rule filtering approach, which shows efficiency and more robust compared for translation between them. 
Although the experiments are conducted using a hierarchical phrase-based transla-tion model, we believe our findings are also valid in other translation models. Acknowledgements. We would like to thank the anonymous reviewers for their great comments. The research work has been par tially funded by the Natural Science Foun-dation of China under Grant No. 60975053 and 61003160 and also supported by Hi-Tech Research and Development Program ( X 863 X  Program) of China under Grant No. 2011AA01A207. In statistical machine translation (SMT), unknown words are the source language translations. The current SMT systems either discard the unknown words or copy cially severe when the available bilingual data becomes very scarce. 
A question arises that what kinds of negative impacts would the unknown words target language. For instance, using our small-scale training data for Chinese-to-(to)  X  X  X  (court)  X  X  ... X  , thus we have no idea about the meaning of this word in the example, if the Chinese verb  X  X  is kept untranslated in the output, we are likely to obtain the wrong phrase reordering to the court  X  X  while the correct one is  X  X  to the court . 
The conventional solution of unknown words is to find their target correspondence with additional resources in different ways[3,5,6,13,16,18,19]. They use multilingual of unknown words. By doing this, they hope to solve the above two problems perfect-ly. However, most of these work only address some part of unknown words, such as Named Entities[7,15], abbreviations[6,13], compounds[6] and morphological va-riants[2,9]. Therefore, many unknown words still remain untouched. Furthermore, for and reordering models are acquired. For example, even if a translation of the Chinese model is trained without any information about the source word  X  X  and its transla-tion appeal . 
From the above analysis, we know that finding translation of unknown words rounding words. As we can see that, it is very difficult to obtain the correct translation rounding words. words, rather than trying hard to get the target translation of unknown words, we aim tion: the lexical selection and reordering of the surrounding words depend on the se-mantic function the unknown word servers as. The semantic function of a word semantic function determines what context the word should take in source and target language. In turn, we can say that two words are similar in semantic function if they mantic function of a word W by another word W X  which shares the most similar con-text with W . Therefore, the central idea of this paper is to find a known word having three steps as follows: 
First, we use the distributional semantic model and bidirectional language model respectively to find an in-vocabulary word in original training data, which shares the most similar context with the unknown word. 
Second, we replace the unknown word with the found in-vocabulary word and in-put the test sentence to the SMT system. Then, we obtain the translation output. 
Third, we find the target language word in the output, which is translated by the in-vocabulary word, and replace it back with the unknown word. The unknown words in translations. 
For example, we have a Chinese sentence  X ...  X  (is)  X  X  X  X  X  X   X  X  X  (about) ... X  in which  X  X  X  X  X  X  is an unknown word that in fact means 6%. Using the proposed with the unknown word  X  X  X  X  X  X  . Then, we replace the unknown word with  X  X  we replace 50% back with the unknown word  X  X  X  X  X  X  resulting  X ... is about  X  X  X  words and it makes the translation more understandable. 
We can see from the method that the first step is the most important. Thus, it is our mantic model and bidirectional language model. Experiments show that, with appro-semantic function with the unknown words and can much improve translation quality as well. In SMT community, several approaches have been proposed to deal with the un-unknown words, [5,20] adopted comparable corpora and web resources to extract translations for each unknown word; [16,18] applied paraphrase model and entailment rules to replace unknown words with in-vocabulary words using large set of addition-al bitexts or manually compiled synonym thesaurus WordNet. More research works address some specific kind of unknown words, such as Named Entities (NEs), com-pounds and abbreviations. [1,7,15] utilized transliteration and web mining techniques with external monolingual and bilingual corpus, comparable data and the web to find form representations for the unknown abbreviations. [9] translated the compound plates. [6] proposed a sublexical translation method to translate Chinese abbreviations and compounds. For translating highly inflected languages, several methods[2,11] the unknown words. [12] addressed the problem of translating numeral and temporal expressions. They translation rule extraction and reordering m odel training consider the special symbol. In the decoding time, if numeral or temporal expression is found, it is substituted by the special symbol so that the surrounding words can be handled properly and finally known words. 
Totally different from all the previous methods, we do not focus on trying great ef-fort to find translations for the unknown words with huge external resources. Instead, we address the problem of the lexical selection and word reordering of the surround-unknown words and apply the distributional semantic model and the bidirectional language model to fulfill this task without any additional resources. Distributional semantics[4] approximates semantic meaning of a word with vectors summarizing the contexts where the word occurs. Distributional semantic models (DSM), such as LSA[10] and HAL[14], have proven to be successful in tasks that aim at measuring semantic similarity between words, for example, synonym detection and words in the test set are not equipped with rich contexts. Therefore, instead of obtain-ing the synonym of the unknown words, we take a step back to find the most appro-priate word which has the similar semantic function with the unknown word with DSM. the in-vocabulary word which has the most similar semantic function with the un-known word. 3.1 Model Construction As it is summarized in [4], the construction of the DSM usually includes seven steps: 5) feature scaling, 6) normalization, and 7) similarity calculation. 
In the linguistic pre-processing, we first merge the source-side of training data TD and evaluation data ED , resulting the whole monolingual data MD . Then, we segment and POS (part-of-speech) tagging the monolingual data MD . In this paper, we just use each unknown word. each row is a vector denoting the context distribution for a target term and each col-umn represents a context term. It is easy to see that both the number of rows and col-umns equals to the size of the vocabulary of MD . Suppose the size of vocabulary is N, then the term-term matrix is NN  X  . chosen if it occurs within a window of K words around the target term. We can distin-different window K s in the experiments to find the best one. two steps, we detail how to construct the vector tw V for each target word tw . occurrence frequency for each context term and use it as the ith element. In order to take the frequency of target word and context word into account, we adopt the associ-ation measures mutual information to do feature scaling. Suppose the occurrence Pointwise Mutual Information (PMI) between tw and cw is: VPMItwcw PMItwcw =  X  . 
Finally, we apply the cosine measure to calculate the similarity between two target 3.2 In-vocabulary Word Search for Unknown Words According to the evaluation data and training data, we can easily distinguish unknown words from in-vocabulary words. Assu me that the unknown words set is UWS and the in-vocabulary words set is IWS . For each unknown word UW , our goal is to find the function with UW . With the similarity function defined above, we can use the follow-ing formula to meet our goal: However, we find that using this formula without any constraint usually cannot obtain good results. Therefore, we require that the resulting in-vocabulary word * IW should have the consistent part-of-speech with the unknown word UW . Accordingly, the search formula will be: It should be noted that our final purpose is to improve the translation quality, but not all of the found in-vocabulary words using formula (4) can guarantee good translation with the context of the unknown word since they usually lack the corresponding combining the context of the unknown word matches an entry in phrase table, it will facilitate the lexical selection and word reordering of the surrounding words. For in-stance, in the example sentence  X ...  X  (is)  X  X  X  X  X  X   X  X  X  (about) ... X  , an in-vocabulary word  X  X  is found using (4) for the unknown word  X  X  X  X  X  X  , and after replacement the sentence becomes  X ...  X  (is)  X  X   X  X  X  (about) ... X  . If there exists a leads to the correct reordering and word sel ection of the context. Therefore, in order any found in-vocabulary word, which combines the context of the unknown word, should have an entry in phrase table. equally without considering the word position. As a result, this model misses a lot of important information. 
A question arises that how to use the context more effectively? In theory, the goal of our task is to find the most appropriate in-vocabulary word for the unknown word given the left and right context of the unknown word. It can be formulized as follows: Now, let us focus on difficult to estimate because the co ndition is too strict. Following the n -gram probabil-concerning only the left context right context vocabulary word with the constraint combining the two backoff probabilities: It is easy to see that the first backoff probability a forward n -gram probability where n equals to the context window K plus one. Thus, we can just use the conventional n -gram probability estimation method to estimate the backoff probability of generating each in-vo cabulary word given the left context. We name this backoff model the forward language model . 
PIW cw . In contrast to the forward language model 
PIW cw can be regarded as a backward language model . The difficulty lies in model can be easily estimated through the reversion of the training sentence[22]. Take the following sentence for example: After reversion, the sentence will be: pw w w  X  X  X  can be estimated using the original sentence (7) and the backward same way to first reverse the test string. 
Therefore, we call the formula (6) using the forward and backward n -gram lan-guage model the bidirectional language model . Like the distributional semantic mod-function (6) resulting the following formula: the context of unknown word must have a corresponding entry in phrase table. 
Compared with distributional semantic model, the bidirectional language model can well model the word order and dependence among context words. In the follow-found in-vocabulary words for the unknown words in test set and evaluate the effec-tiveness of these two models in SMT. 5.1 Set-Up Since the application environment of our model supposes the training data is relative-experiment, we used the Chinese-English FBIS bilingual corpus consisting of 236K sentence pairs with 7.1M Chinese words and 9.1M English words. We employed GIZA++ and grow-diag-final-and balance strategy to generate the final symmetric lingual data and the Xinhua portion of the English Gigaword corpus. NIST MT03 test data is adopted as the development set and NIST MT05 test data is employed as the test set. We used the open-source toolkit Urheen 2 for Chinese word segmentation and POS tagging. POS of unknown words are predicted by the MaxEnt model. The test set consists of 1082 sentences, and there ar e totally 796 distinct unknown words. Ac-273), (NN, 272), (CD, 122), (VV, 99), (NT, 14), (AD, 7), (JJ, 5), (OD, 2) and (M, 2). 5.2 Experimental Results 5.2.1 Accuracy of Semantic Functions The proposed two models aim at finding the most appropriate in-vocabulary word that has the most similar semantic function with the unknown word. However, the models cannot promise correct result for each unknown word. In this section, we investigate the accuracy of the two models respectively. model (DSM) and the bidirectional language model (BLM). Table 1 shows the results of DSM model with different context window size and different constraints. Overall, the accuracy of DSM model is not high. We believe that it is because the context of Specifically, we can see that requiring the found in-vocabulary word should have an Furthermore, among different context windows, the size of 6 performs best. In a deep analysis, we have found that the unknown words whose POS are NN and VV are the main reason for the low accuracy. much better than that of the DSM model. We think this is due to the modeling of con-text word order and dependence between them in the BLM model. We also notice that table performs best and achieves the accuracy 77.6%. 5.2.2 Translation Results model. We use the open-source toolkit Moses to conduct all the experiments. We report all the results with case-insensitive BLEU-4 using shortest length penalty (main metric) and NIST. We employ re-sampling approach to perform statistical signific-ance test [8]. baseline using default Moses. With only POS constraint, the DSM model with win-augmented with translation entry constraint, the model outperforms the baseline in all provement 0.42 BLEU score over the baseline. 
Table 4 illustrates the translation results of the BLM model with different con-translation quality compared with the baseline. Specifically, the BLM model with the POS constraint significantly outperforms th e baseline by 0.54 BLEU score. And when mance and obtains an improvement of 0.64 BLEU score. The results have shown that the BLM model is very effective to handle unknown words in SMT even though the model is relatively simple. 
To have a comprehensive comparison, we have also conducted the experiments with the forward language model and backward language model respectively. Table 5 guage model and backward language model cannot outperform the bidirectional lan-guage model. The results also show that the forward language model performs better than the backward language model. It is co nsistent with the conclusion drawn by [22] that forward language model is more effective than backward language model for Chinese. translation from a new perspective. Instead of trying hard to obtain the translation of the unknown words, this paper have proposed two new models to find the in-vocabulary words that have the most similar semantic function with the unknown words and replace the unknown words with the found in-vocabulary words before reordering for the context of the unknown words during decoding. The experimental language model can both improve the translation quality. Compared with the distribu-tional semantic model, the bidirectional language model performs much better. In the future, we plan to explore more features to handle unknown words in SMT even bet-ter. Furthermore, we are going to figure ou t what is the case if a source-side monolin-gual large corpus is used for the distributional semantic model. Acknowledgement. We would like to thank the anonymous reviewers for their valua-Foundation of China under Grant No. 60975053 and 61003160 and also supported by Hi-Tech Research and Development Program ( X 863 X  Program) of China under Grant No. 2011AA01A207. Real-time query expansion (RTQE) recommends a list of expanded query words when user types a query, which reduces user X  X  keystrokes to complete information retrieval. trieval performance. White and Marchionini [1] showed real-time and interactive query expansion is useful, it can reduce the time needed to perform a query and improve the query quality. 
The most widely used way to RTQE is offering the most frequent query from the inputted by users, the more precise the expanded words are. For example, in the RTQE method used by Microsoft Bing 1 , when the inputted words are  X  X lympic lo X , the first expanded query is  X  X lympic logo X ; when the inputted words change to  X  X lympic lon X , the first expanded query changes to  X  X lympic london 2012 X . used enhanced string matching method to support interactive fuzzy keyword expansion. 
An alternative method for RTQE uses user context information. Arias et al. [4] lacks scalability due to the use of manually built rules collection. Bar-Yossef et al. [5] current input as their prefix. Their method is effective when user input is short. 
However, there is little work on RTQE concerning user X  X  query intention. Stroh-results show that intentional query expansions can be used to diversify result sets and improve click-through ratio. 
This paper proposes a RTQE method using a dependency network to get better query intentions. The dependency network is constructed using dependency relations from the parsing results of large-scale corpus. A dependency relation is either a verb-noun pair or a verb-attributes-noun multiple gram. 
Experiments show that the proposed RTQE method improves the retrieval perfor-mance and reduces user input effort greatly. It is also suggested that dependency rela-tions can be used to determine user X  X  query intentions. dependency network and the RTQE method based on this dependency network. The evaluation and experiments are detailed on section 3. Finally, section 4 gives the con-clusion and future work. 2.1 Representation of Query Intention informational and transactional. He et al. [8] used search results to find query intention. intention, but not good for navigational intention. Meanwhile, they showed that verb-noun pairs can be acquired from dependency relations in sentences. 
But we found the verb-noun pairs are still not sufficient to express query intention illustrating example, two queries  X  X earn English language X  and  X  X earn programming language X  are both represented by the verb-noun pair  X  X earn language X . But the inten-tion difference between them is quite big. So other components needs to be added in verb-noun pair in order to describe query intention better. 
In this research, we add modifiers of noun, namely attributes, to complete the query intention. We mainly consider pre-attributes here. Query intention can be represented by verb-attributes-noun. In English, the normal word order of attributes is determiner, adjective, participle, gerund and noun. Determiner contains article, posses-sive pronoun, substantive genitive, numeral and classifier which have little function in intention expression. So we only consider attributes of adjective, participle, gerund and noun words. 
For the above example, after adding attribute to the verb-noun pair  X  X earn language X , the intention difference between them is clearly expressed. 2.2 Construction of Dependency Relation Network First we collect dependency relations from large-scale sentences parsed according to dependency structures. Then we build dependency relation network by combining and relating these dependency relations. 
Stanford Parser 2 is used here to do dependency parsing on the sentences. From the parsing result of each sentence, we extract dependency relations between verb and noun and relations between this noun word and its attributes. For example, for the sentence  X  X ow to change a car tire X , we get results as shown in Fig. 1 after dependency parsing using Stanford Parser. noun] to represent dependency relation between verb and noun, and an[attribute, noun] to represent dependency relation between noun and its attributes. In this example, we get vn[change, tire] and an[car, tire]. shown in Fig. 2. Because user can input both verb and noun first when inputting que-networks for each query intention. One starts from verb as shown in the left of Fig. 2 and the other one in the right starts from noun. We attach the arrow that represents  X  X n X  relation. Although the network is a little different from dependency network, we still call it dependency network in the following parts of the paper for simplification. ding dress X . In this sentence, we extract vn[buy, dress] and an[used wedding, dress]. linguistic analysis, the closer an attribute is to the noun, the closer relationship it has with the noun. So we first connect the attribute which is closet to noun, and then con-nect the second closet one, and so on. connected first, that is vn[buy, dress]. Then the attribute node  X  X edding X  is connected connected to attribute node  X  X edding X . The network using verb node as starting node is shown in Fig. 3. 
Moreover, for intransitive verbs, they are connected with noun word by a preposition word. So in the network between an intransitive verb node and a noun node, a preposi-vpn[up, vn], vn[look, number] and an[phone, number]. In the dependency network, the preposition node is attached to the path between verb node and noun node. The network of this example using noun node as starting node is shown in Fig. 4. can get all the dependency relations starting from the same verb in one network. Sim-dependency relation based networks. (Fig. 5) 2.3 RTQE Method and expand verb for inputted noun. Then we can expand attribute words for verb-noun pair to express query intention more complete and precise. Furthermore, by construct a RTQE system. Specifically this RTQE method works in the following process: list matched words sorted by their frequency in the corpus (Fig. 6-a). 
Second, when the user selects a word from the list, our system can locate the node in the network that stands for the selected word ( X  X ire X  in this example). Then our system expanded words by inputting prefix (Fig. 6-c). After this step, the intention backbone (verb-noun) is built. If the verb is intransitive, we can get (verb-preposition-noun). 
Third, when the user selects a verb or noun word shown in the expanded list, our attached to this relation, our system will pop-up the attribute words as expanded words. The attributes that are closest to the nouns are expanded first, then the less closest ones (Fig. 6-e). 
The procedure ends when there are no more words can be expanded or the user feels satisfied with the current query. We designed four experiments to test the performance of the RTQE method described in Section 2. 3.1 Experimental Corpus We used corpus from www.ehow.com. This website is an online guide offering step-to-step instructions for how-to questio ns. According to Broder X  X  query intention research [9] we know that informational query intention can be represented by improved method using verb-attributes-noun. The corpus we used has articles of 20 categories from ehow, including Health, Cars, Computers, Food &amp; Drink and so on. The total size of the corpus is 915,600 articles. represents this question. When building dependency relation network, we did depen-dency parsing for each title, like  X  X ow to fix motorcycle tire X . 3.2 Experimental Evaluation Measurement advanced engineering education background. 
First we test how many keystrokes our RTQE method will save. Reducing users X  efforts in generating queries is very important for RTQE method, especially for users of mobile search systems. 
For each query, the keystrokes and mouse clicks needed to generate a query is rec-orded. Each keystroke or mouse click is recorded as an operation. Suppose that for query x, the operations needed when the user does not use RTQE is OPFull x , and the operations needed when using RTQE is OPExpanded x . 
The percentage of saved operations is: Second, we test the expansion success percentage of this RTQE method. For a given expanded list, we call it a successful expansion. The expansion success percentage is the percentage of queries with a successful expansion in all the queries. 
For example, the user wants to search a query  X  X nstall windows system X , if this query can be found in expanded list during RTQE process, then the expansion is successful, otherwise it is not. 
Suppose that the number of all the queries is AllNum, the number of query with a successful expansion is SuccessNum. The expansion success percentage is: Moreover, in the queries with successful expansion, we compare the retrieval perfor-mance of the original query user typed in, the query after verb-noun expansion and the query after verb-attributes-noun expansion. The aim of this test is to know how much effect this RTQE method has for query intention completion. We use precision and nDCG score for evaluation. answers. nDCG is a measure of effectiveness in information retrieval. We grade the relevance means totally relevant. The rank of each result is done by the user who does this search. 
Suppose that rel x means the graded relevance of the result at position x in the search results. For a search result list, its DCG score is: And nDCG score is the DCG score divide the maximum possible DCG score from position 1 till position p. Finally, we compare the RTQE result of our method with that of Microsoft Bing search engine. 
According to the experiments described above, we assigned the following tasks for each test participant: 
In accordance with the limit of the 20 categories, do search for a how-to question each time. Make sure that the tester is clear about the query intention before inputting. Use the RTQE method to expand the query. 
Our system can get the amount of operations of the user for each query. The tester should record whether an expansion is successful or not. search result of the three kind of queries. Rank the results by judging the relevance of each result with the query intention. 3.3 Experimental Results and Analysis By the test of the volunteers, we get 200 queries and their test results as follows: Table 1 shows that the percentage of average saved operations is 63.75% after RTQE. It proves our RTQE method is very useful for saving user effort. 
This is because the structure of the dependency relation network we built directly expanded words our system recommends are components of possible query inten-network using dependency parsing. Moreover, combining the dependency relation network with prefix string matching method makes our RTQE method more effective in reducing the operations needed. 
We use the query in Fig. 6-e for example here. We use a letter  X  X  X  to find the word  X  X otorcycle X . Only in this step, eight operations are saved. We can see that this RTQE method really reduces the operations. 
Table 2 shows the expansion success percentage is 84%. It means most of the que-relation network represents user query intention well. 
Because we built the dependency relation network using a large-scale corpus, so nearly all the possible query intentions of selected categories are included in the net-work. Therefore, this RTQE method can get a high success percentage in this test. 
The data of Table 3 is computed by the 168 queries in Table 2 which are successfully expanded. We found that the retrieval performance after verb-attributes-noun expan-sion is better than the performance after verb-noun expansion. And the performance of the latter is better than the performance of not expanded queries. Although the retrieval performance improves naturally with the increase of query words, but obviously im-porting words that do not relate to the query intention will make the retrieval perfor-mance drop. The significant difference in the performance of the three kinds of queries sulting in good retrieval performance. 
We still use the query in Fig. 6-e as an example here. The search result of word  X  X ire X   X  X ow to store a flat tire X . 
The result after verb-noun expansion is much better. All the results is related to  X  X ix query method. represents the query intention that we want to fix motorcycle tire. The articles found are like  X  X he best way to fix a flat motorcycle tire X . In contrast, the results of the second method cannot represent the feature  X  X otorcycle X . 
However, sometimes the query result after verb-attributes-noun expansion is not the best. For example, the search result of  X  X uy satellite phone X  is not good as that of  X  X uy phone X . The reason is that the attribute  X  X atellite X  imports some articles that do not deal with  X  X uy phone X , like  X  X ow to call a satellite phone X . But this kind of result is quite few so the overall result is influenced very little. 
After this test, we compare our method with the widely used search engine Micro-soft Bing. Bing has two query expansion functions. One is the RTQE function ries after the users do searches. To get English expanded queries, the location option teers in Bing. 
After a preliminary investigation, we found that the RTQE result of Bing differs a lot if the word order of a query changes. For example, if we type words in the order of change the order, Bing cannot recommend the words we want. recommendations (NOT); get correct recommendations only in normal word or-der(NORMAL); can get correct recommendations both in normal order and other word orders(ALL). Table 4 shows the test result of Bing RTQE method (do not con-tain  X  X elated search X  result.) 
As shows in Table 4, half of the queries cannot get correct recommendations while 84% of the queries are successfully expanded in our method. Moreover, only 18% of the queries can get correct recommendations when the query words are not inputted in accordance with the normal word order in Bing, while both verb and noun words are allowed to be inputted first in our method. 
For those queries that are not in the ALL group, we do search when the RTQE fails and judge the recommendation results of the  X  X elated Search X  function. We find that only 13.3% of these queries can get useful query recommendations. 
This test shows that our RTQE method has advantages when compared with indus-trial search engine. After testing, we did survey of the testers on their experience of the RTQE method. We got the following message after summarizing their feedback. 
This RTQE method is quick, and is able to meet the speed requirement of RTQE results are satisfying. 
But sometimes the testers feel inconvenience when doing search because currently the RTQE method can only handle verb, noun and attributes. And this method is cat-pansion result is not good. This paper proposed a novel RTQE method concerning user query intention. We used a large corpus from www.ehow.com to build a dependency relation network of verb, noun and attributes, then designed a RTQE method using this network. The proposed RTQE method is proved to be effective in representing user query intention and hence improve retrieval performance. 
In the future, we will work on reducing the limit on the types of part of speech and do more research on offering personalized expanded results to represent the user intention better. Query Recommendation is an information retrieval technology to recommend identic-query intention through machine learning, mine the expressional forms of query with associated queries. The ultimate purpose is to use the synonymous or associated que-ries as medium to share and mutually recommend better search results. 
Currently, the main research form of query recommendation is to express user X  X  content or click and browsing retrieval behaviors, then measure query semantic con-sistency and intention association, and recommend approximate or related queries because of sparse information, fuzzy structural relations and optional component which contain richer and potential intention informations. Therefore, query itself was long considered  X  X e unrefined X  for intention analysis and query recommendation. external resources associated with the query for intention analysis will introduce a lot of noise which will mislead intention analysis and calculation. So, if there is an effec-achieve the mining of correlation intention and query recommendation from the pers-pective most closed to user as well as inte ntion expressed in behavior. Therefore, we propose a query recommendation method based on divided pretreatment to query targets and query intentions. This method includes three basic steps.  X  intention.  X 
Clustering the words of query intention to mine consistent or similar intention.  X  query recommendation. Query target refers to the target entity, behavior or status which user retrieves. Query impose on the query target. For Example 1, there is a query  X  Where is the No. 18 bus station?  X ..Its query target is  X  No. 18 bus station  X  and query intention is  X  where  X . 
We can effectively overcome the sparsity and ambiguity of the query informations, scale query samples. As the sparsity of the information which describes the intention, such as  X  X ow to get to the 18 bus station? X ,  X  X here is the location of the 18 bus sta-station X  in large-scale query samples. We expand the intention  X  where  X  in example 1 inal query intention. As description of target can cause ambiguity possibility, we can  X  Iphone Quote  X ,  X  Ipod Quote  X ,  X  Computer Quote  X ,  X  Mainboard Quote  X . By extract-rather than  X  Banana Quote  X  . 
In the rest of the paper, we will first introduce the related work in Section 2. Then, we give the framework of a new query recommendation method based on divided pretreatment to targets and intentions in Section 3. Section 4 will show the modeling process of classifying and recognizing the targets and intentions of query. The method of intention cluster will be given in Section 5. We present and discuss the experimen-tal results in Section 6. Finally, we conclude the paper in Section 7. According to the source of corpus, the me thods of query recommendation are divided into two categories: document-based approaches and log-based approaches. constitute candidate recommended queries. Yana n Li et al. [1] proposed a division of document-based approaches into three categories. The first approach based on global document is to find the words or phrases related to the query in relevant documents. But it is difficult to get the relevant documents. The solution is to select top-n pseudo-large-scale click logs to obtain the relevant documents that have not been clicked.The last approach based on manual editing corpus will have accurate results but deal with the new word in the network difficultly. 
Currently, log-based approach is the main direction of research on query recom-mendation. Cao et al. [3] divided log-based approaches into two categories as session-based approach and click-based approach after summary of previous work. For session-based approach, Huang et al. [4] recommended mutually between queries proach, the similar queries were recommended through bipattite graph model which was built up with the user X  X  click and history information. Jimin Wang et al. [5] pro-posed a method based on the numbers of common URLs clicked between queries. The basic assumption is that the more same clicked URLs two queries share, the more similar they are. Yanan Li et al. [6] proposed a method based on weighted SimRank consistent semantic queries. As the sparse of user clicks in query logs, the solution is to compute iteratively by transferring similarity with improved SimRank algorithm or the general search engine with high real-time demand. 
The purpose of document-based approach and log-based approach is to better un-derstand user query intention and recommend more high-quality queries. But the trieval results. Therefore, research on recognition of query intention has become pop-ular in recent years. The main framework is shown in figure 1 which contains two main parts: offline-sys and we determine the similarity of query intention and priori intention clusters with intention similarity matching. The most similar intention cluster is chosen as a candidate. Finally, we measure similarity on each word in cand idate intention cluster with query intention test and rank intentions according to similarity. At last, the most similar intention word and query target will be combined to form recommended queries. 4.1 Query Preprocessing Based on the rules of classification proposed by Broder et al. [8], Beaze-Yates divided give target or intention word(such as query  X  Apple  X ) with amiguous. 
Thus, we propose three filtering rules to recognize and filter the not informational numeral, punctuation or URL. Because considering the internal structure and the componential role information of queries from a semantic perspective, the noise queries in nature. Search engine can identify and recommend a consistent navigation-al link with maximum string matching. 
By taking the refined filter rules and remove the reduplicate queries, we collect 1.9 recommendation research. As the query resources coming from Chinese retrieval mentation, we use ICTCLAS to realize the basic division of query features words. 4.2 Feature Selection We should consider from multiple perspectives such as lexical-based perspective and will be introduced separately.  X  Lexical-Based Perspective 
We choose four empirical classification features (shown in Table 1) from the lexi-cal-based perspective. First, the word itself is an effective feature. The words such as  X   X   X   X ,  X   X   X   X  and  X   X   X   X  have obvious intention performance, while these words like  X   X  X   X ,  X   X  X  X  X   X , and  X   X  X  X   X  express lower probability of intention than proba-bility of target. Secondly, the head or tail character of word can be used as classifica-tion feature. For example, the character  X   X   X  among the words as  X   X  X   X ,  X   X  X   X ,  X   X   X  trast, another group of words  X   X   X   X ,  X   X  X   X  and  X   X   X   X  etc. reflect intention by the main grammatical structure of Chinese is Subject-Verb-Object , which Subject and phrases often reflect the targets, while Verbs and verbal phrases often reflect the spe-reflect user X  X  intention.  X  Context-Based Perspective 
From the context-based perspective, we proposed two features as frequency and example, user does not often use  X   X   X   X  as query independently because its descrip-tion of itention is fuzzy. By using a combination of large different targets such as  X   X   X   X ,  X   X  X   X , and  X   X  X   X  etc, we can form a complete query semantics. So, the inten-categories of words. In contrast, the target words have a single combination with other instance, target word  X   X  X  X   X   X  X  X  X   X   X  can only collocate a limited number of in-tention words such as  X   X   X   X ,  X   X  X  X   X ,  X   X   X   X , and  X   X   X   X  or used as query directly lower than intention words like  X   X   X   X  and so on. phenomenon when user build query, they are used to put some words with purpose in the head or tail of the query, for example  X   X   X  X  X  X  X  X  X  X  X  X  X  III  X  or  X  Facebook  X   X  X  X   X  X  X  X  X  X  X   X . So, the position of word in query is an effect feature. In this paper, follows. Given a specific query w , Ord(w) represents the order of w in the query and calculated as formula (1). query description by taking advantage of intention relation between internal clusters. intention cluster. It aims at cluster the words that are same with user X  X  query intention; 2) Preference-based query intention cluster. It aims at cluster the words that meet user preference. The former is similar to the phenomenon which identify the expression of intention words which point to similar goals. 5.1 Intention Vector associated intentions from semantic . We can only match the consistency of word type that cannot help reach the core purpose of intention cluster. Therefore, we propose the concept of intention vector to build the associated network of intentions. ceptualized as the form of  X &lt;Target + Inte ntion&gt; X . Intention should be combined with the specific target that makes sense. For example, the intention word  X   X   X   X  is unable explicit  X   X   X  X  X  X   X  such as  X   X  X  X   X ,  X   X  X  X   X  and  X   X  X  X  X  X  X  X   X , the whole query has intention words or such as WordNet hyponymy lexical semantic relations features, but through large-scale query samples to describe the targets indirectly which high-frequencily co-occurred with intention words. For example, given a query inten-tion word  X   X   X   X  or  X   X  X  X   X , the form of vector constructed is shown in Table 2. 
By building the intention vectors, we convert the correction intention measure into the similarity measure between vectors. The contact with intention is more closely if the intersection of intention vector is larger between different intention words. 5.2 Intention Similarity Calculation From the description of intention above, we attempt to use the correlation matching of intention based on VSM and Language Model. And such correlation is used as refer-guage Model[10] calculate with Kullback Leibler Divergence[11]. 
Through calculating similarity of intention with vector space model and measuring similarity of probability distribution between intentions with KL divergence, we real-intention clusters and provide reference data for query recommendation.  X  Vector Space Model + Cosine Similarity 
Using VSM to describe intention, we need to estimate the numerical weight which gives to each dimensional target in the model. In this paper, we use TF-IDF to calcu-represents the number of intention vectors that O i and I j co-occurs. The larger the value is, the stronger the features of target are. But, IDF value reflects the contribution that target word O i makes to the global intention. The larger the value is, the weaker the features of target are. Through TF-IDF calculating, we obtain each calculate its cosine similarity. The larger the value is, the more similar the intentions are.  X  Language Model + KL Divergence 
Besides, we introduce the language model to calculate the probability that intention target word o co-occurs with intention word I i . ability distribution of intention vector is and the more similar they are. 5.3 Intention Recommendation words and targets, mine the targets co-occurred in the global query samples firstly. By describe intention separately with cosine similarity and KL divergence to measure the similarity between intention words and prior intention clusters. The most similar clus-ter is chosen as candidate recommended set according to the similarity ranking. Final-form new queries and recommend as related queries. the local property of cluster. In other words, the correction can only represent part of intention will be ignored. That is, the kind of intentions have relevant preference such fine-grained division. At the moment, the relevance always comes from one point or cluster and use for query recombination and recommendation. 1) Adjust cluster para-cluster and the similarity between query intention under test and each intention word introduced in experiment part. firstly; Then, give the main systems include Target and Intention Recognition System, Intention Cluster System and Query Recommendation System which participate in the test; Finally, we give the test results of each system and the corresponding analysis. 6.1 Corpus The experimental data is Sogou2008 query logs which contains user query informa-tional queries and group the queries with same clicked URLs. with an average of 5. The queries in the same group must have at least one common volunteers do research on query recommendation more than one year. and clustered. The form of labeling is to compare the intention words in cluster which provided by the automatic intention clustering system to intention words under test by human-labeling (three volunteers + cross-validation). 
For query recommendation experiment, we extract 2,000 queries randomly and di-presentation of overall performance. 6.2 Evaluation Method line1 is the accuracy of target reaching 66.7%, which assumes that all query words are accuracy of intention reaching 33.3% , which assumes that all query words are inten-tions for the prior probability with th e recall of intention is also 100%. average of all clustering performance. For query recommendation, we use global to evaluate. Consistent precision refers to the recommended queries have the consis-tent semantics with original query(such as  X   X  X  X   X  and  X   X  X  X   X ), that is different words but synonymous. Relevant refers to the recommended queries is relevant with original man. P@n refers to the precision of query recombination and recommendation which use the most similar top-n intention words in cluster. 6.3 Experiment Results and Analysis system, intention cluster system and query recommendation system as well as the analysis.  X  The Performance of Target and Intention Recognition Sys-1 is lexical-based approach used only and Sys-2 uses Context-base approach only. sys-1 and sys-2. But combined two kinds of features, the results improve about 3%~4%. The reason benefits from the advantage of indentifying intention words based on lexical meaning and target words based on context property. 
Besides, the recall of target words in sys-2 is highest reaching 95.2%. Correspon-example, for query  X   X  X  X   X  X  X   X , the target word is  X   X  X  X   X  and intention word is  X   X   X   X  . Its overall meaning is to find the telecom located in Beijing but not the telecom words is low and the position in the query is limited that makes no obvious distinction with target words. So using the feature alone will cause the m isjudgment of implicit words because of the misjudgment. In contrast, sys-1 can improve the recall of inten-tion words but not lose the recall of target. So the lexical-based approach is effect for feature. 
Considering the distribution of target and intention in real, the measure of compre-hensive system performance should take F-value and global accuracy into considera-the classifier mixed with features performs better than baseline1 and baseline2. Espe-cially, the ability of identifying the intentions has improved 9.8 percent while that of sys-3 has reached the condition for further treatment.  X  The Performance of Intention Cluster We construct 1,981 intention vectors to realize two kinds of intention cluster systems. The Apcluster algorithm was chosen because of the advantage without setting the Model and Sys-KL uses Language Model to calculate KL Divergence. The experi-mental results of query intention cluster are verified by three volunteers. As shown in table 4, #1#2#3 represents three experimental samples. Obviously, the performance of Sys-VSM is better than Sys-KL . 
From the data of table 4, we can find the results of manual judgment swing greatly, intentions have brought uncertainties to volunteers. At the same time, the VSM-based based on language model is lower. And at the same experimental samples, the number tion between queries and a more diverse form of combination between queries. mended equally, but ranking according to the similarity of intentions and recommend the prior.  X  The Performance of Query Recommendation The performance of query recombination and recommendation according to similarity is illustrated in Fig.2, whose horizontal axis P@n represents top n(1  X  n  X  10) intention  X  X va i  X   X  1  X  i  X  6  X  indicates different volunteers X  accessments. As shown in the graph, recommendation precision goes down with n increases, and it reaches overall highest at P@1,which is 57.8% on average. We attribute it to the recommendation mechan-ism, which always chose the center-closest intention word first. And these words are follows: Example3:  X   X  X  X  (see a doctor) X ,  X   X  X  X  (register) X ,  X   X  X  X  (diagnose) X ,  X   X  X  (prescribe) X  and  X   X  X  X  (hospitalization) X . (cluster center is  X   X  X  X  (see a doctor) X ). highest probability. 
However, Precisions drop quickly after P@4 , which is caused by the relations be-words outside center are related to the center in different aspects, and the candidate is only related to the center and a certain aspect, the results turn out to be other aspects are of little consistency. For example, a candidate intention is related to  X  hospitaliza-is not a good recommendation for  X  where for hospitalization  X . So, with the increasing n, words far from center and related aspects have higher probability of recommenda-tion, which leads to lower precision. recommendations. Recommendations with high performance are words with obvious the targets, like place names. Simple substitution of the latter ones may cause distor-tions of user intentions. Thus, a classificatio n of queries according to user emphasis is needed to improve the system X  X  performance. 
Average precision of the system is shown in table 6. It shows a low overall preci-intention words. Simple substitution of intention words in our method suffers from the problem of unsmooth semantics and unreasonable logics, which affects about 40% of all the intention words. For example, intention words  X  X recious X  and  X  X xpensive X  are related, but when combined with target word  X  X ir X , the situation is different. It is easy generation, and is an essential part of our future work. Our paper proposed a new query recommendation which concentrates on the analysis and application of query itself. It recognizes the target words and intention words by After that, synonymous or related intention words are obtained by clustering for rec-ommendation. Experiments show that our method can divided target and intention word effectively, and it gets 55.67% performance. P@1 reached 57.83% respectively. 
However, there is still a large room for improvement. The experimental results algorithms. Therefore, future work will focus on feature selection method to enhance analyze the combination of target words and intention words, in order to form a fluent and logical query. Acknowledgements. This research is supported by the National Natural Science Foundation of China (No. 60970056, 60970057, 61003152), Special fund project of the Ministry of Education Doctoral Program(2009321110006, 20103201110021) and Natural Science Foundation of Jiangsu Province, Suzhou City(SYG201030). Relation extraction (RE) is an important information extraction task in natural language extraction is to detect and characterize semantic relationships between pairs of named Person-Social relationship between the person entities  X   X   X  and  X   X  X  X   X  in the Chinese phrase  X   X   X   X  X  X   X  (his wife). 
Generally, machine learning-based methods are adopted in relation extraction due to instances) they can be divided into feature-based methods and kernel-based ones. The key issue of feature-based RE is how to extract various lexical, phrasal, syntactic, and semantic features [1-7], which are important for relation extraction, from the sentence dependency paths [12-13] etc., becomes the central problem. In Chinese relation extraction, many studies focus on feature-based methods, such as [14-16] while kernel-based methods, such as edit distance kernel [17], string kernel[19], convolution tree kernels over parse trees [20-21], have gained wide popularity. 
It is widely held that lexical semantic information plays an important role in relation extraction between named entities, since two words, different in surface but similar in semantic, may represent the same relationship. For example, the two phrases  X   X   X   X  X  X   X  (his wife) and  X   X   X   X  X  X   X  (her husband) convey the same relationship  X  X ER-SOC.Family X  in the ACE terminology, though  X   X   X  (he) and  X   X   X  (she),  X   X  X  X   X  (wife) and  X   X  X  X   X  (husband) are two distinctive, yet semantically similar words. Therefore, different approaches are proposed to exploit this lexical semantic similarity in relation extraction. Chan et al.[6] and Sun et al.[7]use the corpus-based clustering techniques [21] to obtain the semantic codes of entity headwords, and then embed them as semantic features into the framework of feature-based relation extraction. However, levels of relation types to be extracted. semantic similarity based on TongYiCi CiLin [22] or HowNet to an edit distance kernel or a sequence kernel respectively for relation extraction. However, as the convolution widely adopted convolution tree kernel can benefit from such lexical semantic resources. Bloehdorn and Moschitti[24] propose a generalized framework for syntactic and semantic tree kernels for Question Classification, which incorporate semantic information when computing structural similarity between two parse trees. Following their work, we incorporate lexical semantic similarity based on HowNet (abbreviated extraction with that of directly embedding lexical sememes in tree structures. studies while Section 3 introduces the tree structure used for RE in this paper. Section 4 elaborates two methods of exploiting a lexical resource for Chinese relation extraction. paper and points out future directions. applications of semantic information to relation extraction. 
In English relation extraction, there are thr ee previous studies considering some kind country name list and a personal relative trigger word list from WordNet. They  X  X OLE.Residence X  in ACE 2003 and of  X  X ER-SOC.Family X  in ACE 2004. Chan et al. [6] combine various relational predictions and background knowledge, including word clusters automatically gathered from unlabeled texts, through a global inference procedure called ILP (Integer Linear Pr ogramming) for relation extraction. They demonstrate that these background knowledge significantly improves the RE What they mean by semi-supervised learning is that the additional features are induced Nevertheless, the subtle semantic commonality between words seems inherently difficult to be captured by feature-based methods. 
On the other hand in Chinese RE, Che et al. [17] employ the Improved-Edit-Distance (IED) to calculate the similarity between two Chinese strings, and further considering lexical semantic similarity between words based on TongYiCi CiLin, their experiments show that the lexical semantic-embedded IED kernel method performs well for the person-affiliation relation extraction. Liu et al.[18] acquire lexical semantic similarity scores based on HowNet, a widely used Chinese lexical resource, and incorporate them into a sequence string kernel. Experiments on some ACE-defined fine-grained relationships show promising results. Up till now, no attempt has been made to incorporate such semantic information into tree kernel-based relation extraction, which seems more natural than feature-based methods and is exactly the focus of this paper. The two key issues of tree kernel-based rela tion extraction are the representation of tree former while the next section discusses the latter. 
Since the focus of this paper is the exploitation of lexical semantic resources to tree kernel-based RE, we directly adopt a state-of-the-art tree structure--the Unified Parse and Semantic Tree (UPST-FPT)[10]as the tree structure, which incorporates Feature-Paired Tree (FPT) manner, into the Dynamic Syntactic Parse Tree (DSPT). 
Figure 1 illustrates such a tree structure derived from the phrase  X   X  X   X  X   X  (bank president) for a relation instance between the  X   X  X   X  ORG (organization) entity and the  X   X  X   X  PER (person) entity, where  X  X P1 X  denotes the entity type of the 1st entity and likewise  X  X P2 X  denotes that of the 2nd entity. The tree structure on the left of the dotted subtypes etc., is omitted for brevity. incorporating into tree kernel computation and directly embedding into tree structures. 
The first question for exploiting lexical semantic resource is, given too many words as leaf nodes in a parse tree, which of them are useful for relation extraction? Sun et al. [7] conduct a series of experiments using word clusters for different words in a relation instance, such as entity headwords, bag of headwords, the words before and after the entities etc., in a feature-based framework of RE and find that only entity headwords are important for RE. Therefore, we first co nsider the semantic information of lexical items corresponding to two entities involved in a relation instance. Moreover, we also consider the semantic information of the verb as Qian et al [10], if exists, between two entities, as some relation instances are clearly expressed in a verbal form. 4.1 Convolution Tree Kernel The convolution tree kernel [23] counts the number of common sub-trees between two whole tree space. It can be computed as follows: recursively as follows: Step 2; to Step 3; node n, and of sub-trees exceedingly depending on the size of sub-trees. 4.2 Semantic Convolution Tree Kernel: Incorporating Lexical Semantic extraction [8-10, 25], they disregard lexical semantic similarity between words in parse successful application of the syntactic and semantic convolution tree kernel [24] to the task of Question Classification (QC), we adopt a similar Semantic Convolution Tree Kernel (SCTK) to Chinese relation extraction with the lexical semantic similarity being calculated using Chinese lexical semantic resource. CTK except that in Step 1, one additional case should be considered as follows: LexSim( HW1 , HW2 ); otherwise  X  ( n 1 , n 2 )=0; immediately under n 1 and n 2 respectively and LexSim( HW1 , HW2 ) denotes the lexical semantic similarity between these two headwords which can be calculated using lexical resources such as HowNet. 
HowNet 1 , a commonly used Chinese lexical resource, is a lexical knowledge base with rich semantic information, where a word is described as a group of sememes in a complicated multi-dimensional knowledge description language, and the first sememe reflects the major feature of one concept. For example, the Chinese word  X   X  X  (camera  X   X  is the first sememe of  X   X  X   X . Due to its richness in lexical semantics, it has been widely exploited in various NLP researches [29, 30]. similarity scores based on HowNet. The similarity score between content words (entity similarity between primary sememes, that between other sememes, that between sets, and that between feature structures. 
It is worth noting that in most cases, the entity headwords can be used directly to calculate their lexical similarity scores, e.g., in the Chinese relation instance  X   X   X   X   X  However, take the entity mention  X   X  X  X  X  X  X  X  X   X  (DaAn Forest Park) as an example, since this headword is not a well-known proper noun and can not be found in HowNet, any similarity score involving this entity calculated using HN will be zero. Our solution to this problem is to first segment the entity headword into sequential words using the segmentation package and then to take the rightmost word as the new headword. For example, the entity mention  X   X  X  X  X  X  X  X  X   X  is segmented into  X   X  X  X   X  X   X  X   X  meaningless to calculate the similarity between the Chinese characters in person names. This strategy of segmenting the entity headword also applies to the method of embedding lexical sememes in tree structures. 4.3 Incorporating Lexical Seme mes into Tree Structures computation cost brought about by semantic convolution tree kernel. For HowNet, since the first sememe of a lexical item reflects the major propery of one concept, we structures. For example, in the relation instance X   X  X  X   X  X  X  X  X   X  X   X (Taipei DaAn forest park), the first sememes of HowNet corresponding to  X   X  X  X   X (Taipei) and  X   X   X   X (park, the head word) are  X   X  X  X   X (place) and  X   X  X  X   X (facility) respectively. After the sememes are extracted from HowNet, namely,  X   X  X  X   X (place) and  X   X  X  X   X (facility), they  X  X HN2 X  denote semantic information (the first sememes) based on HowNet corresponding to the 1st entity and the 2nd entity. 
In addition, if there is a verb nearest to the 2nd entity along the path connecting two entities, a node  X  X HNV X  followed by the verb X  X  first sememe is also attached to the root node. 
The first sememes of two entities or the verb are extracted from HowNet as follows: 1) find the lexical HW1  X  HW2 corresponding to the 1st and 2nd entity, and find the 2) search HowNet for the first sememes of HW1, HW2 and VLEX; 4) attach HCODE1  X  HCODE2 and HVCODE to the nodes SHN1  X  SHN2 and Chinese relation extraction. 5.1 Experimental Setting The ACE RDC 2005 Chinese corpus is used as the experimental datasets for Chinese semantic relation extraction. The corpus contains 633 documents, which were collected subtypes, 6 major relations types and 18 relation subtypes. 
The corpus is first word-segmented using the ICTCLAS package, and then the corpus is parsed using the state-of-the-art Charniak X  X  parser [32] with the boundaries of all the entity mentions kept. Finally, relation instances are generated by iterating over tree structures and incorporating optional enti ty-related information (e.g., entity types, subtypes). In total, we obtain 9,147 positives and 97,540 negatives for Chinese relation instances. 
In our experimentations, SVMLight-TK toolkit is adopted as our classifier since we usually treat RE as a classification problem. The package is modified to incorporate the Tree (SST) kernel is used since it yields the best performance, while the decay factor  X  (tree kernel) is set to the default value (0.4). evaluation metrics are Precise, Recall, F-meas ure, which can be abbreviated as P/R/F1 respectively. Finally, in order to determine whether an improvement of performance is statistically significant or not, we perform approximate randomization tests similar to significant if p  X  0.01 or 0.01&lt;p  X  0.05 respectively. 5.2 Experimental Results and Analysis We first investigate the impacts of two different methods of using HowNet on the task of relation extraction. Then we compare our system with other state-of-the-art Chinese relation extraction systems. Impact of Incorporating Lexical Semantic Similarity Table 1 compares the performance of P/R/F1 for relation detection (2 types) and major 2005 Chinese corpus when lexical semantic similarity is incorporated in tree kernels for Chinese relation extraction. The DSPT (Dynamic Syntactic Parse Tree) [7] structure is used as the baseline (BL) without any semantic information.  X  X T X  denotes that entity types (namely, major types and subtypes) are augmented into DSPT in the FPT (Feature-Paired Tree) [10] fashion while  X  X N X  or  X  X NV X  means either entity lexical computation. The 2 nd column represents systems which incorporate various features or lexical similarity. For example,  X (1)+HN X  denotes considering the entity similarity on System 1, while  X (3)+HN+HNV X  considers both entity and verb similarity on System 3. The significance tests are conducted between a certain system (e.g.,  X (1)+HN X ) with its base system (i.e., System 1) and the performance increase, which is significant or very significant, is underlined or double-underlined respectively. Additionally, the best scores of P/R/F1 for each subtask are also highlighted respectively. Chinese RE systems achieve better performance no matter whether the entity type shows that: One important trend for these three subtasks, exposed in the table, is that, although the absolute performance scores decrease with th e increase of the number of relations types intuitively explained by the example  X   X   X   X  X  X   X  (his wife), where a relationship  X  X ER-SOC.Family X  exists between the entities  X   X   X  and  X   X  X  X   X . The phrasal structure determines that certain relationship exists, while the lexical semantics of two entities determines the specific type of their relationship. Impact of Embedding Lexical Sememes extraction on the ACE 2005 corpus when the first sememes are embedded in tree structures like in Figure 2. Different from Table 1,  X +HN X  denotes embedding the first computation. Likewise  X +HNV X  denotes embedding the first sememe of the verb, all the other notations are the same as those in Table 1. Table 2 shows that: Table 3 compares the performance of major type relation extraction of our SCTK method with other state-of-the-art systems for Chinese relation extraction on the ACE corpus or evaluation strategies are adopted by different systems. For example, Li at el. [16] adopt a feature-based method using two-fold training/testing strategies while Yu et al. [20] experiment on a subset of the ACE 2005 corpus, though using the same 5-fold single-kernel method achieves promising results and has the potential to combine with other feature-based methods for better performance improvement. In this paper, we empirically demonstrate the impact of lexical semantic resources of HowNet on Chinese relation extraction. We explore two methods of exploiting lexical tree kernels and directly embedding lexical sememes into tree structures. A series of experiments on the ACE 2005 benchmark corpus indicate that HowNet can significantly improve the performance of Chinese relation extraction via incorporating embedding lexical sememes directly into tree structures, though as an intuitive method, improve the performance only when entity types are unknown. We also find that lexical relationships. This suggests that when extracting more specific semantic relationships, lexical semantic resources are preferable. 
For future work, different ways of calculating similarity based on lexical resources could be investigated to find the best one, and we will explore the corpus-based word some domains other than the general domain. Acknowledgement. This work is funded by China Jiangsu NSF Grants BK2010219 and 11KJA520003. In recent years, the application and popularity of WWW provides a huge repository of tracted from the information repository, and then social networks can be constructed consequently. The study of personal relation extraction attracts wide attention current-thods can be divided into two categories: from the Web pages and from plain texts. 
Several representative studies of extracting social relationships from the Web pages are mainly as follows. Kautz et al. [1] and Mika et al. [2] employ the statistics SVM classifier to classify the relationships. 
The methods of personal relation mining from Web pages mainly have two name disambiguation. With the maturity of natural language processing technology, natural language text, and solve the problem of ambiguous person names through coreference resolution within single document as well as cross documents. 
Jing et al. [5] extract the relationships between personal entities and corresponding events from a specific domain of oral transcripts via named entity recognition, Due to the poor quality of transcribed texts, the F1 score of relation extraction is only and dialogue testing, then determine the relationships between the two roles and build employing the existing techniques for extracting rich personal relationships from personal relationship extraction. 
Relation extraction aims to identifying the semantic relation between entities from natural language text (MUC 1987-1998; ACE 2002-2005) [7]. At present, relation extraction methods are mainly feature-based [8-9] and tree kernel-based [10-15]. well as semantic information and other kinds of information. Tree kernel-based by calculating the common sub-tree of them. It can effectively capture the structured better performance in the semantic relation extraction task. relation extraction, we build a personal relation extraction corpus by expanding the ACE instances. First, the SPT further trimmed through new pruning rules. Second, the semantic methods lead to the performance improvement of personal relation extraction. In the rest of this paper, we first introduce the personal relation corpus in Section 2. section is a summary of this paper and some directions for future work. types of two entities are both PER, resulting in a corpus which contains 651 instances of PER-SOC, 8 instances of EMP-ORG, and 12 instances of GEN-AFF. implicit description of social relations induced by dynamic events, such as interaction of annotated event information, of which the events of CONTACT are closely between the entity participants in the event. For example, in the sentence  X   X  X  X  X  X  X   X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  (Yesterday Zhu Rongji called the Canadian Prime Minister Jean Chretien) X , there is an event of the type  X  X ontact. Phone-Write X , and an interaction relationship between the two participants of  X   X  X  X  X  (Zhu Rongji) X  and  X   X  X  X  X  X  X  (Jean Chretien) X  can be induced. 
According to the annotation format of the ACE 2005 documents, first, we pick up the events whose major type is CONTACT. Second, we obtain the event participants, whose event-argument is  X  X erson-Entity X . According to the combinatorial rule, if the number of person participants is greater than or equal to 2, we give each combination CONTACT relationship. If the two involved entities already belong to another type of relationship, we retain the original relationship. 
Eventually, there are 209 CONTACT relation instances obtained from the we got 880 positive relation instances and 18,599 negative relation instances. Because personal relationships mainly contain the PER-SOC type and CONTACT type, the performance. 3.1 Structured Information for P ersonal Relation Instances In tree kernel-based methods, a relation instance between two entities is encapsulated information, and their experimental results illustrate that Shortest Path-enclosed Tree (SPT) obtains the best performance. The SPT is part of a parse tree which is enclosed Sensitive Shortest Path-enclosed Tree (CS-SPT), which includes necessary context Dynamic Relation Tree (DRT) using a series of heuristic rules based on the principle of constituent dependency. The preliminary experimental results show that the satisfactory, which caused by the sentences of personal relation corpus are quite long. Therefore, in this paper, we eliminat e redundant information from the SPT and performance. Three pruning rules are listed as follows.  X 
Removing the entity coordination structure (RMV_ENTITY_CC): When a coordination structure appears in the path connecting the two entities, we can remove most of the coordinates to simplify the SPT structure. Since the semantic relations for all coordinates are the same, in order to highlight the involved entity, removing all other coordinates. As Fig. 1(a) shows, in the phrase  X   X  X  X  X  X  X  X  X  X 
When we consider the semantic relation between  X   X  X  X   X  (Naruhito) and  X   X  X  X   X  order to accurately describe the relation between the entity  X   X  X  X   X  (Naruhito) and  X   X  X  X   X  (daughter), we only retain the coordinate of  X   X  X  X   X  (Naruhito), thus the phrase  X   X  X  X  X  X  X  X   X  (Naruhito's daughter) can accurately reflect the essence of personal relationship.  X 
Removing the NP coordination structure (RMV_NP_CC_NP): As removing the entity coordination structure, this also helps to reduce noise. We can use the same method to eliminate the redundant information of the coordination structure of 
Fig. 1(b) shows, in the sentence,  X   X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X   X  X  X  X  X  X  X  X  X  X  X  X  X   X  (Batlle, and Bolivian President Banzer, Chilean President between  X   X  X  X  X   X  (Batlle) and  X   X  X  X  X   X  (Ricardo Lagos), we just need to retain the noun phrase  X   X  X  X  X   X  X  X  X  X  X  X  X  X  X   X  (Batlle Chilean President Ricardo La-gos). The redundant part of  X   X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X   X  (, and Bolivian between two entities  X   X  X  X  X   X  (Batlle) and  X   X  X  X  X   X  (Ricardo Lagos). Removing large number of person names connecting with conjunction, comma and pause punctuations in relation instances of the CONTACT type, so we also take comma and pause punctuations as special types of coordination structures.  X 
Extending the verb right to the 2 nd entity (EXT_RIGHT_VERB): According to ship. Verbs describe events, actions, states, change of states, and experiences, all of being recovered, this paper only extends the verb phrase structure from the second entity to the lowest common node in SPT. As in Fig. 1(c), in the sentence X   X  X  X  X   X  X  X  X  X  X  X  X  X  X  X  X   X  (A fourth-grade student wrote a letter to aunt), the event participants  X   X  X  X   X  (student) and  X   X  X   X  (aunt) have a CONTACT type relation. corresponding relationship between them. Through the restoration of the verb  X   X   X   X  (wrote), the new structured information can reflect the interaction nature between the entities  X   X  X  X   X  (student) and  X   X  X   X  (aunt). 3.2 Embedding TongYiCi CiLin Semantic Information relationships, and thus play an important role in extracting semantic relations between semantic similarity in TongYiCi CiLin, and attain good performance in the person-affiliation relation. Liu et al. [17] perform Chinese relation extraction on three major improve the relation extraction performance. among which the number of polysemous words is 8,860. It defines 12 major classes, and atomic word groups. 
Semantic information can be embedded into the structured information: First, method, which means adding the CiLin semantic information of two entities to nodes have smaller contribution to the overall similarity for calculating the tree-kernel similarity, thus we add the semantic code information to the root of the parse tree. As shown in Fig. 2, in the sentence  X   X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X   X  (The leader X  X  (leader) and  X   X  X  X   X  (families) are  X  X f10a02 X  and  X  X h01B01 X  respectively. The children nodes of SC1 and SC2 nodes represent the lexical semantic codes of the 1 st entity (E1) and the 2 nd entity (E2). 3.3 Applying Over-Sampling and Under-Sampling Techniques tion of instances makes the SVM classifier heavily biased towards the majority-class dered as a serious problem needing an urgent solution. samples under certain preconditions, and estimates the probability of occurrence of an under-sampling. Over-sampling repeats minority-class samples to reach data balance, while under-sampling randomly select majority-class samples to reach data balance. 
There are a large number of unnecessary repetitive minority-class samples in over-sampling, while many majority-class samples have not been fully utilized. In personal relation extraction, the structured information of negative samples is overwhelmingly large and diverse, so that their structured information has far more diversity than posi-tive instances. The experimental results would not be satisfactory by making the bal-ance between positive and negative samples. Therefore, we made several experiments balance point. Specifically, in the process of under-sampling, we increase the ratio of negative instances to balance the structured information diversity of positive and neg-gradually to make it more reasonable. In order to compare with the full sampling, the test sets for all experimental combinations are the same. This section first introduces the experimental setting, and then gives the experimental results and corresponding analysis. 4.1 Experimental Setting In our experiments, we formalize personal relation extraction as a multi-class so as to separate one class from all others and select the one with the largest margin as the final answer. To take full advantage of corpus resources and reduce the variation of the experimental results, we apply five-fold cross-validation strategy on the corpus. large-scale corpus [10-15]. The evaluation me trics are commonly used precision (P), on major relation types. the entity mentions kept. We iterate over all pairs of entity mentions occurring in the same sentence to generate potential relation instance. Then we insert a tag node on the respectively, or E if the entity is not involved. experimental results is statistically significant or not, we employ approximate random underline are respectively used to stand for p  X  0.01, 0.01  X  p  X  0.05 and p  X  0.05, that is, the difference is very significant, significant and not significant. 4.2 Experimental Results and Analysis Effect of the Pruning Rule s for Structure Information personal relation extraction applying the three pruning rules. Outside the brackets are the results of the overlapping mode (that is, three different pruning rules were consecutively between the total experimental results of the current system and the baseline system. The score increased by 3.5 units. We also find from Table 1:  X 
Both in the standalone and in overlapping modes, the improvement degree of three kinds of pruning rules are different. The RMV_NP_CC_NP rule improves the most; the RMV_ENITTY_CC rule affects less; the EXT_RIGHT_VERB rule contributes the least. The reason is that the number of NP coordination structures for RMV_NP_CC_NP is bigger than that of entity coordination structures for 
RMV_ENITTY_CC, thus the former rule removes much more redundant information than the latter, while the EXT_RIGHT_VERB rule only recovers a small number of verbs and these verbs are quite diverse.  X  The F1 scores of the CONTACT type are significantly lower than those of the PER-
SOC type, mainly caused by the low recall (about 10%-20%). The main reason is that PER-SOC instances are usually refl ected in local range, while CONTACT increases the difficulty of extraction. What X  X  more, the parsing performance for long sentences is worse, thus leading to decrease in the performance of relation extraction.  X 
The contribution of three pruning rules to the CONTACT type is far greater than to the PER-SOC type. Particularly, the recall of CONTACT increases over 11 units, structure of CONTACT instances is more complex. Hence, the removal of noise information helps more. In summary, the appropriate pruning rules can significantly reduce the noisy information method can significantly improve the performance of personal relation extraction. Effect of CiLin Semantic Information personal relation extraction. The baseline system is the best one in Table 1 (SPT-OPT is a each system. Among them, CL_B, CL_M, CL_S, CL_WG and CL_AWG stand for  X  X ig appropriate semantic information can significantly improve the performance of personal relation extraction.  X 
With the granularity of CiLin semantic information increasing gradually, the F1 overall performance rises progressively. When adding the  X  X tomic word group X  semantic code information, we get the best performance of F1 score with 3 units compares to the baseline system. The results indicate that the more detailed semantic information, the better for helping the personal relation extraction.  X 
For both PER-SOC and CONTACT types, the improvements of F1 scores come unchanged, indicating that adding semantic information is helpful to recognize more positive instances. Effect of Re-sampling Techniques sampling experiments, we add all the negative samples to the training set, and randomly Table 2. For comparison, the testing set of every system is the same as the baseline. Because the training data is selected randomly, the performances may differ. Therefore, each system of experiments is repeated five times, taking their average as sampling can improve the performance.  X  instances: 1:12). Under-sampling and over-sampling method balance the ratio of the SVM classifier, leading to significant recall improvements. Although the precision declines a lot, overall, th e F1 score increases significantly.  X 
Over-sampling performance scores are generally better than those of under-sampling, and they almost unchanged in a wide range. The main reason is that 
Under-sampling greatly reduces the number of negative instances in the training set, thus the structured information of negative instance is not fully utilized. On the contrary, Over-sampling just enhances the weight of positive instances; and keeps all the negative instances.  X 
For both under-sampling and over-sampling, the improvement of the CONTACT type is significantly higher than the PER-SOC type. The number of CONTACT in-stances is much less than that of PER-SOC ones, i.e., the ratio of positive and nega-sampling technique has a greater impact on CONTACT type than on PER-SOC type. 
In summary, the re-sampling techniques, especially the over-sampling, can precision. Therefore, the first two language-based methods are better than re-sampling techniques. relation corpus based on use the ACE 2005 corpus. In order to solve the problem of complex syntax trees of personal relation instances, we propose three pruning rules to instances. Finally, for alleviating the data imbalance problem, we employ re-sampling performance of personal relation extraction. 
Although the proposed methods can effectively improve the performance of personal relation extraction, the overall F1 score is only 55%, still far away from the relation corpus, generating more accurate and co ncise structured information, so as to further improve the performance. Acknowledgement. This work is funded by China Jiangsu NSF Grants BK2010 219 and 11KJA520003. across such real-time streams is an urgent and practical problem. 
The research of Topic detection and tracking aims to automatically organize and locate relevant stories from a continuous feed of news stories. There are several sub-tasks defined for the TDT evaluation. Among them, topic tracking task aims to asso-ciate incoming stories with topics that are known in advance. A topic is "known" by its association with stories that describe it. Thus each known topic is defined by one or more on-topic sample training stories (i.e., sample stories) [1]. 
The typical process of tracking system is: 1) building models of known topics and unchanged during tracking process. It is well known that the contents of topic will be enriched and the topic focuses transfer gradually with newly incoming data, which is the additional on-topic stories during tracking process, which can improve topic track-ing performance. 
This paper proposes an adaptive topic tracking method based on DPMM. The re-mainder of the paper is organized as follows: Section 2 discusses related work firstly. Section 3 proposes TTT and ATT based on DPMM. Section 4 presents experiments and result analysis, finally conclusions are given in Section 5. Researchers put forward many correlation estimation methods according to differ-vector space model use Hellinger distance [2], cosine similarity [3] to measure correlations. Topic tracking methods based on language model express correlations of story and topic as a probability model. Taking unigram model for example, the correlation of story S and topic i Z can be calculated as: w , ) | ( i j Z w P is probability of j w under unigram model to conduct topic tracking and obtained good performance. topic, so serious data sparse problem exists in tracking task. To alleviate this problem, representation method based on language model uses data smoothing technique to reestimate parameters, which needs a large qu antity of background corpus as training some parameters [7], such as correlation threshold, which also need training process. If the scale of training data is not large enough, or training data don't have the same and lead to poor tracking performance. 
Latent Dirichlet Allocation [8] (LDA) and DPMM are most commonly used topic vantage of DPMM is the number of mixture components is determined by the model the correlation of stories with known topic. model built by TTT should be poor and not accurate enough [9]. To solve above prob-lems, ATT methods update the topic model based on tracking feedback during track-ing process and following topic tracking process starts from the updated topic models, methods are: add new correlated features to topic model, or continually adjust feature However, the tracking feedbacks add plenty of off-topic information into known topic problem becomes more and more serious in the tracking process. In general, this kind of ATT cannot improve the tracking performance to a great degree. This paper keep unchanged. The correlation between story and known topic is estimated both under initial topic model and tracking feedback with the reliability metric. In our me-tracing computation through a bigger influence metric. Thus, this method can reduce errors brought by off-topic stories and alleviate topic deviation problem. 3.1 Task Description and every topic is described by 1-4 sample stories which compose prior knowledge of the known topics. Topic tracking task aims to associate incoming stories with known topics one by one and detect all on-topic stories from following news stories. can be simplified as: attractive advantage of DPMM [10] is topic information can be determined by the mod-el and the data directly. 3.2 DPMM The proposed method regards news texts as being generated by a sequence of under-lying topics inferred using DPMM. The generation process of a text can be described as: for each word w in the text, firstly choose a component (topic) Z from a distribu-tion  X  . Topic Z is then associated with a distribution over words, is chosen from the graphical model depiction of DPMM, where N refers to the total number of words in text. Assume that  X  and  X  respectively. 
In the paper, we use a Gibbs sampling procedure to infer parameters of the model topic of j w . The relevant conditional distribution is: existing topic or to a new one conditioned on other topic assignments (  X  Z ) is: Where z n ,  X  is the number of words assigned to topic z excluding j w . 3.3 Model Description of ATT Based on DPMM As analyzed in section 2, in most existing ATT algorithms, integrating plenty of off-topic information into known topic model will likely lead to topic deviation. To solve the problem, we propose an ATT system based on DPMM with a "reliability" metric. Reliability is defined as the dependent degree of the tracking feedback. As TTT, our pute correlation between stories and known topics. 
The graphical model depiction of ATT based on DPMM is shown as fig. 2. As-sume that t S is the story at time t . ATT model introduces a new parameter: guidance ance information at time 0 means prior knowledge of known topics. 
The generation process of t S is determined by DPMM as described in section 3.2, but parameters  X  and parts: prior knowledge composed of sample stories and tracking feedback. In ATT model, they guide topic tracking process by different ways respectively. 3.4 Algorithm Flow start of ATT, + i Z col _ is an empty set. 
At time t , the implementation process can be described as: 1. Implement Gibbs sampling, and combine prior knowledge of known topics during sampling, which can be detailed as follows. (a) Random Initialization: (b) Gibbs sampling combining with prior knowledge of known topics: (c) Reach a steady state, end sampling procedure. realizes guiding role of prior knowledge of known topics in topic tracking. (a) Based on sampling results of step 1, estimate parameters in formula (2), get the (b) Combine formula (2), (8) and (9) to compute correlation between t S and tracking. 3. Assign the topic corresponding to the maximum correlation to t S . Finally, add t S to the corresponding stories set. Based on formula (6), sampling procedure allows situation, t S is not associated with whichever known topic. Repeat above steps for incoming news stories. Fig. 3 shows the flow chart. 
The characters of ATT based on DPMM: 1. In ATT model, guidance information consists of two parts: prior knowledge com-posed of sample stories and tracking fee dback. In the implementation process, 
Gibbs sampling of step 1 realizes guiding role of prior knowledge, and formula (10) of step 2 realizes guiding role of tracking feedback. 2. From step 2, DPMM can compute relevant information of topics from the model and the data. Therefore, compared with existing topic tracking methods, ATT based on DPMM can directly estimate the correlations between story and every known topic, not requiring the correlation threshold comparison which needs to be trained via a large scale of in-domain data. known topic models always have bigger influence on correlation calculation than brought by off-topic stories and alleviate topic deviation problem effectively brought by existing adaptive learning mechanisms. 3.5 TTT Based on DPMM According to graphical model depiction of ATT described in section 3.3, this section model depiction of TTT based on DPMM. In this model, guidance information only contains prior knowledge composed of sa mple stories, which keep invariant. TTT based on DPMM is similar to ATT, the difference is TTT based on DPMM don't need between story and every known topic via formula (2), (8) and (9) directly. In TDT bakeoff [12], tracking performances are measured by error cost, Det C , which ing performances. 4.1 Results We use TDT3 Chinese data as experiments test set. The premise of all experiments is fectiveness of TTT and ATT based on DPMM separately. 4.1.1 Experiments of TTT This part is a comparison between performances of TTT based on unigram model (B_TTT) and DPMM (D_TTT), which investigates influence of text feature selection simultaneously. B_TTT applies add-one smoothing to topic tracking task. 
Our experiment designs four features to represent a known topic: feature set com-posed of content words, nouns and verbs, nouns, verbs are denoted by term_c, term_n+v, term_n, term_v respectively. feature selection methods on topic tracking performances. In all experiments, parame-against influencing factors above. 
From fig. 5, we can find out: tracking performance of D_TTT under a fixed feature sets. 2. Among different features, term_n contribute most to the performance, while 
Among the results, when  X  is 3.0, and system choose nouns as feature, system ob-Likewise, fig. 6 shows performance comparison between B_TTT and D_TTT. 
From fig. 6, we can find out: 1. Both B_TTT and D_TTT obtain poorest performance when choosing verbs as fea-ture, which verify importance of feature selection in topic tracking task. reduces to 0.3095 from 0.3989. Therefore, using DPMM to implement topic track-ing can improve the performance of topic tracking. 4.1.2 Experiments of ATT Firstly, we investigate results of ATT system based on DPMM (D_ATT) with differ-system feature. Results are shown as fig.7. 4.1.1 shows that best tracking performance of D_TTT is 0.3095, which is ex-pressed by dot (the first point) in fig.7. Fig.7 shows that: system. When 2 . 0 _ = reli M , system obtains the best tracking performance. Compared with D_TTT, () Det Norm C of D_ATT reduces to 0.1599 from 0.3095. 
Therefore, our ATT method can solve topic excursion problem to some extent. much bigger than those of D_TTT. Based on formula (10), initial known topic models and tracking feedback influence tracking results simultaneously, the influ-known topic models are built via prior knowledge. Inversely, tracking feedback may contain off-topic stories. Thus, when 5 . 0 _ &gt; reli M , tracking feedback have bigger influence on tracking computation than initial known topic models, which lead to bigger error of final tracking results. pressed by B_ATT. B_ATT system still uses DPMM for topic tracking. 
Table 1 shows D_ATT has the much better performance than B_ATT. Compared with B_ATT, () Det Norm C of D_ATT reduces to 0.1599 from 0.2260. The results verify effectiveness of adaptive algorithm proposed in our paper, and our adaptive algorithm can alleviate topic deviation problem effectiv ely brought by existing adaptive learning mechanisms. 4.2 Result Analysis Via experimental results, it can be concluded that: 1. DPMM is suitable for topic tracking task, and improves the tracking performance sig-nificantly compared with commonly used language model. 2. Results verify the importance of topic representation, and optimization of text fea-ture selection algorithm can improve the tracking performance effectively. 3. Results show the influence of parameter of DPMM,  X  , on tracking computation can be neglectable. Based on this conclusion, topic tracking models (TTT and ATT) based on DPMM proposed in this paper don't contain any unknown system para-meters, thus avoiding optimizing model parameters using additional data . The em-pirical results show that just with a few on-topic sample stories, TTT and ATT based on DPMM can achieve high topic tracking performance. 4. Results shown in section 4.1.2 verify the two characters of ATT based on DPMM: 1) 
D_ATT has the much better performance than D_TTT, which prove that our ATT much better performance than B_ATT, which verify our adaptive algorithm can al-mechanisms. Dirichlet Process Mixture Model (DPMM) considering relevant information of known sampling to estimate correlation between a story and each known topic. Prior know-ledge of known topics is exploited in Gibbs sampling procedure. Experimental results prove DPMM can improve tracking performance significantly. Results also verify methods based on DPMM can determine topic information via the model and the data times, and implement topic tracking task with a few on-topic sample stories effectively. excursion and topic deviation problems simultaneously. The basic idea of our adaptive learning mechanism is to endow tracking feedback with a reliability metric. Our me-thod makes initial topic model and tracking feedback influence computation of corre-always influence tracing computation through a bigger influence metric via influence alleviate topic deviation problem. Results verify that our adaptive method can not only solve topic excursion problem to some extent, but also alleviate topic deviation problem effectively brought by existing adaptive learning mechanisms. 
However, one major criticism of original DPMM is "Bag-of-Words" assumption by portant in text modeling applications. Thus, we will investigate how the dependencies work. Nowadays, online community question answering (cQA) portals have become a popu-The asker firstly posts a question in the cQA portals, and then other users can answer this question. When there are some answers to the question, the asker can select one ponding answers of cQA portals have become an online knowledge base. Two of the main Chinese online cQA portals are Soso Wenwen (http://wenwen.soso.com) and Baidu Zhidao (http://zhidao.baidu.com). By May 2012, there have been more than 200 million solved questions on Soso Wenwen. Yang Tang X  X  statistics on Baidu Zhidao[1], it takes about 14 hours for the asker to get the first answer in average. Secondly, the answers are provided by users on Internet. Due to the limitations of single user X  X  knowledge, the quality of many answers is not high, some answers are even wrong. According to the analysis of English cQA portals by Liu et al. [2], about 25%  X  X est Answers X  are not the best among all answers and at least 52%  X  X est Answers X  are not the unique best answers. 
Many cQA portals also support search service like search engine to overcome the similar questions and their links to users. But it also has two limitations. Firstly, after redundant or irrelevant to the queries. 
In order to return high-quality answers to users, previous research mainly focused on two aspects: (1) Trying to predict the quality of cQA answers, and then return the formance, user profile information is generally needed. (2) Using multi-document summarization (MDS) techniques to summar ize answers from different similar ques-tions, and then return the summarized answer to users [5]. Answer generated by MDS techniques is always more comprehensive, but also is less readable. 
To improve the answer quality, almost all well-perform systems introduce a ques-tion taxonomy [6-9]. The question taxonomy proposed by Fan Bu et al. [10] contains of Fan Bu et al. [10] of the questions of Baidu Zhidao, there are 23.8% questions are List-type questions and 19.7% questions are Solution-type questions. These two types of questions almost consist half of all questions. pose appropriate answer generating methods for both List-type and Solution-type questions to generate high quality answers for users. The research framework of this Generating Module X . 
The remainder of this paper is organized as follows: Section 2 introduces a cluster-section we conclude this work. 2.1 Answer Generating Method According to the definition of List-type questions, each answer will be a single phrase containing the word horse? X  Many answers are a single phrase or a list of phrases. For example, the answer  X  X ensual pleasures: Describe the life is very extravagant X  in the  X  X ther Answers X  is a single phrase which can answer the question. In this paper, such single phrase which can answer the List-type question is denoted as answer point. 
By analysis of the answers of List-type questions, we find there are two characte-means  X  X ther Answers X  contain some additional answer points which are not in  X  X est Answer X . But it takes long time for users to find the additional answer points in  X  X th-er Answers X  because  X  X ther Answers X  ofte n contain lot of information, among which example, in the  X  X ther Answers X ,  X  X ensual pleasures X  is redundant because we have  X  X ensual pleasures : Describe the life is very extravagant X  already. (2) Answer points answers. 
Base on the above two characteristics, we propose an answer generating method for List-type questions which is based on the clustering of answer points. Firstly, each answer is split into one or several answer points. Secondly, similar answer points are Table 2. 2.2 Method Result and Analysis answer. The method output is shown in Table 3. The number at the end of each output two same answer points  X  X ensual pleasures: Describe the life is very extravagant X  and another answer point  X  X ensual pleasures X  with the same idiom. From the above example, we find that comp ared to  X  X est Answer X , the answer gener-probability they have high quality and credibility. Because the outputs are ranked, we proposed can improve the answer quality for List-type questions. 
On the other hand, we also want to point out two points for the above answer gene-rating method, for which further research is needed. (1) For the step 1 shown in Table 2, each answer should be split into answer points at first. For this paper, the answer is split by line or sentence, which works for most situations. But in some situations, the answer is not well formatted, which makes it hard to split answer into answer points. (2) For the step 3 shown in Table 2, the threshold is set to be a constant value by au-work is to find appropriate threshold for different List-type questions. 3.1 Visible List Solution-type questions concentrate on solution, and the sentences in answers always 
AA A example question from Baidu Zhidao, its answer contains several visible lists. 
We choose 1179 solved Solution-type qu estions from Baidu Zhidao, and acquire all questions, there are 358(30%) questions X  answers having visible lists, and the av-long. In comparison, the average length of visible lists is around 600 words, which is  X  X est Answer X , which contains more than 6500 words, List 3, List 4, List 5 can give a more brief answer to the question. the example, we can find some visible lists are good answers to the question, such as List 1 and List 2. The remainder of this section is organized as follows. We will pro-pose a method to select the best list from all the visible lists in the second subsection. In the third subsection, we will evaluate this method and the quality of its result. 3.2 Select the Best List list in their similar questions X  answers. In this subsection, we will concentrate on how to select the best list, namely, the most relevant list to the question. Firstly, we choose five features for every visible list as follows:  X  First list is 0. Denote this feature as FirstList .  X  The similarity between guide words and question The visible list often contains guide words . In the example of Table 4, the first several words of List 1,  X  X iagnostic criteria X , are the guide words of List 1. The guide words usually summarize the list, so the similarity between guide words and question can be calculate the cosine distance between the guide words and the words in question title as a feature, denoted as GuideSimilarity . For visible lists without guide words , Guide-Similarity is set to be a default value.  X  Similarity between list content and question also used as a feature, denoted as ContentSimilarity .  X  The ratio of verbs and prepositions in list The answer to a Solution-type question often gives a solution, so high-quality visible prepositions in the content of the list is used as another feature, denoted as VpRatio.  X  Documents summarization based feature In the first section, we mentioned that multi-document summarization (MDS) tech-answers will be disrupted. But the summarized answer often has high information detail, suppose the summarized answer contains N sentences, for every visible list, if k/N. This coverage score is used as a feature, which is denoted as SummaryScore . had more than one visible lists, we manually label a score to all visible lists (the labe-ling standard will be introduced in the next subsection), and use them as the training set. For data training, we use Learning to Rank model to get the weight of every fea-we use the pairwise Ranking Perception model. 3.3 Experiment and Analysis visible lists from their similar questions X  answers. There are 358 (30%) questions with visible lists in their similar questions X  answers, and 196 (55%) of them with more than one visible list, and other 162 (45%) questions with only one visible list. This subsec-high quality and score 0 for low quality. High quality means the visible list is relevant to the question, is complete and can answer the question, while low quality means the question. After labeling all visible lists, there are 69 questions whose visible lists have the same score. As the goal of our experiment is to evaluate the method of selecting best list, these 69 questions have no meaning for this experiment, so we remove them and use remaining 127 questions for the experiment. 
For the remaining 127 questions, if we randomly select a visible list for each ques-tion, the probability that the list is high-quality is 51.7%. If we always select the first list, the probability will increase to 63.8%. Use the method mentioned in the previous subsection, we combine different features to select best list. The result is as follows: 
Table 5 shows, if we use all features, the probability to select a high-quality list is up to 76.4%, much higher than the method of random selection (51.7%) and selecting first list as the best list (63.8%). The most obvious decrease of high quality probabili-deSimilarity is a very important feature to select high-quality list. the answer; for other 192 questions with more than one visible list, the selected best answer, we manually compare the quality of  X  X est Answer X  and visible list for each containing redundant information. There are th ree cases, i.e., visible list is better than  X  X est Answer X ,  X  X est Answer X  is better, or they are around the same. The evaluation result is as shown in Table 6:  X  X est Answer X  on the whole. On the other side, for the questions above, the average length of visible list is 600 words, while the average length is more than 1400 words type question, if the similar questions X  answers contain visible lists, using the method significantly. meaningful. In this paper, relying on the similar questions and their answers from the and Solution-type questions, which two types consists of almost half of all questions. results show that the answer generating methods we propose can improve the answer quality significantly. 
For the answer generating method for List-type questions, we plan to do further re-search to split the answer into answer points more robustly. For the answer generating questions, we will also do further research to generate high-quality answers. Acknowledgement. This work was carried out with the aid of a grant from the Inter-national Development Research Center , Ottawa, Canada, number:104519-006. This work was also supported by the Chinese Natural Science Foundation under grant No.60973104. In the daily life, people often want to know the definition of a specific topic. As shown in Voorhees [1], the definitional questions occur relatively frequently in logs of web search engines. Therefore, it is an important task to generate a definition, which consists of the most important and interesting aspects for the specific topic.

The task of definitional question answering, which aims to generate definitions for given topics, was included in the past QA tracks of TREC [2], and the eval-uation results showed that the task is much more difficult than the factoid and list question answering [3]. Wikipedia, the largest online encyclopedia, contains a large number of topics. Each topic corresponds to a Wikipedia article describing it. For a given query, which represents exactly one topic, many search engines (e.g., Google, Yahoo!, Bing) often rank the corresponding articles in Wikipedia at top positions.

The first sentence or paragraph of a Wikipedia article may provide a brief and concise description of the corresponding concept. However, only one sentence or paragraph may be unable to cover enough important and interesting nuggets in which the users are interested, while the full article in Wikipedia may be too long to read. Thus, it is necessary to summarize the Wikipedia articles to generate definitions.

Some researchers have done a variety of work on using Wikipedia to solve the definition problem. Summarizing definition from a single Wikipedia article can achieve a quite good result [4]. Besides the single Wikipedia article, we find that the related articles can help gener ating a better definition for the topic, due to the following reasons: (1) If the information about a specific topic is mentioned frequently in its related Wikipedia articles, generally the information is more important or interesting than other information to the topic. Hence, the definition should include this information. (2) The motivation of using related articles is similar with the explanation as Wan [5]: From human X  X  perception, users would better understand a topic if they read more related articles. Hence, by adopting the enlarged knowledge within the related articles, the quality of definitions will be improved.

In this paper, we propose to summarize definition from multiple Wikipedia articles. We first introduce a model to use Wikipedia concept to represent seman-tic elements in the articles. By observing the advantage of other related articles, we further propose to use the related articles to summarize definition. Based on the related articles, we can compute bette r weights for semantic elements corre-sponding to the topic. The main contributions of the paper lie in two aspects: (1) We introduce the Wikipedia concept model, by which the semantic elements in the article can be represented more precisely. (2) We utilize related Wikipedia articles, rather than a single article, to generate definitions. We also measure the impact of the related article sets to differ ent topics. Extensive experiments show that our method performs well for definitional questions.

The paper is organized as follows. Section 2 discusses some related work. The method of definition generation is described in Section 3. Experiments in Section 4 show the novelty and advantages of our work. Conclusions and future work are outlined in Section 5. Definitional Question Answering was firstly introduced into the TREC-QA Track in 2003. The definitional question answering is usually recognized as a difficult task. Some researchers attempt to retri eve definitional sentences using some hand-crafted patterns [6,7,8]. Knowledge intensive approaches can retrieve sen-tences of high quality; however, it requires experts to define all possible lexical or syntactic patterns. The method is not scalable as it is time and labor intensive. To overcome the deficiency, Cui et al. [ 9] propose to adopt soft pattern match-ing. Their method outperforms significantly those approaches with manually constructed patterns on the data set of TREC-QA 2004.

The pattern matching based methods are topic dependent, so even the soft pattern methods require that the topics in the training set are not biased. An-other way to answer the definition question is to explore words or terms related to topics, then to use the words or terms to select the definitional sentences. In TREC 2006, Kaisser et al. [10] collected signature words from snippets returned by search engine, and then selected the s entences containing most weight of the terms. In this way, their system outperformed all the other systems significantly. Since the centroid words or terms are quite important to such methods, Kor and Chua [11] proposed to build such centroid terms from Wikipedia, NewsLibrary, Google snippets and other resources like Wordnet.

Since each Wikipedia article is essentially an overview of a concept, Wikipedia is a good source of answering definitional questions. It is straightforward to ex-tract definition for a topic from Wikipedia. The corresponding Wikipedia article is supposed to be highly focused on the topic, so each sentence in the article describes the topic no matter whether it contains the topic terms or not. Based on this assumption, Ye et al. [4] use EDCL to summarize the Wikipedia article as definition to the topic. EDCL is an extended document concept lattice model (DCL [12]) to combine Wikipedia concepts and non-textual features such as the outline and infobox.

However definitional question answering aims to find important and interest-ing nuggets for a topic, as mentioned by Kor and Chua [11], the important and interesting nuggets often come in the form of trivia, novel or rare facts about the topic that tend to strongly co-occur wit h direct mention of the topic keywords. Making use of related articles in Wikipedia will be helpful to generate the defi-nition for the topic. On the other hand, it is challenging to summarize a single Wikipedia article. As analyzed in Ye et al. [4], the written style of Wikipedia articles is quite different from the free text used in traditional summarization tasks. In general, the guideline for composing Wikipedia articles is to avoid re-dundancy. Hence, there are low redundancies between the sentences within a Wikipedia article, compared to other types of documents. This is not compliant with the assumption that in traditional extractive summarization that the con-tents which are repeatedly emphasized sh ould be included [13]. To overcome the problem, Ye et al. [4] try to group similar Wikipedia concepts and seek impor-tant contents by utilizing non-textual features such as outline and infobox. In addition, our proposal that incorporates other related articles is easier to iden-tify key concepts with repeated mentions ; hence, it will be more appropriate to use multiple document summarization methods.

The idea of generating a definition from related Wikipedia articles, as in-troduced above, to some extent is simila r to single document summarization by expanding the single document to a small number of related documents [5,14,15]. Different from free-text documents, Wikipe dia articles are organized structurally. Therefore, the categories of a Wikipedi a article and the links between Wikipedia articles can be useful in finding related articles.

With related articles in hand, the definitional question answering can be ap-proached as a multiple document summarization problem. Document summa-rization is a hot topic these years, and researchers have proposed many methods, such as Maximal Marginal Relevance ( MMR [16]), document cluster centroid based [17] and some other graph-based methods (like LexRank [18] and Tex-tRank [19]. Gillick and Favre [20] present an Integer Linear Program for exact inference under a maximum coverage model for summarization. In the summa-rization framework, it is important to investigate a good way to represent the content of text. Gabrilovich and Markovitch [21] explore Wikipedia articles to be semantic representation for text, whic h inspired researchers to explore ency-clopedia knowledge to help summarizati on work. In this paper, we also propose a representation model making use of W ikipedia articles. Then, we use both the Maximal Marginal Relevance and Maximum Coverage methods to generate summary as definition, and analyze the effects of multiple related articles under different summarization algorithms. In the paper, we propose to summarize a definition from multiple related Wikipedia articles to a given topic. We firstly identify the corresponding Wikipedia article for a given topic. Then, we exp and the Wikipedia article to a related Wikipedia article set, and we finally use multi-document summarization tech-niques to extract the definition for the topic. In this section, we will present the problem formulation, and then we will describe the article expansion methods and the multi-document summarizing methods in detail. 3.1 Representation Model We first give some symbols of the representation model. Denote that d represents a Wikipedia article. For a given topic t , we aim to give a brief definition def t to t . As mentioned, there is a Wikipedia article d t , which focuses on describing the topic t . And with some article expansion methods, we get a Wikipedia article set D t which contains the Wikipedia articles related to d t . Here, Dt at least contains d t . Each article d i consists of n i sentences, denoted as s i j ( j =1to n ). Each sentence is represented as a co ncept vector. Assuming that there are totally K concepts, denoted as c k ( k =1to K ). For each s i j and c k ,wecalculate aweight w i j ( k ) according to the importance of c k in s i j . Similarly, we calculate W t ( k ) representing the importance of c k to specific topic t .

Next, we will give two different methods to construct the concept vector space and calculate the weights for each concep t in a topic: (1) A simple way to repre-sent concepts by words; and (2) A more pr ecise method to represent the concept by Wikipedia concepts.
 Words Model. It is an intuitive way to represent concept by each word. We can calculate w i j ( k )asTF-IDFweight: frequency of c k , which is calculated on the whole Wikipedia corpus.
We calculate W t ( k ) by summing up the weight of all the sentences in all related articles: Wikipedia Concept Model. The word model could not precisely represent the exact concept in a text. Consider a person X  X  name:  X  Jordan Hill  X  X iththe other two different names:  X  Jordan Farmer  X  X nd X  Grant Hill  X . By word model, Jordan Hill overlaps with the other two names in the concept vector space. However, they should be distinguished as three different concepts. Inspired by Gabrilovich and Markovitch X  X  work [21], we investigate to using Wikipedia arti-cles to represent a text instead of word m odel. We define the Wikipedia concepts as the concepts that can be mapped to Wik ipedia articles, a nd we construct the concept vector space by using Wikipedi a concepts, in which way we expect that it is able to better represent the content of sentence.

There are many inner links in Wikipedia. Each link consists of an anchor text and a target Wikipedia concept that i t links to. We collect the anchor text and target Wikipedia concepts of all the inner links. All the anchor text form a phrase set A , and according to the inner links each phrase a in the set is mapped to a set of Wikipedia concepts, which is denoted as C ( a )= { c k } .And by counting link pairs of a and c k , which is denoted as l k ( a ), we could assign a prior probability p a ( k )toeach c k in C ( a )as:
For each sentence s i j in d i , with the dictionary consisting of phrases in A , we detect a set of anchor phrases A i j = { a z } by applying forward maximum matching in s i j . And then we assign a Wikipedia concept to each a z according to both the context of the Wikipedia article and the prior distribution of Wikipedia concept. We operate as follows: 1. Collect the inner links in d i , and count the number of links with different tar-2. For each a z in A i j , the probability of that c k is assigned to a z is calculated 3. For each a z , we assign the Wikipedia concept c k with highest p ( c k | a z ,d i ).
After assigning Wikipedia concepts to all anchor phrases in A i j ,wecalculate the concept frequency cf i j ( k )of c k by counting the occurrence number of c k in s .

On the other hand, for each c k , we also count the number of links whose target Wikipedia concept is c k . Denote the number as l k , it can be obtained by a l k ( a ). And the original importance of c k , denoted as coi ( k ), is calculated as:
The weight of c k in s i j can be calculated by multiplying cf i j ( k )and coi ( k ): We can get W t ( k )bythesamewayasthewordsmodel.
 Additionally, we add each attributes of infobox of d i as pseudo concepts. We rank the sentences in d i according to the similarity of the sentences and the attribute, and add the pseudo concept to the top 2 sentences. The original importance of the concept is manually assigned, and then we treat them as common Wikipedia concepts. 3.2 Wikipedia Article Expansion The motivation to summarize definition from multiple related Wikipedia articles is as follows: (1) The principle of typical extractive summarization approaches is that the contents which are repeatedly emphasized should be included. (2) As Wikipedia articles are human-written overview pages in which redundancy has been avoided, it is difficult to weight the importance of different concepts just using a single Wikipedia article. Therefore, it is more appropriate to summarize definition with multiple related articles. (3) A concept may be important and interesting when the concept mentioned in highly related Wikipedia articles.
In the paper, to make better use of Wikipedia X  X  structural information, we retrieve the related Wikipedia articles to a specified Wikipedia article d t using its inner links. However, even d t contains an inner link links to another Wikipedia article d t , it does not always imply that d t is related with d t . In fact many phrases in a Wikipedia article link to other articles just because there are entries for the corresponding Wikipedia concepts. To verify the relatedness, d t is added into D t if and only if d t and d t link with each other. 3.3 Multi Wikipedia Articles Summarization Related articles also bring in more noises when generating definition by the extractive summarization approaches. To avoid the noises, we make a constraint that when generating definition for topic t , we only extract the sentences from d . As the purpose of the Wikipedia editors, the corresponding Wikipedia article d should always focus on the topic t . With the limitation, we miss some nuggets that exist in other related Wikipedia ar ticles, but we ensure the relatedness between the extracted sentences and the topic t .
 We utilize two multiple documents summarization algorithms: Maximal Marginal Relevance (MMR) and Maximum Coverage (MC). As we model the articles and make a constraint, we will de scribe the summarizing methods next. Maximal Marginal Relevance. A good summary should meet the following two conditions: (1) The summary focus on the topic of the article set; (2) The summary need to avoid redundancy. The MMR algorithm considers both factors, and repeatedly select the sentence that can be more representative for the article set and has less redundancy with the sentences already selected.

As we assume that the content of the corresponding article d t should focus on t , so the representativeness of a sentence s t j for article set D t , denoted as RP ( s t j | D t ), is calculated as:
And the redundancy between two sentences s t j and s t j , denoted as RD ( s t j | s t j ), is calculated as:
So under the condition of existing summary sentence set S t , the maximal marginal relevance sentence s mmr is calculated as: Maximum Coverage. Gillick and Favre proposed a summarization method based on maximum coverage. The method selects sentences with a globally op-timal solution that also address redundancy globally. They choose to represent information at a finer granularity than sentences, with concepts, and assume that the value of a summary is the sum of the values of the unique concepts it contains. They also present the Integer Linear Program for exact inference under the model. We formulate our ILP problem as follows: Here, SumC k represents whether c k occurs in the summary or not, o t j means whether s t j is selected in the summary or not, and Occ t j ( k ) equals 1 if and only if cf j ( k ) (1) can be equally transformed to: (2) removes the second cons traint in (1), and the complexity of (2) is reduced. 4.1 Experiment Setting We evaluate our method on the corpus of TREC-QA in 2004-2006 (TREC 13-15). For each topic, we retrieve the corresponding Wikipedia article. Because the focus of the paper is on summarization evaluation, we simply ignore the topics in TREC-QA where the corresponding articles do not exist in Wikipedia.

We evaluate the summarization performance by pourpre [22]. Like prior stud-ies [9,4], we also treat the answers of factoid/list questions as essential nuggets, and add them to the gold standard list of definition nuggets. To avoid the influ-ence of those nuggets that do not exist in Wikipedia corpus, we only consider the nuggets that could be found in Wikipedia. So we first explore the available answers in Wikipedia Corpus, and the result is shown in Table 1. Among 215 topics in TREC 13-15, we could obtain 190 Wikipedia articles corresponding to the exact topics. As our corpus was downloaded in 2009, the available topic number and available nuggets number in the single article are both larger than those in 2007, which indicates that the Wikipedia not only covers more and more new topics, but also covers more old topics. So the idea of using Wikipedia as a resource to answer definitional questi ons is feasible. As mentioned in Section 3 . 3, we limit our algorithm to select sentences only from d t .Weobservethat this constraint caused a lost of 25% essential nuggets could be found in related Wikipedia article set. It seems to be a huge loss in recall, but we can benefit the precision. We evaluate the precision of the two special ways of summariza-tion, using the entire corresponding article as summary ( S alls ) and using all the related articles in D as summary ( S allm ). The precision of S alls is 0 . 169 while the precision of S allm is 0 . 029. Without the constraint on sentence selection, the number of the nuggets that can be retriev ed will increase, but it introduces too many noises. With comprehen sive consideration about recall and precision, the sentence selection constraint is reasonable.

We examine the quality of definition summary by nugget recall (NR, only consider the nuggets can be found in d t ) and an approximation to nugget pre-cision (NP) on answer length. NR and NP are then combined using F1 and F3 measures. The evaluation is automatically conducted by Pourpre v1.1. 4.2 Performance Evaluation To measure the performance of different models, we evaluate the quality of def-inition produced by MMR and MC algorithms combined with different repre-sentation models and article sets: Wo rd model with a single article (Word), Wikipedia concept model with a single article (Concept), and Wikipedia con-cept model with related article set (C + M). The maximum number of sentences in a summary was set to 10. As the result shown in Table 2.

In both the summarization algorithms, the Wikipedia concept model outper-forms the word model and the related article set helps improving the perfor-mance. We get following observations from the results: 1. Both the two algorithms benefit from the Wikipedia concept model. On all 2. The related article set can help imp roving the performance in both the two 3. The Wikipedia concept model contri butes more to precision than to recall. 4. The related article set leads to more improvement in terms of nugget recall In the paper, we present a framework of summarizing definition from multi-ple Wikipedia articles. Experiments wi th different summarization algorithms demonstrate that the explicit semantic representation via Wikipedia concepts benefits the extraction of definition. The experiment results also show that the related articles can weight concepts more effectively than a single article, particu-larly for those general and popular topics. The framework proposed in the paper achieves excellent results on TREC-QA data, which demonstrates the feasibility of our methods.

However, using the extractive summary as definition still faces some problems, such as the discourse consistency between the extracted textual segments. In the future, we are considering using some generative summarization techniques (such as compression, reordering) to improve the consistency quality.
 Acknowledgement. This work was carried out with the aid of a grant from the International Development Research Center, Ottawa, Canada, number:104519-006. This work was supported by the Chinese Natural Science Foundation under grant No.60973104.
 A large of new information emerged and formed information explosion with the rapid development of information technology and Internet. Many emerging information mining and machine translation appeared in this background. Named entity is the main carrier of information and it expresses the main content of the text. It is the very import part of these researches. The research on named entity recognition has strategic signi-ficance to language understanding and information processing. 
At present, there are a lot of researches on named entity. The methods can be divided time-consuming and labor-intensive and it lacks field adaptive capacity. The second is statistics-based method. Although statistics-based method has a good ability of model learning without human intervention, it is limited by the limited scale of the training corpus. As a result the last work emerged which combine rule-based method and sta-tistics-based method. It aimed to reduce the complexity and blindness of the rule-based method. In recent years, a large number of new words are emerging and most of them are named entity including person names, location names and organization names. Traditional rule-based or statistics-based method can X  X  satisfy the named entity recog-sources in order to improve the performance of the tasks. using open source tools named JWPL. Second, we extract the definition term from the recognition. Finally, we achieve named entity disambiguation using Wikipedia dis-ambiguation pages and contextual information. The experiments show that the use of Wikipedia features can improve the accuracy of named entity recognition. Wikipedia is a multilingual, web-based, free-content encyclopedia. Since its creation tracting the majority of Internet users to contribute their knowledge to achieve a huge amount of data sharing .There were 441,405 articles in Chinese and 3,917,431 articles in English. Next we will describe basic page, disambiguation page, redirection page, structured categories, hyperlinks and information box of wikipedia in detail. 2.1 Basic Page A basic page is also known as an entry which describes a real world entity or concept corresponding to a subject. Basic page has a simple title which usually corresponds to concept. The following paragraphs expand the detailed description of the entity from all angles on the topic. 2.2 Redirection Page describing entities of the real world are synonym s. Redirection page in the wikipedia is to solve the synonym problem. Only one of the most representative words in synonyms in Wikipedia is the title of the basic page and the others are titles of redirection pages. When the word matches to the redirection page it will be automatically re-link to the word named CAS in the search box. CAS is a redirection page ,so it is directly redi-rected to the basic page named Chinese Academy of Sciences. 2.3 Disambiguation Page Disambiguation page is used to deal with the ambiguous name. The so-called ambi-guity means the same name may refer to different entities. For example, Washington Disambiguation page in wikipedia lists the entries which may refer to different entities sentence. For example, we input Washington in the search box. Its disambiguation page lists wikipedia entry named George Washington(first President of the United States of America). Click it and you can enter the corresponding basic page. It also list the entry named Washington State(a state on the Pacific coast of the United States of America) and many other entries related with Washington. 2.4 Categories Wikipedia provide a grid-like classification system which is edited by the public. An formation. For example, entity Li Ning belongs to the category  X 1963 births X ,  X  X iving addition, each category has its own parent categories and subcategories. For example, category X  Chinese business people X  has four parent categories such as  X  X usinesspeople  X  X hinese real estate businesspeople X  and  X  Hong Kong business people X . In this way, Wikipedia's classification system constitutes a hierarchical structure which is not a tree in strict sense, but a directed acyclic graph. 3.1 Extract Wikipedia features We do a summary introduction of wikipedia in the second quarter. The first few para-examples of the first sentence of the Wikipedia: 
It can be seen from the example, the core term in the noun phrase after the defining verb is a very good knowledge which reflects the attributes of the entry. We extract the core terms to observe,  X  The Chinese Academy of Sciences is a academy  X  ,  X  Li Ning  X  1  X  The Chinese Academy of Sciences ( CAS ), formerly known  X  2  X  Li Ning ( Simplified Chinese :  X  X  X  ; Traditional Chinese :  X  X  ; Pinyin :  X  3  X  Beijing is the capital of the People's Republic of China and one of the "agency", "entrepreneur", "city" help us to judge an entry is a name, a local name or a organization name. Therefore, we want to ex tract the core terms used in the first sen-process of named entity recognition. The specific steps to extract the wikipedia features are as follows:  X  wikipedia  X  paragraph  X  defining verb  X  4  X  The wikipedia features extracted by the above steps are shown in Table 1 Soft-WorldInternational 3.2 Add Wikipedia Features to Named Entity Recognition We usually annotate corpus using IOB2 tags as we represent named entities. InIOB2 beginning of an entity, the inside of an entity, and the outside of entities respectively. knowledge of Wikipedia, the suffix "X" represents the wikipedia features. For exam-ple, given a sentence "Recently, Stefanie Sun X  X  album sold well". For example, if we search for "Stefanie Sun" is a Wikipedia entry and extract its wikipedia feature"singer", the marked results of this sentence with wikipedia feature are shown in Figure 1. 4.1 Experiments of Extracting Wikipedia Feature The test set used in this article are randomly selected from the 372,969 Wikipedia page in wikipedia feature extraction module. It contains a total of 1000 pages. We extract Wikipedia features on the 1000 pages and get the correct rate of 91.5%. Because some science(or information studies) is an interdisciplinary field primarily concerned with the analysis, collection, classification, mani pulation, storage, retrieval and dissemina-tion of information  X  . For example,  X  Information science (or information studies) is an interdisciplinary field primarily concerne d with the analysis, collection, classifica-tion, manipulation, storage, retrieval and dissemination of information.  X  The wikipe-of the situations. To solve this kind of error, we can do syntax analysis specially on long sentences to find the core words of the parsing results. This article does not introduce a complex syntactic analysis by taking into account the good correct rate of this method coupled with the high cost of parsing. 4.2 Experiments in Named Entities Coverage of Wikipedia This article use 863 named entity evaluation corpus in 2004 which contain 367 docu-ments and 19,102 sentences. There are 30,955named entities in this 19,102 sen-tences(named entity in this article specially refers to person name, location name and proper nouns. This article compares wikipediaentry with LDC dictionary. We respec-coverage of wikipedia. Test results are shown in Table 2. 12.18% higher than that of the LDC dictionary. Although the scale of wikipedia entries are far smaller than the LDC dictionary(788, 745 less),the matched entries with wiki-pedia have only 4,374 less than that with LDC dictionary in 863 named entity evalua-tion corpus. So we can see that the named entity coverage of wikipedia is high. 
LDC dictio-4.3 Experiments of Named Entity Recognition Based on Wikipedia In this section, we demonstrate the usefulness of the wikipedia feature for NER. We including 17, 102 sentences and the other is the test set including 2000 sentences. We use CRF as the classifier of named entity recognition and do three tests. We carry out the first test with the common features such as word and POS, the second test with LDC dictionary in additional to the common features and the third test with wikipedia feature in additional to the common features. We annotate the 863 named entity evaluation corpus in a word sequence.  X  X heng Huaren X  in the example have appeared both in the LDC dictionary and wikipedia and its wikipedia feature is  X  X conomist X . The annotated corpus is shown in Figure 2. 
This article uses CRF++ tool, the above defined three feature templates and cor-responding three annotated corpus to test the performance of the named entity recog-nition. The test results are shown in Table 3 and 4 : 
As is seen from experimental results, the Wikipedia features improved the accuracy in F-measure by 1.29 points(in word sequence) and 0.7 points(in character sequence) compared with common features. Compared with LDC dictionary, the Wikipedia tively. The wikipedia feature can play a good role in named entity recognition. 
The simple method that extracting defining feature from the Wikipedia page can effectively improve the correct rate of named entity recognition. The results show that ponding Wikipedia, so we will not bring the wrong noise where it might be existed. (2) If a Wikipedia page is non-ambiguous pages, it will describe the main meaning of most features is helped to improve the correct rate of named entity recognition is the main meaning of the entities are frequently used in the corpus. In our method, there is still Wikipedia pages make continuous rapid growth at the current rate, perhaps all of the Wikipedia entity will become ambiguous entity in the latest future. We need a disam-biguation method to find the most appropriate page from the multiple pages listed in the disambiguation pages. Entity ambiguities refers to an entity alleged corresponding to the problem of real-world entities. For example , the following three entities alleged "Washington"  X 
They separately refer to the three real-world entity  X  "America's first president"  X  recognition, the alleged "Washington" may be a person name (George Washington), or a local name (Washington, DC, or Washington), and we need to determine which the true type of the alleged entity are belonged to , and this is named entity disambiguation. 5.1 The Processes of Named Entity Disambiguation Based on Wikipedia Wikipedia disambiguation page lists the entity alleged ambiguities. And it provides us with a good disambiguation information. If there is "Washington" in a sentence, and "Washington" is an ambiguous entity, it may be a person name or a local name. The "American President" entry which "George Washington" entry at, where the Wikipedia disambiguation page list , can help us determine to decide "George Washington" is a name. Besides another cited "Washington State" entry where it has the "American states" entry, can also let us be certain about the "Washington State" is the local name. Through the Classification and Labeling of Wikipedia entries we can properly mark the category of the named entities they belong to in 80% of Wikipedia coverage rate  X  95% of correct rate. Intuitively, we can determine the "Washington" should be which entity through the current context of a sentence, so we think about a method : we calculate the similarity of all the pages listed in the sentence of the documentation and Wikipedia disambiguation page, then find out the most similar Wikipeia page to the current sen-tence, finally give a more accurate label of the current entity through the entity iden-shown in Figure 3: 5.2 Training Wikipedia Corpus with CRF All of the existing Wikipedia entries in Wikipedia page are marked with the symbols dictionary, we marked the entry with the type of the named entity , otherwise the entry does not exist in the named entity dictionary, the symbol of "[[]]" will be stripped, and ultimately we will convert the Wikipedia corpus with a training data of named entity converting are shown in Figure 4 and 5, in which person name, place name, organiza-tion names marked separately with PER  X  LOC  X  ORG . 
The correct marked rate of Wikipedia marked corpus named entities after converting is about 95% ,and the recall rate is about 80%. CRF are maked use of to go on training and self re-marking on the label corpus to achieve a higher recall rate. 5.3 The Examples of the Disambiguation of Wikipedia A simple example can show us the process of the disambiguation. For example, there is a sentence which is "Bloomberg flew to Washington to promote his own ideas" in our wikipedia through the maximum matching wikipedia entry, and in this disambiguation page lists all the possible ambiguity entry, a total of seven, such as "Washington. DC", each entity type. We calculate the similarity of the document of the given sentence and which are "Washington State", "Washington DC" and "Washington Town" ,and add ("Washington Township") * P (organization name | Washington town) and so on to the CRF training and testing. It is more intuitive to see Figure 6. disambiguation characteristics on the basis of the original wiki features, and the results of the study are shown in Table 5. Wikipedia disambiguation feature,and the effect is significant opposed to the limited ambiguation method: "The ITTF has announced the latest world rankings, and the men's singles aspects of German Boer continued in the first place", there are ambigui-ties of " Boer " in this sentence, "boer" can be place names (refer to South Sudan city) wikipedia feature 84.46 88.81 86.64 named entity disambiguation 84.73 89.42 87.07 or names (refer to table tennis players of Germany).At first "boer" is mislabeled as local name, after combining the Wikipedia disamb iguation features, some contextual fea tures, such as Germany, table tennis, men's singles " are used to help the system cor-recting the category into the right type person name. Firstly we introduces of the defining feature of Wikipedia as an additional knowledge added to the based named entity recognition in the CRF, and the method is simple but huge label corpus helps us to further improve the performance of named entity recog-nition, and we make full use of Wikipedia's disambiguation pages and context information to build a double-layer CRF system for carrying out named entity disam-biguation. The study shows that the method proposed can improve the correct rate of named entity disambiguation. 
In named entity recognition, we only make use of the first sentence of the Wikipedia hyperlinks, and other rich resources to further improve the correct rate of named entity recognition. Query recommendation technology is wide ly used by commercial search engines. A search engine provides related queries i n a search result page. If the query does not present useful search results, users can click on any related query to find more web resources. Our analysis shows that u sers click recommended related queries in approximately 15% of search sessions.

The query recommendation click log contains a large volume of information about users X  search intents and their perspectives on search results. A user X  X  click on a recommended query implies that the recommended query describes what he wants, whereas the search results of previ ous do not satisfy his information needs well enough. Because search engine users usually click recommended queries after reviewing the result page, the clicked recommendation should be a direct and precise reflection of the user X  X  in tent. The query recommendation click logs contain less noise than the logs of query reformulation because all clicked queries are selected by both the search engine and the user. In this sense, the data in query recommendation click log is very reliable.

In this paper, we introduce the concept and properties of a query clicking graph, which is a graph that depicts all of the query recommendation clicking information contained in user logs. This graph aggregates all users X  query rec-ommendation click actions during a specific period. In the graph, each node represents a query. A directed edge ( q i ,q j ) indicates that a user clicked the rec-ommended query, q j , when he searched q i . We do both global and local analysis on the graph. These efforts can help us to learn the properties of queries and user intents. We introduce the following applications for the recommendation click graph:
Optimizing search results. Exploring users X  click actions on query recommen-dations can help us to understand users X  search intents. Thus, the recommenda-tion click graph can help the search engine to improve the search performance.
Recognizing ambiguous queries. If users click disparate recommended queries, the previous query might be highly ambiguous. The ambiguity might be caused by the ambiguity of the query expression or the users X  uncertain search intent. Graph analysis algorithms can help us to find ambiguous queries in a recom-mendation click graph.

The remainder of this paper is organized as follows. Section 2 introduces re-lated work. Basic concepts and assumptions of our work are given in Section 3. In Section 4, we discuss the propert ies of recommendation click graphs. Sec-tion 5 discusses our local analyzing methods for recommendation click graphs to improve search performance. Section 6 shows our approach to identifying ambiguous queries. In Section 7, we summarize our work and discuss query rec-ommendation and the recommendation click graph. The Web is naturally presented as a graph. Often, the nodes of a web graph repre-sent the web pages, and the edges represent links between pages. Web graphs are used to estimate the quality of web pages. Link analysis is a data-analysis tech-nique that uses a link graph to estimate page quality. Examples of well-known link analysis algorithms are the HITS [1], PageRank [2] [3] and TrustRank [4] algorithms. These link analysis algorithms have common assumptions: (1) the links imply recommendation and (2) the page quality can spread through the hyperlinks. With these assumptions, link analysis algorithms are often applied to other types of graphs.

Search queries can also be presented by graphs. Baeza-Yates defined 5 types of query graphs [5]: word, session, URL cover, URL link and URL terms. Three types of information are used to construct query graphs: the query terms, the searching and clicking behaviors during the query session and the content of clicked URLs. Baeza-Yates et al. used s ome of the query graphs to mine query logs for semantic relations between queries [6]. The five types of graphs are useful for recommending related queries. A query-flow graph is a representative query graph [7] and performs well in query recommendation applications [8] [9].
Unlike present methods, the recommendation click graph is based on existing query recommendations and the click log. This graph is used to detect ambiguity in queries and to learn users X  intents.

Query recommendation is an important are of study. The query recommenda-tion algorithms focus on finding similar queries. Zaiane et al. used query terms, search result snippets and other simple text information to find similar queries [10]. Because the semantic information from query and search results is limited and hard to analyze, most recent studi es on query recommendation use user behavior data to detect related queries [11] [12] [13]. Some studies [14] [15] use a bipartite graph of search query and clicked URL to determine and recom-mend related queries. These studies assume that two queries are similar if they lead users to click on the same URLs. Kelly et al. studied the usage of query recommendation [16]. Their research showed that query recommendations are frequently used by users a nd can be very helpful.

For a search engine to understand a user X  X  search intent, it must be able to identify an ambiguous query. Some queries are ambiguous in nature. The search intent of such queries can be det ected by analyzing related documents and user behavior. Song et al. summarized three types of queries: ambiguous, broad and clear. They used a supervised learning approach to classify queries by analyzing the text of the search results [17]. He and Jhala developed a method to understand a user query based on a graph of connected related queries [18]. Veilumuthu and Ramachandran developed a clustering algorithm that uses the user session information in the user log and query URL entries to identify clusters of queries that have the same intention [19]. 3.1 Basic Concepts Recommendation click log. A recommendation click log is a part of a search en-gine X  X  user log. It records all clicks on query recommendati ons. Recommendation click pair. A user X  X  click on a query recommendation is represented by an or-dered query pair q i ,q j . This pair indicates that a user submitted query q i to the search engine and clicked recommended query q j on q i  X  X  search result page. We call q i the source query and q j the destination query. Pairs with the same source and destination queries are r ecorded as a single recommendation click pair in our work.

Recommendation click graph. The recommendation click graph uses the pre-vious two concepts.

Definition 1 : The recommendation click g raph is a directed graph G c =( V,E ), where:  X  the set of nodes, V = { q | q appear in the recommendation click log as a  X  E = { ( q i ,q j ) | &lt;q i ,q j &gt; is a recommendation click pair in the recommenda-Like other web graphs, the recommendation click graph might have more than one weakly connected component. Beca use the recommendation click graph is based on recommendation click pairs, there are no isolated vertices in the graph, i.e., there are at least two queries in a co nnected component. More properties of the recommendation click graph are d escribed in following sections. 3.2 Semantic Basis Some basic assumptions on the semantics of query recommendation click actions are fundamental to the recommendation click graph developed here. When a user clicks a recommended query, this action expresses a relationship between the source and destination queries. Using the recommendation click pair q i ,q j as an example, we assume that there are two possible latent meanings for a user X  X  click on a query recommendation:  X  Assumption 1 q j describes the user X  X  informat ion needs more precisely than  X  Assumption 2 q j does not describe the user X  X  information needs more pre-Clicks described by assumption 1 usually make the user X  X  search intent more precise. For example, the recommendation click pair Lady Gaga songs, Lady Gaga poker face corresponds to a more precise intent description. Clicks de-scribed by assumption 2 appear frequently. The recommendation click pair Lady Gaga songs, top 100 songs is an example of this type of click. In most cases, the source query and destination query are related. However, the query pair can be about different topics.

In our work, we are interested in recommendation click pairs that comprise related queries. The recommendation c lick graph can help researchers to view related queries from different perspectives. We extracted a recommendation click log from an August 2010 user log of a Chi-nese commercial search engine. The log recorded 58,334,303 clicks on query rec-ommendations. From these data, we constructed a recommendation click graph that contained 23,516,620 vertices and 31,569,262 directed edges. We obtained statistical distributions for the in-degree and out-degree of each vertex. The dis-tributions are shown in Fig. 1. Both the in-degree and out-degree distributions approximately follow a power law distribution. This property is very similar to a hyperlink graph. Given this similarity, there are many effective and widely used analyzing algorithms developed for hyperlink graphs that might also be appropriate for recommendation click graphs. 4.1 Connected Components We analyzed the connected components in the recommendation click graph with-out regard to the direction of the edges. Of the 2,668,331 components in the graph, 71% have only 2 vertices, i.e., most of the components are very small. Our statistics show that approximately 60 components have more than 100 ver-tices. The largest component has 16,298,916 vertices, which is approximately 70% of the vertices in the graph. This result indicates that many queries are connected in the recommendation click gra ph. These queries include most of the hot topics of the search engine during the period covered by the query log. As shown in Fig. 2, the distribution of the co nnected component sizes approximately follows a power law distribution. 4.2 Strongly Connected Components A strongly connected component is a di rected sub-graph in which there is a directed path between any ordered pair of vertices. In a recommendation click graph, queries that appear in the same strongly connected component can be strongly related.

In our recommendation click graph, there are 20,978,260 strongly connected components. The distribution of component sizes follows a power law distribu-tion. Of the strongly connected components, there are 20,695,423 components that have only one vertex. These vertices account for approximately 88% of the vertex set. This result indicates that approximately 10% of the queries both recommend other queries and are recommended by other queries in the graph. This property is in accord with user be havior patterns and can be explained as follows. Because the user queries follow a power law distribution, most queries have a low frequency. Low frequency queries have no chance to be recommended. As mentioned in Section 3.2, for many recommendation click pairs, the desti-nation query refined the intent of the source query. Therefore, few queries are both source and destination. In the recommendation click graph, the largest strongly connected component contains 1,816,759 vertices, approximately 7.7% of the vertices in the graph.
 In a recommendation click graph, an edge ( q i ,q j ) denotes that a user has clicked the query recommendation q j while he searched q i . Because the recommendation click pair is selected by both the search engine and the user, we assume that adjacent queries in a recommendation c lick graph are semantically similar.
A user clicks a query recommendation when he finds the search result insuffi-cient. He may try a recommended query that describes his needs more precisely or a query that appears to yield better results. The search result page of the related queries might provide what the user is seeking. The local analysis of a recommendation click graph can help us to improve the search result by including some search results of high-quality adjacent queries. 5.1 Local Subgraph Definition 2 :Forquery q i in a recommendation click graph, we define a local subgraph of q i as where The local subgraph of q i contains the queries that are most related to q i . 5.2 HITS applied to a Local Subgraph HITS is a link analysis algorithm for evaluating web pages. The algorithm is applied to a subgraph of a hyperlink graph. The indicators for each web page are called hubs and authorities. The hub value indicates how efficiently users are led to other useful pages using the hyperlinks on the page. The authority value indicates how many good hub pages link to the page.

We apply the algorithm and concepts of HITS to a recommendation click graph. A query that has a high authority value is linked to several other queries. A query that has a high hub value leads users to other queries. In this work, we are interested in the queries that are frequently searched and do not lead to additional clicks of query recommendations. Queries that have a high authority value and a low hub value might satisfy more users X  needs and be less ambiguous. We find such queries using the local graph of query q i and use their search results to improve the search performance of q i . 5.3 Experiments of Optimizing Search Results We randomly selected 442 queries that had an out-degree 8 and extracted their local subgraphs. The local subgraphs of queries that have an out-degree less than 8 might not be large enough to support a HITS analysis.
We collected statistics on the numbers of nodes and edges in the 442 sub-graphs. Fig. 3 shows that the numbers of n odes and edges are linearly related. Table 1 lists the maximum, minimum and average numbers of nodes and edges. These statistics show that the sizes of the local subgraphs can vary widely.
We used the query X  X  local subgraph to optimize the search results. We ran-domly selected 117 queries (Group 1) that had an out-degree = 8 or 9. These queries were not the most frequent que ries in the search engine, but their lo-cal subgraphs were large-enough to support the HITS algorithm. These queries might be a representation of a general user query. We examined Group 1 from the user X  X  perspective. We identified 44 q ueries on which the search engine did not perform very well, but the user found related resources through a query recommendation. We perfo rmed Algorithm 1 on the 44 queries and obtained optimized search results. Three annotators were asked to compare the optimized search results with the original search results and vote for the list that contained results that are more diverse. For 34 of t he 44 selected queries (approximately 77%) the search results optimized using Algorithm 1 were deemed to be more diverse.
 Algorithm 1.
We repeated the above experiment on 61 randomly selected queries (Group 2) that had higher search frequencies and o ut-degrees. These queries might be more ambiguous than the queries in Group 1. We examined the queries and found 18 queries on which the search engine did not perform well. For 14 queries, the search results produced by our algorithm were more diverse. Because the queries in Group 2 are hotter than Group 1, the queries in Group 2 might perform better in the search engine. Note that the percen tage of queries that do not perform well enough is lower for Group 2 (18/61) than for Group 1 (44/117). Approximately 77% of the queries selected in Groups 1 and 2 were optimized by Algorithm 1. The experimental results are shown in Fig. 4.

Our algorithm is naive and does not always yield better results than those obtained by the search engine, particul arly when the search engine performs a query well. Nevertheless, our local subgraph method can improve the search results of queries. 6.1 Inverse PageRank Inverse PageRank is a link analysis algorithm used to determine the seed pages for the TrustRank algorithm [4].
 For a directed graph G =( V,E ), inverse PageRank requires a related graph G =( V,E ), where We perform PageRank on the link-inverted graph G X  to obtain the inverse PageR-ank for G.
The inverse PageRank can be explain ed using the concepts of PageRank as follows:  X  In G , a vertex X  X  inverse PageRank is higher if it has more out-links.  X  In G , a vertex X  X  inverse PageRank is higher if the vertices to which it points Some studies have shown that search engine users often issue short and am-biguous queries [20]. In this case, user s might continue by clicking query recom-mendations. It is reasonable to assume that the level of ambiguity of a query is related to the dispersion of the query X  X  recommendation clicks. Based on this assumption, the inverse PageRank values of a recommendation click graph show the level of query ambiguity. 6.2 Experiments We applied the inverse PageRank algorithm on the recommendation click graph. We sorted the inverse PageRank values into descending order and divided them into 10 buckets. The sums of the inverse PageRank values in each bucket were equal. The sizes of buckets are shown in Fig. 5.

To verify if the inverse PageRank can b e used to determine the ambiguity of queries, we asked 4 annotators to examine queries sampled from the 10 buckets. We randomly selected 1010 queries and asked the annotators to score each query: 3 indicated a clear query, 2 indicated an intent ambiguous or broad query, and 1 indicated a semantically ambiguous query. Through discussions, the annotators ensured that they had no serious disagr eement on the semantics of any query. For 828 of the 1010 queries, at least three annotators give the same score. The kappa coefficient of their annotations was  X  =0 . 466 [21].

For each query, we calculated the average, maximum and minimum scores. In addition, we calculated the average of the 3 types of statistical scores in each bucket. The results are shown in Fig. 6. The scores in the first five buckets are higher than the scores in the last five buckets. Moreover, the scores increase from Bucket 1 to Bucket 5, whereas the scores of the last five buckets are not significantly different. Although the first 5 buckets contain approximately 30% of the queries, they contain most of the ambiguous queries.

We randomly selected 233 queries from t he graph and sorted the queries into descending inverse PageRank order. We divided the queries into 10 buckets of approximately the same size. The annotators labeled the top 3 search results of the 233 queries in 5 levels, and we calculated the NDCG3 for all of the queries. For each bucket of queries, we computed the average and standard deviation of the NDCG3. Fig. 7 shows the statistical results. From Bucket 1 to 10, the average NDCG3 decreases, whereas the standard deviation increases. Although queries that have higher inverse PageRanks contain more ambiguity, these results indi-cate that they perform better than queries that have lower inverse PageRanks. This result seems counterintuitive, but it is reasonable. As mentioned above, the inverse PageRank algorithm gives higher values to the vertices that have more out-links.

In a recommendation click graph, the out-d egree is related to the search fre-quency of the query. Therefore, hot qu eries are more likely to receive higher inverse PageRanks. The higher NDCG3 values obtained in the previous buckets are explained by the better performan ce of hot queries in commercial search engines. The average lengths of the queries increase from Bucket 1 to Bucket 10 (shown in Fig. 8). The work of Jansen et al. [20] showed that most queries in web search engines are short, and long queries are very infrequent. This work confirms that the queries in the preceding buckets are hotter in the search engine.
From the experimental results and analysis, applying the inverse PageRank algorithm on a recommendation click graph is an effective tool to evaluate query ambiguity. According to the definition and construction of recommendation click graphs, the graph is strongly related to user behaviors and the search engine X  X  query recommendation algorithms. Because the user behaviors on query recommenda-tions is the object of our research, the behaviors and algorithms do not affect our methods of constructing and analyzing the recommendation click graph. However, changes in the recommendation algorithm can affect the properties of the graph.

In the commercial search engine that we used to build the recommendation click graph, there are at most 10 recommended queries for each input query. However, the query recommendation algorithm allows the related queries to be updated, and the out-degree may be greater than 10. In the graph for our experi-ment, the largest out-degree was 391, and there were 254,639 queries that had an out-degree greater than 10. In other search engines, the strategies for updating query recommendations might be different; thus, the distribution of the vertices X  out-degrees might be different. In our w ork, we considered the out-degree as a representation of query ambiguity, a nd the recommendation algorithm can affect our judgment of query ambiguity. However , it is reasonable to believe that fre-quent changes in recommended queries imply that the source query is complex and unclear. Therefore, our methods are applicable in different search engines.
In this paper, we noted that the query recommendation click log contains a large volume of information about user search intent and query ambiguity. We proposed a recommendation click graph constructed from the recommendation click log. Our analysis showed that the properties of the recommendation click graph are similar to traditional web graphs. Furthermore, the edges in both web graphs and query graphs have the meaning of  X  X ike X  or  X  X ecommend X . Therefore, it is reasonable to apply well-used link analysis algorithms to the recommenda-tion click graph. In this way, we can improve search performance by mining the recommendation click graph to learn more about users X  behaviors and intents. We have applied the HITS algorithm on a query X  X  local subgraph to select re-liable recommended queries. These related queries can be used to improve the diversity and performance of the query. We have applied the inverse PageRank algorithm to a recommendation click graph to evaluate query ambiguity. The results showed that the inverse PageRank values reflect the query ambiguity. This result was confirmed by both annotation and query length statistics.
An important direction for future work is to analyze the recommendation click graph to learn users X  search intents. Furthermore, an advanced ranking algorithm can be developed to improve search r esults using query recommendations. data is need to be reorganized as semantic data, which is the foundation of many intel-ligent applications. Linked data is such large scale semantic data. Recently, more and more linked data such as DBpedia [1], Freebase and Google knowledge graph is used in many fields including knowledge engineering, machine translation, social compu-ting and information retrieval. Academic link ed data is important for academic social network analysis and mining. However, current academic linked data is based on database like DBLP and mainly describe paper publication information. Therefore, academic activity knowledge are not included by current academic linked data. Aca-demic conferences websites not only contain paper information, but also contain tion, participants, awards, and so on. Obtaining such information is not only useful for predicting research trends and analyzing academ ic social network, but also is the im-portant supplement to current academic linke d data. Since academic conferences Web pages are usually semi-structured and content are diversity, there is no effective way to automatically find, extract and organize these academic information to linked data. To generate linked data from semi-structured pages, it usually needs three phases: Web information extraction, ontology generation and linked data construction. Web information extraction is a classical problem [1-3], which aims at identifying interest-ed information from unstructured or semi-structured data in Web pages, and translates formally represents knowledge as a set of concepts within a domain, and the relation-share knowledge on the Web [7-9]. More and more linked data is generated in recent years, and it contains the knowledge about geographic information, life science, Wi-foundation of many intelligent applications such as semantic search and social network. classical vision-based segmentation algorithm VIPS; (2) We transform the conference blocks as pre-defined categories according to vision, key words, text and content in-formation; The initial classification results are improved by post-processing. Finally, academic information is extracted from the cl assified text blocks. (3) A global ontolo-gy is used to describe the background domain knowledge, and then the extracted aca-demic information of each website is organi zed as local ontologies. Finally, academic linked data is generated by matching local ontologies. Our experimental results on the real world datasets show that the proposed method is highly effective and efficient for extracting academic information from conference Web pages and generating high quality academic linked data. To extract the academic information, we first segment Web pages into blocks by VIPS[10], which is a popular vision-based page segmentation algorithm. VIPS can blocks can be constructed as a vision tree, which assures that all leaf nodes only con-tain text information. VIPS can obtain good segmentation results for most Web pages, troduce DOM-based analysis to improve VIPS segmentation results, especially find-ing missed text blocks. First, we need to obtain the basic vision semantic blocks by analyzing DOM tree of Web pages. A vision semantic block is a text block with independent meaning. A lot of blank nodes are removed from HTML tags. Then we traverse DOM tree to extract vision semantic blocks. A vision semantic block is between two newline tags such as &lt;br/&gt; and only contains style tags and texts. 
Algorithm 1 shows the detail of generating the VIPS complete tree. Let SB be vi-sion semantic block, LN be layout node of vision tree generated by VIPS, and DN be also a vision semantic block; (3) If it does not find a matched layout node, then add this SB into the vision tree. This algorithm not only assures that there is no informa-tion loss, but also preserves the structure of vision tree. Fig. 3 shows the part of VIPS processing of Algorithm 1, new semantic blocks are added to the vision tree, which is the complete tree with correct text blocks. 
Since some blocks such as navigation, copyright and advertisement do not contains the academic information. We regard these blocks as noise, which should be removed from VIPS complete tree. The noise removing process uses some vision features[11]. whether neighbor blocks are overlapped or adjacent. (3) Appearance features in-words of blocks and special order of some words. According to these vision features, we can remove noise nodes from VIPS complete tree. The academic information of a specific conference is distributed within a set of pages. For instance, the Overview page of the conference Web site usually contains the con-formation about conference date : conference begin and end date, submission dead-line, notification date of accepted papers, and so on. (2) Information about conference and institute : organizers, program committee, authors, companies, universities, coun-country. (5) Information about papers : title, authors of papers. 
We divide all text blocks into 10 categories as Table 1 shows: (1) DI : It describes information such as research area; (4) TO : It refers to research topics, and a AR block (7) PA : It is the information about papers; (8) CO : It refers the blocks which is com-bined by the above 7 categories blocks; (9) R : It refers to the interested blocks but not belong to any categories; (10) N : It refers to the blocks not only belong to any catego-corresponding examples. 
According to these categories, we can select some features to measure a given text blocks. We use vectors as Table 2 shows to describe each blocks. For a text block, we construct its features accordin g to vision, key words and text content information. For May 6, 2011 (23:59 UTC -11)&lt;/b&gt;&lt;/li&gt; , its feature can be constructed as: There are many famous existing classificatio n algorithms such as C4.5[8], K-Nearest Neighbors (kNN)[9] and Bayes Network[10]. C4.5 and Bayesian Network are the choose bayesian network model to solve the text blocks classifier problem. 
The classified results can be improved by post-processing, which includes repair-ing wrong classified results and adding missed classified results. The text blocks with wrong classification can be determined by two sides: their categories are not clear. These blocks should be checked further. Therefore, we use context feature to determine the blocks with R category. The context feature con-PaperArea , isPreviousDate , isPreviousPlace , isPreviousTopic , isPreviousPeople , isPreviousPaper and isVisuallySame . We propose some rules to add text blocks without clear classification. 
Rule 1: DateItem complete rule: A DI text block usually appears as three situa-tions: (1) It appears as page title with bold and big font size; (2) It appears with other words, for any situation, key words and other features should be considered. &amp;&amp; isTitle =ture. wordNum &lt;= DATE_ITEM_WORD_NUM . Rule 2: PlaceItem complete rule: PI classification usually has high precision. missed PI block. The rule is: placeNum &gt;1 &amp;&amp; wordNum &lt;PLACE_WORD_NUM. 
Rule 3: Area complete rule: A AR text block usually has big font size. Therefore, its complete rule is isHeader =true. 
Rule 4: Topic complete rule: A TO text block has typical context features. It be-cArea ) &amp;&amp; ( index-areaIndex =1) &amp;&amp; startWithLi . 
Rule 5: Position complete rule: A PO text block usually has big font size and few wordNum &lt; POSITION_WORD_NUM &amp;&amp; positionNum &gt; 0. Rule 6: PeopleItem complete rule: A PE block should consider its context. blocks contain key words about universities or companies. Most PE blocks have capi-wordNum &lt; PEOPLE_WORD_NUM. 
Rule 7: Paper complete rule: A PA block usually begins with &lt;li&gt;, and its list is nameNum &gt; TITLE_UPPER_WORD_NUM. 
After the post-processing, initial classification results will be improved greatly. For tion extraction problem by transforming it into a text block classification problem. formation. Therefore, we first manually build a global ontology as background know-ledge of academic domain, then automatically construct local ontologies for each conference website. 
After investigating a lot of conference websites and some ontologies related to academic domain, we manually construct a global ontology for describing the know-ledge of academic conference. The global ontology contains 97 concepts and 27 prop-stances, and it is stored as an ontology language RDF file. For a local ontology, its concepts and properties are contained in global ontology. We don X  X  consider the new knowledge which is not described in global ontology. mined by the context of the academic inform ation. For example, a paper information usually appears in PA text blocks and contains title and authors, so it is an instance of concept Paper , all authors are instances of concept Author , titles will be the property can generate a local ontology for each conference website. techniques[12-13]. For the reason that text in ontology can describes some semantics, the linguistic-based matching method can discover matching results by calculating We use the ontology matching API provided by ontology matching system Lily [13] system and can produce high quality matching results. 
Our ontology matching strategy is calculating matches for each two ontologies, then associate all ontologies into the linked data by these matches. This strategy has the benefit of handling a number of generated local ontologies, but its disadvantage is data for three conferences. If two instances are matched, they can be combined in the linked data graph. 6.1 System Implementation and Dataset The system is mainly implemented in Java, and the Web page segmentation module is CPU, 2GB RAM and Windows 7. 
We collect 50 academic conference Web sites in computers science field, which has 283 different Web pages and 10028 labeled text blocks. In order to evaluate our approach, 10 students manually tag all the text blocks as reference results. The results are saved in CSV files. 
We randomly select 10 sites which contain 62 pages as training dataset for construct-ing bayes network model. Other 40 conference Websites are used as test dataset. We use Precision, Recall and F1-Measure as criteria to measure the system performance. 6.2 Experimental Results and Analysis First experiment is verifying the complete tree. Table 3 shows the vision tree results 15 conference websites. We can see that the complete trees have more leaf nodes than vision trees. It means our algorithm can find more text blocks than VIPS. 
The experimental results of removing noise blocks are given in Table 4. We can ob-the number of nodes should be processed in extraction and improve the efficiency. 
The second experiment is the comparison between initial classification results and initial classification results only have average 0.75 precision, 0.67 recall and 0.68 F1-measure. After post-processing, the classificat ion results are improved to average 0.96 The average F1-measure on these blocks is 0.99. 
We selected 5 websites: AAAI-11, ASPLOS-11, NIPS-11, ICPR-11 and PKDD-11, then analyze the generated ontologies. Table 6 shows the statistics of some kinds 0.97 precision, 0.99 recall and 0.98 F1-measure. Therefore, our method can generate high quality academic ontologies for conference websites. data. This paper addresses the problem of extr acting academic information from conference Web pages, then organizing academic information as ontologies and finally generat-ing academic linked data by matching these ontologies. An new approach to background domain knowledge, and then the extracted academic information of each website is organized as local ontologies. Fi nally, academic linked data is generated by matching local ontologies Acknowledgments. This work is supported by the NSF of China (61003156 and 61003055) and the Natural Science Foundation of Jiangsu Province (BK2009136 and BK2011335). 
  X  However, GPUs are often not running at peak efficiency  X  Must split the work over multiple GPUs, which is inefficient per -GPU  X  Specifically for image recognition tasks  X  Optimizing CNNs and optimizing RNNs requires different techniques  X  E.g., 300M parameters  X  Efficient implementation of recurrent neural networks (RNNs) on GPU  X  Fitting very large network architectures on one GPU  X  Efficient handling of variable length sequences  X  Dealing with large output vocabularies  X  Dealing with large weight sets in multi -GPU training  X  Choosing models for optimal runtime performance  X  Pre -computation of the model  X  Model quantization  X   X  X EMM X  means matrix multiplication, taken from the BLAS routines  X   X  X perator X  is a layer in a network  X  Most popular for NLP: GRU and LSTM  X  Needed for MT: Attentional GRU/LSTM  X  This cannot be used for any variant, e.g., Attentional RNN  X   X  X ate X  is usually parameterized as a GEMM + sigmoid  X  Has  X  X pdate X  gate to maintain state over many timesteps  X  GRU is 6 matrix -vector multiplications, 14 element -wise functions  X  Note that  X  '  X  " +  X  '  X  " , -is equivalent to  X  ' [  X   X  Where  X  is  X   X   X  matrix,  X  C is a  X   X   X  matrix,  X  is  X   X   X  matrix  X  cublasSgemm () in CuBLAS library  X  The same kernel code is executed on many Cuda cores in parallel  X  Element -wise kernels are easy: Just operate on a single element  X  7,000 GFLOP/sec floating point performance  X  288 GB/sec memory bandwidth (best CPUs have ~100 GB/sec)  X  12 GB RAM  X  3072 CUDA cores 1. Making sure all Cuda cores are always busy 2. Reading from GPU RAM as little as possible  X  A batch of 1024 sentences of length 50 is actually 51,200 words  X  Probably won X  X  fit in memory, and will cause training inefficiencies  X   X  - X  =  X  -;  X  J  X  =  X  J  X   X  -;  X  J  X  =  X  J
The weight matrices (  X 
The same thing can be done for  X   X  Only for input vector, not for recurrent vector  X  Forward:  X   X  C =  X  ; Backward w.r.t. input:  X   X  =  X   X  288 GB/sec memory, 12 bytes per element = 24 GFLOP/sec  X  GPU can do 7,000 GFLOP/sec, so 0.03% utilization  X  And another kernel to do the same in the backward pass  X  It is aware of the layout of the fused  X  m = source hidden size, |S| = number of source words  X  Implemented in CUDA as cublasSgemmBatched ()  X  Must take derivatives and apply chain rule manually  X  Difficult to debug  X  Gradient checkers are useless  X  Subtle bugs won X  X  be caught by end -to -end experiment  X  Very inefficient  X  Easy to minibatch on GPU  X  Everything is  X  X quare X   X  Different buckets can have different batch sizes 3. Assign each training sentence the first bucket it can fit in 5. Each minibatch selects sentences from a single bucket  X  And GEMMs are much more efficient for large minibatches  X  But we can do better  X  This actually takes up the majority of the space for most RNN variants  X   X  X heap X  things can be re -computed inside of the kernel  X  Very simple, but sub -optimal  X  Requires careful analysis of the derivative  X   X   X   X  " ,  X  " ,  X  " were never save before  X  Operators never allocate GPU memory directly  X  No two buckets are run at the same time on the same GPU  X  E.g., training multiple languages at once, training parser + MT model  X  Noise Contrastive Estimation  X  ( Mnih et al., 2012)  X  Importance Sampling  X  ( Bengio &amp; Senecal , 2008)  X  BlackOut Layer  X  (Ji et al., 2015)  X  Hierarchical Softmax  X  ( Mnih &amp; Hinton, 2009)  X  What portion to select?  X  Shortlist based on unigram frequency of training data  X  E.g.,  X  = 8192,  X  = 8192  X  Sampled vocabulary shared across minibatch  X  All target words in minibatch must also be included  X  Inefficient because you have to wait for the slowest batch  X  Only practical downside: Non -deterministic training  X  Must transfer gradients and weights from GPU  X  CPU (or GPU0  X  GPU 1)  X  (254+162)/(254) = 1.60 1. All RNN/fully connected weights 2. Embeddings for words in the minibatch 3. Rows in the output layer that have been sampled  X  E.g.,  X  = 4  X  Total batch size = num workers * batch size per worker  X  Or, if the learning rate is too high, training becomes degenerate  X  Most common use case, inference could also be done on GPU or FPGA  X  For real time applications, we should assume batch size = 1  X  Allows several important optimizations  X  Generate each word in the target one -at -a -time, left -to -right  X  Larger values of  X  generally do not improve results  X  Push the hypothesis onto the  X  X ompleted X  stack  X  E.g.,  X  = 3 . 0  X  MKL is faster than OpenBLAS , and free now  X  Hand -coded assembly by experts  X  16 -bits is more than enough precision for inference time  X  Unfortunately, I don X  X  know of any open source implementations L  X  Weight matrices can be fused, but it doesn X  X  matter that much  X  Fusing GEMMs for input vectors is important  X  Reduces reads/writes to RAM  X  Use lookup tables for activations  X  Can also use SSE/AVX instructions, not shown here for simplicity  X  Take the union of translations for the current source sentence  X  Example tasks: Image captioning,  X  Can use final source vector, max pooling, or average pooling  X  These will already be available at training time  X  Loss function can be max -margin or softmax  X  Do not allow the source representation to change  X   X  "  X   X   X  is a word embedding from an embedding matrix  X   X   X   X  Store result in an  X   X   X   X  lookup table  X  Solution 1: Store pre -computed matrix in quantized 16 -bits  X   X  " g are still computed the same way as before  X  Usually ~4x more expensive, due to the beam search  X  Due to the pre -computation trick  X  E.g., 1024 -dim is 4x the cost of 512 -dim  X  Usually the biggest improvements are from 1 model  X  2 models

Figure 2: (a) Tensor Construction; (b) CP Decomposition ducted on POI recommendation by analyzing users X  check-in his-tory and social constraints patterns [3, 12, 5]. In particular, Ye et al. [11] apply a power-law probabilistic model to capture the geographical influence among users X  check-ins. Zhang et al. [13] consider social and geographical influence from both user and loca-tion perspectives, and develop a recommendation model by fusing Kernel density estimation into the matrix factorization framework. Yuan et al. [12] propose a collaborative recommendation model by incorporating temporal information. In a very recent work, Hu et al. [6] propose to improve the recommendation accuracy by incor-porating the geographical neighborhood information.

Instead of modeling check-ins as a traditional two-dimensional user-location matrix, in this paper, we address the POI recommen-dation problem by incorporating the multi-dimensional contextual information of check-in data with high order tensor factorization to uncover the hidden dependency of multi-dimensional contextual information. Karatzoglou et al. [7] also propose a tensor-based context-aware recommendation framework using tucker decompo-sition. However, they did not consider the internal correlations among the contextual entities. In contrast to their work, we em-ploy three order tensor to uncover the hidden dependency of multi-dimensional contextual information, and further explore the social influence among users, to support more accurate POI recommen-dation.
 This paper makes the following contributions: Notations. In the rest of this paper, scalars will be denoted by lowercase letters, (e.g., m ), vectors by boldface lowercase letters (e.g., u ), matrices by boldface capital letters (e.g., U ), and tensors by Calligraphy (e.g., R ). We will use  X  to denote the outer product,  X  the Khatri-Rao operation, and jjjj F the Frobenius norm.
The POI recommendation problem in this paper can be defined as: given the historical check-in records of m users f u i g m i =1 on n where A i;j indicates the similarity between user i and j , L is the Laplacian matrix induced from users X  social networks matrix A 2 R m m ; L = D A , where D is the diagonal matrix whose i -th diagonal element is the sum of all the elements in the i -th row of A , i.e., D ii =  X  U L ( U ; V ; T ) =  X  U jjR  X  V L ( U ; V ; T ) =  X  V jjR  X  T L ( U ; V ; T ) =  X  T jjR where R (1) U ( V  X  T ) T , R (2) V ( U  X  T ) T and R (3) T ( U  X  V ) T . As Equation 3 is not convex, we adopt the alternating optimization strategy to solve the objective function. In particular, we alternately optimize each of the three parameters U , V and T , while fixing the other two until convergence to find the optimal solution using Equation 4. Dataset. We select the check-in occurred during January 2010 to September 2010 from the original Brightkite [3], we remove users whose checked-in records are fewer than 5 POIs, and then removed POIs with fewer than 5 users checked in. For each user, we ran-domly mark off another 20% of POIs as testing data to evaluate the effectiveness of the recommendation methods. We use Preci-sion@x and Recall@x to evaluate our proposed method ( x = 5, 10, 15, 20).
 Comparison Methods. We compare the proposed models TenInt with the following other methods, which basically consist of two categories: (1) Non-contexts : we implement basic non-negative matrix factorization ( NMF ), user-based collaborative filtering ( UCF ) and item-based collaborative filtering ( ICF ) [10]; (2) Partial con-texts : we develop three models by taking into geographical influ-ence, temporal information and friendship into account, respec-tively.
 combined with friendship, spatial and temporal influence) have bet-ter performance than the ones without or with only partial context-awareness. The basic matrix factorization method has the worst accuracy, as it only works on user-location matrix and does not in-tegrate any contextual information. To sum up, the results demon-strate the effectiveness of incorporating multi-dimensional contex-tual information in a unified tensor based approach in improving the recommendation performance.
 (a) Precision@x (x = 5, 10, 15,
As the parameter dimensionality fundamentally determines the number of latent factors involved in the tensor factorization, in this section, we investigate the impact of the dimensionality by varying the value of dimensionality from 5 to 60 with a step size 5. Figure 4 shows the precision and recall at top 5, 10, 15 and 20 under differ-ent tensor dimensionality. We observe that the precision and recall keep increasing with larger dimensionality, however they slightly drop when dimensionality reaches around 55. The results reveal a larger dimensionality can effectively uncover information of check-ins and improve the recommendation performance. But when the dimensionality exceeds certain threshold, the performance may de-grade because of over-fitting. Larger dimensionality also requires more computational cost. Based on our results, we have set the dimensionality as 50 in above comparison. (a) Precision@x (x = 5, 10, 15,
In this paper, we propose a tensor non-negative decomposition based Point-of-Interest (POI) recommendation approach using users X  social constraints as regularization. We model the check-in records as three dimensional tensor and employ the non-negative tensor factorization method to enable effective POI recommendation in a higher dimensional space. Specially, we propose to impose users X  social constraints as regularization terms on tensor non-negative
