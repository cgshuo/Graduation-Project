 ORIGINAL PAPER Matthew Michelson  X  Craig A. Knoblock Abstract Information extraction from unstructured, ungrammatical data such as classified listings is difficult because traditional structural and grammatical extraction methods do not apply. Previous work has exploited refer-ence sets to aid such extraction, but it did so using supervised machine learning. In this paper, we present an unsupervised approach that both selects the relevant reference set(s) auto-matically and then uses it for unsupervised extraction. We validate our approach with experimental results that show our unsupervised extraction is competitive with supervised machine learning approaches, including the previous super-vised approach that exploits reference sets.
 Keywords Information extraction  X  Unsupervised  X  Semantic annotation  X  Information integration  X  Unstructured data sources 1 Introduction The huge amounts of unstructured and ungrammatical data on the World Wide Web could be useful if the information contained within them could be extracted. Examples of such data sources are internet-classified listings, such as those on Craigslist, 1 internet auction listings such as those found on eBay, or internet forum postings like those posted to the Bid-ding For Travel forum. 2 We consider all of the listings within these sources as  X  X osts. X  Figure 1 shows example posts from Craigslist. In the circled post we want to extract 02 as the year, M3 as the car model, and Convertible as part of the trim.

However, information extraction from posts poses dif-ficulties. The posts are not structured enough for wrapper methods, such as HLRT wrappers [ 11 ]. Neither are the posts grammatical enough to support natural-language based tech-niques, such as Amilcare [ 5 ]. In previous work, the Phoebus system [ 17 ] exploited reference sets to overcome these chal-lenges. A reference set is a relational data set that contains entities and their attributes. An example of a reference set is a set of cars, each with a make, model, and year. The Phoebus algorithm exploits the reference set by first matching each post to the members of the reference set, and then using the attributes from these matching members to aid the extrac-tion. The main benefit of Phoebus is its ability to extract attributes without any assumptions about the structure of the text, because it does not consider grammar or structure for extraction.

However, the Phoebus system is limited because it requires much user input. First, the user provides the reference set. Then, the user labels matches between the posts and the mem-bers of the reference set. Finally, the user labels examples of the extracted attributes. Only after training, can the Phoebus system extract data from unstructured, ungrammatical data sources.

This paper presents an alternative, unsupervised approach to exploiting reference sets for extraction. Using reference sets allows for unsupervised extraction without any struc-tural assumptions on the text. Our contributions replace each supervised step of the Phoebus algorithm with an unsuper-vised alternative, relieving the human dependence from all aspects of Phoebus X  extraction algorithm. This operation improves the scalability, cost, and robustness of extraction. In our approach, first the reference sets are selected by the sys-tem from a growing repository of many reference sets. After choosing the reference sets, the system selects the matches using a vector X  X pace model and performs the extraction in an unsupervised manner.

This work extends our previous work on unsupervised semantic annotation of unstructured sources [ 18 ]. In that work we used the average values of string similarities as one of the splitting criteria for finding matches versus non-matches between the reference set and the set of posts. In this work, we remove that restriction. Furthermore, we pres-ent extensive experiments in this paper investigating and justifying different heuristic choices such as thresholds and similarity metrics. In some cases, we are able to make gen-eralizations about the types of similarity metrics that should be used at different steps in our approach. Lastly, we extend our previous work by describing how to collect reference sets automatically and how to use the technique presented in this paper to automatically include the unstructured, ungrammat-ical data sources in information integration systems.
Unsupervised Information Extraction (UIE) has recently witnessed significant progress [ 3 , 10 , 21 ]. However, current work on UIE relies on the redundancy of the extractions to learn patterns in order to make further extractions. Such pat-terns rely on the assumption that similar structures will occur again to make the learned extraction patterns useful. Initially, the approaches can be seeded with manual rules [ 3 ], example extractions [ 21 ], or sometimes nothing at all, relying solely on redundancy [ 10 ]. Regardless of how the extraction pro-cess starts, extractions are validated via redundancy, and they are then used to generalize patterns for extractions. This approach to UIE differs from ours in important ways. First, the use of patterns makes assumptions about the struc-ture of the data. We cannot make any structural assumptions because our target data for extraction is defined by its lack of structure and grammar. Second, since these systems are not clued into what can be extracted, they rely on redun-dancy for their confidence in their extractions. In our case, our confidence in the extracted values comes from their sim-ilarity to the reference-set attributes exploited during extrac-tion. Lastly, the objectives differ. Previous UIE systems seek to build knowledge bases through extraction. For instance, they aim to find all types of cars on the Web. Our extraction creates relational data, which allows us to classify and query posts. For this reason, we believe that the previous work com-plements ours well because we could use their techniques to automatically build our reference sets. 2 Unsupervised extraction of unstructured, ungrammatical data Our algorithm for unsupervised information extraction of unstructured, ungrammatical data has three distinct steps. Given a set of posts, the first step is to have the system choose the applicable reference sets from a repository of reference sets. Once the reference sets are chosen, the system must then match each post to members of the reference set, which allows it to use these members X  attributes as clues to aid in the extraction. Finally, the system exploits these reference-set attributes to perform the unsupervised extraction. 2.1 Automatically choosing the reference sets Because our repository of reference sets grows over time, the system should choose the reference sets to exploit for a given set of posts. The algorithm chooses the reference sets based on the similarity between the set of posts and the reference sets in the repository. Intuitively, the most appropriate refer-ence set is the one with the most useful tokens in common with the posts. For example, if we have a set of posts about cars, we expect a reference set with car makes, such as Honda or Toyota, to be more similar to the posts than a reference set of hotels.

To choose the reference sets, we treat each reference set in the repository as a single document and the set of posts as a single document. We calculate a similarity score between each reference set and the set of posts. Then, we sort the sim-ilarity scores in descending order, and traverse this list, com-puting the percent difference between the current similarity score and the next. If this percent difference is above a thresh-old, and the score of the current reference set is greater than the average similarity score for all reference sets, the algo-rithm terminates. Upon termination, the algorithm returns as matches the current reference set and all reference sets that preceded it. If the algorithm traverses all of the reference sets without terminating, then no reference sets are relevant to the posts. Table 1 shows the algorithm.

We use the percent difference as the splitting criterion between the relevant and irrelevant reference sets because it is a relative measure. Comparing only the actual similarity values might not capture how much better one reference set is as compared to another. Further, we require that the score at the splitting point be higher than the average score. This requirement is needed in cases where the scores are so small at the end of the list that their percent differences can sud-denly increase, even with a small difference in score. This difference does not mean that we have found relevant refer-ence sets, rather it just means that the next reference set is that much worse than the current, bad one.

By treating each reference set as a single document, the algorithm of Table 1 scales linearly with the size of the repos-itory. That is, each reference set in the repository is scored against the posts only once. Furthermore, as the number of reference sets increases, the percent difference still deter-mines which reference sets are relevant. If an irrelevant ref-erence set is added to the repository, it will score low, so it will still be relatively that much worse than the relevant one. If a new relevant set is added, the percent difference between the new one and the one already chosen will be small, but both of them will still be much better than the irrelevant sets in the repository. Thus, the percent difference remains a good splitting point.

We do not require a specific similarity measure for this algorithm. Instead, our experiments of Sect. 3 find that cer-tain classes of similarity metrics can perform well. So, rather than picking just one as the best we try many different metrics and draw conclusions about what types of similarity scores should be used and why. 2.2 Matching posts to the reference set After choosing the relevant reference sets, the algorithm matches each post to the best matching members of the refer-ence set. When selecting multiple reference sets, the match-ing algorithm executes iteratively, matching the set of posts once to each chosen reference set. However, if two chosen ref-erence sets have the same schema, we only select the higher ranked one to prevent redundant matching.

To match the reference set records to the posts, we employ a vector X  X pace model. Using a vector X  X pace model, rather than machine learning, makes the algorithm unsupervised. Furthermore, a vector X  X pace model allows us to use infor-mation-retrieval techniques such as inverted indexes, which are fast and scalable.

In our model, we treat each post as a query and each record of the reference set as a document, and we use a token-based similarity to define their likeness. Again, we do not tie this part of the algorithm to a specific similarity metric because we find a class of token-based similarity metrics works well, which we justify in our experiments.

However, for our algorithm we modify the similarity met-rics we use. Our modification considers two tokens as match-ing if their Jaro-Winkler [ 25 ] similarity is greater than a threshold. For example, consider the classic Dice similar-ity, defined over a post p and a record of the reference set r , as Dice ( p , r ) =
If the threshold is 0.95, two tokens are put into the intersec-tion of the modified Dice similarity if those two tokens have a Jaro-Winkler similarity above 0.95. This Jaro-Winkler modification captures tokens that might be misspelled or abbreviated, which is common in posts. The underlying assumption of the Jaro-Winkler metric is that certain attri-butes are more likely similar if they share a certain prefix. This works particularly well for proper nouns, which many reference set attributes are, such as  X  X onda Accord X  cars. Using our modified similarity metric, we compare each post, p , to each member of the reference set and return the ref-erence set records with the maximum score, called r max i our experiments, we vary this threshold to test its effect on the performance of matching the posts. We also justify using the Jaro-Winkler metric to modify the Dice similarity, rather than another edit distance metric.

However, because more than one reference set record can have a maximum similarity score with a post ( r max i is a set), an ambiguity problem exists with the attributes provided by the reference set records. For example, consider a post  X  X ivic 2001 for sale, look! X  If we have the following three match-ing reference records: {HONDA, CIVIC, 4 Dr LX, 2001}, {HONDA, CIVIC, 2 Dr LX, 2001} and {HONDA, CIVIC, EX, 2001}, then we have an ambiguity problem with the trim. We can confidently assign HONDA as the make, CIVIC as the model, and 2001 as the year, because all of the matching records agree on these attributes. We say that these attributes are  X  X n agreement. X  Yet, there is disagreement on the trim because we cannot determine which value is best for this attribute. All the reference records are equally acceptable from the vector X  X pace perspective, but they differ in value for this attribute. Therefore, we remove from our annotation all attributes that do not agree across all matching reference set records (e.g., the trim in our example). Once this process executes, we have all of the attributes from the reference set that we can use for extraction. The full algorithm is shown in Table 2 .

Selecting all the reference records with the maximum score, without pruning possible false positives, introduces noisy matches. These false positives occur because posts with small similarity scores still  X  X atch X  certain reference set records. For instance, the post  X  X e pick up your used car X  matches the Renault Le Car, Lincoln Town Car, and Isuzu Rodeo Pick-up, albeit with small similarity scores. However, since none of these attributes are in agreement, this post gets no annotation. Therefore, by using only the attributes in agreement, we essentially eliminate these false positive matches because no annotation will be returned for this post. That is, because no annotation is returned for such posts, it is as if there are no matching records for it. In this manner, by using only the attributes  X  X n agreement, X  we sep-arate the true matches from the false positives.

Once we have matched the posts to a reference set, we can query the posts structurally, as we would a database. This is a tremendous advantage over the traditional keyword search approach to searching unstructured, ungrammatical text. For example, keyword search cannot return records for which an attribute is missing, whereas our approach can. If a post were  X 2001 Accord for sale, X  and the keyword search was Honda, this record would not be returned. However, after match-ing posts to the reference set, if we select all records where the matched-reference-set attribute is  X  X onda X  this record would be returned. Another benefit of matching the posts is that aggregate queries are possible. This mechanism is not supported by keyword search.

Perhaps, one of the most useful aspects of matching posts to the reference set is that we can include the posts in an infor-mation-integration system (e.g., [ 14 , 24 ]). In particular, our approach is well suited for local-as-view (LAV) information integration systems [ 13 ], which allow for easy inclusion of new sources by defining their  X  X ource description X  as a view over known domain relations. For example, let us say we have a reference set of cars from Edmunds car buying guide for the years 1990 X 2005, in our repository. If we include this source in an LAV integration system, the source description is Edmunds(make, model, year, trim) :-Cars(make, model, year, trim)  X  year  X  1995  X  year  X  2005
Now, let us assume a set of posts comes in and we match them to the records of this Edmunds source. We had no idea previously how to include this source in our integra-tion system, but after matching we can use the same source description of the matching reference set, along with a vari-able to output the  X  X ost X  attribute, to define a source descrip-tion of our unstructured, ungrammatical source. We can do this because we are appending records from the unstructured source with the attributes in agreement from the reference set matches. So, the new source description becomes: UnstructuredSource(post, make, model, year, trim) :-Cars(make, model, year, trim)  X  year  X  1995  X  year  X  2005
In this manner, we can collect and include new sources of unstructured, ungrammatical text in LAV information inte-gration systems without human intervention. 2.3 Unsupervised extraction Sometimes a user wants to see the actual extracted values for a given attribute, rather than the reference-set attribute from the matching record. Therefore, once the system retrieves the attributes in agreement from the reference set matches, it exploits these attributes for information extraction. First, for each post, each token is compared to each of the attributes in agreement from the matches. The token is labeled as the attribute for which it has the maximum Jaro-Winkler simi-larity. We label a token as  X  X unk X  if it receives a score of zero against all reference-set attributes.

However, this initial labeling generates noise because the attributes are labeled in isolation. To remedy this, we use our modified Dice similarity, and generate a baseline score between the extracted field and the reference-set field. Then, we go through the extracted attribute, removing one token at a time, and calculate a new Dice similarity value. If this new score is higher than the baseline, the removed token is a candidate for permanent removal. Once all tokens are pro-cessed in this way, the candidate for removal that yielded the highest new score is removed. Then, we update the baseline to the new score, and repeat the process. When none of the tokens yield improved scores when removed, this process terminates. This cleaning is shown in Table 3 . For this part of the algorithm, we use the modified Dice since our exper-iments in the annotation section show this to be one of the best performing similarity metrics. Note, our unsupervised extraction is O ( p 2 ) per post, where p is the number of tokens in the post, because, at worst, all tokens are initially labeled, and then each one is removed, one at a time. How-ever, because most posts are relatively short, this running time is acceptable.

The whole multi-pass procedure for unsupervised extrac-tion is shown in Fig. 2 . By exploiting reference sets, we perform unsupervised information extraction without assumptions regarding repeated structure in the data. 3 Experimental results This section presents results for our unsupervised approach to selecting a reference set, finding the attributes in agreement, and exploiting these attributes for extraction. Before examin-ing the results, we describe the reference sets and posts used in testing. 3.1 Reference sets We use six reference sets, most of which have been used in the past. First, we use the Hotels reference set from [ 17 ], which consists of 132 hotels each with a star rating, a hotel name, and a local area. Another reference set from the same paper is the Comics reference set, which has 918 Fantastic Four and Incredible Hulk comics from the Comic Books Price guide, each with a title, issue number, and publisher. We also use two restaurant reference sets, which were both used previously for record linkage [ 2 ]. One we call Fodors , which contains 534 restaurant records, each with a name, address, city, and cuisine. The other is Zagat , with 330 records.
 We also have two reference sets of cars. The first, called Cars , contains 20,076 cars from the Edmunds Car Buying Guide for 1990 X 2005. The attributes in this set are the make, model, year, and trim. We supplement this reference set with cars from before 1990, taken from the auto-accessories com-pany, Super Lamb Auto. This supplemental list contains 6,930 cars from before 1990. We call this combined set of 27,006 records the Cars reference set. The other reference set of cars also has the attributes make, model, year, and trim. However, it is a subset of the cars covered by Cars .This data set comes from the Kelly Blue Book car pricing service containing 2,777 records for Japanese and Korean cars from 1990 X 2003. We call this set KBBCars . This data set has also been used in the record linkage community [ 20 ]. A summary of the reference sets is given in Table 4 . 3.2 Post sets We chose sets of posts to test different cases that exist for find-ing the appropriate reference sets. One set of posts matches only a single reference set in our collection. It contains 1,125 posts from the forum Bidding For Travel. These posts, called BFT , match only the Hotels reference set. This data set was used previously in [ 17 ].

Because our approach can also select multiple relevant ref-erence sets, we use a set of posts that matches both reference sets of cars. This set, which we call Craig X  X  Cars , contains 2,568 car posts from Craigslist classifieds. Note that while there may be multiple, appropriate reference sets, they also might have an internal ranking. In this case we expect that both the Cars and KBBCars reference sets are selected, but Cars should be ranked first.

Lastly, we must examine whether the algorithm suggests that no relevant reference sets exist in our repository. To test this feature, we collected 1,099 posts about boats from Cra-igslist, called Craig X  X  Boats . Boats are similar enough to cars to make this task non-trivial, because boats and cars are both made by Honda, for example, so that keyword appears in both sets of posts. However, boats also differ from each of the reference sets so that no reference set should be selected. All three sets of posts are summarized in Table 5 . 3.3 Results As stated previously, our algorithm for choosing the ref-erence sets is not tied to a particular similarity function. Rather, we apply the algorithm using many different simi-larity metrics and draw conclusions about which ones work best and why. This gives us an idea of what types of metrics we can plug in and have the algorithm perform accurately. Note that for the following metrics: Jensen-Shannon distance, Jaro-Winkler similarity, TF/IDF, and Jaro-Winkler TF/IDF, we use the SecondString package X  X  implementation [ 6 ].
The first metric we use is the Jensen-Shannon distance (JSD) [ 15 ]. This information-theoretic metric quantifies the difference in probability distributions between the tokens in the reference set and those in the set of posts. Because JSD requires probability distributions, we define our distri-butions as the likelihood of tokens occurring in each docu-ment.

Table 6 shows our results for choosing relevant reference sets using JSD. The reference set names in bold reflect those that are chosen as appropriate. (This means Craig X  X  Boats should have no bold names.) The scores in bold are the sim-ilarity scores for the chosen reference sets. The percent dif-ference in bold is the point at which the algorithm breaks out and returns the appropriate reference sets. In particular, using JSD we successfully identify the multiple cases where we might have a single appropriate reference set, multiple reference sets, or no reference set. Note that, in all experi-ments we maintain the percent difference splitting threshold at 0.6.

The next metric we use is cosine similarity using TF/IDF for the weights. These results are shown in Table 7 . Similarly to JSD, TF/IDF is able to identify all of the cases correctly. However, in choosing both car reference sets for the Craig X  X  Cars posts, TF/IDF incorrectly determines the internal rank-ing, placing KBBCars ahead of Cars. This is probably due to the IDF weights that are calculated for the reference sets. Although we treat the set of posts and the reference set as a single document for comparison, the IDF weights are based on individual records in the reference set. Therefore, since Cars is a superset of KBBCars, certain tokens are weighted more in KBBCars than in Cars, resulting in higher matching scores. These results also justify the need for a double stop-ping criterion. It is not sufficient to only consider the percent difference as an indicator of relative superiority amongst the reference sets. The scores must also be compared to an aver-age to ensure that the algorithm does not errantly choose a bad reference set simply because it is relatively better than an even worse one. The last two rows of the Craig X  X  Boats posts and the BFT Posts in Table 7 show this behavior.
We also tried a simpler bag-of-words metric that does not use any sort of token probabilities or weights. To do this, we use the Jaccard similarity, which is defined as the tokens in common divided by the union of the token sets. That is, given token set S and token set T , Jaccard is: Jaccard ( S , T ) = The results using the Jaccard similarity are shown in Table 8 . One of the most surprising aspects of these results is that the Jaccard similarity actually does pretty well. It gets all but one of the cases correct. It is able to link the BFT Posts to the Hotels set only and it is able to determine that no refer-ence set is appropriate for the Craig X  X  Boats posts. However, with the Craig X  X  Cars posts, it is only able to determine one of the reference sets. It ranks the Fodors and Zagat restau-rants ahead of KBBCars because the restaurants have city names in common with some of the classified car listings. However, it is unable to determine that city name tokens are not as important as car makes and models. This is a problem if, for instance, the post is small and it contains a few more city name tokens than car tokens.

Lastly, we investigate whether requiring that tokens match strictly, as in the above metrics, is more useful than a soft matching technique that considers tokens matching based on string similarities. To test this idea we use a modified ver-sion of TF/IDF where tokens are considered a match when their Jaro-Winkler score is greater than 0.9. These results are shown in Table 9 . This metric is able to correctly retrieve the reference set for the BFT Posts , and for the Craig X  X  Boats posts this metric chooses no appropriate reference set. However, for the Craig X  X  Boats case, this metric is close to returning many incorrect reference sets since with the Zagat reference set the score is very close to the average while the percent difference is huge. The largest failure is with the Craig X  X  Cars posts. For this set of posts the ordering of the reference sets is correct because the Cars and KBBCars have the highest scores, but no reference set is chosen because the percent difference is never above the threshold with a similarity score above the average. The percent differences are low because of the soft nature of the token matching. For example, the Comics contains many matches because the issue number of the comics, such as #99, #199, #299, etc. match an abbreviated car year in a post such as  X 99. So, the similarity scores between the cars and comics reference sets are close. From these sets of results we see that it is bet-ter to have strict token matches, although they may be less frequent. The strict nature of the matches ensures that the tokens particular to a reference set are used for matches, which helps differentiate reference sets.

The performance of each metric is shown in Table 10 .We see each metric and whether or not it correctly identified the reference set(s) for that set of posts. In the case of Craig X  X  Boats, we consider it correct if no reference set is chosen. The last column shows whether the method was able to also rank the chosen reference sets correctly for the Craig X  X  Cars posts.

On the basis of these sets of results, we draw some conclu-sions about which metrics should be used and why. Compar-ing the Jaccard similarity results to the success of both JSD and TF/IDF, we see that it is necessary to include some notion of importance regarding the matching tokens. The results argue that probability distributions of the tokens as defined in JSD are a better metric, since TF/IDF can be overly sensi-tive, i.e., ignoring tokens that may be important, even though they are frequent. This claim is also justified by the fact that using JSD we do not run into the situation where we need to use the average stopping criterion, although we do with TF/IDF. However, since JSD and TF/IDF both do well, we can say that if a similarity metric can differentiate important tokens from those that are not, then it can be used success-fully in our algorithm to choose reference sets. This is why we do not tie this algorithm to any particular metric, since many could work. Another interesting aspect of these results is the poor performance of TF/IDF using the Jaro-Winkler modification. It seems that boosting the number of tokens that match by using a less strict token matching method actually harms the ability to differentiate between reference sets. This suggests that the tokens that define reference sets need to be emphasized by matching them exactly. Lastly, across differ-ent domains and even across different similarity metrics, we see our chosen threshold of 0.6 is appropriate for the cases where the algorithm chooses the correct reference set. In all of the cases where the correct reference set(s) is chosen, this threshold is exceeded.

Once the relevant reference sets are chosen, we use them to find the attributes in agreement for the different sets of posts, because we use these attributes during extraction. For the set of posts with reference sets (i.e., BFT and Craig X  X  Cars), we compare the attributes in agreement found by the vector X  X pace model to the attributes in agreement for the true matches between the posts and the reference set. To evaluate the annotation, we use the traditional measures of precision, recall, and F -measure (i.e., the harmonic mean between precision and recall). A correct match occurs when the attributes match between the true matches and the pre-dictions of the vector X  X pace model. As stated previously, we tried multiple modified token-similarity metrics to draw con-clusions about what types of metrics work and why.

Table 11 shows our results using the modified Dice simi-larity for the BFT and Craig X  X  Cars posts, varying the Jaro-Winkler token-match threshold from 0.85 to 0.99. That is, above this threshold two tokens will be considered a match for the modified Dice. In both cases we see an improve-ment in F -measure as the threshold increases. This is because more relevant tokens are being included when the threshold increases. If the threshold is low then many irrelevant tokens are considered matches, so the algorithm makes incorrect record level matches. For instance, as the threshold becomes low, the results for the year attribute drop steeply because often the difference in years is a single digit, which would often yield errantly matching tokens for a low threshold string similarity. Since errant matches are ignored (because they are not  X  X n agreement X ) the scores are low. Note, however, that once the threshold becomes too high, at 0.99, the results start to decrease. This is because now the edit-distance is too restrictive so it is not capturing some of the possible matching tokens that might be slightly misspelled.

The only attribute where the F -measure increases at 0.99 versus 0.95 is the year attribute of the cars. In this case, the recall slightly increases at 0.99, but the precisions are almost the same, yielding a higher F -measure for 0.99. This is due to more year values being  X  X n agreement X  with the higher threshold since there will be less variation in terms of which reference set values can match for this attribute, so those that do will likely be in agreement. For an example where 0.95 includes years that are not in agreement with high Jaro-Win-kler scores, consider a post with the token  X 19964 X  which might be a price or a year. If the reference set record X  X  year attribute is  X 1994, X  the Jaro-Winkler score between  X 19964 X  and  X 1994 X  is 0.953. If the reference set record X  X  year is  X 1996 X  the Jaro-Winkler score is 0.96. In both cases, a thresh-old of 0.95 includes both years, so if this post matches two reference set records with the same make, model and trim, but differing years of 1994 and 1996, then the year is dis-carded because it is not in agreement. We almost see the same behavior with the trim attribute as well. This is because with both of these attributes, a single difference in a char-acter, say  X  X X X  versus  X  X X X  for a trim (or a digit for the year) yields a completely different attribute, which can then become not  X  X n agreement. X 
Table 12 shows our results using the modified Jaccard sim-ilarity. As with the Dice similarity, the modification is such that two tokens are put into the intersection of the Jaccard similarity if their Jaro-Winkler is above the threshold. The most striking result is that the scores match exactly to those using the Dice similarity. The Dice similarity and Jaccard similarity can be used interchangeably. Further investigation revealed that the actual similarity scores between the posts and their reference set matches are different, which should be the case, but the resulting attributes that are  X  X n agreement X  are the same using either metric. Therefore, they yield the same annotation from the matches.

Table 13 shows results using the Jaro-Winkler TF/IDF similarity measure. Similarly to the other metrics, for the BFT domain we see an improvement in F -measure as the thresh-old increases, until the threshold peaks at 0.95 after which it decreases in accuracy. However, with the Cars domain the modified TF/IDF seems to perform the best with a threshold of 0.99.

From these results, across all metrics, a threshold of 0.95 performs the best for the BFT domain. In the Cars domain, the 0.95 threshold works best for the modified Dice and Jaccard, and at this threshold both methods outperform the TF/IDF metric, even when its threshold is at its best at 0.99. There-fore, the most meaningful comparisons between the different metrics can be made at the threshold 0.95. In the BFT domain, the modified TF/IDF outperforms the Dice and Jaccard met-rics, until this threshold of 0.95. At this threshold level, the Jaccard and Dice metrics outperform the TF/IDF metric on the two harder attributes, the hotel name and the hotel area. In the Cars domain, the TF/IDF metric is outperformed at every threshold level, except on the year attribute. Interest-ingly, at the lowest threshold levels TF/IDF performs terribly because the tokens that match in the computation have very low IDF scores since they match so many other tokens in the corpus, resulting in very low TF/IDF scores. If the scores are low, then many records will be returned and almost no attri-butes will ever be in agreement, yielding very few correct annotations.

These three sets of results allows us to draw some con-clusions about the utility of different metrics for our vec-tor X  X pace matching task. The biggest difference between the TF/IDF metric and the other two is that the TF/IDF met-ric uses term weights computed from the set of tokens in the reference set. The key insight of IDF weights are their ability to discern meaningful tokens from non-meaningful ones based on the frequency. The assumption is that more meaningful tokens occur less frequently. However, almost all tokens in a reference set are meaningful, and it is sometimes the case that very meaningful tokens in a reference set occur very often. The most glaring instances of this occur with the make and year attributes in the Cars reference set used for the Craig X  X  Cars posts. Makes such as  X  X onda X  occur quite frequently in the data set, and given that for 20,076 car records the years only range from 1990 to 2005, the re-occurrence of the same year tokens are very, very fre-quent. These attributes will be deemphasized significantly because of their frequency. If the matching token metric ignores the year, this attribute will often not be in agreement since multiple records of the same car for different years will be returned. Thus, TF/IDF has low scores for the year attribute. TF/IDF also creates problems by overemphasiz-ing unimportant tokens that occur rarely. Consider the fol-lowing post,  X  X ear New Ford Expedition XLT 4WD with Brand New 22 Wheels!!! (Redwood City -Sale This Week-end !!!) $26850 X  which TF/IDF matches to the reference set record {VOLKSWAGEN, JETTA, 4 Dr City Sedan, 1995}.
 In this case, the very rare token  X  X ity X  causes an errant match because it is weighted so heavily. In the case of the BFT posts, since the Hotel reference set has few commonly occurring tokens amongst a small set of records, this phenomena is not as observable. Since weights, whether based on TF/IDF or probabilities, rely on frequencies, such an issue will likely occur in most matching methods that rely on the frequencies of tokens to determine their importance.

Therefore, we draw the following conclusions. In the matching step, an edit-distance should be used to make soft matches between the tokens of the post and the reference set records. If the Jaro-Winkler metric is used, the threshold should be set to 0.95, since that yields the highest improve-ment using the best metrics. Lastly, and most importantly, reference sets do not adhere to the assumptions made by weighting schemes, so only metrics that do not use such schemes, such as the Dice and Jaccard similarities, should be used, rather than TF/IDF.

Since the modified Dice and Jaccard metrics work best using a threshold of 0.95, we now compare those results to our previous work on the Phoebus system [ 17 ], show-ing that our unsupervised approach to semantic annotation is competitive with a machine learning approach. We compare against the F -measure of the record linkage results from the Phoebus paper because that work uses the attributes of the matching record(s) from the reference set as semantic anno-tation. Since the Phoebus work only reports BFT results, we mirror the experimental procedure for the Craig X  X  Cars posts, running Phoebus 10 times using 10% of the data for training. In the cars case, we did not use 30% of the data for training because that produced a larger number of training pairs than Phoebus was built to handle. Table 14 reports the F -measure of Phoebus X  X  record linkage as Ph. F-Mes . Note that Ph. F-Mes is the same for all attributes within a domain because it is the F -measure for record linkage.

Although a direct comparison between the two systems is skewed because our system is unsupervised while Phoebus is not, the results nonetheless are intriguing. Even though our system selected the reference sets itself and our vector X  X pace model is unsupervised, our unsupervised system remains competitive in selecting the correct attributes to exploit for extraction.

While the above results show our unsupervised approach is competitive with supervised approaches, we still need to justify our use of the Jaro-Winkler. Earlier we stated that the Jaro-Winkler metric emphasizes matching proper nouns, rather than more common words, because it considers the prefix of words to be an important indicator of matching. This is in contrast to traditional edit distances that define a transformation over a whole string, which are better for generic words where the prefix might not indicate match-ing. Table 15 compares using Dice similarity modified with the Jaro-Winkler metric to Dice modified with the Smith-Waterman distance [ 23 ]. (Smith-Waterman is a classic edit distance originally developed to align DNA sequences.) As with the Jaro-Winkler score, if two tokens have a Smith-Waterman distance above 0.95 they are considered to be a match in the modified Dice similarity. As the table shows, the Jaro-Winkler Dice score outperforms the Smith-Waterman variant. Since many of the reference set attributes are proper nouns, the Jaro-Winkler is better suited for match-ing, which is especially apparent when using the Cars refer-ence set.

Lastly, Table 16 shows our field-level results for perform-ing information extraction exploiting the attributes in agree-ment. For field-level results, an extraction is correct if and only if all of the tokens that compose that field in the post are correctly labeled, without extra tokens. Our results are shown as UIE. We compare our results to those obtained using three supervised systems: the Phoebus system, Con-ditional Random Fields as implemented in MALLET [ 16 ], and Amilcare [ 5 ], which relies strongly on NLP. We train MALLET and Amilcare on 30% of the data for BFT and Craig X  X  Cars posts. We present the precision, recall, and F -measure for the extractions in Table 16 .

As with the semantic annotation results, the comparison between systems is not direct, yet again our unsupervised approach remains competitive. In fact, over the seven attri-butes, our UIE results have the lowest F -Measure only once , for the star rating of a hotel post. Interestingly, our approach has the highest precision for four of the attributes, but for three of those four, it also has the lowest recall. These results suggest that if we can increase the discovery of the extrac-tions, we can increase the recall, and get even better results. For example, one of the attributes with the highest precision but lowest recall is the local area. In this attribute, we often see acronyms such as  X  X P X  for airport or  X  X T X  for downtown. Supervised systems can be trained to identify such cases, but our approach would need some sort of acronym and syno-nym discovery method to find those. We plan to enhance our system with such functionality in the future.

The largest differences in the F -Measure occur when the attribute to be extracted comes from a field that is often not in agreement. For example, in the Craig X  X  Cars domain, the trim is often not in agreement, leading to poor extraction results. Nonetheless, we feel that the cost of labeling data for the supervised systems outweighs the differences in accuracy from our system. As information extraction begins to scale to huge amounts of data on the World Wide Web, unsupervised information extraction methods will be needed because the cost of labeling the data will be overwhelming. We believe these results validate such an approach. 4 Related work Semantic annotation is an active field of research, particularly as the popularity of the Semantic Web increases. Accord-ing to a recent survey [ 22 ], systems that perform semantic annotation separate into three categories: rule-based, pattern-based, and wrapper induction methods. However, the rule-based and pattern-based methods rely on regularity within the text, which is not the case with posts. Also, beyond exploiting regular structure, the wrapper induction methods use super-vised machine learning instead of unsupervised methods.
The system closest to ours is SemTag [ 9 ], which first iden-tifies tokens of interest in the text, and then labels them using the TAP taxonomy, which is similar to our reference sets. This taxonomy is carefully crafted, which gives it good accuracy and meaning. In contrast, our reference sets are flexible as we incorporate any that we can automatically collect. Including new reference sets in our repository has no effect on the data already in it (because the reference sets are independent). Although our data collection is not as careful as using a full taxonomy, we can much more easily and quickly gather many reference sets, greatly increasing our coverage of items we can annotate.

Further, SemTag focuses on disambiguation which our approach avoids. If one looks up the token  X  X aguar X  it might refer to a car or an animal, because SemTag disambigu-ates after labeling. In our case, we perform disambiguation before the labeling procedure, during the selection of the rel-evant reference sets. If we had reference sets of animals and cars, and we chose cars as the relevant one, the synonymy is avoided since animal records are ruled out.

Selecting reference sets is similar to resource selection in distributed information retrieval, sometimes used for the  X  X idden web. X  In resource selection, different servers are chosen from a set of servers to return documents for a given query. Craswell et al. [ 8 ] compare three popular approaches to resource selection. However, these retrieval techniques execute probe queries to estimate the resource X  X  data cov-erage and its effectiveness at returning relevant documents. These coverage and effectiveness statistics are then used to select and merge the appropriate resources for a query. This overhead is unnecessary for our task. Because we have full access to all of our reference sets in our repository, we already know the full data coverage, and do not need to estimate our repository X  X  effectiveness, because it always returns all of our sets.
Information extraction has previously incorporated out-side information to aid extraction. For example, the CRAM system [ 1 ] is unsupervised, and uses reference sets. How-ever, unlike this paper, CRAM is given the reference set and requires that all tokens receive a label, not allowing for  X  X unk X  in the text. Other work in information extraction incorporates reference sets with machine learning, for example the work of Cohen and Sarawagi [ 7 ]. However, this work requires human selected reference sets and this technique relies on supervised machine learning. 5 Conclusion We introduce a technique for unsupervised information extraction from unstructured, ungrammatical text. Previously, unsupervised extraction used extraction patterns that make assumptions about the regularity of the structure in the data. We relax this assumption by exploiting reference sets to aid the extraction. These reference sets are chosen by the algo-rithm, removing the need for any human intervention.
Furthermore, we investigate different methods for choos-ing the reference sets and finding the matches between the posts and the reference sets. Experimentally, we find that the Jensen-Shannon distance is the superior metric for choos-ing the correct reference sets. We also discover that when matching the posts to the reference set, modifying the sim-ilarity metric to use Jaro-Winkler edit-distance works well. However, this is the case only when the similarity metric does not use weighting schemes such as TF/IDF.

This approach describes a full architecture for collecting and exploiting reference sets for semantic annotation and extraction, allowing local-as-view information integration to automatically incorporate unstructured, ungrammatical data sources which would be inaccessible for structural queries otherwise.

In future work, we plan to investigate methods to improve the accuracy of our extraction and matching. For instance, using domain-specific, text transformations such as acro-nyms could greatly aid the accuracy of both. For example, in the BFT domain, it is useful for both matching and extrac-tion to know that  X  X T X  is the same as  X  X owntown. X  In some of our other research, we have investigated the problem of automatically discovering such textual transformations, but in that research the datasets are already structured, relational data [ 19 ]. Studying how to apply such a method to the case where one set of data is unstructured and not delimited, while the other set of data is structured and relational is a challenge for future investigation.

We also intend to analyze how to apply our method to larger pieces of text, such as the paragraphs associated with classified listings. This will likely involve running an entity-extractor over the text first, to pull out the meaningful tokens, and then concatenating these tokens together to form a new post. In fact, we could use the unsupervised extraction meth-ods mentioned previously in this paper to perform this entity extraction [ 3 , 10 , 21 ].

Lastly, one aspect to investigate is adding reference sets to our repository automatically, thereby increasing the cov-erage of sources for which we can perform extraction. The automatic collection of reference sets could be accomplished in two ways. First, as stated previously, we can use previous unsupervised extraction systems to build reference sets. For instance, KnowItNow [ 3 ] can extract all cars it can find from the Web, creating a reference set in this manner. Another approach to creating reference sets exploits research on infor-mation source discovery and modeling. By combining pre-vious research on labeling the outputs of Web Services [ 12 ] with research on modeling the information service provided by Web Services [ 4 ] an agent can model the information pro-vided by a source. This combination would allow an agent to define a schema using the Service X  X  inputs and its out-puts. Then, modeling the source can classify the source X  X  provided data, therefore creating a new reference set. For example, consider a Web Service takes as input a year and provides all car makes, models, trim and colors for that year. First, the schema is defined based on classifying the inputs and outputs. Then a reference set of records is created by supplying all years and retrieving all car records. Then, a source modeler defines this set of records as cars, and it is included as a new reference set in the repository. The archi-tecture shown in Fig. 3 shows how agents can automatically create reference sets by combining the approaches outlined above, using both the unsupervised extraction methods and source modeling methods. References
