 The Bayesian belief network is a powerful knowledge representation and reasoning tool under conditions of uncertainty. Recently, learning the Bayesian network from a database has drawn noticeable attention of researchers in the field of artificial intelli-gence. To this end, researchers developed many algorithms to induct a Bayesian net-work from a given database [1], [2], [3], [4], [5], [6]. 
Very recently, researchers have begun to tackle the problem of learning the network from incomplete data. A major stumbling block in this research is that when in closed tures. This has led many researchers down the path of estimating the score using pa-rametric approaches such as the expecta tion-maximization (EM) algorithm [7]. How-problem is to use a stochastic search method. 
This paper developed a new data mining algorithm to learn Bayesian networks structures from incomplete data based on extended Evolutionary Programming (EP) method and the Minimum Description Length (MDL) metric. The algorithm presents fitness function by using expectation, which converts incomplete data to complete convergence, we combine the restart technology [8] into EP. Furthermore, our algo-complete variable ordering as input. 
We X  X l begin by briefly introducing Bayesian network and MDL metric. Next we series of experiments to demonstrate the performance of our algorithm and sum up the whole paper in section 5 and 6, respectively. 2.1 Bayesian Network with variables and conditional probability tables of the node variable given its parents in the graph. The joint probability distribution (JPD) is then expressed by the formula: where ) ( i x  X  is the configuration of i X  X  s parent node set ) ( i X  X  . 2.2 The MDL Metric The MDL metric [9] is derived from information theory. With the composition of the description length for network structure and the description length for data, the MDL The MDL score of the network is simply the summation of the MDL score of ) ( i X  X  of every node i X in the network. According to the resolvability of the MDL metric, equation 2 can be written when we learn Bayesian networks from complete data as follow: Although EP was first proposed as an evolutionary algorithm to artificial intelligence, it has been recently applied to many numer ical and combinatorial optimization prob-lems successfully. 
One of EP's key features is its self-adaptation scheme. In EP, mutation is typically the only operator used to generate new offspring. The mutation is often implemented variance). In the widely used self-adaptation scheme of EP, this parameter is evolved, rather than manually fixed, along with the objective variables. 
Premature convergence is a serious issue in evolutionary algorithms since it might significantly degrade the overall performance. EP is easy to fall into local optimums. and of many individuals diminish rapidly because of self-adaptation scheme. 
We define a quantity which characteri ze the premature convergence. Suppose population most excellent individual. Where Then initialize afresh the population &amp; comeback the population variety. So the evo-lution can progress effectively. 
We combine the restart strategy into EP. When mean is less than a positive num-ber threshold which is confirmed beforehand, we consider that the evolution has danger of premature convergence and initializes afresh the population. Based on pre-hold the evolutionary information better. The algorithm we propose is shown below. We have conducted a number of experiments to evaluate the performance of our algo-rithm. The learning algorithms take the data set only as input. The data set is derived from ALARM network (http://www.norsys.com/netlib/alarm.htm). from the data set ten times. Then we select the most perfect network structure as the tures for the ALARM data sets of 5,000 cases is 81,219.74. 
The population size PS is 30 and the maximum number of generations is 5,000. We employ our learning algorithm to solve the ALARM problem. The value of q is set to be 5. We also implemented a classical GA to learning the ALARM network. The one-point crossover and mutation operations of classical GA are used. The crossover our learning algorithm and the classical GA are delineated in Figure 1. From Figure 1, we see that the value of the average of the MDL metric for restart-EP is 81362.1 and the value of the average of the MDL metric for the GA is 8,1789.4. We find our learning algorithm evolves good Bayesian network structures at an aver-4495.4. Thus, we can conclude that our learning algorithm finds better network struc-overcome the premature convergence.

Our algorithm generates 1000, 10000 cases from the original network for training and testing. The algorithm runs with 10%, 20%, 30%, and 40% missing data. The experiment runs ten times for each level of missing data. Using the best network from each run we calculate the log loss. The log loss is a commonly used metric appropri-between our algorithm and reference [10]. 10%, 20%, 30%, and 40% missing data than reference [10] does.
 works from incomplete data. This problem is extremely difficult for deterministic algorithms and is characterized by a la rge, multi-dimensional, multi-modal search structure from incomplete data. 
