 We study a generalized version of the near-duplicate detection problem which concerns whether a document is a subset of another document. In text-based applications, document containment can be observed in exact-duplicates, near-duplicates, or containments, where the first two are special cases of the third. We introduce a novel method, called CoDet, which focuses particularly on this problem, and compare its performance with four well-known near-duplicate detection methods (DSC, full fingerprinting, I-Match, and SimHash) that are adapted to containment detection. Our method is expandable to different domains, and especially suitable for streaming news. Experimental results show that CoDet effectively and efficiently produces remarkable results in detecting containments. H.3.7 [ Digital Libraries ]: Collection, System Issues.
 General Terms : Algorithms, Experimentation, Performance, Reliability Keywords : Corpus Tree, Document Containment, Duplicate Detection, Similarity, Test Collection Preparation. Near-duplicate 1 detection is an important task in various web applications. Due to reasons such as mirroring, plagiarism, and versioning such documents are common in many web applications [17 ]. For example, Internet news sources generally disseminate slightly different versions of news stories coming from syndicated agencies by making small changes in the news articles. Identifying such documents increases the efficiency and effectiveness of search engines. detection problem and investigate whether a document is a subset of another document [2]. In text-based applications, document containment can be observed in near-duplicates and containments. We refer to identifying such document pairs as the document containment detection problem. We study this problem within the context of news corpora that involve streaming news articles. d has, then d C is said to contain d A , which is denoted as d and this relation is called containment . Moreover, if two documents contain roughly the same content, they are near-duplicates [6]. Although near-duplicate condition is a special case of containment, these two cases are not usually distinguished from eac h other [ 15 ]. Similar to the  X  X onditional equivalence X  concept defined by Zobel and Bernstein [17], if d C  X  d A , then a news-consumer who have already read d C would have no need to read d . Of course, d C  X  d A does not necessarily imply d containment relation is asymmetric. By detecting d C consumers that have already seen d C can be informed to skip reading d A . duplicate detection algorithms. Therefore, we compare performance of CoDet with well-known near-duplicate detection approaches. sentence-based containment detection method adaptable to different text-based problem domains, and especially suitable for streaming news; show that our approach outperforms commonly known n ear -duplicate detection methods; and construct a test collection using a novel pooling technique, which enables us to make reliable judgments for the relative effectiveness of algorithms using limited human assessments. In near-duplicate detection similarity measurement plays an important role [11]. By using similarity, two documents are defined as duplicates if their similarity or resemblance [3] exceeds a certain threshold value. Such approaches are applied to identify roughly the same documents, which have the same content except for slight modifications [1]. In comparisons, factors other than similarity may also play a role. Conrad and Schriber [7] after consulting librarians deem that two documents are duplicates if they have 80% overlap and 20 variations in length. calculation. Instead of using each word, a sequence of them, shingles, may be used. In shingling approaches, if two documents have significant number of shingles in common, then they are considered as similar (near-duplicate). Well-known shingling techniques include for example COPS [1] and DSC (Digital Syntactic Clustering) [3]. COPS uses the sentences (or small units) to generate hash codes and stores these in a table to see if a document contains a sentence. Wang and Chang propose using the sequence of sentence lengths for near-duplicate detection and they evaluated different configurations of sentence-level and word-level algorithms [14 ]. issues. As a result a new strategy emerged which is based on hashing of the whole document. I-Match [6] is a commonly known approach that uses this strategy. It filters terms based on collection statistics ( idf values). Charikar X  X  [ 5] Simhash method is based on the idea of creating a hash by using document features (words, bigram, trigrams, etc.). It compares bit differences of these signatures to decide if two documents are near-duplicate or not. Yang and Callan [ 15] use clustering concepts for efficiency. While clustering documents they use additional information extracted from documents and structural relationships among document pairs. duplicate detection by representing documents as real valued sparse k-gram vectors, where weights are learnt to optimize a similarity function. Zhang et al. [18] address the partial-duplicate detection problem by doing sentence level near-duplicate detection and sequence matching. Their algorithm generates a signature for each sentence and sentences that have the same signature are considered as near-duplicates. Theobald et al. [13] propose SpotSigs algorithm that combines stopword antecedents with short chains of adjacent content ter ms to create signatures. CoDet is a novel sentence-based containment detection algorithm . It employs a new similarity measure called containment similarity (CS). It measures to what extent a document d A is contained by another document d C , which is defined as where S A and S C denote the set of sentences in d respectively. The function cs(s i , s j ) indicates containment similarity between sentences s i and s j , which is calculated as word prefix match of the sentences s i and s j , len t is the length of the word sequence t, and w t,k stands for the k th word in the word sequence t. For example, let s 1 be  X  X ohn is happy. X  and s is sad. X  then f(s 1 , s 2 ) is a word sequence (John, is), is 2 two sentences grows significantly as their word prefix match gets longer. The containment similarity of a document to itself is referred to as self-containment similarity (SCS). 
Fig. 1. Insertion of three documents d A :  X  X ASDAQ starts day with an increase. Shares gain 2%. X , d B :  X  X ASDAQ starts the day with a decrease. Shares lose 2%. X , and d C :  X  X hares lose For efficient calculation of containment similarities, we utilize a data structure called corpus tree . The corpus tree begins with a virtual root node which contains a pointer list storing the locations nodes other than the root contain a label and a document list. The l abel represents the node X  X  term and the document list contains visiting document ids. s }. Processing of d A involves processing all of its sentences. Insertion of s i (1 X  i  X  n) to the corpus tree is performed as follows: First, words of s i are sorted according to their idf values in words in s i after sorting. These words are inserted into the corpus tree starting from the virtual root node. If the root has a child with label w 1 , then similarity values of d A with all documents in Otherwise, a new child node with label w 1 is created and added to the root X  X  pointer list. In the next step, we treat as we did the root, and insert the following word w 2 of s insertion of s i finishes after all of its words are processed . The remaining sentences of d A are handled in the same manner. The same is done for the remaining sentences of d A . insertions. In Fig. 1-I, d A  X  X  sentences  X  X ASDAQ starts day with an increase. X  and  X  X hares gain 2%. X  are inserted to the corpus tree starting from the virtual root, which is shown by a dark circle. Since created. Similarly, insertion of the second sentence creates nodes the sentence  X  X ASDAQ starts day with a decrease. X  previously upda ted. Also, two nodes with labels &lt; a, decrease&gt; are created. Insertion of the sentence  X  X hares lose 2%. X  visits the node with similarity value of d A and d B is increased by summation of each revisited node's impact values, which is cal culated by multiplication of node X  X  depth and idf value of its label. For example, contribution of the node with label starts is because its depth is 2 and word starts appears in 2 of 3 documents (in the experiments, the idf values are obtained from a large reference collection). The final structure of the corpus tree after the insertion of d C is shown in Fig. 1-III.
 document d C , CoDet uses CS(d A , d C ) as well as SCS(d (CS(d A , d C ) / SCS(d A )) exceeds the equivalency threshold level values are tested. For each scenario, let n denote the number of documents and let c denote the average number of words per document, which is treated as constant. First Scenario (One Content, n Documents) : In this case, each document has the same content; therefore, corpus tree contains c nodes. Each node contains n integers in its document list. As a result, the memory requirement of the corpus tree is O(n) but due to pairwise containment similarity increase operations the algorithm takes O(n 2 ) time. Second Scenario (n Different Contents, n Documents) : In this case, each document has totally different content. Thus, corpus tree contains nc nodes (one node for each word). Each node contains only one document id in its document list. Therefore, asymptotically the memory requirement of the corpus tree is O(n) and the algorithm takes O(n) time.
 The first scenario is the worst case for CoDet, where the algorithm performs nonlinearly. The second one is the best case for CoDet and the algorithm runs in linear time. In practice the algorithm behaves as if it is linear because average number of near-duplicate per document is significantly smaller than n. Also CoDet is especially suitable for streaming news since with a time window concept , which makes older documents to be removed from the corpus tree, the corpus tree does not grow too much. We used four algorithms to compare their effectiveness and efficiency with CoDet. These algorithms are: DSC: Every three overlapping substrings of size four in th e documents are hashed. I f a document d C contains 60% of d values, we say d C  X  d A [3]. Full Fingerprinting (FFP): For each document, all substrings of size four are hashed. If document d C contains 60% of d values, then, d C  X  d A . I-Match: First two words with the highest idf values are ignored. After that, ten words with the highest idf values are used to create a fingerprint for each document. When a pair of documents has the same fingerprint, the pair is marked as containment [6]. SimHash: First two words with the highest idf values are ignored. Then, each unique term of a document is hashed. We use a vector v, whose size is equal to the hash value bit size, to determine the final updated as follows: If i th bit of the hash value of t is zero , then it is decreased by idf of w. It is increased by the idf otherwise. Finally, if i element of v is positive, i th bit of the SimHash value is set to one ; otherwise it is set to zero . When a pair of documents X  SimHash values has a Hamming distance less than three , the pair is considered as containment. Stopword elimination and a word truncation-based stemming (first-5) are performed before the detection process. I-Ma tch, SimHash and CoDet requires idf values. These values are obtained from a large reference collection (defined in the next section). In order to do a fair evaluation, each algorithm X  X  parameters are optimized to give the best results for efficiency. 2.1Ghz six-core AMD Opteron processors with six 128 KB L1, 512 KB L2, and one 6MB L3 cache. It has 128 GB memory and operating system Debian Linux v5.0.5. There is no gold-standard test collection for containment detection in news corpora; therefore, we prepared a test dataset from the Turkish TDT (Topic Detection and Tracking) news collection (BilCol-2005) [4] which contains 209,305 streaming (time-ordered) news articles obtained from five different Turkish web news sources. 2005 . For effectiveness measurement, we used the first 5,000 documents of BilCol-2005 . It is practically impossible to provide human assessment for each document pair in this sub-collection. Our approach to human assessments is similar to the pooling method used in TREC for the evaluation of IR systems [ 16 ]. For the creation of the dataset, we obtained a number of possible containments by running all five methods (including CoDet) with permissive parameters. In this way, methods nominate all pairs that would normally be chosen with their selective parameters, together with several additional pairs as containment candidates. Since the methods are executed with permissive parameters, we expect that most of the real containments will be added to the test collection. All pairs of documents, which are marked as containments by any of the methods, are brought to the attention of human assessors to determine whether they actually are containments. Note that in order to measure the effectiveness of a new algorithm with this test dataset, adding human assessments only for containment candidates that are nominated solely by this new algorithm to our dataset is sufficient. and false positive (FP) document pairs returned by any of our permissive algorithms. excluding true negative and false negative pairs do not change the relative effectiveness rankings of selective algorithms during the test phase; because, if a permissive algorithm marks a pair as negative (non-containment), then its selective counterpart should also marks that pair as negative. Therefore, including TN and FN pairs of permissive algorithms in our dataset would not contribute to the number of positive pairs (TP X  X  and FP X  X ) returned by any selective algorithm during the test phase. Hence, using our pruned dataset, precision 1 values of the selective algorithms remain unchanged with respect to precision values they would obtain in a full dataset having annotations for all possible document pairs. Similarly, recall values of the selective algorithms decrease proportionally (with the same ratio of total number of containments in the pruned dataset to the total number of containments in the full dataset, for all algorithms) with respect to recall values they would obtain in the full dataset. nominations. We performed a human-based annotation to obtain a ground truth. The pooled document pairs are divided into 20 groups containing about the same number of nominations. Each document pair is annotated by two assessors. The assessors are asked if the nominated document pairs are actually containments. The assessors identified 2,875 containment cases. The size of our dataset is comparable with the annotated test collections reported in related studies [ 13]. opinions about the relevance of a document to a query. A similar situation arises in our assessments. For example, for the document pair d C =  X  X YZ shares increase 10% from 100 to 110. X  and d  X  X YZ shares increase from 100 to 110. X , some assessors may say that d C and d A are near-duplicates, while some others may claim d contains d A , but the d A does not contain d C . In such cases we expect disagreements among human assessors. In order to validate the reliability of the assessments, we measured the agreements of the judgments by usin g the Cohen X  X  Kappa measure, and obtained an average agreement rate of 0.73. This indicates almost a substantial agreement [ 10], which is an important evidence for the reliability of our test dataset. Furthermore, such conflicts are resolved by an additional assessor. In this section, we first investigate the impacts of the following parameters on the performance of CoDet: Processed Suffix Count (PSC), Depth Threshold (DT), and Word Sorting (WS). This discussion is followed by efficiency and effectiveness performance of CoDet with those of four well-known near-duplicate detection algorithms. Effectiveness measurement is done by precision, recall and F 1 values. Impacts of parameters and effectiveness experiments are done on prepared test collection. Efficiency experiment is performed with the whole BilCol-2005 . Processed Suffix Count (PSC): It determines how many suffixes of each sentence are inserted to the corpus tree. If the PSC is 3, the processed suffixes for  X  X ASDAQ starts day with an increase. X  are the sentence itself, &lt;starts, day, with, an, increase&gt; and &lt;day, with, an, increase&gt; Increasing PSC increases space requirement Precision (P) = |TP| / (|TP| + |FP|). where |S| is the cardinality of the set S. Recall (R) = |TP| / (|TP| + |FN|). F 1 = 2PR / (P + R). but do not change effectiveness considerably as shown in Fig. 2: Different PSC values result in close F 1 scores. Fig. 2. Effect of Processed Depth Threshold (DT): It determines how many words of a sentence are processed. If the DT is 3, the processed words  X  X ASDAQ starts day with an increase. X  are &lt; nasdaq, starts, day &gt; . Fig. 4 and 5 show the effect of DT on F 1 score . Sorting words of a sentence by idf values places representative words close to the virtual root. Thus, results ar e better for small DT values when word sorting is enabled. It avoids the noise effect of insignificant words in similarity calculations. In the experiments, DT value of 5 gives the best result; also smaller DT values yield a similar performance. Thus, instead of having the corpus tree structure, an algorithm that considers only a few most significant words from each sentence can improve efficiency without sacrificing effectiveness significantly . Word Sorting (WS): Sorting words in sentences by idf values causes important words to be located close to the virtual root. Since most sentences start with common words, by using word sorting, we avoid many redundant similarity calculations. In the experiments, enabling word sorting decreases average number of calculated similarity values per document from 341 to 3.53. The efficiency results are given in Fig. 3. As the number of documents increase, execution time of full fingerprinting increases non -linearly. It calculates similarity values for each document pair that has at least one substring in common. Hence, it is not feasible for large collections. CoDet performs as the third best algorithm in time efficiency; the corpus tree accesses impose many random memory accesses, which disturb cache coherency. Our results show that I-Match, SimHash and CoDet are scalable to large collections. with a value of 0.85 F 1 score is observed with FFP since it calculates text overlaps between document pairs having a common substring. Therefore, without making any semantic analysis, it is difficult to outperform FFP in terms of effectiveness with a time-linear algorithm. CoDet finds text overlaps by only using important words of sentences and is the second best in terms of effectiveness with an F 1 score of 0.76. I-Match, SimHash, and DSC perform poorly with respective F 1 scores of 0.45, 0.39, and 0.30. FFP is not feasible for large collections; thus, CoDet is the most suitable algorithm for containment detection in news corpora . In this work we investigate containment detection problem, which is a more generalized version of the near-duplicate detection problem. We introduce a new approach, and compare its performance with four other well-known methods. As the experimental results demonstrate CoDet is preferable to all these methods; since it produces considerably better results in a feasible time. It also has desirable features such as time-linear efficiency and scalability, which enriches its practical value. Our method is versatile, can be improved, and can be extended to different problem domains. 
