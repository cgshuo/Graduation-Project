
Antonio Gomariz 1 , , Manuel Campos 2 ,RoqueMarin 1 , and Bart Goethals 3 Sequence Data Mining (SDM) is a well-extended field of research in Temporal Data Mining that consist of looking for a set of patterns frequently enough occurring across time among a large number of objects in a given input database. The threshold to decide if a pattern is meaningful is called minimum support . SDM has been widely studied [6,9,3,5,2], with broad applications, such as the discovery of motifs in DNA sequences, analysis of customer purchase sequences, web click streams, and so forth.

The task of discovering the set of all f requent sequences in large databases is challenging as the search space is extremely large. Different strategies have been proposed so far, among which SPADE , exploiting a vertical database for-mat [9], FreeSpan and PrefixSpan, based on projected pattern growth [3,5] are the most popular ones. These strategies show good performances in databases containing short frequent sequences or when the support threshold is not very low. Unfortunately, when long sequences are mined, or when a very low support threshold is used, the performance of such algorithms decreases dramatically and the number of frequent patterns increases sharply, resulting in too many meaningless and redundant patterns. Even worse, sometimes it is impossible to complete the algorithm execution due to a memory overflow.

One of the most interesting proposals to solve both problems are so called closed sequences [8], based on the same notion for regular frequent closed item-sets, as introduced by Pasquier et al. [4]. A frequent sequence is said to be closed, if there no exists a supersequence with the same support in the database. The final collection of closed sequences provides a much more simplified output, still keeping all the information about the frequency of each of the sequences. Some algorithms have been developed to find the complete set of closed sequences, where most of them are based on the Pattern Growh strategy [8,7].
 In this paper, we propose a new algorithm, called ClaSP (Closed Sequential Patterns algorithm) which exploits several efficient search space pruning meth-ods. Depending on the properties of the database, we argue about the desirability of using the vertical database format as compared to pattern growth techniques. We also show the suitability of the vertical database format in obtaining the frequent closed sequence set, and how, under some database configurations, a standard vertical database format algorithm can already be faster than Pattern Growth algorithms for closed sequences, by only adding a simple post-processing step. Experiments on both synthetic and real datasets show that ClaSP gener-ates the same complete closed sequences as CloSpan [8] but has much better performance figures.

The remaining of the paper is organized as follows. Section 2 introduces the preliminary concepts of frequent closed sequential pattern mining and the nota-tion used in the paper. In Section 3, we pre sent the most relevant related works. In Section 4, the pruning methods and ClaSP algorithm are presented. The per-formance study is presented in section 5 and, finally, we state our conclusions in section 6. Let I be a set of items. A set X = { e 1 ,e 2 ,...,e k } X  X  is called an itemset or k-itemsets if it contains k items. For simplicity, from now on we denote an itemset I as a concatenation of items between brackets. So, I 1 =( ab )and I 2 =( bc )are both two 2-itemsets. Also, without loss of generality, we assume the items in every itemset are represented in a lexicographic order.

A sequence s is a tuple s = I 1 I 2 ... I n with I i  X  X  ,and  X  i :1  X  i  X  n .We denote the size of a sequence | s | as the number of itemsets in that sequence. We denote the length of a sequence ( l = n i =1 | I i | ) as the number of items in it, and every sequence with k items is called a k -sequence. For instance, the sequence  X  = ( ab )( bc ) is a 4-sequence with a size of 2 itemsets.

We say  X  = I a 1 I a 2 ...I a n is a subsequence of another sequence  X  = I b 1 I b 2 ...I b m (or  X  is a supersequence of  X  ), denoted as  X   X  , if there exist integers 1  X  j and the order in the itemsets is preserved. Furthermore, the sequence ( b )( c ) is not a subsequence of ( abc ) .
 In the rest of the work, we use the terms pattern and sequence interchangeably. An input sequence is is a tuple is = id,s with id  X  N and s is a sequence. We call id the identifier of the input sequence. We say that an input sequence is contains another sequence  X  ,if  X  s .
A sequence database D is collection of input sequences D = s 1 s 2 ... s n , incrementally ordered by the identifier o f the contained sequences. In table 1 we show a sample input database D with four input sequences.
 Definition 1. The support (or frequency ) of a sequence, denoted as  X  (  X ,D ), is the total number of input sequences in the input database D that contain  X  .A pattern or sequence is called frequent if it occurs at least a given user specified threshold min sup , called the minimum support. FS is the whole collection of frequent sequences. The problem of frequent sequence mining is now to find FS in a given input database, for a given minimum support threshold.
 Given a sequence  X  = I 1 I 2 ...I n and an item e i , we define the s-extension  X  as the super-sequence of  X  , extending it with a new itemset containing a single last itemset I n of  X  = I 1 I 2 ...I n satisfies ( I n = I n  X  e i ). That is, the item e i is added to I n . For instance, given the sequence  X  = ( a )( b ) andanitem c  X  X  , the sequence  X  = ( a )( b )( c ) is an s-extension and  X  = ( a )( bc ) is an i-extension.

Given two sequences  X  and  X  such that both are s-extensions (or i-extensions) of a common prefix  X  ,withitems e i and e j respectively, we say  X  precedes  X  ,  X &lt; X  ,if e i &lt; lex e j in a lexicographic order. If, on the contrary, one of them is an s-extension and the other one is i-ext ension, the s-extension always precedes the i-extension.
 Definition 2. If a frequent sequence  X  does not have another supersequence with the same support, we say that  X  is a closed sequence. Otherwise, if a frequent sequence  X  has a super-sequence  X  with exactly the same support, we say that  X  is a non-closed sequence and  X  absorbs  X  . The whole set of frequent closed sequences is denoted by FCS . More formally,  X   X  X CS if  X   X   X  X S , X   X , X  (  X ,D ) =  X  (  X ,D ). The problem of closed sequence mining is now to find FCS in a given input database, for a given minimum support threshold.
 Clearly, the collection of frequent close d sequences is smalle r than the collection of all frequent sequences.
 Example 1. In our sample database, shown in table 1, for a support min sup = 2 , we find |FCS| =5 frequent closed sequences, FCS = { ( a ) , ( d )( a ) , ( a )( ab ) , ( a )( bc ) , ( a )( ab )( bc ) } , while the corresponding FS has 27 frequent sequences. Looking for frequent sequences in sequ ence databases was first proposed by Agrawal and Srikant [1,6]. Their algorithms (apriori-based) consist of execut-ing a continuous loop of a candidate generation phase followed by a support checking phase. Two main drawbacks appear in those algorithms: 1) they need to do several scans of the database to check the support of the candidates; and 2) a breath-first search is needed for the candidate generation, leading to high memory consumption.

Later, two other strategies were prop osed: 1) depth-first search based on a vertical database format [9] and 2) projected pattern growth [3,5]. The vertical database format strategy was created by Zaki in the Spade algorithm [9] which is capable of obtaining the frequent sequences without making several scans of the input database. His algorithm allows the decomposing of the original search tree in independent problems that can be solved in parallel in a depth-first search (DFS), thus enabling the processing of big databases.

The Pattern growth strategy was introduced by Han et al. [3] and it consists in algorithms that obtain the whole frequent sequence set by mean of techniques based on the so called projected pattern growth. The most representative algo-rithm in this strategy is PrefixSpan [5]. PrefixSpan defines a projected database as the set of suffixes with respect to a given prefix sequence. After projecting by a sequence, new frequent items are identified. This process is applied in a recursive manner by means of DFS, identifying the new frequent sequences as the concatenation of the prefix sequence with the frequent items that are found.
Prefixspan shows good performance and scales well in memory, especially with sparse databases or when databases mainly consist of small itemsets. However, when we deal with large dense databases that have large itemsets, the perfor-mance of Prefixpan is worse than that of Spade. In order to show this issue, we have conducted several tests. In a sequential database, several important prop-erties have an influence on the algorithms execution, some of which are shown in Table 2.

We define the database density as the quotient  X  = T N .Wehaveusedthe well-known data generator provided by IBM to run Spade and PrefixSpan un-der different configurations. In Figures 1 and 2 we can observe the behaviour of both Spade and Prefixspan when we vary the density and the number of itemsets. In figure 1 we show the running time of the algorithms with a different number of items (100, 500 and 2500 items) with a constant T =20value.Sincefora database, the density grows either if the numerator increases or the denominator decreases, the figures have been obtained just varying the denominator. Besides, figure 2 depicts the behaviour of the algorithms when the number of itemsets is changed between values of C  X  X  10 , 40 , 80 } while we keep the density constant (  X  = 20 2500 ). The higher  X  the more dense the database. We can see that PrefixS-pan shows good results when both density and the number of itemsets are low, but when a database is denser and parameter C grows, we notice how Spade outperforms Prefixspan.

For mining closed sequences, there exist two approaches: 1) run any algorithm for mining all frequent sequences and execute a post-processing step to filter out the set of closed sequences, or 2) obtain t he set of closed sequences by gradually discarding the non-closed ones. Some algorithms have been developed to find the complete set of closed sequences. The mo st important algorithms developed so far, are CloSpan [8] and Bide [7], both derived from Prefixspan. While CloSpan uses a prefix tree to store the sequences and uses two methods to prune non-frequent sequences, Bide executes some c hecking steps in the original database that allows it to avoid maintaining the sequence tree in memory. However, to the best of our knowledge, there exist no algorithms for closed sequence mining based on the vertical database format as is presented here.
 In this section, we formulate and explain every step of our ClaSP algorithm. ClaSP has two main phases: The first one generates a subset of FS (and superset of
FCS ) called Frequent Closed Candidates ( FCC ), that is kept in main memory; and the second step executes a post-pruning phase to eliminate from FCC all non-closed sequences to finally obtain exactly FCS .
 Algorithm 1. ClaSP
Algorithm 1, ClaSP , shows the pseudocode corresponding to the two main steps. It first finds every frequent 1-sequence, and after that, for all of frequent 1-sequences, the method DFS-Pruning is called recursively to explore the cor-responding subtree (by doing a depth-first search). FCC is obtained when this process is done for all of the frequent 1-sequences and, finally, the algorithm ends removing the non-closed sequences that appear in FCC .

Algorithm 2, DFS-Pruning , executes recursively both the candidate genera-tion (by means of i-extensions and s-extensions) and the support checking, re-turningapartof FCC relative to the pattern p taken as parameter. The method takes as parameters two sets with the candidate items to do s-extensions and i-extensions respectively ( S n and I n sets). The algorithm first checks if the current pattern p can be discarded, by using the method checkAvoidable (this algorithm is explained later in algorithm 5). Lines 4-9 perform all the s-extensions for the pattern p and keep in S temp the items which make frequent extensions. In line 10, the method ExpSiblings (algorithm 4) is called, and there, DFS-Pruning is executed for each new frequent s-exten sions. Lines 11-16 and 17 perform the same steps, with i-extensions. Finally, in line 19, the complete frequent patterns set (with a prefix p ) is returned.

To store the patterns in memory, we use a lexicographic sequence tree. The elements in the tree are sorted by a lexic ographic order acco rding to extension comparisons (see section 2 ). In figure 3 we show the associated sequence tree for FS in our example and we denote an s-extension with a line, and an i-extension with a dotted line. This tree is traversed by algorithms 1, 2 and 4, using a depth-first traversal.

There are two main different changes added in ClaSP with respect to SPADE: (1) the step to check if the subtree of a pattern can be skipped (line 3 of algorithm 2), and (2) the step where the remaining non-closed patterns are eliminated (line 6 of algorithm 1).
To prune the space search, ClaSP used the method CheckAvoidable that is inspired on the pruning methods used in CloSpan. This method tries to find those patterns p =  X e j and p =  X e i e j , such that, all of the appearances of p are in those of p , i.e., if every time we find a sequence  X  followed by an item e j , there exists an item e i between them, then we can safely avoid the exploration of the subtree associated to the pattern p . In order to find this kind of patterns, we define two numbers: 1) l ( s,p ), is the size of all the suffixes with respect to p in sequence s ,and2) I ( D p )= n i =1 l ( s i ,p ), the total number of remaining items with respect to p for the database D , i.e. the addition of all of l ( s,p ) for every sequence in the database. Using I ( D ) and the subsequence checking operation, in algorithm 5, ClaSP checks the equivalence between the I ( D ) values for two patterns: Given two sequences, s and s , such that s s ,if I ( D s )= I ( D s ), we can deduce that the support for all of their descendants is just the same.
In algorithm 5, the pruning phase is implemented by two methods: 1) Back-ward sub-pattern checking and 2) Backward super-pattern checking. The first one (lines 8-10) occurs when we find a pattern which is a subsequence of a pat-tern previously found with the same I ( D ) value. In that case, we can avoid exploring this new branch in the tree for this new pattern. The second method (lines 12-16 and 20-24) is the opposite situation and it occurs when we find a pattern that is a super-sequence of another pattern previously found with the same I ( D ) value. In this case we can transplant the descendants of the previous pattern to the node of this new pattern.

In figure 4 we show the ClaSP search tree w.r.t. our example without all pruned nodes. In our implementation, to store the relevant branches, we define a hash function with I ( D ) value as key and the pattern (i.e. the node in the tree forthatpattern)asvalue( I ( D p ) ,p ). We use a global hash table and, when we find a sequence p , if the backward sub-pattern condition is accomplished, we do not put the pair I ( D p ) ,p , whereas, if the backward super-pattern condition is true, we replace all the previous pairs I ( D p ) ,p (s.t. p p ) with the new one I ( D p ) ,p . If instead we do not find any pattern with the same I ( D )value of the pattern p , or those patterns with the same value are not related with p by means of the subsequence operation, we put the pair I ( D p ) ,p to the global hash table (line 28). Note that when one of the two pruning conditions is true, we also need to check if the support for s and s is the same since two I ( D p )and I (
D We also need to consider all of the I ( D ) s for every appearance in a sequence. For instance, in our example shown in table 1, regarding the three first sequences, if we consider just the first I ( D s ) for the first appearance, if we have the pattern value I ( D ) = 5), so we can avoid generating the descendants of ( a )( b ) because the are the same as in ( a )( ab ) . However, we can check that ( a )( bc ) is frequent (with support 3), whereas ( a )( abc ) is not (support 1). This forces us to count in
I ( D
Finally, regarding the non-closed pattern elimination (algorithm 3), the pro-cess consists of using a hash function with the support of a pattern as key and the pattern itself as value. If two patterns have the same support we check if one contains the other, and if this condition is satisfied, we remove the shorter pattern. Since the support value as key provoke a high number of collisions in the hash table (implemented with closed addressing), we use a T ( D p )= n i =1 id ( s i ) value, defined as the sum of all sequence ids where a pattern p appears. How-ever, as the equivalence of T ( D p ) does not imply the equivalence of support, after checking that two patterns have the same T ( D p ) value, those patterns have to have the same support to remove one of them. We exhaustively experimented on both synthetic and real world datasets. To generate the synthetic data, we have used the IBM data generator mentioned above (see section 3). In all our experiments we compare the performance of three algorithms: CloSpan, ClaSP and Spade. For the last algorithm we add the same non-closed candidate elimination phase which is used in ClaSP to obtain FCS .
 All experiments are done on a 4-cores of 2.4GHZ Intel Xeon, running Linux Ubuntu 10.04 Server edition. All the three algorithms are implemented in Java 6 SE with a Java Virtual Machine of 16GB of main memory.
 Figure 5 shows the number of patterns and performance for the dataset D5C10T5N5S6I4 (-rept 1 -seq.npats 2000 -lit.npats 5000). Figure 5(a) shows the number of frequent patterns, the patterns processed by ClaSP, and the number of closed patterns. We can see how there is approximately an order of difference between these numbers, i.e. for every 100 frequent patterns we process around 10 patterns by ClaSP, and approximately only 1 of these 10 patterns is closed. Figure 5(b) shows the running time. ClaSP clearly outperforms both Spade and Clospan. For very low support (below 0.013), we have problems with the execu-tion of Spade due to the space in memory taken for the algorithm.

Figure 6 shows a dataset with larger parameters of C, T and a lower N. This database is denser than the database above and in figure 6(a) we can observe that the difference between frequent patterns and processed patterns is not so big. Therefore, the pruning method chec kAvoidable is not so effective and ClaSP and CloSpan are closer to the normal behavior of Spade and PrefixSpan, as is shown in figure 6(b). The results show that both ClaSP and Spade are much faster than Clospan.

Finally we test our algorithm with the gazelle dataset. This dataset comes from click-stream data from gazelle.com , which no longer exists. The dataset was once used in KDDCup-2000 competition and, basically, it includes a set of page views (each page contains a specific product information) in a legwear and legcare website. Each session contains page views done by a customer over a short period. Product pages viewed in one session are considered as an itemset, and different sessions for one user is considered as a sequence. The database contains 1423 different products and assortments which are viewed by 29369 different users. There are 29369 sequences, 35722 sessions (itemsets), and 87546 page views (items). The average number of se ssions in a sequence is around 1. The average number of pageviews in a session is 2. The largest session contains 342 views, the longest sequence has 140 sessi ons, and the largest sequence contains 651 page views. Figure 7 shows the runtime with several support (from 0.03% to 0.015%). We compare the runtime behavior for both ClaSP and CloSpan and we can see how ClaSP outperforms CloSpan.

All the experiments show that ClaSP outperforms CloSpan, even if databases are sparse. This is because of, in very sparse databases, a high number of patterns are found only with extremely low suppor t. Therefore, the lower the support is, the more patterns are found and the more items are chosen to create patterns. In this point, CloSpan, as PrefixSpan, is penalized since the algorithm has to projects several times the same item in the same sequence, having a worse time with respect to ClaSP. Besides, in those algorithms where Spade is better than Prefixspan, ClaSP is also faster than CloSpan. In this paper, we study the principles for mining closed frequent sequential pat-terns and we compare the two main existing strategies to find frequent patterns. We show the benefits of using the vertical database format strategy against pattern growth algorithms, especially wh en facing dense datasets. Then, we in-troduced a new algorithm called ClaSP to mine frequent closed sequences. This algorithm is inspired on the Spade algorithm using a vertical database format strategy and uses a heuristic to prune non-closed sequences inspired by the CloSpan algorithm. To the best of our knowledge, this is the first work based on the vertical database strategy to solve the closed sequential pattern mining problem. In all our tests ClaSP outperforms CloSpan, and even, under certain datasets configuration, Spade also outperforms CloSpan when the non-closed elimination phase is executed after it.

