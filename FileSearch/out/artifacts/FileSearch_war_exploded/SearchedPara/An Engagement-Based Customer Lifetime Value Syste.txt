 Acomprehensiveunderstandingofindividualcustomervalue is crucial to any successful customer relationship manage-ment strategy. It is also the key to building products for long-term value returns. Modeling customer lifetime value (CLTV) can be fraught with technical di ffi culties, however, due to both the noisy nature of user-level behavior and the potentially large customer base. Here we describe a new CLTV system that solves these problems. This was built at Groupon, a large global e-commerce company, where confronting the unique challenges of local commerce means quickly iterating on new products and the optimal inventory to appeal to a wide and diverse audience. Given current purchaser frequency we need a faster way to determine the health of individual customers, and given finite resources we need to know where to focus our energy.

Our CLTV system predicts future value on an individual user basis with a random forest model which includes fea-tures that account for nearly all aspects of each customer X  X  relationship with our platform. This feature set includes those quantifying engagement via email and our mobile app, which give us the ability to predict changes in value far more quickly than models based solely on purchase behavior. We further model di ff erent customer types, such as one-time buyers and power users, separately so as to allow for dif-ferent feature weights and to enhance the interpretability of our results. Additionally, we developed an economical scor-ing framework wherein we re-score a user when any trigger events occur and apply a decay function otherwise, to en-able frequent scoring of a large customer base with a complex model. This system is deployed, predicting the value of hun-dreds of millions of users on a daily cadence, and is actively being used across our products and business initiatives.  X  This work was done when the author was at Groupon. Customer Lifetime Value, E-commerce, Random Forests
Groupon 1 is a large global e-commerce company, oper-ating via the web and the popular Groupon Mobile App. Currently serving more than 30 countries and 500 markets, Groupon is the place you start when you want to buy just about anything, anytime, anywhere. We o ff er physical mer-chandise through our Goods business, travel deals through our Getaways business, and we are a market leader in Local commerce. As of Q3 of 2015, we have nearly 50 million ac-tive customers worldwide, more than one million merchants connected through our suite of online tools, and over 900 million units sold. At Groupon we are trying to develop a robust marketplace, and as such we need to understand at an individual level the supply and service needed to develop adailyhabitforourcustomers.Howdoesfeaturingthelo-cal burger place down the block compare to featuring a big chain when it comes to increasing a user X  X  future spending? Given the number of local choices a customer has, how many Groupon options do we provide to promote a daily habit? What is the added value of providing X  X hite glove X  X ustomer service?
Answering such questions requires a comprehensive un-derstanding of user-level value and how it is changing with time. Accordingly, in this paper we tackle the problem of modeling and monitoring customer lifetime value (CLTV)  X  X predictionofthenetdollarvalueattributedtoourfu-ture relationship with each individual customer. It is rel-atively easy to predict aggregate business measures for fi-nancial purposes. Accurately modeling the future value of individual users, however, is a far more di ffi cult task, as users spend di ff ering amounts of time on product research and have various engagement levels prior to a final purchase decision. Nonetheless, accurate CLTV predictions can have enormous value for an e-commerce company like Groupon. The historically most common use cases for such a system pertain to marketing and customer relationship management (CRM). With CLTV predictions it is straightforward to find the highest value customers, customers most likely to attrite, and customers most likely to make their first purchase. As http://www.groupon.com such one can determine the ideal target audiences for pro-motional o ff ers, personalized customer messaging, exclusive deals, rewards programs, and customer service treatment.
In addition to the huge marketing value, CLTV predic-tions are generally useful as a powerful performance metric in all areas of the business. For example, in product experi-mentation we can use CLTV scores to segment users and to build for longer term value returns. We can identify causal versus correlated relationships of certain behaviors with user value by encouraging and incentivising said behaviors and then measuring outcomes. We can also use these predictions in personalization, to tune results to di ff erent stages in the customer lifecycle, and for user behavior analysis. How does CLTV change based on o ff ering (e.g. Goods versus Local) or as a result of specific actions? How does the adoption of anewplatformchangeauser X  X valueasafunctionoftime? We can also use CLTV predictions for business unit goal setting; an example of such a goal might be for the CRM team to increase the value of previously one-time buyers by X %foragivenquarter.

Here at Groupon we have developed a new methodol-ogy for the modeling and daily tracking of individual cus-tomer value. Our system is novel in several ways, including: 1. we engineer features that quantify the level of engage-ment that each customer has with us on each of our plat-forms, 2. we build highly accurate two-stage random forest models, 3. we use separate models for di ff erent types of pur-chasers, e.g. one-time buyers versus very active users, and 4. we re-score a user when any trigger events occur and ap-ply a decay function otherwise. Using engagement-based features allows us to detect any changes in value, including attrition, far more quickly than we could otherwise. We have found random forest models to have the best performance over other machine learning frameworks, and modeling dif-ferent customer types separately allows us to have di ff erent feature weights for each while simultaneously overcoming interpretability issues by giving business units a straightfor-ward way to discover and track the most important features for each group. Employing a decay function allows us to ef-ficiently update our scores on a daily cadence despite having averycomplexmodel,leadingtoafactorof20speedupin overall run time. By implement ing these model features we are able to achieve Spearman correlations between actual and predicted customer values of 0.53 and 0.77 for quar-terly and yearly timeframes, respectively, showing moderate to high correlations at a very high statistical significance ( p&lt; 0 . 0001).

In what follows we present our CLTV system, which pre-dicts customer value for three rolling time windows that we will denote as  X  X hort, X   X  X edium, X  and  X  X ong, X  that are on the order of a quarter to a year. We use a proxy measure for customer value that we will call  X  X urchase value X  that is based on purchasing behavior, focusing on this behavior in-stead of profit because the latter is subject to margins that can fluctuate and that are not related to user intent. We focus on value over these three time windows as opposed to full lifetime value; given the age of Groupon, a one-year pur-chase prediction is a significant portion of the overall average customer life as of today. There is not enough training data to make accurate predictions with a longer-term model but the future intent is to add longer time periods as a repre-sentative core customer base matures and the data becomes available. Hereafter and we will often refer to these values as the user X  X   X  X cores. X  The system described herein is currently deployed in production and successfully running at scale, scoring hundreds of m illions of users on a daily cadence and being used by more than half a dozen business and product use cases.
This work is related to a few di ff erent areas in the mar-keting and machine learning literature, most importantly CLTV, CRM, and random forests.
Gupta et al. [9] provide a comprehensive review of di ff er-ent CLTV methodologies, relating them to what they state as the three phases of the relationship between a company and a customer: acquisition, retention, and expansion. They review six basic types of modeling approaches, including the historically popular RFM and related models that deal with the recency, frequency, and value of each customer X  X  pur-chase history. They present evidence from the literature that machine learned prediction models like the one pre-sented below are superior because they can incorporate a variety of additional variables, and they mention that ran-dom forest models had historically performed well in churn prediction competitions. See also [12, 8, 17] for further re-view on the history behind CLTV prediction. In all of the CLTV literature there is the argument that, rather than fo-cusing on the short-term, CLTV modeling allows us to focus on long-term profitability [19, 2].

There are various methodologies that have been used to model CLTV in a computer science framework, including optimization [7], SVM [6], quantile regression [3], and port-folio analysis [5]. In a 2009 CLTV prediction competition [14], several di ff erent frameworks for predicting CLTV on an individual and an aggregate basis were tested against each other. We find that a new approach  X  two-stage random for-est models for each of several user behavior segments, with a comprehensive feature set such as what we have engineered here  X  performs better than any of the competition entrants. This methodology allows us to incorporate features related to user engagement with our platform, which allow us to quickly detect changes in customer value.
The idea that we should invest resources into analyzing customer behavior and maintaining a positive relationship with our customers is nothing new. For example, Kotler and Keller [11] showed that the costs associated with obtaining new customers can be five times larger than the costs as-sociated with maintaining a good relationship with existing customers. A thorough literature review of various subjects related to CRM, including CLTV modeling techniques, can be found in [16]. The need to fully understand our cus-tomers so that we can maintain these relationships is one of the strongest motivations for accurate customer lifetime value monitoring. This is the main impetus to build a reli-able monitoring system at Groupon.
Random forests is an ensemble learning method that works by producing many random decision trees and then boot-strap aggregating (or  X  X agging X ) the results to turn an en-semble of weak learners into a strong one that automatically avoids overfitting. The methodology was developed by Leo Breiman and Adele Cutler [4]. We make use of the func-tionality in R [13] and the H2O package [1] 2 .H2Oisa scalable machine learning framework that supports many of the standard algorithms, including random forests.
Our model uses over 40 features, which provide a com-prehensive view of each user X  X  demographics, general pur-chasing behavior, engagement, and overall relationship with Groupon. Here we describe each type of feature in turn. AdiscussionofrelativefeatureimportanceisinSection6 below.
The most important features that we include are those that characterize user engagement with our product. These rich features allow us to detect changes in customer value far sooner than we could if we were only relying on purchase history data. In this first version of our model we include engagement scores from email and the mobile app, currently two of our most important sources of tra ffi c. There are var-ious key ways in which users interact with each of these platforms, for example with opens and clicks for email and with deal impressions and searches on the app. We coa-lesce these behaviors into one composite score for each plat-form by weighting a quantification of each behavior (e.g. the number of email clicks) by a numerical factor related to the historical conversion coming from that behavior. The final score is then the weighted sum. For example, where N open is the number of email opens,  X  is the corre-sponding historical conversion rate (average orders per email open), N click is the number of email clicks, and  X  is the corre-sponding historical conversion rate (average orders per email click, given an email open). Note that most emails that are opened are not clicked on, and that a single email can be clicked on multiple times. Thus we consider each behav-ior (opening and clicking) separately, using the appropriate conversion rates to avoid double counting. We further track email unsubscription and mobile app downloads, and define features related to each.
There are many ways in which the Groupon user experi-ence is correlated with future customer value. Firstly, the quantity of available nearby deals can play a large role, es-pecially for newer and re-engaging users. We quantify this local supply by counting the number of deals within a repre-sentative radius of the user X  X  home location. We also deter-mine the typical purchase volume in their geographical area, which acts as a proxy for local Groupon brand recognition.
Another key part of the user experience is related to cus-tomer service. We track numbers of refunds and customer service tickets, customer service phone and email wait times, and whether the user has given positive or negative responses to post-interaction surveys. We also track average shipping times for our Goods (physical merchandise) business. http://www.h2o.ai
Virtually all CLTV models use purchase history as a key variable, and ours is no exception. We have features relating to purchase value on a variety of historical timescales, along with the number of days since the most recent purchase. We also include Goods versus Local preferences and whether or not those preferences are changing as a function of time. Also redemption behavior can be important, and as such we include as features the typical time between purchase and redemption and the number of unredeemed vouchers that the user is currently holding on to. We further track how much each user is predisposed to using discount codes for their purchases or for purchasing  X  X oss leader X  deals.
In addition to the aforementioned features we also use basic demographic variables such as gender, age, and loca-tion, including city size and the distance between home and city center. We also track the circumstances regarding their original subscription and first purchase, including the cohort years for each.
The features in the previous section are used by our se-ries of CLTV models. We train models for each combina-tion of our six user segments and our three time windows  X  X hort,medium,andlong. Eachmodelisretrainedona quarterly basis due to the relatively young age of our busi-ness and the fast pace of business changes. In this section, we describe our user segments, our two-stage random forest model framework, and our empirical seasonality and decay function models. The design of the overall system that calls upon these models is discussed in Section 5 below.
Of our 40+ features described above, some will invariably be more important predictors of future purchasing behavior than others. However, which variables are most important will be di ff erent for di ff erent types of customers. For ex-ample, for a user who purchases very frequently we may be able to accurately predict future purchasing behavior largely from previous purchasing behavior. On the contrary, for a user who has not yet purchased we will have to rely en-tirely on other (non-purchase history) variables. Thus we divide our users into six  X  X urchase cohorts X  based on past purchase frequency. Each of these purchase cohorts then is modeled separately from the others to allow for di ff ering feature weights for each.

These cohorts are determined from the past five quarters of data, excluding travel and other outlier price point pur-chases. The total number of cohorts is six, which was de-termined based on a balance between model accuracy and re-training time. The time window for cohort determination (five quarters) was determined based on a balance between having enough data to detect a pattern but also allowing for the possibility that users may change their purchasing behavior over their lifetimes.

In this paper, we will focus on a subset of our purchase cohorts, defined and named as follows: The specific thresholds for what constitutes a sporadic buyer versus a power user versus any other cohort will vary for di ff erent businesses that have di ff erent typical purchase fre-quencies. We choose thresholds that are optimized for Grou-pon. Note also that the cohorts developed here are op-timized for the purposes of understanding user segments, modeling individual user behavior, and informing product and marketing e ff orts. These do not reflect the cohort strat-egy used to assess overall business performance. These sepa-rate concerns require very di ff erent segmentation strategies.
For each purchase cohort and time window, we use random forest models in R, using the H2O [1] package. In addition to previous studies finding random forests to be superior for CLTV modeling [9], we further found it to be the most ac-curate from a series of experiments where we tested various algorithms, including rpart [18] and SVM [15], and various normalizations and parameter tunings. We found that a lower mtry (the number of features randomly sampled as candidates at each split) and a higher ntree (the number of trees to generate) performed the best for our purposes here. Performance was judged based on a combination of RMSE and Pearson and Spearman correlations between actual and predicted values for our cross-validation data sets.
There are significant benefits and disadvantages of using random forests in this application. The advantages include allowing for missing values, accommodating many nonlin-ear features (e.g. the number of days since the most recent purchase), and generally better model performance includ-ing avoiding overfitting. The main disadvantage relates to interpretability because the result is an average over many decision trees. We overcome this disadvantage by modeling each of our purchase cohorts on its own to extract the most important features. Then we create cohort metrics for our end business partners (marketing, customer service, etc.) so that they can act on these metri cs as they change with time. For example, we find that one of the most important features for predicting the future value of our power users is the typ-ical number of days from purchase to redemption. Accord-ingly we track redemption time and CLTV as we test new redemption-related products, such as redemption reminder emails and expired voucher trade-in programs.

Our model has two stages, each using random forests: 1. Predict purchase propensity, a binary classification for 2. Predict the dollar value for users who were predicted
We use two stages because the data sets tend to be highly imbalanced; for example power users are far more likely to purchase than not, whereas the opposite is true for one-time buyers. Thus for stage 1 we downsample the majority class to get a 50-50 training set. The full combined training sets contain 40,000 samples for each combination of purchase cohort and time window. The classification cuto ff has been optimized for each purchase cohort to minimize the bias in the number of purchasers and their overall value. We build separate two-stage models to predict future values for each of the short, medium, and long time windows for each cohort.
We employ two additional models which are both built empirically from historical data. Firstly, since our train-ing data is necessarily on at least a quarterly lag, we need to make quarter-to-quarter seasonality adjustments. These adjustment factors are predicted from historical timeseries data for each purchase cohort separately and are re-calculated every quarter.

The second empirical model that we use is related to our scoring procedure. In order to get the most out of a CLTV monitoring system, we require re-scoring on a relatively fast cadence. Ideally we would like daily updates to capture sudden changes in value due to any purchases ,engagement, customer service interactions, etc. However, in a large global business such as Groupon we have potentially hundreds of millions of users to score. Doing so with an extensive six-cohort two-stage random forest model would be computa-tionally very expensive. We reduce the computation cost by leveraging our knowledge of customer behavioral patterns.
We note that the value of users who go without any inter-actions with Groupon, including any engagement, decays in averypredictablemannerinlinewithFigure1. Wemea-sure this decay function as a multiplicative factor, which is afunctionofthenumberofdayssincethelastinteraction. We calculate this function from historical time-series data and re-calculate it at the start of every quarter. We then use this function as a part of the daily scoring procedure: 1. Find users who had any  X  X rigger events, X  which we de-2. For triggered users, run their updated feature set through 3. For users without any trigger events, multiply their Trigger events include purchases, any interactions with email or the mobile app, and any customer service interactions including calls, emails, and refunds.

With this scoring procedure we reduce our daily scoring time by a factor of 20, a savings of 95%. This time quickly adds up. In this way we are able to update each user X  X  predicted values for each of the short, medium, and long time windows on a daily cadence, with each of these time windows on a rolling basis. For instance, on September 1st, 2015, a quarterly prediction gave the expected purchase value for the current date through the current date plus 90 days, or November 30th. Similarly a one-year prediction gave the expected value until September 1st of 2016.
We show a high-level overview of the Groupon CLTV sys-tem in Figure 2. The computation and collection of the feature sets is done within Groupon X  X  enterprise data ware-house. We additionally assign purchase cohorts and build 0.2 0.4 0.6 0.8 1.0 Normalized average purchase value Figure 1: How average quarterly user value decays as a function of the number of days since the last interaction ( X  X rigger event X ), shown for various com-binations of purchase cohort and first purchase year cohort. All customer values have been normalized by a constant factor such that the average value for 2013 power users is equal to one.
 Figure 2: High-level overview of our CLTV system. Timeseries models and features are built within our enterprise data warehouse, then R and H2O [1] are used to build random forest models and calculate user values. Results are stored in our Hadoop clus-ter, joined with engagement score predictions, and then ported into our internal marketing system for easy access by our business partners. our timeseries models within this database. Models are built and users are scored using R in conjunction with the H2O package [1] on a server with 44 Gb of RAM and 12 CPU cores. We then store the results in our Hadoop cluster, com-bine the results with engagement score predictions (from aseparate,independentmodel),andporteverythingtoa Groupon internal marketing system that allows our busi-ness partners to segment users for personalized marketing campaigns. Several secondary scores are also calculated, in-cluding a predicted percentage value change that compares the past quarter to the future quarter, and an associated binary 0/1 decline alert flag. Feedback from our business partners, typically in the form of feature requests and up-dates, is fed back into the data model.

We split the CLTV scoring process into a daily process and aquarterlyprocessasdescribedinthesubsectionsbelow. The quarterly process runs on the first day of each quarter (January 1st, April 1st, July 1st, and October 1st) while the daily process runs on all other days of the year. Both processes have built-in accuracy monitoring and alerts at each stage of each process.
On a quarterly basis we complete the following steps: 1. Assign purchase cohorts for each user based on the 2. Compute seasonality functions for each purchase co-3. Compute decay functions for each purchase cohort and 4. Harvest new training data sets and retrain all models 5. Calculate all new CLTV scores for everyone with either 6. Everyone without either a purchase in the past six
The requirements to receive a new score (versus an auto-matic score of zero value) were determined based on exper-imentation on the level of inactivity required to be certain of zero value according to our models.
On a daily basis (excluding the first day of the new quar-ter) we complete the following steps: 1. Update the feature set for all users. 2. Find users with trigger events since the last scoring, 3. Re-score all triggered users for each time window. 4. Apply the decay functions to the scores for all users
As mentioned above, all of these time windows  X  short, medium, and long  X  are on a rolling basis and each set of scores is thus updated every night when the CLTV system runs. We are currently building alerts to monitor model drift.
The system described herein has been successfully imple-mented at large scale, scori ng hundreds of m illions of users on a daily cadence. The system has been stable for sev-eral months and takes approximately four hours to run each night. In this section we review 1. the performance of our model using test data, 2. various measures from Q3 of 2015, and 3. the most important features for each stage of our model for our di ff erent user segments. In the results below, all metrics are for large random subsets of users and are purely indicative of model performance. Whenever the time window (short, medium, or long) is not specified it is to be assumed that we are defaulting to the short time period; when that is the case then it can further be assumed that the results are similar for the longer time windows.
For all of our model experiments we trained with features as of the end of Q1 of 2014, thereby predicting customer values starting from the beginning of Q2 of 2014. We used out-of-time cross-validation data sets that were shifted one quarter later, such that we were predicting values starting from the beginning of Q3 of 2014. We use data from 2014 (as opposed to 2015) in order to have enough data to test the results for our long time window. For each new model exper-iment we drew new random subsamples of the population to create new random training and cross-validation data sets. Each of our cross-validation and testing data sets contained 10,000 samples from each purchase cohort. We evaluate the performance of Stage 1 based on accuracy, precision, recall, and false positive rate. We evaluate the performance of the overall (stage 1 and 2) model based on Pearson and Spear-man correlation coe ffi cients, RMSE, bias in the averages, and a comparison of actual versus predicted distributions.
In the first stage of our model we predict a binary yes/no response for whether each user will make a purchase in the time window of interest. In Figure 3 we show the ROC curves  X  the true positive rate versus the false positive rate as we vary the cuto ff parameter for the binary classification scheme  X  for each purchase cohort. In this figure the cut-o ff ranges from 0.2 to 0.8. The shallowness of these curves reflects the inherent di ffi culty in predicting individual user behavior. The final cuto ff parameters are chosen so as to minimize the bias in predicting the total number of pur-chasers for each cohort.

In Table 1 we show the resulting accuracy, precision, re-call, and false positive rate for each purchase cohort for this stage 1 model. The X  X verall X  X umbers are for all cohorts com-bined, with equal numbers of users in each cohort X  X  sample. Note in particular the very high recall and low false posi-tive rate for users who have never purchased, and the very high false positive rate for power users. It is easy to predict when an inactive user will make their first purchase, but it is more di ffi cult to predict when a previously very active user will drop o ff . This is because we tend to have more advance warning for the former, in the form of increased engagement prior to purchase. In the latter case (when a previously very active user disengages), the shift is typically very sud-den. However, we find that a majority of these dropped-o ff users will return in the following quarter, which means that they will be receptive to CRM e ff orts once we have enough 0.2 0.4 0.6 0.8 1.0 True Positive Rate Figure 3: The ROC curves for our stage 1 model for each purchase cohort, showing the true positive rate versus the false positive rate as we vary the classification cuto ff parameter from 0.2 to 0.8. Table 1: The accuracy, precision, recall, and false positive rate (FPR) for each purchase cohort and overall for our stage 1 model. All results are per-centages.
 One-time buyers 95 37 32 2 Sporadic buyers 81 41 37 10 engagement data to detect the drop-o ff . Note also that it is very di ffi cult to predict the purchases of sporadic buy-ers because this is the group with the noisiest purchasing behavior.
After we determine a binary classification for which users are predicted to purchase, we then predict the purchase value for the positives and assign a zero value to the neg-atives to achieve our final results. In Table 2 we show the Pearson and Spearman correlation coe ffi cients between ac-tual and predicted values at the end of this process, for each combination of purchase cohort and time window. All of these show a moderate to high level of correlation [10], all with very high statistical significance ( p&lt; 0 . 0001). Typi-cally we find that our accuracy measures tend to improve as the time period is increased. This is because purchase value per user is still a rather noisy measure for the short timeframe for this dataset.

We present the RMSE for each combination of purchase cohort and time window in Table 3. These values are given in units of the average actual purchase value for each com-bination in question. Once again we see that longer time windows and more active customers are easier to predict, be-cause in these situations we have more data and less noise; the most di ffi cult prediction by far is for the unactivated group over the short time window, whereas the easiest pre-Table 3: RMSE for each cohort for each time win-dow, presented in units of the average actual pur-chase value for each case.
 diction is for the power users over the long time window. For reference, the typical user in our data set purchases 1-2timesovertheshorttimewindow,withtheunactivated users purchasing 0.008 times, the sporadic users purchasing 0.3 times, and the power users purchasing 7 times on aver-age. Our overall results outperform the winner of the 2009 CLTV modeling competition presented in Ref [14].

Because our model was tuned to avoid a systematic bias, we find that the actual and predicted numbers of purchasers are very close, and accordingly that our systematic bias in average purchase value per user is low in each purchase co-hort. Overall the bias in average value is less than 1%. This general lack of a systematic bias was verified with testing on a sample to predict Q1 of 2015 (thereby including the confounding factor of the holiday spike).

We further find a good concordance between the actual and predicted distributions of spend per user, as shown in Figure 4. The distribution of actual spend is somewhat more spread out  X  having both more low-and high-spend users  X  than the distribution of predicted spend. In particular it is impossible to predict the long tail of very rare high-value purchases. We also find that we are predicting a reduced number of very low-spend users. This is due to our two-stage model, in which these users are typically classified as non-purchasers in stage 1 and thus given an automatic zero value in stage 2. Testing the use of the probability (as opposed to abinaryclassification)instage1tomitigatethise ff ectis the subject of future work.
This system went live at the start of Q3 of 2015. At the end of the quarter we completed additional validation tests to determine how useful our results were for Q3. Firstly we re-computed the metrics detailed above to successfully con-firm that all of our performance metrics were in line with our initial model testing. These computations are automat-ically checked at the end of every quarter. In particular we find that the correlation coe ffi cients for our entire Q3 cus-tomer base are indeed consistent with what we found from Figure 4: A comparison of the distributions of the actual (top) and the predicted (bottom) purchase value for the short time window, showing logs of counts for clarity. The horizontal and vertical scales are the same for both top and bottom panels and financial information is obfuscated. our original out-of-time 2014 testing sets. Below we discuss afewadditionalmetrics,allforarandomsubsetofusers from Q3 of 2015.

Comparing the top 10% to the bottom, we find that users who were predicted to be in the top 10% ended up spending on average 14 times more than users who were predicted to be in the bottom 90%, and 20 times more than those pre-dicted to be in the bottom 10%. We further find that users who were predicted to decline in their value ended up having on average a 55% decline and users who were predicted to increase in their value ended up having on average a 62% increase.

Furthermore when we predicted that someone would at-trite in Q3 of 2015, we ended up being correct 72% of the time. These attriting customers are naturally grouped ac-cording to their purchase cohort, which informs our mar-keting teams as they design CRM campaigns to keep these customers in Q4. We further check the feature set for each user predicted to attrite to find if there are any specific rea-sons for the disengagement, such as a slow shipping time or an expired voucher. Specific actions include sending promo-tions for general disengagement and initiating customer ser-vice treatment when a disengagement trigger has been iden-tified. Then by tracking CLTV pre-and post-intervention, we determine the e ffi cacy of these di ff erent strategies.
We use these predictions to analyze user behavior and to 0.2 0.4 0.6 0.8 1.0 Normalized average purchase value  X  30  X  10 10 30 Figure 5: Average purchase value for each purchase cohort for the short time window as a function of the number of days since an unsubscription from emails (left) and a download of the mobile app (right). All customer values have been normalized by a constant factor, and the horizontal and vertical scales are the same for each panel. Power users have been left out because their value is much higher, but their behavior is qualitatively the same. determine the value changes stemming from various lifecycle events as a function of time. We show two examples in Figure 5. Following an unsubscription from all emails we find a steady decline in predicted value due to a reduction in engagement. On the other hand, a download of our mobile app dramatically increases predicted value, due both to the download itself (a feature in the CLTV model) and due to the increase in app engagement. These behaviors are in line with what we have seen from historical data.

These CLTV predictions are so far being used at Groupon by several of our business units, including our marketing team which uses them for CRM campaigns to find the high-est value customers and the customers most likely to attrite. For the sake of illustration we show some examples of the sorts of user segmenting that can be done in Figure 6 with asmallsamplingofdata(howweemploythesepredictions in practice for user segmenting is outside the scope of this paper). For example, we can use these results to determine which users would benefit most from being sent promotional o ff ers. For Q3 of 2015, users who were characterized as power users who were predicted to purchase in that quarter ended up using $4.8 million in discounts from promotional o ff ers. That adds up to roughly $20 million per year in dis-counts to users who are extremely active and predicted to purchase anyway. Of course, experimentation is required to determine which users would benefit most from promotional o ff ers. These CLTV scores can be used to select a prospec-tive set of users and then the final decision of who gets the promotional o ff er would be decided based on the responses of user segments in a test campaign.

Also at Groupon we are using these results in product experimentation, both for segmenting users and as a longer-term KPI. For example, products related to redemption are geared towards increasing long-term customer satisfaction and thus cannot be adequately tested using typical shorter-term conversion-based A/B experimentation. For this we use CLTV as a performance metric and track the shifts in redemption-related features in the testing versus control groups over time.
As discussed above, we build di ff erent models for our dif-ferent  X  X urchase cohorts, X  with the intuition being that dif-ferent features will be most important for customers at dif-ferent stages of their lifecycle. We find that this intuition is indeed correct. For example, the top four most important features for our power users are, in no particular order:
Note that each of these features is related to purchasing behavior. On the other end of the spectrum, we cannot rely on purchase-related features to predict value for our one-time buyers. For these users the most important features include:
In this paper we have described the challenges faced, the choices made, and the lessons learned in building a cus-tomer lifetime value monitoring system for a large global e-commerce company. Our model uses engagement scores for email and our mobile app as features, and this gives us the ability to quickly and accurately detect changes in customer value. We gain additional accuracy by building separate models for di ff erently-behaving sets of customers, such as one-time buyers and power users. We further use a trigger-ing system and an empirical decay function to dramatically speed up the scoring process and make daily re-scoring with such a complex model technologically feasible. This system is currently running in production, scoring hundreds of mil-lions of users on a daily cadence, and the results are being utilized by various business units including our marketing and experimentation teams.

In the future we plan to enhance this system in several ways. These include testing other machine learning algo-rithms (e.g. zero inflated binomial and gradient boosted de-cision trees) and adding more features, such as scores for other types of engagement including organic desktop web activity. We also plan to add more information regarding the use of discounts, and other features that other business units request for their specific needs. Our use of scalable technologies means that our system will continue to perform as our business grows. However we will still continue to de-termine ways to further speed up the system and decrease the use of resources. one-to-one correspondence and financial information is obfuscated.
As we continue to gather more data, we will evaluate longer term trends and explore the use of longer time frames for our purchase cohorts. We will also fully quantify the business costs associated with model errors. For example, afalsenegativefornewerusersmighthavelargerimplica-tions for building a long-tern relationship, whereas it would not be as detrimental to a frequent user. We will further continue to create cohort metrics for our end business users, for instance tracking redemption time for the redemptions team as mentioned above. We plan to similarly track cohort metrics for our other product and marketing partners and continue to iterate to develop the CLTV system and new products in tandem. [1] S. Aiello, T. Kraljevic, P. Maj, and with contributions [2] C. Bailey, P. Baines, H. Wilson, and M. Clark. [3] D. Benoit and D. V. den Poel. Improving customer [4] L. Breiman. Random forests. Machine Learning , [5] P. Cermak. Customer profitability analysis and [6] Z.-Y. Chen and Z.-P. Fan. Distributed customer [7] M. Crowder, D. Hand, and W. Krzanowski. On [8] F. Dwyer. Customer lifetime valuation for marketing [9] S. Gupta, D. Hanssens, B. Hardie, W. Kahn, [10] D. Hinkle, W. Wiersma, and S. Jurs. Applied Statistics [11] P. Kotler and K. Keller. Aframeworkformarketing [12] V. Kumar and M. George. Measuring and maximizing [13] A. Liaw and M. Wiener. Classification and regression [14] E. Malthouse. The results from the lifetime value and [15] D. Meyer, E. Dimitriadou, K. Hornik, A. Weingessel, [16] T. Mirzaei and L. Iyer. Application of predictive [17] S. Rosset, E. Neumann, U. Eick, and N. Vatnik. [18] T. Therneau, B. Atkinson, and B. Ripley. rpart: [19] L. Valenzuela, E. Torres, P. Hidalgo, and P. Farias.
