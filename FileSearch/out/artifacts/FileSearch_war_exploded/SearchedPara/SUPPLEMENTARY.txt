 A.1. On the Relation between the Natural and Surrogate Risk minimiser by using the surrogate minimiser.
 by this approximation error.
 conditional expectation E [  X | x ] to relate the risk functions.
 A.2. Reproducing Kernel f  X  X  X , and all h  X  X  Y it holds that is greater than zero. This is obviously fulfilled and  X  has an associated RKHS H  X  . A.3. Case Study III: Smooth Quotient Operators which is in H X for a given x , is a valid choice. The approximation is hence Theorem B.1. Each F  X  X   X  is a bounded linear operator from H X to H Y . Proof. (a) Each operator in L = { P n i =1  X  ( f i ,  X  ) h i : n  X  N ,f i  X  X  X ,h i  X  X  Y } linear as for the equivalence of the closure of L and H  X  .
 have for an arbitrary g  X  X  X that g a  X  such that k F g  X  F  X  g k l &lt; .
 simultaneously smaller than / 3.
 Hence, for a given we have a F such that F is linear. QQQ have a limit  X  F f in H Y . We have Since F n f converges to  X  F f in H Y and F n converges to F in H  X  we have that  X  F f = F f  X  X  Y . (d) Finally, each F is bounded as an operator from H X to H Y as if F k ( x,  X  ) = G k ( x,  X  ) for all x  X  X . This proves the first statement. Theorem B.3. For every F  X  X   X  there exists an adjoint F  X  in H  X   X  such that for all f  X  X  X and h  X  X  Y Proof. (a) We first derive the explicit expression of F  X  for F  X  X  . This is nearly trivial, we have (b) Next, we verify some properties of (T L ), where (T L )[ P n i =1  X  f i h i ] = P n i =1  X   X  h where we used that  X  u is a linear operator. (ii) (T L ) is norm preserving, as Furthermore, (T L ) is continuous as k (T L ) k op = sup F  X  X  , k F k (iii) (T L ) is bijective. Take an arbitrary G  X  L  X  then G = P n i =1  X   X  h that (T L ) F = (T L ) F 0 then, we have as (T L ) is norm preserving and we conclude F = F 0 . T : H  X  7 X  X   X   X  of (T L ) (Werner, 2002)[Satz II.1.5]. Furthermore, k T k op = k (T L ) k op = 1. For F 0 , G 0  X  X  we have that G k  X   X k F 0  X  F k  X  + k G  X  G 0 k  X  &lt; / 3 and, consequently, that k T F 0  X  T G 0 k  X   X  &gt; 2 / 3 . Furthermore, k T F  X  T F 0 k  X   X  , k T G 0  X  T G k  X   X  &lt; / 6 and we have and F 6 = G . QQQ L  X  converging to G . Now, we have exactly one F n  X  X  such that T F n = G n . As { G n }  X  it follows that { F n }  X  n =1 is also a Cauchy-sequence: and because of the completeness of H  X  the sequence { F n }  X  n =1 has a limit F . Because of the continuity of T it follows that and T is surjective. QQQ Kor.IV.3.4 in Werner (2002). as T is continuous and preserves the norm for elements in L . in L that converges to it. Then as T is continuous and maps to the adjoint for elements in L .
 Theorem B.4. The set of self-adjoint operators in H  X  is a closed linear subspace. holds that f,g  X  X  X we have that for all n  X  N we have that | X  F f,g  X  k  X  X  F n f,g  X  k | &lt; .
 Using this we have that for arbitrary f,g and F is also self-adjoint.
 C.1. Change of Measure C.1.1. Absolute Continuity that is singular wrt. P and the absolute continuous part.
 If we have only control over RKHS functions and not arbitrary measurable functions, but we might consider the a strong indicator that the empirical measures are not absolute continuous.  X  E
Q k ( y i ,  X  ) 6 = 0 then we know that for the empirical versions  X  Q and not a sufficient one.
 to 0 in the sample size.
 C.2. Product Integral  X  Fubini H Aronszajn (1950)[Sec. 8]. The RKHS H X  X  X  Y has the reproducing kernel We denote the RKHS with H X  X  Y := H X  X  X  Y .
 m H X  X  Y then the Riesz theorem guarantees us that such an element exists with which that, under suitable assumptions, E X  X  Y g ( x,y ) = E X E Y g ( x,y ). guarantees us that Note, that not every g  X  H X  X  X  Y needs to be of this particular form as H X  X  X  Y is the completion of the direct product between H X and H Y .
 The second case of interest is when you have a kernel on the product space X  X  Y that does not arise from deal with the limit points in H X  X  X  Y .
 such that wrt. P X we get Using the usual regularised empirical version and W from eq. 2 we get the estimate C.3. Conditional Expectation The adjoint of the estimate we derived for the conditional expectation in eq. 5 is with W defined in eq. 2. If we use f = k ( x,  X  ) we get This is also straight forward from a direct evaluation of  X  ( x ) as vector-valued RKHS H  X  . Because, elements  X   X  X  are finite sums we have that where  X  i =  X  h,h i  X  l .
 H { X  h, X  n (  X  )  X  l }  X  n =1 is a Cauchy-sequence in H X , i.e. that of E being a finite sum or a limit point in H  X  .
 D.1. Sum Rule  X  Change of Measure on Y Nikod  X ym derivative r ( x ) is a.e. upper bounded by b , we get: Using Micchelli &amp; Pontil (2005)[Prop. 2.1 (f)], the second term can be bounded by In total, we get the upper bound D.2. Kernel Bayes X  Rule  X  Change of Measure on X | y expectation risk evaluated wrt. Q . The kernel function is here  X  ( h,h 0 ) :=  X  h, A h 0  X  l B . Theorem D.2. We assume that the integrability assumptions from suppl. F hold, that Q X P X and that the |
E in other words there exists a positive constant C such that Proof. In the following, we use the short form E Y 0 for E y 0  X  P Y . needed at the end to bound the error of E . We have that
E c, Q [ G ] = sup bound already in eq. 9: E c, Q [ E ]  X  b E c, P [ E ]. (b) We have that To see this we first observe that the conditional expectation E [ f | y ] is integrable wrt. Q Y and every E [ f | y ] with k f k k = 1. Finally, we can pull the scaling outside through H With the same argument we have E X  X  Y 0 l ( y,  X  )  X  f = E Y 0 l ( y,  X  ) E [ f | y 0 ]. (d) We have that (e) Building up on (d) we get the bound that using the assumption that | e  X  o | X | e | / 2 we get that that | l ( y,y 0 ) | X  c . The bound is now f and integrate wrt. E Y . Under our assumption that E Y 0 l ( y,y 0 ) &gt; q this turns into sup  X  to apply the convergence results from Caponnetto &amp; De Vito (2007). rates are known for the convergence of the mean element in the RKHS norm. This implies convergence of the measure Q X and for any &gt; 0 there exists a constant C such that to denote the product measure for n copies of Q X .
 E.1. Assumptions extra effort.
 the integrability assumptions from F hold.
 E.2. Rates for the Conditional Expectation In this section we derive risk bounds and convergence rates for the natural risk function The approach we take is to derive convergence rates for the surrogate risk function We start by linking the two cost functions in the next.
 E.2.1. Relating the Risk Functions Also note that the approach is based on a conditional expectation argument as discussed is Supp. A.1. that for any h  X  X  Y : E [ h | x ] = E  X  [ h ]( x ) P X -a.s., then for any E  X  X   X  : Proof. (i) follows from (ii) by setting E := E  X  . Using the assumption (ii) can be derived: E risk functions fulfills two risk functions are equal to E  X  .
 in H  X  then we have there is a second minimiser E  X  then above calculation shows that P operators uniquely we have E  X  = E  X  .
 E there is a second minimiser E  X  then for all h  X  X  Y we have the same argument as in case one we can follow equivalence of the operators. this is an approximation error term.
 such that sup k E k Proof. First, observe that if E  X  X   X  then we have due to the Jensen inequality | E X E [ E  X  k ( x,  X  ) | x ]  X  E X  X  E  X  k ( x,  X  ) , E  X   X  k ( x,  X  )  X  l | X k E  X  k ( x,  X  ) k l E X E  X k E  X  k ( x,  X  ) k l We can now reproduce the proof of Lemma E.1 with an approximation error. For any E  X  X   X  we have In particular, Like in the proof of Theorem E.1 we have for any E that and hence p E E  X  k ( x,  X  )  X  l for any h with k h k l  X  1 and eq. 12.
 Now, for E := E s observe that E s [ E s ] + 2 E s is a E s minimiser we have |E s [ E  X  ]  X  X  s [ E s ] | X  2 Furthermore, with k E  X  k ( x,  X  ) k l  X k E k  X  k A 1 / 2 k op k B k 1 / 2 op p k ( x,x ) we have Similarly, for E := E  X  we have E.2.2. Convergence Rates for the Surrogate Risk with E n we have: Theorem E.3. Under assumptions E.1 we have that for every &gt; 0 there exists a constant C such that and closed ball B X  X  X  X , which contains all k ( x,  X  ), as the input space. subset is too. (b) H Y must be a separable Hilbert space. Like in (a) this is fulfilled. We continue with Hypothesis 1 from Caponnetto &amp; De Vito (2007). N  X  N we have that does not change the argument. H  X  which is for a given basis { e i } m i =1 of H Y finite as A and B are bounded. Hence, both operators are Hilbert-Schmidt operators. QQQ the boundedness assumption of k ( x,  X  ). For a basis { e i } m i =1 of H Y we have that which is bounded as f is bounded.
 The final assumptions we need to verify are the ones in Hypothesis 2 from Caponnetto &amp; De Vito (2007). and H Y functions are integrable we have that the following expectation is well defined on { l ( y,  X  ) : y  X  Y } .
 E.2.3. Convergence Rates for the Natural Risk We now combine the upper bound argument with the convergence rate for the upper bound. such that sup k E k for every &gt; 0 there exists a constant D such that can not represent the conditional expectation exactly with our RKHS H  X  . E.2.4. Convergence Rates for the Approximate Sum Rule Q denotes the product measure over n samples, whereas we assume that all the samples are iid. such that sup k E k lim sup We restate the theorem in a more readable form. For this we combine the approximation error terms: and we simplify the theorem to: Theorem E.6. Let E  X  be a minimiser of the approximation error E A . Under assumptions E.1 and if Q X P X with a bounded Radon-Nikod  X ym derivative we have that for every &gt; 0 exist constants a,b,c,d such that expectations of functions h  X  X  Y given a x  X  X . We use Lebesgue integrals based on Fremlin (2000). imply this assumption (see also Berlinet &amp; Thomas-Agnan (2004)).  X  b functions on the product space we assume that these functions are integrable wrt. P X  X  Y . We can then define a conditional expectation
