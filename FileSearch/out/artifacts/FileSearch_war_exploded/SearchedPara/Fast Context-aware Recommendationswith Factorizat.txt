 The situation in which a choice is made is an important information for recommender systems. Context-aware rec-ommenders take this information into account to make pre-dictions. So far, the best performing method for context-aware rating prediction in terms of predictive accuracy is Multiverse Recommendation based on the Tucker tensor fac-torization model. However this method has two drawbacks: (1) its model complexity is exponential in the number of con-text variables and polynomial in the size of the factorization and (2) it only works for categorical context variables. On the other hand there is a large variety of fast but specialized recommender methods which lack the generality of context-aware methods.

We propose to apply Factorization Machines (FMs) to model contextual information and to provide context-aware rating predictions. This approach results in fast context-aware recommendations because the model equation of FMs can be computed in linear time both in the number of con-text variables and the factorization size. For learning FMs, we develop an iterative optimization method that analyti-cally finds the least-square solution for one parameter given the other ones. Finally, we show empirically that our ap-proach outperforms Multiverse Recommendation in predic-tion quality and runtime.
 H.3.3 [ Information Systems ]: Information Search and Re-trieval X  Information filtering ; I.2.6 [ Artificial Intelligence ]: Learning X  Parameter Learning Algorithms, Experimentation, Measurement, Performance Context-Aware Recommender System, Factorization Machine, Rating Prediction, Tensor Factorization
Rating prediction in recommender systems relies primar-ily on the information of how (which rating, e.g. on a scale from 1 to 5 stars) who (which user) rated what (which item, e.g. movie, news article, or product). There are many meth-ods that take additional data about the who (demographic information about the user, age, profession, gender) or the what (item attributes like movie genres or keywords from the product description) into account.

Besides such data about the entities involved in the rating events, there is possibly also information about the situation in which the rating event happens, e.g. the current location, the time, who is nearby, or the current mood of the user. Such situational information is usually called context . Be-cause it is known from decision psychology that the setting and the mood of a person do influence their behavior, it is desirable to exploit context information in recommender systems. Context-aware rating prediction relies on the in-formation of how who rated what in which context (see also figure 1).

Classical recommender system methods do not take con-text information into account. Some approaches perform pre-or post-processing of the data to make standard meth-ods context-aware. While such ad-hoc solutions may work in practice, they have the shortcoming that all steps in the process need supervision and fine-tuning. Methods that in-tegrate all kinds of input data into one model are more practical in this respect, as well as theoretically more ele-gant. Currently, the most flexible and strongest approach in terms of prediction accuracy is Multiverse Recommenda-tion [5] that relies on Tucker decomposition and allows to work with any categorical context. However, for real-world scenarios its computational complexity is too high as it is in O ( k m ) where k is the dimensionality of the factorization and m the number of modes/variables involved.

In this paper, we propose a context-aware rating pre-dictor that is based on Factorization Machines (FM) [14]. FMs include and can mimic the most successful approaches in recommender systems including matrix factorizion [18], SVD++ [6] or PITF [15]. We show how FMs can be applied to a wide variety of context domains including categorical, set categorical or real-valued domains. Besides this flex-i bility in modeling, the complexity of FMs is linear both in k and m which allows fast prediction and learning with context-aware data. For learning the model parameters of FMs, we propose a new algorithm that is based on alter-nating least squares (ALS). Our algorithm directly finds the optimal solution for one model parameter given all the other ones and a joint optimum is found within a few iterations. Like for stochastic gradient descent (SGD), the complexity for one iteration of our ALS algorithm is in O ( | S | m k ) where | S | is the number of training examples. The main advantage of our new ALS algorithm over SGD is that no learning rate has to be determined. This is very important in practice, because the quality of SGD learning relies largely on a good learning rate and thus an expensive search has to be done. This is not necessary for ALS.

In our experiments, we show empirically that context-aware FMs can capture context information and improve predictive accuracy. Furthermore, FMs outperform the state-of-the-art method Multiverse Recommendation both in pre-diction quality and largely in runtime. 1. In contrast to other context-aware rating predictors, 2. We develop a new learning algorithm for FMs that di-3. Compared to the state-of-the-art context-aware rating
We first describe the standard rating prediction task and then extend it to context-aware rating prediction. Next we show how this task can be expressed as a regression task from real-valued feature vectors under extreme sparsity. We also discuss shortly why standard regression models are not effective in this setting.
Standard rating prediction can be defined as a regression task over users U = { u 1 , u 2 , . . . } and items I = { i where a target function y : U  X  I  X  R has to be estimated. The target function represents the rating, e.g. y ( u, i ) is the rating of user u for item i . We denote the observed part of y by S  X  U  X  I ; i.e. for all ( u, i )  X  S , we know the rating y ( u, i ) in advance. The task of rating prediction is to estimate a function  X  y that can predict  X  y ( u, i ) for any user-item combination.
In context-aware recommender systems, it is assumed that some additional information is available that influences the rating behavior. We define a context as a variable c  X  C . Examples are the mood the user was in when (s)he rated an item (e.g. C = { happy , sad , . . . } ), the time at which a rating was given (e.g. C = R + ), the last items seen (e.g. C = P ( I )) or the location (e.g. C = R 2 ).
If multiple contexts C 3 , . . . , C m are allowed the task is to estimate the following rating function: Note that we start the index of the context variables with 3 because from a technical point of view, users and items can be seen as the first and second  X  X ontext X .

Figure 2 shows an example for context-aware data on the left side. There are users U in mood C 3 watching movies I together with other users C 4 : The first tuple in Fig. 2 states that A lice rated TI tanic with 5 stars and that she has watched this movie with C harlie while she was H appy .
Most research in recommender systems focus on context-unaware methods that analyze only the user-item interac-tion. Here matrix factorization approaches (e.g. [18, 6]) have become very popular as they usually outperform tra-ditional k-nearest neighbor methods (e.g. [17]). There is also research in incooperating meta-data like user or item attributes into the prediction, like Stern et al. [19] who extend a matrix factorization model. However, meta-data often yields in only little or no improvement over strong baseline methods for rating prediction if enough feedback data is present [12]. The difference between such user/ item attributes and context is that attributes are attached only to either an item or user (e.g. a genre is attached to a movie) whereas context is attached to the whole rating event (e.g. the mood of a user when rating an item).

In contrast to the huge literature on standard recom-mender systems, there is only little research on context-aware recommender systems. The most basic approaches are contextual pre-filtering and post-filtering where a stan-dard context-unaware recommender system is applied and the data is either preprocessed based on the context of in-terest before applying the recommender or the results are postprocessed [11]. Examples for pre-processing are item-splitting [3] or the multidimensional model of Adomavicius et al. [1] which is based on OLAP cubes. More sophisti-cated approaches use all the context and user-item infor-mation simultaneously to make predictions. Oku et al. [9] use SVMs for context-aware predictions  X  which have limi-tations in sparse applications like recommender systems as no 2-way interaction between items and users can be es-timated directly [14]. Li et al. [8] suggest to see context as a user feature which is dynamic, i.e. can change. There is also some research on context-aware recommendation sys-tems for item prediction [16], which is a ranking task instead of a regression task like the problem we are dealing with in this paper.
R ecently, Karatzoglou et al. [5] have proposed to apply the Tucker decomposition [20] to factorize the tensor over user, items and all categorical context variables directly. They called their approach Multiverse Recommendation and have shown empirically that their approach results in better prediction accuracy than item-splitting [3] and the OLAP approach [1]. In our evaluation, we will compare our Factor-ization Machine approach to Multiverse Recommendation .
As both our approach and Multiverse Recommendation are based on factorization models, we shortly recapitulate their approach and highlight the differences. Their model equation is the Tucker Decomposition, which decomposes an m -mode tensor into a smaller core tensor B and one factor matrix V ( m ) per mode. For context-aware recommender systems, the first mode is the user, the second the item and the remaining m  X  2 modes are the context variables. The model equation can be written as:  X  y ( u, i, c 3 , . . . , c m ) := with The major drawback of this model is that its computational complexity is in O ( Q m l =1 k l ). Assuming factorization dimen-sions of equal size, i.e. k := k l , this means a computational complexity of O ( k m ). Thus the complexity of computing one context-aware rating is exponential in the number of modes and polynomial in the number of factors. This leads to both poor learning and poor prediction runtime as soon as the number of factors growths. In Figure 5 we compare the learning runtime of Multiverse Recommendation empiri-cally to our FM approach. A second limitation of Multiverse Recommendation in comparison to our approach is that only categorical context can be modeled  X  e.g. categorical set variables or real-valued variables are not possible. Finally, in the related task of item or tag recommendation (which is a ranking task), it has been shown that often it is better to factorize several lower variable interactions (e.g. pairwise ones [15]) instead of one m -ary relation (like the Tucker De-composition). The reason is that under high sparsity, factor-ized pairwise relations can be estimated well but factorized m-ary relations are harder to estimate. Our FM approach follows this idea and models all nested interactions up to pairwise ones.
In contrast to the little work on general context-aware methods, there is much more research on attribute-aware or specialized recommender systems. For example [19] or [2] present extensions of the matrix factorization model that can handle user and item attributes. There are also sev-eral works on taking time-effects into account, e.g. [7, 21]. However, all of these approaches are designed only for spe-cific problems and cannot handle the general problem setting of context-aware recommendation that we examine in this work. For sure for specific and important problems (e.g. time-aware or attribute-aware recommendation) it is bene-ficial to investigate specialized methods that are supposed Figure 1: Attribute-aware methods can take addi-t ional information about the user or the item sep-arately into account (left), whereas context-aware methods are more general and can analyze data that is simultaneously attached to all  X  X odes X , i.e. the whole rating event (right). to be the best models for a certain problem, but on the other hand also research on general methods like context-aware recommenders is important as they offer the largest flexibility and may serve as (strong) baselines for specialized models.
For the model class of matrix factorization, Bell and Koren [4] proposed an ALS method that alternates between opti-mizing all user factors and all item factors. As the whole factor matrix of all users (resp. items) is optimized jointly, the computation complexity is O ( k 3 ). This complexity is-sue of standard ALS is the reason why SGD approaches are more popular in the recommender system literature than ALS. Pil  X aszy et al. [13] have proposed to optimize the fac-tors within each user (resp. item) one after the other which results in an ALS algorithm constant in k , i.e. O ( k ) because the matrix inversion is avoided. The general idea of optimiz-ing one factor at a time is the same idea that we apply for our ALS algorithm for FMs. Both approaches [4, 13] work only for matrix factorizations and thus cannot handle any context like our proposed FMs which model all interactions. Furthermore our ALS algorithm also learns the global bias and basic 1-way effects.
FMs are a generic model class that subsumes and can mimic several of the most successful recommender systems, among them matrix factorization [18], SVD++ [6] or PITF [15]. We shortly recapitulate the FM model and then show in detail how it can be applied to context-aware data and what happens inside an FM using such context-aware data. In the second main part, we propose a new fast alternating least square (ALS) optimization algorithm that makes FM much easier applicable compared to SGD algorithms because it works without any learning rate.
A factorization machine (FM) [14] models all interactions between pairs of variables with the target, including nested ones, by using factorized interaction parameters 1 : where  X  w i,j are the factorized interaction parameters between pairs: and the model parameters  X  that have to be estimated are: That means w 0 is the global bias, w i models the interaction of the i-th variable to the target and  X  w i,j models the factor-ized interaction of a pair of variables with the target. Note also that unlike other factorization models like matrix fac-torization or PARAFAC, FMs can work with any continuous input data x .

In [14] it has also been shown that an FM (eq. 2) can be computed very efficiently in O ( k m ( x )) as it is equivalent to:  X  y ( x ) = w 0 +
For the task of regression, the most widely used loss func-tion is square loss. To prevent overfitting it is common to add a regularization term  X  usually L2. In total, we use the following regularized least square criterion for optimization: where  X  (  X  ) is a regularization (hyper-)parameter for the model parameter  X  . In general, the regularization term can be cho-sen individually for each model parameter. But in practice it makes sense to use the same regularization parameters for similar model parameters  X  in our experiments we use:  X  ( w 0 ) = 0 as there is no need to regularize the global bias; the same  X  ( w ) for all parameters w i  X  w and the same  X  for all parameters v i,f  X  V .
A wide variety of context-aware data can be transformed into such a prediction task using real valued feature vectors x  X  R n (see figure 2, right side for an example). We will show possible mappings z : C  X  R n z for different kinds of domains:
W e restrict our discussion to 2-way FMs ( d = 2). The final feature vector x can be obtained by concatenating the single mappings: x ( u, i, c 3 , . . . , c m ) = ( z 1 ( u ) , z 2 ( i ) , z This data vectors x are then the input for the FM (eq. 2).
Figure 2 shows a complete example how to transform the context-aware data of section 2.3 into a prediction problem from real-valued features. The example contains the cate-gorical domains user U , item I and mood C 3 as well as a set-categorical domain, the friends the movie has been watched with, C 4 . To get an insight of how an FM will work, we investigate shortly the model equation of an FM applied to this data 2 . When we look at the feature vector x of our example, one can see that most of them are 0 and thus the FM model equation can be rewritten as:  X  y ( x ( u, i, c 3 , c 4 )) = w 0 + w i + w u + w c 3 + X That means the FM model contains the bias term for the individual item i , user u , mood c 3 and the average bias of the co-watchers c 4 . Furthermore it factorizes all pairwise in-teractions between these four variables. Comparing the FM model with the standard matrix factorization model (e.g. [18]) for the user u and the item i , one can see that the FM contains also exactly this factorization: h v i , v u i . Addition-ally, it factorizes all pairwise interactions with all context variables. This shows how FMs automatically include one of the best performing recommender system models, the ma-trix factorization model.
As the model equation of FMs can be calculated in linear time (see eq. 5), it is straightforward to develop a stochastic gradient descend (SGD) optimization algorithm for a vari-ety of loss functions. However SGD requires to find a good learning rate which is big enough to have convergence after a reasonable amount of iterations and small enough that the gradient steps are made towards the minimum which is espe-cially important in latter iteration stages. In the following,
P lease note that we do this analysis only to explain what happens implicitly in the FM  X  nothing has to be done ex-plicitly when applying an FM. The end-user (domain expert) has only to specify the input features (i.e. the x vector) and run a generic FM tool with this data. with. we suggest a new alternating least square (ALS) learning al-gorithm that finds the optimal value for a model parameter given the remaining ones.
The analytical least-square solution for a model parameter can be found for FMs because they are linear functions (in each model parameter) and there is an analytic solution for every linear function. We will show this in the following two lemmas.

Lemma 1 (Linearity in  X  ). An FM is a linear func-tion with respect to every single model parameter  X   X   X  and thus can be reexpressed as: where g (  X  ) and h (  X  ) are independent of the value 3 of the pa-rameter  X  .

Proof. We proof this by stating g and h explicitly for the model parameters. For the global bias w 0 , the FM can be rewritten as: f or w l :  X  y ( x | w l ) = w l x l
T he functions g and h are indexed with the name of the parameter  X  because their form depends on the variable  X  (e.g. is different for w 0 and w l ) but they do not depend on the value of  X  .
 And for the factorized 2-way interactions v l,f :  X  y ( x | v l,f ) := v l,f + w 0 + | {z }
Remark 1 . For linear decomposable functions like  X  y of an FM, the function h (  X  ) ( x ) is the gradient of  X  y with respect to  X  : Differentiating  X  y leads directly to this result.
Lemma 2 (Optimal value for  X  ). The regularized least-square solution of a single parameter  X  for a linear model  X  y ( x |  X  ) is:
Proof. To gain the regularized least-square solution an-alytically, the first derivative of the optimization criterion (eq. 6) w.r.t.  X  has to be found: The minimum is where this derivative is 0:  X  X  X   X  =  X 
L emma 2 with the gradients h (  X  ) (see remark 1 and eq. (10)) allows to analytically find the optimal value of each model parameter  X  of the FM given the remaining model parameters. A joint optimum of all model parameters  X  can be found iteratively by calculating the optimum of each model parameter one after another and repeating this sev-eral times. In figure 3 such an algorithm is sketched  X  the meaning and purpose of e and q will be explained later. First the model parameters are initialized, where the 0-and 1-way interactions ( w 0 and w l ) can be initialized with 0 and the factorization parameters with small 0-centered random values. In the main loop the parameters are optimized one after the other. The idea here is to optimize first lower inter-actions and then higher ones because for lower interactions more data is observed and thus their estimates are more reliable. Within the factors of the 2-way interactions, first all features of the first factor dimension are optimized, then the features of the second factor dimension, etc. This allows the f -th factor to find the residuals for the 1st to ( f  X  1)th factor dimensions. This optimization main loop is repeated several times until convergence to the joint optimum of all model parameters.
Straightforward computation of the optimum for each pa-rameter with eq. (10) would mean to calculate g and h for each training example and in each parameter update. Now, we show how to calculate the values efficiently for FMs. This involves three improvements: (1) precomputing error terms, (2) precomputing h-terms for 2-way interactions and (3) us-ing the sparsity in S . In total, this will lead to an update algorithm where a full iteration over all parameters is in O ( | S | m S k )  X  i.e. linear in the number of non-zero elements in the whole dataset and the number of factors.

The first bottleneck in updating the parameter  X  with eq. (10) is calculating ( g (  X  ) ( x )  X  y ) for each training case ( x , y )  X  S . Obviously, a trivial computation of this is in O ( m ( x ) k ).
 Now we will show, how to compute this in constant time O (1) if the error is known. Lets define for each training case, the error 4 e ( x , y |  X ) of the model given the model parameter as: This allows to rewrite:
A ctually, e is not the error, but e 2 is. Nevertheless, we use the term error for convenience. 1: procedure LearnALS ( S ) 2: w 0  X  0  X  Initialize the model parameters 3: w  X  (0 , . . . , 0) 4: V  X  X  (0 ,  X  ) 5: for ( x , y )  X  S do  X  Precompute e and q 6: e ( x , y |  X )  X   X  y ( x , y )  X  y 7: for f  X  X  1 , . . . , k } do 8: q ( x , f |  X )  X  P n i =1 v i,f x i 9: end for 10: end for 11: repeat  X  Main optimization loop 13: e ( x , y |  X )  X  e ( x , y |  X ) + ( w  X  0  X  w 0 ) 14: w 0  X  w  X  0 15: for l  X  X  1 , . . . , n } do  X  1-way interactions 17: e ( x , y |  X )  X  e ( x , y |  X ) + ( w  X  l  X  w l ) x l 18: w l  X  w  X  l 19: end for 20: for f  X  X  1 , . . . , k } do  X  2-way interactions 21: for l  X  X  1 , . . . , n } do 26: end for 27: end for 28: until stopping criterion is met 29: return w 0 , w , V 30: end procedure Figure 3: Alternating least algorithm that optimizes the model parameters w 0 , w and V for least-square in O ( | S | m | S | k ) t ime (see section 4.3.3) where | S | are the number of training examples and m | S | t he aver-age number of non-zero elements in an input vector x.
 Which can be used in the nominator of eq. (10). We store the error in a vector e  X  R | S | over all training examples and precompute it at the beginning. After changing the value of a model parameter from  X  to  X   X  , the error also changes. This change on the error can be computed analytically by: where  X   X  is the set of all model parameters where only the value of  X  has changed to  X   X  .

After storing error terms, the computation complexity only depends on the complexity of the h (  X  ) -functions. For the parameters w 0 and w i , the complexity of h (  X  ) is con-stant, thus also computing the terms within the sums of the nominator and denominator as well as the error update is in constant time. But for the factorized parameters, com-puting h contains a loop over all variables. We will show now how to compute this in constant time. First, we can reformulate h ( v with This term q is independent of l and can be precomputed for each training case and factor in a matrix Q  X  R | S | X  k With a precomputation of the q -terms, the h -function can be computed in constant time with eq. (14). When updating a parameter v l,f to v  X  l,f , we have to update the corresponding q -term as well. This can be done in constant time with: Again  X   X  are the new parameters after the value of v l,f changed to v  X  l,f , while the rest of the parameters keep un-changed.

With the two enhancements described so far, the compu-tation of each term within the sums of the nominator and denominator of the update rules eq. (10) can be calculated in constant time. Now, we investigate the overall complexity. First, for the w 0 parameter, the optimization complexity is O ( | S | ). Secondly, for each update (eq. (10)) of the 1-and 2-way interactions of the l -th parameter, one has to loop only over training examples where h is nonzero  X  these are the training examples where x l 6 = 0. Thus the complexity for updating a 1-way or 2-way parameter is in O ( m l ) 5 . In total there are n 1-way and n k 2-way parameters, so the complex-ity for one whole iteration (i.e. updating all parameters) is O (
In this section, we empirically investigate if the greater model flexibility and better runtime of FMs comes to the price of less prediction quality compared to the state-of-the-art context-aware method Multiverse Recommendation . Furthermore we want to examine the sensitivity of SGD to the choice of the learning rate and if our ALS optimization can successfully work without this hyperparameter. ing prediction is highly relevant in practice, there are only a few publicly available datasets: the Food dataset [10], the Adom. dataset [1] as well as the Yahoo! Webscope dataset 6 enriched with context-aware information by [5]. The Food dataset contains 6360 ratings (1 to 5 stars) by 212 users for 20 menu items where one context variable captures if the situation in which the user rates is virtual or real (i.e.
R emind that m l is the number of training cases in S where x 6 = 0 which is usually low in our sparse setting http://research.yahoo.com/Academic_Relations ydata-ymovies-user-movie-ratings-content-v1 0 i f (s)he imagines to be hungry or (s)he really is) and the second one how hungry the user is. The Adom. dataset contains 1524 rating events (1 to 15 stars) for movies with five context variables about companion, the weekday and other time information. For the Yahoo! Webscope dataset (221367 rating events) we follow [5] and apply their method to generate datasets with an increasing number of depen-dency of the target on the context  X  we generate 9 datasets for  X  =  X   X  X  0 . 1 , . . . , 0 . 9 } .
 Methods. We compare context-aware FMs to Multiverse Recommendation . Note that Multiverse Recommendation has been shown to outperform other context-aware recom-mender systems on the Yahoo! webscope , Food and Adom. dataset [5]. As context-unaware baseline we use FMs ( FM (nocontext) ) where only the user and item variables are used generating the feature vectors which is equivalent to matrix factorization with bias terms (see [14] for a proof) which is one of the strongest context-un aware recommender al-gorithms. All models are optimized for regularized least-squares (eq. 6). The FMs are optimized with our proposed ALS algorithm. Multiverse Recommendation is learned with an SGD algorithm similar to the one proposed in [5]. is used as validation set for tuning the hyperparameters for optimal MAE. After hyperparameter search, we make a 5-fold cross validation on the remaining 95% of the dataset  X  i.e. the validation set for hyperparameter tuning is not used any more. We report the mean RMSE and MAE over the 5 experiments. All methods are implemented in C++ and were run on the same hardware.
 with ALS optimization, a Multiverse Recommendation im-plementation and the dataset generation scripts can be down-loaded from our website 7 .
In figure 4, we compare the test error of an SGD imple-mentation to our ALS-approach on the Webscope dataset 8 . The leftmost figure shows the prediction quality on the test set after each iteration. The two plots on the right side show the minimal RMS error on the test set for several learning rates after 10 and 100 iterations. One can see that the pre-diction quality of SGD depends largely on the learning rate and the number of iterations that is chosen. If the SGD learning rate is chosen too large (e.g., here 0 . 01), the quality of ALS cannot be reached. With a small learning rate one can reach the quality of ALS  X  provided that the number of iterations is large enough (see rightmost graphs) and that one stops learning near the minimum (see leftmost graph) on a validation set to prevent overfitting.

This experiment shows that SGD needs a careful and time-consuming search for the learning rate. In contrast to this, ALS requires no such search because it does not have this h ttp://www.libfm.org/
The model is a context-aware FM with k = 64; the regu-larization parameters have been tuned individually for each learning method. algorithm works without such a hyperparameter. hyperparameter. With regard to prediction quality, SGD can perform as good as ALS but only with the right choice of the learning rate. This makes ALS clearly favorable in application.
One of the main disadvantages of Multiverse Recommen-dation for practical use is that the computation complexity of the model is in O ( k m ) which is an issue for both learning and prediction. That means even for standard non-context-aware recommender systems, the runtime is quadratic in the dimensionality of the factorization k and for context-aware problems at least cubic. This makes it hard to apply for larger number of factors k . In contrast to this, the com-putational complexity of factorization machines is linear in both k and m : O ( k m ).

In this experiment, we compare the runtime for one full iteration over all training examples of Multiverse Recom-mendation to context-aware FMs. As dataset, we used Ya-hoo! Webscope with m = 4. To show the polynomial growth of the runtime with an increasing number of the dimensionality factorization, we run both models with k  X  { 1 , 2 , 4 , 8 , 16 , 32 , 64 , 128 }  X  for Multiverse Recommendation the maximal k we use was 32. In figure 5, the results are reported. These empirical results match to the theoretical complexity analysis. The runtime of the FM is linear and one full iteration over all training examples can be made for example in about 11 seconds (for k = 128) whereas the complexity of Multiverse Recommendation growths with k 4 such that the runtime for k = 16 is already 30 minutes and for k = 32 about 8 hours.
Finally, we want to investigate if the flexibility and fast runtime of context-aware FMs comes to the price of poor prediction quality compared to the Multiverse Recommen-dation method. Therefore we compare the prediction qual-ity on the Food , Adom and Webscope dataset 9 . Figure 6
F or the Adom. dataset our implementation of Multi-verse Recommendation performed much worse than reported in [5]. Thus in favor of Multiverse Recommendation we re-port their (better) MAE value.
 Figure 6: The context-aware methods M ultiverse Recommendation [5] and our proposed context-aware Factorization Machine benefit from incoop-erating the context-information into the rating pre-diction. shows that context-aware FMs and Multiverse Recommen-dation have comparable prediction quality for the Food and Adom. datasets. Furthermore, both methods outperform the context-unaware method (here equivalent to a matrix factorization model).

In a third experiment, we investigate the artificially en-riched Yahoo! Webscope dataset (see [5]) with an increasing influence of context variables on the rating. Figure 7 shows that both context-aware FM and Multiverse Recommenda-tion benefit from ratings that have a stronger dependence on the context. In contrast to this, the prediction quality of context-unaware FM gets worse when the rating depends stronger on the context because for context-unaware FMs the context is unobserved and thus they cannot explain this dependency.

Comparing both context-aware methods among each other, one can see that FMs throughout generate much better pre-dictions than Multiverse Recommendation . E.g. for RMSE the difference is about 0 . 10 to 0 . 15 points whereas for MAE it is about 0 . 08 to 0 . 10 points. This matches to results from the related area of tag recommendation, where a pairwise interaction model (PITF) outperforms the Tucker decom-position empirically in sparse problems [15]. Note that the Multiverse Recommendation [5] it is polynomial (here O ( k ) ). E.g. for a factorization size of k = 16 ( k = 32 ), takes about 30 minutes ( 8 hours).
 Multiverse Recommendation model is the Tucker decompo-sition and the FM includes all possible pairwise interactions (like PITF).
Our experiments have shown that context-aware FMs are able to take contextual information into account to enhance predictions like the state-of-the-art method Multiverse Rec-ommendation . In terms of runtime, FMs are linear which makes them applicable to a large dimensionality of factors, many context-variables and many observations. In contrast to this Multiverse Recommendation cannot handle a large number of factorization dimensions or modes. The advan-tage in terms of runtime is not traded in for prediction qual-ity. Instead, on the dense datasets ( Food and Adom. ) the prediction quality of both methods is comparable whereas FMs outperform Multiverse Recommendation largely in sparse settings ( Yahoo! Webscope ). Finally, ALS optimized FMs are easily applicable as they do not require an expensive search for the learning rate like SGD optimized FMs or Mul-tiverse Recommendation .
In this paper, we have shown how to apply factorization machines to the task of context-aware recommender sys-tems. FMs are easily applicable to a wide variety of con-text by specifying only the input data. This allows them to solve scenarios where standard tensor factorization ap-proaches cannot be applied (e.g. categorical set domains, real-valued domains). We have developed a new learning algorithm for FMs that analytically solves the least-square problem for each model parameter independently. This is especially helpful in practice as no learning rate has to be specified like in SGD approaches. Compared to Multiverse Recommendation which is the best-performing method for context-aware rating prediction so far, our approach achieves much faster runtime both in training and prediction ( O ( k m ) instead of O ( k m )) as well as a better prediction quality. To summarize, context-aware FMs combine the flexibility to be applied in many different scenarios and high prediction ac-curacy due to factorized interactions with fast and scalable computation due to the linear model complexity.
We would like to thank Hideki Asoh, Gediminas Adomavi-cius and Alexander Tuzhilin for sharing their data sets. [1] G. Adomavicius, R. Sankaranarayanan, S. Sen, and [2] D. Agarwal and B.-C. Chen. Regression-based latent [3] L. Baltrunas and F. Ricci. Context-based splitting of [4] R. M. Bell and Y. Koren. Scalable collaborative [5] A. Karatzoglou, X. Amatriain, L. Baltrunas, and [6] Y. Koren. Factorization meets the neighborhood: a (context) to explain the shift in the ratings. [7] Y. Koren. Collaborative filtering with temporal [8] Y. Li, J. Nie, Y. Zhang, B. Wang, B. Yan, and [9] K. Oku, S. Nakajima, J. Miyazaki, and S. Uemura. [10] C. Ono, Y. Takishima, Y. Motomura, and H. Asoh. [11] U. Panniello, A. Tuzhilin, M. Gorgoglione, [12] I. Pil  X aszy and D. Tikk. Recommending new movies: [13] I. Pil  X aszy, D. Zibriczky, and D. Tikk. Fast als-based [14] S. Rendle. Factorization machines. In Proceedings of [15] S. Rendle and L. Schmidt-Thieme. Pairwise [16] A. Said, S. Berkovsky, and E. W. De Luca. Putting [17] B. Sarwar, G. Karypis, J. Konstan, and J. Reidl. [18] N. Srebro, J. D. M. Rennie, and T. S. Jaakola. [19] D. H. Stern, R. Herbrich, and T. Graepel. Matchbox: [20] L. Tucker. Some mathematical notes on three-mode [21] L. Xiong, X. Chen, T.-K. Huang, J. Schneider, and
