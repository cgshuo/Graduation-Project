 This paper introduces a new tool that recommends an op-timized partitioning solution called Multi-Level Partitioned Primary Index (MLPPI) for a fact table based on the queries in the workload. The tool implements a new technique us-ing a greedy algorithm for search space enumeration. The space is driven by predicates in the queries. This technique fits very well the Teradata MLPPI scheme, as it is based on a general framework using general expressions, ranges and case expressions for partition definitions. The cost model implemented in the tool is based on the Teradata optimizer, and it is used to prune the search space for reaching a fi-nal solution. The tool resides completely on the client, and interfaces the database through APIs as opposed to previ-ous work that requires optimizer code extension. The APIs are used to simplify the workload queries, and to capture fact table predicates and costs necessary to make the recom-mendation. The predicate-driven method implemented by the tool is general, and it can be applied to any clustering or partitioning scheme based on simple field expressions or complex SQL predicates. Experimental results given a par-ticular workload will show that the recommendation from the tool outperforms a human expert. The experiments also show that the solution is scalable both with the workload complexity and the size of the fact table.
 H.2.2 [ Database Management ]: Physical Design; D.2.2 [ Software Engineering ]: Design Tools and Techniques Star Schema, Fact Table, Multi-Level Partitioning  X 
This work was done while Young-Kyoon Suh was at Tera-data Corporation.

Tables and materialized views in Teradata are hash dis-tributed based on a user-specified column or set of columns called primary index . Each virtual processor (unit of paral-lelism called AMP in Teradata) receives a subset of the data and stores it in hash order. Teradata users typically choose primary index fields, so that the data is evenly distributed among the AMPs. The primary index fields are also chosen to reflect join fields in workloads to accomplish cheaper local joinsthatdonotrequiredatashuffling.

PPI (Partitioned Primary Index) [8] is an optional hor-izontal partitioning scheme applied locally on each AMP X  X  data. Note that in other database products, this type of partitioning would most likely be called clustering .Forthe rest paper, we only use the term partitioning , not cluster-ing. Database Administrators (DBAs) usually choose the columns for PPI, based on join fields and single table pred-icates to optimize the queries in the workload. The PPI columns are used to physically cluster data with the same values together in contiguous data blocks. This allows par-tition elimination in scans and joins, which improve query performance. PPI can be specified as single or multiple (or nested) levels. This type of partitioning scheme is referred to as Multi-Level PPI or MLPPI [3].

By allowing non-qualified partitions to be eliminated, MLPPI can reduce significantly the amount of data to be scanned to answer a query. However, a large number of partitions can create significant overhead especially in joins and table maintenance operations such as inserts or deletes, so that the selection of partitions is usually a balancing act.
Now, let us see how the DBAs can define an MLPPI on the fact table from the Star Schema Benchmark (SSB) [6]. The SSB [6] is an extension of the TPC-H [9] benchmark, and it provides a Star Schema based on a retail model. The fact table LINEORDER is representative of orders along with their associated line items. There are four dimension tables covering date, customers, parts and suppliers.

Figure 1 represents a query set Q consisting of only two queries q 1and q 2 based on the SSB. Our focus is on provid-ing an MLPPI solution for the fact table in the star schema. Based on q 1and q 2 in Figure 1, there are five single ta-ble constraints on LINEORDER ,aslistedinFigure2;thecon-straints are identified by the query number and the sequence number of the constraint in each query in Q .

The DBAs need to look at partitioning options based only on the two fields and a total of the five constraints. Again, the constraints of interest are for the fact table. There are many possibilities from a fine-grained partition set to
Figure 2: Single Table Constraints on LINEORDER a loser definition. Considering for the time being the col-umn LO_DISCOUNT , for instance, one solution could be to identify each value for the IN predicate in q 1.1 and use as is. This yields the following partitioning expression for LO_DISCOUNT :
The above expression minimizes the size of the partitions by focusing exactly on the values required to satisfy the con-straints, but creates a large number of small partitions.
Another possibility would be to look at the maximum and minimum values in the IN set and to build an AND clause equivalent to a between clause yielding the following:
The above partitioning solution decreases the number of partitions, compared to the previous one, but still focuses sharply on the constraints with still relatively small parti-tions. Another possibility would be to make only two par-titions by using the minimum and maximum values of the overall constraints on LO_DISCOUNT as shown below:
Similar considerations apply to the partitioning associated with LO_QUANTITY this time with ranges and covering prob-lems difficult to deal with. The resulting partitioning scheme for the table includes both LO_DISCOUNT and LO_QUANTITY , so that the number of possible combinations is the product of the potential combinations for each field. Also, there are two queries, and one partitioning scheme may be better for one query or the other. As a result, the DBAs will be faced with a daunting combinatorial search problem and no clear basis to decide on which combination is the best. This state of affairs begs for a tool to assist the DBAs.
 In the sequel, we introduce an automated tool, called MLPPI wizard . Indeed, the wizard is the first physical database design tool developed at Teradata. The tool is based on a novel technique using a greedy algorithm for search space enumeration. This tool based on a general framework allowing general expressions, ranges and case ex-pressions for partition definitions is particularly well-suited for MLPPI definition. The predicate-driven technique used by the tool can be applied to any clustering or partitioning based on simple fields/expressions or complex SQL predi-cates. In addition, the wizard borrows the Teradata DBMS optimizer cost model, which is used to prune the search space for reaching a final solution.
 Figure 3 illustrates how the tool recommends the final MLPPI customized for a given workload consisting of a set of queries and corresponding weights. The wizard goes through a series of phases: Pre-processing Phase , Initial Phase , Opti-mized Phase . We will provide more details about each phase in Section 2, using the query set Q in Figure 1 in the rest of the paper.

To the best of our knowledge, there is no existing indus-trial tool to solve the multi-level partitioning problem except our wizard. The wizard is contained completely on the client side, as opposed to previous work [1, 4, 5] requiring optimizer code extension. Our tool simply leverages exist-ing APIs to simplify the queries and to capture fact table predicates and costs, and uses these items to make a recom-mendation. This makes the wizard extensible and portable to different releases of the Teradata DBMS server.
The paper is organized in the following way. Section 2 presents the algorithms for the wizard. Section 3 reports the performance evaluation results. Section 4 provides related work, and Section 5 concludes the paper.
In this section, we describe each phase of the wizard in more details, using the query set Q provided in Figure 1.
Algorithm 1: Pre-processing Phase input : Q (input query set) output : R (non-overlapping range set), M 1 Query Simplification (on Q ) 2 Range Extraction (from Q ) 3 Non-Overlapping Range ( R ) Construction 4 Query-to-Range-Set Map ( M ) Construction 5 Partition Count Limit Check (on R )
Algorithm 1 represents the pre-processing phase. First, we simplify the predicate(s) of each query by removing re-dundant conditions (in line 1). In the running example of Q , this simplification is not needed, as the queries are neat. Next, the tool locates the fact table fields in the simpli-fied predicates, extracts from the predicates, and groups the ranges along with each field (in line 2). The ranges, for example, can be derived from the predicates in Figure 2. The extracted whole range set then becomes a kind of two-dimensional array, called R , in which each entry under a field represents a pair of (start and end) values forming the range. In the continuing example, the range set R is formed by the entries below: R :
Any overlapping ranges found in R are broken into a pair of consecutive, non-overlapping ranges for a merge in further phases (in line 3). In the example, the split is applied on the overlapping ranges -(  X  , 30], [25 , 35] -under LO_QUANTITY , and after that, their common range -[25 , 30] -is inserted into R . Then, R will be like as below: R :
The tool in turn creates a bi-directional query-to-range-set map, called M , between Q and R (in line 4), as shown below:
M : where R [ i , j ] denotes the interval value for the i -thfieldand j -th range in R .

R may be used to define an MLPPI using each field as one level, as R produces a total of 16 partitions (= # ranges in LO_DISCOUNT  X  # ranges in LO_QUANTITY ) below the parti-tion count limit (65,536) allowed in an MLPPI (in line 5). Note that each field counts one more range corresponding to the  X  NO CASE OR UNKNOWN  X . In the running example, in-deed, the tool can immediately enter the optimized phase. However, to continue our discussion, let us assume that the limit is 15 in the paper. Then, the tool needs to proceed to the initial phase, so that the total partitions by R can drop below the limit.
In this phase, we will reach a feasible MLPPI solution by incrementally merging a range pair in R .

By and large, overall I/O cost may be increased by the merge. It is because when answering a query, from the merged partition we may read non-qualified rows that would not be found before the merge, thereby paying more I/O. For that reason, we will pick up and merge the range pair incurring the least I/O cost for all queries.

Algorithm 2: Initial Phase input : M (query-to-range-set map), R (input range output : R with # partitions  X  partition limit 1 while (# partitions by R&gt; partition limit) do 2foreach range pair ( rp )in R do 4foreach q in M do 5 L  X  Get the range set mapped to q in M 6i f ( rp  X  L ) =  X  then /* Affected */ 7 T s += w q  X  computeScanCost ( q , rp , L ) 8else T s += w q  X  sc q // sc q : q  X  X  existing 9end 10 Record T s with rp . 11 end 12 Update M and R along with rp with the least T s . 13 end 14 return R ;
Algorithm 2 describes the initial phase. Given M and R , partitions get incrementally reduced, as we consolidate the range pair producing the least I/O when merged. While R has more partitions than the limit (in line 1), for each range pair ( rp )in R , the wizard examines whether each query ( is affected by the merge of rp (in lines 2-6). To do so, the tool 1) gets the range set ( L )mappedto q from M ,and 2) sees if L includes either ranges of rp (lines 5-6). If gets affected by the merge of rp , we obtain the scan cost of q by sending a scan cost query ( s )of q to the server via an API call (in line 7). s can be built by 1) copying L into L , 2) removing and merging the ranges of rp from L , 3) adding the merged range in L , and 4) restoring the equivalent conjuncts ( C )from L , and 5) constructing a full scan query like  X  SELECT * FROM F WHERE C  X . Thereafter, the tool extracts the estimated block counts from the result of s , and uses it as the scan cost of q .Ifthemergeof rp does not affect q , then the previously computed q  X  X  scan cost is recycled to avoid the expensive API call (in line 8).
In this way, we can compute the total I/O cost ( T s )in-curred when rp is merged, by adding up the weighted scan costs of queries (in line 10). Once all range pairs are exam-ined, we pick up rp with the least T s for a merge. In the example, suppose that the range pair of R [2, 2] and R [2, 3] produces the least T s . Then, the ranges are merged, and and R are accordingly updated along with the chosen range pair (in line 12) as shown below: R : M :
We repeat the above steps until the number of partitions in
R drops below the maximum number of partitions that can be defined in an MLPPI. Now that the total partitions (12=4  X  3) by R are under the assumed limit (15), we face the optimized phase, carrying the up-to-date M and R .
In this regard, we can have an initial MLPPI recommenda-tion for the LINEORDER fact table, using R . However, having fewer partitions may enhance the overall workload perfor-mance, as the Teradata query optimizer has an operational threshold within which it can handle partitions simultane-ously. Hence, we attempt to incrementally improve the ini-tial MLPPI solution by further merging partitions in R .
Algorithm 3: Optimized Phase input : M (query-to-range-set map), R (input range output : MLPPI such that the total query cost is 1 T p  X  The total query cost obtained using the initial
MLPPI by R 2 while (Pre-defined iterations or R =  X  ) do 3foreach range pair rp in R do 5foreach q in M do 6 L  X  Get the range set mapped to q in M 7i f ( rp  X  L ) =  X  then /* Affected */ 8 T s += w q  X  computeQueryCost ( q , rp , M , R ) 9e lse T q += w q  X  qc q // qc q : q  X  X  existing 10 end 11 Map T q to rp . 12 end 13 T  X  Find the least T q 14 if T  X  T p then Reduce T p to T , and update M 15 else break 16 end 17 return an (optimized) MLPPI by the current R ;
Algorithm 3 describes the optimized phase. We first ob-tain the weighted query cost sum ( T p )of Q using an initial MLPPI by R (in line 1). (The way of obtaining the query cost will be shortly described.) Next, until the given num-ber of iterations reaches, or R has no ranges (in line 2), for each range pair ( rp )in R , the tool computes the weighted query cost sum ( T q )on rp (in lines 3-12). If q is affected by the merge of rp (in line 7), the query cost of q , instead of the scan cost of q , is (re)computed and added to T q (in line 8). To get the query cost, we reflect the merge of rp into M and then update R . After that, via an API call, the tool requests the server to return the query cost -estimated processing time -of q on the fact table supposedly applying an MLPPI definition using R . (This sounds similar to the  X  what-if  X  mode proposed in previous literature [2].) Unless the merge affects q , the existing query cost of q is recycled, and its weighted cost is added to T q (in line 9). Once all range pairs are examined, the tool finds the least T q ( T (with the range pair producing T ) (in line 13). If T is less than or equal to T p , then the tool reduces T p to T ,updates M and R along with the range pair with T ,andrepeats the iteration (in line 14). Otherwise, the tool finishes the phase, recommending an (optimized) MLPPI solution us-ing the most up-to-date R (in lines 15-17). In the running example, suppose that in the first iteration ranges R [1, 1] and R [1, 2] are chosen and merged, but in the next iteration there is no further merge that can reduce T p .Then,wecan see the final MLPPI recommendation for Q , as shown below.
In this section, we present our performance evaluation re-sults. Section 3.1 describes our environment settings -the workload, software and hardware used. Section 3.2 shows the actual results of our solution, and compares them with those of no partitioning and typical solutions by DBAs. The performance evaluation was done on the Teradata DBMS server machine running Unix. We generated two workloads consisting of 10 queries (10Q) or 20 queries (20Q) for the experiments. Each query is based on a template that joins the LINEORDER fact table and the DDATE dimension one with constraints defined by the predicates. This template is common in customer cases like reports and form templates. We ran the above 10Q and 20Q query sets on two size vari-ants of the fact table LINEORDER in the SSB data model. In the first case, LINEORDER is loaded with 1 Terabyte of data (1TB), and in the second case it is populated with 3 Terabytes of data (3TB).
We compared the performance of our partitioning solu-tions (WIZARD) with the other solutions -no partitioning (NOPPI) and the partitioning (EXP) by a human expert. We obtained EXP and WIZARD solutions for 10Q and 20Q on the populated 1TB and 3TB tables. In turn, we applied the corresponding EXP and WIZARD solutions to, and then ran 10Q and 20Q on the altered tables. For the NOPPI so-lution, we just executed 10Q and 20Q on the raw 1TB and 3TB fact tables with no MLPPI.

Figure 4 represents the measured total elapsed times (in secs) of 10Q and 20Q on each of the fact tables. As shown in Figure 4(a), we observe in our experiments on the 1TB table that WIZARD shows around 4.36x and 2.30x (running time) improvements on 10Q, and 4.34x and 1.85x on 20Q, compared with those of NOPPI and EXP, respectively. The improvements still maintained in spite of doubly increased workload size (from 10Q to 20Q). Even in the experiment 1000 2000 3000 4000 5000 6000 7000
Total Elapsed Time (secs) using the 3TB fact table, the WIZARD solutions yielded about 4.29x and 1.82x (running time) speedups on 10Q, and 3.85x and 1.44x on 20Q in comparison with those of the NOPPI and EXP ones, as seen in Figure 4(b). Although the size of the fact table became 3x larger, the performance of WIZARD did not suffer from any degradation.
The MLPPI wizard seems similar to some DBMS vendors X  tools from Oracle [7], IBM DB2 [4], and MS SQL Server [1]. However, there is no published technical detail about Oracle Partitioning Advisor [7], and as far as the other tools are concerned, our wizard is fundamentally different from them in light of problem scope and approach.

DB2 MDC Advisor [4] actually tackles a different problem of automatically recommending the most well-suited MDC keys for a given workload. Agrawal X  X  work [1] discusses an-other problem of merging single-level range partitioning son objects such as tables or indexes. This paper, however, ad-dresses the multi-level partitioning problem. Hence, the ex-isting solutions cannot be directly applied to our wizard.
Regarding the approach, DB2 MDC Advisor [4] uses the search space driven by fields . Meanwhile, our search space is driven by query-predicates , which is superior because predi-cates are more customized to a workload. Agrawal X  X  scheme [1] also uses the search space driven by simple range predicates. However, his approach cannot reach a globally-optimized so-lution, as his work first produces a solution customized to each query and then consolidates solutions between queries. On the other hand, our wizard generates the whole search space upfront and subsequently merges partitions, leading to a globally-optimized recommendation. Moreover, only a single column is considered in his work [1], whereas our tool deals with multiple fields.

Furthermore, implementations of previous tools [1, 4, 5] required instrumentation for optimizer code. These instru-mentations are needed to facilitate the required information for the physical design tools API calls. The instrumentation code need to be enhanced and tested for new database re-leases which add complexity and additional cost for software upgrades. The Teradata optimizer has a rich set of existing APIs originally coded for system and workload management tools. These APIs are sufficient for the MLPPI functionality which helped us avoid the costly optimizer code change and instrumentation.
Given workloads, it is very hard for DBAs to select the appropriate fields in partitioning the fact table due to large search space. To help relieve the DBAs X  concern, we pre-sented the Teradata MLPPI wizard that recommends a fact table multi-level partitioning solution customized for a work-load. The tool derives its search space from the query predi-cates of the input workload, incrementally reduces the space by merging the range pair with the least scan or query cost, and eventually recommends an optimized MLPPI solu-tion for the workload. We demonstrated that the produced MLPPI solutions by the wizard could improve the workload execution cost by up to about more than a factor of four and two, compared with those of no partitioning approach and partitioning done by a human expert, respectively. The MLPPI solutions of our wizard also scaled well with increas-ing workloads and fact table sizes.
We would like to thank Prof. Bongki Moon for his in-sightful comments.
