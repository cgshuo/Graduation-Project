 Retrievability provides a different way to evaluate an In-formation Retrieval (IR) system as it focuses on how eas-ily documents can be found. It is intrinsically related to retrieval performance because a document needs to be re-trieved before it can be judged relevant. In this paper, we undertake an empirical investigation into the relation-ship between the retrievability of documents, the retrieval bias imposed by a retrieval system, and the retrieval perfor-mance, across different amounts of document length normal-ization. To this end, two standard IR models are used on three TREC test collections to show that there is a useful and practical link between retrievability and performance. Our findings show that minimizing the bias across the doc-ument collection leads to good performance (though not the best performance possible). We also show that past a cer-tain amount of document length normalization the retrieval bias increases, and the retrieval performance significantly and rapidly decreases. These findings suggest that the rela-tionship between retrievability and effectiveness may offer a way to automatically tune systems.
 Categories and Subject Descriptors: H.3.3 [Informa-tion Storage and Retrieval] : Information Search and Re-trieval -Performance Evaluation Terms Theory, Experimentation Keywords Retrievability, Simulation
Traditionally IR systems are evaluated in terms of effi-ciency and performance [13]. However IR systems can also be evaluated in terms of retrievability [3], which assesses how often documents are retrieved (as opposed to the speed of re-trieval and the quality of retrieval respectively). Retrievabil-ity is fundamental to IR because it precedes relevancy [3]. In this paper, we investigate the relationship between retriev-ability and retrieval performance in the context of ad-hoc topic retrieval for two standard best match retrieval models. To this end, we shall first formally define retrievability be-fore discussing how it relates to performance. Next, we per-form an empirical analysis where we hypothesize that lower retrievability bias will lead to better retrieval performance.
In [3], retrievability was defined as a measure that provides an indication of how easily a document could be retrieved under a particular configuration of an IR system. Put for-mally the retrievability r ( d ) of a document d with respect to an IR system is: where q is a query in a very large query set Q , k dq is the rank at which d is retrieved given q , and f ( k dq ,c ) is an access function which denotes how retrievable the document d is for the query q given the rank cutoff c . This implies that retrievability of a document is obtained by summing over all possible queries Q . The more queries that retrieve the document, the more retrievable the document is.

A simple measure of retrievability is a cumulative-based approach, which employs an access function f ( k dq ,c ), such that if d is retrieved in the top c documents given q , then provides an intuitive value for each document as it is sim-ply the number of times that the document is retrieved in the top c documents. A gravity based approach instead weights the rank at which the document is retrieved, as fol-lows: f ( k dq ,g ) = 1 /k dq g . This introduces a discount g , such that documents lower down the list are assumed to be less retrievable.
 Retrievability Bias : To quantify the retrievability bias across the collection, the Gini coefficient [8] is often used [3, 5, 6] as it measures the inequality within a population. For example, if all documents were equally retrievable according to r ( d ), then the Gini coefficient would be zero (denoting equality), but if all but one document had r ( d ) = 0 then the Gini coefficient would be one (denoting total inequality). Usually, most documents have some level of retrievability and the Gini coefficient is somewhere between zero and one. Essentially, the Gini coefficient provides an indication of the level of inequality between documents given how easily they can be retrieved using a particular retrieval system and con-figuration.
 Uses : Retrievability -and the theory of -has been used in numerous contexts, for example, in the creation of re-verted indexes that improve the efficiency and performance of retrieval systems by capitalizing on knowing what terms within a document makes that document more retrievable [10]. Retrievability has also been used to study search engine and retrieval system bias on the web [2] and within patent col-lections [4] to improve the efficiency of systems when prun-ing [14]. It has also been related to navigability when tagging information to improve how easily users browsing through the collection could find documents [12]. While these show that retrievability is useful in a number of respects, here, we are interested in how retrievability relates to performance. There have been a few works exploring this research direc-tion in different ways [3, 1, 5, 6].
 Relating Retrievability and Performance : In [1], Az-zopardi discusses the relationship with respect to the defi-nition of retrievability and claim that a purely random IR system would ensure equal retrievability (resulting in a Gini = 0). However, the author argues that this would also re-sult in very poor retrieval performance. Conversely, if an oracle IR system retrieved only the set of known relevant documents, and only these documents, then there would be a very high inequality in terms of retrievability across the collection. They suggest that neither extreme is desirable and instead suggest that there is likely to be a trade-off be-tween retrievability and retrieval performance. In [3], it is acknowledged that some level of bias is necessary because a retrieval system must try and discriminate relevant from non-relevant. The preliminary study conducted in [1] inves-tigating this relationship on two small TREC test collections showed that as retrieval performance increased, the retriev-ability bias tended to decrease: suggesting a positive and useful relationship.
 In [5], Bashir and Rauber studied the effect of Pseudo Relevance Feedback (PRF) on performance and retrievabil-ity. They found that standard Query Expansion methods, while increasing performance, also increased the retrievabil-ity bias. To combat the increase in bias, they devised a method of PRF that used clustering; this resulted in a re-duction in bias, as well as an increase in performance over other QE techniques. When employing their PRF technique to patent retrieval, they showed that the decrease in bias led to improved recall for prior art search [6]. This later work suggested that there may be a positive correlation between recall and retrievability. In [7], they compared a number of retrieval models in terms of retrievability bias and perfor-mance as a way to rank different retrieval systems. They found that there was a positive correlation between the bias and performance for standard best match retrieval models. It is important to note that since retrievability can be esti-mated without recourse to relevance judgements it provides an attractive alternative for evaluation.
Previous work suggests that the relationship between re-trievability bias and retrieval performance is much more complicated than previously thought and the relationship is somewhat unclear. In this paper, we shall perform a more detailed analysis across larger TREC collections and explic-itly plot the relationship between the retrievability bias and retrieval performance in order to form a deeper understand-ing of how they relate to each other.
 To this end, three TREC test collections were used AP, Aquaint (AQ) and DotGov (DG) along with their corre-sponding ad-hoc retrieval topics (see Table 1 for details). These collections had stop words removed and were Porter stemmed. For the purposes of this study, we will focus on the effect of document length normalization within two stan-dard retrieval models (Okapi BM25 and DFRs PL2). This is due to the fact that retrieval models will often favour longer documents which introduces an undesirable retrieval bias that if accounted for, has been shown to improve per-formance [11]. For each of these retrieval models we shall investigate how retrievability bias and performance changes as we manipulate the normalization parameter b and c re-spectively. In our experiments for BM25 X  X  b parameter we used values between 0 and 1 with steps of 0.1 1 , and for PL2 X  X  c parameter we used values 0 . 1 , 1 ,... 10 , 100 with steps of 1 between 1 and 10.

For each collection, model and parameter value, we recorded the retrieval performance (Mean Average Precision (MAP) and Precision @ 10 (P@10)) and calculated the retrievability bias using the approach employed in prior work [3, 1, 5, 7]. This was performed as follows; we extracted all terms that co-occurred more than once with each other from the col-lection and used these as bigram queries (Table 1 shows the number of queries per collection issued). These queries were issued to the retrieval system given its parametrization and the results were used to compute retrievability measures. We computed both cumulative scores with cut-offs of 5, 10, 20, 50, 70, 100, and gravity based scores with cut-off of 100 and  X  (where  X  is the discount factor) values of 0.5, 1, 1.5 and 2. For brevity, we only report on a subset of these. These were used to compute the corresponding Gini coefficient for each measure, model, normalization parameter and collection.
During the course of our analysis we examined the follow-ing research questions: How does the retrievability bias change across length normalization parameters? In Figure 1, the left hand plots show the relationship between the retrievability bias (denoted by the Gini coefficient for different retrievability measures) and the parameter settings for BM25 and PL2. Immediately we can see that as the parameter setting is manipulated, the retrievability bias changes. For BM25, the minimum bias was when b=0.7 on AP and AQ, and b=0.9 on DG, whereas on PL2 the minimum bias was was between c=1 and c=3 (see Table 2). This was regardless of the Gini measure used i.e. the cumulative and gravity based mea-sures resulted in the same minimum. Subsequently, for the remainder of this paper, we will use the gravity based re-trievability measure when g = 1. What was different be-tween Gini measures was the magnitude of the bias. For example, when the cut-off is low then the bias observed was higher than when the cutoff was increased. This finding is consistent with prior work [3]. When the parameter value of the retrieval was increased or decreased, then we observed that retrieval biased increased, sometimes quite dramati-cally. This suggests that either longer or shorter documents were being favoured depending on the setting of b and c . In BM25, as b tends to 1, longer documents are penalized, while when b tends to 0, longer documents are favoured. For PL2, as c tends to 0 then longer documents are penalized, and as c tends to infinity, longer documents are favoured. How does the retrievability of documents change across length normalization parameters? To examine this intuition, we sorted the documents in each collection by length and placed them into buckets. We then computed the mean length within each bucket, and the mean retrievability r ( d ) given the documents in the bucket. The plots on the right in Figure 1 show how the retrievability changes when the length normalization parameter changes on BM25/PL2 and AQ/DG for a subset of parameters with a retrievabil-ity measure of g = 1. The plots clearly indicate that when the retrieval bias is minimized (for example, when b = 0 . 7 on AQ) the retrievability across length is about as equal/fair as it gets across the collection. This shows that the measures of retrievability bias (i.e. the Gini coefficients) capture the bias towards longer and shorter documents as the document length parameter is manipulated.
 What is the relationship between Performance and Retrievability? In Figure 2, we plotted the Gini coeffi-cient against MAP for each collection using the results from BM25. Similar plots can be found in Figure 3 for other re-trieval models and other performance measures. On these plots, a large diamond has been added to the lines to indi-cate the length normalization parameter that favours longer documents (ie. b = 0 and c = 100). As the parameter value is increased/decreased for BM25/PL2 the retrieval model setting favours shorter documents (as shown in the plots in Figure 1).

The plots provide a number of key insights into the rela-tionship between retrievability bias and performance. Firstly, the relationship is non-linear with a trend that suggests that minimizing bias leads to better performance. If we consider the relationship in Figure 2 for DotGov, we can see that a reduction in bias leads to successive improvements in terms of performance for MAP (and P@10). With this collection, the effect is the most pronounced and minimizing retrieval bias does tend to the best retrieval performance. Once too much length normalization has occurred, such that shorter documents are overly favoured, then the retrievability bias increases and performance begins to decrease.

When we examine the relationship for AP and AQ, we see that as shorter documents become more favoured, a trade-off between bias and performance quickly develops, but again once the minimum bias point has been reached, the kick back (i.e. loss in performance and increase in bias) is much more pronounced. These findings suggest that as document length normalization is applied in order to penalize longer documents, going past the point of minimum bias will de-grade performance and retrievability.

Table 2 shows the performance of the retrieval models when bias is minimized and when performance is maximized. Included in the table are the results of t-tests conducted to determine whether there was a statistical difference in per-formance between the two settings (assuming p &lt; 0 . 05,  X  indicates a significant difference). For AP and AQ, we found there was a significant difference for BM25 (both MAP and P@10), however the differences were less pronounced for PL2 and on DotGov. These findings suggest that minimizing bias may not lead to the best performance. It does however, tend to give reasonably good retrieval performance that on many occasions, is not significantly different from the best possi-ble retrieval performance. We speculate that the mis-match between may be due to the length bias with the TREC rel-evance pools as discovered in [9]. Thus, in the absence of relevance judgements tuning a retrieval system such that it minimizes retrieval bias is likely to be a good starting point. Gini Figure 2: MAP vs. Gini for all collections on BM25.
In this paper we have analysed the relationship between retrievability bias and performance in the context of ad-hoc retrieval across length normalization parameters for two standard IR models. Empirically, we have shown that the relationship is much more complex that previously thought and in fact non-linear. Nonetheless, we have shown that reducing the bias within the collection leads to reasonably good performance, and crucially, if too much document length normalization is performed then this will invariably result in a degradation of performance and an increase in bias. This is a useful finding suggesting that retrievability could be used to tune retrieval systems without recourse to rele-vance judgements. Future work will be directed to studying the relationship between retrievability and performance (in terms of MAP and P@10) in more detail across other col-lections and across different parameter settings (i.e. query parameters, smoothing parameters) and with different re-trieval models (language models, query expansion, link/click evidence, etc).
 Acknowledgments This work is supported by the EPSRC Project, Models and Measures of Findability (EP/K000330/1). [1] L. Azzopardi and R. Bache. On the relationship [2] L. Azzopardi and C. Owens. Search engine Gini Figure 3: MAP (Top plots) and P@10 (Bottom plots) vs. Gini for all collections on BM25 (right plots) and PL2 (left plots). [3] L. Azzopardi and V. Vinay. Retrievability: An [4] R. Bache. Measuring and improving access to the [5] S. Bashir and A. Rauber. Improving retrievability of [6] S. Bashir and A. Rauber. Improving retrievability of [7] S. Bashir and A. Rauber. Improving retrievability and [8] J. Gastwirth. The estimation of the lorenz curve and [9] D. E. Losada, L. Azzopardi, and M. Baillie. Revisiting [10] J. Pickens, M. Cooper, and G. Golovchinsky. Reverted [11] A. Singhal, C. Buckley, and M. Mitra. Pivoted [12] A. Singla and I. Weber. Tagging and navigability. In [13] C. J. van Rijsbergen. Information Retrieval . 1979. [14] L. Zheng and I. J. Cox. Document-oriented pruning of
