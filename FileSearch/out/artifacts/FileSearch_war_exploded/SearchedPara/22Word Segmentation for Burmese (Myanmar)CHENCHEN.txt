 CHENCHEN DING, YE KYAW THU, MASAO UTIYAMA, and EIICHIRO SUMITA , Word segmentation to determine the boundaries of words for those languages with-out word separators in orthography is a basic task in natural language processing. For Asian languages, most research on this task has focused on the segmentation and morphological analysis of Chinese, Japanese, and Korean, for which the standard, state-of-the-art technique using conditional random fields has achieved satisfactory performance [Zhao et al. 2010; Kudo et al. 2004; Na 2015]. In this note, we focus on applying word segmentation techniques to an understudied language, Burmese. 1 Pre-vious studies on Burmese word segmentation are scarce and scattered; most adopted a simple forward maximum matching approach and used extra rules with cases [Hla Hla Htay and Murthy 2008; Tun Thura Thet et al. 2008; Win Pa Pa and Ni Lar Thein 2008]. An important reason for this is that there are few linguistically annotated resources for Burmese; thus, only dictionary-and rule-based approaches are feasible. An approach integrating dictionaries and unsupervised Bayesian models was reported recently [Ye Kyaw Thu et al. 2014]. However, in that report, the best performance was still achieved by forward maximum matching (Table 1 in Ye Kyaw Thu et al. [2014]), and the training and test data were quite limited in the experiments. The work of this note is based on an in-house annotated Burmese version of Basic Travel Expression Corpus , translated form the original English-Japanese ver-sion [Kikui 2003] and manually processed by Burmese-native speakers to segment sentences into words. At present, we have 62,000 segmented sentences with approxi-mately 480,000 words. Although the data size is still not comparable to the large-scale corpora of Chinese and Japanese, statistical approaches for Burmese word segmenta-tion are still feasible. In this study, we test and compare classic and state-of-the-art word segmentation techniques, including crisp forward , backward ,and bidirectional maximum matching approaches, N-gram language model approaches using Viterbi de-coding, a support vector machine (SVM) approach, and a standard sequence labeling framework using conditional random fields (CRF). From the experimental results, we discover that the machine learning approach of SVM and CRF and the Viterbi decoding using a tri-gram language model with modified Kneser-Ney discounting have nearly the same good performance, where the F-score on the word boundary insertion is as much as an average of . 97 in cross-validation on training data and . 98 on a test set. The experimental results indicate that word segmentation of Burmese at least is not a more difficult task than for the other languages, for which well-developed, off-the-shelf techniques can work well. In addition to the numerical results, we further discuss the linguistic features of Burmese and suggest the direction of future work on processing it.

The remainder of the note is organized as follows. In Section 2, we provide an in-troduction to the Burmese language and its script and also discuss the syllable struc-ture and syllable segmentation. In Section 3, we describe the word segmentation ap-proaches. In Section 4, we report experimental settings and results. In Section 5, we discuss the experimental results. Section 6 draws conclusions and describes future work. Morphologically, Burmese is highly analytic with no inflection of morphemes. Simi-larly to Chinese, morphemes can be combined freely with no changes. In language classification, Burmese and Chinese both belong to the Sino-Tibetan language family, etymologically sharing common basic vocabulary.

Syntactically, Burmese is typically head-final where the functional dependent mor-phemes succeeding content independent morphemes and the verb constituent working as the root of a sentence always comes at the end of a sentence. Subordinative clauses are also placed before their modifying parts and before the main clause of a sentence. These features are similar to Japanese and Korean, two typical agglutinative, head-final languages in east Asia. 2
Orthographically, there is no specific rule or convention on the use of spaces to sepa-rate words in Burmese, and spaces are actually used in Burmese texts inconsistently to segment meaning constituents. One intrinsic problem is that the definition of  X  word  X  is not intuitive for Burmese; this is also a problematic issue in processing Chinese, Japanese, and Korean. In Chinese, the problem has been handled in a very practical way by combining segmentation guidelines , lexicon ,and segmented corpus [Huang and Zhao 2007]. In Japanese, the analysis of sentences actually avoids the definition of  X  X ord X  and is established practically on the concept of  X  morpheme . X  However, there are still several parallel morpheme systems for Japanese, depending on different defini-tions and sets of parts-of-speech (POS). In Korean, spaces are used to separate words in orthography under a general principle that dependent morphemes are to be attached to independent morphemes, but there are cases for which it is difficult to judge whether a space is to be used even for Korean-native speakers, and the insertion of space diverges among individuals. In this note, the  X  Burmese word  X  we refer to is comparable to the morpheme of Japanese, that is, the minimum meaning unit judged by native speakers, disregarding the dependence of morphemes. Consequently, inconsistency among dif-ferent Burmese-native speakers cannot be avoided. Here, we first provide an intuitive example in Figure 1 to illustrate the Burmese word we refer to in this note. Detailed discussion of this issue will be provided in Section 5. The Burmese script is an abugida ( alphasyllabary ) in the Brahmic family, containing 33 basic consonant letters. Each standalone consonant letter can form a complete syllable by itself with the help of an inherent vowel (/ a changed to other vowels through the use of various diacritic marks. Further diacritic marks include tone marks, 3 dependent consonant marks for syllable onset clusters, 4 and a virama ( X  asat  X , which means kill in Burmese) used to suppress the inherent vowel of a consonant letter for nasal or glottal syllable codas. 5 Basic consonant letters can be  X  X tacked X  in ancient loanwords from Pali and in some modern loanwords from English. We show two examples in Figures. 2 and 3 to illustrate the features of Burmese script.
As described and illustrated by the examples, basic consonant letters, around which optional modifications are added, comprise syllables in the writing system of Burmese. Consequently, the syllable boundaries in Burmese texts are orthographically unam-biguous in most cases and can be determined using trivial rules, that is, all kinds of diacritics and asat -ed consonant letters must not be separated from the neighboring basic consonant letter they modify. A complex case is the stacked consonants, where two syllables are partly merged, and the boundary is positioned between the vertically stacked consonants. However, the contracted syllables do not create confusion in tex-tual processing 8 because they never cross word (morpheme) boundaries just as general, simple syllables. 9 For brevity and clarity, the term  X  syllable  X  X fBurmesethatweusein this note from now on is used to refer to the unbreakable units composed of basic and dependent letters, which are actually single syllables in most cases and also contain contracted multiple-syllable blocks with stacked consonant letters. Formally, the task of Burmese word segmentation is the process of inserting spaces into textual data without other replacing or rewriting operations. As mentioned in the introduction to Section 2.2, syllables are unbreakable writing units in Burmese, with the result that all approaches tested in this note first apply a syllable segmentation process on input, that is, insert spaces into syllable boundaries, and then decide how the syllables form words, that is, delete spaces between syllables. For a general machine learning approach, the task becomes to be a binary classification problem for syllables based on whether the succeeding space is to be deleted. The task is exactly the same as Chinese word segmentation if the syllables in Burmese are considered as Chinese characters. The approaches tested in experiments are described as follows. Maximum matching by looking in a prepared dictionary and matching the longest sub-string in an input sentence is a classic word segmentation approach for Chinese. The matching can be conducted from the beginning of a sentence to its end or in reverse. The former method is referred to as forward maximum matching ( fmm ) and the latter as backward (or reverse ) maximum matching ( rmm ). The two directional processes can be combined to form a bidirectional maximum matching ( bmm ), in which further heuristic rules are used to select the better result from those of fmm and rmm . Specifically, we calculate the probabilities of segmentation using uni-grams to decide which result is better for our experiments.

Although the maximum matching approach is simple, its performance can be mediocre, and it is typically used as a baseline approach in Chinese word segmen-tation tasks (e.g., Sproat and Emerson [2003]). Because the simplicity and speed of this classic approach is a great advantage, the approach is still widely used in practical engineering and has been studied in recent research (e.g., Sassano [2014]). Word segmentation using a statistical language model is more reasonable than dictionary-based matching, because it uses the probabilities of words in real textual data. Consequently, segmentation results containing more common words are better than those containing obscure words. Statistical approaches correspondingly require more training data than a dictionary-based approach.

Given a statistical language model, for example, an N-gram language model, the word segmentation task becomes a search problem to determine the segmentation with the highest probability according to the model. A simple Viterbi-like (or Dijkstra-like) dynamic programming algorithm can be applied by scanning an input sentence to generate the best segmentation up to each syllable until the best segmentation of the entire sentence is constructed. We attempted several N-gram language models in ex-periments, including the maximum-likelihood estimated uni-gram model ( uni.mle ), the absolute discounting (  X  0 . 5) uni-gram model ( uni.abs ), and the modified Kneser-Ney discounting uni-, bi-, and tri-gram models ( uni.mkn , bi.mkn ,and tri.mkn , respectively). For bi.mkn and tri.mkn , interpolated probabilities were used when there were high-order N-grams, and otherwise the probability (also interpolated) of the corresponding one-order lower N-gram was used. 10 As mentioned, the word segmentation task can be treated as a classification task in a machine learning framework, or, more specifically, a sequence labeling task, due to the properties of textual data, and several standard learning frameworks have been established and developed. We tested two machine learning approaches, CRF in the CRF++ toolkit 11 [Lafferty et al. 2001; Sha and Pereira 2003] ( crf ) and SVM in the KyTea toolkit 12 [Neubig et al. 2011] ( svm ).

Feature engineering for input and tag-set design for output are import issues for machine learning approaches. In experiments, the input features we used are only N-gram of syllables , 13 because our data have quite simple structures, in which only spaces of word boundaries are inserted and no extra information is available. The tag-set of output is also restricted to being binary. We consider that our data is still limited for large-scale machine learning, hence we postpone the work of sophisticated tuning to future work when more data become available. As mentioned, we currently have 62,000 manually segmented sentences. We separate these into two sets for training and test, containing 60,000 and 2,000 sentences, re-spectively. Details of the data sets are listed in Table I. As for the vocabulary, the entire training set contains 16,379 different types of words, covering 98 . 5% of the words in the test set. 14 On the syllable level, the entire training set has 2,522 different types of syllables, covering 99 . 9% of syllables in the test set.

The settings of the matching-based and statistical approaches are as described in the previous section. As to handling unknown words, the matching approaches skip one syllable and moves to the next syllable for further matching. The length of unknown words was also restricted to one syllable and given a penalty in the decoding of sta-tistical approaches. Consequently, unknown words tended to be segmented excessively in matching and decoding. For the SVM and CRF machine learning approaches, we basically used up to 5-gram on syllables as features for input. Specifically, the -charn and the -charw options were set to five for KyTea. 15 The features used for CRF++ are S n ( k lables, where it judges whether to insert a word boundary after the syllable S 0 . The performances of compared approaches were measured by the precision, recall, and F-score on word boundary insertion ; that is, the task was taken purely as a classification problem, even for those rigid matching approaches.

The results of the cross-validation on the training set are listed in Table II, where we split the training set into six subsets of 10,000 sentences; each subset was used for test, and other five subsets for training in turn. The first syl column lists the results of inserting a word boundary after each syllable. Because the process inserted all possible boundaries, the recall is simply 100%; nearly 40% of the insertions are superfluous in terms of the precision. The forward and backward maximum matching approaches achieved F-scores of approximately . 92, which were not low. The bidirectional match-ing using a uni-gram model obviously performed better than single-direction matching, from which we find that even a simple statistical model does work. The probabilities of the uni-grams we used for bidirectional matching were estimated by maximum-likelihood. We found that further discounting approaches could not improve the per-formance of bidirectional matching, because the probabilities were used only to select the better of two candidates.

The statistical approaches using uni-gram language models achieved F-scores of approximately . 95, which were significantly higher than those of the matching ap-proaches. Compared with the maximum-likelihood estimated uni-gram model, further discounting approaches provided a few gains, indicating that better statistical mod-els can lead to better performance. When the order of the language model increased to bi-grams, the performance improved, but a tri-gram model did not provide further improvement. We also tried a 4-gram language model, but this made no difference compared with the tri-gram model. The results show that a well-estimated bi-gram language model is sufficient for a statistical approach.

The SVM and CRF machine learning approaches provided the best performance in cross-validation. Generally, they achieved nearly the same performance, with the CRF being slightly better. It can be seen that statistical approaches have much higher recalls than SVM and CRF. We attribute this phenomenon to the overall excessive segmentation of unknown words, which were split into syllables in decoding.
Table III shows the F-scores of the compared approaches on the 2,000-sentence test set. The results are generally identical to those of cross-validation. However, the difference between SVM, CRF, and the bi-and tri-gram language models has nearly disappeared with the increase in training data. This phenomenon confirms our hypoth-esis that the difference of these approaches was due primarily to the unknown words, for which a machine learning approach performs better than a language model-based approach. The last row of Table III is the  X  X heating X  result, for which the words in the test set are collected as a dictionary and used in maximum-likelihood estimation for a uni-gram model. We consider our experimental results to be satisfactory compared with the cheating results. Hence, we conclude that a general machine learning framework is sufficiently powerful to handle the Burmese word segmentation task; a well-estimated bi-or tri-gram language model also works well, considering the simplicity of N-gram language models. As illustrated, the experimental results were good in terms of the numerical mea-sures. However, several problems still need to be discussed concerning our temporarily manually annotated corpus, the basis of the experiments.
 As described, the data used in our experiments are based on a basic travel expression corpus, which contains primarily colloquial words, simple expressions, and short sen-tences. Table I shows that the average length of sentences is approximately only eight words. Although the vocabulary is of considerable size with more than 16 , 000 words, there is a problem of inconsistency in manual annotation, which will be discussed later. Broadly, the vocabulary in our experiment is unbalanced and limited, and the evalua-tion is in-domain, that is, restricted to the particular field of the corpus. The compared approaches in this note must be tested further on various text genres.

There is a special stratum in the Burmese vocabulary, consisting of the literary words (e.g., Buddhistic and academic words) derived from Pali. 16 One typical phenomenon is that stacked consonants, which are usually used in spelling Pali derived words, 17 are rare in our data. 18 This not only leads to low coverage of vocabulary but also results in many syllable combinations remaining unseen for machines learning approaches. Hence, the trained models based on our current data are still not practical for processing real text, such as news articles. The construction of annotated corpora of Burmese is still in progress and is in need of much more development. As stated in our introduction to the Burmese language, the term  X  X ord X  does not have a clear, natural definition. In this note and in the experiments, a word of Burmese is treated as a minimum unit of meaning. However, consistency in annotation was not always achieved.

We investigated the vocabulary of the training set. In Chinese word segmentation, there are two typical ambiguities, combinational and overlapping .Let  X  ,  X  ,  X  be strings of characters. A combinational ambiguity arises if  X  ,  X  ,and  X  X  are all words at the same time, and an overlapping ambiguity arises when  X  X  and  X  X  are words at the same time. In the training set vocabulary of 16,379 words, we found 5,086 instances of combinational ambiguities and 1,358 instances of overlapping ambiguities. It is noteworthy that the number of combinational ambiguities is almost nearly one third of the entire vocabulary, largely due to inconsistencies in the manual segmentation. Here, we discuss a typical example.

Figure 4 illustrates a Burmese compound suffix for nouns. The meanings of the entire expression and of the partial morphemes can be compared with their Japanese and Korean glosses. As our segmentation standard, the Burmese suffix must be segmented into three words . Actually, it appears 40 times in the training data, being split into three words 31 times. For the remaining occurrences, the entire suffix is taken as a single word for six times, and the last two morphemes are taken as a single word for three times (i.e., a space is inserted between the first and second morphemes). The result is convincing that this is a very stable compound suffix in Burmese and that a casual annotator might not realize that it can be analyzed even further (or not consider that it requires analysis). Additionally, the connection between the last two morphemes is stronger than that between the first two morphemes, hence there are annotators that separate only the first morpheme as a word but no annotators that separate only the last morphemes as a word. Interesting facts can be observed in the Japanese and Korean translations of the suffix. The Japanese translation can be or . In analysis using the IPA system for Japanese morphemes, the previous one is separated into three morphemes, as shown in Figure 4, while the latter is taken as a single morpheme. This might be due to the native speakers X  sense that will be inserted between the first and the second corresponding morphemes, because the first and the last morphemes are dependent ones that cannot be separated from their preceding independent morphemes.

From the example, to define  X  X ord X  for Burmese, as in the case for Japanese and Ko-rean, we must consider the linguistic and practical aspects at the same time. Although it is difficult to provide an ultimate unambiguous definition, a more standardized prin-ciple must be established. 19 Broadly, the word segmentation task must be merged into a POS tagging framework, in which a well-defined POS tag set will provide a better explanation of what a word is. This is a long-term project that we intend to conduct gradually in the future. We conducted a systematic comparison of various Burmese word segmentation ap-proaches based on a manually annotated corpus. The experimental results illustrated that state-of-the-art techniques can process Burmese textual data well. We also dis-cussed several problems in the construction of the Burmese corpus. Our preliminary future work is to continue the construction of an annotated corpus 20 and to define a standardized POS system for practical natural language processing of Burmese.
