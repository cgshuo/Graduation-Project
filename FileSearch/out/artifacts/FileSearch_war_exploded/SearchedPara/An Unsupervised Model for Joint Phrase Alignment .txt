 The training of translation models for phrase-based statistical machine translation (SMT) systems (Koehn et al., 2003) takes unaligned bilingual train-ing data as input, and outputs a scored table of phrase pairs. This phrase table is traditionally gen-erated by going through a pipeline of two steps, first generating word (or minimal phrase) alignments, then extracting a phrase table that is consistent with these alignments.

However, as DeNero and Klein (2010) note, this two step approach results in word alignments that are not optimal for the final task of generating phrase tables that are used in translation. As a so-lution to this, they proposed a supervised discrimi-native model that performs joint word alignment and phrase extraction, and found that joint estimation of word alignments and extraction sets improves both word alignment accuracy and translation results.
In this paper, we propose the first unsuper-vised approach to joint alignment and extraction of phrases at multiple granularities. This is achieved by constructing a generative model that includes phrases at many levels of granularity, from minimal phrases all the way up to full sentences. The model is similar to previously proposed phrase alignment models based on inversion transduction grammars (ITGs) (Cherry and Lin, 2007; Zhang et al., 2008; Blunsom et al., 2009), with one important change: ITG symbols and phrase pairs are generated in the opposite order. In traditional ITG models, the branches of a biparse tree are generated from a non-terminal distribution, and each leaf is generated by a word or phrase pair distribution. As a result, only minimal phrases are directly included in the model, while larger phrases must be generated by heuris-tic extraction methods. In the proposed model, at each branch in the tree, we first attempt to gener-ate a phrase pair from the phrase pair distribution, falling back to ITG-based divide and conquer strat-egy to generate phrase pairs that do not exist (or are given low probability) in the phrase distribution.
We combine this model with the Bayesian non-parametric Pitman-Yor process (Pitman and Yor, 1997; Teh, 2006), realizing ITG-based divide and conquer through a novel formulation where the Pitman-Yor process uses two copies of itself as a base measure. As a result of this modeling strategy, phrases of multiple granularities are generated, and thus memorized, by the Pitman-Yor process. This makes it possible to directly use probabilities of the phrase model as a replacement for the phrase table generated by heuristic extraction techniques.
Using this model, we perform machine transla-tion experiments over four language pairs. We ob-serve that the proposed joint phrase alignment and extraction approach is able to meet or exceed results attained by a combination of GIZA++ and heuristic phrase extraction with significantly smaller phrase table size. We also find that it achieves superior BLEU scores over previously proposed ITG-based phrase alignment approaches. The problem of SMT can be defined as finding the most probable target sentence e for the source sen-tence f given a parallel training corpus  X  X  , F X 
We assume that there is a hidden set of parameters  X  learned from the training data, and that e is condi-tionally independent from the training corpus given  X  . We take a Bayesian approach, integrating over all possible values of the hidden parameters:
P ( e | f ,  X  X  , F X  ) =
If  X  takes the form of a scored phrase table, we can use traditional methods for phrase-based SMT to find P ( e | f ,  X  ) and concentrate on creating a model for P (  X  | X  X  , F X  ) . We decompose this posterior prob-ability using Bayes law into the corpus likelihood and parameter prior probabilities In Section 3 we describe an existing method, and in Section 4 we describe our proposed method for modeling these two probabilities. There has been a significant amount of work in many-to-many alignment techniques (Marcu and Wong (2002), DeNero et al. (2008), inter alia ), and in particular a number of recent works (Cherry and Lin, 2007; Zhang et al., 2008; Blunsom et al., 2009) have used the formalism of inversion transduction grammars (ITGs) (Wu, 1997) to learn phrase align-ments. By slightly limit reordering of words, ITGs make it possible to exactly calculate probabilities of phrasal alignments in polynomial time, which is a computationally hard problem when arbitrary re-ordering is allowed (DeNero and Klein, 2008).
The traditional flat ITG generative probabil-ity for a particular phrase (or sentence) pair P ble  X  t and a symbol distribution  X  x . We use the fol-lowing generative story as a representative of the flat ITG model. 1. Generate symbol x from the multinomial distri-2. According to the x take the following actions.
By taking the product of P f lat over every sentence in the corpus, we are able to calculate the likelihood
We will refer to this model as FLAT . 3.1 Bayesian Modeling While the previous formulation can be used as-is in maximum likelihood training, this leads to a degen-erate solution where every sentence is memorized as a single phrase pair. Zhang et al. (2008) and others propose dealing with this problem by putting a prior probability P (  X  x ,  X  t ) on the parameters.
We assign  X  x a Dirichlet prior 1 , and assign the phrase table parameters  X  t a prior using the Pitman-Yor process (Pitman and Yor, 1997; Teh, 2006), which is a generalization of the Dirichlet process prior used in previous research. It is expressed as where d is the discount parameter, s is the strength parameter, and P base is the base measure. The dis-count d is subtracted from observed counts, and when it is given a large value (close to one), less frequent phrase pairs will be given lower relative probability than more common phrase pairs. The strength s controls the overall sparseness of the dis-tribution, and when it is given a small value the dis-tribution will be sparse. P base is the prior probability of generating a particular phrase pair, which we de-scribe in more detail in the following section.
Non-parametric priors are well suited for mod-eling the phrase distribution because every time a phrase is generated by the model, it is  X  X emorized X  and given higher probability. Because of this, com-mon phrase pairs are more likely to be re-used (the rich-get-richer effect), which results in the induc-tion of phrase tables with fewer, but more helpful phrases. It is important to note that only phrases generated by P t are actually memorized and given higher probability by the model. In FLAT , only min-imal phrases generated after P x outputs the terminal symbol TERM are generated from P t , and thus only minimal phrases are memorized by the model. While the Dirichlet process is simply the Pitman-Yor process with d = 0 , it has been shown that the discount parameter allows for more effective mod-eling of the long-tailed distributions that are often found in natural language (Teh, 2006). We con-firmed in preliminary experiments (using the data described in Section 7) that the Pitman-Yor process with automatically adjusted parameters results in su-perior alignment results, outperforming the sparse Dirichlet process priors used in previous research 2 . The average gain across all data sets was approxi-mately 0.8 BLEU points. 3.2 Base Measure P base in Equation (2) indicates the prior probability of phrase pairs according to the model. By choosing this probability appropriately, we can incorporate prior knowledge of what phrases tend to be aligned to each other. We calculate P base by first choosing whether to generate an unaligned phrase pair (where | e | = 0 or | f | = 0 ) according to a fixed probabil-ity p u 3 , then generating from P ba for aligned phrase pairs, or P bu for unaligned phrase pairs.

For P ba , we adopt a base measure similar to that used by DeNero et al. (2008): P M P pois is the Poisson distribution with the average length parameter  X  . As long phrases lead to spar-sity, we set  X  to a relatively small value to allow us to bias against overly long phrases 4 . P m 1 is the word-based Model 1 (Brown et al., 1993) probabil-ity of one phrase given the other, which incorporates word-based alignment information as prior knowl-edge in the phrase translation probability. We take the geometric mean 5 of the Model 1 probabilities in both directions to encourage alignments that are sup-ported by both models (Liang et al., 2006). It should be noted that while Model 1 probabilities are used, they are only soft constraints, compared with the hard constraint of choosing a single word alignment used in most previous phrase extraction approaches.
For P bu , if g is the non-null phrase in e and f , we calculate the probability as follows: Note that P bu is divided by 2 as the probability is considering null alignments in both directions. While in FLAT only minimal phrases were memo-rized by the model, as DeNero et al. (2008) note and we confirm in the experiments in Section 7, us-ing only minimal phrases leads to inferior transla-tion results for phrase-based SMT. Because of this, previous research has combined FLAT with heuris-tic phrase extraction, which exhaustively combines all adjacent phrases permitted by the word align-ments (Och et al., 1999). We propose an alterna-tive, fully statistical approach that directly models phrases at multiple granularities, which we will refer to as HIER . By doing so, we are able to do away with heuristic phrase extraction, creating a fully proba-bilistic model for phrase probabilities that still yields competitive results.
 Similarly to FLAT , HIER assigns a probability P terized by a phrase table  X  t and a symbol distribu-tion  X  x . The main difference from the generative story of the traditional ITG model is that symbols and phrase pairs are generated in the opposite order. While FLAT first generates branches of the derivation tree using P x , then generates leaves using the phrase distribution P t , HIER first attempts to generate the full sentence as a single phrase from P t , then falls back to ITG-style derivations to cope with sparsity. We allow for this within the Bayesian ITG context by defining a new base measure P dac ( X  X ivide-and-conquer X ) to replace P base in Equation (2), resulting in the following distribution for  X  t .
P dac essentially breaks the generation of a sin-gle longer phrase into two generations of shorter phrases, allowing even phrase pairs for which c (  X  e, f  X  ) = 0 to be given some probability. The generative process of P dac , similar to that of P f lat from the previous section, is as follows: 1. Generate symbol x from P x ( x ;  X  x ) . x can take 2. According to x take the following actions. A comparison of derivation trees for FLAT and HIER is shown in Figure 1. As previously de-scribed, FLAT first generates from the symbol dis-tribution P x , then from the phrase distribution P t , while HIER generates directly from P t , which falls back to divide-and-conquer based on P x when nec-essary. It can be seen that while P t in FLAT only gen-erates minimal phrases, P t in HIER generates (and thus memorizes) phrases at all levels of granularity. 4.1 Length-based Parameter Tuning There are still two problems with HIER , one theo-retical, and one practical. Theoretically, HIER con-tains itself as its base measure, and stochastic pro-cess models that include themselves as base mea-sures are deficient, as noted in Cohen et al. (2010). Practically, while the Pitman-Yor process in HIER shares the parameters s and d over all phrase pairs in the model, long phrase pairs are much more sparse than short phrase pairs, and thus it is desirable to appropriately adjust the parameters of Equation (2) according to phrase pair length.

In order to solve these problems, we reformulate the model so that each phrase length l = | f | + | e | has its own phrase parameters  X  t,l and symbol parame-ters  X  x,l , which are given separate priors: We will call this model HLEN .

The generative story is largely similar to HIER with a few minor changes. When we generate a sen-tence, we first choose its length l according to a uni-form distribution over all possible sentence lengths where L is the size | e | + | f | of the longest sentence in the corpus. We then generate a phrase pair from the probability P t,l (  X  e, f  X  ) for length l . The base measure for HLEN is identical to that of HIER , with one minor change: when we fall back to two shorter phrases, we choose the length of the left phrase from l  X  U nif orm (1 , l  X  1) , set the length of the right phrase to l r = l  X  l l , and generate the smaller phrases
It can be seen that phrases at each length are gen-erated from different distributions, and thus the pa-rameters for the Pitman-Yor process will be differ-ent for each distribution. Further, as l l and l r must be smaller than l , P t,l no longer contains itself as a base measure, and is thus not deficient.

An example of the actual discount values learned in one of the experiments described in Section 7 is shown in Figure 2. It can be seen that, as ex-pected, the discounts for short phrases are lower than those of long phrases. In particular, phrase pairs of length up to six (for example, | e | = 3 , | f | = 3 ) are given discounts of nearly zero while larger phrases are more heavily discounted. We conjecture that this is related to the observation by Koehn et al. (2003) that using phrases where max ( | e | , | f | )  X  3 cause significant improvements in BLEU score, while us-ing larger phrases results in diminishing returns. 4.2 Implementation Previous research has used a variety of sampling methods to learn Bayesian phrase based alignment models (DeNero et al., 2008; Blunsom et al., 2009; Blunsom and Cohn, 2010). All of these techniques are applicable to the proposed model, but we choose to apply the sentence-based blocked sampling of Blunsom and Cohn (2010), which has desirable con-vergence properties compared to sampling single alignments. As exhaustive sampling is too slow for practical purpose, we adopt the beam search algo-rithm of Saers et al. (2009), and use a probability beam, trimming spans where the probability is at least 10 10 times smaller than that of the best hypoth-esis in the bucket.

One important implementation detail that is dif-ferent from previous models is the management of phrase counts. As a phrase pair t a may have been generated from two smaller component phrases t b and t c , when a sample containing t a is removed from the distribution, it may also be necessary to decre-ment the counts of t b and t c as well. The Chinese Restaurant Process representation of P t (Teh, 2006) lends itself to a natural and easily implementable so-lution to this problem. For each table representing a phrase pair t a , we maintain not only the number of customers sitting at the table, but also the identities of phrases t b and t c that were originally used when generating the table. When the count of the table t a is reduced to zero and the table is removed, the counts of t b and t c are also decremented. In this section, we describe both traditional heuris-tic phrase extraction, and the proposed model-based extraction method. 5.1 Heuristic Phrase Extraction The traditional method for heuristic phrase extrac-tion from word alignments exhaustively enumerates all phrases up to a certain length consistent with the alignment (Och et al., 1999). Five features are used in the phrase table: the conditional phrase proba-bilities in both directions estimated using maximum likelihood P ml ( f | e ) and P ml ( e | f ) , lexical weight-ing probabilities (Koehn et al., 2003), and a fixed penalty for each phrase. We will call this heuristic extraction from word alignments HEUR -W . These word alignments can be acquired through the stan-dard GIZA++ training regimen.

We use the combination of our ITG-based align-ment with traditional heuristic phrase extraction as a second baseline. An example of these alignments is shown in Figure 3. In model HEUR -P , minimal phrases generated from P t are treated as aligned, and we perform phrase extraction on these alignments. However, as the proposed models tend to align rel-atively large phrases, we also use two other tech-niques to create smaller alignment chunks that pre-vent sparsity. We perform regular sampling of the trees, but if we reach a minimal phrase generated from P t , we continue traveling down the tree un-til we reach either a one-to-many alignment, which we will call HEUR -B as it creates alignments simi-lar to the block ITG, or an at-most-one alignment, which we will call HEUR -W as it generates word alignments. It should be noted that forcing align-ments smaller than the model suggests is only used for generating alignments for use in heuristic extrac-tion, and does not affect the training process. 5.2 Model-Based Phrase Extraction We also propose a method for phrase table ex-traction that directly utilizes the phrase probabil-ities P t (  X  e, f  X  ) . Similarly to the heuristic phrase tables, we use conditional probabilities P t ( f | e ) and P t ( e | f ) , lexical weighting probabilities, and a phrase penalty. Here, instead of using maximum likelihood, we calculate conditional probabilities di-rectly from P t probabilities: To limit phrase table size, we include only phrase pairs that are aligned at least once in the sample.
We also include two more features: the phrase pair joint probability P t (  X  e, f  X  ) , and the average posterior probability of each span that generated  X  e, f  X  as computed by the inside-outside algorithm during training. We use the span probability as it gives a hint about the reliability of the phrase pair. It will be high for common phrase pairs that are gen-erated directly from the model, and also for phrases that, while not directly included in the model, are composed of two high probability child phrases.
It should be noted that while for FLAT and HIER P t can be used directly, as HLEN learns separate models for each length, we must combine these probabilities into a single value. We do this by setting for every phrase pair, where l = | e | + | f | and c ( l ) is the number of phrases of length l in the sample.
We call this model-based extraction method MOD . 5.3 Sample Combination As has been noted in previous works, (Koehn et al., 2003; DeNero et al., 2006) exhaustive phrase extrac-tion tends to out-perform approaches that use syn-tax or generative models to limit phrase boundaries. DeNero et al. (2006) state that this is because gen-erative models choose only a single phrase segmen-tation, and thus throw away many good phrase pairs that are in conflict with this segmentation.
Luckily, in the Bayesian framework it is simple to overcome this problem by combining phrase tables from multiple samples. This is equivalent to approx-imating the integral over various parameter configu-rations in Equation (1). In MOD , we do this by taking the average of the joint probability and span prob-ability features, and re-calculating the conditional probabilities from the averaged joint probabilities. In addition to the previously mentioned phrase alignment techniques, there has also been a signif-icant body of work on phrase extraction (Moore and Quirk (2007), Johnson et al. (2007a), inter alia ). DeNero and Klein (2010) presented the first work on joint phrase alignment and extraction at multiple levels. While they take a supervised approach based on discriminative methods, we present a fully unsu-pervised generative model.

A generative probabilistic model where longer units are built through the binary combination of shorter units was proposed by de Marcken (1996) for monolingual word segmentation using the minimum description length (MDL) framework. Our work dif-fers in that it uses Bayesian techniques instead of MDL, and works on two languages, not one.

Adaptor grammars, models in which non-terminals memorize subtrees that lie below them, have been used for word segmentation or other monolingual tasks (Johnson et al., 2007b). The pro-posed method could be thought of as synchronous adaptor grammars over two languages. However, adaptor grammars have generally been used to spec-ify only two or a few levels as in the FLAT model in this paper, as opposed to recursive models such as HIER or many-leveled models such as HLEN . One exception is the variational inference method for adaptor grammars presented by Cohen et al. (2010) that is applicable to recursive grammars such as HIER . We plan to examine variational inference for the proposed models in future work. We evaluate the proposed method on translation tasks from four languages, French, German, Span-ish, and Japanese, into English. TM (en) 1.80M 1.62M 1.35M 2.38M TM (other) 1.85M 1.82M 1.56M 2.78M LM (en) 52.7M 52.7M 52.7M 44.7M Tune (en ) 49.8k 49.8k 49.8k 68.9k Tune (other) 47.2k 52.6k 55.4k 80.4k Test (en) 65.6k 65.6k 65.6k 40.4k Test (other) 62.7k 68.1k 72.6k 48.7k 7.1 Experimental Setup The data for French, German, and Spanish are from the 2010 Workshop on Statistical Machine Transla-tion (Callison-Burch et al., 2010). We use the news commentary corpus for training the TM, and the news commentary and Europarl corpora for training the LM. For Japanese, we use data from the NTCIR patent translation task (Fujii et al., 2008). We use the first 100k sentences of the parallel corpus for the TM, and the whole parallel corpus for the LM. De-tails of both corpora can be found in Table 1. Cor-pora are tokenized, lower-cased, and sentences of over 40 words on either side are removed for TM training. For both tasks, we perform weight tuning and testing on specified development and test sets.
We compare the accuracy of our proposed method of joint phrase alignment and extraction using the FLAT , HIER and HLEN models, with a baseline of using word alignments from GIZA ++ and heuris-tic phrase extraction. Decoding is performed using Moses (Koehn and others, 2007) using the phrase tables learned by each method under consideration, as well as standard bidirectional lexical reordering probabilities (Koehn et al., 2005). Maximum phrase length is limited to 7 in all models, and for the LM we use an interpolated Kneser-Ney 5-gram model.
For GIZA ++, we use the standard training reg-imen up to Model 4, and combine alignments with grow-diag-final-and . For the proposed models, we train for 100 iterations, and use the final sample acquired at the end of the training process for our experiments using a single sample 6 . In addition, FLAT MOD 1 13.48 136k 19.15 125k 17.97 117k 16.10 89.7k HIER MOD 1 16.58 1.02M 21.79 859k 21.50 751k 23.23 723k HLEN MOD 1 16.49 1.17M 21.57 930k 21.31 860k 23.19 820k HIER MOD 10 16.53 3.44M 21.84 2.56M 21.57 2.63M 23.12 2.21M HLEN MOD 10 16.51 3.74M 21.69 3.00M 21.53 3.09M 23.20 2.70M we also try averaging the phrase tables from the last ten samples as described in Section 5.3. 7.2 Experimental Results The results for these experiments can be found in Ta-ble 2. From these results we can see that when using a single sample, the combination of using HIER and model probabilities achieves results approximately equal to GIZA ++ and heuristic phrase extraction. This is the first reported result in which an unsu-pervised phrase alignment model has built a phrase table directly from model probabilities and achieved results that compare to heuristic phrase extraction. It can also be seen that the phrase table created by the proposed method is approximately 5 times smaller than that obtained by the traditional pipeline.
In addition, HIER significantly outperforms FLAT when using the model probabilities. This confirms that phrase tables containing only minimal phrases are not able to achieve results that compete with phrase tables that use multiple granularities.
Somewhat surprisingly, HLEN consistently slightly underperforms HIER . This indicates potential gains to be provided by length-based parameter tuning were outweighed by losses due to the increased complexity of the model. In particular, we believe the necessity to combine probabilities from multiple P t,l models into a single phrase table may have resulted in a distortion of the phrase probabilities. In addition, the assumption that phrase lengths are generated from a uniform distribution is likely too strong, and further gains could likely be achieved by more accurate modeling of phrase lengths. We leave further adjustments to the HLEN model to future work.

It can also be seen that combining phrase tables from multiple samples improved the BLEU score for HLEN , but not for HIER . This suggests that for HIER , most of the useful phrase pairs discovered by the model are included in every iteration, and the in-creased recall obtained by combining multiple sam-ples does not consistently outweigh the increased confusion caused by the larger phrase table.
We also evaluated the effectiveness of model-based phrase extraction compared to heuristic phrase extraction. Using the alignments from HIER , we cre-ated phrase tables using model probabilities ( MOD ), and heuristic extraction on words ( HEUR -W ), blocks (
HEUR -B ), and minimal phrases ( HEUR -P ) as de-scribed in Section 5. The results of these ex-periments are shown in Table 3. It can be seen that model-based phrase extraction using HIER out-performs or insignificantly underperforms heuris-tic phrase extraction over all experimental settings, while keeping the phrase table to a fraction of the size of most heuristic extraction methods.

Finally, we varied the size of the parallel corpus for the Japanese-English task from 50k to 400k sen-tences and measured the effect of corpus size on translation accuracy. From the results in Figure 4 (a), it can be seen that at all corpus sizes, the re-sults from all three methods are comparable, with insignificant differences between GIZA ++ and HIER at all levels, and HLEN lagging slightly behind HIER . Figure 4 (b) shows the size of the phrase table in-duced by each method over the various corpus sizes. It can be seen that the tables created by GIZA ++ are significantly larger at all corpus sizes, with the dif-ference being particularly pronounced at larger cor-pus sizes. In this paper, we presented a novel approach to joint phrase alignment and extraction through a hierar-chical model using non-parametric Bayesian meth-ods and inversion transduction grammars. Machine translation systems using phrase tables learned di-rectly by the proposed model were able to achieve accuracy competitive with the traditional pipeline of word alignment and heuristic phrase extraction, the first such result for an unsupervised model.
For future work, we plan to refine HLEN to use a more appropriate model of phrase length than the uniform distribution, particularly by attempting to bias against phrase pairs where one of the two phrases is much longer than the other. In addition, we will test probabilities learned using the proposed model with an ITG-based decoder. We will also ex-amine the applicability of the proposed model in the context of hierarchical phrases (Chiang, 2007), or in alignment using syntactic structure (Galley et al., 2006). It is also worth examining the plausibility of variational inference as proposed by Cohen et al. (2010) in the alignment context.
 This work was performed while the first author was supported by the JSPS Research Fellowship for Young Scientists.

