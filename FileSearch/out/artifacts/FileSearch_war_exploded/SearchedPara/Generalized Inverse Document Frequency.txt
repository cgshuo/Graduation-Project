 Inverse document frequency (IDF) is one of the most useful and widely used concepts in information retrieval. There have been various attempts to provide theoretical justifica-tions for IDF. One of the most appealing derivations follows from the Robertson-Sparck Jones relevance weight. How-ever, this derivation, and others related to it, typically make a number of strong assumptions that are often glossed over. In this paper, we re-examine these assumptions from a Bayes-ian perspective, discuss possible alternatives, and derive a new, more generalized form of IDF that we call generalized inverse document frequency. In addition to providing theo-retical insights into IDF, we also undertake a rigorous em-pirical evaluation that shows generalized IDF outperforms classical versions of IDF on a number of ad hoc retrieval tasks.
 H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval Algorithms, Experimentation, Theory inverse document frequency, formal models, estimation
Inverse document frequency (IDF) is arguably one of the most important and widely used concepts in information re-trieval. It was first introduced by Sparck Jones in 1972 [9] with the aim of improving automatic indexing and retrieval systems. Since that time, IDF has become a standard way of measuring the global importance or discriminative power of terms in text. When IDF is used in combination with term frequency (TF), the result is a very robust and highly effec-tive term weighting scheme that has been applied across a Copyright 2008 ACM 978-1-59593-991-3/08/10 ... $ 5.00. wide range of application areas, including databases, knowl-edge management, text classification, natural language pro-cessing, and, of course, information retrieval.

Sparck Jones X  original proposal for IDF was based on em-pirical observations of global term frequency, whereby highly frequent terms should be given less weight than less frequent terms since they are more common and less discriminative. Two years after the original proposal, Robertson and Sparck Jones derived the so-called binary independence retrieval (BIR) model [16], which is more appropriately called the linked dependence model [3]. Later, Croft and Harper [4] showed that, under a number of assumptions, IDF can be derived directly from the BIR model in the form of a RSJ relevance weight. A similar line of research was also ex-plored by Robertson and Walker [19], who investigated the relationship between RSJ relevance weights and IDF in more depth. Other derivations of IDF have been proposed [1, 13], but the one that is most appealing, especially from an in-formation retrieval point of view, is the one based on the RSJ weighting. Therefore, it is this derivation that is the primary focus of our investigation in this paper.
In the field of information retrieval, there have been many attempts to change how the TF component in TF.IDF term weighting is computed [8, 18, 21] However, there has been few, if any, attempts to improve upon the small number of  X  X lassical X  IDF formulations. This may be the case because it is non-trivial to change the standard IDF formulation in a theoretically meaningful way while improving effectiveness. There may be heuristic ways to alter the IDF formulation, but doing so leads to little in the way of improved under-standing as to why things improved. In this paper, we inves-tigate, at a very fundamental level, the various assumptions that are entwined in the RSJ weighting derivation of IDF. By digging deeply into these assumptions, we are able to develop a better understanding of how and why IDF works. This analysis also allows us to propose a new, generalized form of IDF that we call generalized IDF (GIDF).

Our proposed generalized IDF results from taking a Bayes-ian view of the BIR model and decomposing the statistical assumptions. We not only examine the statistical assump-tions made by previous researchers with respect to IDF, but also propose a new set of assumptions that lead to new IDF formulations. Our assumptions and formulations are backed up by empirical data that suggest the classical IDF formu-lations are non-optimal and can be improved upon.
This paper has four primary contributions. First, we pro-vide a novel look at the BIR model in terms of a hierarchical Bayesian model. Second, we derive a generalized IDF for-mulation based on the model. Third, we propose a number of new IDF formulations based on GIDF that are inspired by data analysis and previous research, including an IDF that does not depend on document frequency, Finally, we test the effectiveness of our new IDF formulations on a number of TREC ad hoc retrieval data sets. Our results show that our proposed IDF formulations can consistently and signifi-cantly improve effectiveness over a number of classical IDF formulations.
 The remainder of this paper is laid out as follows. First, in Section 2 we describe related work, and discuss several pre-viously proposed derivations, including the classical deriva-tion as a Robertson-Sparck Jones relevance weight. Then, in Section 3 we derive our generalized IDF, show how it re-lates to classical definitions of IDF, and propose several new IDF measures based on our derivation. In Section 4, we com-pare and contrast classical definitions of IDF with several in-stantiations of our proposed generalized IDF and show that our newly proposed IDF measures can consistently improve upon classical IDF measures on several ad hoc retrieval test collections. Finally, in Section 5 we conclude the paper and describe various potential areas of future work.
In this section we review the previous research that has laid the theoretical and empirical foundations for the cur-rent understanding of IDF. Later, we will build upon this foundation to develop a generalized formulation for IDF.
We begin our discussion of related work by revisiting var-ious theoretical derivations of IDF. We primarily focus on the derivation of IDF as a RSJ relevance weight and briefly describe other theoretical derivations that have been pro-posed.

As we discussed in the introduction, IDF can be theoret-ically derived as a relevance weight within Robertson and Spark Jones X  BIR model [16]. Within the model, documents d are ranked according to P ( R = 1 | D = d ). Here, R is a binary random variable that represents relevance. Further-more, D is a random variable that represents a document. Thus, the model ranks documents in decreasing likelihood of being relevant, which adheres to the well-known Probability Ranking Principle [14].

The model assumes that d  X  { 0 , 1 } V is a random binary vector which has one entry for every term in the vocabulary V . Here d i = 1 if and only if term i occurs in the document. The standard BIR model ranking function derivation is as follows 1 :
For notational convenience, we will use r and r as short-hand for R = 1 and R = 0, respectively, and d as shorthand for D = d , as well. Sparck Jones (RSJ) relevance weight. This derivation in-vokes a number of assumptions. First, it is explicitly as-sumed that P ( d | r ) = Q |V| i =1 P ( d i | r ), which imposes an in-dependence assumption on the term occurrences. It is also implicitly assumed that P ( D | r ) is distributed according to a multivariate Bernoulli distribution (i.e., each P ( d i | r ) is dis-tributed as a Bernoulli). This assumption is implicit due to the fact that documents are represented as binary vectors. More recently, other distributional assumptions have been explored in contexts similar to the BIR model, including the Poisson [5, 7, 17, 20], multinomial [11], and Dirichlet com-pound multinomial [23] distributions, although most of the widespread IDF formulations are based on the multivariate Bernoulli distribution.
 To use the BIR model, one must estimate P ( d i | r ) and P ( d i | r ) for every term i in the vocabulary for a total of 2 |V| parameters. It is feasible to estimate the probabilities if relevance information is available. For example, one may use the estimates originally proposed by Robertson and Sparck Jones, which are: where c ( d i , R ) is the number of relevant documents that term d i occurs in, c ( d i , C ) is the number of documents that term d i occurs in, | R | is the total number of relevant docu-ments, and | C | is the total number of documents in the col-lection. This results in the following RSJ relevance weight: wt i = log c ( d i , R )  X  ( | C | X  X  R | X  c ( d i , C ) + c ( d
In many retrieval scenarios it is difficult or impossible to determine | R | and c ( d i , R ) for each term. This makes the BIR model, as is, difficult to use in practice. Later, Croft and Harper proposed two assumptions that would al-low the model to be used when no relevance information was available [4]. The first assumption they propose states that of any term occurring in the non-relevant class of documents is approximately equal to the probability of that term occur-ring in the entire collection. Their second assumption states that P ( d i | r ) =  X  for every term i that occurs in the query. Under these two assumptions, the RSJ relevance weight for term i reduces to:
It should also be noted that a similar IDF formulation can be derived from the RSJ relevance weight in Equation 1 if we assume that c ( d i , R ) = | R | = 0. After adding the com-monly used 0.5  X  X orrection X , the following IDF formulation is achieved: which we will refer to as the RSJ IDF formulation.
The first assumption made my Croft and Harper is realis-tic an tends to hold in practice, as most of the documents in a reasonably sized collection will not be relevant to a given query. The second assumption, however, is an oversimplifi-cation. In practice, as others have shown [6], and as we will also show, P ( d i | r ) tends to increase for very frequent terms. This makes intuitive sense, because very common terms are at least as likely as less common terms to occur in relevant documents. In addition, the IDF derived as the result of imposing the assumptions can result in negative IDF values for very frequent terms, which is undesirable, both from a theoretical and practical point of view.

These factors led Robertson and Walker to propose a new estimate for P ( d i | r ) that increases as the frequency of term i increased within the collection [19]. Their proposed estimate has the form: where  X  is the same constant used in the Croft and Harper estimate. However, P ( d i | r ) =  X  only when c ( d i , C ) = 0. Furthermore, P ( d i | r ) = 1 if c ( d i , C ) = | C | . Therefore, the estimate is more in line with empirical observations. Using this estimate and the 0.5  X  X orrection X , the RSJ relevance weight becomes: which we will call the RSJ positive IDF . Indeed, not only does this estimate tend to model the actual data better, but it also tends to lead to better retrieval effectiveness than other IDF formulations derived as a RSJ relevance weight.
Recently, Lee proposed a new estimate for P ( d i | r ) that is similar to Laplacian smoothing [12]. The proposed estimate has the form: where L is a smoothing parameter that controls how many pseudo-counts get added to both the document frequency as well as the number of documents. This estimate results in the following RSJ weight: It is easy to see that when L = N , the formulation takes on the standard IDF form. Lee suggests that replacing the constant L with a function L ( i ) that depends on the term may be useful for constructing a more effective IDF.
As should now be clear, most of the work that has gone into improving IDF has looked at new estimates for P ( d i Even in this vein of research there has been very little done, and very few experimental results to show for it. Further-more, the estimate for P ( d i | r ) has been largely ignored. Thus, the goal of our work is to consider various general estimates for both P ( d i | r ) and P ( d i | r ) and rigorously eval-uate the effectiveness of each in order to develop an even better understanding of how the various components of IDF interact with each other.
Although we have primarily focused on IDF, as derived from the BIR model, there have been other derivations pro-posed in the literature, almost exclusively from an informa-tion theoretic point of view. These include the derivation proposed by Aizawa [1] and Papineni [13], both of which made theoretical arguments based on information theoretic principles. These derivations are appealing because they use different tools and machinery to derive the same notion that was originally derived using information retrieval ideas and concepts. Therefore, rather than viewing these derivations as competing, it is useful to glean as much from each in or-der to develop a deeper understanding of IDF from multiple perspectives. We now describe the details of our proposed generalized IDF formulation. Our derivation and subsequent estimates are similar in nature to those proposed previously. However, we take a fresh view of things and propose novel estimates that go beyond those that are found in the literature. Our proposed estimates are not only theoretically well-founded, but also highly effective, as we will show later. The general framework laid out in the remainder of this section can be used to easily derive new IDF variants that are more flexible and robust than the current variants and lead to improved retrieval effectiveness for a wide variety of tasks.
The original BIR model does not explicitly have a random variable for modeling the query. Everything is assumed to be implicitly conditioned on some information need. How-ever, it is often more convenient and more flexible to include the query within the model. Lafferty and Zhai showed that the BIR model can easily be generalized to include a query random variable Q [10]. We build upon their work to for-mally derive our generalized form of IDF as follows: RSV ( q, d, J ) = P ( r, | d, q, J ) where Q is a random variable that represents the query or information need and q is a specific query in the event space of Q , and J is set of relevance information. Here, J , which is novel to our derivation, stands for  X  X udgments X , and can be any source of relevance data available, includ-ing (pseudo-)relevance feedback documents or click-through logs. The set J is partitioned into J r and J r , which are the relevant and non-relevant judgment data in J , respectively. These sets provide evidence when computing the probabil-ity of relevance (or non-relevance) for a given query. From this derivation, we obtain the following RSJ-like relevance weight: which we can split into a relevant component ( IDF r ) and a non-relevant component ( IDF r ). As we will show, different Figure 1: Graphical model representation of the generalized probabilistic model. instantiations of our generalized IDF will result in different formulations for these two components.

Thus, we must estimate P ( d i | q, r, J ) and P ( d i | q, r, J ) for every term i in the vocabulary. Although this is very simi-lar in nature to the estimation problem encountered in the original BIR model, it is unique in the sense that we now have two additional pieces of evidence ( q and J ) from which the probabilities can be estimated more effectively. Unlike the BIR model, which models P ( d i | r ) and P ( d i | r ) accord-ing to multivariate Bernoulli distributions, we take a hi-erarchical Bayesian approach to estimate P ( d i | q, r, J ) and P ( d i | q, r, J ). The graphical model representation of our hi-erarchical model is shown in Figure 1. Under the model, for each term i , a query q , relevance setting r , and a set of relevance information J imposes a probability distribution  X  from which the term i is sampled for each document in the collection.

We assume that P ( d i |  X  ) is distributed according to a Bernoulli distribution and P (  X  | q, r, J ) is distributed according to a Beta distribution with hyperparameters  X  i ( q, r, J ) an  X  One particularly nice property of the Beta distribution is that it is a conjugate prior to the Bernoulli. In addition, as we will show, it models the actual distribution of the model parameters well in practice. Thus, under these assumptions, our estimates can be computed as follows: Therefore, the estimates are simply expectations over  X  , which can be efficiently computed as: since the mean of a Beta random variable with hyperparam-
Using these estimates, we obtain the following: We call this formulation the generalized IDF , since it pro-vides a great deal of flexibility and generalizes most of the previously proposed IDF formulations. With the original RSJ relevance weight-based IDF formulation, the problem boiled down to estimating P ( d i | r ) and P ( d i | r ). In this case, we are still estimating probabilities, but we have shifted our focus from directly estimating the probabilities, to finding reasonable settings of the model hyperparameters  X  and  X  . As we will now show, this is not a difficult task, and it is quite simple to come up with a range of new, easily in-terpretable IDF formulations based on the generalized IDF formula.
If we have relevance information (i.e., J 6 = {} ) in the form of judged documents, then there are many reasonable ways of setting the  X  and  X  parameters. For example, one possible setting is the following: where | J r | is the number of documents in J r and c ( d the number of documents in J r that term i occurs. These settings result in the following IDF estimate: which is the original RSJ relevance weight without the 0.5  X  X orrection X  (Equation 1). The 0.5  X  X orrection X  that is com-monly used can be obtained by adding 0.5 to all of the hyper-parameters above. This biases the mean of P (  X  | q, r, J ) away fectiveness.

Thus, we have derived the RSJ relevance weight with rele-vance information using our generalized IDf framework. In-deed, it is easy to derive other relevance-based IDF formu-lations within this framework by simply changing the model hyperparameters. We hope this will stimulate further re-search into new and more robust IDF formulations, espe-cially now that novel types of relevance information, such as click-through data, is available.

We do not run any experiments using relevance informa-tion in this work. Therefore, this derivation is more for theoretical completeness. As part of future work we plan to carry out experiments that validate the effectiveness and robustness of this estimate versus the original RSJ relevance weight.
The more interesting, and common, case arises when we have no relevance information available (i.e., J = {} ). There are two possible cases to consider here. First, we may as-sume, in a similar fashion to Croft and Harper, that ev-ery document in the entire collection is not relevant. This is equivalent to setting J r = C . Although the Croft and Harper assumption has been shown to hold in practice, we do not feel that it is theoretically well-founded, since we have not actually observed any non-relevant documents. We believe that the Croft and Harper assumption should be en-coded directly into the model prior, since this is a priori knowledge, rather than actual observed evidence. There-fore, we argue that a more proper formulation simply treats J r as empty and enforces any a priori knowledge into the prior over  X  via the hyperparameters  X  and  X  . In the next two sections we describe various ways of setting the hyper-parameters when no relevance information is available.
We begin by examining various ways of formulating IDF r , which is the part of the IDF formula that is based on the relevant class of documents. Since we only consider general-ized IDF, formulating IDF r is equivalent to finding settings for  X  i ( q, r ) and  X  i ( q, r ) which determine the distribution of models in the relevant class. Notice that we have dropped J from the hyperparameter formulation, since it is assumed to be empty.

We now describe two sets of assumptions over  X  i ( q, r ) and  X  ( q, r ), their corresponding IDF r formulations, and their statistical interpretation within the model. Our first assumption, which is very simple, states that IDF r is a constant. This is achieved by making the following assumptions for  X  i ( q, r ) and  X  i ( q, r ): where  X   X  (0 , 1) is a free parameter. The smaller  X  is, the smaller the ratio of  X  i ( q, r ) to  X  i ( q, r ) is, which results in more pseudo-counts being assigned to the non-occurrences of the term than occurrences of the term in the prior. This results in the following IDF r formulation:
Perhaps a simpler, more straightforward interpretation of this assumption can be gleaned from the resulting estimate for P ( d i | q, r ), which simply is: Therefore, this assumption states that P ( d i | q, r ) is constant for all terms. This is equivalent to Croft and Harper X  X  as-sumption, which is known to be inaccurate [6]. Indeed, for the data sets that we consider in this paper, this assump-tion also is inaccurate. Figure 2 plots the probability that a word occurs in a relevant document versus the probability that the word occurs in any document. Each point repre-sents a single term in a query. The plots are generated across large set of queries. As the plot shows, there is a noticeable trend in the data that discredits the Croft and Harper as-sumption. Indeed, the plots support previous research that suggests P ( d i | q, r ) is increasing as a function of P ( d
Given this evidence, we seek to formulate an P ( d i | q, r ) that increases as P ( d i | C ) increases. There are many ways that this can be done within the generalized IDF framework. However, for the purpose of this paper, we consider the the following assumption: terms in our training set. This represents the  X  X verage X  probability of a term occurring in a relevant document. In occurring in the entire collection. This results in the follow-ing IDF r formulation: and the following probability estimate: Therefore, the estimate smooths the collection probability with the average probability that any term will occur in a relevant document. As long as  X  &gt; 0, this estimate will in-crease as P ( d i | C ) increases, thereby matching the empirical observations.

This estimate is similar to the popular Jelinek-Mercer smoothing from statistical language modeling. There are many other possible ways to interpolate, or smooth, the collection probability against the sample mean, including Laplacian smoothing, absolute discounting, and Dirichlet smoothing [24].
We now investigate four assumptions for  X  i ( q, r ) and  X  As with the relevance distribution assumptions, by setting these parameters, we are imposing a form on IDF r , which is the part of the IDF formula that deals with the distribution of non-relevant documents.

As before, we describe each assumption, the correspond-ing IDF r formulations, and the statistical properties sur-rounding the assumption.
Our first assumption for the non-relevant distribution is exactly the same as our assumption for the relevant distri-bution, and is as follows: which results in an analogous formulation for IDF r : as well as an analogous probability estimate: which, as before, is constant for every term in the vocabu-lary. Of course, this is a poor assumption, but the formu-lation does not depend on any collection statistics, such as document frequency. Indeed, when this assumption is used in conjunction with assumption set 1 from the relevant class, we obtain a formulation of IDF that does not depend on the document frequency in any way. Since the same weight is assigned to every query term, this is equivalent to using no IDF information whatsoever. Therefore, we expect this par-ticular combination of assumption sets to perform poorly. the probability the word occurs in any document ( P ( d | C ) ) across four TREC data sets.
Our first assumption set for the non-relevant class of doc-uments attempts to mimic the Croft and Harper IDF r , by using the following settings for  X  i ( q, r ) and  X  i ( q, r ): where, as before,  X  is a free parameter. We must set  X  as to ensure  X  i ( q, r ) and  X  i ( q, r ) remain positive to ensure the Beta distribution remains well-defined. In our experiments, we only consider  X   X  0. This setting of the hyperparameters results in the following IDF r : which, as we had hoped for, is reminiscent of Croft and Harper X  X  formulation. Indeed, if we set  X  = 0 . 5, then we obtain the commonly used 0.5  X  X orrection X . Here, it is very easy to see exactly where the 0.5 comes from and what its meaning actually is. The actual probability estimate for P ( d i | q, r ) using this formulation is: This estimate degenerates to P ( d i | C ) when  X  = 0 and con-verges towards 1 2 as  X   X   X  . However, this estimate still suffers from the problem of resulting in negative IDF r val-ues for highly frequent terms, which is undesirable and can hurt retrieval effectiveness. Therefore, to overcome the problem of problem of negative IDF r values, we propose the following settings for the Beta hyperparameters: Notice that the only difference between this assumption set and the previous one is that c ( d i , C ) is no longer subtracted from  X  i ( q, r ), thereby eliminating the possibility for  X   X  ( q, r ), which is the cause for the negative values. This slightly modified assumption set results in the following IDF formulation: and the following probability estimate:
Our final assumption set for the non-relevant distribution mirrors assumption set 2 from the relevance distribution. The assumptions is as follows: and queries in our training set. Rather than estimating this from the set of judged non-relevant documents, which is very biased, we estimate this from the entire collection. The resulting IDF r formulation is then: and our probability estimate is: which, as before, is a linear combination of the collection probability and the sample mean observed in the training data. Different settings of  X  result in different estimates and control the amount by which P ( d i | q, r ) increases with P ( d i | C ).
It is easy to derive the classical IDF formulations within the generalized IDF framework using the various assump-tion sets proposed here. By interpreting classical IDFs in this way, we are able to shed new insights into how these formulations are related to each other and also motivate new and interesting directions for further research into improved IDF measures.

The RSJ IDF, as defined above, can be derived using gen-eralized IDF using assumption set 1 for IDF r with  X  = 0 . 5 and assumption set 2 IDF r with  X  = 0 . 5. Furthermore, the RSJ positive IDF, as defined above, can also be derived us-ing assumption set 1 for IDF r with  X  = 0 . 5 and assumption set 3 IDF r with  X  = 0 . 5. Although the formulation pro-posed by Lee cannot be derived using any of our proposed assumption sets, it is easy to see that it is very similar to using assumption set 2 for IDF r and assumption set 2 for Table 1: Overview of TREC collections and topics. IDF r . It is easy to use the generalized IDF framework to derive the exact IDF formulation by introducing a new as-sumption set for IDF r .

Therefore, our generalized IDF is truly general, in the sense that most of the previously proposed IDF measures that have been derived as a RSJ relevance weight, can easily be derived within the framework.
We now describe our empirical evaluation of our general-ized IDF framework.
Our experiments are conducted over four standard TREC ad hoc retrieval data sets. The data sets are are summa-rized in Table 1. The WSJ, AP, and ROBUST data sets are medium size and consist solely of news articles, whereas the WT10G data set is much larger and contains web doc-uments.

Each data set is divided into a training and a test set. For all of our experiments, we only use the training set for tun-ing model parameters. However, we generally report both training and test set effectiveness for the sake of complete-ness.

Our evaluation methodology follows the standard TREC procedures. Our primary evaluation metric of interest is mean average precision computed to a depth of 1000 docu-ments per query. All training is done to directly maximize mean average precision. All statistical significance tests re-ported use of a one-tailed t -test.

All of the experiments were carried out using the soon to be released Searching using Markov Random Fields (SMRF) toolkit, which provides a robust experimental layer that sits on top of an Indri index [22]. All documents are stopped using a standard stopword list and stemmed using the Porter stemmer.
Before describing our ad hoc retrieval experiments, we first empirically analyze various characteristics of the TREC data sets.

First, we wish to determine how realistic it is to assume that P (  X  | q, r ) and P (  X  | q, r ) is distributed according to a Beta distribution. We chose this distribution because it is conjugate to the Bernoulli, which makes the mathematics and analysis simpler. However, this does not ensure that the Beta models the actual distributions well at all. Rather than proving that the fit is good using statistical analysis, we take a much simpler, higher level approach. We first plot a histogram of the observed P (  X  | q, r ) and P (  X  | q, r ) in the data. Then, we fit a Beta distribution to the data. We can then see how good or bad the corresponding fit is.
The resulting histograms and fitted Beta distributions are plotted in Figure 3. The Beta distribution is fit to the em-pirical distribution by using the method of means, which sets the mean and variance of the Beta equal to the sample mean and variance. Given the mean and variance of the sample, the  X  and  X  hyperparameters are set as follows: It is easy to verify that this setting yields a Beta distribution where the mean and variance are equal to the sample mean and variance, respectively. For illustrative purposes, the es-timated  X  and  X  values for each of our data sets is provided in Table 2. It is interesting to note that E [  X  | r ] tends to in-crease as the collection gets larger, and is especially large for the WT10G collection. This suggests that almost all of the relevant documents contain all of the query terms. A similar observation was also made by Buckley et al. [2]. For data sets with large E [  X  | r ] we expect IDF r to be large for most terms in order to promote a coordinate-level matching type of ranking function.

The fitted distributions in Figure 3 appear to be good fits to the empirical distributions. Therefore, the Beta assump-tion seems reasonable. However, it is important to note that this distribution is estimated over all terms, rather than in-dividual terms, as is actually done with generalized IDF. However, since our data is so sparse, there is no reasonable way to do a similar analysis for each term. It would be in-teresting to revisit this analysis for a much larger training set in order to determine if each term actually follows a Beta distribution or not.
Our first set of experiments investigate how our general-ized IDF formulations compare to each other and to classical IDF formulations using an entirely IDF-based ranking func-tion of the form:
It is well understood that IDF-only ranking functions are far inferior to TF.IDF ranking functions. However, by elim-inating TF from the equation, we can focus on the effective-ness of the various IDF formulations in isolation. We will evaluate the formulations using a TF.IDF ranking function later, however.

Table 3 lists the results of our IDF-only experiments. For each data set, we provide both the training and test set mean average precision values for every combination of IDF r and IDF r assumption sets. We also include the classical RSJ and RSJ positive IDF formulations, which we showed ear-lier to be special cases of our generalized IDF framework. It is not our expectation to significantly improve upon the classical IDF formulations. Instead, the goal of these ex-periments is to show that the generalized IDF framework is very simple to use to construct new, potentially effective, IDF formulations.

In the table, bold values represent the best result within a given column. The first thing to notice is that one of the moments.
 non-relevant classes.  X  .1417 .1257 .0625 .0594  X   X   X  .1421 .1254 .0623 .0556  X   X  .1421 .1254 .0623 . 0556  X  X   X  X  .1421 .1254 .0621 .0555 tests were only performed on the test sets. generalized IDF formulations is always the best, both for the training and test sets. This result shows the robustness of the framework. Furthermore, the superscripts indicate statistically significant improvements over the classical IDF formulations. Such tests were only computed on the test sets, since that is the most meaningful analysis. We see that the generalized IDF is significantly better for a number of data sets, with relative improvements in mean average precision ranging up to 7%.

Unfortunately, no one formulation stands out as being the most effective across all data sets. However, the IDF r as-sumption set 2 does appear to be consistently better than the IDF r assumption set 1, which supports previous obser-vations, as well [19].
Now that we have shown that our proposed framework is effective when an IDF-only ranking function is used, we investigate whether or not using the new IDF formulations will be as effective when combined with TF in a TF.IDF ranking function. It is not immediately clear how, if at all, the TF component will interact with the IDF component.
For TF, we use the state of the art Okapi TF [18], which artfully normalizes for document length normalization and accounts for term frequency saturation. The Okapi TF is the TF component used in the widely used and highly effective BM25 ranking function [15]. The form of the Okapi TF that we use has the following form: where tf ( d i , D ) is the number of times that term d i in D , | D | is the number of terms D , | D | avg is the average document length, and k 1 and b are free parameters, which are tuned on the training set.

We then combine the Okapi TF with our generalized IDF in the following way:
The results of our TF.IDF experiments are shown in Ta-ble 4. As before, the bolded results indicate the best result for the given column. As with the IDF-only ranking func-tion, one of the generalized IDF formulations is always the most effective.

The results show that the TF.IDF model washes away some of the improvements in effectiveness that was observed with the IDF-only ranking function. However, the same gen-eral trends, observations, and analysis still holds. There still remain a number of results that are statistically significantly better than the classical IDF formulations, which suggests that the generalized IDF formulations proposed here can be used effectively in combination with powerful TFs, such as the Okapi TF.

Based on these results, and the results from the IDF-only ranking function, we would recommend using IDF r assump-tion set 2 with IDF r assumption set 4 as a good  X  X ut of the box X  IDF. Of course, these IDF formulations require train-ing unlike most of the classical formulations. However, there are two sides to the coin. While training data can be difficult to come by, it does exist for many tasks. If the data exists, then there is no reason why an uninformed IDF should be used, when a more general, robust IDF can be used instead.
In this paper, we derived a new IDF formulation using a hierarchical Bayesian model that generalizes the classical BIR model. Our new IDF formulation, which can be inter-preted as a RSJ relevance weight, is called generalized IDF. As we showed, it can be used to derive all of the classical IDF formulations and is extensible, allowing researchers to easily develop new IDF formulations based on empirical and theoretical analysis.

We evaluated the usefulness of our proposed framework by experimenting with eight derived IDF formulations. Ex-periments were carried out using both an IDF-only ranking function, as well as a TF.IDF ranking function. Our results showed consistent and significant improvements over classi-cal IDF formulations on using both IDF-only and TF.IDF-based ranking functions. Our results also show that our proposed IDF is valuable and highly effective in isolation, but when intertwined with TF, some of the improvements are washed away, due to complex interactions with the TF component. This suggests that we extend our work to other  X   X  .2285 .2886 .2230 . 1973  X   X  X  .2283 .2886 .2263 .2006  X  X  tests were only performed on the test sets. distributions, such as the multinomial or Poisson, so that our derivations can naturally account for term frequency. We believe that such a model should be able to consistently and significantly improve over standard TF.IDF formula-tions, if modeled properly.

Furthermore, we plan to explore how the framework can be used for relevance and pseudo-relevance feedback. The framework has a natural mechanism for including both types of information. It would be interesting to compare the ef-fectiveness of this new formulation against classical RSJ rel-evance weights estimated using relevance information.
