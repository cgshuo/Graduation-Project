 Mining frequent patterns is a general and important issue in data mining. Complex and unstructured (or semi-structured) datasets have appeared in major data mining applications, including text mining, web mining and bioinformatics. Min-ing patterns from these datasets is the focus of many of the current data mining approaches. We focus on labeled or-dered trees, typical datasets of semi-structured data in data mining, and propose a new probabilistic model and its effi-cient learning scheme for mining labeled ordered trees. The proposed approach significantly improves the time and space complexity of an existing probabilistic modeling for labeled ordered trees, while maintaining its expressive power. We evaluated the performance of the proposed model, compar-ing it with that of the existing model, using synthetic as well as real datasets from the field of glycobiology. Experimental results showed that the proposed model drastically reduced the computation time of the competing model, keeping the predictive power and avoiding overfitting to the training data. Finally, we assessed our results using the proposed model on real data from a variety of biological viewpoints, verifying known facts in glycobiology.
 Categories and Subject Descriptors: I.2.6 [Artificial Intelligence]: Learning X  knowledge acquisition, parameter learn-ing ; I.5.1 [Pattern Recognition]: Models X  Statistical General Terms: Algorithms and Experimentation Keywords: Labeled Ordered Trees, Probabilistic Models, Maximum Likelihood, Expectation-Maximization
Mining frequent patterns is a general and important issue in data mining. The data in conventional data mining is well structured, and so relatively simple patterns, such as associations and sequences, are targeted. However, datasets Copyright 2006 ACM 1-59593-339-5/06/0008 ... $ 5.00. of more complex and unstructured (or semi-structured) data such as trees and graphs have recently appeared in major data mining applications, such as text mining [21], web min-ing [4] and bioinformatics [2]. A typical example is a tree-structured text format called XML on the web [1]. Mining XML documents has become an important domain in the field of data mining [22]. XML documents are datasets of semi-structured data, or more specifically, labeled ordered trees, and methods based on kernel functions [11] and fre-quent pattern mining [23] have appeared in recent years.
Probabilistic modeling and learning is a noise-robust, pow-erful and efficient approach in machine learning and data mining. Bayesian networks [16], which can handle directed acyclic graphs, have been used quite frequently since its in-troduction. However, directed acyclic graphs are much more complex than labeled ordered trees, and so simpler prob-abilistic models and more efficient learning algorithms for trees have been proposed. For example, the hidden tree Markov model (HTMM) [6] is a probabilistic model for la-beled trees that handles parent-child dependencies in a given tree. However, it does not handle sibling dependencies, meaning that HTMM is not applicable to labeled ordered trees. The probabilistic sibling-dependent Tree Markov Model (PSTMM) [18, 19] has been developed for labeled ordered trees, containing both parent-child and sibling dependen-cies. These models, which are subclasses of Bayesian net-works, significantly reduce the high complexity of Bayesian networks by restricting the target to labeled trees or labeled ordered trees [19]. The extension of HTMM to PSTMM for trees is equivalent to that of the hidden Markov model (HMM) [3, 17] to the probabilistic context free grammars (PCFGs) [12, 14] for sequences (strings). Consequently, the time and space complexity of the learning algorithm of HTMM roughly increases by a factor of the number of states in PSTMM. This is a sizable difference, since labeled ordered trees in the real world, such as XML documents, are often large (or long), while natural language sentences dealt with by PCFG are rather short. Applying PSTMM to such large data would make it intractable due to its high time and space complexity. On the other hand, for small-sized data such as those found in glycobiology, PSTMM would overfit. Thus, it is necessary to reduce its complexity in order for it to be applicable to real-world problems involving labeled ordered trees.

In light of the above, we propose a new and efficient probabilistic model, which we call the ordered tree Markov model(OTMM).Wepresentthetimeandspaceefficiental-gorithms for the three problems arising when OTMM is ap-plied to real-world problems: likelihood computation, learn-ing (estimating) the probabilistic parameters based on max-imum likelihood, and parsing (finding the most likely path). OTMM has sibling dependencies as well as parent-child de-pendencies, but the parent-child dependencies are confined to those between just the eldest siblings and their parents. Consequently, our model reduces the complexity of PSTMM to the same level as that of HTMM. The maximum likeli-hood estimation of the parameters of OTMM is based on an EM (Expectation-Maximization) algorithm [5, 15]. This is an extension of the Baum-Welch (Forward-Backward) al-gorithm for HMMs, which updates the forward and back-ward probabilities based on dynamic programming. This extension is similar to that of HTMM and PSTMM, but we emphasize that these estimation algorithms are signifi-cantly different from each other. HTMM deals with parent-child dependencies only, and so the learning algorithm of HTMM is a straightforward modification of the forward and backward algorithm in a string to compute the parent-to-child (downward) and child-to-parent (upward) probabili-ties. On the other hand, PSTMM handle both the parent-child and sibling dependencies, and so the algorithm must deal with both upward and downward probabilities as well as forward and backward probabilities. In PSTMM, every child depends on its parent, and the upward (downward) probability of a node can be directly updated from its child (parent) easily. In contrast, the parent-child dependencies in OTMM are limited to the dependencies of the eldest siblings on their parents. Thus, for OTMM, we needed to develop an algorithm by which we could compute the four (upward, downward, forward and backw ard) probabilit ies under the constraint of using the limited parent-child dependencies.
We empirically evaluated the effectiveness of our proposed scheme using synthetically generated datasets as well as real datasets in bioinformatics. From the results we obtained, we found that for any test setting, our approach significantly reduced the computation time for learning PSTMM and avoided overfitting to the training data, while either main-taining or even improving the predictive power of PSTMM. In particular, we emphasize that a typical increase in compu-tation time ranged from four-to seven-fold in our experimen-tal setting, and that this factor increases as the number of states increase. In addition to these comparison results, we analyzed biological data by our method using data from gly-cobiology, which is the study of carbohydrate sugar chains, or glycans, that can be modeled as labeled ordered trees. Glycans are considered the third major class of biomolecules next to DNA and proteins [20], and it is known that the or-dering of their leaves is used in recognition and signaling events in various biological processes. Thus we studied the patterns learned from this data and verified known facts re-lated to sibling-dependencies in glycans that were captured by our model.
We describe the notations that will be used throughout this paper. A tree is an acyclic connected graph. In this paper, we refer to a vertex of a tree as a node of the tree. A rooted tree is a tree that has a special node called the root. Any node x on a unique path from the root to a node y is called an ancestor of node y ,inwhichcase y is called a descendant of x . Each of the closest descendants of x (that is, a node that is only one edge away from node x ) is called a child of x ,inwhichcase x is called the parent of the child. We call nodes x and y siblings if x and y have the same parent. We call a node having no children a leaf .A subtree of tree T is a tree consisting of all descendants of a node. An ordered tree is a rooted tree in which the children of each node are ordered. A labeled tree isatreeinwhichalabelis attached to each node. We will often simply use the term tree in place of an ordered, labeled and rooted tree.
Let T = { T 1 ,...,T | T | } be a set of labeled ordered trees, where T u =( V u ,E u )and V u (= { x u 1 ,...,x u | V u | V u  X  V u are a set of nodes and a set of edges, respectively. Let x u 1 be the root of tree T u ,and | V | =max u | V u assume that nodes are indexed by level order, which can be done by traversing the tre e in breadth-first order. An example of trees with their indices is shown in Figure 1. From this indexing of nodes, for a node j ,wecanreferto the immediately elder and younger siblings of j as j  X  1and j + 1, respectively. Let t u ( i ) be a subtree of T u ,having x and youngest children of node p , respectively. Let C u ( p ) { 1 ,..., | V u |} be a set of indices of children of x u p |
C | =max u,p | C u ( p ) | .Let X u ( j )beasetofindicesofall the younger siblings of x u j in T u .Eachnode x u j has label o j  X   X , where  X  = {  X  1 ,..., X  |  X  | } is a set of labels. For simplicity, we will often use node j instead of x u j and p as a parent node, if understood from the context.
 Let  X  denote a set of parameters of a probabilistic model. For simplicity, we may use  X  = {  X  1 ,..., X  n } as a set of pa-rameters, such that  X  i = {  X  i, 1 ,..., X  i, |  X  i | } and for i =1 ,...,n . A probabilistic model has  X  X tates, X  each of which is a so-called latent (or hidden ) variable that cannot be seen directly, and each state has a probabilistic parame-ter which probabilistically generates a label at a node. Let of node j in a tree. For simplicity, we may also use j instead of z u j and q as the state of a parent node, if understood from the context.
We propose a new efficient probabilistic model for mining labeled ordered trees which we hereafter call OTMM, for Or-dered Tree Markov model. Here we briefly describe the dif-ferences between OTMM and two other similar tree Markov models, called the Hidden Tree Markov Model (HTMM) [6] and the Probabilistic Sibling-dependent Tree Markov Model (PSTMM) [18, 19].

OTMM is a first-order Markov chain model, meaning that a state depends on only one state. This is also true of HTMM, which is a probabilistic model for labeled trees. Figure 2 illustrates the dependencies in HTMM for the tree T u in Figure 1 (a), where the state of a node depends on the state of its parent node. In OTMM, the state of a node depends on either the state of its parent or its immediately elder sibling. Figure 3 shows the dependencies in OTMM for the tree T u in Figure 1 (a). As shown in the figure, if the node of a state is the eldest sibling, this state depends on its parent; otherwise, this state depends on its immediately elder sibling. Thus an important difference between OTMM and HTMM is that sibling dependencies are considered in children. Figure 2: Graphical representation of HTMM for tree T u in Figure 1. Figure 3: Graphical representation of OTMM for tree T u in Figure 1.
 OTMM but not in HTMM. Obviously, this difference makes the expressive power of OTMM for labeled ordered trees greater than that of HTMM.

On the other hand, PSTMM is a probabilistic model for labeled ordered trees, and the state of a node always de-pends on two different states, except for the eldest siblings. Figure 4 illustrates the dependencies in PSTMM for the tree in Figure 1 (a). As shown in the figure, the dependencies on the immediately elder sibling and on the parent are both considered in PSTMM. This feature gives PSTMM rich ex-pressive power, but it is computationally expensive and re-quires more memory space in estimating the probabilistic parameters of PSTMM. In addition, this high complexity of PSTMM has the additional risk of overfitting to data. In fact, these problems have appeared when applying PSTMM to real data. OTMM avoids these problems and achieves better performance in practical situations where PSTMM suffers from the above problems.

In addition, recall that hidden Markov models (HMMs) enable the indirect capture of distant (long-range) depen-dencies in a sequence. We emphasize that OTMM can also capture such indirect dependencies in a similar manner. For example, a dependency between a node and its distant sib-ling, which cannot be captured by HTMM, may be detected by OTMM (and PSTMM). A dependency between a node and its distant sibling X  X  descendant may also be captured Figure 4: Graphical representation of PSTMM for tree T u in Figure 1. by OTMM (and PSTMM) but clearly not by HTMM. From the viewpoint of capturing such indirect dependencies, we can say that the expressive power of OTMM is at the same level as that of PSTMM and that it is greater than that of HTMM.

The algorithms for OTMM are derived by making some significant modifications to the algorithms of PSTMM, al-though OTMM is a simplification of PSTMM. These modi-fications are necessary to account for the reduced dependen-cies between parent and child. Thus in OTMM, the ances-tor information cannot be transfered through parent-child dependencies directly except to the eldest siblings, although before, this information was easily (and directly) sent from a parent to all its children in PSTMM. As a result, it was necessary to reconstruct both the definitions of some prob-abilities and the equations using these probabilities in the dynamic programming procedure for learning PSTMM.
OTMM has three types of probability parameters,  X  , a , and b . The initial state probability  X  [ l ](= P ( z u 1 the probability that the state z u 1 of the root node x u The state transition probability a further takes two types: s the state of a node is s m given that the state of its parent is s q .  X  [ l, m ] is the conditional probability that the state of anodeis s m given that the state of the immediately elder sibling is s l . The label output probability b [ l,  X  h ](= P ( o  X  | z u j = s l ;  X  )) is the conditional probability that the output label of a node is  X  h given that the state of this node is s Note that and
Given the probabilistic model, there are three key prob-lems of interest that must be solved for the model to be used in real world applications [17]: 1) Likelihood computa-tion: computing the likelihood of a given tree, 2) Learning: estimating the probability parameters from a set of given trees, and 3) Parsing (Prediction): finding the most likely state transition for a given tree using the estimated prob-ability parameters. In the following three subsections, we Figure 5: Updating (left) U u ( q, p ) and (right) B ( m, j ) . The sparse shaded node is p for U u ( q, p ) and j for B u ( m, j ) . Dense shaded areas are used for updating. will explain our efficient algorithms for OTMM for each of the above three problems.
We define an upward probability and a backward prob-ability. The upward probability U u ( q, p ) is the probability that all labels of subtree t u ( p ) are generated and the state of node p is s q . The backward probability B u ( l, j )isthe probability that for node j , all labels of a subtree for each of the younger siblings and node j are generated, and the state of j is s l .

We can compute these two probabilities using a bottom-up (B-up) dynamic programming (DP) procedure. This computation is formulated as follows:
Figure 5 depicts the above calculation of the upward and backward probabilities. The upward probability at a node is computed using the backward probability of its eldest child, so this computation is repeated from the (eldest) child to its parent. The backward probability at a node is computed using the backward probability of its immediately younger sibling, meaning that this computation is successively re-peated from a node to its immediately elder sibling. So the whole computation proceeds from the leaves to the root, in reverse breadth-first order, going bottom-up and right-to-left (R-to-L) using dynamic programming.

We can compute the likelihood for a given tree T u by using the upward probability at the root of the tree, as follows: The likelihood for a given set of trees is defined as the prod-uct of the likelihood for each tree in the set.
 The above computation is relatively similar to that of PSTMM. A significant difference is that the backward prob-ability of OTMM is a tri-tuple, while that of PSTMM is a quatro-tuple. This feature of OTMM reduces both the time and space complexity of PSTMM.
Maximum likelihood is a general criterion used to estimate the probability parameters of a probabilistic model from the given training examples. We employ an EM algorithm [5, 15], a general and popular scheme to maximize the likelihood for a given set of examples.
We define forward and downward probabilities and use them with the backward and upward probabilities, both of which were defined in the previous section. The for-ward probability F u ( l, j ) is the probability that all labels of tree T u except for those of subtree t u ( j ) and of all subtrees t ( k )( k  X  X u ( j )) are generated and that the state of node x j is s l . The downward probability D u ( l, j ) is the probabil-ity that all labels of tree T u except for those of subtree t are generated and that the state of x u j is s l .
We can compute these two probabilities using a top-down (T-down) dynamic programming procedure. This computa-tion is formulated as follows: F ( l, j )=
D u ( l, j )=
Figure 6 depicts these calculations for the forward prob-ability at the eldest sibling and at another node. The for-ward probability at the eldest sibling is computed using the downward probability of its parent, meaning that this com-putation is repeated from a parent to its eldest child. The forward probability at another node is computed using the forward and upward probabilities of its immediately elder sibling, meaning that this computation is repeated from a node to its immediately younger sibling. Figure 7 depicts the calculation of the downward probability. The downward probability at a node is computed using its forward proba-bility (and the backward probability of its younger sibling), and so at each node the downward probability must be com-puted after the forward probability is computed. Overall, the entire computation proceed s from the root to the leaves, in breadth-first order, in a top-down and left-to-right (L-to-R) dynamic programming procedure.
 Figure 6: Updating F u ( l, j ) (left) at the eldest sib-ling and (right) at another node. The sparse shaded node is j , and dense shaded areas are used for up-dating.
 Figure 7: Updating D u ( l, j ) . The sparse shaded node is j , and dense shaded areas are used for updating.
The updating rules of the forward and downward prob-abilities are significantly different from those of PSTMM. In PSTMM, the downward probability of a node was easily computed by using the downward probability of its parent. However, in OTMM, the state of a node (except the eldest siblings) does not depend on the state of its parent. There-fore, the downward probability needed to be computed from somewhere else, either the forward and/or backward prob-abilities. That is, we needed to incorporate the downward (i.e. parent to child ) dependencies into either the forward or the backward probabilities. This was not possible for the backward probabilities, because the parent-child dependen-cies are limited to the eldest siblings only. Thus, at the eldest sibling, we compute the forward probability using the downward probability of its parent, and the forward prob-ability carries the downward dependencies from parent to child. As a result, the forward probability of OTMM con-tains richer dependency information than that of PSTMM, making it completely different from that of PSTMM.
We note that the forward probability of OTMM is a tri-tuple, while it is a quatro-tuple in PSTMM. Thus we can expect that the algorithms for OTMM will run faster than those for PSTMM. We also note that the likelihood of a tree can be computed by OTMM using the upward and down-ward probabilities at node i as follows:
The EM algorithm for OTMM iterates the following E-and M-steps alternately, using the above four probabilities. E-step: M-step:
We repeat these E-and M-steps alternately until a certain convergence criterion is satisfied 1 . A possible criterion is that the increase of the likelihood at an iteration is less than a minimal threshold value.
 Figure 8 is sample pseudocode for this EM algorithm for OTMM. This algorithm starts with parameter initialization and likelihood L ( T ) computation using the initial param-eter values (lines 1-2). The parameters can be randomly
Note that it has been proven that the EM algorithm theo-retically converges to a local maximum solution [15]. 1: for each  X  i,j do Initialize  X  ; 2: Calculate L (0) ( T ) using the initial  X  ; 3: t := 0; 4: repeat 5: for each  X  i,j do  X  T (  X  i,j ):=0; 6: for u := 1 to | T | do 7: for k := | V u | downto 1 do /* B-up &amp; R-to-L DP */ 8: for each q  X  S do Calculate U u ( q, k ); 9: for each m  X  S do Calculate B u ( m, k ); 10: for k := 1 to | V u | do /* T-down &amp; L-to-R DP */ 11: for each q  X  S do Calculate D u ( q, k ); 12: for each m  X  S do Calculate F u ( m, k ); 13: for each  X  i,j do Calculate  X  u (  X  i,j ); 14: for each  X  i,j do  X  T (  X  i,j ):=  X  T (  X  i,j )+  X  u (  X  15: for each  X  i,j do Update  X  i,j using  X  T (  X  i,j ); 16: t := t +1; 17: Calculate L ( t ) ( T ) using the current  X  ; 18: until | L ( t ) ( T )  X  L ( t  X  1) ( T ) | &lt; 19: output  X  ; Figure 8: Pseudocode of the EM algorithm for OTMM. initialized, satisfying repeating the E-and M-steps (lines 3-4). In each iteration, we first initialize the expectation value to  X  T (  X  i,j ) := 0 (line 5). In the E-step, we compute the upward and backward probabilities by a bottom-up and right-to-left dynamic pro-gramming procedure (lines 7-9) and the downward and for-ward probabilities by a top-down and left-to-right dynamic programming procedure (lines 10-12). We then update the expectation values  X  T (  X  i,j ) (lines 13-14). In the M-step, we update the probability parameters using the expectation values (line 15). At the final part of the iteration, we com-pute the likelihood L ( T ) (line 17) and confirm whether some fixed convergence criterion is satisfied (line 18). After the it-erations, we output the probability parameters learned from the given trees (line 19).
Once the model is trained, we need to retrieve what was learned by retrieving the most likely state paths. Let Z  X  { q tree T u . We first reformulate the upward and backward probabilities to find the maximum probabilities,  X  U and  X  B u ( m, j ), respectively. That is,  X  U u ( q, p )isthemaxi-mum probability that all labels of subtree t u ( p ) are gener-ated and the state of node p is s q (maximum probability of U ( q, p )), and  X  B u is the corresponding maximum probabil-ity for B u ( m, j ). In order to obtain these two  X  X aximum probability X  values, we first replace the and (2) with max and then iteratively compute these prob-abilities using the bottom-up and right-to-left dynamic pro-gramming procedure. This procedure is generally called the Viterbi algorithm, and the probability that all labels are outputted along the most likely state transition is given as P  X  =max l  X  [ l ]  X  U u ( l, 1).

We further define two functions  X  U u ( q, p )and  X  B u ( m, j ), each of which returns the most likely state for the given node, and it can be computed by replacing Table 2: Time and Space Complexity Comparison of HTMM and PSTMM with arg max. The most likely state transition starts with the most likely state for the root, which can be obtained by q 1 = arg max l  X  [ l ]  X  U u ( l, 1) . We can then retrieve the most likely state transition by computing the following equations in the top-down and left-to-right manner as done for the forward and downward probabilities: q  X  j =  X  U u ( q  X  p x  X  ( p ); otherwise q  X  j =  X  B u ( q  X  j  X  1 ,j  X  1).
Table 1 summarizes the time and space complexity of the above algorithms for OTMM, including the likelihood com-putation and parameter estimation. We note that the com-plexity of the parsing is the same as that of the likelihood computation. As clearly shown in the table, the time com-plexity of the most time consuming parts is O ( | T | X | S and the space complexity is upper-bounded by max { O ( | S |
V | ) ,O ( | S | 2 ) ,O ( | S | X |
Table 2 shows the comparison of the time and space com-plexities between OTMM and the other two tree Markov models, HTMM and PSTMM. This table illustrates that the space and time complexity of OTMM are kept at the same as those of HTMM and are significantly lower than those of PSTMM.

Furthermore, we note that algorithms used for estimat-ing probability parameters of probabilistic models for more general (complex) objects like graphs have higher compu-tational complexity. For example, both the time and space complexity of the junction tree algorithm [13] for probabilis-tic inference in a Bayesian network reaches O ( | T | X | S for a directed acyclic graph with | V | nodes. Thus we can say that our model and its learning algorithm can provide a rel-atively low complexity by confining them in labeled ordered trees.

We examined the performance of our proposed model on both synthetic and real datasets, comparing it with that of PSTMM in terms of predictive accuracy and computation time. The predictive accuracy was computed in a supervised learning manner. That is, we trained the model using a set of positive examples and examined the ability of the trained model to discriminate the positive examples from the negatives. Using the real dataset, we confirmed the validity of our method by examining the results from a variety of biological viewpoints.
The models were trained using positive examples only, and so we synthetically generated three datasets, i.e. positive training, positive test and negative test datasets. We kept the size of each of these three datasets the same, which is denoted by | T | , and in our experiments, we tested various values of | T | and numbers of states | S | .Weset | and | V u | = 20 in all of our experiments.

Each positive example contains a tree fragment as a pat-tern. Figure 9 shows the six tree fragments Q1 to Q6 that we used in our experiments. In each tree fragment in Fig-ure 9, the solid circle indicates a fixed label, and the dashed circle indicates that the label is randomly selected. So, for example, in Q5, the labels of the eldest and third siblings are fixed, whereas the labels of the parent and the second sibling are randomly applied. We generated K different la-bel patterns for each tree fragment. In positive example generation for a particular tree fragment pattern, a positive example was generated by the following two steps. First, we generated a random tree by itera tively: randomly generat-ing zero to five children and randomly assigning a label to each of the children, until the number of generated nodes reached twenty. Second, we randomly embedded one of the K label patterns of the tree fragment into the tree. A nega-tive example was generated in the same manner as the first step above, except that the random generation of labels fol-lows the distribution of parent-child labels in the positive examples.
We evaluated the discriminative performance of the mod-els by AUC, the area under the ROC (Receiver Operator Characteristic) curve [7, 8]. We can compute the AUC by first sorting examples by their computed likelihoods and then by using Equation (3).
 where n n ( n p ) is the number of negative (positive) examples and R n is the sum of the ranks of the negative examples. We note that n n = n p in our experiments.

We first evaluated the amount of overfitting to the train-ing data that occurred, using Q1 as the tree fragment. In this experiment, | T | = 100 and K = 1, such that the com-plexity of the data was relatively low and the tendency to overfit would be higher. Figure 10 shows the AUC for the training 2 and test datasets, setting the range of | S | between
We used the negative examples in the test set to compute Figure 10: Performance comparison of OTMM with PSTMM when | T | = 100 . two and twelve. The AUCs of the two models for the train-ing examples increased with | S | and reached around 100% when | S | was eight or more. On the other hand, the AUCs for the test examples decreased with | S | . In particular, the AUC of PSTMM for the test data went down to 70% from around 95%, which was the highest obtained when | S | was four. This phenomenon clearly illustrates overfitting to the training data. A similar tendency was found with the AUC of OTMM but it was always more than 85%, which was around 15% better than the worst AUC of PSTMM. Thus, can say that OTMM reduced the overfitting problems of PSTMM. We can infer from these results that PSTMM with more than four states is too complex to be trained from the dataset we used in this experiment and that OTMM is more appropriate for this dataset.

Figure 11 shows how the AUC and computation time for the test examples change with different values of | T | and |
S | ,using K = 3 and Q1 as the tree fragment. We note that the complexity of the dataset increases with | T | ,and the complexity of the model increases with | S | .Sowhen |
T | was relatively small, say 100, and | S | was large, say ten, overfitting occurred. However, when | T | was large, say 400 and 600, and | S | was small or a moderate size, say 2 to 6, the AUCs remained at the maximum, meaning that overfitting was avoided. Under such conditions, e.g. | T | = 400 and |
S | =6,wecanseethatthetwomodelsachievedalmost the same predictive performance, indicating that OTMM kept approximately the same predictive power as that of PSTMM. On the other hand, regarding the computation time, the two models were clearly different. For example, for |
T | = 400 and | S | = 10, the computation time of PSTMM reached 4,000 seconds while that of OTMM was just under 1,000 seconds. OTMM clearly reduces the computational cost of PSTMM greatly, keeping its predictive power and avoiding overfitting. This result indicates that OTMM is more practical for mining from large datasets of labeled or-dered trees compared to PSTMM.

We next fixed | T | at 200 and changed K , still using Q 1as thetreefragment. As | T | increases with K ,sodoesthecom-plexity of the data, and so the results in Figure 12 are similar to those in Figure 11. However, we note that in this case, under the conditions when overfitting did not occur, OTMM achieved better performance than PSTMM. That is, when K was set between two and four, we found that PSTMM achieved the highest AUC at | S | = 6, where overfitting did not occur, and that the AUC of OTMM was clearly better than that of PSTMM. This indicates that in this experi-the AUC of the training data.
 Q1 Q2 Q3 Q4 Q5 Q6 |
T | = 100 | T | = 200 |
T | = 400 | T | = 600 mental setting, OTMM had better predictive power than PSTMM.

Finally, we checked the predictive performance and com-putation time of the two models for all the six tree frag-ments, Q1 to Q6. The parameter settings we used in this experiment were K =3, | T | = 400 and | S | =6,thesame conditions as when overfitting did not occur for Q1 using both OTMM and PSTMM. Table 3 shows the AUC and computation time. The predictive performances of the two models were almost equivalent except for Q6, where the AUC of OTMM was roughly seven percent better than that of PSTMM. Thus we can say that OTMM has almost the same or better predictive power compared to PSTMM. On the other hand, the difference in computation time between the two models was significant. In Table 3, we indicate the ratio of the computation time of OTMM to that of PSTMM in parentheses, which shows that the typical computation time improved by a factor of at least three to at most six, which increases as | S | increases. Consequently, we have clearly demonstrated the advantage of OTMM in compu-tation time and that it is more practical than PSTMM for mining complex labeled ordered trees.
We used real data derived from glycobiology (an overview of this field is provided in a book by Varki et al. [20]), which is the study of carbohydrate sugar chains, or glycans. Gly-cans can be modeled as branched and directed tree struc-tures. The basic component of glycans is the monosaccha-ride unit (or sugar), which corresponds to a label, and each sugar may have one or more child sugars bound to it, such that they are ordered. Thus glycans can be considered la-beled ordered trees. The glycans we used in our experiments are all derived from the KEGG GLYCAN database [9, 10]. Figure 13: Performance comparison of OTMM with PSTMM using real datasets.
Glycans are classified based on their structural properties, and we selected two of the major classes called  X  X -Glycans X  and  X  X -glycans, X  which were used as the positive and nega-tive datasets, respectively. We examined whether N-Glycans could be discriminated from O-Glycans by the model trained using N-Glycans. We performed a five-fold cross-validation to evaluate the performance of OTMM and PSTMM. That is, we randomly divided a given dataset into five blocks of roughly equal size. Then the first block would be reserved as test data while the remaining four were used as training data. This was repeated for each of the five blocks. We re-peated this random division five times, and the results were averaged over the total 25 (= 5  X  5) runs. We used those glycans containing five to eighteen sugars only, because the dataset of the other glycans was very small. In total, we used 1,826 N-Glycans and 606 O-Glycans. In this experi-ment, | S | = 6 since the predictive performance of OTMM and PSTMM was always the best under this setting in the synthetic data experiments.

Figure 13 shows the AUC and computation time of the two models for the real datasets. The two models achieved approximately the same AUC values for all | S | we used. However, the amount of computation time of OTMM was much smaller than that of PSTMM. That is, at | S | =8, the computation time improvement reached a factor of ap-proximately six to seven. Thus, from the real datasets, we K =1 K =2 K =3 K =4 confirmed that the time efficiency of OTMM was consistent with the results using the synthetic datasets.
Next, we verified the actual patterns learned in the data to verify the biological characteristics retrieved. We focus on the set of N-Glycan structures because it is the most well-studied among all the glycan classes. Using the N-Glycan data set from the previous experiment, we extracted the most likely state paths and found some interesting patterns reflecting the biological properties of sugars. Figure 14 is an example a glycan tested and the most likely state transitions for it. Here we see reflected the fact that the Gal (galactose) residues and GalNAc (N-acetylgalactosamine) residues end up with the same state. This corresponds with the fact that GalNAc is a modification of Gal, such that they may be considered very similar and sometimes appear in place of each other. Thus they may be aligned together as well. A rather large number of glycans have this pattern of sugars at the leaves, where Gals may be GalNAcs and vice versa.
In another trial, we assessed the most likely state transi-tions for glycans representing the three sub-classes of N-Glycans: High-mannose, Complex, and Hybrid, given in Figure 15. Here we see these three classes being distin-guished based on the states learned. In particular, the states at the tri-mannose core distinguishes these three sub-classes. The High-mannose type is characterized by state 2 appear-ing at both child mannoses. For the Complex type, these mannoses are both state 1. Consequently, the Hybrid type, which is a hybrid of these two types having one each of a High-mannose type subtree and a Complex-type subtree, receives state 1 on one mannose, and state 2 on the other. Thus, the states learned at just the core N-Glycan structure can determine the sub-class without needing to actually tra-verse the rest of the structure. In summary, we have developed a new probabilistic model, OTMM, and an efficient learning scheme for mining labeled ordered trees. We empirically evaluated the effectiveness of OTMM using both synthetic and real datasets and found Figure 14: Example of the states learned using OTMM for a specific glycan structure. that in all of the experimental settings we conducted, OTMM achieved equivalent or better performance compared to the more complex probabilistic model, PSTMM, for mining la-beled ordered trees. In particular, the amount of computa-tion time of OTMM is significantly less than that of PSTMM in all experimental settings.

The key property of our method that contributes to these improvements is its concise model structure, in which the state of a node depends on only one state, and the effi-cient learning algorithm by which the model captures both the sibling and parent-child dependencies in labeled ordered trees. Thus, we have developed the ultimate model in terms of efficiency and accuracy for mining labeled ordered trees. [1] S. Abiteboul, P. Buneman, and D. Suciu. Data on the [2] K. F. Aoki, N. Ueda, A. Yamaguchi, T. Akutsu, [3] E. Baum and T. Petrie. Statistical inference for [4] S. Chakrabarti. Mining the Web: Analysis of [5] A. Dempster, N. Laird, and D. Rubin. Maximum [6] M. Diligenti, P. Frasconi, and M. Gori. Hidden tree [7] D. J. Hand and R. J. Till. A simple generalisation of [8] J. A. Hanley and B. J. McNeil. The meaning and use [9] K. Hashimoto, S. Goto, S. Kawano, [10] M. Kanehisa, S. Goto, M. Hattori, K. F.
 [11] H. Kashima and T. Koyanagi. Kernels for [12] K. Lari and S. J. Young. The estimation of stochastic [13] S. L. Lauritzen and D. J. Spiegelhalter. Local [14] C. Manning and H. Sch  X  utze. Foundations of Statistical [15] G. J. McLachlan and T. Krishnan. The EM Algorithm [16] J. Pearl. Probabilistic Reasoning in Intelligent [17] L. R. Rabiner and B. H. Juang. An introduction to [18] N. Ueda, K. F. Aoki, and H. Mamitsuka. A general [19] N. Ueda, K. F. Aoki-Kinoshita, A. Yamaguchi, [20] A. Varki, R. Cummings, J. Esko, H. Freeze, G. Hart, [21] S. M. Weiss, N. Indurkhya, T. Zhang, and F. J. [22] M. Zaki and C. Aggarwal. Xrules: An effective [23] M. J. Zaki. Efficiently mining frequent trees in a
