 We study the problem of personalized, interactive tag recom-mendation for Flickr: While a user enters/selects new tags for a particular picture, the system suggests related tags to her, based on the tags that she or other people have used in the past along with (some of) the tags already entered. The suggested tags are dynamically updated with every ad-ditional tag entered/selected. We describe a new algorithm, called Hybrid , which can be applied to this problem, and show that it outperforms previous algorithms. It has only a single tunable parameter, which we found to be very robust.
Apart from this new algorithm and its detailed analysis, our main contributions are (i) a clean methodology which leads to conservative performance estimates, (ii) showing how classical classification algorithms can be applied to this problem, (iii) introducing a new cost measure, which cap-tures the effort of the whole tagging process, (iv) clearly identifying, when purely local schemes (using only a user X  X  tagging history) can or cannot be improved by global schemes (using everybody X  X  tagging history).
 H.5 [ Information Interfaces and Presentation ]: H.5.2 User Interfaces; H.3 [ Information Storage and Retrieval ]: H.3.1 Content Analysis and Indexing Algorithms, Experimentation, Human Factor, Measurement flickr, tagging systems, tag recommendation, tag co-occurrence
Although the state-of-the-art in multimedia retrieval is certainly advancing fast, large scale content-based multime-dia retrieval systems remain infeasible. For example, there is still no large-scale image retrieval system, where a user can ask for  X  X ll pictures showing a horse X . In all current systems, the retrieval process relies on the fact that the key-word  X  X orse X , or possibly a related term, had been manually added to all relevant pictures. Then, traditional text-based retrieval techniques are employed. 1 These keywords are usu-ally added in the form of tags. Without these added tags, it would be impossible to find even a single relevant image.
The user X  X  motivation to add such tags is two-fold. First, it allows them to later find their own pictures related, e.g., to a particular event or a particular person. Second, it increases the accessibility to the public, as other people interested in a particular topic can now find relevant images. 2 user studies reveal that users annotate their pictures with the motivation of indeed making them more accessible to others [1]. However, most of the time users add very few tags or even none at all. In the case of the Flickr photo sharing system, at least 20% of public photos have no tag at all 3 and cases with 1  X  3 tags constitute 64% of the cases with any tags [9]. One of the reasons for this seems to be that users are often reluctant to enter many useful tags or indeed any at all. Tagging an object takes considerably more time than just selecting it for upload. Also note that any particular object (an image) is only tagged by a single user (the owner). This has to be contrasted with the setting for social bookmarking services such as del.icio.us, where a single object (a webpage) can be tagged by multiple users. Only in the latter case can standard collaborative filtering techniques be applied.

In this work, we analyze methods to support the user dur-ing the tagging process. Concretely, we consider the follow-ing interactive tag recommendation problem : Whenever the user wants to add another tag, we recommend a ranked list of relevant tags to the user. The computation of this list de-pends on the tags already present. The user can then choose whether (i) to select any item from the list of recommenda-tions, or (ii) to ignore all the recommendations and enter a tag herself. In either case, after the new tag has been added, the list of recommendations is updated in real time and the process is repeated.
In the setting of video retrieval, recent progress has been made by applying speech-to-text conversion to the audio data of a videos. The transcription obtained this way can then be searched just as normal text.
As usual, this causes tag spam to occur, where lots of ir-relevant tags are added, as certain people want their objects to be found, regardless of the user X  X  interest [6].
This estimate was obtained by our sample of Flickr from crawling it using friendship links. People who maintain their friendship links are, however, usually more  X  X ctive X  than oth-ers and also tend to tag more.
This problem is independent of any particular applica-tion, but we only evaluated our algorithms in the context of Flickr. Note that the methods we will study are personal-ized in the sense that they explicitly use knowledge about the particular user X  X  tagging behavior in the past. This way they can, for the same input tags, recommend different tags to different users. E.g., for the input tag  X  X aguar X , they will propose  X  X ar X  to one and  X  X at X  to another user. Similarly, for the tags  X  X ausanne X  and  X  X pfl X  we could recommend  X  X witzer-land X  to one user,  X  X uisse X  to another and  X  X chweiz X  to a third depending on one X  X  language.

The quality of algorithms for this problem can be assessed in two different ways. First, for a certain number of al-ready selected tags, the quality of the recommended list can be evaluated within the usual precision-recall kind of setup known from information retrieval. However, this focuses on one particular point in time is not really adequate, as we are interested in the performance over the whole sequence of added tags. Therefore, we also propose and use a second evaluation setup.

In absolute numbers, our best method, called Hybrid, achieves the following performance: When half the tags of a picture are given as the input to the algorithm, and only the tags in the other half are considered as relevant, we achieve an average precision at the first position (P@1) of around 55% for users who have tagged 1  X  31 pictures before, 65% for users who have tagged 32  X  255 pictures before and around 70% for users with even more tagged images. The typical success among the first five positions (S@5) correspondingly lies between 75% and 90%. Note that, due to how we defined relevance, these are underestimates of the system X  X  true per-formance. As shown in the experimental section, our system performs considerably better than two other schemes for this problem, recently presented at the WWW conference [3, 9].
If we suppose the user has a cost of 10 units (in some cost measure) for typing a tag herself, and a cost of 1 for checking if a given tag is relevant (and clicking it, if it is), then we can reduce the average cost for inputting a tag, averaged over all the tags of the picture, from 10 units to 8 . 4 in a conservative setting and to 6 . 8 in a more realistic setup.
Apart from the presentation and the analysis of our algo-rithms, our main contribution is the description of a clean methodology for the problem studied and the identification of key factors governing the performance, such as the  X  X ov-erage X , defined in Section 6.1. Furthermore, we will point out several pitfalls concerning the use of a user study for the problem and we will explain how to obtain conservative performance evaluations without such data. This will allow us to evaluate our methods not only for a few hundred pho-tos (for which the suggested tags were manually classified as relevant or not) but even for thousands of photos.

Note that the algorithms studied are not inherently re-stricted to tags. They only require  X  X ets of items X  and, to give personalized results, for each user a history of past sets of items. Other use cases would involve the suggestion of additional items to buy, as a customer fills her shopping basket. Here, we can use both knowledge about what other people have bought together in the past and about what the particular customer has bought before. This can be seen as a personalized and soft version of association rules mining. By  X  X oft X  we mean that there are no hard rules found, which depend on a specific level of  X  X onfidence X  and  X  X upport X , but rather just a soft ranked list of recommended items.
The rest of this paper is organized as follows. In Section 2 we discuss work related both to the use of tags on Flickr, as well as to automated tagging in general. In particular, the relation to two previous works on tag recommendation for Flickr will be clarified. In Section 3 we introduce our methodology. Section 4 shows how other classification al-gorithms could be used for the problem of tag recommen-dation. Before we describe the algorithms in Section 6, we discuss the (partly novel) evaluation measures in Section 5. Finally, our experimental setup and results are presented in Sections 7 and 8.
The two pieces of work, which are most closely related to our current work, were both presented at the WWW con-ference this year [9] [3]. Both study the problem of recom-mending tags to Flickr users. Apart from the actual meth-ods used to compute recommendations, the main differences to the work in [9] are that (i) our tag recommendation are personalized , (ii) the evaluation setup avoids the significant problems of a user study (see Section 3.1), (iii) the removal of all crucial tuning parameters, (iv) the performance study in an interactive setting, (v) a detailed breakdown of the performance across various axes (such as the size of a user X  X  profile and the  X  X overage X  of the input tags). Similarly, the main difference to the work in [3] are points (iii) and (v) above. All three schemes are also directly compared to each other in Section 8. The scheme presented in the current work outperformed both by a significant margin, while also being the simplest of them.

In a broader context, various research projects have ex-ploited the tag information in the Flickr community and other tagging systems to automatically extract useful seman-tics. The GPS and time stamps of photographs have been used along with the tags to extract events ( X  X Y Marathon X ) and places ( X  X ogan Airport X ) [8]. With enough pictures au-tomatically annotated with GPS or time data, such ideas could also be applied to our problem. E.g., a picture whose GPS coordinates are in Switzerland is more likely to be tagged with  X  X witzerland X . Similarly, the tag  X  X inter X  will be more prominently used (on the Northern hemisphere) in January than in July. Although we plan to experiment with these ideas in the future, we did not consider them for this work, as more straightforward personalization already gave very satisfactory results.

Automatically assigning/suggesting tags to blog posts [7, 10] is related to our problem. But as blog posts contain full text, finding similar posts and hence related tags is easier than for pictures. Similarly, automatic tagging of web pages can be improved/personalized, if the system has access to the surfer X  X  desktop [2]. If we had additional knowledge from external sources about a particular user, for example from email conversations, then such techniques could also lead to an improvement for our problem. Here, the biggest problem would be how to obtain such data for many users.
In settings where several users collaboratively tag the same items, such as for social bookmarking sites, yet other tech-niques can be applied [11]. Algorithms for mining associa-tion rules [5] are also relevant. But they (i) are comparably slow, (ii) would not give any result if the initial set of tags does not occur in any picture, and (iii) would not generate a ranked result list. Still, we might explore adaptations of those techniques in the future.
To evaluate the quality of the recommended tags, one needs to know which tags a user would have entered or se-lected for a given picture. Here two approaches are possible. First, one could evaluate the relevance of the returned tags via a user study and, second, one can work with only the tags originally provided by the Flickr user.
Given a particular image and a particular tag one would at first think that any intelligent user could easily judge whether this tag is relevant to this picture or not. However, as we will argue here, only the actual owner can ultimately decide this question.
 Multi-lingual tags: Given the picture of a Swiss flag, the tag  X  X witzerland X  is somehow obviously relevant for the picture. However, some users might use the tag  X  X uisse X ,  X  X chweiz X  or  X  X vizzera X  4 . Given only the picture, it is impossible to judge the relevance of such terms. One  X  X olution X  would be to consider only (American) English tags as relevant, but this is an unsatisfactory approach.
 Missing context: The tag  X  X olidays X  for a picture taken in, say, Berlin will only make sense to the user, if the image was taken while on holidays. But this contextual informa-tion is impossible to obtain for any external human judge. Similarly, one cannot simply assume that the tag  X  X olidays X  is clearly relevant for any picture showing a beach, as the owner of the picture might live at this location. The same problem of missing context applies to tags such as  X  X e X  or  X  X riends X  or any personal name or even most place names. Missing meta data: Often professional users tag pictures with the brand name of the camera (e.g.,  X  X ikon X ) or with information about the shutter speed or the focal settings. Given only the picture itself, the relevance of such tags is impossible to judge, but a diligent judge could try to obtain such information from meta data, if it is available. Differences of level of detail: Consider a picture of the cathe-dral in Lausanne, so  X  X ausanne X  is probably a relevant tag. What about  X  X aud X  (the Swiss canton Lausanne is in)? What about  X  X witzerland X ,  X  X urope X ,  X  X arth X  or  X  X niverse X ? All of this are  X  X orrect X  in some sense. But for some (Swiss) users even the tag  X  X aud X  might be redundant. A similar problem occurs with standard portraits. Should an ordinary portrait be tagged as  X  X ose X ,  X  X yes X ,  X  X outh X ,  X  X ostrils X ,  X  X yelashes X  etc. or simply as  X  X ace X ? What would be considered a rele-vant tag by the actual user is unclear.
 Interdependence of tags: What if in the list of suggested tags for a black and white picture there are the tags  X  X lackand-white X ,  X  X lack X ,  X  X hite X ,  X  X w X . Should all of them be counted as relevant? What about  X  X sa X ,  X  X nitedstates X ,  X  X nited-statesofamerica X ,  X  X merica X  and  X  X s X ? What if only the tag  X  X lant X  and not  X  X ower X  is suggested for the image of a power station? What about just  X  X ngine X  and no  X  X ire X ?
Note that these cases are not rare exceptions, but they ac-tually constitute the majority of cases. We therefore believe that simple user studies, such as used in [9], do not give a re-liable estimation of the quality of a system, especially if the
The term  X  X witzerland X  in French, German and Italian re-spectively. category  X  X ood X  on the scale  X  X ery good X ,  X  X ood X ,  X  X ot good X  and  X  X on X  X  know X  is counted as relevant in all experiments. Our own belief is that they tend to give overestimates of the true quality of a system, as hundreds of tags can be viewed as  X  X ood in some sense X  for any given picture.

If one still wants to make use of a user study, it is impera-tive to give clear and unambiguous guidelines to the human judges about how to evaluate the relevance of a tag, in par-ticular addressing all the cases above. Our own approach, discussed in the next section, avoids such problems.
For any picture we only consider tags already added by the original user as relevant. A subset of them is then given as input to the system to generate a list of recommended tags. Only the remaining tags, which were not given to the system, are considered as relevant. All other tags are considered as not relevant.

Note that this is the standard machine learning approach for classification: Some labels (tags in our case) are given to the system, which then learns a model. Given this model, the system generates possible labels, whose quality is evalu-ated using hold-out data, not used in the training phase.
In our setting this gives an underestimation of the per-formance of the system as (i) all relevant tags were actually added by the user, who presumably considered them rel-evant enough to add, and (ii) there might be other tags, which the user would consider relevant.
To be absolutely sure to not overestimate the quality of a system, the following two issues need to be addressed. Tags, which the user cannot decide to add: Flickr has a lot of special tags, such as  X  X upershot X  or  X  X bigfave X , which are awarded to high quality pictures. These tags are not added by the user herself, when she uploads the picture, but only later after a sufficient number of people has acknowledged the quality of the picture and the picture is then invited to special groups.
 To suggest such tags to the user, simply does not make any sense, as the user cannot simply choose to add such a tag (or is at least not supposed to). Therefore, we removed a list of the 15 most prominently used tags ( X 15fave X ,  X  X big-fave X ,  X  X nawesomeshot X ,  X  X plusphoto X  and 11 others) from the consideration of the system. These tags, even if present in the original picture, are counted as non-existent. Tagging many pictures at once: Flickr allows the batch up-load of pictures, where the same set of tags can be added to all pictures at once. Especially, for schemes using the user X  X  profile to suggest tags, this can lead to an overestimate of the quality of a scheme. If we happen to use one of these pic-tures for the evaluation, and if we give, say, half of its tags to the system, then the system can note that there are already a number of pictures with all of these tags present. So it can easily suggest the remaining tags. However, this would be an artifact of the evaluation setup, as all these pictures were actually tagged at the same time , so that the case we are evaluating would never occur in practice. Therefore, we again take a conservative approach, and only consider the distinct tag sets for each user. So if a user used the tag com-bination  X  X ausanne X  and  X  X ake X  several times, we only count it once, as we cannot know, whether it was created by batch tagging. This will further ensure that our reported numbers are a lower bound for the real setting.
As it might not be obvious, we will demonstrate how any classification algorithm can be applied to the problem of tag suggestion, as long as it can cope with extremely high dimensional but sparse feature vectors and a huge number of possible classes. Recall, that any classification algorithm first trains on labeled data, that is, feature vectors with ground truth label. Then, once whichever algorithm has learned a model, the algorithm can predict the label for a new feature vector.

The main  X  X rick X  in mapping our problem to this setting is to use tags both as features and as labels. A feature vector will then have dimensions corresponding to the tag vocab-ulary, where all entries are binary. Concretely, a picture tagged with  X  X  X ,  X  X  X  and  X  X  X  would lead to a training set of three feature vectors (corresponding to  X  X  X , X  X  X , to  X  X  X , X  X  X , and to  X  X  X , X  X  X ) with the three labels  X  X  X ,  X  X  X  and  X  X  X  respec-tively.

Given a set of tags already input by the user for a new picture, the algorithm can then predict a new tag. As most classification algorithms internally compute some soft, i.e. non-binary, relevance score for each possible class, we can obtain a ranked list of tags to suggest.
We used two different kinds of evaluation setups. The first is a standard information retrieval setup for evaluating the quality of a ranked list, which is well-established in the literature. The second setup is especially tailored for our setting and takes the whole input process into account.
In this setting, we evaluate the quality of a single ranked list of tags when a certain number of tags have already been provided by the user. The quality of this list is evaluated using the following classical metrics.
 P@1 - X  X recision at one X : The percentage of runs, in which the first recommended tag was relevant. It is, by definition, equal to S@1.
 P@5 - X  X recision at five X : The percentage of relevant tags among the first five, averaged over all runs.
 S@5 - X  X uccess at five X : The percentage of runs, in which there was at least one relevant tag among the first five re-turned tags.
 MRR - X  X ean reciprocal rank X : If the first relevant returned tag is at rank r , then the MRR is 1 /r , where this measure is also averaged over all the runs.
While the previous measures all looked at the static case of evaluating a single ranked list of tags, the measure we will introduce here takes the whole input sequence into account.
At each point of the tagging process, even when no tags have been added so far, a ranked list of recommended tags is presented to the user. The user then goes through this list from the top and checks each tag, one after the other, for its relevance. The  X  X ost X  for this checking is essentially the time it takes to read the tag and we charge a unit cost for this action. Once the user has checked a tag, she can select it by clicking, for which we do not charge any further cost. If the user does not find a single relevant tag in the list of k tags presented to her, she first pays a cost of k for checking all tags and then an additional cost of t , where t &gt; 1, for typing a relevant tag herself. Regardless of whether she could simply click a recommended tag or whether she had to type a tag herself, at the end there is one additional given input tag and the list of recommended tags is updated.
For fixed values of k and t we can thus look at the average cost for adding a single tag. In the best case, this cost is close to 1, as the user can often simply select a tag from close to the top of the list by clicking. A cost of t can be obtained trivially when no recommendations are shown and the user has to type all the tags herself. If the list of recommendations only contains irrelevant tags, then a cost of k + t will be incurred. Note that the cost of entering a tag will, in practice, depend on the number of tags already entered.

Several refinements for this model are possible. Firstly, we can take the length of the tag manually added by the user into consideration, and have a tag-dependent t . Secondly, we could also charge more or less than 1 for the cost of reading a tag. For example, there might be a certain  X  X nnoyance factor X  associated with some tags (e.g., obscene tags), or there might be an  X  X ducational factor X  associated with other tags (when the system suggests a tag the user likes, but would not have thought of herself).

For our evaluation, we used the simple model with t = 10 (the typing cost) and k = 5 (the length of the list of suggestions). The particular value of k was chosen as it seemed to be the case that, if a relevant tag is found by our system, it is found among the top 5 tags. The value of t was chosen to agree, at least approximately, with the typing overhead and also with the missed opportunity of exploring other tags. Also note that the value of k could be set by the user. There could be a short list of high-quality suggestions, and then the option to click  X  X ore X  to see further recommendations.
Let the number of different tags in the system, i.e. the tag vocabulary, be n . Let m be the number of pictures used by the system 5 . m ( x ) denotes the number of pictures tagged with x and m ( x ) u denotes the number of pictures in the collection of user u which are tagged with x . We will call the set of tags already input by the user u , for which we want to suggest further tags, a query and we denote it by q u . q u is simply an n -dimensional binary vector which has 1 X  X  in only the dimension for the given tags. | q u | is the number of input tags. The algorithms will make crucial use of (at least one of) the following entities: H u -the history of user u . Concretely, H u is a matrix with dimensions n  X  m u , where m u is the number of pictures tagged by user u in the past. H u is a binary matrix for which an entry ( i,j ) is 1, if and only if user u tagged picture j with tag i . Most of the time, u will be the user for whom we are generating a list of tags to suggest. In this case,
More correctly, we use the number of tag sets (see Section 7.2), to avoid overestimating the quality of our methods. any information derived from H u is referred to as  X  X ocal X . Similarly, if the user considered is a different user, we call such information  X  X on-local X  or  X  X lobal X .
 G -the global history. Concretely, G is a matrix with di-mensions n  X  m , defined in a similar way to H u . Information derived from G is also referred to as  X  X lobal X .
 C -the global co-occurrence matrix. C is defined as GG t and an entry ( i,j ) counts on how many images the tags i and j co-occur in the global history. Similarly, we can define C u = H u H t u for a specific user u . Note that both of these matrices will be dominated by their diagonal entries, which simply count how often a certain tag x was used ( m ( x ) m
We also need to define the important notion of coverage of a query q u . A query q u is said to have a coverage of v if the user u has a picture which contains v of the | q u tags, but no picture which contains v + 1 of these tags. The intuition is that if for a set of 5 input tags there is already a picture containing all 5 of these tags (and probably others), then we can simply  X  X opy-and-paste X  the remaining tags of that picture to get good suggestions. So a low coverage v signifies a potentially hard instance.
One ingredient of our best performing scheme is the use of a Naive Bayes classifier to obtain tag recommendations based on H u . See [4] for details about Naive Bayes. Con-cretely, we attribute the following score s q ( x ) to tag x for input query q :
Here, P ( x ) = H u ( x,x ) /m u = m ( x ) u /m u is the (estimated) probability of tag x being used by user u and P 0 ( i | x ) is the smoothed probability of seeing tag i (which is present in the query) given tag x . Concretely, P 0 ( i | x ) =  X P ( i ) + (1  X   X  ) P ( i | x ) where P ( i | x ) = H u ( x,i ) /H u our experiments, we used a value of  X  = 0 . 1 to avoid zero probabilities. When there is no input tag, i.e. | q | = 0, or none of the input tags has been used by the user in the past, then we simply set s q ( x ) = P ( x ) and rank according to the frequency with which a tag was used in the past.

In Table 1 this scheme is referred to as Local .
This scheme uses only global information and does not personalize the suggestions in any way. It starts by deriving a new matrix  X  C from C in two steps. First, all diagonal values of C are set to zero. Note that the diagonal values (i) dominate the matrix and (ii) are not directly useful for generating recommendations, as they do not contain any in-formation about the co-occurrence of different tags. Second, each column of this new matrix is normalized, i.e. scaled, so that the maximum in each column is 1. 7 The vector of scores s q is then computed as
The coverage depends on the user u , but this additional subscript is dropped for simplicity.
Tags which never co-occur with any other tag are useless and are removed at the beginning.

Here, the  X   X   X  stands for the component-wise product of two vectors and idf( x ) = log( m/m ( x ) ) is a vector of  X  X n-verse document frequencies X  (or rather  X  X nverse tag frequen-cies X ). Note that  X  C can be seen as a normalized version of a standard document-term matrix, so that this scheme is just a simple tf-idf retrieval. Also observe that whereas in the Naive Bayes scheme the contributions from the various in-put tags (the P 0 ( i | x )) are multiplied , here the corresponding contributions are added . We also experimented with using the Naive Bayes Formula 1 for the global setting, but this gave considerably worse results.

In Table 1 this scheme (not using Naive Bayes) is referred to as Global .
In settings with a high coverage v q (see the end of Sec-tion 6.1), local schemes such as the one in Section 6.2 work well, and global information does not lead to a (significant) performance improvement. However, in settings with a low coverage or even v q = 0, the local information can only serve to give information about the overall frequency of in-dividual tags, but does not provide any useful co-occurrence information. In these cases, the use of global information gives a significant improvement in performance.

Using this simple observation we define a new hybrid scheme as follows.
Here, the s ` q is the s q defined in the (local) Equation 1 and, correspondingly, the s g q is the s q defined in the (global) Equation 2. The query (and user) dependent weight  X  q is not a tuning parameter, but it is defined as v q / | q | , i.e. as the highest percentage of input tags covered by any of the users past pictures.

Note that it would also have been sensible to choose the aggregation parameter  X  in dependence of the size of the user X  X  history, i.e. depending on m u . However, as Table 3 shows, the performance difference between the local and the global scheme depends much more distinctly on the coverage v than on m u .

Note that the only tuning parameter involved in the whole scheme is the  X  for the smoothing in Naive Bayes (see Section 6.2). Here, we found the performance to be fairly robust within a range of roughly [0 . 05 , 0 . 2] and we expect a value of  X  = 0 . 1 to also work well for other collections.
In Table 1 this scheme is referred to as Hybrid .
In our previous work [3], we presented a different approach to using both global and local information. This scheme works in three phases.
 First, it uses a ranking of local tags according to the follow-ing formula. Here, idf( x ) = log( m/m ( x ) ). Of course, the addition of q is irrelevant, as tags originally present will never be counted as valid suggestions, but we also use this vector s expanded query in the following steps.
 Second, it finds a set of top k = 10 relevant groups using the cosine product H v s ` q . 8 Note that as groups on Flickr also
Query-independent components, only involving H v , H u come with pictures, they can be treated just as regular users. However, using related groups (and not users) gives slightly better results, as groups are more semantically focused than individual users. For each of the 10 groups we obtain sugges-tions from their history H v according to the same Equation 4, where q u is now replaced by the (expanded) s ` q and the C u of the original user is replaced by C v . All of these 10 dif-ferent sets of scores where then added into a single  X  X emote X  set of scores s r q .
 Third, it combines the initial ranking s ` q and the rankings obtained from the groups s r q in a query-independent manner. The final scores are then given by In Table 1 this scheme is referred to as Old .
Recently, a research group from Yahoo also presented a method for tag suggestion at the WWW conference. They use the global co-occurrence matrix C in a similar way to Equation 2. Additionally, they also use the values m ( x ) punish both very frequent and very infrequent tags. The details can be found in [9].

The main differences are the following. First, they use 4 parameters of which at least 3 have an crucial impact and need to be fine-tuned. To report fair performance numbers for their system, we had to invest quite a bit of time into find-ing appropriate values. The parameters used in [9], namely m = 25, k s = 9, k d = 11 and k r = 4, gave a P@1 of a mere . 05. The parameters we finally used were m = 25, no k s k = 7, k r = 4, which led to a P@1 of around . 15 (see Table 1). Second, they cannot easily sum the contributions per-taining to different input tags, as is done in Equation 2, as these contributions are not normalized. However, without removing the diagonal values of the matrix C , these values cannot be normalized in a meaningful way, as we also ob-served in our own experiments. Thus, they have to combine different contributions by looking at the rank of a tag in each contributing list. Besides the computational overhead of an additional sort operation for each such list, this then creates the need for the parameter k r , which governs the combination of such ranked lists.

In Table 1 this scheme is referred to as WWW .
We obtained all of our data for the experiments using the publicly available Flickr API 9 . In total, we downloaded tag information for 50 million public photos with at least one tag. Out of these, we extracted 24 million tag sets (see Section 7.2). These pictures/tag sets came from 77 , 166 dif-ferent users. In our experiments we looked at breakdowns according to two dimensions, discussed in the following two sections.
Users on Flickr differ widely with respect to (i) how many pictures they uploaded to Flickr and (ii) how likely they are to add tags to these pictures. For our study, only users who and not q u , can also be used, but this did not lead to an performance improvement, at least not when the query q u is already expanded to s ` q . http://www.flickr.com/services/api/ have tagged at least one picture were considered. Figure 1 shows the distribution of big and small users. Figure 1: Number of users (on the y -axis) with a cer-tain number of distinct tag sets (on the x -axis). We only used the distinct tag sets for each user to avoid inflating the performance numbers. See Section 7.2 for details.

A picture taken randomly from the set of tagged pictures is far more likely to come from a user, who has tagged a lot of pictures in the past than from a user who has only tagged relatively few pictures. So sampling pictures uniformly at random does not lead to a uniform sampling distribution over the users. As methods using local information derived from the user X  X  past history are more likely to give good results for such  X  X ig X  users, we tried to avoid this inflation of the reported numbers by sampling users from the following three categories.
 Small users : These are users with only 1  X  31 previously tagged pictures 10 . Due to the sparseness of local information for them, methods using global information will work better for them.
 Medium users : These are users with between 32 and 255 previously tagged pictures. Figure 1 shows that typical users (who tag at all) fall into this category.
 Big users : These are users with at least 256 previously tagged pictures. For these users, methods using local in-formation will work best.
The number of tags used per picture differs widely be-tween 1 and several hundreds. The typical range of the number of tags is between 1 and 6 [9]. However, for our (conservative) evaluation setup, pictures with less than 4 tags cannot be used in a meaningful way, as there is not enough data to evaluate the list of recommended tags.
This does not mean that we assume in our experiments that the user has already entered 4 or more tags. In the basic setting, we give half the tags (rounded up) to the system and then evaluate the quality of the list. This is the setting used for Tables 1 and 3. However, in Table 2 we also give a
Wherever it says  X  X icture X  or  X  X mage X  it should say  X  X istinct tag set X . See Section 7.2 for a detailed explanation. breakdown of the quality of the returned lists for any given number of input tags, thereby analyzing the whole tagging process, and not just a snapshot. This should be contrasted with the approach used in [9], which used all the given tags as input and then used a user study to evaluate the quality of the suggestions.

For our experiments, we bucketed the pictures into three categories. One for pictures with 4  X  7 tags, one with 8  X  15 tags and one with 16  X  X  X  tags. As we only use given tags as relevant tags, there are more possible tags to be found for pictures with more tags, and so the performance for these pictures is higher (see Table 3).
For our basic setup we chose half the given tags (rounded up) as input to whichever system we evaluated. These tags were obtained in an alternating manner, meaning, that the first, third, fifth and so on tag would be used as input, whereas the second, fourth etc. tag were used as hold-out data to evaluate the performance.

For the Input Cost setting (Section 5.2), we started with no tag given as input. Here, the system simply ranked tags according to their past usage frequencies. Then, we followed the  X  X low X  of either typed or clicked tags. This way, input tags were added one after the other, and the exact sequence depended on the previous output of the system. P@1 Figure 2: A detailed analysis of when our Hybrid scheme (solid red line) has the biggest performance advantage compared to the simple local scheme (dot-ted blue line). The P@1 ( y -axis) results shown are for pictures with 8  X  15 tags coming from differ-ent kinds of users (square =  X  X mall X  users, circle =  X  X edium X  users, triangle =  X  X ig X  users). To restrict the evaluation to a certain maximum coverage ( x -axis), all other pictures in the user X  X  profile with a higher coverage than the threshold were artificially ignored by the system. For low coverage, which cor-responds to the situation of both new users and a new tag topic for old users, the improvement in P@1 between the local and the hybrid scheme is between 100% and 200% . This can be seen by comparing, for a fixed symbol, the two different colors (or line types).
Small
Medium
Big Table 1: Comparison of several methods for 1 , 000 pictures in each set. Since no limit was set for the coverage of tags, the hybrid method only slightly improves over the purely local method for medium and big users. See Table 3 for a breakdown of the first three methods for different levels of coverage.
Our  X  X ybrid X  scheme outperforms both our own previ-ous system ( X  X ld X ) and another recently presented scheme ( X  X WW X ). See Table 1. For cases of low coverage,  X  X ybrid X  improves dramatically over the simple  X  X ocal X  scheme. See Table 3 and Figure 2. For a simple model of measuring the cost of inputting tags, our scheme improves the average cost of tagging by at least 16% in a conservative setting and by at least 32% in more realistic setting. See Table 2.
In this work, we implictly defined a tag as  X  X ood X , if it would be selected by a user, when recommended to her. In the future, we plan to investigate other definitions of  X  X ood-ness X  related to either (i) the usefulness to other users (e.g., using query logs for image searches) or (ii) the navigability of the user X  X  collection (e.g., using entropy measures to avoid that the user tags all her pictures in a very similar fashion). [1] M. Ames and M. Naaman. Why we tag: motivations [2] P. A. Chirita, S. Costache, W. Nejdl, and [3] N. Garg and I. Weber. Personalized tag suggestion for [4] T. Hastie, R. Tibshirani, and J. H. Friedman. The [5] J. Hipp, U. G  X  untzer, and G. Nakhaeizadeh.
 [6] G. Koutrika, F. A. Effendi, Z. Gy  X  ongyi, P. Heymann,
Small users .28 .11 .40 .9 .33 .17 .45 1.3 .33 .17 .45 1.3 .31 .15 .44 1.0 .43 .25 .56 1.8 .49 .32 .75 2.3 .27 .15 .42 1.0 .36 .24 .53 1.8 .59 .51 .72 6.1
Medium users .46 .18 .62 1.5 .51 .23 .68 1.9 .51 .23 .68 1.9 .47 .25 .68 1.7 .59 .36 .78 2.7 .62 .42 .82 3.3 .49 .28 .71 1.8 .60 .39 .81 3.1 .73 .61 .89 6.4
Big users .43 .19 .65 1.7 .50 .24 .73 2.3 .50 .24 .73 2.3 .50 .26 .73 1.9 .66 .41 .85 3.1 .68 .46 .87 3.7 .51 .27 .71 1.9 .61 .39 .82 3.4 .75 .62 .90 6.5 [7] G. Mishne. Autotag: a collaborative approach to [8] T. Rattenbury, N. Good, and M. Naaman. Towards [9] B. Sigurbj  X  ornsson and R. van Zwol. Flickr tag [10] S. C. Sood, K. J. Hammond, S. H. Owsley, and [11] Z. Xu, Y. Fu, J. Mao, and D. Su. Towards the Table 2: Details of the Input Cost measure (see Sec-tion 5.2) for our Hybrid scheme and for 1 , 000 pic-tures with 8  X  15 tags from  X  X edium X  users. The cost continues to rise as, in our conservative set-ting, there are fewer and fewer relevant tags left to find. The numbers in parentheses refer to the set-ting, where we stop suggesting, when there are only three tags left to find. The average cost for entering a tag, averaged over all 1 , 000 pictures, is 8 . 4 in the original setting and 6 . 8 , when the suggestion process stops when there are only three tags left.
