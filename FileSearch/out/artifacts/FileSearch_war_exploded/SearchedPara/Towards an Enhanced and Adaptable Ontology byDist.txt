 In this paper, we investigate the problem of making better use of semantic knowledge obtained from different encyclopedia sources. We propose a framework to integrate different encyclopedias and reorganize the information. We also utilize Learning to Rank mod-els to distill out more functional knowledge from the encyclopedic information and then align the knowledge with a WordNet-like on-tology. Finally as a demonstration, a Chinese semantic knowledge repository named JNet is constructed based on this framework. Ex-periments show that the proposed methods work well and the three steps reinforce each other towards a more powerful ontology. H.3.1 [ Information Storage and Retrieval ]: Content Analysis and Indexing X  Dictionaries, Thesauruses JNET, CCD, Encyclopedia, Ontology
In the era of information explosion, the deluge of knowledge causes an unremitting emergence of neologisms. As a result, the vocabulary knowledge needed to understand a language dwarfs the coverage of any existing manual lexical knowledge base. There-fore, an automatically built lexicon with wider coverage is of great help for interpreting semantic information. The knowledge from the folk wisdom amassed in the online encyclopedias is a rewarding resource owing to the constant contribution of the numerous users. Web users are likely to introduce a neologism to an online encyclo-pedia as soon as it comes into vogue. In contrast, neologisms will  X 
Supported by NSFC with Grant No. 61073081 and 973 Program with Grant No. 2014CB340405  X  Corresponding author be added into an expert-edited lexicon in the next edition several months or even several years later.
 This problem is even more urgent in Chinese. Different from English and many other languages, the Chinese words are not sep-arated from each other by the space symbol in writing. As a result, term segmentation is the primary step in Chinese language pro-cessing. If the semantic repository employed in this step is not able to recognize the words and separate them correctly, the semantic information of the sentences will be misinterpreted or even lost. Therefore, a powerful semantic repository is of significant impor-tance in Chinese language processing. In this paper, we mainly focus on Chinese ontology enhancement.

The online encyclopedia, albeit rich in knowledge storage, is ca-sually organized. Besides, overlaps exist between different ency-clopedias and even data obtained from the same source needs to be reorganized. In encyclopedias like English Wikipedia, most am-biguous words are separately explicated in different articles. Nev-ertheless, in most Chinese online encyclopedias which are not well developed, words are sometimes organized in terms of word forms and different meanings of a word are exhibited in the same page. Besides, alternative names for the same concepts are scattered in different articles. To merge online encyclopedias into a semantic lexicon using concepts as basic elements, homograph disambigua-tion and synonym detection are prerequisites.

Although a broad vocabulary knowledge is one of the main aims to build an enriched ontology, it is in effect sometimes unnecessary to retain all the information obtained from the online encyclope-dias. Because of the collaborative way of editing, the quality of some articles in online encyclopedias is rather low. Noises will be brought in by spam articles and impair the quality of the en-riched ontology. Moreover, some supplementary knowledge, on one hand is precious treasure, on the other hand could be a big bur-den in practical applications sometimes. An ontology encapsulat-ing all the entries of an online encyclopedia will include millions of words, some of which are quite rarely used. In comparison, expert-edited lexicons such as WordNet usually contain tens of thousands of words. For applications which utilize the ontology as a basic tool, the space and time consumption will be increased by a large scale due to the huge size of the ontology. We believe that the sup-ply of the supplementary knowledge should be abundant but not superfluous.

In this paper, we integrate two online Chinese encyclopedias with a WordNet-like semantic lexicon of contemporary Chinese named Chinese Concept Dictionary (CCD) and construct an en-riched ontology. We firstly crawl articles from the online ency-clopedias, and then the encyclopedia articles are reorganized and grouped into synsets in the next step. After that, the synsets are ranked according to several criteria. Then different versions of the enriched ontologies are constructed by adding different proportions of the highly-ranked synsets from the encyclopedias to the expert-edited lexicon.
The main goal of encyclopedia integration is to reorganize the encyclopedic information in terms of word meanings. Entities with the same name but referring to different meanings should be dis-criminated and entities which are synonymous but have different names should be assembled.
Encyclopedia entities are the basic units of an encyclopedia and are presented by encyclopedia articles:
Encyclopedia Entity : An encyclopedia entity is composed of a six-tuple h name, Edt, Intro, Cites, Rel, Ctg i . Here name is the name of the entity, initially composed of its article title. After integration, name could be a synset containing several alter-native names of the entity. Edt refers to the edit information and is composed of sets of two-tuples &lt;Edtor, Num&gt; which means the editor Edtor has edited the article for Num times. And Intro is the introduction of the entity, namely the text of the article body. Cites denotes the set of cites which appear in the article and link to other articles. Rel indicates the set of related words and Ctg denotes the set of categories the entity belongs to.
 Concepts are fundamental elements of a WordNet-like ontology. Words of the same meaning are packed into the synonym set of a concept and a semantic network is constructed by linking concepts with various semantic relations. In this paper, we mainly focus on hyponymy relation.

Ontology Concept : An ontology concept  X  in a WordNet-like on-tology is denoted as a four-tuple h Syn, Def, Hyper, Hypo where Syn is the synonym set, Def is the definition of the concept, Hyper and Hypo denote the hypernym and the hyponym set of the concept respectively.
The encyclopedia entity discrimination (EED) aims at discrimi-nating homographic entities. We mainly use the encyclopedia arti-cles to tackle the task. If two articles have very similar content, they are likely to refer to the same meaning for the following reasons. i) The introduction of the article should contain some basic infor-mation (e.g., when introducing the Apple company, it is necessary to point out that it is an American corporation and sells consumer electronics, computers, etc.). ii) Due to the open way of editing and lack of strict copyright protection, editors sometimes borrow sen-tences or even paragraphs from other encyclopedias during editing.
However, similarity measurement based on bag-of-words or n-gram model may misdirect the identification results when the di-vergent part is larger than the consistent part between the articles. A solution to this problem is to measure the similarity between two articles based on the common contiguous sequence (CCS) in them:
Definition 1 : CCS -Considering two text fragments represented as w ij denotes the j -th word in t i ( i =1 or 2) . Sequence h w w 1 ,p + l i (1  X  p  X  m  X  l and l  X  0) is a CCS of t 1 and t exists q (1  X  q  X  n  X  l ) , such that w 1 ,p = w 2 ,q , w ...,
Definition 2 : closed CCS -Following the notations used in Defi-of t 1 and t 2 if there does not exist any other CCS 2 = h w 1 ,o ..., w 1 ,o + r i ( r &gt; l ) in which we can find an integer
Given two articles, we make use of closed CCS to measure the similarity between them. To eliminate very short CCSs, a length threshold  X  L is set to select the qualified closed CCSs. The lengths of the qualified closed CCSs are denoted as l 1 , l 2 , ..., and propose some constraints which should be satisfied: Constraint a ) is designed to guarantee that texts with more closed CCSs are more similar. Constraint b ) is based on the insight that a single contiguous sequence is a more convincing evidence to re-veal high similarity than the discretely distributed ones with the same length in summation. Constraint c ) ensures that CCSs with stronger contiguity or larger length in summation contribute more to the similarity. The following formula can be proved fulfilling all the three constraints:
If entity e 1 and e 2 have the same name and the value of tion of e 1 .Intro and e 2 .Intro is larger than a threshold e will be identified as an identical entity. If they are not recognized as being identical by the CCS-based method, related words and cat-egory information will be utilized to give guidance for further de-cision. Different from the text of the detailed introduction, related words and categories are more elaborately selected and more se-mantically coherent with the entity. If the proportion of the overlap to the total number of categories and related words is higher than a threshold  X  rc , e 1 and e 2 will be treated as an identical entity.
To further leverage the semantic information, it is necessary to assemble entities which are synonymous but with different names together. Instead of randomly comparing two entities and check-ing whether they refer to the same meaning, we automatically ex-tract synonyms of an entity and collect entities having overlaps in their synonym set as candidates. Primarily, redirection information can provide important hints. If two entities have a redirection link between them, they are confirmed to be synonymous. A supple-mentary step is to extract alternative names from the introduction of the entities by using patterns such as  X  A is known as B  X . Then entities sharing a common alternative name are further checked by the method used for EED.

The procedure of the encyclopedia integration is summarized as follows. There are three major steps in the main routine, namely, alternative name extraction, redirected entity merging and entity discrimination. The first two steps are straightforward as discussed above. In the third step, the discrimination operation is performed for each pair of entities if they have not been disambiguated in any encyclopedia yet and have a common name.
In general, encyclopedic information can be viewed as a rich knowledge resource. For some applications such as the search en-gine, bringing in all the knowledge is helpful. However, for some other applications which is sensitive to the size of the ontology, it is necessary to keep balance between the vocabulary coverage and the practicability of the ontology. To make the new ontology more adaptive, we distill the encyclopedic information by casting the problem as an entity ranking problem. Entities ranked higher will be more likely to be functional knowledge and lower-ranked entities tend to be more narrowly-used or noise.
Two types of Learning to Rank model are employed to solve the entity ranking problem in this paper. One is pairwise approach and the other is pointwise approach. SVM-Rank is employed to con-duct the pairwise approach in our framework. It solves the ranking problem with Support Vector Machine (SVM) approach [10]. Par-tial orders should be given to each pair of the training examples and the ranking order of the rest data will be automatically determined by SVM-Rank. Different from pairwise approach, pointwise ap-proach reduces the ranking problem to a regression or classification problem on single elements. Linear Regression based on formulae proposed by [26, 25] is carried out to implement the pointwise ap-proach. It estimates the target variable as the value of an affine function of one or more explanatory variables.

Entities are represented by vectors in a space which takes fea-tures reflecting relevant characteristics of the entities as dimensions. Given manually rated examples, the rest entities will be automati-cally ranked by the trained models from these examples. We employ five features to represent entities in both methods. Among them, web popularity (WP) and TrustRank (TR) are fea-tures to capture the importance of the entity. Edit quality (EQ), edit times (ET) and article length (AL) are intended to reveal the quality of the article. Web popularity is measured by the number of results returned by a search engine when setting the entity name as a query. Edit times is how many times the article is edited and article length equals to the number of words in the introduction. The collection of the above three features is straightforward and the details are omitted due to space limitation. More details about TrustRank and edit quality are given as follows.
We employ the TrustRank model [5] to separate the valuable ar-ticles introducing well-known entities from the less important ones introducing little-known entities. Primarily, inverse PageRank of each article is calculated to select seed articles and a certain num-ber, say K , of articles with high inverse PageRank scores are se-lected as seeds and manually marked as: a ( 1  X  i  X  K ) is a selected article and d is an N -dimension vector. For other articles which are not selected as seeds, their cor-responding values in vector d will be set to be  X  d ( 0 &lt;  X  The vector d is normalized as d = d / | d | and the TrustRank vector t is initialized as d in the next step.

The TrustRank scores are finally computed by iterating the fol-lowing formula: where  X  is a dampening factor and T is a transition matrix. t /B j , where t ij is the number of links from the j -th article to the i -th article, and B j is the number of outlinks of the j -th article.
In general, the more experienced an editor is, the more probable it is that the edited articles are of good quality. And the more high-quality articles an editor have edited, the more skilled the editor tends to be. Taking advantage of this mutual reinforcement rela-tionship, we can involve the HITS algorithm [13] to estimate the quality of articles. A formal definition is given as follows: q is the quality of the i -th article and a j is the editing ability of the j -th editor. M ij means how many times the j -th editor has edited the i -th article. After some times of iteration, the vector will converge. Then, vector q gives the edit quality of the articles.
The online encyclopedias are rich in semantic relations, but the relations are neither labeled nor arranged. In WordNet, words are packed into synsets and various relations are hold between them, among which hyponymy is one of the most universal relations. Based on hyponymy relation between concepts, a tree structure of the ontology can be built. To make better use of the existing struc-ture of a WordNet-like ontology, we reorganize the encyclopedic information by aligning the two kinds of semantic knowledge re-sources through finding the hypernym-style categories of entities.
Generally, categories are sometimes likely to be the hypernym of the entity because  X  A belongs to category B  X  implies that A is a kind of B . However, some categories are not limited to the hyponymy relation but tend to be a kind of tag information.
 A special section named  X  X elated words X  is contained by most Chinese encyclopedias. These words are elaborately selected by editors and are closely related to the entity. For example, related words for individuals are usually people who work in the similar fields or have personal relations with the target entity. We make use of this special resource to find hypernym-style categories and propose an algorithm based on several observations: i) hypernym-style categories tend to be in the same branch in the hyponymy tree because they are represented as ancestor nodes of the target en-tity, while the tag-style categories are likely to be in other branches which do not contain the target entity as a family member; ii) the related words of the target entity are more semantically related to the hypernym-style categories than the tag-style categories because they tend to be located in the neighborhood of the target entity; iii) reasonable hypernym-style categories are more likely to be recom-mended in different encyclopedias than tag-style categories.
Given an entity e , we first find the lowest super-ordinate of its categories in the hyponymy tree and denote it as lso semantic relatedness between the related words and every branch whose root is one of the child nodes of lso is calculated as: SR ( r, e.Rel ) = where r is the root of the branch and  X  is set of nodes which are in the branch (including r and its descendants) and contain any element of the category set of e (i.e., e.Ctg ). e.Rel is the set of related words of e and N  X  is how many times category  X  appears in different encyclopedias. We use log ( N  X  + 1) instead of confine the influence of N  X  . Since sim ( w,  X  ) (it will be introduced later) is less than 1, the SR value will be dominated by N is used directly. Hence the log form is taken and the logarithmic base is set to be the number of encyclopedias used. sim ( w,  X  ) the semantic similarity between w and  X  and is defined as: sim ( w,  X  ) =  X   X   X   X   X   X   X  where  X  is the hyponymy tree. len ( w,  X  ) is the length of the short-est path between w and  X  in  X  . depth ( c ) is the depth of equals to the length of the path from c to the root of  X  . The intuition for this formula is: 1) the shorter the path between the concepts is, the more related the concepts are; 2) concepts with a deeper depth seem to be more closely related than the shallower ones which have the same length of shortest path between them [9].

The sub-branch with the highest SR score is selected and the search scope is narrowed to this branch. The above procedure is repeated until the branch contains no element of the category set except the root.

In this way, encyclopedia entities will be mapped to the WordNet-like ontology and a new ontology named JNet is constructed . JNet is generated from the entities integrated from more than one en-cyclopedia and different versions of JNet can be built by adding different proportion of highly ranked entities. The framework for construction of JNet is summarized in Algorithm 1.

Algorithm 1: Framework of construction of JNet In our experiments, we build several different versions of JNet in Chinese. Chinese online encyclopedias have a booming develop-ment in recent years. Some of them have comparable scale of En-glish Wikipedia and are far beyond Chinese Wikipedia. Two largest Nov. 30th 2011, containing 3,819,124 and 1,190,528 articles of Baidu Baike and Hudong Baike respectively. In the ontology merg-ing stage, the WordNet-like ontology employed is Chinese Concept Dictionary (CCD) [28, 14]. We use the version of CCD updated in June 2009, which is the latest version. It contains 99,642 concepts with 142,913 different word forms.
As mentioned in Section 2.2, the target of encyclopedia entity discrimination (EED) is to distinguish two classes correctly, namely entities with the same name but referring to different meanings (ho-mographs) and entities having the same name as well as the same meaning (identical entity). Basically, we can use F-measure to evaluate the results. There are two types of F-measure score which are widely used, i.e., Macro-average F-measure score ( F ma ) and Micro-average F-measure score ( F mi ).

Macro-average F-measure score : Primarily, the F-measure for class c i is calculated as F i = 2 P i R i and recall for each class c i respectively. Then the macro-average F-measure is F ma = P i F i
Micro-average F-measure score : The global precision and recall are defined as: P = P i T P i  X  L and  X  f (  X  rc = 0 . 17 ).
 F P i and F N i are true positive, false positive and false negative number for class c i . The Micro-average F-measure is F mi = 2 P R
Given that identical entities outnumber homographs by a wide margin in practice, we adopt the Macro-average F-measure score (
F
For encyclopedia entity discrimination, the three parameters men-tioned in Section 2, namely  X  L ,  X  f and  X  rc , are determined by op-timizing the performance on a random sample of 500 article-pairs. Each pair is composed of two articles with the same title. Gener-ally, the performance of  X  rc is comparatively stable. The effects of  X 
L and  X  f are much greater than that of  X  rc and the results with combination of different  X  L and  X  f are shown in Figure 1. The ex-perimental results show that the best performance is achieved when  X  ,  X  f and  X  rc are set to be 3, 7 and 0.17 respectively.
For synonym detection, the patterns utilized for synonym extrac-tion are shown in Table 1.

In total, 121,602 entity pairs from the intersection of the two encyclopedias are identified as identical entities and the rest 4,855 words are marked as homographs. To check whether the selected parameters also work for the rest part of the data, we randomly sample another 500 article-pairs. A manual check finds 8 of them are homographs and the remaining 492 pairs are identical entities. Our method classifies 21 of them to be homographs and the rest 479 of them as identical entities, among which 7 of the homographs and 478 of the identical entities are correctly classified, leading to an For synonym detection, we find 77,986 redirection links in total. To ensure that the automatic extraction of alternative names brings as less noise as possible, the patterns are used very strictly. Finally, 446 synonym sets are automatically extracted from the articles and 423 of them are correct, which leads to a precision of 94.84%.
The quality of automatic merging is affected by the quality of the articles. An entity may be mapped to an improper position in JNet if most of its categories given by the editors are not closely related to it. To evaluate the performance of the mapping algorithm for entities at different ranking positions, the integrated entities are divided into four bins and the mapping results of an example set in each bin are manually checked.
The first bin (Bin1) is composed of the top 20% entities and the second bin (Bin2) includes entities ranked from 20% to 50%. Enti-ties ranked from 50% to 80% constitute the third bin (Bin3) and the bottom 20% entities are put into the fourth bin (Bin4). 150 map-ping cases are randomly sampled from each bin. Based on the two different ranking orders given by the Learning to Rank methods, we totally sample 1,200 cases.

The method proposed by Yamada and Torisawa [27] is also im-plemented. Since neither Baidu Baike nor Hudong Baike provides tem, the categories are used as the source for hypernym acquisi-tion. Among the hypernym candidates, the one which generates the highest SVM score is regarded as the final winner. Besides, stemming is not very useful for dealing with Chinese encyclope-dias so suffix is not used as a feature. Yamada X  X  method achieve a precision of 73.25%. Table 2 shows the precision of mapping for the samples from each bin.

Highly-ranked entities are mapped more accurately because they are generally better edited and the category information can provide better guidance for the mapping. For some entities which are not rated highly, the related words and category information are some-times quite limited. We manually check some uncorrect cases in Bin4 and find that the reason for misjudgement of the hypernym-style category is that all the categories are tag-style.
Yamada X  X  method works very well in Japanese Wikipedia while it is not so effective in the Chinese encyclopedias used in this pa-per. One main reason is that the tool for hypernym acquisition [21] used in Yamada X  X  method is not applied to the Chinese encyclope-dias used in this paper since they do not provide the required semi-structured data of the articles. As reported by Yamada and Torisawa [27], the tool can achieve a precision of 90%. The inapplicability of this powerful tool may be responsible for the underperformance of Yamada X  X  method in Chinese encyclopedias.

As discussed above, the ranking of entities affects the mapping accuracy. To evaluate the ranking results more directly, the match-ing degree of the ranking given by the Leaning to Rank methods and the ranking given by human is investigated. We invite volun-teers to give manual ratings of a random sample of 200 examples from the integrated entities. Each entity is given a numerical score out of 100 and the criteria are the same as those in the scoring of training samples. SVM-Rank achieves an NDCG value of 0.872 and Linear Regression achieves a value of 0.873.
Besides the utility in Natural Language Processing (NLP), se-mantic knowledge repositories are also widely used in other fields like information retrieval and text mining. Considering that text mining tasks can involve huge amount of data which will challenge the knowledge coverage of an ontology, we employ different ver-sions of JNet in some text mining tasks to see whether the refined ontologies can retain valuable information with a smaller size.
We construct four different versions of JNet. The proportion in Algorithm 1 is set to be 20%, 50%, 80% and 100%, and the cor-responding JNets constructed are denoted as JNet1, JNet2, JNet3 and JNet4 respectively.

Two ontology-based feature selection methods are adopted to conduct the experiment, namely, the method proposed by Hotho et al. [7] and TCRL [9]. Hotho X  X  method reflects a synset with a fix number of levels of its hypernyms as one dimension. TCRL re-structures the feature space more dynamically. Leaf nodes will be merged to their parent nodes in the hyponymy tree by TCRL recur-sively, on condition that the parent nodes have a stronger ability of representing the documents.
 In our experiments, we use a document collection provided by Sohu, one of the most famous Internet corporations in China. It in Dec. 2009. The documents are manually categorized into 13 pre-defined classes. We use five classes and randomly select 1000 documents from each class. We conduct experiments on text clas-sification and clustering with SVM (LibSVM [3] is employed) and K-Means algorithms (Cluto [12] is employed) respectively. For clustering, purity [29] is employed as the evaluation criterion. For classification, Macro-average F-measure score ( F ma ) and Micro-average F-measure score ( F mi ) are calculated respectively.
The results are shown in Figure 2.  X  X L X  denotes the baseline which uses the original data without feature selection.  X  X VM-Hotho X  means the results are obtained by using feature selection method proposed by Hotho et al. and employing JNets with en-tity distilling based on SVM-Rank.  X  X R-TCRL X  uses TCRL and Linear Regression, and so forth. Both Hotho X  X  method and TCRL outperform the baseline, which indicates that borrowing semantic information from the ontology will benefit text mining. All ver-sions of JNet perform better than CCD because JNet imports more semantic information from the online encyclopedias. The refined versions of JNet (i.e., JNet1, JNet2 and JNet3) achieve a competi-tive performance of a coarse version that keeps all the entities with-out distilling (JNet4) in classification. Moreover, some refined ver-sions of JNet perform outstandingly in clustering. JNet2 performs best in clustering. The best result achieved by JNet2 is 0.822, while the best result achieved by JNet4 is 0.784. The above facts imply that the downsizing of information does not hurt the quality of the ontology and the refined ontologies may even benefit the practical application because they contain less noise information.
As the most successful online encyclopedia, Wikipedia has at-tracted much attention from researchers. Abundant semantic rela-tions can be extracted form both the raw text and the structured data of Wikipedia. There are proposed methods based on link analysis between Wikipedia categories [4], using sentence analyzer [6] and taking learning algorithm [23]. Besides, Wikipedia is also used as a rich knowledge bank to enhance other ontology such as WordNet (e.g. [19], [18] and [27]) or enable weakly-supervised learning with automatically generated training examples [2]. Large-scale ontology can be built by restructuring Wikipedia information. Clas-sical works include DBpedia [1] and Yago [20], etc. Yago [20] ex-tracts several dozens of relations from Wikipedia. Yago exploits the redirection system of Wikipedia to extract alternative name of entities and build  X  means  X  relation. Yago also extracts  X  subClas-sOf  X  from the category structure. However,  X  subClassOf  X  relation is mainly used to connect the leaf categories with WordNet in Yago, while our method focuses on finding the most suitable hypernym-style categories for all entities. The most notable difference be-tween Yago and our work is that Yago only focuses on Wikipedia while ours integrates various encyclopedia sources and distills the encyclopedic information.

Wikipedia is also used as a bridge cross difference languages [15, 16].As a multi-language semantic knowledge repository with con-venient online access, Wikipedia is very suitable for the challeng-ing cross-language tasks. However, there are only a few works focusing on Chinese Wikipedia [24] and automatically building se-mantic knowledge database from online Chinese encyclopedias [17].
As for entity ranking, previous works mainly focus on better an-swering a query [22, 11], while our work aims at distilling out more useful and important information from the view of lexical resource construction. For the assessment of article quality of Wikipedia, Hu et al. [8] propose a model making use of interaction between articles and editors acquired from the edit history. Their work takes the words in the articles as the basic units while ours takes the en-tire article because the encyclopedias we use do not provide such detailed information in the edit history.
In this paper, we propose a method to make better use of the in-formation obtained from different encyclopedias and further align the encyclopedic information with a WordNet-like ontology to con-struct an enhanced semantic knowledge repository named as JNet. By carefully filtering out the low-quality and less important enti-ties, we can get refined versions of JNet. The experimental results show that the downsizing of the refined version does not hurt the quality of JNet. Furthermore, the refined versions become more practical than the coarse version which retains all the entities from the encyclopedias. To the best of our knowledge, this is the first work about integrating different online encyclopedias to construct an enhanced ontology from the distilled encyclopedic information in Chinese. In general, the method proposed in this paper is not limited to Chinese. However, some technical details may have to be adjusted since Chinese language has its own characteristics.
