 This paper introduces the first generic version of data depen-dent dissimilarity and shows that it provides a better closest match than distance measures for three existing algorithms in clustering, anomaly detection and multi-label classifica-tion. For each algorithm, we show that by simply replacing the distance measure with the data dependent dissimilari-ty measure, it overcomes a key weakness of the otherwise unchanged algorithm.
 Data dependent dissimilarity; distance measure; distance-based neighbourhood; probability-mass-based neighbourhood; k nearest neighbours.
Many data mining algorithms rely on a distance measure to provide the closest match between a test instance and in-stances in a database in order to find its nearest neighbours. The distance calculation is the core process that has been applied to all aspects of data mining tasks, including density estimation, clustering, anomaly detection and classification.
Despite its widespread applications, research in psychol-ogy has pointed out since 1970 X  X  that distance measures do not possess the key property of dissimilarity as judged by humans [12, 20], i.e., the characteristic where two instances in a dense region are less similar to each other than two in-stances of the same interpoint distance in a sparse region. Researchers have suggested that this data dependent dissim-ilarity is a better measure than the data independent geo-metric model based distance measure in psychological tests [12]. For example, two Caucasians will be judged as less similar when compared in Europe (where there are many Caucasians) than in Asia (where there are few Caucasians and many Asians.)
We introduce a data dependent dissimilarity which has the above-mentioned characteristic, and we provide concrete evidence that it is a better measure than standard distance measures for three existing algorithms which rely on dis-tance. Using probability mass rather than distance as the means to find the closest match neighbourhood heralds a fundamental change of perspective.

The neighbourhood of an instance has been used for dif-ferent functions in various data mining tasks. Table 1 shows the key functions and key weaknesses of three existing algo-rithms relying on distance measure. It is instructive to see that a mere replacement of distance with the data dependen-t dissimilarity in these algorithms changes the perspective. The corresponding  X  X ew X  functions are described in the last column of Table 1. Although the rest of the procedures in each algorithm are unchanged, it overcomes the key weak-nesses of these algorithms.

This paper makes the following contributions: 1. A generic data dependent dissimilarity, named mass-2. Deriving a new neighbourhood function from the data 3. Through our empirical evaluation, we demonstrate that
The remainder of the paper is organised as follows. Sec-tion 2 introduces the related work. Sections 3 and 4 present the proposed dissimilarity and the new neighbourhood func-tion. Section 5 describes an existing similarity which is a special case of the proposed dissimilarity. Section 6 intro-duces how the dissimilarity can be applied to three exist-ing algorithms. Section 7 presents the empirical evaluation results. A discussion of related issues and conclusions are provided in the last two sections.  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  Algorithm Key function Key weakness Replacement function
Density-based clustering kNN anomaly detector
Multi-label kNN classifier (MLkNN)
Psychologists since 1970 X  X  have expressed their concern-s on the use of geometric model for dissimilarity measure [12, 20]. The psychological tests they have conducted have clearly shown that the dissimilarity between two instances, as judged by humans, is influenced by the context of mea-surements and other instances in proximity. It is suggested that a dissimilarity measure which is akin to human X  X  judged dissimilarity is one that interprets two instances in a dense region to be less similar than two instances of equal inter-point distance but located in a less dense region [12].
The first version of data dependent similarity [2], which has the characteristic prescribed by psychologists above, has been shown to provide a more effective match than distance measures in nearest neighbour search for k nearest neigh-bour classifiers and information retrieval. It is named m dissimilarity and defined in the same form as the ` p -norm, except that the dissimilarity in dimension i is probability mass in a region P ( R i ( x,y )) rather than distance | x Region R i ( x,y ) is defined as an interval from min ( x to max ( x i ,y i )+  X  , where  X  is some small constant value.This implementation requires a search to find all instances in the region, and is not efficient.

This paper presents a general definition of data depen-dent dissimilarity in which m p -dissimilarity [2] is a special case. The proposed implementation is efficient and robust for more applications.

A recent definition of mass estimation [7] is the basis of the proposed dissimilarity. We effectively extend from the mass estimation of one point to a dissimilarity measure of two points.

Based on information theory, Lin [14] suggests a proba-bilistic measure of similarity in ordinal domain as follows: where o j is an ordinal random variable.

Like most distance measures, Lin X  X  measure [14] yields a constant maximum value for self similarity; and it has the opposite characteristic of the judged dissimilarity, i.e., two instances are more similar in a dense region than two instances of equal interpoint distance in a sparse region.
Another existing data dependent dissimilarity is shared nearest neighbour ( SNN ) which has been investigated in clustering only thus far [9, 11]. A detailed discussion of the relationship between SNN and the proposed mass-based dissimilarity is devoted in Section 5.
Geometric model based measures solely depend on geo-metric positions to derive their distance measures. Instead, a data dependent dissimilarity mainly depends on data dis-tribution, i.e., the probability mass of the (smallest) region covering the two instances. We use the terms: probability mass or mass or probability, interchangeably hereafter; and name the proposed measure: mass-based dissimilarity .
Let D be a data sample from pdf (probability density function) F ; and H  X  H ( D ) be a hierarchical partitioning model of the space into non-overlapping and non-empty re-gions. The definitions for the domain of R d , where d is the number of dimensions, are given as follows.

Definition 1. R ( x,y | H ; D ) is the smallest local region cov-ering x and y wrt H and D is defined as: where 1 (  X  ) is an indicator function.

Definition 2. Mass-based dissimilarity of x and y wrt D and F is defined as the expected probability of R ( x,y | H ; D ): where P F (  X  ) is the probability wrt F ; and the expectation is taken over all models in H ( D ).

In practice, the mass-based dissimilarity would be esti-mated from a finite number of models H i  X  H ( D ) ,i = 1 ,...,t as follows: where  X  P ( R ) = 1 | D | P z  X  D 1 ( z  X  R ).

Note that R ( x,y | H ; D ) is the smallest local region cover-ing x and y , it is analogous to the shortest distance between x and y used in the geometric model. Hereafter D is dropped in the notations when the context is clear.
One key difference of the mass-based dissimilarity is self-dissimilarity. Unlike self-dissimilarity of other measures (which usually take a constant value equal to the minimum dissim-ilarity or 0 in [0,1]), m e ( x,x ) is not a constant and ranges over [0,1], depending on the data distribution and the par-titioning strategy used. Table 2: m e versus ` p .  X  (  X  ,  X  ) is a dissimilarity function. m e [0,1] [0,1]  X  x 6 = y,m e ( x,x )  X  m e ( x,y ) ` p 0 (0,1]  X  x ; y 6 = z,` p ( x,x ) &lt; ` p ( z,y ) (a) True density distribution (b) Distribution of m e ( x,x | D ) Figure 1: (a) A true density distribution; (b) m e ( x,x | D ) based on Random Trees (level=8), where D has 1500 instances randomly sampled from (a) The differences between m e and ` p are shown in Table 2. The reasons of the two properties of m e are given below:
The distribution of the self-dissimilarity is equivalent to mass distribution [18], and its properties depend on the mod-els H used:
Though there are many methods to implement a mod-el to define regions for mass estimation [19], we employ a method based on completely random trees 2 to implement mass-based dissimilarity in this paper.

We use a recursive partitioning scheme called iForest (iso-lation Forest) [15] to define regions. Though iForest was ini-tially designed for anomaly detection [15], it has been shown that it is a special case of mass estimation [19].
Note that mass distribution is not uniform even for a uni-formly distributed pdf. See [7, 18] for details.
Note that the random trees are not RandomForests [5] be-cause the trees are completely random, built without class labels and any attribute selection criterion.

The implementation can be divided into two steps. There are two input parameters to this procedure. They are: t -the number of iTrees (isolation Trees); and  X  -the sub-sampling size used to build each iTree . The height limit h of each iTree is automatically set by  X  : h = d log 2  X  e .
The first step is to build an iForest consisting of t iTrees as the partitioning structure R . Each iTree is built inde-pendently using a subset D  X  D , where |D| =  X  . A ran-domly selected split is employed at each internal node of an iTree to partition the sample set at the node into two non-empty subsets until every point is isolated or the maximum tree height h is reached. In the experiments described in Section 7, we perform axis-parallel split at each node of an iTree to build iForest . The details of the axis-parallel split iTree building process can be found in the Appendix.
After an iForest is built, all instances in D are traversed through each tree in order to record the mass of each node.
The second step is the evaluation step. Test points x and y are parsed through each iTree to calculate the sum of mass of the lowest nodes containing both x and y , i.e., P i | R ( x,y | H i ) | . Finally, m e ( x,y ) is the mean of these mass values over t iTrees as defined below:
We introduce a new function:  X  -neighbourhood mass, which denotes the number of points in a region defined by the mass-based dissimilarity, is defined as follows:
Like -neighbourhood density 3 ,  X  -neighbourhood mass pro-duces an estimate based on a region defined by a dissimilar-ity measure. However, the region is defined by the expected probability mass (instead of distance.) Like , the parame-ter  X  controls the size of the region: large (small)  X  defines a large (small) region.

Figures 2(a) and 2(b) compare -neighbourhood with  X  -neighbourhood on a dataset having three areas of different densities. Note that the volume of the region defined by  X  -neighbourhood mass depends on data distribution X  X mall in dense area and large in sparse area, shown as areas A, B and C in Figure 2(b). Note that the overall shape is not sym-metrical. In contrast, -neighbourhood forms a region which is independent of data distribution with constant volume, in addition to regular and symmetrical shape.

Figure 3(a) shows that a  X  -neighbourhood region becomes symmetrical only in the case of a uniform density distribu-tion. Figure 3 shows that the shape depends on the imple-mentation: axis-parallel and non axis-parallel random trees yield diamond and spherical shapes, respectively.

A conceptual comparison between the two neighbourhood functions is given in Table 3. In order to obtain non-zero M  X  must be set higher than max z  X  D m e ( z,z ). In other words,  X  -neighbourhood mass employs mass distribution (i.e., the distribution of self-dissimilarity) as the reference.
For the purpose of clustering, mass can be used in a similar way as density to identify core points, i.e., instances having high mass values are core points; and those having low mass values are noise.
As used in DBSCAN [10]. SNN ( k = 200 and  X  = 0 . 3 ) from a sample of 1500 instances of (a). (a) -neighbourhood region (b)  X  -neighbourhood region Figure 2: (a) and (b) show the two regions de-fined by -neighbourhood density ( = 0 . 25 ) and  X  -neighbourhood mass (  X  = 0 . 55 ), respectively, on a dataset having three areas of different densities, with reference to the red point (0.5,0.5). The blue-coloured dots denotes the points and the dark-coloured dots denotes the points within the region defined by the -or  X  -neighbourhood estimator. (a)  X  -neighbourhood region-A (b)  X  -neighbourhood region-B Figure 3: (a) and (b) show the two regions de-fined by  X  -neighbourhood mass (  X  = 0 . 55 ) using axis-parallel iForest and non axis-parallel iForest , respec-tively, on a dataset with uniform density distribu-tion with reference to the red point (0.5,0.5).

The next subsection describes a characteristic of  X  -neigh-bourhood mass which makes it a better candidate than -neighbourhood density to identify core points, especially in a dataset which has clusters of varying densities.
One interesting characteristic of  X  -neighbourhood mass is that valleys of a data distribution can be constrained within a small range of mass values by setting an appropriate  X  , where a valley is the area surrounding a local minimum in the distribution. This characteristic is especially important in clustering algorithms which rely on a global threshold to identify core points before grouping the core points into separate clusters. Having all the valleys close to a small mass value, a global threshold slightly larger than this value will identify the majority of the core points of all clusters, irrespective of the densities of the clusters.

In contrast, -neighbourhood density does not possess this characteristic as it estimates density distribution and its val-leys can have huge varying densities. As a result, a clus-tering algorithm, such as DBSCAN [10] which employs the -neighbourhood density estimator and relies on a global threshold to identify core points, is unable to detect all clus-ters of varying densities. This kind of distribution, shown in Figure 4(a), is called hard distribution because it is hard for DBSCAN [10] to identify all the clusters in the distribution.
Possessing the above-mentioned characteristic, the hard distribution (in terms of density) will exhibit as easy dis-tribution (in terms of mass). Examples of -neighbourhood density and  X  -neighbourhood mass are given in Figures 4(b) and 4(c), respectively.

In summary,  X  -neighbourhood mass converts all valleys of different densities to become valleys of almost equal low mass by using an appropriate  X  , if it exists.

The same applies to peaks, i.e., the difference in mass between peaks at dense and sparse regions can be reduced to a smaller value close to zero with an appropriate  X  . However, in the clustering context, it is more important to have valleys reduced to about the same value so that a global threshold can be used to easily identify all clusters.
A measure based on shared nearest neighbours ( SNN ) in k nearest neighbours has been proposed for clustering [11]:
SNN (dis)similarity has been used to replace the distance measure in DBSCAN as a way to overcome its inability to find all clusters of varying densities [9].

Let s k ( x,y ) = SNN ( x,y ) /k , where SNN ( x,y ) is the number of shared nearest neighbours of k nearest neighbours of x and y , which include both x and y ; SNN ( x,y ) = 0 if both x and y are not included.

The neighbourhood function based on the SNN similarity can be expressed as:
Note that M  X  ( x ), like M  X  ( x ), cannot be treated as den-sities as the volume used to compute M  X  for every x is not a constant. In other words, we reveal that SNN clustering algorithm is a mass-based method, and not a density-based method as previously thought [9, 17]. This is despite the fact that SNN employs the same DBSCAN procedure by simply replacing the distance measure with the (inverse) SNN sim-ilarity [9, 17].

Using the same notation, let R ( x,y | H ) be the (implicit) region which covers the shared nearest neighbours of x and y , where H is kNN. SNN ( x,y ) can be defined as follows: An example of M  X  ( x ) due to SNN is given in Figure 4(d).
The advantages of using iForest instead of k nearest neigh-bour to estimate the neighbourhood mass are:
Here we show that the mass-based dissimilarity can sim-ply replace the distance measure used in existing algorithms in three tasks: clustering, anomaly detection, and multi-label classification. The pertinent details are described in the following three subsections.
DBSCAN [10] is a natural choice not only because it is a commonly used clustering algorithm, but also it employs -neighbourhood density estimation. Here we convert DB-SCAN to MBSCAN, i.e., from density based to mass based, by simply replacing distance measure with mass-based dis-similarity, leaving the rest of the procedure unchanged. This effectively changes the use of -neighbourhood density esti-mation to  X  -neighbourhood mass estimation, as described in Section 4. This enables a global threshold to be used to identify all clusters of varying densities in hard distribution as shown in Figure 4.
Here we use one of the simplest anomaly detector which is based on k th nearest distance [4]. The anomaly score for a test instance x is defined as the distance between x and its k th nearest neighbour. Anomalies are instances which have the largest scores, i.e., they have the longest distances to their k th nearest neighbours in a given data set.
This distance-based anomaly detector is used to show that a simple replacement of the distance measure with the mass-based dissimilarity can overcome its inability to detect local anomalies and improve its overall detection performance.
The replacement of ` p with m e effectively converts the en-tire operation from distance-based to mass-based. Anoma-lies are redefined as those which have the highest probability mass to their k th lowest probability mass neighbours in a given data set.

The advantage of using mass is that fringes of any clus-ters with the same structure but different densities will have about the same probability mass. This enables (local) anoma-lies of dense clusters to be ranked at similar positions as anomalies of sparse clusters. The mass-based version is de-noted as M-kNN 4 to contrast it with kNN.
In multi-label classification tasks, each instance is associ-ated with several class labels simultaneously. MLkNN [22] has been proposed to adapt the (single-label) kNN approach to multi-label problems, which makes a prediction via the maximum a posteriori rule. The rule is defined as follows.
In the context of multi-label learning, let y i,j  X  { 1 , 0 } be the j -th label of the i -th instance, where 1 and 0 indicate the instance X  X  association and dissociation, respectively. The prediction rule of the j -th label for x i is given by [22]: where c i,j denotes the number of instances in the neigh-bourhood (i.e., k nearest neighbours) of x i with the j th-label, P ( y i,j = 1 | c i,j ) is the posterior probability of y given that x i has c i,j neighbours with the j th-label; and P ( y i,j = 0 | c i,j ) is similarly defined.

Bayes theorem yields that P ( y i,j = 1 | c i,j )  X  P 1) x i has c i,j neighbours when y i,j = 1. Both P ( y i,j and P ( c i,j | y i,j = 1) can be estimated via frequency count-ing on the training set. In the same manner, we can obtain P ( y i,j = 0 | c i,j ) via P ( y i,j = 0) and P ( c i,j | y
Though there is a fundamental change of perspective, we have opted to reuse the same name of the original algorithm to emphasize that the same algorithm is employed with the exception of dissimilarity measure only.
Getting an effective closest match neighbourhood of a test instance is of prime importance in MLkNN, which directly influences the calculation of the likelihood. Thus, the dissim-ilarity measure employed plays a critical role in this process. In the original MLkNN, Euclidean distance is applied. The use of mass-based dissimilarity has the potential to enhance the nearest neighbour search and leads to a better multi-label classification result. We denote the version employing mass-based dissimilarity, M-MLkNN. We evaluated the mass-based version of DBSCAN, named MBSCAN, and compared it with DBSCAN, SNN [9] and OPTICS [1]. Note that the only difference among DBSCAN, SNN and MBSCAN is the dissimilarity matrix, which is pre-processed and serves as input to these algorithms. We used iForest with the default setting (i.e.,  X  = 256 and t = 100) [15] to generate the mass-based dissimilarity matrix as the input for MBSCAN.

Th evaluation is conducted on 2 synthetic and 8 real-world datasets from UCI Machine Learning Repository [13]. Ta-ble 4 presents the properties of the datasets.

S1 and S2 are synthetic datasets. S1 is a  X  X ard distribu-tion X  which contains 3 Gaussian clusters N ( mean,std ) with means located at ( x (1) ,x (2) )=(3.3, 9.3), (8, 5), (12, 12), and standard deviations std = 3 , 3 , 8 in each dimension; and each cluster has 300 instances. S2 is an  X  X asy distribution X  which has 3 Gaussian clusters N ( mean,std ) with means located in each dimension; and each cluster has 500 instances. The density plots of S1 and S2 are shown in Figure 5. (a) S1:  X  X ard distribution X  (b) S2:  X  X asy distribution X 
We recorded the best F-measure 5 of a clustering algorithm on a dataset. Because iForest is a randomised method, we reported the average result over 10 trials.

For each clustering algorithm, the search range of either ,  X  or  X  was from the minimum to the maximum value of pairwise dissimilarity in the given dataset. The search range of MinPts in DBSCAN, SNN and MBSCAN was in the range { 2 , 3 ,..., 10 } . The parameter k in SNN was set to the square root of the data size as suggested by some researchers [16]. For OPTICS, we searched MinPts to produce the required hierarchical plots, and then searched threshold  X  each plot.
 Figure 6 shows the best F-measure of DBSCAN, OPTICS, SNN and MBSCAN. The counts in the last column of the table reveal that MBSCAN and SNN performed the best in 5 and 3 datasets, respectively.

Table 5 shows the performance ratio of OPTICS, SNN, and MBSCAN with reference to DBSCAN. The geometric mean reveals that MBSCAN enhances DBSCAN the most by 27%; and SNN enhances DBSCAN by 21%.

Although MBSCAN and SNN have similar F-measures in many datasets, MBSCAN is significantly better than SNN in two datasets: S1 and WDBC. For example, MBSCAN enhances DBSCAN by more than 80% compared with less than 50% achieved by SNN on S1. Because S1 is a hard dis-tribution for DBSCAN, this shows that mass-based dissim-ilarity provides a much better solution than shared nearest neighbour similarity in finding clusters of varying densities. Except for S1, WDBC and Thyroid, other datasets appear to have a lesser degree of hard distribution since their en-hancements over DBSCAN are less than 40%.

The post-hoc Nemenyi test 7 reveals that both MBSCAN and SNN are significantly better than both DBSCAN and OPTICS. Figure 7 shows the average rank of each algorithm and its critical difference.
 Figure 7: Critical difference (CD) diagram of the post-hoc Nemenyi test (  X  = 0 . 05) . The difference between two algorithms is significant if their CDs do not overlap.
Given a clustering result, we calculate the precision score p and the recall score r i for each cluster based on the confusion matrix, and then the overall F-measure is the average over
Parameter  X  is used to identify downward and upward ar-eas of a hierarchical plot in order to extract clusters. This hierarchical extraction method was proposed in the original OPTICS paper [1].
This test (after the Friedman test) [8] is conducted to ex-amine whether the performance difference between any two algorithms is significant. Firstly, the algorithms were ranked on each dataset according to their F-measures, where the best one is rank 1. Then, the post-hoc Nemenyi test is used to calculate the critical difference value (CD) for each algo-rithm. on each dataset is underlined.
 In this section, we evaluate M-kNN in two experiments.
We first examined the ability of kNN and M-kNN to detect local anomalies on a synthetic dataset. The dataset contains one sparse cluster and one dense cluster, and each cluster has 500 instances. The instances at the fringes of the dense cluster are anomalies relative to this cluster only because they have higher densities than most instances in the sparse cluster; that is why they are called local anomalies. Figure 8: The ability to detect local anomalies in the dense cluster. Contour of the scores of k-nearest neigbour anomaly detectors using ` p and m e , with k = 100 on a synthetic dataset. The lighter the colour, the higher the anomaly score.
 Figure 8 shows the contour of anomaly scores on the dataset. As expected, kNN is unable to detect local anomalies. Yet, M-kNN is able to detect all instances at the fringes of the dense cluster as anomalies. Note that the fringes of both dense and sparse clusters have similar high scores, allowing all of them to be identified as anomalies.

In the second experiment, we compared the performance on 6 real-world benchmark datasets 8 . The data size, dimen-sions and percentage of anomalies are shown in Table 6. A state-of-the-art local anomaly detector named Local Outlier Factor (LOF) [6] is also used in the comparison.
Velocity is from http://openscience.us/repo/defect/ck/ and others are from UCI Machine Learning Repository [13]. Dataset Size Dimensions % Anomaly Velocity 229 20 34.06% Mfeat 410 649 2.44% BloodDonation 604 4 5.63%
Diabetes 768 8 34.9% annThyroid 7200 6 7.42% p53Mutant 10387 5408 0.51%
We search k from 10% to 50% of the data size and present the result in terms of AUC (Area Under ROC Curve.)
M-kNN was run over 10 trials using different random seeds in computing the dissimilarity matrix for each dataset. kNN yields a deterministic result for a given data set; thus, it is run once only for each data set.

Comparing their best AUC values in Figure 9, M-kNN performs equivalent to or better than kNN on every dataset. M-kNN always outperforms kNN on Velocity and annThy-roid with any k . On BloodDonation, M-kNN with a high k value performs better than kNN. Compared with LOF, M-KNN performs equally or better than kNN on 5 out of 6 datasets in terms of the highest AUC. Table 7: Properties of multi-label datasets. W is the average number of labels per instance.
 In this section, we compare the original distance-based MLkNN[22] with the mass-based version M-MLkNN. The best k is searched in the range: 1,3,5,10,15 via 5-fold cross-validation on the training set.

We evaluate their performance in terms of Hamming loss, ranking loss, coverage, one error and average precision on five datasets 9 , commonly used for multi-label classifier eval-uations. Table 7 shows the properties of these datasets.
The result presented in Table 8 shows that M-MLkNN is significantly better (by two standard errors) than MLkNN in four out of the five datasets in terms of all five performance measures. The improvement of M-MLkNN over MLkNN is noteworthy (range between 30% and 50%) on the Birds dataset in terms of the last four measures.
 Figure 10: The proportion of k nearest neighbours having a particular label on the Emotions dataset for different k . The left and right figures show the pro-portions for the majority and minority labels (i.e., the labels which have the largest and smallest num-ber of instances), respectively, on the dataset.

The improvement is due to the fact that mass-based dis-similarity provides a better closest match neighbourhood, where instances are more likely to hold the same label and
Source: http://mulan.sourceforge.net/datasets-mlc.html. the proportion of the majority label is larger. Figure 10 provides an insight into two of the labels individually. Each of the majority and minority labels (in the dataset) has a higher proportion in the k nearest neighbours found using mass-based dissimilarity than that using the distance mea-sure. In fact, this occurs for every label on this dataset.
Visualisation provides another perspective of the advan-tage, i.e., how far apart instances are from one another according to each of the two dissimilarity measures. Here we performed multidimensional scaling (MDS) 10 . The MDS plot based on ` 2 is shown in Figure 11(a); and the one based on m e is shown in Figure 11(b). It shows that instances of different labels are easier to separate using m e . Figure 11: MDS plots using distance matrix and mass-based dissimilarity matrix on Emotions. Green and red points represent the positive and neg-ative instances of the majority label, respectively.
The only difference between the ` p and m e versions of the algorithms in the above three subsections is the computation time for dissimilarity matrix. After the matrix is computed and served as input, the algorithm has the same runtime regardless of the dissimilarity used to compute the matrix.
Using ` p to compute the dissimilarity matrix has O ( dn 2 time complexity. m e builds iForest and computes the dis-similarity matrix based on iForest , which yields O ( t X log X  + n tlog X  ) time complexity. For large datasets,  X  n , the time cost is O ( n 2 ). The time complexity of SNN similarity is O ( n 3 ) in the worst case when k =
Multidimensional scaling is a technique for visualising the information contained in a dissimilarity matrix [21]. An MDS algorithm aims to place each data point in a p -dimensional space ( p = 2 is used here), while preserving as much as possible pairwise dissimilarities between them. gives the time and space complexities of dissimilarity matrix calculation based on ` p , m e and SNN .
 Table 9: Time and space complexities of dissimilar-ity matrix calculation based on ` p , m e and SNN . Table 10: Runtime of the dissimilarity matrix cal-culation for the three dissimilarities (in seconds). (Data size) (2310) (7200) (10992) (10387) (Dimension) (19) (6) (16) (5408)
Table 10 shows the runtime of the dissimilarity matrix calculation for the three dissimilarities on four real-world datasets. In small dimensional datasets, m e has almost the same runtime as SNN , but takes longer to compute than ` p due to the use of iForest . However, in large and high dimensional datasets such as p53Mutant, m e is much faster than both ` p and SNN because it is independent of the data dimensionality, i.e., each split node of a tree chooses one attribute randomly up to the certain height limit only.
Concepts . Dissimilarity measures are assumed to be a metric or a pseudo-metric as a necessary criterion for all data mining tasks. This work shows for the first time that this assumption is incorrect in three tasks.

We show that distance measures are the source of key weaknesses in three existing algorithms, highlighted in Ta-ble 1. Having recognised the source issue and created an effective alternative to distance measure, the solution be-comes simple X  X erely replacing the distance measure with the data dependent dissimilarity; the otherwise unchanged algorithm can now overcome its weakness.

The result of not recognising the source issue can often lead to a solution which is more complicated than necessary and may not resolve the issue completely. An example is the inability of density-based clustering to find all cluster-s of varying densities. This issue is well-known and many suggestions have focused on density-based solutions [1, 9]. The fact that the -neighbourhood density estimator em-ployed relies on distance measure, which is the source of the weakness, has been overlooked.

It is interesting to note that one of the existing solution-s, i.e., SNN clustering has been incorrectly designated as density-based [9, 17] thus far. Our analysis in Section 5 has revealed that when replacing SNN (dis)similarity with the distance measure in -neighbourhood density estimator, the result is a mass estimator, not a density estimator.
Viewed from the conceptual perspective, simply changing the distance measure to a data dependent measure converts a density-based algorithm to a mass-based algorithm.  X  -neighbourhood mass can be viewed as a more general version of -neighbourhood density. However, the shape of -neighbourhood density region is fixed while the shape of  X  -neighbourhood mass region depends on data distribution. We provide an example in Figure 3 that  X  -neighbourhood mass has a regular shape region like -neighbourhood esti-mator in uniform density distribution only.

Implementations . An efficient implementation of m p dissimilarity [2] employs a fixed-width one-dimensional his-togram [3]. It partitions each attribute space into a fixed-number of cells independently, and the dissimilarity of any two instances is the total number of instances in the cells between the two instances in each attribute. In the exper-iments we have done in clustering and anomaly detection tasks, we found that using m p with this implementation per-forms better than using distance on most datasets. However, their overall results are worse than those using m e with the iForest implementation.

The use of iForest can be viewed as estimating probabili-ty from multiple variable-size multi-dimensional histograms.
Parameter  X  in iForest, used in the  X  -neighbourhood esti-mator, is a smoothing parameter similar to k in a k -nearest neighbour density estimator. High  X  yields large trees which are sensitive to local variations in data distribution X  X imilar effect of setting small k .

Since the default setting of iForest (  X  = 256 and t = 100) can be used to provide good performance on many dataset-s, the implementation of mass-based dissimilarity based on iForest does not create additional limitations, i.e., each ex-isting algorithm, which has been transformed with the mass-based dissimilarity, has the same time complexity and the same number of parameters as in the original algorithm.
The generic formulation of mass-based dissimilarity al-lows different implementations, including different variants of iForest ; and m p dissimilarity [2] and SNN (dis)similarity are its special cases X  X ll of them possess the characteristic of judged dissimilarity as prescribed by psychologists [12].
It is possible to use SNN in the contexts of kNN anoma-ly detection and MLkNN. However, its use has two issues. First, there are two k parameters as kNN is employed sepa-rately in the dissimilarity matrix calculation and the decision making process. Second, the high time complexity shown in Table 9 makes it prohibitive in large datasets.

Note that a mass-based neighbourhood function can be implemented using distance measure, as in the case of SNN . But, it is not only an indirect way to estimate mass but also an expensive one, as mentioned in Section 5.

Our implementation of data-dependent dissimilarity using trees opens up new research directions that worth investi-gating. kNN-based methods are traditionally regarded as distance-based methods; they become tree-based methods, though still employ dissimilarity measures, as we have shown here. The distinction between tree-based and distance-based methods are not clear-cut any more. Tree-based methods are usually thought to be less amenable than distance-based methods in dealing with high-dimensional datasets, espe-cially when they consists of mostly relevant attributes. This research raises the question whether this is still true when data-dependent dissimilarity is used.
We introduce a generic mass-based dissimilarity which is readily applied to existing algorithms in different tasks. The data dependent dissimilarity implemented with iForest overcomes key weaknesses of three existing algorithms that rely on distance, and effectively improves their task-specific performance on density-based clustering, kNN anomaly de-tection and multi-label classification.

These existing algorithms are transformed by simply re-placing the distance measure with the mass-based dissimi-larity, leaving the rest of the procedures unchanged.
As the transformation heralds a fundamental change of perspective in finding the closest match neighbourhood, the converted algorithms are more aptly called lowest probabil-ity mass neighbour algorithms than nearest neighbour algo-rithms, since the lowest mass represents the least dissimilar.
In the future, we will further explore other implementa-tions of data dependent dissimilarity and investigate their influence in different data mining tasks.
This material is based upon work partially supported by the Air Force Office of Scientific Research, Asian Office of Aerospace Research and Development (AOARD) under award number: #FA2386-13-1-4043 and 111 Project (B14020) (Kai Ming Ting); and NSFC (61333014) and the Collaborative Innovation Center of Novel Software Technology and Indus-trialization (Zhi-Hua Zhou). The anonymous reviewers have provided many helpful suggestions to improve this paper. [1] M. Ankerst, M. M. Breunig, H.-P. Kriegel, and [2] S. Aryal, K. M. Ting, G. Haffari, and T. Washio. [3] S. Aryal, K. M. Ting, G. Haffari, and T. Washio. [4] S. D. Bay and M. Schwabacher. Mining distance-based [5] L. Breiman. Random forests. Machine Learning , [6] M. M. Breunig, H.-P. Kriegel, R. T. Ng, and J. Sander. [7] B. Chen, K. M. Ting, T. Washio, and G. Haffari. [8] J. Dem X sar. Statistical comparisons of classifiers over [9] L. Ert  X  oz, M. Steinbach, and V. Kumar. Finding [10] M. Ester, H.-P. Kriegel, J. S, and X. Xu. A [11] R. A. Jarvis and E. A. Patrick. Clustering using a [12] C. L. Krumhansl. Concerning the applicability of [13] M. Lichman. UCI machine learning repository, 2013. [14] D. Lin. An information-theoretic definition of [15] F. T. Liu, K. M. Ting, and Z.-H. Zhou. Isolation [16] B. W. Silverman. Density estimation for statistics and [17] P.-N. Tan, M. Steinbach, and V. Kumar. Introduction [18] K. M. Ting, G.-T. Zhou, F. T. Liu, and J. S. C. Tan. [19] K. M. Ting, G.-T. Zhou, F. T. Liu, and S. C. Tan. [20] A. Tversky. Features of similarity. Psychological [21] F. Wickelmaier. An introduction to MDS. Sound [22] M.-L. Zhang and Z.-H. Zhou. ML-KNN: A lazy iForest consists of t iTrees , each built independently using a subset D , sampled without replacement from D , where |D| =  X  . The maximum tree height h = d log 2  X  e . Note that the parameter e in iTree is initialised to 0 at the beginning of the tree building process.
 Algorithm 1 iTree ( X , e , h ) Input: X -input data; e -current height; h -height limit. Output: an iTree . 1: if e &gt; h OR | X | 6 1 then 2: return exNode { Size  X  X  X |} ; 3: else 4: Randomly select an attribute q ; 5: Randomly select a split point p between min and 6: X l  X  filter ( X,q &lt; p ), X r  X  filter ( X,q &gt; p ); 7: return inNode { Left  X  iTree ( X l ,e + 1 ,h ) , 8: end if
