 Empty categories are phonetically null elements that are used for representing dropped pro-nouns ( X  X ro X  or  X  X mall pro X ), controlled elements ( X  X RO X  or  X  X ig pro X ) and traces of movement ( X  X  X  or  X  X race X ), such as WH-questions and rela-tive clauses. They are important for pro-drop lan-guages such as Japanese, in particular, for the ma-chine translation from pro-drop languages to non-pro-drop languages such as English. Chung and Gildea (2010) reported their recover of empty cat-egories improved the accuracy of machine trans-lation both in Korean and in Chinese. Kudo et al. (2014) showed that generating zero subjects in Japanese improved the accuracy of preordering-based translation.

State-of-the-art statistical syntactic parsers had typically ignored empty categories. Although Penn Treebank (Marcus et al., 1993) has annota-tions on PRO and trace, they provide only labeled bracketing. Johnson (2002) proposed a statistical pattern-matching algorithm for post-processing the results of syntactic parsing based on minimal unlexicalized tree fragments from empty node to its antecedent. Dienes and Dubey (2003) proposed a machine learning-based  X  X race tagger X  as a pre-process of parsing. Campbell (2004) proposed a rule-based post-processing method based on lin-guistically motivated rules. Gabbard et al. (2006) replaced the rules with machine learning-based classifiers. Schmid (2006) and Cai et al. (2011) integrated empty category detection with the syn-tactic parsing.

Empty category detection for pro (dropped pro-nouns or zero pronoun) has begun to receive at-tention as the Chinese Penn Treebank (Xue et al., 2005) has annotations for pro as well as PRO and trace. Xue and Yang (2013) formalized the problem as classifying each pair of the location of empty category and its head word in the de-pendency structure. Wang et al. (2015) pro-posed a joint embedding of empty categories and their contexts on dependency structure. Xiang et al. (2013) formalized the problem as classifying each IP node (roughly corresponds to S and SBAR in Penn Treebank) in the phrase structure.
In this paper, we propose a novel method for empty category detection for Japanese that uses conjunction features on phrase structure and word embeddings. We use the Keyaki Treebank (But-ler et al., 2012), which is a recent development. As it has annotations for pro and trace, we show our method has substantial improvements over the state-of-the-art machine learning-based method (Xiang et al., 2013) for Chinese empty category detection as well as linguistically-motivated man-ually written rule-based method similar to (Camp-bell, 2004). The Keyaki Treebank annotates the phrase struc-ture with functional information for Japanese sen-tences following a scheme adapted from the Anno-tation manual for the Penn Historical Corpora and tree based on Xiang et al. X  X  (2013) formalism) the PCEEC (Santorini, 2010). There are some ma-jor changes: the VP level of structure is typically absent, function is marked on all clausal nodes (such as IP-REL and CP-THT) and all NPs that are clause level constituents (such as NP-SBJ). Disambiguation tags are also used for clarifying the functions of its immediately preceding node, such as NP-OBJ *  X  *(wo) for PP, however, we re-moved them in our experiment.

Keyaki Treebank has annotation for trace mark-ers of relative clauses (*T*) and dropped pronouns (*pro*), however, it deliberately has no annota-tion for control dependencies (PRO) (Butler et al., 2015). It has also fine grained empty categories of *pro* such as *speaker* and *hearer*, but we unified them into *pro* in our experiment.
HARUNIWA(Fang et al., 2014) is a Japanese phrase structure parser trained on the treebank. It has a rule-based post-processor for adding empty categories, which is similar to (Campbell, 2004). We call it RULE in later sections and use it as one of two baselines.

We also use Xiang et al X  X  (2013) model as an-other baseline. It formulates empty category de-tection as the classification of IP nodes. For ex-ample, in Figure 1, empty nodes in the left tree are removed and encoded as additional labels with its position information to IP nodes in the right tree. As we can uniquely decode them from the extended IP labels, the problem is to predict the labels for the input tree that has no empty nodes.
Let T = t 1 t 2  X  X  X  t n be the sequence of nodes produced by the post-order traversal from root node, and e i be the empty category tag associated with t i . The probability model of (Xiang et al., 2013) is formulated as MaxEnt model: where  X  is a feature vector,  X  is a weight vector to  X  and Z is normalization factor: where E represents the set of all empty category types to be detected.

Xiang et al. (2013) grouped their features into four types: tree label features, lexical features, empty category features and conjunction features as shown in Table 1. As the features for (Xiang et al., 2013) were developed for Chinese Penn Tree-bank, we modify their features for Keyaki Tree-bank: First, the traversal order is changed from post-order (bottom-up) to pre-order (top-down). As PROs are implicit in Keyaki Treebank, the de-cisions on IPs in lower levels depend on those on higher levels in the tree. Second, empty category features are extracted from ancestor IP nodes, not from descendant IP nodes, in accordance with the first change.

Table 2 shows the accuracies of Japanese empty category detection, using the original and our modification of the (Xiang et al., 2013) with ab-lation test. We find that the conjunction features Table 1: List of features of Xiang et al. X  X  (2013). (* indicates the features we changed for the Keyaki Treebank) original (Xiang et al., 2013) 68.2  X  0 . 40 modified (Xiang et al., 2013) 68.6 -
Table 2: Ablation result of (Xiang et al., 2013) are highly effective compared to the three other features. This observation leads to the model pro-posed in the next section. In the proposed model, we use combinations of path features and three other features, namely head word feature, child feature and empty category feature. Path feature (PATH) is a sequence of non-terminal labels from the current node to the ances-tor nodes up to either the root node or the nearest CP node. For example, in Figure 1, if the current node is IP-REL, four paths are extracted; IP-REL, IP-REL  X  NP, IP-REL  X  NP  X  PP and IP-REL  X  NP  X  PP  X  IP-MAT.

Head word feature (HEAD) is the surface form of the lexical head of the current node. Child fea-ture (CHILD) is the set of labels for the children of the current node. The label is augmented with the surface form of the rightmost terminal node if it is a function word. In the example of Figure 1, if the current node is IP-MAT, HEAD is  X  X  X  (tsure) and CHILD includes: PP- X  (wo), VB, VB2, AXD- X  (ta) and PU- X  . Empty category feature (EC) is a set of empty categories detected in the ancestor IP nodes. For example in Figure 1, if the current node is IP-REL, EC is *pro*.

We then combine the PATH with others. If the current node is the IP-MAT node in right-half of Figure 1, the combination of PATH and HEAD is:IP-MAT  X   X  X  X  (tsure) and the combinations of PATH and CHILD are: IP-MAT  X  PP- X  (wo), IP-MAT  X  VB, IP-MAT  X  VB2, IP-MAT  X  AXD- X  (ta) and IP-MAT  X  PU- X  . 3.1 Using Word Embedding to approximate A case frame lexicon would be obviously useful for empty category detection because it provides information on the type of argument the verb in question takes. The problem is that case frame lex-icon is not usually readily available. We propose a novel method to approximate case frame lexicon for languages with explicit case marking such as Japanese using word embeddings. According to (Pennington et al., 2014), they designed their em-bedding model GloVe so that the dot product of two word embeddings approximates the logarithm of their co-occurrence counts. Using this charac-teristic, we can easily make a feature that approxi-mate the case frame of a verb. Given a set of word embeddings for case particles q 1 ,q 2 ,  X  X  X  ,q N  X  Q , the distributed case frame feature (DCF) for a verb w i is defined as: In our experiment, we used a set of high frequency case particles  X  (ga),  X  (ha),  X  (mo),  X  (no),  X  (wo),  X  (ni),  X  (he) and  X  X  X  (kara) as Q . 4.1 Dataset We divided the Keyaki Treebank into training, de-velopment and test sets. As of May 8, 2015, there are 22,639 sentences in Keyaki Treebank. We used 1,000 sentences as the development set, 1,003 sentences as the test set. They were taken from the files blog KNB.psd (blog), spoken CIAIR.psd (transcript), newswire MAINICHI-1995.psd (newswire) to balance the domain. The remaining 20,646 sentences are used for training. Further statistics are shown in Table 3.

We used GloVe as word embedding, Wikipedia articles in Japanese as of January 18, 2015, are used for training, which amounted to 660 million words and 23.4 million sentences. By using the development set, we set the dimension of word embedding and the window size for co-occurrence counts as 200 and 10, respectively. 4.2 Result and Discussion We tested in two conditions: gold parse and sys-tem parse. In gold parse condition, we used the trees of Keyaki Treebank without empty cat-egories as input to the systems. In system parse condition, we used the output of the Berkeley Parser model of HARUNIWA before rule-based ing the word-position-level identification metrics described in (Xiang et al., 2013). It projects the predicted empty category tags to the surface level. An empty node is regarded as correctly predicted surface position in the sentence, type (T or pro) and function (SBJ, OB1 and so on) are matched with the reference.

To evaluate the effectiveness of the proposed distributed case frame (DCF), we used an exist-ing case frame lexicon (Kawahara and Kurohashi, 2006) and tested three different ways of encod-ing the case frame information: BIN encodes each case as binary features. SET encodes each combi-nation of required cases as a binary feature. DIST is a vector of co-occurrence counts for each case particle, which can be thought of an unsmoothed version of our DCF.

Table 4 shows the accuracies of various empty category detection methods, for both gold parse and system parse. In the gold parse condition, the two baselines, the rule-based method (RULE) and the modified (Xiang et al., 2013) method, achieved the F-measure of 62.6% and 68.6% respectively.
We also implemented the third baseline based on (Johnson, 2002). Minimal unlexicalized tree fragments from empty node to its antecedent were extracted as pattern rules based on corpus statis-tics. For *pro*, which has no antecedent, we used the statistics from empty node to the root. Al-though the precision of the method is high, the re-call is very low, which results in the F-measure of 38.1%.

Among the proposed models, the combination of path feature and child feature (PATH  X  CHILD) even outperformed the baselines. It reached 73.2% with all features. As for the result of system-parse condition, the F-measure dropped consider-ably from 73.2% to 54.7% mostly due to the pars-ing errors on the IP nodes and its function.
We find that there are no significant differences among the different encodings of the case frame lexicon, and the improvement brought by the pro-posed distributed case frame is comparable to the existing case frame lexicon.

Table 5 shows the ablation result of the pro-posed model. It indicates conjunction between PATH and CHILD feature is most effective.

Proposed 72.1 -53.9 - X  CHILD 47.4  X  24 . 7 33.7  X  20 . 2  X  HEAD 70.0  X  2 . 1 51.6  X  2 . 3 Table 5: Ablation result of PATH  X  (CHILD + EC + HEAD) model In this paper, we proposed a novel model for empty category detection in Japanese using path features and the distributed case frames. Although it achieved fairly high accuracy for the gold parse, there is much room for improvement when applied to the output of a syntactic parser. Since the accu-racy of the empty category detection implemented as a post-process highly depends on that of the un-derlying parser, we want to explore models that can solve them jointly, such as the lattice parsing approach of (Cai et al., 2011). We would like to report the results in the future version of this pa-per.

