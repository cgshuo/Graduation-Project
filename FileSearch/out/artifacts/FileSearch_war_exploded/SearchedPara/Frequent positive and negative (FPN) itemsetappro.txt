 Data Mining and Optimization Research Group, Center for Artificial Intelligence Technology, Faculty of Technology and Information Science, Universiti Kebangsaan Malaysia, Bangi, Selangor, Malaysia 1. Introduction
An enormous data, created from our everyday routines, surrounds us. Data mining is an information extraction activity that seeks to discover the hidden facts contained in a database. Knowledge and inter-esting patterns are hidden beneath those data and data that exist often do not comply with the general behaviour or model of the data. Data that grossly differ from or are inconsistent with the remaining set of data are called outliers. The outlier detection is an important task in data mining and it is relevant to various applications, including fraud detection, marketing analysis, network intrusion, and medical analysis [1]. Outlier detection has become a fascinating problem in real application in data mining re-search [2 X 11] and network intrusion detection has gained the most attention. The definition of outlier detection is a data point that has different characteristics than the rest of the data in one or more mea-sures [12]. While outlier mining is the process of identifying outliers in a dataset. Every dataset consist-ing data points that provide useful insight into strange system behaviours. Many data mining algorithms attempt to minimise the influence of outliers or eliminate them altogether. However, this could result in losing important hidden information as a person X  X  noise could be another person X  X  signal [13]. The outliers themselves may hold particular interest, for example as in fraud detection, where the outliers may indicate fraudulent activity.

Outliers might be induced in the data for a variety of reasons, such as malicious activity (such as credit card fraud, network intrusion); machine error (such as defects in machines); change in the environment (such as a climate change, a new buying pattern among consumers); and human error (such as data entry or a data reporting error). All of the reasons have a common characteristic that they are interesting to the analyst. The interestingness outliers are rel evant to real life issues, which is a key feature of outlier detection a nd distinguishes it from noi se removal [14]. While, noise accommodation deal with unwanted outlier in the data. Noise in the data does not have a real significance by itself, but acts as an interference to data analysis. Noise removal is driven by the need to remove the unwanted objects before any data analysis is performed on the data. Noise accommodation refers to inoculating a statistical model estimation against outlying observations.

Initially, outlier detection in research work started from statistics. In the statistical-based outlier detec-tion, outliers are determined according to the probability distribution. The data distribution is assumed to fit into a model and outliers are those that do not fit into the probability distribution [15]. However the problems arise when data distributions are unknown and the database is high-dimensional. In later years, outlier detection methods can be found in com puter science. Outlier detection methods can be classified into the following major groups: distance-based, density-based and clustering-based models. These methods typically consider each data point and compare it with all other data points (using dis-tance [16], density [17,18], or clustering distance [19] respectively) in order to determine whether it is an outlier. However these methods are very sensitive to parameter values and furthermore, the parameters are hard to set. For example, if the number of clusters is not set properly for cluster-based method, the outlier detection can be very inaccurate. The above methods are u seful techniques for outlier de tection available for numerical data. Since, they have not performed well in discrete data or transaction data. Outlier in the transaction data is defined as those transactions whose items occur very rarely [15]. Since the frequent itemsets discovered by the association rule is a sign of the common patterns in the dataset and it is intuitive to regard those data points which contain less frequent patterns as rare pattern or in another name outliers. An outlier transaction is defined as a transaction that is expected to contain some items that actually did not appear there [19].

He et al. [19] proposed the usage of frequent itemset (FIS) in the outlier detection problem. Since then, the FIS had become an interesting research for outlier mining. Agrawal et al. [20] introduced the concept of FIS using an Apriori algorithm for the association rule task. The FIS, essential in mining association rules, guarantees that all generated rules are strong and interesting. However, discovering the FIS is a difficult task, because the mining space required for generating the FIS is much greater than generating the rules. The rules can be Positive Association Rule and Negative Association Rule (PAR and NAR, respectively). The knowledge obtained from the NAR mining process is unique and significant. Something outside the normal pattern or knowledge is more interesting than the strong positive rules, which are predictable and common [21]. The NAR is thus beneficial in detecting abnormal knowledge, like fraud and intrusion.

In this paper, we present a novel approach in generating significant itemsets from the NAR perspective called the frequent positive and negative (FPN) itemset approach that will be used to detect outliers. In FPN itemset approach, an outlier transaction is defined as a transaction that is expected to contain some items whether positive or negative that actually did not appear there. While in FIS, only positive items are considered for detecting outlier transaction. Indirectly, the FPN can overcome several FIS discovery disadvantages for better performance in outlier detection. The FPN generated itemset increases the de-tection performance in the outlier mining algorithm. The conducted experiment in this paper proved that the FPN approach can detect outliers better than several other outlier detection algorithms.
The remainder of this paper is organised as follows. Section 2 reviews related works on the itemset mining and outlier detection. Sections 3 and 4 present the proposed approach, the FPN and FPN-OD. Section 5 discusses the experimental results and discussion, and we conclude the paper in Section 6. 2. Problem statement
Exploring extraordinary outlier b ehaviour will help uncover valu able knowledge hidden underneath it. All outliers are not created equal and should not be universally removed from the analysis. Some outliers are worth considering and extremely important; they must therefore be addressed or overcome, especially when these outliers produce significant concern. Many traditional outlier detection methods rely heavily on datasets having mostly numerical features, as these types of features can easily be an-alyzed and manipulated using traditional statistical techniques. For the features that are non-numerical or categorical in nature, attempts are often made to map the categorical values to numerical values to facilitate statistical analysis. However, these mappings often attempt to entail some sort of order on the categorical features, which may be slanted in many instances, which means that the results of the analysis are not reliant or consistent from mapping to mapping. Thus, the use of numerical analysis on categorical data may not be as effective as it could be. An alternative approach for datasets with both numerical and categorical features is to convert numerical attributes to categorical attributes and apply techniques that are more suited to categorical data.

Another fundamental taxonomy of outlier detection methods is between parametric (statistical) meth-ods and non-parametric methods that are model-free. In contrast to statistical methods, data mining related methods are often non-parametric, thus, do not assume an underlying generating model for the data. These methods are designed to manage large databases from high-dimensional. A major drawback of statistical approach is that most tests are for single attributes, yet many data mining problems require finding outliers in multidimensional space. Moreover, the statistical approach requires knowledge about the parameters of the dataset, such as the data distribution (such as the mean and variance). However, in many cases, the data distribution may not be known. The limitation of statistical methods
The association rule mining for outlier detection, mainly using FIS from the positive of items or item-sets, has been researched [19,22 X 25]. Though the FIS approach consumes high memory, especially with high-dimensional data, it has better performance or detection rates than state-of-the-art outlier detection approaches, including the Replicator Neural Network (RNN) [26], k Nearest Neighbour (kNN) [16] or Cluster-Based Local Outlier Factor (CBLOF) [27], which focus on numerical data sets. In this paper, we target categorical record databases by observing the attribute values. We reviewed the selected outlier detection methods and discussed the relevant measures. He et al. [19,27] highlighted that common pat-terns are objects with large percentages, while outliers are objects with small percentages that belong to rare cases. In datasets where the distribution of instances contains common and rare cases, outliers are thus assumed to belong to rare cases. Outliers are rare and represented in a dataset in small proportions, ranging from 4% to 10% for data mining outlier detection.

He et al. [27] propose the concept of class outlier detection, which is essentially segmenting the data using the class labels, and then applying a known clustering based outlier detection technique to detect outliers within this subset. The objective of any outlier detection technique is to detect outliers in a data set. A labeling type of outlier detection technique is typically evaluated using any of the evaluation techniques from 2-class classification literature. First of all a benchmark data set is chosen. The primary requirement is that the outliers should be labelled in the data set. In the data mining community, the UCI Machine Learning Repository [28] is a valuable repository of benchmark data sets for evaluating different data mining algorithms. Validation data sets for outlier detection don X  X  include in the repository, but several authors have adapted some of the available data sets to be used for outlier detection. The data sets containing rare class are chosen for this purpose and the instances belonging to the rare class are treated as outliers in the data. Such analysis is done by Aggarwal and Yu [29]. Another technique is to take a labelled data set and remove instances of any one class. This reduced set forms the normal data. All or few instances of the removed class are injected in the normal data as outliers. This evaluation methodology is used by William et al. [30] to evaluate different outlier techniques. Such benchmark data sets allow a standardized comparative evaluation of outlier detection techniques and hence are very useful. Such techniques need to be evaluated not only for how well they detect outliers in a data set. A key observation here is that such evaluation measures the performance of the entire outlier detection setting (including the technique, the features chosen and other related parameters/assumptions). Such benchmark data sets allow a standardized comparative evaluation of outlier detection techniques and hence are very useful.

Many studies have been conducted on the outlier detection problem. Out of them, mainly focused on identifying outliers from data points. He et al. [31]) believed that outliers are data points that contain few common patterns or subsets, as the FIS have common patterns found in the dataset. The Frequent Pattern Outlier Factor (FPOF) for every data point is calculated, as in Eq. (1), as a measure of outlier objects. Lower FPOF scores denote that data points are likely to be outliers. Data points with less frequent patterns are considered outliers. Let D = { t 1 ,t 2 ,...t n } be a database containing a set of n transactions, each with a set of items from I . Given a threshold minsupport (a predefined minimal support), the set of all frequent itemsets is denoted as the FIS (D, minsupport) . For each transaction t , the FPOF is calculated. The FPOF value is between 0 and 1.

Otey et al. [24] also proposed an outlier detection from the FIS perspective. They evaluated on the outlier score, as in Eq. (2), for each data point and compared the sc ores to infrequent itemsets. Given an infrequent subset I of x , the outlier score of x is the sum of the opposite of the length of the infrequent itemsets. If a data point has a lower FIS, the outlier score is high. The outliers are the k points with the maximum outlier score. This method also relies on frequent itemsets, which assign to each point an anomaly score inversely proportionate to the infrequent itemsets. They calculate the score for each data point x thus: Several research methods using infrequent itemsets, including Koufakou et al. [25] and Taha and Hegazy [23] focused on the outlier detection from data points. Koufakou et al. [25] proposed the At-tribute Value Frequency (AVF) score, as in Eq. (3), where f ( x il ) is the number of times the l -th attribute successfully minimises data scan and detects outliers more quickly. Assume that the dataset contains n data points, x i ,i =1 ...n . If each data point has m attributes, then x i =[ x i 1 ,...,x il ,...,x im ] ,where x whether point x i is an outlier: Taha and Hegazy [23] introduced the Squares of the Complement of the Frequency (SCF), as in Eq. (4), where c j is the number of categories in the j -th categorical variable. The SCF uses the sum of squares of the complement of the marginal frequency instead of the sum of the marginal frequency to emphasise the difference between frequent and infrequent categories. In contrast to other outlier iden-tification methods in the categorical data sets, it considers the number of categories in the categorical variables. The data points are sorted in a descending order and the top k are outliers.

The methods discussed above implement only outlier detection during the candidate generation and none use rules to detect outliers. However, Narita and Kitagawa [22] presented the Outlier Degree (OD), as in Eq. (5), which uses the association rules to detect data points that are likely outliers. Let t + be the associative closure of transaction t  X  T for high-confidence rule set R . The OD was derived from the ideal form of data point t , t + , which includes t completely and does not violate any high-confidence rules. If t has fewer items with a strong dependency on an itemset in t , the difference between t and its ideal form t + must be larger. If t has fewer unobserved rules, t + must be similar to t and must not be interesting as an outlier.

Shaari et al. [18] proposed a new measure for the outlier mining, the Rough Set Outlier Factor (RSetOF), as in Eq. (6). This method introduced the Non-Reduct concept, which originates from the Reduct concept in the Rough Set Theory (RST) for detecting outliers. Reducts are sets of essential at-tributes that generate interesting rules, whereas the Non-Reducts are dispensable and the attribute sets are considered redundant. Reducts are interesting attributes that are employed in the attribute selection process. However, the Non-Reducts are uninteresting attributes with hidden knowledge that are assumed as signals rather than noise. The rare Non-Reduct case can discover hidden knowledge in the outliers. The RSetOF has a different concept than the algorithms discussed above as it adopts the FIS concept. However, the nature of a Non-Reduct is generally similar to that of an infrequent itemset.

Table 1 presents the main features of the measures in outlier detection as discussed in Section 2. The majority of measures focused on detecting outliers from data point. The difference is in the type of itemset used whether frequent or infrequent. He et al. [18,19] and Otey et al. [24] used frequent set item whereas Kaufakou et al. [25] and Taha and Hegazy [23] used infrequent set item. The itemset cardinality is used in calculating the outlier degree for every data point. Most selected measures used the Apriori algorithm to generate itemset different from the RSetOF which uses the Rough Set Theory as the basis for their approach. However, the concept in RSetOF is similar to infrequent itemset. Meanwhile, Narita and Kitagawa [22] proved that the association rules can be used in detecting outliers.

The AVF is a simple measure to implement but there was a possibility of missing some outlier data points. The Otey X  X  score has the advantage of considering mix attributes and the disadvantage is the process for detecting outlier is complicated. Meanwhile, the OD needs an additional process which was to generate rules from the set item. However, the OD introduced a new approach in detecting outliers, in which used rules instead of itemsets for mining outliers. The FPOF is among the first measures that used FIS itemset for the outlier detection. As discussed before, discovering the FIS is still a major disadvantage due to huge mining space. The RSetOF can solve the problem of generating the FIS by using the Rough Set Theory with the concept of None-Reduct attribute. The disadvantage of RSetOF is the low detection rate for outlier data which have higher support as discussed in Shaari et al. [18]. 3. Frequent itemset (FIS) Agrawal et al. [20] introduced the FIS discovery in a database using an Apriori algorithm as follows. a transaction database, where T represents the transaction for a set of items such that T  X  I .Let X = { i 1 ,i 2 ,i 3 ,...,i k } , be a set of items called an itemset. A transaction T is said to contain X if and only if X  X  T . An itemset is in the form { A, B } ,where A  X  I,B  X  I ,and A  X  B =  X  . Each itemset has its own measure, namely support. Support is calculated in a way similar to the frequency of transaction in D containing A and B . This is also the probability P ( A  X  B ) . Apriori is a well known algorithm for itemset. Apriori has two phases, the candidate and rule generation phases, and it is known as a support-confidence framework [32]. An itemset is discovered during the candidate generation phase and given a user-defined threshold called the minimum support. Itemsets with a value greater than the minimum support are considered FIS. However, discovering significant itemset is the bottleneck in association rule mining as several scannings of the database are required to count the support value for each itemset combination created. The threshold value eliminates weak and uninteresting items or itemsets in association rules and it successfully reduces the mining space.

The FIS was first explored to produce positive associations between items. The FIS indicates that the presence of some items implies the presence of others within the same transaction. An itemset { A, B } indicates that  X  A and B occur together X . In a negative association, items that exist do not positively associate with others. We call an itemset { A,  X  B } a negative itemset, indicating that the presence of one item implies the absence of another in the same transactions. A negative itemset contains a nega-tion of an item. Every positive itemset contains three types of negative itemset: { A,  X  B } , { X  A, B } , and { X  A,  X  B } . A negative itemset is derived from the concept of NAR, introduced by Brin et al. [33]. The NAR arises from the correlation between antecedent and consequent in the association rules. The correlation can be positive, negative or independent. The NAR indicates that there is a negative rela-tionship between itemsets in the association rules. Since its discovery, the NAR mining has attracted serious attention from researchers. A negative relationship implies the presence of items by the absence of others in the same transaction; for example,  X  X ustomers that buy Coffee do not buy Tea X . The NAR can identify items that conflict with or complement each other. We call rules of the form A  X  X  B negative rules, negating an item. As for an itemset, three NAR types exist in every positive association rule (PAR): A  X  X  B,  X  A  X  B ,and  X  A  X  X  B [34]. Unfortunately, incorporating negation into the association rule framework is challenging as the ratio of the average number of items per transaction to the total number of possible items produces a huge number of possible association rules with nega-tion [35]. The total number of generated positive and negative association rules is 4(3 m  X  2 m +1 +1) of which 3 m  X  2 m +1 +1 , roughly only one quarter are positive association rules [36]. Furthermore, the number of possible infrequent itemsets for negative association rules is often far greater than the number of frequent itemsets in a database [13,37,38]. Specific and efficient mining techniques are therefore crit-ical for discovering negative itemsets, especially from the infrequent itemset perspective. The negative itemset helps the positive itemset identify deviant data points. The negative itemset can give accurate and complete information that cannot be captured by the positive item. The idea is also to eliminate weak and inaccurate positive itemsets using accurate negative itemsets. The knowledge from a negation item-set can provide more comprehensive information when used with a positive itemset for better decision making. In this paper, we propose to apply both negative and positive itemsets in the outlier detection mining. 4. Frequent positive and negative (FPN) itemset approach
As discussed, there are two types of itemsets: infrequent and frequent. Commonly, calculating the support (supp) for an itemset is based on the presence of a particular itemset in a transaction. In this paper, we propose calculating the support from the absence of items or itemsets. The inspiration comes from the NAR mining that considering frequent negative itemset given an advantage when discovering a strong NAR [39]. Nevertheless, frequent positive still plays an important role in exploring the FIS. Therefore, in this paper the use of frequent itemset, consists of negative and positive itemset is proposed. This approach is called the frequent positive and negative (FPN) itemset. The FPN is expected to produce important itemset and that will be effective to detect outlier. We now present the implemented algorithm of FPN as in Fig. 1. We look at the generation of a frequent itemset in the Apriori. This algorithm mines both the positive and negative association itemsets. However, only frequent itemset, including negative itemset with the calculated support above the minimum support value will be extended to be considered for significant itemset.

For easier comprehension of the FPN approach, we conducted a test with a small dataset to illustrate the discovery of FPN itemset. Table 2 contains a transaction dataset, whereas Table 3 denotes an frequent discovered itemset. L k denotes all frequent k -itemsets. Table 3 lists all generated frequent itemsets, including positive and negative, with a minimum support = 4. Candidate generation was discontinued at we only focus on itemsets. In Table 3, item E is not considered for frequent itemset discovery if we only regard the positive frequent itemsets. As a result, the generated rules miss the strongest item in the negative manner. In Section 5, we prove that these frequent items are important for increasing the detection rates for outlier mining.

Furthermore, this example proves that the number of negative association itemsets is dominant com-pared to the positive association itemsets. Only two positive association itemset exists in L 2 ,which conversely contains eight negative items. Therefore, the mining space increases tremendously because of the negative association itemsets. If we look at L 3 , all frequent itemsets are from the negative itemsets. Though the mining space is huge when we consider the negative association itemsets, the knowledge hid-den is far more crucial when examining critical data. Therefore, a pruning strategy plays an important role when considering an negative itemset which will be elaborated in Section 4. 5. Frequent positive and negative for outlier detection (FPN-OD)
The FPN-OD offers five main phases in handling the outlier detection, as in Fig. 2. The algorithmic version of the FPN-OD is as in Fig. 3. The FPN-OD interested in infrequent itemset from FPN itemsets. It is presumed that discovering outliers from infrequent itemsets is more efficient because outlier is a rare class of a dataset and significantly improves the detection rate. Furthermore, the infrequent object points are related to the support values for itemsets in each object in the dataset that can be translated to the density of objects in the cluster. 5.1. Phase 1: Mining FPN itemset
The first phase introduces the procedure for mining the FPN itemsets. Itemsets with more than the specified minimum support will be proceeding with the next phase. The algorithm in Fig. 1 is used in this phase. Later, the FPN-OD is exploring on infrequent itemsets from FPN itemsets as the rare object points, reflected as the scarceness of objects (outliers) from the common objects and it will be beneficial for calculating t he outlier degree o f each data point. 5.2. Phase 2: Pruning strategy
The second phase implements a pruning measure to reduce the mining space for exploring significant itemsets. The pruning strategy in mining negative association itemsets is vital. We therefore chose the collective strength [19,40] as a pruning measure. This step eliminates insignificant itemsets, whether positive or negative association itemsets, so the mining space decreases by focusing strongly on col-lective itemsets. Collective strength is a criterion stressing on both the importance of the actual item correlations with one another. The collective strength of an itemset is of a value between 0 and  X  .A value of 0 indicates perfectly a negative correlation, while a value of  X  indicates perfectly a positive correlation. A value of 1 indicates the  X  X reak-even point X , corresponding to an itemset present at the expected value. An itemset is in violation of a transaction if some items are present in the transaction and others are not. The concept of violation thus denotes how many times a customer may buy at least some, but not all, items in the itemset.

The collective strength is the agreement ratio divided by the violation ratio in Eq. (7) [40]. The agree-ment ratio as the ratio of the number of nonviolations to the expected number of nonviolations and the violation ratio as the ratio of the number of violations to the expected number of violations. An itemset is said to be in the violation of a transaction, if some of the items are present in the transaction, and others are not. In other words, the probability that t he itemset occurs and the pr obability that none of the items occur in the transaction. For example, if the given itemset { A, B } , the collective strength will be as in Eq. (7)
As in an example for small dataset in Table 2, the notion of collective strength is measured. As illus-trated in Table 4, the collective strength of the itemsets B  X   X  E and D  X   X  F among the highest. Since, the correlation value of those itemsets are not so high and the support value for itemset D  X   X  F is also low, only 4. Thus, this example illustrates the better applicability of this measure, collective strength to situ-ations in which we would like to find itemsets which strongly collective itemsets. It is important to note that the notion of collective strength is designed to see how the value of one attribute affects the value of another, which contain information about the absence of items [32,40]. For each combination of items, we need to use information regarding both the presence and absence of the items in order calculate its collective strength. However, the notion of collective strength is more closely related to the correlation value than the support value. Collective strength thus illustrates the better appropriate to situations in which we wish to find negative association rules. 5.3. Phase 3: Calculating outlier degrees
The third phase describes the procedure for calculating the outlier degrees of each transaction using the proposed measure, FPNOF as in Eq. (8). The FPN (D, minsupport) is a frequent positive and neg-ative (FPN) itemset with predefined minimal support (minsupport). The FPNOF value is based on the frequency of FPN itemsets for each transaction in the dataset divide by the frequency of total number of FPN in the dataset. This value indicates the degree of outlier-ness for each object. If the objects have larger values, than the objects have a common pattern, while objects with small values are regarded as outliers. Let D = { t 1 ,t 2 ,...,t n } be a database containing a set of n transactions with the items I .Given a threshold minsupport , the set of all frequent patterns is denoted as: FPN ( D , minsupport ). For each transaction t ,the FPNOF t is defined as:
The interpretation of Eq. (8) is if a transaction t contains more frequent patterns, its FPNOF value will be big, which indicates that it is unlikely to be an outlier. In contrast, transactions with small FPNOF values are likely to be outliers. From the viewpoint of knowledge discovery, frequent patterns reflect the  X  X ommon patterns X  that apply to many objects, or to the large percentage of objects in the dataset. In contrast, outlier detection focuses on a very small percentage of data objects. Hence, the idea of making use of frequent patterns for outlier detection is very intuitive. In this approach, the frequent pattern comes from frequent positive and negative itemsets. Outliers are those transactions which are infrequent in the dataset. Additionally, an  X  X deal X  outlier transaction in a categorical dataset is one whose each and every item or itemsets value is extremely infrequent. The infrequent-ness of an itemsets value can be measured by computing the number of times this value is assumed by the corresponding itemsets in the database.
In another explanation, the numerator of Eq. (8) represents the frequency of frequent positive and neg-ative itemsets in the transaction FPN ( X ). The dominator defines the total frequent positive and negative itemsets in database FPN ( D ). Therefore, a transaction contains more frequent pattern is more normal, while one contains less frequent is more abnormal and more possible to be an outlier. As FPNOF ( t ) is low, the corresponding transaction t is likely to be less normal. If the FPN itemsets not contained in the transaction and it is said that the FPN itemset is contradictive to the transaction therefore are good candidates for describing why the identified outliers are abnormal. It represents the number of elements which are in FPN but not in it , means that longer itemsets give a better description than that of short ones. It is possible to identify the contribution of each itemset to the outlying-ness of the specified transaction, that is each itemset in the FPN ( D ) but not in that specified transaction. 5.4. Phase 4: Ranking data points
In phase four, all data points are ranked in the ascending order of their outlier degree. If the data point is a rare class, the value of outlier degree is small and will be ranked at the top. Therefore, significant itemsets are essential in order to be used in calculating the outlier degree for each data point. Those itemsets are infrequent in terms of positive or negative items to indicate the rareness of a particular data point. The percentage of rare cases for a dataset, which ranges from 1% to 10%, are based on the definition given in the literature [29,30].

We output top-k transactions in the normal degree ascending order, as outliers. This FPN-OD algo-rithm is shown in Fig. 3. The process of FP algorithm is as follows. First the algorithm using FPN algorithm to generate the frequent positive negative (FPN) itemsets from the dataset with a given min-isupport. Then, for each transaction t in the dataset T ,find t  X  X  frequent positive and negative pattern set FPN ( t ). After that, we could calculate t  X  X  outlier degree FPNOF ( t ). When the traversing is over, the value of every t X  X  FPNOF ( t ) is computed. Then order all the transactions by their FPNOF ( t )valuein ascend. Finally, the top-k FPN-outliers are output. 5.5. Phase 5: Calculating detection rate
Finally, the fifth phase is to detect outliers by using the ranked list from phase four. If all the outlier data points were ranked at the top of the list, the detection rate will be perfect. If there are common data points at the top of the ranked list, it will reduce the detection rate. In this study, the measures of top ratio and coverage ratio evaluate the outlier detection performance of each method [18,19,41]. The top n ratio determines the cutoff point when searching for outliers in a dataset. A small value of the top n ratio indicates a shorter time when searching for outliers, thus indicating faster speeds in outlier detection. The coverage ratio calculates the numbers of outliers that belong to a rare class. When searching for outliers in a dataset using the top n ratio, the coverage ratio checks whether the object detected belongs to a rare class. If it does, then the number of outliers belonging to the rare class at the uncovered top n ratio is the coverage ratio. 6. Experimental results and discussion
We conducted an experiment on ten different datasets from the UCI Machine Learning Reposi-tory [18,28] to exhibit the effectiveness of the FPN-OD. Table 4 demonstrates the characteristics of selected datasets. The rightmost column shows the percentage of rare classes in the particular dataset. To fairly compare the relevant algorithms, we employ the same datasets as in Table 4. All datasets have imbalanced class distributions, which are created by removing certain amounts of data from certain classes. The outlier coverage for each dataset as shown in the rightmost column in Table 4. The steps follow the process of preparing datasets for outlier detection by William et al. [30] and He et al. [31]. The parameters for this experiment are minimum support for discovering frequent itemsets is fixed at 10% and the collective strength threshold value is 1, which allows only frequent itemsets with a collective strength value greater than 1 to be extended in the candidate generation phase.

Table 6 depicts the comparisons of the top ratio results between our method, the FPN-OD, and other algorithms, the FPOF and RSetOF. Each row in the table displays the results from the dataset named in the leftmost column. Each row in the table shows the top ratio measure for each dataset named in the leftmost column. As denoted in Table 6, the results for the FPN-OD method represents a significant improvement in outlier detection in all datasets compared to the FPOF. Compared to the RSetOF, the FPN-OD has only lower detection rates in three datasets, the BCE, CLV and ZOO datasets; however, the difference is small, less than 5%.

Comparing our method with the RSetOF produces competitive results, as the gaps are slim, compared to the FPOF. In the GLS dataset, the difference is large, and it is even larger when compared to the differ-ence with the FPOF. The FPN-OD has a perfect score in detection rates for four datasets, as demonstrated in Table 6 (bold format), which also shows that the top ratio values of the FPN-OD are similar to the total coverage outliers in particular datasets. The perfect score means that all outliers in each dataset ranked at the top of the data point list therefore the fastest detection rates are achieved. However, even though the results of GLS dataset are not among the perfect scores but the results were improved and better than the results of the FPOF and RSetOF which are huge compared to other datasets. From Table 6, we can see that the wins/ties/loss record of the FPN-OD against the RSetOF and FPOF is 4-3-3. All the ties results are with the RSetOF, which are the perfect scores. Meanwhile, for losses, all of datasets losses to the RSetOF.
To clearly understand the comparison, Table 7 shows the results of the top ratio versus coverage ratio for the HDE and GLS datasets. For the HDE dataset, the FPN-OD can detect all outliers with a 100% coverage ratio at the top 6.25% and similar with the RSetOF. Conversely, the FPOF can detect all outliers with a similar coverage ratio at the top 7.50%. A poor detection rate in the FPOF is observed where at the top ratio range 5.0% to 6.25%, the method can detect only 80% of the outliers. This indicates that the FPN-OD and RSetOF detect outliers more quickly than the FPOF. Meanwhile, in the GLS dataset, the FPN-OD result wins over the RSetOF and FPOF. The FPN-OD detects all outliers at the top 4.69%, FPOF at the top 56.34% and RSetOF at the top 87.32%. The FPN-OD managed to detect outliers in earlier similar with RSetOF but at the end FPN-OD successfully detects all outliers faster than other methods. The trend of results of remaining datasets can be found in Fig. 4. The results are based on top n ratio (x axis) versus the coverage ratio (y axis). Figure 4 illustrates that the FPN-OD has better detection rates in datasets ACC, COL, ECO, and GLS compared to other methods. Meanwhile in datasets HDE, IRS, and LYM, the FPN-OD have similar detection rate with the RSetOF which is the perfect score. However, in the BCE, CLV and ZOO datasets, the FPN-OD has a slow detection rate compared to RSetOF.

The FPN-OD results show that the outlier detection performance decreases when the number of de-cisions or classes in each dataset increases, as in the ZOO dataset, where the top ratio values are more 10% compared to other datasets with detection rates less than 10%. However, the results demonstrate that the FPN itemsets effectively generate important frequent itemsets for outlier mining. In this experi-ment we used a statistical paired t-test to measure the significance of the differences in the detection rate. A statistical paired t-test was carried out to measure the significance of the performance results between FPN-OD and the comparative methods (FPOF and RSetOF) in relation detection rate as shown in the last row, titled SIG in Table 6. The result of the FPN-OD versus FPOF shows that the significance of the difference is 0.033, which is less than the p value, 0.05, indicating that FPN-OD have significant perfor-mance. While, the result of the FPN-OD versus RSetOF indicates that the significance of the difference is 0.142, greater than p value, 0.05, indicating that both methods have comparable performance. 7. Conclusion
The novelty of this study is the application of negative itemset in outlier mining. We modified the traditional Apriori algorithm to produce strong negative itemset from the frequent absent point of view. We devised an enhanced algorithm to embed the FPN approach in outlier detection methods called the FPN-OD. We evaluate our approach by using several datasets which are real world datasets from UCI Machine Learning Repository [28]. A variety of datasets are required to verify the FPN-OD method and to ensure that its working well with different characteristics and background. For example in medical datasets, several datasets such as BCE, CLV and HDE are selected with the idea that they are suitable in predicting an abnormality of the rare cases in medical data sets. Each dataset describes the profiles of patients with symptoms of sickness or being healthy. The results show that FPN-OD method able to diagnose any symptom of the sickness of a patient from the medical information kept in a dataset. The lower detection rate indicates fast detection that may help medical officer to react at fast speed towards patient survivals. A normal pattern refers to a pattern in the data which is not an outlier. The output of an outlier detection technique could be labelled patterns (outlier or normal). A single outlying record will be an outlier and would be interesting as it would indicate some problem with a patient X  X  health. Outlier detection in the medical and public health domains typically work with patient records. Thus the outlier detection is a very critical problem in this domain and requires a high degree of accuracy.
This research contributes to solve the problem of generating significant itemsets by using the frequent positive and negative itemsets to identify outliers accurately and quickly as well as to enhance the detec-tion rate. In the FPN approach, the knowledge from frequent negative itemsets helps frequent positive itemsets identify outliers. The experiments indicated that the FPN method can detect outliers with better detection rates than other outlier detection met hods. The FPOF and RSetOF have been evaluated on the performance of detection rate for outliers together with FPN-OD. Results denote that the FPN-OD pro-duces highly competitive solutions. Furthermore, it is evident that considering the negative itemsets may improve the outlier detection rate compared to considering only the positive itemsets. For future studies, the FPN-OD will be beneficial to real data such as network data to solve intrusion detection problem. References
