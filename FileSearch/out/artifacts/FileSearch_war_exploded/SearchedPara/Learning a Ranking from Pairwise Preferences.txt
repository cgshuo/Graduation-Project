 We introduce a novel approach to combining rankings from multiple retrieval systems. We use a logistic regression model or an SVM to learn a ranking from pairwise document pref-erences. Our approach requires no training data or relevance scores, and outperforms a popular voting algorithm. Categories and Subject Descriptors: H.3.3 Information Search and Retrieval: Search Process General Terms: Algorithms, Design, Experimentation, Performance Keywords: information retrieval, data fusion
It has been shown that relevant documents not retrieved by one system might be successfully retrieved by another, and as a result, combining multiple rankings can produce a better new ranking. Previous methods have used a linear combination of document scores [2], voting algorithms based on preference rankings [1], or taking the maximum, mini-mum, or average of a set of scores [3, 5]. We explore a sta-tistical method based on pairwise preferences of documents. Specifically, given a ranking of documents d 1 d 2 d 3  X  X  X  , where i j means i is ranked above j , we extract all doc-ument pairs ( d i ,d j ) and assume that if d i d j , then d preferred to d j . For example, if one ranking is d 1 ,d we say that system prefers d 1 to d 2 ,d 3 , and d 4 ; it prefers d to d 3 and d 4 , and it prefers d 3 to d 4 .
Suppose documents have a relevance weight  X  i such that a large  X  i means the document is of  X  X igh relevance X  and a low  X  i means the document is of  X  X ow relevance X . The best possible ranking of documents would be in descending order of the weights. A retrieval system can be seen as sampling from a relevance distribution p i (  X  i ) to estimate the weight for each document, and then ordering accordingly.

Under this assumption, given two documents d i and d j , the probability that d i is ranked higher than d j is P (  X   X  ), where P is the cumulative density function of p (  X  ) = p MLE ranking is by the parameter vector (  X  i ) which maxi-mizes the likelihood of the set. For all pairs of documents d ,d j , let y i = 1 if d i d j and y i = 0 if d j d i likelihood is defined as: For simplicity we assume that the relevance weights are dis-tributed normally with mean  X  i and variance 1 2 . Then, the likelihood can be simplified to a logistic regression with pro-bit link function: where  X  is the normal cumulative distribution. We can find  X 
MLE using a mathematical toolkit such as R or Matlab. We adapted this model from Mease [6].

With just one system as input, the resulting ranking will be the same as the original. With more than one rank-ing, documents that are consistently ranked highly are more likely to be relevant, and documents that are ranked higher than documents that are consistently ranked highly are even more likely to be relevant, even if they make fewer appear-ances in the ranked lists.
We sampled runs randomly from all 74 retrieval runs sub-mitted to the ad hoc track of TREC-6. For each topic, we extracted all document pairwise preferences from the top 20 documents retrieved by each system. We then found the parameter values that maximized the likelihood func-tion above. We compared the resulting ranking to the set of input rankings. On average the resulting ranking was bet-ter than all of the input rankings except the best one, i.e. it is expected that combining rankings will be better than selecting a ranking at random. Figure 1(a) shows results.
A baseline algorithm that, like ours, requires no train-ing and no document scores is the Borda count voting algo-rithm [1]. A Borda count for a document is simply the num-ber of documents ranked below it. Documents are ranked by their Borda count. The logistic regression model outper-forms this algorithm.
Logistic regression can be interpreted as learning a deci-sion hyperplane. A support vector machine (SVM) can also learn decision hyperplanes, and it has two advantages over logistic regression: first, it makes no assumption about the distribution of relevance, and second, linear kernel SVMs can be optimized more efficiently than the logistic regres-sion likelihood function.

To learn the SVM decision hyperplane, we solve the opti-mization problem: Documents are ranked by the  X  that minimizes this. Here x is a vector of length n associated with a pair of documents such that if d i d j , then y k = 1 ,x k [ d i ] = 1 ,x k and everything else is 0.
We again take random samples of TREC-6 runs and trun-cate them after the top 20 documents. With the SVM, the learned ranking was on average the best or second best of all input rankings. The results are shown in Figure 1(a). The SVM outperforms both the logistic regression and Borda models.

Linear SVM optimization is very efficient using algorithms such as Keerthi and DeCoste X  X  [4], so the SVM model scales better than the logistic regression model. We can go deeper in the list than we can with logistic regression. Figure 1(b) shows the result of truncating after 100 documents retrieved by each system.

We can reduce the amount of data by randomly sampling document pairs. We find that we can reduce the number of preferences by as much as 75% with no noticible performance degradation.
We can use statistical estimation models to learn a new ranking of documents given other rankings with no training data. Our models do not use scores, making them more flexible, and they empirically outperform the Borda count voting algorithm.

Our models can be enhanced when training data is avail-able. Given some relevance information, we can exclude any preference that contradicts it. For example, if we know d is relevant, d i should be preferred to everything else; if d nonrelevant, it should not be preferred to anything. Given some knowledge of relative system quality, we can weight preferences according to how good the system that gener-ated them is. A simple weighting scheme is to scale to an integer value n and then duplicate preferences n times. Pre-liminary experiments show improved rankings when some training data is available.
 This work was supported in part by the Center for Intelli-gent Information Retrieval, in part by the Defense Advanced Research Projects Agency (DARPA) under contract number HR0011-06-C-0023 and through the Department of the In-terior, NBC, Acquisition Services Division, under contract number NBCHD030010. Any opinions, findings, and con-clusions or recommendations expressed in this material are the authors X  and do not necessarily reflect those of the spon-sor.
