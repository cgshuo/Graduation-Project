 Representing words as vectors in some latent space has long been a central idea in natural lan-guage processing. The distributional hypothesis , perhaps best stated as  X  X ou shall know a word by the company it keeps X  (Firth, 1957), has had a long and productive history, as well as a recent revival in neural-network-based models (Mikolov et al., 2013). These methods generally construct a word by context matrix, then either use the vectors di-rectly (often weighted by term frequency and in-verse document frequency), perform some factor-ization of the matrix, or use it as input to a neu-ral network which produces vectors for each word. The resultant vectors can be used in a wide array of tasks, from information retrieval to part-of-speech tagging and parsing.

There has also been some recent work address-ing how to create these vectors when informa-tion from multiple languages is available. Two recent attempts involve using canonical correla-tion analysis (CCA) to project pre-trained vec-tors from each of two languages into a common space (Faruqui and Dyer, 2014b) and using an alignment matrix to heuristically project the vec-tors from one language onto the words in another language (Guo et al., 2015). These methods gener-ally only work with two languages at a time, how-ever.

In this paper, we introduce a technique for con-structing multilingual word embeddings that is in-spired by the notion of translational invariance. CCA and the heuristic projection mentioned above both attempt to construct vectors such that words that are translations of each other are close in the vector space, but the method we introduce formal-izes this as part of the objective function of the original decomposition . We further show how to optimize this objective function with a method that scales linearly in the size of the input data. This results in a scalable, single-step method that is in-formed by both the monolingual corpus statistics and the multilingual alignment data. We show ex-perimentally that this results in vectors that outper-form prior work on multilingual tasks and match the performance of prior work on monolingual tasks.
The contributions of this paper are the follow-ing:  X  Problem formulation: we formalize the no- X  Scalable algorithm: we introduce scalable  X  Effectiveness: we present state-of-the-art re-
The code and data used in this paper are pub-licly available at https://sites.google. com/a/umn.edu/huang663/ . The informal problem definition is the following: Informal Problem. Given a set of cooccurrence statistics between words in each of several lan-guages, and a translation table containing align-ment counts between words in each of these lan-guages, Find a latent representation for each word in each language that (1) captures information from the cooccurrence statistics and (2) is invari-ant to translations of the cooccurrence statistics between languages.
 More formally, suppose we have M 1 words and N 1 contexts in the first language ( X  X nglish X ), and M 2 and N 2 for the second language ( X  X panish X ). Then, we are given two matrices of cooccurrence statistics (one for each language), with dimensions M 1  X  N 1 and M 2  X  N 2 , and two dictionary matri-ces containing translations from English to Span-ish, and from Spanish to English, repesctively. A more detailed description on how the data is ob-tained can be found in (Faruqui and Dyer, 2014b). For simplicity in what follows, we denote these matrices as  X  X : a single multilingual cooccurrence matrix  X  D 1 : a word dictionary matrix (with all the  X  D 2 : a context dictionary matrix (with all the
We seek decompositions of X that are invari-ant to multiplications along each mode by its re-spective D matrix. Note that, while we only de-scribed the case where we have two languages, it is straightforward to extend this to having many languages in the combined X , D 1 and D 2 matri-ces, and we do this in some of the experiments described below. Without the side information provided by the dic-tionary matrices, the classic method for generat-ing word vectors finds a low-rank decomposition of the data matrix X : With proper scaling (see our discussion in  X  4.2), the rows of U (or rows of V ) are the word embed-dings (or  X  X ontext embeddings X ). It is well-known that the solution is given by the principal compo-nents of the singular value decomposition (SVD) of X . Generating word embeddings in this way is known as latent semantic analysis (LSA) (Deer-wester et al., 1990).

Our method extends LSA to incorporate infor-mation from many languages at a time, with the constraint that the decomposition should be invari-ant to translation between these languages. We call this method translation-invariant LSA (TI-LSA).
 In order to take the dictionary matrices D 1 and D 2 into consideration, we propose to seek a de-composition that can simultaneously explain the original matrix X and various translations of it. We can formalize this in the following objective function: min By expanding and combining all four quadratic terms, we can see that the above problem is equiv-alent to (up to a constant difference) where
Taking the SVD of  X  X does not seem numeri-cally appealing at first glance: even though D 1 , D 2 , and X are all very sparse, forming itly will introduce a significant amount of nonze-ros. However, as we will explain below, it is not necessary to explicitly form  X  X in order to find a few principal components of it.

We propose to use the Lanczos algorithm (Golub and Van Loan, 1996, Chapter 9) to calcu-late the SVD of  X  X . The Lanczos method can be viewed as a generalization of the power method for computing an arbitrary number of principal components, and the basic operation required is only matrix-vector multiplication. For our prob-lem specifically, the required matrix-vector multi-plications  X  X  X  and  X  X T  X  can be carried out very efficiently with three sparse matrix-vector multi-plications, each with complexity linear in the num-ber of nonzeros in the sparse matrix involved, so that any dense intermediate matrix is avoided. As a result, by using our implementation of the Lanc-zos method, the time required for calculating the SVD of  X  X is not much more than that of X , even though  X  X is significantly denser than X . We present three experiments to evaluate the method introduced in this paper. The first experi-ment uses our word embeddings in a cross-lingual dependency parsing task; the second experiment looks at monolingual (English) performance on a series of word-similarity tasks; and the final ex-periment shows the scalability of our method by applying it to multiple languages. 4.1 Cross-lingual evaluation Guo et al. (2015) recently introduced a method for using multilingual word embeddings to perform cross-lingual dependency parsing. They train a neural-network-based dependency parsing model using word vectors from one language, and then test the model using data and word vectors from another language. They used the embeddings ob-tained by Faruqui and Dyer (2014b), along with a heuristic projection. Because we used the same data to obtain our embeddings, our method is di-rectly comparable to the CCA method of Faruqui and Dyer, and the projection method of Guo et al. We used code and data graciously provided by Guo to run experiments, training a dependency parsing model on their English treebank, and test-ing it on the Spanish treebank. We report the re-sults below for the methods used by Guo et al. and the method introduced in this paper. We could not exactly reproduce Guo X  X  result with the code we were provided, so we report all results from our use of the provided code, in case some parame-ter settings are different from those used in Guo X  X  paper. The results are shown in Table 1. As can be seen in the table, our first method for obtain-ing multilingual embeddings outperforms both the CCA method of Faruqui and Dyer, and the heuris-tic projection used by Guo et al. 4.2 Monolingual evaluation While our focus is on generating embeddings that are invariant to translations (and thus most suited to multi-or cross-lingual tasks), we would hope that the addition of multiple languages would not hurt performance on monolingual tasks. We used wordvectors.org (Faruqui and Dyer, 2014a) to evaluate our learned vectors on a variety of English-language word similarity tasks. The tasks are mostly all variations on performing word simi-larity judgments, finding the correlation between the system X  X  output and human responses. We used the same data as that used by Faruqui and Dyer (2014b) (English-Spanish only), and thus our method for obtaining multilingual embeddings is directly comparable to their technique for do-ing the same (CCA). We used the first 11 tasks on wordvectors.org, and obtained Faruqui and Dyer X  X  results from that website. Due to space con-straints, we only report the average performance across these 11 tasks for each of the methods we tested. The results are shown in Table 2. To test statistical significance, we performed a paired per-mutation test, treating performance on each task as Method Average Correlation CCA (Faruqui &amp; Dyer) 0.638 LSA 0.626 TI-LSA 0.628 paired data. The important thing to note from the table is that the differences between the methods are all quite small, and none of them are statisti-cally significant.

Note that LSA on just the English data performs on par with all of the other methods presented; we have not found a way to improve performance on this monolingual task from using multilingual our multilingual methods do not hurt performance on these monolingual tasks, either X  X e get the benefits described in our other evaluations without losing performance on English-only tasks. 4.3 Scalability We mentioned in Section 3 that our method is lin-ear in the number of nonzeros in the data, as we are simply using the Lanczos algorithm to com-pute a sparse SVD. To show this in practice, we briefly present how the running time of our algo-rithm scales with the number of languages used. Each additional language adds roughly the same amount of data to the X matrix. Figure 1 shows that our method does indeed scale linearly with the number of nonzeros in the matrix, and thus also with the number of languages used (assuming each language has roughly the same amount of data). All the experiments are performed in MATLAB 2013a on a Linux server with 32 Xeon 2.00GHz cores and 128GB memory. We discuss here two points on the flexibility of the method we have introduced. First, note that the dictionary matrices we used contained infor-mation about translations between languages. It is also possible to include information about para-phrases in this dictionary. For instance, a resource such as the Paraphrase Database (Ganitkevitch et al., 2013) could be used to further constrain the embeddings obtained; this could be useful if the resource used to obtain a paraphrase dictionary contained more or different information than the corpus statistics used in the decomposition.
Second, note that we have two dictionaries, one for the words and one for the contexts. These dictionaries correspond to the modes of the ma-trix; we have one dictionary matrix per mode, and we always multiply the dictionary along its corre-sponding mode. It would be easy to extend this method to a setting where the data is a 3-mode tensor instead of a matrix, e.g., if the data were (subject, verb, object) triples, or relation triples in some knowledge base. In these settings, the dictio-naries used for each mode might be more different; in the subject-verb-object example, one of the dic-tionaries would only have verbs, while the other two would only have nouns, for instance. Stan-dard tensor decompositions could be augmented with a translation-invariance term, similar to what we have done with matrices in this work. The most closely related work is that of Faruqui and Dyer (2014b), whose CCA-based method we have already mentioned; however, it is not obvi-ous how CCA-based methods can be applied to more than two languages at a time. Our work is also similar to prior work on multilingual latent semantic analysis; Bader and Chew (2008) also include a translation dictionary when decompos-ing the X matrix, though their formulation uses a term-document matrix instead of a word-context matrix, and the way they use the translation dic-tionary is quite different. We have presented a new technique for gener-ating word embeddings from multilingual cor-pora. This technique formalizes the notion of translation invariance into the objective function of the matrix decomposition and provides flexi-ble and scalable means for obtaining word vec-tors where words that are translations of each other are close in the learned vector space. Through three separate evaluations, we showed that our technique gives superior performance on multilin-gual tasks, matches prior work on monolingual tasks, and scales linearly in the size of the input data. The code and data used in this paper are available at https://sites.google.com/ a/umn.edu/huang663/ .
 Research was supported by the National Science Foundation Grant No. IIS-1247489, IIS-1247632, and a gift from Google. This work was also sup-ported in part by a fellowship to Kejun Huang from the University of Minnesota Informatics In-stitute. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the funding parties.

