 Similarity search methods based on hashing for effective and efficient cross-modal retrieval on large-scale multime-dia databases with massive text and images have attracted considerable attention. The core problem of cross-modal hashing is how to effectively construct correlation between multi-modal representations which are heterogeneous intrin-sically in the process of hash function learning. Analogous to Canonical Correlation Analysis (CCA), most existing cross-modal hash methods embed the heterogeneous data into a joint abstraction space by linear projections. However, these methods fail to bridge the sema ntic gap more effectively, and capture high-level latent semantic information which has been proved that it can lead to better performance for im-age retrieval. To address these challenges, in this paper, we propose a novel Latent Semantic Sparse Hashing (LSSH) to perform cross-modal similarity search by employing Sparse Coding and Matrix Factorization. In particular, LSSH uses Sparse Coding to capture the salient structures of images, and Matrix Factorization to learn the latent concepts from text. Then the learned latent semantic features are mapped to a joint abstraction space. Moreover, an iterative strategy is applied to derive optimal solutions efficiently, and it helps LSSH to explore the correlation between multi-modal repre-sentations efficiently and automatically. Finally, the unified hashcodes are generated through the high level abstraction space by quantization. Extensive experiments on three dif-ferent datasets highlight the advantage of our method under cross-modal scenarios and show that LSSH significantly out-performs several state-of-the-art methods.
 H.3.1 [ Information Storage and Retrieval ]: Content Analysis and Indexing; H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval Algorithms; Performance; Experimentation Hashing; Cross-Modal Retrieval; Heterogeneous Data Sources; Correlation; Sparse Coding; Matrix Factorization
Similarity or Nearest Neighbor (NN) search, a method of searching semantically related results from a collection of objects for a query, lays the foundation for many important applications, such as information retrieval, data mining, and computer vision. Hashing-based methods [10, 6], one of the most well-known Approximate Nearest Neighbor search (ANN) methods, has garnered considerable interest in recent years for their great efficiency gains in massive data. The goal of hashing is to learn binary-code representation for data while preserving the similarity structure in the original feature space. One of the most famous models, locality-sensitive hashing (LSH) [1], which employs random linear projections to map feature vectors to binary codes, is quite efficient in both space and time. However, LSH may lead to ineffective codes in practice because it is data-independent [34]. Several machine learning techniques are used to design more effective hashing to overcome this problem, such as Boosting algorithm, Restricted Boltzmann Machines, Man-ifold Learning, Supervised Learning, Kernel Learning and PCA, which respectively generate Parameter sensitive Hash-ing [26], Semantic Hashing [25], Spectral Hashing [30], Su-pervised Hashing [16], Kernelized Hashing [13] and PCA Hashing [29]. Moreover, several literatures take the quan-tization of Hamming space into account, and have achieved superior results, such as K-means Hashing [8], ITQ Hashing [7] and Double-Bit Hashing [12].

Most existing hashing methods can only be applied to unimodal data. However, with the fast growth of multimedia content on the Web, like Wikipedia, Flickr and Twitter, the cross-modal retrieval problem, returning similar results of all modals for a given query, have attracted increasing attention and more studies about it emerge. Taking Wikipedia as example, it contains images and text. When a query word or picture is given, the system should return both relevant articles and images. This is central to many applications of practical interest [23]. However, designing effective and efficient hashing methods over heterogeneous cross-modal datasets is still remaining as an open issue.
The core problem of cross-modal hash function learning (HFL) is how to construct correlation between multi-modal representations which are heterogeneous intrinsically in the process of HFL. Recently, a few studies designed new hash-ing techniques to index multi-modal data into a common Hamming space [33, 14, 11, 3, 36, 27]. As shown in Figure 2, analogous to Canonical Correlation Analysis (CCA) [9], these models find the linear projections to embed the hetero-geneous data into a joint abstraction space while maximizing the cross-correlation between images and text on a training set. Then a quantization rule is applied to map the ab-straction representations to binary hash codes. In complex situations, i.e. the semantic gap between multi-modal data (e.g. visual features and text features) is large, however, these models do not extract useful joint features because they fail to capture the common latent information. There-for, they fail to generate effective hashcodes when dealing with complex multi-modal data.

Prior works have shown that the model which combines semantic abstraction for both images and text with explicit modeling of cross-correlations in a joint space can achieve better results for cross-multimedia retrieval [19, 23, 24]. Motivated by this observation, we propose a novel Latent Semantic Sparse Hashing (LSSH) algorithm to learn binary codes for multimedia data sources with text and images. As illustrated in Figure 1. up, LSSH represents text and im-age features in a new latent semantic space respectively, in which the heterogeneous representations of the same topic will show more common properties [23, 24]. In fact, LSSH uses Sparse Coding (SC) to capture the salient structures (e.g. edges) of images, and Matrix Factorization (MF) to learn the latent concepts from text. Then the learned la-tent semantic features are mapped to a joint abstraction space. Furthermore, an iterative strategy is applied to de-rive optimal solutions, and it helps LSSH to explore the cross-correlation between multi-modal representations effi-ciently and automatically in the process of HFL. Finally, the unified hashcodes are generated from high level abstrac-tion space by quantization. The contributions of LSSH can be summarized as follows: 1. We propose a novel cross-modal hashing framework to 2. An iterative strategy is used to help LSSH explore the 3. Extensive experiments on three datasets highlight the
The rest of this paper is organized as follows. We for-mulate several related cross-modal hashing methods and Canonical Correlation Analysis (CCA) within the same frame-work in Section 2. Section 3 presents our proposed method. Section 4 provides extensive experimental validation on three datasets. The conclusions are given in Section 5.
In this section, we show that a variety of cross-modal methods (CMH), including CCA [9], Data Fusion Hashing (DFH) [3], and Cross-View Hashing (CVH) [14] can be for-mulated within the framework of correlation analysis where correlation is used as the objective function. Obviously, cor-relation of heterogeneous features is directly related to the empirical ANN performance for cross-modal retrieval tasks.
Consider random vector of the form ( x , y ) (i.e. image and text feature), and the given samples { ( x i , y i ) } n i project x respectively y onto directions w x and w y : then maximise the correlation between the two modalities which can be defined as follows: where  X  is a diagonal matrix whose diagonal entry  X  ii = 1 /n ,and E P [ f ( x , y )] denotes the weighted empirical expec-tation of the function f ( x , y ), which is computed by the following equation The optimization of (1) can be solved as a generalized eigen-value problem (GEV) : E
 X  [ yx T ] 0 where  X  is the generalized eigenvalue.
DFH [3] embeds the input data from two arbitrary spaces into the Hamming space in a supervised way. Given sam-ple pair ( x i , y i ) and similarity label s i  X  X  +1 ,  X  maximizing: where sign ( u )=  X  1if u&lt; 0, or 1 otherwise,  X  u  X  R is sign function. Discarding the sign function, Equation (3) is closely related to a simpler correlation function: the constraints are added to avoid trivial solutions, and  X  is a diagonal matrix whose the i -th diagonal entry equals 1 / |
S + | if s i = 1, or 0 otherwise, where S + = { ( x i , y +1 ,  X  i } . The definition of  X   X  is analogous. And formula (4) can be solved by Singular Value Decomposition (SVD).
CVH [14] maximises the weighted cumulative correlation: max s.t. w T x E  X  [ xx T ] w x =1 , w T y E  X  [ yy T ] w y =1 where W ij be the similarity between instances i and j , L = 2 L + D , D is a diagonal matrix such that D ii = i W ij and L = D  X  W is the Laplacian matrix. Anyway, formula (5) can be transform to a GVE problem [14]:  X  E E W [ yx T ]  X  E Actually, CCA can be viewed as a special case of CVH by setting W = I [14].

All aforementioned cross-modal models assume that het-erogeneous data can be embedded into a common abstrac-tion space directly. However, the assumption may not fit into real world scenarios, especially when the semantic gap Figure 2: Flowchart of LSSH and existing CMH methods, illustrated with toy data. Up) Existing CMH methods learn independent hash codes for each modal of instances. Bottom) LSSH, an inte-grated hashing method for cross-modal, represents image and text feature by unified hashcodes. between multi-modal data (e.g. visual features and text fea-tures) is large, it may reduce the accuracy of cross-modal similarity search significantly. Moreover, prior works have shown that the high-level latent semantic information can lead to better performance for image retrieval and bridge the semantic gap more efficiently [19, 23, 24]. Hence, the proposed LSSH constructs correlation between two modali-ties in latent semantic spaces.
In this section, we present a novel approach for cross-modal similarity search. We restrict the discussion to multi-modal instances consisting of images and text as they are the most common and important scene in real world.
Suppose that O = { o i } n i =1 is a set of multi-modal in-stances, which only consists of an image and its accom-panying text, i.e. o i =( x i , y i ), where x i  X  R m is the m -dimensional image descriptor, and y i  X  R d is the d -dimensional text feature (usually, m = d ). Given the code-words length k , the purpose of LSSH is to learn a integrated binary code which can bridge the semantic gap between het-erogeneous data (i.e. image and text features) effectively while preserving the intrinsic similar structure of instances. As illustrated in Figure 2, queries of any type would be mapped to a common Hamming space according to related learned hash functions, which makes LSSH deal with queries with partial missing modalities. Scanning over the hash ta-ble linearly, the system returns similar results of all modal-ities for the given mapped query. CMH is quite efficient for online similarity search task, since only bit XOR operations are applied when calculating Hamming distance between bi-nary codes. Moreover, compared with existing CMH, which learn independent hash codes for each modal of one instance, LSSH can cut down the online search time and the storage space of binary codes by half, while also promoting the re-trieval precision significantly.
Previous works have shown that the semantic modeling has at least two advantages for cross-modal retrieval [23, 24]. Firstly, it provides a high-level abstraction which can lead to substantially better performance for image retrieval. Secondly, the semantic spaces of heterogeneous data which describe the same instances are isomorphic. Motivated by these observations, LSSH constructs the cross-correlations in the latent semantic spaces.

As in Figure 1. up, we project the original image and text features to the latent semantic space respectively: where P I and P T denote the projections, and M and D is the dimension of S M I and S D T respectively. Then the isomorphic latent semantic features are mapped into a common high level abstraction space by linear projection, which is the simplest isomorphic function: where R I  X  R M  X  k and R T  X  R D  X  k . In order to construct cross-correlation between two modalities, we require image and text features of the same instance to be equal in A k At last, binary hashcodes are obtained by the non-linear quantization function: where H k is the k -dimension Hamming space. Several quan-tization algorithm has been proposed [17, 7, 12], however, this is not the focus of our research, hence, we simply regard the quantization function Q as a sign function. Image Sparse Coding has been popularly used as an effective image representation in many applications, such as image classification [31], face recognition [32], image denoising [5] and image restoration [20]. The standard sparse coding, describing each sample using only several ac-tive vectors of dictionary, has at least two advantages for image representation. Firstly, the natural images may gen-erally be described in terms of a small number of struc-tural primitives [22], and the sparsity constraint in function (7) allows the learned representation to capture the salient structures. Secondly, the over-complete dictionary provides sufficient descriptive power for low-level features. Based on these observations, we use the Sparse Coding to capture the salient structures of images in LSSH. Let X be a set of m -dimensional image descriptors, i.e. X =[ x 1 , ..., x R m  X  n , the standard space coding with 1 regularization is: where B  X  R m  X  M is the overcomplete basis set, i.e. M&gt;m , ||  X  || F denotes Frobenius norm, and  X &gt; 0 is the parameter to balance the reconstruction error and sparsity. Text Matrix Factorization, as one of the most successful tools for learning the concepts or latent topics from text, has a wide range of applications in text mining and information retrieval. Let Y be a set of d -dimensional text descriptors, i.e. Y =[ y 1 , ..., y n ]  X  R d  X  n , LSSH learns the latent con-cepts by matrix factorization: where U  X  R d  X  D , A  X  R D  X  n . In fact, each column vector U i captures the higher-level features of original data, and each column vector A  X  i is the D -dimensional representation in latent semantic space [4].
Let each bit of hashcodes represents a latent semantic con-cept in Matrix Factorization, i.e. D = k , then the formula (6) can be rewrite by left multiplication inverse of R T : where R = R  X  1 T R I is the linear projection. There is an intuitive interpretation about formula (9), that is a latent concept can be described by several salient structures from images. Moreover, we can approximate formula (9) by opti-mizing the cross-correlation: The overall objective function, combining the Sparse Cod-ing on image features O sc given in formula (7), the Matrix Factorization on text features O mf given in formula (8), and the cross-correlation between the latent semantic spaces O given in formula (10), is written as below: where  X &gt; 0 leverages the discrimination power of images and text latent representations,  X &gt; 0 controls the linear connection of latent semantic spaces, and  X   X  1 is typically applied to avoid trivial solutions.
The optimization problem (11) is non-convex with five matrices variables B , A , R , U , S . Fortunately, it is convex with respect to any one of the five variables while fixing the other four. Therefore, the optimization problem can be solved by an iterative framework with the following listed steps until convergency.

Step1: Learn sparse representations S by fixing others variables, the problem (11) becomes We solve the problem (12) by using SLEP (Sparse Learning with Efficient Projections) package 1 .

Step2: Learn latent semantic concepts A by fixing others variables, the problem (11) becomes By taking the derivative of formula (13) with respect to A and setting it to 0, we can obtain the close form solution: where I is the identity matrix.

Step3: Learn B , R , U respectively using the Lagrange dual [15]. In fact, the learning problem of B , R , U is essen-tially same, hence we only show how to optimize B . Fixing others variables, the problem (11) becomes the least squares problem with quadratic constraints: http://parnec.nuaa.edu.cn/jliu/largeScaleSparseLearning.htm Algorithm 1 Latent Semantic Sparse Hashing Input: Output: 1: Initialize U , A , R and B by random matrices respec-2: repeat 3: Fix U , R , B and A ,update S as illustrated in Step1; 4: Fix U , R , B and S ,update A by Equation (14); 5: Fix U , R , A and S ,update B as illustrated in Step3; 6: Fix U , B , A and S ,update R by optimizing: 7: Fix R , B , A and S ,update U by optimizing: 8: until convergency. 9: H = sign ( A ).
 Consider the Lagrangian: where  X  i &gt; 0 is the Lagrange multipliers. Letting the deriva-tive of (16) with respect to B equal to zero, the close form solution of (15) can be obtained by where  X  is diagonal matrix whose diagonal entry  X  ii =  X  i and is got by optimizing following Lagrange dual problem min  X  Tr ( XS where Tr (  X  ) denotes the trace of matrix, i.e. the sum of diagonal. Problem (18) can be solved by using Newtons method or conjugate gradient. The algorithm is summarized in Algorithm 1.
In practice, the components of a new query can be quite diverse, now we discuss it in the following three situations.
Image only .Wedenote x as original image feature of the query, and then obtain the sparse coding by solving where dictionary B is given by Algorithm 1. Let s  X  be the optimal solution, then the hash codes h = sign ( Rs  X  ).
Text only .Wedenote y as original text feature of the query, and the close form matrix factorization factor is In most cases, U is full column rank, then ( U T U )  X  1 ex-ists. Otherwise, we may approximate a  X  by a  X  =( U T U +
I )  X  1 ( U T y ), where &gt; 0 is a small real number (e.g. 0.001), then we can get the hash codes h = sign ( a  X  ).

Both Text and Image . Wecanusethesamewayto get hash codes described in Image only and Text only . Moreover, a  X  also can be obtained according formula (14), which uses both image and text information, and then h = sign ( a  X  ). We investigate the performance of cross-modal retrieval for these diverse queries in Section 4.2.6.
In this section, we will show that LSSH is available for large-scale datesets. The time consuming for training LSSH includes sparse coding learning, latent semantic concepts learning, and Lagrange dual learning. Typically, solving (12) and (13) requires O ( nM 2 ) 2 and O ( d 3 )respectively. TheLa-grange dual (18), which is independent of sample size n ,can be solved by using Newtons method or conjugate gradient, which has been shown more efficient than gradient descent [15]. In a word, the total time complexity of training LSSH is linear to n , which is really scalable for large-scale datesets compared with most existing cross-modal hashing methods.
We conduct experiments on three real-world datasets for cross-modal similarity search to verify the effectiveness of LSSH. Specifically, datasets involved in our experiments con-sist of text and images, and we use text as query to search similar images and image as query to search similar texts. Furthermore, we analysis the parameter sensitivity, check the convergence property of Algorithm 1 and the influence of different query type to the similarity search performance.
Wiki 3 .The Wiki dataset was collected from Wikipedia consisting of 2,866 multimedia documents. Each document contains 1 image and at least 70 words. Each image is rep-resented by a 128-dimension SIFT [18] histogram and each text is represented by a 10-dimension topics X  vector gener-ated by latent Dirichlet allocation (LDA) model [2]. Totally 10 categories are considered in this dataset and each doc-ument (image-text pair) is labeled by one of them. Docu-ments are considered to be similar if they belong to the same category.
 LabelMe 4 . The LabelMe dataset is created by MIT Computer Science and Artificial Intelligence Laboratory which is made up of 2688 images. Each image is annotated by sev-eral tags which denote the objects in this image, such as  X  X ea X  and  X  X each X . Tags occurs in less than 3 images are discarded and 245 unique tags remain. This dataset is di-vided to 8 unique outdoor scenes such as  X  X oast X ,  X  X orest X  and  X  X ighway X  and each image belongs to one scene. Each image is represented by a 512-dimension GIST [21] feature and each text is represented by an index vector of selected tags. Image-text pairs are regarded as similar if they share the same scene label.

NUS-WIDE 5 . The NUS-WIDE dataset is a real-word image dataset created by Lab for Media Search in National University of Singapore [28]. This dataset contains 81 con-cepts but some are scarce. So we select 10 most common
The complexity of lasso algorithms is O ( nM 2 + M 3 ), but usually, n M . http://www.svcl.ucsd.edu/projects/crossmodal/ http://people.csail.mit.edu/torralba/code/spatialenvelope/ http://lms.comp.nus.edu.sg/research/NUS-WIDE.htm concepts, and thus 186,577 images are left from 269,648 im-ages. Furthermore, we select 1000 most frequent tags from 5,018 unique tags in this dataset. Each image is represented by a 500-dimension SIFT histogram and each text is repre-sented by an index vector of selected tags. Each image-text pair is annotated by at least 1 of 10 concepts. Pairs are considered to be similar if they share at least one concept.
Our method is compared against four state-of-the-art hash-ing methods for cross-modal similarity search as below.
CVH, IMH and DFH generates different hash codes for each modal of an instance, i.e., the hash codes of differ-ent modals of an instance is different, but these methods try to make these codes more similar for one instance. In our experiment, they will generate different hash codes for image and text separately of an instance(image-text pair). When a new image(text) query comes, they first generate its hash codes, and then search similar data from text(image) database. CHMIS generates unified hash codes for an in-stance combining all modals. However, if any one modal of an instance is unavailable, it can X  X  generate hash codes for this instance, which is too demanding for real-world scenar-ios. This method improves search accuracy by combining multiple information sources of one instance, and actually is not implemented for cross-modal similarity search. Yet we still compare LSSH to CHMIS to verify the ability of LSSH to promote search performance by merging knowledge from heterogeneous data sources.
We adopt mean Average Precision (mAP) as the evalua-tion metric for effectiveness in our experiment. This eval-uation metric has been widely used in literatures [27][35]. mAP has shown especially good discriminative power and stability to evaluate the performance of similarity search. A larger mAP indicates better performance that similar in-stances have high rank. Given a query and a set of R re-trieved instances, the Average Precision (AP) is defined as where L is the number of relevant instances in retrieved set, P ( r ) denotes the precision of top r retrieved instances
We implemented it ourselves because the code is not pub-licly available.
The source code is kindly provided by the authors. which is defined as the ratio between the number of relevant instance and the number of retrieved instance r ,and  X  ( r )is a indicator function which equals to 1 if the r th instance is relevant to query or 0 otherwise. Then the AP of all queries are averaged to obtain the mAP.

Furthermore, we also report two types of performance curves. One is precision-recal l curve which shows the pre-cision at different recall level. The other is topN-precision curve which reflects the change of precision with respect to the number of retrieved instances.
The experiments are carried out as follows. For image data, we first apply Principle Component Analysis (PCA) to reduce the feature dimension to 64, which can also remove noise from image data. Then, the length of sparse codes, i.e., the size of dictionary B , is set to 512, and the sparsity pa-rameter  X  is set to 0.2. LSSH has two model parameters,  X  which leverages the discrimination power between images and texts, and  X  which controls the linear connection of la-tent semantic spaces. In the coming sections, we provide empirical analysis on parameter sensitivity, which verifies that LSSH can achieve stable and superior performance un-der a wide range of parameter values. When comparing with baseline methods, we use the following parameter set-tings for all experiments:  X  =  X  =1,whichshowsgood performance on all three datasets. For baseline methods, we carefully tune the parameters for them and report the best results of them.

Furthermore, considering IMH and CHMIS requires too much resource to learn hash functions on NUS-WIDE with all data. We select randomly 10 , 000 instances for all meth-ods to train hash functions and then they are applied to the other instances in database to generate hash codes for them as in [27]. Moreover, hashing methods should have the abil-ity to handle out-of-sample instances since data is keeping coming into database as time goes by in real world. So we simulate the situation by this experiment setting. For LSSH, hash codes for instances in database is generated as follows. We first generate sparse codes S for images by (7). Then we use (14) to generate unified codes A for all instances combin-ing both images and texts. Finally, we apply sign function on A to obtain hash codes. And for query instances which only have one modal, i.e., image or text, we use methods introduced in 3.6 to generate hash codes. Moreover, we set R = 100, and all of the results are averaged over 10 runs. All the experiments are conducted on a computer which has Intel Xeon E5520 2.27GHz CPU, 16GB RAM.
We select 75% of the data as database and the rest as the query set. The mAP of LSSH and baseline methods are shown in Table 1., and the performance curves are shown in Figure 3. We can observe that LSSH can significantly outperform baseline methods on both cross-modal similarity search tasks which verifies the effectiveness of LSSH. The semantic gap between two views of Wiki is quite large. In fact, text can better describe the topic of the image-text pair than image. When a image query comes, since it X  X  not quite related to it X  X  topic, it X  X  difficult to find semantically similar texts. So the mAP of image query is low for all meth-ods. Even so, LSSH can achieve best performance, especially with long hash codes. LSSH can reduce the semantic gap between modals in database which makes the hash codes of images quite related to the topics of instances. Actually, the hash codes for images are also for the instances. So when a text query which is highly related to its topic comes, it can obtain semantically similar images of it. But the hash codes of images generated by baseline methods still show little rel-evance to their topics. That X  X  why LSSH can improve mAP by 18% at least which also shows the importance to reduce semantic gap between different modals.

We further observed that the PR-curve of several methods looks strange, e.g. the PR-curve of CVH for text query to image database at 64 bits shows that it behave like random guess in experiments. This phenomenon also happened in [35] and [36] and a reasonable explanation is given by [29]. Actually, all baseline methods are solved by eigenvalue de-composition and have orthogonality constraints on each bit so that each bit shows no correlation to each other. The first few projection directions may have high variance and their corresponding hash bits can be quite discriminative, which is quite useful to similarity search. However, as the code length increases, the hash codes will be dominated by bits with very low variance. Actually, since the variance is too low, the lower bits are meaningless and ambiguous. So these indiscriminative hash bits may lead the method to make random guess in experiments. 75% of the data are chosen as the database and the re-maining to form the query set. The mAP of LSSH and base-line methods are shown in Table 1. and Figure 4. shows the performance curves of them. LSSH shows more supe-rior performance than baseline methods with different code length. Furthermore, the images and texts of LabelMe are quite related to each other, thus LSSH can learn more effec-tive hash functions to increase similarity search performance by merging knowledge from heterogeneous data. Moreover, the mAP of image query is even higher than text query, this also shows the power of Sparse Coding to capture high-level semantic information of images.

We can also observe that as code length increase, LSSH performs better because LSSH can learn more precise de-scriptions for instances with more latent concepts and longer codes can encode more information. However, the perfor-mance of baseline methods degrades significantly as the in-crease of code length. This phenomenon has also been ob-served in [29] and [17]. The main reason is that the baseline methods are spectral-hashing-based methods which have or-thogonality constraints on the projection directions to make each hash bits uncorrelated to each other. However, these orthogonality constraints sometimes lead to practical prob-lem. It is well known that for most real-world datasets, most of the variance is contained in top few projections. With longer codes, these constraints will force them to progres-sively pick directions with low variance, which may reduce the quality and discriminative power of hash codes [29].
We select 2% of the data as the query set and the rest as the database. As mentioned above, we select 10 , 000 in-stances from databases randomly as the training set to learn hash functions and then they are applied to other instances in database to generate hash codes. The mAP and per-formance curves are shown in Table 1. and Figure 5. re-spectively. Similar to results above, we observe that LSSH outperforms baseline methods significantly and it performs better with longer hash codes.

In real-world applications, the size of database can be so large that it X  X  impossible to learn hash functions on the whole database because of the limitation of computational resources. And new data is keep coming into database as time goes by and hash codes for them need to be computed. So the hashing methods should be able to deal with out-of-sample instances (other instances in database and new coming instances). The common solution is to learn hash functions which project new data to a feature space and then quantize new features to binary hash codes by sign operation. The ability to deal with out-of-sample instances can test the effectiveness of hash functions which can further judge the ability of hashing methods to apply to practical problems. The experiment settings on NUS-WIDE is quite similar to real-world scenario. The experiment results show that LSSH can deal with out-of-sample instances easily and it has superior ability to handle large-scale database.
We conduct empirical analysis on parameter sensitivity on all datasets, which validates that LSSH can achieve stable and superior performance under a wide range of parameter values, verifying that LSSH is robust to parameters.
When analyzing one parameter, we keep other parameters fixed to the settings mentioned in Section 4.1.4. Due to the limit of space, we only present the results at 64 bits on all datasets in Figure 6. The dashed lines are the best performance of baselines with all experiment settings, e.g. the red dashed line in the first figure shows the result of DFH at 16 bits, which, as be observed from Table 1., is the best result of all baselines varying code length for  X  X mage to Text X  task. We can observe that LSSH can outperform all best results of baselines when  X   X  [0 . 05 , 10] and  X   X  [0 . 005 , 10]. The parameter  X  leverages the power of images and texts. Actually, utilizing the information from both modals can lead to better results. When  X  is too small, e.g.,  X &lt; 0 . 05, our model just focuses on images while ignoring texts. When  X  is too large, e.g.,  X &gt; 10, our model prefers information from texts. Specifically, it X  X  easy to choose a proper value for  X  because we can observe that LSSH shows stable and superior performance when  X   X  [0 . 05 , 10].

The parameter  X  controls the connection of latent seman-tic spaces. If  X  is too small, the connection between dif-ferent modals is weak with imprecise projection in formula (10), which will lead to poor performance for cross-modal similarity search. However, if  X  is too large, the strong con-nection will make the learning of latent representations of images and texts, i.e., Sparse Coding and Matrix Factoriza-tion, to be quite imprecise. Because images and texts are represented by imprecise features, it X  X  reasonable the the performance will degrade. Fortunately, it X  X  also effortless to choose proper  X  from the range [0 . 005 , 10];
Since LSSH is solved by an iterative procedure, we empir-ically check its convergency property. Figure 7. shows that the value of objective function (averaged by the number of training data) can decrease steadily with more iterations and it can converge with 20 iterations on all datasets at 64 bits, which validates the effectiveness of Algorithm 1. The results at other code length are similar to at 64 bits.

Furthermore, the average time cost for each iteration at 64 bits on Wiki, LabelMe and NUS-WIDE is 3 . 98 seconds, 3 . 65 seconds and 12 . 73 seconds respectively. So we can see that LSSHcanbesolvedwith10 , 000 training data in less than 5 minutes, which shows the high efficiency of Algorithm 1.
As mentioned in Section 3.6, a coming query instance can be quite diverse, e.g., it may consists of a single image, or a single text or both. Here we test the influence of query type on the similarity search performance. When an image query comes, we use (7) to generate its codes while (20) is applied to text query. And as mentioned above, we use (7) and (14) to generate codes for instances with both image and text. Figure 7. shows the mAP results on three datasets of different query type varying code length from 16 to 128.
We can observe that query with both modals can slightly outperform the query with partial missing modal. This is reasonable because query with both modals contains more information and the hash codes for it can be more semanti-cally informative, which leads to better performance.
Furthermore, the performance gap between image query and text query differs from datasets. In fact, for all datasets, texts belonging to one category show much similarity, so text query works well. The images in LabelMe belonging to one category, such as  X  X ighway X , are quite similar to each other, that X  X  why image query in LabelMe can achieve supe-rior performance. But the images in Wiki are quite diverse. For example, images belonging to  X  X istory X  can be related to a building, a man or a weapon. So it X  X  difficult to find semantically similar instances for an image query.
In this paper, we propose a novel hashing method, referred to as Latent Semantic Sparse Hashing, for large-scale cross-modal similarity search between images and texts. Specifi-cally, we utilizes Sparse Coding to capture high level salient structures of images, and Matrix Factorization to extract latent concepts from texts. Then these high level seman-tic features are mapped to a joint abstraction space. The search performance can be promoted by merging multiple comprehensive latent semantic descriptions from heteroge-neous data. We propose an iterative strategy which is highly efficient to explore the correlation between multi-modal rep-resentations and bridge the semantic gap between heteroge-neous data in latent semantic space.

We conduct extensive experiments on three multi-modal datasets consisting of images and texts. Superior and stable performances of LSSH verifies the effectiveness of it com-pared against several state-of-the-art cross-modal hashing methods. With longer hash codes, LSSH can conduct matrix factorization more accurately and encode more information, which leads to better performance, while the baseline meth-ods perform worse with longer hash codes because of the or-thogonality constraints on their objective function. Experi-ments on NUS-WIDE, which is a large-scale datasets, show that LSSH can deal with out-of-sample easily and has the ability to handle large-scale database. The analysis on pa-rameter sensitivity shows that LSSH is very robust to model parameters which can achieve stable and superior perfor-mance under a wide range of parameter values. Our conver-gence study shows the proposed learning algorithm is indeed effective and can be solved efficiently. And the study on query diversity shows the influence of different query types on the search performance and combining information from multiple source can help increase search performance.
This research was supported by the National Basic Re-search Project of China (Grant No. 2011CB70700), the Na-tional HeGaoJi Key Project (No. 2013ZX01039-002-002), and the National Natural Science Foundation of China (Grant No. 61271394). And the authors would like to thank the re-viewers for their valuable comments. [1] A. Andoni and P. Indyk. Near-optimal hashing [2] D. M. Blei, A. Y. Ng, and M. I. Jordan. Latent [3] M.M.Bronstein,A.M.Bronstein,F.Michel,and [4] S. C. Deerwester, S. T. Dumais, T. K. Landauer, [5] M. Elad and M. Aharon. Image denoising via sparse [6] A. Gionis, P. Indyk, R. Motwani, et al. Similarity [7] Y. Gong and S. Lazebnik. Iterative quantization: A [8] K. He, F. Wen, and J. Sun. K-means hashing: an [9] H. Hotelling. Relations between two sets of variates. [10] P. Indyk and R. Motwani. Approximate nearest [11] S. Kim, Y. Kang, and S. Choi. Sequential spectral [12] W. Kong and W.-J. Li. Double-bit quantization for [13] B. Kulis and K. Grauman. Kernelized [14] S. Kumar and R. Udupa. Learning hash functions for [15] H. Lee, A. Battle, R. Raina, and A. Ng. Efficient [16] W. Liu, J. Wang, R. Ji, Y.-G. Jiang, and S.-F. Chang. [17] W. Liu, J. Wang, S. Kumar, and S. F. Chang.
 [18] D. G. Lowe. Distinctive image features from [19] Z. Lu and Y. Peng. Latent semantic learning by [20] J. Mairal, M. Elad, and G. Sapiro. Sparse [21] A. Oliva and T.Torralba. Modeling the shape of the [22] B. A. Olshausen and D. J. Field. Sparse coding with [23] N. Rasiwasia, J. Costa Pereira, E. Coviello, G. Doyle, [24] N. Rasiwasia, P. J. Moreno, and N. Vasconcelos. [25] R. Salakhutdinov and G. Hinton. Semantic hashing. [26] G. Shakhnarovich, P. Viola, and T. Darrell. Fast pose [27] J. Song, Y. Yang, Y. Yang, Z. Huang, and H. T. Shen. [28] R. H. H. L. Z. L. T. Chua, J. Tang and Y. Zheng. [29] J. Wang, S. Kumar, and S.-F. Chang. Semi-supervised [30] Y. Weiss, A. Torralba, and R. Fergus. Spectral [31] J. Yang, K. Yu, Y. Gong, and T. Huang. Linear [32] M. Yang, L. Zhang, J. Yang, and D. Zhang. Robust [33] D. Zhang, F. Wang, and L. Si. Composite hashing [34] D. Zhang, J. Wang, D. Cai, and J. Lu. Self-taught [35] Y. Zhen and D. Yang. A probabilistic model for [36] Y. Zhen and D.-Y. Yeung. Co-regularized hashing for
