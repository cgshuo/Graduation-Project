 H.3.1 [ Information Storage and Retrieval ]: Content Analysis and Indexing; Linguistic processing Keywords: Question answering, fine-grained named entity recognition, relation extraction. In most QA systems, sentences or passages that are regarded as the most relevant to the question are extracted and then answers are retrieved by using NLP tec hniques (especially named entity recognition and syntactic pattern). Recent advances have brought some systems to within 90% accuracy when classifying named entities into broad categories, such as person, organization, and location. But more finely grained named entity recognition and advantages in QA [1, 2, 3, 7]. In most researches have gone into the coarse categorization of named entities and the syntactic rela tion pattern extraction, we are not aware of much previous work using machine learning algorithms to perform fine-grain ed NER and relation extraction between these fine-grained named entities. Fleischman and Hovy describe a method for automatically classifying person instances into eight finer-grained subcategories [1]. They use a supervised learni ng method. But a training data is highly skewed, because they use a simple bootstrapping method to generate the training data automatically. And they classify only person and location, but we need to classify all kinds of NE types. Mann explores the idea of a fine-grained proper noun ontology and its use in QA [2]. He builds a proper noun ontology from unrestricted text. The disadvantag e of this method is that its coverage is small, because he uses simple textual co-occurrence patterns. Agichtein and et al. show that a relatively small number of binary relationships account for the most of the queries in the sample [3]. classes) and do not show the relation extraction method. Ravichandran and Hovy explore the surface text patterns for QA bootstrapping method. But they do not cope with various surface expressions of the sentence, because of the word ordering and long distance dependencies. In this paper, we describe a fine-grained NER and relation extraction between fine-grained named entities and show the effect on QA. We define 147 fine-grained NE ty pes in consideration of user X  X  asking points for finding answer candidates of a QA system. They have 15 top levels and each top node consists of 2~4 layers. The organization, location, civilization , data, time, quantity, event, animal, plant, material , and term . In this case, Conditional Random Fields (CRFs) can not be applied directly, because CRFs which have many classes are too time consuming. To solve this problem, we break down the NER task in two parts; boundary det ection and NE classification. such as a sequence of words of sentences in a document. Let y = &lt;y with a NE type and a boundary (i.e. B and I). We define the conditional probability of a state sequence y given an input sequence x as follows: ...c T &gt; is a set of NE class states. We define the conditional probability of a boundary state sequence b given an input sequence x using CRFs as follows: 
P Higher  X  weights make their corre sponding FSM translations more likely [4]. We calculate the conditional probability of a NE class c NE (b i , x i ) extracted by boundary detector as follows: The primary advantage of CRFs over HMM is their conditional nature, resulting in the relaxati on of independence assumptions required by HMMs. Additionally , CRFs avoid the label bias problem, a weakness exhibited by Maximum Entropy Markov Models (MEMMs). CRFs outperform both MEMMs and HMMs on a number of real-world sequence labeling tasks [4]. We define 37 binary relationshi ps between fine-grained named entities in consideration of user X  X  asking points. Table 1 shows the 10 most frequent relations obs erved in our training set. associated with two named entities (i.e. y i and y input sequence x as follows: given an input sequence x as defined by Equation 1. We calculate the conditional probability of a relation state z given two named entities (y i , y j ) as follows: The experiments for fine-grain ed NER were performed on our Korean fine-grained NE data set. The data set consists of 6,000 documents tagged by human annotators. We used 5,500 documents as a training set and 500 documents as a test set, respectively. We trained the model by L-BFGS using our C++ implementation of CRFs and Maximum Entropy (ME). We use a Gaussian prior of 1 (for ME) and 10 (for CRFs). We perform 500 iterations for training. Table 2 shows the performance of boundary detection and fine-grained NER using CRFs and ME . In boundary detection, we obtained the performance, 83.9% F1 and 84.3% F1 using ME and CRFs respectively. In NER, our proposed model (2 stages: CRFs+ME) obtained 78.6% F1. The baseline model (1 stage: ME) obtained 77.7% F1. We also reduced the training time to 27% without loss of performance comp ared to the baseline model. The experiment for relation ex traction was performed on our Korean relation extraction data set which consists of 485 documents tagged by human annot ators. We used 455 documents (one-year IT news articles) as a training set and 30 documents (one-month IT news articles) as a test set, respectively. Table 3 shows the performance of the relation extraction for the 5 most frequent relations and the overall performance of 37 relations. To show the effect of the fine-grained NER and relation extraction, we performed anothe r experiment for a QA system. We modified ETRI QA Test Set [5 ], which consists of 402 pairs of question and answer in encycl opedia, as the IT news domain and used 50 factoid questions in the experiment. Table 4 shows the performance of QA systems. The QA system with fine-grained NER and re lation extraction achieved about 48% improvement over QA with coarse NER. In this paper, we describe a fine-grained NER and relation extraction using CRFs and ME for QA. Using the proposed approach, we obtained 78.6% F1 for 147 fined-grained NE types and 72.8% F1 for 37 relations. In the QA system, the performance with fined-grained NER and relation extraction archived about 33% improvement over QA with coarse NER. [1] M. Fleischman and E. Hovy. Fine grained classification of [2] G. Mann, Fine-Grained Proper Noun Ontologies for Question [3] E. Agichtein, S. Cucerzan, and E. Brill. Analysis of Factoid [4] S. Fei, F. Pereira. Shallow Parsing with Conditional Random [5] H. Kim, J. Wang, C. Lee, C. Lee, M. Jang. A LF based Answer [6] K. Han, H. Chung, S. Kim, Y. Song, J. Lee, H. Rim. Korea [7] D. Ravichandran and E. Hovy. Learning surface text patterns for 
