 Abhishek Kumar abhishek@cs.umd.edu Vikas Sindhwani vsindhw@us.ibm.com IBM T.J. Watson Research Center, Yorktown Heights, NY 10598 US A Prabhanjan Kambadur pkambadu@us.ibm.com IBM T.J. Watson Research Center,Yorktown Heights, NY 10598 USA A data matrix X of size m  X  n is said to admit a Non-negative Matrix Factorization (NMF) with inner-dimension r , if X can be expressed as X = WH where W , H are two non-negative matrices of dimensions m  X  r and r  X  n respectively. In many applications, a compact (i.e., small r ) approximate NMF tends to provide a natural and interpretable part-based decom-position of the data ( Lee &amp; Seung , 1999 ), which is ap-pealing in many applications, e.g., modeling topics in text and hyperspectral image analysis ( Cichocki et al. , 2009 ).
 Figure 1 shows the geometry of the NMF problem. As a point cloud in R m , all the n columns of X are contained inside a cone that is generated by r non-negative vectors in R m comprising the columns of W . For any matrix A , let cone ( A ) denote the set obtained by taking linear combinations of the columns of A with non-negative coefficients. Then, the goal is to find a non-negative matrix W , with just r columns, such that: cone ( X )  X  cone ( W )  X  R m + , where R m + denotes the non-negative orthant in R m . Such polyhe-dral nesting problems studied in computational geom-etry are known to be NP-hard, which makes the exact and approximate NMF problem also NP-hard ( Vava-sis , 2009 ). Faced with such results, almost the en-tire algorithmic focus in the NMF literature, e.g., Ci-chocki et al. ( 2009 ); Lee &amp; Seung ( 1999 ); Lin ( 2007 ); Hsieh &amp; Dhillon ( 2011 ), has centered on treating the problem as an instance of general non-convex pro-gramming, leading to heuristic procedures that lack optimality guarantees beyond convergence to a sta-tionary point of the objective function for approxi-mate NMF. Very recently, in a series of elegant pa-pers ( Arora et al. , 2012a ; b ; Bittorf et al. , 2012 ; Gillis &amp; Vavasis , 2012 ; Esser et al. , 2012 ; Elhamifar et al. , 2012 ), promising alternative approaches have been de-veloped based on certain separability assumption on the data which enables the NMF problem to be solved exactly. Geometrically, the assumption states the fol-lowing: all columns of X reside in a cone generated by a small subset of r columns of X . In algebraic terms, X = WH = X A H so that the r columns of W are hidden among the columns of X (indexed by an unknown subset of indices A ). Equivalently, a cor-responding subset of r columns of H happen to con-stitute the r  X  r identity matrix. We refer to these columns as anchors ( Arora et al. , 2012a ). Informally, in the context of topic modeling problems where X is a document-word matrix and W , H are document-topic and topic-term associations respectively, the sep-arability assumption equivalently posits the existence of special anchor words in the vocabulary, whose oc-curence uniquely identifies the presence of a topic, and whose usage across the corpus is collectively predictive of the usage of all the other words. The separability as-sumption was investigated earlier by Donoho &amp; Stod-den ( 2003 ) in the context of deriving uniqueness con-ditions for NMF. In order to place our contributions in the right context, we first briefly provide a flavor of recently proposed separable NMF algorithms.
 Related Work : Assuming that the columns of X are normalized to have unit l 1 -norm, the separable NMF problem reduces to that of finding the extreme points (that is, points inexpressible as convex com-binations of other points) of the convex hull of the columns ( Arora et al. , 2012a ). A Linear Program (LP) can be setup to attempt to express a given column as a convex combination of the other columns. If this LP declares infeasibility, an extreme point is identi-fied. This approach ( Arora et al. , 2012a , Section5) requires solving n feasibility LP X  X  each involving n  X  1 variables which is not scalable for many problems of interest. A noise-robust version of the procedure fur-ther requires knowledge of parameters that are hard to estimate apriori. Bittorf et al. ( 2012 ) formulate a single LP whose solution resolves the exactly separable NMF problem. An extension is also developed for noise-robustness. Instead of invoking a general LP solver, a specialized algorithm is derived based on an incremen-tal stochastic gradient descent procedure, and its par-allel (multithreaded) implementation is benchmarked on large datasets. On the other hand, this algorithm requires estimates of primal and dual step sizes, con-verges only asymptotically, and does not explicitly ex-ploit the sparsity of the final solution. Gillis &amp; Vavasis ( 2012 ) develop a highly scalable approach closely re-lated to rank-revealing QR factorizations for column subset selection. A perturbation analysis of this al-gorithm under noise is also presented. In Esser et al. ( 2012 ), column subset selection is cast essentially as a form of multivariate regression with row-sparsity in-ducing norms, e.g., see Bien et al. ( 2010 ). Algorithms derived in this framework are asymptotically conver-gent, and sensitive to near-duplicate columns, making it necessary to perform certain adhoc preprocessing steps.
 Contributions : We present a new family of highly scalable and empirically noise-robust algorithms for separable NMFs, with several favorable properties:  X  The algorithms produce a correct solution for the separable case after exactly r iterations. They re-quire no additional parameters. They are closely related to convex and conical hull finding proce-dures proposed in the computational geometry lit-erature ( Clarkson , 1994 ; Dula et al. , 1998 ). Com-putationally, the algorithms bear some resemb-lence to simultaneous Orthogonal Matching Pur-suit ( Buhlmann &amp; Geer , 2010 ; Tropp et al. , 2006 ) for sparse greedy reconstruction of multiple target variables from the same subset of input variables.
We also derive a variant based on this connection that performs quite well under noise.  X  Under controlled noise conditions in synthetic datasets and on real-world topic modeling problems, our algorithms consistently outperform other sepa-rable NMF techniques with respect to multiple per-formance metrics. Note, however, that a formal noise analysis in support of these empirical findings is not provided in the current paper. Our methods are also highly competitive with existing non-convex
NMF algorithms, but are free of sub-optimal local minima and associated initialization issues.  X  The solution for ( r  X  1) target anchors is contained in the solution for r target anchors (unlike non-convex
NMF methods), which makes it easier to do model selection on real-world datasets by keeping track of performance on a validation set.  X  The algorithms are highly scalable and have small memory footprint. The sparsity of the data, the intermediate variables and the final solution is care-fully exploited in a high-performance parallel and distributed implementation which scales excellently on both shared-and distributed-memory machines.
For example, a twitter corpus with 125-thousand tweets can be factorized for r = 100 in less than 10 seconds on a commodity 8-core machine.  X  Column normalization, as suggested in prior work, is not needed in our approach. Such normalization tends to interfere with the TFIDF weightings rou-tinely used in text modeling applications, leading to performance loss.  X  Unlike Esser et al. ( 2012 ), the algorithms do not require any special preprocessing to eliminate du-plicate or near-duplicate columns.
 An informal description : Figure 2 provides some geometric intuition underlying the proposed approach. The algorithm executes r iterations. In each iteration a new anchor column is identified. This corresponds to expanding a cone one extreme ray at a time, until the entire dataset is eventually contained in the cone defined by the full set of anchors. Figure 2 illustrates one step of the algorithm where there is an existing cone defined by three extreme rays (marked 1 to 3). To identify the next extreme ray, the algorithm picks a point outside the current cone (a green point) and projects it to the current cone to compute a residual vector (we call this the projection step ). This resid-ual vector separates the current cone from at least one non-selected extreme ray that can be found by maxi-mizing a specific selection criteria (we call this the de-tection step ). Intuitively, the algorithm picks a face of the current cone (spanned by rays 1 and 3 in Figure 2 ) that  X  X ees X  exterior points and rotates this face to-wards the exterior until it hits the  X  X ast X  point. In the example shown in Figure 2 , ray 4 is identified as a new extreme ray. These geometric intuitions are inspired by Clarkson ( 1994 ); Dula et al. ( 1998 ) who present LP-based algorithms for general convex and conical hull problems. Their algorithms are also directly applica-ble in our NMF setting, provided the data satisfies the separability assumption exactly. In this case, the residual of any single exterior point can be used to cor-rectly expand the cone as described above. However, anchor detection criteria derived from multiple resid-uals demonstrates radically superior noise robustness, as we report in the experimental section. The empha-sis on scalability and noise-robustness thus leads us to a new family of algorithms whose implementation (and associated proof of correctness) is distinct from prior work.
 Cones, Extreme Rays and Projection : Here, we provide a short background on relevant geometric con-cepts and set some notation. Recall that a cone C is a non-empty convex set that is closed with respect to taking conic combinations (i.e., linear combinations with non-negative coefficients) of its elements. A ray in C generated by a vector x 6 = 0  X  C is the set of all vectors { t x : t  X  0 } . A ray R is an extreme ray if its generators cannot be expressed by taking conic combinations of elements in C that do not themselves belong to R . A cone is called finitely generated if its elements are conic combinations of a finite set of vec-tors, and pointed if it does not contain both a vector x as well as its negation  X  x . A fundamental result (e.g., see ( Nemirovski , 2010 )) states: a pointed, finitely generated cone C possesses a finite and unique set of extreme rays, and C is the conical hull of the gener-ators of these extreme rays . Furthermore, the gener-ators of these extreme rays are a subset of the finite set of vectors used to originally express the cone. In the NMF context, note that any cone contained in R + is pointed. This implies that cone ( X ) can also be described by a minimally compact set of generators, i.e., cone ( X ) = cone ( X A ) where A uniquely indexes the extreme rays (anchors). Thus, a non-negative ma-trix X admits a separable NMF with inner-dimension r if the number of extreme rays of cone ( X ), i.e. size of A , coincides with r . A face of a cone is the in-tersection between the cone and a supporting hyper-plane. The projection of a point x onto the cone generated by columns of a matrix W , i.e. comput-ing z  X  = arg min z  X  cone ( W ) k x  X  z k 2 2 , can be obtained by solving a non-negative least squares problem, i.e., computing h  X  = arg min h  X  0 k x  X  W h k 2 2 and setting z  X  = W h  X  . All columns of X can be simultaneously projected by solving H  X  = arg min H  X  0 k X  X  WH k 2 2 . We will use the notation R to denote the residual ma-trix after a projection operation, i.e., R = X  X  WH  X  . We will use the notation X i , R i to denote the i th col-umn of X and its corresponding residual. The nota-tion q + will denote the vector obtained by setting all negative entries of the vector q to 0.
 Numerical Description : Algorithm 1 details the steps of the proposed family of algorithms which we call Xray . Each iteration consists of two steps: (i) a detection step that finds a column(s) of X to be added as an anchor, and (ii) a projection step where all data points are projected onto the current cone to get the residuals. Projection is done by solving simultaneous nonnegative least squares problem using Algorithm 2 . Every residual vector R i obtained after the projection step is normal to one of the faces of the current cone. In the selection step, we pick a face of the current cone (identified by its normal R i ), normalize all the data points to lie on the hyperplane p T x = 1 Y j = X j for a strictly positive vector p , and expand the current cone by selecting an extreme ray that maximizes the Algorithm 1 Xray : Algorithms for Separable NMF inner product R T i Y j . The selection step can be im-plemented in various ways -some options are listed in Algorithm 1 .
 To show that Xray correctly identifies all the extreme rays, we need the following lemmas.
 Lemma 2.1. The residual matrix R , obtained after projection of columns of X onto the current cone sat-isfies R T X A  X  0 , where X A are the extreme rays of the current cone.
 Proof. Residuals are given by R = X  X  X A H , where H = arg min B  X  0 k X  X  X A B k 2 F .
 Forming the Lagrangian for Eq. 6 , we get L ( B ,  X  ) = k X  X  X A B k 2 F  X  tr (  X  T B ), where the matrix  X  con-tains the nonnegative Lagrange multipliers. Differ-entiating w.r.t. B and evaluating at the optimum B = H , we have the following from the KKT con-ditions: 2 X T A ( X A H  X  X )  X   X  = 0  X   X  2 X T A R =  X   X  0  X  R T X A  X  0 Lemma 2.2. For any point X i exterior to the current cone, we have R T i X i &gt; 0 , where R i is the residual of X i obtained by projecting it onto the current cone. Proof. Let R = X  X  X A H , where H = arg min B  X  0 k X  X  X A B k 2 F and X A are the extreme rays of the current cone. From the KKT conditions (used in the proof of Lemma 2.1 ) we have 2 R T X A =  X   X  T , where  X  are the Lagrange multipliers. Hence, Algorithm 2 Solver for: arg min B  X  0 k X  X  X A B k 2 2 2 R T i X A =  X   X  T i ( i th row of both left and right side matrices). From the complementary slackness prop-erty, we have  X  ji H ji = 0  X  j, i . Hence, 2 R T i X A H  X   X  T i H i = 0.
 Hence we have R T i X i = R T i ( R i + X A H i ) = k R i R Using the above two lemmas, we prove the following theorem regarding the correctness of Algorithm 1 . Theorem 2.1. The data point X j  X  added at each it-eration in the Detection step of Algorithm 1 , if the maximizer in Eqn. 1 is unique, is an extreme ray of C that has not been selected in previous iterations. Proof. Let the index set A identify all the extreme rays of C . Under the separability assumption, we have X = X A H . Let the index set A t identify the extreme rays of the current cone C t .
 p is strictly positive, the inverse exists). Hence Y j = Y have p T Y j = 1 and p T Y A = 1 T . Hence, we have 1 = p T Y j = p T Y A C j = 1 T C j .
 Using Lemma 2.1 , Lemma 2.2 and the fact that p is strictly positive, we have max 1  X  j  X  n R T i Y j = R i Y j  X  0 using Lemma 2.1 and there is at least one j = i /  X  A t for which R t i Y j &gt; 0 using Lemma 2.2 . Hence the maximum lies in the set { j : j /  X  A t } . second inequality is the result of the fact that k C j k 1 = 1 and C j  X  0. This implies that if there is a unique maximum at a j  X  = arg max j /  X  A t R T i Y j , then X j  X  is generator of an extreme ray of the cone C . Remarks : (1) If the maximum occurs at two points j 1 and j extreme rays of the cone C . Hence both are added to anchor set A . If the maximum occurs at more than two points, some of these are the generators of the extreme rays of C and others are conic combinations of these generators. We can identify the extreme rays of this subset of points by calling Algorithm 1 recursively and add them to anchor set A . (2) Note that the algorithm is not influenced by presence of repeated anchors. (3) In the Algorithm, the vector p simply needs to satisfy p
T x i &gt; 0 , i = 1 . . . n . In our implementation, we sim-ply used p = [1 , . . . 1]  X  R m , i.e., p T x i = k x i k Note that unlike Gillis &amp; Vavasis ( 2012 ), we do not need X A to be full-rank.
 Exterior Point Selection : It can be noted that residual of any point exterior to the current cone (i.e., any R i 6 = 0) can be used in the selection step of Al-gorithm 1. This gives us multiple ways of expanding the current cone depending on which i is chosen -all of which solve the separable problem but may behave very differently in the presence of noise. Some natural options are listed in Algorithm 1: choosing a random exterior point (Eqn. 2 ), one with maximum residual norm (Eqn. 3 ) or one which defines a normal to a sup-porting hyperplane of the current cone which  X  X ees X  maximum  X  X ass X  of points in its positive halfspace, as measured by Eqn. 4 . In the experiments, we will refer to these variants as Xray (rand), Xray (max) and Xray (dist) respectively.
 A Greedy variation for noisy data : In high dimen-sional noisy data almost all the points may masquerade as anchors. A natural choice is to expand the current cone greedily by selecting a point that best minimizes the current residual, i.e., j  X  = arg min j min b &gt; 0 k R  X  X j b T k 2 F . This selection criterion simplifies to Eqn. 5 in Algorithm 1 (referred as Xray (greedy) henceforth). One may view this approach as implementing a non-negative variant of simultaneous orthogonal matching pursuit ( Tropp et al. , 2006 ), which is a greedy ap-proach to the problem of sparse regression of multi-ple response variables on the same subset of explana-tory variables, i.e., for solving min B  X  0 k X  X  XB k 2 F s.t. k B k 0 , 1 = r where k B k 0 , 1 pseudo-norm counts the number of non-zero rows in B . In the context of sep-arable NMF, both response variables and explanatory variables are the columns data matrix X . A relaxed version of this problem is solved in Esser et al. ( 2012 ) (min B  X  0 k X  X  XB k 2 F +  X  k B k 1 ,  X  ). It is also possible to have k B k 1 , 2 penalized variant ( Tropp , 2006 ; Bien et al. , 2010 ) which is natural for sparse multivariate regression problems. Note that the greedy approach is not guaranteed to solve the separable NMF prob-lem, but may perform well in the noisy settings as we observe in our experiments. Intuitively, this variant is concerned with greedily optimizing all residuals on av-erage at every iteration, instead of making a decision based on the residual of a single, albeit well-chosen, exterior point.
 Solution Refinement and Model Selection : In practice, the separable solution ( W , H ) as obtained from Algorithm 1 may be further refined with a few steps of alternating optimization with respect to a di-vergence measure of interest (e.g., Frobenius recon-struction k X  X  WH k 2 F ). Also, in real-world datasets, the value of r is typically unknown. Since our algo-rithms build the solution one anchor at a time, r can be set based on a performance measure evaluated on held-out data. Alternatively, Algorithm 1 can exit if the amount of improvement from introducing a new anchor falls below a prespecified threshold. Scalability and Parallelization Here we describe various implementation details that allow us to gracefully scale to large sparse datasets (e.g., document-term matrices). The detection step can be parallelized by scoring the candidate anchors simultaneously. Likewise, the projection step involves solving Eqn. 6 , which is separable in the columns of B and hence can be optimized in parallel.
 Detection Step : We avoid materializing the dense residual matrix R in the evaluation of the anchor se-lection criteria. Instead, we score candidate anchors on-the-fly as we compute (but not explicitly material-ize) a matrix Q = R T X C = X T X denotes a covariance matrix (word-by-word for topic modeling applications). Here, the potential sparsity, symmetry of the covariance matrix C as well as the non-negativity of H can be further exploited. For example, if C ij = 0, the corresponding entry in the product ( C A H ) need not be computed, since the resulting negative value is anyway reset to zero by the ( ) + thresholding operator. On a P core machine, the selection criteria may be evaluated in O ( nnz ( C ) r P ) time where nnz ( C ) is the number of non-zeros in C . If C is dense, we compute Q using parallel dense BLAS-3 operations. The one time computation of C is done via a parallel aggregation of rank-one outer-product terms defined by the rows of X .
 Projection Step : Algorithm 2 gives the steps of a cyclic block coordinate descent algorithm orga-nized around very light-weight incremental sparsity-exploiting updates for solving Eqn. 6 (derivation omit-ted for brevity). The algorithm can be invoked in par-allel on columns of X to compute the corresponding columns of B . The previous value of B is used to warm start the optimization and typically a very small num-ber of iterations is needed for convergence. Here, we report extensive comparisons on synthetic and medium-scale topic modeling problems, and benchmark our parallel implementation on large text datasets on multicore machines and distributed sys-tems. We compare with the methods proposed in Bit-torf et al. ( 2012 ) (abbrv. as Hottopixx ) and Gillis &amp; Vavasis ( 2012 ) (abbrv. as GV), as well as tra-ditional NMFs based on alternating optimization( Ci-chocki et al. , 2009 ). The source codes for Hottopixx and GV were taken from the respective authors X  web-sites. In comparisons with Esser et al. ( 2012 ), it was observed that it tends to select near-duplicate anchors, as also mentioned in Esser et al. ( 2012 ). This charac-teristic causes it to consistently perform less favorably compared to other methods unless the data is prepro-cessed in an adhoc fashion to remove similar columns of X ; hence we do not include it in our list of baselines. We also do not compare with Arora et al. ( 2012a ) since Hottopixx reportedly performs better ( Bittorf et al. , 2012 ) and the algorithm requires parameters which are hard to guess apriori. 3.1. Synthetic experiments We perform a synthetic experiment that injects con-trolled amount of noise to corrupt the separable struc-ture. Each entry of the matrix W  X  R 200  X  20 + is gen-erated i.i.d. according to a uniform distribution be-tween 0 and 1. The matrix H  X  R 20  X  210 + is taken to generated according to a Dirichlet distribution whose parameters are chosen uniformly in [0 , 1]. The data matrix X is set to WH + N where each entry of noise matrix N is generated i.i.d. according to a Gaussian distribution with zero mean and std. dev.  X  . Fig. 3 plots the fraction of correctly recovered anchors (aver-aged over 10 runs for each value of  X  ) against the noise level  X  ranging from 0 to 1 . 5. The proposed Xray (max) shows the best noise-robustness in terms of an-chor recovery, followed by Xray (dist) and GV. Al-though Xray (greedy) does not perfectly resolve the separable NMF problem (  X  = 0), it performs better than Hottopixx and is competitive with GV for near-separable case (  X  &gt; 0). As described below, on real datasets it turns out to be highly competitive. Xray (rand), although solves the separable problem (  X  = 0), degrades significantly under noise, which shows that proper selection of an exterior point to expand the current cone is crucial for noise-robustness. 3.2. Medium-scale Topic modeling problems We evaluate the proposed methods on three human-labeled text datasets that are commonly used in topic modeling literature: TDT-2 ( TDT2 ) ( m = 9394 , n = 19528 , r = 30), BBC ( Greene &amp; Cunningham , 2006 ) ( m = 2225 , n = 9635 , r = 5) and Reuters ( Reuters ) ( m = 7285 , n = 18221 , r = 10). We used standard tf-idf representation with document frequency thresh-olding in constructing the data matrix X . As required in Hottopixx and GV, we use  X  1 -normalized columns of X (referred as matrix X (  X  1 ) henceforth) to identify the anchor column indices A (  X  1 ) , and use the unnormal-ized data X (and the corresponding anchor columns X
A (  X  1 ) ) for classification and clustering tasks. The use of X (  X  1 ) A in clustering and classification (for any in-dex set A ) resulted in significantly worse performance uniformly for all methods so these results are not re-ported. For the sake of clarity in the figures, we do not show the results for Xray (max) which performed almost similar to Xray (dist) in these experiments. For an illustration of topics and anchor words recov-ered from these text datasets, the reader is referred to ( Kumar et al. , 2012 ).
 Classification experiments: Figure 4 shows the classification accuracy results obtained with the fea-tures (columns of the document-term matrix restricted to anchor words) selected by different methods on the three datasets. Black dotted line is the classifica-tion accuracy with full features (all the words). We use 5% of the documents for training and the rest 95% for testing to emulate a semi-supervised learning sce-nario where we view various methods as inducing a topical representation based on all (unlabeled) data. We use multiclass SVM classifier as implemented in LIBLINEAR ( Fan et al. , 2008 ) and use four-fold cross validation to select the parameter C . Among sepa-rable NMF techniques, the proposed Xray (greedy) and Xray (dist) (with exception on Reuters) outper-form Hottopixx and GV on all the three datasets, more so on TDT. On average, traditional NMFs with local optimization perform quite well on these datasets es-pecially when r is small, but can show significant per-formance variance (shown as error bars) with respect to initialization. As the number of topics increases, the performance gap between the proposed methods and the local optimization method rapidly diminishes. In this regime our techniques are a viable alternative to local optimization methods, and have the advan-tage of being local-minima-free, i.e., eliminating un-certainty with respect to initialization and therefore not requiring multiple runs.
 Clustering experiments : We also evaluated clus-tering performance by assigning a cluster label to each document based on the maximum element in the cor-responding row of W . We refine the solution with a few iterations of alternating optimization. Figure 5 shows the clustering performance in terms of Normal-ized Mutual Information (NMI) as these iterations pro-ceed. We also show the NMI obtained with local search method after it has converged to a local optimum (av-eraged NMI from ten runs with different random ini-tializations is shown; error-bar indicates the variation around the average). Again, the proposed Xray meth-ods are among the best performing methods in terms of clustering performance and do not require multiple runs as traditional NMFs do.
 Effect of column normalization: It can be seen from Table 1 that anchors selected using the un-normalized X (i.e., pure tf-idf features) achieve signif-icantly better classification accuracy than anchors se-lected using  X  1 -normalized version of X . These empir-ical results suggest that discarding the norm informa-tion by explicit  X  1 normalization can adversely affect the predictive quality of the selected anchors. Such normalization is not required by our approach. We advocate that previous algorithms, which ignore this issue, should be carefully modified accordingly. 3.3. Large-scale Experiments We implemented a shared-and distributed-memory parallel version of Xray in C ++ . That is, our im-plementation can exploit parallelism when running on multi-core machines, or on clusters of multi-core machines. For shared-memory parallelism, we use PFunc ( Kambadur et al. , 2009 ), a lightweight and portable library that provides C and C ++ APIs to ex-press task parallelism. For distributed-memory par-allelism, we use MPI 1 , a popular library specification for message-passing that is used extensively in high-performance computing.
 To test the shared-memory performance and scala-bility of Xray , we ran experiments on daniel , a dual-socket, quad-core Intel R Xeon TM X5570 ma-chine with 64GB of RAM running Linux Kernel 2.6.35-24 (total 8 cores). For compilation, we used GCC v4.4.5 with:  X  -O3 -fomit-frame-pointer -funroll-loops  X  in addition to PFunc 1.02, Open-MPI 1.4.5 and untuned ATLAS BLAS. We ran large-scale experiments on three datasets: RCV1 ( Lewis et al. , 2004 ), co-occurence matrix of people and places from ClueWeb09 dataset ( Lemur ), and IBM Twitter (IBMT) dataset. The statistics relating to these three large datasets are presented in Table 2 .We report scal-ability results for Xray (greedy) -other variants are computationally very similar.
 Figure 6 depicts the multi-threaded performance of our implementation on daniel while detecting 100 topics. Our implementation is able to factorize RCV1 in 409 seconds on 8 cores and achieve 4 . 2 x speedup over 8 threads when compared to the sequential implemen-tation. Similarly, for IBMT we achieve 4 . 5 x speedup, while completing the factorization in 9 . 8 seconds on 8 cores. For the dense X T X case, we are able to factor-ize PPL2 in 1147 seconds with just 8 cores. We believe that further speedup improvements can be demon-strated on these problems by (a) optimizing the data layout of various sparse matrices to alleviate memory contention amongst threads, and (b) in dense problems such as PPL2, by using a version of BLAS tuned to our architecture and by reorganizing our implementa-tion around more BLAS-3 operations that have better memory to compute ratio than BLAS-1 or 2 opera-tions. Our implementation showed good scalability on distributed-memory machines as well (details omitted for brevity).
 To compare our performance against the state-of-the-art Hottopixx algorithm ( Bittorf et al. , 2012 ), we ran their algorithm on daniel with the options  X  --dual 0.01 --epochs 10 --splits 8 --hott &lt;R&gt; --normse 1 --primal 1e-6  X  set in close con-sultation with the authors. A detailed comparison is shown in Table 3.2 . Note that a head-to-head compar-ison is difficult because of the different performance characteristics of Hottopixx and Xray . For example, Hottopixx  X  X  per-epoch runtime is not dependent on r , the number of topics, but it X  X  accuracy is dependent on E , the number of epochs, while our methods execute exactly r iterations, where each iteration has a superlinear dependence on r . Nonetheless, for all three datasets with r = 25 , 50 , 100, we see that Xray performs better than Hottopixx even when Hottopixx is run only for 5 epochs. In particular, for the sparse datasets IBMT and RCV1, in comparison to Hottopixx , Xray runs to completion in significantly shorter amount of time.
 Acknowledgments: We thank Victor Bittorf and Ben Recht for help with Hottopixx . Haim Avron, Christos Boutsidis, Ken Clarkson, Nicolas Gillis, Rick Lawrence and Ankur Moitra provided helpful feed-back. Research was sponsored by the U.S. Defense Advanced Research Projects Agency (DARPA) un-der the Social Media in Strategic Communication (SMISC) program, Agreement Number W911NF-12-C-0028. Abhishek Kumar was a SMISC summer intern at IBM Research when this work was done.
 Arora, Sanjeev, Ge, Rong, Kannan, Ravi, and Moitra,
Ankur. Computing a nonnegative matrix factoriza-tion  X  provably. In STOC , 2012a.
 Arora, Sanjeev, Ge, Rong, and Moitra, Ankur. Learn-ing topic models -going beyond svd. In FOCS , 2012b.
 Bien, J., Xu, Y., and Mahoney, M. CUR from a sparse optimization viewpoint. In NIPS , 2010.
 Bittorf, Victor, Recht, Benjamin, Re, Christopher, and Tropp, Joel A. Factoring nonnegative matri-ces with linear programs. In NIPS , 2012.
 Buhlmann, Peter and Geer, Sara Van De. Statistics for High Dimensional Data . Springer, 2010.
 Cichocki, A., Zdunek, R., Phan, A. H., and Amari, S. Non-negative Matrix and Tensor Factorizations . Wiley, 2009.
 Clarkson, K. More output-sensitive geometric algo-rithms. In FOCS , 1994.
 Donoho, D. and Stodden, V. When does non-negative matrix factorization give a correct decomposition into parts? In NIPS , 2003.
 Dula, J. H., Hegalson, R. V., and Venugopal, N. An algorithm for identifying the frame of a pointed fi-nite conical hull. INFORMS Jour. on Comp. , 10(3): 323 X 330, 1998.
 Elhamifar, Ehsan, Sapiro, Guillermo, and Vidal, Rene.
See all by looking at a few: Sparse modeling for finding representative objects. In CVPR , 2012. Esser, Ernie, Mller, Michael, Osher, Stanley, Sapiro,
Guillermo, and Xin, Jack. A convex model for non-negative matrix factorization and dimensionality re-duction on physical space. IEEE Transactions on Image Processing , 21(10):3239  X  3252, 2012. Fan, R.-E., Chang, K.-W., Hsieh, C.-J., Wang, X.-R., and Lin, C.-J. Liblinear: A library for large linear classification. JMLR , 2008.
 Gillis, Nicolas and Vavasis, Stephen A. Fast and ro-bust recursive algorithms for separable nonnegative matrix factorization. arXiv:1208.1237v2 , 2012. Greene, Derek and Cunningham, P  X adraig. Practical solutions to the problem of diagonal dominance in kernel document clustering. In ICML , 2006.
 Hsieh, C. J. and Dhillon, I. S. Fast coordinate descent methods with variable selection for non-negative matrix factorization. In KDD , 2011.
 Kambadur, P., Gupta, A., Ghoting, A., Avron, H., and Lumsdaine, A. PFunc: Modern Task Paral-lelism For Modern High Performance Computing. In ACM/IEEE conference on Supercomputing , 2009. Kumar, Abhishek, Sindhwani, Vikas, and Kambadur,
Prabhanjan. Fast conical hull algorithms for near-separable non-negative matrix factorization. arXiv:1210.1190 , 2012.
 Lee, D. and Seung, S. Learning the parts of objects by non-negative matrix factorization. Nature , 401 (6755):788 X 791, 1999.
 Lemur. http://lemurproject.org/clueweb09/ .
 Lewis, D, Yang, Y, Rose, T, and Li, F. RCV1: A new benchmark collection for text categorization re-search. Journal Of Machine Learning Research , 5: 361 X 397, 2004.
 Lin, C.-J. Projected gradient methods for non-negative matrix factorization. Neural Computation , 2007.
 Nemirovski, A. Lecture Notes: Introduction to Linear Optimization . 2010.
 Reuters. archive.ics.uci.edu/ml/datasets/ Reuters-21578+Text+Categorization+Collection . TDT2. http://www.itl.nist.gov/iad/mig/tests/ tdt/1998/ .
 Tropp, J. A. Algorithms for simultaneous sparse ap-proximation. part ii: Convex relaxation. Signal Pro-cessing , 86:589 X 602, 2006.
 Tropp, J. A., Gilbert, A. C., and Strauss, M. J. Algo-rithms for simultaneous sparse approximation. part i: Greedy pursuit. Signal Processing , 86:572 X 588, 2006.
 Vavasis, S. On the complexity of non-negative matrix factorization. SIAM Journal on Optimization , 20
