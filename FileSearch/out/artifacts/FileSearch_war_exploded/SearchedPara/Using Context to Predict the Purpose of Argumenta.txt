 Incorporating natural language processing into sys-tems that provide writing assistance beyond gram-mar is an area of increasing research and commer-cial interest (e.g., (Writelab, 2015; Roscoe et al., 2015)). As one example, the automatic recognition of the purpose of each of an author X  X  revisions allows writing assistance systems to provide better rewrit-ing suggestions. In this paper, we propose context-based methods to improve the automatic identifica-tion of revision purposes in student argumentative writing. Argumentation plays an important role in analyzing many types of writing such as persuasive essays (Stab et al., 2014), scientific papers (Teufel, 2000) and law documents (Palau and Moens, 2009). In student papers, identifying revision purposes with respect to argument structure has been used to pre-dict the grade improvement in the paper after revi-sion (Zhang and Litman, 2015).

Existing works on the analysis of writing revi-sions (Adler et al., 2011; Bronner and Monz, 2012; Daxenberger and Gurevych, 2013; Zhang and Lit-man, 2015) typically compare two versions of a text to extract revisions, then classify the purpose of each revision in isolation. That is, while limited con-textual features such as revision location have been utilized in prior work, such features are computed from the revision being classified but typically not its neighbors. In addition, ordinary classifiers rather than structured prediction models are typically used. To increase the role of context during prediction, in this paper we 1) introduce new contextual features (e.g., the impact of a revision on local text cohesion), and 2) transform revision purpose classification to a sequential labeling task to capture dependencies among revisions (as in Table 1). An experimental evaluation demonstrates the utility of our approach. There are multiple works on the classification of revisions (Adler et al., 2011; Javanmardi et al., 2011; Bronner and Monz, 2012; Daxenberger and Gurevych, 2013; Zhang and Litman, 2015). While different classification tasks were explored, similar approaches were taken by extracting features (lo-cation, text, meta-data, language) from the revised text to train a classification model (SVM, Random Forest, etc.) on the annotated data. One problem with prior works is that the contextual features used were typically shallow (location), while we cap-ture additional contextual information as text cohe-sion/coherence changes and revision dependencies.
As our task focuses on identifying the argumen-tative purpose of writing revisions, work in argu-ment mining is also relevant. In fact, many fea-tures for predicting argument structure (e.g., loca-tion, discourse connectives, punctuation) (Burstein and Marcu, 2003; Moens et al., 2007; Palau and Moens, 2009; Feng and Hirst, 2011) are also used in revision classification. In addition, Lawrence et al. (2014) use changes in topic to detect argumen-tation, which leads us to hypothesize that different types of argumentative revisions will have different impacts on text cohesion and coherence. Guo et al. (2011) and Park et al. (2015) both utilize Condi-tional Random Fields (CRFs) for identifying argu-mentative structures. While we focus on the differ-ent task of identifying revisions to argumentation, we similarly hypothesize that dependencies exist be-tween revisions and thus utilize CRFs in our task. While our task is similar to argument mining, a key difference is that the revisions do not always appear near each other. For example, a 5-paragraph long essay might have only two or three revisions located at different paragraphs. Thus, the types of previous revisions cannot always be used as the contextual information. Moreover, the type of the revision is not necessarily the argument type of its revised sen-tence. For example, a revision on the evidence argu-ment can be just a correction of spelling mistakes. Revision purposes. To label our data, we adapt the schema defined in (Zhang and Litman, 2015) as it can be reliably annotated and is argument-oriented. Sentences across paper drafts are aligned manually based on semantic similarity and re-vision purpose categories are labeled on aligned sentences. The schema includes four categories ( Claims/Ideas , Warrant/Reasoning/Backing , Rebut-tal/Reservation and Evidence ) based on Toulmin X  X  argumentation model (Toulmin, 2003), a General Content category for revisions that do not directly change the support/rebuttal of the claim (e.g. ad-dition of introductory materials, conclusions, etc.), and three categories ( Conventions , Clarity and Or-ganization ) based on the Surface categorizations in (Faigley and Witte, 1981). As we focus on argu-mentative changes, we merge all the Surface sub-categories into one Surface category. As Zhang and Litman (2015) reported that both Rebuttals and mul-tiple labels for a single revision were rare, we merge Rebuttal and Warrant into one Warrant category 1 and allow only a single (primary) label per revision.
Corpora. Our experiments use two corpora con-sisting of Drafts 1 and 2 of papers written by high school students taking AP-English courses; papers were revised after receiving and generating peer feedback. Corpus A was collected in our earlier pa-per (Zhang and Litman, 2015), although the origi-nal annotations were modified as described above. It contains 47 paper draft pairs about placing contem-poraries in Dante X  X  Inferno. Corpus B was collected in the same manor as A with agreement Kappa 0.69. It contains 63 paper draft pairs explaining the rhetor-ical strategies used by the speaker/author of a previ-ously read lecture/essay. Both corpora were double coded and gold standard labels were created upon agreement of two annotators. Two example anno-tated revisions from Corpus B are shown in Table 1, while the distribution of annotated revision purposes for both corpora are shown in Table 2. 4.1 Adding contextual features Our previous work (Zhang and Litman, 2015) used three types of features primarily from prior work (Adler et al., 2011; Bronner and Monz, 2012; Dax-enberger and Gurevych, 2013) for argumentative re-vision classification. Location features encode the location of the sentence in the paragraph and the lo-cation of the sentence X  X  paragraph in the essay. Tex-tual features encode revision operation, sentence length, edit distance between aligned sentences and the difference in sentence length and punctuation numbers. Language features encode part of speech (POS) unigrams and difference in POS tag counts.
We implement this feature set as the baseline as our tasks are similar, then propose two new types of contextual features. The first type ( Ext ) extends prior work by extracting the baseline features from not only the aligned sentence pair representing the revision in question, but also for the sentence pairs before and after the revision. The second type ( Coh ) measures the cohesion and coherence changes in a Utilizing the cohesion and coherence difference. Inspired by (Lee et al., 2015; Vaughan and McDon-ald, 1986), we hypothesize that different revisions can have different impacts on the cohesion and co-herence of the essay. We propose to extract fea-tures for both impact on cohesion (lexical) and im-pact on coherence (semantic). Inspired by (Hearst, 1997), sequences of blocks are created for sentences in both Draft 1 and Draft 2 as demonstrated in Fig-ure 1. Two types of features are extracted. The first type describes the cohesion and coherence be-tween the revised sentence and its adjacent sen-tences. The similarity (lexical/semantic) between the revised sentence block and the sentence block before ( Sim ( Block U p, Block U p Self )) and af-ter ( Sim ( Block Down, Block Down Self )) are calculated as the cohesion/coherence scores Coh Up and Coh Down. The features are extracted sep-second type describes the impact of sentence mod-ification on cohesion and coherence 4 . Features Change Up and Change Down are extracted as the division of the cohesion/coherence scores of two
A bag-of-word representation is generated for each sentence block after stop-word filtering and stemming. Jaccard similarity is used for the calcu-lation of lexical similarity between sentence blocks. Word embedding vectors (Mikolov et al., 2013) are used for the calculation of semantic similarity. A vector is calculated for each sentence block by sum-ming up the embedding vectors of words that are culated as the cosine similarity between the block vectors. This approach has been taken by multiple groups in the SemEval-2015 semantic similarity task (SemEval-2015 Task 1)(Xu et al., 2015). 4.2 Transforming to sequence labeling To capture dependencies among predicted revisions, we transform the revisions to a consecutive sequence and label it with Conditional Random Fields (CRFs) as demonstrated in Figure 2. For both drafts, sen-tences are sorted according to their order of occur-rence in the essay. Aligned sentences are put into the same row and each aligned pair of sentences is treated as a unit of revision. The  X  X ross-aligned X  are broken into deleted and added sentences (i.e, the cross-aligned sentences in Draft 1 are treated as deleted and the sentences in Draft 2 are treated as added.). After generating the sequence, each re-vision unit in the sequence is assigned the revision purpose label according to the annotations, with un-changed sentence pairs labeled as Nochange .
We conducted labeling on both essay-level and paragraph-level sequences. The essay-level treats the whole essay as a sequence segment while the paragraph-level treats each paragraph as a segment. After labeling, the label of each changed sentence Our prior work (Zhang and Litman, 2014) proposed an approach for the alignment of sentences. The ap-proach achieves 92% accuracy on both corpora. In this paper we focus on the prediction task and as-The first four columns of Table 3 show the perfor-mance of baseline features with and without our new contextual features using an SVM prediction using 10-fold (student) cross-validation with 300 For the SVM approach, we observe that the Coh features yield a significant improvement over the baseline features in Corpus B, and a non-significant improvement in Corpus A. This indicates that changes in text cohesion and coherence can in-deed improve the prediction of argumentative revi-sion types. The Ext feature set -which computes features for not only the revision but also its im-mediately adjacent sentences -also yields a slight (although not significant) improvement. However, adding the two feature sets together does not fur-ther improve the performance using the SVM model. The CRF approach almost always yields the best results for both corpora, with all such CRF results better than all other results. This indicates that de-pendencies exist among argumentative revisions that cannot be identified with traditional classification approaches. To have a better understanding of how the sequence labeling approach improves the classification perfor-mance, we counted the errors of the cross-validation results on Corpus A (where the revisions are more evenly distributed). Figure 3 demonstrates the com-
We notice that the CRF approach makes less er-rors than the SVM approach in recognizing Claim changes ( General-Claim, Evidence-Claim, Warrant-Claim, Surface-Claim ). This matches our intuition that there exists dependency between revisions on supporting materials and revisions on Claim . We also observe that same problems exist in both ap-proaches. The biggest difficulty is the differentia-tion between General and Warrant revisions, which counts 37.6% of the SVM errors and 40.1% of CRFs errors. It is also common that Claim and Evidence revisions are classified as Warrant revisions. Ap-proaches need to be designed for such cases to fur-ther improve the classification performance. In this paper we proposed different methods for uti-lizing contextual information when predicting the argumentative purpose of revisions in student writ-ing. Adding features that captured changes in text cohesion and coherence, as well as using sequence modeling to capture revision dependencies, both sig-nificantly improved predictive performance in an ex-perimental evaluation.

In the future, we plan to investigate whether per-formance can be further improved when more sen-tences in the context are included. Also, we plan to investigate whether revision dependencies exist in other types of corpora such as Wikipedia revisions. While the corpora used in this study cannot be pub-lished because of the lack of required IRB, we are starting a user study project (Zhang et al., 2016) on the application of our proposed techniques and will publish the data collected from this project. We would like to thank our annotators, especially Jiaoyang Li, who contributed significantly to the building of our corpus. We also want to thank the members of the SWoRD and ITSPOKE groups for their helpful feedback and all the anonymous reviewers for their suggestions. This research is funded by the Learning Research and Development Center of the University of Pittsburgh.
