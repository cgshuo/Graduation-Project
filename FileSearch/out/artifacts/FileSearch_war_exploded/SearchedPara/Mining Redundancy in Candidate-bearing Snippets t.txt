 Conventional question answering (QA) techniques indepen-dently process candidate-bearing snippets to select an ex-act answer to a question from candidate answers. This paper presents two novel way s of utilizing r edundancy in candidate-bearing snippets to help select an exact answer to a question in our Web QA system, i.e., cluster-based language model (CLM-M) and unsupervised SVM classifier (U-SVM) techniques. The comparative experiments demon-strate that the proposed methods significantly outperform the language model-based (LM-M) and supervised SVM-based (S-SVM) techniques th at do not utiliz e this redun-dancy in the candidate-bearing snippets. Using the CLM-M, the top 1 score is increased from 36.03% (LM-M) to 46.96%; and the top 1 improvement in the U-SVM over the S-SVM is about 23%. Moreover, a cross-model comparison shows that the performance ranking of these models is: U-SVM &gt; CLM-LM &gt; LM-M &gt; S-SVM &gt; R-M (the retrieval-based model).
 H.3 [ Information Storage and Retrieval ]: H.3.3 Infor-mation Search and Retrieval Algorithms, Performance, Languages Data Redundancy, Support Vector Machine, Language Model, Question Answering
The purpose of question answering is to return the ex-act answer to a natural language question, which has been identified from a large scale collection of documents. Many Table 1: Snippets Containing Candidate  X 1969 X 
It is believed that the first Crip gang was formed in late 1969. During this time in Los Angeles there were ... ... the first Bloods and Crips gangs started forming in
Los Angeles in late 1969, the Island Bloods... ... formed by 16-year-old Raymond Lee Washington in 1969. Williams joined Washington in 1971 ... had come to be called the Crips. It was initially started to elimi-nate all street gangs ... approaches have been presented for QA, for example, the retrieval-based model, the pattern-based model, the deep natural language processing (NLP)-based model, and the machine-learning-based model. The common characteris-tic among these approaches is that they process candidate-bearing snippets independently and do not use the redun-dancy in these candidate-bearing snippets to help select the exact answer from candidate answers. Therefore, it is not always possible to extract answers to questions when an an-swer occurs in a text snippet that has low similarity with re-spect to the question. By observing the test questions of the TREC QA track (http://trec.nist.gov/), however, we can determine that those candidate-bearing snippets containing the same candidate answer roughly express the same sub-meaning. For example, submitting TREC 2004 test ques-tion  X  X hen was the first Crip gang started? X  to Google, 8 different candidates can be extracted from the top 30 Google snippets. Table 1 shows the Google snippets that contain the candidate  X 1969 X .

From this table, we find that all these Google snippets ex-press the same sub-meaning, i.e., the time of establishment of  X  X he first Crip gang X . Considering these snippets as the aligned snippet/clusters and utilizing them to help select an answer to the question, the Web QA system may perform more effectively than those systems that independently pro-cess the candidate-bearing snippets.

To address this issue, this paper presents two novel ways of utilizing these aligned snippet/clusters to help select an exact answer from candidate answers, i.e., a cluster-based language model (CLM-M) and unsupervised SVM classifier (U-SVM). To the best of our knowledge, no research on this kind of study discussed here has been reported. For com-parison, we implement three widely used QA models that do not utiliz e the redundancy in candida te-bearing snippets, i.e., language model-based (LM-M), supervised SVM classi-fier (S-SVM), and retrieval-based (R-M) techniques. The algorithm 1 1. Query Google by using the combination of question keywords { q i | i =1 , 2 , ...k } ; 2. Download the top m snippets { s i | i =1 , 2 , ...m } trieved by Google for candidate answer extraction; 3. Use candidate extractor to extract the top n candi-dates { c i | i =1 , 2 , ...n } according to answer type of ques-tion, and n candidates { c i | i =1 , 2 , ...n } mean n aligned clusters { C i | i =1 , 2 , ...n } . 4. Retain those snippets that contain the candidate { c i and at least one question keyword q i 5. Organize the retained snippets into n aligned clusters { C i | i =1 , 2 , ...n } . If a snippet includes L different candi-dates, the snippet belongs to L different aligned clusters. comparative experiments in terms of the Chinese version of the TREC QA test data set (to be defined in Section 5) show that the proposed U-SVM and CLM-M significantly outper-form the LM-M and the S-SVM. The CLM-M increases the top 1 score of the LM-M from 36.03% to 46.96%; and the top 1 improvement of the U-SVM over the S-SVM is about 23%. Moreover, the cross-model comparison shows that the performance ranking of these models is: U-SVM &gt; CLM-M &gt;
LM-M &gt; S-SVM &gt; R-M.
Redundancy refers to cases where the same meaning is expressed repeatedly in different documents. In our pro-posed methods of using this redundancy for QA tasks, we first organizes redundant expressions into the aligned snip-pet/clusters.

The retrieval results of a relevant snippet retriever, such as snippets returned by Google in our Web QA system, are usu-ally a mixture of multiple sub-topics (called clusters in this paper) related to the user X  X  question. To group these snip-pets into their corresponding aligned snippet/clusters, we assume that the candidate answer c i in each Google snippet can represent the  X  X ignature X  of its aligned snippets/cluster C . In other words, the Google snippets containing the same candidate c i belong to the same aligned snippets/cluster and all aligned snippets/clusters { C i | i =1 , 2 , ..., n } posed of the aligned collection C . The implementation in-cludes the first-stage Google search (FGS) and the second-stage Google search (SGS).

The FGS is done to extract candidates to a test ques-tion and organize the Google snippets into the aligned snip-pet/clusters, which can be implemented by algorithm 1 . Note that the aligned snippets and the aligned clusters have the same meaning in this paper. In algorithm 1 ,thenum-by the number of candidates { c i | i =1 , 2 , ...n } . Accordingly, the cluster name of each cluster C i is the candidate answer c .Using algorithm 1 , we have obtained n clusters, and each cluster has its snippets, which are referred to as the FGS data. In the FGS data, however, some aligned clus-ters have very few snippets, which results in significant data sparseness. Therefore, more snippets must be collected to resolve this problem. Here, the SGS is presented, which is implemented by algorithm 2 . algorithm 2 for each candidate c i 2. Submit the new query q n to Google and download 3. Retain the snippets containing the candidate c i and end
Through algorithm 1 and algorithm 2 , the aligned col-lection C is obtained, which is the basis of the proposed U-SVM and CLM-M that utilize this collection to help se-lect answers to questions. We will explain the proposed ap-proaches in Sections 3 and 4.
The CLM-M states the problem of selecting a candidate c as the exact answer to the given question q , as follows:
Calculating p ( c i | q ), the probability of each candidate being the exact answer to the given question q and select-ing the candidate c  X  i that has the biggest probability as the exact answer.

That is, we rank the candidates c i ,i =1 , 2 , ..., n , according Bayes X  theorem, we have where p ( q ) can be ignored in ranking because it is the same for all candidates. Moreover, the prior probability p ( c assumed to be a uniform distribution. Thus, we rank can-didates in proportion to p ( q | c i ), which can be expressed by where s c ij is a snippet associated with candidate c i , question keyword, p ( c i | s c ij ) is an answer model, and is a question model. In estimating the question model and the answer model, a two-stage Dirichlet smoothing strategy was adopted that can use the aligned collection. Therefore, the question model is estimated by where s c i is the set of the snippets belonging to the cluster C ,and | s c ij | and | s c i | indicate the length of snippet and the cluster C i , respectively. Similarly, the answer model p ( where the Dirichlet parameters  X  1 and  X  2 are set to 2000 in the experiments.

To sum up, the CLM-M first estimates the keyword dis-ters, and then uses them to smooth p ( q i | s c ij )and p Thus, the CLM-M uses the redundancy among candidate-bearing snippets to build the question model and answer model. The CLM-M extends the idea of expert finding [6]. However, the language model for expert finding does not use the two-stage Dirichlet smoothing which proved to be helpful in our experiments. Moreover, no similar approaches have been used in QA tasks.
The U-SVM states the problem of selecting a candidate c as the exact answer to the given question q , as follows: 1. Considering answer selection as a kind of text classi-fication task, the aligned collection C is the classification training data and the user question q is the test data to be classified into one of the aligned C i clusters; 2. Training an SVM classifier to classify question q ,and assuming that the exact answer to question q is the cluster name c i of cluster C i of question q that is assigned by the classifier.

For the example in Section 1, the training data consists of eight aligned clusters, and the task of the U-SVM is to classify the test question into one of the eight clusters. If the SVM classifier classifies the question into a cluster whose cluster name is 1969 ,then 1969 is the exact answer.
The U-SVM adopts the LIBSVM toolkit(http://www.csie. ntu.edu.tw/  X  cjlin/) to implement the SVM classifier, uses data, and extracts three types of features for classification, i.e., SBFS, BMFS, and WWFS feature sets. The details of these feature sets are explained in our previous paper[9].
To summarize, the U-SVM starts only from a user ques-tion, uses redundancy among candidate-bearing snippets to automatically learn the aligned clusters as the training data for each test question, and classifies the question into one of the aligned clusters to select an answer from the candidates.
We did not have access to an English named entity recog-nition (NER) tool at the time of the experiments. There-fore, this paper evaluates the CLM-M and U-SVM in terms of a Chinese Web QA system. Although our models are in-dependent of question type and language, we adopt some additional conditions that allow us to concentrate on the answer selection part. The conditions are (1), only those questions whose answers are named entities are selected for convenience in candidate extraction; (2), all test questions used in the evaluation have at least one exact answer in the retrieved Google snippets. The test data set CTREC in-cludes 247 Chinese questions translated from TREC 2004 and 2005 FACTOID test questions.

In the experiments, the top m (50) Google snippets are adopted to extract candidates by using a Chinese NER tool[8]. The number of candidates ( n ) extracted from the top m Google snippets is adapted for different test questions, but it does not exceed 30. The results are evaluated in terms of three scores, top 1, mrr 5andtop 5.
The LM-M is a special case of the CLM-M. When the pa-rameter  X  2 is equal to  X  , the CLM-M backs up the LM-M. Therefore, the difference between CLM-M and LM-M is that the former utilizes the aligned snippet/clusters to estimate the probabilities, whereas the latter does not. The aim of this experiment is to explore the contribution of this redun-dancy to the CLM-M. Table 2 summarizes the comparative performance on the CTREC test data.

The results in Table 2 indicate that the CLM-M signif-icantly outperforms the LM-M: the top 1/mrr 5/top 5im-provements are 10.9%/10.8%/9.3%. This experimental re-sult demonstrates that mining redundancy of candidate-bearing snippets is reasonable and effective, and the CLM-M can benefit from this redundancy. To explore the effectiveness of the redundancy in the U-SVM, we compare it with the supervised SVM classifier, S-SVM, in this experiment. The S-SVM[2][5][1] indepen-dently processes each snippet to select an answer from the candidates; therefore, it does not use the redundancy.
The S-SVM adopts SVM as the classifier, uses the hand-tagged question-answer pairs as the training data, and ex-tracts features to classify the candidate answers into posi-tive or negative. To train the S-SVM, this paper manually tagged 807 Chinese question-answer pairs.

The U-SVM results are compar ed with the S-SVM results in Table 3.

From this table, we can see that the U-SVM greatly out-performs the S-SVM in all measurements. For example, the top 1/mrr 5/top 5 improvements of the U-SVM are 15.79%/ 13.32%/8.50%. This result also demonstrates that the U-SVM can benefit from the redundancy of the candidate-bearing snippets. Note that fewer features 1 are used here than in [2][5][1]. However, the comparison is still effective because the models are compared in terms of the same fea-tures. The main reason the U-SVM outperforms the S-SVM is that in the S-SVM, all of the test questions share the same training pairs, whereas our U-SVM learns the unique aligned collection for each test question.
The U-SVM and the S-SVM only use the SBFS and BMFS feature sets in [9].
The conclusion from this comparative experiment is that the U-SVM performs better than the S-SVM does, even though the U-SVM is an unsupervised technique and no hand-tagged training data is provided.
The R-M is widely used in the TREC QA tracks that select the candidate with the shortest distance to all question keywords as the correct answer. In this experiment, the R-M is implemented based on the snippets returned by Google; the U-SVM is based on the SBFS, BMFS and WWFS feature sets in [9]. Table 4 summarizes the comparative performance on the CTREC test data set.
 Table 4: Comparison of R-M, CLM-M, LM-M, U-SVM, and S-SVM top 1 31.58% 36.03% 35.63% 46.96% 58.70% mrr 5 46.42% 49.52% 50.58% 60.30% 67.84% top 5 71.26% 73.68% 76.52% 83.00% 83.40%
This table shows the following: 1. The proposed CLM-M and U-SVM greatly improve the performance of the R-M, LM-M and S-SVM. As compared with the R-M, LM-M, and S-SVM, the top 1improve-ments of the U-SVM are 27.12%, 22.67%, and 23.07%, respectively; while the top 1 improvements of the CLM-
M are 15.38%, 10.93%, and 11.3%, respectively. 2. The U-SVM is more effective than the CLM-M: the top 1/mrr 5 improvements of the U-SVM over the CLM-
M are 11.74%/7.54%. This is because the U-SVM utilizes more features than the CLM-M does. Moreover, the U-SVM classifier may be more suitable to our task than the
CLM-M. 3. There is no significant difference between the S-SVM and the LM-M, even though the S-SVM uses question-answer training pairs. The LM-M does not apparently uti-lize the aligned clusters in estimating the question model and the answer model. It, however, takes the sum of all snippets s c ij containing the candidate c i to calculate the probabilities, as shown in equation (2)  X  (3). That means the LM-M also impliedly uses the redundancy of candidate-bearing snippets.

From the cross-model comparison, we can conclude that the performance ranking of these models is: U-SVM &gt; CLM-M &gt; LM-M &gt; S-SVM &gt; R-M. In recent years, some pioneering studies have investigated Web data redundancy to help to select answers. Clarke et al. [4] measured the contribution of data redundancy to vote for the most likely answer by using a simple count of the number of snippets in which a candidate appears. Magnini et al. [3] exploits the redundancy of Web information into an answer validity score to estimate the correctness of a candidate. Dumais et al. [7] used answer redundancy, i.e., multiple, differently phrased, and answer occurrences, to fa-cilitate answer selection. This paper, however, presents two distinguished approaches for mining web data redundancy to improve QA, which justifies the previous studies[3][7].
Based on the observation that candidate-bearing snip-pets with the same candidate usually express the same sub-meaning, this paper grouped these snippets into aligned clusters and presented two novel ways, i.e., a cluster-based language model and an unsupervised SVM classifier, to use these aligned clusters to improve the performance of the Web QA system. The results of our experiments are encouraging. Compared with the S-SVM, the top 1performancesofthe U-SVM and CLM-M are significantly higher by 23.07% and 11.33%, respectively. Moreover, the U-SVM outperforms the CLM-M.

These experiments have validated the proposed U-SVM and CLM-M on named entity types of questions, which ac-counted for about 82% of all TREC2004 and 2005 FACTOID test questions. Our approaches, however, are independent of question type and language only if the candidates can be extracted from Google snippets. In the future, we will explore the effectiveness of our techniques for other types of questions and directly compare our methods with related redundancy-based approaches. We will also apply the U-SVM and CLM-M to Web QA systems in other languages, such as English and Japanese, in the future.
This work was supported by the Grant-in-Aid for Scien-tific Research on Priority Areas in Japan as part of Cyber In-frastructure for the Information Explosion Era, under grant No.A03-24. [1] Abdessamad Echihabi, and Daniel Marcu. A [2] Abraham Ittycheriah, Salim Roukos. IBM X  X  Statistical [3] Bernardo Magnini, Matteo Negri, Roberto Prevete, [4] Charles L. A. Clarke, Gordon V. Cormack, Thomas R. [5] Jun Suzuki, Yutaka Sasaki, Eisaku Maeda. SVM [6] Krisztian Balog, Leif Azzopardi, Maarten de Rijke. [7] Susan Dumais, Michele Banko, Eric Brill, Jimmy Lin, [8] Youzheng Wu, Jun Zhao and Bo Xu. Chinese Named [9] Youzheng Wu, Ruiqiang Zhang, Xinhui HU and Hideki
