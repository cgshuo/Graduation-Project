 Collaborative filtering [10] generates personalized recommendation to match users X  interests and its performance can be improved by exploiting temporal information, as the tendency of user preferences and item attractiveness is not static [10,11]. There are four general approaches in temporal CF, i.e, heuristic, binning-based, online updating and dynamic-based approaches. Heuristic ap-proach penalizes the importance of data before a pivot point[6,10], which tends to undervalue the past data. In binning-based approach, training and testing data could be from the same interval [11]. The prediction for users X  interests is actually post hoc about what interests would have been in the past, rather than what interests would be in the future. Although online updating approach only uses past information to make prediction [8,12], it usually focuses on scalability and ignores the dynamics of user taste and item attractiveness. Dynamic-based approach explicitly models temporal dynamics by a stochastic state space model, which shows advantages over the other methods [13]. However, in such an ap-proach, item attractiveness is assumed to be static with Gaussian distributions, which may be oversimplified. To overcome such problems, we first utilize particle filtering [16] as a dynamic technique to model non-Gaussian behaviors and track latent factors representing user preferences and item attractiveness.
Furthermore, under the temporal context, the data sparsity problem [19] be-comes challenging as many users would be inactive for some consecutive time slots. Although matrix factorization [15] could realize sparsity reduction, the tendency tracked by particle filtering may be unreliable due to insufficient ob-servations at every time step. Exploiting additional information, such as con-textual information or common patterns [10], or imputing all or most missing data are two common approaches in CF to reduce sparsity. However, this extra information collection is usually not only infeasible in practice but also compu-tational complexity. Therefore, we do not consider utilizing side information in this paper. Missing data are usually imputed as negative [18,14] or other heuris-tic values [10]. Deterministic optimization [17] and current model estimation [9] are also used to yield some reasonable imputed values. However, these methods are only developed under static context and based on some heuristic rules or point estimators. Meanwhile, all these methods impute all or most missing data, leading to an unaffordable computatio nal complexity for succeeding recommen-dation algorithms, since it not only heavily reduces the scalability of algorithms but also influences the recommendation accuracy.

In this paper, we aim to solve the problems of data sparsity and scalability in particle filtering-based temporal dynamic recommender systems. We utilize self-training principle [22] to dynamically construct feedback data to enhance our particle filtering-based recommendation method. In particular, we use la-tent factors to compactly represent user p references and item attractiveness re-spectively at each time step, whose initial settings are learned by probabilistic matrix factorization (PMF) [15]. Based on such a representation, we develop a novel self-training mechanism based on the distributions of the current personal-ized prediction to complement recent obs ervations with negative items sampled from missing data. The mechanism is then cooperated with particle filtering techniques to simultaneously track the tendency of user preferences and item at-tractiveness. For top-k recommendation [21,4], a personalized list for each user is generated based on current user and item latent factors.

We discuss related work in Section 2. P article filtering for PMF is described in Section 3. Self-training for the particle filtering based method is developed in Section 4. Experimenta l results are in Section 5. Conclusion is presented in Section 6. There have been few studies [13,11,20] on exploiting temporal dynamics to im-prove the performance of RS. Among these studies, [13] is the most related one to our work, which uses Kalman filter as temporal priors to track user latent fac-tors. However, item latent factors are o nly updated but not tracked. Meanwhile, the usage of Kalman filter restricts the dynamic and observation functions to be linear and Gaussian. Particle filtering h as been used to dynamically update a log-normal distribution that models user preferences [3] in music recommendation, assuming the staticness of item popularity. Moreover, the method is not based on latent factors, and very application-sp ecific (otherwise, no proper features).
Conventional CFs with imputation [19] all suffer from the domination of im-puted ratings. Sampling missing feedback is only used in non-temporal context and one class CF (OCCF) problem [5]. An OCCF problem can be deducted from our problem by setting, for example, relevant ratings as positive examples. User-oriented and item-oriented mec hanisms, which only based on the times that items or users present, are proposed in [14] as sampling methods. In these methods, recommendation a ccuracy is compromised in order to boost the scal-ability. To obtain a more accurate recommendation, samples are selected based on pairwise estimation for OCCF to iteratively train the model parameters [21]. All of these sampling methods are developed under static context for OCCF. Unlike our proposed method, these algorithms do not aim to solve problems of scalability and sparsity at the same time. Probabilistic matrix factorization method, as a model-based approach in CF, has been widely used due to its simplicity a nd efficiency. In particular, assuming N users and M items, let R  X  X  N  X  M be a user-item prefer ence matrix with an entry r u,i representing the rating given by user u to item i . Rating r u,i is gener-ated with a Gaussian distribution P ( r u,i | U u ,V i ) conditioned on K dimensional user and item latent factors U  X  X  N  X  K and V  X  X  M  X  K . Prior distributions P ( U )and P ( V ) are formulated to contain regularization terms [15]. These latent variables are further assumed to be marginally independent while any rating r u,i is assumed to be conditionally independent given latent vectors U u and V i for user u and item i [15]. The likelihood distribution over preference matrix R is, where N ( x |  X ,  X  1 ) is a Gaussian distribution with mean  X  and precision ,and Y u,i is an indicator variable with value 1 when rating r u,i is not missing and value 0 when the rating is not observed. Priors P ( U )and P ( V )aregivenas,
Maximizing the log-posteriors over U and V is equivalent to minimizing the sum-of-square error function with quadratic regularization terms for PMF [15], leading to the following objective function,
E = where  X  U = U / ,  X  V = V / ,and ||  X  || Fro denotes the Frobenius norm.
We use a state space approach [16] to track the tendency of user preferences and item attractiveness. With linear and Gaussian assumption, it is straight-forward to define the state to be a joint vector of user and item latent factors, due to the existence of analytical and tractable solution. However, it is shown [15] that empirical distributions of the p osterior Hence, we use particle filter-ing to simultaneously track these latent factors. Particle filtering iteratively ap-proximates regions of high density as discrete sample points. As the number of particles goes to infinity, the approximation converges to the true distribution [16]. In practice, given d -dimensional state space, the number of required parti-cles should be O (2 d ) to achieve a satisfiable result [16]. To make a compromise between the accuracy of user/item representation and tractability of particle filtering, we separately track late nt factors for each user and item.
We assume that the tendency of user u  X  X  preference and item i  X  X  properties follows a first-order random walk driven by multivariate normal noise, due to the lack of prior knowledge. The transition functions at time t are as follows, where c u t  X  X  (0 , X  U I )and d i t  X  X  (0 , X  V I ) are defined as unrelated Gaussian process noises. The posterior distribution of U u t is approximated by particle using dynamics in Eq (3). The estimation of item v  X  X  latent factors at time t is obtained in a similar way. Using the transition prior in Eq (3) as the proposal distribution, the weight at time t for all the particles is evaluated recursively as w
The observation function should reflect the ability of a particle to reconstruct given ratings. The objective function in Eq (2) is an immediate candidate in which P ( R | U,V, X  )  X  e  X  E . However, this candidate function is sub-optimality for a top-k recommendation task, because an algorithm attempting to minimize the root-mean-squared-error in prediction does not have a satisfiable performance for a top-k recommendation task [4]. Moreover, the objective function in Eq (2) assumes that unobserved data in both training and testing cases are missing at random. That is, the probability that a rating to be missing is independent of its value. Nevertheless, it is shown [18] that feedback in RS is generally not missing at random (NMAR). Low ratings are much more likely to be missing than high ratings [18] because users are free to choose items to give feedback.
To design a suitable observation function, the key idea is to consider the ranking of all the items, no matter whether they are observed or not. By treating all missing data as negative with weights (wAMAN), the observation function over imputed ratings  X  R u t for s -th particle of user u is as follows, value for all the missing data, which is regarded as the average value of ratings in the complete but unknown data. Weight W u,i is defined to reflect the confidence over imputation and set as a global constant w m for simplicity [18]. Latent factors V t and their weights w function over s -th particle of item i is defined similarly. The distributions are no longer Gaussian after the introduction of w m and an imputed value for all the missing data. Meanwhile, no regular ization terms exist in Eq (4) because we obtain point mass approximation of posterior distributions via particle filtering attempting to avoid overfitting. This method is named as PFUV hereafter. In practice, feedback is usually unavailable before recommendation is made, which implies observation R t at the current period is not available to estimate the tendency of user preferences and item attractiveness before recommendation. It is straightforward to use all the historic observations R 1: t  X  1 to approximate the estimation. However, the ratings would be dominated by the past information and cannot represent the recent tendency. An alternative approximation uses the most recent observation R t  X  1 instead. However, under temporal context, the rat-ings are too sparse for each user or item to track the current tendency. The data sparsity can be reduced by imputing all the missing data as shown in Eq (4). The the dynamics in this approximation will drift away from the true tendency due to the domination of imputed ratings in  X  R t  X  1 . Meanwhile, this approximation does not have a satisfiable scalability due to the usage of all the missing data.
Therefore, we exploit self-training principle [22] to solve the above mentioned problems. Instead of treating wAMAN, for each user at every time step, we will dynamically select a subset of missing it ems as negative samples to complement the user X  X  most recent observation. This per sonalized and self-tr aining procedure not only distinguishes the past and recen t information but also avoids to domi-nate recent observation with imputed data. These samples are the most confident negative samples w.r.t the current prediction distribution for each user. 4.1 Self-training Sampling Method Given user u and its current unobserved items I m,u t ,asetof N n,u t items I n,u t  X  t is selected by a multi-nominal distribution. The distribution is where N m,u t is the number of unobserved items for user u until time t , { x i | i  X  { 1 ,...,N m,u t }} represents the times that unobserved item i would be selected as negative, and {  X  i | i  X  X  1 ,...,N m,u t }} is the probability that unobserved item i is disliked by user u . Without restricting x i  X  X  to binary variables, this personalized selection is an adaptive mechanism. An unseen item with a high probability will be selected more frequently than those with lower probability. As the accumula-tion of w m for the same negative sample in Eq (4), more emphasis will be placed on the sample. By imposing such restriction, N n,u t different items will be chosen. We will adopt this restriction for simplicity.
 Confidence Estimation. A candidate negative sample should have a small prediction error and a small estimation variance if the sample was negative. This criterion resembles the bias a nd variance decomposition o f generalized errors [10]. where  X  r u,i = r m represents the event that the predicted rating equal to the imputed value and var ( X  r u,i ) represents the variance of the prediction. Assuming prediction error and variance are conditionally independent given U t and V t , where the predicted joint distribution of latent factors is estimated using particle filtering described below, S and S are the number of particles used to track user u  X  X  latent factors and item i  X  X  latent factors, respectively.
 Predication with Canonical Particles. To estimate a particle X  X  weight for U t in Eq (6), we need a weight of a particle for V t . Likewise, we use a weight of a particle for U t to reweight a particle for item latent factors. As the computation over all possible pairs of user and item particles is too expensive, we resort to canonical particles [7]  X  U t and  X  V t to respectively represent the total effect of particles on the estimation for each user and item latent factors at time t . 1 ...S } and { ( w s V,t ,V s t ) | s  X  1 ...S } . To avoid the degeneracy problem [16] in particle filtering, we will resample particles proportional to their weights Af-ter resampling, we use the expectation of posterior distributions of U t and V t  X  V
Combining with canonical particles  X  V t and Eq 6, the prediction distribution of user u  X  X  preference over item i is estimated as follows, between the imputed value and the predicated rating usually means a high confi-dence that the item should be negative. Thus, the probability of prediction error variance estimation, the probability of prediction variance can be estimated by 4.2 Two-Phase Self-training Method Considering the large size and high sparsity of user-item preference matrix, the previous sampling mechanism considering all the unobserved items is infeasible in practice. We use a two-phase approach to reduce the computational complexity.
In phase I, for each user u , we sample a subset I n,u t from the unobserved items I m,u t . Generally, these sampling schem es can be implemented in terms of any distribution that properly represents NMAR. It is shown in [18] that arbitrary data missing mechanism is NMAR as long as they are missing with a higher probability than relevant ratings do. We use a uniform distribution of used to handle some large datasets [21,14]. This simple and efficient distribution is a rational choice as long as we set N n,u t to be a reasonably large value, which will be discussed in Section 5. For simplicity, we set |I n,u t | =2  X  N n,u t . In phase II, personalized probability  X  i will be computed only for candidates t . Based on Eq (5), negative samples will be selected and then combined with observed data R t  X  1 to construct a sparsity reduced data  X  R t at time t .User and item latent factors are thus tracked by using this dynamically built data. We name this two-phase self-training method as ST-PFUV hereafter. Let N n,u t =  X  N for simplicity. For computational complexity, at each time step, PFUV takes O ( KSMN ). As sampling size  X  N is usually comparable with K min ( N,M ), ST-PFUV takes O ( KS ( M + N )  X  N )  X  O ( K 2 S ( M + N )), which scales efficiently as a linear function of user and item size. The performance of the proposed algorithm is tested on the popular Movie-lens 100K [1] and HetRec MovieLens datasets [2]. Both are public benchmark datasets. MovieLens 100K contains 530 users and 1493 items with sparsity 93.3% (at the 16-th week in 1998). HetRec contains 1775 users and 9228 items with sparsity 95.1% (in December 2008). MovieLe ns spans 8 months with integer scale 1 -5 while HetRec spans 12 years with half mark scales from 1 to 5.
 Protocol. In our experiments, ratings are grouped into time frames based on their timestamps. All the ratings before a predefined time instance t test are training data, and ratings after it are testing data. This setting is preferred over a random split over all the data as it is infeasible to make prediction utilizing information in the future in a real-world deployment. The training periods for MovieLens 100K and HetRec are Sept.  X  Dec. 1997 and Sept. 1997  X  Dec. 2007, respectively. Their testing periods are the 1st  X  16th weeks in 1998 and Jan.  X  Dec. 2008, respectively. Different units of time frame are selected to ensure that ratings for each user in a time slot are not too sparse. Therefore, the task in our experiments is to predict individual X  X  ranking over all the items in next time frame t based on all the information upto t  X  1. All the algorithms are repeated 10 times and the average results are reported. They are all implemented in Matlab with 3.3G Hz CPU and 8G memory.
We use precision@k, recall@k [4] to meas ure recommendation relevance, which directly assess the quality of top-k recommendations. To measure user satisfac-tion in recommendation, we use top-k hitrate [4,18]. As top-k hitrate recall is proportional to top-k hitrate precision [18], we only test top-k hitrate recall and name it top@k. In order to test temporal p erformance, the tem poral extensions of those metrics are defined. Convention al accuracy metric s adopted to RS can be found in [4,10], which are omitted here due to space limitation.

For user u , precision@k over month t is denoted as prec ( k,u,t ). During the prediction at time t  X  1, instead of using all the items in testing data as con-ventional precision@k does, only items in month t are scanned to determined their relevance to a user. The t emporal precision is defined as, prec temp ( k )= frames T . The temporal recall recall temp ( k ) and temporal hitrate top temp ( k )are defined in a similar fashion. As a common p ractice, we treat items with rating 5 as relevant items, and measure k = 10 for precision. For hitrate, we set k =10 and each relevant item is mixed with 500 randomly sampled unobserved items to avoid spending too much computational power on evaluation. We set k = 100 for recall as we are ranking all the items in temporal context. We tune all the model parameters under temporal recall and use the identical setting to test the performance of algorithms under temporal recall and hitrate.
 Baseline Methods. In order to test the performance of the proposed algo-rithms (ST-PFUV) that balances imput ation and the recent observations, we compare it with the following five algorithms as part of baseline methods: PFUV, ST-PFUV-User, TopPopular, PureSVD [4] and AllRank [18]. PFUV is used to verify the efficiency and scalability of our sampling methods on incorporating temporal dynamics. It is empirically shown [14] that user-oriented sampling based on user preference has better performance than uniform sampling, we therefore hybrid the PFUV method with user-oriented sampling, and name it ST-PFUV-User. As TopPopular is a non-personalized algorithm that ranks item higher when the item is more often rated as relevant, which is included to verify the benefits of considerin g personalized reco mmendations in temporal context. PureSVD and AllRank are state-of-art algorithms developed to pursuit the top-k recommendation task. They are selected to illustrate the ability of our sampling methods to cope with non-Gaussian behaviors. These baseline algorithms are designed without exploiting any temporal information. In our experiment, to make these algorithms dynamic, we retrain all the learned models at each time step. This simple extension, which is a c ommon practice in real-world deploy-ment, is important to make the comparison fair. To the best of our knowledge, most of developed algorithms in temporal RS are compared with static versions of some baseline algorithms. We name these dynamic extensions as DynTop-Popular, DynPureSVD and DynAllRank. To confirm the necessity of exploiting temporal information, we also adopt PMF as the only static baseline method, which always predicts the ranking without updating model parameters. To bal-ance the accuracy, variance and scalability, we set S = S = 1000 and K =8 for all the PFUV-based methods. The imputed value is set to r m = 2 which is a lower value than the average observed ratings in MovieLens 100K and Hetrec datasets. All the PFUV-based methods are initialized by AllRank.
 MovieLens 100K. Table 1a shows results of above methods under temporal accuracy metrics. By cross valid ation method [10], we set weight w m =0 . 05, regularization constant  X  to 0 . 05 and K = 12 for DynAllRank, and K =10 for DynPureSVD and PMF. To reduce variance in particle filtering, we set  X 
U =  X  V =0 . 05. By cross validation, we set sampling. The low values in the table are due to the fact that few relevant items exist for each user in a time frame. Compared with these baseline methods, ST-PFUV has the best performance. All the improvement is statistically significant under paired t -tests [10] with p&lt; 0 . 01. This result verifies that the efficiency of proposed self-training sampling mechanism on modeling temporal dynamics in data and systematically selecting informative negative samples for each user. To verify the benefit of tracking item latent factors, we set V t = V 0 in ST-PFUV, and name it as ST-PFU. Table 1a also shows the benefit of tracking the tendency for both users and items.

To further illustrate the power of self-training and the importance of distinguish-ing recent and historic ratings, we test the performance of PFUV under two extra settings, where ratings consist of 1) the recent observation and wAMAN, 2) all the historic data and missing data sampled by user-oriented distribution. We name these methods as PFUV-Rect-wAMAN and PFUV-Hist-User. We also extend the best baseline method DynAllRank by replacing wAMAN with missing data sam-pled by user-oriented distribution. We name it DynAllRank-User. For easy com-parison, DynAllRank-User has the same setting as DynAllRank. Table 1b shows the performance of these methods. Combining with the results in Table 1a, results in Table 1b further confirm the ability of our methods to balance information in-trinsic in recent observations and the spars ity reduction introduced by imputation to better incorporate temporal and dynamic information.
 HetRec MovieLens. In following experiments, we focus on the study of ac-curacy and scalability of ST-PFUV over a longer period, and the influence of the number of selected samples. We comp are ST-PFUV with the best baseline method DynAllRank and the static PMF method. Similar to previous experi-ments, ST-PFUV has the best performance in accuracy. Due to space limitation, only temporal recall is shown here. By cross validation, we set w m =0 . 08, K =40 and  X  =0 . 12 for DynAllRank, and  X  U =  X  V =0 . 1 for ST-PFUV. For PMF, we set K = 40, step size for gradient descent as 0 . 005 and  X  =0 . 05.

Table 2a shows the results of these methods under temporal recall, where  X  N = 50. The results show that ST-PFUV significantly outperforms other meth-ods in terms of recommendati on accuracy. All the improvement is statistically significant under paired t-tests with p&lt; 0 . 01. Compared with results on Movie-Lens 100K, ST-PFUV has much greater accuracy improvement over baseline methods, verifying its much better exploiting temporal and dynamic informa-tion, especially over a longer period. As a sequential approach, our proposed algorithms do not require retraining stages. Thus, we average the total running time (both retraining time and testing time) to compare the scalability as shown in Table 2a. These empirical results confirm that the proposed sampling methods are much faster than baseline methods, and the two-phase method ST-PFUV is comparable with its one-phase counterpart. Note that PMF has been among the fastest state-of-art CF methods.

The results with different number of samples are shown in Table 2b. Upto  X  N = 50, the performance of ST-PFUV is constantly being improved as the effect of sparsity reduction. The performance is not improved significantly when  X  N is larger, as the built observations are cluttered by negative samples.
To further evaluate temporal behaviors of ST-PFUV, we define the average of accumulated improvement (AAI) over time. Let the performance of any two methods under temporal recall in month t be Rec 1 ( t )and Rec 2 ( t ), respectively. The AAI in month t 1 is 1 t among ST-PFUV, ST-PFUV-User, DynAllRank and PMF methods. Except in the first month of red curve (ST-PFUV vs ST-PFUV-User), all the curves are above zero, showing that our method cons tantly outperforms baseline methods over time by selecting negative items via personalized self-training sampling scheme. Meanwhile, compared with DynPMF and DynAllRank, the tendency of dash and dot dash curves demonstrates that ST-PFUV is more efficient at exploiting the underlying temporal patterns. While baseline methods require longer training period, ST-PFUV performs well even if the training period is short (within 5 months) and accumulated amount of ratings is few. In order to simultaneously solve the problems of data sparsity and scalability for temporal dynamic recommender systems, we have developed a novel two-phase self-training mechanism to dynamically construct a small but delicate set of observations from missing data. Cooperating with a particle filtering-based dynamic model, this work facilitates to track temporal dynamic user preference and item attractiveness in recommender systems.

The proposed algorithms are evaluated on two public benchmark datasets under the temporal accuracy metrics. The empirical results show that the pro-posed methods significantly improve recommendation performance over a vari-ety of state-of-art algorithms. The experiments also illustrate the efficiency and scalability of the developed self-training temporal recommendation algorithms.
In future, we would like to investigate more sophisticated techniques to even better represent and learn the underlying dynamics of user preferences and item characteristics. It is also worth exploring likelihood functions for other recom-mendation tasks considering multiple criterion.

