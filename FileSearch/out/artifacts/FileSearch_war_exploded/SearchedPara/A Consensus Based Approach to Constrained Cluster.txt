 Managing large-scale software projects involves a number of activities such as viewpoint extraction, feature detection, and requirements management, all of which require a human analyst to perform the arduous task of organizing requirements into meaningful topics and themes. Automating these tasks through the use of data mining techniques such as clustering could potentially increase both the efficiency of performing the tasks and the reliability of the results. Unfortunately, the unique characteristics of this domain, such as high dimensional, sparse, noisy data sets, resulting from short and ambiguous expressions of need, as well as the need for the interactive engagement of stakeholders at various stages of the process, present difficult challenges for standard clustering algorithms. In this paper, we propose a semi-supervised clustering framework, based on a combination of consensus-based and constrained clustering techniques, which can effectively handle these challenges. Specifically, we provide a probabilistic analysis for informative constraint generation based on a co-association matrix, and utilize consensus clustering to combine multiple constrained partitions in order to generate high-quality, robust clusters. Our approach is validated through a series of experiments on six well-studied TREC data sets and on two sets of user requirements. H.3.3 [Information Search and Retrieval]: Clustering, Information filtering.
 Documentation, Management. Clustering, requirements. Software development projects include a number of human intensive activities that can benefit significantly from automated support. For example, activities such as feature detection, requirements elicitation [7 ], and certain types of automated traceability [ 14] all rely upon a human analyst to organize an extensive set of requirements into meaningful topics and themes. This is illustrated in the requirements elicitation process, where stakeholders document their needs as short unstructured statements which must then be manually reviewed, analyzed, and classified. The challenge of performing these tasks in a large project can be daunting. However data mining techniques such as clustering can be used to organize and manage stakeholders X  feature requests in order to increase efficiency , provide scalable software engineering processes, and improve reliability of the results. Unfortunately, the unique characteristics of this domain, such as high dimensional, sparse, noisy data sets resulting from short and ambiguous expressions of need, present difficult challenges for standard clustering algorithms. Furthermore, the context in which the clusters will be used, dictates the need to create extremely fine-grained, high-quality clusters, sometimes containing as few as 10-20 requirements. Although high quality is clearly a goal of all clustering algorithms, it is especially important in the requirements domain because project stakeholders will directly interact with and scrutinize the generated clusters, and those which lack a clear and dominant theme, or clusters with e ven a few misplaced requirements may cause project stakeholders to lose trust in the automated approach and will lead to poor adoption of related tools and processes. In prior studies, we broadly investigated the use of standard clustering techniques such as K-means, agglomerate hierarchical clustering, bisecting, and probabilistic techniques to determine if any of these basic approaches could consistently return requirements clusters at the quality needed to support the proposed software engineering activities [ 15]. Each of the algorithms was evaluated against several requirements datasets, using both standard coupling and cohesion metrics, and also by comparing the generated clusters to known answer sets. We also conducted a subjective user analysis of the answer sets because this provided more insight into the strengths and weaknesses of the clustering process. For example, one of the smaller datasets we evaluated represented a set of 366 feature requests gathered from MS students describing their needs for an Amazon-like student-centric web portal. A subjective analysis found that the generated clusters included very few highly cohesive ones and that almost all clusters contained misfits. Furthermore there were a significant number of clusters containing no obviously dominant theme. As a result of this extensive study which included multiple algorithms and data sets , we concluded that fully automated single -technique clustering algorithms do not appear to produce sufficiently high quality results to adequately support the targeted software engineering tasks. In this paper, we propose a semi -supervised clustering framework, based on a combination of consensus -based and constrained clustering techniques, wh ich can effectively handle the challenges described above . The new approach takes advantage of the high levels of interactive user feedback expected in the requirements elicitation task to constrain future clusterings in an ensembl e clustering framework . The quality of the initial baseline clustering in the ensemble is also significantly improved through a consensus -based approach . Each clustering is generated by selecting a sub -sample of needs and then using the generated clusters to classify remaining needs. This ensemble is then used to identify a set of constraints that maximize the benefits obtained from the costly constraint collection process. The framework is tested against six TREC datasets , which have been used in related work [23], and two sets of feature requests . A ll of these are discussed in greater detail later in the paper.
 The remainder of the paper is laid out as follows. Section s 2 and 3 provide a background discussion of constrained and consensus clusteri n g both of which are adopted in our proposed framework. Section 4 then introduces our consensus -based constrained clustering framework, and section 5 reports on a series of experiments we conducted to validate it within the requirements domain . Section 6 concludes with an overall analysis of the results. Notations that are used throughout the remainder of this paper are defined in Table 1. A number of researchers have investigated the use of semi -supervised clustering techniques in which the clustering process is guided by prior knowledge or constraints collected through expert user feedback. These constraints can be generally classified as cluster level or instance level, where cluster level constraints dictate global rules suc h as not permitting empty clusters, and instance level constraints specify something about the relationships between a pair of elements. The most commonly used instance level constraints are pair -wise Must -Link (ML) and Cannot -Link (CL) constraints, indic ating respectively whether a pair of instances must be placed in the same or in separate clusters. Due to inconsistencies when constraints are gathered from real users , for example and , while rules. This paper, as with most other papers on semi -supervised clustering, utilizes ML/CL constraints as opposed to other types of instance constraints primarily because from a practical perspective, th is simple form of constraint is the easiest one to design and collect from users. Semi -supervised clustering has been investigated across a wide variety of algorithms, including hierarchical clustering [ 10 ], non -negative matrix factorization [ 21,28 ], and partitioned clustering, especially the variants based on K -means which has been shown to be very efficient. Constrained K -means variant algorithms can be further categorized as constraint enforcement, learning distance metric, seeding, viola tion penalty, and hybrid approaches. One representative technique for the constraint enforcement algorithm is COPK -means [ 25 ], which strictly enforces both ML and CL constraints during the cluster assignment stage. The algorithm proposed by Xing [ 26 ] tries to learn a diagonal or full covariance matrix from the constraints, and then calculate the Mahala no bis distance for points to reflect the impact from the constraints. Seeded -KMeans proposed in [ 2 ] utilizes cluster labeli ng information to initialize the ce ntroids and constrain the cluster assignment. PCK -means, which is a violation penalty algorithm [ 3 ], modifies the objective function in K -means by adding a penalty for constraint violations in the form of a weighted number of violations. MPCK -means [ 6 ] is a metric learning -enhanced violation penalty algorithm, which combines the ideas from [ 26 ] and [ 4 ]. Additionally, model -based and probabilistic partitioning algorithms incorporating pair -wise constraints have been studied extensively in [ 4,9, 29 ].
 As large scale constraint collection is extremely expensive in practice, it is important to maximize the potential usefulness of the constraint set, namely, the degree to which the set of constraints can improve the clustering quality. Besides the feasibi lity criteria of finding a partitioning that can satisfy all of the ML and CL constraints discussed in [11, 12, 13] , Davidson et al proposed the two utility metrics of informativeness, which measur es the amount of information in a constraint set that the clustering algorithm could not have determined on its own, and coherence which represent s the amount of agreement wi thin the constraints themselves with respect to a given distance metric. Although mu ch of th e early work on constrained clustering focused on low -dimensional data, the current need to effectively cluster large volumes of textual data has led to a new emphasis on using constrained approaches to cluster high -dimensional data. The use of co nstrained clustering is particularly pertinent in the requirements domain because the high level of interaction with stakeholders makes feedback gathe ring quite attractive.
 Tang et al . [ 23 ] proposed a hybrid method named SCREEN which , like our framework, was designed specifically for constrained document clustering. SCREEN projects the instance vectors using an orthonormal matrix derived from constraints so that in the new feature space the similarity between instances involved in a M L constraint are maximized and the similarity between instances in a CL constraint are minimized. However, our 
Symbol (s) Description analysis of this approach indicates that the experiments Tang et al . reported against six TREC datasets used a far from optimal baseline against which to compare their results. Essentially they did not include an important optimization step in spherical K -means. The details and impact of this are discussed further in section 5.5 . 1 .
 One of the weaknesses of these well -known approaches for constrained clustering is that the pairs of instances for which ML and CL constraints will be generated tend to be randomly selected from an answer set . Typically, for experimental purposes, a pair of documents is randomly selected and a ML or CL constraint is generated according to the true assignment of the documents within the answer set . However , as our results will show, the random approach does not perform well on requirements datasets which are typically characterized by large numbers of finely grained clusters , and composed of documents that tend to be very short and ambiguous . A few researchers have also investigated active learning techniques for constraint generation [ 3,19 ] , as this approach can improve informative ness of future constraints based on feedback gathered incrementally from the use r. In actual practice, active constraint generation techniques assume the existence of an oracle that can respond to incremental feedback , and this may not always be feasible in practice. This paper therefore explores a constraint generation technique that can produce more informative constraints without the benefit of active learning. The following section provides a brief description of consensus clustering methods and then introduces a new technique for constraint selection based upon a co -associat ion matrix generated by a consensus algorithm. As our results will show later in this paper, this approach is particularly well suited to constrained -clustering of software requirements. One difficulty of many clustering algorithms such as K -m eans, spectral clustering, and model -based probabilistic methods, is that their output is not deterministic , and therefore the quality of the generated clustering is dependent upon the initial configuration . Consensus clustering part ially addresses this problem by generating an ensemble of multiple clusterings and then combining the results through use of a voting mechanism . In this way, a higher quality and more robust set of final clusters can be generated . There are many ways to build a clustering ensemble. In addition to various initialization parameters, multiple clusterings can be obtained by using different feature selection methods or instance sampling methods. For ensemble integration, m any consensus clustering algorithms are based on the concept of a n N by N co -association matrix M . Let be the set of instances to be clustered . A clustering ensemble represents R partitioning s of where the partitioning represents a set of clusters such that . T hen each element of the co -association matrix M represents a voting score between a pair of instances where is the number of times the instance pair is assigned to the same cluster over the ensemble. The underlying assumption of using a co -association matrix is that instances that should be placed together are very likely to appear in the same cluster across multiple clusterings . Usually either hierarchical clustering or graph partitioning algorithms are used t o generate the final partition ing from M . Hierarchical clustering was adopted in [ 16,18,24 ], which used single -link or average -link agglomerative hierarchical clustering algorithms [ 20 ] to clust er over a co -association matrix, while graph partitioning was used in [ 17,22 ], whi ch transformed the co -association matrix into a weighted graph and then partition ed the graph into K parts through finding the K disjoint clusters of vertices wit h the objective of minimizing the multi -cut. The results reported in these papers have demonstrated the effectiveness and robustness of consensus clustering. T o address the problem of generating more informative constraints and improving the quality of constrained clustering especially for software requirements , we propose a unified consensus based constrained clustering framework . This framework, which is depicted in Figure 1 , is comprised of the two main phases of constraint generation and constrained clustering. In phase 1, an initial clustering ensemble and a related co -association matrix M are generated, and used to identify a set of constraints. In phase 2, these constraints are used to generate a second improved clustering ensemble against which a third clustering is performed to create the final output P *. In related work, Yan et al . [ 27 ] used a consensus -based approach to cluster genes. They divided the feature space into random subsets and then applied a distance -metric learning -based constrained clustering to each feature subset, producing multiple subspace clusterings. Our approach adopts some of the same techniques as Yan et al, but is customized to our domain thr ough our selection of algorithms for building the ensemble, selecting constraints, and generating the final partitioning. These phases are described in more detail below. The 1 st ensemble in our framework is created through rando mly selecting and clustering subsets of the documents from the full feature space, and then generating complete partitions through classification. Co -association matrix M i s then produced by summing the frequency with which each pair of instances occurs together across all of the clusterings in the ensemble.
 To generate the 2 nd ensemble, our framework does not choose constraint s randomly, but pair -wise constraints ML/CL are selected only from the subset of instance pairs in M that exhibit voting scores within a given interval window [ a , b ] . This bounding method is statistically justified in section 4 . 2 and empirically validate d in section 5 , however for now we simply provide an intuitive explanation of the underlying idea . As previously stated, it is important to select constraints that maximize the benefits of constrained clustering. Intuitively speaking, it is likely that the greatest benefits will be obtained by identifying borderline relationships. For example, pairs of instances exhibiting either very high or very low proximity scores, already tend to get correctly placed together or separately by the unsuperv ised clustering algorithm, and so a sking a user to categorize t hem as ML or CL provides less useful information than asking for f eedback on pairs of instances with more intermediate proximity values . In this section we provide a probabilistic analysis for bou n ding the voting score of target constraints in order to obtain more informative constraints . In the general case, a random pair of instances are selected from the entire voting score range of [0,1]. In the restricted case, the lower limit is increased so as to eliminate as man y obvious CLs as possible, and the upper range is decreased in order to eliminate as many obvious MLs as possible . The remaining interval is expected to contain a large number of border constraints. Formally, we define a constraint l with a voting score in the co -association matrix, as a border constraint if the probability that l belongs to ML is close to the probability that l belongs to CL , namely given that . Even though the exact forms of these two probabilities are hard to attain, assuming there exist s a range where most border constraints occur, we can reason about an important property of such a range. Let constraints will be drawn , then if a n occurrence amou n ts to drawing a constraint from w , as each drawing has a n approximately equal probability of being ML or CL , the distribution of drawing ML times out of total draws is apparently which has the mode constraints can be relaxed to finding a window over which there exists an approximate equal probability of drawing either an ML or a CL, namely, For a specific data set, we can calculate and MLs and CLs whose voting scores are between a and b , and then divid ing them by the total number of possible constraints. In a purely random generation of constraints, where the window w is set to [0,1], the scores of a,b are listed in the first column of Table 2 . For all of them, especially for the SUGAR and STUDENT, the difference between P 0,1 ( l ML ) and P 0,1 ( l CL ) is significant. In contrast, by narrowing the window w and moving it away from 0, will be usually lower than random. As can be seen in the second data column of Table 1, when we applied a narrower bounded window , and that th e bounded constraint selection will generate more informative constraints in most cases. We will validate this hypot hesis empirically in section 5.5 .3. Finally it should be noted that this bounded strategy based on co -association matrix for constraint gene ration is generic and should therefore be applicable across different data types and proximity metrics. In our framework, once constraints are selected and defined as either ML or CL, another set of partitions are generated using a collection of constrained clustering algorithms based on the constraints generated from M . Although it is possible to use constraints to supervise almost any basic cluste ring technique, Wagstaff [ 25 ] and Davidson et al. [ 11 ], demonstrated that most constrained clustering algorithms are sensitive to the order in which the constraints are applied and in which instances are assigned to clusters. In fact some orderings may lea d to clusterings that are worse than those produced by a fully unsupervised algorithm. By using consensus integration of multiple constrained partitions, incorrect placements in one clustering can be outvoted by correct placements in others. To enhance the diversity of the clustering ensemble, it is helpful to permute both the order of instances and constraints before applying a constrained algorithm. In our framework, a new co -association matrix is d erived from the constrained ensemble clustering [ 20 ] is applied on the co -association matrix to obtain the final clustering P* . Although s everal choices exist for clustering the co -association matrix M , such as variants of hierarchical a gglomerative clustering , spectral clustering, and graph partitioning algorithms , the a verage -link algorithm was chosen as it was demonstrated in our experiments to be very stable across ensembles of various si zes and characteristics . A series of experiments were conducted to evaluate the effectiveness of both consensus clustering and our constrained consensus -based framework for clustering requirements documents. The Normalized Mutual Information ( NMI ) was used to measure a greement between the consensus or constrained clustering and the reference clustering. NMI measures the extent that the knowledge of one clustering reduces uncertainty of the other. Formally, NMI first calculates the mutual information between two partitions and where k a and k b are the cluster numbers of two partitions, n is the total number of instances , is the number of shared instances in cluster of clustering and cluster of clustering , and a similar explanation applies to . The result is then normalized using an arithmet ic mean, re -scaling it to the range [0,1]: where is the entropy of a clustering [ 18 ]. S ection 5.1 describes the data used throughout the experiments , s ection 5.2 reports the experimental results of consens us clustering, and then section 5.3 , 5.4, and 5.5 describe the setup and results of the experiment using consensus -based constrained clustering. The experiments were conducted using six well -known TREC data sets , Tr11, Tr12, Tr23, Tr45, Tr41, and Tr31 [ 23 ] , as well as two domain representative sets of requirements named STUDENT and SUGAR respectively . STUDENT represents a small collection of 366 feature requests created by 36 graduate level students for an Amazon -like student web -portal system. A reference set, which created an  X  X deal X  clustering, was developed by two of the researchers in the SAREC lab. SUGAR is comprised of 1000 feature requests mined from SugarCRM, an open source customer relationship management system that supports campaign management, email marketing, lead management, marketin g analysis, forecasting, quote management, case management and many other features. The feature requests were contributed by 523 different stakeholders over a two year period, and distributed across 309 threads. For the SUGAR data, a reference set was con structed through reviewing and modifying the natural discussion threads created by the SugarCRM users. Modifications included merging singleton threads with other relevant ones, manual re -clustering of large mega -threads into smaller more cohesive ones, a nd then manually reassigning misfits to new clusters. In preprocessing, for each of the eight data sets, stop words are eliminated, remaining words are stemmed to their root forms, words that occur less than three times are also eliminated, and then all wo rds are indexed using tf -idf . Each requirement is then represented as a vector of real values in which each position corresponds to a term in the document space . Finally, for better performance in calculating the inner product of vector pairs, each vector is normalized to unit length. An experiment was conducted to evaluate the improvements obtainable by using consensus clustering on the six TREC datasets and two requirement data sets STUDENT and SUGAR. A clustering ensemble of size R was produced by repeating the following sub -sampling steps R times. A proportion of the whole dataset was randomly extracted and then partitioned into K clusters using spherical K -means (SPK), which is described in section 5. 3 . The remaining instances were then classified into the most closely related clusters. Based on extensive experiments applied to several TREC do cument data sets, we determined that a quality ensemble containing viable yet dissimilar clusterings, for data sets with several thousand s of data points could be generated by setting R to 2 00 and to a value in the range [ 0.5 , 0.8] . The experiment comp ared the NMI scores of each of these datasets when clustered using 2 -stage SPK versus the consensus algorithm described in the previous section, which used average linkage hierarchical clustering to partition the co -association matrix. Because SPK generat es different results depending upon initial seedings, the algorithm was run 200 times for each dataset, and minimum, maximum, and mean scores are reported. These results, which are depicted in Figure 2, show that consensus clustering results were always a bove the mean obtained using SPK, but usually just below the maximum . An interesting exception to this is that for the SUGAR data, representing a very realistic medium sized project in our target domain, the consensus clustering scores were higher than th e maximum obtained using SPK. Additional work is needed to build more domain related answer sets so that we can investigate this phenomenon further. In general, these results are highly significant because they demonstrate that consensus clustering is mo re consistent and robust than Sph erical K -Means.
 The concept of co -association matrix M was introduced in section 3 . For the remainder of this discussion, p air -wise values in M are referred to as voting scores (VS), while the values in the original similarity matrix O , are referred to as similarity scores represented by the cosine direction values between pairs of instance vectors. To understand why consensus clustering can improve the clustering quality, the distributions of voting scores and similarity scores for the six well -known TREC data sets Tr11, Tr12, Tr23, Tr45, Tr41, and Tr31 were analyzed in respect to knowledge of MLs and CLs from the published answer sets.
 In Figure 3 , e ach of these TREC data set s is represented by a single col umn. The graphs in the first row show the voting score distributions of pairs of documents assigned together in the answer clustering (namely ML) in M , while the second row shows the voting scores of pairs that are not assigned together in the answer clustering (CL) in M . The third and fourth rows show similar values for the cosine values in O . NMI scores , depicting the similarity between the consensus clustering and the published reference clustering, are show n on the top row in parenthesis next to each dataset name.
 These results clearly indicate that all of the datasets had similar distributions of ML and CLs for the original proximity matrices, with very high concentrations over the range near 0 . In contrast the scores in the co -association matrix provided a cleare r differentiation between MLs and CLs by boosting ML scores. This implies that the pair -wise voting scores compiled from the consensus algorithm tend to approach the ir  X  X rue X  similarity scores. It also explains why the proximity -based algorithms such as ag glomerative hierarchical clustering give poor results on the similarities directly from O but perform much better on M . It can be observed that Tr11 and Tr45 score d the highest NMI values while Tr23 score d lowest, and it is interesting to note that th ese differences correspond to the different shapes of ML/CL distribution shown in Figure 3 , where the easy -to -cluster Tr11 and Tr45 have high concentrations approaching one (i.e. towards the right of the graph) for ML and more concentrations towards 0 for CL, while the hard er -to -cluster Tr23 did not exhibit such strong differe ntiation, meaning that even in the co -association matrix there was still significant disagreement about the MLs. To increase the variability of the constrained partitioning used in phase two of our framework, two consensus -base d constrained partitioned clustering algorithms were selected: COPK -means [25 ] and PCK -means [ 3 ]. Since they are both built on basic K -means algorithms, we will describe our edition of spherical K -means first and then point out the different ways in which COPK -means and PCK -means enforce constraints .
 A two -stage spherical K -means approach was adopted for experimental purposes (Figure 4 ) which appends an incremental optimization of the objective function after the usual batch instance assignme nt of K -means. This enhancement is critical, especially when cluster sizes are small. In a series of experiments, this simple optimization consistently demonstrated improved NMI values , as will be shown in section 5. 5 .1.
 The original COPK -means and PCK -means are based on the K -means algorithm shown in Figure 4 and differ only in their methods for constrained instance assignment, which corresponds to step 2a in the algorithm. COPK -means enforces a hard constrained instance assignment in which an instance fro m cluster is assigned to the nearest cluster if this assignment results in no conflict, namely, for each , , and also f or each , . PCK -means, on the other hand, applies a soft constrained assignment for whic h instance is assigned to whichever cluster maximizes where is the cluster label for constraint -involved instance , is a binary function that returns 0 if boolean variable is false and 1 otherwise, and w * is a penalty parameter, which is used to adjust the  X  X ardness X  of the constrained assignment and set to 0.001 in our experiments [3] .
 Unlike many previous experiment s [ 3,23 ], our implementation of COPK -means and PCK -means not only appl ies constrained instance assignment to the usual batch assignment stage (step 2a in Figure 4 ), but also to the incremental optimization stage (step3a in Figure 4 ). Other clustering t echniques such as metric learning enhanced constrained clustering, including HMRFK -means [ 4 ] or MPCK -mean [ 6 ] , were not used to help build the ensemble, as they are computationally expensive and exhibit coarse approximations for clustering high -dimensional data in which using a covariance matrix to calculate Mahala no bis distance is not precise. Furthermore, as the experiments in [ 23 ] have empirically shown, when applied to document clustering, these approaches did not introduce significant quality improvem ent. Pair -wise constraints ML/CL were generated using the bounde d consensus method described in the previous section for which the selection window [ a , b ] was set to [0.1, 0.5]. For comparison purposes, the results were also compared to random constraint generation. This choice of window needs some explanation . In a series of experiments we found that narrow windows span ning less than 0.2 , or windows approaching either 0 or 1, d id not return good results. These results support our earlier suppositions that performance would improve if constraints were selected from the area of low confidence in the center of the scoring range. Optimal results were found using either a window of [0.1,0.5] or any other intermediate wind ow with a span &gt; 0.2 . MLs were preprocess ed before being used in the constrained clustering algorithms ; specifically, connected components were identified among the instances involved in ML, CL constrai nts were propagate d along these connected components and then each connected component was treat ed as a weighted input instance to the clustering algorithms. In this way, only CLs need ed to be handled by constrained clustering algorithms COPK -means and P CK -means. A total of 20 clusterings were generated, 10 using COPK -means and 10 using PCK -means. Constraints were applied in random order and instances were also processed in random order so as to enhance the diversity of the ensemble. The entire experiment was repeated ten times, and the average scores reported. 2 00 rounds of the spherical K -means were run with and without incremental optimization. These are labeled S PK and SPK -I respectively, and average performances are shown in Table 3 . These results show that a substantial difference exists between the two editions of spherical K -means , and that the post processing was quite effective at improving cluster quality . Moreover, it appears that a smaller average cluster size leads to a more significant difference showing a high negative linear correlation of -0.83. This could explain why the results reported in [ 23 ] often show a steep rise of performance even only given 10 constraints, as those constraints gain back some of the improvements that would have been achievable in a mo re optimal clustering algorithm . Bounded constraints from 10 to 100 with an interval of 10 w ere generated and applied to three approaches of COPK -means, PCK -means, and consensus clustering of constrained ensemble, denoted as B -COPK, B -PCK, and B -Cons respectively in Figure 5 . The performance wa s then compared with the SCREEN approach reported by Tang et al [ 23 ]. As there were insufficient details to re -implement SCREEN for additional datasets , the comparison was only made against the six TREC data sets for which results were extrapolated from the published graphs [ 23 ]. The results are shown in Fi gure 5 . C onsensus integration over a constrained partition ensemble (B -Cons) significantly outperformed base algorithms COPK -means and PCK -means, and was generally comparable to SCREEN. Furthermore, excluding STUDENT and SUGAR, the data sets over which consensus constrained clustering outperforms SCREEN are Tr12, Tr23, and Tr45, which are in fact the three data sets which contain the shortest documents amo ng all six sets. This suggests that our proposed approach might be highly applicable for clustering requirements documents, which tend be relatively short. This experiment investigated the difference between bounde d consensus based constraint generation and random constraint generation over a much larger number of constraints ranging from intervals of 50 up to 1000 constraints. As COPK -means tends to 
Table 3 . Performance of SPK and SPK -I . N is the number suffer from a feasibility problem when presented with a large number of CLs, and because it has been shown to have comparable performance to PCKmeans, it was not used in any of the additional experiments. The results of this experiment are shown in Figure 6 . The results of PCKmeans, consensus clustering using bounded and random constraint generation as well as baseline spherical K -means are denoted as B -PCK, B -Cons, R -PCK, R -Cons, and SPK respectively in the plots.
 The TREC datasets returned rather mixed results. For example, the bounded approach appeared to do better for datasets Tr11 , Tr41 , and Tr45 , but did not do well in datasets Tr23 and Tr31 . Furthermore, despite a good start in Tr12 , there was a significant drop off after about 600 constraints. An initial observation of these results suggested that the bounded approach did best when applied to datasets with a larger number of small clusters , meaning that there could be more borderline cases. For example, datasets Tr11 , Tr41 , and Tr45 which performed well had 9 or 10 clusters each, while Tr23 and TR31 had only 6 cluster s each . TR12, which also started off well and then dropped off had 8 clu sters. It should be noted however, that datasets can be clustered at any level of granularity, but that an ideal granularity exists for each dataset in respect to a given task. Our observations are therefore based on fixed levels of granularity. In th e software engineering domain, tasks such as feature extraction and requirements management typically require fine levels of granularity, in order to create manageable clusters which can be used by stakeholders to perform their tasks [ 7,8,14,15 ]. As repor ted in our prior work, SUGAR is clustered at a granularity of 40 clusters resulting in average sized clusters of 25 , while Student has 2 9 clusters containing an average of 13 feature requests. These fine grained granularities suggest that requirements doc uments are potentially highly suited to the bounded constraint generation. In fact, the results for these two requirements datasets, which are also depicted in Figure 5 , show a marked improvement obtained through using bounded constraint generation. Figure 7 provides a more subjective view of our results, by depicting twelve feature requests from the STUDENT dataset clustered around the general topic of security. The cluster was generated using the consensus -based constrained framework with 1000 cons traints. This particular cluster contained 21 requirements, 20 of which were judged to be security related with only one conceptual misfit that referred to customer transactions but had no reference to security. A subjective analysis of the clusters produ ced by the constrained consensus -based framework showed a significant improvement in the cohesiveness of each of the clusters. These observations were supported by the increase in NMI scores achieved during the experimental analysis. This paper has described a new framework for clustering high dimensional data sets such as requirements documents . The framework adopts a hybrid model which combines both c onsensus and constrained clustering techniques , and in which constr aints are selected that are expected to maximize supervisory potential for improving cluster quality . The reported experimental results demonstrated the effectiveness of th is approach especially for clustering short documents into finely grained partition s. These characteristics closely match those of the targeted requirements domain, and in fact the clustering results were especially promising for the SUGAR and STUDENT datasets. In future work we intend to build a far more extensive set of requirements r elated datasets and corresponding answer sets, so that we can further assess and fine -tune the usefulness of our framework.
 The work in this paper was primarily motivated by our research in automating and scaling up components of the requirements process, and our subsequent observations that rudimentary clustering techniques did not produce sufficiently cohesive clusters to support our intended tasks. T he clustering improvements obtained through use of the framework described in this paper, have significantly mitigated this problem, to the extent that they are anticipated to support future research and tool development that will enable us to move towards higher levels of automation in the requirements engineering domain. The work described in this paper was partially funded by NSF grants CCR -0306303, CCR -0447594, and IIS -0430303. [1] Banerjee , A. and Ghosh, J. 2002. Frequency sensitive [2] Basu, S., Banerjee, A., and Mooney, R. J. 2002. Semi -[3] Basu , S., Banerjee, A. , and Mooney , R. J. 2004. Active [4] Basu, S., Bilenko, M., and Mooney, R. J. 2004. A [5] Bennett, K.P., Bradley, P.S. and D emiriz, 2000. A. [ 6 ] Bilenko, M., Basu, S., and Mooney, R. J. 2004. Integrating [ 7 ] Castro -Herrera, C., Duan, C., Cleland -Huang, J. and [ 8 ] Cleland -Huang, J. and Mobasher, B. 2008. Using Data [ 9 ] Cohn, D., Caruana R., and McCallum, A. 2003. Semi -[ 10 ] Davidson, I. and Ravi, S.S. 2005. Hierarchical clustering [ 11 ] Davidson I. and Ravi S.S. 2006. Identifying and Generating [ 12 ] Davidson I., Wagstaff, K., and Basu, S. 2006. Measuring [ 13 ] Davidson, I. and Ravi, S. S. 2007. Intractability and [ 14 ] Duan, C. and Cleland -Huang, J. 2007. Cl ustering support for [15] Duan, C., Clustering and its Application in Requirements [ 16 ] Fern, X. Z. and Brodley, C. E. 2003. Random projection for [ 17 ] Fern, X. Z. and Brodley, C. E. 2004. Solving cluster [ 18 ] Fred, A. L. and Jain A. K. 2005. Combining Multiple [ 19 ] Greene, D. 2007. Constraint Selection by Committee: An [ 20 ] Jain, A. K. and Dubes, R. C. 1988. Algorithms for Clustering [ 21 ] Li, T., Ding C., and Jordan M.I. 2007. Solving Consensus [ 22 ] Strehl, A. and Ghosh, J. 2003. Cluster ensembles  X  a [ 23 ] Tang, W., Xiong, H., Zhong, S., and Wu, J. 2007. Enhancing [ 24 ] Topchy, A., Jain, A. K., and Punch, W. 2003. Combining [ 25 ] Wagstaff, K., Cardie , C., Rogers, S., and Schr X dl, S. 2001. (1) The system shall protect stored confidential information. (2) System must encrypt purchase/ transaction information. (3) A privacy policy should be implemented to describe in (4) Transmission of personal information should be encrypted. (5) Transmission of financial transactions should be encrypted. (6) The system must use encrypt &amp; decrypt in some fields. (7) Allow the user to view their previous transactions. (8) Databases should use the TripleDES encryption standard for (9) The site should ensure that payment information is kept (10) Because our system will be used buy books, then we should (11) Correct usage of cryptography techniques should be applied (12) Sessions that handle payment transactions have to be 
