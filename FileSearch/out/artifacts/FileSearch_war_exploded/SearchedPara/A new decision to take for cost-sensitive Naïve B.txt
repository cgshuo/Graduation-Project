 1. Introduction
The task of data classification is today commonly applied in many contexts, ranging from customer target marketing to as non-spam is less serious ( Kolcz, 2005 ).
 costs that accompany such decisions. In this cost-sensitive framework, we can choose a learning algorithm and optimise problems, i.e. when we have only two categories c 1 and c one category for an object o if the likelihood ratio (which is the ratio between the two probabilities P  X  o j c  X  fication costs (the details of this formulation are presented in Section 3 ): where t is the threshold that depends on the misclassification costs and on the priors of the two categories, P  X  c 2011; Metsis, Androutsopoulos, &amp; Paliouras, 2006 ).
 constant from the discriminant function for its negation X  X . This decision can be written in the following way:
To the best of our knowledge, this simple step, which has been empirically shown to be very effective, has never been formally proven.
 The main contributions of this paper are: the following linear form: where m and q depend on the mis-classification costs and can be seen as the angular coefficient and the intercept of a linear function. Note that for q  X  0 we can derive Eq. (1) with m  X  (sub)optimal decision on a two-dimensional space.
 We use this graphical interpretation to analyse different approaches to find the best decision on four different Na X ve
Bayes (NB) classifiers: Gaussian, Bernoulli, Multinomial, and Poisson, on different standard collection. We show that the graphical interpretation significantly improves the understanding of the models and opens new perspectives for new research studies.
 some of the related works, while in Section 8 , we give our final remarks. 2. Binary classification
Often, real world classification problems have more than two classes, for example a set of classes C  X f c set C . Given an object o and a set of categories C , if we want to decide whether o should be assigned to category c build a simple probabilistic classifier that checks the following statement: where c i  X  C n c i . Therefore, if the probability of the class c the object to c i . 1 Since we do not know the value of P  X  c in order to predict the probability P  X  c i j o  X  we need to reverse it by using the Bayes rule: where P  X  c i  X  is the probability of choosing the category c object o randomly, and P  X  o j c i  X  is the likelihood function. P  X  o j c estimate by means of some training examples. The hypothesis we make about how the mathematical model influences the as a set of Normal distributions (see Appendix A for examples of different models and details about the use of the independence assumption). In any case, once we have the model we are able to compute the probability P  X  o j c the probability of the category given the object.

The probability P  X  c i j o  X  is a combination of three variables: P  X  o j c the two categories are not balanced, i.e. P  X  c i  X  6  X  P  X  c usually towards the  X  X  X tronger X  X  category ( Domingos &amp; Pazzani, 1997 ). For example, suppose that P  X  c time), but the particular object o we are observing is much more likely to be generated by c object, we obtain: Which means: justification of this constant within a Bayesian decision theory framework. 3. Bayesian decision theory accompany such decisions. Whenever we have an object to classify, if we take the decision to classify it under c function k  X  c i j c j  X  which tells us what is the loss in taking the decision c the quantity of risk we are taking in choosing that decision. A conditional risk R  X  c minimum: which, by using Eq. (6) , is equal to
We can group common terms and obtain:
In binary classification problems, it is very common to use a zero X  X ne loss function. This function has the terms k  X  c j c i  X  X  k  X  c i j c i  X  X  0 which means that we have no loss when we give the correct answer, and k  X  c to Eq. (3) .
 Different costs can be used to balance the marginal probabilities of the categories:
For example, we can decide to set the costs of the loss function to completely cancel out the marginal probabilities, k  X  c i j c i  X  k  X  c i j c i  X  X  P  X  c i  X  and k  X  c i j c i  X  k  X  c ing the class with the highest marginal probability as the one with the highest risk of error. two differences. Therefore, we have only one parameter, and this parameter can incorporate the ratio algebra to generalise (12) by including the additive factor as suggested by Domingos and Pazzani (1997) . 4. Conditional risk with additional costs Domingos and Pazzani suggest using a constant to adjust the decision function of the Bayes classifier ( Domingos &amp; with standard approaches for developing cost-sensitive classifiers which weight the training examples according to the  X  X ostliness X  of misclassifying that example ( Zadrozny, Langford, &amp; Abe, 2003 ). the higher risk we take. The following formula adapts these two terms in the definition of conditional risk: where the new cost k 0  X  c i j c j  X  has been expanded in two addends k  X  c which indicates that for the decision  X  X  X ategorise under c
Proposition 1. Let us consider a set of categories C  X f c and a loss function k 0  X  c i j c j  X  , if we can find for each category c k  X  c j c j  X  X  k  X  c i j c j  X  X  l c i P  X  o  X  such that one term is the original loss function k  X  c divided by the probability P  X  o  X  , then the conditional risk is Proof. By expanding Eq. (16) , we obtain:
At this point, we can show how our new definition of conditional risk in terms of this new loss function is the bridge between the empirical definition of the discriminant function of Domingos and Pazzani (1997) and the likelihood ratio ( Duda et al., 2001 ).

Proposition 2. Let us consider a set of categories C  X f c and c i with Eq. ( 17 ) , the optimal decision function which minimises the Bayesian risk is:
Proof. Let us assume that the original cost function meets the  X  X  X easonableness X  X  conditions ( Elkan, 2001 ) where k  X  c j c i  X  &gt; k  X  c i j c i  X  and k  X  c i j c i  X  &gt; k  X  c
Definition 1. A  X  X seudo X  zero X  X ne loss function is a loss function k k  X  c i j c j  X  X  k  X  c i j c j  X  X  l c i P  X  o  X  , with k  X  c Eq. (22) contains two important results: particular, whenever we use a decision function like: parameter l incorporates the difference l c i l c i , the difference k  X  c
Second, it is a generalisation of Eq. (12) . In fact, when l for a mis-classification, we obtain: 4.1. Likelihood space coordinates of an object in this two-dimensional  X  X ikelihood space X  are x  X  P  X  o j c
Bayesian classifier can be viewed as a simple linear discriminant. classical conditional risk approach, we can only adjust the slope m of the linear function. On the other hand, if we group the terms of Eq. (22) , we obtain: 4.1.1. A toy example
In order to show how the intercept q of the linear discriminant function can radically change the decision of a binary and the class of circles with only two examples. The class of triangles is our  X  X ositive X  class c solution found by Eq. (22) .
 terms of constant costs. 5. Experiments
The experiments we present in this section are designed to tackle the following research questions: performance of the classifier, e.g. the sample size, the learning model, the algorithm to find the decision function.
Volume I (RCV1) dataset. The two Reuters datasets and the 20 Newsgroup dataset are so sparse that we could not work change anything in the formulation of the problem, but we need to be more careful when we interpret the decision line. See Section 6.6 for more thoughts on this issue.

We implemented the cost sensitive learning for NB binary classifier code in R, and we made available all the processed datasets for future analysis (details of the URL in the following sections).

In every experiment, in order to avoid zero-probabilities we used Laplacian smoothing. We compare the results of our non-linear kernel in some cases.
 For each category and for each model, we found the best parameters by optimising the F1-measure on the training set. The F1-measure is defined as the harmonic mean between Recall (R) and Precision (P): averged F1 measure ( Sebastiani, 2002 ) which is the average of the F1-measure across all the categories. 5.1. Datasets 5.1.1. Iris dataset classification contains real value data, which make this dataset a good benchmark for a Gaussian NB classifier. Introduced by Fisher dataset available with the standard R installation. 7 5.1.2. Reuters-21578 text classification model the documents (Bernoulli, multinomial, Poisson) and compare the three probabilistic models.
The top 10 most frequent categories of the Reuters-21578 8 composed of 6494 documents and the test set of 2548 documents. Table 2 shows the number of training and test documents frequent words of the English language. 9 Finally, the English Porter stemmer available online. 11 5.1.3. 20 Newsgroups dataset
The 20 Newsgroup is another standard dataset for classification. It contains approximately 20,000 newsgroup documents, partitioned almose evenly across 20 different categories. and test documents are shown. Some of the categories are closely related to each other (i.e. comp.sys.ibm.pc.hardware/ and does not include newsgroup-identifying headers. 13 5.1.4. Reuters Corpus Volume I (RCV1) collection Reuters Corpus Volume I (RCV1) is an archive of over 800,000 manually categorised newswire stories made available by available by the original authors of the collection were used to train the categorisation system. 5.2. Learning the best linear discriminant function
In the experiments, we used the following three approaches: a zero X  X ne (0 X 1) loss function, a simple learning ( sim ) compute the performances on the test set.

Micarelli, 2004 ) in the following way: fit a linear model for the positive training examples Y c and another linear model for the negative training examples Y find the bisecting line y  X  m x  X  q of the angular region created by Y an example of angular region and the solutions found by the simple and the advanced learning approach on the same random generated dataset.
 SVM) with performances that are very close to the optimal ones.
 Algorithm 1. Angular Region algorithm ( Di Nunzio &amp; Micarelli, 2004 ) adapted to the case of Likelihood Spaces Require: X c i ; Y c i ; X c i ; Y c i ; ; d
Ensure: m opt ; q opt 6. Results and discussion a more specific analysis of the factors that can affect the performances (Section 6.5 ). 6.1. Iris dataset show the F1 values of: produce better results, but with a very slow training time.
 ues of one example of a flower, P  X  o j c i  X  and P  X  o j c
The dashed line represents the solution found by Eq. (31) where we can adjust the angular coefficient but we are still (one more triangle below the line) and decrease precision (one more circle below the line).
There is one last thing we need to remark regarding the comparison between NB and SVM. In this experiment, we transform the multi class classification problem of n classes into n binary classification problems. However, The SVM implementation of the e1071 R package can solve a multi class classification problem with a one-against-one-approach, cases where an object belongs to more than one class (like the Reuters-21578 dataset). problem, the reformulation of conditional risk is a very promising approach which can make a big difference on the bigger and more sparse datasets. 6.2. Reuters 21578
The overall results of the experiments on the Reuters-21578 dataset are summarised in Table 7 . For each category, we magnitude greater than the linear SVM.
 a clear advantage in using the multinomial approach. This result has been confirmed by many experiments in the past has a slight advantage over the others. Results with SVM are comparable and the magnitude of the difference between NB and SVM it is very similar to the one found for the Iris dataset. 6.3. 20 Newsgroups the F1 measures of the following models (both trained with the simple and the advanced approaches are summarized in the default linear SVM. We did not optimise any costs for SVM because the performance with the default training were multinomial model. Note that the other two models, Bernoulli and Poisson, have a macro-averaged F1 which is even lower algorithm, around 90 s to train one category for SVM with default values.
 6.4. RCV1
When we use the reformulation of the conditional risk, the difference among the three NB models almost disappears. For implementation of the SVM in the e1071 R package is not well optimised. We will further investigate these issues in future experiments.
 6.5. Analysis of the results
In order to analyse these results carefully, we discuss four important issues separately and in the following order:
We will use Figs. 4 X 6 to illustrate some of the points about these three issues. 6.5.1. Influence of the sample size
There are several reasons which explain why the number of training examples of a class influences the classification corresponds to the sample size. Therefore, when we estimate the parameters of our NB model, we have some uncertainty we mean to what extent the sample is generalizable and representative of the entire population. The Reuters-21578 is a performance of the classifier, especially on smaller categories. For example, the distribution of unique words in the documents changes dramatically: in the training set, there are almost 28,000 unique features in about 6500 documents, while in the test set we have 7000 new unique words, which is almost 25% more, in only 2500 documents. This problem is thousands features, for the 8000 test documents we have about 7000 additional unique words, which is almost 12% more. about 240,000 unique words that are not present in the training set. This problem may be less perceivable for complex the category, the lower the performance. 6.5.2. Influence of the model becomes an issue for two of the models: the Bernoulli NB model and the Posson NB model. presenting the numbers of the performances. We can clearly see that in two cases, Bernoulli and Poisson, documents are model we have the term P m k  X  1 log  X  1 h f k j c i  X  (Eq. (A.4 )), and in the Poisson model we have the term (Eq. (A.10) ), while in the multinomial model a summation over the whole terms is absent.
The following question is: why are these addends only affecting the X axis? Usually, Y remains quite stable since it the Reuters dataset. 16 Suppose that we choose a Bernoulli model and that for 500 features of the positive category c this 500 features in the term P m k  X  1 log  X  1 h f k j c that for the same 500 features we get an estimate equal to h all the categories of the 20 Newsgroups dataset and the RCV1 dataset. 6.5.3. Influence of the definition of conditional risk recover the fact that the points are shifted along the X axis by adjusting the costs of the conditional risk. 6.5.4. Influence of the learning algorithm available solution among the many possible ones, 17 we use an SVM to find the maximum margin hyperplane on the two compared to the angular region algorithm.
 better compromise between training time and efficacy of the classifier.

There is something important about the quality of the distribution of points on the log-likelihood space that can be model scatters points further apart and it is easier to find the space for a solution. 6.6. Further comments and future work
In this section, we want to give some final remarks about the experiments we presented and reflect on possible future work.
 function. This non-linear decision function has the following equation: we can use the first order Taylor series expansion of the logarithm as a rough approximation of the logarithm: where in the last passage we further approximate by relaxing the constant one. best logarithmic curve, instead of its approximation, for future work.
 consequence of the reduction of the shift on the X axis since we are reducing the contribution of the sum
P problem is an important future work that requires a careful analysis.

Another problem is the question of how probability smoothing affects the performance of the NB model. In ( Eyheramendy effect of the shift on the X axis, which is also confirmed by recent experiments ( Di Nunzio &amp; Sordoni, 2012 ).
Finally, we think that there are some connections between the optimal hyperplanes and hard margin separating if we were to add constraints to the linear decision function, like maximising the distance between the points and the find with the weighted SVM, but we believe we need a better approach for parameter optimisation to better understand the relation between the two. 7. Related works 2001 ) and, for a different view of the same problem, another on signal detection theory ( Van Trees, 2004 ).
On of the best papers to study the optimality of NB models is the seminal paper by Domingos and Pazzani (1997) and the to produce optimal ranking of documents. Lewis also defined the Probability Thresholding Principle for automated text on the probability of the positive class: et al., 2006 ), the authors present a spam filtering approach with NB classifiers which uses formulation P  X  c the system.
 adjustment.

About NB models and binary classification we can identify the following works which study: the mathematical model, the model and propose a variation to incorporate document length in the models. The work presented by McCallum and Nigam (1998) makes a very clear presentation on NB models in general. In particular, they compare the multinomial and the sometimes performs better than the multinomial at small vocabulary sizes. However, the multinomial usually outperforms 2003 ), the authors review the systemic problems of the NB classifier, in particular when one class has more training retrieval problems ( Di Nunzio, 2009 ). 8. Conclusions performance of each of the classifiers.
 classifiers.
 Acknowledgements N. 247590 (FP7/2007-2013).
 Appendix A. Na X ve Bayes models
This formulation is very convenient if we apply the log transformation because it transforms the probability P  X  o j c Gaussian model and the mathematical expression of each log-likelihood.
 A.1. Multivariate Bernoulli model In the multivariate Bernoulli model, an object is a binary vector over the space of features. Given a set of features
F ; j F j X  m , each object o j is represented as a vector of m Bernoulli random variables o variables are independent given the class variable. Formally: where x k is either 0 or 1 indicating whether feature f k performed: A.2. Multinomial model ing that each feature event is independent of each other, an object o correspond to features. This vector is drawn from a multinomial distribution: where N k ; j indicates the number of times feature f k appears in the object o plugged into the decision function):
We apply the log transformation to avoid zero-probabilities: A.3. Poisson model
In the Poisson model, an object is generated by a multivariate Poisson random variable. Each object o m -dimensional vector of frequencies, o j  X  N 1 ; j ; ... ; N Using the NB conditional independence assumption, we can write the probability of the object as: and, by taking the logs we obtain: A.4. Gaussian model
In the Gaussian model, an object is generated by a multivariate Gaussian random variable. Each object o an m -dimensional vector of real values, o j  X  g 1 ; j ; ... ; g Using the NB conditional independence assumption, we can write the probability of the object as: and, by taking the logs we obtain: References
