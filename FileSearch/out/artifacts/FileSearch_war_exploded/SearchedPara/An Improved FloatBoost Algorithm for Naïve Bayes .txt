 Text classification is the activity of automatically building, by means of machine learning techniques, automatic text classifiers, i.e. programs capable of labeling natural language texts with thematic categories from a predefined class set. A wealth of different methods have been applied to it, including probabilistic classifiers, decision trees, decision rules, regression methods, batch and incremental linear methods, neural networks, example-based methods, and support vector machines (See [2] for a review). In recent years, the method of classifier committees has also gained popularity in the text classification community. 
The boosting method [1] occupies a special place in the classifier committees literature. Since the boosting technique was developed [1], it has been considered to be one of the best approaches to improving classifiers in many previous studies. In particular, boosting contributes to significantly improve the decision tree learning algorithm [3,4]. FloatBoost [6] is an improved AdaBoost method for classification, which incorporates into AdaBoost the idea of Float Search, originally specified in [5] for feature selection. FloatBoost achieves a stronger classification consistency of fewer weak classifiers than AdaBoost and has shown its performance in face detection field [6]. 
When boosting is used to handle scenarios in complex environment with outliers, its limitations have been pointed out by many researchers [4,7], some discussion and approaches have been proposed to address these limitations [8,9]. In [8], S-AdaBoost algorithm which applying the Divide and Conquer Principle to the AdaBoost algorithm was proposed to enhance AdaBoost X  X  capability of handling outliers in face detection field. In this paper, we focus on boosting Na X ve Bayes classifier, which is a simple yet surprisingly accurate technique and has been used in many different classification attributes (features). By integrating Divide and Conquer Principle with FloatBoost for boosting Na X ve Bayes text classifier, we propose an improvement FloatBoost algorithm, called DifBoost. 
The rest of the paper is organized as follows. In section 2, preliminary backgrounds are introduced. In Section 3, we describe in detail our proposed DifBoost algorithm. The results of its experimentation and comparisons between DifBoost and other methods are described in Section 4. In section 5, we conclude and predict future work. 2.1 Na X ve Bayes Learning Framework for Text Classification Bayes method assumes a particular probabilistic generation model for text classification. That is, every document is assumed to be generated according to a probability distribution defined by a set of parameters, denoted by  X  . The probability distribution consists of a mixture of components c j  X  C={c 1 ,...,c |C| } and each component is parameterized by a disjoint subset of  X  . To classify a given document, Bayes learning method estimates the posterior probability of a class via Bayes rules, that is, simplification that words independence and words position independence, which results in the following classification function 
To generate this classification function, Na X ve Bayes learning estimates the parameters of the generative model using a set of labeled training data D={d 1 ,...d |D| }. The estimate of  X  is written as  X  estimate, thus finding argmax  X  Pr(  X  |D). Which is the value of  X  that is most probable given the evidence of the training data set and a prior. The estimated probability of a word w t given a class c j is the equation: || Similarly, the class prior probabilities cj  X  2.2 The FloatBoost Algorithm with Na X ve Bayes This section presents the FloatBoost algorithm with Na X ve Bayes learning using notation introduced in the previous section and Na X ve Bayes learning framework. Different from AdaBoost, FloatBoost backtracks after a newest weak classifier h M is added and deletes unfavorable weak classifiers h m from the ensemble, following the idea of Float Search [5] for feature selection. 
The FloatBoost procedure is shown in Table 1. Let H M ={h 1 ,h 2 ,...h M } be the so-far-best set of M weak classifiers,  X  (H M ) be the error rate achieved by weighted sum of weak classifiers m Mm with an ensemble of m weak classifier. In step 2 (forward inclusion), given already selected, the best weak classifier is added one at a time. In step 3 (conditional exclusion), FloatBoost removes the least significant weak classifier from H M , subject to no more removals can be done. The procedure terminates when the error rate is acceptable or the maximum number M max is reached. Incorporated with the conditional exclusion, FloatBoost usually needs fewer weak classifiers than AdaBoost to achieve the same error rate  X  . 3.1 Basic Idea As mentioned before, to make a classifier capable of handling complex environment strategy for robust classification is to separate outliers from other patterns. We apply Divide and Separate Principle [8] through dividing the input pattern space X into a few subspaces and conquering the subspaces by dealing them differently during training weak classifiers. As in [8], input space is divided into 4 subspaces relative to a classifier f(x): X=X no +X sp +X ns +X hd , where X no are normal patterns those can be easily classified by f(x), X sp are special patterns those can be classified correctly by f(x) with bearable adjustment, X ns are noise patterns and X hd are patterns hard to be classified by f(x). 
A typical input pattern space is shown in Figure 1. The first two subspaces are further referred to as Ordinary Pattern Space and the last two are called Outliers: X Na X ve Bayes in FloatBoost to classify X od well compared to classify the whole input pattern space X. After the division, weak classifiers can concentrate more on X sp in X od , instead of X ol , which can often improve the generalization of the algorithm. 3.2 Incorporating Divide and Conqu er Principle into FloatBoost: DifBoost To incorporate Divide and Conquer Principle into FloatBoost, a challenging problem is some trained weak classifiers, we can see a prominent difference between X ol and X od is that the misclassification count on X ol with weak classifiers is much larger than misclassification count on X od . So a threshold can be used to separate X ol from X od . To improve the accuracy of outlier isolation, isolation is not performed during the initial training stage. In our experiment, 1/2M max is often used as a turning point, which means Furthermore, for the boundary between X hd and X od is often not obvious in practice, we deal X ns and X hd differently. X ns patterns once identified, they will be removed from between X ns and X hd as we can see is that their misclassification rates tend to be high. Meanwhile, a major difference between them is that, a noise pattern will often be misclassified to a specific wrong class, on the other hand, a hard-to-classify pattern will tend be misclassified to different wrong classes. 
With the proposed isolation method and treatment of different outlier patterns, table 2 shows modification to the FloatBoost algorithm which is integrated with Divide and Conquer Principle. As shown in clause 2.(3), when we calculate the misclassification Input: Three same inputs as in Table 1, in addition, the outlier threshold  X  ol. Output: A classifier function misclassification d i */ (2) min m  X  =max-value (for m=1,...,M max ),M=0, 0  X  ={}, X ns ={},X hd ={},X od =D; 2. Forward Inclusion: 3. Conditional Exclusion: 4. Output the final classifier: rates of a weak classifier, patterns in X ns are not taken into account and patterns in X hd considered in 2.(3.1) during forward inclusion. Correspondingly, 3.(2) considers whether patterns in X ns and X nd should be reconsidered as ordinary patterns. 
In our algorithm, an important parameter is  X  ol , which is used to determine whether a pattern should be regarded as an outlier. The optional value of  X  ol is associated with the classification task itself and the nature of patterns in X. Experiments were conducted to determine the optimal value for the threshold  X  ol . From the experiments conducted, DifBoost performed reasonably well when the value of  X  ol was around 0.85-0.95. In order to evaluate our proposed method, we have conducted experiments on two data sets: the Reuters-21578 collection and 20-Newsgroups Data. Reuters-21578 consists of Reuters newswire stories from 1987, and is the most popular data set in the pre-labeled with one or more of 135 topics. We use the modified Apte split (Mod Apte), which assigns 9,603 documents dated before April 8, 1987 to the training set and 3,299 documents dated from April 8, 1987 to the test set. In our experiments, we use ninety topic categories that have at least one relevant (positive) training documents and at least one relevant test document. The second data set 20-Newsgroups consists of 20,000 Usenet articles collected by K. Lang from 20 different newsgroups. For this data set, about 70% documents in each newsgroup are used for training (700 documents per class), while left documents are used for testing (300 documents per class). 
We preprocess both data sets by removing the low-frequency words, which are the words appear less than 2 times in a document. Stop-words are removed and term space selection is usually beneficial in that it tends to reduce both overfitting and the space reduction. If not specially mentioned, the number of features in our experiment is 600. We use macro-average F1 and micro-average F1 as [9] the evaluation measures of the text classifiers. 
Table 3 shows a comparison of the performances of 4 different classifiers on our data sets Reuters and 20-Newsgroups respectively. All the parameters for different classifiers are tuned to yield the best performance. For AdaBoost , FloatBoost and DifBoost, we set training around M max to be 400 in both data sets. The parameter  X  ol in Performances.better.than .AdaBoost, while DifBoost gains most prominent performance, which outperforms FloatBoost further. Na X ve Bayes(NB) 0.785 0.804 0.796 0.798 
Results of different methods on both data sets are shown in Figure 2-5 .Figure 2 and 3 show the effectiveness of individual methods on part of Reuters evaluated by Macro-average F1 and Micro-average F1 respectively. Figure 4 and 5 shown effectiveness of each method on 20-Newsgroups.The X-axis of each figure represents the number of training documents. To explore the capacity of handling outliers of DifBoost method, in Reuters we use only 10 largest and 10 smallest categories from the ninety categories. Experimental parameters are set as follows, M max in three boosting algorithm are set to be 400,  X  ol is 0.9 in DifBoost and optional parameter  X  * in DifBoost is not set, which defaults to be 0. 
As shown in Figures 2-5, we have observed that the proposed DifBoost method is successful in boosting Na X ve Bayes. AdaBoost could increase the quality of Na X ve Bayes classifier with an average increase about 10% in both F1 measures over pure Na X ve Bayes algorithm (NB). FloatBoost could increase the quality of Na X ve Bayes classifier with an average increase about 16% in both F1 measures and DifBoost outperforms Na X ve Bayes about 20% in both F1 measures. Note in some cases, AdaBoost is worse than Na X ve Bayes in our experiments, the phenomenon was also observed in previous experiments [10]. On our selected sub Reuters data set, DifBoost performance much better than all the other three methods. The Macro-average F1 measure of DifBoost is about 25% better than Na X ve Bayes and 10% better than FloatBoost, and the Micro-average F1 measure of DifBoost is 15% better than FloatBoost on sub Reuters data set. Ex perimental results indicate that DifBoost performs best with medium-size training set. We have described DifBoost, a boosting algorithm derived by FloatBoost with Na X ve Bayes by integrating with the Divide and Conquer Policy, and we have reported the results of its experimentation on Reuters-21578 and 20-Newsgroup data sets. The basic idea behind our method is to increase the capability of FloatBoost algorithm to handle outliers in the field of text classification. To this end, we have endowed the FloatBoost algorithm with the capacity of outlier detecting and handling. Experimental results show the effectiveness of the proposed algorithm. In the future, we plan to combine kNN, support vector machine algorithms with DifBoost algorithm since they are long used and are effective in text classification also. 
