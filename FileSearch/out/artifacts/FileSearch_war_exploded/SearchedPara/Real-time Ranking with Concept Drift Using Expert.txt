 In many practical applications, one is interested in gener-ating a ranked list of items using information mined from continuous streams of data. For example, in the context of computer networks, one might want to generate lists of nodes ranked according to their susceptibility to attack. In addition, real-world data streams often exhibit concept drift, making the learning task even more challenging. We present an online learning approach to ranking with concept drift, using weighted majority techniques. By continuously mod-eling different snapshots of the data and tuning our measure of belief in these models over time, we capture changes in the underlying concept and adapt our predictions accord-ingly. We measure the performance of our algorithm on real electricity data as well as a synthetic data stream, and demonstrate that our approach to ranking from stream data outperforms previously known batch-learning methods and other online methods that do not account for concept drift. H.4 [ Information Systems Applications ]: Miscellaneous; I.2.6 [ Artificial Intelligence ]: Learning Algorithms, Experimentation, Performance Concept drift, data streams, online learning, ranking
In this paper we present a solution to the problem of gen-erating ranked lists of a given set of items in a dynamic, real-time setting. Each item in our system is represented by a collection of attribute-value pairs, where the values
This work has been partly supported by a research contract from Consolidated Edison Company of New York Copyright 2007 ACM 978-1-59593-609-7/07/0008 ... $ 5.00. can change over time. We apply inductive learning tech-niques to past data to construct predictive models so that when new data arrives, we can generate a new ranking that is accurate with respect to some phenomenon of interest. This application was inspired by the problem of generating real-time rankings of the components of an electrical system according to their susceptibility to impending failure based on the current status of the network and on information gathered during past failures. Specifically, in this applica-tion, we want components that are about to fail to appear toward the top of the current ranking so that engineers and operators can focus their attention on the most-at-risk com-ponents and can thus take appropriate measures to prevent failures or mitigate their consequences. We believe that our proposed solution can be useful for other applications and therefore present the problem and our solution in a generic form.

This problem presents several challenges. First, in order to build predictive models we use supervised machine learn-ing algorithms. Most successful algorithms in practice do not handle dynamic data, hence we need to convert the in-coming stream of data to a batch representation. We do this by using fairly standard window-based aggregations of the time-dependent data. We will not include any details on this data-assembly process since this is not the main fo-cus of this paper. More importantly, in real applications one cannot assume that the underlying system is static. For example, the electricity system changes quite dramatically depending on the environmental conditions, and a computer network may behave differently at different times of the day or the week due to different usage patterns. Thus, models trained at a given time may perform well for a while but could cease to do so if there is a change of context in the underlying system. We assume that these changes of con-text happen and alter the behavior of the underlying system that we are trying to model, but when or why this happens is not known to the algorithm. This phenomenon is gener-ally referred to in the literature as learning in the presence of concept drift [20, 16, 37, 22, 35, 42, 38, 8, 41, 43] and is the focus of this paper.

Our solution is based on the weighted majority algorithm [28], an online learning algorithm that given a set of  X  X x-perts X  (predictors that are used as black boxes) uses a com-bination of the experts X  predictions with strong theoretical worst-case performance bounds. The weighted majority al-gorithm and its multiple variants [7, 13, 4, 27, 40] keep a weight or score for each expert, updating it as new feedback of the experts X  performance becomes available. All these al-gorithms are based on the plausible assumption that experts that have performed well recently are more likely to perform well in the future than experts that have had poor perfor-mance. Clearly, this assumption breaks down in presence of concept drift. In order to account for concept drift, we have extended this algorithm in several ways. First, the original formulation in [28] uses a fixed set of experts available in the beginning of the learning process. In our case, we periodi-cally add new experts as we gather new training data over time. Therefore, we need to drop experts to avoid having to store and monitor an infinitely growing set of experts. Sec-ondly, our algorithm uses rankers as underlying predictors, whereas all algorithms in the learning from experts X  frame-work are designed for classification tasks. Finally, we have included several ad-hoc parameters to control when and how new experts are added, as it will be explained in Section 4.
The paper is organized as follows. Section 2 we briefly survey related work and situate our work in context; Sec-tion 3 presents the problem formally and introduces some notation and concepts; Section 4 describes OnlineRank , our proposed solution to the problem; Section 5 presents experimental results in the context of the feeder ranking ap-plication, and Section 6 concludes with a discussion of the system and some directions for future work.
As sensor, communication, computing and storage devices improve, the need of algorithms that can keep up with, make sense of and exploit the tremendous flow of informa-tion grows. It is therefore not surprising that the problem of dealing with concept drift in the context of learning from data streams is receiving so much attention recently, see e.g. [20, 16, 37, 22, 35, 42, 38, 8, 41, 43].

When one learns from a data stream, examples are pre-sented one at a time. Hence, online learning algorithms are ideal for this task, as they are able to process exam-ples incrementally as they arrive. Therefore a straightfor-ward solution to learning from data streams is to use online learning algorithms directly (e.g. perceptron, winnow, etc. [2]). Unfortunately, many successful machine learning algo-rithms are batch in the sense that all examples need to be available as soon as the learning process starts. In order to use these algorithms in the context of learning from data streams one needs to transfo rm them into online learning algorithms. This sometimes results in complicated methods that require  X (1) time per example, although oftentimes speed-up heuristics and approximations exist that alleviate this problem. Examples of typically batch algorithms with online versions are decision trees [11, 20, 39], SVMs [10, 14, 30], and bagging and boosting [31, 32].

An alternative approach is what we refer to as the wrap-per approach. In the wrapper approach, the batch machine learning algorithms are not modified but are used within a meta-learning layer that applies them to different subsets of the input data and combines them in multiple ways. The most common strategy in wrapper algorithms is to divide the input stream of data into subsets of sequential data (or  X  X ata windows X ), and to repeatedly train models with the batch machine learning algorithm using only one or sev-eral contiguous windows of data at a time. The algorithms proposed to date within this approach differ in how a sin-gle or a combination of window-specific models are used to make future predictions. Within this group of algorithms we distinguish two broad categories: the ones that maintain a single model and replace it with newer ones to cope with the concept drift, and ensemble-based methods that keep a set of models and use combinations of the existing models to make predictions. An example of the former type is the work of Gama et al. [16, 36, 15, 17], where a process monitors the error rate of the current predictive model and when-ever the performance significantly degrades, a new model is generated. Most of the existing work, however, falls into the category of ensemble-based wrappers. An early exam-ple of this is the system of [42], where in addition to dealing with concept drifting using a weighted ensemble of classi-fiers, they also introduce the notion of hidden context :as-suming that the concept drift is due to a change of a hidden context, they store representations successful under differ-ent contexts so that their algorithm can take advantage of the already learned concepts if some previous context reap-pears. This meta-learning level should be clearly beneficial if the concept drift is cyclic and the same contexts re-appear. This meta-level idea of hidden context is exploited also in the system of [43]. Other ensemble-based algorithms that use averages or weighted averages for future predictions in-clude [37, 38, 41, 8]. All these algorithms are similar in that they use heuristics to estimate the predictive accuracy of the ensemble models and use these to weigh models X  pre-dictions. Additionally, the system in [38] favors ensembles with diverse members since it has been empirically shown that having a diverse set of models benefits predictive ac-curacy. Additionally, the work of Klinkenberg et al. [24, 22, 23, 25, 35] describes and compares several strategies for dealing with concept drift such as selecting base learners adaptively, selecting window size adaptively, selecting ex-amples adaptively, etc.

Our solution falls into the category of ensemble-based wrappers. The main difference is that instead of using heuris-tics or boosting-like combinations of underlying models we follow the framework of learning from expert advice .This framework has been thoroughly studied in the theory com-munity, and it offers very strong performance guarantees. After the seminal paper of [28] several variants and exten-sions have been proposed [7, 13, 4, 27, 26, 40]. For example, [13] can handle the fact that experts may choose not to make predictions for a while. The algorithm in [4] works well if the concept drift is only among a small number of experts. Although this algorithm has been developed in the theory community, it has been applied to several domains success-fully [3, 19, 9].

Our solution extends the existing algorithms in several ways: (1) it handles concept drift by continually adding new experts to the ensemble, (2) it has been adapted to the prob-lem of ranking, and (3) it uses several ad-hoc parameters to control various aspects of the learning and meta-learning. The algorithm of [26] uses a similar idea of adding and drop-ping experts throughout the execution of the algorithm but differs from our approach in the type of base learners they use, in the set of tunable parameters, and in the fact that we are performing ranking instead of classification or regres-sion.
Consider a network of interconnected nodes, where each node generates a stream of data representing its state in-formation. We want to rank th ese nodes according to some event of interest, for instance susceptibility to failure in elec-trical grids or vulnerability to attack in computer networks. We assume that these events are observable and that we have access to past observations of events as well as state information of the nodes during the events, and so we are dealing with a supervised learning problem.

Any real-time ranking system should generate rankings that change over time in order to reflect the changing state of the network. Hence, we assume that Ranking ( t )isa ranked list of nodes in the network at time t ,and Rank ( n, t ) is the rank of node n at time t . How quickly the system should update its rankings will be driven by how quickly new data becomes available or by the requirements of the actual application. The system is effectively guessing that nodes at or near the top of the ranking are more likely to be involved in the next occurrence of the event of interest than nodes ranked low in the list. Naturally, we want these guesses to be accurate and therefore we use the following natural performance metric to measure the quality of the rankings over time.

We use pairs ( n, t ) to represent the fact that a node n has been involved in an event at time t .Let Events ( start, end ) be the set of event pairs { ( n i ,t i ) } i listing all the events ob-served between start and end in a network with N nodes. Assume there are K events between the start and end dates. To evaluate the performance of a system generating time-varying rankings Ranking ( t )weusetheformula performance ( { ( n i ,t i ) } i )=1  X  1
For example, in the context of electrical component fail-ures in an electricity grid, suppose that the network consists of 943 components and that 3 of them fail while ranked at positions 100, 231, and 51. The corresponding performance the ranking a failure is, the better (higher) the performance is; 0.5 is the expected value if rankings are random.
In order to produce time-varying rankings of the nodes, we must capture snapshots of the continuously streaming data and assemble training and test sets. We assume that the set of attributes for each node consists of static data, which changes rarely, and dynamic data that continuously changes over time. It is the dynamic data that requires us to constantly adapt our models to reflect the latest state of the system.

Training data and ranking models. Training datasets are assembled with respect to date intervals, capturing the state of (selected) nodes in the network during the specified period. We assume the system has access to the procedure TrainingDataset ( start, end ) that operates as follows: for each event within the given time interval, it samples nodes involved in the event and nodes that were not involved, col-lects their data from the data stream at the time of the event and labels them according to whether the nodes were involved in the event or not. 1 The ranking models are ob-tained by applying machine learning ranking algorithms to training data; these models attempt to make generalizations about how the static and dynamic attributes relate to the labels seen in the training datasets.
To be precise, we have experimented with alternative ways of assembling datasets but for brevity we will omit an ex-planation of these in this paper.

Test data and rankings. Test or snapshot data at a given time t is compiled by obtaining the readings of all dy-namic attributes at time t for each node, together with the known static values. The procedure Snapshot ( t ) assembles test datasets. Given a ranking model m i , the ranking con-structed by applying m i to the snapshot at time t is denoted by Ranking i ( t )= m i ( Snapshot ( t )).

The problem. Given an interconnected set of nodes in a network with associated data streams reflecting the state of each node over time, produce a ranking of the nodes in real-time such that the performance over time as given by Formula (1) with respect to some observable event is maxi-mized.
Our online ranking approach consists of multiple batch-trained models that we refer to as experts, and a meta-model which determines how to combine the expert predictions. We use training datasets from different time intervals and a diverse base of learning algorithms to create a diverse base of experts. To assess the quality of experts X  predictions we use Formula (1) on each expert X  X  ranking and use this as feedback to the meta-model. While the experts are used to model different states of the system, the meta-model is used to determine which captured states are better at predicting the current state of the system. The meta-model is an adap-tive (online) learning model, meaning that the training and testing is performed by feeding in each example, receiving the model X  X  prediction, and altering the model based on the loss of the experts with respect to the example. The set of labeled examples for the meta-model in our case consists of the nodes that participate in the event of interest. For exam-ple, if we are interested in generating a ranking of electrical components according to their failure susceptibility, our la-beled examples would consist of the electrical components that failed. The final ranking output of the meta-model is a weighted average of experts X  rankings, where the weight of each expert translates to the meta-model X  X  measure of belief in that expert.

Our online ranking algorithm is based on the principle of learning from expert advice, and draws on ideas from the Continuous Weighted Majority algorithm [28]. To cope with concept drift, new models, trained with the most re-cent data, are periodically added to the existing ensemble. In order to avoid growing an infinitely large ensemble, mod-els are removed according to a function of their age and performance; the age of a model is the number of days since its creation.

Periodically, we train and add new models to the cur-rent ensemble. A parameter f determines how frequently this should happen, i.e., new models will be added every f iterations. When new models are created, we assign each of them a weight to be used as an individual performance measure. We add these models to the ensemble of experts used by the algorithm in making its predictions. The ex-pert ensemble is then presented with a set of items to rank and each expert makes a separate prediction. The algorithm combines these predictions by ranking the items according to their weighted average rank. It then receives the true ranking of the items and updates the weights of the experts in the ensemble. Our weight update function is similar to the one discussed in [28]: each expert X  X  weight is multiplied by a function of its loss. The loss of each expert is a mea-sure of its performance, relative to the other experts. As a result, experts with better performance will have higher weights and will contribute more to the final prediction.
There are several input parameters that can be used to tune the performance of the algorithm. The learning rate  X   X  [0 , 1) is used in the weight update function to adjust the fraction by which the weights are reduced. A larger value of  X  corresponds to a slower learning rate, making the al-gorithm more forgiving to experts that make a mistake by reducing their influence by a smaller fraction. We also use a parameter B (or budget ) to limit the number of models that the algorithm can keep track of at each iteration. Since we do not use a static set of experts as in the traditional weighted majority approach, we have to make sure that our ensemble does not grow infinitely when we add new models. We can also adjust the number of models that the algorithm uses for prediction. In the traditional approach, the advice of all experts in the ensemble is combined to make the fi-nal prediction. By using a parameter E for the number of predictors, we can try to further enhance the performance, combining the advice of top performing experts only.
Since we add and remove models from our expert ensemble throughout the algorithm, additional parameters are intro-duced. Let n be the number of new models added to the ensemble. This parameter n depends on how many machine learning algorithms we use (2 in our experiments) and on how many training sets we assemble (we vary the training data windows, currently set to 7 and 14 days). When these new models are added, they are assigned an initial weight w new . This weight can be also adjusted to reflect our trust in these new models, and should be relative to the weights of the existing models in the ensemble. We use a parameter p that determines what weight to assign new models as a percentile in the range [ w min ,w max ] for the minimum and maximum weights of the existing models. We also need to decide what models to drop when the ensemble size grows larger than the budget B . We order the experts according to a function of their performance and age, where  X   X  (0 , 1) is a parameter used to set the exponential decay by age. Pseudocode of our online ranking algorithm can be found in Figure 1. In the algorithm, we keep a set of current models M .Every f iterations, we add n new models and if nec-essary remove the worst models that exceed our budget B . The weights of models w i are computed according to the update formula w i = w i  X   X  l i ,where l i is the loss of model m i in the last event. The loss of model m i is computed ac-cording to its relative performance with respect to the other function normalizes the losses of experts to the range [0 , 1] and could be used with any (even unbounded) performance metric, which in our case is given by formula (1).
In this section, we present various experiments with the goal of studying and evaluating the online ranking algo-rithm. We perform our experime nts on real electricity data and a synthetically generated data stream. As a baseline, we compare our performance to that of a commonly used online learning algorithm as well as a batch-trained model.
In our experiments we use as  X  X xperts X  two types of learn-ing algorithms: SVMs and MartiRank [18], a ranking al-gorithm based on the boosting framework in [29]. To ob-OnlineRank ( B, E,  X ,  X , p, n, f ) 1 M X  X } 2 while ( true ) 3 do at time t 4 Ranking i ( t )  X  m i ( Snapshot ( t )) for m i  X  X  5 E X  E top-scoring models according to weights w i 6 Ranking ( t )  X  WAv g ( { w i  X  Ranking i ( t ) | m i  X  X } 7 if new event( x, t ) 8 then s i = performance m i ( x, t ) for m i  X  X  11 for m i  X  X  14 if no models generated in f iterations 15 then train n models m |M| +1 ,...,m |M| + n 16 w new  X  percentile ( p, { w 1 ,...,w S } ) 19 if |M| &gt;B 20 then remove |M| X  B worst 21 models according to 23 normalize weights w i Figure 1: Pseudocode for our online ranking algo-rithm. tain a ranking from the outputs of the SVMs we simply use the margin scores to sort the examples appropriately. Since rankers are used as black boxes we could use any other rank-ing algorithm e.g. [12, 6, 34].
Our first set of experiments are performed on data col-lected from an electricity distribution system. In particular, we examine attributes of electrical feeders 2 with the intent to rank them according to their failure susceptibility. The electricity data is very diverse, not only in nature but also in location, type, format, etc. A significant amount of work has been devoted to understanding, processing and merging this data into attribute-value vector datasets that can be used by standard machine learning algorithms. The data used for these experiments ranges from June 2005 to December 2006. The main i nput data sources are:
Feeders are cables, usually 10-20 km long, over which elec-tricity is sent at mid-level voltage (tens of thousands of volts) from substations to local transformers.
We compare the performance of our online ranking algo-rithm against two separate baseline experiments. The first one uses the perceptron algorithm [33]. We chose the percep-tron particularly because it is an online method that learns directly from the labeled feeder examples, as opposed to using a meta-learning approach like ours. To generate a ranking from the perceptron classification predictions, we sort the feeders according to their (signed) distance from the boundary in descending order.

The second experiment involves using a single batch-trained model throughout the whole run. The batch model is trained using a ranking algorithm based on the boosting framework in [29], which proved to be the best performing batch learn-ing method on this specific dataset [18]. The performance of these baseline approaches can be seen alongside our on-line ranking method in Figure 2 for the summer of 2005 and Figure 3 for the winter of 2006. For both the summer and the winter months our online ranking approach outperforms both the perceptron and the batch model.

The default parameters used in our system are: learning rate  X  =0 . 9, budget B = 50, ensemble size E = 10, new model frequency f = 7, age decay a =0 . 99, and new weight percentile p =0 . 7.
 Figure 2: OnlineRank daily performance over base-line methods -Summer 2005 (6/1/2005-8/31/2005)
In order to optimize the online algorithm X  X  performance, we examine the effects of varying the value of a single pa-rameter while keeping the rest constant. In doing so, we can observe the change in performance (or lack thereof) associ-ated with each parameter setting, and determine the optimal input values to the algorithm.

Figure 4 shows the performance of the system during the summer of 2006 for differ ent values of the budget B ,which corresponds to the maximum number of experts that the algorithm can select from to make its prediction. We can observe that the performance of the system is directly cor-related with the number of existing experts. Intuitively, when there is a larger pool of models to select from, we have a higher chance of selecting the top performing models amongst them, especially if the predicting ensemble size E is small.

Another parameter that we are interested in observing is the new models X  weight percentile p , which determines our Figure 3: OnlineRank daily performance over base-line methods -Winter 2006 (1/1/2006-3/31/2006) Figure 4: OnlineRank daily performance with vary-ing budget size (B), here E =1 -Summer 2006 (6/1/2006-8/31/2006) degree of belief in the incoming models. The performance of the online ranking system with varying weight percentile during the summer of 2006 can be seen in Figure 5. Al-though the difference in performance is more subtle than that of the budget variations, it is still clear that there are values that lead to better performance than others. Assign-ing new models the lowest weight in the range is under-standably a poor choice since new models are trained with the latest snapshot of the data, thus carrying an up to date information about the system which should help increase the accuracy of the prediction. On the other hand, assign-ing too high a weight may force the system to use the newest models always, which may not be a good choice if an older model has been found to work the best. Notice that the per-formance of the algorithm for the 90th and 70th new weight percentile setting is almost identical.

To understand the reason for this results, we performed a weight analysis on the experts used for prediction. We found that more than 80% of the total weight is taken up by 10 equally weighted models. This was not the case for the algorithm X  X  run with new models X  weight in the 10th, 30th and 50th percentiles where the models were more di-verse in terms of their weight and duration of stay in the top E . We can attribute the prevalence and consistency of the top 10 models in the case of 90th and 70th new weight per-centile values to the fact that assigning low weights to the incoming models prevented them from rising to the top of the ensemble, making them more likely to be dropped. The weight similarity among the top E models may imply that although these models are different in the time and method of training they may be similar in their predictions. We plan to explore a measure of model similarity in future work in order to gain insight into the system X  X  behavior.
From our experiments, we found that assigning new mod-els a weight value in the middle of the range generally yields better performance. This can be seen by observing the per-formance of the 50th percentile new weight value in Figure 5, relative to the other percentile values.
 Figure 5: OnlineRank daily performance with vary-ing new weight percentile (p) -Summer 2006 (6/1/2006-8/31/2006)
The maximum ensemble size E is the parameter that con-trols how many models out of the available pool contribute to the final prediction. While in theory we should com-bine the advice of all available experts, in practice it may be useful to ignore some of the lower-weighted experts for the purpose of prediction but still keep them in the pool in case they redeem themselves. For instance, in cases where at any given time there are only a few experts that have an accurate model of the system, it may be useful to com-bine the advice of these top E without the majority of the experts that would weigh down the performance. Figure 6 shows the performance of the online ranking system during the summer of 2005, using different values for the predicting ensemble size E .

Finally, we have experimented with the learning rate  X  of the ranking algorithm. Figure 7 shows the performances ob-tained for different settings of t his parameter. Theoretically, the learning rate should affect the performance by control-ling the fraction by which weights are reduced. However, our experiments show that varying the learning rate has a minor impact to the overall performance change. To under-stand why this is the case, we examine the weights of the top soring models for each setting of  X  . We plot the weights of the top E experts per day as a stacked bar, where the total y -value corresponds to the fraction of the weights of models in the budget B that are also in the predicting ensemble E . Each color uniquely identifies a particular model so we can observe which models are dominant and how long they stay in the ensemble. Figure 8 shows the weight distribution for learning rate setting  X  =0 . 1andFigure9showstheweight distribution for  X  =0 . 9. The weight contribution of the two most dominant ensemble models in both plots explains the similarity of the results.

Please note that we chose this particular example since it is easier to interpret and render. In most of our weight analysis plots there is a lot more diversity of models and movement, although similar patterns still exist between the plots for the algorithm X  X  runs with different learning rate settings. This results is somewhat surprising and we plan to experiment with different weight update functions in future work.
 Figure 6: OnlineRank daily performance with vary-ing ensemble size (E) -Summer 2005 (6/1/2005-8/31/2005) Figure 7: OnlineRank daily performance with vary-ing learning rate (  X  ) -Winter 2006 (1/1/2006-3/31/2006)
In order to gain a deeper understanding of the behavior of our system, we have  X  X anually X  generated datasets for which the concept drift is controlled. The idea is that we pick models at random and use them to generate events. A parameter c  X  [0 , 1] controls the rate of change of the models used to generate the events. That is, with probability c we will substitute the event-generating model, otherwise we will continue to use the current one. The data streams used in these experiments are the same as in the experiments with electricity data, the only difference being in the events that Figure 8: Ensemble models X  weights using learning rate  X  =0 . 1 for Winter 2006 (1/1/2006-3/31/2006) occur over time. As a consequence, the labels of training and test sets will differ from the experiments in Section 5.1. To generate an event for a given ranking model m at time t , we generate snapshot data and use the model to produce aranking Ranking m ( t ). Another parameter d (for depth percentile ) introduces noise by picking the node event uni-formly at random among the top-ranked d  X  X  Ranking m ( t ) nodes of Ranking m ( t ).

Figures 10 and 11 show the performance of our system vs. the online learning algorithm perceptron under different settings of the concept change rate c and depth percentile noise d . Our system dominates although the difference is not as overwhelming as in the previous section.

We have also experimented with an alternative data gen-eration approach. We generate random hyperplanes and random examples in euclidean space and label them accord-ing to the hyperplane. In each iteration we tweak the hy-perplane to simulate concept drift. Using this approach we have observed that perceptron consistently outperforms our system. In hindsight, this is not surprising since we are us-ing hyperplanes as concepts, namely the hypotheses used by perceptron whereas our system explores a completely differ-ent hypothesis space, thus favoring perceptron. The reader should note that in practice it is unlikely that real-world data is linearly separable.
We have presented a real-time algorithm that generates time-varying rankings of nodes in a network based on data streams associated with each node. We have demonstrated its validity in the real-world scenario where one wants to generate rankings of components based on their suscepti-bility to impending failure. In fact, this is the context in which the algorithm was originally developed. Our system outperforms a standard online learning algorithm using syn-thetic data although results are not as dramatic. We are still investigating alternative ways of generating data in realis-tic ways to further understand the behavior of our system. Additionally, we are devising visualization methods for the Figure 9: Ensemble models X  weights using learning rate  X  =0 . 9 for Winter 2006 (1/1/2006-3/31/2006) Figure 10: OnlineRank vs. perceptron on synthetic data for concept drift rate c =0 . 1 and depth per-centile noise d =0 . 01 . Vertical bars mark a change of concept. internal state of our algorithm such as the weight plots of Figures 8 and 9. Such visualization methods should not only give us a better understanding of the system X  X  behavior and performance but also give us insights into the underlying system itself (for example, factors or causes of failure in the electricity grid).

There are several ways in which we could improve the performance of our system. We are planning to include an engine for the detection of concept drift, so that instead of periodically adding new experts we will only add when a change is detected [16, 21]. We also want to control for the diversity of the ensemble. Currently we do not check whether we have multiple copies of the same model in the ensemble. If we are about to add a new model, we should make sure it is not already present in the ensemble. This should also ensure that the ensemble is diverse, which has been found to help predictive performance [38].

Another direction of future work is to exploit re-occurring scenarios. If there are cyclical patterns in the behavior of the system under consideration, information learned during Figure 11: OnlineRank vs. perceptron on synthetic data for concept drift rate c =0 . 05 and depth per-centile noise d =0 . 1 . Vertical bars mark a change of concept. one phase can be re-used when the system re-enters that phase again. For example, in the electricity data applica-tion of section 5.1, load demand increases greatly when the temperature is high, and hence the stresses on the grid in the summer differ greatly from those in the winter. It is natural to think that experts that have performed well in the summer are likely to perform well in the upcoming sum-mer. We want to extend our online algorithm to include a meta-learning layer that is able to discover such correlations so that the weight update can be further informed by the information of results in previous years. In other words, the hidden context discussed in [42] would correspond to a differ-ent environmentally-driven condition. Since these different contexts re-appear with periodicity of one year, we should be able to take advantage of what has been learned in the past. We have accumulated enough data to start to look at such patterns. Examples of other systems that exploit this meta-layer are [42, 43].

Finally, even though in our application the items we need to rank are interconnected and form a graph, we do not make use of this structure and use a feature-vector represen-tation instead (although some of our static features model the underlying topology of the network). The area of re-lational learning deals with problems where examples are interconnected and we should generalize our algorithm by using learning methods that can exploit the network struc-ture [5, 1]. An obvious way to incorporate this style of learning into our algorithm would be to use such algorithms as experts, although a more challenging problem would be to incorporate this information directly at the meta-learning level in the online learning algorithm.
 We gratefully acknowledge the support provided by Consol-idated Edison Company of New York. We wish to thank all of those who gave so generously in time, advice, and effort. Additionally, a number of Columbia faculty, students, staff, and researchers contributed to wards this effort: Phil Gross, Albert Boulanger, Chris Murphy, Luis Alonso, Joey For-tuna, Gail Kaiser, Roger Anderson, and Dave Waltz. [1] A. Agarwal, S. Chakrabarti, and S. Aggarwal. [2] A. Blum. On-line algorithms in machine learning. In [3] A. Blum. Empirical support for winnow and [4] O. Bousquet and M. K. Warmuth. Tracking a small [5] S. Brin and L. Page. The anatomy of a large-scale [6] C. J. C. Burges, T. Shaked, E. Renshaw, A. Lazier, [7] N. Cesa-Bianchi, Y. Freund, D. Haussler, D. P. [8] F. Chu and C. Zaniolo. Fast and light boosting for [9] V. Dani, O. Madani, D. Pennock, S. Sanghai, and [10] C. Domeniconi and D. Gunopulos. Incremental [11] P. Domingos and G. Hulten. Mining high-speed data [12] Y. Freund, R. D. Iyer, R. E. Schapire, and Y. Singer. [13] Y. Freund, R. E. Schapire, Y. Singer, and M. K. [14] G. Fung and O. L. Mangasarian. Data selection for [15] J. Gama and G. Castillo. Learning with local drift [16] J. Gama, P. Medas, G. Castillo, and P. P. Rodrigues. [17] J. Gama, P. Medas, and P. P. Rodrigues. Learning [18] P. Gross, A. Boulanger, M. Arias, D. L. Waltz, P. M. [19] D. P. Helmbold, D. D. E. Long, T. L. Sconyers, and [20] G. Hulten, L. Spencer, and P. Domingos. Mining [21] D. Kifer, S. Ben-David, and J. Gehrke. Detecting [22] R. Klinkenberg. Meta-learning, model selection, and [23] R. Klinkenberg and T. Joachims. Detecting concept [24] R. Klinkenberg and I. Renz. Adaptive information [25] R. Klinkenberg and S. R  X  uping. Concept drift and the [26] J. Z. Kolter and M. A. Maloof. Using additive expert [27] Y. Li and P. M. Long. The relaxed online maximum [28] N. Littlestone and M. K. Warmuth. The weighted [29] P. M. Long and R. A. Servedio. Martingale boosting. [30] O. L. Mangasarian and D. R. Musicant. Large scale [31] N. Oza and S. Russell. Online bagging and boosting. [32] N. C. Oza and S. J. Russell. Experimental [33] F. Rosenblatt. The perceptron: A probabilistic model [34] C. Rudin. Ranking with a p-norm push. In COLT , [35] M. Scholz and R. Klinkenberg. An ensemble classifier [36] M. Severo and J. Gama. Change detection with [37] K. Stanley. Learning concept drift with a committee [38] W. N. Street and Y. Kim. A streaming ensemble [39] P. E. Utgoff, N. C. Berkman, and J. A. Clouse. [40] V. G. Vovk. Aggregating strategies. In Proceedings of [41] H. Wang, W. Fan, P. S. Yu, and J. Han. Mining [42] G. Widmer and M. Kubat. Learning in the presence of [43] Y. Yang, X. Wu, and X. Zhu. Combining proactive
