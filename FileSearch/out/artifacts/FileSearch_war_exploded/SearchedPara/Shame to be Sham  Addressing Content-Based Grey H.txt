 W e present an initial study identifying a form of content-based grey hat search engine optimization, in which a Web page contains both potentially relevant content and manipu-lated content: we call such pages sham documents, because they lie in the grey area between  X  X am X  (clearly normal) and  X  X pam X  (clearly fake). Sham documents are often ranked ar-tificially high in response to certain queries, but also may contain some useful information and cannot be considered as absolute spam. We report a novel annotation effort per-formed with the ClueWeb09 benchmark where pages were labeled as being spam, sham, or legitimate content. Sig-nificant inter-annotator agreement rates support the claim that there are sham documents that are highly ranked by a very effective retrieval approach, yet are not spam. We also present an initial study of predictors that may indicate whether a query is the target of shamming.
 T he Web constitutes an adversarial retrieval setting: in Web search, content creators try to have their pages ranked high in response to queries, to increase visibility and traf-fic to their sites. To that end, they often employ various search engine optimization (SEO) techniques [11]. Search engines, on the other hand, aim to rank pages in response to queries by the overall relevance of their content and strive to minimize the effect of potential adversarial methods.
A distinction is made between white hat SEO, which typi-cally does not involve deceptive techniques and obeys search engine anti-spam guidelines, and black hat SEO, which uses deceptive content, links and other manipulation that ulti-mately may result in the site using them being penalized or banned by search engines. The black hat SEO area has at-tracted much research attention; e.g., there is a large body of work on spam classification [11, 5, 8, 19], where a page is d eemed spam if it bears no useful content. Grey hat SEO, on the other hand, is an intermediate form of search ranking manipulation that yields neither a spam page, nor a (very) high-quality representative of relevant content for queries seeking information on that topic. Grey hat SEO has at-tracted very little research attention in the literature [13]. There is no rigorous definition of what makes for a grey hat SEO attempt: because grey hat documents can often con-tain relevant information as well as SEO content, identifying such documents is often a subjective decision.

In this paper we address the problem of identifying a spe-cific, highly important type of grey hat SEO: pages whose content has been manipulated so as to have the page ranked higher than it should with respect to some queries. How-ever, these pages are not spam in the absolute sense, in that they can still satisfy information needs. We use the term sham to refer to such pages 1 . For example, excessive use of query variants can turn an otherwise useful page to sham but not to spam, along with more obvious manipulations like keyword stuffing [11]. An example of a sham passage is shown in Figure 1.

By definition, sham pages can have considerable negative impact on the effectiveness of relevance ranking. For exam-ple, the frequency of query terms in them can be a mislead-ing signal as to the actual information needs these pages can satisfy and the extent to which they can satisfy them. Our motivation for focusing on content-based shamming is three-fold. First, relevance is typically determined by the ability to satisfy an information need using the page content. Second, content-based features are highly important when applying learning to rank methods for Web search [16]. Third, it is relatively easier to manipulate the content of pages com-pared to other derived relevance signals such as clickthrough rate, hyperlink structure, etc.

This paper provides two main contributions toward an ini-tial understanding of the nature of sham . First, we describe the results of a sham annotation pilot effort applied to the publicly available ClueWeb09 benchmar 2 [ 7]. Documents that were highly ranked in response to queries by an effec-tive learning-to-rank (LTR) method were classified into one of three categories: spam, sham, or legitimate. We found significant inter-annotator agreement for the task of label-ing sham pages. Furthermore, many top-ranked pages were agreed to be sham: some were relevant to the queries, while most were not. Second, we present an initial study of predic-
A ppropriately, in English the word  X  X ham X  can denote some-thing fake or not genuine. w ww.lemurproject.org tors for when a query is likely to be the target of shamming efforts. Our results show that query-specific features out-perform an approach that uses spam classification for this task as well as content-based query-independent document quality measures [2].
T here is much work on spam classification (e.g., [11, 14, 20, 8, 5, 19]) and on nullifying its effects on retrieval effec-tiveness [14, 9]. As already noted, the task of sham clas-sification that we pursue here is different from spam clas-sification by definition. Specifically, sham documents are manipulated for specific queries, and do bear useful content, while spam documents are absolutely useless. There has been some work on using query-dependent features to clas-sify spam in its absolute query-independent sense [20]. We use query-dependent features for sham classification.
Using query-independent document quality measures is known to improve retrieval effectiveness [3, 2]. We note that sham documents are not necessarily of low quality. Further-more, we show that the methods we study for the task of predicting whether a query is the target of shamming out-perform methods that use content-based document quality measures which are very effective for search [2].

The realization that  X  X ot all content that complicates rank-ing is spam X  was echoed in previous work [13]. However, we are not aware of any previous studies that study the diffi-culty of labeling Web pages as grey hat SEO, or using pre-and post-retrieval features to predict how likely a query is to be the target of such SEO attempts.
A s a first step in examining the shamming phenomenon, we conducted an annotation effort, wherein a total of 300 documents, 30 queries  X  10 documents per query, were eval-uated. (Further details regarding the set of documents and the queries are provided in Section 5.) The documents were evenly divided between five Ph.D. and M.Sc. students in the Information Retrieval field. Each document was labeled by two annotators as being either spam, sham, or legitimate page. In case of a disagreement between the first two anno-tators, a third annotator was asked to label the document. Thus, for each document we have either two or three labels.
Annotators were instructed to label a document as sham if any of the following conditions was met. (i) The docu-ment contained text that did not seem to satisfy any infor-mation need, including information needs satisfied by other parts of the same document (ii) The page contained text that appeared to be related to information needs satisfied by this page, but the text contained many artificial, repeated, or otherwise unnecessary extra words, phrases or sentences added to the page solely for the purpose of promoting the page in the result lists; and (iii) The page contained sec-tions of content that were copied from other pages (e.g., from a Wikipedia page) for presumably the sole purpose of promoting the page. A document was labeled as spam if it contained no useful information at all for satisfying any information need.

The annotators were asked to base their decision only upon the content of the document, and to ignore non-content elements such as incoming and outgoing links, page URL, site domain, or ads and sponsored links that might appear on the page. The documents were presented to the anno-tators in a random query-independent order and the query itself was not presented.
W e next examine the problem of predicting which queries are more susceptible to sham, as measured, e.g., by the frac-tion of sham documents in the top 10 results. To this end, we study the use of various predictors which were developed to estimate retrieval effectiveness [4]. As we show below, some of these are negatively correlate with the percentage of sham, typically because most sham documents are non-relevant, while others have a positive correlation.
Pre-retrieval predictors are based on properties of the query and the corpus. Post-retrieval predictors utilize, in addition, information about the result list of documents, D r es , which was returned in response to the query [4]. Details regarding the retrieval model used in our experiments are provided in Section 5. In what follows we present the different classes of predictors analyzed for our prediction task.
 sures the similarity between the query and the collection using the sum of the TF.IDF values of the query terms [21]. SumVar [21] is the sum over query terms of the variance of the TF.IDF values of the term in documents in the corpus in which it appears. The SumIDF predictor is the sum of the IDF values of the query terms [10]. Another pre-retrieval predictor that we use is QueryLength . Since sham docu-ments are assumed to negatively affect relevance ranking, a lower pre-retrieval predicted value is assumed to be corre-lated with higher percentage of sham.
 P ost-retrieval predictors. T he Clarity [10] prediction value is the KL divergence between a relevance language model [15], R , induced from the documents in the result list D a nd the (unsmoothed) language model of the collection. For-mally, let p ( w | d ) and s ( d ) be the probability assigned to term w by an unsmoothed language model induced from document d , and the score assigned to d by the search al-gorithm that was used to create D r es , respectively. Then, the probability assigned to w by R is defined as p ( w |R ) P documents for a given query are similar to one another; for example, due to the repeated terms that they contain. Thus, t hese documents presumably form a cluster whose language model is focused with respect to that of the corpus.
We also consider a variant of WIG [22] which is sim-ply the average retrieval score in D r es : 1 | D The premise is that sham documents are likely to be as-signed high retrieval scores, as by definition they have been manipulated to be promoted in result lists.
 tropy of the term distribution in a document, Entropy , the stopwords to non-stopwords ratio in a document, SW1 , and the percentage of stopwords that appear in the docu-ment, SW2 , were shown to be highly effective content-based query-independent quality measures for Web search [2], and for predicting query performance over the Web [17]. We used INQUERY X  X  stopword list [1]. The document PageR-ank score [3] is another measure widely used for improving retrieval effectiveness. We use these measures for our sham prediction task. The prediction value for the result list D i s the sum of the per-document values assigned by a measure. While Entropy, SW1 and SW2 are based on the content of a document, PageRank is based on the hyperlink structure. The content-based measures quantify the presumed textual content breadth in the document. We hypothesize that the content breadth in sham documents is low, as there is focus on a small subset of terms that are the target of shamming. Conversely, since by definition sham documents can still sat-isfy information needs, we assume that the PageRank score of such documents is likely to be high.
 S pam-based predictor. T o study the connection between spam and sham documents, we use the recently proposed NS predictor [17]. Documents in D r es t hat are classified as spam by Waterloo X  X  spam classifier are treated as  X  X on-relevant X ; the rest are treated as  X  X elevant X . Then, the mean average precision (MAP) at cutoff |D r es | i s computed based on these artificially created  X  X elevant X  and  X  X on-relevant X  la-bels. This score is then multiplied by the total number of  X  X elevant X  (non-spam) documents.
 the following features from the query logs of a commercial search engine. RawImpressions : the total frequency the query (with stopwords) was used to search; ClicksOnRe-sult : percentage of clicks on the search engine results page (SERP) that were on a search result vs. other parts of the page, such as query suggestions; ClicksForPaging : the percentage of clicks on the SERP used to get to the next or previous page of results. For all features we used one month of traffic from January 2013 originating from the U.S. locale. O ur experiments were conducted using the ClueWeb09 Category B collection, which contains about 50 million En-glish Web pages. We used queries 1-150 from TREC 2009-2011. The Indri toolkit (www.lemurproject.org/indri) was used for experiments. We applied Krovetz stemming upon queries and documents, and removed stopwords on the IN-QUERY list [1] only from queries.

To create a highly effective result list, D r es , we did the fol-lowing. First, for each query q , the documents in the collec-tion were ranked using the negative cross entropy between the unsmoothed unigram language model induced from q , and the Dirichlet-smoothed unigram language model induced from a document (with the smoothing parameter  X  set to 1000). Then, following previous work [9], documents as-signed by Waterloo X  X  spam classifier with a score below 50 were removed top to bottom from that ranking until 100 (presumably non-spam) documents were accumulated. These documents were then re-ranked using SVM r ank [ 12] applied with 130 features. Ten-fold cross validation was performed.
We used a subset of the features used by Microsoft X  X  learn-ing to rank datasets 3 . Our features do not include the Boolean Model, Vector Space Model, LMIR.ABS, and the number of outgoing links. The SiteRank score, the two qual-ity measures, QualityScore and QualityScore2, and the click-based features were also not included as these information types are not available for the ClueWeb09 collection. Three additional features that we used, and which were found to be highly effective for Web retrieval [2], are Entropy, SW1, and SW2 which were described in Section 3. As is the case with the features mentioned thus far, these three features were also computed separately for all the text in the document, its anchor text, URL, body, and title. Waterloo X  X  spam classifier score [9] is another feature that was used. The BM25 score was computed with k 1=1 and b =0 . 5, following LMIR.DIR and LMIR.JM, the language-model-based fea-tures, were computed with  X  =1000 and  X  =0 . 9, respectively. Lastly, 30 queries were randomly sampled, and the top 10 non-Wikipedia documents were used to form D r es . We as-sume that Wikipedia pages are not sham.

We used two approaches to aggregate the labels assigned by the annotators to the documents. According to the first approach, a document is considered sham if it was labeled as such by at least one of the annotators. This  X  X eak X  label definition is henceforth referred to as AtLeastOne . The second approach, denoted AtLeastTwo , is more strict: a document is considered sham if it was marked as sham by at least two annotators.
 To evaluate the quality of the various predictors, we report Pearson X  X  correlation [4] between the values assigned by a predictor to each of the tested queries, and the percentage of sham documents computed for a query, according to the AtLeastOne and AtLeastTwo approaches described above.
The number of terms used to construct the relevance lan-guage model, which is used by the Clarity predictor, is set to 50. Given that documents assigned with a score below 50 by Waterloo X  X  spam classifier are removed in the process of cre-ating D r es , to determine the non-spam documents that are used by the NS predictor we use a threshold. The threshold is selected from { 60 , 70 , 80 , 90 } so as to optimize the predic-tion quality of NS. Documents assigned with a score above that threshold are considered as non-spam ( X  X elevant X ).
Annotation Results. The inter-annotator agreement on the label pairs from the two main judges for each docu-ment was 0 . 69, computed using the free-marginal multi-rater kappa measure [18]. Overall, 79% of the 300 label pairs were in agreement. The average percentage of sham documents per query was 30% based on AtLeastOne labels, and 21% based on AtLeastTwo labels.

Query shamming predictor analysis. The correla-tions of the predictors described in Section 4 with the per-w ww.research.microsoft.com/en-us/projects/mslr T able 1: Pearson X  X  correlation (r) of various predic-tor variables (Section 4) with % sham documents in the top 10 results, for weak (AtLeastOne) and strict (AtLeastTwo) sham label definitions. centage of sham pages retrieved for a query are presented in Table 1. We can see that the pre-retrieval predictors are negatively correlated with shamming. As these predictors were designed to predict the retrieval effectiveness of using the query for search, we can conclude that sham documents are prevalent in queries whose performance is predicted to be low. Indeed, we found that about 75% of sham documents are non-relevant , for both AtLeastOne and AtLeastTwo.
QueryLength is negatively correlated with the percentage of sham documents, for both weak (AtLeastOne) and strict (AtLeastTwo) sham label definitions. That is, a shorter query has a higher chance of being the target of shamming. This might be attributed to the fact that short queries are more ambiguous, and as a result documents returned in re-sponse to these queries might be promoting the page with respect to different information needs, not necessarily the information need that the current query expresses.

The post-retrieval predictors, Clarity and WIG, are posi-tively correlated with the percentage of sham documents, as hypothesized in Section 4. However, the prediction quality is lower than that for the pre-retrieval predictors.
We can also see that the correlation between the query-independent document-quality measures and shamming is relatively low. Furthermore, the NS predictor is not cor-related with sham. These findings suggest that sham docu-ments might be of either high or low quality, and that there is no evident connection between sham and spam documents. For query log features, the positive correlation of Raw-Impressions with the level of sham is consistent with the fact that frequent queries are more susceptible to shamming. ClicksForPaging is positively correlated with the percentage of sham, while the ClicksOnResult feature is negatively cor-related. These findings resonate with the fact that sham-ming degrades retrieval performance.

Finally, for the task of predicting sham percentage for a given query at low ( &lt; 0 . 3), medium ( 2 [0 . 3 , 0 . 6)), or high (  X  0 . 6) levels, we performed ordinal regression [6] using all the predictors with default regression settings. The mean absolute error (MAE), the absolute difference between the predicted class ordinal and the true ordinal, averaged over 100 randomized trials with a 2:1 train/test split, was 0 . 532. For comparison, an oracle run using one rater X  X  sham labels to predict sham levels derived from other raters X  labels, had MAE 0 . 37; while always guessing category  X  X edium X  had MAE 0 . 67. These initial results show that different predic-tors can be effectively integrated for predicting per-query sham levels.
W e have provided an initial study on the identification of sham documents: a form of grey-hat SEO using content-based feature manipulation, along with an analysis of fea-tures associated with low-or high-sham queries. Even search engines that use highly effective ranking methods still suf-fer from sham documents in the top-most rankings that ad-versely affect retrieval quality. These findings call for a prin-cipled treatment of query-specific grey hat SEO.
 Acknowledgments. We thank the reviewers for their com-ments. This work was supported by and carried out at the Technion-Microsoft Electronic Commerce Research Center.
