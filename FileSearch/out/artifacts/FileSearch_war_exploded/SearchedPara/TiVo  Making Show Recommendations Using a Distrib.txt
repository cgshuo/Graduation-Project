 We describe the TiVo television show collaborative recommendation system which has been fielded in over one million TiVo clients for four years. Over this install base, TiVo currently has approximately 100 million ratings by users over approximately 30,000 distinct TV shows and movies. TiVo uses an item-item (show to show) form of collaborative filtering which obviates the need to keep any persistent memory of each user s viewing preferences at the TiVo server. Taking advantage of TiVo s client-server architecture has produced a novel collaborative filtering system in which the server does a minimum of work and most work is delegated to the numerous clients. Nevertheless, the server-side processing is also highly scalable an d parallelizable. Although we have not performed formal empirical evaluations of its accuracy, internal studies have shown its recommendations to be useful even for multiple user households. TiVo s architecture also allows for throttling of the server so if more server-side resources become available, more correlations can be computed on the server allowing TiVo to make recommendations for niche audiences.
 I.2.6 [ Artific i al Intelligence ]: Learning Algorithms.
 Collaborative -Filtering , Clustering Clickstreams With the proliferation of hundreds of TV channels, the TV viewer is faced with a search task to find the few shows she would like to watch. There may be quality information available on TV but it may be difficult to find. Traditionally, the viewer resorts to "channel surfing" which is akin to random or linear search. The mid 1990's in the USA saw the emergence of the TV-guide channel which listed showings on other channels: a form of indexing the content by channel number. This was followed in the late 1990's by the emergence of TiVo. TiVo consists of a television -viewing service as well as software and hardware. The client -side hardware consists of a TiVo set-top box which records TV shows on a hard-disk in the form of MPEG streams. Live T V and recorded shows are both routed through the hard-disk, enabling one to pause and rewind live TV as well as recordings. TiVo's main benefits are that it allows viewers to watch shows at times convenient for the viewer, convenient digital access to thos e shows and to find shows using numerous indices. Indices include genre, actor, director, keyword and most importantly for this paper: the probability that the viewer will like the show as predicted by a collaborative filtering system.
 The goal of a recommender system (an early one from 1992 is the Tapestry system [10]) is to predict the degree to which a user will like or dislike a set of items such as movies, TV shows, news articles or web sites. Most recommender systems can be categorized into two types: content-based recommenders and colla borative -based recommenders. Content-based recommenders use features such as the genre, cast and age of the show as attributes for a learning system. However, such features are only weakly predictive of whether viewers will like the show: there are only a few hundred genres and they lack the specificity required for accurate prediction. Collaborative filtering (CF) systems, on the other hand, use a completely different feature space: one in which the features are the other viewers. CF systems require the viewer for whom we are making predictions -henceforth called the "active viewer" -to have rated some shows. This 'profile' is used to find like -minded viewers who have rated those shows in a similar way. Then, for a show that is unrated by the active viewer, the system will find how that show is rated by those similar viewers and combine their ratings to produce a predicted rating for this unrated show. The idea is that since those like-minded viewers liked this show, that the active viewer may also like it. Since other viewers likes and dislikes can be very fine-grained, using them as features rather than content-based features leads to recommendations not just for the most popular shows but also for niche shows and audiences. We use a Bayesian content -b ased filtering system to overcome the cold-start problem (making predictions for new shows) for shows for which we do not have thumbs data from other users.
 The rest of the paper is organized as follows. In Section 2, we describe previous work on collaborative filtering systems, specially those making TV or movie recommendations. We outline the issues and the spectrum of approaches used to address those issues. Section 3 gives more detail on the flow of data starting with a viewer rating a show and culminating in the TiVo client making a recommendation to the viewer for a new show. Section 4 explains the performance algorithm: how the server computes correlations between pairs of shows and how each client uses those correlations to make show suggestions for its viewer. Section 5 details the server-side learning needed to support computation of the correlations. Section 6 outlines future challenges and directions and we conclude by summarizing the state of this massively fielded system. Recommender systems have been around since the Tapestry [10 ] system from 1992 which relied on a small community of users to make recommendations for each other. However, the first system we know of for making movie recommendations was the Bellcore Video Recommender system from 1995 [11 ]. Recommender systems can be categorized into content-based filtering systems and collaborative-filtering systems. In turn, collaborative filtering systems can be categorized along the following major dimensions: 1. "User -user " or "item -item" syst ems: In user -user systems, correlations (or similarities or distances) are computed between users. In item -item systems (e.g. TiVo and [15 ]) , metrics are computed between items. 2. Form of the learned model: Most collaborative filterin g systems to date have used k-nearest neighbor models (also referred to as Memory -based systems or Instance-based systems ) in user-user space. However there has been work using other model forms such as Bayesian networks [3], decision trees [3 ], cluster models [3 ], "Horting" (similarity over graphs) [1] and factor analysis [ 4 ]. 3. Similarity or distance function: Memory -based systems and some others need to define a distance metric between pairs of items (or users). T he most popular and one of the most effe ctive measures used to date has been the simple and obvious Pearson product moment correlation coefficient ( e.g. [9] ) as shown in Equation 1 below . As applied for item -item systems, the equation computes the correlation r between two items (e.g. shows ) : s1 and s2 . The summation is over all N users u that rat ed both items (shows) . t s1u is the rating given by user u to show s1 . T average rating (over those N users) given to s1 . s1 deviation of ratings for s1 . Similar definitions apply for the other show s2 . Other distance metrics used have included the cosine measure (which is a special case of the Pearson metric where means are zero) and extensions to the Pearson correlation which correct for the possibility that one user may rate programs more or less harshly than another user. Following in the track of TFIDF (Term -Frequency Inverse Document-Frequency ) as in [14] , another extension gives higher weight to users that rate infrequently; this is referred to as the 'inverse frequency' approach. Another approach is case amplification in which a non-linear function is applied to the ratings so that a rating of +3 , for example, may end up being much more than three times as much as a rating of +1. 4. Combination function: Having defined a similarity metric between pairs of users (or items), the system needs to make recommendations for the active user for an unrated item (show) . Memory -based systems typically use the k -nearest neighbor formula (e.g. [8 ]) . Equation 2 shows how the k -nearest neighbor formula is used to predict the rating t s for a show s . The predicted rating is a weighted average of the ratings of correlated neighbors t , weighted by the degree r s,s' to which the neighbor s' is correlated to s . Only neighbors with positive correlations are considered.
 In item-item systems, the k-nearest neighbor s are other items which can be thought of as points embedded in a space whose dimensions or axes correspond to users. In this view, equation 1 can be regarded as computing a distance or norm between two items in a space of users. Correspondi ngly, in user-user systems, the analog of equation 1 would be computing the distance between two users in a space of items. Bayesian networks have also been used as combination functions; they naturally produce a posterior probabil i ty distribution over th e rating space and cluster models produce a density estimate equivalent to the probability that the viewer will like the show. K-nearest neighbor computations are equivalent to density estimators with piecewise constant densities. 5. Evaluation criterion: The accuracy of the collaborative filtering algorithm may be measured either by using mean absolute error (MAE) or a ranking metric. Mean absolute error is just an average, over the test set, of the absolute difference between the true rating of an item and its rating as predicted by the collaborative filtering system. Whereas MAE evaluates each prediction separately and then forms an average, the ranking metric approach directly evaluates the goodness of the entire ordered list of recommendations. This allows the ranking metric approach to, for instance, penalize a mistake at rank 1 more severely than a mistake further down the list. Breese et al. [3 ] use an exponential decay model in which the penalty for making an incorrect prediction decreases exponentially with increasing rank in the recommendation list. The idea of scoring a ranked set is similar to the use of the DCG measure (discounted cumulative gain [12]), used by the information-retrieval community to evaluate search engine recommendatio ns . Statisticians have also evaluated ranked sets using the Spearman rank correlation coefficient [9] and the kappa statistic [ 9 ]. Now we summarize the findings of the most relevant previous work. Breese et al found that on the EachMovie data set [6], the best methods were Pearson correlation with inverse frequency and "Vector Similarity" (a cosine measure) with inverse frequency. These methods were statistically indistinguishable. Since Pearson correlation is very closely related to the cosine approach, this is not surprising. Case amplification was also found to yield an added benefit. On this data set, the clustering approach (mixture of multinomial models using AutoClass [5]) was the runner-up followed by the Bayes ian -network approach. The authors hypot hesize that these latter approaches worked relatively poorly because they needed more data than was available in the EachMovie set (4119 users rating 1623 shows with the median number of ratings per show at 26). Note that for the EachMovie data set, the true rating was a rating from 0 to 5, whereas the data sets on which Bayes ian net work did best only required a prediction for a binary random variable.
 Sarwar et al. [15] explore an item-item approach. They use a k-nearest neighbor approach, using Pearson correlation and cosine. Unfortunately for comparability to TiVo, their evaluation uses MAE rather than a ranking metric. Since TiVo presents a ranked list of suggestions to the viewer, the natural measure for its recommendation accuracy is the ranking metric, not MAE. Sarwar et al. find that for smaller training sizes, item-item has lower predictive error on the MovieLens data set than user-user but that both systems asymptote to the same level. Interestingly, they find the minimum test error is obtained with k=30 in the k -nearest neighbor computation on their data set. They find again that cosine and Pearson perform similarly but the lowest error rate is obtained by their version of cosine which is adjusted for different user scales. The issue of user-scale s is that even though all users may be rating shows over the same rating scale, one user may have a much narrower variance than another, or one may have a different mean than another. User-scale correction will normalize these users scores before comparing them to other users.
 Another issue facing recommendation systems is the speed with which they make recommendations. For systems like MovieLens which need to make recommendations in real-time for users connected via the Web, speed is of the essence. The T iVo architecture obviates this by having the clients make the recommendation instead of the server. Furthermore, each TiVo client only makes recommendations once a day in batch for all upco ming shows in the program guide whereas some collaborative systems may change their recommendations in real-time based upon receiving new ratings from other users.
 Lack of space prevents us from discussing work on privacy preservation ( e.g. [4 ]), explainability 1 and the cold-start issue: how to make predictions for a new user or item . In this section we describe the flow of data that starts from a user rating a s how to upload of ratings to the server, followed by server -side computation of correlations, to download and finally to the TiVo cli ent making recommendations for new shows.
 Every show in the TiVo universe has a unique identifying series ID assigned by Tribune Media Services (TMS). Shows come in two types: movies (and other one-off events) and series which are recurring programs such as 'Friends'. A series consists of a set of 1 An example of a collaborative-filt ering system employing explainability is the A mazon .com feature: "Why was I recommended this". This feature explains the recommendation in terms of previously rated or bought items. episodes . All episodes of a series have the same series ID. Each movie also has a "series" ID. Prediction is made at the series level so TiVo does not currently try to predict whether you will like one episode mor e than another.
 The flow of data starts with a user rating a show. There are two types of rating: explicit and implicit. We now describe each of these in turn.
 Explicit feedback : The viewer can use the thumbs-up and thumbs -down buttons on the TiVo remote control to indicate if she likes the show. She can rate a show from +3 thumbs for a show she likes to -3 thumbs for a show she hates (we will use the notation -n thumbs for n presses of the thumbs -down button ). When a viewer rates a show, she is actually rating the series, not the episode. Currently ( Jan. 2004), the average number of rated series per TiVo household is 98. Note that TiVo currently does not build a different recommendation model for each distinct viewer in the household in the remainder of the paper we will assume there is only one viewer per household.
 Implicit feedback : Since various previous collaborative filtering systems have noted that users are very unlikely to volunteer explicit feedback [13], in order to get sufficient data we decided that certain user actions would implicitly result in that program getting a rating . The only forms of explicit feedback are pressing the thumbs-up and thumbs-down buttons: all other user actions are candidates for implicit feedback. Currently, the only user action that results in an implicit rating happens when the user choose to record a previously unrated show. In this event, that show is assigned a thumbs rating of +1. Currently the prediction algorithms do not distinguish the + 1 rat ing originating from explicit user feedback (thumbs-up button) from this implicit +1 rat ing. Note that it is not clear if the +1 rating from a requested recording is stronger or weaker evidence than a +1 rating from a direct thumbs event. Viewers have been known to rate shows that they would like to be known as liking but that they don t actually watch. Some viewers may give thumbs up to high-brow shows but may actually schedule quite a different class of show for recording. One user action that could be an indicator even better than any of these candidates is number of minutes watched.
 Other possible user actions that could serve as events for implicit feedback are selection of a 'season pass', deletion, promotion o r demotion of a season pass. A 'season pass' is a feature by which the viewer indicates that she wants to record multiple upcoming episodes of a given series.
 The following sequence details the events leading to TiVo making a show suggestion for the viewer: 1. Viewer feedback : Viewer actions such as pressing the 2. Transmit profile : Periodically, each TiVo uploads its 3. Anonymization : The server anonymizes the thumbs 4. Server -side computation : The server computes pair-wise 5. Correlation download : Correlation pairs are downloaded 6. Client -side comput ation : The collaborative-filtering 7. Suggestions List : The collaborative predictions are 8. Inferred -recording s: If there is enough space on the hard Privacy is preserved by this architecture in a number of ways. There is no persistent (upload to upload) storage of any user object on the server since we are computing show to show correlations instead of the usual user -user approach. Not even an anonymized version (hence necessitating the upload of full thumbs profile) is maintained at the server. The communications between the server and client are encrypted. Once the TCP-IP connection is broken between server and client, the log file name is anonymized so that the server no longer knows which TiVo the log came from. The time and date-stamp of thumbs logs are also reset to Jan 1, 1970 to preclude anyone from correlating thumbs logs to error logs. All these measures made it quite difficult for us to debug upload errors during development! In this section we will describe the prediction performance task: to create a list of upcoming shows sorted by the predicted degree to which the user will like the show. This list is used to populate the Suggestions List that appears in the TiVo interface (Figure 1) . It is also used by the program scheduler to decide which suggested shows should be recorded if there is unused space on the disk.
 TiVo uses two algorithms for predicting how much the viewer will like the show: a Bayesian content-based filtering algorithm and the collaborative-filtering algorithm described in this paper. It is necessary to augment collaborative-filtering with such a content -based approach to address the cold-start problem [ Schein ]: namely the situation in which for new users and shows there is insufficient correlation data. Features for content-based filtering include the genres of the show and the cast: actors, directors etc. In order to construct this list, TiVo first removes from consideration episodes that the user has explicitly requested either as a single recording or as a season pass since obviously we do not want to recommend a show already scheduled for recording. The remaining episodes are considered as 3-tuples as follows: &lt;seriesID, thumbs, confidence&gt; Note that we have mapped from episode to series here. If there is more than one upcoming episode for a series, the first upcoming one will be used as the representative for the series. In the three -tuple , thumbs is an object with value from -3 to +3 and the confidence is a integer from 0 to 255. The series are sorted first by thumbs and then by confidence. In Figure 1, series that have rat ed by the user at +n thumbs appear with n thumbs-up icons. Series that are predicted by the TiVo to have +3 thumbs appear between those that are rat ed at +3 and those rat ed at +2. This ordering reinforces to the user that shows TiVo has more confidence in bringing to her attention shows she has rat ed versus shows it has predicted. There are three paths by which a series may appear in the suggestions list: 1. Explicitly rat ed but not scheduled for recording : 2. Predicted by collaborative filtering: Collaborative 3. Predicted by content-based filtering: Content -based TiVo has a background Linux thread running at low priority that makes content-based and collaborative-based predictions whilst not interrupting the responsiveness of the system for the viewer. This s uggestions engine runs periodically; at least once a day. The prediction algorithm is shown in Figure 2. The inputs to the c ollaborative subsystem -Collaborative -o f the suggestions engine are the series to be predicted along with the set of correlation pairs objects -Pairs . Each object in Pairs is a un rat ed, Collaborative finds the subset of the correlation objects that predict for S ; lets denote this set Pairs(S) . Next, Pairs(S) is sorted with respect to the absolute degree of the correlation, which we will denote as r(s1,s2) . Finally, a weighted linear average over the top k correlates is computed to yield the predicted thumbs level as a floating point number. The integer portion of this ( ) is made visible in the suggestions list (Figure 1) and the fractional part ( ) is used a as a confidence level . Unfortunately , for proprietary reasons we cannot reveal the exact value of k or the optimizations we applied to this apparently quadratic algorithm in order to make fast predictions. Previous work in collaborative-filtering had to deal with issues of making fast predictions at the server for many simultaneous users. This issues goes away completely for us in our distributed client architecture since in this averaging phase of the computation, each TiVo is only making predictions for its own user . If a show is un rat ed and does not have any correlation objects, we invoke the Content -Filtering algorithm to produce a prediction = &lt; , &gt; consisting of a predicted scalar thumbs level and confidence level . Series from the se three sources ( Content -Filtering, Collaborative -Filtering and orphaned episodes) are merged and sorted to produce the Suggestions list.
 Some comments on the scale of the operation are in order. I n order to produce a suggestions list daily for each user we are required to produce approximately 100 suggestions per user and since we have one million users, this requires the system as a whole to make on the order of 100 million suggestions daily. The system of Breese et al. [3 ] from 1998 made 13 suggestions per second on a 266MHZ Pentium so in one day ( about 10 5 seconds) it is capable of making on the order of 1 million suggestions. A modern 2.6GHZ machine might make 10 million suggestions so server -side, we would need 10 such machines running 24 hours a day just to make the suggestions that the million TiVo boxes make almost as a side -effect on the client -side. This refers to the client -side weighted average calculation. It does not take into account the more computationally challenging calculation of the correl ation pairs. In this section we will first describe the scalable server-side architecture which enables us to deal with the large volume of series pairs correlations and in the second part we will describe computational details we took to ensure statistical reliability of the series -series c orrelation estimates. There are approximately 300,000 distinct series airing in the USA might need to be calculated. However, we do not have user rat ings for all the shows: t he total number of rat ings is 100 million and the number of distinct rat ed shows is much less than this. Thus th e rat ings ar e sparse with respect to the number of possible series pairs and that poses a challenge if our correlation estimates are to be accurate for the less p opular series.
 In order to compute the correlation between two series s1 and s2 it is sufficient to retain a 7 by 7 counts matrix where the count n in the ij -th cell of the matrix is the number of viewers who gave i thumbs to s1 and j thumbs to s2 . Fi gure 3 shows the distributed, extensible server-side architecture. The basic insight is that since we are limited by data sparsity and server -side computational resources. Data sparsity means that if we require, for example, a minimum of 100 viewers to have rated both shows in a pair, we only need to compute on the order of 30,000 pairs with our current user population of one million client TiVos. Th e minimum n umber of viewers required to have rat ed a show, min-pair , is an important parameter to the server -side computation. The second limit is computational: our current server setup is surprisingly lean. If more server machines become available one can simply decrease the value of min-pair which in turn will allow us to calculate correlations for less popular shows and provide more niche recommendations. Alternatively, as our user population increases, even with our current value of 100 for min -pair , the amount of calculation we will have to do will increase. Our goal has always been to push out as many correlation pairs as possible within budget limits so we can cater to as many niche audiences as possible.
 In Figure 3, the first layer (horizontal) consists of logs servers: these machines accept thumbs logs and diagnostic error logs from the TiVo boxes. Each log is assigned a fictitious user ID. The lifespan of these user IDs is for the duration of the correlations computation. During computation, the log of a single viewer may be split among many machines indexed by series ID (second layer). We need a temporary anonymized user ID so that when we re -join the data to do pairs computation we can align thumbs for different shows from the same user.
 The second layer consists of machines which are indexed by series ID. Each machine is assigned a series ID range and it is responsible for counting the number of votes received for series in its range. A second parameter to the architecture is min-single -the min imum number of distinct users that have to give thumbs to a series for it to be even considered for pairs computation. Note that for a pair of series s1, s2 to have at least min -pair ratings jointly, then s1 and s2 separately must also have also received at least min-pair ratings. So it is an admissible heuristic if we prune series that by themselves receive less than min-pair ratings. However, if we set min-single &gt; min-pair then we run a possibility that we will prune series that jointly may have met the min-pair criterion. For example, if min -pair is 100 and min-single is 150 then we will prune away two series each of which singly received 145 ratings -yet it is possible (although unlikely) that had we retained them, we would have found that that pair ha d jointly received more than 100 ratings. Machines at this second layer are responsible for forwarding series IDs only of series that have been rated on at least min -single TiVos. They also forward, for each surviving series, the anonymized user IDs and thumbs values of all TiVos that had ratings for that series.
 partitioned over the space of pairs: series * series. For each series -series pair, the machine first does a simple quick filtering cal culation: it computes the number of TiVos which yielded ratings for that pair. If this number is less than min-pair, the series -series pair is pruned. Otherwise, the machine can embark on a more expensive calculation to compute the 7 by 7 matrix for that p air and then to compute the linear correlation. We use linear correlation rather than measures of association over discrete variables (e.g. Phi coefficient or Cramer's V [9]) because we treat the thumbs on each series as a discrete ordinal variable rather than a nominal (categorical) variable. Doing so affords greater statistical power. The output of this compute layer is a vector of 3 -tuples of the form &lt;series, series, correlation&gt;. Another way of taming the scale problem is to not conduct these server -side correlations for the entire series universe every day. For example, if we only have one server machine, we can compute 1/16 of the series-series space each day and thus complete one calculation over 16 days. The disadvantage of this appr oach of course is that the server cannot serve "fresh" correlations each day; it can only serve new correlations every 16 days. However this not so much of a disadvantage since the correlations between series tend not to vary much from day to day, or even week to week. Over time ranges of months, however, they do vary as the flavor of a series may change. To reduce network bandwidth and cost, the size of the download from the server to the TiVo is limited, so we round-robin serve TiVos over the 16 day period so this reason also allows us to spread the computation out over a multi-day period. Thus, within any given 16 day cycle, the server is serving pairs correlations from the previous cycle and is working on completing its current cycle. Now we examine the issue of the statistical reliability of the correlations. The correlations we compute are estimates of the "true" degree to which the two shows are correlated. We do not know the true degree of correlation between two shows even though we collect logs from all our viewers because viewers who like those shows may not have given thumbs to them, because our viewers are only a statistical (and non-random) subset of all viewers and because the show may be newer than other shows so fewer people have had a chance to give it a rating. For estimates based on sparse data, there may be significant error in the around our point estimate into consideration. For popular shows, we may have lots of evidence (support) to compute their pair -wise number of TiVos from which we have thumbs data for both shows. We may have a computed correlation of 0.8 between two may have a computed correlation of 0.8 between two rare series these two estimates; to assign a lower number to the 0.8 correlation arising from the support of 10? Currently we attack this problem by computing a 95% confidence interval around the correlation point estimate and we use the lower boundary of the confidence interval as a pessimistic, support-penalized correlation estimate. In our example, the confidence interval around the rare shows will be wider so the lower boundary will be lower than the lower boundary for the popular shows. Hence, after this support-same correlation number.
 have a normally-distributed quantity z with mean and standard-deviation and 95% confidence interval [ The refore, we can use the quantity and convert it back to a r-value and use that r value as our estimate of correlation.

Equation 3. Transform linear correlation to get nor mally Fo r example, the 95% confidence interval around the popular number as its correlation. For the rare show pair, the confidence estima te. Therefore, whereas the two pairs both had a 0.80 correlation estimate to begin with, now the show pair which has more thumbs evidence and support receives a higher estimate than the rare pair with less support. The number one item on our agenda for future work is a thorough empirical evaluation of the quality of suggestions. So far we have only evaluated the suggestions engine among TiVo employees by used to bootstrap TiVo's se rver -side computations in 2000. Other future work falls into three categories: user interface, server -side, and client-side new features. In the user-interface, suggestions can be used in a number of ways. Currently, the TiVo Suggestions screen serves the dual functions of a Scout (finding orphaned episodes of rated series) as well as suggesting unrated programs to be sorted in th e live -guide by predicted thumbs value. channels sorted not by channel number, but by the probability Figure 4. Plot of support (x-axis ) versus correlation (y-axis ) for 100,000 shows that you would like that other show. Undoubtedly, this would pose some legal issues because networks pay more to obtain lower numbered channels which are more likely to be channel surfed and watched than some obscure network at channel 1000. Another feature we have prototyped is 'Teach TiVo'. This feature would allow the user to explicitly rate genres and cast members. documentaries. It would also allow you to look at your complete set of thumbs so if someone in your household or a visitor example, a visiting child may thumbs lots of children s shows causing TiVo to suggest children s programming in an otherwise adult household.
 On the client, we might make improvements in the following ways: On the server, we might take TD-IDF steps so that shows that got few er votes would be more heavily weighted. Following Bree se [2] we could also give differential weights to users so that users that only have a few votes would receive greater user weights. TiVo has afforded us a rare opportunity to try collaborative filtering on an very large scale. The collaborative filtering system described here has been fielded in one million TiVo client boxes and is used daily by millions of users. Each of these viewers has rat ed approximately one hundred shows on average leading to a total set of one hundred million rat ings. TiVo uses an item-item form of collaborative filtering with strong provisions for privacy preservation. It uses k-nearest neighbor with Pearson correlation to make show recommendations. Correlations computed over less support are penalized in a principled way using the pessimistic estimate from a 95% confidence interval. The collaborative filtering system is augmented on the client by a content-based Bayesian recommendation system to address the cold start problem for new users and shows . The server architecture is highly scalable with capacity for many more users and can be throttled to provide more correlations to cover niche recommendations. TiVo s novel distributed collaborative-filtering approach reduces load on the server by having each client in parallel make recommendations for its own user.
 ACKNOWLEDGMENTS Thanks to Howard Look and Jim Barton at TiVo . Thanks to Mike Pazzani and Cliff Brunk for feedback.. [1] Aggarwal C.C., Wolf J.L., Wu K -L . an d Yu P.S. Horting . [2] Billsus, D. and Pazza ni, M. (1998). Learning Collaborative [3] Breese J.S., Heckerman D and Kadie C. (1998). Empirical [4] Canny J. (2002). Collaborative Filtering with Privacy via [5] Cheeseman, P. and Stutz, J. (1995). Bayesian Classification [6] Digital Equipment Research Center. [7] Cohen, W.W. and Fan, W. (2000). Web -Collaborative [8] Duda, R.O. and Hart, P.E. (1972). P attern Classification and [9] Everitt B. S. (2002). The Cambridge Dictionary of Statistics . [10] Goldberg, D., Nichols, D., Oki, B.M. and Terry, D. (1992). [11] Hill, W., Stead, L., Rosenstein, M., and Furnas, G. (1995). [12] J X  rvelin, K. and Kek  X l X  inen J. (2000). IR evaluation methods [13] Nichols D. (1997). Implicit rating and filtering. In [14] Salton, G. and Buckley, C. (1988). Term weighting [15] Sarwar, B., Karypis, G., Konstan, J. and Riedl, J. Item -Based 
