 Recent advanced technologies in DNA microarray analysis are intensively applied in disease classification, especially for cancer classification. Most recent proposed gene expression classifiers can successfully classify testing samples obtained from the same microarray experiment as trai ning samples with the assumption that the symmetric errors are co nstant among training and testing samples. However, the classifi cation performance is degraded with heterogeneous testing samples obtained from different microarray experiments. In this paper, we propose the  X  X mpact factors X  (IFs) to measure the variations between individual classes in training samples and heterogeneous testing samples, and integrate the IFs to classifiers for classification of heterogeneous samples. Two publicly available lung adenocarcinomas gene expression data sets are used in our experiments to demonstrate the effectiveness of the IFs. It shows that, with the integration of the IFs to the Golub and Slonim (GS) and k-nearest neighbors (kNN) classifiers, the classifiers can be further improved on the classification accuracy of heterogeneous samples. Even more, the classification accuracy of the integrated GS classifier is around 90%. Gene expression data, classification, feature selection, significance analysis of microarrays Recent advanced technologies in DNA microarray analysis allow us to examine gene expression levels for a huge amount of genes in a single experiment. Different data mining techniques are used to analyze and discover knowledge from gene expression data. Since the number of examined genes in an experiment is in term of thousands, data mining techniques have been intensively applied in the analysis of gene expression data [6]. Microarray data contain two kinds of errors, namely symmetric and random errors [12], and normalization is a common technique to minimize the symmetric errors. Symmetric errors are defined as controllable errors which induce almost-equally variations at microarray experiments [5], a nd random errors are defined as uncontrollable errors which induce different degrees of variations at microarray experiments by chance. Normalization is a common pre-processing step to remove or minimize the influence of the symmetric errors and achieve data centralization. Some common approaches are global normali zation, log-transformation, regression normalization, intensity -dependent normalization, and etc. Data centralization is a symmetry property of any two samples whose genes have equal or similar ratio of their expression levels. The objective is to identify a symmetry line among the gene expression levels of two samples, and transforms asymmetric gene expression levels into symmetric one with a transformation function [18]. Unfortunately, there is no efficient way to eliminate the random errors [15]. The symmetric errors are constant among samples in the same microarray experiments, but sa mples in different experiments have different symmetric errors. The symmetric errors within a microarray experiment can be considered as intra-experimental variations . Suppose that there are different data sets obtained by different microarray experiments, and then the symmetric errors of these data sets are different [21]. Hence, the differences of experimental variations among di fferent microarray experiments can be defined as inter-experimental variations . When we consider a combined data set of data sets from different microarray experiments, most normalization approaches, which work fine for a single data set, are incompatible for normalizing such combined data set because there are inter-experimental variations. Assume that the annotation of accession numbers and number of genes are the same. Practically, we can merge the data sets by appending, but this merging does not consider the inter-experimental variations, and thus the variations are distributed and shared over other data sets. The final result is like the one after an averaging process. That is, data sets originally with fewer variations are become unstable since data sets with more variations distribute their variations to them. In this paper, we propose the  X  X mpact factors X  (IFs) to measure the variations between individual classes in training samples and heterogeneous testing samples obta ined from different microarray experiments, and then integrate the IFs to classifiers for classification of heterogeneous samples. The reminder of this paper is organized as follows. We review classification approaches of gene expression data in section 2. Among different classifiers, we discuss the Golub and Slonim (GS) classifier in section 3 since it is one of the recent proposed classifiers, designed for classification of gene expression data with biological relevance [16]. Furthermore, we adopt the significant analysis of microarrays (SAM) (i.e. a feature selection method) to select features at IFs comput ation [19], and the details are described in section 4. In section 5, we present the algorithm of the IFs. Experimental results are presented and discussed in section 6. Finally, we give our concluding remarks and future works. Several publications have done a comprehensive review on the progress of cancer classification [6], [11]. Their works involve of reviewing a number of traditional and microarray-specified feature selection methods and classification approaches. For the classification with machine lear ning approaches, support-vector machines (SVM) [1], artificial neural network (ANN) [22], k-nearest neighbors (kNN) [11], and self-organizing map (SOM) [9] have been successfully applied to cancer classification. Their classification performances with different distance metrics, like Pearson correlation, Euclidean distance, cosine coefficient and signal-to-noise distance, are comp ared [6]. In addition, other approaches are Fisher linear discriminant analysis (FLDA) [9], CAST [3] and boosting [2]. Since gene expression data have a huge amount of genes, feature selection and extraction methods are used to find informative genes. These methods include principal component analysis, linear discriminant analysis, projection pursuit, and etc. A comprehensive review of feature selection and extraction methods can be found in Jain et al. [10]. A binary-class classifier, especially designed for classification of binary gene expression data, is called Golub and Slonim (GS) classifier [16]. Among different cl assifiers, only tree-based (i.e. boosting) and partitioning-based (i .e. GS classifier) classifiers have the interpretation of biologi cal relevance by the means of revealing the relationships between genes in a step-wise approach [11]. The processes of splitting a whole data set into a number of smaller subsets and performing correlation analysis on these smaller subsets provide insight understanding of selection mechanism and correlation analysis among genes. Other non-biological relevance classifiers, like ANN, SVM, and CAST, consider whole gene expression data as a set of distribution and perform classification based on th e distribution of gene expression levels without the consideration of individual relationships among genes, and their correlation analysis is encapsulated in a  X  X lack box X , implying that it is difficult to analyze biological relevance. Here, we describe the approach of the GS classifier. The first step is to calculate the  X  X ignal-to-noise X  (SNR) distance for genes. Assume that the expression levels of gene g in n training samples be represented by an expression vector g=(e 1 , e 2 , ..., e donates the expression level of g in tuple i , c={Normal, Cancer} be the class vector donating the classes of tuple i , and (  X 
Normal (g)) and (  X  Cancer (g),  X  Cancer (g)) be the class mean expression level and standard deviation of g in normal and cancer classes. First of all, g is normalized across samples with the mean expression level  X  (g) and standard deviation  X  (g) of the gene. The SNR of g is: The second step is to construct two class-vectors to represent the overall similarities of testing sample Y to normal and cancer classes, respectively. From the SNR -values, a positive SNR -value represents that the gene is in normal class, while a negative SNR -value represents that the gene is in cancer class [16]. Very often, certain representative genes are su fficient to represent the overall similarities of the classes. Hence, only k/2 genes having the highest and lowest SNR -values are selected and expressed as sets G corresponding classes, when k genes are required. Assume that avg(g) be the average value of the class mean expression levels of normal and cancer classes for gene g (i.e. avg(g)=(  X 
Cancer (g))/2 ). A similarity measure called vote factor , v(g) , is calculated for g in Y by v(g)=SNR(g)*[Y g -avg(g)] , where Y normalized gene expression levels of g in Y with respects to the  X  (g) and  X  (g) of the same gene in the training samples. The overall similarities corresponding to the classes, which are expressed as V positive and V negative for normal and cancer classes, are V 0) | V positive |&gt;|V negative | . If otherwise, it is classified as cancer. difference, which is called prediction strength (PS) , between them is calculated (in equation 2). If the PS is sufficiently large, the assigned class label is confirmed. Otherwise, the new sample is classified as  X  X ncertain X . A feature selection method called Significance Analysis of Microarrays (SAM) has been proposed by Tusher et al. to identify significant genes in microarray experiments based on the variations of the standard deviations in repeated measurements [19]. It aims at measuring fluctuations of the expression levels for a gene across a number of microarray experiments. It first calculates an observed relative difference , d in training samples X based on the ratio of change in gene expression to standard deviation for that gene. When using the same terminology as the GS classifier, d r (g) is: where s(g) is a measurement of variations for gene g (in equation size [7].  X  is the average number of m easurements in both classes. , where SAM uses a technique of permutations of the repeated measurements to identify significant genes. The idea is to assume that a significant gene in a class should have differential expression levels over another cl ass. However, after a certain number of permutations, such differentiations between two classes are eliminated because the permutations re-arrange the gene expression levels of both classes. After p permutations, it result, there are p d p (g)  X  X  for the gene, and the mean relative [19]. d (g) is used as a reference point to identify those genes with differential expression levels, which are called significant genes in this context. For a significant gene, its d comparably different from its d r (g) . A threshold, which is the distance in both directions away from a straight line with slope equal to 1 across the origin, is used to exclude the insignificant genes, which are enclosed by the range. When the threshold value is set, the smallest positive d least negative d r (g) are used as cutoff lines to predict the number of falsely-significant genes for th e corresponding threshold value. The number of predicted falsely-significant genes for d computed by counting the number of genes at its d exceed the cutoff lines. Finally, the number of predicted falsely-significant genes for the threshold value is the average number of predicted falsely-significant genes for all d r (g)  X  X . We propose the impact factors (IFs) to measure the variations between individual classes in training samples and a heterogeneous testing sample obtai ned from different microarray experiment. The rationale is to measure the inter-experimental variations (i.e. the differences between two microarray experiments) based on the significant genes in the training samples, and set up individual re ference points corresponding to classes in the training samples from the extracted significant genes. Every reference point is then used to calculate its own relative scaling factor for the corresponding class, and this factor is used to rescale the gene expression levels in the testing sample with respects to the reference point of that class individually. With the rescaled expression levels, relative distances to the training samples are calculated. In order to enhance the discriminative powers of the IFs, only those genes with a higher relative difference are selected and integrated to classifiers for classification of heterogeneous sample. Figure 1 shows the algorithm of IFs computation. In SAM, the number of predicted falsely-significant genes depends on the choice of the threshold values. In our studies, there is a convergence point for the number of predicted falsely-significant genes with an increasing threshold value. At this point, the number of predicted falsely-significant genes becomes minimal, and thus the extracted significant genes at this point have the lowest probability to be falsely-significant. Hence, the optimal threshold value, pro ducing the optimal number of significant genes, corresponding to the smallest number of predicted falsely-significant genes at the convergence point is chosen. For example, Figure 2 shows a graph of the threshold values against the number of predicted falsely-significant genes, and the optimal threshold value is chosen at the convergence point. With the feature selection method, significant genes are extracted from original genes in training samples. Assume that sets G and G X  be the sets of original and extracted significant genes, and hence we have G X   X  G . For each g  X  G X  , we calculate class trim-mean expression level. In a k % trim-mean value, data members are sorted, and k/2 % data members at each end of the sorted list are discarded. The k % trim-mean value is then calculated from those un-discarded members only. The idea of the trim-mean value is to eliminate outliners. We choose 30% of data members to be trimmed, the 30% class trim-mean expression levels are significant gene g, where g  X  G X  , in normal and cancer classes. For heterogeneous samples, handling of missing values and feature alignment are required. Assume that set G genes in a heterogeneous testing sample Y h . In fact, some extracted significant genes may not be existed in G G h  X  G X   X  G X  ). The objective is to construct a gene set G and thus G h  X  has the same set of genes as the extracted significant performed by looking for some commonly existed genes between G and G X  (i.e. G h  X   X  G h  X  G X  ), while the handling of missing value is done by the nearest neighbor (NN) approach. The training sample with the smallest Euclidean distance to the testing sample is the nearest neighbor, and the gene expression levels in the nearest neighbor are copied to the missing expression levels of the corresponding genes in the testing sample (i.e. G h  X   X  G X /G We estimate relative scaling fact ors for individual classes in the training samples based on the extracted significant gene set G X  and the constructed gene set for the heterogeneous sample G acting as reference points to rescale the gene expression levels in the heterogeneous sample. We first calculate baselines of, respectively, normal and cancer classes in the training samples. The baselines for the classes are the sum of all the corresponding class trim-mean expression levels of the significant genes g , expressing as  X  X   X  Normal (g) and  X  X   X  Cancer (g) , where g normal and cancer classes, respectively. Similarly, the baseline for a heterogeneous sample Y h is the sum of gene expression levels of the constructed genes g , expressing as g  X 
G h  X  . Since the relative scaling factors are used to minimize the inter-experimental variations, one possible way to perform the minimization is to amplify or reduce (i.e. rescale) the gene expression levels of the heterogeneous sample with respects to the reference points of the corresponding individual classes. Thus, the relative scaling factors are ratio of the baselines corresponding to the classes in the training samples to the baseline of the heterogeneous testing sample. They are: The relative scaling factors are then used to rescale the gene expression levels in the heterogeneous sample. The magnitudes of R Normal and R Cancer define different rescaling operations. When R &gt;0 , where c  X  {Normal, Cancer} , the total expression levels of extracted significant genes of class c in training samples are higher than the total gene expression levels, corresponding to extracted significant genes, in testing sample. Hence, a process of signal enhancement is required. Similarly, signal reduction is required for the case R c &lt;0 , and nothing happens for the case R The dissimilarity measures, d Normal (g) and d Cancer where g  X  G h , in the heterogeneous sample to both classes in the training samples are calculated by computing the ratio between them. For each g  X  G h , they are: where  X  where  X  The total dissimilarities of the heterogeneous sample to both classes are calculated by the sum of all d Normal (g)  X  X  and d respectively. By comparing these two values, we know the closeness of the heterogeneous sample to each class. If  X  d
Normal (g)&lt;  X  d Cancer (g) , the testing sample is closer to normal class than cancer class. Similar conclusion holds for  X  d difference, r(g) , for gene g between the two measures is too small, and the discriminative powers are small. In order to increase the discriminative powers of the IFs, only those genes that have a large r(g) in equation 10 are considered. Hence, we define a threshold value t , and only those genes with r(g) higher than t are taken into the IFs computation. Finally, the IFs for both classes are: where n is the number of the significant genes whose r(g)  X  X  are higher than a required threshold value t . Since d Normal (g) and d Cancer (g) are both in absolute values, IF and IF Cancer must be non-negative values. The lower-bounds of IFs are equal to 0. However, the upper-bound of IFs depends on the factor Y h g  X  R c , where c  X  {Normal, Cancer} . Since this factor is unbounded, the upper-bound is unbounded too. Consider the example of IFs computation in Figure 3. The expression levels of training samples and a heterogeneous sample are listed. First of all, SAM is performed, and significant gene set G X  is extracted. For the heterogeneous sample, significant genes 1071_at and 1319_at are aligned corresponding to the extracted significant genes, and their expression levels are copied to G Also, significant gene 1439_s_at is missed, and thus NN approach is performed. During the operation, assume that the first sample in cancer class is the nearest neighbor. Therefore, the gene expression level in the nearest neighbor corresponding to the missing gene expression level in the testing sample is copied to G in equation 6, 7, 8, and 9. With these fours parameters, the rescaling is performed, and the corresponding d d
Cancer (g) are calculated for gene g . Suppose that the threshold value is set to 2 in equation 11 and 12. When calculating the r(g) IFs computation since other r(g)  X  X  are excluded by the required threshold value. 
Accession Normal Cancer H eterogeneous sample (G h ) 100_g_at 362 178 296 273 192 272 -1071_at 689 532 250 15 97 194 16 1319_at 1233 809 546 482 551 221 63 1439_s_at 1533 1461 586 240 499 159 -1440_s_at 35 54 30 10 51 2 -37659_at 362 832 178 272 576 136 -37033_s_at 3424 6519 1170 2042 309 1823 -Gene selection is a useful preprocessing to select informative genes for classification and hence improves classification performances. In gene expression data, samples have too many genes (i.e. features) associated with them, and many of genes are noisy or irrelevant for the differentiability between normal and cancer classes. Their inclusion in classification not only introduces confusions with informative genes, but also increases computational complexity. The calculation of the IFs only selects a set of informative genes in training samples. The general criterion for the selection is that the genes should have sufficient differentiability between two classes. From previous works, gene selection is mainly achieved by statistical measurements, which can be divided into either correlation or distance measurements. Some common correlation measurements include the Pearson correlation [2] and Spearman correlation [6], while some common distance measurements are the Euclidean distance [11], Cosine coefficient [6], signal-to-noise ratio [9] and ratio of between-groups to within-groups [8]. Most statistical-based gene selection measurements identify informative genes based on differential expression levels of a gene between two classes, assuming that all genes (i.e. features) are direct-relevant to tissues (i.e. either normal or cancer) of samples. In fact, genes, differentially expressed between two classes, do not always imply that the genes are direct-relevant to tissue of either class since some differential expressions may be caused by different compositions of cell types. In fact, normal and cancer cells have different compositions of cell types (i.e. a kind of biological mechanism), and some genes are also responsible for the mechanisms in the compositions. Some informative genes extracted by the statistical measurements may be caused by the effects of different compositions, instead of direct-relevant to the tissues of either class. This issue is called gene or sample contamination [2], [11]. SAM uses a technique of permutations to minimize biological mechanisms in gene expression data, and extracts informative genes without the inclusion of biological mechanisms. Its significance has been demonstrated by the problem: the transcriptional response of lymphoblastoid cells to ionizing radiation [19]. The impact factors (IFs) are integrated to classifiers so that the integrated classifiers have the capability of classifying heterogeneous samples. The IFs are dissimilarity measures, namely IF Normal and IF Cancer , for normal and cancer classes, and they express the inter-experimental variations between the corresponding classes in training samples and heterogeneous testing sample. Bias factors,  X  Normal and  X  Cancer , are introduced in the integration of the IFs to minimize the impacts of unequal proportions of extracted significant genes among classes. When the expression data are acquired from microarray experiments, it is possible that the proportions of tissue-specific genes, related to normal and cancer classes, are uneven, and hence the proportions of extracted significant genes, identified by SAM, are uneven too. Therefore, the IFs have different degrees of bias according to the classes. As a result, we introduce bias factors in the integration to adjust the importance of the IFs for the corresponding classes. In fact, the possible range of the bias factors is subjective, and may be varied from data sets to data sets. For most classifiers using similarity/ dissimilarity measures for making classification decisions, one way to perform the integration is to multiply IFs directly to the measures since the IFs are dissimilarity measures. There are two cases for the integration. If there is a dissimilarity measure, the IF of a class is multiplied to the measure with the same class as the corresponding IF. In contrast, if there is a similarity measure, the IF of a class is multiplied to the measure with another class as the corresponding dissimilarity and similarity measures before the integration, while d X  similarity measures after the integration. The general approaches of the integration are: For the choice of classifiers, we adopt the Golub and Slonim (GS) classifier for the first integration because (1) the GS classifier is specially designed for binary-class gene expression data, and there are two classes of samples in our data sets. It makes the classifier performs well for the classification. (2) The GS classifier calculates features with the  X  X ignal-to-noise X  distance, which measures variations of a gene between two classes. Also, the SAM measures the variations of a gene between two classes. Hence, the feature selection strategy of the classifier and IFs is similar. (3) The semi-final deliveries of the GS classifier are two vote factors (i.e. V positive and V negative dissimilarity measures for the IFs (i.e. IF Normal and IF The integration is performed by multiplying the IF of a class to the V -value of another class since V -values are similarity measures. Supposed that factors to normal and cancer classes, respectively. We have: After the inter-experimental variations are included, the remaining steps are the same as the ordinary GS classifier. The testing sample is classified as normal for Similarly, it is classified as cancer for |  X  | | more, the class label  X  X ncertain X  is also assigned to it, whose new prediction strength is smaller than a required threshold value. K-nearest neighbors (kNN) classifier is a multi-class classifier. It first selects features from training samples by some feature selection measurements. From the selected features, dissimilarity measures are computed between training and testing samples, and then k training samples with the lowest dissimilarity are selected as the k -nearest neighbors corresponding to the testing sample. The final step is to assign a class label to the testing sample from the k neighbors based on majority voting (i.e. the most frequent class label among the k neighbors). The choice of k is subjective. However, for most cases, k is chosen as the total number of unique class labels plus 1. The ratio of between-groups values to within-groups values is used to calculate the significance of features for classification. Assume that  X  (g) be the mean expression level of gene g , the class mean expression level of gene g in class c , and x expression level of gene g in class c . Hence, the between-groups value, BSS(g) , and within-groups value, WSS(g) , for gene g in training samples X are [8]: With the BSS(g) and WSS(g) , the score, expressed as score(g) , of gene g is the ratio between them (i.e. score(g)= BSS(g)/WSS(g) ). Among all genes, some genes with higher scores are selected as features for classification. From the selected genes, the Euclidean distance is used to measure the dissimilarities between training samples X and testing samples Y for gene g (in equation 20) The integration of IFs into kNN classifier is different from that into GS classifier. Compared with the GS classifier, there is only one distance measure, expressing as d Euclidean (X, Y) , instead of two measures in the GS classifier, expressing as V positive Before the k training samples are selected, the d multiplied by the corresponding IF whose class label is same as the class label of the training samples X since both d Euclidean and IFs are dissimilarity measurements. Assume that  X  Y X d  X  Y X d From the new distance ) , (  X  y x d lowest dissimilarity are selected, and then the class label of the testing samples are assigned based on majority voting. adenocarcinomas. Two publicly published data sets of lung adenocarcinomas are used. The first one is published by Bhattacharjee et al. [4], and another one is Ramaswamy et al. [14]. Since there are different accession number annotations, namely Hu680/ Hu35KsubA and U95A, we mapped the Hu680/ Hu35KsubA annotation into the U95A annotation according to the mapping table done by Ramaswamy et al. [13]. In fact, the mapping is not simply one-to-one mapping. There may be duplicated accession numbers in the mapped data set. Thus, a pre-processing is performed to merge the expression levels by averaging all expression levels of the same accession number. The IFs with different values of the relative difference r for gene g are compared, and the results are shown in Table 1 and Table 2. Column 1 and 2 show different r X  X  and the IFs of corresponding classes. The meta-column 3 and 4 are the mean and standard deviation (SD) of the IFs for normal and cancer classes. Since the IFs are measurement of dissimilarity, the corresponding class with the smallest mean IF-value is supposed to be the predictive class of the testing samples, showed in meta-column 5. The use of the relative difference r is necessary since it excludes those genes with a smaller r -value between the IFs of the classes, enhancing the differentiability of the IFs. In Table 1, the predictive classes, which are determined by the smallest mean IF-value among two classes, are always same as the actual classes. The differentiability of the IFs is acceptable in Table 1 since the predictive classes of the smallest mean IF-value are same as the actual classes in all cases, while r -value is set to be 3 in Table 2 in order to enhancing the differentiability. In Table 2, the predictive classes of the smallest mean IF-value are different from the actual classes for the first two r -values. When r -values  X  3, the results have the same trends as in Table 1 (i.e. the predictive classes of the smallest mean IF-value are same as the actual class). In our investigation, a number of d Normal Therefore, the values of IF Cancer  X  X  are much lower than those of IF
Normal  X  X  for r =0 and 1. The results become reasonably for r =3 r =0 and 1, are excluded. From the results, r -values should be equal to or higher than 3 so that those genes with small r -values are excluded, and hence the differentiability of the IFs can be enhanced. The IFs for the data set of Bhattacharjee have higher discriminative powers than that of Ramaswamy et al. Although the class of the smallest mean IF-value is the predictive class, it is unavoidable to have overlapping cases. However, if the SD X  X  of the IFs are small, the chance of overlapping cases will be small. discriminative powers of the IFs for this testing data set (i.e. Bhattacharjee) are high. However, in Table 2, the SD X  X  of the IFs are quite high for most IF Normal  X  X , which means that overlapping cases are more frequent in this testing data set (i.e. Ramaswamy). The facts can be shown in the classification accuracy in next experiments. In those experiments, the classification accuracy for classifying the data set of Bhattacharjee et al. is higher than that of Ramaswamy et al since the discriminative powers of IFs for the data set of Bhattacharjee et al. are higher. r IFs Mean SD Mean SD Normal Cancer 0 IF 1 IF 2 IF 3 IF 4 IF r IFs Mean SD Mean SD Normal Cancer 0 IF 1 IF 2 IF Normal 5.9 7.7 20.4 16.9 IF Normal IF Cancer 3 IF 4 IF Classification results on heterogeneous testing samples, which are performed by the ordinary and integrated GS and kNN classifiers, are compared. We first evaluate the classification results of the ordinary GS classifiers with heterogeneous testing samples and different values of prediction strength (PS). Then, the IFs are integrated to the classifiers, and the same evaluation is performed again. In the experiments, r is set to 3, and set as 1 and 1.5, respectively, for achieving the best performance. Compared Table 3 with Table 5, the integration of the IFs can increase the relative difference between V positive hence fewer samples are classified as  X  X ncertain X . The number of  X  X ncertain X  samples is reduced because the IFs can successfully enhance the relative difference, and thus their corresponding PS X  X  exceed the required threshold values. When the training and testing data sets are reversed (in Table 4 and Table 6), the number of  X  X ncertain X  samples is increased with the integration because the integrated classifiers can successfully classify the samples, but the corresponding PS X  X  are still too small to pass the required threshold values. In fact, the number of misclassifications remains 17 for all cases with the ordinary GS classifiers because all normal samples are misclassified (in Table 4). Therefore, the number of  X  X ncertain X  samples is low. With the integration, the number of misclassifications is reduced by more than half for most cases, but the number of  X  X ncertain X  samples is increased. Actually, the number of correct classifications is still increased. Unfortunately, their PS X  X  are insufficient to let the correctly-classified samples pass the required threshold values. It can be proved when PS=0 . In addition, classification of  X  X ncertain X  is better than a misclassification. From the results, the integration of the IFs can still assign a correct class label to samples even though the confidence (i.e. prediction strength) is sometimes insufficient. 
No. of features .0 .1 .2 .3 .0 .1 .2 .3 
No. of features .0 .1 .2 .3 .0 .1 .2 .3 Bhattacharjee, obtained by the integrated GS classifiers. No. of features .0 .1 .2 .3 .0 .1 .2 .3 
No. of features .0 .1 .2 .3 .0 .1 .2 .3 For the classification results with the kNN classifiers,  X 
Cancer are set as 1 and 1.8, respectively, r is set to 3, and the same procedures, evaluating the GS classifiers, are performed. In Table 7, the performance of the integration is optimal only at certain number of features like 200 features, and becomes degraded for other feature sizes, especially for larger feature sizes like 300 features. In fact, a majority of normal samples are classified as cancer for 300 features, which is similar to the one for the GS classifiers shown in Table 4. In contrast, certain amounts of cancer samples are misclassified as normal for smaller feature sizes like 50 and 100 features. The optimal results are at 200 features since a reasonably number of samples, including normal and cancer samples, can be successfully classified into the correct class label. From the results, the integrated classifiers achieve the best performance with 200 features. Although the number of correct-classified normal samples with the integrated kNN classifiers is smaller than that of the ordinary kNN classifiers, the integrated kNN classifiers have a significant improvement for classifying cancer samples, which is always better than the corresponding ordinary GS classifiers with the same number of features. In Table 8, from 50 to 200 features, the integrated kNN classifiers perform almost the same as the ordinary kNN classifiers, but it can gradually enhance the performance with a higher number of features like 300 features. 
No. of features
No. of features A contingency table of binary-class classification is used to evaluate the classification performance [17]. Table 9 shows a generic contingency table for binary-class classification in our context. From the table, the following measurements are defined. 1. Accuracy (acc)  X  it measures the proportion of correctly 2. Sensitivity (S n )  X  it measures the fraction of actual positive 3. Specificity (S p )  X  it measures the fraction of actual negative Actual Cancer False Positive (FP) True Negative (NT) In Figure 4 and Figure 5, they show the classification accuracy, and there is a significant improvement with the integration. The accuracy of the integrated GS classifier is higher than or sometimes equals to that of the ordinary GS classifier. In Figure 4, there is no significant improvement for the integrated GS classifier with 50, 100 and 200 features. However, with 300 and 500 features, the improvements are around 10%. In addition, Figure 5 shows even better results. The accuracy of the integrated GS classifier is always higher than that of the ordinary GS classifier. The improvements are also around 10% in average. The trends of performance enhancement are kept for the integrated kNN classifier too although the rates of enhancement are not as high as for the integrated GS classifier. In Figure 4, the highest accuracy of the integrated kNN classifier is around 75%, while the integrated GS classifier is always higher than 85%. Although better results are shown in Figure 5, the highest accuracy of the integrated kNN classifier is still lower than the average accuracy of the integrated GS classifier. In fact, the integration to both classifiers most likely has higher performance than the corresponding ordinary classifiers. In terms of the classification accuracy, we can conclude that (1) the IFs improve the classification accuracy for classification of heterogeneous samples most of the times, and (2) even there is no improvement, the IFs do not deteriorate the results of the ordinary classifiers, which are not integrated with the IFs. For the sensitivities (in Figure 6 and Figure 7), the integrated GS classifiers outperform the ordinary GS classifiers, while the integrated kNN classifiers perform worse than the ordinary kNN classifiers for some cases because of certain amount of bias to normal samples. The integrated GS classifiers can recall higher or sometimes equal proportion of normal samples than the ordinary GS classifiers. The same sensitivities between the integrated and ordinary GS classifiers are hold with 50, 100 and 200 features in Figure 6. For the integrated kNN classifier, it sometimes behaves worse than the ordinary kNN classifier. In Figure 7, the integrated kNN classifier has serious performance degradation with 50 and 100 features. In fact, the ordinary kNN classifiers have a higher sensitivity since it has bias to normal class, showing in the corresponding specificity. Actually, almost all cancer samples are illogically misclassified as normal. Without the consideration of these two feature numbers, the integrated kNN classifier can most likely maintain the same sensitivities as the ordinary kNN classifier. Hence, it is still able to maintain the ordinary sensitivities to some extents. For the specificities (in Figure 8 and Figure 9), the integrated GS classifiers are most likely to have the same performance as the ordinary GS classifiers, while the integrated kNN classifiers have quite significant improvements over some cases. For all cases, both the integrated and ordinary GS classifiers can achieve 100%. For the kNN classifiers, there is the most significant improvement with 50 and 100 features as shown in Figure 9. For other cases, the integrated kNN classifiers also have better performance than the corresponding ordinary classifiers, even though the improvements are not as high as the one with 50 and 100 features. 
Figure 9. Specificity (S p ) for data set of Bhattacharjee et al. Gene expression data are biased since there are symmetric errors in microarray experiments. Most normalization methods can minimize the errors among samples obtained from the same microarray experiments. However, normalization of samples from different microarray experiments is more challenging since there are inter-experimental variations. With the feature selection by SAM, we have proposed the  X  X mpact factors X  (IFs) to measure the variations between individual classes in training samples and heterogeneous testing samples. The IFs are integrated to the Golub and Slonim (GS) and k-nearest neighbors (kNN) classifiers for classification of heterogeneous samples. Our evaluations show that the discriminative powers of the IFs between normal and cancer class are high. In addition, the integration of the IFs into performance of heterogeneous samples. After the IFs integration, the classification performance for heterogeneous samples is either improved and no deterioration in many cases for our experiments. Even more, the classification accuracy of the integrated GS classifier is around 90%. The future works are to integrate the impact factors into a meta-classification framework. In such framework, other classifiers, like ANN, SVM, etc, are consider ed, and the final classification decision is made by the integration of all or some results from these classifiers. Furthermore, other feature selection methods will also be considered to decide the robustness of the impact factors. [1] Aliferis, C.F., Hardin, D. and Massion, P.P. (2002): Machine [2] Ben-Dor, A., Bruhn, L., Friedman, N., Nachman, I., [3] Ben-Dor, A., Shamir, R. and Yakhini, Z. (1999): Clustering [4] Bhattacharjee, A., Richards, W., Staunton, J., Li, C., Monti, [5] Bilban, M., Buehler, L.K., Head, S, Desoye, G. and [6] Cho, S.B. and Won, H.H. (2003): Machine learning in DNA [7] Chu, G., Narasimhan, B., Tibshirani, R. and Tusher, V. [8] Dudoit, S., Fridlyand, J., and Speed, T.P. (2002): [9] Golub, T.R., Slonim, D.K., Tamayo, P., Huard, C., [10] Jain, A.K., Duin, R.P.W. and Mao, J. (2000): Statistical [11] Lu, Y. and Han, J. (2003): Cancer classification using gene [12] Morrison, N. and Hoyle, D.C. (2003): Normalization: [13] Ramaswamy, S., Ross, K.N., Lander, E.S. and Golub, T.R. [14] Ramaswamy, S., Tamayo, P., Rifkin, R., Mukherjee, S., [15] Schuchhardt, J., Beule, D., Malik, A., Wolski, E., Eickhoff, [16] Slonim, D., Tamayo, P., Mesirov, J., Golub, T. and Lander, [17] Tan, A.C. and Gilbert, D. (2003): An Empirical Comparison [18] Tsodikov, A., Szabo, A. and Jones, D. (2002): Adjustments [19] Tusher, V.G., Tibshirani, R., and Chu, G. (2001): [20] Virtanen, C., Ishikawa, Y., Honjoh, D., Kimura, M., [21] Yang, Y.H., Dudoit, S., Luu, P., Lin, D.M., Peng, V., Ngai, [22] Yao, X. and Liu, Y. (1999): Neural networks for breast Benny Y. M. Fung is a MPhil student at Department of Computing, the Hong Kong Polytechnic University. He received a Bachelor degree in Computing from the Hong Kong Polytechnic University in 2002. His current research interests include data mining and bioinformatics. Dr. Vincent T. Y. Ng is the Associate Professor for the Hong Kong Polytechnic University. He received a PhD degree from the Simon Fraser University. Prior to joining the Polytechnic in 1994, he worked for many years in epidemiology and biometry. He has been involved in the research and development of the patient information system, cancer mapping, and many other clinical studies. He has been awarded the Best Teacher and the Best Consultant of the Department in 1999 and 2000, respectively. In 2000, he received "The President's Award for Outstanding Performance/Achievement 1999" in the teaching category. At present, his research interests include databases, data mining, XML, Internet computing and medical informatics. 
