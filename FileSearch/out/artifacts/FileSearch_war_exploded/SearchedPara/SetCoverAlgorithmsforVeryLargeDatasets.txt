 The problem of Set Cover |to  X nd the smallest subcol-lection of sets that covers some universe|is at the heart of many data and analysis tasks. It arises in a wide range of settings, including operations research, machine learning, planning, data quality and data mining. Although  X nding an optimal solution is NP-hard, the greedy algorithm is widely used, and typically  X nds solutions that are close to optimal.
However, a direct implementation of the greedy approach, which picks the set with the largest number of uncovered items at each step, does not behave well when the in-put is very large and disk resident. The greedy algorithm must make many random accesses to disk, which are unpre-dictable and costly in comparison to linear scans. In order to scale Set Cover to large datasets, we provide a new algo-rithm which  X nds a solution that is provably close to that of greedy, but which is much more e X cient to implement using modern disk technology. Our experiments show a ten-fold improvement in speed on moderately-sized datasets, and an even greater improvement on larger datasets.
 H.2.8 [ Database Management ]: Database Applications| Data mining ; F.2.2 [ Analysis of Algorithms and Prob-lem Complexity ]: Nonnumerical Algorithms and Prob-lems Algorithms, Experimentation set cover, greedy heuristic, disk friendly  X  Work partially done at AT&amp;T Labs{Research.

The problem of Set Cover arises in a surprisingly broad number of places. Although presented as a somewhat ab-stract problem, it captures many di X erent scenarios that arise in the context of data management and knowledge mining. The basic problem is that we are given a collec-tion of sets, each of which is drawn from a common universe of possible items. The goal is to  X nd a subcollection of sets so that their union includes every item from the universe. Places where Set Cover occurs include:
In these situations, and others too, the goal is to  X nd a minimum set cover: a cover which contains the smallest number of sets. This corresp onds to the simplest explana-tion in mining, the cheap est solution to facilit y location, etc. Unsurprisingly , the Set Cover problem,  X nding the smallest set cover, is NP-hard, and so we must  X nd e X cien t appro ximate techniques which can  X nd a good solution. For-tunately , there is a simple algorithm that provides a guar-anteed solution: the natural \greedy" algorithm, which re-peatedly picks the set with the most unco vered items. The greedy algorithm is guaran teed to  X nd a cover which is at most a logarithmic factor (in the number of items in the universe) larger than the optimal solution. Moreo ver, no algorithm can guaran tee to impro ve this appro ximation by much [5].

For even moderately-sized instances, one migh t complain that this logarithmic factor is too large. However, it has been observ ed across a large number of instances that this metho d is surprisingly good in practice, especially when compared with other appro ximation algorithms [10, 9]. The greedy approac h is often found to choose only a small percen tage ( &lt; 10%) more sets than the optimal solution, and is dramat-ically cheap er to compute. Note that the optimal solution is usually determined by exhaustiv e exploration of exponen-tially many options. Therefore, the greedy metho d for Set Cover is widely used to solve instances across the broad set of applications outlined.

Having said this, a direct implemen tation of this metho d scales surprisingly poorly when the data size grows. As our abilit y to record data via sensors, instrumen tation and log-ging increases, the size of the instances of Set Cover to solve can rapidly become very large: (man y) millions of sets, drawn over universes of (man y) millions of items. The growth of such data is rapidly outstripping that of main memory (whic h has traditionally increased at a much slower rate than computing power or data volumes). It is therefore increasingly importan t to deal with instances that do not  X t convenien tly into main memory , but which are disk residen t.
In these cases, the seemingly simple greedy metho d be-comes surprisingly challenging. The basic idea, to repeat-edly pick the set with the maxim um number of unco vered elemen ts, becomes very costly , due to the need to update all other sets every time the curren t \largest" set is selected, to re X  X ct the items which have just been covered. Our exper-imen ts on several natural implemen tation choices for data that are disk residen t (detailed later) took many hours to ex-ecute on even moderately-sized instances of only megab ytes to gigab ytes. This presen ts a fundamen tal problem: how to scale this importan t computation to modern data sizes? Our Contributions. We consider the problem of solving large instances of Set Cover . In doing so, the contributions in this paper are: Outline of the paper. Section 2 lays down some tech-nical background, and suggests some natural approac hes to implemen ting the greedy algorithm. Section 3 describ es our new algorithm and analyzes its worst case behavior, while Section 4 shows that it is highly e X cien t in practice and pro-vides excellen t results. We outline some possible extensions in Section 5, and conclude in Section 6.
Despite the importance of Set Cover , there has been rel-atively little study of how to  X nd covers e X cien tly until quite recen tly. The oldest study of this question is due to Berger et al. [1] (15 years after the heuristic was  X rst analyzed). That work is concerned with parallelizing the heuristic, by allowing multiple computing entities with shared memory to choose sets at the same time. There, randomization is nec-essary to ensure that multiple processors do not pick sets which cover the same elemen ts redundan tly. The algorithm assumes full random access to memory for all processors, and so does not seem to apply to the case of disk-based data.
More recen tly, the ideas of Berger et al. have been applied to the distributed case, to  X nd (partial) set covers under the MapReduce paradigm [4]. This approac h requires ran-domization, and requires a number of passes over the data cubic in the logarithm of the size of the data in the worst case. Their algorithm used hundreds of invocations of Map-Reduce on a dataset with about 5M sets to matc h the qualit y of the greedy solution.
 Lastly , Saha and Geto or develop an e X cien t algorithm for Set Cover in the streaming model [15]. Their approac h re-quires multiple passes, however, and is built on an algorithm for Max k -Co vera ge . Their analysis shows that it requires O (log 2 n ) passes (at least logarithmically many even in the best case) to  X nd an appro ximate set cover. For the large datasets we consider, this equates to potentially hundreds of passes. For a gigab yte-sized dataset, a single pass takes time on the order of a minute, so the total cost can be many hours. In experimen ts in [15], the metho d achieves solutions Figure 1: Example input of m = 10 sets over the universe f A ; B ; C ; D ; E ; F ; G ; H ; I g of size 9 . that are somewhat worse than the greedy (o X ine) solution, by around 10%. The absolute time cost of the metho d is not describ ed.
We consider the standard (unweighted) Set Cover prob-lem. There are a universe X of n items and a collection S of m subsets of X : S = f S 1 ; S 2 ; : : : ; S m g . We assume that the union of all of the sets in S is X , with j X j = n . The aim is to  X nd a subcollection of sets in S , of minim um size, that covers all of X . An example input is shown in Figure 1.
Set Cover was one of the early problems to have been identi X ed as NP-hard; Ver tex Cover is an importan t spe-cial case.
 Greedy heuristic. The best-kno wn algorithm for Set Cover is based on a greedy heuristic [11]. Let  X  be the set of indices of sets in the solution so far and let C be the elemen ts covered so far. Initially (see Figure 1) there are no sets in the solution and every elemen t is unco vered, so that  X  = ; and C = ; . We repeat the following steps until all elemen ts are covered, that is, C = X :
Figure 2 shows the e X ect of this algorithm on the sample input shown in Figure 1. Initially the largest unco vered set is of size 5 (there are two such sets, so the algorithm arbi-trarily picks S 1 = AB CDE ). After this step, many items are covered|these are shown as lower case in Figure 2. Now the largest \unco vered set" is of size 2: there are two sets of this size, but they both contain the same unco vered items (FG), and from these the greedy algorithm arbitrarily picks S . The next step picks a set containing H, and the  X nal step covers the last remaining unco vered item, I, and termi-nates. Note that the optimal solution picks only three sets: S ; S 6 ; S 7 are su X cien t to cover all items. In this case, it is easy to argue that this is indeed optimal: consider elemen ts D, H and I. Since these never appear in any set together, the optimal solution must contain at least three sets, one for each of these elemen ts.

It is an oft-rep eated observ ation that this metho d hap-pens to perform well in practice (see, for example, [8]). It clearly runs in polynomial time, and is also an appro xima-tion algorithm for Set Cover (indeed, it is often one of the early examples shown in textb ooks on Appro ximation Algorithms).
 Appro ximating Set Cover . It is worth repeating a short proof of the following lemma.
 S 2 abdFG S 3 aFG S 4 bcG S 5 GH S 6 eH S 7 cI
S 3 afg S 4 bcg S 5 gH S 6 eH
S 7 cI S 8 a S 9 e S 10 I
S 3 afg S 4 bcg S 6 eh S 7 cI
S Figure 2: Execution of the greedy algorithm on ex-ample input
Lemma 1. The greedy algorithm in the previous para-graph produces a solution within a factor 1 + ln n of the optimum for Set Cover .

Proof. Let the number of sets in the optimal solution be  X  . Let C t be the set of covered elemen ts after t iterations of the greedy algorithm. We know at each iteration that there is some set that covers at least j X n C t j = X   X  previously-unco vered elemen ts, otherwise there would not be an opti-mum solution of size  X   X  . Since the greedy algorithm chooses the set with the largest number of unco vered elemen ts, it covers at least j X n C t j = X   X  new elemen ts. Therefore which means that after t iterations Consequen tly, if t is at least  X   X  ln n , the number of unco vered elemen ts is strictly less than 1, and thus a solution has been found. We note that  X   X  ln n migh t not be an integer, and thus we can only guaran tee a solution of size d  X   X  ln n e . This is strictly bounded by 1 +  X   X  ln n  X   X   X  (1 + ln n ).
Interestingly , in an appro ximation sense, this is essen tially the best that can be achieved for Set Cover . Feige showed that no (e X cien t) algorithm can guaran tee an appro ximation of the problem within a factor of (1  X  o (1)) ln n unless there are e X cien t algorithms for the class NP [5].
 A more naive algorithm. Several previous studies of the qualit y of the greedy algorithm have compared results to a so-called naive heuristic [4]. It proceeds as follows:
In the worst case this heuristic cannot provide an appro xi-mation with appro ximation ratio better than n= 6. Consider an instance in which S i = f 2 i  X  1 ; 2 i; : : : ; 2 k + i g for some k and all i  X  k . Now, j S i j = 2 k + 2  X  i , so the naive algorithm will process the sets in the order S 1 ; S 2 ; : : : ; S k adding the set S i to the solution, because it contains one unco vered elemen t, 2 i + 1. The optimal solution, however, comprises just S 1 and S k . Therefore the ratio of the sizes of the naive and optimal solutions is k= 2 = n= 6. A 1, 2, 3, 8 B 1, 2, 4 C 1, 4, 7 D 1, 2 E 1, 6, 9 F 2, 3 G 2, 3, 4, 6 H 4, 6 I 7, 10
Nevertheless, we include this heuristic in our experimen tal study for comparison with the greedy metho d and our new algorithm, and for consistency and comparison with prior work which has also used this heuristic.
The chief problem with e X cien tly implemen ting the greedy algorithm is that it demands picking the set with the largest number of unco vered items. As each new set is chosen, because it covers items that migh t be presen t in other sets, it is necessary to  X nd all sets which contain the covered items, and adjust the coun t of unco vered items in each set accordingly . Thus, algorithms for Set Cover must retriev e information from disk or main memory (depending on the implemen tation) in order to calculate the sizes of sets and to determine which items are in which set and vice versa. On large problem instances, these frequen t calls for data have a signi X can t e X ect on the running time of the al-gorithm. Although modern computers have large amoun ts of main memory , there is a memory hierarc hy, and e X ectiv e use of the cache often relies on locality of reference. This e X ect is even more pronounced on disk, since the cost of processing a single block once in main memory is, in most circumstances, orders of magnitude less than that of retriev-ing the block from disk. Hence, random access to data on disk can be highly expensiv e. Indeed models for the running time of disk-based algorithms often essen tially ignore the in-ternal computation costs, since these can be dominated by the I/O cost. In this paper, we analyze the performance of algorithms based on this external memory model of the running time.
We assume that the problem instance speci X es for each i a succinct description of the elemen ts that are in set S i consumes O ( P m i =1 j S i j ) words of memory or disk, assuming an elemen t can be represen ted in one word.

There are two canonical approac hes to implemen ting the greedy heuristic. At each step of the greedy algorithm we need to  X nd a set that has maxim um j S i n C j . The  X rst approac h involves the use of an inverted index (or  X le), the second involves multiple passes over the original data. In this section, we provide a basic description of each approac h. Further details and optimizations are describ ed in Sections 4 and 5.
 Inverted index. We can  X nd the maxim um j S i n C j by main taining these values in a large priorit y queue. In order to have up-to-date j S i n C j values, as a set S i  X  is added to the solution, we need to determine which other sets contain the items in S i  X  , that is, those items freshly included in C . This can be done by use of an inverted index, in which for each item j we have a succinct represen tation of As a prepro cessing step, the algorithm creates the T j 's, and then looks them up as the greedy iterations proceed. Fig-ure 3 shows the inverted index for the sample dataset shown in Figure 1. For each elemen t, the index lists the indices of the sets in which it belongs.

If we assume a cost model in which random accesses to locations in the memory hierarc hy take a constan t amoun t of time, the use of an inverted index seems to make the main tenance of the priorities in the priorit y queue e X cien t In this cost model, the running time of this approac h is O ((log m ) P n j =1 j T j j ). Note this is somewhat of an overes-timate as the updating of the j S i n C j in the priorit y queue can be done once for each i at each iteration of the algo-rithm. Generating the inverted index requires examining the full description of all the sets, which is subsumed by the expression above.

However, the (pre-)pro cess that generates the inverted in-dex is very unlik ely to observ e locality of reference in its construction of the T j sets. Moreo ver, consider retrieving the T j 's as the greedy algorithm proceeds: it seems very hard to predict which T j will be needed at which stage in the algorithm, and the memory accesses are likely to be ar-bitrary . Hence, in practice, the cost of this algorithm can be painfully high due to the random accesses to many locations on disk.
 Multiple passes. An alternativ e approac h avoids the pri-ority queue and the inverted index completely . Instead, we simply main tain the set C of elemen ts covered so far. At each iteration, we loop through all of the (previously-unadded) sets and note the value of j S i n C j , by a (simple) comparison of the elemen ts in the two sets. The running time depends on O (  X  P i j S i j ), or equiv alently (for comparison with the inverted index approac h) O (  X  P j j T j j ), where  X  is the size of the solution obtained.

The algorithm makes a linear number of passes over the data, which seems e X cien t when data are residen t on disk, since the number of random accesses (seeks of a disk head) is minimized. However, it will be very slow when  X  is large, since it essen tially has to read through the entire dataset to add a single set to the solution. An optimization can be applied when the number of items remaining to be covered becomes small. Once j S  X  i n C j drops below a certain threshold  X  , we migh t take a di X eren t approac h. For each value T  X   X  we loop through the (remaining) sets and then add a set if j S i n C j equals T . Determining the best value of  X  requires some experimen tation, but the running time is now O ((  X   X  ) P j j T j j ), where  X   X  is the number of sets in the solution whose size is at least  X  .

This multiple pass approac h has the advantage that it sweeps through memory sequen tially , without needing to generate, nor access, an inverted index. We later study em-pirically the tradeo X  between the large number of passes in this algorithm and the cost of the random accesses in the inverted  X le approac h.
Re X  X cting on the two approac hes to the greedy algorithm, there is considerable e X ort expended to  X nd the set S i  X  the maxim um value of j S i n C j . This requires either a pri-ority queue and an inverted index, or many passes through the instance description. What if we did not insist on  X nd-ing the set with the absolute maxim um j S i n C j , but just a Figure 4: The new algorithm executed on the sample input set whose unco vered elemen t coun t were close to maximal? Could we do this in a way that not only interacted with disk (or memory) in a friendly manner, with few passes through the data, but also had a reasonably good appro ximation fac-tor and performed well in practice?
More formally , supp ose instead of picking the exact max-imum, we instead chose a set which is within a very small constan t factor of the largest. We  X rst consider the impact on the appro ximation factor: Lemma 2. If an iterative algorithm always chooses a set S c to add to the solution with for  X   X  1 , then it has approximation factor which is at most 1 + (ln n ) = X  for Set Cover .

Proof. Consider the proof of Lemma 1. The greedy algorithm guaran tees that it chooses a set with at least j X n C t j = X   X  unco vered elemen ts. In the algorithm at hand, we can guaran tee that the set chosen has at least  X  j X n C t j = X   X  unco vered elemen ts. This  X  factor carries through the calculations of that proof, so that when t is at least  X   X  (ln n ) = X  there is fewer than one unco vered ele-ment. Again this quan tity migh t not be an integer, so the solution size is strictly less than Munagala et al. [14] suggest a similar heuristic for the Pipelined Set Cover problem, but our result in the Lemma above is novel.
The approac h we use is to partition the sets into sub-collections based on the sizes of the sets. To that end, we select a real-v alued parameter p &gt; 1, which will gov-ern both the appro ximation factor and running time of our algorithm. Initially , we assign set S i to subcollection S if p k  X  j S i j &lt; p k +1 ; let K be the largest k with non-empt y S ( k ) . The algorithm then proceeds in two loops:
Note that for values of p close to 1, the algorithm will separately track subcollections of sets whose sizes are 1 ; 2 ; 3 ; : : : ; 1 =p . Related notions are outlined in Berger et al. [1] for designing an e X cien t parallel version of the greedy algorithm for Set Cover . However, their phases concern not the sizes of the sets, but the number of sets that the elemen ts are in, that is, the j T j j values.
 Example. Figure 4 shows an example execution of the al-gorithm on the sample input of Figure 2, with parameter p = 2. Thus, we break the sets initially into those of size 1, 2{3 and 4{7. The algorithm  X rst considers the sets of size 4{7, and selects the  X rst set found, ABCDE (indicated via underlining). The next set now has only 2 ( &lt; 4) unco v-ered items (F and G), so a reduced version is appended to list 2, and the  X rst step  X nishes. Note that the original items ABD in S 2 have been remo ved, and are shown in paren the-ses. That is, not only have items ABD been covered, but in this reduced version, in S (1) , those items are not even recorded, to save space (see Section 3.3 for the bene X ts).
At the start of the next step, many items are now covered (indicated in lower case); however, the \unco vered sizes" of the sets are not known to the algorithm until these sets are inspected. As the algorithm In step 2, AFG has two unco v-ered elemen ts and so is added to the solution. Consequen tly, when the algorithm passes through the list of sets supp osed to be of size 2{3, it  X nds that there are no sets of \unco vered size" remaining in this range, due to items' being covered. In step 3, each of the sets I and GH has one unco vered item when inspected, and so is selected (and underlined). The remaining sets in list 1 have no unco vered items. Thus, the chosen cover is ABCDE, AFG, GH, I. Observ e that this is di X eren t from the cover chosen by greedy , but has the same number of sets (four). The appro ximation factor. The key fact is that a set added to the solution has at least p k unco vered elemen ts. From the design of the algorithm the following is clear.
Proposition 1. At the time in which subcollection S ( k ) is being processed, there is no set that has at least p k +1 covered elements.

Consequen tly, since p k =p k +1 = 1 =p , we know that the set chosen satis X es (1) with  X  = 1 =p and therefore Lemma 2 implies the following.

Lemma 3. The algorithm describ ed in this section is a 1 + p ln n approximation for Set Cover . Running time. Consider what happ ens to some set S i . Whenev er it is processed, each of its elemen ts is checked for presence in C . It is then either added to the solution and never accessed again, or it is added to a subcollection of smaller sets. In the worst case, the set S i is moved to the next subcollection in each round. Moreo ver, it is guaran teed to shrink by at least a factor of p every second time it is moved. Hence, we can bound the worst case total number of items in all of the manifestations of S i based on the following geometric series j S i j + j S i j
Consequen tly, we can bound the worst case running time of the algorithm by O ([1 + 1 = ( p  X  1)] P i j S i j ), which is at most a factor of 1 + 1 = ( p  X  1) as great as the time to scan the data. In practice, we do not expect to see such worst case examples: it is more likely that a set, if not picked, will move down multiple levels, rather than just one. More importan tly, if the subcollections are written to disk, each in a separate  X le, then the  X le accesses are sequen tial. Memory/disk considerations. Our algorithm's main ap-peal is that it behaves extremely well on external memory , and reasonably well on main memory . The initial partition-ing of S can be done with one sweep through S . Subse-quen tly, within each subcollection S ( k ) , the sets can be pro-cessed in two passes. In the  X rst pass, we simply add each set sequen tially , either in the initial phase of partitioning S or when processing some S ( k 0 ) with k 0 &gt; k . The second pass is the step in the for loop in which each set is examined in turn for its j S i n C j value. The other bene X t of our algo-rithm is that even when a set is pushed down to a lower subcollection, it has become smaller.

Referring to the analysis in the paragraph on running time, if information is read from disk in blocks of size B , then reading the input requires appro ximately D = d P i j S i disk reads. Each subcollection S ( k ) requires at most disk reads. Summing this over all subcollections, we have an upper bound of disk reads, where K  X  log p max j S i j .
The datasets used in our experimen ts come from the Fre-quen t Itemset Mining Dataset Repository , based on work-shops in 2003 and 2004 [7]. Each line in one of these  X les describ es a set in a natural way, as a white space-separated sequence of integers, where each integer represen ts an item. Table 1 describ es the prop erties of the problem instances. We acknowledge in particular the authors who made avail-able the acciden ts.dat [6], retail.dat [2] and webdocs.dat datasets [12].
 System. The system used for conducting the experimen ts is a 2.8GHz Intel Core i7 running the Mac OS X operating system version 10.6.3 under a light load. This system has 4 cores, 256KB L2 cache per core, 8MB L3 cache, 8GB of main memory and 2TB of hard disk.
The item and set indices, j and i , were stored as long integers, requiring eight bytes. For each problem instance, the values of m; n; max j T j j , and max j S i j were calculated in advance, so that only the required resources were used by the Set Cover algorithms. The sorting of the j S i j in the naive algorithm was done using the built-in C qsort function. Main tenance of the j S i n C j values and selection of a minim um such value were achieved with an array-based heap implemen tation of a priorit y queue.

The external memory greedy algorithms write out a copy of the problem instance (in S i form) to disk in a program-friendly format before doing further processing. Construct-ing an inverted index on external memory required calculat-ing the j T j j values; this was done by a  X rst pass through the program-friendly disk-stored S i data.

For each algorithm, we recorded the maxim um amoun t of main memory allocated on the heap at any one time during the program execution. We also recorded the total number of bytes written to and read from disk, not including the initial reading of the problem instance, nor of writing the solution out to a  X le. Note that the solution description is a  X le of integers, one to a line, in ascending order corresp ond-ing to the input  X le, of the indices i in  X . The recorded running time, however, included all steps in the program execution.

The output of each Set Cover program was veri X ed by a separate checking program to be a cover.

The threshold  X  used in the multiple pass greedy algo-rithm is 40, and was determined by trial and error.
We found that in practice our algorithm performed better if a threshold of p k  X  1 , rather than p k , was used to determine whether a set in S ( k ) should be added to the solution. The experimen tal results that are reported below use the p k  X  1 threshold. Note then that the appro ximation bound for the algorithm, as tested, is 1 + p 2 ln n , rather than 1 + p ln n .
The maxim um number of  X les for the S ( k ) , as used in our algorithm, allowed by the operating system was 250. When implemen ted in main memory , we allowed for up to 20,000 such subcollections. This limit on k imposes a constrain t on the size of p : a very small p would require a very large number of  X les. Speci X cally , if  X  is the upper bound on k , i.e., the maxim um number of subcollections, then p must be at least (max j S i j ) 1 = X  in order to ensure that (max j S
The results of our experimen ts are presen ted in Table 3 and Table 2. The algorithms used are as follows: greedy The greedy algorithm using an inverted index, as multipass The multiple pass greedy algorithm, without an naive The naive algorithm mentioned in Section 2.1. Disk-F riendly Greedy , aka DFG The algorithm pre-|Solution| (x 10 )
We  X rst note that we do not show results for the external memory versions of the greedy and multip ass algorithms for the largest problem instance. They required more than ten hours of computing time and seemed to place a very heavy burden on the hard disk. This underlines our main point in this paper: even on modern hardw are, what is curren tly considered a moderate-to large-size instance (about a mil-lion sets, totaling about 1.5GB) is simply beyond the reach of reasonable implemen tations of the traditional greedy al-gorithm.
 Disk-based results. Our algorithm was primarily designed to handle external datasets, and here it shines. Table 2 shows the memory usage, maxim um amoun t of disk used, running time of each algorithm, along with the size of the solution produced ( j Solution j =  X  ). Except on the smallest instances, our algorithm is dramatically faster than previous greedy metho ds. The speedup increases as the instance size increases: ranging from about 10 times faster on medium in-stances (acciden ts.dat, pumsb.dat, pumsbStar.dat) to over 37 times faster on kosarak.dat. As noted above, webdocs.dat took a matter of minutes for our algorithm to process, whereas greedy had not completed after several hours of run time. Even on kosarak.dat, multipass was over a hundred times slower than DFG.
 In-memory results. When there is su X cien t memory to run the algorithms in memory , our prop osed metho d com-pares very favorably to existing metho ds. Table 3 shows the results. We observ e that in terms of time, there is rel-atively little to choose between the metho ds until we reach the larger instances. Arguably on the medium size instances (acciden ts.dat, kosarak.dat), the greedy algorithm is slower, while our algorithm is about as fast as naive. On the largest instance (webdocs.dat), the speed advantage is clearer: our algorithm is over twice as fast as greedy , and even faster than naive, while multipass is just impractical. In terms of qualit y, our algorithm is essen tially equiv alent to the greedy algorithm, impro ving in some cases, and never  X nding a so-lution with more than 0.5% more sets than greedy nor mul-tipass. In contrast, naive is always worse, over 80% worse in one instance.
 Impact of parameter p . To understand better the behav-ior of the algorithm as p is varied, we show further exper-imen ts in Figure 5 for the in-memory case (disk-based was quite similar). This shows that while the time cost increases as p is decreased, this is not so signi X can t: increasing the accuracy guaran tee by a factor of 1000 only adds 30s to the initial cost of around 60s. This is much better than worst case analysis, which predicts that for p = 1 + 2  X  10 , the time overhead is at most 32 times the overhead for p = 1 + 2  X  5 Mean while, there is a clear bene X t of decreasing p closer to 1: we see that the solution size decreases, although less dramatically so. Although hard to see in the  X gure, there is impro vemen t as p decreases further, although minor: for the smallest values of p tried, the variation in solution size is only one or two sets out of 400,000.
 Summary . Clearly , implemen tations of greedy do not scale to modern data sizes even on modern hardw are; our tech-nique appears to scale well across a variety of settings. Per-haps most surprising is that the disk-based results for DFG are very close to those of the memory-based ones|in par-ticular, for the largest instance, our algorithm took about 90 seconds whether in memory or out (the lower duration in external memory is due to di X eren t choice of the parameter p ). This, and the fact that DFG was slightly faster than the naive algorithm, indicates that the new algorithm is not I/O bound, i.e., is not waiting for disk before it can proceed.
In terms of the qualit y of the results, our algorithm re-mains almost identical to greedy , whether in memory (Ta-ble 3) or on disk (Table 2), as discussed above. Note that while greedy obtains the same solution, our algorithm was run with di X eren t parameters ( p = 1 : 001 in memory , p = 1 : 05 on disk), and so obtains slightly di X eren t results. However, in both cases the results are close to those found by greedy , and better in some cases.
Here, we outline some variations to consider for extensions of this work in the future.
 Compact Represen tation of Covered Items. All our algorithms assume that it is feasible to store the set of items that have been covered so far in memory (for example, in hash table to allow quick searc h and update). When n , the size of the universe of items, is truly immense, it may be infeasible to store even this in memory exactly . Instead, a more compact represen tation is needed. If the universe is structured simply as the integers 1 ; 2 ; : : : ; n , then a simple bitmap index will su X ce. More generally , when the items are larger and less well-structured (such as web page URLs), a Bloom  X lter can capture the set of covered items e X cien tly with a constan t number of bits per universe item [3]. The data structure has a small false positiv e rate, meaning that the algorithm may leave a few items unco vered. Empirical analysis will be needed to determine the signi X cance of this appro ximation.
 Greedy on external memory . An additional heuristic to try is to attempt to combine the inverted index and multiple pass approac hes on external memory . First, partition the sets into two  X les, based on whether j S i j &gt;  X  or not. The  X le with the large sets would be traversed relativ ely e X cien tly because each block would have only a small number of sets and therefore would be read only a small number of times even if the sets were read in a somewhat arbitrary order. On the other hand, the blocks with sets of size at most  X  would be read in a sequen tial manner, because the algorithm switc hes to a multiple pass approac h. Choosing a good value of  X  to coincide with block size could be done empirically . We should keep in mind, however, that the inverted index migh t be the dominan t cost.
 Reducing sets. Our DFG algorithm rewrites a set S i to disk or memory in a \reduced" form, with the covered ele-ments remo ved, if it is not immediately added to the solu-tion. Applying this impro vemen t would potentially be useful in other cases, such as the greedy algorithm (especially the multiple pass version), although this migh t not make enough of a di X erence to the general poor performance.
Motiv ated by the increasing size of datasets compared the speed of external memory access, we studied the problem of e X cien tly  X nding set covers for large inputs. We ob-served that previous metho ds simply do not scale well, but that a simple algorithm has guaran teed performance and scales very gracefully to very large instances. It has best-in-class speed, and best-in-class solution qualit y, a combination achieved by none of the other algorithms.

A clear next step is to ask the same questions of other common large scale mining and optimization scenarios. In particular, many modern datasets are represen ted by large graphs with millions of nodes and edges (such as the web graph, social networks, comm unication graphs). It is natural to ask how to e X ectiv ely  X nd good qualit y covers, partitions and dominating sets within such large, disk-residen t, graphs.
Anthony Wirth's visit to AT&amp;T Labs{Researc h was sup-ported by the Australian Researc h Council. We thank David Johnson for helpful commen ts and conversations. [1] B. Berger, J. Romp el, and P. Shor. E X cien t NC [2] T. Brijs, G. Swinnen, K. Vanho of, and G. Wets. Using [3] A. Broder and M. Mitzenmac her. Surv ey: Network [4] F. Chieric hetti, R. Kumar, and A. Tomkins.
 [5] U. Feige. A threshold of ln n for appro ximating set [6] K. Geurts, G. Wets, T. Brijs, and K. Vanho of. [7] B. Goethals. Frequen t itemset mining dataset [8] L. Golab, H. Karlo X , F. Korn, D. Srivastava, and [9] F. Gomes, C. Meneses, P. Pardalos, and G. Viana. [10] T. Grossman and A. Wool. Computational experience [11] D. Johnson. Appro ximation algorithms for [12] C. Lucc hese, S. Orlando, R. Perego, and F. Silvestri. [13] M. Mihail. Set cover with requiremen ts and costs [14] K. Munagala, S. Babu, R. Motwani, and J. Widom. [15] B. Saha and L. Geto or. On Maxim um Coverage in the
