 Pattern discovery in sequences is a popular data mining task. Typically, the dataset consists of a single event seque nce, and the task consists of analysing the order in which events occur, trying to identify correlation among events. In this paper, events are items, and we look for association rules between itemsets. Usually, an itemset is evaluated based on how close to each other its items occur (cohesion), and how often the itemset itself occurs (frequency).

Recently, we proposed to combine cohes ion and frequency into a single mea-sure [2], thereby guaranteeing that if we encounter an item from an itemset identified as interesting, there is a high probability that the rest of the itemset can be found nearby. The proposed algorithm suffered from long runtimes, de-spite the efficient pruning of candidates. We now propose relaxing the pruning function, but making it much easier to compute. As a result, we prune less, but the algorithm runs much faster.

We further propose to introduce the concept of association rules into this setting. We wish to find itemsets that imply the occurrence of other itemsets nearby. We present an efficient algorithm to mine such rules, taking advantage of two factors that lead to its efficiency  X  we can mine itemsets and rules in parallel, and we only need to compute the confidence of a simple class of rules, and use them to evaluate all other rules. The concept of finding patterns in sequences was first described by Mannila et al. [7]. Patterns are described as episodes, and can be parallel, where the order in which events occur is irrelevant, serial, where events occur in a particular order, or a combination of the two. We limit ourselves to parallel episodes containing no more than one occurrence of any single ev ent type  X  in other words, itemsets. In this setting, Mannila et al. propose the Winepi method to detect frequent itemsets. The method slides a window of fixed length over the sequence, and each window containing the itemset counts towards its total frequency, which is defined as the proportion of all windows that contain it. The confidence of an association rule X  X  Y , denoted c ( X  X  Y ), is defined as the ratio of the frequency of X  X  Y and the frequency of X . Once the frequent itemsets have been found, rules between them are generated in the traditional manner [1].
Mannila et al. propose another method, Minepi , to search for frequent item-sets based on their minimal occurrences [7 ]. Here, however, association rules are of the form X [ win 1 ]  X  Y [ win 2 ], meaning that if itemset X has a minimal oc-currence in a window W 1 of size win 1 ,then X  X  Y has a minimal occurrence in a window W 2 of size win 2 that fully contains W 1 . As we plan to develop rules that tell us how likely we are to find some items nearby, we do not wish to use any fixed window lengths, so these are precisely the sort of rules we wish to avoid.
Generating association rules based on a maximum gap constraint, as defined by Garriga [4], was done by M  X  eger and Rigotti [8], but only for serial episodes X and Y ,where X is a prefix of Y . Most other related work has been based on the Winepi definitions, and only attempted to find the same rules (or a representative subset ) more efficiently [3, 5].

Consider sequence s = cghef ababcidjcgdlcmd , that will be used as a running example throughout the paper. Assume that the time stamps associated with the items are integers from 1 to 20. This short sequence is enough to demonstrate the unintuitiveness of the Winepi method for evaluating association rules. We see that for each occurrence of an a ,thereisa b right next to it. Similarly, for each g ,thereisa c right next to it. Logically, association rules a  X  b and g  X  c should be equally confident (to be precise, their confidence should be equal to 1). Winepi , with a window size of 2 (the number of possible windows is thus 21, as the first window starts one time stamp before the sequence, and the last one ends one time stamp after the sequence), results in fr ( a )= 4 21 ,fr ( ab )= larger window always gives similar results, namely c ( g  X  c ) &lt;c ( a  X  b ) &lt; 1. Due to space restrictions, we only present related work on association rules. A more extensive discussion of related work on discovering patterns in sequences can be found in [2] and [6]. As our work is based on an earlier work [2], we now reproduce some of the necessary definitions and notations that we will use here. An event is a pair ( i, t ), consisting of an item and a time stamp, where i  X  I , the set of all possible items, and t  X  N . Two items can never occur at the same time. We denote a sequence of events by S . For an itemset X , the set of all occurrences of its items is denoted by N ( X )= { t | ( i, t )  X  X  and i  X  X } . The coverage of X is defined as the probability of encountering an item from X in the sequence, and denoted The length of the shortest interval containing itemset X for each time stamp in N ( X ) is computed as The average length of such shortest intervals is expressed as The cohesion of X is defined as the ratio of the itemset size and the average length of the shortest intervals that contain it, and denoted Finally, the interestingness of an itemset X is defined as I ( X )= C ( X ) P ( X ). Given a user defined threshold min int , X is considered interesting if I ( X ) exceeds min int . An optional parameter, max size , can be used to limit the output only to itemsets with a size smaller or equal to max size .
 We are now ready to define the concept of association rules in this setting. The aim is to generate rules of the form if X occurs, Y occurs nearby ,where X  X  Y =  X  and X  X  Y is an interesting itemset. We denote such a rule by X  X  Y ,andwecall X the body of the rule and Y the head of the rule. Clearly, the closer Y occurs to X on average, the higher the value of the rule. In other words, to compute the confidence of the rule, we must now use the average length of minimal windows containing X  X  Y , but only from the point of view of items making up itemset X . We therefore define this new average as The confidence of a rule can now be defined as Arule X  X  Y is considered confident if its confidence exceeds a given threshold, min conf .

We now return to our running example. Looking at itemset cd ,weseethat the occurrence of a c at time stamp 1 will reduce the value of rule c  X  d , but not of rule d  X  c . Indeed, we see that W ( cd, 1) = 12, and the minimal window containing cd for the other three occurrences of c is always of size 3. Therefore, W ( c, d )= 21 4 =5 . 25, and c ( c  X  d )= 2 5 . 25 =0 . 38. Meanwhile, the minimal window containing cd for all occurrences of d is always of size 3. It follows that W ( d, c )= 9 3 =3and c ( d  X  c )= 2 3 =0 . 67. We can conclude that while an occurrence of a c does not highly imply finding a d nearby, when we encounter a d we can be reasonabl y certain that a c will be found nearby. We also note that, according to our definitions, c ( a  X  b )=1and c ( g  X  c ) = 1, as desired. The algorithm proposed in [2] and given in Algorithm 1, finds interesting itemsets as defined in Section 3 by going through the search space (a tree) in a depth-first manner, pruning whenever possible. The first call to the algorithm is made with X empty, and Y equal to the set of all items.
 Algorithm 1. INIT ( X, Y ) finds interesting itemsets
The algorithm uses the UBI pruning function, that returns an upper bound of the interestingness of all itemsets Z such that X  X  Z  X  X  X  Y . If this upper bound is lower than the chosen min int , the subtree rooted at X, Y can be pruned. The UBI function is defined as This pruning function prunes a large number of candidates, but the algorithm still suffers from long runtimes, due to the fact that each time a new itemset X is considered for pruning, W ( X, t ) needs to be computed for almost each t  X  N ( X ). For large itemsets, this can imply multiple dataset scans just to decide if a single candidate node can be pruned.

We propose a new pruning function in an attempt to balance pruning a large number of candidates with the effort needed for pruning. As the main problem with the original function was computing the exact minimal windows for each candidate, we aim to estimate the length of these windows using a much simpler computation. To do this, we first compute the exact sum of the window lengths for each pair of items, and we then use these sums to come up with a lower bound of the sum of the window lengths for all other candidate nodes.
We first note that We then note that each window around an item x  X  X must be at least as large as the largest such window containing the same item x and any other item y  X  X . It follows that Naturally, it also holds that To simplify our notation, from now on we will denote t  X  N ( { x } ) W ( xy, t )by s ( x, y ). Finally, we see that giving us a lower bound for the sum of windows containing X around all occur-rences of items of X . This gives us a new pruning function: This new pruning function is easily evaluated, as all it requires is that we store s ( x, y ), the sum of minimal windows containing x and y over all occurrences of x , for each pair of items ( x, y ), so we can look them up when necessary.
The exact windows will still have to be computed for the leaves of the search tree that have not been pruned, but this is unavoidable. Even for the leaves, it pays off to first check the upper bound, and then, only if the upper bound exceeds the threshold, compute the exact interestingness. The new algorithm uses NUBI instead of UBI , and is the same as the one given in Algorithm 1, with replacing line 3.

Let us now return to our running example, and examine what happens if we encounter node { a, b, c } , { d, e } in the search tree. We denote X = { a, b, c } and Y = { d, e } . With the original pruning technique, we would need to compute the exact minimal windows containing X for each occurrence of a , b or c .Afterafair amount of work scanning the dataset many times, in all necessary directions, we would come up with the following: t  X  N ( X ) W ( X, t )=7+5+4+3+3+3+ 7 + 11 = 43. The value of the UBI pruning function is therefore: Using the new technique, we would first compute s ( x, y ) for all pairs ( x, y ). The relevant pairs are s ( a, b )=4, s ( a, c )=8, s ( b, a )=4, s ( b, c )=6, s ( c, a ) = 27, s ( c, b ) = 25. We can now compute the minimal possible sum of windows for each item, giving us resulting in a sum of x  X  X max y  X  X \{ x } ( s ( x, y )) = 41. The value of the NUBI pruning function is therefore We see that by simply looking up a few precomputed values instead of scanning the dataset a number of times, we get a very good estimate of the sum of the window lengths. Unlike the traditional approaches, which need all the frequent itemsets to be generated before the generation of association rules can begin, we are able to generate rules in parallel with the interesting itemsets. When finding an inter-esting itemset X , we compute the sum of all minimal windows W ( X, t )foreach x  X  X apart, before adding them up into the overall sum needed to compute I ( X ). With these sums still in memory, we can easily compute the confidence of all association rules of the form x  X  X \{ x } ,with x  X  X , that can be generated from itemset X . In practice, it is sufficient to limit our computations to rules of precisely this form (i.e., rules where the body consists of a single item). To compute the confidence of all other rules, we first note that A trivial mathematical property tells us that As a result, we can conclude that which in turn implies that Meanwhile, we can derive that and it follows that As a result, once we have evaluated all the rules of the form x  X  X \{ x } ,with x  X  X , we can then evaluate all other rules Y  X  X \ Y ,with Y  X  X , without Algorithm 2. AR ( X, Y ) finds interesting itemsets and confident association rules within them further dataset scans. The algorithm for finding both interesting itemsets and confident association rules is given in Algorithm 2.

Looking back at our running example, let us compute the confidence of rule ab  X  c . First we compute c ( a  X  bc )= 3 4 =0 . 75 and c ( b  X  ac )= 3 3 . 5 =0 . 86. From Eq. 1, it follows that c ( ab  X  c )= 4 2 that this corresponds to the value as defined in Section 3. In our experiments, we aim to show three things: 1. our algorithm for finding interesting itemsets works faster than the one given 2. our algorithm for finding association rules gives meaningful results, without 3. our algorithm for finding association rules runs very efficiently, even with a To do this, we designed a dataset that allowed us to demonstrate all three claims. To generate a sequence in which some association rules would certainly stand out, we used the Markov chain model given in Table 1. Our dataset consisted of items a , b , c , and 22 other items, randomly distributed whenever the transition table led us to item x . We fine tuned the probabilities in such a way that all items apart from c had approximately the same frequency, while c appeared approximately twice as often. The high probability of transitions from a to b and c ,andfrom b to a and c should result in rules such as a  X  c and b  X  c having a high confidence. However, given that c appears more often, sometimes without an a or a b nearby, we would expect rules such as c  X  a and c  X  b to be ranked lower. Later on we show that our algorithm gave the expected results for all these cases.

First, though, let us examine our first claim. We used our Markov model to generate ten sequences of 10 000 items and ran the two algorithms on each sequence, varying min int ,atfirstchoosing max size of 4. Figures 1(a) and 1(c) show the average runtimes and number of evaluated candidate nodes for each algorithm, as well as the actual number of identified interesting itemsets. While our algorithm pruned less (Figure 1(c)), it ran much faster, most importantly at the thresholds where the most interesting itemsets are found (see Figure 1(b) for a zoomed-in version). Naturally, if min int = 0, the algorithms take equally long, as all itemsets are identified as interesting, and their exact interestingness must be computed. In short, we see that the runtime of the original algorithm is proportional to the number of candidates, while the runtime of our new algorithm is proportional to the number of interesting itemsets.

To support the second claim, we ran our association rules algorithm with both min int and min conf equalto0.Weset max size to 4. We now simply had to check which rules had the highest confidence. In repeated experiments, with various 500 000 items long datasets, the results were very consistent. The most interesting rules were a  X  c and b  X  c , with a confidence of 0.52. Then followed rules a  X  bc , b  X  ac and ab  X  c , with a confidence of 0.34. Rules a  X  b and b  X  a had a confidence of 0.29, while rules ac  X  b and bc  X  a had a confidence of 0.2. Rule c  X  ab had a confidence of around 0.17, while rules not involving a , b or c had a confidence between 0.13 and 0.17. We can safely conclude that our algorithm gave the expected results.
 To confirm this claim, we ran our algorithm on three text datasets: English 1 , Italian 2 and Dutch 3 . In each text, we considered the letters of the alphabet and the space between words (denoted ) as items, ignoring all other symbols. In all three languages, rule q  X  u had a confidence of 1, as q is almost always followed by u in all these languages. In all three languages, there followed a number of rules with in the head, but the body varied. In Italian, a space is often found near f ,as f is mostly found at the beginning of Italian words, while the same is true for j in English. Rules involving two letters revealed some patterns inherent in these languages. For example, rule h  X  c wasrankedhighinItalian(where h appears very rarely without a c in front of it), while rule j  X  i scored very high in Dutch (where j is often preceded by an i ). A summary of the results is given in Table 2.

To prove our third claim, we ran the AR algorithm on ten Markov chain datasets (each 10 000 items long), using min int of 0.025 and max size of 4, each time varying min conf . The average runtimes and number of found association rules are shown in Figure 1(d). We can see that the exponential growth in the number of generated rules had virtually no effect on the runtime of the algorithm. In this paper we presented a new way of identifying association rules in sequences. We base ourselves on interesting itemse ts and look for association rules within them. When we encounter a part of an items et in the sequence, our measure of the rule X  X  confidence tells us how likely we are to find the rest of the itemset nearby.
On our way to discovering association rules, we found a way to improve the runtime of the algorithm that finds the interesting itemsets, too. By relaxing the upper bound of an itemset X  X  interestingness, we actually prune fewer candidates, but our new algorithm runs much faster than the old one. To be precise, the runtime of the new algorithm is proportional to the number of identified item-sets, while the runtime of the old algorithm was proportional to the number of evaluated nodes in the search tree. Due t o being able to generate association rules while mining interesting itemsets, the cost of finding confident association rules is negligible.

Experiments demonstrated the validity of our central claims  X  that our al-gorithm for finding interesting itemsets runs much faster than the original one, particularly on long datasets, and that our algorithm for finding association rules gives meaningful results very efficiently.

