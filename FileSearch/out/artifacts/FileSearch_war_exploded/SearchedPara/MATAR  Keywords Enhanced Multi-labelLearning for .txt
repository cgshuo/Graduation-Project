 Tags have been widely used to categorize and search online content in many on-line applications such as blogs, question-answering sites, and online newspapers. For example, the question-answering site Stack Overflow 1 uses tags to categorize programming questions so that the questions can be easily found by appropri-ate answerers. Although tags are important for these applications, it also brings additional burdens to users. For example, when a user posts a programming question on Stack Overflow, he/she is required to add several appropriate tags for this question, which is not usually an easy task. This motivates the research to automatically recommend appropriate tags for users in these online applica-tions.

Roughly, existing tag recommendation methods can be classified into two cat-egories: collaborative-filtering metho d and content-based method (see Section 2 for a review). Methods in the first category r ely on users X  historical behavior, and these methods are more suitable to tag a r elatively fixed set of items (e.g., music and movies). On the other hand, methods in the second category mainly use the content information, and they are more suitable for content-based applications (e.g., blogs and question-answering sites). In this work, we focus on recommending tags for content-based applications. Our key observation is that in content-based applications, many tags actually have appeared in the content, and these tags are potential keywords in the con-tent. Fig. 1 shows an illustrative example from Stack Overflow. There are three tags (i.e.,  X  X # X ,  X  X vents X , and  X  X disposable X ) for the given question in this example. They all appeared in the question content, and they all revealed key informa-tion of the question. Actually, we empirically found that around 70% tags have appeared in the corresponding questions on Stack Overflow. Based on this ob-servation, we aim to boost the accura cy of tag recommendation by combining two content-based methods: multi-label learning and keyword extraction. Spe-cially, we first model the tag recommendation problem as a multi-label learning problem, and then incorporate a keyword extraction method into multi-label learning. Further, since one of the challenges of tag recommendation is from the large scale of the content data, we also speedup the proposed method by employing the locality-sensitive hashing strategy.

The main contributions of this paper are summarized as follows:  X  We propose a tag recommendation method (MATAR) for improving recom- X  We propose a fast version of MATAR, which employs approximation methods  X  Experimental results on two real dat a sets show that the proposed MATAR The rest of the paper is organized as follows. We first review related work in Section 2. Then, we present the proposed MATAR algorithm together with its fast version in Section 3. We present the e xperimental results in Section 4 and conclude this paper in Section 5. In this section, we briefly review related work.

We roughly divide existing tag recommendation methods into two categories: collaborative-filtering method and content-based method.

For collaborative-filtering method, the key insight is to employ the tagging histories (i.e., user-item-tag tuples) from all users. For example, Symeonidis et al. [18] adopt tensor factorization model on the user-item-tag tuples; Ren-dle et al. [11,12] further model the pairwise rankings into tensor factorization method. Other examples in this category include [4,7,15,3]. Overall, methods in this category are more suitable to tag a relatively fixed set of items (e.g., music and movies), and they are not able to recommend tags for the new content that has not been tagged yet.

In the second category of content-based method, the content itself is used as input. For example, some studies focus on the feature aspect by finding useful textual features (such as tag co-occurrence [2,21] and entropy [8]). In the al-gorithm aspect, classification models and topic models are widely used for tag recommendation. For example, Saha et al. [13] train a classification model for each tag and recommend tags based on multiple classifiers; Krestel et al. [5] use topic models to find latent topics from the content and then recommend tags based on these latent topics; Si and Sun [14] further propose a variant LDA model to link tags with the latent topics. Other content-based methods include [16,17,20]. In this work, we focus on content-based methods as we target at recommending tags for content-based applications.

There are also some other lines of research that are potentially useful for tag recommendation. Tag recommendation can be viewed as a multi-label learning problem by treating tags as labels. For example, TagCombine [21] is one of the few methods that apply multi-label learning for tag recommendation. However, TagCombine suffers from the class imbalance problem (i.e., each tag is only used by a very small portion of posts). Keyword extraction is another potentially use-ful tool for tag recommendation, as tags ma y appear as keywords in the content. For instance, keywords may be directly us ed as tags [10]; association rules between keywords and tags are also considered [19]. In this work, we propose incorporat-ing keyword extraction into multi-label learning for better tag recommendation. To the best of our knowledge, we are the first to combine multi-label learning and keyword extraction for tag recommendation in content-based applications. In this section, we present our algorithms for tag recommendation. The pro-posed MATAR algorithm combines multi-label learning with keyword extraction to enhance the recommendation accuracy. We further propose MATAR-fast to speedup the computations of MATAR. 3.1 Problem Statement Before presenting our algorithms, we first introduce some notations and define the tag recommendation problem. We use X m  X  d to denote the feature matrix where each row contains the features for an instance. Without loss of generality, we assume that there are m instances and d features for each instance. We use Y m  X  T to denote the associate tag matrix where T is the number of tags. We use subscripts to indicate the entries. That is, X i and Y i indicate the feature vector and the tag vector of the i th instance, respectively. Then, element Y i ( t )  X  { 0 , 1 } indicates whether the i th instance is assigned with the t th tag ( Y i ( t )=1 indicates that the instance is assigned with the t th tag and Y i ( t ) = 0 indicates otherwise). Based on the above notations, the tag recommendation takes the existing X matrix and Y matrix as input, and aims to predict the tag vector Y r for a given new instance X r . 3.2 The Proposed MATAR Algorithm Next, we describe the MATAR algorithm. To make the descriptions of our algo-rithms easier, we further define the following notations. For a given instance X i , we define N ( X i )asthesetofits K -nearest neighbors. Based on the tag vectors of these neighbors, we define a counting vector for X i as Basically, S i ( t ) counts the number of instances that have been assigned with the t th tag among the neighborhood of X i .
 Next, for a given new instance X r , we define two families of probability events. The first one is U t g ( g  X  X  0 , 1 } ), where U t 1 is the event that tag t belongs to X r and U t 0 is the event that tag t does not belong to X r . The other one is V k ( k have tag t among the K -nearest neighbors of X r . Based on these two families of probability events, we aim to estimate the probability P r ( t )bywhichthe new instance X r would be tagged with tag t (i.e., the t th tag). Following the K -nearest neighborhood multi-label learning framework [22], we can define P r ( t ) as In this work, we use the cosine similarity (i.e., the cosine distance between X r and X i ) to find the neighborhood.
Applying the Bayesian rule and decomposing the V t S Eq. (2) as Computing P ( U t g )and P ( V t k | U t g ). As shown in Eq. (3), we need to estimate { 0 , 1 ,...,K } ) to compute the probability P r ( t ). We resort to keyword extraction and frequency counting to compute these prior and posterior probabilities.
As mentioned in introduction, the key intuition of our method is that many tags actually appeared as keywords in the content. Let w t be the corresponding word of tag t ,wehave where P ( w t ) is the probability of which the word w t is a keyword in the content, and P (  X  w t ) is the probability of which the word w t is not a keyword in the content. Here, P (  X  w t )=1  X  P ( w t ).
 For P ( w t ), various keyword extraction methods can be used here. In this work, we adopt the TextRank [9] method. The basic idea of TextRank is to model the text as a weighted graph. Each vertex in this graph represents a unique word in the text, and an edge between two words indicates that these two words are close to each other in the text. Then, pagerank algorithm is applied on this graph to find the keywords. The output of TextRank is a score vector in which each word is assigned with a score to indicate the importance of this word. In other words, the score for each word indicates the probability by which this word could be a keyword. Therefore, we use this score as P ( w t ).

To estimate P ( U t g | w t ), we can approximate the result by making a few statis-We first find the instances where the corresponding word of tag t appears. Then, c w counts the number of these instances and c w,t counts the number of these instances that are tagged with t . Based on these two variables, we can estimate probability P ( U t g | w t )as where s = 1 is a smoothing parameter.
 for each tag t . We first find the instances where the corresponding word of tag t does not appear, and then c  X  w counts the number of these instances and c  X  w,t Algorithm 1. The Training Stage of Our MATAR Algorithm counts the number of these instances that are tagged with t . Based on these two variables, we can estimate probability P ( U t g | X  w t )as on the cases when the tags appeared in the content. If the tags did not appear in the content, we may directly estimate P ( U t g )as
Next, we show how we compute P ( V t k | U t g ). For each tag t , we first define two arrays c and c of length K + 1. We first find the training instances whose K -nearest neighbors contain exactly k instances with tag t ,andthen c [ k ]and c [ k ] count the number of these training instances that are tagged with t and without t , respectively. Based on these two arrays, we can estimate the posterior probabilities as
The overall MATAR algorithm is summarized in Alg. 1 and Alg. 2 (training stage and predicting stage, respect ively). Steps 1-4 in Alg. 1 compute P ( w t ) shown in Alg. 2. After computing N ( X r )and P ( w t )for X r , Steps 3-9 compute the probability P r ( t ) for each possible tag. During the iterations, we update Algorithm 2. The Predicting Stage of Our MATAR Algorithm mend a ranking list based on the P r ( t ) score for each possible tag. Algorithm Analysis. Here, we present some algorithm analysis for MATAR. Iterations of Step 2 in Alg. 1 require O ( md ) time. The main time consumption is from Step 3. Iterations of this step require O ( m 2 d ) time. From Step 5 to Step 9, we need O ( mTK ) time. In general, the time complexity of the training stage of our MATAR algorithm is O ( m 2 d + mTK ). Since K is usually much smaller than m and T , and the feature dimension d is a fixed constant, the time complexity of the training stage can be rewritten as O ( m 2 + mT ).

ForAlg.2,Step1requires O ( md ) time. Step 2 requires O ( d ) time. Steps 3-9 compute the probabilities P r ( t ), which requires O ( TK ) time. To sum up, the time complexity of the predicting stage of our MATAR algorithm is O ( md + TK ), which can be rewritten as O ( m + T ). 3.3 The Proposed MATAR-Fast Algorithm Next, we present a fast version of MATAR. The main time consumption of our MATAR algorithm is from the nearest neighbor computations. That is, we need to compute the similarities between any two instances. To speedup MATAR, we employ an approximate method to linea rly identify neighbors. Specially, we use locality-sensitive hashing [1] whose basic idea is to hash the input instances so that similar instances are mapped to the same bucket with high probability (the number of the buckets is usually much smaller than the number of the input instances). Therefore, we only need to compute the similarities within each bucket.

We consider the family of hash functions defined as follows. For each hash function in this family, we first choose a random vector v d  X  1 where d is the dimension size, and then define the following hash function h v : Further, we randomly pick L hash functions from this family where L determines the number of buckets. Applying these L hash functions to one input instance would output a bit vector composed of 1s or 0s (the bit vector is regarded as the encoding of a bucket). Then, we can compute the similarities within each bucket as an approximation for similarity computations. The detailed algorithm is omitted for simplicity.
 Algorithm Analysis. Finally, we analyze the complexity of MATAR-fast. As we stated above, the locality-sensitive hashing strategy is applied to find the neighborhood in Alg. 1 to reduce the complexity. The time complexity of MATAR-fast mainly depends on the L parameter, which is O ( m 2 2 L ). Typically, we can set L = O (log m ) (e.g., let L =log m 500 , there are averagely 500 instances in each bucket). Therefore, the s imilarity computation for each X i requires O (1) time as it only involves the instances in the same bucket. Overall, the time com-plexity of the training stage of our MATAR-fast algorithm is O ( md + mTK ), which can be rewritten as O ( mT ). In this section, we present the experime ntal evaluations. The experiments are designed to answer the following questions:  X  Effectiveness : How accurate are the proposed algorithms for recommending  X  Efficiency : How scalable are the proposed algorithms? 4.1 Experiment Setup Data Sets. We use the data from two real world sites, i.e., Stack Overflow (SO) and Mathematics Stack Exchange (Math), to evaluate our algorithms. They are popular CQA sites for programming and math, respectively. For both data sets, they are officially published and publicly available 2 . For features, we adopt the commonly used  X  X ag of words X  model. For tags, we put our focus on common tags and remove some rare tags. A tag is rare if it appeared less than times ( is equal to 0.5% of the maximum value of tag appearance in the data set). To compare with the existing methods, we also randomly select a small subset of the SO data. The statistics of the three data sets are summarized in Table 1. Evaluation Metrics. For effectiveness evaluation, recall is more important than precision in our problem setting. Therefore, we adopt the recall@k metric as defined below.
 where m is the number of test instances, Tag i is the actual tags for instance i , Rec i is the top-k ranked tags recommended for instance i .

For efficiency, we report the wall-clock time. All the efficiency experiments were run on a machine with eight 3.4GHz Intel Cores and 32GB memory. 4.2 Experimental Results Effectiveness Results. We first compare the effect iveness of the proposed algorithms (MATAR and MATAR-fast) with SVMSim [13], TagCombine [21], LDASim [5], MLKNN [22], and Snaff [6] on Math data and SO-Small data. We randomly choose 90% data for training and use the rest 10% data for testing. We set K = 60 for Math data and K = 70 for SO-Small data. The results on the two data sets are shown in Table 2.

We make several observations from Table 2. First, the proposed MATAR al-gorithm performs the best. For example, on Math data, MATAR improves the best competitor (i.e., SVMSim) by 3.7% and 5.2% for recall@5 and recall@10, respectively; on the SO-Small data, it improves the best competitor (i.e., Tag-Combine) by 6.0% and 18.9% for recall@5 and recall@10, respectively. Second, MATARalsoperformsmuchbetterthanMLKNN.Forexample,onMathdata, MATAR improves the MLKNN method by 16.7% and 13.6% for recall@5 and recall@10, respectively. Since MATAR can be seen as an extension of MLKNN, this result indicates that the keyword extraction indeed helps in tag recommen-dation. Third, although the performance of the proposed MATAR-fast algorithm is not as good as MATAR, it still better than the compared methods.
We also give the results on the entire SO data in Fig. 2. Here, we only show the results of MATAR-fast and Snaff as only these two algorithms can return results within 24 hours on the entire SO data. As we can see from the figure, our MATAR-fast performs much better than Snaff, i.e., it improves Snaff by 59.2% and 61.6% for recall@5 and recall@10, respectively.
 Quality-Speed Tradeoff. Next, we study the quality-speed tradeoff of different algorithms in Fig. 3. In the figure, we plot the recall@10 on the y-axis and the wall-clock time on the x-axis. Ideally, we want an algorithm sitting in the left-top corner. As we can see, our MATAR and MATAR-fast are both in the left-top corner. For example, compared with TagCombine, MATAR-fast is 5.3x faster wrt wall-clock time and 18.9% better wrt recall@10 on the SO-Small data. Although Snaff is faster than our methods, the accuracy of this method is much worse. Overall, our MATAR-fast achieves a g ood balance between the recommendation accuracy and the efficiency.
 Scalability Results. Finally, we study the scalability of our proposed MATAR-fast algorithm on the whole SO data. We vary the size of training data, and report the wall-clock time in Fig. 4. As we can see from the figure, the running time of our MATAR-fast algorithm scales linearly wrt the number of training data size, which is also consistent with our algorithm analysis in Section 3.3. In this paper, we have proposed a novel tag recommendation algorithm MATAR. The proposed MATAR combines multi-label learning and keyword extraction to enhance the recommendation accuracy. The basic intuition behind MATAR is the fact that many tags actually have appeared in the content in content-based applications. We have further proposed a fast version for MATAR based on the locality-sensitive hashing strategy. Experimental evaluations on two real data sets demonstrate the effectiveness and efficiency of the proposed methods. Acknowledgment. This work is supported by the National 973 Program of China(No. 2015CB352202), and the National Natural Science Foundation of China(No. 91318301, 61321491, 61100037).

