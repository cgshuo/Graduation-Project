 Feature selection is a process of selectin g a small number of highly predictive fea-tures out of a large set of candidate attributes that might be strongly irrelevant or redundant. It plays a fundamental role in pattern recognition, data mining, and more generally machine learning tasks [6], e.g., facilitating data interpreta-tion, reducing measurement and storage requirements, increasing predeceasing speeds, improving genera lization performance, etc.

Most feature selection methods approach the task as a search problem, where each state in the search space is a possible feature subset. Suppose we are given a set of input vectors { x n } N n =1 along with corresponding targets { y n } N n =1 drawn i.i.d from an unknown distribution P( x , y ), where x n  X  X  X  X  D is a training instance and y n  X  Y = { 0,1,. . . , C -1 } is its label, N , D , C denote the training set size, the input space dimensionality and the total number of categories respec-selection is to select a subset of M ( M D ) most predictive features, i.e., to ||  X  || the feature selection criterion function be represented by J (  X  ). Formally, the problem of feature select ion can be formalized as:
There are two basic elements in a typic al feature selection method [4,8]: ( i ) the search strategy , a procedure to generate candidate  X  ;and( ii )the evalu-ation criterion J (  X  ), a measure to assess the goodness of  X  .Existingsearch approaches are generally divided [8] into complete (exhaustive, best first, branch and bound, beam, etc.), heuristic (forward/backward sequential, greedy, etc.), or random (simulated annealing, genetic algorithm, etc.). Feature weighting is a greedy algorithm. It assigns to each feature a real valued number to indicate its usefulness, making possible to efficiently select a subset of features simply by searching in a continuous space. For this reason, this paper will fix the search strategy at feature weighting and focus mainly on the evaluation criterion aspect. However, extensions to other search schemes are straightforward.

Most of the existing evaluation criteria are based on heuristic intuitions or domain knowledge, and therefore still lacks rigorous theoretical treatment. For example, the Relief [9] algorithm is recently interpreted as a method to max-imize the average margin [15,5]. However, the definition of margin is based on heuristics. The secret behind the margin is unclear.

In this paper, we present an algorithmic (evaluation criterion) framework for feature selection, which selects a subset of features by minimizing the nonpara-metric Bayes error. Many existing appr oaches as well as new ones can be natu-rally derived from this framework. In particular, we find that the Relief algorithm attempts to greedily minimize the nonparametric Bayes error that is estimated by k -nearest-neighbor ( k NN) method. This new interpretation of Relief not only reveals the secret behind the margin concept, but also enables us to identify its weaknesses so as to establish new algorithms to mitigate the drawbacks. In this paper, an alternative algorithm, called Parzen-Relief, is proposed, which re-sembles the standard Relief algorithm but using the Parzen method to estimate the Bayes error. We will show that the empirical performance of Parzen-Relief usually outperforms Relief. In addition, we find that Relief makes an implicit assumption that the class distribution P ( c )=1 / 2. This undesirable assumption heavily limits its performance in handling imbalanced or multiclass data set. To address this drawbacks, we propose a MAP-Relief algorithm, which incorporate the class distribution into the margin maximization objective function. Both Parzen-Relief and MAP-Relief are of the same computational complexity as the standard Relief algorithm. However, both of them show significant performance improvement compared with Relief.

The organization of this paper is as follows. Section 2 presents the algorithmic framework. Section 3 offers a new interpr etation of Relief and presents a new al-ternative, i.e., the Parzen-Relief algorithm. Section 4 establishes the MAP-Relief algorithm to handle imbalanced and/or multiclass data. Section 5 presents the comparison results and Sectio n 6 summarizes the whole paper. 2.1 Nonparametric Bayes Error Minimization Theoretically, two different, but closely-related, optimal evaluation criteria can be identified. The first one [10] is based on information theory, which attempts to model the dependence between patterns and their labels in a reduced-dimensional space while retaining as much information as possible, i.e.,: two distribution p ( x )and q ( x ). We refer this theoretical criterion as representa-tive optimal criterion ( ROC ) to emphasize its aim to rule out irrelevant features. Various practical criter ia are related to ROC. Examples include the entropy or mutual-information (MI) criterion and its variations [6], and so on.
We propose here another cr iterion. In contrast to ROC, this criterion is more straightforward and pragmatic. It considers classification directly and naturally reflects the Bayes error rate in the reduced space, i.e.: where  X  denotes a decision rule, E x { err (  X  |  X  ( x )) } is the generalization error of  X  in the reduced space, c  X  X  0 , 1 , ..., C  X  1 } . We called this optimal criterion discriminative optimal criterion ( DOC ) to highlight its goal to maximize the discriminating ability of features.

The ROC criterion has been proved powe rful for its keeping as much informa-tion for modelling the posterior distribution, which is useful for many domains [1]. It is also closely related with the Bay es error for classification [7]. However, for many applications where x have high dimensionality, modelling the posterior probability with limited samples is not only risky, but also wasteful of resources. In contrast, there are several compelling reasons for using DOC to assess the quality of features if we only wish to make classification decisions. One justi-fication is from Vapnik [16] that  X  X ne should solve the problem (classification) directly and never solve a more general problem (modelling P ( y | x )) as an inter-mediate step. X 
In practical cases, we cannot compute the Bayes error exactly because the precise distribution for generating the data is not available. Therefore, approxi-mate methods for estimating the Bayes error is necessary. There are in general two distinct approaches.  X  Wrapper Methods optimizes the generalization error of  X  with respect to  X  Filter Methods 1 estimates the Bayes risk directly without using a specific
Given a set of training data, the expectation over x can be approximated by the empirical average, i.e.: To estimate P ( c |  X  ( x n )), there are two types of methods, namely, the parametric and nonparametric estimation methods. When the parametric methods [1] are concerned, one typically assumes a generative model for ( x , y ) and estimates P ( x ,y ) through, e.g., maximum-likelihood or Bayesian estimation methods.
We use non-parametric estimators to estimate P ( c |  X  ( x n )) and approximately minimize the Bayes error by solving the following problem:
An obvious advantage to use nonparametric estimators is that the results will be robust to any probability distribution since the estimators do not rely on specific distribution assumptions. However, to directly estimate P ( c |  X  ( x n )) is practically difficult because x n is usually in a high dimensional continuous and use nonparametric method to estimate P ( x n | c ). 2.2 Related Works Saon et al [14] also proposed to reduce the input dimensionality by minimizing the Bayes error. However, their methods were established for the purpose of lin-ear feature transformation, not for feature subset selection. In addition, they used indirect approaches to approximately minimize Bayes error, i.e., by maximizing the average pairwise divergence or minimizing the union Bhattacharyya error bounds . To make their approach tractable, Gaussian assumption has to be made about the class-conditional densities, which strongly limits the performance of their methods, because multimodality, or non-Gaussian distribution is frequently observed in practical applications. These drawbacks are also shared by similar methods such as the Bhattacharyya distance approach [3,19]. In the context of vision recognition, Vasconcelos [17] proposed a feature selection approach based on the infomax principal. Although their criterion were shown closely related to Bayes error, it is suboptimal in the minimum Bayes error sense, too. Carneiro et al [2] proposed a joint feature extraction and selection algorithm by minimizing the Bayes error. However, they used Ga ussian mixture models to estimate the class-conditional distribution. A technical difficulty is that the number of mixture components, which has a dominant importance in their method, is very difficult to be determined in practice. Weston et al [18] proposed a learning algorithm that achieves variable selection by minimizing the zero-norm regularization to enforce sparseness of a kernel machine. To make the computation tractable, con-vex loss functions, e.g., the hinge loss function used in support vector machines (SVM), have to be employed as the optimization objective. Although the Bayes error can be naturally reflected by the zer o-one loss function, these convex sur-rogate loss functions offer poor approximation to the zero-one loss. Therefore, their approach does not directly minimize Bayes error, either. Among the existing feature weighting methods, Relief [9,15] is considered one of the most successful one due to its effectiveness, simplicity and particularly the ability to tackle dependent features [13]. Recently, Gilad et al [5] established a new variation of Relief based on the concept of margin. This idea was further explored by Sun [15] to provide a new interpretation of Relief as a max-margin convex optimization problem. This new p erspective simplifies the computation significantly. However, the secret behind the success of Relief is still unclear. We will show that Relief approximately minimizes the nonparametric Bayes error estimated by k NN method via a greedy feature weighting search scheme.
According to [15], Relief is equivalen t to a convex optimization problem: margin for the pattern x n , H ( x n )and M ( x n ) denote the nearest-hit (the near-est neighbor from the same class) and nea rest-miss(the nearest neighbor form different class) of x n respectively. By using the Lagrangian technique, a simple close-form solution to Eq.(7) can be derived, i.e.:. where m = 1 N n n =1 m n is the average margin, (  X  ) + denotes the positive part.
We now show how Relief minimizes the nonparametric Bayes error via greedy feature weighting. Since Relief is originally established for binary classification tasks, we first consider binary labels, i.e., y n  X  X  0,1 } , and will extend the results to multi-class scenarios in Section 4. Suppose the class distribution P ( c =1)= P ( c =0)=0.5, estimating the class-conditional probability by 1-NN estimator, we have: where V (1) and V (2) denote the volumes of the hyper-spheres from x n to H ( x n ) and to M ( x n ) respectively. However, to obtain the optimal  X  (  X  ), minimizing Eq.(9) is an NP-hard combinatorial optimization problem. Therefore, heuristic search is necessary. Considering a greed y search scheme, which searches each dimension independently with a feature weighting scheme: That is: where M ( d ) n and H ( d ) n denote the nearest-miss and nearest-hit of x n in the d -th dimensional subspace, the last line of Eq.(10) follows from the consideration to avoid numerical overflows. Clearly, Eq.(11) will be identical to Eq.(7) when we of using D 1-dimensional nearest-miss X  X  M ( d ) n ,weuseasingle D -dimensional nearest-miss M ( x n ) and approximate the d -th 1-dimensional nearest-miss M ( d ) n with the d -th element M ( d ) ( x n )of M ( x n ). We now establish two alternatives by exploring the new interpretation.
 Remark 1. Parametric estimation methods a re usually consid ered preferable when a proper generative model (based on prior knowledge) is available. Partic-
We refer this algorithm as  X  Fisher-Score-based Feature Weighting  X  ( FFW ), since it has close similarity with the feature ranking criterion Fisher weights obtained from Eq.(12) is identical to the ranking results of Fisher score. Remark 2. Parzen widow estimator is also applicable. Here we consider a specific Parzen widow function, i.e., the truncated potential function: We have: denote the Homogenous and Heterogeneous Neighbor Set of x n .

Following a series of similar simplification, we have a new feature weighting method, which we called  X  Parzen-Relief  X ( P-Relief ): where the margin m p n = x An undesirable assumption made by the Relief algorithm is that P ( c )=1 / 2for all the classes c =0 , 1 ,...,C . However, this is less likely the case in practice. To address this problem, we modify the definition of the margin to include the prior probability of each class, i.e.: is the class distribution. We term this algorithm as MAP-Relief ( M-Relief ). From our new interpretation, the rationale of this formulation is clear. In Section 5, we will show that while the performance of other algorithms in the Relief family degrades significantly when the data set is strongly imbalanced, M-Relief is less sensitive to such sampling bias.

Another advantage of M-Relief algorithm is its ability of handling multi-class data. The original Relief algorithm only works for binary classification problems. ReliefF [11] extends it to multi-class tasks by a heuristic updating rule, which is equivalent to solve Relief with the margin vector: where M c ( x n ) is the nearest miss of x n in class c , c  X  X  0,..., C  X  1 } . Therefore, one needs to search for k -nearest-hit and k  X  ( C  X  1)-nearest-miss for each sample to solve ReliefF. However, form our new interpretation of Relief, this is clearly unnecessary, because in general the fo llowing relationship always holds: The Iterative-Relief (I-Relief, [15]) al gorithm deals with multi-class data using a margin vector defined somewhat similar with our definition m  X  n , but with an implicit assumption P ( y n )=0.5. Obviously, this assumption is inappropriate for problems involving ( C&gt; 3) categories. This could become more severe when C goes larger such that the  X  X ne-versus-rest X  splits (i.e.: { x i : y i = y n } and { x j : y = y n } ) of the data set become more and more imbalanced.

It is interesting to see that M-Relief possesses advantages of both ReliefF and I-Relief, and at the same time mitigates their drawbacks: ( i ) Similar with ReliefF, M-Relief incorporates the class distribution to tackling imbalanceness; ( ii ) Similar with I-Relief, M-Relief needs only one, instead of k  X  ( C  X  1), nearest-miss for each pattern. Both advantages, i.e., computational efficiency and ability to handling imbalanceness, would become especially prefer able when problems with very large C are faced. We conduct extensive experiments to evaluate the effectiveness of the proposed methods. Twelve benchmark machine learning data sets from UCI collection are selected because of their diversity i n the numbers of features, instances and classes, as summarized in Table.1. To facilitate the comparison, fifty irrelevant features (known as  X  X robes X ) are added to each pattern, each of which is an independently Gaussian distributed random variable, i.e., N (0,20). Two distinct metrics are used to evaluate the effectiven ess of the feature sel ection algorithms. One is the classification accura cy, which is estimated by the k NN classifier (in some cases also by the Lagrangian Support Vector Machine (LSVM, [12]), an efficient implementation of SVMs). The other metric is the Receiver Operating Characteristic (ROC) curve [15], which ca n indicate the effectiveness of different feature selection algorithms in identifying relevant features and at the same time ruling out useless ones. To eliminate statistical deviations, all the experiments are averaged over 20 random runs. The hyper-parameters, i.e., the number of nearest neighbors k in k NN and the regularization parameter C in LSVM are determined by five-fold cross validation on the training data set.
 We first apply Relief and P-Relief to the eight binary classification data sets. For this comparison, both k NN and LSVM are tested. The hyper-parameter in 1 . 2  X || x n  X  M n || , so as to keep the running time of both methods comparable. The top two lines of Fig.1 shows the average testing error of each selector-classifier combination, as a function of the number of top-ranked features. The ROC curves are plotted in the bottom two lin es of Fig.1. As a reference, the best average classification error and standard deviation of each algorithm are also plotted as a bar chart in Fig.3. We can see that, although P-Relief shares the same computational complexity as Relief, it usually achieves better performance than Relief. In particular, P-Relief outperforms Relief significantly in five (out of eight) data sets and performs comparably on the other three when the testing error metric is concerned. In the meanwh ile, for all the eight data sets, P-Relief has a larger area under ROC curve than Relief does.

We now compare Relief, ReliefF and the proposed M-Relief in handling im-balanced/multiclass data. For this purpose, four binary data sets, which are relatively more imbalanced, and four multiclass data sets are used. To facilitate the comparison, a further bias-sampling procedure is applied to the four binary classification data sets to make them more imbalanced, i.e., 80% of the patterns randomly sampled from the minority class are discarded. Since Relief is orig-inally established for binary classification, to enable it to apply to multiclass tasks, we use the margin definition used in I-Relief. To make a fair comparison, one nearest hit and C -1 nearest misses (one for each class) are used in Reli-efF. This configuration ensures that th e differences are mainly resulted by the strategies used to handling imbalanceness.

ReliefF is relatively more time-consuming than the other two algorithms since it needs searching nearest miss for each class. However, the differences are not very significant because the number of classes are not very large. For conve-nience, only the k NN classifier is used to estimate the classification error. The top two lines of Fig.2 show the testing error of each approach, as a function of the number of top-ranked features. The ROC curves are plotted in the bottom two lines. And the bar plot indicating the best average testing errors and standard deviations are also shown in Fig.3. Note that for binary classification, ReliefF and Relief are identical to each other. From these results, we arrive at the fol-lowing observations: ( i ) the performance of Relief is degraded significantly when the data is highly imbalanced; ( ii ) M-Relief, with a simple trick that does not introduce much extra computation, improves the performance significantly. It performs the best in six (out of eight) data sets with respect to the classification error metric, and in seven data sets with respect to the ROC curve metric. A natural optimal criterion for feature selection would be the Bayes error mini-mization in the reduced space, because t he generalization error of any classifier is lower bounded by Bayes error, hence, the Bayes error only depends on fea-tures rather than classifiers. However, this criterion is difficult in practice where only training data is given. This paper has presented an algorithmic framework for feature selection based on nonparametric Bayes error minimization. When feature weighting are used as the search strategy, this framework reveals that the Relief algorithm greedily attempts to minimize Bayes error estimated by k NN estimator. As an alternative, we have presented a new algorithm named Parzen-Relief. In addition, to enhance its ability to deal with imbalanced and/or multiclass data, we have proposed a MAP-Relief algorithm.

