 University of Sydney University of Sydney have achieved high performance in so many Natural Language Processing ( we outperform the parser X  X  F-score by 9.04%.
 tions can now make use of NP structure. 1. Introduction methods for parsing NP s have not achieved high performance until now. tion carried within NP s. Question Answering ( QA ) systems need to supply an example, consider the following extract: and the question: rizing the phrase structure trees. The additional NP annotation provides these systems with more detailed structure, increasing performance. However, this that using supervised techniques trained on gold-standard to these unsupervised methods.

However, it does not annotate internal NP structure. The NP prices , is left flat in the Penn Treebank. Even worse, NP semantic structure inside base-NP s.
 754 quantifying how much structure we have added, and how it is distributed across to represent noun phrase structure more accurately.
 ties in parsing NP s. This shows that the primary difficulty in bracketing a lack of lexical information in the training data.
 post-processor achieves an F-score of 79.05% on the internal the parser output baseline of 70.95%.
 time. Whereas before it was difficult to even evaluate what now take advantage of the crucial information present in NP 2. Background semantically indicative X  X cts as the head of the NP structure. , to which we can add modifiers and determiners to form a saturated of X-bar theory, the head is an N-bar, as opposed to the fully formed do, making NP s such as *the the dog ungrammatical.
 bracket the saturated NP s being apposed.
 much more difficult because of this reduction in specificity, although the interpreted with the appropriate context. 2.1 Noun Phrases in the Penn Treebank The Penn Treebank (Marcus, Santorini, and Marcinkiewicz 1993) annotates ently from any other constituent type. This special treatment of the annotation guidelines (Bies et al. 1995, page 120): phrases, instead leaving them flat. The Penn Treebank representation of two recover NP structure.
 the Treebank phrase structure to Combinatory Categorial Grammar ( 756 represent the flat structure of the Penn Treebank base-NP incorrect in many cases. Looking at the following example standard is (a), whereas the correct bracketing would be (b). It did not include the annotation of NP structure, however. add it.
 such as: annotators: are the decisions made by our annotators for these two NP page 1343) have this to say: Accordingly, an annotator with sufficient expertise at bracketing unfamiliar with. This hypothesis will be tested in Section 4.1. 2.2 Penn Treebank Parsing than 40 words on Section 23 of the Penn Treebank.
 Charniak 2000), in the creation of derived corpora (e.g., and for numerous other purposes.
 for sentences with fewer than 40 words, but is simpler and much faster. ( on sentences with fewer than 40 words.
 758 decreasing accuracy.

NP structure (e.g., the PARC parser [Riezler et al. 2002]). our work.
 Each grammar production is framed as follows: using the chain rule yields the following expressions: and m + 1 th modifiers), which is generated when there are no more modifiers. included while still calculating useful probability estimates. section. 2.2.2 Generating NPs in Collins X  X  Models. Collins X  X  models generate where we make alterations to the model and analyze its performance. For base-are changed as shown: the previous modifier, and will be experimented with in Section 5.4. namely, that it is rare for words to precede a determiner in an of the NP , dog , results in incorrectly generating Yesterday as part of the an is taken wherein NP brackets that do not dominate any other non-possessive are relabeled as NPB . For consistency, an extra NP bracket is inserted around example of this transformation can be seen here: 2.3 NP Bracketing Recursive NP bracketing X  X s in the CoNLL 1999 shared task and as performed by Daum  X  e III and Marcu (2004) X  X s closer, but still less difficult than full
Neither of these tasks require the recovery of full sub-NP decide whether it is left branching (a) or right branching (b): (a) ((crude oil) prices) 760 (b) (world (oil prices))
Most approaches to the problem use unsupervised methods, based on competing Figure 2.
 accuracy using POS tags to identify bigrams in the training set. by a hyphen. This results in an impressive 89.3% accuracy. decision tree classifier, using 362 manually annotated NP ( duplication in Lauer X  X  data set, however.
 This algorithm is used as part of an annotation tool, where three-word word NP s only (because there is no gold-standard for the complete and 65% on two different data sets.
 annotate NP s, as well as how a widely used parser generates parser and an NP bracketing system.
 3. Annotation Process section will describe the process of manually annotating such a corpus of The data will then be used in the parsing experiments of Section 5 and the is one of the major contributions of this article.
 do not meet our requirements. DepBank (King et al. 2003) fully annotates structure, but is again comparatively small and not widely used in the parsing being annotated remains similar.
 new NP annotations.
 sentences in the corpus were manually examined during the annotation process. 3.1 Annotation Guidelines of examples.
 ets are inserted around left-branching structures. implicit structure: 762 binarized. Binarization can have a harmful effect on parsers using in Section 5.5. Not inserting it, however, makes the annotator X  X  task simpler. or an adjective. Examples using the JJP label are shown here: additional label. JJP s can easily be reverted back to ADJP Section 5.2.
 formerly Mr. McNeil .
 on how coordinations are bracketed in Appendix A.2.1 how premodifiers attach to each other and to their head. biologist to annotate: modifier scope in NP sisresolvable.
 interpreted. X  3.2 Annotation Tool annotator for disambiguation. An ambiguous NP is any (possibly non-base) three or more contiguous children that are either single words or another they are filtered out by the tool. The complete list of patterns is: , PRP $ ** ,and * * POS . The latter pattern also inserts a two tokens.
 surrounding the ambiguous NP . During the annotation process, most bracketed without specifically reading this information, because the the no amount of surrounding context was informative. For these leaving difficult cases flat was applied. We did not mark flat many there are. suggest a bracketing, using rules based mostly on named entity tags. These are drawn from the BBN Pronoun Coreference and Entity Type Corpus (Weischedel 764 tags. Some of the NER tags have subcategories, for example, tags for the annotation tool suggestions.
 PER DESC PER DESC PER DESC (person descriptor).
 a bracket around the possessor John Smith is suggested.
 annotation. 3.3 Annotation Post-Processes carried out since the annotation was first completed. Firstly, 915 bracketed consistently.
 Sections A.2.1, A.2.2, and A.2.3 in the annotation guidelines appendix. while still allowing for a number of simple errors to be noted and corrected. remaining cases, such as the NP below: Treebank sometimes already has the structure annotated under an learning whether it is appropriate to use an NML or NP label. 3.4 Annotation Time 2,500 ambiguous NP s (i.e., annotating took approximately 5 seconds per accustomed to, hence the task can be performed quite quickly. well. 4. Corpus Analysis brackets inserted by the annotator. This is as we expect, as the majority of branching. Of the brackets added, 26,372 were NML nodes, and 894 were s that the bracketing tool presents. We find there are 32,772 as there was in the original Penn Treebank.
 the entire corpus. An example is given showing typical words that match the For NML and JJP , the example shows the complete NP node, rather than just the
Treebank, and should have an ADJP bracket already. 766 4.1 Inter-Annotator Agreement the primary annotator made the second pass mentioned in Section 3.3. problematic, as the definition of what constitutes an NML from the NP s remain the same.
 uninformative.
 and recall are the same.
 were discussed and the annotations revised.
 is because it is a harsher evaluation, as there are many NP example, consider an NP that both annotators agree is right-branching: or increasing the inter-annotator agreement measure accordingly. in 2,667 of 2,908 cases (91.71%), or 613 of 721 (85.02%) NP is a harsher evaluation as partial agreement is not taken into account. that even difficult cases could be resolved by a relatively short discussion. revision occurred for a small number of repeated instances, such as: The second annotator felt that Goldman , Sachs should form its own whereas the first annotator did not.
 768 the two annotators in our process reached. 4.2 DepBank Agreement parser.
 two evaluation scores, the dependencies themselves and how many form a dependency between mostly and real .
 Whereas we have always separated the company name from post-modifiers such as found by Collins X  X  rules will be incorrect. For example, in the direction is different and so no match is found.
 annotator agreement is at an excellent level. 4.3 Evaluating the Annotation Tool X  X  Suggestions in an F-score of 0.0%.
 already been completed. Also note that these experiments use gold-standard although whenever they can be applied, they are almost always correct. 770 word.
 have set a significant challenge for finding further improvement. 5. Statistical Parsing 5.1 Initial Experiments on the original Penn Treebank, and the next three are all using the extended process are excised. The next figures, for NML NML NML and  X   X   X   X  s.
 easier task. However the parser X  X  performance on NML and JJP on the parser model itself. 5.2 Relabeling NML and JJP Bikel X  X  parser does not come inbuilt with an expectation of constituents is undefined. Further, changing the structure of we ran an experiment where the new NML and JJP labels were relabeled as and negates the need for any change to the parser itself. gold standard, which falsely suggests a precision of 100%. The incorrect 772 the following gold-standard NP : (NP (ADVP (RB formerly) ) (DT a) (NML (NNP W.R.) (NNP Grace) ) (NN vice) (NN chairman) ) The parser output had no bracket around W.R. Grace .
 with the NNP POS tag should be bracketed. Even though NML we must change the parser itself to deal with the new labels properly. 5.3 Head-Finding Rules The first and simplest change we made was to create head-finding rules for NML s, where the head is usually the right-most child. To define the copy those for NP sand ADJP s, respectively. We also add to the rules for NML and JJP nodes can be recursively examined, in the same way that
This change is not needed for other labels, as NML sand JJP shown are against the original results from Table 6.
 only a small amount overall, which chiefly comes from the rule conditions the modifier lung on the head cancer .This informative, as the set of heads is likewise restricted. An also quite effective.
 achieves similar performance. 5.4 The Base-NP Submodel The next alteration to the parser is to turn off the base-improve performance in doing so.

All three techniques involved editing the parser code: 1. Changing the isBaseNP() method to always return false. This means that 2. Removing the preprocessing step that creates NPB nodes. This alteration it functions.
 is most notable for the large drop in performance on the internal base-NP submodel. Collins (1999,  X 8.2.2) explains why the distinction between the Treebank, are given too high a probability. The parser needs to know where 774 flat structure. Furthermore, the third change effectively treats 5.5 Bracket Structure We have now seen how a Collins-style parser performs on internal He advocates one level of bracketing structure per X-bar level. are biased towards an increased number of non-terminal nodes. right-branching structure. For example, in the NP the New York Stock Exchange when the parser comes across New York Stock Exchange composite trading assumed implicitly to be there.
 the more than 10% increase in F-score when evaluating internal right-branching structure should be left implicit. 5.6 Test Set Results same as the suggestion baseline in this case.
 overall performance. 5.7 Error Analysis 776 column of the table. We can further divide these errors into general currently not available.
 distinguishing between using NP and NML labels, as well as the bracket as JJP .
 does not generate punctuation as it does other constituents. or because they dominate only a single token. The only single token in the example where figures is actually a noun.
 so would be a challenging task.
 778 6. Noun Phrase Bracketing improve upon the parser X  X  performance of only 67.44%.
 Simple NP s:. are exactly three words long and contain only nouns. 6.1 Data supervised models, to train on as well. We extract both a simple and a complex set from our extended Penn Treebank. and as right-branching otherwise. This method (assuming that all part of the NP is being looked at. This also allows us to retrieve simple sequences from Grolier X  X  encyclopedia.
 assistant state attorney , as these tokens are not right-most in the Exchange as these tokens are not (even implicitly) dominated by single node.
Lauer X  X  set (244 NP s) or created their own small set (  X  rather than using unsupervised methods.
 all other data sets. The distribution of left-and right-branching in child node to represent the entire constituent. This means we can treat such in square brackets, leaving only the heads of those constituents. any that has been created previously.
 780 ternatives are close to equally likely. The entropy figure for the bracket, as there is no good baseline decision.

NP bracketing is far from a trivial task. 6.2 Unsupervised Experiments the bigram probability, and  X  2 .
  X  w indicates any word except w  X  using search engine counts.
 theirs.
 782 engines give reasonably similar results on both data sets. superior. On the 1,556 NP s that remain, using Web 1T counts and the  X 
The other count sources and association measures show the same trend. possessive marker. The full list is shown in Table 17  X  Google ( (  X  and then combined. Rather than implementing such a complex algorithm, we per-identified is unlikely to be as effective for any other data set. training, development, and test data for the first time. 784 6.3 Supervised Models Supervised models typically outperform unsupervised models for most large-scale supervised NP bracketer.
 measures are equal, then neither binary feature is on.
 mance figures.
  X   X   X   X   X   X   X   X   X  even though both models are using the same information to base their decisions corpus.
 about 9%. now add lexical features for all unigrams, bigrams, and the trigram within the of these features are labeled with the position of the n -gram within the before the NP ,afterthe NP , and either before or after the the BBN Pronoun Coreference and Entity Type Corpus (Weischedel and Brunstein dow features, replacing the words with their POS and NER tags. even though all the words in the NP are nouns for these simple they may be proper and/or plural. We use the coarse-grained
B  X 
B  X 
B  X  and I  X  I  X  I  X  .
 but there is also an unordered bag-of-hypernyms for all senses. with the Web 1T searches. Of the supervised features, the context window and F-score is obtained by removing this group.
 improved over the unsupervised baseline by 6.6%, demonstrating that the voting consistent. 786 6.4 Complex NP s other non-nominal parts of speech also complicate the task. Barker X  X  algorithm is shown in Figure 5.
 case it grows to the left.
 for training and 5,357 for development and testing. score 0.0, as no explicit brackets are needed.
 that are entirely correct, and the model X  X  performance on the three word precision and recall are always equal. Finally, note that the three-word directly comparable. counts from the Web 1T corpus, because performing Web searches has become im-To do this, we run Barker X  X  algorithm on the 42,854 complex entropy parsing model. The complex NP data produces 95,964 training examples. features. Firstly, we add features encoding the non-head words when the window position in the window and added as features. For example, consider the with the context. Thirdly, we measured the entropy of every for left-and right-branching, rather than features for each been informative in PCFG parsing. For the parent and grandparent of the feature for the phrase label, the head-word and its POS tag, 788  X   X   X   X   X   X   X   X   X   X   X   X   X  rule, and parser features all have a negative impact on performance. 500 iterations in MegaM, to allow the estimation to converge. 6.5 Parser Post-Processor more difficult by the fact that the post-processor is dependent on parser, which are incorrect in approximately 10% of cases. examples. Doing so provides a more realistic view of a PP same applies to our NP Bracketing system.
 Bracketing for evaluation. We reject brackets that cross an error). This results in a development set of 3,946 complex also produced in the same way from Section 23.
 and our best model utilizes only the Web 1T, lexical, POS bracket F-score.
  X   X   X   X   X   X   X   X   X   X   X   X   X  790 that the evaluation might not be entirely accurate. Finally, the 6.5.1 Parsing Evaluation. Finally, we can now put the rebracketed improved on the suggestion baseline established earlier. (p preformance than has ever been possible in the past. 7. Future Work This work is the first to create and make use of a large-scale corpus of Our experiments with this new data have set a high benchmark for experiments on a data set that is limited in scale and coverage. Our for future work on the subject of NP parsing. 7.1 NP Annotation widely used corpus can be used to train parsers to recover of only one other corpus that has been annotated with a large volume of first NP annotation schemes, it seems probable that they can be improved. terms.
 all word pairs). Lauer (1995) found that 35 out of the 279 non-error government decisions , resulting in all three possible dependencies. described in the following paragraphs), whereas we would separate the two. 792 where both readings are true.
 have affected our experiments and/or their evaluations. For example, an most words when there is not. Similarly, an indeterminate we can note that the annotator only marked 915 of the 60,959 inspected rather than the linguistic complications described here. 7.2 Parsing NP Structure Our experiments in Section 5 highlighted the difficulty of parsing to incorporate the information sources that were successfully applied to into a Collins-style parser. In particular, the possibility exists to include a specialized NP submodel may make this process easier and/or more effective. than the Barker (1998) algorithm. This is only one way to bracket complex
Bergsma and Wang (2007) function in a similar manner. 8. Conclusion data can all benefit from the extended resource we have created. the difficulty that statistical methods have in bracketing annotation tool X  X  suggestion feature outperformed the parser by 5.81%. ing points. In particular, the continued importance of the base-information in the training data.
 that can analyze NP s of arbitrary length and complexity. The initial simple with even higher performance, however.
 complex NP Bracketing is an interesting task with much room for innovation. line and improved on the parser X  X  result by 9.04% F-score. Our performs better than a state-of-the-art parsing model.
 using the extended corpus we created, is now able to identify sub-resolution, and many other downstream NLP tasks.
 Appendix A: Annotation Guidelines pass over the data.
 A.1. Bracketing NPs and this is precisely the behavior that we have annotated. Indeed,
However, we can still resolve this ambiguity, as (with our emphasis) 794 but the correct bracketing of most NP s is simple to ascertain. our extension, we assume a right-branching structure in all of bracketing required and thus increases legibility. This means that not need further bracketing: (NP (DT The) (JJ average) (JJ seven-day) (NN compound) (NN yield) )
And the implicit structure represented is: (NP (DT The) (NODE (JJ average)
When a left-branching modifier is present, as in this NP , (NP (NN lung) (NN cancer) (NNS deaths) ) around those words: (NP (NML (NN lung) (NN cancer) ) (NNS deaths) ) Though less frequent, brackets can also be necessary in non-base-examples: (NP-SBJ (NML (JJ former) (NNP Francis) (NNP J.) (NNP McNeil) ) (NP (NML (NNP Inc.) ) not formerly Mr. McNeil.
 implicitly right-branching. (NP (NML (JJ chief) (JJ financial) (NN officer) ) (NNP John) (NNP Pope) ) (NP (NML (JJ hot-dipped) (JJ galvanized) (NN sheet) ) (NNS products) ) galvanized (if no NML node was used).
 (NP (NML (JJ composite) (NN trading) ) normal case. (NP (NP (DT the) (NNP Carper) (POS  X  X ) ) (NNP Creek) (NN wine) ) overall NP ,notjust Carper X  X  Creek .
 A.1.1 Node Labels and ADJP labels, so that we can analyze them separately. This approach has the advantage that they can be mapped back to the existing labels if needed. when the modifier X  X  head is a noun, as in previous examples, whereas the head is adjectival, as in this example: (NP (JJP (JJ dark) (JJ red) ) (NN car) ) 796
The label should also be JJP in cases where the head is a gerund. (NP (DT the) (JJP (JJS fastest) (VBG developing) ) (NNS trends) ) A
JJP node is needed when an adverb modifies an adjective: (NP (JJP (RB relatively) (JJR higher) ) (NNS rates) ) to be JJP . We do not have a label similar to UCP . (NP (PRP$ its) (JJP (JJ current) (NNS ratepayers) ) (NP (DT the) (JJP (JJ British) (NNS troops) ) cases with unusual heads, like the following one where DT labeled NML . (NP (NML (DT any) (NNS warrants) ) correct POS tag, rather than propagate the error.
 A.1.2 Ambiguous Cases alternatives seem equally likely, then they should leave the genuinely flat structure. (NP (NN college) (NN basketball) (NNS players) ) (NP (NN army) (NN ordnance) (NN depot) ) (NP (NNP John) (NNP A.) (NNP Smith) ) A.1.3 Head Derivation Head-finding rules for NML and JJP constituents are the same as for node.
 rules.
 A.1.4 Identifying the Correct Bracketing can be done: 2. Removal  X  This test involves trying to force one word to modify another 3. Postmodifier  X  If we move a premodifier to the end of the 798 A.2. Specific Cases A.2.1 Coordinations Coordinations are one of the most difficult structures to bracket in and . It does not need further bracketing. (NP (NNP Bill) (CC and) (NNP Ted) )
On the other hand, the following example does need the NML (NP (DT the) (NML (NNPS Securities) (NNP Commission) )
Otherwise, its implicit structure would be as follows: (NP (DT the) (NODE the correct the Securities Commission and the Exchange Commission . are needed there. (NP (NML (NN rock) (NNS stars) ) (CC and) (NML (NN royalty) ) ) (NP (JJ rude) (NNS words) (CC and) (NNS actions) ) these guidelines.
 (NP (NN royalty) (CC and) (NN rock) (NNS stars) ) insert brackets around rock stars and royalty as before. (NP (NML (NN royalty) ) (CC and) (NML (NN rock) (NNS stars) ) ) to this example.
 (NP (NNS cars) (, ,) (NNS trucks) (CC and) (NNS buses) )
This is true even when the conjunction is missing: (NP (NP (DT no) (NN crack) (NNS dealers) ) (, ,) (NP (, ,) (NP outside the list, as shown: (NP (NP (NNP Mazda) (POS  X  X ) ) (NNP U.S.) (NML (NNS sales) (NNS operations) )
A list of attributes separated by commas does not need any bracketing: (NP (JJ tricky) (, ,) (JJ unproven) (NN chip) (NN technology) ) acting as modifiers on technology , like in the NP : big red car . 800 (NP-SBJ (DT Neither) (NP (NNP Lorillard) ) (CC nor) (NP A.2.2 Speech Marks
Tokens surrounded by speech marks should be bracketed: (NP-PRD (DT a) (NML ( X  X   X  X ) (JJ long) (NN term) ( X  X   X  X ) ) (NN decision) ) speech marks are right-most: (NP-PRD (DT a) (JJP ( X  X   X  X ) (JJ long) ( X  X   X  X ) ) (NN decision) ) (NP-PRD (DT a) (NML ( X  X   X  X ) (JJ long) (NN term) ( X  X   X  X ) ) ) in the previous block, where JJP is used.
 then a new bracket should not be added. (NP-PRD ( X  X   X  X ) (JJ long) (NN term) ( X  X   X  X ) ) s with speech marks. Firstly, in these examples: (NP ( X  X   X  X ) (NP-TTL (DT A) (NNP Place) (IN in) (NNP Time) ) (, ,) ( X  X   X  X ) (NP still add a NML bracket around the speech marks: (NP (NML ( X  X   X  X ) (NP stranded in another constituent. For example, the following speech mark: (NP (DT the) ( X  X   X  X ) (NML (NN type) (NN F) ) (NN safety) (NN shape) ) (NP (NP (DT the) ( X  X   X  X ) (, ,) ( X  X   X  X ) (NP existing structure. So once again, we do not add any new brackets in
In the next example, the speech marks have not been put in the right place: (NP-PRD ( X  X   X  X ) (DT a) (JJ worst-case) ( X  X   X  X ) (NN scenario) ) 802 inserted, no brackets should be added at all.
 A.2.3 Brackets previous section. (NP (DT an) (JJP (-LRB--LCB-) (VBG offending) (-RRB--RCB-) ) (NN country) )
An example of another corner case is shown here: (NP (-LRB--LCB-) (NML (NNP Fed) (NNP Chairman) ) (NNP Alan) (-RRB--RCB-) (NNP Greenspan) ) bracket Fed Chairman , but beyond that, no other brackets should be added. A.2.4 Companies
Company names may need to be bracketed a number of ways. When there are post-longer than one word. (NP-SBJ (NML (NNP Pacific) (NNP First) (NNP Financial) ) (NNP Corp.) ) (NP (NML (NNP W.R.) (NNP Grace) ) (CC &amp;) (NNP Co.) ) (NP (NML (NNP Goldman) (CC &amp;) (NNP Co.) ) need to be bracketed separately. (NP (NP (NN today) (POS  X  X ) ) (NML (NNP New) (NNP England) ) (NNP Journal) ) (NP (DT the) (NML (NNP Trade) (NNP Ministry) ) A.2.5 Final Adverbs
The tokens preceding a final adverb should be separated: (NP (NML (NN college) (NNS radicals) ) (RB everywhere) ) A.2.6 Names
Names are to be left unbracketed: (NP (NNP Brooke) (NNP T.) (NNP Mossman) )
However, numbers, as well as Jr. , Sr. , and so forth, should be separated: (NP (NML (NNP William) (NNP H.) (NNP Hudnut) ) (NNP III) )
Titles that are longer than one word also need to be bracketed separately. (NP (NML (NNP Vice) (NNP President) ) (NNP John) (NNP Smith) ) A.2.7 Possessives s preceding possessives need to be bracketed. (NP (NML (NNP Grace) (NNP Energy) ) (POS  X  X ) ) A.2.8 Postmodifying Constituents The words preceding a postmodificational constituent, such as a preposition or not need to be bracketed. (NP (DT the) (JJ common) (NN kind) (PP (IN of) 804 A.2.9 Unit Traces the NP . (NP (RB over) ($ $) (CD 27) (-NONE-*U*) ) be inside the bracket. (NP (DT a) (NML ($ $) (CD 27) (-NONE-*U*) ) (NN charge) ) A.2.10 Unusual Punctuation these cases, a bracket should be added to join them back together, as shown: (NP (NNP Finmeccanica) (NML (NNP S.p) (. .) ) (NNP A.) ) (NP (NML (: :) ) A.3. Future Improvements the corpus, and all of them require another full pass.
 A.3.1 Flat Structures corpus. To do this, we intend to use a marker on the NP , shown: (NP-FLAT (NNP John) (NNP A.) (NNP Smith) ) (NP (NML-FLAT (NNP John) (NNP A.) (NNP Smith) ) (NNS apples) ) A.3.2 Appositions shown here, with a person X  X  name and their position separated by a comma: (NP-SBJ (NP (NNP Rudolph) (NNP Agnew) ) (, ,) (NP co-referential), and whether to discriminate between different types. A.3.3 Head Marking example, IBM is the head, but Australia would be found. (NP (NNP IBM) (NNP Australia) ) look at NP s of length three or more.
 References 806 808
