 A continuous-time Markov chain (CTMC) is a model of a dynamical system which, upon entering some state, remains in that state for a random real-valued amount of time (called the dwell time or occupancy time) and then transitions randomly to a new state. CTMCs are used in a wide variety of domains. In stochastic chemical kinetics, states may correspond to the conformation of a molecule such as a protein, peptide or nucleic acid polymer, and transitions correspond to conformational changes (e.g., [1]). Or, the state may correspond to the numbers of different types of molecules in an interacting system, and transitions are the result of chemical reactions between molecules [2]. In phylogenetics, the states may correspond to the genomes of different organisms, and transitions to the evolutionary events (mutations) that separate those organisms [3]. Other application domains include queueing theory, process control and manufacturing, quality control, formal verification, and robot nagivation.
 Many computational problems associated with CTMCs have been solved, often by generalizing methods developed for discrete-time Markov chains (DTMCs). For example, stationary distribu-tions for CTMCs can be computed in a manner very similar to that for DTMCs [4]. Estimating the parameters of a CTMC from fully observed data involves estimating state transition probabilities, just as for DTMCs, but adds estimation of the state dwell time distributions. Estimating parameters from partially observed data can be done by a generalization of the well-known Baum-Welch algo-rithm for parameter estimation for hidden Markov models [5] or by Bayesian methods [6, 7]. When the state of a CTMC is observed periodically through time, but some transitions between observa-tion times may go unseen, the parameter estimation problem can also be solved through embedding techniques [8]. In scenarios such as manufacturing or robot navigation, one may assume that the state transitions or dwell times are under at least partial control. When control choices are made once for each state entered, dynamic programming and related methods can be used to develop opti-mal control strategies [9]. When control choices are made continuously in time, methods for hybrid system control are more appropriate [10].
 Another fundamental and well-studied problem for CTMCs is to compute, given an initial state and time, the state distribution or most likely state at a later time. These problems are readily solved for DTMCs by dynamic programming [11], but for the CTMCs, solutions have a somewhat different flavor. One approach is based on the forward Chapman-Kolmogorov equations [4], called the Mas-ter equation in the stochastic chemical kinetics literature [12]. These specify a system of ordinary differential equations the describe how the probabilities of being in each state change over time. Solving the equations, sometimes analytically but more often numerically, yields the entire state distribution as a function of time. Alternatively, one can uniformize the CTMC, which produces a DTMC along with a probability distribution for a number of transitions to perform. The process obtained by choosing the number of transitions, and then producing a trajectory with that many tran-sitions from the DTMC, has the same state distribution as the original CTMC. This representation allows particularly efficient computation of the state distribution if that distribution is only required at one or a smaller number of different times. Finally, especially in the chemical kinetics commu-nity, stochastic simulation algorithms are popular [13]. These approaches act by simply simulating trajectories from the CTMC to produce empirical, numerical estimates of state distributions or other features of the dynamics.
 Despite the extensive work on a variety of problems related to to CTMCs, to the best of our knowl-edge, the problem of finding most likely trajectories has not been addressed. With this paper, we attempt to fill that gap. We propose dynamic programming solutions to three variants of the problem: (i) an initial value problem, where a starting state and final time are given, and we seek the most likely sequence of states and dwell times occurring up until the final time, (ii) a boundary value problem, where initial and final states and times are given, and we seek the most likely intervening trajectory, and (iii) a problem involving partial observability, where we have a sequence of  X  X bser-vations X  that may not give full state information, and we want to infer the most likely trajectory that the system followed in producing the observations. s  X  S . Let S t  X  S denote the state of the system at time t  X  [0 , +  X  ) . The rules for the evolution time t , when the system is in state S t = s , the system stays in state s for a random amount of time that is exponentially distributed with parameter  X  s . When the system finally leaves state s , the next state of the system is s 0 6 = s with probability P ss 0 .
 U system up until time t . In particular, this means that there are k t state transitions up until time t (where k t is itself a random variable), the system enters state s k t sometime at or before time t , and remains in state s k t until sometime after time t .
 Given the initial state, S 0 , and a time t , the likelihood of a particular trajectory U is l ( U t = U | S 0 ) = When P i t i &gt; t , the likelihood is zero, because it means that the specified transitions have not completed by time t . Otherwise, the terms inside the first parentheses account for the likelihood of the dwell times and the state transitions in the sequence, and the term inside the second parentheses accounts for the probability that the dwell time in the final state does not complete before time t . With this notation, the initial value problem we study is easily stated as where s  X  S and t &gt; 0 are both given. The boundary value problem we study is Here, the given s and s 0 are any states in S , possibly the same state, and t &gt; 0 is also given. A hidden continuous-time Markov chain (HCTMC) adds an observation model to the CTMC. and it is in state s  X  S , the observer sees observation o  X  O with probability P so . Let O = ( o 1 , X  1 ,o 2 , X  2 ,...,o m , X  m ) denote a sequence of observations and the times at which they are made. We assume that the observation times are fixed, being chosen ahead of time, and depend in no way on let U ( t ) denote the state of the system at time t implied by that sequence. Then, the probability of an observation sequence O given the trajectory U can be written as The final problem we study in this paper is that of finding the most likely trajectory given an obser-vation sequence: In this section we develop solutions to problems (2) and (3). The first step in this development is to show that we can analytically optimize the dwell times if we are given the state sequence. This is covered in the next subsection. Following that, we develop a dynamic program to find optimal state sequences, assuming that the dwell times are set to their optimal values relative to the state sequence. 3.1 Maximum likelihood dwell times that S 0 = s 0 , as we have no need to consider U starting from the wrong state, and let us maximize state s k . Then we can write the desired optimization as It is more convenient to maximize the logarithm, which gives us Dropping the terms that do not depend on any of the t i and rearranging, we find the equivalent problem The solution can be obtained by inspection. If  X  s k  X   X  s i for all 0  X  i &lt; k , then we must have all t largest expected dwell time (corresponding to the smallest  X  parameter), then the most likely setting of dwell times is obtained by assuming all of the time t is spent in state s j , and all other transitions happen instantaneously. This is not unintuitive, although it is dissatisfying in the sense that the most likely set of dwell times are not typical in some sense. For example, none are near their expected value. Moreoever, the basic character of the solution X  X hat all the time t goes into waiting at the slowest state X  X s independent of t . Nevertheless, being able to solve explicitly for the most likely dwell times for a given state sequence makes it much easier to find the most likely U t . So, let us press onwards. 3.2 Dynamic programming for the most likely state sequence Substituting back our solution for the t i into Equation (1), and continuing our assumption that s 0 = S , we obtain This leads to a dynamic program for finding the state sequence that maximizes the likelihood. As is typical, we build maximum likelihood paths of increasing length by finding the best ways of extend-ing shorter paths. The main difference with a more typical scenario is that to score an extension we need to know not just the score and final state of the shorter path, but also the smallest dwell time transitions, ends at state s k = s , and for which the smallest dwell time parameter of any state along the trajectory is  X  . Then define F k ( s, X  ) to be the maximum achievable l ( U t = U | S 0 ) , where we restrict attention to U that are ( k,s, X  ) -trajectories. We initialize the dynamic program as: that the minimum dwell time parameter along the trajectory can be no greater than  X  s . So, we only compute F k ( s, X  ) for  X   X   X  s .
 trajectory must come from some ( k,s 0 , X  ) -trajectory. That is, the length k trajectory must already have a dwell time parameter of  X  along it. The state s 0 can be any state other than s . If  X  =  X  s , then the best ( k + 1 ,s, X  ) -trajectory may be an extension of any ( k,s 0 , X  0 ) -trajectory with  X  0  X   X  and s 6 = s 0 . To be more concise, define We then compute F for increasing k as: next two terms account for the dwell in s 0 and the transition probability to s . The final term accounts for any difference between the smallest dwell time parameters along the k and k + 1 transition trajectories. Figure 1: A continuous-time Markov chain used as a demonstration domain. The five circles corre-spond to states, and the arrows to transitions between states. States are also labeled with their dwell time parameters.
 s  X  S . The size of the table F k for each k is thus at most | S | 2 . If we limit k to some maximum value To solve the initial value problem (2), we scan over all values of k , s and  X  to find the maximum value of F k ( s, X  ) . Such a value implies that the most likely state sequence ends at state s after k state transitions. We can use a traceback to reconstitute the full sequence of states, and the result of the previous section to obtain the most likely dwell times. To solve the boundary value problem (3), we do the same, except that we only scan over values of k and  X  , looking for the maximum value of F ( S t , X  ) . 3.3 Examples In this section, we use the toy chain depicted in Figure 1 to demonstrate the algorithm of the previous section, and to highlight some properties of maximum likelihood trajectories. First, suppose that we know the system is in state x at time zero and in state z at time t . There are two different paths, P xy P yz = 2 3  X  1 = 2 3 , whereas the direct path ( x,z ) simply has probability P xz = 1 3 . However, if we consider the dwell times as well, the story can change. For example, suppose that t = 1 . Note of it leaving y before time t = 1 is quite small. If we run the dynamic programming algorithm of the previous section to find the most likely trajectory, it finds ( s 0 = x,t 0 = 0 ,s 1 = z ) to be most likely, with a score of 0 . 1226 . Along the way, it computes the likelihood of the most likely path time t in state y , because that state is most likely to have a long dwell time. However, the total score of this trajectory is still only 0 . 0603 , making the direct path the more likely one. On the other hand, the path through y still has a likelihood of 0 . 0245 , whereas the direct path has a likelihood below 2  X  10  X  5 , because it is highly unlikely to remain in x and/or z for so long.
 Next, suppose that we know S 0 = a and that we are interested in knowing the most likely trajectory out until time t , regardless of the final state of that trajectory. For simplicity, suppose also that  X  a =  X  b . There is only one possible state sequence containing k transitions for each k = 0 , 1 , 2 ,... , and the likelihood of any such sequence turns out to be independent of the dwell times (assuming the dwell times total no more than time t ): If  X  &lt; 1 , this implies the optimal trajectory has the system remaining at state a . However, if  X  = 1 arbitrarily large likelihood, but no maximum likelihood trajectory. Intuitively, because the likelihood of a dwell time can be greater than one, the likelihood of a trajectory can be increased by including short dwells in states with high dwell parameters  X  .
 In general, if a continuous-time Markov chain has a cycle of states ( s 0 ,s 1 ,...,s k = s 0 ) , such Figure 2: Abstract example of a continuous-time trajectory of a chain, along with observations taken at fixed time intervals. trajectories with ever-increasing likelihood can be found starting from any state from which the cycle is reachable. One should, thus, always check the chain for this property before seeking maximum likelihood trajectories. This can be easily done in polynomial time. For example, one can label the edges of the transition graph with the weights log P ss 0  X  s for the edge from s to s 0 , and then check the graph for the existence of a positive-weight cycle X  X  well-known polynomial-time computation. We now turn to problem (12), where we are given an observation sequence O = ( o 1 , X  1 ,o 2 , X  2 ,...,o m , X  m ) and want to find the most likely trajectory U . For simplicity, we as-sume that  X  1 = 0 . The following can be straightforwardly generalized to allow the first observation to take place sometime after the trajectory begins. Similarly, we restrict attention to trajectories extrapolating the trajectory beyond the final observation time. The conditional likelihood of such a trajectory can be written as The term in the first parentheses is P ( O | U  X  m = U ) , and the term in the second parentheses is l ( U  X  m = U ) . The only differences between the second parentheses and Equation (1) is that we now include the probability of starting in state s 0 , and we have implicitly assumed that P i t k  X   X  m , as mentioned above. This form, however, is not convenient for optimizing U . To do this, we need to rewrite l ( U  X  m = U ) in a way that separates the likelihood into events happening in each interval of time between observations. 4.1 Decomposing trajectory likelihood by observation intervals the full paper. The likelihood of the trajectory can be written in terms of the events in each ob- X   X  respect to the start of the time interval. The component of the likelihood of the whole trajectory U likelihood of the whole trajectory can be written as 4.2 Dynamic programming for the optimal trajectory Combining Equations (12) and (14), we find The first two terms account for the probability of the initial state and the probability of the first observation given the initial state. The terms inside the product account for the likelihood of the i th interval of the trajectory, and the probability of the ( i + 1) st observation, given the state at the end of the i th interval of the trajectory.
 One immediate implication of this rewriting of the conditional likelihood is the following. At times  X  hood, it had better be that the fragment of the trajectory between those two times, U i , is a maximum tive, higher likelihood trajectory fragment could be swapped into U , resulting in a higher conditional likelihood. Let us define to be the maximum achievable likelihood by any trajectory from state s to state s 0 in time t . Then a necessary condition for U to maximize the conditional likelihood is Moreover, to find an optimal U , we can simply assume that the above condition holds, and con-(Of course, the endpoint of one interval must be the same as the initial point of the next interval.) [  X  , X  i ] , accounting for the first i observations, and ending at state s . The we can compute J as follows. To initialize, we set Then, for i = 1 , 2 ,...,m  X  1 , We can then reconstruct the most likely trajectory by finding s that maximizes J m ( s ) and tracing back to the beginning. This algorithm is identical to the Viterbi algorithm for finding most likely state sequences for hidden Markov models, with the exception that the state transition probabilities computed based on the results of the previous section. 4.3 Examples To demonstrate this algorithm, let us return to the CTMC depicted in Figure 1. We assume that  X  a =  X  b = 1 , that the system always starts in state x , and that when we observe the system, we get a real-valued Gaussian observation with standard deviation 1 and means 0 , 10 , 3 , 100 and 100 for states x , y , z , a and b respectively. 2 The left side of Figure 3 shows three sample sequences of 20 observations. The right side of the figure shows the most likely trajectories inferred under different assumptions. First, if we assume the time interval between observations is t = 1 , and we consider observations O A , then the most likely trajectory has the system in state x up through the 10 th observation, after which it instantly transitions to state z and remains there. This makes sense, as the lower observations at the start of the series are more likely in state x . If we consider instead observations O B , which has a high observation at time t = 11 , the procedure infers that the system was in state y at that time. Moreover, it predicts that the system switches into y immediately after the 10 th observation, and says there until just before the 12 th observation, taking advantage of the fact that longer dwell times are more likely in state y than in the other states. If we consider observations O
C , which have a spike at t = 5 , the transit to state y is moved earlier, and state z is used to explain observations at t = 6 onward, even though the first few are relatively unlikely in that state. If we Figure 3: Left: three length-20 observation sequences, O A , O B , and O C . All three are the same at most points, but the 11 th observation of O B is 10 , and the 5 th observation of O C is 10 . Right: most likely trajectories inferred by our algorithm, assuming the underlying CTMC is the one given in Figure 1, with parameters given in the text. return to observations O A , but we assume that the time interval between observations is t = 2 , then the observations, the most likely trajectory has the system transitioning from x to y immediately after the 10 th observation and dwelling there until just before the 11 th observation, where the state becomes z . This is because, as explained previously, this is the more likely trajectory from x to z given t = 2 . If we assume the time interval between observations is t = 20 , then a wider range of observations during the trajectory are attributed to state y . Intuitively, this is because, although the observations are somewhat unlikely under state y , it is extremely unlikely for the system to dwell for so long in state z as to account for all of the observations from the 11 th onward. We have provided correct, efficient algorithms for inferring most likely trajectories of CTMCs given either initial or initial and final states of the chain, or given noisy/partial observations of the chain. Given the enormous practical import of the analogous problems for discrete-time chains, we are hopeful that our methods will prove useful additions to the toolkit of methods available for analyzing continuous-time chains. An alternative, existing approach to the problems we have addressed here is to discretize time, producing a DTMC which is then analyzed by standard methods [14]. A problem with this approach, however, is that if the time step is taken too large, the discretized chain can collapse a whole set of transition sequences of the CTMC into a single  X  X seudotransition X , obscuring the real behavior of the system in continuous time. If the time step is taken to be sufficiently small, then the DTMC should produce substantially the same solutions as our approach. However, the time complexity of the calculations increases as the time step shrinks, which can be a problem if we are interested in long time intervals and/or there are states with very short expected dwell times, necessitating very small time steps.
 continuous-time chain under similar informational assumptions. By this, we mean that the dwell times, rather than being optimized, are marginalized out, so that we are left with only the sequence of states and not the particular times they occurred. In many applications, this state sequence may be of greater interest than the dwell times X  X specially since, as we have shown, maximum likeli-hood dwell times are often infinitessimal and hence non-representative of typical system behavior. Morever, this version of the problem has the advantage of always being well-defined. Because state sequences have probabilities rather than likelihoods, a most probable state sequence will always exist.
 Funding for this work was provided in part by the National Sciences and Engineering Research Council of Canada and by the Ottawa Hospital Research Institute. [1] FG Ball and JA Rice. Stochastic models for ion channels: introduction and bibliography. [2] D.J. Wilkinson. Stochastic modelling for systems biology . Chapman &amp; Hall/CRC, 2006. [3] M. Holder and P.O. Lewis. Phylogeny estimation: traditional and Bayesian approaches. Nature [4] H.M. Taylor and S. Karlin. An introduction to stochastic modeling . Academic Press, 1998. [5] D.R. Fredkin and J.A. Rice. Maximum likelihood estimation and identification directly from [6] R. Rosales, J.A. Stark, W.J. Fitzgerald, and S.B. Hladky. Bayesian restoration of ion channel [7] M.A. Suchard, R.E. Weiss, and J.S. Sinsheimer. Bayesian selection of continuous-time Markov [8] DT Crommelin and E. Vanden-Eijnden. Fitting timeseries by continuous-time Markov chains: [9] M. L. Puterman. Markov Decision Processes: Discrete Stochastic Dynamic Programming . [10] S. Hedlund and A. Rantzer. Optimal control of hybrid systems. In Decision and Control, 1999. [11] D.P. Bertsekas. Dynamic programming and optimal control . Athena Scientific Belmont, Mass, [12] NG Van Kampen. Stochastic processes in physics and chemistry . North-Holland, 2007. [13] D. T. Gillespie. Exact stochastic simulation of coupled chemical reactions. Journal of Physical [14] A. Hordijk, D.L. Iglehart, and R. Schassberger. Discrete time methods for simulating continu-
