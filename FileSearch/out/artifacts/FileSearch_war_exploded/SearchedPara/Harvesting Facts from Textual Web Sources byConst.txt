 There have been major advances on automatically constructing large knowledge bases by extracting relational facts from Web and text sources. However, the world is dynamic: periodic events like sports competitions need to be interpreted with their respective timepoints, and facts such as coaching a sports team, holding political or busi-ness positions, and even marriages do not hold forever and should be augmented by their respective timespans. This paper addresses the problem of automatically harvesting temporal facts with such extended time-awareness. We employ pattern-based gathering tech-niques for fact candidates and construct a weighted pattern-candidate graph. Our key contribution is a system called PRAVDA based on a new kind of label propagation algorithm with a judiciously designed loss function, which iteratively processes the graph to label good temporal facts for a given set of target relations. Our experiments with online news and Wikipedia articles demonstrate the accuracy of this method.
 H.1.0 [ Information Systems ]: Models and Principles X  General Algorithms, Design, Experimentation Knowledge Harvesting, Label Propagation, Temporal Facts Who were the teammates of the legendary Argentinian player Diego Maradona? How often has the pop singer Madonna been mar-ried, and to whom? Questions like these often arise from the informa-tion needs of Internet users, but are not supported by keyword-based search engines. Instead, answering them requires an understanding of entities and relationships embedded in Web contents, or even bet-ter, a knowledge base that contains facts about people, organizations, locations, etc. In the last few years, such knowledge bases have in-deed been built and made publicly accessible, for example: DBpedia [1], YAGO [17], TextRunner [9], ReadTheWeb [6], the commercial knowledge portals freebase.com and trueknowledge.com , the com-putational knowledge engine wolframalpha.com , the entity search engine entitycube.research.microsoft.com , and others. Most of these have been automatically compiled, either by integrating structured sources (e.g., geospecies.org, geonames.org, world factbook, etc.), by harvesting semi-structured sources such as Wikipedia info boxes, lists, and categories, or by information extraction from Web pages. Some of them involve a substantial amount of manual curation for quality assurance. Moreover, there is a strong momentum towards semantically inter-linking many knowledge sources into the Linked Data cloud [10], see linkeddata.org .
 However, the world is highly dynamic and nothing lasts forever! Knowledge evolves over time, and many facts are fairly ephemeral, e.g., winners of sports competitions, and occasionally even CEOs and spouses. In addition, many information needs by advanced users require temporal knowledge . For example, consider the following variations of the above example questions: Who were the team mates of Diego Maradona during the 1990 FIFA World Cup? When did Madonna get married, when did she get divorced? None of these questions are supported by existing knowledge bases. The problem that we tackle in this paper is to automatically distil, from news articles and biography-style texts such as Wikipedia, temporal facts for a given set of relations. By this we mean instances of the relations with additional time annotations that denote the validity point or span of a relational fact. For example, for the winsAward relation between people and awards, we want to augment facts with the time points of the respective events; and for the worksForClub relation between athletes and sports clubs, we would add the timespan during which the fact holds. This can be seen as a specific task of extracting ternary relations, which is much harder than the usual information extraction issues considered in prior work.
Our system called PRAVDA (label PRopagated fAct extraction on Very large DAta) gathers fact candidates and distills facts with their temporal extent based on a new form of label propagation (LP) . This is a family of graph-based semi-supervised learning methods, applied to (in our setting) a similarity graph of fact candidates and textual patterns. LP algorithms start with a small number of manually labelled seeds, correct facts in our case, and spread labels to neighbors based on a graph regularized objective function which we aim to minimize. The outcome is an assignment of labels to nodes which can be interpreted as a per-node probability distribution over labels. In our scenario, the labels denote relations to which the fact in a correspondingly labelled node belongs.
 We adopt the specific algorithm of [18], coined MAD (Modified Adsorption), with an objective function that combines the quadratic loss between initial labels (from seeds) and estimated labels of ver-tices with a data-induced graph regularizer and an L2 regularizer. The graph regularizer is also known as the un-normalized graph Laplacian, which penalizes changes of labels between vertices that are close. We develop substantial extensions, and show how to judiciously construct a suitable graph structure and objective func-tion. Notably, we consider inclusion constraints between different relation labels for the same node in the graph. For example, we may exploit that a relation like joinsClub (with time points) is a sub-relation of worksForClub (with time spans).

Research on temporal fact extraction is still in its infancy: prior work is scarce and has major limitations. [23] showed how to extract temporal facts from Wikipedia infoboxes and lists, but can handle only semi-structured input and does not carry over to textual sources. [27] and [11] used an elaborated learning machinery to tap into temporal statements in text, but are computationally expensive and reliant on extensive training data. In contrast, our method is much more efficient and requires only minimal supervision. NLP research (e.g., [21, 5]) has also looked into temporal phrases, but has focused on detecting events (e.g.,  X  X er birthday X ), relative expressions (e.g.,  X  X ast Sunday X ), and time-order relations (before, after, during, etc.). These basic tasks are still far from actual fact extraction. LP algo-rithms have been applied to knowledge-harvesting problems, but only to the simpler task of acquiring taxonomic relations between entities and classes (instanceOf) and among classes (subclassOf), see most notably the work of [19]. For harvesting arbitrary types of binary relations and especially for ternary relations with time points or time spans, we need a very different graph construction and loss function, and we have extended the MAD algorithm to consider also inclusion constraints (i.e., subsumptions) among relations.
In summary, this paper makes the following contributions:  X  a new model for distilling temporal facts from text sources like  X  an algorithm for extended label propagation with consideration  X  experimental studies, comparing the extraction of base facts
This section gives a brief overview on the relation types we har-vest and the system architecture. In particular, we introduce the distinction between base (without temporality) and temporal rela-tions (including temporality).
We distinguish between base and temporal relations . Base re-lations indicate the relationships between two entities without any temporal information. A base relation R b is commonly described with a type signature, sig ( R b ) = ( TYPE 1 , TYPE 2 ) , meaning that the relation R b only holds its semantic meaning for two entities whose types are TYPE 1 and TYPE 2 , respectively. A base relation contains a set of base facts. Each base fact is in the form of ( e e ), where e i and e j indicate two entities whose types are consistent with the corresponding base relation X  X  type signature. For example, worksForClub is a base relation with signature ( PERSON , CLUB ). The base fact (Lionel_Messi, FC_Barcelona) holds for the worksForClub relation, where Lionel_Messi is a PERSON entity and FC_Barcelona is a CLUB entity.

Temporal relations indicate the relationships between two enti-ties at specific time points or during particular time spans. Similar to base relations, a temporal relation is always associated with a type signature indicating the pertinent entity types. A temporal relation contains a set of temporal facts in the form of ( e where e i and e j are two entities with pertinent types, and t dicates a temporal mention (either a time point or a time span). For example, joinsClubTemp is a temporal relation with type sig-nature ( PERSON , CLUB ). The temporal fact (David_Beckham, Real_Madrid)@2005 holds for the joinsClubTemp relation, in-dicating that David_Beckham joined Real_Madrid in 2005.
Figure 1 gives an overview of our system for fact harvesting from Web sources. The system consists of four components: can-didate gathering , pattern analysis , graph construction , and label propagation . The components are invoked in four phases. 1. Candidate Gathering : This phase serves to collect potentially 2. Pattern Analysis : Based on a (small, manually crafted) set of 3. Graph Construction : To facilitate the knowledge harvesting 4. Label Propagation : Finally, a graph-based semi-supervised
In this section, we first describe our candidate gathering approach, raising surface (text) strings to entities. Then, we introduce our pattern analysis method, which is based on  X  X asic X ,  X  X eed X  and  X  X act X  patterns. Basic patterns are (entity) type-aware word-level n-grams of surface strings. Based on statistical analytics, those patterns yielding in high quality results for a certain relation become seed patterns. Fact patterns are finally derived from sentences where the similarity of an observed basic pattern and seed patterns exceeds a pre-defined threshold. Finally, we will explain the underlying (pattern-candidate) graph model between patterns and candidate sentences. To facilitate the following discussions, we use bold capital letters to indicate matrices, and normal lower-case letters to indicate scalar values.
Our approach is driven by target relations for which we would like to gather instances. The set of relations of interest is denoted as R ={ R 1 , R 2 , ..., R m }. Each is associated with a type signature and is either a base relation or a temporal relation. Type ( R ) contains all possible entity types in the type signatures of the relations in R .
We assume that there exists a knowledge base with typed entities which contains the individual entities whose types are in Type ( R ) . In this paper, we consider relations of interest from the domains of soccer and celebrities, and use YAGO [17] as knowledge base. The YAGO knowledge base contains almost all the pertinent entities harvested from Wikipedia, such as persons (both sports people and celebrities), clubs, locations, awards, universities, and even a large fraction of people X  X  spouses. YAGO also provides us with a fairly complete dictionary of surface names for entities, including abbrevi-ations (e.g.,  X  X anU X  for the entity Manchester_United ). Thus, detecting mentions of entities in an input text is straightforward.
We consider a textual corpus C , e.g., a set of Wikipedia articles, news articles, or Web pages, as input for candidate gathering. The corpus is pre-processed to generate meaningful segments. In this pa-per, we consider sentence-level segments: the corpus is decomposed into a set of sentences where each sentence is a sequence of tokens.
Given a sentence, an entity recognition and disambiguation mod-ule first checks whether at least two entities (and a temporal mention for temporal relations) are mentioned in the same sentence. This is primarily based on YAGO X  X   X  X eans X  relation, where a string means an entity (e.g.,  X  X anU X  meaning Manchester_United ). The entity recognition engine works in four stages: 1. Based on a sliding window technique  X  X nteresting X  strings are 2. Entities that can be mapped unambiguously (by plain string 3. The  X  X round-truth entities X  and Wikipedia anchor links are 4. Disambiguation of each  X  X nteresting X  string to a single (high-
We then check whether the entity types are compatible with the type signature of some target relation R m . If so, a candidate fact is generated, and the sentence is added to the set of interesting sen-tences. For example, the sentence  X  David_Beckham played for Real_Madrid and LA_Galaxy  X  will create the candidate facts ( David_Beckham , Real_Madrid ) and ( David_Beckham , LA_Galaxy ), but not ( Real_Madrid , LA_Galaxy ) as there is no relation of interest between two entities of type CLUB .
Our notion of patterns builds upon the prior work of [13], where sets of word-level n-grams from sentences are used as patterns. We extend this approach by exploiting the type signatures of the target relations for which we perform fact harvesting.

Consider a fact candidate ( e i , e j ) and its corresponding candidate sentence, written in the form x e i y e j z where x , y and z are sur-face string surrounding the entities. We consider the context surface string y as an (initial) indicator of the relationship with fact ( e e ). For example, we observe a fact candidate ( David_Beckham , LA_Galaxy ) in the sentence s =  X  David_Beckham finally moved from Real_Madrid before his recent joining LA_Galaxy in 2007. X , the context surface string  X  X inally moved from Real_Madrid before his recent joining X  is the evidence for the joinsClubTemp relation-ship between David_Beckham and LA_Galaxy .

Context surface strings are usually long and over specific. It is extremely rare to observe other entity pairs sharing exactly the same context surface strings. To overcome this problem, we generalize the pattern representation. One possible solution is to adopt word level n-grams to represent patterns. Before generating the n-grams, we first do compression and lifting on context surface strings.
In order to avoid getting misguided by problems induced from natural language processing, we convert each surface string into a compressed surface string . These contain only -preserving their original order -nouns, verbs, and prepositions after stop words re-moval based on Wikipedia X  X   X  X ist of English Stop Words X  1 verbs and prepositions are identified by LingPipe Part-of-Speech tagger 2 . All verbs are transformed into present tense based on the verb dictionary provided by Assert (Automatic Statistical SEmantic Role Tagger) 3 . Nouns are stemmed with PlingStemmer 4 .
The compressed surface strings are further generalized by con-sidering the set of types that constitute the type signatures of our target relations. This is done by replacing entities by their types (e.g. PERSON , CITY , CLUB , and DATE ). Thus, the corresponding lifted surface string of sentence s is { X  X ove from CLUB before join X  X . For a lifted surface string, we generate word-level n-grams as its basic pattern (with n typically set to 2). For example, the basic pat-tern of s w.r.t. the entity pair ( David_Beckham , LA_Galaxy ) is denoted as BP ( s , David_Beckham , LA_Galaxy ) = ( X  X ove from X ,  X  X rom CLUB  X ,  X  CLUB before X ,  X  X efore join X ).
Given a set of positive and negative samples for the facts in a specific relation, we are able to measure how good a pattern is for a particular relation. The best patterns are then regarded as seed patterns for this relation. The overall procedure of identifying seed patterns for a specific relation is similar to [13].

A positive seed fact ( e i , e j ) (or ( e i , e j )@ t k R b (or a temporal relation R t ), is a valid fact of the relation R R ). A negative seed fact ( e i , e j ) (or ( e i , e j )@ t is an entity pair (with temporal mention) that is not a fact of R http://en.wikipedia.org/wiki/Stop_words http://alias-i.com/lingpipe/web/demo-pos.html http://cemantix.org/assert.html http://www.mpi-inf.mpg.de/yago-naga/javatools/ R ). PF ( R i ) and NF ( R i ) denote the manually crafted positive and negative seed facts for R i .

For each base relation R b we identify the sentences that contain two entities from a positive seed in PF ( R i ) and the sentences that contain two entities from a negative seed in NF ( R i ) . These sen-tences are collected into the sets PS ( R b ) and NS ( R b Analogously, for each temporal relation R t , the positive sentences are those that contain both entities and the temporal expression (date) of a positive seed, and the negative sentences are those that contain a negative seed. The sets of positive and negative sentences are denoted as PS ( R t ) and NS ( R t ) , respectively.
Given a relation R i  X  X  , a candidate sentence s with lifted sur-face string L ( s ) containing a basic pattern p , support and confidence are defined as follows: The support value captures the frequency of a pattern in conjunc-tion with positive seed facts, whereas the confidence value denotes the ratio of a pattern co-occurring with positive seed facts versus negative seed facts. If supp ( p,R i ) is above predefined thresholds, the basic pattern p is considered as a seed pattern of R i denotes the set of all seed patterns for R i . In our experiment, we vary the threshold of support in different settings.
Given a candidate sentence s = x e i y e j z , the fact pattern of the sentence w.r.t. the entity pair ( e i , e j ) is defined as: TYPE 1 and TYPE 2 are the types of e i and e j , respectively. BP ( s , e , e j ) is a basic pattern of sentence s w.r.t. the entity pair ( e In addition, a fact pattern is associated with a weight vector that contains the weights of the fact patterns with regard to the target relations of interest: ( w ( BP ( s , e i ,e j ) , R 1 ) , w ( BP ( s , e ... , w ( BP ( s , e i ,e j ) , R m )) .

Given a relation R k , if its type signature is the same as the fact pattern X  X  type signature, the weight is defined by Equation (2): weight ( BP ( s,e i ,e j ) ,R k ) = max where the similarity between the basic pattern BP ( s ) and a pattern p in the seed patterns of R i is defined based on the Jaccard coefficient with regard to n-grams q :
A weighted undirected graph G ( V,E, W ) is employed to facil-itate the knowledge harvesting, where V is the set of vertices, E is the set of edges, and W is the weight matrix (i.e., an entry W indicating the similarity of the corresponding vertices v
We divide the set of vertices into two subsets: fact candidate vertices ( V F ) and fact pattern vertices ( V P ), where V V = V F  X  V P . A vertex in V F corresponds to a fact candidate, i.e., either a base fact ( e i , e j ) or a temporal fact ( e i V
P corresponds to a fact pattern in the form of ( TYPE 1 , TYPE as defined in formula (1), where p indicates a basic pattern. Edges between a fact candidate vertex and a fact pattern vertex: An edge between a fact candidate vertex v f and a fact pattern vertex v is generated, if there is a candidate sentence containing both. Intuitively, more candidate sentences provide higher support for the fact candidate v f held in the relation v p . Let S ( v f candidate sentences, then the weight of the edge ( v f , v parameter  X  smoothens similarity values by reducing the impact of outliers with undesirable high cardinality such as spam.
Edges between two fact pattern vertices: If two fact patterns are similar, their corresponding vertices are connected by an edge with a similarity-based weight. The intuition is that similar patterns often indicate the same relation. Two fact pattern vertices v p considered dissimilar (similarity is zero) if their corresponding type signatures are inconsistent (i.e., v p i . TYPE 1 6 = v p their type signatures are consistent, the edgeweight is defined by the similarity of the basic patterns v p i . p and v p j . p as follows: 1. If the two patterns share the same verb and preposition, the 2. If one pattern contains a verb and the other one does not, the 3. If neither of them contains a verb, the similarity is the distance-The distance-weight of an n-gram in a basic pattern is defined based on its position in the sentence. The closer the n-gram appears to the target entity, the higher the weight. This way, we can effectively deal with relatively long patterns. For instance in the example graph in Figure 2, although the two patterns V P 2 and V P 3 share the n-gram  X  X ove from X , its weight in V P 3 is very low, thus resulting in a very low similarity between the two patterns.
After constructing the graph of fact candidates and fact patterns, a semi-supervised label propagation algorithm is applied to extract the relations that hold between fact candidates. The idea of label propa-gation is that the labeling of vertices (here the possible relations) is propagated to nearby vertices via weighted edges until convergence. In this process, the vertices linked by highly weighted edges are more likely to have the same labels, because the weights of edges represent the vertex similarities. All vertices are treated uniformly, regardless of whether they represent fact candidates or fact patterns.
Suppose the graph contains n vertices, consisting of fc fact can-didate and fp fact pattern vertices, where | V | = n , | V | V P | = fp , and n = fc + fp . In addition, there are m + 1 labels. The first m labels correspond to the m relations in the set R of target relations, plus a  X  X one X  label (denoting no or an unknown relation). In the following, we will use labels and relations interchangeably.
The matrix Y  X  R n  X  ( m +1) + denotes the graph X  X  initial label assignment. The vector Y i  X  denotes the i th row of matrix Y , with each element Y ik  X  [0 , 1] indicating vertex v i  X  X  confidence score for label k , where k  X  X  1 , 2 , ..., m + 1 } . The ` th column of matrix Y is denoted by vector Y  X  ` .

When a fact candidate vertex v i corresponds to a positive seed fact of relation R k , the vertex is labelled as R k with confidence 1 , i.e., Y ik = 1 . A fact pattern vertex v j is labelled as R k with confidence 1 (namely Y jk = 1 ), if its weight for a seed pattern of relation R (evaluated by Equation 2) is greater than a pre-specified threshold value  X  . Note that if v j has a strong weight (greater than  X  ) with the seed patterns of more than one relation, all the corresponding relations are labelled. All other entries in Y are initially set to 0 .
When the label propagation process finishes, each vertex is asso-ciated with a vector indicating the estimated labels. We use matrix b Y  X  R n  X  ( m +1) + to indicate the estimated labels for all vertices. Note that the matrix b Y has exactly the same ordering of vertices and labels as Y . Finally, for each vertex, the labels with sufficiently high confidence scores are selected.
As shown in [18], the process of label propagation is realized by minimizing the following objective function: where Y  X  ` and b Y  X  ` denote the ` th column vector of the initial assignment matrix Y and estimated label matrix b Y , respectively.
The intuition of keeping estimated labels centred around initial labels is realized by the first term of the objective function (3), which is the quadratic loss that measures the discrepancies between the initial labels and the estimated labels. In the original MAD algorithm of [18], the diagonal matrix S ` is identical for every label, with p inj i on the main diagonal for labeled vertices and 0 for unlabeled ones, where p inj i is set to be proportional to vertex v degree. Since the initially labelled fact pattern vertices may be noisy, the corresponding elements on the main diagonal of S ` should also be able to control the confidences on the label. Assuming that we are considering label ` and v i is a fact pattern vertex, we represent the confidences on the label ` by multiplying p inj i with the vertex v  X  X  fact-pattern weight on label ` (evaluated by Equation 2). The label spreading effect is achieved by the unnormalized graph Laplacian matrix L , which smoothes label similarities between linked vertices based on edge weights. This property becomes clearer when  X  1 b Y T  X  ` L b Y  X  ` is rewritten as weight between vertices v i and v j . Following [18], we set p be inversely proportional to vertex v i  X  X  degree. The hyper-parameter  X  controls the influence of the graph Laplacian. In order to mini-mize the function, similarities of labels between vertex pairs linked with high edge-weight are enforced, while different labels are toler-ated for those vertex pairs with low edge-weights.

The last term in the objective function (3) can be regarded as a combination of an L2 regularizer for the m relations of interest and a loss function for the  X  X one X  label. R  X  ` is the ` th column of the abandon matrix R  X  R n  X  ( m +1) + . Specifically, R  X  ` is a zero-valued column vector for every `  X  { 1 , 2 , ..., m } , which works exactly as an L2 regularizer for the estimated labels of the m relations of interest to avoid over-fitting to their seed labels. However, the vector of the last label ( m + 1 ) is R  X  ( m +1) = ( p abnd p n ) T , where p abnd i is set to 1 -p inj i -p cont i like in the original MAD algorithm [18]. The hyper-parameter  X  2 adjusts the degree of regularization and the trust in the  X  X one X  relation.

Since we only change the diagonal matrices S ` , the objective func-tion can still be solved by the optimization algorithm of MAD [18].
Sometimes, a relation R i implies another relation R j , meaning that a valid fact of R i should also be a valid fact of R case, there is an inclusion constraint (subsumption) that R R . For example, the relation isCapitalOf between cities and coun-tries implies the isCityOf relation. Inclusion dependencies arise in specific forms for temporal relations, that is, between events and their implied lasting relations, and also between base relations and their temporal counterparts. For example, the event relation join-sClubTemp implies worksForClubTemp and the base relation works-ForClub . The temporal fact joinsClubTemp (David_Beckham, Real_Madrid) @2003 also implies the temporal fact worksFor-ClubTemp (David_Beckham , Real_Madrid) @2003, which in turn implies the base fact worksForClub (David_Beckham , Real_Madrid) .

Inclusion constraints provide an additional asset: it can generate (or reinforce) the evidence of a fact by observing another fact of a different relation. Thus, if relation ` implies relation ` for a fact candidate v m having label ` , that the estimated confidence value b Y m` 0 should be greater or equal than the estimated confidence value b Y m` . For instance, if a fact candidate v m is estimated as joinsClubTemp , its estimated labels should satisfy the following following, we will show how to incorporate inclusion constraints into an extended label propagation framework.
We first introduce some notations. The matrix C  X  R records the inclusion dependencies between different relations of interest. Specifically, if relation ` implies relation ` 0 1 . Given a label ` , imply ( ` ) denotes the labels that imply ` and implied ( ` ) those labels that are implied by ` .

We introduce a new objective function, described in equation (5), to incorporate inclusion constraints between different relations: L ( b
Y ) = X The last term in Equation (5) serves to smoothen the estimated labels which satisfy the inclusion constraints. Note that only ver-tices containing initial labels are smoothed. In contrast to the MADDL algorithm [18], which assumes all correlations among classes to be symmetric, our approach does not assume symmetry of correlations. For example, assume that joinsClubTemp implies worksForClubTemp and the fact candidate (David_Beckham, Real_Madrid) @2003 holds for joinsClubTemp with high proba-bility (e.g. 0.9), but has low probability for the worksForClubTemp relation (e.g. 0.1). This is undesirable and should be smoothed, as joinsClubTemp implies worksForClubTemp . Therefore, the objec-tive function (5) will pull the estimated value for worksForClubTemp closer to the value for joinsClubTemp by reducing the labels X  dif-ferences. On the contrary, if the same fact candidate holds for worksForClubTemp with high probability (e.g., 0.9), but has low probability for the joinsClubTemp (e.g., 0.1), the objective func-tion (5) will not pull the estimated value for the joinsClubTemp .
In the Appendix, we proove that adding inclusion constraints does not change the convexity of the objective function in Equation (3). The convex optimization problem is solved by cyclic coordinate de-scent [3]. The algorithm iterates through each coordinate of solves sequentially the following sub-problems until it converges: where lb and ub are the lower and upper bounds of b Y v` , respectively.
In each sub-problem, all variables of b Y v` except the one at the current coordinate are fixed. So this technique optimizes the objec-tive function by solving a sequence of single-variable optimization problems. Because the problem is a weighted sum of quadratic functions, there is a closed-form solution for each sub-problem. For coordinate b Y v` , we set the first partial derivative with respect to b Y v` to zero. Thus, the optimum in the absence of constraints is: b From the inclusion constraints we can infer a lower ( lb ) and an upper bound ( ub ) for the solution. Specifically, if imply ( ` ) is not is not empty, ub = min ` 0  X  implied ( ` ) b Y v` 0 , otherwise ub = 1 . For example, the lower bound of worksForClubTemp is the maximum of joinsClubTemp and leavesClubTemp , with the upper bound 1 . Similarly, the upper bound of joinsClubTemp / leavesClubTemp is the value on worksForClubTemp , with the lower bound 0 . The final optimum is computed by max(min( b Y v` , ub ) , lb ) to ensure that the optimal solution satisfies the inclusion constraints.

We refer to this algorithm as ICMAD (Inclusion-Constraints-aware MAD). The complete method is shown in Algorithm 1. Com-pared to the original MAD method, the lower and upper bounds are checked after each label update.
The above ICMAD algorithm can be applied to either base or temporal facts and their patterns. So far, however, these would be Algorithm 1 ICMAD Algorithm.
 two separate instantiations of the same algorithm. In this subsection, we discuss how to combine the two settings for enhanced output.
For base relations, seed patterns may be noisy, especially when positive seeds (entity pairs) hold for multiple relations. For example, a person may be born and may die in the same place, so both relations isBornIn and hasDiedIn have similar seed patterns, which is undesired. However, the corresponding temporal relations are better distinguishable, as -usually -people do not die on the same date (or same year) they are born. Thus, by combining base and temporal graph, the seed patterns from the temporal graph can help to counter ( X  X orrect X ) noisy patterns in the base graph.
The pattern nodes in the temporal graph are actually a subset of those in the base graph. Thus, we can unify both graphs by sharing all pattern nodes and conceptually connecting the fact candidate nodes from base and temporal graph with their respective patterns. A new objective function is described in Equation (8), where matrix G  X  R p b  X  p t + records the relationship of the same pattern nodes in base and temporal graph. p b and p t are the number of pattern nodes in base and temporal graph. If a pattern node v i in the base graph is exactly the same as the pattern node v j in the temporal graph, then we assign G ij to 1. Otherwise, G ij is set to 0.
 label matrices of base and temporal graph, respectively. Likewise, b matrices of base and temporal graph, respectively. S ` and L can also be re-written analogously. On this basis, we define Equation (8) as the new objective function as follows: L ( b
Y ) = X where label ` 00 is the corresponding temporal relation of base label ` (e.g., ` for worksForClub with ` 00 for worksForClubTemp ).
The new objective function shown in Equation (8) keeps the same form as Equation (5). Therefore, it is still solvable by cyclic coordinate descent. All methods used in this paper are implemented in Java using Sun JDK 1.6. We then detect and disambiguate entities based on the Stanford named entity recognizer 5 and YAGO X  X   X  X eans X  relation as oulined in Section 3.1. Temporal mentions are identified by regular expressions (currently yearly granularity). The actual label propagation algorithm is based on the Junto library [18]. Experiments have been conducted on a workstation with two Intel Xeon 2.40GHz, 4-Core CPUs, and 40GB memory. Except parameters  X  = 0 . 05 and  X  = 0 . 5 , all the other hyper parameters used in our experiments are manually tuned on a separate dataset.
Data Sets: Our methods have been evaluated against two corpora from the soccer and celebrity domain. In the soccer domain, we selected more than 23.000 soccer players X  Wikipedia articles. In addition, we retrieved around 110.000 on-line news articles (e.g., BBC, Yahoo! news, ESPN, etc.) by searching for players contained in the  X  X IFA 100 list X  6 . Likewise, the celebrity corpus has been populated with more than 88.000 news articles of persons mentioned in the  X  X orbes 100 list X  7 in addition to their Wikipedia articles.
Relations of Interest: In the current experiments, we are interested in four base relations and nine temporal relations. The relations and their type signatures are summarized in Table 1. It is worth men-tioning that temporal relations (highlighted in grey) such as isBorn-InTemp or hasDiedInTemp are  X  X xtensions X  of their corresponding base relations isBornIn resp. hasDiedIn by temporal information for the identified event (e.g. birth, death, etc.).

Seed Selection: For each relation positive and negative seed facts have been manually selected. Facts with prominent entities are chosen as seed facts due to their frequency. Thus, k co-occurring entities are chosen as seed facts for each relation type signature.
For example, when selecting seed facts for the worksForClub relation, we compute the co-occurrence statistics (eventually sup-ported by keyword filtering) of PERSON and CLUB entities based on the same sentence. Starting with the highest ranked entity pair, http://www-nlp.stanford.edu/software/CRF-NER.shtml http://en.wikipedia.org/wiki/FIFA_100 http://www.forbes.com/lists/2010/53/celeb-100-10_The-Celebrity-100.html we manually check in Wikipedia if the relation of interest is satisfied. If approved, it becomes a positive seed fact, otherwise a negative.
Performance Metrics: Baseline for our experiments is the state-of-the-art PROSPERA system [13]. We evaluate both approaches in terms of precision correctness of the extracted facts. Due to the size of the corpus, evaluation of recall is impossible. Precision estimates are based on sampling (50 facts per relation) with a human evaluation against ground truth information in Wikipedia.
Sample Output: For the relations of interest we generate patterns and (temporal) facts. Table 2 depicts randomly chosen results from both corpora. Results for temporal facts are marked in grey. Table 2: Patterns and Facts extracted for Relations of Interest We first compare base fact extraction with PROSPERA and PRAVDA. The support threshold in pattern analysis is supp ( p,R 10 with 100 positive and 10 negative seed facts for each relation. Table 3: Base Fact Extraction (100 positive, 10 negative seeds)
Table 3 summarizes the results of this experiment. As one can see both approaches achieve comparable precision. However, PRAVDA clearly outperforms PROSPERA with respect to the number of extracted facts. The reason for this major gain can be explained with LP X  X  propagation of the labels X  confidence scores from one node to another. This results in confidence propagation among very similar, but less frequently occuring patterns ( X  X e born in X  and  X  X orn in X ). Of course, this comes with the risk of inducing more noise, which becomes apparent for more complex relations such as isMarriedTo . The explanation for this  X  X isbehavior X  is simple: a prominent couple will be frequently mentioned in the media in completely different contexts (e.g. when dating, talking, visiting, etc.), thus, propagating high confidence to less useful labels, too.
In order to quantify the impact of the number of seed facts, we now evaluate the performance of two base relations ( worksForClub and isMarriedTo ) for varying numbers of positive seed facts while keeping the number of negative seeds fixed with 10. We vary them from 10, 20, 50 to 100 by simultaneously adopting the support supp ( p,R i ) used in pattern analysis to 4, 6, 8 and 10 respectively. Table 4: Impact of Varying the Number of Positive Seed Facts
Table 4 summarizes the results on the impact of varying the num-ber of positive seed facts. PRAVDA extracts far more facts than PROSPERA, particularly if the number of positive seeds is low. Both approaches achieve similar precision quality throughout the ex-periments (except for PROSPERA for the isMarriedTo relation with 10 positive seeds given). It can again be recognized that PRAVDA is slightly prone to generate more noise, particularly when the relation of interest is more complex. As an exapmle of the isMarriedTo relation, please consider the following text from our corpus:
Since we do not use a dependency parser for efficiency reasons, there is no evidence that the two entities Rosanna Arquette and Heather Mills are not directly connected. Consequently, the surface string pattern  X  X ife X  links Rosanna Arquette with Heather Mills and identifies them (incorrectly) as a couple.
In the following we will present the results of our methods on temporal fact extraction with inclusion constraints (yearly granu-larity). Since PROSPERA is not geared for harvesting temporal facts, we only report the performance of our method. We evaluate inclusion constraints for two relations:  X  joinsClubTemp and leavesClubTemp imply worksForClubTemp  X  getsMarriedWithTemp and getsDivorcedFromTemp imply isMar-The results are depicted in Table 5 (10 positive and negative seed facts for the first seven relations) with a special highlighting in dark grey of the relations worksForClubTemp and isMarriedToTemp . Without giving any seed facts to the worksForClubTemp and isMar-riedToTemp relations, inclusion constraint are successfully applied to extract facts for the two relations.

We see that results with respect to the number of extracted facts and precision are not so good as for base fact extraction. This is not surprising for at least three reasons. First, our extraction currently operates on the sentence level, meaning that entity pair and temporal expression have to be in the very same sentence. Second, we are currently able to detect only explicit temporal expressions. Third, associating temporal expressions to the appropriate relation and/or entity/ies is sometimes complex for human, too.

Extraction of facts for the leavesClubTemp and joinsClubTemp relations are particularly difficult. Usually media coverage starts a long time before the actual transfer actually occurs. Consequently, this induces a lot of noise to the leavesClubTemp relation. In addi-tion, both before mentioned relations are -of course -affected by unsupported rumors, which are common to sport news coverage. Table 5: Temporal Fact Extraction with Inclusion Constraints
In the following we now investigate the results of joint base and temporal fact extraction. In contrast to the experiments before, we now combine the base and temporal graphs and extract the facts simultaneously. Table 6 compares the joint approach with the separate (for base facts resp. temporal facts) ones. For the sake of clarity, results of temporal fact extraction have been highlighted in grey. Experiments have been conducted with 10 positive/negative seed facts per base relation and 5 positive/negative seed facts per temporal relation (except worksForClubTemp and isMarriedToTemp in order to evaluate the impact of inclusion constraints).
As we can see from Table 6, particularly base fact extraction ben-efits from the joint apporach, while temporal fact extraction remains more or less unchanged. It is worth mentioning that the worksFor-Club relation increases a lot in terms of number of extracted facts as well as slightly with respect to precision. Even more, the number of extracted facts as well as the precision for the worksForClub rela-tion is higher than its separate  X  X ounterpart X , which is the 20-seed baseline compared with 10 seeds for worksForClub + 5 seeds for joinsClubTemp + 5 seeds for leavesClubTemp in the joint approach.
Finally, we study the impact of inclusions constraints on the joint approach. From the results in Table 7 we can see that inclusion constraints play an important role in the joint extraction approach. Both base relations ( worksForClub and isMarriedTo ) benefit from their inclusion, while precision slightly decreases for worksForClub . Accuracy for temporal relations (colored in grey) remains almost unchanged. Please note that no facts have been discovered for worksForClubTemp and isMarriedToTemp (indicated in dark grey in Table 7) as they have been extracted via inclusion constraints, only.
Relation extraction aims at detecting and classifying semantic relationships between entities typically from text. Many machine learning methods have been exploited to address this task. Super-vised learning methods, such as [8, 29] suffer from a large amount of manually labelled training data. Semi-supervised methods (e.g. [28]) require less labelled data, and particularly [7] first attempted to apply label propagation for relation extraction. Recently distant supervision for relation extraction(e.g. [12, 26]) became popular. Instead of using a labelled corpus, the input of distant supervision Table 6: Comparison of Joint and Separate Fact Extraction
Table 7: Joint Extraction With and Without Constraints only requires an existing knowledge base. These advances in infor-mation extraction and the success of knowledge communities like Wikipedia have enabled the largely automated construction of fairly comprehensive knowledge bases such as DBpedia [1], YAGO [17], freebase.com , trueknowledge.com , the TextRunner project [9, 24], the StatSnowball project [30], and others. None of these knowledge bases has a temporal dimension. They are all designed for time-invariant knowledge and built as snapshots in time. For example, while several of them know that people can have multiple spouses, none of them can capture the time intervals for the various marriages. Birth and death dates of people are available, though, as they can be extracted at the level of base-facts such as Diego_Maradona bornOn 30-October-1960 . This is much simpler than the kind of temporal facts that tackled in this paper.

Temporal knowledge as a first-class citizen in knowledge bases has been addressed solely by a few prior papers: [27], [23], and [11]. [27] focuses on extracting business-related temporal facts such as terms of CEOs. It uses an NLP machinery, with deep parsing of every sentence, and machine-learning methods for classifiers for specifically interesting relations. It workes reasonably well, but is computationally expensive, requires extensive training, and cannot be easily generalized to other relations. [23] focuses on extracting relevant timepoints and intervals from semistructured data in Wikipedia: dates in category names, lists, tables, infoboxes. However, there is no support for processing free text. [11] uses training data with fine-grained annotations to learn an inference model based on Markov Logic. This involves using consistency constraints on the relative ordering of events. This machinery is computationally expensive and cannot easily be scaled up.
The NLP community has event extraction tasks in its SemEval workshop series [20], using representations such as TimeML and reference corpora such as Timebank [5]. The TARSQI toolkit [21] provides a suite of techniques for detecting events and temporal expressions, based on a dictionary of time-related adverbial phrases and regular-expression matching. [25] use a small set of seeds to learn pattern rules for various complex relations, such as joining (leaving) a job. The algorithm works on parsed input data, which requires preprocessing by a dependency parser. Moreover, temporal mentions associated with these relations are not considered.
Temporal facts or temporal information are also considered in other fields. For instance, [22] focuses on logics-based querying over uncertain temporal facts, but does not address the extraction and temporal fact harvesting process. There is also recent awareness of temporal IR: ranking of search results for keyword queries with temporal phrases [4, 14].

Label propagation belongs to a family of graph-based semi-supervised learning methods. The first LP method [31] assumed that initial labels are fully correct. This assumption is inappropriate for our setting, as the seed patterns are not fully correct, but only associated with a confidence value. The Adsorption algorithm [2] copes with noisy initial labels, and is successfully used in video rec-ommendation. The MAD algorithm [18] optimizes the Adsorption algorithm. The MADDL algorithm [18] incorporates possible depen-dencies on different labels into the optimization function. However, these dependencies must be symmetric, in contrast to the asymetric inclusion constraints in our work. In a recent survey [19], MAD is regarded as the most effective label propagation algorithm across various data sets and experimental conditions. The nature of label propagation, which can be iteratively approximated by the Jacobi method, makes it nicely scalable [2]. For example, [15] describes a scalable implementation of label propagation on MapReduce to deal with Web-scale graphs.
This paper introduced a unified framework for harvesting base facts and temporal facts from textual Web sources. Our experimental results with news articles and Wikipedia pages demonstrate the viability and high accuracy of our approach. Moreover, for the base-facts case, we have shown much higher recall over one of the best state-of-the-art baselines, while achieving similar precision. We believe that extended forms of constrained label propagation, as developed in this paper, are a very promising alternative to prior methods like Max-Sat reasoning or sampling over Markov-Logic models or factor graphs.

Our extended LP method is nicely geared for scale-out on dis-tributed platforms. So far, however, our experiments were at a scale where we did not need to utilize this property. Our future work aims at experiments with Web-scale datasets (e.g., the TREC ClueWeb corpus), and will explore the scalability behavior of our method. This work is supported by the 7 th Framework IST programme of the European Union through the focused research project (STREP) on Longitudinal Analytics of Web Archive data (LAWA) under contract no. 258105. [1] S. Auer, C. Bizer, G. Kobilarov, J. Lehmann, R. Cyganiak, [2] S. Baluja, R. Seth, D. Sivakumar, Y. Jing, J. Yagnik, S. Kumar, [3] M. S. Bazaraa, H. D. Sherali, and C. M. Shetty. Nonlinear [4] K. Berberich, S. J. Bedathur, O. Alonso, and G. Weikum. A [5] B. Boguraev, J. Pustejovsky, R. K. Ando, and M. Verhagen. [6] A. Carlson, J. Betteridge, B. Kisiel, B. Settles, E. R. H. Jr., [7] J. Chen, D.-H. Ji, C. L. Tan, and Z.-Y. Niu. Relation extraction [8] A. Culotta and J. S. Sorensen. Dependency tree kernels for [9] O. Etzioni, M. Banko, S. Soderland, and D. S. Weld. Open [10] T. Heath and C. Bizer. Linked Data: Evolving the Web into a [11] X. Ling and D. S. Weld. Temporal information extraction. In [12] M. Mintz, S. Bills, R. Snow, and D. Jurafsky. Distant [13] N. Nakashole, M. Theobald, and G. Weikum. Scalable [14] M. Pasca. Towards temporal web search. In SAC , pages [15] D. Rao and D. Yarowsky. Ranking and semi-supervised [16] C. E. Rasmussen. Gaussian processes for machine learning. [17] F. M. Suchanek, G. Kasneci, and G. Weikum. Yago: a core of [18] P. P. Talukdar and K. Crammer. New regularized algorithms [19] P. P. Talukdar and F. Pereira. Experiments in graph-based [20] M. Verhagen, R. Gaizauskas, F. Schilder, M. Hepple, [21] M. Verhagen, I. Mani, R. Sauri, J. Littman, R. Knippen, S. B. [22] Y. Wang, M. Yahya, and M. Theobald. Time-aware reasoning [23] Y. Wang, M. Zhu, L. Qu, M. Spaniol, and G. Weikum. Timely [24] F. Wu and D. S. Weld. Automatically refining the wikipedia [25] F. Xu, H. Uszkoreit, and H. Li. A seed-driven bottom-up [26] L. Yao, S. Riedel, and A. McCallum. Collective [27] Q. Zhang, F. Suchanek, and G. Weikum. TOB: Timely [28] Z. Zhang. Weakly-supervised relation classification for [29] G. Zhou, M. Zhang, D.-H. Ji, and Q. Zhu. Tree kernel-based [30] J. Zhu, Z. Nie, X. Liu, B. Zhang, and J.-R. Wen. Statsnowball: [31] X. Zhu, Z. Ghahramani, and J. D. Lafferty. Semi-supervised We show that the function 5 is convex, following the convention of [16], we rewrite the estimated labels of b Y into a vector length n  X  ( m + 1) . b y = ( b Y 11 ,..., b Y n 1 , b Y 12 ,..., b Y n 2 ,..., b In the same way, the initial label assignment matrix Y can be converted into a vector y . Following the same idea, the graph Laplacian L is block diagonal with the matrices L 1 ,..., L our case, L i = L . And the diagonal matrix S is represented analogously. Then the matrix  X  C implied by inclusion constraints has the same form as the unnormalized graph Laplacian, which is  X  C = D W
C . The elements W  X  C have the same positive weights C `` 0  X  Y v` 0 , if an inclusion constraint between labels ` and ` 0 is included. The remaining entries of W are zeros. Then the degree matrix D  X  C is a diagonal matrix with D
With the new formulation of the problem, the optimization prob-lem including inclusion constraints takes the form
The Hessian matrix of the objective function with respect to which is positive definite if  X  2 &gt; 0 (  X  C is positive semi-definite following the same reason as for the unnormalized graph Laplacian). Finally, as the feasible region is defined by linear constraints, the optimization problem is convex.
