 The design of Multi-Classifier Systems (MCSs) is inspired by the group decision making process [13,14]. The motivation behind MCSs is that each classifier has its own strengths and weaknesses, and hence a group of classifiers could poten-tially leverage the wisdom of crowds. If each classifier in an MCS has expertise in classifying samples in some portions of a data space, the final output that is aggregated from all classifiers would become more reliable. More precisely, effective classifiers in an MCS are those that are accurate and independent. The former means that a classifier in an MCS is expected to provide performance at least better than random guessing, while the latter means that correlation be-tween outputs of classifiers is expected to be small. This also implies that their outputs are expected to be diverse.

Diversity could be captured by disagr eements between classifiers in an MCS and it plays a significant role in the success of MCSs [10]. However, the fol-lowing research question becomes important for the design of MCSs: Is there a relationship between diversity (between the member classifiers of an ensemble) and accuracy (of the ensemble)? We address this research question by taking a different route and building a relationship between diversity and correlation, which could be related to accuracy.

Fig. 1 illustrates the focus of this paper. The relationship between diversity and accuracy is ambiguous in theory (e.g. that elusive diversity [9]). The rela-tionship between correlation and the accura cy is relatively clear to researchers. Fig. 1 also gives two more examples of such relationships: Tumer and Ghosh build such a relationship for simple averaging ensemble [15], while Bremian re-lates correlation to performance of Random Forests [1].

This paper provides a proxy to the relationship between diversity and accu-racy, while it has a potential to assist with a design guideline for MCSs.
The rest of this paper is structured as follows. Section 2 gives a theoretical analysis and Section 3 discusses experim ental results. Section 4 is a brief review of some related work, while Section 5 gives conclusions and future work. Diversity has been studied by many resear chers [3,6], but its relationship to ac-curacy is not clear. One difficulty is that there exists an elegant bias-variance-covariance decomposition framework for regression tasks, but the framework does not directly apply to classification tasks [4]. Here we do not directly connect diver-sity to accuracy. Rather we build a relatio nship between diversity and correlation. Notations. For a set of N instances and two classifiers, N 11 and N 00 denote the numbers of instances for which both classifiers are correct and incorrect, respectively; N 10 and N 01 denote the numbers of instances for which only the first and the second classifier is correct, r espectively. The following definitions are with respect to outputs of classifiers i and j .
 Definition 1. Disagreement measure (Dis) representing diversity [10].
 Definition 2. Q-statistic or Q [10,11].
 Definition 3. Correlation [10]. One could calculate system-wise values by averaging all pairs, so we ignore the subscripts i and j for concise representation. Using these definitions and the in-equality of arithmetic-geometric-harmonic means, we obtain Corollary 1, as given below.
 Corollary 1. Relationship between disagreement measure and Q-statistic. Corollary 1 helps us connect diversity to c orrelation, since a connection between Definition 2 and Definition 3 is that the absolute value of correlation will be bounded by the absolute value of Q  X  statistic . Next we define f ( x ) based on Corollary 1. Since x = Dis and hence x  X  (0 , 1), we have f (0) = 1 and f (1) =  X  1. As the goal is to have zero correlation, we would like to know the interception of f ( x ) and x-axis. We call the interception the critical value of x ( x c )orthe critical point of Dis , and the following critical value is straightforward: Before this critical point, higher diversity reduces correlation. This supports the intuition that higher diversity between classifiers is usually associated with a better MCS. When diversity crosses the critical point, increasing diversity would increase correlation while highly correlated classifiers usually correspond to an inferior MCS. For each trial for a data set, we randomly draw samples and accordingly train a decision tree (without pruning). Similarly, we generate a disjoint set of samples and use it as a test set for each trial for a data set. To control the variable in, we create a dummy classifier for each d ecision tree. We repeat this 100 times and create 100 pairs of classifiers in every experiment, using the corresponding test set to evaluate each pair of classifiers, calculating values of disagreements, Q  X  statistic , and correlation. Figures in Appendix illustrate the results. Our findings are summarized as below:  X  The relationship between disagreement measure and Q  X  statistic is not linear.  X  For some data sets, the theoretical upper bounds of the values of Q  X  statistic  X  There are exceptions that are larger than the theoretical upper bounds cor- X  As N increases, curves move to the right. The critical point is a function of  X  It is not always the case that we observe critical points in experiments. For Now we take a couple of steps further and use our analysis result to explain some interesting phenomenon. [7] showed theoretically that heterogeneity (i.e. using dif-ferent algorithms in an MCS) would improve diversity among member classifiers in an MCS. Furthermore, [8] showed empirically that one could obtain such an improvement more often in bagging setting than in boosting setting; in addition it empirically showed that AdaBoost with heterogeneous algorithms would work better when the data set is larger. Compared to bagging, AdaBoost often provides higher diversity. When we introduce heterogeneity into AdaBoost, diversity will probably be increased. As discussed earlie r, increasing diversity has positive effect in the left region (between 0 and the critical point) of the graph of f ( x ), but it has negative effect in the right region (between the critical point and 1) of the graph of f ( x ). Moreover, the smaller the data set, the smaller the critical point, the smaller the left region. Therefore, using heterogeneous algorithms in AdaBoost on small data sets may actually have negative effect to the performance. The importance of reducing correlation between classifiers in an MCS has been recognized [2]. Tumer and Ghosh discuss a framework that quantifies the need to reduce correlation between classifiers in an MCS, and associate the number of training samples (i.e. the size of the training set) with the effect of correlation reduction [15]. Our analysis suggests that, for example, the critical point of Dis depends on N . Mane et al. prove that classifiers trained by using independent feature sets give more independent estimations and their combination gives more accurate estimations [12].

The term anti-correlation is confusing. In [13] McKay and Abbass describe it as a mechanism to promote diversity, but they do not explain why anti-correlation is equivalent to diversity promoting. Our analysis, however, explains this: When we promote diversity to a certain level (i.e. we have diversity in the neighborhood of the critical point), we decrease the upper bound of the absolute value of correlation and thus it is possible to observe very low or even negative correlation.
In [5] Chung et al. argue that, given th e average the accuracy (or performance) of classifiers, there is a linear relations hip between correlation and disagreement measure. Nevertheless, our analysis clearly shows that the relationship is not linear and our experimental results do not reveal the linear relationship as given in [5]. In this paper we explored the relationship between diversity, represented by dis-agreement, and correlation between classifiers in MCSs, conducting a theoretical analysis and experiments for the relationship between diversity and correlation. As a result, we demonstrated a nonlinear function for the relationship, while the experimental results reveal some inte resting insights. Therefore, this paper contributes to a better understanding of the role of diversity in MCSs.
Future work includes (1) investigating a tighter theoretical bound of Q-statistic, (2) integrating our analysis into those proposed by others in order to build a more elegant relationship between diversity a nd accuracy, and (3) using our analysis result to assist with classifier selection and/or combination algorithms for MCSs. Acknowledgements. The research reported herei n was supported by the Na-tional Aeronautics and Space Administration via award number NNX08AC36A, by the National Science Foundation via award number CNS-0931931, and a gift from Huawei Telecom. We gratefully acknowledge all our sponsors. The findings presented do not in any way represent, ei ther directly or through implication, the policies of these organizations.
 Inthesefigures,thex-axisisthevalueo f disagreement measure (representing diversity) and y-axis corresponds to values of Q  X  statistic or correlation  X  . A (blue) diamond and a (pink) square re present respectively an observed Q  X  statistic and an observed correlation, while a (yellow) triangle gives an upper bound of the corresponding value of Q-statistic. We report results for 100 and 1000 training samples for each data set.

