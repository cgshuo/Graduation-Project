 Content-based image retrieval (CBIR ) has been researched for decades, but it has yet to be widely applied in common Web search engines. In our view, one of the reasons for this is that CBIR is normally performed by computing the dis-similarity, e.g., Euclidean distance, be tween objects and queries based on their multidimensional feature vectors in content feature spaces, for example, colour, texture and structure features. There is a well-known gap, called the  X  X emantic gap X , between low-level feature of an image and its high-level meaning to a user. Another reason that CBIR is not yet widely used is that most existing CBIR sys-tems are designed principally for evaluating search accuracy. Less attention has been paid to designing interactive visual systems that support users in grasping how feedback algorithms work and how they can be manipulated.

One way to bridge this gap and increase the use of CBIR systems is to make the CBIR system more human-centric. A human-centric system should deliver a user-oriented search making the user f eel that they, rather than the system, are driving the search process. In [1], Bates addressed two issues for search system design:  X (1) the degree of user vs. system involvement in the search, and (2) the size, or chunking, of activities; that is, how much and what type of activity the user should be able to direct the system to do at once. X  To investigate the first issue, we had developed an interact ive relevance feedback (RF) mechanism named four-factor user interaction model in our early research, which aims to improve the interaction between user s and the system as well as to improve the search accuracy. According to the resu lts of our simulated experiments, the model can improve the search accuracy in some circumstances. However, we are not able to do user evaluation on the ease of use and usefulness of the interactive functionalities without an effect visual search interface.

In terms of the second issue, White, et al. in [13] has also addressed  X  X hen providing new search functionality, system designers must decide how the new functionality should be offered to users. One major choice is between (a) offering automatic features that require little human input but give little human control; or (b) interactive features which allow human control over how the feature is used, but often give little guidance over how the feature should be best used. X  X ne question arises here for our study: How should the functionalities be presented visually to the user by the interface to enable users to directly control the model in a effective way?
In this paper, we introduce a novel CBIR framework, which delivers a four-factor user interaction model we have developed aiming at providing a user-oriented search platform. The framework provides functionalities that support the user X  X  interactive search process and allow the user to control all four factors in our model. The design of an innovative CBIR search interface is the main focus of the paper. This section will review the related work and explain our motivation on resolving the interaction issue (the degree of searc h control deployed to users and system) and the design issue (the best way to deliver the framework functionalities to users through interface) from two aspect s, namely: the user interaction models and interactive search interface. 2.1 User Interaction Models In this section, we review and analyze a number of existing UI models that inspired our four-factor user interaction model.

In [10] Spink, et al. proposed the three-dimensional spatial model to support the user interactive search for text retri eval. The model emphasizes that partial relevance is as important as binary relevance / non-relevance and indeed it can be more important to the inexperienced us er. The three dimensions are: levels of relevance, regions of relevance and t ime. The levels of relevance indicates why a particular document is relevant to the user. The regions of relevance indicates how relevant the document is. And the third dimension -time -captures information seeking stage and successive searches. We consider that their model is a useful foundation from which to develop further detailed user interaction models and techniques for CBIR.
 Other research has tended to focus more on a single dimension, such as time. For example, Campbell in [2] proposed the Ostensive Model that indicates the degree of relevance relative to when a user selected the evidence from the results set. The model includes four relevance pr ofiles: increasing, d ecreasing, flat and current profiles. Later, Urban, et al. applied the increasing profile to CBIR [12]. Their preliminary study showed that the system based on the Ostensive Model was preferred by users over traditional CBIR search engines.

Ruthven, et al. [9] adapted and combined two dimensions from Spink, et al. three-dimensional spatial model, namely: regions of relevance and time to text retrieval. Their experimental results showed that combining partial and time relevance criteria does help the interaction between the user and the system. It will be interesting to see how the combined model performs in our CBIR framework.

Most of these models were tested and a pplied in text retrieval, we were mo-tivated to investigate what the outcome would be were we to adapt combined three-dimensional spatial model with the Ostensive Model together, and, further, to add another factor -frequency -to the combination, into CBIR. Therefore, we developed a new model for CBIR, namely  X  X our-factor user interaction model X , which includes relevance region, relev ance level, time and frequency [6]. Our hypothesis is that the four-factor user interaction model will provide enhanced search experience in terms of the level of interaction between the system and users and the search accuracy. However , without a visual search interface, we are not able to test the interaction aspects. This motivated us to develop a novel interactive CBIR framework and an innovative interactive user interface in order to enable effective user interactions and evaluate the model with real, as opposed to simulated, users. 2.2 Interactive Search Interface When providing new search functionality, we should decide how the new func-tionality should be delivered to users [1, 13]. In this section we investigate a number of search interfaces in order to explain why we developed the search interface in the way we did.

Flexible Image Retrieval Engine (FIRE) [3] is one tool that allows users to provide non-relevant feedback from the r esult set. The research in [4, 7, 8] also usefully referred to the importance of providing both negative and positive ex-amples as feedback. In addition, from the result of our simulated experiments, we found that limiting user X  X  selection o f non-relevant feedback to the poorest matches in the results list will improve search accuracy, but we realized this is not going to be intuitive to users. Therefore, we are encouraged to design the system to enable users to provide the ne gative examples from the worst matches in a natural way.

Urban, et al. developed an image search system based on the Ostensive Model [12]. Like FIRE, this is a browsing based search system, which uses a dynamic tree view to display the query path and results, thus enabling users to re-use their previous queries at a later stage. Whilst the query path functionality is useful, the user display becomes overly crowded even after a relatively small number of iterations. This limitation w ould become even more evident were the system to allow the user to provide negative as well as positive examples. Why not then harness the benefits of the query path functionality but in a search-based system, which separates query and results and applies the linear display to both queries and results?
Later, Urban, et al. in [11] presented another system X  X ffective Group Orga-nization (EGO), which is as a personalized image search and management tool that allows the user to search and group the results. The user X  X  groupings are then used to influence the outcome of the results of the next search iteration. This system supports long-term user and search activity by capturing the user X  X  personalized grouping history, allowing users to break and re-commence later without the need to re-create their search groupings from scratch. From this study, we can see that providing personalized user search history can improve the interaction between the system and users.

In [5] Hopfgartner, et al. defined explicit and implicit feedback. They con-cluded that explicit feedback is given actively and consciously by the user to instruct the system what to do. Whereas, implicit feedback is inferred by the system from the what the user has done unconsciously and here the system as-sumes or infers the user X  X  purpose. In other words, explicit feedback means the user is actively controlling the search process and implicit feedback means the system is exercising control over the se arch process. Their simulated user study results showed that combining implicit RF with explicit RF may provide better search results than explicit RF by itself. We are then encouraged to combine the implicit and explicit RF in our system.

In the following sections, we will present our proposed interactive CBIR frame-work, namely uInteract, which will implement the ideas we have developed to overcome the shortcomings of the related work and to apply the inspiration from the related work. Table 1 shows how the related work maps to the features of the uInteract (note that in this paper we only compare the CBIR features and ignore the textual search features). Moreover, the next sections will describe how we developed uInteract to deliver our f our-factor user interaction model. Our framework consists of two logically different modules, an interaction model based frontend and a backend search engine with a fusion controller supported by the interaction model. In addition, the login functionality of the framework supports personalized long-term searching. 3.1 Four-Factor User Interaction Model The four factors taken into account in the model are relevance region, relevance level, time and frequency. The relevanc e region comprises two parts: relevant (positive) evidence and non-relevant (ne gative) evidence. The relevance level here is a quantitative level, which indicates h ow relevant/non-relevant the evidence is, and differs from the original qualitative level presented in the work of Spink, et al. We adapted the Ostensive Model to t he time factor to indicate the degree of relevance/non-relevance relative t o when the evidence was selected, which is different from the original OM that only applies to positive evidence. The new factor -frequency -captures the number of appearances of an image in the user selected evidence both for positiv e and negative evidence separately.
The weight scheme of the model is given by W opf , which is the multiplication of W o and W p and W f [6]. W o is the ostensive weight, which can be different depending on the profile of the Ostensive Model: e.g., increasing profile (the earlier selected images deemed less impo rtant), decreasing profile (the earlier selected images deemed more important), flat profile (all the selected images treated equally) and current profile (only the latest selected images considered). The value of W o relies on the number of search iterations when the query was applied; W p is the partial weight, which is the range of relevance/non-relevance level based on the relevance and non-relevance regions. The value of W p depends on the score of the positive and negative feedback provided by the users; W f is the frequency weight, which is the number of times an image appears (frequency) in the query across all the iterations. The value of W f is the frequency that the image is taken as a query image. 3.2 Two Fusion Approaches In many CBIR applications, it is important to take more than one image as a query and so a fusion approach is needed to merge the results of each query im-age. We used two separate fusion approaches to support two different relevance feedback scenarios: Firstly, the vector space model (VSM) [8] approach was de-ployed for positive query images only. By combining this with the four-factor user interaction model the approach is represented by: where the D is the overall dissimilarity value between a query (containing a number of positive examples) and an object image. It is computed as the sum of the dissimilarity values between all the example images in the query and the object image. D ij is the original distance betw een the feature vectors for the query image i and the object image j . W opf is the weight scheme of the four-factor user interaction mod el (see section 3.1).
 Secondly, because the VSM in [8] only uses positive RF, we applied k-Nearest Neighbours (k-NN) for both positive and negative RF [8]. Here, by taking into account the weighting scheme of our four-factor user interaction model, k-NN is given by: where D is the dissimilarity value between an object image and the example images (positive and negative) in a query.  X  is a small positive number to avoid division by zero. N and P denote the sets of positive and negative images in the query. In our view a appropriate interface is vital to allow our new interaction CBIR framework to fully function, because the interface is the communication plat-form between the system and user. We will outline our developed interface and describe how it underpins the four-factor interaction model.

The search interface (see Figure 1) take s on a simple search-based grid style so that the user does not need to learn the new visual layout before they start a search. Different colour backgrounds have been applied to the different panels which is aimed at supporting user navigation and appreciation of the differences between the panels. Each panel provides a different level of interaction to the user, where some of the four factors are controlled indirectly and others more directly. Table 2 shows how the interface supports each of the four factors (note: the numbers on the table indicate the functionalities on the screen shot). The rest of this section describes the features of those panels.
 4.1 Query Image Browsing Panel (Region 1) The query image panel is a browsing panel. The user browses the query panel and selects one or more images from the provided query images as an initial query image(s) prior to starting the search. 4.2 Positive Query Panel (Region 2) The positive query panel contains images that the user considers are good posi-tive examples of what they are searching for. Users can provide as many images as they want as positive queries. These i mages can be selected from the query im-ages, the search results or a combination of both. Users are also able to eliminate positive examples by simply clicking on them.

After the user selects positive images, the system automatically gives their importance score by their display order. If the user is not happy with the default score, he can re-score the importance of the images by changing the number (integer 1-20, bigger is better) in the text box underneath each image. This functionality delivers the  X  X elevance level X  factor. The intention of the design is to provide users an explicit control to the importance level of query image examples. 4.3 Negative Query Panel (Region 7) The negative query panel has similar functionality to the positive query panel but this time for negative queries. The only difference is that negative examples may only be selected from the previous s earch results. The score of these nega-tive example images indicates the level of non-relevance (integer 1-20, bigger is worse).

In summary, both the positive and negative query panels deliver the  X  X elevance region X  factor, such as relevant and non-relevant region. The score of image ex-amples in both panels indicates the  X  X elev ance level X  factor X  X  scale of relevance and non-relevance. Combining the findings in [9, 10] and [7, 8], our hypothesis is that blending the non-binary relevance level with both positive and negative re-gions will enhance user interaction on the one hand and increase search accuracy on the other. 4.4 Results Panel (Regions 4 and 5) Whereas a common linear d isplay search system may display only the best matching results, our system displays both the best and poorest matches. In our view this added functionality allows users to gain a better understanding of the data set they are searching. By seei ng both good and bad results, the user can gain better understanding of the data they are searching. Additionally, for experienced users, the extreme results c an aid their special search purposes, for instance, when a user searches for two ext remely different colour images, say one pink and one blue.

Furthermore, users can indicate positive examples from the good matches and negative examples from the poorest matches by selecting them with a single mouse click. The selected images will appear automatically in either the positive or negative query panels. According to our simulated experimental results, taking the worse matches as negative query examples outperforms the query example from good matches. Therefore, we design ed the interface to support the search mechanism by showing the poorest as well as the best matches. Users will need some training on the way that the interface works. We assume that the users will be able to search naturally after a couple of search iterations although this functionality is not intuitive to start with.

To aid navigation, we have inserted a horizontal line between the good and bad results to clearly divide the two. 4.5 Positive History Panel (Region 8) This is an important feature of our search system. This panel records the user X  X  earlier positive queries used during previous search iterations. This enables the user to go back and reuse a previous que ry if required. This might be needed, for instance, if the user got lost during the search process.

In addition, this panel delivers two important factors to our four-factor user interaction model: Firstly, the  X  X ime X  factor which is computed by the Ostensive Model and takes a search iteration as a time unit. Secondly, the  X  X requency X  factor that judges the importance of an image by reference to how many time theimagewasusedasaquery.

These two factors are fully controlled b y the system, and all previous queries will be taken into account in the final weighting scheme. 4.6 Negative History Panel (Region 9) This panel is similar to the positive history panel but instead records the neg-ative queries selected from each search iteration. The negative query history is introduced together with the negative query as two of the new features of our search interface. The introduction of query history functionality has been en-couraged in [2,12] and we would like to investigate the effects on user interaction and search accuracy by adding the negative factor. 4.7 Summary In summary, the key features of the proposed interface are: (1) Users can provide both positive and negative examples to a search query, and further expand or reformulate the query. This is a way to deliver the  X  X ele-vance region X  factor. (2) By allowing the user to override the automatically generated score of positive and negative query images, we are enabling the user to directly influence the importance level of the feedback. The  X  X  elevance level X  factor is generated by the score functionality. (3) The display of the results in the interface takes a search-based linear display format but with the addition of showing not only the best matches but also the worst matches. This functionality aims to enable users to control the model directly in a natural way. (4) The query history not only provides users with the ability to reuse their previous queries, but also enables them to expand future search queries by taking previous queries into account. The positive and negative history panels together with the current query feed the  X  X ime X  and  X  X requency X  factor of our four-factor user interaction model. The goal of our evaluation is testing whether users can adapt to the new search strategy easily and whether users find the uInteract system useful. 5.1 Evaluation Baseline The baseline systems we used to compar e with the uInteract system are: base-line1 -a typical Relevance Feedback (R F) mechanism, where users are allowed to give positive RF from search results through a simplified interface; baseline2 -a system based on Urban, et al. model [12] provides positive query history functionality which is in addition to baseline1; baseline3 -a system based on Ruthven, et al. model [9] enhances baseline2 by adding partial relevance (we call it importance score in this paper) functionality. 5.2 Evaluation Setup A total of 17 subjects, who are a mixture of males and females, undergraduate and postgraduate students, and academic staff from a variety of departments with different age and levels of image se arch experience, participated in the evaluation. Subjects can be classified into two groups -younger/older and inex-perienced/experienced -b ased on age and image search experience respectively. We take 26 as the age cut off point. Everybody older then 26 is in the older group, otherwise they are in the younger group. We consider that people are experienced users if they search images a t least once a week, and otherwise they are inexperienced users.

The subjects attempted four different c omplexity level search tasks on the four systems randomly in a random order (limited five minutes for each task) and provided feedback on their experience through questionnaires and comments made during informal discussions. The complexity of the tasks is based on the task description. Task one provides both search topic and example images, so we consider it is the easiest task. Task two gives example images without a topic description. Task three has only a topic but no image examples. Task four described a broad search scenario without any clear topic and image examples, so it is the most complex task in our view. The questionnaires used five point Likert scales. The questions are about the general feeling, ease of use, novelty, search result, search performance an d satisfaction wit h the systems, etc. 5.3 Evaluation Results The following preliminary results were obtained based on the ANOVA analysis (with  X  =0 . 05) of questionnaires and informal feedbacks from the subjects. (1) The uInteract system gain higher novelty score than the baseline1 (p-value  X  0.05). The search result of uInteract showed better result than base-line1 (p-value  X  0.05). This result is promising, which encouraged us to do further detailed analysis on other data we obtained from the evaluation, such as, videos and actual image search results. (2) Task factor had very strong impact on users X  responses to most of the questions. Most of the subjects thought that task two and task four, and task three and task four were significantly different (p-value  X  0.05). One interesting observation is that the subjects liked to give higher scores to the questions when they performed easier tasks. Due to the tasks strong impact on the results, the score of different systems will become less sensitive. (3) The person factor is another important factor which affects some of the question results. Different subjects s eemed to have very different opinions on scoring the questions, but what is the trend on this factor? In an effort to find the trend, we took the age and image search experience into account. The sta-tistical results showed that older su bjects and experienced subjects feel more comfortable with using the systems th an younger and inexperienced subjects. (4) The older subjects thought the systems helped them better understand the quality of the results they could get from the collection data more than younger subjects, and the score is estimated to in crease by 0.05 per year as age increases. There is a clear evidence that age and i mage search experience affect system satisfaction, and the score is estimate d to be higher for older and experienced users. There is also clear evidence th at age and image search experience affect feeling in control using the system and s earch result satisfaction, but the score is estimated to be higher for yo unger and inexperienced users. (5) The image search experience of the subjects also impacted on the opin-ions of query history usefulness, negative query ease of use and negative result usefulness. The experienced subjects gav e higher score on these questions than inexperienced subjects (p-value  X  0.05). (6) While the subjects though the uInterace framework was useful, they felt the ease of use of the functionalities can be improved by making the negative result optional, so that subjects can see and use it when they need to do, and using drag and drop to higher or lower position to indicate the importance of the query images in the query panel, enabling them to show the images in every query when the mouse is over to the query history labels, etc. This work was undertaken to achieve three objectives: (a) to deliver an effective interactive CBIR framework, in particular through a novel four-factor user in-teraction model, (b) to design the interaction activities of the interface to enable users to directly control the model in a natural way, and (c) to test the easy of use and usefulness of the new search functionalities through a use study.
The uInterace framework was built on our observations about the insufficien-cies of previous research and inspirations from the literature. In order to evaluate the interactive framework, we have done a user evaluation on the ease of use and usefulness of the framework. The preliminary evaluation result showed, while the framework was considered as easy of use and useful in general, there are still places to improve and more data to analyze.

Our next step is to extract some factor s from the captured video of the evalu-ation, such as, how many queries did the subjects use to complete the tasks, how many pages did the subjects look through the results, etc. Furthermore, we will also look into the image result data the s ubjects selected for completing each tasks, and analyze the difference on th e results of the evaluated systems. This work was partially supported by the PHAROS project sponsored by the Commission of the European Communities as part of the Information Society Technologies programme under grant number IST-FP6-45035, and AutoAdapt project funded by the UK X  X  Engineering and Physical Sciences Research Council, grant number EP/F035705/1.
