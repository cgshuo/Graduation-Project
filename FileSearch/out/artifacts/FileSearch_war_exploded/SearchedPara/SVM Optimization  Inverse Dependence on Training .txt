 Shai Shalev-Shwartz SHAI @ TTI -C . ORG The traditional runtime analysis of training Support Vec-tor Machines (SVMs), and indeed most runtime analysis of training learning methods, shows how the training runtime increases as the training set size increases. This is because the analysis views SVM training as an optimization prob-lem, whose size increases as the training size increases, an d asks  X  X hat is the runtime of finding a very accurate solution to the SVM training optimization problem? X . However, this analysis ignores the underlying goal of SVM training, which is to find a classifier with low generalization error. When our goal is to obtain a good predictor, having more training data at our disposal should not increase the run-time required to get some desired generalization error: If we can get a predictor with a generalization error of 5% in an hour using a thousand examples, then given ten thou-sand examples we can always ignore nine thousand of them and do exactly what we did before, using the same runtime. But, can we use the extra nine thousand examples to get a predictor with a generalization error of 5% in less time? In this paper we begin answering the above question. But first we analyze the runtime of various SVM optimization approaches in the data-laden regime, i.e. given unlimited amounts of data. This serves as a basis to our investigation and helps us compare different optimization approaches when working with very large data sets. A similar type of analysis for unregularized linear learning was recently presented by Bottou and Bousquet (2008) X  X ere we han-dle the more practically relevant case of SVMs, although we focus on linear kernels.
 We then return to the finite-data scenario and ask our origi-nal question: How does the runtime required in order to get some desired generalization error change with the amount of available data? In Section 5, we present both a theoreti-cal analysis and a thorough empirical study demonstrating that, at least for linear kernels, the runtime of the subgra-dient descent optimizer PEGASOS (Shalev-Shwartz et al., 2007) does indeed decrease as more data is made available. We briefly introduce the SVM setting and the notation used in this paper, and survey the standard runtime analysis of several optimization approaches. The goal of SVM train-ing is to find a linear predictor w that predicts the label y  X   X  1 associated with a feature vector x as sign(  X  w , x  X  ) This is done by seeking a predictor with small empirical (hinge) loss relative to a large classification  X  X argin X . We assume that instance-label pairs come from some source distribution P ( X , Y ) , and that we are given access to la-beled examples { ( x ing a SVM then amounts to minimizing, for some regular-ization parameter  X  , the regularized empirical hinge loss: where  X  ` ( w ) = 1 max { 0 , 1  X  y  X  w , x  X  X  is the hinge loss. For simplicity, we do not allow a bias term. We say that an optimization method finds an -accurate solution  X  w if  X  f Instead of being provided with the feature vectors di-rectly, we are often only provided with their inner products through a kernel function. Our focus here is on  X  X inear ker-nels X , i.e. we assume we are indeed provided with the fea-ture vectors themselves. This scenario is natural in severa l applications, including document analysis where the bag-of-words vectors provide a sparse high dimensional repre-sentation that does not necessarily benefit from the kernel trick. We use d to denote the dimensionality of the feature vectors. Or, if the feature vectors are sparse, we use d denote the average number of non-zero elements in each feature vector (e.g. when input vectors are bag-of-words, is the average number of words in a document).
 The runtime of SVM training is usually analyzed as the required runtime to obtain an -accurate solution to the op-timization problem min Traditional optimization approaches converge linearly, o r even quadratically, to the optimal solution. That is, their runtime has a logarithmic, or double logarithmic, depen-dence on the optimization accuracy . However, they scale poorly with the size of the training set. For example, a na  X   X ve implementation of interior point search on the dual of the SVM problem would require a runtime of  X ( m per iteration, with the number of iterations also theoreti-cally increasing with m . To avoid a cubic dependence on m , many modern SVM solvers use  X  X ecomposition tech-niques X : Only a subset of the dual variables is updated at each iteration (Platt, 1998; Joachims, 1998). It is possi-ble to establish linear convergence for specific decompo-sition methods (e.g. Lin, 2002). However, a careful ex-amination of this analysis reveals that the number of itera-tions before the linearly convergent stage can grow as m 2 . In fact, Bottou and Lin (2007) argue that any method that solves the dual problem very accurately might in general require runtime  X ( dm 2 ) , and also provide empirical ev-idence suggesting that modern dual-decomposition meth-ods come close to a runtime of  X ( dm 2 log(1 / )) . There-fore, for the purpose of comparison, we take the runtime of dual-decomposition methods as O ( dm 2 log 1 / ) . With the growing importance of handling very large data sets, optimization methods with a more moderate scaling on the data set size were presented. The flip side is that these approaches typically have much worse dependence on the optimization accuracy. A recent example is SVM-Perf (Joachims, 2006), an optimization method that uses a cutting planes approach for training linear SVMs. Smola et al. (2008) showed that SVM-Perf can find a solution with accuracy in time O ( md/ (  X  )) .
 Although SVM-Perf does have a much more favorable de-pendence on the data set size, and runs much faster on large data sets, its runtime still increases (linearly) wit h m . More recently, Shalev-Shwartz et al. (2007) presented PEGASOS, a simple stochastic subgradient optimizer for training linear SVMs, whose runtime does not at all in-crease with the sample size. PEGASOS is guaranteed to  X  O ( d/ (  X  )) . Empirical comparisons show that PEGASOS is considerably faster than both SVM-Perf and dual decom-position methods on large data sets with sparse, linear, ker -nels (Shalev-Shwartz et al., 2007; Bottou, Web Page). These runtime guarantees of SVM-Perf and PEGASOS are not comparable with those of traditional approaches: the runtimes scale better with m , but worse with , and also depend on  X  . We will return to this issue in Section 4. The goal of supervised learning, in the context we consider it, is to use the available training data in order to obtain a predictor with low generalization error (expected error ov er future predictions). However, since we cannot directly ob-serve the generalization error of a predictor, the training er-ror is used as a surrogate. But in order for the training error to be a good surrogate for the generalization error, we must restrict the space of allowed predictors. This can be done by restricting ourselves to a certain hypothesis class, or i n the SVM formulation studied here, minimizing a combina-tion of the training error and some regularization term. In studying the generalization error of the predictor mini-mizing the training error on a limited hypothesis class, it i s standard to decompose this error into:  X  The approximation error  X  the minimum general- X  The estimation error  X  X he difference between the ap-A similar decomposition is also possible for the somewhat more subtle case of regularized training error minimiza-tion, as in SVMs. We are now interested in the generaliza-tion error ` (  X  w ) = E  X  w = arg min w  X  f  X  ( w ) minimizing the training objective (1). Note that for the time being we are only concerned with the (hinge) loss, and not with the misclassification er-ror, and even measure the generalization error in terms of the hinge loss. We will return to this issue in Section 5.2.  X  The approximation error is now the generaliza- X  The estimation error is now the difference between the The error decompositions discussed so far are well under-stood, as is the trade-off between the approximation and estimation errors controlled by the complexity of the hy-pothesis class. In practice, however, we do not minimize the training objective exactly and so do not use the math-ematically defined  X  w . Rather, we use some optimization algorithm that runs for some finite time and yields a pre-dictor  X  w that only minimizes the training objective  X  f to within some accuracy acc . We should therefore con-sider the decomposition of the generalization error ` (  X  w ) this predictor. In addition to the two error terms discussed above, a third error term now enters the picture:  X  The optimization error is the difference in general-This more complete error decomposition, also depicted in Figure 1, was recently discussed by Bottou and Bousquet (2008). Since the end goal of optimizing the training er-ror is to obtain a predictor  X  w with low generalization error ` (  X  w ) , it is useful to consider the entire error decomposition, and the interplay of its different components.
 Before investigating the balance between the data set size and runtime required to obtain a desired generalization er-ror, we first consider two extreme regimes: one in which only a limited training set is available, but computational resources are not a concern, and the other in which the training data available is virtually unlimited, but compu-tational resources are bounded. 3.1. The Data-Bounded Regime The standard analysis of statistical learning theory can be viewed as an analysis of an extreme regime in which train-ing data is scarce, and computational resources are plenti-ful. In this regime, the optimization error diminishes, as w e can spend the time required to optimize the training objec-tive very accurately. We need only consider the approxi-mation and estimation errors. Such an analysis provides an understanding of the sample complexity as a function of the target error: how many samples are necessary to guarantee some desired error level.
 For low-norm (large-margin) linear predictors, the esti-mation error can be bounded by O k w  X  k  X  Mendelson, 2003), yielding a sample complexity of m = O k w  X  k 2 2 to get a desired generalization error of ` ( w (tighter bounds are possible under certain conditions, but for simplicity and more general applicability, here we stic k with this simpler analysis). 3.2. The Data-Laden Regime Another extreme regime is the regime in which we have vir-tually unlimited data (we can obtain samples on-demand), but computational resources are limited. This is captured by the PAC framework (Valiant, 1984), in which we are given unlimited, on-demand, access to samples, and con-sider computationally tractable methods for obtaining a predictor with low generalization error. Most work in the PAC framework focuses on the distinction between poly-nomial and super-polynomial computation. Here, we are interested in understating the details of this polynomial dependence X  X ow does the runtime scale with the parame-ters of interest? Discussing runtime as a function of data se t size is inappropriate here, since the data set size is unlim-ited. Rather, we are interested in understanding the runtim e as a function of the target error: How much runtime is re-quired to guarantee some desired error level.
 As the data-laden regime does capture many large data set situations, in which data is virtually unlimited, such an analysis can be helpful in comparing different optimizatio n approaches. We saw how traditional runtime guarantees of different approaches are sometimes seemingly incom-parable: One guarantee might scale poorly with the sample size, while another scales poorly with the desired optimiza -tion accuracy. The analysis we perform here allows us to compare such guarantees and helps us understand which methods are appropriate for large data sets.
 Recently, Bottou and Bousquet (2008) carried out such a  X  X ata-laden X  analysis for unregularized learning of linea r separators in low dimensions. Here, we perform a similar type of analysis for SVMs, i.e. regularized learning of a linear separator in high dimensions. To gain insight into SVM learning in the data-laden regime we perform the following  X  X racle X  analysis: We assume there is some good low-norm predictor w a generalization error (expected hinge loss) of ` ( w has norm k w ing objective  X  f Since we have access to an unrestricted amount of data, we can choose what data set size to work with in order to achieve the lowest possible runtime.
 We will decompose the generalization error of the output predictor  X  w as follows: The degradation in the regularized generalization error, f (  X  w )  X  f  X  ( w  X  ) , which appears in the second term, can be bounded by the empirical degradation: For all w with k w k 2  X  2 / X  (a larger norm would yield a worse SVM ob-jective than w =0 , and so can be disqualified), with proba-bility at least 1  X   X  over the training set (Sridharan, 2008): f  X  ( w )  X  f  X  ( w  X  )  X  2 h  X  f  X  ( w )  X   X  f  X  ( w  X  ) where [ z ] accurate minimizer of  X  f Returning to the decomposition (2), the third term is non-positive due to the optimality of w  X  , and regarding  X  as a constant we obtain that with arbitrary fixed probability: In order to obtain an upper bound of ` ( w the generalization error ` (  X  w ) , each of the three remaining terms on the right hand side of (4) must be bounded from above by O ( ) , yielding: Using the above requirements on the optimization accuracy acc , the regularization parameter  X  and the working sam-ple size m , we can revisit the runtime of the various SVM optimization approaches.
 As discussed in Section 2, dual decomposition approaches require runtime  X ( m 2 d ) , with a very weak dependence on the optimization accuracy. Substituting in the sample size required for obtaining the target generalization erro r of ` ( w 0 ) + , we get a runtime of  X  d k w 0 k 4 4 . We can perform a similar analysis for SVM-Perf by substi-tuting the requirements on acc ,  X  and m into its guaranteed runtime of O dm matching that in the analysis of dual decomposition meth-ods above. It should be noted that SVM-Perf X  X  runtime has been reported to have only a logarithmic dependence on 1 / acc in practice (Smola et al., 2008). If that were the case, the runtime guarantee would drop to  X  O d k w 0 k 4 explaining the faster runtime of SVM-Perf on large data sets in practice.
 As for the stochastic gradient optimizer PEGASOS, sub-stituting in the requirements on acc and  X  into its  X  O ( d/ (  X  acc )) runtime guarantee yields a data-laden run-regime, where we can choose a data set of arbitrary size in order to obtain some target generalization error, the runti me guarantee of PEGASOS dominates those of other methods, including those with a much more favorable dependence on the optimization accuracy.
 The traditional and data-laden runtimes, ignoring logarit h-mic factors, are summarized in the following table:
Method acc -accurate ` (  X  w )  X  ` ( w We have so far considered two extreme regimes: one in which learning is bounded only by available data, but not by computational resources, and another where it is bounded only by computational resources, but unlimited data is available. These two analyzes tell us how many samples are needed in order to guarantee some target er-ror rate (regardless of computational resources), and how much computation is needed to guarantee this target error rate (regardless of available data). However, if we have jus t enough samples to allow a certain error guarantee, the run-time needed in order to obtain such an error rate might be much higher than the runtime given unlimited samples. In terms of the error decomposition, the approximation and estimation errors together would already account for the target error rate, requiring the optimization error to be ex -tremely small. Only when more and more samples are available might the required runtime decrease down to that obtained in the data-laden regime.
 Accordingly, we study the runtime of a training method as a decreasing function of the available training set size. As a r-gued earlier, studied this way, the required runtime should never increase as more data is available. We would like to understand how the excess data can be used to decrease the runtime.
 In many optimization methods, including dual decompo-sition methods and SVM-Perf discussed earlier, the com-putational cost of each basic step increases, sometimes sharply, with the size of the data set considered. In such algorithms, increasing the working data set size in the hope of being able to optimize to within a lower optimization ac-curacy is a double-edged sword. Although we can reduce the required optimization accuracy, and doing so reduces the required runtime, we also increase the computational cost of each basic step, which sharply increases the run-time.
 However, in the case of a stochastic gradient descent ap-proach, the runtime to get some desired optimization ac-curacy does not increase as the sample size increases. In this case, increasing the sample size is a pure win: The desired optimization accuracy decreases, with no counter effect, yielding a net decrease in the runtime.
 In the following sections, we present a detailed theoreti-cal analysis based on performance guarantees, as well as an empirical investigation, demonstrating a decrease in PE -GASOS runtime as more data is available. 5.1. Theoretical Analysis Returning to the  X  X racle X  analysis of Section 4 and substi-tuting into equation (4) our bound on the optimization ac-curacy of PEGASOS after running for time T , we obtain: ` (  X  w )  X  ` ( w 0 ) +  X  O ( The above bound is minimized when  X  =  X   X ( q d yielding ` (  X  w )  X  ` ( w Inverting the above expression, we get the following bound on the runtime required to attain generalization error ` (  X  w )  X  ` ( w 0 ) + using a training set of size m : This runtime analysis, which monotonically decreases with the available data set size, is depicted in the bottom panel of Figure 2. The data-bounded (statistical learning the-ory) analysis describes the vertical asymptote of T (  X  ; ) what sample size is it at all possible to achieve the desired error. The analysis of the data-laden regime of Section 4 described the minimal runtime using any amount of data, and thus specifies the horizontal asymptote inf T ( m ; ) = lim m  X  X  X  T ( m ; ) . The more detailed analysis carried out here bridges between these two extreme regimes.
 Before moving on to empirically observing this behavior, let us contrast this behavior with that displayed by learn-ing methods whose runtime required for obtaining a fixed optimization accuracy does increase with data set size. We can repeat the analysis above, replacing the first term on the right hand side of (8) with the guarantee on the optimiza-tion accuracy at runtime of T , for different algorithms. For SVM-Perf, we have acc  X  O ( dm/ (  X T )) . The opti-mal choice of  X  is then  X  =  X  q dm time needed to guarantee generalization error ` ( w when running SVM-Perf on m samples is T ( m ; ) = guarantee is depicted in the middle panel of Figure 2. As the sample size increases beyond the statistical limit m  X ( k w 0 k 2 / 2 ) , the runtime indeed decreases sharply, un-til it reaches a minimum, corresponding to the data laden bound, precisely at 4 m times larger than the minimum required to be able to reach the desired target generalization error. Beyond this point , the other edge of the sword comes into play, and the run-time (according to the performance guarantees) increases as more samples are included.
 The behavior of a dual decomposition method with runtime  X ( m 2 d log 1  X   X  m )) and depicted in the top panel of Figure 2. Here, the optimal sample size is extremely close to the statistica l limit, and increasing the sample size beyond the minimum increases the runtime quadratically. 5.2. Empirical Analysis The above analysis is based on upper bounds, and is only descriptive, in that it ignores various constants and even certain logarithmic factors. We now show that this type of behavior can be observed empirically for the stochastic subgradient optimizer PEGASOS.
 taken from two large data sets, the Reuters CCAT and the of the learned predictor on a (fixed) held-out test set. For each training set size, we found the median number of it-erations (over multiple runs with multiple training sets) f or achieving some target average hinge loss, which was very slightly above the best  X  X est X  hinge loss that could be re-liably obtained by training on the entire available train-ing set. For each training set size we used the optimal  X  dian) required number of iterations is displayed in Figure 3. For easier interpretability and reproducibility, we rep ort the number of iterations. Since each PEGASOS iteration takes constant time, the actual runtime is proportional to the number of iterations.
 So far we have measured the generalization error only in terms of the average hinge loss ` (  X  w ) . However, our true goal is usually to attain low misclassification error, P ( Y 6 = (median) number of iterations required to achieve a target misclassification error, which again is very slightly above the best that can be hoped for with the entire data set. These empirical results demonstrate that the runtime of SVM training using PEGASOS indeed decreases as the size of the training set increases. It is important to note that PEGASOS is the fastest published method for these datasets (Shalev-Shwartz et al., 2007; Bottou, Web Page), and so we are indeed investigating the best possible run-times. To gain an appreciation of this, as well as to ob-serve the runtime dependence on the training set size for other methods, we repeated a limited version of the experi-ments using SVM-Perf and the dual decomposition method SVM-Light (Joachims, 1998). Figure 4 and its caption re-port the runtimes required by SVM-Perf and SVM-Light to achieve the same fixed misclassification error using vary-ing data set sizes. We can indeed verify that PEGASOS X  X  runtime is significantly lower than the optimal SVM-Perf and SVM-Light runtimes on the CCAT dataset. On the CoverType data set, PEGASOS and SVM-Perf have sim-ilar optimal runtimes (both optimal runtimes were under a second, and depending on the machine used, each method was up to 50% faster or slower than the other), while SVM-Light X  X  runtime is significantly higher (about 7 seconds). We also clearly see the increase in runtime for large train-ing set sizes for both SVM-Light and SVM-Perf. On the CoverType dataset, we were able to experimentally observe the initial decrease in SVM-Perf runtime, when we are just past the statistical limit, and up to some optimal training set size. On CCAT, and on both data sets for SVM-Light, the optimal data set size is the minimal size statistically r e-quired and any increase in data set size increases runtime (since the theoretical analysis is just an upper bound, it is possible that there is no initial decrease, or that it is very narrow and hard to detect experimentally).
 In order to gain a better understanding of the reduction in PEGASOS X  X  runtime, we show in Figure 5 the average (over multiple training sets) generalization error achiev ed by PEGASOS over time, for various data set sizes. It should not be surprising that the generalization error de-creases with the number of iterations, nor that it is lower for larger data sets. The important observation is that for smaller data sets the error decreases more slowly, even be-fore the statistical limit for that data set is reached, as op -posed to the hypothetical behavior depicted in the insert of Figure 5. This can also be seen in the dotted plots of Figure 3, which are essentially contour lines of the generalizatio n error as a function of runtime and training set size X  X he error decreases when either runtime or training set size in-crease. And so, fixing the error, we can trade off between the runtime and data set size, decreasing one of them when the other is increased.
 The hypothetical situation depicted in the insert occurs when runtime and dataset size each limit the attainable er-ror independently. This corresponds to  X  X  X -shaped con-tours: both a minimum runtime and a minimum dataset size are required to attain each error level, and once both requirements are met, the error is attainable. In such a situation, the runtime does not decrease as data set size increases, but rather, as in the  X  X  X -shaped graph, remains constant once the statistical limit is passed. This happens , e.g., if the optimization can be carried out with a single pas s over the data (or at least, if one pass is enough for getting very close to ` (  X  w ) ). Although behavior such as this has been reported using second-order stochastic gradient de-scent for unregularized linear learning (Bottou &amp; LeCun, 2004), this is not the case here. Unfortunately we are not aware of an efficient one-pass optimizer for SVMs. We suggest here a new way of studying and understanding the runtime of training: Instead of viewing additional trai n-ing data as a computational burden, we view it as an asset that can be used to our benefit. We already have a fairly good understanding, backed by substantial theory, on how additional training data can be used to lower the general-ization error of a learned predictor. Here, we consider the situation in which we are satisfied with the error, and study how additional data can be used to decrease training run-time. To do so, we study runtime as an explicit function of the acceptable predictive performance.
 Specifically, we show that a state-of-the-art stochastic gr a-dient descent optimizer, PEGASOS, indeed requires train-ing runtime that monotonically decreases as a function of the sample size. We show this both theoretically, by analyz-ing the behavior of upper bounds on the runtime, and em-pirically on two standard datasets where PEGASOS is the fastest known SVM optimizer. To the best of our knowl-edge, this is the first demonstration of a SVM optimizer that displays this natural behavior.
 The reason PEGASOS X  X  runtime decreases with increased data is that its runtime to get a fixed optimization accuracy does not depend on the training set size. This enables us to leverage a decreased estimation error, without paying a computational penalty for working with more data.
 The theoretical analysis presented in Section 5.1, and we believe also the empirical reduction in PEGASOS X  X  run-time, indeed relies on this decrease in estimation error. Th is decrease is significant close to the statistical limit on the sample size, as is evident in the results of Figure 3 X  X  roughly 10 X 20% increase in sample size reduces the run-time by about a factor of five. However, the decrease di-minishes for larger sample sizes. This can also be seen from the theoretical analysis X  X aving a sample size which is greater than the statistical limit by a constant factor en -ables us to achieve a runtime which is greater than the the-oretical (data-laden) limit by a constant factor (in fact, a s the careful reader probably noticed, since our data-laden theoretical analysis ignores constant factors on and it seems that the training set size needed to be within the data-laden regime, as specified in equation (7), is the same as the minimum data set size required statistically). Such  X  X onstant factor X  effects should not be discounted X  X aving four times as much data (as is roughly the factor for Cover-Type) is often quite desirable, as is reducing the runtime by a factor of ten (as this four-fold increase achieves). We are looking forward to seeing methods that more ex-plicitly leverage large data sets in order to reduce runtime , achieving stronger decreases in practice, and being able to better leverage very large data sets. Although it seems that not much better can be done theoretically given only the simple oracle assumption of Section 4, a better theoretical analysis of such methods might be possible using richer as-sumptions. We would also like to see practical methods for non-linear (kernelized) SVMs that display similar be-havior. Beyond SVMs, we believe that many other prob-lems in machine learning, usually studied computationally as optimization problems, can and should be studied using the type of analysis presented here.

