 In this paper, we describe a system for the construction of taxonomies which yield high accuracies with automated categorization systems, even on Web and intranet documents. In particular, we describe the way in which measurement of five key features of the system can be used to predict when categories are sufficiently well defined to yield high accuracy categorization. We describe the use of this system to construct a large (8800-category) general-purpose taxonomy and categorization system. I.2.7 [ Artificial Intelligence ]: Natural language Processing X  Text Analysis ; H.3.3 [ Information Storage And Retrieval ]: Information Search and Retrieval X  Information filtering Experimentation Taxonomy development, quality measurements, clustering for document selection, text categorization In recent years taxonomies have become an important factor in organizing knowledge for both the web and corporate intranets [3,4,6,8,13]. For a successful implementation of the taxonomies within any large-scale application the most important question is how to label large amounts of data with high accuracy according to the taxonomy. Manual tagging is often considered the method of choice. However, the cost of manually tagging documents can be estimated to be at least $4 per document, assuming a trained librarian can be expected to tag 30-50 documents per day and an hourly rate of $40 / hour [14,15]. Thus even a moderate-sized enterprise with a few hundred thousand documents to tag may find itself spending more than $1M for manual tagging of such a set of documents. This cost can be multiplied by the number of times that each document has to be retagged as the taxonomy changes, and by the number of facets (taxonomies) to which each document must be tagged, so that the real cost may be much higher. Some organizations choose to do this manual tagging at the time documents are authored. Although this may sufficiently distribute the cost so that it is less visible, it actually may increase the overall cost of tagging compared to centralized tagging by librarians or similar workers. This is because of two factors: 1) the authors are less familiar with the taxonomy and therefore more prone to categorize inaccurately, leading to increased need for others to check the tagging, and 2) because of what happens when a taxonomy used by the authors changes after the documents are initially tagged. In many cases this may require a fairly elaborate and expensive system for routing documents back to authors, or to back-up individuals if the author is not available. So for all practical purposes manual tagging is not a viable option for large document collections. A further complications is that manual tagging accuracy drops as the size of the taxonomy increases. However, corporate intranets are notoriously complex, often containing documents that are not related to the company focus, such as HR documents, travel guidelines and resumes, just to name a few, and the corporate taxonomies must encompass this complexity. The same often applies to pages found on corporate websites. As an illustration, the website of IBM X  X  Almaden Research Center contains pages of how to behave if you meet a mountain lion. Automated classification systems are an obvious solution to this problem. Unfortunately, the difficulty with automated systems of classification is that they often yield accuracies much less than those of manual (human) systems, particularly when applied to heterogeneous document collections of lower quality such as those found on the Web or in corporate intranets. Thus, several years ago our group started an effort to create a large-scale categorization system that would yield near-human accuracies even on Web and intranet documents and against very large taxonomies. Unlike other efforts that have focused on building more sophisticated machine-based classifiers, the focus of our effort has been to achieve high classification accuracies by focusing on the construction of high-performance taxonomies. In particular, we have found that we can achieve near-human accuracies by using very careful targeting of key measurements that can be used as guidelines in creating categories that perform extremely well with machine-based classifiers. The remainder of the paper is organized as follows. In section 2 we describe the system in more detail and the process we used to ensure a constantly improving development of the categorization system. In section 3 we present the experiences we had during system development. Section 4 describes applications for this system. Section 5 contains the discussion of the results and lessons learned over the past few years with this system. The system contains several components that are focused on different parts of the overall task. A Taxonomy Workbench is the core of the overall system, manipulating the taxonomy structure itself and the information associated with the taxonomy nodes. Each change in the taxonomy via the Taxonomy Workbench GUI may trigger several processes. The purpose of the Taxonomy Workbench is to select an optimal set of training data for a given category and to extract from that data a model to represent that category. The other major component is a standalone Categorizer that uses the category models created by the Taxonomy Workbench to classify user data. The Categorizer deployment takes place as a UIMA annotator [9], this allows easy integration into a large variety of applications. Due to the central role that taxonomy creation has in our system, our group consists of librarians as well as computer scientists. The technical focus is on providing a system that maximizes the effectiveness of the librarians X  input while minimizing the work they must do. Numerous commercial and research systems for taxonomy creation and automated categorization exist. Most current approaches for creating taxonomies and the corresponding automated classifiers try to minimize the number of labeled training documents needed by various technologies that either use some sort of bootstrapping or clustering based on a set of documents [1,5,7] or seeding the process based on some examples that are (semi-)automatically extended [12,19]. Our approach focuses on using document sets retrieved based on search engine queries defined per category. Assuming the query results in a document set that is mostly about the intended topic we can use clustering algorithms to select just those documents that are on the desired topic [2]. Many other approaches use customer data, rather than Web data, for training purposes, and few have any measures of how  X  X ood X  a particular user-constructed taxonomy or set of training data is for auto-categorization purposes. It is difficult, however, to compare results of these systems, because of the difficulty in comparing the results with one taxonomy to those with another, and the lack of published results on these systems with any standard set of data approximating the data available in a typical intranet. It is interesting to note, however, that in our experience, data for some categories when we use the entire Web containing billions of documents; hence it would appear likely that systems that use corporate intranet data for training purposes will inevitably have much more severe problems finding high quality training data. The Taxonomy Workbench consists of a category constructing component, a Taxonomy Manager, a build-daemon component, and a measurement system. The category constructing component in turn consists of 4 sub-components: 1) a training data collector, 2) a per-category filtering sub-component, 3) a sub-component that eliminates overlap among categories within a collection of related categories (a  X  X upercategory X ), and 4) a sub-component that creates models for supercategories and categories. The Taxonomy Manager component allows taxonomists to manage the building of the taxonomy: 1) to add/delete/move/modify interior nodes and leaf nodes; 2) to compose/submit queries to the taxonomy constructing component for training data collection and constructing categories; and 3) to receive feedback about the categories from the measurement component. The system also gives the librarian hints about query construction, as needed. The build daemon component ensures 1) that categories added, modified or deleted in the English-language Workbench system will be reflected in other languages (our system also constructs categories for German, French and Italian); and 2) that the models for all categories and supercategories are always up to date, reflecting latest changes in individual categories. We have found using supercategories of 6 to 350 categories allows us to divide the computationally-intensive portions of the processing into much more manageable portions and significantly accelerates the processing. One of the key design decisions with the Workbench was to use external (Web), rather than company-supplied training data. This has two primary advantages: 1) it vastly increases the supply of potential training data, so the system can be much more selective in picking training data, and 2) it yields training data that is much more general, and hence reusable in various deployments. Thus, to create a new leaf-node category, the librarian first uses the Taxonomy Manager to composes a query (see Figure 1), in the target language (English, normally), which will be submitted to various Web search engines by the Training Data Collector. The Training Data Collector then automatically retrieves up to several thousand of the candidate training data described in the hit lists for a given category. Once retrieved locally, this set of  X  X aw X  training data will go through the step using the  X  X iltering X  subsystem, in which training documents are selected that are as much on the single desired topic as possible and that are as representative of the category as possible while keeping the needed variety to properly train the category. Categories are automatically assigned to one or more of a set of manually-created supercategories based on simple distance measures; for example, a category on  X  X dge Computing X  is assigned to a supercategory called  X  X ervers. X  After the various filters are applied the documents are used to train the classifier by selecting the features that best differentiate each category to create a model for that category. The resulting models are then ready for the categorizer deployment. After the automated processing by the Workbench has completed, the librarian can evaluate whether a category is successful by examining the feedback information provided by the measurement subsystem at various points of the process, as described below. If the category is successful, the librarian can move on to next new category. Otherwise, the librarian will need to modify the query and restart the training data collection/selection process. Figure 2 shows this iterative process and Figure 3 shows a sample report on the five key measurements discussed in Section 2.6. In this report, each of the five key measurements are judged on a pass/fail basis, and weighted 20 points each. A total score is then assigned to each category, with 100 points indicating a category that has passed all five key measurements. The current version of the taxonomy contains approximately 8800 unique categories (leaf nodes) and about 2200 interior categories. The overall taxonomy size is about 16,000 nodes; this includes categories (or even complete subtrees) that occur in multiple locations in the tree. Because the structure is not used for training the taxonomy system, multiple occurrences of the same category in the tree do not affect the training process. The categorizer only categorizes to the leaf categories. One of the main design points for the taxonomy is to have minimally-overlapping categories. As described below the system automatically checks the categories for overlap and reports feedback about that to the taxonomists who then can adjust the category definitions if needed. The positioning of the categories in the taxonomy is entirely up to the librarians. However, having categories that are minimally overlapping makes construction of the taxonomy itself much easier. Driving the process by humans ensures that the resulting taxonomy is sensible to human users. The intent was to create a system that has a very broad coverage; with the current 8800-category system our group has made substantial progress against that goal. For most projects the taxonomy has to be extended in some areas (e.g. the focus areas of the company related to the project), but by experience this can be done very quickly and without affecting more than a few of the already existing categories. Section 4 will present further details about this. Every category (taxonomy leaf node) is linked to a query that defines the category topic by looking at the web pages that are retrieved by that query (see section 2.5 for details). This makes all the data on the Web available as training data, thus eliminating the sampling bias often occurring when using a limited set of training documents from only one source for training. All of the processing that occurs after the query is select is completely automatic; i.e., we do not allow the taxonomist to hand-select training data, nor to select the features in the model. The update the system later on; you just resubmit the query and get a current set of documents. It also makes the system more scalable by limiting the amount of librarian intervention that is required. However, it should be noted that the effectiveness of query terms changes over time so the librarian must be periodically recheck all categories. It should be noted that one of the initial design decisions for the taxonomy was to exclude the use of proper nouns, including individuals, products and places. We did this for two reasons: 1) proper-named categories often have only a few features differentiating them from other proper-named categories and are thus difficult for a general-purpose subject categorizer to distinguish from one another, and 2) such categories tend to be very specific to a given company or setting, and as such are best constructed in the context of a specific company X  X  needs. Therefore in the current system these are considered as company-specific extensions to the taxonomy. This results in another layer in the tree beneath what is considered the leaf node layer in the taxonomy described up to now. This type of company-specific category is handled by a separate product name categorizer that looks for exact word or phrase matches. Thus, the trained part of the taxonomy contains nodes like Laptop &amp; Notebook Computers , but there can be a company-specific node beneath that such as ThinkPad T42p . The product name categorizer is normally used in conjunction with the general-purpose categorizer; e.g. occurrence of the phrase ThinkPad T42p is used as evidence that the category may be Laptop &amp; Notebook Computers . The only time that we have used proper names for general-purpose categories is where those categories are only described by proper names, e.g., computer operating systems. It is interesting to note that these categories perform in general less well than other general-purpose categories. The system currently uses two types of classifiers: a centroid-based classifier and an exact phrase matcher. The centroid-based classifier utilizes a cosine-based distance measure and operates at very high speeds: the overall categorization speed is about 400,000 documents per hour on a 3GHz desktop class machine. The centroids used are based on feature vectors created from the training documents by applying tokenization and stemming with stop word elimination; these are compared to similar vectors created from the test documents. The current implementation only uses words present in a restricted lexicon; this enables good control over the features used during training. The product name nodes mentioned in section 2.3 are matched with the document content utilizing the phrase matcher. The phrase matcher requires exact match, including capitalization. the user can specify the number of times any given phase must occur in order for the phrase matcher to report that the document matches that category. It is also possible for the librarian to define synonyms for either type of node; these synonyms are handled by the phrase matcher. In addition, any product name nodes or synonyms found are then combined with the categories found by the centroid-based classifier; i.e., synonyms and product name nodes are used as additional evidence for the general-purpose nodes detected by the cosine-based classifier. Due to the large number of categories the centroid-based classifier is designed as a two-stage categorizer; this reduces computational time as only parts of the model have to be compared for categorizing a new document. The first stage of the categorization consists of estimating the general subject area of a document by its distance from any of a list of 160 supercategories and then the second stage consists of choosing the final document label based on the distance of the document vector to the categories that are members of the top n supercategories from the list of matching supercategories. The value for n is dynamically calculated based on the similarity score of the top matches and ranges from 1 to 3. Depending on the value of n, there is roughly a 30-fold increase in speed with the 2-stage categorizer compared to the single-stage categorizer. Any category can be a member of up to 8 supercategories. Typically, we report out only the top general-purpose category but as many product-name nodes as we find; however the reporting is configurable. The training process, illustrated in Figure 4, is controlled by the Workbench system (see section 2.2). This section outlines the steps involved in training. The training assumes the documents are already retrieved based on the query related to the category. These documents are analyzed by a process based on clustering technology [2] to narrow down the original document set to one focused on a single topic. If the query for a specific category is returning a document set that is not mostly about the semantic concept intended for a category then this process will of course fail. There are several tests integrated into the system that try to address these problems and give feedback to the librarian about categories that are likely to be not correctly working (see section 2.6). This step also eliminates duplicate and almost identical documents, deletes very short documents and limits to two the number of documents used from any site, all to avoid a sampling bias. This filtering step is typically run immediately after the document retrieval so that the librarian can have immediate feedback. After this per-category filtering of the document set all the training documents for a supercategory are analyzed for possible overlap between categories in that supercategory. In this subsystem, documents are eliminated if they are roughly equidistant between any pair of categories in the supercategory. This forces the categories to be less overlapping and therefore makes the categorization task easier because the categories are better distinguishable from the algorithmic point of view. This again bears the risk of shifting the topic learned by the system away from the intended topic, the automated tests try to detect this and the system alerts the librarian if this happens . After the final set of training documents is selected the training process creates a model for each category that contains the most relevant (differentiating) words for the category. After creating these the system calculates the most relevant words resulting for each supercategory. The maximum size of the vector finally used for the categories is 200 features; the maximum size of the supercategory vector is 1200 features. Both of these numbers of features have been optimized based on the five key measures described below. The system may decide to drop categories, at least temporarily, if there are not enough training documents surviving the filtering steps. In this case the Workbench will report this to the librarian so that she or he can resolve the problem. The complete training process for 8800 categories takes about one night in the distributed Workbench system. From the initial 8 million retrieved documents about 930,000 are finally used for training, so on average 12% of the data is finally kept. The average number of training documents per category is 105. Early in the development of this system, we considered several methods for measuring the quality of the categories produced. In practice, this proved very difficult, because there is no pre-existing standard for what constitutes a good set of categories. Furthermore, it is clear that there are many interaction effects among the various categories, and these interactions grow much larger as the size of the taxonomy increases. We therefore experimented with several measures, and finally settled on 5 key such measures. These measures were based on our attempt to understand the key factors that are associated with high-accuracy automated categorization. The factors we identified are: 1) having at least a minimum number of training documents in each category to reduce the possibility of one or a few documents unduly influencing the model we built for that category; 2) minimal overlap with  X  X earby X  (related) categories; 3) having categories where the training data were neither too dissimilar nor too similar to one another; 4) having the top features selected to represent each category have some close similarity to the words and phrases used by the librarian to define the category in order to ensure that the derived category was similar to that intended by the librarian, and 5) that the training data from a given category be able to be automatically categorized back to the correct category even in the presence of thousands of other categories in the system. These five factors were translated into the 5 key measures described below. As described the Workbench drives the training process and delivers feedback about non-working categories as detected during the training process. This feedback is mostly generated from a series of automated tests that are applied to every category after the training is completed, as described below. These tests are applied after the iterative category creation process. Failures of any or all of the tests are reported back to the librarian for remediation; however, as noted above, the only remediation possible is for the librarian to change the query defining the category. This test simply checks the number of training documents left in a category after the iterative category creation process. The standard imposed here is that each category must contain at least 25 training documents in order to pass this test. Ten of our 8800 categories currently fail this test. The Overlap Test tries to measure the similarity between two or more categories based on the cosine distance between the centroid vectors for two given categories. The threshold values here depend on the number of categories involved in an overlap: the cosine between two categories must be &gt;0.85, or between 1 category and 2 or more of its neighbors must be &gt; 0.83 in order for a category to fail this test. Only 6 of our 8800 categories currently fail this test The breadth test checks the average cosine value between the category centroid and each of the final set of training documents used. If the value is too small (currently, less than 0.27) the category is considered too broad or inhomogeneous to be correctly trained. If the value is too high (currently larger than 0.60) the documents are too similar and therefore there is not enough variance to adequately train for the topic. Typically the latter situation arises when there are too many documents which are from the same ultimate source (e.g., when the search engines return data from many similar or identical mirror sites). The Top Word Test checks if the words required in the library-created query (excluding stop words and other non-differentiating words) are also among the top 10 words that are reported as the most relevant for the category. Only 5 of our 8800 categories currently fail this test. The Recategorization Test checks the percentage of training documents that actually get categorized back to the same category it was used to train (i.e., is categorized to the correct category among the 8800 in our system). The percentage is expected to be above 60% for the category to pass this test. The average value for the test is 86% correctly re-categorized documents when looking at all categories currently in our system, and 88% for those categories which pass the Recategorization Test. Approximately 5% of the categories in our system currently still However, we believe that it is possible to achieve close to 100% achieved on this test sets an upper limit of the accuracy of categorization achieved with that category on test data, so it is particularly important to fix categories that fail this test. Recategorization Score Percentage of Categories Less than 50% 2% 50.0 up to 60% 3% 60% up to 70% 6% 70% up to 80% 13% 80% up to 90% 26% 90% up to 100% 50% Table 1: Distribution of recategorization scores We then calculate an overall score by a very simple heuristic: each of the 5 tests is scored as 20 points if passed and 0 if failed. As shown in Figure 5, the overall score over the last 2.5 years has risen from 88.5 with 6700 categories to 99.1 with more than 8800 categories. This rise in scores has come in part because we have also used the overall score to judge how best to optimize various parameters in our system. For example, increasing the number of features for supercategories from 200 to 1200 resulted in an increase of 2.1 points in the overall score for the system (November 2003 in Figure 5). Similarly, a decision, phased in over a several-month period, to allow categories to belong to multiple supercategories resulted in another 2-point improvement (mid 2004 in Figure 5). Another part of the system that is not covered by this paper but worth mentioning is the multilingual categorization subsystem which uses machine translation of the training data into other languages to generate training data for those languages. This subsystem includes machine translation software as well as multilingual tokenization, stemming and segmentation to leverage the human effort used to develop the English-language taxonomy . We have found this approach works especially well for languages for which there is high-quality English-to-target-language MT. Currently, we have developed French, German, Italian and Chinese versions of models for the categorizer using this approach. During the development we gained considerable experience on understanding the system performance. The next two sections will give some examples on how the measurement system supports the librarian creating new or improving existing nodes and on some results of empirical evaluations we did during the evaluation. As described earlier the system constantly applies several measurements and reports the results back to the system user enabling him to improve individual categories. Several examples may serve to illustrate the power of using our five key measurements to check whether a category is likely to yield high accuracy and be the topic the librarian intended. However, they also illustrate the challenge to the librarian in finding ways to fix the problems associated with these categories. Categories often fail the Minimal Training Data Test simply because there are not enough pages on the Web about a given topic, or because the librarian needs to discover multiple ways of asking for a given topic. However, sometimes the problem is more subtle. For example, this category relates to documents describing the mammal Minks; it should not be about fur coats from these animals. So an early attempt for the query was: +minks -re -price -coat  X  X ost (We use  X  X e to exclude newsgroup postings, which tend to be of low quality.) The category failed the Minimal Training Data Test: even though there are plenty of data on the Web that satisfy this query, the system could not find a reasonable cluster during the filtering step among the documents returned by the query. The 5 top words by frequency in the centroid for this category were  X  X ink, farm, jasmine, music, fur. X  By looking at the training data, it became apparent that these arose from a mixture of documents about mink farming with those about the bands The Minks and Jasmine Minks that were picked up by the query. The query was therefore revised to: +"Minks" animal -farm OR +"mink" animal  X  X arm With these modifications the top words in the centroid were  X  mink fur fish male mustela X  ; hence, the librarian judged the category to be working correctly with the new query. During the initial phases of category design, categories often fail the Overlap Test. For example, an early version of the query for this category was: +"high density lipoproteins" -ldl OR +"good cholesterol" -intake  X  X dl.
 With this query the category failed the Overlap Test by overlapping with the category low density lipoproteins and in addition also failed the Recategorization Test due to this overlap. In this case, the librarian realized that almost all documents discussing HDL also discuss LDL; hence the solution in this case was to combine the two categories. The final query for the combined category is: +"high density lipoproteins" OR +"low density lipoproteins" OR +"good cholesterol X -intake OR  X  X ad cholesterol X . Often a query fails more than one test; this frequently provides clues about why the query is faulty. In this example, the topic of this category is companies or individuals offering welding contracting. After several attempts by the librarians to improve the query it looked like this: +"Welding Contracting" -"welding equipment" OR +"Welding Contractors" -"welding equipment" OR +"welding contractor" -"welding equipment". This version failed both the Top Words Test (query terms not in the set of most relevant words) and the Breadth Test (documents too similar). The most relevant words reported were these:  X  contractor, business, glance, builder, advertising.  X  Failure of the Top Words Test often indicates that the documents are about a range of topics, whereas the Breadth Test here indicated the training documents were very similar to one another. Here, the librarian suspected the query was picking up too many directory pages listing a range of local businesses. Adding  X  X lorist to the query fixed this problem (as a lot of directories also feature florist words are now these:  X  contractor, welding, metal, equipment, steel X . We often find that the measurement system identifies cases where the librarian has made subtle errors in query construction. This category is intended to be about tree sap; the first query attempt was: +"tree sap" OR +sap plant This failed 3 of our 5 tests, the top words for this one being :  X  X ap, business, maintenance, management company. X  So this version picked up mostly documents about the software company SAP instead of the intended topic tree sap. This was fixed by changing the query to: +"tree sap" OR + X  X lant sap X  This version actually passed all of the tests, but the top words were still a bit strange:  X  X ap, tree, remove, car, paint X  . This version basically picked up documents about removing tree sap from cars, which was not exactly what the librarian intended the topic should be. This led to a further version of the query: +"tree sap" -car -auto -removing -remove -removal OR +"plant sap" -car -auto -removing -remove -removal OR +"plant saps" -car -auto -removing -remove  X  X emoval The drawback of this version is that the category fails the Minimal Training Data Test (now has insufficient training data). The librarian X  X  decision in this case was to drop the category. Some categories fail repeatedly despite the best efforts of our librarians, usually because the categories are extremely broad or ill-defined. As an example, this category had gone through 38 iterations when the librarian arrived at the query: +"Art Deco Architecture" -music -furniture -beach -re -books -examples -prints OR +"deco architecture" -music -furniture -beach -re -prints -examples OR +"art deco architects" -music -furniture -re -books -prints  X  X xamples Using this query the category failed the Breadth Test (too broad) in addition to the Minimal Training Data Test. This basically means that best cluster the filtering process could find was still very broad. This lead to the next try: +"Art Deco building" -music -beach -re -books -prints OR +"deco architecture" -music -beach -re -prints OR +"art deco buildings" -music -re -books  X  X rints This query passed the tests but still has a slightly strange top word list:  X  X eco, design, architect, architectural, hotel. X  This suggests the document set is biased to tourist offerings or something similar. The final try for this category was: +"Art Deco building" -music -beach -re -books -prints -hotels OR +"art deco buildings" -music -re -books -prints  X  X otels. This version fixed all problems and yielded the intended results, with top words of :  X  X eco, design, architect, architectural, floor. X  Not infrequently the measurement system detects problems with the training data that are relatively easily fixed. For example, the query for the category  X  X sparagine X  (an amino acid) was: +Asparagine -syndrome -link  X  X omain. The problem here is that it fails the Overlap Test by overlapping the category amino acid biosynthesis , some closer investigation showed that the overlap was due to a lot of documents that were technical abstracts with links to a PDF file; most of the text was instructions for downloading the PDF file. Hence the fixed version was: +Asparagine -syndrome -link -domain -text  X  X df.
 This should of course be applied to the other category, too. A better solution would be to add a filter for this type of document prior to the filtering processes. In rare cases, problems identified by the measurement system cannot be fixed by changing the queries. For example, these two categories repeatedly failed the Recategorization Test, despite several attempts to improve the queries. When we investigated, we found that this was because they did not match well to any of our existing supercategories. In this case the solution was to create a new supercategory Blood that contains all the categories categories both passed the Recategorization Test. These case studies document the feedback generated by the system and how it helps the librarian to identify problem categories. The solutions for the different cases currently still require careful investigation, thoughtful reasoning and knowledge about the system details to correct the query. We are constantly working on the feedback so that the level of detail reported to the librarian is optimized. The goal is to provide a suggestion on how to fix the problem that will work in most cases. One of the more difficult aspects of this project has been the lack of a general method for testing the accuracy of thousands of categories. Indeed, the tests described in the previous sections were developed in part to overcome the lack of more direct methods of measuring the accuracy of categorization for a given category. In essence, the problem is that to test a new category, you must have a significant number of documents known to belong to the category; however, manual assembly of such test collections can be extremely expensive, depending on the degree of certainty about the category labels that is required. Hence, we have tried several variants of a basic strategy: test a sample of documents in a small number of categories to discover whether the existing key measures adequately predict the accuracy of the categorizer for a given category, then improve the measures until the predictions are as close as possible to the observed category accuracy. The variants of this strategy have included the following: During these evaluations we categorized large document collections (documents from the IBM intranet, IBM external web pages, customer Web sites, and large Web document collections) to measure the accuracy of selected categories. For these tests the documents are displayed based on the category they were categorized into, so the testers had the ability to choose a category from the taxonomy tree and then evaluate the documents the system claimed to belong to these categories. During these empirical tests the average accuracy was between 80% and 90%, although some categories were detected that were not working based on some errors in the category definition. These cases were reviewed to try to extract new tests that could be automatically applied to detect these errors. The categories checked were randomly chosen from the category tree. It is interesting to note that the number of clear misclassifications was extremely low; the typical problems found were cases in which the categorizer picked one topic of multiple present on a single web page. This issue is one we are addressing in our current research. Variants of these tests used statistical information about the corpus collected during the training data collection phase to look for possible non-working categories. This corpus included the 90% of the potential training documents that were later discarded from use as training data. The system compared the category a document was collected for with the category a document was classified into. By looking at the documents the tester then decided if the system was working correctly. By focusing on categories that had a large number of differences between the document set collected for it and the document set categorized for it we tried to maximize the probability of finding non-working categories. During these tests the results were similar to the other tests, with 80% to 90% accuracy. We also examined the documents which the system did not categorize at all. The coverage, or fraction of documents categorized, varied considerably depending upon the collection, from 20% to 80%. In some of the failing cases, we simply needed to add new categories to our taxonomy to improve the coverage. However, in most cases where the documents did not categorize, it was because the documents were ones that a human classifier would have great difficulty categorizing, because they were either very short, poorly written, error messages, in a foreign language, marketing documents with little significant content, or pages with multiple topics. We found, however, that the use of the product name categorizer described above could significantly improve the coverage even with these types of documents; in the case of IBM X  X  external Web sites, for example, it increased the coverage to 80% from 40% of the documents with no loss in accuracy. The impression from these empirical studies is that the categorization system has an acceptable (near-human accuracy) overall quality and reasonable coverage for most application areas. The single exception to this so far has been a site (Office of the Controller of the Currency, http://www.occ.treas.gov/) where the categories in our general-purpose taxonomy were too broad compared to the narrow focus on the topics on that Web site. In most such cases, it appears that the taxonomy simply needs to be extended to support the more detailed categories found on the site. The portion of this system that is deployed in particular applications is quite lightweight, namely the categorizer with associated data files. The categorizer can either be used integrated with a search engine (e.g., IBM WebSphere Portal Sever and IBM WebSphere Information Integrator OmniFind Edition ) or with custom applications. We have created tools to help customers extend the taxonomy; however, we have generally found that best results are achieved when an expert taxonomist is involved, often on a consulting basis. In addition to the integration into these products the system is also in use in several internal and external IBM projects. The most notable uses here are the ibm.com (IBM X  X  external Web site) search engine and a system to organize large amounts of learning material. The focus for the ibm.com integration is to support the user during his search effort (see Figure 6). When the user enters a search term (e.g. Linux ) the result list is displayed to the user. The top categories from the taxonomy that are found in the result list; in the given example shown, Linux (documents that mainly are about Linux and not about, say, applications on top of Linux) and Databases and Middleware (e.g. DB2 on top of Linux). If the user selects a restriction the updated list is then again searched for the most frequent categories; e.g., if the user selects Databases and Middleware as the restriction the new list of further restrictions has topics like Data Mining Software and Database Management Tools and Utilities . Using the automated categorization enables IBM to offer a category-based drill-down that supports the user in restricting the query to the document set in which he is really interested. The advantage of using an automated categorization system is the flexibility: the document set can quickly be retagged (and therefore the search be improved) by adding new categories to the system. This feature is especially needed when new products or even complete product lines are added to the taxonomy. The automated categorization system has been in production use on IBM site search since 2001. In this paper we presented a taxonomy construction and classification system that covers an extremely wide range of subjects with a high level of accuracy. The system has been proven in several application scenarios and continuously improved and extended. We believe that the power of this system arises from several key features, primarily 1) the focus on creating minimally-overlapping categories, which increases the effective signal-to-noise-ratio in the training data; 2) collecting training data from the Web, rather than company data, which gives us the ability to select, using clustering, documents that are on single topics, again increasing the effective SNR, 3) the use of librarians to ensure that topics make sense to human users, and 4) the use of a set of measurements to guide category and system improvements. We gained considerable experience while creating the system; this enabled us to improve the feedback delivered to the librarians. This in turn helped them to reduce the average time to create a category and get it fully working within the system. However, writing precise queries and creating minimally-overlapping categories remain challenging for our librarians. We think we can also improve the system performance by adding new categorization algorithms. As our system creates a set of pre-labeled training documents we can use that to train a large variety of classification algorithms. Comparing these will be an interesting project. Another area for improvement is our current effort to combine our search-based approach with other technologies that try to use unlabeled data to enrich a limited set of labeled training data. While looking at these technologies we also have tried using clustering technology to discover new topics that can be suggested to the librarians. First results looked promising. Because this is another way to minimize the human involvement in the taxonomy creation, we hope to be able to extend the Workbench in this direction. One specific area of improvement still needed is that we would like to develop a deeper understanding of why categories fail the Recategorization Test. As noted above, almost 5% of the categories in our system currently fail this test; that is a far larger percentage than currently fail any other of our five tests. This higher percentage largely reflects the considerably greater can take about 2 hours or more of librarian time to fix such problems. We believe that this test identifies a number of issues, such as overlap between categories in separate supercategories, categories where the training documents contain multiple topics, and failure of individual training documents or whole sets of documents to fit well in any of our existing supercategories. We are currently working to develop individual tests which would identify documents in each of these situations. The authors gratefully acknowledge the contributions of the many librarians and software engineers who have contributed to this effort, including Lenny Scolaro, Nancy Ordman, Stacie Raffaele, John Jou, Jose Menes, Paul Hardiman, Paul Wan, Karen Weber-Gecaj, and Michele Thompson. We also appreciate the support of the ibm.com team, particularly Mike Moran and Pat Velderman, and that of Andreas Prokoph and Andrea Baader of the IBM software development teams. [1] Adami, G., Avesani, P., and Sona, D. 2003. Bootstrapping for [2] Aggarwal, C. C., Gates, S. C., and Yu, P. S. 1999. On the merits [3] Anagnostopoulos, A., Broder, A. Z., and Carmel, D. 2005. [4] Broder, A. Z. and Ciccolo, A. C. 2004. Towards the next [5] Byeungwoo Jeon and David Landgrebe, Partially Supervised [6] Cody, W. F., Kreulen, J. T., Krishna, V., and Spangler, W. S. [7] Cohn, D. A.; Ghahramani, Z.; and Jordan, M. I. 1995. Active [8] Eirinaki, M., Vazirgiannis, M., and Varlamis, I. 2003. SEWeP: [9] Ferrucci, D. and Lally, A. 2004. UIMA: an architectural [10] Michelangelo Ceci, Floriana Esposito, Michele Lapi, Donato [11] Neff, M. S., Byrd, R. J., and Boguraev, B. K. 2004. The Talent [12] Nigam, K., McCallum, A. K., Thrun, S., and Mitchell, T. 2000. [13] Pelikan, M., Leous, J., Pearce, R., Smith, M. E., and Vaught, R. [14] Pohs, W., Pinder, G., Dougherty, C., and White, M. 2001. The [15] Pohs, Wendi, In: Practical Knowledge Management: The [16] Prieto-D X az, R. 1991. Implementing faceted classification for [17] Spangler, S. and Kreulen, J. 2002. Interactive methods for [18] Tzitzikas, Y., Spyratos, N., and Constantopoulos, P. 2005. [19] Zhang, L., Liu, S., Pan, Y., and Yang, L. 2004. InfoAnalyzer: a 
