 Ryohei Fujimaki rfujimaki@sv.nec-labs.com Kohei Hayashi hayashi.kohei@gmail.com University of Tokyo An important challenge in learning hidden Markov models (HMMs) is model selection of the number of hidden states. A well-known difficulty is non-regularity in their maximum likelihood (ML) estima-tors, under which classical information criteria such as Bayes information criterion (BIC) (Schwarz, 1978) lose their theoretical justifications 1 . Bayesian inference provides a natural and sophisti-cated way to address the issue by selecting the model which maximizes marginal log-likelihood (equivalent to the logarithm of the model posterior probability with an uniform model prior). Markov chain Monte Carlo methods (MCMCs) (Robert et al., 2000) and variational Bayesian inference (VB) (MacKay, 1997; Beal, 2003) approximate computationally and analyt-ically intractable marginal log-likelihoods, using re-spectively, sampling and variational approximation techniques. The former has an advantage over the latter in approximation accuracy but has a disadvan-tage in computational efficiency, and thus the choice of appropriate inference algorithm has been decided on the basis of a trade-off between accuracy and computational efficiency. In terms of modeling, in-finite HMMs (iHMMs) employ a hierarchical Dirich-let process prior in order to express an infinite num-ber of hidden states (Beal et al., 2002). In them, the number of components is determined on the ba-sis of mild prior knowledge expressed by a few hyper-parameters. The state-of-the-art inference of iHMMs proposed by van Gael et al. (2008) uses a beam sam-pling technique which is more efficient than well-known Gibbs sampling techniques (Beal et al., 2002). Al-though the beam sampling technique considerably re-duces the computational cost of MCMC inference, it is still higher than that of HMMs using variational non-parametric Bayesian inference (VBHMMs), while acceleration of iHMMs has been discussed from the viewpoints of parallelization (Bratieres et al., 2010). In addition, iHMMs have a few hyper-parameters which mildly control the number of hidden states, and deter-mination of them requires further computational costs. Fujimaki and Morinaga (2012) have recently proposed a new Bayesian approximation inference method for mixture models. They use the terms factorized in-formation criterion (FIC) and factorized asymptotic Bayesian inference (FAB). FIC is an asymptotically-consistent approximation of marginal log-likelihood using the  X  X actorized X  Laplace method, and FAB is its asymptotically-consistent lower bound maximiza-tion algorithm. FAB has been reported to outper-form state-of-the-art variational Bayesian method (Fu-jimaki &amp; Morinaga, 2012). Hereinafter, we denote FIC and FAB for mixture models as FIC mm and FAB mm , and those for HMMs as FIC hmm and FAB hmm .
 This paper generalizes FIC and FAB for learning HMMs which contain time dependent hidden vari-ables, in contrast to FAB mm , which requires mutual independencies among hidden variables. A key obser-vation is that a  X  X actorized X  Laplace method is ap-plicable by decomposing, using the Markov property of hidden states, the complete joint distribution in a specific form of a variational lower bound. FIC hmm can then be derived as an asymptotic approximation of marginal log-likelihoods of HMMs. The iterative optimization of FAB hmm can be seen as a natural gen-eralization of the expectation-maximization (EM) al-gorithm (Dempster et al., 1977), and, interestingly, unique regularizers appear as exponentiated update terms in our FAB forward-backward algorithm. Simi-lar to FAB mm , FAB hmm has several desirable prop-erties for learning HMMs, such as asymptotic con-sistency of FIC hmm with marginal log-likelihood, a shrinkage effect for hidden state selection, and mono-tonic increase of the lower bound of FIC hmm through the iterative optimization. An advantage over iHMM is that it has no hyper-parameter, and thus its model selection process can be fully automated though we understand that prior knowledge injection can also be an advantage because we can control models. Further, our experimental results show that model selection ac-curacy of FAB is competitive to or even better than iHMMs, with significantly-lower computational costs. Let X = X 1 ,...,X T and Z = Z 1 ,...,Z T be re-spective sequences of observed and hidden random variables. Z t = ( Z t 1 ,...,Z t K ) is an indicator vec-tor, and Z t k = 1 if X t is generated from the k -th hidden state, and Z t k = 0 otherwise. We denote the number of hidden states as K . Let us assume that we observe independent N sequences 2 and denote them as x N = x 1 ,..., x N . The n -th sequence is de-noted as x n = x 1 n ,..., x T n n , where T n is the length of the n -th sequence. We further denote the sequence of latent variables corresponding to x N and x n as z
N = z 1 ,..., z N and z n = z 1 An HMM is described as p ( X |  X  ) = P Z p ( X,Z |  X  ) = P
Z p ( Z where  X  = (  X  ,  X  ,  X  ). p ( Z 1 |  X  ), p ( Z t | Z t  X  1 an initial probability, a transition probability, and an emission probability, and they are, re-spectively, described as p ( Z 1 |  X  ) = Q K k =1  X  Z p ( X t | Z t ,  X  ) = Q K k =1 p ( X t |  X  k ) Z t k , and p ( Z Q define p k ( Z t |  X  k ) as p k ( Z t |  X  k )  X  p ( Z t |  X  = (  X  1 ,..., X  K ),  X  = (  X  1 ,...,  X  K ), and  X  = (  X  1 ,...,  X  K ) are respective parame-ters (  X  k = (  X  k 1 ,..., X  kK )). The parameters  X  and  X  satisfy P K k =1  X  k = 1 and P K j =1  X  kj = 1, respec-tively. A standard parameter inference follows the EM algorithm with a specific expectation step known as either the forward-backward algorithm (Rabiner, 1989) or the Baum-Welch algorithm (Baum, 1972). Let us make a few mild assumptions: A1 the transition matrix  X  is row-independent (i.e.,  X  k tually independent), A2 p ( X,Z |  X  ) is bounded (does not diverge to infinity), and A3 p ( X |  X  k ) satisfies the regularity condition. A1 and A2 are usual assump-tions in HMMs. A3 is much milder than a regularity assumption on p ( X |  X  ), and many HMMs (e.g., a HMM with categorical observations, a HMM with Gaussian observations) satisfy this assumption.
 Let us denote a model of p ( X |  X  ) as M . We allow K emission probabilities to be different from one another in their model representations (e.g., for logistic regres-sion emissions, different hidden states can have feature configurations with different complexities). This can be seen as an HMM-extension of the so-called  X  X etero-geneous mixture models X  (Fujimaki et al., 2011). In order to distinguish different model representations, we denote a model of  X  k as S k . That is, our model M is specified by K and emission models S k , i.e., M = ( K,S 1 ,...,S K ). Although parameter represen-tations depend on the corresponding models, we here-inafter omit them for notational simplicity. FIC hmm considers the following lower bound of marginal log-likelihood as: In contrast to the standard VB lower bound, P which makes a mutual independence assumption be-tween z N and  X  on variational distributions, this lower bound does not. Although (1) is also used in FIC mm and collapse VBs (Teh et al., 2006; Kurihara et al., 2007), both of them require mutual indepen-dences among hidden variables, and thus they are not directly applicable for models having time-dependent hidden variables (e.g., HMMs).
 On the basis of the Markov property of hidden states, A1 , and A2 , the numerator of (1) is factorized as fol-lows: p ( x N , z N | M ) = where p (  X  | M ) = p (  X  | M ) Q K k =1 p (  X  k | M ) p (  X  parameter prior, and we assume that the priors X  di-mensionalities are asymptotically small. Each one of the factorized distributions, p ( z 1 n |  X  ), p k ( z and p ( x t n |  X  k ), is a regular model ( A3 ) to which the Laplace method is applicable. Then, as with FIC mm , the  X  X actorized X  Laplace method around the ML esti-mator 3  X   X  of p ( x N , z N |  X  ) gives us a second order ap-proximation of complete likelihood as follows: log p ( x N , z N |  X  )  X  log p ( x N , z N |  X   X  )  X   X  The notation [  X  F  X  ] represents a (centered) quadratic form with respect to  X  , e.g., [  X  F  X  ] = (  X   X   X   X  ) T  X  F  X   X  ).  X  F  X  ,  X  F  X  approximated by (complete) data instances as follows:  X   X  F  X  F It is easy to show that they respectively converge to Fisher information matrices of p ( Z 1 |  X  ), p k ( Z t | and p ( X |  X  k ) with N  X   X  , and it guarantees their determinants to be O (1). The complete marginal like-lihood can then be asymptotically approximated as: Here, D  X  is the dimensionality of  X  ( D  X  = K  X  1 and D By substituting (5) into (1) and ignoring asymptoti-cally small terms, we obtain an asymptotic approxi-mation of log p ( x N | M ) as follows: We here have two regularization terms depen-dent on hidden states, D  X  D totic approximation of the emission (observation) probability, and appears in FIC mm . Contrastingly, the former is an unique regularization term in FIC hmm Notably, these two regularizers contain dependencies between parameters (  X  k and  X  k ) and hidden states, which the variational lower bound of VB methods usually ignore on their variational distributions (Beal, 2003). These regularizers will be discussed in more detail in sub-sections 4.4 and 4.5 The following theorem justifies FIC hmm as an approx-imation of marginal log-likelihood: Theorem 1 FIC ( x N ,M ) is asymptotically consis-tent with log p ( x N | M ) .
 A rough sketch of the theorem can be described as follows. Because of the regularity condition A3 , indi-vidual Laplace approximations have asymptotic con-sistency, and thus their product (5) is also consistent with p ( x N , z N | M ). Then, since there is a q which satisfies the equality in (1), the theorem holds. 4.1. FAB Lower Bound Since  X   X  is defined on both x N and z N and thus is not available in practice, we cannot evaluate FIC hmm itself. Instead, FAB maximizes the lower bound of FIC hmm . As is similarly done in (Fujimaki &amp; Morinaga, 2012), we employ two inequalities to de-rive the lower bound. First, on the basis of the definition of the ML estimator, log p ( x N , z N |  X  log p ( x N , z N |  X  ) holds. Second, on the basis of the con-cavity of the logarithm function, log( P N,T n n,t =1  X  q (the same holds for log( P N,T n  X  1 n,t =1 z L ( a,b ) = log b + ( a  X  b ) /b . The lower bound of (6) is then derived as follows:
FIC ( x N ,M )  X G ( q,  X  q, x N ,  X  ) (8) =
X  X  log q ( z N ) i + (log(  X  where  X  t is a normalization constant for P K k =1  X  t k = 1. The underlined part is referred to in (19).
 FAB hmm learns HMMs by solving the following opti-mization problem (recall that  X  and q are respective functions of M ): Here we have a newly-introduced parameter  X  q which is also optimized in FAB hmm . Let us fix  X  and q . By making the differential of (9) with respect to  X  q ( z zero, we obtain the following optimality conditions: Clearly,  X  q = q satisfies (12) for arbitrary  X  q ( z result will be used in the next subsection. 4.2. Iterative Optimization with FAB Let us first fix K and consider the optimization of (11) with respect to S ,  X  ,q,  X  q , where S = ( S 1 ,...,S Since their simultaneous optimization would be in-tractable, FAB hmm works on the basis of iterations of two sub-steps (V-step and M-step). Let the super-scription ( i ) represent the i -th iteration. V-step (FAB Forward-Backward Algorithm) In the i -th V-step, we fix  X  =  X  ( i  X  1) and also fix  X  q = q ( i  X  1) on the basis of (12). FAB hmm then op-timizes q by maximization in (9). The terms in (9) de-pendent on q can be decomposed in terms of sequences, and thus we can independently optimize q ( z n ) for indi-vidual sequences. Further, the maximization problem for q ( z n ) with respect to P z P E-step in the EM algorithm for HMMs. In fact, the the regularization terms D  X  D Notably, the term z t nk log  X  t k is a product of a hidden variable and a log-probability because  X  t k is normalized as is defined in (10). The maximization problem can thus be solved on the basis of the forward-backward algorithm described as follows:  X  n is a normalization constant for P tions are calculated as follows: In the above FAB forward-backward algorithm, the only difference from the standard forward-backward algorithm is the exponentiated update term,  X  t ( i  X  1) k is interesting that FAB mm has a similar exponentiated update term in its V-step, and it generates significant differences from standard ML estimation using the EM algorithm. Similarly,  X  t ( i  X  1) k generates essential differ-ences for learning HMMs between FAB hmm estimation and ML estimation. Roughly speaking, (10) indicates that the smaller and more complex components are likely to become even smaller through the iterations. Further, (13) indicates that the regularization effect of  X  k propagates forward and backward (e.g., largely-regularized hidden states in f t ( i ) nk make only small con-Mstep Let us fix q = q ( i ) and  X  q = q ( i ) . FAB then optimizes  X  by maximization in (9). First, we have the following parameter updates for  X  and  X  : We then update S k and  X  k by solving the following optimization problem: It is worth noting that FAB hmm provides a natural way of seeking emission probability types S k because the M-step can be decomposed into optimization problems for individual hidden states (otherwise, we must take into account an exponential number of hidden state combinations.) With a finite set of emission proba-bility candidates, we first optimize  X  k for each ele-ment of a fixed S k and then select the optimal one by comparing them. If we employ a standard HMM having single emission type, (16) is reduced to the M-step of the standard EM algorithm for HMMs, i.e.,  X  k = arg max  X  k ther, FAB hmm is reduced to the standard EM algo-rithm with N  X  X  X  (because  X  t k = 1) and thus can be seen as its natural generalization. 4.3. Convergence and Stopping Criterion Let us denote the lower bound of FIC as follows: Then, as is done for mixture models (Fujimaki &amp; Mori-naga, 2012), FAB hmm is guaranteed to monotonically increase FIC ( i ) LB ( x N ,M ) through the VM-iterations, which can be summarized in the following theorem. Theorem 2 For the VM iterations of FAB hmm , the following inequality is satisfied: as a stopping criterion. One issue is computation of the term P z problem using a way similar to that for the variational free energy for VBHMMs (Beal, 2003). In summary, FIC ( i ) LB ( x N ,M ) can be computed as follows (we omit the derivation here because of space limitations): 4.4. Automatic Hidden State Selection An interesting property of the asymptotic exponenti-ated regularizer  X  t k (10) is a shrinkage effect for select-ing the number of hidden states, which we fixed in the previous section, and thus it provides over-fitting mitigation despite our asymptotic ignoring of priors. larized without relation to the observation and previ-ous paths. While b t ( i ) nk does not explicitly have such a regularization effect, one notable fact is that each next small contribution to the update of b t ( i ) nk . Then, in (14), the k -th hidden state with a small  X  t ( i  X  1) k value has a tion in (10), the smaller hidden state has the smaller  X  value. This means such a hidden state gradually be-comes smaller through the VM iterations. Similarly, the frequency of transitions between the j -th and the k -th states having respective small  X  t  X  1 j and  X  t ues becomes smaller because P N,T n  X  1 n,t q ( i ) ( z gradually becomes smaller.
 On the basis of the above insight, FAB hmm shrinks hidden states using a thresholding operation as follows: q Starting from an appropriately-large number ( K max ) of hidden states, FAB hmm iteratively optimizes S ,  X  , and q . During the VM steps, the number of hidden states might become smaller due to the shrinkage op-eration. Then, at a convergence point, we obtain the optimal model M  X  = ( K  X  , S  X  ), the optimized param-eter  X   X  , and the variational distribution q  X  . A similar shrinkage effect has been reported in FAB mm (Fuji-maki &amp; Morinaga, 2012), and FAB hmm naturally in-herits it for Markov hidden variables. 4.5. Discussion: Comparison with VB and BIC We here compare three approximation inference meth-ods (FAB, VB and BIC) and discuss their differences. FAB approximates marginal log-likelihoods by (19). Let us denote normalization constants for forward-backward algorithms of ML and VB estimations as  X  ( ML ) and  X  t n ( V B ), respectively. Variational free energy F V B and BIC 4 can then be respectively com-puted as follows (see (Beal, 2003) for VB free energy): + BIC = where D = D  X  + P K k =1 ( D  X  Here (19), (21), and (22) all have closely similar representations, i.e., data fitting term + complexity. The data fitting terms (  X  -related terms) are differ-ent in posterior (or variational) distributions of hid-den states, on which respective complete log-likelihood functions are marginalized (of course, parameter es-timators are also different.) Interesting differences appear in the complexity terms. The complexity term of VB is the Kullback-Leibler divergence be-tween the variational posteriors and priors on param-eters. Therefore, VB regularizes parameters (precisely speaking, variational posteriors) to be apart from pri-ors. One disadvantage is that the complexity term is dependent on a choice of priors (and their hyper-parameters), and model selection results will thus be similarly dependent, though we understand that this can also be an advantage because we can control mod-els using priors. On the other hand, FAB does not have manually-tunable parameters in (19). Notably, only the FAB complexity takes into account the distribution on hidden states (i.e.,  X  q  X  ( z N ) = q  X  ( z N )), and FAB au-tomatically adjusts regularization levels on the basis of ual hidden states. The regularization term of BIC is stronger than that of FAB (all hidden states are regu-larized with the scale log P N n =1 T n .) Interestingly, the stochastic complexity of HMMs is theoretically proven to be considerably smaller than D / 2 log P N n =1 T n (Ya-mazaki &amp; Watanabe, 2005). Our result also suggest that a brute-force application of BIC to HMMs over-estimates the complexity term. We conducted simulations using artificial data for investigating basic behaviors of HMMs with FAB (FABHMMs), and evaluation using real world e-book data. FABHMM was compared with VBHMM, iHMM, and HMMs with ML estimation and BIC model selection (MLHMMs). We implemented FABHMM and MLHMM by Python, while we used Matlab softwares for iHMM 5 and VBHMM 6 . 5.1. Simulations with Artificial Data By following settings similar to those in (van Gael et al., 2008), we conducted evaluations on HMMs with either one-dimensional Gaussian emissions or cat-egorical emissions. The true model had four hid-den states, either Gaussian emission probabilities with categorical emission probabilities with 8-alphabet, and transition probability described as follows: where the left and right matrices correspond to tran-sition and categorical emission probabilities. We set the initial probability of the first state as one and the rests were zero. We randomly generated a single training sequences with the length of T  X  { 250 , 500 , 1000 , 2000 } , and a test sequence with the length of T test = 5000. For iHMMs, we randomly initialized the hidden state sequences by 10 different states. We set two hyper-priors in iHMMs as (1)  X ( a,b ) denotes the Gamma distribution with the shape parameter a and the scale parameter b . The former setting (iHMM1) were used in (van Gael et al., 2008) as a default setting and the latter setting (iHMM2) was less informative. VBHMM and MLHMM were performed with setting K = 1 ,...,K max and selected the optimal K  X  which minimized the respective free energy and BIC values. For FABHMM, VBHMM and MLHMM, K max is set to be ten (FAB started from K max hidden states and automatically searched the optimal value.) We evaluated the estimated number of hidden states (model selection accuracy), predic-tive log-likelihood (PLL) against the testing set (gen-eralization performance), and training time (computa-tional efficiency). If the training time of each method exceeded 10 minutes, we stopped the training proce-dure and used the result at the time (iHHM violated this time limit a few times.) The results below are the averages of ten runnings.
 Table 1 shows FABHMM almost perfectly estimated the true number of hidden states for both emission types. Surprisingly, despite FAB being an asymp-totic method, it outperformed the others with rela-tively small data sizes. iHMMs also performed well, but their results were somehow affected by a choice of hyper-priors. VBHMM and MLHMM were signifi-cantly inferior to the others in terms of model selec-tion performance. One plausible reason for the wrong performance of VB is the independence assumption to variables; VB approximates the marginal likelihood with ignoring the variable dependency, which makes its approximation worse than the others. BIC does not have theoretical justification in HMMs and, in fact, the MLHMM poorly performed.
 Fig. 1 shows the training times and the PLLs (left: Gaussian, right: categorical). With respect to PLLs, FABHMM was competitive or slightly better than the best among the others, while none of the others did not perform well for both cases. With respect to training times, FABHMM and VBHMM were competitive and 3-4 times faster than iHMMs while all of their training time increased only linearly with the sequence length. Both PLLs and training times of MLHMM were sig-nificantly worse than those of the others. This was because MLHMM was likely to be captured by bad local minima solutions which significantly degraded both estimation performance and the convergence of its optimization while additional computational cost might mitigate this issue. An interesting observation was that FABHMM was robust for such local minima solutions because the exponentiated regularizer auto-matically removed such  X  X ad X  hidden states, and thus FABHMM mitigated the issue. 5.2. E-Book Character Sequences Next, we evaluated the feasibility for text prediction. We prepared six books 7 , Alice X  X  Adventures in Won-derland (Alice), THe Art of War (Sunzi), The Meta-morphosis (Kafka), The Republic (Plato), The United States Declaration of Independence (DOI), and The Adventures of Sherlock Holmes (Sherl). In this setting, each letter in the texts was treated as a categorical observation. The letters included some special charac-ters, and the number of categorical alphabets varied from 32 to 50 in these data sets. The first 5000 letters of the first chapter in each book were used for training and the next 5000 letters for testing. For fair com-parison, we eliminated the alphabets from the testing sets which did not appear in the training sets. Here We compared FAB with iHMM1 and VBHMM. Since these data are more complicated than artificial data, we here set K max = 20 for FABHMM and VBHMM.
 Fig. 2 shows a typical behavior of FABHMM and iHMM and we confirmed the prediction perfor-mance of FABHMM was improved with increasing T . FABHMM achieved competitive prediction perfor-mance with iHMM for larger T  X  3000. Although FABHMM required a longer sequence for reasonable estimation than those in the artificial simulations, we believe that the length (about 3000 -5000) is not large in recent large scale data scenarios. Notably, the es-timated number of hidden states was much smaller than that of iHMM. This means FABHMM could ob-tain more compact HMM representations than iHMM and that is usually desirable in practice. Table 2 shows the training times, estimated number of hid-den states, and PLLs ( T = 5000). For all data, with respect to PLLs, FABHMM and iHMM compa-rably performed, and VBHMM performed the worst. The training times of FABHMM were roughly three or four times faster, which agree with the results in the previous section. These results indicate clear ad-vantages of FABHMM with respect to model selection accuracy over VBHMM, and with respect to compu-tational efficiency over iHMM. Finally, we emphasize that FABHMM does not have hyper-parameters in its criterion, and all the above strong model selection pro-cedures were automatically done. This paper has addressed the model selection is-sue for HMMs by generalizing factorized informa-tion criteria and factorized asymptotic Bayesian in-ference. We have theoretically shown several desirable properties (asymptotic consistency of FIC, an auto-matic shrinkage effect, monotonic increase in the FIC lower bound, etc) and also have experimentally shown that FABHMM outperforms the states-of-the-art vari-ational Bayesian and non-parametric Bayesian meth-ods with respect to model selection accuracy and com-putational efficiency.
 We have several issues for future study. First, it is interesting to extend FAB for more flexible HMM families as iHMM has been extended for factorial HMM (Ghahramani &amp; Jordan, 1997; van Gael et al., 2009). Second, both FAB mm and FAB hmm are de-signed for discrete hidden variables, and thus FAB for continuous hidden variable models is still an open problem. Third, theoretical details, such as rates of convergence (e.g., how fast the FAB iteration con-verges and how fast FIC converges to marginal log-likelihood), should be investigated.

