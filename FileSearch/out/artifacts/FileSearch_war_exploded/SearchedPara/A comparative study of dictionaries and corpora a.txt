 Shinsuke Mori 1  X  Graham Neubig 2 Abstract In this paper, we investigate the relative effect of two strategies for language resource addition for Japanese morphological analysis, a joint task of word segmentation and part-of-speech tagging. The first strategy is adding entries to the dictionary and the second is adding annotated sentences to the training corpus. The experimental results showed that addition of annotated sentences to the training corpus is better than the addition of entries to the dictionary. In particular, adding annotated sentences is especially efficient when we add new words with contexts of several real occurrences as partially annotated sentences, i.e. sentences in which only some words are annotated with word boundary information. According to this knowledge, we performed real annotation experiments on invention disclosure texts and observed word segmentation accuracy. Finally we investigated various lan-guage resource addition cases and introduced the notion of non-maleficence , asymmetricity , and additivity of language resources for a task. In the WS case, we found that language resource addition is non-maleficent (adding new resources causes no harm in other domains) and sometimes additive (adding new resources helps other domains). We conclude that it is reasonable for us, NLP tool providers, to distribute only one general-domain model trained from all the language resources we have.
 Keywords Partial annotation Domain adaptation Dictionary Word segmentation POS tagging Non-maleficence of language resources The importance of language resources continues to increase in the era of natural language processing (NLP) based on machine learning (ML) techniques. Well defined annotation standards and rich language resources have enabled us to build very accurate ML-based analyzers in the general domain. Representative tasks include word segmentation for languages without word boundaries such as Japanese and Chinese, part-of-speech (POS) tagging, and many others. These are the first step in NLP for many languages and have a great impact on subsequent processes. However, while the accuracies are more than 97 % for texts from the same domain as the training corpus, large drops in accuracy are seen when using these models in a different domain. To cope with the problem of adapting to new domains, there are many attempts at semi-supervised training and active learning (Tomanek and Hahn 2009 ; Settles et al. 2008 ; Sassano 2002 ). However, the simple strategies of corpus annotation or dictionary expansion are still highly effective and not unbearably costly. In fact, according to authors X  experience with annotation for Japanese word segmentation, it only took 7 h 9 10 days to annotate 5000 sentences (about 40 words per sentence) with word-boundary information including two check processes. 1 As shown in the subsequent parts of this paper, 5000 annotated sentences are often enough to achieve large gains in domain adaptation for sequence labeling.

The 5000 sentences mentioned above were so-called fully annotated sentences, where all the positions between two characters are annotated with word-boundary information. Within the context of sequence labeling, however, a variety of resources can be used, including partially annotated sentences and dictionaries. In contrast to fully annotated sentences, partially annotated sentences lack labels at some points. These annotated sentences give us information about word use in context, without requiring the annotator to annotate the full sentence. On the other hand, dictionaries lack context information but are often available at large scale.
In this paper, we first investigate the relative effect of dictionary expansion and annotated corpus addition (full annotation and partial annotation) for the Japanese morphological analysis (MA) problem (a joint task of word segmentation and POS tagging) and the word segmentation problem. Then we present some real adaptation cases, including invention disclosures and recipe texts. Finally we introduce a notion of non-maleficence of language resources, which means that the addition of language resources in a certain domain causes no harm to segmentation accuracy for another domain. The task we have chosen is Japanese MA, a joint task of word segmentation and POS tagging. Although in Japanese most of the ambiguity in MA lies in word segmentation, 2 in the first half of the experimental evaluation we address the full MA task for comparison between two standard methods in the field: the joint sequence-based method (Kudo et al. 2004 ) and the 2-step pointwise method (Neubig et al. 2011 ). In the second half, we focus on word segmentation.
After a long history of rule-based MA methods, the first statistical method was proposed by Nagata ( 1994 ). It was based on a hidden Markov model whose states correspond to POS tags. Then Mori and Kurata ( 2005 ) extended it by lexicalizing the states like many works in that era, grouping the word-POS pairs into clusters inspired by the class-based n -gram model (Brown et al. 1992 ), and making the history length variable like it has been done for a POS tagger in English (Ron et al. 1996 ). In parallel, Kudo et al. ( 2004 ) applied conditional random fields (CRFs) (Lafferty et al. 2001 ) to this task and showed that it achieved better performance than a POS-based Markov model. This CRF-based method does not have an unknown word model and large drops in accuracy are seen when it is applied to a different domain from the training data. For unknown words Nakagawa ( 2004 ) and Kruengkrai et al. ( 2009 ) proposed a word-based model (not CRF-based) equipped with an unknown word model represented by position-of-character tags and reported comparable accuracies to the CRF-based method.

Along with the model evolution, the NLP community has been becoming more and more aware of the importance of language resources. In addition we have observed a drastic degradation in word segmentation accuracy in domain adaptation cases. Because most unknown words are nouns and the rest fall into other content word categories such as verbs, adjectives, it is less difficult to estimate the POS given the correct segmentation than word segmentation of sentences including unknown words. Thus the Japanese MA research focus has been shifted to word segmentation. Given this background Mori and Oda ( 2009 ) proposed a method for referring to the words in a dictionary prepared for humans, not computers. The entries in these dictionaries tend to be compound words but not words defined according to the annotation standard. 3 Another important extension to enlarge language resource availability is CRFs trainable from partially annotated sentences (Tsuboi et al. 2008 ). This is applied to Chinese word segmentation capitalizing on so-called natural annotations such as tags in hyper-texts (Yang and Vozila 2014 ). Because the training time of sequence-based methods tends to be long, a simple method based on pointwise classification has been shown to be comparable to sequence-based methods (Neubig et al. 2011 ). Since the pointwise method decides whether there is a word boundary or not between two characters without referring to the decisions on the other points, we can train the model from partially annotated sentences in a straightforward way. 4 Japanese MA takes an unsegmented string of characters x I 1 as input, segments it into morphemes w J 1 , and annotates each morpheme with a part of speech t J 1 . This can be formulated as a two-step process of first segmenting words, then estimating POS (Ng and Low 2004 ; Neubig et al. 2011 ), or as a single joint process of finding a morpheme/POS string from unsegmented text (Nagata 1994 ; Mori and Kurata 2005 ; Kudo et al. 2004 ; Nakagawa 2004 ; Kruengkrai et al. 2009 ). In this section we explain these approaches briefly, and contrast their various characteristics. 3.1 Joint sequence-based MA Japanese MA has traditionally used sequence-based models, finding the highest-scoring POS sequence for entire sentences as in Fig. 1 a. The CRF-based method presented by Kudo et al. ( 2004 ) is a widely used baseline in this paradigm. CRFs are trained over segmentation lattices, which allows for the handling of variable-length sequences that occur due to multiple segmentations. The model is able to take into account arbitrary features, as well as the context between neighboring tags.
The main feature of this approach in the context of the current paper is that it relies heavily on a complete and accurate dictionary. In general when building the lattice of candidates from which to choose, it is common to consider only candidates that are in a pre-defined dictionary, only adding character sequences that are not in the dictionary when there are no in-vocabulary candidates. 5 Thus, if the dictionary contains all of the words present in the sentences we want to analyze, these methods will obtain relatively high accuracy, but any words not included in the dictionary will almost certainly be given a mistaken analysis.
 We follow (Kudo et al. 2004 ) in defining our feature set, as summarized in Table 1 . 6 Lexical features were trained for the top 5000 most frequent words in the corpus. It should be noted that these are word-based features, and information about transitions between POS tags is included. When creating training data, the use of word-based features indicates that word boundaries must be annotated, while the use of POS transition information further indicates that all of these words must be annotated with POS (Table 2 ). 3.2 Two-step pointwise MA In the two-step approach (Neubig et al. 2011 ), on the other hand, we first segment character sequence x I 1 into the word sequence w J 1 with the highest probability, then tag each word with POS t J 1 . This approach is shown in Fig. 1 b.

Word segmentation is formulated as a binary classification problem, estimating boundary tags b I 1 1 . Tag b i  X  1 indicates that a word boundary exists between characters x i and x i  X  1 , while b i  X  0 indicates that a word boundary does not exist. POS estimation can also be formulated as a multi-class classification problem, where we choose one tag t j for each word w j . These two classification problems can be solved by tools in the standard machine learning toolbox such as logistic regression (LR), support vector machines (SVMs), or CRFs.

As features for these classification problems, it is common to use information about the surrounding characters (character and character-type n -grams), as well as the presence or absence of words in the dictionary (Neubig et al. 2011 ). The character n -gram features fire binary features for each character n -grams of length 1 X 3 in the neighborhood of the current segmentation boundary. The character type n -gram features similarly are binary features, but with each character generalized from its surface form to one of six different types of characters widely used in the Japanese language: kanji , hiragana , katakana , arabic number , alphabet , and symbol . kanji is ideograms, while hiragana and katakana are phonograms mainly used for function words and imported words, respectively. Dictionary features include l s and r s which are rightofthepresentwordboundary,and i s whichisactiveifthepresentwordboundaryis included in a dictionary word of s characters. Dictionary feature d jk for POS estimation can indicate whether the current word w j occurs as a dictionary entry with tag t k .
Compared to the joint sequence-based method described in Sect. 3.1 (Kudo et al. 2004 ), the two-step approach is a dictionary-light method. In fact, given a corpus of segmented and POS-tagged sentences, it is possible to perform analysis without the dictionary features, relying entirely on the information about the surrounding n -grams learned from the corpus. However, as large-coverage dictionaries often exist in many domains for consumption by either computer or human, having the possibility to use these as additional features is expected to give a gain in accuracy, which we verify experimentally in the following section.

Previous work using this two-stage approach has used sequence-based prediction methods, such as maximum entropy Markov models (MEMMs) or CRFs (Ng and Low 2004 ;Penget al. 2004 ).However,asLianget al.( 2008 )note,sequence-basedpredictors are often not necessary when an appropriately rich feature set is used. One important methods is that we rely only on features that are directly calculable from the surface string,withoutusingestimatedinformationsuchaswordboundariesorneighboringPOS tags. 7 This allows for training from sentences that are partially annotated practically. To observe the difference between the addition of annotated sentences to the training corpus, and addition of entries to the dictionary, we conducted the experiments described below. 4.1 Experimental setting The task we use as our test bed is the domain adaptation of Japanese MA. We use the Core part of the Balanced Corpus of Contemporary Written Japanese (BCCWJ) (Maekawa 2008 ) as the data for our experiments. The BCCWJ Core data is divided into six sections, each from a different source, so this is ideal for domain adaptation experiments.
 As our target domain, we use data from the Web (Yahoo! Chiebukuro in BCCWJ) and as the general domain we use a joint corpus built by putting together the other five domains of BCCWJ Core data. Table 3 shows the specifications of the corpus and dictionary.

As morphological analyzers, we use the following two publicly available tools. 8 1. MeCab : CRF-based joint method (Kudo et al. 2004 ) 2. KyTea : two-step pointwise method (Neubig et al. 2011 ) We compare the following adaptation strategies for the two morphological analyzers.  X  No adaptation : Use the corpus and the dictionary in the general domain.  X  Dictionary addition (no re-training): Add words appearing in the Web training corpus to the dictionary. As the dictionary includes weights, we set the weight of all new words to the same value as infrequent words of the same POS tag, following the instructions on the MeCab Web page 9 ( MeCab only 10 ).
  X  Dictionary addition (re-training): Add words appearing in the Web corpus to the dictionary and estimate the weights of the model on the general domain training data again.  X  Corpus addition : Combine the training corpora from the general and Web domains, and train the parameters on the result. 4.2 Evaluation criterion As an evaluation criterion we follow Nagata ( 1994 ) and use precision and recall based on word-POS pairs. First the longest common subsequence (LCS) is found between the correct answer and system output. Then let N REF be the number of word-POS pairs in the correct sentence, N SYS be that in the output in a system, and N
LCS be that in the LCS of the correct sentence and the output of the system, so the recall R and precision P are defined as follows: Finally we calculate F-measure defined as the harmonic mean of the recall and the precision: 4.3 Results and discussion Table 4 shows the experimental result. From this table, we can see that just adding entries to the dictionary has a large positive effect on the accuracy. By adding entries to the dictionary (no re-training in the MeCab case 11 ) the accuracies of MeCab and KyTea increase by 1.39 and 1.21 % respectively. However, by actually adding annotated sentences to the training corpus we can further increase by 0.26 and 0.40 % respectively. That is to say, 75 X 84 % of the accuracy increase can be achieved through dictionary expansion and the remaining 16 X 25 % can be realized only by adding the context information included in the corpus.

The following are the examples of increases realized only by the corpus addition for MeCab .  X 
In books and newspaper articles (what) is written in the Chinese character be written by hiragana , (phonogram). Thus the morphological analyzer divides the string into the auxiliary verb (become, get) and its inflectional ending which appear many times in these domains.  X 
Smiley faces are rare in the general domain but often used in Web domain. And characters including are a single word in many cases. Thus we need to add a Web domain training corpus to estimate that the smiley face is sufficiently common as a single word and should not be divided.  X  segmented into a verb and inflectional endings but using this word as a noun is common in the Web domain.

Another remark is that the accuracy gain is almost the same in CRF-based joint method ( MeCab ) and two-step pointwise method ( KyTea ) contrary to our expectation that MeCab depends more on the dictionary than KyTea . Thus both morphological analyzers are making good use of dictionary information, but also can be improved with the context provided by the corpus. The experimental results that we described in the previous section are somewhat artificial or in-vitro. In the corpus addition case, it is assumed that the sentences are entirely annotated with word-boundary information and all the words are annotated with their POS.

In this section, we report results under two other adaptation methods used in real or in-vivo adaptation scenarios. In both cases, the language resources to be added are partially annotated corpora (Neubig and Mori 2010 ). Because MeCab is not capable of training a model from such corpora, we only report the result of KyTea .
As the problem, we focus on word segmentation, because in Japanese most ambiguity in MA lies in word segmentation as we mentioned in Sect. 2 , especially in the domain adaptation situation where most of unknown words are nouns and the rest fall into other content word categories such as verbs, adjectives, etc. 5.1 Recipe domain The first case we examine is the adaptation to cooking recipe texts. A cooking recipe text consists of sentences describing procedures. Because they do not contain difficult language phenomena for NLP such as tense, aspect, viewpoint, etc., they can be thought to be relatively easy for computers to understand. Thus they are suitable as a benchmark for the language understanding research. In addition, there is a large demand for more accurate NLP in recipe domains for intelligent recipe processing (Wang et al. 2008 ; Yamakata et al. 2013 ).

In the following experiment we used the recipe flow graph corpus (r-FG corpus) (Mori et al. 2014 ). Table 5 shows the specifications of the r-FG corpus relating to the word segmentation experiment. In the corpus word sequences important for cooking are annotated with types (recipe named entities; r-NEs) and they are correctly segmented into words. Figure 2 shows two example sentences. 5.1.1 Experimental setting As the adaptation strategies, we used the following two methods in addition to  X  X  No adaptation . X  X  The examples are taken from Fig. 2 . F and Ac are type tags and mean food and action by the chef, respectively.

Dictionary : Use the training data as a dictionary. 1. Extract r-NEs from the training data, 2. Make a dictionary containing the words in these r-NEs, 3. Use the dictionary as the additional language resource to train the model.
Partial annotation : Use the training data as partially annotated data. 1. Extract n occurrences at maximum of the r-NEs from the training data (see 2. Convert them into partially segmented sentences in which only both edges of 3. Use the partially annotated data as an additional language resource to train the 5.1.2 Result and discussion Table 6 shows the word segmentation accuracies (WS F-measure) of No adaptation and the strategies that we explained above. The results of the partial annotation strategy vary depending on the parameter n (the maximum number of occurrences). The table shows these results with the real average number of occurrences in the partially segmented sentences (Table 7 ).

From the result we can note several things. The BCCWJ results show that adding language resources in the recipe domain has no effect on the general domain accuracy. Regarding the results of the first recipe, the addition of new words as the dictionary to the training data improves the word segmenter. This is consistent with the results shown in Table 4 . Second, the partial annotation strategy with one occurrence ( n = 1) is as good as the dictionary addition strategy. And as we increase the number of occurrences ( n ), the segmenter improves. The degree of improvement, however, shrinks as n increases. In a real situation, we have to prepare such partially annotated data and the annotation cost is proportional to the number of occurrences to be annotated. Therefore it is good to start annotating new words in descending order of frequency, selecting a threshold based on the number of occurrences. We describe a concrete way of doing so for real unannotated data in the following section. 5.2 Invention disclosure domain Finally we report the result of a real adaptation experiment that we performed. The target domain is invention disclosure texts from patents, which are an important domain for NLP, especially information extraction (Nanba et al. 2011 , interalia) and machine translation (Goto et al. 2011 , interalia). 5.2.1 Setting Based on the knowledge we described above, we adopted the partial annotation strategy. Concretely, we performed the following procedure. 1. Extract unknown word candidates based on the distributional similarity from a 2. Annotate three occurrences with word-boundary information to make partially For frequent word candidates, i.e. in the beginning of the annotation work, the three-occurrence annotation corresponds to the case of those with the maximum occurrence count of 4 (average: 2.36) and 8 (average: 3.26) in Table 6 , because the average number of the occurrences is expected to be three.

In practice, we first assign each word a default annotation, then ask an annotator to check unknown word candidates with three different contexts in the raw corpus and correct the word boundary information if the default is incorrect. Figure 3 shows two example word candidates with their three occurrences. The default segmentation assumes that the candidate word is really a word. In the first example case, is assumed to be a word, but it is a concatenation of a word fragment in word-boundary information as shown in  X  X  X fter annotation work. X  X  In the second example case, in the first line (context) is a concatenation of a word fragment in information as shown in  X  X  X fter annotation work. X  X  In the second and third line (context), however, (lamination) is a word and the annotator leaves it as the default without change. 5.2.2 Result and discussion The learning curve is shown in Fig. 4 . The leftmost point corresponds to the  X  X  No adaptation  X  X  case. The accuracy in this case is high compared with the recipe domain (Table 6 ) because the invention disclosure domain is not as stylistically different from the general domain containing newspaper articles etc. The most important thing to note is that the accuracy gets higher as we add more unknown word candidates to the training data as partially annotated sentences. After 12 h of annotation work, we succeeded to eliminate 12 % of the errors. The absolute F-measure is almost the same as that of the state-of-the-art word segmenter on the test set in the same domain as the training data (Neubig et al. 2011 ). This improved word segmenter model is capable of contributing to various NLP applications in the invention disclosure domain in Japanese. In addition the accuracy does not seem to be saturating, thus we can improve more by only more annotator work based on the partial annotation strategy.
 In this section we introduce a notion of non-maleficence 13 of language resources. Briefly this means that the effects of a language resource in a certain domain does no harm in another domain.

Our experiments indicate that non-maleficence holds for Japanese word segmentation. In order to show this empirically, we provide results for an experiment in which we executed the adaptation of word segmenter KyTea to Twitter (micro blog) for 47 h in addition to two domains which we described in Section 5 . The adaptation method is exactly the same as the invention disclosure domain (patent).

In the experiment we made five models of KyTea in total. The first one is the default model without adaptation. The next three are models adapted to the recipe domain, invention disclosure (patent) domain, and Twitter. They are trained from the language resources in each domain in addition to the default language resources. In the recipe case we set n  X  8 because it is realistic as we discussed above. In the patent case we used the maximum size of partially annotated sentences, i.e the 12-h work result corresponding to the rightmost point in Fig. 4 . The final KyTea model is trained from the language resources in all three adaptation domains in addition to the default language resources. Then we measured the accuracy of each model on the test data in the general domain, recipe domain, patent domain, and Twitter.
Table 8 shows the results. From these results we first see that the corpus addition to each domain improves the performance in that domain. In addition it does not degrade the performance in other domains. 14 We call this characteristic of the pair of a task and language resources non-maleficence. Its conditions are as follows:  X  Language resource addition in a target domain is efficient for the target domain  X  Language resource addition in other domains is not harmful for the target domain We also see that in some cases language resource addition in other domains has a positive effect on the target domain. We term this additivity of language resources. A clear example is the model trained from the recipe tested on the patents. The reason may be that the recipe texts covers how-to expressions and the many frequent technical terms have already been covered by the general texts. On the contrary, language resource addition in the patent domain does not improve the performance on the recipe domain. Thus, we can say that the language resource addition is not symmetric . The model adapted to all the three domains performs almost always the best. 15 In this case as well, language resource addition is non-maleficent. The users of the NLP tool can always use the model trained from the maximum language resources, but not the model trained only from the language resource in the target domain. 16 In this paper, we first reported to what extent two strategies of language resource addition contributed to improvements in word segmentation and POS tagging accuracy in Japanese. The first strategy is adding entries to the dictionary and the second is adding annotated sentences to the training corpus. In the experimental evaluations, we first showed that the corpus addition strategy allows for achievement of better accuracy than the dictionary addition strategy in the Japanese morphological analysis task.

We then introduced the partial annotation strategy, in which only important points are annotated with word-boundary information, and reported the real cases focusing on word segmentation in Japanese. The experiment showed that adding word candidates to the training data as partially annotated data with about three different contexts is efficient to improve a word segmenter.

Finally we investigated various language resource addition cases and introduced the notion of non-maleficence, additivity, and asymmetricity of language resources for a task. Briefly non-maleficence means that language resource addition in a certain domain does no harm in another domain, additivity means that language resource addition in other domains has a positive effect on the target domain, and asymmetricity means that the effect of language resource addition is not symmetric. In the WS case, language addition is non-maleficent and sometimes additive. So it is enough for us, NLP tool providers, to distribute the only one model trained from all the language resources we have.
 References
