 1. Introduction
Authorship analysis has a long history mainly due to research on literary works of disputed or unknown authorship. The Federalist Papers (some of them claimed by both Alexander Hamilton and James Madison) is a famous case ( Mosteller &amp; Wallace, 1984 ). However, in certain cases, the results of authorship attribution attention to authorship analysis in the framework of practical applications, such as verifying the authorship of emails and electronic messages ( Abbasi &amp; Chen, 2005; Argamon, Saric, &amp; Stein, 2003; de Vel, Anderson, Cor-ney, &amp; Mohay, 2001 ), plagiarism detection in student essays ( van Halteren, 2004 ), and forensic cases ( Chaski, 2001 ).
Authorship identification is the task of predicting the most likely author of a text given a predefined set of candidate authors and a number of text samples per author of undisputed authorship ( Peng, Shuurmans,
Keselj, &amp; Wang, 2003; Stamatatos, Fakotakis, &amp; Kokkinakis, 2000 ). From a machine learning point of view, this task can be seen as a single-label multi-class text categorization problem ( Sebastiani, 2002 ) where the can-didate authors play the role of the classes. As concerns the text representation, various measures have been proposed in order to quantify the stylistic choices of the authors. Among them, function word frequencies, character n -gram frequencies, vocabulary richness measures, word-class frequencies, and syntactic analysis measures. Holmes (1998) provides an excellent review of the different stylometric techniques while Zheng, Li, Chen, and Huang (2006) emphasize on modern approaches.

Very often, a common problem in authorship identification cases (at least for some of the candidate authors) is the lack of text samples of undisputed authorship to be used for training. It is not unusual, only extremely limited text samples to be available for some authors. On the other hand, a big amount of text samples may be available for other candidate authors. Note that text samples should be of comparable length. Another real-istic scenario is to have (more or less) equal amount of texts of undisputed authorship for all the candidate authors, however short texts are available for some of them and long texts for others. Hence, in the procedure of normalizing the length of training text samples, few text samples will be produced for some authors and many text samples for others. From a machine learning point of view, this constitutes the class imbalance prob-lem (i.e., uneven distribution of the training set over the classes) in a classification task. This problem has been studied mainly within the framework of two-class datasets ( Japkowicz &amp; Stephen, 2002 ). The main approaches to deal with class imbalance attempt to re-balance the training set by performing: Under-sampling of the majority class, and Over-sampling of the minority class.

In general, it is unclear which approach is more effective and there have been attempts to combine them ( Estabrooks, Jo, &amp; Japkowicz, 2004 ). Another main approach is to attempt to modify the sensitivity of the classification algorithm so that errors on minority class to be costlier than errors on majority class ( Veropo-ulos, Campbell, &amp; Christianini, 1999 ). Last but not least, the SMOTE approach ( Chawla, Bowyer, Hall, &amp;
Kegelmeyer, 2002 ) creates new synthetic training data for the minority class. This is achieved by adding a small random value to some of the features of original training data and producing new data which lie close to the original ones in the multi-dimensional space of the problem.
 Given a text categorization task, each training text is considered as a unit for constructing the training set. Usually, the length of the training texts is fixed or defined by the source of the documents ( Sebastiani, 2002 ).
Little work has been done on how the training texts can be efficiently segmented in order to provide multiple training text samples to assist the re-balancing of the training set. Moreover, text categorization often requires several thousand of features producing sparse data. Hence, producing synthetic data based on an approach such as SMOTE does not seem to fit well.

In this paper, we present methods to segment the training texts into text samples according to the size of the class. The main idea is that textual data can be handled in a flexible way so that to produce a variable amount of text samples of variable length. That is, minority class can be segmented into short samples and majority class into longer samples. Therefore, we transform an imbalanced multi-class textual training set into a bal-anced set. Moreover, we explore text re-sampling methods in order to construct a training set according to a desirable distribution over the classes. In other words, text re-sampling can be viewed as providing new syn-thetic data that increase the training size of a class. Based on two text corpora, namely, newswire stories in
English and newspaper reportage in Arabic, we present a series of authorship identification experiments on various multi-class imbalanced cases.

A basic assumption of inductive learning is that the test set distribution over the classes is similar to the training set distribution. This is obvious in tasks such as disease detection, where the disease may appear only in a few cases. Hence, the percentage of disease cases in training and test set should be similar. However, in other tasks, such as author identification, the distribution of the training set over the classes is affected by fac-tors irrelevant to the dataset itself. For instance, only a couple of texts of unquestioned authorship may be available for a certain author. This should not be taken as evidence that this author is unlikely to be the author of an unknown text. Therefore, where tasks as author identification are examined, the test set should not fol-low the distribution of the training set. Rather, the test set should be equally distributed over the classes so that the performance of the produced model to be fairly evaluated. In this paper, we follow this procedure.
The rest of this paper is organized as follows. Section 2 describes our authorship identification approach focusing on language-independent text representation. Section 3 briefly presents the text corpora and imbal-anced multi-class datasets used in this study. Section 4 includes the presented methods and the evaluation experiments. Finally, Section 5 summarizes the main conclusions drawn by this study and indicates future work directions. 2. Author identification 2.1. Representing style
One great challenge is to define an appropriate quantitative text representation so that the stylistic choices of the author to be revealed. Several types of features have been used so far including lexical and character features, syntactic features (part-of-speech frequencies), structural features (use of greetings, signatures) etc., ( Zheng et al., 2006 ). Since the focus of this study is on text sampling techniques to avoid the class imbal-ance problem, features on the document level (e.g., use of greetings, types of signatures, number of paragraphs etc.) are not appropriate. Moreover, the features should be stable in representing the style of very short text samples.

The most straightforward approach to represent a text is by using word frequencies, a method widely applied to topic-related text categorization as well. To this end, the most appropriate words for stylistic pur-poses may be selected in an arbitrary way ( Mosteller &amp; Wallace, 1984 ) according to their discriminatory potential on a given set of candidate authors. Burrows (1987) first indicated that the most frequent words of the texts (like  X  X nd X ,  X  X o X , etc.) have the highest discriminative power for stylistic purposes. Interestingly, these words are usually excluded from topic-related text categorization systems. Moreover, using high-frequency words as style markers is a language-independent procedure.
 A simple but powerful text representation technique for stylistic purposes is a  X  X ag of character n -grams X .
Character n -grams (contiguous characters of fixed length) are able to capture complicated stylistic information on the lexical, syntactic, or structural level. For example, the most frequent character 3-grams of an English tion. 1 Kjell, Woods, and Frieder (1994) used character bigrams and trigrams to visualize stylistic differences while Keselj, Peng, Cercone, and Thomas (2003) proposed a model based on character n -gram representation for author identification. A variation of this model achieved the best results in the ad-hoc authorship attribu-tion contest ( Juola, 2004 ), a competition based on a collection of 13 text corpora in various languages (English, French, Latin, Dutch, and Serbian-Slavonic). Interestingly, character n -grams proved to be a useful representation for topic-based text categorization as well ( Lodhi, Saunders, Shawe-Taylor, Cristianini, &amp; Wat-kins, 2002 ). Note also that using the n -gram representation on the character level, the sparse data problems that arise in n -grams on the word level are significantly reduced. 2.2. The bag of character n-grams approach
In this paper, we are based on the frequencies of occurrence of the most frequent character n -grams of the training corpus in order to represent a text sample. Let G frequency of occurrence) of the most frequent n -grams (i.e., character sequences of length n ) of the training set.
Consider f ij as the normalized frequency of occurrence of the j th n -gram of G represented as the ordered vector  X  f i 1 , f i 2 , ... , f represented as a vector of 5000 character 3-gram frequencies of occurrence.
A support vector machine, a supervised learning algorithm based on the structural risk minimization prin-ciple ( Vapnik, 1995 ), is then applied to these multi-dimensional vectors. This is an appropriate classification algorithm for text categorization tasks since its learning ability is independent of the feature space dimension-ality, because it measures the complexity of the hypotheses based on the margin with which they separate the data, instead of the features.

Note that this approach is language-independent. However, for achieving best results one should explore the most appropriate amount and length of n -grams for a particular natural language (e.g., for Arabic less and longer n -grams would be more effective). This is out of the purpose of this study since we focus on eval-uating the performance of authorship identification under imbalance conditions. 3. Text corpora and datasets
Two text corpora have been used in this study, one in English and one in Arabic. Both include texts by ten different authors (100 texts per author). In more detail, each text corpus consists of texts belonging to the same genre:
English corpus: newswire stories in English taken from the publicly available Reuters Corpus Volume 1 (RCV1) ( Lewis, Yang, Rose, &amp; Li, 2004 ). There are four main topic classes in RCV1: CCAT (corpo-rate/industrial), ECAT (economics), GCAT (government/social), and MCAT (markets). Each of these main topics has many subtopics and a document may belong to a subset of these subtopics. Although, not particularly designed for evaluating author identification approaches, the RCV1 corpus contains  X  X y-lines X  in many documents indicating authorship. The top 10 authors with respect to the amount of texts belonging to the topic class CCAT were selected.

Arabic corpus: newspaper reportage in Arabic downloaded from the website of Al-Hayat . Ten authors were selected according to the amount of available texts online.

In both corpora, only the main body of the text was considered (titles, author names, dates, etc. were excluded). No pre-processing was performed on these texts apart from removing xml and html tags irrelevant to text content. Note that for the English corpus steps to reduce both the genre and the topic factor have been taken to be hoped that the authorship differences will be a more significant factor in differentiating the texts.
On the other hand, in the Arabic corpus the texts for a certain author may cover several topics. On the other hand, some authors may share specific topics. Since the features used in this study (character n -grams) are able to capture stylistic as well as thematic information, the topic factor may strengthen or weaken the differences between the authors of the Arabic corpus. Moreover, the average text of the Arabic corpus (4378 characters) is longer than the average English corpus text (3089 characters). Each corpus was divided into 50 training and 50 test texts per author. 3.1. Multi-class imbalanced datasets
In the case of two-class classification problems, the class imbalance can be easily defined as the imbalance ratio of the majority class size to the minority class size. Although, any multi-class problem can be reduced to a series of two-class classifications we need a simpler way of expressing the degree of imbalance of a multi-class problem.

In order to simulate the imbalance conditions of a multi-class real-world authorship identification case, we assume a Gaussian distribution of training texts over the candidate authors. Given this setting, the multi-class imbalance ratio of the problem can be defined as where peak is the size (in training texts) of the biggest class and base is the size of the smallest class. Fig. 1 shows examples of producing artificially imbalanced distributions of the training set over 10 classes. Note that all authors have at least base training texts while each author has a separate imbalance ratio raging from 1 to peak / base . Authors near the center of the distribution are considered majority classes while the authors at both sides of the distribution are considered minority classes. By modifying base and peak values it is possible to construct multi-class imbalanced datasets that resemble a real-world scenario.

Based on the English and Arabic corpora we formed a number of datasets representing different multi-class imbalance conditions. In particular, we applied a Gaussian distribution to the entire training set using different combination of base (2, 5, and 10) and peak (10 and 50) values giving imbalance ratios from 2 to 25. The pro-duced combinations (cases) can be seen in the first columns of Table 2 . For comparative purposes, the bal-anced cases of equal values of peak and base (10 or 50) are also considered. 4. Experiments 4.1. Tested methods In order to handle the class imbalance problem, several methods were tested. In more detail
Method-1: Under-sampling of the majority classes based on training texts. For all authors, an amount of training texts equal to the base were used. The length of each text is not modified.

Method-2: Under-sampling of the majority classes based on training text lines. All the available training texts per author were concatenated in one big text. Let x Then, the first x min text lines of each big file were segmented into text samples of length a (in text lines).
Note that, in both corpora, each text line comprised at least one full sentence. It was observed that small values of a (2 or 3) tend to provide better results. The results presented in Table 2 correspond to a =3.
Method-3: Re-balancing the dataset by text samples of variable length. As previously mentioned, one big file is produced per author. Then, each big file is segmented into text samples according to the length of the file. That is, the text samples are of length x i / k ( k is a predefined parameter). Majority authors have long text samples and minority authors have short text samples. Thus, a balanced dataset is produced having k text samples per class. Experiments for k = 10, 20, and 50 were performed. Table 2 presents results for k = 50 which was the best in the majority of the cases. Note that each text line of the training corpus is used exactly once in the text samples.

Method-4: Re-balancing the dataset by text re-sampling. Again, one big file is produced per author. Let x and x max be the text-length (in text lines) of the i th author and the longest file, respectively. Then, k + x text samples each having x i / k lines are produced for each author ( k is a predefined parameter). Hence, a variable number of text samples is produced per author according to the length of the big file. However, the relation is now inversed. Many short text samples are produced for the minority classes and less but longer text samples are produced for the majority classes. In addition, the text lines included in a text sample are selected randomly. A text line may be included in more than one text sample. Fifty runs of this method were performed and the average accuracy is presented in Table 2 (for k = 50).

Table 1 summarizes information about the amount of text samples per author and the text-length (in text lines) of each text sample produced by each examined method. The last column of this table indicates whether different text samples of the same author may share some text lines or not. Note that although method-1, method-2, and method-3 produce balanced training sets, method-4 produces an imbalanced training set. How-ever, the originally minority classes are now represented by more samples in comparison to the originally majority classes. This is clarified in Fig. 2 . Fig. 2 (left) depicts the distribution of the original training set,
Fig. 2 (right) shows the training set distribution produced by method-1, method-2, method-3 and method-4, respectively. Recall that the length of the text samples of the original training set is determined by the source of the texts. On the other hand, the length of the training text samples produced by methods 3 and 4 depends on the size of the class. Majority classes have longer text samples while minority classes have shorter text sam-ples. Moreover, method-4 attempts to compensate this disadvantage of minority classes by adding more text samples into the training set. The smaller a class is in the original training set, the more training samples are produced for it. 4.2. Evaluation results In order to evaluate the performance of a method handling the class imbalance problem we need a baseline.
For each case, the baseline accuracy is provided when no special technique is used to re-balance the training set (each training text is considered as unit and all training texts are used by the classification model). More-over, the balanced cases of having 10 or 50 training texts per author were also examined for comparative pur-poses. The latter case (i.e., base = peak = 50) could be viewed as an indicative upper bound for the performance of the remaining imbalanced cases since it is based on a significantly larger training set.
Table 2 shows the performance of the aforementioned methods on the English and the Arabic corpus. Note that in all cases the test set is the same (50 texts per author) and not overlapping with the training set. So, for a given corpus, the accuracy results obtained by different methods are directly comparable. For the balanced cases (first two lines of each corpus) the method-1 is exactly the same with the baseline approach. Note that this is an indication of the difficulty of the two corpora. Hence, the English corpus is more difficult (recall that in average the English corpus texts are shorter than the Arabic corpus texts). The first important point is that some of the examined methods achieve to improve the performance of the baseline approach in three out of four balanced cases (ratio = 1). This can be explained by the fact that Method-3 and Method-4 take into account the differences in text-length among the candidate authors. That is, despite the fact that we have equal amount of training texts per author, the variation in text-length produces another type of imbalanced training set.

Considering the imbalanced cases (ratio &gt; 1), method-1 fails to outperform the baseline in most of the cases. Recall that this method does not take into account the full training set available per author.
Method-2 is better than the baseline for the English datasets but worse on the Arabic datasets. Recall that this method depends on the number of available text lines per author. Although Arabic texts were longer than
English in average, x min was roughly half of the corresponding value of the English datasets. However, this method was the best in three cases. It has also to be noted that low a values (many short training text samples per class) were found to perform better in most of the cases. The performance of method-3 was really com-petitive, especially for the Arabic datasets. However, method-4 was superior in the majority of the cases.
As concerns the imbalance ratio of the cases, it does not seem to be strongly relevant with the success of a particular method. In the Arabic datasets, for a given base , the best accuracy results are significantly improved as peak increases. This means that the extra training texts that become available as peak increases significantly contribute to the classification model. This happens to a lower extent in the English datasets. However, recall, that the English datasets have lower baseline accuracy.

A closer look to the identification results will reveal significant properties of these methods. Table 3 shows the identification accuracy per author for the English corpus imbalanced by base = 5 and peak = 20. The sec-ond column indicates the number of training texts available for each author while the third column shows the results for each author using the baseline approach. The performance of methods 1 X 4 is indicated as deviation from the baseline. As can be seen, the performance of the baseline method roughly resembles the distribution of training texts over the authors. That is, the more training texts available for one author, the better the iden-tification accuracy. Method-1 improves the accuracy for the minority authors (A1, A2, A9, and A10) but fails to keep the accuracy of the majority authors on high level. Method-2 achieves better results. It improves the identification for the minority authors (more or less the same with method-1) without a dramatic loss in majority authors. Method-3 achieves to retain the identification accuracy for the majority authors on very high level (it even improves some of them) but it fails to significantly improve the minority authors.
On the other hand, method-4 considerably improves minority authors with the cost of a slight reduction on accuracy for the majority authors. 5. Conclusions
Many text categorization tasks, including authorship identification, suffer from the class imbalance prob-lem. Extremely few training texts are available for some authors while plenty of training texts are available for other authors. We presented an approach to handle multi-class imbalanced textual data effectively in order to re-balance the training set in favor of the minority classes. To this end, various text sampling and re-sampling methods were examined. The main idea of the most successful method was to produce many short text samples for the minority classes and less but longer text samples for the majority classes. Since textual data can be eas-ily segmented in small pieces, they can be handled more flexibly in comparison to other kinds of data. A character n -gram representation was used in order to quantify the stylistic choices of the authors.
Although it requires higher dimensionality, the sparseness of the data is significantly reduced in comparison with word-based approaches. This enables efficient representation for short text samples (e.g., each comprising 1 X 5 text lines).

By following method-4, it is easy to construct synthetic data by concatenating text lines selected randomly from the available training texts. Recall that in the used corpora each text line comprised at least one full sen-tence. A basic assumption of this method is that such a text sample will still resemble the style of the author.
To this end, the bag of n -grams representation is quite suitable since it is practically independent of the context of words or even sentences. It remains to be tested whether this method can be applied to topic-related text categorization tasks as well. On the other hand, a more sophisticated approach could be followed in order to select the most suitable text lines to form as good training text samples as possible.

The basic methods presented here can be combined in order to further improve the results. For instance, method-3 and method-4 can be applied together in order to train an enhanced classification model. Alterna-tively, they could be used to train different classification models which, then, can be combined in an ensemble of classifiers. Recall from Table 3 that the classification errors made by these methods are to a great extent uncorrelated, a crucial condition to build effective ensembles.

An important factor, not considered in this paper, is the amount of candidate authors. Both corpora were based on ten different authors. Although this number seems sufficient for many real-world author identifica-tion cases, it should be tested whether the presented methods are affected by scaling into more/less classes. Another interesting direction is the examination of different text representations for authorship identification. To this end, both word-based schemas and variable-length character n -grams could be tested.
 Acknowledgement The author thank the anonymous IP&amp;M reviewers for their valuable and insightful comments.
 References
