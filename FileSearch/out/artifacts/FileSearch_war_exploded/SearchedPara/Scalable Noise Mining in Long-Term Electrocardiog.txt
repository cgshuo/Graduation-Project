 Cardiac disease is the leading cause of death around the world; with ischemic heart disease alone claiming 7 million lives in 2011. This burden can be attributed, in part, to the absence of biomarkers that can reliably identify high risk pa-tients and match them to treatments that are appropriate for them. In recent clinical studies, we have demonstrated the ability of computation to extract information with sub-stantial prognostic utility that is typically disregarded in time-series data collected from cardiac patients. Of partic-ular interest are subtle variations in long-term electrocar-diographic (ECG) data that are usually overlooked as noise but provide a useful assessment of myocardial instability. In multiple clinical cohorts, we have developed the pathophys-iological basis for studying probabilistic variations in long-term ECG and demonstrated the ability of this information to effectively risk stratify patients at risk of dying following heart attacks. In this paper, we extend this work and focus on the question of how to reduce its computational complex-ity for scalable use in large datasets or energy constrained embedded devices. Our basic approach to uncovering patho-logical structure within the ECG focuses on characterizing beat-to-beat time-warped shape deformations of the ECG using a modified dynamic time-warping (DTW) and Lomb-Scargle periodogram-based algorithm. As part of our efforts to scale this work up, we explore a novel approach to address the quadratic runtime of DTW. We achieve this by develop-ing the idea of adaptive downsampling to reduce the size of the inputs presented to DTW, and describe changes to the dynamic programming problem underlying DTW to exploit adaptively downsampled ECG signals. When evaluated on data from 765 patients in the DISPERSE2-TIMI33 trial, our results show that high morphologic variability is associated with an 8-to 9-fold increased risk of death within 90 days of a heart attack. Moreover, the use of adaptive downsampling with a modified DTW formulation achieves a 7-to almost 20-fold reduction in runtime relative to DTW, without a significant change in biomarker discrimination.
 J.3 [ Computer Applications ]: Life and Medical Sciences; H.4 [ Information Systems ]: Information System Appli-cations; I.2.1 [ Computing Methodologies ]: Applications and Expert SystemsMedicine and science cardiovascular disease; electrocardiogram; adaptive down-sampling; time-warping; noise mining Heart disease is the leading cause of death around the world. The burden of ischemic heart disease alone in 2011 was 7 million lives [23]. In the United States, heart disease claims 830,000 lives each year (34% of all deaths, or one death ev-ery 38 seconds) [14]. Nearly 151,000 of these deaths take place in patients under the age of 65, and a third occur before the age of 75 [14]. Much of the difficulty in reduc-ing this burden follows from an inability to match cardiac patients to treatments that are most appropriate for their individual risk. As one example of this, devices such as implantable cardioverter defibrillators (ICDs) can be life-saving for patients who experience fatal arrhythmias (over 300,000 sudden cardiac deaths in the U.S. each year among patients with diagnosed coronary disease) [16]. In most of these cases, the effects of the arrhythmia can be reversed if the victim is treated with an electrical shock within the first few minutes. However, existing decision-making methods fail to prescribe ICDs to the majority of patients who die [4]. Conversely, most of the patients who do presently re-ceive an ICD do not receive any benefit from their device [4], resulting in an unnecessary risk to patients and unnecessary costs to the healthcare system.

Physicians use a variety of biomarkers to estimate patient risk and to match patients to treatments. In the setting of heart disease, these biomarkers are typically limited to information available through blood-based measurements of biochemical substrates (e.g., troponin I, C-reactive protein, and brain natriuretic peptide), or through imaging (e.g., left ventricular ejection fraction obtained through echocardiog-raphy) [12, 10]. In both these cases, the focus is on studying information that is present in instantaneous (i.e.,  X  X napshot X ) data, and where this information can be directly measured with limited or no computational aid. Despite these efforts, however, finding biomarkers that can accurately assess pa-tient risk remains a challenge. For instance, while depressed left ventricular ejection fraction is commonly used to iden-tify high risk patients following heart attacks, the absolute number of deaths is far greater among patients with rela-tively preserved left ventricular function [8].

Our recent research aims to address this situation through novel biomarkers that are computationally derived from phys-iological time-series, and that are designed to offer comple-mentary information to existing blood-or imaging-based markers. Of particular interest is information potentially available in long-term electrocardiographic (ECG) signals. The ECG provides a continuous assessment of the electrical activity of the heart, and is routinely collected from pa-tients during hospitalization to determine heart rate and detect arrhythmias. ECG has the advantage of being easy to acquire; the electrical activity of the heart can be mea-sured on the surface of the body in an inexpensive and non-invasive manner over long periods. In an in-patient setting, the ECG is typically captured by bedside monitors. In an out-patient setting, a Holter monitor (a portable ECG de-vice worn by patients) can record data continuously over multiple days. Since the ECG is routinely collected from patients in a wide variety of clinical settings, computational biomarkers deriving from long-term ECG time-series can be incorporated broadly into clinical practice without the need for any new hardware or without creating any additional burden on patients or caregivers.

In this paper, we focus on the question of how we can leverage long-term recordings of ECG activity (over days) to discover information that is unrecognized in previous stud-ies centered on small snippets of data (over seconds). Our recent experiments, reported in the clinical literature [21], have approached this question by exploring the hypothesis that much of what is commonly perceived as noise in ECG signals may contain subtle but useful information about the health of the heart. The theory underlying this work is that increased variability in the morphology of ECG time-series is likely associated with a lack of consistency and repeata-bility in the electrical function of the heart. In other words, persistent fluctuations in the shape of the ECG waveform may suggest electrical instability in the heart muscle pre-disposing patients to fatal arrhythmias. The challenge in detecting this variability, however, is in being able to distin-guish between shape deformations associated with patholog-ical phenomena reflecting the health of the underlying heart muscle, and changes associated with artifacts that represent true noise. Making this distinction is difficult in short ECG recordings, but with the availability of long-term ECG time-series, pathological variations can be distinguished from true noise as structure that is persistent over long periods of time.
In preliminary clinical studies, we have developed the com-putationally generated ECG biomarker of pathological mor-phologic variability (which we subsequently refer to only as morphologic variability or MV) in long-term ECG to predict death following heart attacks. As a first step, our approach to measuring MV makes use of a modified dynamic time-warping (DTW)-based algorithm to quantify time-warped shape deformations in ECG time-series over long periods of time. As a second step, we draw upon a Lomb-Scargle pe-riodogram approach to analyze the resulting non-uniformly sampled time-series representation of aggregate noise in the ECG for pathological structure. While the use of this method-ology has shown promise in clinical cohorts [21] and provides information that is complementary to existing markers based on patient history and physical exam findings, echocardio-graphy, and blood-based laboratory reports, our prior work has demonstrated the clinical utility of MV and not focused on the question of how to scale this basic approach to large databases of long-term ECG time-series or use in embed-ded devices (e.g., pacemakers and ICDs). To achieve this, we investigate here a novel approach that reduces the com-putational runtime of DTW through an adaptive downsam-pling of time-series inputs. The use of adaptive downsam-pling significantly reduces the ECG data presented to DTW while preserving rapidly changing waves (e.g., the QRS com-plex) smoothed out by existing downsampling approaches. However, due to adaptively downsampling rapidly changing parts of the ECG time-series less than more slowly chang-ing parts of the signals, this approach also requires changes to the dynamic programming problem underlying DTW. In this paper, we present solutions for both the goal of adap-tively downsampling ECG time-series, and for modifying the DTW dynamic programming formulation to leverage adap-tively downsampled inputs.

We evaluate our ideas on data from 765 patients pre-senting with acute coronary syndrome in the DISPERSE2-TIMI33 trial. Our results show that high MV is associated with an 8-to 9-fold increased risk of death within 90 days of a heart attack. Moreover, the use of adaptive downsam-pling with a modified DTW formulation achieves an almost 4-fold reduction in runtime relative to DTW, without a sig-nificant change in biomarker discrimination. In contrast, existing downsampling approaches obtain a similar reduc-tion in runtime but with noticeably worse performance for risk prediction.

The remainder of this paper is organized as follows. Sec-tion 2 presents background on ECG time-series and the pathophysiological basis for our efforts to mine noise in ECG time-series for information relevant to cardiac risk predic-tion. Section 3 describes the methodology for measuring MV using a modified DTW-and Lomb-Scargle periodogram-based approach. Section 4 proposes the concept of adaptive downsampling, and details how adaptive downsampling can be incorporated within the measurement of MV to scale it up to large amount of long-term time-series data. Section 5 presents the datasets and evaluation methodology for our study. Section 6 discusses the results of our experiments. Section 7 offers a summary and conclusions. We begin with a brief background on ECG time-series and on the pathophysiological basis for mining noise in ECG time-series for information useful in predicting future cardiac risk. The ECG is a continuous recording of the electrical activity of the heart muscle or myocardium[13]. At rest, each car-diac muscle cell maintains a voltage difference across its cell membrane. During depolarization (i.e., the  X  X iring X  of the heart muscle), this voltage increases. Consequently, when depolarization is propagating through a cell, there exists a potential difference on the membrane between the part of the cell that has been depolarized and the part of the cell at resting potential. After the cell is completely depolarized, its membrane is uniformly charged again, but at a more positive voltage than initially. The reverse situation takes place during repolarization, which returns the cell to base-of ECG waveform. line. These changes in potential, summed over many cells, can be measured by electrodes placed on the surface of the body, leading to the ECG time-series.

The ECG is a quasi-periodic signal (i.e., corresponding to the quasi-periodic nature of cardiac activity). Three major segments can be identified in a normal ECG. The P wave is associated with depolarization of cardiac cells in the upper two chambers of the heart (i.e., the atria). The QRS com-plex (comprising the Q, R and S waves) is associated with depolarization of cardiac cells in the lower two chambers of the heart (i.e., the ventricles). The T wave is associated with repolarization of the cardiac cells in the ventricles. The QRS complex is larger than the P wave because the ven-tricles are much larger than the atria. The QRS complex also coincides with the repolarization of the atria, which is therefore usually not seen on the ECG. The T wave has a larger width and smaller amplitude than the QRS complex because repolarization takes longer than depolarization[13]. Figure 1(a) presents a schematic representation of the nor-mal ECG, while Figure 1(b) shows an example tracing of a continuous ECG time-series over a few seconds. In a healthy myocardium, the pathways of depolarization through excitable cells are usually similar for consecutive heart beats. However, in the presence of abnormalities, the conducting system has multiple irregular islands of severely depressed and unexcitable myocardium [7] (Figure 2). These lead to discontinuous electrical characteristics of the heart muscle, and create a situation analogous to the presence of race conditions within the heart. The presence of several possible adjacent pathways (e.g., the pathways shown in red and blue in Figure 2) that can potentially invade the non-functioning area leads to variations in the spatial direction of the invading vector [2]. Measured electrical activity in this setting is best described in a highly variable form, stemming from subtle unstable conduction bifurcations. Furthermore, the propagation of a beat may be dependent on the route of propagation of the previous beat. The overall effect of such minor conduction inhomogeneities is not well understood, but it is believed that they correlate with myocardial elec-trical instability and have potentially predictive value for ventricular arrhythmias [2] or other adverse events. Figure 2: Illustration of process underlying noise-like pathological variability in ECG time-series. In healthy myocardium, electrical impulses are con-ducted smoothly through the heart muscle. In un-healthy myocardium (e.g., due to the presence of is-chemia), unstable conduction bifurcations result in the path of impulse propagation, and consequently the ECG morphology, changing from beat to beat. In this section, we describe our basic approach to measure MV in ECG time-series. In developing this methodology, we note that key challenges to measuring pathological variabil-ity that is often overlooked as noise in ECG signals include dealing with the presence of true noise and time-skew in these data, and addressing the need to discover potentially low amplitude disease signatures in the presence of high am-plitude baseline activity. Section 4 describes how this basic approach can be scaled to very large volumes of ECG time-series through the use of adaptive downsampling.

For every pair of consecutively occurring beats in an ECG time-series, we quantify how the shapes of the beats dif-Figure 3: Comparison of time-warped shape de-formations in ECG beats using DTW. In contrast to comparing activity that is time-aligned but not physiologically aligned (left), we use DTW in our study to relate similar parts of the ECG waveforms across beats in the presence of time skew. fer using a variant of DTW (Figure 3). Given time-series Q = q 1 ,...,q n and C = c 1 ,...,c m , DTW first constructs an n -by-m distance matrix where each entry ( i,j ) repre-sents the distance d ( q i ,c j ). The l 2 norm is typically used to measure d ( q i ,c j ). DTW then finds the minimum cost path W = w 1 ,...,w k ,...,w K through this distance matrix where w k = ( i k ,j k ) relates the i k -th sample of Q to the j sample of C . The minimum cost path has the cost: and is subject to several constraints, including boundary conditions, continuity, and monotonicity[3]. This optimal path can be found efficiently using dynamic programming with the following recurrence: where  X  ( i,j ) is the cumulative distance of the path from the start to cells ( i,j ). From simple observation, DTW ( Q,C ) =  X  ( n,m ) and the time and space complexity of this method is O ( nm ).

We restrict the local range of the alignment path in the vicinity of a point to prevent biologically implausible align-ments of large parts of one beat with small parts of an-other. For example, for an entry ( i,j ) in the distance ma-trix, we only allow valid paths passing through ( i  X  1 ,j  X  1), an adaptation of the Type III and Type IV local continuity constraints proposed by Myers et al. [17] and ensures that there are no long horizontal or vertical edges along the op-timal path through the distance matrix, corresponding to a large number of different samples in one beat being aligned with a single sample in the other. This leads to the following recurrence relation (also shown graphically in Figure 4):  X  ( i,j ) = d ( q i ,c j )+ Figure 4: Illustration of possible path alignments.
The process described here transforms the original ECG time-series into a sequence of time-warped morphology dif-ferences between consecutive beats. To characterize patho-logical structure within this sequence, we study its spec-tral characteristics. Since the activity of the heart is quasi-periodic (i.e., since the heart does not beat at an exact rate), the time gap between the samples of the sequence con-structed through DTW is not uniform. We address this issue by estimating the power spectral density of the morphology differences time-series using the Lomb-Scargle periodogram [15]. For a time series where the value m [ n ] is sampled at time t [ n ], the Lomb-Scargle periodogram estimates the en-ergy at frequency  X  as: where  X  and  X  are the mean and variance of the m [ n ], and  X  is defined as :
We define our computationally generated biomarker, MV, as energy between 0.30 and 0.55 Hz (as estimated from the Lomb-Scargle periodogram) in the time-series of aggregate morphology changes constructed using DTW. The range of 0.30 to 0.55 Hz is based on theoretical and empirical ob-servations suggesting that the discriminative ability of MV for predicting death following heart attacks is maximized over this range [22]. A flow chart of the whole process for generating MV is shown in Figure 5. The basic methodology described in Section 3 parallels the approach investigated in our earlier clinical experiments [21]. Here, we focus on the question of how this methodology can be scaled for use with very large volumes of ECG data. Figure 5: Flow chart of the process for generating Morphological Variability (MV).

The runtime of measuring MV is dominated by the time taken to quantify time-warped morphology differences be-tween consecutive beats. For a total of p beats in an ECG time-series of length less than n , the computational com-plexity of this step is O ( pn 2 ). While reducing the number of consecutive pairs of beats to be examined (i.e., reducing p ) offers one approach to reduce the overall runtime of MV, this approach is made challenging by multiple factors (e.g., poorer estimation of spectral energy, less data available to distinguish between persistent pathological variations and true noise, increased latency for real-time decision-making etc.). As a result, our efforts largely center on addressing the quadratic runtime of DTW (i.e., reducing the n 2 term above). We achieve this by exploring the idea of adaptively downsampling the time-series inputs to DTW, and describe how the dynamic programming problem inherent in DTW can be evolved to handle adaptively downsampled signals. Our work builds upon the use of downsampling to improve the efficiency of the basic DTW algorithm.The general idea underlying this approach is to reduce the O ( n 2 ) runtime of DTW by reducing the sizes of its inputs. Popular ex-isting approaches such as aggregate approximation (PAA) [9] and FastDTW [19] achieve this by downsampling signals uniformly, i.e., by a constant factor across time. We note that while the use of downsampling improves the runtime and space efficiency of DTW, the decision to carry out this downsampling by a constant factor over time causes both rapidly and slowly changing parts of a signal to be treated similarly. Downsampling in this case may be associated with the loss of important information.

We believe that this process can be improved by exploiting slowly changing parts of a signal by downsampling them at a higher rate than rapidly changing regions. In contrast to PAA and FastDTW, we therefore propose the idea of adaptive downsampling where the rate of reduction of time series varies according to the rate of changes taking place locally. This allows for the reduction of time series, while retaining sharp changes that would otherwise be smeared if downsampling were applied uniformly to the entire signal.
We achieve adaptive downsampling using trace segmenta-tion [11]. While we describe this approach in more detail subsequently, the basic idea underlying trace segmentation is to divide the signal into regions with equal cumulative derivative activity. This places a higher number of bound-aries for downsampling in regions that are rapidly changing (i.e., have higher cumulative derivative activity).
More formally, given a signal Q = q 1 ,...,q n and a number of frames  X  to downsample this signal to, we first calculate the cumulative difference D Q [ k ] for k = 2 ,...,n between each neighboring pair of samples: with D Q [1] = 0. The sum of the total differences in Q is given by D Q [ n ]. The cumulative difference in each adaptive downsampling bin is then set to d Q = D Q [ n ]  X  . Using this, downsampling proceeds by finding the sample numbers t for i = 0 ,..., X  such that for all values of i we have:
The corresponding amplitudes of Q at samples t i are given by x i = q t i . We can then use interpolation to approximate the fractional sample numbers  X  t i where we would expect D
Q [  X  t i ] = d Q  X  i . For i = 0 ,..., X  using the notation: we have:
The resulting adaptively downsampled representation of the original signal Q is given by two series corresponding to time and amplitude:
This process can be carried out in time that is linear in the size of the input. Figure 6 presents the trace segmentation approach for downsampling graphically.

For ECG time-series, trace segmentation can preserve im-portant information related to sharply changing parts of the signal (e.g., the QRS complex). This is illustrated in Fig-ure 7. In contrast to PAA, a similar number of adaptively 0 50 100 150 200 250 300 350 400  X 1 0 1 0 50 100 150 200 250 300 350 400 0 5 10 15 0 50 100 150 200 250 300 350 400  X 1 0 1 Figure 6: An illustration of the trace segmentation process. downsampled segments provide a better characterization of notching within the R wave and also the sharpness of the S wave. While PAA achieves good results in a variety of real-world application domains, we believe the distinctions retained by adaptively downsampling are relevant to the spe-cific goal here of measuring MV to predict death following heart attacks. DTW searches for the optimal alignment between two se-quences in an efficient manner using dynamic programming. For uniformly downsampled signals, the dynamic program-ming process is essentially unchanged, although it is applied to reduced representations of the original signals. For adap-tively downsampled signals, however, the cost of alignment cannot be calculated in a similarly simple manner from the Euclidean distance between the samples of the downsampled representations. Since the original signal is now divided into segments of variable lengths, this length information needs to be factored into consideration when deriving the distances for the DTW dynamic programming recurrence.
 We represent two adaptively downsampled signals Q and C as comprised of segments s q (1), ... , s q (  X  ) and s respectively, with  X  corresponding to the number of down-sampled segments. The amplitude of each segment s q ( i ) is
Figure 7: Adaptive downsampling of ECG signals. represented by x q ( i ) and the duration by l q ( i ) (similar nota-tion is used for the amplitude and duration of each segment s ( i )). Using this notation, we describe the process through which the dynamic programming of DTW can be modified to handle adaptively downsampled segments.

Figure 8 shows, from left to right, three separate pos-sibilities when aligning adaptively downsampled segments. In each case, the alignments of the adaptively downsam-pled segments are illustrated at the top, and the alignments of the original signals are illustrated below. The leftmost subfigure shows the situation where the adaptively down-sampled segments are diagonally aligned, i.e. segment s q is aligned with segment s c ( j ), while s q ( i + 1) is aligned with segment s c ( j + 1). Intuitively, we expect the warp-ing path between the samples comprising the segments s q ( i ) and s c ( j ) in the original signals to be close to the diagonal. Without solving for the optimal path of alignment between these original samples, we approximate the cost of align-ment between the segments s q ( i ) and s c ( j ) as the product of d ( s q ( i ) ,s c ( j )) (i.e., the Euclidean distance of x x ( j )) and max( l q ( i ) ,l c ( j )) (i.e., an estimate for the length of a diagonal path). We adopt a similar approach for the subfigure shown in the middle of Figure 8. In this case, the adaptively downsampled segment s q ( i ) is aligned with both s c ( j ) and s c ( j + 1). Again, without solving for the optimal path of alignment between the original samples for these segments, we expect the path of alignment for the Figure 8: Illustration of derivation for adaptive re-currence equation. Left subfigure shows the diago-nal case, the middle shows the horizontal case, while the right subfigure shows the vertical case. samples comprising s q ( i ) and s c ( i ) to be roughly horizontal. We therefore approximate the length of this path to be the the rightmost subfigure (i.e., a roughly vertical path of align-ment for the samples comprising s q ( i ) and s c ( j )) is treated analogously.

We note that our approach of modifying the dynamic pro-gramming of DTW for use with adaptive downsampling ap-proximates the path length in each case, and this approx-imation may not be optimal. However, this approach pro-vides a simple way to augment the dynamic programming of DTW. In particular, in this setting, the recurrence relation for the cumulative path distance  X  ( i,j ) till the adaptively downsampled segments i and j can be represented as: where the cumulative path distance  X  ( i,j ) depends on the direction in which the path proceeds next (i.e., diagonal d , horizontal h , or vertical v ) and:  X  ( i,j, h ) = d ( s q ( i ) ,s c ( j )) l q ( i ) + min  X  ( i,j, v ) = d ( s q ( i ) ,s c ( j )) l c ( j ) + min In Section 3 we described a modification to the basic DTW recurrence relation to find more biologically plausible align-ments (i.e., the situation shown in Figure 4). In this case, we made use of the recurrence relation:  X  ( i,j ) = d ( q i ,c j )+
We adopt an analogous approach to constrain DTW with adaptive downsampling for more meaningful alignments. Since the original signal is divided into unequally sized segments, we note that the above recurrence would not be directly ap-plicable. Instead of restricting valid paths to pass through no more than 3 consecutive horizontal or vertical steps, we therefore restrict the path to traverse through at most k steps such that no such implausible alignment would occur. In other words, a segment s q ( i ) is only allowed to align with segments of s c such that the total length of those k segments is no greater than three times the length of s q ( i ), which can be expressed as 3  X  l q ( i )  X  P k n =1 l c ( j  X  n ). We evaluated our research on ECG data from patients in the DISPERSE2-TIMI33 trial [5]. Patients in the DISPERSE2-TIMI33 trial were enrolled if they experienced ischemic symp-toms at rest for a duration exceeding 10 minutes with either biochemical marker evidence of myocardial infarction (de-fined as Tronponin-T, -I, or creatinine kinase-MB elevation greater than the local myocardial infarction decision limit) or ECG evidence of ischemia (defined as the presence of new or presumably new ST-segment depression  X  0.05 mV, transient ST-segment elevation  X  0.1 mV, or T-wave inver-sion  X  0.1 mV in 2 or more contiguous leads). As part of this study, continuous electrocardiographic (ECG) data were recorded for a median duration of 4 days. Three-lead Life-Card CF Holter monitors were placed within 48 hours of the initial event, and the data were sampled at 128 Hz. Patients were followed up for a period of 90 days for cardiovascular death. In our study, we used data from the first 24 hours of ECG recording during hospitalization to predict the risk of death following heart attacks. There were a total of 765 pa-tients in the DISPERSE2-TIMI33 trial with available ECG signals used in this analyses, with 14 deaths during follow-up. On average, each 24 hour recording contained 103,180 instantaneous heart rate measurements.

We evaluated MV, MV measured with downsampling us-ing PAA, and MV measured with adaptive downsampling for cardiovascular risk stratification in multiple ways. First, we measured the areas under the receiver operating charac-teristic curves (AUROCs), which reflects the ability of the different MV approaches to discriminate between patients who died during follow up and those that remained event free. The AUROC is widely used in medicine, and is gen-erally considered the standard for evaluating risk stratifica-tion methods [1]. As part of this evaluation, we compared the AUROC values for the downsampled MV approaches to the basic MV algorithm without downsampling using the method proposed by DeLong et al. [6] to assess whether the changes are statistically significant. Second, we also as-sessed the MV models with downsampling relative to the basic MV algorithm without downsampling by measuring the integrated discrimination improvement (IDI) proposed Table 1: Univariate association of MV and other clinical variables with death following heart attacks. by Pencina et al. [18]. This was done by translating the MV values obtained through each approach into regression-based probabilistic risk estimates, and then measuring the differ-ence between the mean predicted probabilities of events and non-events.

In addition to evaluating the predictive accuracy of MV measured through each approach, we also evaluated the run-time of the algorithms as the average time taken across ten runs to compute MV for all patients. These experi-ments were performed on a machine with quad-core Intel Xeon X3450 processors (2.67 GHz, 8MB Cache) and 8 GB RAM. The distance metrics were uniformly implemented in C++ on the Red Hat Enterprise Linux Server release 5.6 (Tikanga).

Finally, we also assessed how the relative ranking of pa-tients between the different MV approaches changed with adaptive and non-adaptive downsampling. This metric was used to study how downsampling moves patients relative to each other while measuring MV. To measure this informa-tion, we computed the average absolute difference in the ranking of each patient by MV across different approaches. The basic MV algorithm achieved an AUROC of 0.75 for discriminating between high and low risk patients following heart attacks. When the MV predictions were dichotomized at a simple threshold (MV &gt; 50 vs. MV  X  50), patients with high MV were found to be at a significantly increased risk of death following heart attacks (Figure 9). For compari-son, we show the relative increase in risk between patients with high and low MV, as well as the relative increases in risk for a variety of existing clinical variables in Table 1. In the DISPERSE2-TIMI33 dataset, MV identified a group of patients at a higher relative risk than any of these other metrics. These results are consistent with our earlier find-ings reported in the clinical literature [20]. Table 2 compares the AUROC for the basic MV algorithm with the AUROCs obtained for MV measured with down-sampling using PAA and MV measured with adaptive down-sampling. For both the downsampling approaches, we exper-imented with downsampling the original heart beat signals to down to 30, 50 and 70 samples.
 Figure 9: Kaplan-Meier mortality curve for patients in high MV (MV &gt; 50; shown in red) and low MV (MV  X  50; shown in blue) groups. Patients with high MV were at a consistently elevated risk of death over the 90 day period following a heart attack.
 Table 2: Comparison of AUROCs between DTW, PAA-DTW, and ADAP-DTW
In general, downsampling the original signal led to a re-duction in the discriminative ability of MV (although this difference was not significant at the 5% level given the sam-ple size). In all of our experiments, however, MV with adap-tive downsampling achieved a higher AUROC than down-sampling with PAA for a similar factor of reduction. These results suggest that our use of adaptive downsampling re-tained more information that was relevant to the task of distinguishing between high and low risk patients than the use of PAA for this application.

We also assessed changes in the clinical utility of MV with downsampling by computing the IDI for each downsampled approaches relative to the basic MV algorithm. These re-sults are presented in Table 3. In this case, the data from our experiments show that (consistent with the AUROC case) the use of downsampling with PAA led to a small decrease in performance. Conversely, the use of adaptive downsam-pling actually resulted in an increase in discriminative per-formance relative to the basic DTW algorithm as measured by the IDI. The differences for both downsampling with PAA and with adaptive downsampling relative to the basic DTW algorithm was not significant at the 5% level given the sam-ple size. Table 3: IDI comparing DTW with PAA-DTW and ADAP-DTW Table 4: Average change in patient ranks relative to the basic DTW algorithm (shown as percentages of the DISPERSE2-TIMI33 population).
 Table 5: Timing of the different MV algorithms.
 The relative changes in ranks of patients between the basic DTW algorithm and the DTW approaches with downsam-pling are shown in Table 4. Consistent with the AUROC and IDI results, DTW with adaptive downsampling resulted in a smaller relative change in rank within the DISPERSE2-TIMI33 population relative to the basic DTW algorithm.
Finally, the timing results for the different methods are shown in Table 5. While downsampling reduced the runtime of the basic DTW algorithm substantially in each case, this reduction was greater for PAA than with the use of adap-tive downsampling. This result can be attributed to the additional work that needs to be done to solve the modified dynamic programming problem for adaptively downsampled DTW. Comparing the PAA and adaptively downsampled approaches based on time rather than downsampling fac-tor, however, still showed a higher level of performance with adaptive downsampling than with the use of PAA (e.g., for PAA 70 AUROC: 0.693, IDI: -0.007, average rank change: 20.9% and time: 9,207 vs. for ADAP 30 AUROC: 0.718, IDI: 0.017, average rank change: 18.3% and time: 7,831) In this paper, we studied the question of mining noise-like variations in long-term ECG time-series to identify patients at an increased risk of death following heart attacks. To achieve this, we made use of a modified DTW-and Lomb-Scargle periodogram-based approach that first transforms ECG time-series into sequences of beat-to-beat time-aligned morphology differences, and then relates properties of these sequences to patient risk. While the ideas underlying this work derive from our earlier experiments reported in the clinical literature [21], we focused here on the question of how this basic approach can be scaled to very large ECG time-series databases and for use in embedded devices. As part of this work, we investigated a novel approach to ad-dress the quadratic runtime of DTW. In particular, we pro-posed the idea of adaptive downsampling, i.e., downsam-pling slowly changing parts of a signal much more than rapidly changing parts of the same signal, to reduce the size of the inputs presented to DTW while retaining a good rep-resentation of the original time-series being compared. We also described changes to the dynamic programming under-lying DTW to exploit such adaptively downsampled signals, where the downsampled segments may be of varying lengths.
We evaluated our ideas on real-world data from patients within the DISPERSE2-TIMI33 trial. Our experiments sug-gest that measuring MV with adaptive donwsampling sub-stantially reduces runtime while providing similar perfor-mance to the basic MV algorithm that is not optimized for large volumes of data. In addition, the use of adaptive down-sampling leads to more accurate performance than down-sampling through the commonly used approach of PAA. We believe that these ideas may have additional merit in clini-cal applications where physiological time-warping is an issue during stratification. We thank Dr. Benjamin Scirica and the TIMI Study Group at the Brigham and Women X  X  Hospital for the data used in our experiments. We are also grateful to Profs. John Guttag and Collin Stultz at MIT for their input on improving this work. This research was supported, in part, by the National Science Foundation and the American Heart Association. [1] D. Altman. Practical statistics for medical research . [2] S. Ben-Haim, B. Becker, Y. Edoute, M. Kochanovski, [3] D. Berndt and J. Clifford. Using dynamic time [4] F. BJ. Defibrillators are lifesaver, but risks give pause. [5] C. Cannon, S. Husted, R. Harrington, B. Scirica, [6] E. DeLong, D. DeLong, and D. Clarke-Pearson.
 [7] N. El-Sherif, B. Scherlag, R. Lazzara, and R. Hope. [8] J. Goldberger, M. Cain, S. Hohnloser, A. Kadish, [9] E. Keogh, K. Chakrabarti, M. Pazzani, and [10] H. Krumholz, P. Douglas, L. Goldman, and [11] M. Kuhn, H. Tomaschewski, and H. Ney. Fast [12] L. Lilly. Pathophysiology of heart disease . Lippincott, [13] L. Lilly. Pathophysiology of Heart Disease:: A [14] D. Lloyd-Jones, R. Adams, T. Brown, M. Carnethon, [15] N. Lomb. Least-squares frequency analysis of [16] R. Myerburg. Sudden cardiac death: exploring the [17] C. Myers, L. Rabiner, and A. Rosenberg. Performance [18] M. Pencina, R. D X  X gostino Sr, R. D X  X gostino Jr, and [19] S. Salvador and P. Chan. Fastdtw: Toward accurate [20] Z. Syed, B. Scirica, S. Mohanavelu, P. Sung, [21] Z. Syed, B. Scirica, C. Stultz, and J. Guttag. [22] Z. Syed, P. Sung, B. Scirica, D. Morrow, C. Stultz, [23] WHO. The top 10 causes of death.
