 Chinese words in sentences are not explicitly separated by spaces. Chinese word segmentation is the preliminary task to segment Chinese sentences into words in order to do deeper processing such as part-of-speech tagging and parsing. learning, annotated data is crucial for the performance of word segmentation. But annotated corpora are limited in size and scope due to the time-consuming and labor-intensive annotating.
 improve word segmentation is therefore researched in order to make use of more annotated data [1,2]. For example, annotated sentence (b) in Figure 1 can be used together with (a) by using annotation adaptation methods, though the two annotations are not totally consistent. But still, fully annotated data is limited. also contain clues for word segmentation. For example, the sentence (c) in Figure 1 with brackets is from the Chinese Wikipedia. The bracketed phrase  X   X   X   X   X   X  used to make a hyperlink to another page is also a valid phrase in sentence (a) and (b). Comparing to the sentences intentionally segmented by skilled annotators to make training data, partially annotated sentences by general netizens can be obtained easily. Another example is the anchor texts in the HTML files on the Web, which is indeed web-scale. improve Chinese word segmentation: the learning algorithm needs to be adapted to learn from partial annotations; a relation is needed to bridge the gap between the arbitrary annotation by netizens and the annotation of words.
 partially annotated data and propose an adapted perceptron algorithm that can learn from partially annotated data for both supervised and semi-supervised learning (Section 2). Fully and partially annotated sentences are mixed and not distinguished in this algorithm.
 tially annotated sentences for word segmentation (Section 3). In such repre-sentation, sentences annotated with brackets (Figure 1 (c)) and sentences from heterogeneous corpus (Figure 1 (b)) can be treated in the same way. the Chinese Treebank 5 (Section 5). Sentences from the MSR corpus, People X  X  Daily corpus as well as Baidu Baike (a Chinese wikipedia-like website) are used as partially annotated sentences to improve the performance of the baseline model.
 tion with partially annotated data which can treat various resources as partially annotated data; 2) we use the naturally annotated sentences provided by com-mon netizens as a resource to improve the performance of word segmentation. 2.1 Partially Annotated Data as Training Data x , a unique y i is given as the gold standard output.
 not be determined by using only the partial annotation. Instead, a nonempty subset Y i of GEN( x i ) which contains the unknown gold standard output is given. The training examples are thus represented as { ( x i ,Y i ) } , where  X  X  X  Y i  X  GEN( x i ) and y i  X  Y i .
 where Y i = { y i } . 2.2 Perpectron Algorithm for Partially Annotated Data Collins [4] proposed a perceptron algorithm for structured classification tasks such as part-of-speech tagging. Since it is widely used for Chinese word segmen-tation, we decide to adapt this algorithm for partially annotated data. work [3]. The adapted algorithm is a natural extension of the traditional one [4]. When Y i = { y i } holds for all the training example, the adapted perceptron algorithm degenerates to the traditional one.
 tated example ( x i , GEN( x i )), since z i  X  GEN( x i ) is always true, the updating in the if statement will never be executed.
 adapted algorithm is not theoretically guaranteed. But fortunately, this algo-rithm works well in practice as we will show. 2.3 Self-Training with Partially Annotated Data Partially annotated sentences can be also used for semi-supervised algorithms such as self-training.
 tences in the training process. In Step 4, we use the margin to define the confi-dence: training algorithm. First, in Step 2, examples that not fully annotated in P are also used to train the model (we call it  X  X  train X  in the experiments). Second, in Step 3, when segmenting an example ( x i ,Y i ) in P , the search space of the decoding is limited in the set Y i (we call it  X  X  predict X  in the experiments). 2.4 Distributed Learning for Large-Scale Training Data As we mentioned that available partially annotated sentences are large-scale, there are two reasons to use distributed learning for large-scale training data. First, when partially annotated sentences are much more than fully annotated sentences, the learning is harder to converge. Second, our distributed method is faster and is suitable for incremental learning.
 model using these sets are  X  (1) ... X  ( n ) , respectively. Then we calculate the pa-rameters of the final model as: where  X  ( k ) i is the i -th parameter of the k -th model. Note that the denominator in this equation is not n . The reason is that when  X  ( k ) i = 0, it is not because this feature is not important, but because this feature is unseen in the training process of the k -th training data.
 new training data is acquired, we only need to train a new model  X  ( n +1) using the new data and then update the parameter  X  without using any old data. Now we narrow down our discussion to the Chinese word segmentation task. seperate words, while a segmented sentence is a sentence where words are sep-arated by spaces. For example,  X   X   X   X   X   X   X   X  is a raw sentence, and  X   X   X   X   X   X   X   X  is one of the possible segmented sentences corresponding to the raw sentence.
 sentence. The corresponding span set z for the segmented sentence above is ginning and end of the word  X   X   X   X .
 mented corpus or wikitexts, y can not be determined. Instead, we need to define the set Y = { z j } which include the gold standard output ( y  X  Y ) and exclude some impossible outputs. Before giving the definition of Y , we introduce some basis concepts in the next subsection. 3.1 Agreement between Span Sets We used spans in the span set z to indicate words. Note that spans can also be used to indicate other linguistic components such as morphemes and phrases. Roughly speaking, all these components of a sentence are organized as a hierar-chical tree. In other words, following the definition by Klein and Manning [5], the spans of any two components will never cross: agree (or are non-cross).
 if any span in S agrees with any span in S 0 .
 phrases in the same sentence.
 sumption to define partially annotated examples. Although we do not directly use this assumption in this paper, it is the motivation that we choice spans and the  X  X gree X  relation to define Y . In the following subsections we will give two more assumptions based on our observation. They may not always hold as the previous one but can be used to define Y effectively. 3.2 Partially Annotated Sentences from Heterogeneous Segmented For a sentence x from a heterogeneous segmented corpus, the given gold standard output y 0 may be different with the gold standard output y we expected. Using ( x,y 0 ) as the training data will result in bad performance. agree with each other ( y  X  y 0 ).
 between different annotation specifications is about the granularity of words. This means that a word under one annotation specification is generally still a word, phrase or morpheme in another annotation specification.
 The set Y is defined by using heterogeneous annotation y 0 . And with Assumption 1, we have y  X  Y . 3.3 Partially Annotated Sentences from Wikitexts Based on our observation, although the annotation in wikitexts is more arbitrary, bracketed texts are still usually words, phrases or morphemes. So similar to the method we used for the heterogeneous corpora, we give an assumption: same sentence agree with each other.
 where b is the span set of bracketed texts. 3.4 Mixed Training Data It is not sufficient that we only use partially annotated sentences defined above as the training data. If so, the algorithm may result in an unexpected optimum that the model segments every single character as a word. Those results will agree with any span sets.
 sentences and randomly shuffle them as the training data. And our learning algorithm can treat them in the same way. In recent years, learning with partially annotated data is concerned by re-searchers of machine learning [6] as well as natural language processing. Partially annotated data can be used for corpus construction [7], sequence labeling [8], syntactic parsing [9,10] and other NLP tasks [11]. Our algorithm can be seen as a version of the latent structure perceptron [12] which can learn from examples with hidden variables [3]. Zettlemoyer and Collins [13] used similar algorithm for semantic parsing.
 such as co-training [15] may also benefit from partially annotated sentences with our method.
 to generate new features to improve the performance of joint word segmentation and part-of-speech tagging model. Sun and Wan [2] further used the re-training method to transform the heterogeneous corpus in order to use it directly as the training data. Jiang et al. [1] further used iterative annotation transformation with predict-self reestimation to improve the performance.
 tistical information of the unannotated corpus [17,18,19,20]. Punctuation marks can be seen as artificial annotations for natural language. Li and Sun [21] used the punctuation marks in the unsegmented corpus as clues for word boundaries. Spitkovsky et al. [22] used hyper-text annotations for unsupervised English pars-ing.
 word segmentation model proposed by Zhang and Clark [24,25] and the linear-time incremental shift-reduce parser proposed by Huang and Sagae [26]. We use Penn Chinese Treebank 5 (CTB) as the main corpus of our experi-ments. The partitions of training set (18,086 sentences) and test set are the same with [25]. We use the training data of the MSR corpus from SIGHAN bake-off 2005 (86,924 sentences) and People X  X  Daily (PD) corpus from Peking University (294,239 sentences) as the heterogeneous corpora.
 tional Chinese characters and the translation method is not straightforward, we turn to use another wiki-like site Baike from Baidu 3 containing only simplified characters. One million sentences with brackets are used, which is still a small part of the total sentences with brackets that can be extracted.
 first place in the CIPS-SIGHAN bakeoff 2012 [27] as our baseline model 4 . The feature templates are listed in table 1. In all the semi-supervised experiments the parameter k in the self-training algorithm (Figure 3) is set to 10. Since there are no hyper-parameters that are tuned, we directly show the results on the test set instead of the development set.
 5.1 Supervised Learning with Partially Annotated Data We mix the training data of CTB and the partially annotated sentences gener-ated from other resources together as the training data for supervised learning. heterogeneous corpus and wikitexts can improve the performance. Note that the PD corpus is much large than the MSR corpus. Probably because the converging is harder when the rate of partially annotated sentence is high, we find that using a quarter of these sentences are even better than using all at once. 5.2 Self-Training with Partially Annotated Data Three different self-training algorithms are performed and compared. The con-ventional self-training algorithm without using any partially annotated infor-mation is denoted as  X  X aseline X . The self-training algorithm proposed by us in Figure 4 is denoted as  X  X  train+p predict X . We also use an algorithm like  X  X  train+p predict X  but does not use partially annotated sentences in the train-ing process which is denoted as  X  X  predict X .
 rithms outperform the supervised algorithm. The algorithm  X  X  train+p predict X  can always improve the performance. For a corpus like MSR where the partial annotation is relatively rich, only using  X  X  predict X  can also improve the perfor-mance. But for Baike 50K where annotated information is rare, the difference between the performances of  X  X  predict X  and  X  X aseline X  is not obvious. 5.3 Distributed Learning with Large Data From the experiment results of the supervised learning we already find that simply using large partially annotated dataset is not always helpful. distributed learning to train the models. PD corpus in the self-training algorithm. Slashed lines are four curves of using a quarter of the PD corpus, respectively. The solid line is the curve of the averaged model based on these four small models using Equation 2.
 from Baike (divided into 25 sets). Table 3 shows the final results of our method and the results of related work. It is not surprise that our method do not out-perform the annotation adaptation method [1], since we only treat the hetero-geneous corpus as a partial annotated corpus. But with the same method, we can make use of the partial annotation information in the wikitexts. Our word segmentation model using one million Baike sentences is comparative to the joint word segmentation and part-of-speech tagging model [20] using approximately 208 million additional words from Xinhua newswire.
 We presented a learning method with partially annotated sentences for Chinese word segmentation. Naturally annotated data such as wikitexts can be treated as partially annotated sentences and be used as training data together with fully annotated sentences. Our method is potentially suitable for domain adaptation where in-domain fully segmented sentences are limited.
 for Chinese language processing. In the future, we will try to use similar method for word-based active learning and syntax parsing.
 The authors want to thank ZHANG Junsong from the cognitive lab and SHI Xiaodong and CHEN Yidong from the NLP lab of Xiamen University for the support of experiments.
 for the Doctoral Program of Higher Education of China (Grant No. 20120121120046) and Natural Science Fundation of Fujian Province (Grant No. 2010J01351).

