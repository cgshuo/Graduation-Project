 Hua-Yan Wang wanghy@cis.pku.edu.cn Qiang Yang qyang@cse.ust.hk Hongbin Zha zha@cis.pku.edu.cn Multiple instance learning (MIL) has become an ac-tive area of investigation in machine learning since it was first put forward for drug activity predic-tion(Dietterich, 1997). In MIL, we consider  X  X nstance-bags X , which are unordered sets of instances. Each instance is represented as a feature vector. Accord-ing to the original definition, a bag of instances is la-beled as positive if at least one of its instances is posi-tive, and it is labeled as negative if all of its instances are negative. In real-world applications of MIL, the focus is on assigning labels to bags rather than in-stances. Many methods have been proposed to solve the MIL problem, including Axis-Parallel Rectangles (Dietterich, 1997), Diverse Density (Maron, 1998), EM-DD (Zhang, 2001), Citation k -NN (Wang, J., 2000), and variations of SVM (Andrews, 2003; Gart-ner, 2002; Kwok, 2007; Bunescu, 2007).
 A major difficulty of MIL arises from the ambiguity caused by not knowing which instances determined the bag labels. According to the original definition of MIL (Dietterich, 1997), a bag can be labeled as positive based on just one positive instance in it. However, since the instance-labels are unknown in the outset, we need to leverage the available information conveyed by all instances to determine the label of a bag. This mo-tivates us to carefully examine the underlying mech-anism of how the bag labels are determined by the instances within the bag.
 Firstly, examining the algorithmic aspect of the mech-anism we could conclude that, even if there is an un-ambiguous intrinsic mechanism ( e.g. , a bag is posi-tive iff at least one instance is positive), it can hardly benefit a MIL algorithm deterministically due to the unknown instance labels. Instead, possible instance labels are usually leveraged in a probabilistic manner. For example, (Zhang, 2001) computes posteriors of in-stance labels in an EM-like algorithm; (Kwok, 2007) marginalizes a kernel function over possible instance labels. In other words, virtually all instances in a bag can contribute to a bag label.
 Our second observation is that the intrinsic mecha-nisms of how the instances determine the bag-labels can vary in different application domains of MIL; these mechanisms do not necessarily obey the original defi-nition of MIL. Instead, they must be relaxed to allow more flexibility. Recall that in the original definition of MIL, a bag is positive iff at least one instance is posi-tive. This clearly defines MIL problems in some appli-cations such as drug activity prediction, because the  X  X ositive X  instances in these applications could serve as strong or even definite evidence for labeling a bag as positive. For example, if a molecule binds well to some target protein (positive instance), the molecule undoubtedly binds well and the associated bag is la-beled positive. However, in other applications, this restriction is too limiting. In many real world applica-tions, the bag-label determining mechanism can allow a bag label to be negative even when there exists a pos-itive instance in it; such relationship between instances and bags should be probabilistic in nature. For exam-ple, in content-based image retrieval (Zhang, 2002), images are represented as  X  X ags X  of localized features (regions). The low-level instance representation, de-scribing color, texture, and shape, may have no direct correspondence to high-level image-labels ( e.g. , human faces, buildings, the sky etc. ). Instead, they only serve as weak evidences that should be integrated together to determine the image-label. Intuitively, the localized image feature descriptors can be  X  X  region appears like a human eye or a region appears like a human nose X . The decision to label an image as a  X  X uman face X  should leverage many such pieces of evidence, because a non-face images (negative bags) can also contain some other object that appears like a human nose (positive instance). In this example, a positive in-stance can be found in a negative bag, which violates the traditional definition of MIL. Therefore, our solu-tion to the MIL problem should be flexible enough to allow for different bag-label determining mechanisms . The mechanisms of how the instances determine the bag labels is an essential issue in MIL. However, to our best knowledge, none of existing MIL techniques has explicitly addressed the issue of the cross-domain differences of this mechanism . In this paper, we pro-pose a new framework for MIL that includes the orig-inal definition of MIL as a special case, and yet allows for more flexible cases. Our solution is to automat-ically adapt the instance-to-bag-label mechanism to accommodate the differences in various formulations of the MIL problems. Our main contribution is to capture the mechanism by a simple model, embod-ied in a parameter p of a kernel function (Sch  X olkopf, 2001) defined over the bags. This parameter is learned from labeled bags in the training data without a priori knowledge of that mechanism .
 Our adaptive framework for MIL is supported by a number of motivations. First, explicitly describing the mechanism (as (Dietterich, 1997) did for drug activity prediction) for an application domain calls for strong domain knowledge. Second, a hand-crafted mecha-nism could be subjective and unreliable. Third, de-signing different MIL methods for different application domains is inefficient, given the large number of appli-cations that has a potential to be formalized as MIL. Thus it is better to design an adaptive formalism for this task.
 In our framework, a two-phase learning procedure is adopted to characterize a kernel function on the bags, which can be used as a distance function in classifi-cation via algorithms such as SVM, or as a similarity measure for information retrieval.
 The first learning phase exploits the unlabeled in-stances with a mixture model to characterize the in-trinsic structures of the feature space of instances. Each bag is represented by some aggregate posteriors on a mixture of components, which summarizes the bag as compositions of different  X  X atterns X . While the first learning phase adapts to different char-acteristics of the instance space, the adaptive nature of our approach is shown mostly in the second learn-ing phase. We define a kernel mapping by computing a power p of the aggregate posteriors. As we will show in the rest of the paper, the parameter p explicitly captures the domain-specific mechanism of how the instances determine the bag-labels, where the parame-ter p is learned by optimizing an objective function de-fined over the labeled bags. In this way, our framework can adapt MIL algorithms to different instance-to-bag-label mechanisms in many application domains, even if we have no a priori knowledge about them. 2.1. Aggregate posteriors We use lowercase x to denote instances, and upper-case X to denote bags. In MIL, we are provided with a training set { ( X i , y i ) } N i =1 consisting of labeled bags, where y i  X  { +1 ,  X  1 } are labels 1 . Let { x i } n be all instances available for the learning algorithm, where each instance x i resides in a feature space R D . The training set includes both the instances in labeled bags and possibly a large number of other unlabeled instances, because the unlabeled instances are often much easier to obtain than the labeled bags in many sampling domain could demonstrate sophisticated pat-terns due to the underlying unknown generative model of instances. Previous approaches usually impose over simplified assumptions on the generative model; for example, APR (Dietterich, 1997) assumes that  X  X osi-tive X  instances reside in an axis-parallel rectangle, and Diverse Density (Maron, 1998) assumes that the  X  X os-itive X  instances demonstrate a Gaussian-like pattern around some concept point. In our approach, a mix-ture model approximates the underlying generative model of instances, which is much more flexible and informative. We make no additional constraint on the instances used; the instances are chosen for training as long as they are from the same underlying generative model. Note that this is different from semi-supervised learning (Zhu, 2005), for which we usually require the unlabeled samples to come from the specific classes of labeled samples.
 We approximate the underlying generative model of instances by several mixture models in R D . Fitting the mixture models to all unlabeled instances with a given number of mixture components K results in the optimal parameters and weights { (  X  i , w i ) } K i =1 . For Gaussian mixture models (GMM) adopted in our ex-periments, we have { (  X  i ,  X  i , w i ) } K i =1 . Given the above, the likelihood of an instance x under the i -th mixture component is denoted as: For a bag of instances X = { x i } M i =1 and a mixture model { (  X  i , w i ) } K i =1 , we define the aggregation poste-riors of a bag on the mixture components: Definition 1 (Aggregate Posteriors) The aggre-gate posteriors of a bag of instances X = { x i } M i =1 with respect to the mixture model { (  X  i , w i ) } K i =1 noted as:  X  ( X ) := C where C is a normalizing operator indicating dividing a vector by the sum of all its elements. It is straightforward to validate that w j p j ( x i ) P K the posterior probability that x i is generated from the j -th mixture component. The normalizing operator C is induced such that the kernel function (defined later) would be unbiased towards sizes of bags;  X  X arge X  bags and  X  X mall X  bags are treated equally. The aggregate posteriors summarize frequencies of different  X  X attern X  within the bag, which could be viewed as a  X  X ayesian X  histogram because a frequentist would replace the K -component mixture model with K -means clustering, and replace the posteriors with a deterministic vote. Thus, the aggregate posteriors degenerate to a nor-malized histogram.
 The first learning phase of our framework is itself en-dowed with much flexibility and can be customized for specific situations. For example, in some appli-cations the number of available unlabeled instances may be small. We therefore have to reduce the de-gree of freedom in the mixture model accordingly. For example, we could add the restriction that the com-ponents of the Gaussian mixture model have diagonal dimensionality of the instance space is too high to fit a Gaussian mixture model, we can also adopt the fre-quentist X  X  point of view by representing the training bags as histograms obtained by K -means clustering of the instances. 2.2. The order-p kernel mapping We have defined a mapping from bags of instances X to aggregate posteriors  X  ( X )  X  S K , where S K is the ( K  X  1)-simplex that consists of all positive constant-sum real vectors. The aggregate posteriors summarize example, consider a toy case where X 1 , X 2 , and X 3 are three bags in some MIL problem, and we have: We will carefully examine this toy case for an intu-itive understanding of our approach. Aggregate pos-teriors of these bags all demonstrate relatively high values on the third mixture component, and low val-ues on others. According to the definition of aggregate posteriors, the bags all have  X  X ajor patterns X  repre-sented by the third mixture component, with a lot of instances contributing to that pattern, and  X  X inor patterns X  represented by other components, with fewer instances contributing to them.
 The kernel function for the bags serves as a similarity measure that affects the decisions in label prediction. Therefore how to define the kernel function depends on the intrinsical mechanism that the bag-labels are determined. Since the mechanism varies in different application domains, the kernel function should vary accordingly. On one hand, in some applications such as drug activity prediction, positive bags are determined by a few (at least one, actually) positive instances serving as strong evidences, and there can be many negative instances in positive bags. Hence the  X  X i-nor patterns X  in the aggregate posteriors are endowed with considerable significance, given that the  X  X ajor patterns X  could be dominated by overwhelming neg-ative instances. On the other hand, in other applica-tions such as image classification, the positive bags are determined by integrating a lot of low-level weak evi-dences from instances. Hence we should focus on the  X  X ajor patterns X  in accordance with the voting-like mechanism . The  X  X inor patterns X , however, should be underrated because they are attributable to random noise and outliers. For example, in an image classifi-cation task, the  X  X inor patterns X  could be generated by the image background clutter.
 For the toy case, the similarity in minor patterns be-tween X 1 and X 2 is greater than that between X 2 and X 3 , but the similarity in major patterns between X 2 and X 3 is greater than that between X 1 and X 2 . Ac-cording to our previous analysis, whether X 2 should be considered more similar to X 1 or X 3 depends on whether we should place more emphasis on major or minor patterns; the latter in turn depends on the domain-specific instance-to-bag-label mechanism . To endow the kernel function with such flexibility, we de-fine the p -posterior-mixture-model ( ppmm ) kernel: Definition 2 ( p -Posterior-Mixture-Model Kernel) The p -posterior-mixture-model (ppmm) kernel func-tion on a pair of bags X 1 and X 2 is defined as where p  X  (0 ,  X  ) , and &lt;  X  ,  X  &gt; denotes the standard inner-product in R K .
 For the toy case, it is easy to validate that: The parameter p tunes the kernel in a way that a larger p makes it put more emphasis on major pat-terns, and a smaller p draws more attention to the minor patterns. According to our previous analysis, we can predict that a larger p is preferred in appli-cations such as image classification, and a smaller p is preferred in applications such as drug activity pre-diction. However these judgements are based on the fact that we already have sufficient a priori knowledge about these two application domains. If we encounter a novel application domain of MIL, for which we have no a priori knowledge, the p -posterior-mixture-model kernel can be adapted to that novel domain by learning the domain-specific instance-to-bag-label mechanism . Learning the mechanism is implemented by optimizing an objective function of p defined on labeled bags. learn the parameter p via maximizing the alignment (Cristianini, 2002) between the p -posterior-mixture-model kernel and the ideal kernel, which measures the kernel X  X  degree of agreement with the bag-labels: where &lt;  X  ,  X  &gt; F denotes the Frobenius inner-product between matrices. K p is the p -posterior-mixture-model kernel matrix.
 The optimization problem in (2) is easily resolved by exhaustive search within a certain interval of p ( e.g. p  X  (0 , 3] in our later experiments). Because the target function is extremely easy to evaluate and empirically quite smooth, and the search space is only one dimen-sional, even the exhaustive search is fast and scales linearly with respect to the interval considered. 3.1. Synthetic data To empirically validate our analysis in previous sec-tions, we simulate three different multiple instance learning datasets endowed with different instance-to-bag-label mechanisms .
 MIL dataset 1 is synthesized as follows: 1) randomly generate isotropic-covariance Gaussian mixture mod-els in R D with K equally weighted components, from which N  X  S instances are sampled; 2) one mixture component is randomly chosen, and the instances gen-erated by that component are labeled as positive; 3) all N  X  S instances are randomly put into N bags, with S instances in each; 4) each bag is labeled as positive iff there is at least one positive instance in it. MIL dataset 2 and 3 are synthesized similarly. But the instances generated by K 5 mixture components are labeled as positive in MIL dataset 2, and each bag is labeled as positive iff positive instances in the bag ex-ceed 20%. The instances generated by K 2 mixture com-ponents are labeled as positive in MIL dataset 3, and each bag is labeled as positive iff positive instances in the bag exceed 50%.
 Although the synthetic datasets are endowed with dif-ferent instance-to-bag-label mechanisms , all other as-pects of these datasets are the same, which can not be exploited by the algorithm to distinguish these datasets. They all have approximately the same ratio of positive and negative bags if K and S are properly chosen. Although these tasks have different ratios of positive and negative instances, the instance labels are kept from the learning algorithm, which only observe 50%-50% bag-labels. Our approach is expected to dis-cover the mechanism difference among these datasets in such a challenging setting. Moreover, in the first learning phase, the number of mixture components is deliberately set to be different from K , in order to sim-ulate the fact that the characteristics of the underlying true generative model are usually unknown.
 We repeated this experiment for many different choices of the bag size S , mixture model size K , instance space dimensionality D , and we observed that the optimal p value is almost always the smallest for MIL dataset 1, intermediate for MIL dataset 2, and the largest for MIL dataset 3. In Figure 1 we plotted the kernel alignment as a function of p in a typical run of the experiment with K = 20, D = 5, S = 13, and to-tal number of bags N = 200. Note that this specific setting results in approximately the same number of positive bags and negative bags in all datasets. 3.2. MIL benchmark datasets We tested our method on standard MIL benchmark in various application domains including drug activity prediction, image classification, and text classification. 3.2.1. Drug activity prediction The concept of multiple instance learning had been originated from the application of drug activity predic-tion. In this application, the molecules are regarded as bags, and various shapes a molecule can adopt consti-tute instances within the bag. A molecule is considered a  X  X ositive X  bag if it binds well to some target protein, which is true if at least one of its shapes (instances) binds well. The instances are represented as vectors describing that shape. In bio-chemical experiments, we can only observe whether a molecule binds well or not, but if the molecule binds well, we cannot further identify which shape(s) binds well and contributes to the positive bag-label.
 Datasets of drug activity prediction for MIL are MUSK1 and MUSK2. The MUSK1 dataset consists of 47 positive bags, 45 negative bags, and totally 476 instances, each represented as a 166 dimensional vec-tor. The MUSK2 dataset consists of 39 positive bags, 63 negative bags, and totally 6598 instances. 3.2.2. Image classification Content-based image classification/retrieval is another application domain of multi-instance learning. Its ma-jor difficulty arises from the fact that an image consists of not only the object-of-interest, which determines its category label, but also background clutter, which may take up even a larger portion of the image. To segment the object-of-interest from background clutter is yet a challenging open problem in computer vision. A com-mon strategy to perform classification without identi-fying the object-of-interest beforehand is to represent an image by many localized feature vectors instead of a single global feature description. Each localized fea-ture is computed based on a small region of the image, so we could expect that the object-of-interest is cap-tured by a number of local features, even if there are also other irrelevant local features arisen from back-ground clutter. It is therefore quite natural to formal-ized content-based image classification/retrieval as a multi-instance learning problem, where images are the bags and local features are the instances.
 The MIL benchmark dataset includes three image clas-sification tasks X  X o discriminate images that contain elephant , tiger , and fox from irrelevant images, respec-tively. Each image (bag) is segmented into a set of regions (instances), and each region is represented as a 230 dimensional vector describing its color, texture, and shape characteristics. Each classification tasks has 100 positive bags and 100 negative bags. 3.2.3. Text classification Another application domain of multiple instance learn-ing is text classification/retrieval. A document can be divided into a number of segments, which could have different topic focuses. And the category label of the whole document (bag) should be decided by taking into account all these segments (instance), which con-stitutes a multiple instance learning problem. The MIL benchmark dataset contains text data cho-sen from the OHSUMED (Hersh, 1994) dataset on medical issues. We perform two text classification tasks: TREC1 and TREC2. Each consists of 200 positive documents (bags) and 200 negative docu-ments. Each document is segmented into overlapping 50-word-passages, which results in over 3000 passages (instances) in either of the tasks. Each passage is in-dexed by a sparse high-dimensional (over 60,000 index terms) feature vector. 3.2.4. Results In order to make our results comparable to previ-ous published results on these datasets, our exper-iments are conducted in the same way as in most previous works. For each classification task, we use 10-fold cross-validation. Classification accuracies are measured on the 10% hold-out data. Our method is compared with a number of existing multiple instance learning techniques. We replicated the results reported in their original papers for comparison if their results are measured similarly (using 10-fold cross-validation). Some results not available in their papers are marked as N/A (see Table 1).
 For our approach (The PPMM Kernel), the only pa-rameter that has to be manually set is the dimension-ality K of aggregate posteriors ( i.e. number of mix-ture components). We set K = 30 for drug activ-ity prediction data and image data, and K = 40 for text data, since the instances in the text data have higher dimensionality and there are more labeled bags for training. Note that K is chosen subjectively but not carefully tuned for each task X  X uning the param-eter for each task could results in higher classification performance but may be impractical in real-world sce-narios. Other implementation details of our approach in these experiments are the same as in Section 3.1, except that we adopt K -means clustering and his-togram representations for these tasks, because they generally have high dimensional instance representa-tion, and relatively small number of instances. Due to the local-optimal nature of K -means clustering, we tried multiple randomly seeded runs of algorithm, and chose the best one based on their performances on the training set.
 One major advantage of our approach is the capac-ity to utilize a large number of unlabeled instances, but no extra unlabeled instance is available for the benchmark datasets, which implicates that the poten-tial performance of our approach could possibly be un-derestimated in these experiments. Nevertheless, our approach performs generally better than or compara-ble with other MIL techniques (see Table 1). Since the benchmark datasets for MIL are rather small (number of bags ranges from tens to hundreds), slight differ-ences in classification accuracy should not be overly emphasized. Instead, the most encouraging result we obtained is the optimal p values learned in these datasets. Note that the p values for drug activity prediction tasks (MUSK1 and MUSK2) are generally smaller than that for image classification tasks (ELE-PHANT, TIGER, and FOX). Although the p value learned in the FOX dataset is smaller than other im-age datasets, we can further observe that all methods perform unsatisfactorily on the FOX dataset, which may indicate that this classification task itself could be impractical, hence the learned p value may be unre-liable. Interestingly, comparing Table 1 and Figure 1 we could observe that the p values learned in real-world drug activity prediction tasks are close to that learned in synthetic task 1, and the p values learned in real-world image classification tasks are close to that learned in task 2 and task 3. We can also observe that the p values for text classification tasks (TREC1 and TREC2) are also small; possibly this is because the in-stance representation in the text domain are also high-level, and serving as strong evidences for bag-labels. In contrast, instance representation in the image domain are generally low-level ( e.g. color, texture, shape), and they can only be considered as weak evidences for the high-level bag-labels ( i.e. elephant, tiger, fox). The concept of multiple instance learning was origi-nally proposed in (Dietterich, 1997) for the applica-tion of drug activity prediction. The author assumes that positive instances all reside in an axis-parallel rectangle (APR), which implicates specific constraints that the shape should satisfy in order to bind well to some target protein. Although this assumption can be appropriate for this specific application, it is not clear how to adapt it to other applications, which may have more complex intrinsic structures in the instance space, and different instance-to-bag-label mechanisms . Diverse Density (DD) (Maron, 1998) is another general framework for MIL. The author assumes that positive instances form a Gaussian-like pattern around some  X  X oncept point X  in the instance space, which is ex-pected to be close to at least one point in each pos-itive bag and far away from all instances in negative bags. This assumption on the structure of instance space could also be over-simplified for some applica-tions. And the algorithm, by definition, relies on the instance-to-bag-label mechanism in the original defi-nition of multiple instance learning.
 Citation k -NN adapts the memory based classification method k -NN to MIL, which considers not only the references , but also the citers as neighbors of a bag in determine its label, in order to be less affected by the negative instances in positive bags. It had been empirically proved to be more robust than standard k -NN. Nevertheless, the role of instance-to-bag-label mechanism is not clear in this framework.
 Support vector machines (SVM) and the kernel trick (Sch  X olkopf, 2001) have been very successful in tradi-tional supervised learning. There also have been many attempts to apply them to MIL. These works falls into two major categories as summarized in (Kwok, 2007). The first family of methods try to modify the optimization problem of SVM, such as MI-SVM and mi-SVM (Andrews, 2003), which may result in non-convex optimization problems and suffer from local minima. The second family of methods design kernel functions on the bags, including (Gartner, 2002) and (Kwok, 2007). Our approach also falls into the second category, but it possesses a unique characteristic as adapts to various application domains with different instance-to-bag-label mechanisms .
 The aggregate posteriors are essentially positive constant-sum real vectors, which reside in a simplex. Data in a simplex had been addressed from the met-ric learning perspective (Lebanon, 2003; Wang, H.-Y., 2007), which are related to our approach because the kernel defined for aggregate posteriors also gives rise to a distance metric on the simplex.
 The idea of defining a kernel function based on pos-terior probabilities on mixture models had also been exploited in (Hertz, 2006). The author proposed the KernelBoost algorithm for learning with a large num-ber of unlabeled data and few labeled data, in which the weak kernel mappings are defined as posterior probabilities on mixture models.
 The two-phase learning scheme in our approach makes use of both unlabeled instances and labeled bags. It is therefore conceptually related to semi-supervised learning (Zhu, 2005) and self-taught learning (Raina, 2007). In this paper, we proposed a novel framework for adapting multiple instance learning to different mech-anisms of how the instances determine the bag-labels. We showed that this mechanism is different in differ-ent application domains of multiple instance learning, and our approach well captures this domain-specific mechanism through learning with unlabeled instances and labeled bags.
 To the best of our knowledge, this paper is the first work that addresses the problem of adapting multiple instance learning to different application domains with different instance-to-bag-label mechanisms . The ma-jor advantage of such a self-adaptive framework lies in that, if we are encountered with some novel applica-tion domain, which could be well formalized as multi-ple instance learning, but we have no a priori knowl-edge about the instance-to-bag-label mechanisms in that domain, we can learn the mechanisms from la-beled bags, and design a kernel function adapted to this mechanism .
 This work was supported in part by NKBRPC No. 2004CB318000, NHTRDP 863 Grant No. 2006AA01Z302, and No. 2007AA01Z336. Qiang Yang thanks Hong Kong CERG grants 621307 and and CAG grant HKBU1/05C.
 Hua-Yan Wang would like to thank Haiyan Sun for her encouragements and insightful comments.
 Andrews, S., Tsochantaridis, I., and Hofmann, T. (2003). Support vector machines for multiple-instance learning. In NIPS 15 Bunescu, R.C., Mooney, R.J. (2007) Multiple Instance
Learning for Sparse Positive Bags. In Proceedings of the 24th ICML Cristianini, N., Shawe-Taylor, J., Elisseeff, A., and Kandola, J. (2002). On kernel-target alignment. In NIPS 14 Dietterich, T.G., Lathrop, R.H. and Lozano-Perez, T. (1997). Solving the multiple instance problem with axisparallel rectangles. Artificial Intelligence , 89, 31-71.
 Gartner, T., Flach, P., Kowalczyk, A., and Smola, A. (2002). Multi-instance kernels. IIn Proceedings of the 19th ICML Hersh, W., Buckley, C., Leone, T.J., Hickam, D. (1994). OHSUMED: an interactive retrieval evalu-ation and new large test collection for research. In SIGIR X 94 Hertz, T., Hillel, A.B., Weinshall, D. (2006). Learn-ing a Kernel Function for Classification with Small Training Samples. In Proceedings of the 23rd ICML Kwok, J.T. and Cheung, Pak-Ming. (2007). Marginal-ized Multi-Instance Kernels. In IJCAI X 07 Lebanon, G. (2003).  X  X earning Riemannian metrics X , In Proceedings of the 19th UAI Maron, O., Lozano-P  X erez, T. (1998). A Framework for Multiple-Instance Learning. In NIPS 10 Raina, R., Battle, A., Lee, H., Packer, B., Ng, A.Y. (2007). Self-taught learning: Transfer Learning from Unlabeled Data. In Proceedings of the 24st ICML Sch  X olkopf, B., Smola, A.J. (2001). Learning with Ker-nels: Support Vector Machines, Regularization, Op-timization, and Beyond. MIT Press Wang, H.-Y., Zha, H., Qin, H. (2007).  X  X irichlet ag-gregation: unsupervised learning towards an opti-mal metric for proportional data X , In Proceedings of the 24th ICML Wang, J. and Zucker, J.-D. (2000). Solving the multiple-instance problem: a lazy learning ap-proach. Proceedings of the 17th ICML Zhang, Q., Goldman, S.A. (2001). EM-DD: An im-proved multiple instance learning technique. In NIPS 13 Zhang, Q., Goldman, S.A., Yu, W. and Fritts, J. (2002). Content-based image retrieval using multiple-instance learning. In Proceedings of the 19th ICML Zhou, Z.-H., and Xu, J.-M. (2007). On the Rela-tion Between Multi-Instance Learning and Semi-Supervised Learning. In Proceedings of the 24th ICML Zhu, X. (2005). Semi-supervised learning literature survey. Technical Report 1530, Department of Com-
