 the true reward function without needing to specify it.
 expert  X  especially a good expert  X  should make the problem ea sier, not harder. to another was first proposed by Zadrozny &amp; Langford [9].
 are expensive or difficult to obtain. that the expert follows a deterministic policy, and assumpt ion we do not make. belongs to the set  X  H =  X   X  ( H times)  X   X  . In this case,  X  taking action a in state s at time t . Also, if  X  is nonstationary, then  X  taken in state s at time t .
 We define the value function V  X  manner: So V  X  The value of a policy is defined to be V (  X  ) , E [ V  X  that satisfies  X   X  , arg max We write  X  E to denote the (possibly nonstationary) expert policy, and V E V t ( s ) that the values of these policies are with respect to the unknown reward function. Let D  X  t , which generates a trajectory ( s an abbreviation for D  X  E pair ( s, a )  X  D  X  reduction is defined, and also justify the utility of such a re duction. In a classification problem, a learning algorithm is given a t raining set h ( x where each labeled example ( x Y of the learning algorithm is to find a hypothesis h  X  H such that the error Pr small.
 algorithm A such that, whenever A is given a training set of size m = poly( 1 runs for poly( 1 we have Pr error of the best hypothesis in H . The expression poly( 1 not germane to our discussion.
 trajectories from the expert X  X  policy  X  E , where the i th trajectory is a sequence s i The key is to note that each ( s i D ping the state space S to the finite action space A . If m = poly( 1 t , the apprentice can use a PAC learning algorithm for H to learn a hypothesis  X  h with probability at least 1  X  1 bound, this inequality holds for all t with probability at least 1  X   X  . If each  X   X  natural choice for the apprentice X  X  policy  X  A is to set  X  A classifiers to imitate the behavior of the expert.
 assumption about the apprentice X  X  policy.
 Pr remainder of this paper is devoted to confirming this intuiti on. This is indeed the case, as the next theorem shows.
 Theorem 1. If Assumption 1 holds, then V (  X  A )  X  V (  X  E )  X  2  X   X H 2 R max . is because, although each state-action pair ( s i according to D E policy.
 more convenient.
 Lemma 1. Let  X   X  be a deterministic nonstationary policy. If Pr all  X  Proof. Fix any  X  where the first inequality holds because Pr inequality holds because Pr the assumption of the lemma.
 write sa to denote a trajectory, where sa = (  X  s have every state and time step, then its value is not much worse the value of  X  E .  X  t ( s,  X   X  t ( s ))  X  1  X   X  Proof. Say a trajectory sa is good if it is  X  X onsistent X  with  X   X   X  that is,  X   X  (  X  s t  X  and that sa is bad otherwise. We have where the first inequality holds because, by the union bound, P inequality holds because good trajectories are assigned at least as much measure by P because  X   X  is deterministic.
 value of  X  E .
 Pr Proof. Say a trajectory sa is good if  X  E otherwise. We have The first inequality holds because, by the union bound, P inequality holds by our assumption that all rewards are nonn egative. We are now ready to combine the previous lemmas and prove Theo rem 1. choose any  X  Now construct a  X  X ummy X  policy  X   X  as follows: For all time steps t , let  X   X  state s where  X  E and by Lemma 3 Combining these inequalities yields Since  X  a near-optimal policy.
 Theorem 2. If Assumption 1 holds, then V (  X  A )  X  V (  X  E )  X  4  X H 3 R max +  X  V (  X   X  )  X  V (  X  E ) is the suboptimality of the expert X  X  policy  X  E . follows: If our goal is to learn an apprentice policy whose va lue is within  X  other hand, Theorem 2 suggests that the error rate must be red uced by a factor of four. a either set  X  A ( s ) = a s . However, since  X  E is optimal, both actions a s the apprentice policy  X  A will be optimal as well.
 6 below  X  that has a much weaker dependence on the classificati on error  X  when  X  2, we will make a particular choice for the base policy.
 Fix a deterministic nonstationary policy  X  B, X  that satisfies if be chosen so that  X  can be as small as we like.
  X  t ( s,  X  B, X  ( s )) &lt; 1 for all actions a  X  A , and otherwise let  X  B \  X  state s and time step t , the distribution  X  B \  X  probability assigned to action  X  B, X  where  X  B the proof of Lemma 4, it is actually immaterial how the distri bution  X  B \  X  cases; we choose the uniform distribution for definiteness.
 Let  X  B + be a deterministic policy defined by for all states s  X  S and time steps t . In other words,  X  B + assuming that the policy  X  B is followed thereafter.
 ordered pairs { (  X  i ,  X  ( i )) } N nonstationary policy, P N either  X  i prove Theorem 2.
 Lemma 4. V (  X   X  B, X , + )  X  V (  X  B ) .
 Proof. The proof will be by backwards induction on t . Clearly V  X   X  B, X , + s , since the value function V  X  V t ( s ) = R ( s ) + E of best action with respect to V  X  B Lemma 5. V (  X   X  B, X , + )  X  (1  X   X H ) V (  X  B, X  ) +  X HV (  X   X  ) . identical to  X  B, X  , and the value of any component policy is at most V (  X   X  ) . Lemma 6. If  X  &lt; 1 Proof. Combining Lemmas 4 and 5 yields And via algebraic manipulation we have because of our assumption that  X  &lt; 1 We are now ready to combine the previous lemmas and prove Theo rem 2. choose any  X  t , let  X   X   X   X  ( s,  X  A t ( s )) = 1 . By Lemma 3 we have Substituting V (  X  E ) = V (  X   X  )  X   X   X  where we used Lemma 6, (2) and (1), in that order. Letting  X 
