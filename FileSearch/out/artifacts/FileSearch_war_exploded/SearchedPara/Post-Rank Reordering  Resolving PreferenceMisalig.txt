 No search engine is perfect. A typical type of imperfection is the preference misalignment between search engines and end users, e.g. , from time to time, web users skip higher-ranked documents and click on lower-ranked ones. Although search engines have been aggressively incorporating click -through data in their ranking, it is hard to eliminate such misalignments across millions of queries. Therefore, we, i n this paper, propose to accompany a search engine with an  X  X lways-on X  component that reorders documents on a per-query basis, based on user click patterns. Because of posi-tional bias and dependencies between clicks, we show that a simple sort based on click counts (and its variants), albei t intuitive and useful, is not precise enough.

In this paper, we put forward a principled approach to reordering documents by leveraging existing click models. Specifically, we compute the preference probability that a lower-ranked document is preferred to a higher-ranked one from the Click Chain Model (CCM), and propose to swap the two documents if the probability is sufficiently high. Because CCM models positional bias and dependencies be-tween clicks, this method readily accounts for many twisted heuristics that have to be manually encoded in sort-based approaches. For this approach to be practical, we further devise two approximation schemes that make online compu-tation of the preference probability feasible. We carried o ut a set of experiments based on real-world data from a ma-jor search engine, and the result clearly demonstrates the effectiveness of the proposed approach.
 H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval  X  Relevance feedback  X 
Any opinions, findings, and conclusions or recommenda-tions expressed here are those of the authors and do not necessarily reflect the views of the employers and funding agencies.
 Algorithms, Experimentation Web search, Result reordering, Preference models
Web search has become indispensable from everyday life, but it is yet perfect. Although commercial search engines have been leveraging numerous signals (including user clic k-through data) in document ranking, search users still need to skip top-ranked ones and click on lower ones from time to time. Click patterns like this suggest preference misali gn-ments between the search engine and end users. In general, such misalignments are inevitable, because it is hard, if no t totally impossible, for a ranker to consolidate users X  pref -erences across millions of queries into a single ranker. We therefore need an automated mechanism to resolve misalign-ments on a per-query basis, based on users X  clickthrough data.

A quick solution is to sort the top-10 documents in de-scending order of the number of clicks each document re-ceives in the past. This solution assumes that more clicks indicate higher relevance, which could be correct for some queries ( e.g. , navigational) but may not be so in general because of the following reasons: Therefore, in order to sort by clicks, one has to (1) properly interpret clicks by accounting for click biases and disenta n-gling dependencies, and (2) craft a set of rules to determine when and what documents to sort. As one can imagine, the set of rules will likely involve many correlated quantities , e.g. , the absolute number of clicks, clickthrough rate, posi-tions, and their corresponding thresholds. Instead of hack -ing heuristics and tuning parameters, we ask the question: Is there a principled approach to resolving misalignments?
Inspired by statistical hypothesis testing, we propose to respect the original ranking unless there is strong click ev -idence against it. Specifically, we assumes that a search engine puts d i before d j because it prefers d i to d j . We take this preference, denoted by d i  X  d j , as a null hypothesis  X  and check whether user clicks suggest otherwise; if the null hypothesis is disproved with statistical significance, the two documents should be swapped. In this way, misalignments are resolved pair by pair, instead of through a global sortin g.
Unfortunately, the inherent dependencies between clicks prevent many testing techniques from applying because they usually rely on the i.i.d. assumption. We therefore propose to compute the probability that d j is preferred to d i , de-noted by P ( d j  X  d i ), directly from clicks, and account for the dependencies in calculating the probability. If this pr ob-ability is above a threshold , say 0.75, the two documents will be swapped. As will be seen, is the only parameter for preference probability-based methods, and it can be easily set either intuitively or through experiments.

Computing P ( d j  X  d i ) is however non-trivial because it has to account for biases and dependencies. Previous re-search on click models ( e.g. , [6, 9, 13]) considers biases and dependencies, and provides a point estimation r for the rel-evance of each document, but it remains unclear how to compute P ( d j  X  d i ) from r j and r i .

We solve this problem by utilizing the Click Chain Model (CCM) (whose details are briefed in Section 2 and [12]). Different from previous models, CCM models the document relevance in a Bayesian way, so that the relevance estimate i s a full posterior p ( r ) with r  X  [0 , 1], rather than a single num-ber. From posteriors p ( r ) X  X , the preference probability (PP) P ( d j  X  d i ) can be analytically computed by integrating over the area with r j &gt; r i on the joint probability of p ( r show, through experiments based on real-world click data, that document swapping based on the computed PP sig-nificantly outperforms the sort-based methods (Section 5). Furthermore, for this approach to be practical, we devise two methods that compute PPs at query time. Also worth mentioning is that the two methods actually uncover how to relate r i  X  X  to the preference probabilities for the CCM case.
In summary, this study makes the following contributions: 1. We propose post-rank reordering to resolve X  X isaligned X  2. This study expands on the existing work on Click Chain 3. Motivated by practical requirements, we discover how Figure 1: The graphical model representation of CCM. Shaded nodes are observed click variables.
The rest of the paper is organized as follows. Section 2 introduces preliminaries and re-caps the CCM. Section 3 elaborates on the computation of preference probabilities from CCM, and how to use them for document reordering. The two methods that compute preference probabilities at query time are discussed in Section 4. We report on the experimental evaluations in Section 5, and discuss related work in Section 6. Finally, Section 7 concludes this study.
We first introduce definitions and notations that will be used throughout the paper. A web search user initializes a query session by submitting a query to the search engine. Any re-submission or reformulation of the same query is re-garded as another distinct query session. We use document impression to refer to the web documents (or URLs) pre-sented in the first result page, and discard other elements in this page, e.g. , sponsored ads and related search. A docu-ment impression is represented by D = { d 1 , . . . , d M ally M = 10), where each d i can be an index into a set of documents for the query. We say document d i ranks higher than d j if d i appears before d j , i.e. , i &lt; j . All the follow-ing discussion is restricted to a given query unless otherwi se noted.

Click models account for positional bias and dependen-cies between clicks by treating examination and clicks as probabilistic events that are correlated through document relevance. Specifically, for a particular query session, a b i-nary random variables E i is assigned to denote whether the snippet of d i on the search result page is examined by the user, and C i for the click event. For example, P ( E i = 1) is the probability that position i is examined and P ( C i = 1) is the corresponding probability for click. In the followin g, we brief the Click Chain Model. Readers interested in the technical details about CCM and its comparison to other models are referred to [12].
 Figure 1 depicts the graphical model representation of CCM. A distinct feature of CCM from previous models [6, 9, 13] is its Bayesian modeling of the document relevance. In CCM, the relevance of d i is modeled as a random variable R i  X  [0 , 1], and its probabilistic dependencies with E i  X  X  and C  X  X  are specified as below: P ( C i = 1  X  E i = 0) = 0 (1) P ( C 1 = 1  X  E i = 1 , R i ) = R i (2) P ( E i +1 = 1  X  E i = 0) = 0 (3) P ( E i +1 = 1  X  E i = 1 , C i = 0) = 1 (4)
P ( E i +1 = 1  X  E i = 1 , C i = 1 , R i ) = 2 (1  X  R i ) + where  X  X  are user behavior parameters that can be auto-matically learnt from data.

Effectively, these dependencies describe a web user with following behaviors: the user starts the examination of the search result from the top. At each position i , the perceived relevance R i determines how likely d i is clicked (Eq. 2). Clicking or not, the user can continue the examination or abandon the current query session, and the probability to examine the next document d i +1 depends on her action at the current position i . Specifically, if she skips d i ( C this probability is 1 (Eq. 4); on the other hand, if d i is clicked, the probability further depends on the relevance R (Eq. 5). Intuitively, if d i is perceived very relevant, the user will be less motivated to examine the next: parameters 2 and 3 correspond to the probability that d i is extremely irrelevant and relevant, respectively.

Figure 2 illustrates what CCM does using click logs: be-fore observing any log data, CCM assumes a uniform rele-vance prior for each query-URL pair; then upon observing click data, it adjusts the posterior according to Eqs. 1  X  5 using Bayesian inference (details in [12]). Intuitively, i f the URL is clicked, its posterior is pushed to the right and oth-erwise to the left if not clicked, for each of the search sessi ons in log. In general, the more data a query-URL pair has, the spikier the posterior would be; so for unfrequent query-URL pairs, the posterior could be relatively flat but still differ ent from the uniform prior.

By exploiting the particular dependence structure of CCM, [12] derives the posterior for each document in closed form: where m  X  X  are constants that are determined by  X  X , C 1: U represents the U search sessions for the given query, and N is the normalizer for the probability density function. Eq. 6 shows that the relevance posterior is fully characterized b y the (2 M + 3) integers { e m } 2 M +2 m =0 , which can be obtained by a single scan of the click log and incrementally updated as new log data comes in. These properties, together with its proven effectiveness in modeling clicks and accounting for positional bias [12], make CCM a good choice for reordering documents in practice. For uncluttered notations, we denot e p ( R i  X  C 1: U ) by p ( R i ) in what follows.

As illustrated in Figure 2, we can take the distribution mean i of p ( R i ) as the point estimation of the relevance for d i , and compute the preference probabilities between two documents analytically, as detailed in the next section.
The idea of reordering documents is simple: for each doc-ument pair ( d i , d j ) with i &lt; j , we compute the preference probability from d j to d i , as defined below, and if the prob-ability is above the threshold , d i and d j are swapped. Definition 1 (Preference Probability (PP)).
 Given a document pair ( d i , d j ) , the preference probability from d j to d i , denoted by P ( d j  X  d i ) , is defined as where R i , R j  X  [0 , 1] are the random variable for the rele-vance of d i and d j respectively.

By assuming posterior independence, P ( d j  X  d i ) can be computed as below: where  X  i is the cumulative distribution function (CDF) of R .

As can be seen here, there is no special treatment for infrequent queries in computing the preference probabilit y. The large uncertainly of document relevance for infrequent queries has already been encoded in the relatively flat pos-terior, and hence no heuristics is needed to differentiate be -tween frequent and infrequent query-URL pairs.
 Although we have p ( R ) in closed form, directly plugging Eq. 6 into Eq. 7 does not give a tractable solution, so we compute P ( d j  X  d j ) through numeric integration. Specif-ically, we evaluate p j ( R j ) and  X  i ( R j ) at B sample points r The accuracy of Eq. 8 increases as B gets larger, and B = 1000 is sufficient for most cases. With a little misnomer, we call the value coming from Eq. 8 as the exact PP .
With preference probabilities computed, we now define -inconsistent document pairs as below. Algorithm 1 PPSwap(  X  , ) Input:  X  : The original ranking of top-M documents Output:  X  : The re-ordered ranking based on PP 1: for i = 1 to (M-1) 2: for j = M to i + 1 3: if P (  X  [ j ]  X  X  X  [ j  X  1])  X  4: Swap (  X  [ j ] ,  X  [ j  X  1]) 5: return  X  ;
Definition 2 ( -Inconsistent Pair). A document pair ( d , d j ) is a -inconsistent pair if i &lt; j and P ( d j  X  d where is a constant greater than 0.5, and P ( d j  X  d i ) is called the inconsistency probability of the pair.
Because the preference probability from CCM does not constitute triads, document reordering is similar to a sort , whose comparison operator is replaced by checking if the in-consistency probability is above the threshold . This gives the algorithm listed in Algorithm 1. The algorithm is pre-sented using a bubble sort template for its clarity, althoug h any comparison sort template can be used. In practice, in-sertion sort could be a better choice because (1) it is very efficient on nearly-sorted list, which is exactly the case for web ranking and (2) it is ideal for sorting small list and perfectly fits the top-10 scenario.

Although Algorithm 1 seems to be the same as sorting, there are two major differences. First, the reordered list from Algorithm 1 depends on the original ranking whereas sort-based approaches are independent of the original. For example, any document pairs ( d i , d j )  X  s i &lt; j with P ( d d )  X  will not be swapped. Second, Algorithm 1 solely relies on the preference probability and there is no score explicitly associated with each document that can be used for sorting.
Post-rank reordering can be implemented in two ways in practice. The first is to pre-compute reordered rankings for interested queries using Algorithm 1, and serve the reorder ed rankings online through result caching [11]. In this way, th e exact PP can be computed offline using Eq. 8. The sec-ond approach is to reorder documents at query time, which could help avoid some limitations of result caching in the first approach, e.g. , query coverage and query dynamics [3]. For the second approach, because it is impractical to pre-compute all PP X  X  and load them online, we need efficient ways to compute preference probabilities at query time.
Eq. 8 suggests a naive approach: if the B sample points of the relevance posterior are registered with each document, the probabilities can be computed at query time with the same accuracy as offline. But the challenge is whether we can compute preference probability from much fewer regis-tered values than B . In the following, we will describe two approximation methods that require only one value regis-tered to each document. The first method is through re-gression (Section 4.1) and the other is by parameterizing the Bradley-Terry model (Sections 4.2). Finally, Section 4 .3 reveals that the two methods are roughly equivalent.
Figure 3: Goodness of Fit with Regression-Model
As we can compute exact PPs based on Eq. 8, a natu-ral approach is to regress the exact PP based on some key quantities from the relevance posterior. We therefore ran-domly sampled 10000 document pairs and their preference probabilities from the experimental data (see Section 5.1 f or data detail), and randomly split them into 3:1 for training and testing. For each document pair ( d i , d j ), we take as the regression features, and obtain the following linear regression model which, as expected, is proportional to the differences in . Its goodness-of-fit (GoF), as evaluated on the test set, is de -picted in Figure 3(a), which exhibits a general lack of fitnes s, especially when the exact PP is near 0 and 1. We augmented regression features with other quantities like standard de via-tion and median, and did not improve the GoF in any visible way.

The lack of fitness is unsurprising, because PP is not lin-ear to the difference in . For example, suppose i = 0 . 90 and j = 0 . 95. Although the difference is merely 0 . 05, P ( d j  X  d i ) could be nearly 1 as posteriors with in that range are likely very spiky. This observation suggests that should be somehow stretched to emphasize the spikiness of the posteriors when is near 0 or 1.

We therefore apply the log-odd function f ( x ) = ln ( x 1  X  x to both and the exact PP, and obtained the following regression model f (  X  P ( d j  X  d i )) = 1 . 0096  X  f ( 1 )  X  1 . 0080  X  f ( whose GoF is depicted in Figure 3(b), which looks much better than Figure 3(a).

Readers may have noticed that Eq. 10 is essentially a lo-gistic regression of P ( d j  X  d i ) with f ( ) X  X  as the features. We verified that a logistic regression with  X  X  as the feature does no much better than Figure 3(a), which indicates that log-odd transformation is the key to good GoF.

Finally, we note that we here intentionally forgo nonlinear kernel regressions because (1) they need to remember a large number of support vectors, whose memory cost is exactly what we try to avoid, and (2) in kernel regression, many kernel functions will be evaluated for each document pair, and the time cost will delay the response time. As Eq. 10 has already approximated exact PP pretty well and is easy to compute, we will use it in this study. (a) Parameterize  X  ( d i ) = i
The Bradley-Terry (BT) model [4] is commonly used for paired comparison [7], and has recently been adopted in machine learning, e.g. , estimating multi-class probabilities from pairwise binary classification results [14] and learni ng to rank [5, 28]. In its general form, BT model states that where i and j are the  X  X erit X  of d i and d j respectively. The BT model can be viewed as a special linear model [7]: where  X  ( d i ) characterizes the utility of d i and F :  X 7 X  X  X  is a symmetric CDF, meaning that F (  X  X  X  ) = 0, F (+  X  ) = 1, and F ( x ) = 1  X  F (  X  x ). By choosing we have which is equivalent to parameterizing i with e  X  ( d  X  ) .
Based on Eq. 12, we need to figure out a proper util-ity function  X  ( x ) for each document such that Eq. 12 ap-proximates the exact PP. We tried  X  ( d i ) = log ( i ) and  X  ( d i ) = i , which correspond to taking i and e  X  as the merit of the document d i , but neither gave satisfying results. Figure 4(a) depicts the goodness-of-fit with  X  ( d i ) = i which is the better of the two.

The dramatic effect of log-odd transformation in the above subsection reminds us that the same log-odd transformation may help with the BT model as well. By taking  X  ( x ) = f ( x ), the log-odd function, we obtain and it gives what is shown in Figure 4(b). It is better than Figure 4(a), and is comparable to Figure 3(b).

We also tried the Thurstone-Mosteller (TM) model, which is essentially the same linear model (Eq. 11) but with F ( x ) being the unit Gaussian CDF. Because of the similar curva-tures between the sigmoid function and the unit Gaussian CDF, TM model behaves similarly as the BT model, and is hence not included in this study.
The above two methods are dramatically different on the surface: one is supervised and the other unsupervised. But the great similarity in GoF as shown in Figures 3(b) and 4(b) hints some underlying connections between them.
Noticing that f ( x ) = F  X  1 ( x ), and applying F ( x ) to both sides of Eq. 10, we get  X 
P ( d j  X  d i ) = 1 where = e 0 . 0292  X  1 and the last equation holds by ap-proximately taking 1.0096 and 1.0080 as 1.

Eq. 15 indicates that the regression model learnt from data is roughly the same model as that by parameterizing the BT model with  X  ( x ) = f ( x ). This means that the ad hoc log-odd transformation as used in the regression model actually giv es the  X  X erit X  that is appreciated by the BT model. Although the relevance of a document is fully characterized by the relevance posterior in CCM, ln ( 1  X  ) can serve as a single-number summary of the relevance. This summary is better than in that its difference between two documents approx-imates the preference probability (Eq. 10), which could be very handy in learning-to-rank algorithms.

However, it is yet unknown whether such observations generalize to relevance estimates from other click models. But based on previous studies that show strong consensus among various models [13, 12], we expect an affirmative an-swer. If the observation does generalize, then we have ac-tually found a justifiable way to compute preference prob-ability for non-Bayesian models. We will investigate this direction in the future.

Finally, we note that the = e 0 . 0292 in Eq. 14 corre-sponds to the threshold that controls the probabilities of ties. Specifically, it means that the regression model rep-resents a robust BT model that takes ( d i , d j ) pairs with old . This echoes the idea of a previous work on  X  X earning to rank with ties X  [28].
We report on the experimental evaluation in this sec-tion. We first describe the experiment setup in Section 5.1, and then estimate the optimal value for threshold in Sec-tion 5.2. Finally, Section 5.3 elaborates on the comparison between PP-based and sort-based approaches with details.
As this study tries to quantify the relevance improvement of different reordering algorithms over a major search en-gine, substantial real-world click data is needed for the ex -periment. However, to the best of our knowledge, no public data is appropriate for this purpose. The data prepared for the  X  X orkshop on Web Search Click Data X  1 looks promis-ing, but it is unfortunately insufficient for building CCMs, http://research.microsoft.com/  X nickcr/wscd09/ e.g. , unclicked documents are not available in the data. We therefore construct the experiment data as below.
We randomly sampled 2826 distinct queries from a com-mercial search engine, and cached the top-10 URLs for each query. Each query-URL pair was then rated by a trained human assessor in five scales from  X  X erfect X  to  X  X ad X . We computed the CCM relevance posterior for each query-URL pair, and consequently the pairwise preference probabilit ies for URLs w.r.t. the same query, using two weeks X  click log in Aug. 2008. The query frequencies (capped at 10,000) in that time period is depicted in Figure 5. As can be seen, neither infrequent nor frequent queries were excluded in th e experiment.

The Normalized Discounted Cumulative Gain (NDCG) [15] is used to compare the relevance of each ranking be-fore and after reordering. The NDCG at position n for a given ranking is calculated by where r i is the rating assigned to the i th document in the ranking ( r i = 5 for  X  X erfect X  and 1 for  X  X ad X ), and N is a constant that makes NDCG equal to 1 for a perfect rank-ing. In order not to reveal the raw NDCG of the search en-gine, the NDCG difference between the reordering and the original ranking is taken as the evaluation metric, so that a positive number means improved relevance and otherwise for a negative value. For convenience, this NDCG change is multiplied by 100. As in previous literature, the NDCG changes at positions 1, 3, 5 and 10 are reported.
The following six algorithms are compared in this study: Figure 6: NDCG Improvements w.r.t. Parameter We also tried to augment the three sort-based algorithms by considering the positional bias (using the examination cur ve in Figure 1 of [19]). Specifically, we normalized a click on position i as a click with weight (1/the probability position i is examined), but this failed to improve the relevance. This result agrees with the findings in [25]. A possible reason for this result is the brittleness of the examination curve that is summarized from a subject study, whereas the true ex-amination curve likely varies from query to query and even from session to session. We therefore agree with [25] that it is non-trivial to incorporate positional bias and dependen -cies through heuristics. In contrast, as will be seen in the following, the PP-based algorithms uniformly improve the relevance, because CCM implicitly estimates the positiona l bias for each query based on its click data and quantita-tively measures the extent to which one URL is preferred to another.
Before comparison, we need to first determine the thresh-old value for PP-based algorithms. For this purpose, we hold out 10% of the data, which consists of 283 distinct queries, and run the three PP-based approaches with dif-ferent threshold values. Figure 6 plots the NDCG changes with different thresholds at different positions.
The four sub-figures exhibit the same trend across dif-ferent algorithms and positions: the NDCG improvement starts from somewhere unimpressive with = 0 . 55, stably picks up as goes up, and finally drops as  X  0 . 99. This is a well-expected behavior: when is too low, many pairs with only marginal PP values are swapped. On the other hand, when is too high, many pairs with inconsistency probabilities high enough but not over are prevented from being swapped.

As seen from the four sub-figures, = 0 . 75 appears to be a good choice, and this value also appeals to human intuition: 0.999 are written as 0 and 1, respectively if a lower-ranked URL is preferred to a higher-ranked one with more than 0 . 75 probability, the two should be swapped.
Besides suggesting a proper threshold, the four sub-figures also show that ExactPP is consistently better than RegPP and BTPP . This observation reaffirms the validity of the exact PP, because it shows that the lack of GoF in Fig-ures 3(b) and 4(b), even a little, immediately translates in to a degradation in reordering quality. Although we cannot say the exact PP is the optimal measure to guide reordering, it is guaranteed that both RegPP and BTPP will perform better if they manage to achieve higher GoF with the exact PP.
After determining the value of the parameter using the 10% data, we take the rest 90% for testing. We note that here the generalization test is not across queries, but is in -stead on how different algorithms better aligns documents with human relevance judgments through click-based re-ordering, so the experiment is not a  X  X n-sample X  test.
Figure 7 presents the comparison, and demonstrates the superiority of PP-based methods over those based on sorting. Within the sort-based methods, NumClk and NumLastClk are comparable with NumClk being slightly bet-ter, and NumOnlyClk appears to be the worst of the three. The unsatisfactory performance of NumOnlyClk is due to the following reasons. First, most only clicks are on URLs associated with frequent queries ( e.g. , navigational), for which the current search engine has done pretty well, and hence there is no much space to further improve. Second, NumOnlyClk credits a URL only when the URL is the solely clicked one within a session, and this renders the number of only clicks less discriminative than the number of clicks . For this similar reason, we see NumLastClk is slightly worse than NumClk . While sort-based approaches are in general less effective, NumClk and NumLastClk clearly pick up for NDCG@5. This result indicates that clicks are indeed use-ful signals for reordering: in general, relevant documents in lower positions could be brought up by clicks, but refinement is needed to maximize the relevance improvement.
On the other hand, the three PP-based methods improve the relevance across all positions. BTPP and RegPP achieve comparable performance while both of them are inferior to ExactPP . The comparable effectiveness between BTPP and RegPP is unsurprising because of their similar GoF as shown in Figures 3(b) and 4(b).

In order to quantify the comparison between algorithms, a set of pairwise hypothesis testings are performed between different algorithms and the original ranking. The objec-tive is to quantify whether a method is better than another with statistical significance. The one-side Fisher X  X  Sign T est [20] is used so that for each pair of methods ( A, B ) ( e.g. , ( ExactPP , NumClk )), A is better than B with statistical sig-nificance if the p -value is less than 0.05.

Table 1 lists the testing p-values with NDCG@1, 3, 5, and 10, respectively, and method names are shorthanded for typesetting requirements. The number within each cell is the p -value for the corresponding test, and those less than 0 . 05 (meaning significant) are highlighted in bold. For example, the ( ExactPP , NumClk ) cell in Table 1(c) means that ExactPP is better than NumClk with statistical signifi-cance, based on measurements on NDCG@5. To see whether NumClk is better than ExactPP , one needs to resort to the cell ( NumClk , ExactPP ). As can be seen, the relative superi-ority between algorithms are consistent with what is shown in Figure 7. This study, in the first place, relates to the U Rank 2 and SearchWiki 3 features, which fall into the category of person-alized search ( e.g. , [21, 16, 26]). Both features allow regis-tered users to explicitly reorder search results for their o wn use, i.e. , the re-ordered results will not affect the ranking for the general audience. Our study differs from them in the following two aspects. First, we aim at improving the general ranking while the two features facilitate personal iza-tion and/or sharing between friends. Second, our reorderin g is automatically derived from users X  implicit feedbacks ( i.e. , clicks) whereas the two features let users explicitly edit t heir own ranking. Despite the differences, the three approaches all agree that the original ranking is unlikely 100% aligned with users X  preferences, and adjustments, either implicit ly or explicitly, are needed as appropriate. Also, it is worth mentioning that here the reordering is post-rank, which is different from relevance feedback for retrieval models that is practiced in a  X  X n-rank X  fashion [24].

Second, this study builds upon, and hence relates to, click data analysis, which is a big and active research area. To some extent, clicks are like user votes on web search results , but with the caveat that, different from conventional votes, web users may not examine and vote on each document with equal care. Therefore, disciplined approaches are needed to interpret users clicks. Joachims et al. pioneer the study based on some eye-tracking experiments, and examine how users browse and click search results [18, 19]. Their study suggests that positional bias is significant, by showing tha t even when the ranking is totally reversed, top results (in th e reversed order) still receive more clicks.

In an attempt to explain positional bias, Craswell et al. compare four click models that try to explain how the first click arises based on a real-world experiment [6]. Some follow-up work [10, 8, 9, 13] extends the study to model-ing how multiple clicks arise in a session, and provides es-timates of document relevance from clicks. While effective, the relevance estimates are mostly point-estimate, and it r e-mains obscure how to derive preference probabilities from these point estimates. In this study, we leverage the Click Chain Model (CCM) [12] for its Bayesian modeling of the relevance, and compute the preference probabilities outri ght from relevance posteriors. Moreover, we also solve the prob -lem of deriving preference probabilities from point estima tes for the CCM case. We will investigate whether and how these CCM-specific findings generalize to other models in the future.

Preference probability is an important quantity because search is simply about learning preferences, and learning http://research.microsoft.com/projects/urank/ http://googleblog.blogspot.com/2008/11/ searchwiki-make-search-your-own.html preferences from clicks is not new. As early as 2002, Joachims propose rules like  X  X lick  X  skip above X  to derive document preferences for the use of the Ranking SVM al-gorithm [17]. Later, he and his collaborators investigate a spectrum of rules based on the eye-tracking experiments [18, 19], and generalize the preference derivation from sin gle query to across query chains [22]. A recent study by Shok-ouhi et al. shows that document reordering based on these preference rules does  X  not lead to consistent improvement  X , and  X  may even lead to poorer results if not used with care  X  [25]. In this paper, we circumvent heuristics by computing preference probabilities from CCM. Not only does this give a continuous probability instead of a binary preference, bu t it also saves many hassles and heuristics that have to be dealt with in practice, e.g. , query frequencies and document positions, etc.

We expect that the computed preference probabilities can be useful for many applications, and learning to rank [27] is one of them. In the first place, we can attach the probabil-ities to those binary preferences, which could immediately lead to a cost-sensitive version of Ranking SVM for instance . Second, the computed preference probabilities can also aug -ment the training data for algorithms ( e.g. , RankNet [5]) that currently rely on human judgements for pairwise pref-erences. Third, the document X  X erit X  that is appreciated by the BT model can be incorporated as supplemental training features for learning a better ranker, which is line with pre -vious work done by Agichtein et al [1, 2]. All these are in our list of future work.

Finally, in more general sense, this study is related to paired comparisons [7], because we find that a proper parametrization of the Bradley-Terry model coincides with the preference probability that is computed from CCM. Re-stricted to information retrieval, the BT model is usually used to regress the outputs from a ranking function for two documents to the desired preference between them [23, 28, 5]. In this study, we find that the odd of the point estimation is actually the  X  X erit X  appreciated by the BT model. This finding not only leads to an online computation method for preference probability, but it also backs the validity of th e preference probability. As said before, it could be interes t-ing, as well as rewarding, to check how the result generalize s to other relevance estimates.
In this paper, we proposed to accompany a search en-gine with an  X  X lways-on X  component that resolves unsat-isfactory orderings between documents by monitoring user clicks. Contrary to human intuitions, we showed that simple sorting by clicks is not precise enough. We put forward a preference probability-guided approach to reordering sea rch results, and devised a principled way to compute preference probabilities from clicks, both precisely and approximate ly. A set of experiments based on real-world data were reported on in this study, which validated the effectiveness of the pro -posed approach. As outlined in the above section, there are many topics to be investigated in the future. [1] E. Agichtein, E. Brill, and S. Dumais. Improving web [2] E. Agichtein, E. Brill, S. Dumais, and R. Ragno. [3] R. Baeza-Yates, A. Gionis, F. Junqueira, V. Murdock, [4] R. Bradley and M. Terry. The rank analysis of [5] C. Burges, T. Shaked, E. Renshaw, A. Lazier, [6] N. Craswell, O. Zoeter, M. Taylor, and B. Ramsey. An [7] H. A. David. The Method of Paired Comparisons . [8] G. E. Dupret, V. Murdock, and B. Piwowarski. Web [9] G. E. Dupret and B. Piwowarski. A user browsing [10] G. E. Dupret, B. Piwowarski, C. A. Hurtado, and [11] T. Fagni, R. Perego, F. Silvestri, and S. Orlando. [12] F. Guo, C. Liu, T. Minka, Y.-M. Wang, and [13] F. Guo, C. Liu, and Y.-M. Wang. Efficient [14] T.-K. Huang, R. C. Weng, and C.-J. Lin. Generalized [15] K. J  X  arvelin and J. Kek  X  al  X  ainen. Cumulated gain-based [16] G. Jeh and J. Widom. Scaling personalized web [17] T. Joachims. Optimizing search engines using [18] T. Joachims, L. Granka, B. Pan, H. Hembrooke, and [19] T. Joachims, L. Granka, B. Pan, H. Hembrooke, [20] E. Lehmann and J. P. Romano. Testing Statistical [21] J. Pitkow, H. Sch  X  utze, T. Cass, R. Cooley, [22] F. Radlinski and T. Joachims. Query chains: learning [23] F. Radlinski and T. Joachims. Active exploration for [24] G. Salton and C. Buckley. Improving retrieval [25] M. Shokouhi1, F. Scholer, and A. Turpin.
 [26] J. Teevan, S. T. Dumais, and E. Horvitz.
 [27] A. Trotman. Learning to rank. Information Retrieval , [28] K. Zhou, G.-R. Xue, H. Zha, and Y. Yu. Learning to
