 Real-time entity resolution (ER) is the process of matching a query record in sub-second time with records in a database that represent the same real-world entity. To facilitate real-time matching on large databases, appropriate indexing ap-proaches are required to reduce the search space. Most avail-able indexing techniques are based on batch algorithms that work only with static databases and are not suitable for real-time ER. In this paper, we propose a forest-based sorted neighborhood index that uses multiple index trees with dif-ferent sorting keys to facilitate real-time ER for read-most databases. Our technique aims to reduce the effect of er-rors and variations in attribute values on matching quality by building several distinct index trees. We conduct an ex-perimental evaluation on two large real-world data sets, and multiple synthetic data sets with various data corruption rates. The results show that our approach is scalable to large databases and that using multiple trees gives a no-ticeable improvement on matching quality with only a small increase in query time. Our approach also achieves over one order of magnitude faster indexing and querying times, as well as higher matching accuracy, compared to another recently proposed real-time ER technique.
 H.2.8 [ Database management ]: Database Applications X  Data mining ; H.2.4 [ Database management ]: Systems X  Textual databases Algorithms, Experimentation Dynamic indexing; braided tree; real-time matching; record linkage; data matching.  X  Fun ded by the Australian Research Council, Veda, and Funnelback Pty. Ltd., under Linkage Project LP100200079.
Massive amounts of data are being collected by many business and government organizations. Given that many of these organizations rely on up-to-date information for their decision making processes, the quality of the collected data has a direct impact on the quality of the produced outcomes [2]. Data cleaning is generally employed to im-prove data quality. One important practice in data cleaning is entity resolution (ER), the task of matching all records that refer to the same real-world entity. An entity can be a person, a consumer product, or a business. ER is challeng-ing because databases usually do not contain unique entity identifiers. Therefore, identifying attribute values (such as names and addresses) need to be used for matching, which requires approximate string matching techniques [2].
As services are being moved online, organizations increas-ingly require to perform real-time ER (with sub-second re-sponse times) on query records that need to be matched with existing entity databases [4]. However, most current ER techniques are batch algorithms that compare and re-solve all records in one or more database(s) rather than re-solving those relating to a single query record. There is a need for new techniques that support ER for large read-most databases that can resolve streams of query records in real-time. A major aspect of achieving this goal is to develop indexing techniques that allow updates and facilitate real-time matching by generating a small number of high-quality candidate records. In the context of our work, we define read-most as the situation where queries are inserted into the index and database, and where the majority of queries only requires a minor update of the index data structure, as will be described in Section 3.

Contributions: We propose an index that uses multi-ple trees with different sorting keys (described below) that can be used for real-time ER for read-most databases. We then conduct an experimental evaluation on several large data sets and we compare our approach with an alternative real-time ER indexing approach recently proposed [8]. We investigate the scalability of our approach, and the effect of using different numbers of trees with different sorting keys on both matching quality and query time.
The ER process consists of several steps [2]: data pre-processing, indexing, record pair comparison, record pair classification (into matches and non-matches), and evalua-tion with regard to matching accuracy and completeness. This paper is mostly concerned with the indexing step. Figure 1: The table shows a small example data set, wit h record r 10 being a query record. The figure represents one tree in the F-DySNI built from this data set after inserting the query record.

Standard blocking and the sorted neighborhood method (SNM) indexing techniques are commonly used in the ER process. Standard blocking [2] is based on inserting records into blocks according to a blocking key criterion and only comparing records that are in the same block. The SNM [5] arranges all records in the database(s) to be matched into a sorted array using a sorting key criterion. Then a fixed-size window is moved over the sorted records, comparing only those records that are within the sliding window at any one time. Both blocking and sorting keys are usually based on one or a concatenation of attribute values.

Other indexing techniques developed for ER include q-gram indexing, suffix array indexing, canopy clustering, and mapping-based indexing [2]. However, all these techniques are aimed at batch processing of static databases and not suitable for real-time ER.
 Only limited research has so far concentrated on real-time ER. A first query-time ER approach was proposed based on collective clustering [1]. An average query time of 31.28 sec was reported, which makes this approach not suitable for real-time ER. Ioannou et al. [6] proposed an approach based on using links between the entities in a probabilistic database to resolve these entities. This approach works with dynamic databases and can be used for real-time ER.
Christen et al. [3] proposed a similarity-aware indexing technique where similarities between attribute values are pre-calculated when the index is built. Although this in-dex facilitates real-time ER it is only applicable for static databases. Ramadan et al. [8] extended this index to work with dynamic data. The authors stated that the growing size of the index does not affect the average record insertion and query times, which were around 0.1 ms and 10 ms, re-spectively, on a data set of 2.5 million records. While this indexing technique is based on the idea of standard block-ing [2], we propose a novel dynamic real-time indexing ap-proach that is based on the SNM [5, 11] The forest-based dynamic sorted neighborhood index (F-DySNI) is an index that facilitates real-time ER and that can be used with read-most databases. The index consists of multiple tree data structures where each tree is built us-ing a different sorting key (SK). Using several trees with dif-ferent SKs can help improve the quality of results in cases where errors and variations occur at the beginning of at-tribute values. For example,  X  X hristine X  and  X  X ristine X  will not be inserted into the same tree node if a first name at-tribute is used as a SK, but they might be inserted into the same node in another tree where another SK is used. The F-DySNI has two phases: (1) a build phase where trees are built using records from an existing entity database, and (2) a query phase where the built index is queried by retrieving candidate records for a query record from all index trees, and the index is updated. We next describe the data structure of the tree, and then the build and query phases. Tree Data Structure: Trees in the index are braided AVL trees which combine the properties of both a height balanced binary tree and a double-linked list [9]. Each node in the tree has a link to its alphabetically sorted predecessor node ( X  X rev X ) and its successor node ( X  X ext X ), and a list of identifiers of all records that have that node X  X  key value as their sorting key value (SKV) [7]. Figure 1 illustrates an example tree built for the shown example data set. Nodes in the tree are sorted alphabetically based on SKV. A SKV is generated for each record in the database, and a record X  X  identifier is inserted into the tree based on its SKV. Identifiers of records that have the same SKV will be appended to the same tree node as a list (as shown in Figure 1). Assuming there are k different SKVs (nodes) in a tree, and n records in the database to be indexed (with k &lt; n , potentially k  X  n ), searching for a SKV will be reduced from O ( log ( n )) to O ( log ( k )) compared to the static array-based SNM [5].

Build Phase: During the build phase, multiple trees are built using different SKs where a record is inserted into every tree. To build one tree, records are loaded from a database, their SKVs are generated and inserted into the tree, with each unique SKV becoming a node in the tree. For example, node N 1 in Figure 1 was generated when record r 1 with SKV  X  X ercysmith X  was inserted into the empty index, while node N 3 was generated when record r 3 with SKV  X  X obinstevens X  was inserted. If the SKV for a certain record already exists in the tree as a node key, then only the identifier of this record needs to be added to the corresponding list. The complete records with full attribute values are also inserted into an inverted index or disk-based database table R to be used later in the matching process.

The steps described above for building one tree are re-peated to build all trees in the forest using different SKs.
Query Phase: In the query phase, a query record q is in-serted into the F-DySNI and then it is matched in real-time against all trees that were constructed in the build phase. A new unique identifier is created for q and the different SKVs that are associated with the different trees are gener-ated. q is then inserted into all trees using these SKVs and its record identifier is added in the same way records were inserted during the build phase. The full attribute values of q are also added to R .

To retrieve candidate records from a single tree, a win-dow of size w of nodes neighboring the node containing the query record q is generated. All record identifiers that are stored in the nodes in the window are added to the candi-date record set C . We implemented both a fixed and an adaptive window approach. In the fixed-size approach [5], a pre-defined window size is used to include neighboring nodes, while in the adaptive window approach [11] the window ex-pands based on the similarities between the SKV of the node that has the query record and its neighboring nodes (calcu-lated using an approximating string similarity function [2]). The window expands until the similarity between SKVs is below a specific threshold.

The steps described above are repeated for each tree, each adding candidate records into the overall candidate record set C . Then the query record q is compared in detail with all records in C using similarity comparison functions [2]. The actual attribute values are retrieved from R . The compared candidate records are returned as a list M sorted according to their overall similarities with q , where only candidates with similarities above a specific threshold are added to M .
We implemented the F-DySNI approach, as well as the dynamic similarity-aware inverted index (DSAI) [8] which we used as a baseline, using Python (version 2.7.3). We ran experiments with memory-fitting data sets on a server with 128 GB of main memory and a 2.4 GHz Intel Xeon CPU. The results in Figure 3 (d) on the CCA data set (described below) were measured on a server with 64 GB of main memory and a 2.0 GHz Intel Xeon CPU. To facilitate repeatability of our experiments, the prototype codes and the synthetic data sets are available from the authors. Due to space limitations we only include results using an adaptive window size as it showed better results than using a fixed window size. We evaluated our approach on the following data sets. NC : this is a real voter registration data set from North since October 2011 to build a temporal data set that con-tains the names, addresses, and registration numbers of vot-ers. The data set contains 7,997,234 records (905 MB as CSV file). This data set contains realistic temporal informa-tion about a large number of people. We identified 142,673 individuals with two records, 3,566 with three, 92 with four, and two with five records in this data set.

CCA : this is a confidential commercial database which contains names and addresses of tens of millions of individu-als, as well as a log file of query records against this database. 1 Av a ilable from: ftp://www.att.ncste.gov/data To evaluate the scalability of our indexing approach, we generated four subsets of different sizes by randomly se-lecting records from the full CCA data set. The first sub-set, CCA-1, contains 689,928 database records and 50,190 query records, CCA-3 contains 2,064,823 database records and 151,343 query records, CCA-10 contains 6,900,163 data-base records and 504,226 query records, and CCA-30 con-tains 20,708,303 records and 1,513,233 query records. The number of records in the larger subsets relative to CCA-1 is 3 times, 10 times, and 30 times, respectively.

OZ-x : We generated four synthetic data sets with var-ious corruption ratios using the GeCo data generator and corrupter [10], for the purpose of investigating the effect of having different levels of data quality in attribute values on matching quality. The four data sets each contain 345,876 records (14.5 MB as CSV files) of personal details (such as names and addresses) selected randomly from a clean Aus-tralian telephone directory, and modified by adding dupli-cate records that had randomly corrupted attribute values based on typing, scanning and OCR errors, or phonetic vari-ations. A single record in each synthetic data set has an average of five corrupted duplicates.

With a real-time ER technique, the aim is to match a query record with all records in the index that represent the same entity in the shortest possible time. Thus, we measure the quality of the obtained results, and query time of the approach. The evaluation metrics we use are: -Recall: which is the fraction of true matching index records correctly included in the retrieved candidate set C . -Mean reciprocal rank (MRR): which is the average of the reciprocal of the rank of the first true matching record in the returned result set C for a set of queries. -Time: we measure the time to insert a single record into the index, and the time to query and match a single record.
In our first set of experiments, we evaluated the effect of using multiple trees and different SK combinations (using all possible single attributes and concatenated pairs of at-tributes) on recall, MRR, and average query time. The OZ-x data sets (with different data corruption ratios) were used for this set of experiments. Figure 2 illustrates the results for the OZ-1 data set (where one attribute in each record is corrupted). From the figure, plot (a) shows that when using three trees recall can increase significantly compared to when using a single tree. The MRR values (in plot (b)) also improve when using several trees. As for average query time, plot (c) shows that using more trees in general leads to longer query times. However, SKVs that are generated from a concatenation of two attribute values achieve fast average query times while still attaining high recall values (which makes them suitable for use in real-time ER). We obtained similar results with the other OZ-x data sets where 2, 3, or 4 attribute values have been corrupted (see Figure 3 (a)).
In our second set of experiments, we evaluated the effi-ciency of F-DySNI using an adaptive window approach [11] with a similarity threshold of 0.8 to investigate the scala-bility of the approach to large databases, and we compared the results with the baseline DSAI approach [8]. We mea-sured the average time required to insert a single record into the index, and the average time required to match a single query record across the growing size of the index. These experiments were conducted using the full NC data set us-ing different numbers of trees and different SK based on two concatenated attributes. The results illustrated in Fig-ure 3 (b) show that the average insertion times using the various numbers of trees is not affected by the growing size of the index data structure, while plot (c) shows that the average query time only increases slightly as the index be-comes larger. As expected, the results show that using more trees increases the average insertion and query times, but the achieved times are still very fast (around 1 ms and 15 ms insertion and query times, respectively) for three trees. Our approach also achieves over one order of magnitude faster query times compared to the DSAI approach. The memory required for the index of one, two, and three trees for the NC data set was 1.8, 3.6, and 4.3 GBs, respectively. In the last set of experiments we investigate how the F-DySNI approach scales to large real-world data sets with tens of millions of records. We used subsets of the CCA data set and measured the average query time using one, two, and three trees. An adaptive window approach [11] was used with similarity thresholds ranging from 0.7 to 0.9 The results in Figure 3 (d) show that for the increasing size of the data set the average query time increased sub-linearly yet was still very fast with an average query time of 4 ms for a data set with over 20 million records. We proposed a dynamic forest-based index for real-time ER. The index uses multiple trees with different sorting keys to reduce the effects of errors and variations at the begin-ning of attribute values on the quality of matching results. Our evaluation shows that our approach is scalable with respect to database size and that using multiple trees has a noticeable improvement on matching quality with only a small increase in query time when using sorting keys based on several concatenated attribute values. For future work, we plan to compare our approach with the static SNM and investigate the effect of using a disk-based index data struc-ture on the approach. We also plan to investigate techniques to learn optimal sorting keys for building trees in the index and optimal tree selection for query records. We intent to parallelize our multiple-tree index to improve performance. [1] I. Bhattacharya and L. Getoor. Query-time entity [2] P. Christen. Data Matching -Concepts and [3] P. Christen, R. Gayler, and D. Hawking.
 [4] X. L. Dong and D. Srivastava. Big data integration. In [5] M. A. Hernandez and S. J. Stolfo. The merge/purge [6] E. Ioannou, W. Nejdl, C. Nieder  X ee, and Y. Velegrakis. [7] B. Ramadan, P. Christen, and H. Liang. Dynamic [8] B. Ramadan, P. Christen, H. Liang, R. Gayler, and [9] S. V. Rice. Braided AVL trees for efficient event sets [10] K.-N. Tran, D. Vatsalan, and P. Christen. GeCo: an [11] S. Yan, D. Lee, M. Y. Kan, and L. C. Giles. Adaptive
