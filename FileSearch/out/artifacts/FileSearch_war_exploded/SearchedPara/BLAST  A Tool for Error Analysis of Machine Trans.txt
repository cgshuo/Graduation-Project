 Machine translation evaluation is a difficult task, since there is not only one correct translation of a sentence, but many equally good translation options. Often, machine translation (MT) systems are only evaluated quantitatively, e.g. by the use of automatic metrics, which is fast and cheap, but does not give any indication of the specific problems of a MT sys-tem. Thus, we advocate human error analysis of MT output, where humans identify and classify the prob-lems in machine translated sentences.

In this paper we present B LAST , 1 a graphical tool for performing human error analysis, from any MT system and for any language pair. B LAST has a graphical user interface, and is designed to be easy and intuitive to work with. It can aid the user by highlighting similarities with a reference sentence. B
LAST is flexible in that it can be used with out-put from any MT system, and with any hierarchical error typology. It has a modular design, allowing easy extension with new modules. To the best of our knowledge, there is no other publicly available tool for MT error annotation. Since we believe that error analysis is a vital complement to MT evaluation, we think that B LAST can be useful for many other MT researchers and developers. Hovy et al. (2002) discussed the complexity of MT evaluation, and stressed the importance of adjusting evaluation to the purpose and context of the trans-lation. However, MT is very often only evaluated quantitatively using a single metric, especially in re-search papers. Quantitative evaluations can be au-tomatic, using metrics such as Bleu (Papineni et al., 2002) or Meteor (Denkowski and Lavie, 2010), where the MT output is compared to one or more hu-man reference translations. Metrics, however, only give a single quantitative score, and do not give any information about the strengths and weaknesses of the system. Comparing scores from different met-rics can give a very rough indication of some major problems, especially in combination with a part-of-speech analysis (Popovi  X  c et al., 2006).
Human evaluation is also often quantitative, for instance in the form of estimates of values such as adequacy and fluency, or by ranking sentences from different systems (e.g. Callison-Burch et al. (2007)). A combination of human and automatic metrics is human-targeted metrics such as HTER, where a hu-man corrects the output of a system to the clos-est correct translation, on which standard metrics such as TER is then computed (Snover et al., 2006). While these types of evaluation are certainly useful, they are expensive and time-consuming, and still do not tell us anything about the particular errors of a system. 2
Thus, we think that qualitative evaluation is an important complement, and that error analysis, the identification and classification of MT errors, is an important task. There have been several suggestions for general MT error typologies (Flanagan, 1994; Vilar et al., 2006; Farr  X  us et al., 2010), targeted at different user groups and purposes, focused on either evaluation of single systems, or comparison between systems. It is also possible to focus error analysis at a specific problem, such as verb form errors (Murata et al., 2005).

We have not been able to find any other freely available tool for error analysis of MT. Vilar et al. (2006) mentioned in a footnote that  X  X  tool for high-lighting the differences [between the MT system and a correct translation] also proved to be quite useful X  for error analysis. They do not describe this tool any further, and do not discuss if it was also used to mark and store the error annotations themselves.
Some tools for post-editing of MT output, a re-lated activity to error analysis, have been described in the literature. Font Llitj  X  os and Carbonell (2004) presented an online tool for eliciting information from the user when post-editing sentences, in or-der to improve a rule-based translation system. The post-edit operations were labeled with error cate-gories, making it a type of error analysis. This tool was highly connected to their translation system, and it required users to post-edit sentences by mod-ifying word alignments, something that many users found difficult. Glenn et al. (2008) described a post-editing tool used for HTER calculation, which has been used in large evaluation campaigns. The tool is a pure post-editing tool and the edits are not clas-sified. Graphical tools have also successfully been used to aid humans in other MT-related tasks, such as human MT evaluation of adequacy, fluency and system comparison (Callison-Burch et al., 2007), and word alignment (Ahrenberg et al., 2003). B
LAST is a tool for human annotations of bilingual material. Its main purpose is error analysis for ma-chine translation. B LAST is designed for use in any MT evaluation project. It is not tied to the informa-tion provided by specific MT systems, or to specific languages, and it can be used with any hierarchi-cal error typology. It has a preprocessing module for automatically aiding the annotator by highlight-ing similarities between the MT output and a refer-ence. Its modular design allows easy integration of new modules for preprocessing. B LAST has three working modes for handling error annotations: for adding new annotations, for editing existing annota-tions, and for searching among annotations.
B LAST can handle two types of annotations: er-ror annotations and support annotations. Error an-notations are based on a hierarchical error typology, and are used to annotate errors in MT output. Error annotations are added by the users of B LAST . Sup-port annotations are used as a support to the user, currently to mark similarities in the system and ref-erence sentences. The support annotations are nor-mally created automatically by B LAST , but they can also be modified by the user. Both annotation types are stored with the indices of the words they apply to.

Figure 1 shows a screenshot of B LAST . The MT output is shown to the annotator one segment at a time, in the upper part of the screen. A segment nor-mally consists of a sentence and the MT output can be accompanied by a source sentence, a reference sentence, or both. Error annotations are marked in the segments by bold, underlined, colored text, and support annotations are marked by light background colors. The bottom part of the tool, contains the er-ror typology, and controls for updating annotations and navigation. The error typology is shown using a menu structure, where submenus are activated by the user clicking on higher levels. 3.1 Design goals We created B LAST with the goal that it should be flexible, and allow maximum freedom for the user, based on the following goals:  X  Independent of the MT system being analyzed,  X  Compatible with any error typology  X  Language pair independent  X  Possible to mark where in a sentence an error  X  Possible to view either source or reference sen- X  Possible to automatically highlight similarities  X  Containing a search function for errors  X  Simple to understand and use The current implementation of B LAST fulfils all these goals, with the possible small limitation that the error typology has to be hierarchical. We believe this limitation is minor, however, since it is possible to have a relatively flat structure if desired, and to re-use the same submenu in many places, allowing cross-classification within a hierarchical typology.
The flexibility of the tool gives users a lot of free-dom in how to use it in their evaluation projects. However, we believe that it is important within ev-ery error annotation project to use a set error typol-ogy and guidelines for annotation, but the annotation tool should not limit users in making these choices. 3.2 Error Typologies As described above, B LAST is easily configurable with new typologies for annotation, with the only restriction that the typology is hierarchical. B LAST currently comes with the following implemented ty-pologies, some of which are general, and some of which are targeted at specific language (pairs):  X  Vilar et al. (2006)  X  Farr  X  us et al. (2010)  X  Flanagan (1994) (slightly modified into a hier- X  Our own tentative fine-grained typology
The error typologies can be very big, and it is hard to fit an arbitrarily large typology into a graphical tool. B LAST thus uses a menu structure which al-ways shows the categories in the first level of the ty-pology. Lower subtypologies are only shown when they are activated by the user clicking on a higher level. In Figure 1, the subtypologies to Word order were activated by the user first clicking on Word or-der , then on Phrase level .

It is important that typologies are easy to extend and modify, especially in order to cover new target languages, since the translation problems to some extent will be dependent on the target language, for instance with regard to the different agreement phe-nomena in languages. The typologies that come with B
LAST can serve as a starting point for adjusting ty-pologies, especially to new target languages. 3.3 Implementation B LAST is implemented as a Java application using Swing for the graphical user interface. Using Java makes it platform independent, and it is currently tested on Unix, Linux, Mac, and Windows. B LAST has an object-oriented design, with a particular fo-cus on modular design, to allow it to be easily ex-tendible with new modules for preprocessing, read-ing and writing to different file formats, and present-ing statistics. Unicode is used in order to allow a high number of languages, and sentences can be dis-played both right to left, and left to right. B LAST is open source and is released under the LGPL li-cense. 3 3.4 File formats The main file types used in B LAST is the annotation file, containing the translation segments and annota-tions, and the typology file. These files are stored in a simple text file format. There is also a configu-ration file, which can be used for program settings, besides using command line options, for instance to configure color schemes, and to change preprocess-ing settings. The statistics of an annotation project are printed in a text file in a human-readable format (see Section 4.5).

The annotation file contains the translation seg-ments for the MT system, and possibly for the source and reference sentences, and all error and support annotations. The annotations are stored with the indices of the word(s) in the segments that were marked, and a label identifying the error type. The annotation file is initially created automatically by B
LAST based on sentence aligned files. It is then updated by B LAST with the annotations added by the user.

The typology file has a header with main informa-tion, and then an item for each menu containing:  X  The name of the menu  X  A list of menu items, containing: The typology files have to be specified by the user, but B LAST comes with several typology files, as de-scribed in Section 3.2. B
LAST has three different working modes: annota-tion, edit and search. The main mode is annotation, which allows the user to add new error annotations. The edit mode allows the user to edit and remove er-ror annotations. The search mode allows the user to search for errors of different types. B LAST can also create support annotations, that can later be updated by the user, and calculate and print statistics of an annotation project. 4.1 Annotation The annotation mode is the main working mode in B
LAST , and it is active in Figure 1. In annotation mode a segment is shown with all its current er-ror annotations. The annotations are marked with bold and colored text, where the color depends on the main type of the error. For each new annotation the user selects the word or words that are wrong, and selects an error type. In figure 1, the words no television , and the error type Word order  X  Phrase level  X  Long are selected in order to add a new error annotation. B LAST ignores identical annotations, and warns the user if they try to add an annotation for the exact same words as another annotation. 4.2 Edit In edit mode the user can change existing error an-notations. In this mode only one annotation at a time is shown, and the user can switch between them. For each annotation affected words are highlighted, and the error typology area shows the type of the error. The currently shown error can be changed to a dif-ferent error type, or it can be removed. The edit mode is useful for revising annotations, and for cor-recting annotation errors. 4.3 Search In search mode, it is possible to search for errors of a certain type. To search, users choose the error type they want to search for in the error typology, and then search backwards or forwards for error annota-tions of that type. It is possible both to search for specific errors deep in the typology, and to search for all errors of a type higher in the typology, for instance, to search for all word order errors, regard-less of subclassification. Search is active between all segments, not only for the currently shown segment. Search is useful for controlling the consistency of annotations, and for finding instances of specific er-rors. 4.4 Support annotations Error annotation is a hard task for humans, and thus we try to aid it by including automatic preprocess-ing, where similarities between the system and refer-ence sentences are marked at different levels of sim-ilarity. Even if the goal of the error analysis often is not to compare the MT output to a single reference, but to the closest correct equivalent, it can still be useful to be able to see the similarities to one ref-erence sentence, to be able to identify problematic parts easier.

For this module we have adapted the code for alignment used in the Meteor-NEXT metric (Denkowski and Lavie, 2010) to B LAST . In Meteor-NEXT the system and reference sentences are aligned at the levels of exact matching, stemmed matching, synonyms, and paraphrases. All these modules work on lower-cased data, so we added a module for exact matching with the original casing kept. The exact and lower-cased matching works for most languages, and stemming for 15 languages. The synonym module uses WordNet, and is only available for English. The paraphrase module is based on an automatic paraphrase induction method (Bannard and Callison-Burch, 2005), it is currently trained for five languages, but the Meteor-NEXT code for training it for additional languages is in-cluded.

Support annotations are normally only created au-tomatically, but B LAST allows the user to edit them. The mechanism for adding, removing or changing support annotations is separate from error annota-tions, and can be used regardless of mode. 4.5 Create Statistics The statistics module prints statistics about the cur-rently loaded annotation project. The statistics are printed to a file, in a human-readable format. It con-tains information about the number of sentences and errors in the project, average number of errors per sentence, and how many sentences there are with certain numbers of errors. The main part of the statistics is the number and percentage of errors for each node in the error typology. It is also possible to get the number of errors for cross-classifications, by specifying regular expressions for the categories to cross-classify in the configuration file. B
LAST is under active development, and we plan to add new features. Most importantly we want to add the possibility to annotate two MT systems in paral-lel, which can be useful if the purpose of the annota-tion is to compare MT systems. We are also working on refining and developing the existing proposals for error typologies, which is an important complement to the tool itself. We intend to define a new fine-grained general error typology, with extensions to a number of target languages.

The modularity of B LAST also makes it possible to add new modules, for instance for preprocess-ing and to support other file formats. One example would be to support error annotation of only specific phenomena, such as verb errors, by adding a prepro-cessing module for highlighting verbs with support annotations, and a suitable verb-focused error typol-ogy. We are also working on a preprocessing module based on grammar checker techniques (Stymne and Ahrenberg, 2010), that highlights parts of the MT output that it suspects are non-grammatical.
Even though the main purpose of B LAST is for error annotation of machine translation output, the freedom in the use of error typologies and support annotations also makes it suitable for other tasks where bilingual material is used, such as for anno-tations of named entities in bilingual texts, or for analyzing human translations, e.g. giving feedback to second language learners, with only the addition of a suitable typology, and possibly a preprocessing module. We presented B LAST ; a flexible tool for annotation of bilingual segments, specifically intended for error analysis of MT. B LAST facilitates the error analysis task, which we believe is vital for MT researchers, and could also be useful for other users of MT. Its flexibility makes it possible to annotate translations from any MT system and between any language pairs, using any hierarchical error typology.
