 We describe a method for predicting query difficulty in a precision-oriented web search task. Our approach uses visual features from retrieved surrogate document representations (titles, snippets, etc.) to predict retrieval effectiveness for a query. By training a supervised machine learning algorithm with manually evaluated queries, visual clues indicative of relevance are discovered. We show that this approach has a moderate correlation of 0.57 with precision at 10 scores from manual relevance judgments of the top ten documents retrieved by ten web search engines over 896 queries. Our findings indicate that difficulty predictors which have been successful in recall-oriented ad-hoc search, such as clarity metrics, are not nearly as correlated with engine performance in precision-oriented tasks such as this, yielding a maximum correlation of 0.3. Additionally, relying only on visual clues avoids the need for collection statistics that are required by these prior approaches. This enables our approach to be employed in environments where these statistics are unavailable or costly to retrieve, such as metasearch. H.3.5 [ Information Storage and Retrieval ]: Online Information Services  X  Web-based services Algorithms, Performance, Experimentation. Query Difficulty, Web Search Web search presents a new dimension to the problem of predicting query difficulty, in that difficulty is closely tied to users X  perceptions of what is relevant and what is not. When examining web search results, users are not likely to read every document returned to them. More likely, they will make an initial determination of relevance based on the information contained in the brief document representations displayed by the search engine, called surrogates. Surrogates are designed to contain enough information for a human searcher to determine whether or not the represented document is relevant to the search in question. The surrogates for most web search engines include a title, a URL, a short snippet (keyword in context), and the rank. These surrogates allow the user to make a preliminary determination of what is likely to be most relevant, and to click on and examine only those documents in greater detail. Given that users X  perceptions of relevance are based on a visual examination of the provided surrogates, it stands to reason that visual features extracted from these surrogates might be good predictors of a query X  X  difficulty. Other domains in information Teoma X , Altavista X , AllTheWeb X , Lycos X , Gigablast X , MSN X , and the MSN X  TechPreview (hereafter anonymized and referred to in no particular order as E1-10). In this collection, the average number of unique results for a given query across all engines is 43 and the mean percentage of results in common between any two engines is 37%. We designated 2/3 of the 896 queries as the training set and 1/3 as the testing set and held these sets constant across experiments. To examine the utility of query-based predictors from prior studies, we used collection statistics from the TREC WT10g web collection to estimate the standard deviation of inverse document frequencies of each query term, ratio of maximum query term IDF to minimum, and simplified clarity score [3]. Although they rely on collection statistics, each of these are pre-retrieval predictors that could be applied in a practical web search environment. The correlations of each of these predictors with mean P@10 across all 10 engines for all 896 queries are shown in Table 1. As in prior work, we use the non-parametric Spearman rank correlation coefficient throughout our experiments. To provide a baseline for our supervised methodology, we also applied the learning algorithm to these predictors to learn the optimal linear function of each of them independently and all of them combined ( qDifficulty ) on our training set (flipping the sign of the correlation due to negative learned weights). While simple clarity is not affected substantially, it is clear that learning the appropriate weight and intercept for the IDF-based predictors vastly improves performance. To evaluate the effectiveness of our visual predictors, we extracted 31 visual features from the surrogate representation of each result document, and learned the relative weights of these features to predict average P@10 across engines using the SMO support vector machine regression implementation in WEKA (http://www.cs.waikato.ac.nz/ml/weka), terming this vDifficulty . As is evident from Table 1, this has a substantially stronger correlation than any of our query-based feature baselines. Somewhat surprisingly, learning on the union of all visual and query-based features does not improve performance. To verify that aggregating these surrogate features was appropriate, we also experimented with using the SMO algorithm to learn binary classifications of each surrogate as either relevant or not relevant and calculating the predicted P@10 scores from those classifications. Also surprisingly, this yielded a correlation of only 0.54, underperforming the regression over the aggregated features. To analyze the reliability of our predictions on each engine individually, we performed the SMO regression on each engine independently, using features aggregated over only that engine X  X  surrogates. To examine what differences in feature weights might exist between engines, we included the entire set of visual and query-based features in this learning. The correlations with each engines X  P@10 and the largest magnitude feature weight from the visual features versus the query-based features are shown in Table 2. The top visual feature was always the average percentage of character n-grams in surrogate titles, while the top query-based feature was maximum IDF for all others. Although the correlations fluctuate somewhat from engine to engine, they are relatively the same level. None of the individual engines X  correlations are quite as large as that of the entire set combined, perhaps due to the 
