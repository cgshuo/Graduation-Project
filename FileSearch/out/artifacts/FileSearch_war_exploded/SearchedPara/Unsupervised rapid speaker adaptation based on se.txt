 a article info 1. Introduction As various types of smart devices, such as smartphones, smart
TVs, and even the forthcoming humanoid robots, have been introduced, the necessity for convenient and user-friendly ways of interaction between device and user has also increased. Among many useful interactive interfaces, voice is the most convenient and natural while conveying useful and intuitive non-verbal information as well as verbal information ( Fong et al., 2003 ). Since the advent of voice-based search or voice-driven communication through mobile applications, human  X  machine interaction via voice interface is not a new idea at all. In addition to human speech recognition, people now expect to experience more natural and effective voice interaction with machines.

Even though signi fi cant advancements in speech recognition technology motivated smart devices's ability to correctly recognize continuous human speech, it is still a challenge to recognize the natural or conversational voice in adverse environments ( Ting et al., 2013; Amrouche et al., 2010 ). In particular, most speech recognition systems tend to demonstrate different performance among speakers, only working well with speakers adhering to the characteristics of the acoustic model (e.g. Hidden Markov Model (HMM)) constructed for the system. Such a limitation is a more dominant feature in speech emotion recognition tasks, in which acoustic models may rarely handle human's different ways of expressing emotional states, thus producing considerable perfor-mance variations among speakers ( Park et al., 2009 ).
This tendency for speaker-dependent recognition performance is mainly induced by different characteristics between a system user and a set of speakers involved in the construction of acoustic models. Such an acoustic model is called the speaker-independent (SI) model, which is constructed with an amount of speech data obtained from a speci fi c group of speakers, irrespective of the system users. Another type of acoustic model is the speaker-dependent (SD) model, which is built with a suf fi cient amount of speech data collected only from one user. SD models preserve the user's acoustic characteristics, thus providing the best recognition performance. They are ideal for real-world devices, but not practical because of the dif fi culty of collecting a suf of speech data from individual users.

Speaker adaptation techniques were introduced to overcome the drawbacks of the two types of acoustic model ( Shinoda, 2011 ). Speaker adaptation transforms the standard SI models using a small amount of speech data obtained from the system user (also called a target speaker), as illustrated in Fig. 1 . The transformed models have characteristics adapted to the target speaker, and convey acoustic characteristics similar to the speaker, thus provid-ing the user with better performance compared to the SI model.
Although many adaptation techniques have been successfully applied to speech recognition tasks, they still have weaknesses in voice interaction with real-world devices that are generally accessed by various anonymous users. In this paper, we propose an ef fi cient speaker adaptation technique for handling voice interac-tion between a device and unspeci fi ed users.

This paper is organized as follows. Section 2 introduces the conventional speaker adaptation techniques and their drawbacks in real-world applications. Section 3 describes the proposed method in detail. Section 4 demonstrates and discusses the experimental results.
Finally, conclusions are presented in Section 5 . 2. Conventional speaker adaptation approaches and their ef fi ciency in real-world applications 2.1. Conventional adaptation approaches
Many speaker adaptation techniques have been studied to reduce mismatches of speaker characteristics between training data and tion, recent studies have attempted to apply this technique to other applications to solve similar mismatch problems ( Kim et al., 2012; Li and Dong, 2013; Mohammadi et al., 2014 ).

The adaptation techniques are generally classi fi ed into three main categories ( Shinoda, 2011; Woodland, 2001 ): parameter transformation-based approaches using Maximum Likelihood Lin-ear Regression (MLLR) ( Gales, 1998 ), parameter re-estimation-based approaches such as Maximum A Posteriori (MAP) ( Gau-vain and Lee, 1994 ), and speaker clustering-based approaches like the eigenvoice technique ( Kuhn et al., 1998 ).

The parameter transformation-based approaches assume that a set of model parameters, such as means and variances in the Gaussian
Mixture Model (GMM), can be transformed by a type of transforma-tion matrix. The basic procedure is to calculate one matrix or several transformation matrices from adaptation data collected from a target speaker. Parameter re-estimation-based approaches, also called the
Bayesian adaptation, re-estimate individual model parameters, using a priori knowledge (usually SI model parameters) on the basis of a
Bayesian framework. MLLR and MAP are known as the standard successful adaptation techniques for each of the two approaches. But, they operate well only when suf fi cient adaptation data are available (Kuhn et al., 2000; Goronzy and Kompe, 1999; Wang, 2003 ). The eigenvoice-based speaker adaptation approach was introduced to ove-rcome this drawback of MLLR and MAP. The fundamental concept of be de fi ned by a linear combination of a small number of parameters obtained from existing SD models. The representative parameters are called eigenvoices. The main objective is to de fi ne the eigenvoices representing relevant characteristics of the target speaker. The conventional adaptation techniques can be handled using two types of learning approach: supervised and unsupervised ( Matsui and
Furui, 1998; Wallace et al., 2009 ). In supervised adaptation, prior knowledge of adaptation data, such as speaker information or man-ually de fi ned labels, is required. On the other hand, unsupervised approaches determine the labels of adaptation data by recognizing them automatically and do not require speaker-speci fi c information.
For supervised adaptation, ever y target speaker is encouraged to record predetermined words or sentences in the system to obtain adaptation data, whereas an unsupervised adaptation system allows th e user to speak any words or sentences freely. 2.2. Ef fi ciency of the conventional approaches in real-world application to voice interaction Speaker adaption techniques are capable of adjusting the voice interaction system adopted in smart devices toward a speci
Nevertheless, the conventional techniques have weaknesses in real-world application because most of them concentrate on investigating ef fi ciency for either a small amount or a large amount of adaptation data without the consideration of general user cases.
 In general environments, the period of interaction with a device may be different for each user. For instance, a smart device like a service robot that confronts strangers in a public space can have a short-term or a long-term conversation with a user. This tendency concludes that the voice interaction system may obtain different amounts of adaptation data according to the target speaker. Therefore, a desirable speaker adaptation technique should always provide reliable performance regardless of the amount of adaptation data. As addressed in Section 2.1 , the performance of MLLR and MAP relies highly on the amount of adaptation data. In general, MLLR adaptation demonstrates better performance with a small amount of data.
However, the adaptation process becomes saturated at some point its performance rapidly reaches an upper limit and does not allow further improvement despite an increase in the amount of data. On the other hand, the performance of MAP adaptation greatly improves in accordance with increasing amounts of data. But it requires considerable time in large-scale speech recognition to re-estimate model parameters. Several studies investigated ways of combining these two approaches, but the performance of the combined approaches also demonstrated gr eat dependency on the amount of adaptation data ( Goronzy and Kompe, 1999; Wang et al., 2009 ). The eigenvoice approach provides better performance with a relatively small amount of adaptation data. And it is capable of further improving its performance with increased amounts of data. Another case should be considered in terms of the way adaptation data is collected. It is impractical to obtain a transcrip-tion or labels from target speakers by asking them to record sequences of speci fi c words or sentences while collecting adapta-tion data. Hence, the unsupervised adaptation manner is more desirable for general voice interaction systems than the supervised manner. Among the MLLR and MAP techniques, MLLR has been characterized as better suited to unsupervised adaptation because it is robust against labeling errors ( Woodland et al., 1996 ). Never-theless, many studies have reported that the eigenvoice approach signi fi cantly outperforms MAP and MLLR in unsupervised condi-tions ( Kuhn et al., 1998 ).
 The above-mentioned general tendencies of users who interact with a device prompt the conclusion that the eigenvoice approach is more desirable for real-world voice interaction in comparison with the other approaches. For this reason, this study proposes a more ef fi cient eigenvoice-based adaptation technique applicable to voice interaction directed toward any device user. 3. Unsupervised rapid speaker adaptation based on selective eigenvoice merging 3.1. Eigenvoice-based speaker adaptation
The eigenvoice technique that was introduced for rapid speaker adaptation is a speaker clustering-based adaptation approach ( Kuhn et al., 2000 ). Its principle is to construct a new speaker model as a linear combination of a small number of model parameters (eigen-voices) obtained from existing SD models. The eigenvoices are char-
The main objective of this technique is to de fi ne the appropriate eigenvoices corresponding to the target speaker.

Fig. 2 summarizes the general procedure for constructing ada-pted models based on the eigenvoice approach. Given R SD mod-els, the supervector is de fi ned as X r  X  X  X   X  1 r  X  T  X   X  m is the mean vector of the m th Gaussian mixture of the r th SD model and n is the number of all Gaussian mixtures in the SD main procedure of the eigenvoice approach is to fi nd a dominant subspace (called an eigenspace) of much smaller dimension than the space of supervectors that contains a huge amount of data, and to obtain P eigenvectors satisfying P o R . For this task, a Dimen-sionality Reduction Technique (DRT) is applied to R supervectors. R
The set of P  X  1 eigenvoices is mainly employed for constructing adapted models by using adaptation data from a target speaker.
Given the adaptation data, the supervector for the target speaker can be obtained by a linear combination of the eigenvoices, such that
X  X  e  X  0  X  X  w  X  1  X  e  X  1  X  X   X   X  w  X  P  X  e  X  P  X  X  1  X  can be estimated using several iterations of Maximum Likelihood
Eigen-Decomposition (MLED). 3.2. Weaknesses of the conventional eigenvoice approaches
The eigenvoice-based speaker adaptation techniques have been successfully applied to speech recognition tasks because of their robustness to the amount of adaptation data. Nevertheless, most of the conventional studies have concentrated on methods for improving the DRT method ( Kennyetal.,2005;Maketal.,2006;Duchateauetal., 2008) or combining eigenvoice and other adaptation techniques ( Chen et al., 2000; Zhang et al., 2012 ) without regard to general user environments. Several studies h ave applied non-linear PCA using kernel methods to consider diverse speaker characteristics ( Mak et al., 2005; Ansari and Almasganj, 2012 ). But the approaches require more training cost because of its operation in higher dimensional feature space and additional decoding cost in comparison to linear PCA approaches ( Peter, 2010; Roupakia et al., 2012 ).
General voice interaction devices may be exposed to various adverse environments, such as background noise. And publicly used devices such as service robots may be confronted by abnormal users who have peculiar speaking styles. These general tendencies can adversely affect the conventional eigenvoice approaches. SD models are generally constructed from speech data that were recorded in silent environments by normal speak-ers. Such types of SD models inef fi ciently handle adverse environ-ments or abnormal users, thus producing an incorrect supervector from the user. This weakness is de fi nitely observed in speech emotion recognition tasks, in which the variation between SD models is too high to generate adapted models that suf fi preserve acoustic characteristics of the target speaker.
There are very few studies that consider the main advantages of eigenvoice-based adaptation. In the standard eigenvoice app-roach, the most desirable adapted models can be constructed by eigenvoices properly indicating the target speaker's characteris-tics. Because the eigenvoices are estimated on the basis of SD model parameters, an increasing number of SD models support the eigenvoices to preserve the target speaker's characteristics more suf fi ciently, thus improving the correctness of the adapted models. Real-world application provides useful conditions for increasing the number of SD models, because interactive devices can collect a suf fi cient amount of speech data from an individual user while interacting with that user, thus acquiring new SD models from individual users.

In this study, we propose an ef fi cient approach for eigenvoice-based speaker adaptation in consideration of general user envir-onments. The proposed approach can be summarized as selective eigenvoice merging. 3.3. Selective eigenvoice merging for user-speci fi c model adaptation 3.3.1. Selective eigenvoice merging
General eigenvoice adaptation approaches employ a set of eigenvoices that are estimated using entire SD models. Among the SD models, those that preserve different acoustic character-istics from the target speaker may reduce the discriminative speaker characteristics in the adapted model. Our approach, called the selective eigenvoice merging, aims at constructing user-spe-ci fi c adapted models by only using eigenvoice sets relevant to the target speaker's characteristics.
 Fig. 3 summarizes the procedure of the proposed approach. First, existing SD models are divided into N groups according to environmental types (e.g. noisy or clean) or speaker types (e.g. gender or age). If it is not feasible to divide models according to explicit types because of a lack of models, they can be divided randomly. Next, N eigenvoice sets are estimated from each group of SD models. Based on N eigenvoice sets, several combinations can be organized. The number of all possible combinations is P r  X  1 N r . Our goal is to de fi ne an optimal combination of eigenvoice sets yielding adapted models that are most relevant to the target speaker. First, one or more eigenvoice sets pertaining to each combination are merged. Next, a combination is selected that provides the most desirable eigenvoice sets for the given adaptation data. For the selection process, we take advantage of an auxiliary function that was introduced for MLLR adaptation (Leggetter and Woodland, 1994 ).
 Get R supervectors
Estimate eigenvectors 
Obtain eigenvoices: p+1
In this MLLR study, the auxiliary function was designed to the best model parameters that maximize the function, while updating the parameters and using the current model parameters as a basis. Based on the auxiliary function, we attempt to best combination of eigenvoice sets while evaluating the merged eigenvoice sets. The proposed selection method is described as ^
E  X  arg max Q  X  ; ^  X  no where E s refers to the s th eigenvoice combination and Q auxiliary function calculated with E s .  X  is a base model, and an adapted model obtained from the merged eigenvoice sets pertain-ing to E s .Thebasemodelisupdatedas ^  X  E s whenever Q E s P r  X  1 N r . The summation part in this auxiliary function approximates the estimation of an observation probability in the HMM theory. n is the dimensionality of feature parameters, and C  X  s  X  m represents the
HMM. o t is the observation vector at time t ,and  X   X  s  X  to concentrate on  X   X  s  X  m , which is a Gaussian mean value among HMM parameters. In this function,  X   X  s  X  m demonstrates a parameter of the adapted model obtained from the merged eigenvoice sets of E fi nishing the calculation of the auxiliary function for each E combination that maximizes the function is determined as the optimal eigenvoice sets.

The results that the auxiliary function provides demonstrate how target speaker. The selected eig envoice sets that give a maximum result of the function are expected to preserve model information fairly pertinent to the target speaker. In comparison with the standard approaches that construct the adapted models using all eigenvoice sets, the proposed approach contributes to the construction of more desirable adapted models for the target speaker. However, our approach has an inevitable disadvantage, which is the extended time required for fi nding optimal eigenvoice sets. Hence, we continue to propose an ef fi cient way to reduce the time complexity. 3.3.2. Rapid eigenvoice merging In general, eigenvoice adaptation spends most of the time estimat-ing eigenvoices based on the DRT ( Kuhn et al., 2000 ). In the proposed selective eigenvoice merging, a large portion of time is also consumed during the process of estimating eigenvoice sets to be merged. One of the most ef fi cient DRTs is Singular Value Decomposition (SVD) approaches for eigenvoice merging based on SVD and analyze the ef fi ciency of each approach in terms of time complexity.
 One of the approaches is designated as a simple merging method, in which several groups of SD models corresponding to the eigenvoice sets are fi rst combined, and then new eigenvoices are estimated following the SVD process. And the other approach that is designated herein as an advanced merging method esti-mates eigenvoices from respective SD model groups and then merges all sets of eigenvoices. The procedures for the two merging methods are illustrated in Fig. 4 .
 To describe the two merging approaches using a numerical expression, we fi rst de fi ne the SVD process with (4), according to the standard SVD de fi nition ( Robert, 1999 ):
X  X 
U  X 
X  X   X   X  X  X  V  X  X  X  T  X  4  X  where U ( X ) refers to an eigenvoice matrix estimated from super-vectors X . Let us denote n and M as the dimension of the supervector and the number of all supervectors, respectively. And P is denoted as the number of eigenvoices estimated by SVD. Then X and U ( X ) are, respectively,  X  n M  X  and  X  n P matrices.  X   X  X  X  is a  X  P P  X  diagonal matrix that has spread values only in diagonal elements, and V  X  X  X  T is an  X  R P  X  unitary matrix that contains information about the supervectors projected into eigenspaces. In the same way, the SVD process obtaining U ( Y ) from supervectors Y of other SD models can be de fi ned as
Y  X 
U  X  Y  X   X   X  Y  X  V  X  Y  X  T  X  5  X  The simple merging method, in which new eigenvoices are estimated using both X and Y , can be described as
Z  X 
U  X  Z  X   X   X  Z  X  V  X  Z  X  T  X  6  X  where Z is de fi ned as  X  X ; Y . On the other hand, the advanced merging method obtains new eigenvoices directly from U ( X ) and
U ( Y ). The goal of this approach is to estimate new eigenvoices more rapidly than the simple method by direct use of the existing eigenvoice sets, while maintaining the property of new eigen-voices, similar to U ( Z )in (6), which retains unre fi ned eigenvoice information of the original supervectors.

First, we consider a general tendency of SD models. Due to speaker ences between SD models, thus producing an internal distance within a group of supervectors as well as an external distance between two groups of supervectors. This tendency may induce a distortion in eigenspaces after the merging process. To minimize this problem, we normalize the supervectors before the SVD, by subtracting the mean of the supervectors. The standard SVD process is rede fi ned as
X  X   X  X  X  1 M U  X  X  X   X   X  X  X  V  X  X  X  T  X  7  X  where  X   X  X  X  is an  X  n 1  X  matrix that contains the mean calculated in each dimension for supervectors X . 1 M refers to a  X  1 M in which all elements are 1. This way has an effect on maintaining common properties between SD models, while reducing variance between supervectors. Eqs. (5) and (6) are also similarly rede follows:
Y  X   X  Y  X  1 N U  X  Y  X   X   X  Y  X  V  X  Y  X  T  X  8  X 
Z  X   X  Z  X  1 M  X  N U  X  Z  X   X   X  Z  X  V  X  Z  X  T  X  9  X 
Next, to derive a more explicit de fi nition of U ( Z ) using eigen-voice sets U ( X ) and U ( Y ), we de fi ne U ( Z )as
U  X  Z
 X  X   X  R  X  10  X  where  X  refers to an orthonormal basis matrix that spans U ( X ), proceed to de fi ne  X  and R by only using eigenspace information of
X and Y in order to fi nally obtain U ( Z ). Given S as the number of new eigenvoices, U ( Z ),  X  , and R form  X  N S  X  ,  X  N S  X  matrices, respectively.

Handling U ( X ) as base eigenvoices of U ( Z ),  X  can be de  X  X  U  X  X  X  ;  X   X  11  X  where  X  is an  X  N T  X  orthonormal basis matrix for the compo-of columns designated as T is de fi ned as S P , where P is the column size of U ( X ). To compute the orthonormal basis matrix, we employ the Gram  X  Schmidt orthonormalization process that has been greatly applied for orthonormalization of vectors in pattern classi fi cation tasks ( Ramesha and Raja, 2011 ).

Next, R can be induced from (9) and (10) . The left side of (9) is generalized as
Z  X   X  Z  X  1 M  X  N  X  X  X  X   X  X  X  1 M  X   X   X  X  X  1 M  X   X  Z  X  1 M ; Y  X   X  1 N  X   X  Z  X  1 N  X  U  X  X  X   X   X  X  X  V  X  X  X  T  X   X   X  X  X  1 M  X   X  Z  X  1 M ; U  X   X   X  Y  X  1 N  X   X  Z  X  1 N  X  12  X  And the right side of (9) can be described as
U  X  Z  X   X   X  Z  X  V  X  Z  X  T  X   X  R  X   X  Z  X  V  X  Z  X  T  X  13  X 
From (11)  X  (13) , the following equation is derived:  X 
U  X  X  X  ;  X  T  X  U  X  X  X   X   X  X  X  V  X  X  X  T  X   X   X  X  X  1 M  X   X  Z
U  X  Y  X   X   X  Y  X  V  X  Y  X  T  X   X   X  Y  X  1 N  X   X  Z  X  1 N  X  R  X   X  Z  X 
After R is calculated in (14) , the merged eigenvoice U ( Z )de (9) is fi nally obtained. Because both  X  and R are de fi ned by U ( X ) and U ( Y ), U ( Z ) can be obtained directly from U ( X ) and U ( Y ). 3.3.3. Computational ef fi ciency of the proposed eigenvoice merging approach The advanced merging approach directly uses the eigenvoice sets U ( X )and U ( Y ) to obtain the merged eigenvoice U ( Z ), whereas the simple method employs supervectors X and Y obtained from respec-tive SD model groups. Because U ( X )and U ( Y )haveamuchsmaller dimension than X and Y , the advanced approach is capable of signi fi cantly reducing the time for eigenvoice merging. We next give an in-depth analysis of the approaches in terms of time complexity.
The time complexity required in the standard SVD process is de fi ned as O  X  nR 2  X  n 2 R  X  R 3  X  , when supervectors are de  X  n R  X  matrix, where n and R refer to the dimension of the supervector and the number of all supervectors, respectively (Golub and VanLoan, 1996; Brand, 2002 ). As a result, we can conclude that the time complexity of SVD is typically determined by the number of supervectors, rather than the dimension.
The simple merging method requires time to estimate new eigenvoices from two groups of supervectors, X and Y .Given M and method can be derived as O  X  n  X  M  X  N  X  2  X  n 2  X  M  X  N  X  X  X  advanced merging method requires time for two main procedures: eigenvoice estimation and eigenvoice merging. But the eigenvoice merging process is generally fi nished within a constant time frame once the two eigenvoice sets are provided. Therefore, the advanced method spends most of the time in eigenvoice estimation. The time complexity required during eigenvoice estimation can be analyzed as O  X  nR 2  X  n 2 R  X  R 3  X  in which R  X  max  X  M ; N  X  ,assumingthattheestima-these analyses, we conclude that the advanced merging method is more ef fi cient in time complexity, whereas the simple method requires more time, which increases together with the number of SD models. 4. Experimental results and discussion 4.1. Experimental setup
To evaluate the proposed speaker adaptation approaches, we performed speech recognition experiments on the Defense Adv-anced Research Projects Agency's Resource Management (RM) corpus ( Pallett et al., 1991 ). The DARPA's RM corpus contains continuous speech data of a suf fi cient number of individual spea-kers to be used for SD model training and evaluation. For this reason, it has been widely used for speaker recognition and speaker adaptation tasks, in addition to continuous speech recog-nition. For experiments, we divided speaker data sets of this corpus into several groups, following a way in Kubala et al. (1991) . The fi rst group consists of 109 speaker sets, all of which were used for building the standard SI model as well as individual SD models. The second group, including 12 speaker sets, was assigned as adaptation data of target speakers and veri fi data. Each speech data was parameterized into 12-dimensional Mel-Frequency Cepstral Coef fi cients (MFCC) along with normal-ized log-energy and their fi rst-and second-order time derivatives, yielding a 39-dimensional feature vector. Acoustic models were trained as continuous HMMs, with each HMM having three states and six Gaussian mixtures for each state. 4.2. Results and discussion
First, we compared the performance of the conventional repre-sentative adaptation techniques: MLLR, MAP, and eigenvoice. We also evaluated the performance of the st andard speaker independent (SI) system as a baseline. In this experi ment, we constructed mono-phone
HMMs to concentrate on their basic performance without regard to the effect of HMM structure. To investigate performance according to the amount of adaptation data, we divided all adaptation data of a target speaker into fi ve sets and made fi ve categories from 1 to 5, while sequentially adding each set in the next category. Each set includes three sentences spoke n by the target speaker and each sentence is approximately three to fi ve seconds in terms of duration. the second data set additively participated with the fi rst category to produce the second category (2). The fi nal category (5) contained all data from the speaker. Fig. 5 demonstrates the results in terms of the word recognition accuracy that is useful for verifying continuous speech recognition performance. As the amount of adaptation data increased, the performance of each technique improved. Eigenvoice outperformed MAP and MLLR overall. In particular, eigenvoice demon-strated outstanding performance in the fi rst category, whereas MAP and MLLR showed poor accuracy for such a small amount of adaptation data. MAP yielded lower accuracy than the SI system in the fi rst and second categories. This result supports the general property of the conventional approaches, addressed in Section 2 ,that the performance of MLLR and MAP greatly depends on the amount of adaptation data, but eigenvoice provides stable performance.
Next, we attempted to verify the ef fi ciency of the eigenvoice merging approaches explained in Section 3.3.2 . Two approaches that are characterized as  X  simple  X  and  X  advanced  X  were evaluated.
For building acoustic models, we used the general tri-phone HMM structure, which can reduce eigenvoice processing time while providing better results compared to mono-phone models. In this experiment, we investigated the word recognition accuracy achi-eved by the two approaches while increasing the amount of adaptation data using the same categories as the fi rst experiment.
In addition, we varied the number of eigenvoices for an in-depth analysis, extending from two eigenvoices (EV 2 ) to 10 eigenvoices (EV ) and 30 eigenvoices (EV 30 ). EV 2 ,EV 10 , and EV 30 were respectively designed as a small, medium, and large number of eigenvoices. Finally, to implement the eigenvoice merging appro-aches, we divided entire speaker models into two groups, desig-nated as SG1 and SG2. Among 109 SD models trained from the RM corpus, 55 and 54 SD models belonged to SG1 and SG2, respec-tively, to balance the two model groups. This is similar to sim-ulation of a case where 55 conventional SD models are used in the construction of an eigenvoice-based speech recognition system, and 54 new SD models additively participate in the enha-ncement of this system. In this case, the simple merging method estimates new eigenvoices using SG1 and SG2, disregarding the conventional eigenvoices estimated from SG1. On the other hand, the advanced approach fi rst estimates eigenvoices from SG2 and then calculates new eigenvoices, directly using both the conven-tional eigenvoices from SG1 and eigenvoices from SG2.
 Table 1 exhibits the results. To compare the results of the adapted models with that of the standard SI model, we state the accuracy of the SI model as a baseline that is not shown in this table. The performance of the SI model that was built from all 109 speaker sets was 93.4%. All results in this table that were achieved with the eigenvoice-based speaker adaptation are higher than the baseline accuracy, indicating that compared with the SI model, the adapted models convey better conditions for speech recognition for the target speaker.
 From this table, several distinguishing features are observed.
First, recognition accuracy increased according to the number of eigenvoices in both the simple merging method (Simple) and the advanced method (Advanced). This result demonstrates that the number of eigenvoices affects the performance of the adapted models. However, too many eigenvoices may induce overestima-tion of the SD models, thus deteriorating recognition accuracy.
Second, despite increasing amounts of adaptation data, outstand-ing improvement was not observed. The same phenomenon was investigated in Fig. 5 . This result is related to the advantage of the eigenvoice technique, which is the reliable adaptation perfor-mance provided regardless of the amount of adaptation data.
The fi nal analysis of this experimental result is directed toward the performance comparison between two merging approaches.
As shown in this table, the two approaches demonstrated similar recognition accuracy under all conditions related to the number of eigenvoices and the amount of adaptation data. In fact, the performance of the simple approach can be regarded as the upper limit of eigenvoice merging techniques because it produces optimal eigenvoices conveying the most suf fi cient speaker infor-mation by using entire SD models. On the other hand, merging techniques that do not allow direct use of the SD models may lose intrinsic speaker characteristics of the original SD models, thus weakening the effect of the adapted models. Nevertheless, the merged eigenvoices produced by the advanced approach achieved recognition accuracy similar to the upper limit, indicating only a 0.19% average difference between the two approaches. This out-standing result supports our idea that the advanced approach producing new eigenvoices by incorporating two eigenvoice sets has a great effect on eigenvoice merging.
 In addition to the achievement of adaptation performance superiority, the advanced merging approach aims to reduce computational time required for the estimation of new eigen-voices. For this reason, we investigated computational time for eigenvoice estimation, while changing the number of SD models pertinent to SG1 and SG2. We composed fi ve cases on the basis of the 109 SD models, while sequentially increasing the number of
SD models for SG1 by 20 models from initial 10 models. The remaining SD models in each case belonged to SG2. Thus, the numbers of SD models [#(SG1), #(SG2)] belonging to SG1 and SG2 in fi ve cases are de fi ned as [10:99], [30:79], [50:59], [70:39], and [90:19]. Computational time was measured in terms of CPU time, and the shortest time was chosen from among several repeated trials in order to minimize the effect of other processes running concurrently. Fig. 6 illustrates the results. The time in the simple
Accuracy (%) approach does not change, because this approach estimates new eigenvoices using all 109 SD models in all cases. On the other hand, the advanced approach distributes eigenvoice estimation time by SG1 and SG2, and only requires a longer time between time to estimate eigenvoices from SG1 and estimation time from
SG2, as described in Section 3.3.3 . As a result, the advanced approach demonstrated a great amount of time reduction for estimating eigenvoices compared to the simple one. Based on this result, we calculated an average time for estimating eigenvoices from an SD model and con fi rmed that the estimation fi nished within 10 s. This short period for obtaining eigenvoices suggests a strong possibility of applying our advanced merging approach for enhancing the performance of voice interaction systems in near real time.

In addition to computational time, the recognition accuracy in each case was also investigated in Fig. 6 .Ascon fi rmed in the previous experiment, the advanced approach also exhibited stable performance near the upper limit (the simple approach) in all cases.
Finally, we attempted to verify the ef fi ciency of the proposed selective eigenvoice merging approach. We fi rst divided the 109
SD models into eight groups, considering that at least 13 or 14 SD models are suf fi cient for providing common properties within a group. Although the most ideal division is based upon environ-mental type or speaker type, such a strict division necessarily requires prior knowledge about the speaker and environmental conditions for all data. This study targets unsupervised speaker adaptation tasks, and therefore, the SD models were randomly divided. In fact, such a random division can be handled as a baseline for the selective merging approach, because it will naturally give worse performance than that of supervised tasks.
For eight SD model groups, we conducted the proposed selective merging process to estimate new eigenvoices suitable for a given target speaker. We compared the performance of the proposed approach (EVM_adv  X  SEV) with that of other approaches, includ-ing the standard speaker independent (SI) system as a baseline, the simple merging approach (EVM_simp), and the advanced merging approach (EVM_adv). Fig. 7 represents the results. The proposed selective merging approach signi fi cantly outperformed the other approaches overall with regard to the amount of adaptation data. In particular, the proposed approach achieved superior performance even against the simple merging approach, which can be treated as a baseline for eigenvoice merging techniques. In average accuracy, the EVM_adv  X  SEV approach showed approximately 39%, 19%, and 23% relative improvement over SI, EVM_simp, and EVM_adv, respectively. This result reveals that our approach ef fi ciently selects the eigenvoice sets retaining acoustic characteristics relevant to the target speaker, thus obtain-ing the optimal eigenvoices for the speaker, even when a small amount of adaptation data is available.

Several previous works reported that combination of eigenvoice and other adaptation techniques such as MLLR and MAP can enhance speaker adaptation capability compared to individual approaches (Shinoda, 2011; Peter, 2010). For this reason, we attempted to verify the ef fi ciency of the proposed eigenvoice approach when it is combined with other techniques. The conventional eigenvoice approach was combined with MAP (EV  X  MAP), by using model parameters estimated by eigenvoice adaptation as the priors for MAP adaptation. In addition, for combination with MLLR (EV MLLR), we estimated transformation matrices using models re by eigenvoice adaptation. In the same way, the proposed eigenvoice approach handling a selective merging process was combined with strates the results. The proposed eigenvoice approach signi outperformed the conventional approach when combined with MAP and MLLR overall with regard to the amount of adaptation data. Combination with MAP and MLLR signi fi cantly improved the perfor-mance of the proposed approach itself (EV_SM) as the amount of adaptation data increased. However, combination of the conventional approaches failed to achieve the performance of the EV_SM with a small amount of adaptation data.

In the set of experiments addressed in this section, the proposed advanced merging approach achieved signi fi cant performance improvement over the standard SI model and also exhibited out-standing performance similar to the upper limit of the simple was successfully veri fi ed. Our experimental results demonstrate that the proposed approaches exhibit effects on the construction of user-Time (sec) Accuracy (%) Accuracy (%) speci fi c adapted models from target speakers, regardless of the amount of adaptation data. The results provide the strong possibility of applying our approaches for reliable voice interaction between a smart device and unspeci fi ed users. 5. Conclusion
This study proposed an ef fi cient eigenvoice-based speaker adaptation technique for the purpose of constructing reliable adapted models relevant to the target speaker. The proposed approach selects optimal eigenvoices for the speaker in accor-dance with an advanced merging technique. To evaluate this approach, we performed speech recognition experiments using the DARPA's RM corpus. Our selective merging approach exhibited superior performance to conventional adaptation methods. This result indicates that the proposed approach constructs optimal adapted models representing acoustic characteristics of the target speaker and is very applicable to user-speci fi c voice interaction systems.

In future work, we will evaluate the proposed approach under other speech corpora in order to verify ef fi ciency using more SD models. Based on the corpora, we will also validate our approach in comparison with the conventional kernel eigenvoice adaptation approaches in terms of speaker adaptation capability. In addition, we will investigate combination of these two eigenvoice adapta-tion approaches with the expectation of better adaptation performance.
 Acknowledgements
This research was supported by Basic Science Research Program through the National Research Foundation of Korea (NRF) funded by the Ministry of Education (2014R1A1A2057751).
 Appendix A. Supplementary data
Supplementary data associated with this paper can be found in the online version at http://dx.doi.org/10.1016/j.engappai.2015.01. 010 .
 References
