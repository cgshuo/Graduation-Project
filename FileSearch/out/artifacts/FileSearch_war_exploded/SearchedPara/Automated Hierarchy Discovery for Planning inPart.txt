 { lcharlin,ppoupart } @cs.uwaterloo.ca Planning in partially observable domains is a notoriously d ifficult problem. However, in many real-world scenarios, planning can be simplified by decomposing t he task into a hierarchy of smaller planning problems. Such decompositions can be exploited in planning to temporally abstract sub-and Zhou [10] proposed various algorithms that speed up plan ning in partially observable domains by exploiting the decompositions induced by a hierarchy. Ho wever these approaches assume that a policy hierarchy is specified by the user, so an important que stion arises: how can we automate the discovery of a policy hierarchy? In fully observable domain s, there exists a large body of work on hierarchical Markov decision processes and reinforcement learning [6, 21, 7, 15] and several hier-archy discovery techniques have been proposed [23, 13, 11, 2 0]. However those techniques rely on the assumption that states are fully observable to detect ab stractions and subgoals, which prevents their use in partially observable domains.
 corresponding to the hierarchy and policy parameters. We pr esent an approach that searches in the non-convex optimization problem that we tackle using three approaches: generic non-linear solvers, a mixed-integer non-linear programming approximation or a n alternating optimization technique that can be thought as a form of hierarchical bounded policy i teration. We also generalize Hansen and Zhou X  X  hierarchical controllers [10] to allow recursive controllers. These are controllers that may recursively call themselves, with the ability of repres enting policies with a finite number of parameters that would otherwise require infinitely many par ameters. Recursive policies are likely to arise in language processing tasks such as dialogue manag ement and text generation due to the recursive nature of language models. We first review partially observable Markov decision proces ses (POMDPs) (Sect. 2.1), which is the framework used throughout the paper for planning in partial ly observable domains. Then we review how to represent POMDP policies as finite state controllers ( Sect. 2.2) as well as some algorithms to optimize controllers of a fixed size (Sect. 2.3). 2.1 POMDPs POMDPs have emerged as a popular framework for planning in pa rtially observable domains [12]. A POMDP is formally defined by a tuple ( S, O, A, T, Z, R,  X  ) where S is the set of states, O is Pr  X  ( s 0 | s, a ) = Pr( s 0 | s, a )  X  the past actions and observations. Thus we define a policy  X  as a mapping from histories of past probability distribution over states, taking into account the information provided by past actions and observations. Given a belief b , after executing a and receiving o , we can compute an updated constant. The value V  X  of policy  X  when starting with belief b is measured by the expected sum of the future rewards: V  X  ( b ) = P value function also satisfies Bellman X  X  equation: V  X  ( b ) = max where Pr( o | b, a ) = P 2.2 Policy Representation E of directed edges Each node n has one outgoing edge per observation. A controller encodes a policy encoded by a controller is executed by doing the actio n a n traversed at time step t and following the edge labelled with observation o n distributions over actions and successor nodes. More preci sely, let Pr which an action a is sampled in node n and let Pr is computed by solving the following system of linear equati ons: n ( s ) = While there always exists an optimal policy representable by a deterministic controller, this con-troller may have a very large (possibly infinite) number of no des. Given time and memory con-explains why searching in the space of stochastic controlle rs may be advantageous.
Table 1: Quadratically constrained optimization program f or bounded stochastic controllers [1]. 2.3 Optimization of Stochastic Controllers The optimization of a stochastic controller with a fixed numb er of nodes can be formulated as a quadratically constrained optimization problem (QCOP) [1 ]. The idea is to maximize V  X  by varying the controller parameters Pr and the joint distribution Pr( n 0 , a | n, o ) = Pr is a proper distribution and that P 0 convergence to a local optimum. Several techniques have bee n tried including gradient ascent [14], SNOPT (based on sequential quadratic programming) [1, 8]. E mpirically, biased-BPI (version of have been shown to outperform the other approaches on some be nchmark problems [19, 1]. We alternates between policy evaluation and policy improveme nt. Given a policy with fixed parameters improvement can be viewed as a linear simplification of the pr ogram in Table 1 achieved by fixing V optimizing the controller parameters Pr( n 0 , a | n, o ) and the value V Hansen and Zhou [10] recently proposed hierarchical finite-state controllers as a simple and intu-edges as in a flat controller, however some nodes may be abstra ct, corresponding to sub-controllers themselves. As with flat controllers, concrete nodes are par ameterized with an action mapping  X  and edges outgoing concrete nodes are parameterized by a suc cessor node mapping  X  . In contrast, abstract nodes are parameterized by a child node mapping ind icating in which child node the sub-controller should start. Hansen and Zhou consider two schem es for the edges outgoing abstract nodes: either there is a single outgoing edge labelled with a null observation or there is one edge which the subcontroller terminated.
 Subcontrollers encode full POMDP policies with the additio n of a termination condition. In fully observable domains, it is customary to stop the subcontroll er once a goal state (from a predefined Hansen and Zhou propose to terminate a subcontroller when an end node (from a predefined set of terminal nodes) is reached. Since the decision to reach a ter minal node is made according to the successor node mapping  X  , the timing for returning control is implicitly optimized. Hansen and Zhou propose to use | A | terminal nodes, each mapped to a different action. Terminal nodes do not have any outgoing edges nor any action mapping since they alr eady have an action assigned. The hierarchy of the controller is assumed to be finite and spe cified by the programmer. Subcon-trollers are optimized in isolation in a bottom up fashion. S ubcontrollers at the bottom level are made up only of concrete nodes and therefore can be optimized as usual using any controller op-timization technique. Controllers at other levels may cont ain abstract nodes for which we have to to concrete actions, but rather to children nodes. Hence, th e immediate reward of an abstract node  X  n corresponds to the value V This transition probability can be computed by solving the f ollowing linear system: Subcontrollers with abstract actions correspond to partia lly observable semi-Markov decision pro-cesses (POSMDPs) since the duration of each abstract action may vary. The duration of an action is important to determine the amount by which future rewards sh ould be discounted. Hansen and Zhou propose to use the mean duration to determine the amount of di scounting, however this approach tion (i.e., Pr solve POSMDPs with the same algorithms as POMDPs. Hence, giv en the abstract reward function linear system in Eq. 2, we have a POSMDP which can be optimized using any POMDP optimization technique (as long as the discount factor is absorbed into th e transition function). Hansen X  X  hierarchical controllers have two limitations: t he hierarchy must have a finite number of may have infinitely many levels. We also describe an algorith m to discover a suitable hierarchy by simultaneously optimizing the controller parameters and h ierarchy. 3.1 Recursive Controllers In some domains, policies are naturally recursive in the sense that they decompose into subpolicies that may call themselves. This is often the case in language p rocessing tasks since language models such as probabilistic context-free grammars are composed o f recursive rules. Recent work in dia-logue management uses POMDPs to make high level discourse de cisions [24]. Assuming POMDP urally arise. Similarly, language generation with POMDPs w ould naturally lead to recursive policies that reflect the recursive nature of language models.
 We now propose several modifications to Hansen and Zhou X  X  hie rarchical controllers that simplify posed of any node (including the parent node itself) and tran sitions can be made to any node any-nodes may be shared across levels. Second, we use a single ter minal node that has no action nor any action is executed upon termination of the subcontroller. H ence, the actions that were associated with the terminal nodes in Hansen and Zhou X  X  proposal are ass ociated with the abstract nodes in our proposal. This allows a uniform parameterization of act ions for all nodes while reducing the number of terminal nodes to 1. Fourth, the outer edges of abst ract nodes are labelled with regular observations since an observation will be made following th e execution of the action of an abstract transition probabilities (i.e., Pr Figure 1: The figures represent controllers and transitions as written in Equations 5 and 6b. Along-side the directed edges we X  X e indicated the equivalent part of the equations which they correspond to. 3.2 Hierarchy and Policy Optimization problem corresponds to the optimal policy (and hierarchy) f or a fixed set N of concrete nodes n and a fixed set  X  N of abstract nodes  X  n . The variables consist of the value function V the occupancy frequency oc ( n, s | n (Eq. 3) is the expected value P b . The constraints in Equations 4 and 5 respectively indicate the expected value of concrete and abstract nodes. The expected value of an abstract node corre sponds to the sum of three terms: the expected value V immediately after the termination of the subcontroller and the future rewards V transitions (dashed line) or the beginning/termination of a subcontroller (bold/dotted line). Edges are labelled with the corresponding transition probabilit y variables.
 Note that the reward R ( s Hence we need to compute the probability that the last state v isited in the subcontroller is s This probability is given by the occupancy frequency oc ( s Figure 1b illustrates graphically the relationship betwee n the variables in Eq. 6b. Eq. 7 prevents to the labelling of all abstract nodes, which induces an orde ring on the abstract nodes. Only the node. This constraint ensures that chains of child node mapp ings have a finite length, eventually needed to guarantee that the policy parameters and the child node mappings are proper distributions. 3.3 Algorithms solve. We consider three approaches inspired from the techn iques for non-hierarchical controllers: Non-convex optimization: Use a general non-linear solver, such as SNOPT, to directly t ackle the optimization problem in Table 2. This is the most convenient approach, however a globally optimal solution may not be found due to the non-convex nature of the p roblem.
 Mixed-Integer Non-Linear Programming (MINLP): We restrict Pr( n 0 , a | n, o ) and Pr( n still non-convex but can be tackled with a mixed-integer non -linear solver such as MINLP BB 2 . Bounded Hierarchical Policy Iteration (BHPI): We alternate between (i) solving a simplified ver-ables. More precisely, we fix V are now cubic, involving products of variables that include a single continuous variable. This per-Table 2: Non-convex quarticly constrained optimization pr oblem for hierarchy and policy discovery in bounded stochastic recursive controllers. mits the use of disjunctive programming [2] to linearize the constraints without any approximation. The idea is to replace any product BX (where B is binary and X is continuous) by a new continuous variable Y constrained by lb where lb ing mixed-integer linear program (MILP) and update V values for V time limit is reached. Although, convergence cannot be guar anteed, in practice we have found BHPI to be monotonically increasing. Note that fixing V 3.4 Discussion Discovering a hierarchy offers many advantages over previo us methods that assume the hierarchy is possible to find a better one. Note however that discovering t he hierarchy while optimizing the policy is a much more difficult problem than simply optimizin g the policy parameters. Additional variables (e.g., Pr( n 0 , a | n, o ) and oc ( s, n | s increases. Our approach can also be used when the hierarchy a nd the policy are partly known. It is bounds. This also has the benefit of simplifying the optimiza tion problem.
 k Recursive controllers allow abstract nodes to call subcont rollers that may contain themselves. An nodes. As a comparison, recursive controllers are to non-re cursive hierarchical controllers what and interpret than flat controllers given their natural deco mposition into subcontrollers and their possibly smaller size. We report on some preliminary experiments with three toy pro blems (paint, shuttle and maze) from the POMDP repository 3 . We used the SNOPT package to directly solve the non-convex o ptimization We optimized hierarchical controllers of two levels with a fi xed number of nodes reported in the column labelled  X  X um. of Nodes X . The numbers in parentheses indicate the number of nodes at with minimal computational time. In contrast, BHPI is less r obust and takes up to several orders of magnitude longer. MINLP BB returns good solutions for the smaller problems but is una ble to find feasible solutions to the larger ones. We also looked at the h ierarchy discovered for each problem the one hand coded by Pineau in her PhD thesis [16]. Given the r elatively small size of the test problems, these experiments should be viewed as a proof of co ncept that demonstrate the feasibility of our approach. More extensive experiments with larger pro blems will be necessary to demonstrate the scalability of our approach. lems. We model the search for a good hierarchical policy as a n on-convex optimization problem with variables corresponding to the hierarchy and policy pa rameters. We propose to tackle the op-timization problem using non-linear solvers such as SNOPT o r by reformulating the problem as an approximate MINLP or as a sequence of MILPs that can be thou ght of as a form of hierarchical bounded policy iteration. Preliminary experiments demons trate the feasibility of our approach, how-restricted to a smaller range) which simplifies the optimiza tion problem and improves scalability. We also generalize Hansen and Zhou X  X  hierarchical controll ers to recursive controllers. Recursive ment and text generation where recursive policies are expec ted to naturally capture the recursive nature of language models.
 Acknowledgements: this research was supported by the Natural Sciences and Engi neering Re-search Council (NSERC) of Canada, the Canada Foundation for Innovation (CFI) and the Ontario Innovation Trust (OIT).

