 Compressive sensing (CS), also known as compressive sampling, has recently received increasing attention in many areas of science and engineering [3]. In CS, an unknown sparse signal is recon-structed from a single measurement vector. Recent theoretical studies show that one can recover certain sparse signals from far fewer samples or measurements than traditional methods [4, 8]. In this paper, we consider the problem of reconstructing sparse signals in the multiple measurement vector (MMV) model, in which the signal, represented as a matrix, consists of a set of jointly sparse vectors. MMV is an extension of the single measurement vector (SMV) model employed in standard compressive sensing.
 The MMV model was motivated by the need to solve the neuromagnetic inverse problem that arises in Magnetoencephalography (MEG), which is a modality for imaging the brain [7]. It arises from channels [6], echo cancellation [9], magenetoencephalography [12], computing sparse solutions to linear inverse problems [7], and source localization in sensor networks [17]. Unlike SMV, the signal in the MMV model is represented as a set of jointly sparse vectors sharing their common nonzeros lead to improved performance in signal recovery [5, 10, 16, 21].
 Several recovery algorithms have been proposed for the MMV model in the past [5, 7, 18, 24, 25]. Since the sparse representation problem is a combinatorial optimization problem and is in general NP-hard [5], the algorithms in [18, 25] employ the greedy strategy to recover the signal using an iterative scheme. One alternative is to relax it into a convex optimization problem, from which the global optimal solution can be obtained. The most widely studied approach is the one based on the (2 , 1) -norm minimization [5, 7, 10]. A similar relaxation technique (via the 1 -norm minimization) is employed in the SMV model. Recent studies have shown that most of theoretical results on the convex relaxation of the SMV model can be extended to the MMV model [5], although further the-oretical investigation is needed [26]. Unlike the SMV model where the 1 -norm minimization can be solved efficiently, the resulting convex optimization problem in MMV is much more difficult to solve. Existing algorithms formulate it as a second-order cone programming (SOCP) or semdefinite programming (SDP) [16] problem, which can be solved using standard software packages such as SeDuMi [23]. However, for problems of moderate size, solving either SOCP or SDP is computa-tionally expensive, which limits their use in practice.
 In this paper, we derive a dual reformulation of the (2 , 1) -norm minimization problem in MMV. min-max problem, which can be solved efficiently via the prox-method with a nearly dimension-independent convergence rate [19]. Compared with existing algorithms, our algorithm can scale to larger problems while achieving high accuracy. Interestingly, our theoretical analysis reveals the close relationship between the resulting min-max problem and multiple kernel learning [14]. We have performed simulation studies and our results demonstrate the scalability of the proposed algo-rithm in comparison with existing algorithms.
 Notations: All matrices are boldface uppercase. Vectors are boldface lowercase. Sets and spaces as k v k p := ( r, p ) -norm of A is defined as: In the SMV model, one aims to recover the sparse signal w from a measurement vector b = Aw for a given matrix A [3]. The SMV model can be extended to the multiple measurement vector (MMV) model, in which the signal is represented as a set of jointly sparse vectors sharing a common simultaneously. It has been shown that the MMV model provably improves the standard CS recovery by exploiting the block-sparse structure [10, 21].
 Specifically, in the MMV model we consider the reconstruction of the signal represented by a matrix Each column of A is associated with an atom, and a set of atom is called a dictionary. A sparse representation means that the matrix W has a small number of rows containing nonzero entries. Usually, we have m  X  d and d &gt; n .
 Similar to SMV, we can use k W k p, 0 to measure the number of rows in W that contain nonzero en-tries. Thus, the problem of finding the sparsest representation of the signal W in MMV is equivalent to solving the following problem, a.k.a. the sparse representation problem: Some typical choices of p include p =  X  and p = 2 [25]. However, solving (P0) requires enumer-and is in general NP-hard [5]. Similar to the use of the 1 -norm minimization in the SMV model, one tion problem (P1): The relationship between (P0) and (P1) for the MMV model has been studied in [5].
 For p = 2 , the optimal W is given by solving the following convex optimization problem: Existing algorithms formulate Eq. (5) as a second-order cone programming (SOCP) problem or a semidefinite programming (SDP) problem [16]. Recall that the optimizaiton problem in Eq. (5) is equivalent to the following problem by removing the square in the objective: second-order cone programming (SOCP) formulation: Based on this SOCP formulation, it can also be transformed into the standard semidefinite program-ming (SDP) formulation: The interior point method [20] and the bundle method [13] can be applied to solve SOCP and SDP. However, they do not scale to problems of moderate size, which limits their use in practice. In this section we present a dual reformulation of the optimization problem in Eq. (5). First, some preliminary results are summarized in Lemmas 1 and 2: Lemma 1. Let A and X be m -by-d matrices. Then the following holds: When the equality holds, we have k X k 2 , 1 = k A k 2 ,  X  .
 Proof. It follows from the definition of the ( r, p ) -norm in Eq. (1) that k X k 2 , 1 = max 1  X  i  X  m k a i k 2 for 1  X  k  X  m . Thus, k A k 2 ,  X  = k a k k 2 , and we have Clearly, the last inequality becomes equality when k X k 2 , 1 = k A k 2 ,  X  .
 Lemma 2. Let A and X be defined as in Lemma 1. Then the following holds: that  X  k = 0 for k /  X  Q ,  X  k  X  0 for k  X  Q , and of Lemma 1 become equalities if and only if we construct the matrix X as follows: as in Eq. (9).
 Based on the results established in Lemmas 1 and 2, we can derive the dual formulation of the optimization problem in Eq. (5) as follows. First we construct the Lagrangian L : The dual problem can be formulated as follows: It follows from Lemma 2 that Note that from Lemma 2, the equality holds if and only if the optimal W  X  can be represented as Following the definition of the (2 ,  X  ) -norm, we can reformulate the dual problem in Eq. (12) as a min-max problem, as summarized in the following theorem: Theorem 1. The optimization problem in Eq. (5) can be formulated equivalently as: where the matrix G i is defined as G i = a i a T i (1  X  i  X  d ) , and a i is the i th column of A . Proof. Note that k A T U k 2 2 ,  X  can be reformulated as follows: Substituting Eq. (14) into Eq. (12), we obtain the following problem: Since the Slater X  X  condition [2] is satisfied, the minimization and maximization in Eq. (15) can be exchanged, resulting in the min-max problem in Eq. (13).
 Based on the solution to the dual problem in Eq. (13), we can construct the optimal solution to the primal problem in Eq. (5) as follows. Let W  X  be the optimal solution of Eq. (5). It follows from the equality constraint AW  X  = B . The main result is summarized in the following theorem: k  X  optimal solution to the problem in Eq. (13).
 that the partial derivative of the objective function with respect to U  X  in Eq. (13) is 0, that is, Next we prove the reverse direction by assuming AW  X  = B . Since W  X  = diag (  X  ) A T U  X  , we have Define the function  X  (  X  1 ,  X  X  X  ,  X  d , U ) as is concave with respect to U , thus its maximum is achieved when its partial derivative with respect to U is zero. It follows from Eq. (16) that  X  X   X  U is zero when U = U  X  . Thus, we have With a fixed U = U  X  ,  X  (  X  1 ,  X  X  X  ,  X  d , U  X  ) is a linear combination of  X  i (1  X  i  X  d ) as: By the assumption, we have k ( A T U  X  ) i k = k A T U  X  k 2 ,  X  , if  X  i &gt; 0 . Thus, we have Therefore, for any U ,  X  1 ,  X  X  X  ,  X  d such that (  X  , U  X  ) is the optimal solution to the problem in Eq. (13).
 Theorem 2 shows that we can reconstruct the solution to the primal problem based on the solution to the dual problem in Eq. (13). It paves the way for the efficient implementation based on the min-max formulation in Eq.(13). In this paper, the prox-method [19], which is discussed in detail in the next section, is employed to solve the dual problem in Eq. (13).
 An interesting observation is that the resulting min-max problem in Eq. (13) is closely related to the optimization problem in multiple kernel learning (MKL) [14]. The min-max problem in Eq. (13) can be reformulated as where the positive semidefinite (kernel) matrix G is constrained as a linear combination of a set of base kernels The formulation in Eq. (18) connects the MMV problem to MKL. Many efficient algorithms [14, 22, 27] have been developed in the past for MKL, which can be applied to solve (13). In [27], an extended level set method was proposed to solve MKL, which was shown to outperform the one based on the semi-infinite linear programming formulation [22]. However, the extended level set method involves a linear programming in each iteration and its theoretical convergence rate of O (1 / the next section. We propose to employ the prox-method [19] to solve the min-max formulation in Eq. (13), which has a differentiable and convex-concave objective function. The algorithm is called  X  X MV prox  X . The prox-method is a first-order method [1, 19] which is specialized for solving the saddle point problem and has a nearly dimension-independent convergence rate of O (1 /N ) ( N denotes the number of iterations). We show that each iteration of MMV prox has a low computational cost, thus it scales to large-size problems.
 The key idea is to convert the min-max problem to the associated variational inequality (v.i.) prob-Eq. (13) is equivalent to the following associated v.i. problem [19]: where In solving the v.i. problem in Eq. (19), one key building block is the following projection problem: and Following [19], we present the pseudocode of the proposed MMV prox algorithm in Algorithm 1. In by the parameter  X  ). It has been shown in [19] that, when  X   X  1  X  w t, 2 = w t, 1 always holds. Moreover, Algorithm 1 has a global dimension-independent convergence rate of O (1 /N ) .
 Algorithm 1 The MMV prox Algorithm Input: A , B ,  X  , z 0 = (  X  0 , U 0 ) , and  X  Output:  X  , U and W .

Step t ( t  X  1 ) : Set w t, 0 = z t  X  1 and find the smallest s = 1 , 2 , . . . such that Final Step : Set  X  = Time Complexity It costs O ( dmn ) to evaluate the operator F (  X  ) at a given point.  X   X  in Eq. (22) involves the Euclidean projection onto the simplex [1], which can be solved in linear time, i.e., in Our analysis shows that MMV prox scales to large-size problems.
 In comparison, the second-order methods such as SOCP have a much higher complexity per iter-ation. According to [15], the SOCP in Eq. (6) costs O ( d 3 ( n + 1) 3 ) per iteration. In MMV, d is typically larger than m . In this case, the proposed MMV prox algorithm has a much smaller cost per iteration than SOCP. This explains why MMV prox scales better than SOCP, as shown in our experiments in the next section. Table 1: The averaged recovery results over 10 experiments ( d = 100 , m = 50 , and n = 80 ). In this section, we conduct simulations to evaluate the proposed MMV prox algorithm in terms of the recovery quality and scalability.
 Experiment Setup We generated a set of synthetic data sets (by varying the values of m , n , and normal distribution N (0 , 1) ; W  X  IR d  X  n (the ground truth of the recovery problems) was generated in two steps: (1) randomly select k rows with nonzero entries; (2) randomly generate the entries of those k rows from N (0 , 1) . We denote by W p the solution obtained from the proposed MMV prox algorithm. Ideally, W p should be close to W . Our experiments were performed on a PC with Intel Core 2 Duo T9500 2.6G CPU and 4G RAM. We employed the optimization package SeDuMi [23] for solving the SOCP formulation. All codes were implemented in Matlab. In all experiments, we terminate MMV prox when the change of the consecutive approximate solutions is less than 1e-6. Recovery Quality In this experiment, we evaluate the recovery quality of the proposed MMV prox algorithm. We applied MMV prox on the data sets of size d = 100 , m = 50 , n = 80 , and reported the averaged experimental results over 10 random repetitions. We measured the recovery quality in terms of the mean squared error: which measures the violation of the constraint in Eq. (5). The experimental results are presented in Table 1. We can observe from the table that MMV prox recovers the sparse signal successfully in all cases.
 Next, we study how the recovery error changes as the sparsity of W varies. Specifically, we applied MMV prox on the data sets of size d = 100 , m = 400 , and n = 10 with k (the number of nonzero rows of W ) varying from 0 . 05 d to 0 . 7 d , and used measure. The averaged experimental results over 20 random repetitions are presented in Figure 1. We can observe from the figure that MMV prox works well in all cases, and a larger k (less sparse W ) tends to result in a larger recovery error. Scalability In this experiment, we study the scalability of the proposed MMV prox algorithm. We generated a collection of data sets by varying m from 10 to 200 with a step size of 10 , and setting n = 2 m and d = 4 m accordingly. We applied SOCP and MMV prox on the data sets and recorded their computation time. The experimental results are presented in Figure 2 (a), where the x -axis corresponds to the value of m , and the y -axis corresponds to log( t ) , where t denotes the computa-tion time (in seconds). We can observe from the figure that the computation time of both algorithms increases as m increases and SOCP is faster than MMV prox on small problems ( m  X  40 ); when m &gt; 40 , MMV prox outperforms SOCP; when the value of m is large ( m &gt; 80 ), the SOCP for-mulation cannot be solved by SeDuMi, while MMV prox can still be applied. This experimental result demonstrates the good scalability of the proposed MMV prox algorithm in comparison with the SOCP formulation. To further examine the scalability of both algorithms, we compare the execution time of each itera-tion for both SOCP and the proposed algorithm. We use the same setting as in the last experiment, i.e., n = 2 m, d = 4 m , and m ranges from 10 to 200 with a step size of 10 . The time comparison of SOCP and MMV prox is presented in Figure 2 (b). We observe that MMV prox has a significantly lower cost than SOCP in each iteration (note that SOCP is not applicable for m &gt; 80 ). This is consistent with our complexity analysis in Section 4.
 We can observe from Figure 2 that when m is small, the computation time of SOCP and MMV prox is comparable, although MMV prox is much faster in each iteration. This is because MMV prox is a first-order method, which has a slower convergence rate than the second-order method SOCP. Thus, there is a tradeoff between scalability and convergence rate. Our experiments show the advantage of MMV prox for large-size problems. In this paper, we consider the (2 , 1) -norm minimization for the reconstruction of sparse signals in the multiple measurement vector (MMV) model, in which the signal consists of a set of jointly sparse vectors. Existing algorithms formulate it as second-order cone programming or semdefinite programming, which is computationally expensive to solve for problems of moderate size. In this paper, we propose an equivalent dual formulation for the (2 , 1) -norm minimization in the MMV model, and develop the MMV prox algorithm for solving the dual formulation based on the prox-method. In addition, our theoretical analysis reveals the close connection between the proposed dual formulation and multiple kernel learning. Our simulation studies demonstrate the effectiveness of the proposed algorithm in terms of recovery quality and scalability. In the future, we plan to algorithm. In addition, we plan to examine the efficiency of the prox-method for solving various MKL formulations.
 This work was supported by NSF IIS-0612069, IIS-0812551, CCF-0811790, NIH R01-HG002516, NGA HM1582-08-1-0016, and NSFC 60905035.
