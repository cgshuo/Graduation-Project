 Enterprise database always has hundreds of inner-linked tables. Users who are unfamiliar with the dataset must comprehend the database schema before their query or development. Consider the example schema based on the TPCE [1] benchmark in Fig. 1, which is small, compared to most enterprise databases. Even so, it is challenging to understand such a complex schema, which leads to a growing interest in automatic methods for summarizing the database schema called schema summarization, an effect ive method of reducing the database schema complexity.

An expected schema summarization provides a succinct overview of the entire schema in the form of clustered categorie s, each of which is represented by a top-ical table, making it possible to explore relevant schema components. Recently there have been much related research w ork on how to generate schema summa-rization automatically. Existing research on schema summarization is conducted mainly on three types [2]. The first one focused on ER model abstraction [3]. It aims to cluster ER entities into abstract entities and rely heavily on the se-mantic relationships. The second one focused on semi-structured database, such as XML database [4]. It generates schema summarization of hierarchical data models. The third one summarizes schema on relational database schema [5]. This paper focuses on the last type of schema summarization.

Considering the existing relational database schema summarization researches, most of them use unsupervised clustering algorithm to summarize schemas. For example, the method in [6] is developed for schema summarization with the Weighted k -Center Algorithm and the method in [7] uses hierarchical clustering algorithm to build hierarchy clusters. We defer a detailed discussion of these al-gorithm to Section 4.4. Our experiments show that they are not effective enough for large scale database.

In this paper, we describe SSLP: a Schema Summarization Approach based on Label Propagation. It generates the schema summarization of a relational database automatically based on semi-supervised label propagation algorithm [8,9], which can predict the information of unlabeled nodes by a few of la-beled nodes. It finds that unlabeled data, when used in conjunction with a small amount of labeled data, can produce considerable improvement in learning ac-curacy. Labels propagate to all nodes according to their similarity. We define a table similarity kernel function to compute the table similarity, based on Ra-dial Basis Function kernel(RBF kernel) [10], mathematically analyze some of its properties comprehensively. The design of a good kernel function underlies the success of schema summarization. Prope r similarity measure can improve the accuracy of label propagation. All the tables in the schema graph will have their own label at the end of the propagation. Intuitively, tables having the same label belong to the same category. SSLP can generate a schema summarization in the form of a set of categories, and each category is represented by a topical table. Further discussion of SSLP approach appears in Section 3.

In summary, this paper makes the following contributions:  X  We propose a new approach to quantifying table similarity using a novel  X  We propose the SSLP approach to create the schema summarization, which  X  We have extensively evaluated SSLP over TPCE. The results demonstrate Aschemagraph G is defined as ( V,E ), where each node v  X  V in the graph denotes as a table and each edge e  X  E denotes as a foreign key constraint. A table may have multiple foreign keys. If there are multiple foreign key constraints between the same pair of tables, we coll ect and represent them with one edge. A labeled schema graph G L is appending the function L to the schema graph G and extending G to a fully connected graph, as definition 1 interprets. Definition 1. Labeled Schema Graph. A labeled schema graph G L is a complete graph, denoted as a triple ( V,E,L ), where V is a set of tables in schema graph G and E = V  X  V is a set of edges. L is a labeling function that assigns a label to each table. L ( v )= l denotes that the label of table v is l .The weight on each edge, denoted as weight ( v i ,v j ), is a function of the similarity between nodes v i and v j .
 The labeled schema graph can be regarded as a sparse labeled data region V is the number of graph nodes and k is the initial number of the labeled nodes), combining with a dense unlabeled data region V U = { v k +1 ,v k +2 ,...,v n } ( V U  X  V ), where  X  v  X  V U , L ( v )= null .

Our approach aims to estimate the label of V U from V L automatically and group tables into categories based on their labels. Based on the above notions, we define our problem as follows.
 Definition 2. Schema Summarization. Given a labeled schema graph G L = ( V,E,L ) for a relational database, a summary of G L with size k is a k -partition C = { C 1 ,C 2 ,  X  X  X  ,C k } over the tables in V . Each category c  X  C has a labeled topical table in V L , defined as t ( c ). For each category c , all the tables included have the same label with t ( c ):  X  c  X  C,  X  v  X  c,L ( v )= L ( t ( c )). The summarized summary of the labeled schema graph is represented as the k -partition C = { C 1 ,C 2 ,  X  X  X  ,C k } .
 This section describes our approach of rel ational database schema summariza-tion. SSLP takes a relational database schema graph as the input and returns the partition of tables as the output. It is made up of three major modules: table similarity computing, table importance tagging and label propagation. 3.1 Table Similarity Computing Intuitively, a cogent summary should be one such that tables within the same category are similar while tables from different categories are diverse. A prob-lem with previous similarity measures is that each of them is tied to a particular dataset or assumes a particular domain model. For example, name-based similar-ity model[11] assume that table names conform to the nomenclature. If database tables are named non-compliantly, n ame-based measures will not work.
This paper presents a definition of similarity that satisfies our intuitions about similarity. These intuitions are listed here. Each followed by a detailed review explanation of its underlying rationale.
 Intuition 1. Name Similarity. If two tables are similar, their table names and attribute names may  X  X ook X  like similar.

We extend Vector Space Model to calculate the similarity between the instance names. Each table, denoted as a vector W , is represented as a text document, which contains keywords from the name of the table and the names of its at-tributes. W is obtained by TF*IDF function [12]. Given table v i and table v j . Let Sim n ( v i ,v j ) be the name similarity of v i and v j , corresponds to the simi-larity between two vectors W i and W j , which may be evaluated via the Cosine function as follows.
 Intuition 2. Value Similarity. If two tables are similar, they may have several attributes containing similar values [13].

We employ the Jaccard similarity coefficient function to calculate the value similarity. For every two attributes A and B , the similarity between them is defined as follows.

The value similarity of a table pair can be computed as the average attribute similarity. Given table v i and table v j .Let Sim v ( v i ,v j ) be the value similarity of v i and v j , which is defined as follows.
 where | v | is the number of attributes in v and Z is an attribute collection of all the matching attribute pairs based on a greedy-matching strategy. Intuition 3. Cardinality Similarity. The cardinality defines the relationship between the entities in terms of numbers. The three main cardinality relation-ships are: one-to-one, expressed a s1:1;one-to-many,expressedas1: M ;and many-to-many, expressed as M : N . Based on intuition, the table similarity in 1 : 1 relationship is much greater than in 1 : M and M : N relationship. and v j , which is defined as follows.
 where  X  are the tuple of table v , fan (  X  ) is the number of edges incident to tuple  X  , q counts the number of tuples satisfying fan (  X  ) 0.

In machine learning, the (Gaussian) radial basis function kernel, or RBF ker-nel, is a popular kernel function. We define a table similarity kernel function based on RBF kernel to get the accurate table similarity.
 Definition 3. Table Similarity Kernel Function. The table similarity kernel function on table v i and table v j is defined as where K ( v i ,v j ) is the measure of the table similarity.  X  is a feature map which maps the space of inputs into some dot product space. This kernel function is controlled by a parameter  X  , which is studied by average label entropy method in Section 4.2. dist ( v i ,v j ) measures the distance between two tables, which is defined with all the similarity properties mentioned above. The distance function is defined as where Sim represents the p -dimensional similarity feature vector and p is the number of similarity properties. Note that each property is normalized to ad-just similarities measured on different scales to a notionally common scale.  X  is denoted as a p -dimensional parameter vector which is applied to quantify the strength of the relationship between dist and Sim . b is defined to capture all other factors which influence the dist function other than the known similarity feature.

The table similarity kernel function has high scalability. If other novel simi-larity features are proposed in future work, the similarity function still works.
A weight to each edge of the labeled schema graph is positively correlated with the table similarity, see below. 3.2 Table Importance Tagging Intuitively, a cogent summary should be informative. We select important tables as the labeled data in labeled schema graph. The definition of the table importance introduced in [6] is equivalent to the stationary distribution of a random walk process. Each table is first given an initial importance as where IC ( v ) represents the initial information content of the table v , | v | is the number of tuples in v , v.A is an attribute of v and H ( v.A )presentstheentropy of the attribute A , which is defined as where k is the number of different values of attribute v.A .Let v.A = { a 1 ,...,a k } and p i is the fraction of tuples in v that have value a i on attribute A . An n  X  n probability matrix  X  reflects the information transfer between tables. Let v i , v j present two tables, q A denote the total number of join edges involving attribute v i .A .

The importance vector I denotes the stationary distribution of the random walk defined by the probability matrix  X  . It can be computed by the iterative approach until the stationary distribution is reached.

We tag the k most important tables as the labeled data in labeled schema graph G L . In our approach, we regard the k labeled tables as the t ( c )( c  X  C ) for k categories C = { C 1 ,C 2 ,  X  X  X  ,C k } , which means the clustered category will center on the most important tables and the summarized summary will present important schema elements. 3.3 Label Propagation Label propagation algorithm starts with a labeled schema graph G L ,whichaims to estimate the label of unlabeled data in G L . Each node can be reconstructed from its neighborhood. This process will it erate until convergence is reached, and all the tables are labeled. The label of a table propagates to other tables through the edges for each iteration. The larger edge weights, the easier label propagates. Meanwhile, we fix the labels on the labeled data to make labeled tables act like sources that push out labels through unlabeled tables.
We define a n  X  n probabilistic transition matrix T (by probability matrix we mean a matrix of non-negative numbers so that each row sums up to 1) as follows.
 define a n  X  k label matrix Y ,whose i -th row representing the label probabilities of table v i . The final label distribution in Y does not depend on the initial values, which means the initialization of them is not important. The initialization independence was proved in [14].
 The label propagation algorithm is as follows.
 Algorithm 1. Label Propagation
Step 5 is critical, we clamp the category distributions of labeled tables to avoid the labeled source fade away, so the probability mass is concentrated on the given category. The intuition is that, any possible classification of unlabeled data should not influence the data that we have known their labels exactly. Step 2-6 are the iteration process to propagate labels until Y is convergent. Here,  X  X onvergence X  means that the predicted labels of the data will not change in several successive iterations.

As shown in Algorithm 2, SSLP proceeds a s follows. First, get the weight on each edge by the function of the table similarity. Next, choose the top-k ones as the labeled tables ranking by t he table importance. Finally, use the label propagation algorithm to extend the labeled set consisting of tables for summarization based on the labeled schema graph. Tables are then grouped into categories according to their labels and displayed in the form of a partition. Algorithm 2. Schema Summarization In this section, we firstly introduce our experimental settings, including the datasets and accuracy evaluation. Then we conducted a set of extensive ex-perimental study to compare our SSLP approach against recent proposals on schema summarization with the same accuracy evaluation. 4.1 Experimental Setups Datasets: We evaluate our schema summarization methods over TPCE bench-mark dataset. TPCE is a benchmark database portraying a brokerage firm with customers who generate transactions related to trades, account inquiries, and market research. The brokerage firm in turns interacts with financial markets to execute orders on behalf of the customers and updates relevant account informa-tion. The table classification is provided as part of the benchmark. That is the reason why we use this dataset. It makes convenient to compare the generated summaries with the pre-defined table classification.

TPCE has 33 tables. However, since no active transactions are considered, table TRADE REQUEST is empty. Therefore, our experiment is performed only on the remaining 32 tables. The TPCE database tables are pre-grouped into four categories: Customer , Broker , Market , Dimension . Thus, we are interested in discovering the four categories.
 Evaluation Metric: We use the accuracy model mentioned in [6] to compare the performance of the SSLP approach and other approaches. It assumes that a proper clustering model should be one such that tables within the same category are more similar to each other than tables in different categories. It measures how many tables are categorized correctly as follows.

For each category C i , t ( C i ) determines the topical table. Let m ( C i )denotethe number of tables in the category C i that belongs to the same category as t ( C i ), in the pre-defined labeling. Th en the accuracy of a summary C = { C 1 ,C 2 ,  X  X  X  ,C k } is where n is the total number of database tables. 4.2 Parameter Learning There is a kernel parameter  X  in the table similarity kernel function. The op-timization parameter will make the function have the best performance. When  X   X  0, the weight on every edge is close to 0, which means each point belongs to a separate category. When  X   X  X  X  , the weight on every edge is close to 1, which means the whole dataset shrinks to a single point. Both of the limiting cases totally overlook the real similarity relationship because of the unbefitting parameter.
The usual parameter learning criterion is to maximize the probability of the labeled data. However, in our approach the labeled data are fixed, so data label probability does not make sense as a criterion in our setting, especially with very few labeled data. Intuitively the quality of the solution depends on how unlabeled data assigned labels, we use average label entropy as the heuristic criterion for parameter learning. The average label entropy H is defined as where H is the sum of the entropy on unlabeled data.
 Fig. 2 shows that H has a minimum 0 at  X   X  0, but it is not always desirable. Thiscanbefixedbysmoothing T .Wesmooth T with a uniform transition matrix U ,where U ij =1 /n .
 T is then used in place of T in the SSLP approach. Fig. 2 shows H vs.  X  before and after smoothing with different  X  values. With the smoothing, the nuisance minimum at 0 gradually disappears as the smoothing factor  X  grows as shown in Fig. 2. When we set  X  =0 . 05, the minimum entropy is 33 . 417 at  X  =0 . 3. In the following, we use the value  X  =0 . 05 and  X  =0 . 3. Although we have to add one more parameter  X  to learn  X  , the advantage will be apparent if we introduce multiple parameter in the future work. In the table similarity kernel function, distance betw een tables is computed with  X  parameter vector. We propose three features to measure the table similarity, Name Similarity , Value Similarity and Cardinality Similarity . The distance between table v i and table v j can be rewritten as follows. We estimate the unknown parameters by multivariable linear regression model. The experiment result shows that  X  1 =6 . 3877,  X  2 =4 . 8351,  X  3 =2 . 0534 and b =0 . 7918. In the following experiments, we use them in the table similarity kernel function. 4.3 Table Similarity Kernel Function In this section, we evaluate the table similarity model compared with the name similarity [11], value similarity [11] and cardinality similarity [6]. To simplify the problem, we first tie the other two dimensions, table importance and label propagation algorithm.
 Fig. 3 plots the accuracy for the four similarity functions mentioned above. In each case, the SSLP similarity model has the highest accuracy. For k =4,the accuracy of SSLP similarity model reaches 82 . 14%, which is nearly 50% higher than other models. Clearly, SSLP similarity model performs better. Thus, the ta-ble similarity kernel function emphasizes to measure various similarity properties comprehensively and guarantee to cluster the most similar tables correctly. No-tice that the accuracy remains the same when k =3and k = 4, which is related with the fixed dimension, label propagation algorithm. The detailed analysis is shown in Section 4.4. 4.4 Label Propagation Algorithm In this section, we compare Weighted k -Center( WKC ), Hierarchical Cluster-ing Algorithm( HCA ), and Label Propagation Algorithm( LPA ), by fixing table importance dimension and table similarity dimension.
 Weighted k -Center: It is an approximation algorithm for the NP-hard K-means problem. It starts by creating one cluster and assigning all tables to it. It then iteratively chooses the table whose w eighted distance from its cluster center is largest and creates a new cluster with that table as its center. However, if a newly chosen cluster center is isolated in the graph, clusters are unbalanced. For TPCE, when k = 2, the first cluster has 27 tables, whose center is TRADE (has the highest table importance), and the second cluster contains only 5 tables, whose center is CUSTOMER (has the minimum similarity with TRADE and the maximum table importance among the left tables). When k = 3, the new cluster whose center is FINANCIAL includes only two tables. For k =4ormore, it follows a similar trend. Although the accuracy is increasing which is shown in Fig. 4, the unbalanced clustering result shows that Weighted k -Center does not work very well over TPCE.
 Hierarchical Clustering Algorithm: It builds a hierarchy from the individ-ual elements by progressively merging clusters, mentioned in [7]. Each obser-vation starts in its own cluster, and only merges two elements as one moves up the hierarchy, which makes it too slow for large data sets. Moreover, hier-archical clustering algorithm does not have a redistributive capacity, which will affect the accuracy of the cluster ing result. For TPCE, table SECURITY and table WATCH ITEM are merged into a cluster at some move, however, they actually belong to different categories. This merge leads a set of tables that are similar with WATCH ITEM clustered into SECURITY group, which affects the accuracy greatly. Fig. 4 plots the a ccuracy for the methods above, as well as the alternative Label Propagation. Clearly, Label Propagation based semi-supervised learning performs much better than Weighted k -Center algorithm and hierarchical clustering algorithm. Notice that when k = 4, the accuracy reaches the maximum. For k  X  5, the accuracy gradually decreases, which we do not show due to space constraints. Thus, it gives a clear signal that there are only 4 categories in this dataset, and it is meaningless to compute categories for k&gt; 4. 4.5 The SSLP Approach In this section, we evaluate the effectiv eness of schema summarization approach. In our experiment, we also study two alternative schema summarization solution. The CWKC approach defined in [6] requires Cardinality Similarity Model to compute the table similarity and Weighted k -Center Algorithm to summarize the schema summary, as proposed in previous work. The VHCA approach defined in [7] requires the value similarity measure to compute the affinity along tables, and use Hierarchical Clustering Algorithm as the schema clustering algorithm. In Fig. 5, we plot the accuracy value for these approaches. As mentioned in Section 4.4, it is meaningless to compute k&gt; 4 clusters. The graph clearly shows that the most accurate summaries are obtained for the SSLP approach. It is expected because the SSLP approach outperforms others on whatever dimension. It further indicates the effectiveness of SSLP. We introduced the problem of retrieving information from complex schema of modern database. In this paper, we propose a new approach for generating schema summarization automatically. The SSLP approach is unique in that we use a new kernel function to measure the table similarity by considering several relevant features comprehensively. Based on the table similarity, we proposed to exploit label propagation algorithm to compute high quality schema sum-maries automatically. An e xperimental assessment of our summaries shows that our approach can find good summaries for a given database and outperform the existing approaches significantly. We believe a set of categories summarized is a really valuable means to help users understand complex databases.
 Acknowledgments. This work is supported by National Natural Science Foun-dation of China under Grant No. 61170184, an d Tianjin Municipal Science and Technology Commission under Grant No. 13ZCZDGX02200, 13ZCZDGX01098 and 14JCQNJC00200.

