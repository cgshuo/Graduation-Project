 Hidden Markov SVMs are a recently-proposed method for predicting a label sequence given the input sequence [3, 17, 18, 1, 2]. They combine the benefits of the power and flexibility of kernel methods with the idea of Hidden Markov Models (HMM) [11] to predict label sequences. In this work we introduce a generalization of Hidden Markov SVMs, called Hidden Semi-Markov SVMs (HSM SVMs). In HM SVMs and HMMs there is a state transition for every input symbol. In semi-into a new state. During this segment of time the system X  X  behavior is allowed to be non-Markovian. may depend on the start and end of the segment.
 One of the largest problems with HM SVMs and also SHM SVMs is their high computational complexity. Solving the resulting optimization problems may become computationally infeasible already for a few hundred examples. In the second part of the paper we consider the case of using content sensors (for whole segments) and signal detectors (at segment boundaries) in SHM SVMs. We motivate a simple, but very effective strategy of partitioning the problem into independent sub-problems and discuss how one can reunion the different parts. We propose to solve a relatively small optimization problem that can be solved rather efficiently. This strategy allows us to tackle significantly larger label sequence problems (with several thousands of sequences).
 To illustrate the strength of our approach we have applied our algorithm to an important problem in computational biology: the prediction of the segmentation of a pre-mRNA sequence into exons and introns. On problems derived from sequences of the model organism Caenorhabditis elegans we can show that the SHM SVM approach consistently outperforms HMM based approaches by a large margin (see also [13]).
 The paper is organized as follows: In Section 2 we introduce the necessary notation, HM SVMs and the extension to semi-Markov models. In Section 3 we propose and discuss a technique that allows us to train SHM SVMs on significantly more training examples. Finally, in Section 4 we outline the gene structure prediction problem, discuss additional techniques to apply SHM SVMs to this problem and show surprisingly large improvements compared to state-of-the-art methods. determine a discriminant function F : X  X  Y  X  R that assigns a score to every input x  X  X := X  X  and every label sequence y  X  Y :=  X   X  , where X  X  denotes the Kleene closure of X . In order to obtain a prediction f ( x )  X  Y , the function is maximized with respect to the second argument: 2.1 Representation &amp; Optimization Problem In Hidden Markov SVMs (HM SVMs) [3], the function F ( x , y ) :=  X  w ,  X ( x , y )  X  is linearly parametrized by a weight vector w , where  X ( x , y ) is some mapping into a feature space F . Given labeling y n scores higher than all other labelings y  X  Y n := Y \ y n with a large margin, i.e. F ( x n , y n ) argmax y  X  X  mization problem (appeared equivalently in [3]): constraints: F ( x n , y n )  X  max y  X  X  n F ( x n , y )  X  1  X   X  n for n = 1 , . . . , N [3]. If P ( w ) = k w k 2 , it can be shown that the solution w  X  of (2) can be written as F ( x , y ) as 2.2 Outline of an Optimization Algorithm The number of constraints in (2) can be very large, which may constitute challenges for efficiently solving problem (2). Fortunately, only a few of the constraints usually are active and working set methods can be applied in order to solve the problem for larger number of examples. The idea is to smaller problem and then identifies labelings y  X  Y n that maximally violate constraints, i.e. where w is the intermediate solution of the restricted problem. The new constraint generated by the negative labeling is then added to the optimization problem. The method described above is also known as column generation method or cutting-plane algorithm and can be shown to converge to the optimal solution w  X  [18]. However, since the computation of F involves many kernel computations labeled sequences often seems computationally too expensive. 2.3 Viterbi-like Decoding of the sequence and decomposed as vectors, i.e. (  X  &gt;  X 
F ( x , y ) = X Thus we have positionally decomposed the function F . The score at position i + 1 only depends on x , i and labels at positions i and i + 1 (Markov property).
 Using this decomposition we can define as the maximal score for all labelings with label  X  at position i . Via dynamic programming one can using backtracking one can recover the optimal label sequence. 3 involves computing potentially large sums of kernel functions, the decoding step can be computa-tionally quite demanding X  X epending on the kernels and the number of examples. 2.4 Extension to Hidden Semi-Markov SVMs Semi-Markov models extend hidden Markov models by allowing each state to persist for a non-unit number  X  i of symbols. Only after that the system will transition to a new state, which only be non-Markovian [14]. Semi-Markov models are fairly common in certain applications of statistics [6, 7] and are also used in reinforcement learning [16]. Moreover, [15, 9] previously proposed an extension of HMMs, called Generalized HMMs (GHMMs) that is very similar to the ideas above. Also, [14] proposed a semi-Markov extension to Conditional Random Fields.
 In this work we extend Hidden Markov-SVMs to Hidden Semi-Markov SVMs by considering se-quences of segments instead of simple label sequences. We need to extend the definition of the generalize the mapping  X  to: With this definition we can extract features from segments: As  X  j and  X  j +1 are given one can for the segment. Decomposing F results in: Analogously we can extend the formula for the Viterbi-like decoding algorithm [14]: can be obtained as before by backtracking. Also the above method can be easily extended to produce the K best labelings (cf. Footnote 3). 3.1 Preliminaries for extracting content information about segment j . Also, for considering signals we assume it to be sufficient to consider a window  X   X  around the end of the segment, i.e. we only consider  X  start of the segment. Moreover, we assume for simplicity that x  X   X   X  is appropriately defined for every  X  = 1 , . . . , l ( x ) . We may therefore define the following feature map: where [[ true ]] = 1 and 0 otherwise. Then the kernel between two examples using this feature map can be written as: k (( x , y ) , ( x 0 , y 0 )) = X rewriting F ( x , y ) where and signals. 3.2 Two-Stage Learning By enumerating all non-zero  X   X  X  and valid settings of j 0 in F  X  and F  X , X  , we can define sets of and  X  n  X  appropriately chosen  X   X  X . For sequences  X   X  m that do not correspond to true segment boundaries, the coefficient  X   X  m is either negative or zero (since wrong segment boundaries can only appear in SVM classification functions recognizing segments and boundaries of all kinds.
 Hidden Semi-Markov SVMs simultaneously optimize all these functions and also determine the relative importance of the different signals and sensors. In this work we propose to separate the learning of the content sensors and signal detectors from learning how they have to act together using the kernels k c and k s on examples with known labeling. For every segment type and seg-ment boundary we generate a set of positive examples from observed segments and boundaries. As negative examples we use all boundaries and segments that were not observed in a true labeling. estimated independently.
 The advantage of this approach is that solving two-class problems X  X or which we can reuse existing large scale learning methods X  X s much easier than solving the full HSM SVM problem. However, functions are obtained independently from each other and might not be scaled correctly to jointly produce the correct labeling. We therefore propose to learn transformations t  X , X  and t  X  such that F are one-dimensional mappings and it seems fully sufficient to use for instance piece-wise linear case as where we simply replaced the feature with PLiF features based on the outcomes of precomputed support points used in the PLiFs.
 long labeled sequences. The problem of gene structure prediction is to segment nucleotide sequences (so-called pre-mRNA process called splicing the introns are removed from the pre-mRNA sequence to form the mature mRNA sequence that can be translated into protein. The exon-intron and intron-exon boundaries are defined by sequence motifs almost always containing the letters GT and AG (cf. Figure 4), re-spectively. However, these dimers appear very frequently and one needs sophisticated methods to recognize true splice sites [21, 12, 13].
 So far mostly HMM-based methods such as Genscan [5], Snap [8] or ExonHunter [4] have been that our newly developed method is applicable to this task and achieves very competitive results. We call it mSplicer . Figure 2 illustrates the  X  X rammar X  that we use for gene structure prediction. between these states defines an intron.
 For our specific problem we only need signal detectors for segments ending in state two and three. same, the length of them can vary quite drastically. We therefore decided to use one content sensor  X  F
I for the intron transition 2  X  3 and the same content sensor we include in the feature map (7), which amounts to using PLiFs for the lengths of all transitions. Also, note 4  X  1 ; cf. Figure 2). 5 We have obtained data for training, validation and testing from public sequence databases (see [13] Set 3 is used for model selection of the HSM SVM; and Set 4 is used for the final evaluation. These are large scale datasets, with which current Hidden-Markov-SVMs are unable to deal with: The C. elegans training set used for label-sequence learning contains 1,536 sequences with an average then the predictions of  X  F  X , X  and  X  F  X  on the sequences used for the HSM SVM are skewed in the margin area (since the examples are pushed away from the decision boundary on the training set). We therefore keep the two sets separated. 4.1 Learning the Splice Site Signal Detectors an exon or intron of the sequence and have AG or GT consensus. We train an SVM [19] with soft-and  X  j := d  X  j + 1 . We used a normalization of the kernel  X  k ( x , x 0 ) = k ( x , x 0 )  X  have been tuned on the validation set (Set 2). SVM training for C. elegans resulted in 79,000 and 61,233 support vectors for detecting intron start and end sites, respectively. 4.2 Learning the Exon and Intron Content Sensors To obtain the exon content sensor we derived a set of exons from the training set. As negative examples we used sub-sequences of intronic sequences sampled such that both sets of strings have roughly the same length distribution. We trained SVMs using a variant of the Spectrum kernel [21] of degree d = 6 , where we count 6 -mers appearing at least once in both sequences. We applied the same normalization as in Sec. 4.1 and proceeded analogously for the intron content sensor. The model parameters have been obtained by tuning them on the validation set.
 Viterbi-like algorithm (cf. (6)): One needs to extend segments ending at the same position i to made drastically faster. 4.3 Combination  X  F
I uniformly between  X  5 and 5 (typical range of outputs of our SVMs). For the PLiFs concerned with length of segments we chose appropriate boundaries in the range 30  X  1000 . With all these with a total of 270 parameters.
 Finally, we have modified the regularizer for our particular case, which favors smooth PLiFs: content sensors to be monotonically increasing. 6 Having defined the feature map and the regularizer, we can now apply the HSM SVM algorithm outlined in Sections 2.4 and 3. Since the feature space is rather low dimensional (270 dimensions), we can solve the optimization problem in the primal domain even with several thousands of examples employing a standard optimizer (we used ILOG CPLEX and column generation) within a reasonable time. 7 4.4 Results To estimate the out-of-sample accuracy, we apply our method to the independent test dataset 4. For C. elegans we can compare it to ExonHunter 8 on 1177 test sequences. We greatly outperform the ExonHunter method: our method obtains almost 1/3 of the test error of ExonHunter (cf. Table 1). Simplifying the problem by only considering sequences between the start and stop codons allows us to also include SNAP in the comparison on the dataset 4 X , a slightly modified version of dataset 4 with 1138 sequences. 9 The results are shown in Table 1. On dataset 4 X  the best competing method achieves an error rate of 9.8% which is more than twice the error rate of our method. We have extended the framework of Hidden Markov SVMs to Hidden Semi-Markov SVMs and suggested an very efficient two-stage learning algorithm to train an approximation to Hidden Semi-Markov SVMs. Moreover, we have successfully applied our method on large scale gene structure prediction appearing in computational biology, where our method obtains less than a half of the error rate of the best competing HMM-based method. Our predictions are available at Wormbase: http://www.wormbase.org . Additional data and results are available at the project X  X  website http://www.fml.mpg.de/raetsch/projects/msplicer .
 Acknowledgments We thank K.-R. M  X  uller, B. Sch  X  olkopf, E. Georgii, A. Zien, G. Schweikert and Moreover, we thank D. Surendran for naming the piece-wise linear functions PLiF and optimizing the Viterbi-implementation.

