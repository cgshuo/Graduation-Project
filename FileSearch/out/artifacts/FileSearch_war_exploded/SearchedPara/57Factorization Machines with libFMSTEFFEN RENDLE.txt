 telligent information systems and machine learning. They have shown excellent prediction capabilities in several important applications, for example, recommender systems. The most well studied factorization model is matrix factorization [Srebro and Jaakkola 2003], which allows us to predict the relation between two categorical variables. Tensor factorization models are an extension for relations over several categorical variables; among the proposed t ensor factorization approaches are Tucker Decomposition [Tucker 1966], Parallel Factor Analysis [Harshman 1970], or Pairwise Interaction Tensor factorization [Rendle and Schmidt-Thieme 2010]. For specific tasks, specialized factorization models have been proposed that take noncategorical variables into account, for example, SVD++ [Koren 2008], STE [Ma et al. 2011], FPMC [Rendle et al. 2010] (for a set categorical variable), timeSVD++ [Koren 2009b], and BPTF [Xiong et al. 2010] (for an additional numerical variable). For the basic matrix factorization model, many learning and inference approaches have been studied X  et al. 2010]), variational Bayes [Lim and Teh 2007], and Markov Chain Monto Carlo (MCMC) inference [Salakhutdinov and Mnih 2008a]. However, for more complex factorization models, only the most simple learning method of gradient descent is mostly available.

Even though factorization models have a high prediction quality in many applica-tions, it is nontrivial to work with them. For each problem that cannot be described with categorical variables, a new specialized model has to be derived, and a learning algorithm has to be developed and implemented. This is very time-consuming, error-prone, and only applicable for experts in factorization models.

On the other hand, in practice, the typical approach in machine learning is to de-scribe data with feature vectors (a preprocessing step aka feature engineering) and to apply a standard tool for example, LIBSVM [Chang and Lin 2011] for support vector machines, a toolbox such as Weka [Hall et al. 2009], or a simple linear regression tool. This approach is easy and applicable even for users without in-depth knowledge about the underlying machine-learning models and inference mechanisms.

In this article, factorization machines (FM) [Rendle 2010] are presented. FMs com-bine the high-prediction accuracy of factorization models with the flexibility of feature engineering. The input data for FMs is described with real-valued features, exactly like in other machine-learning approaches such as linear regression, support vector machines, etc. However, the internal model of FMs uses factorized interactions be-tween variables, and thus, it shares with other factorization models the high predic-tion quality in sparse settings, like in recommender systems. It has been shown that FMs can mimic most factorization models just by feature engineering [Rendle 2010]. This article summarizes the recent research on FMs, including learning algorithms based on stochastic gradient descent, alternating least-squares, and Bayesian infer-ence using MCMC. FMs and all presented algorithms are available in the publicly available software tool LIB FM. With LIB FM, applying factorization models is as easy as applying standard tools, such as SVMs or linear regression.

The article is structured as follows: (1) the FM model and its learning algorithms that are available in LIB FM are introduced; (2) several examples, for input data are given, and the relation to specialized factorization models is shown; (3) the LIB FM software is briefly introduced; and (4) experiments are conducted. Let us assume that the data of a prediction problem is described by a design matrix X  X  R n  X  p ,wherethe i th row x i  X  R p of X describes one case with p real-valued variables and where y i is the prediction target of the i th case (see Figure 1 for an ex-ample). Alternatively, one can describe this setting as a set S of tuples ( x , y ), where (again) x  X  R p is a feature vector and y is its corresponding target. Such a represen-tation with data matrices and feature vectors is common in many machine-learning approaches, for example, in linear regression or support vector machines (SVM).
Factorization machines (FM) [Rendle 2010] model all nested interactions up to order d between the p input variables in x using factorized interaction parameters. The factorization machine (FM) model of order d = 2 is defined as where k is the dimensionality of the factorization and the model parameters = { w The first part of the FM model contains the unary interactions of each input variable x with the target X  X xactly as in a linear regression model. The second part with the two nested sums contains all pairwise interactions of input variables, that is, x j x j . The important difference to standard polynomial regression is that the effect of the interaction is not modeled by an independent parameter w j , j but with a factorized that the effect of pairwise interactions has a low rank. This allows FMs to estimate re-liable parameters even in highly sparse data where standard models fail. The relation of FMs to standard machine-learning models is discussed in more detail in Section 4.3.
In Section 4, it will also be shown how FMs can mimic other well known factoriza-tion models, including matrix factorization, SVD++, FPMC, timeSVD, etc.
 Complexity. Let N z be the number of nonzero elements in a matrix X or vector x . where  X  is the indicator function
The FM model in Equation (1) can be computed in O ( kN z ( x )) because it is equivalent [Rendle 2010] to The number of model parameters | | of an FM is 1 + p + kp and thus linear in the number of predictor variables (= size of the input feature vector) and linear in the size of the factorization k .

Multilinearity. An appealing property of FMs is multilinearity, that is, for each model parameter  X   X  , the FM is a linear combination of two functions g  X  and h  X  that are independent of the value of  X  [Rendle et al. 2011]. with The definition of g  X  is omitted because in the following it is never used directly. If its
Expressiveness. The FM model can express any pairwise interaction, provided that k is chosen large enough. This follows from the fact that any symmetric positive semidef-inite matrix W can be decomposed into VV t (e.g., Cholesky decomposition). Let W be any pairwise interaction matrix that should express the interactions between two dis-tinct variables in an FM. W is symmetric, and as the FM does not use the diagonal elements (because j &gt; j in Eq. (1)), any value X  X nd especially also arbitrary large values X  X or the diagonal elements are possible, which will make W positive semidefi-nite.

Please note that this is a theoretical statement about the expressiveness. In prac-tice, k p , because the advantage of FMs is the possibility to use a low-rank ap-proximation of W , and thus, FMs can estimate interaction parameters even in highly sparse data X  X ee Section 4.3 for a comparison to polynomial regression which uses the full matrix W for modeling interactions.

Higher-Order FMs. The FM model of order d = 2 (Eq. (1)) can be extended by fac-torizing ternary and higher-order variable interactions. The higher-order FM model [Rendle 2010] reads with model parameters Also for higher-order interactions, the nested sums in Eq. (8) can be decomposed for a more efficient computation. In the remainder of this article, we will deal only with second order FMs because in sparse settings X  X here factorization models are espe-cially appealing X  X ypically higher-order interactions are hard to estimate [Rendle and Schmidt-Thieme 2010]. Nevertheless, most formulas and algorithms can be transfered directly to higher-order FMs, as they share the property of multilinearity with second-order FMs. Three learning methods have been proposed for FMs: stochastic gradient descent (SGD) [Rendle 2010], alternating least-squares (ALS) [Rendle et al. 2011], and Markov Chain Monte Carlo (MCMC) inference [Freudenthaler et al. 2011]. All three of them are available in LIB FM.
 Optimality of model parameters is usually defined with a loss function l where the task is to minimize the sum of losses over the observed data S .
 Note that we add the model parameters to the model equation and write  X  y ( x | ) when we want to stress that  X  y depends on a certain choice of . Depending on the task, a loss function can be chosen. For example, for regression least-squares loss, or for binary classification ( y  X  X  X  1 , 1 } ), where  X  ( x )= 1 1+ e  X  x is the sigmoid/ logistic function.

FMs usually have a large number of model parameters  X  X specially if k is chosen large enough. This makes them prone to overfitting. To overcome this, typically L2 regularization is applied, which can be motivated by maximum-margin [Srebro et al. 2005] or Tikhonov regularization.
 where  X   X   X  R + is the regularization value for the model parameter  X  .Itmakessenseto use individual regularization parameters for different parts of the model. In LIB FM, model parameters can be grouped X  X or example, one group for the parameters describ-ing the users, one group for the items, one for time, etc. (see Figure 1 for an example of groups) X  X nd each group uses an independent regularization value. Moreover, each factorization layer f  X  X  1 ,..., k } as well as the unary regression coefficients w and w 0 can have an individual regularization (again with groups). In total, the regularization structure of LIB FM is where  X  : { 1 ,..., p } X  X  1 ,..., } is a grouping of model parameters. That means, for example, the regularization value for v l , f would be  X  v f , X  ( l ) .

Probabilistic Interpretation. Both loss and regularization can also be motivated from a probabilistic point of view (e.g., Salakhutdinov and Mnih [2008b]). The least-squares loss corresponds to the assumption that the target y is Gaussian distributed with the prediction as mean For binary classification, a Bernoulli distribution is assumed. where b : R  X  [0 , 1] is a link function, typically the logistic function  X  or the cumula-tive distribution function (CDF) o f a standard normal distribution ( ).

L2 regularization corresponds to Gau ssian priors on the model parameters. The prior mean  X   X  should be grouped and organized the same way as the regulariza-tion values  X   X  (see Eq. (14)).

The graphical model for the probabilistic view can be seen in Figure 2(a). The max-imum a posteriori (MAP) estimator for this model (with  X  =1,  X   X  =0)isthesameas the optimization criterion of Eq. (13).

Gradients. For direct optimization of the loss functions, the derivatives are for least-squares regression, or for classification, Finally, due to multilinearity of the FM model, the partial derivative of the model equation with respect to  X  corresponds to h  X  (Eq. (7)). Stochastic gradient descent (SGD) algorithms are very popular for optimizing factor-ization models as they are simple, work well with different loss functions, and have low computational and storage complexity. Algorithm 1 shows how FMs can be optimized with SGD [Rendle 2010]. The algorithm iterates over cases ( x , y )  X  S and performs updates on the model parameters. where  X   X  R + is the learning rate or step size for gradient descent.

Complexity. The SGD algorithm for FMs has a linear computational and constant storage complexity. For one iteration over all training cases, the runtime complexity of SGD is O ( kN z ( X )) because, for each single case ( x , y )  X  S , the complexity for the gradient steps is O ( k p i =1  X  ( x i =0))= O ( kN z ( x )).

Hyperparameters. For performing SGD, there are several critical hyperparameters.  X  Learning rate  X  : The convergence of SGD depends largely on  X  :if  X  is chosen too high, the algorithm does not converge, and if it is chosen too small, convergence is slow. Typically,  X  is the first hyperparameter that should be determined.  X  Regularization  X  : As noted in Section 3.1, the generalization capabilities of FMs and thus the prediction quality, depends largely on the choice of the regularization  X  . The regularization values are typically searched on a separate holdout set, for example, using grid search. As there are several regularization parameters (see Eq. (14)), the grid has exponential size and thus this search is very time-consuming.
To make the search more feasible, the number of regularization parameters is usu-ally reduced, for example, groups are dropped and all factor layers use the same regularization value.  X  Initialization  X  : The parameters for the factorized interactions ( V )havetobeinitial-ized with nonconstant values. In LIB FM, the values are sampled from a zero-mean normal distribution with standard deviation  X  . Typically small values are used for  X  .

SGD with Adaptive Regularization. In Rendle [2012], it has been shown how the regular-ization values can be adapted automatically in SGD, while the model parameters are learned. LIB FM includes the adaptive regularization algorithm proposed there and extends it with groups.
 The optimization approach of SGD is based on iterating over cases (rows) of the train-ing data and performing small steps in the direction of a smaller loss. Coordinate descent or alternating least-squares (ALS) takes another approach by minimizing the loss per model parameter. For least-squares regression with L2-regularization, the op-timal value  X   X  for one model parameter  X  given all remaining parameters \{  X  } can be calculated directly [Rendle et al. 2011] as where e i is the  X  X rror X  term/ residual of the i th case. This allows us to derive a least-squares learning algorithm (see Algorithm 2) that it-eratively solves a least-squares problem per model parameter and updates each model parameter with the optimal (local) solution, This is performed iteratively over all parameters until convergence.

Complexity. The main effort of ALS learning (Eq. (22)) is computing the following two quantities. With a trivial implementation, updating one model parameter would require comput-the corresponding column j is nonzero ( x i , j = 0). For example, for updating the model this would have to be computed for each of the 1 + p ( k + 1) model parameters.
In Rendle et al. [2011], it has been shown how one full iteration over all model parameters can be done efficiently in O ( N z ( X ) k ) by precomputing the caches e  X  R n (see Eq. (23)) and Q  X  R n  X  k such that which allows us to compute h quickly in O (1). cache value q and e can be done in constant extra time (see Rendle et al. [2011]).
However, the speed-up comes at the price of higher memory consumption for the caches. The approach presented in Rendle et al. [2011] has an additional memory complexity of O ( nk ) because of the Q-cache. LIB FM provides a more efficient imple-mentation with only O ( n ) memory complexity for the Q-cache (see Algorithm 2). The idea is that the model parameters are updated per layer f (i.e., first all parameters v Q of the same layer have to be present. This means that LIB FM stores (and updates) only the Q-cache for one layer (and thus the storage is O ( n )), and when changing the layer, the Q-values of the new layer are computed/initialized. The initialization of the Q-values per layer has no negative effect on the overall computational complexity.
Hyperparameters. A clear advantage of ALS over SGD is that ALS has no learn-ing rate as hyperparameter. However, two important hyperparameters remain: regularization and initialization. Finding good regularization values is especially computational-expensive.

Classification. The ALS algorithm described so far is restricted to least-squares re-gression and cannot solve classification tasks. LIB FM contains classification capabil-ities for ALS/coordinate descent, which is based on using a probit-link function. This approach is motivated from the probabilistic interpretation (Section 3.1) and will be described at the end of the MCMC section. The Bayesian model used so far can be seen in Figure 2. Both ALS and SGD learn the best parameters which are used for a point estimate of  X  y . MCMC is a Bayesian inference technique that generates the distribution of  X  y by sampling. For MCMC inference in FMs using Gibbs sampling, the conditional posterior distributions for each model parameter are [Freudenthaler et al. 2011] where and H are the hyperparameters When comparing the conditional posterior of model parameters for MCMC (Eq. (30))  X   X  =  X   X   X  with  X  =1and  X   X  = 0. The difference is that MCMC samples from the posterior distribution, while ALS uses the expected value.

A major advantage of MCMC over ALS and SGD is that it allows us to integrate the regularization parameters H into the model, which avoids a time-consuming search for these hyperparameters. For integration of H , the Bayesian FM model is extended (Figure 2) by placing distributions on the priors (hyperprior distributions). For each pair (  X   X  , X   X  )  X  H of prior parameters, a Gamma distribution is assumed for  X   X  and a normal distribution for  X   X  .Thatis, where  X  0 , X  0 ,aswellas  X   X  and  X   X  , describe the hyperprior distributions. Finally, a Gamma distribution is also placed on  X  . In total, the hyperpriors lead to the following new parameters 0 . MCMC allows us to integrate H into the inference process, that is, values for H are found automatically by sampling from the ir corresponding conditional posterior distributions [Freudenthaler et al. 2011]. with
Complexity. The Gibbs sampler for MCMC inference is sketched in Algorithm 3 and has the same complexity as the ALS algorithm. This follows directly from the ob-servation that for both algorithms, the same summations have to be computed for the conditional posterior distribution in MCMC and the expected value in ALS. The overhead of MCMC is the inference over the H , that is, computing the posteriors (Eqs. (35), (36), and (37)), but even with a straightforward implementation, this is in O ( kN z ( X )).

Hyperparameters. A major advantage of MCMC is that the regularization values H are automatically determined. This comes at the price of introducing parameters for hyperpriors 0 . However, (1) the number of hyperpriors | H | is smaller than the num-ber of regularization parameters | 0 | , and (2) more importantly, MCMC is typically insensitive to choices of 0 . That is, a trivial choice for the values of 0 works well. In LIB FM, the following trivial values for 0 are used:  X  0 =  X  0 =  X   X  =  X   X  =  X  0 =1and  X  0 =0.

The only hyperparameter that remains for MCMC is the initialization  X  .Ingen-eral here, one can even use a value of 0 (which is not possible for ALS and SGD), because MCMC X  X  posterior uncertainty will identify the factorization; however, choos-ing a proper value can speed up the sampler. One can usually see in the first few samples if an initialization  X  is a good choice.

Classification. The MCMC Algorithm 3 solves regression tasks. It can be extended for that defines the Bernoulli distribution for classification [Gelman et al. 2003]. That means, the MCMC algorithm will predict the probability that a case is of the positive class. LIB FM uses the CDF of a normal distribution as mapping, that is, b ( z )= ( z ), because the posteriors are then easy to sample from.

TheonlytwochangesthathavetobemadetoAlgorithm3forclassificationis (1) that for prediction,  X  y is transformed by , and (2) that instead of regressing to y , the regression target y is sampled in each iteration from its posterior that has a truncated normal distribution. Sampling from this distribution is efficient [Robert 1995].

As noted before ALS, for regression, can be seen as a simplification of MCMC where the model parameters are not sampled but their expectation value is taken in each update. A classification option for ALS is available in LIB FM that follows the same idea, and instead of sampling from the truncated normal (as it is done in MCMC), for classification with ALS, the expected value of the truncated normal is computed. An overview of the properties of the learning algorithms in LIB FM can be found in Table I.
 First, examples for input data are shown, including how they relate to other special-ized factorization models. Note that FMs are not restricted to the choices presented here. Second, other generic factorization models are compared to FMs. Third, FMs are compared to polynomial regression.
 In this section, the generality of FMs will be discussed by comparing them to other specialized state-of-the-art factorization models. This also shows how to apply FMs by defining the input data (i.e., features). It is crucial to note that, in practice, only the feature vector x has to be defined; the rest is done implicitly by the FM X  X either an explicit reformulation of the model equation nor developing of new prediction nor a learning algorithms is necessary. The analysis of the FM model equation that is done in this section is just to show the theoretical relations to other models. 4.1.1. Matrix Factorization. Assume data about two categorical variables U (e.g., a user) and I (e.g., an item) should be used in a FM. The straightforward way to describe a that is, where the u th entry in the first part of x is 1, the i th entry on the second part of x is 1, and the rest is 0 (e.g., see the first two groups of Figure 1). Using this data in an FM, the FM will be exactly the same as a (biased) matrix factorization model (FM) [Paterek 2007; Srebro et al. 2005]. 4.1.2. Pairwise Interaction Tensor Factorization. If three categorical variables should be described, for example, U , I ,and T (e.g., tags), a straightforward representation with
An FM using this data representation would be similar to the pairwise interaction tensor factorization model (PITF) [Rendle and Schmidt-Thieme 2010]. The difference between this FM and the original PITF is that this FM contains lower-order interactions and shares the factors V between interactions. Besides this, both approaches are identical. 4.1.3. SVD++ and FPMC. Assume that there are two categorical variables (e.g., U and I ) and one set-categorical variable (e.g., P ( L )). One simple representation of this data ( u , i , { l 1 ,..., l m } )  X  x =(0 ,..., 1 , 0 ,... where each of the m elements of the set { l 1 ,..., l m } is described by a nonzero value, for example, by 1 / m in the corresponding column (e.g ., see the first three groups of Figure 1). With this data, the FM will be equivalent to  X  y ( x )=  X  y ( u , i , { l 1 ,..., l m } )= the same as the SVD++ model [Koren 2008; Salakhutdinov and Mnih 2008b; Tak  X  acs et al. 2009]. The first part (annotated with SVD++ ) is exactly the same as the original SVD++ [Koren 2008]; the second part (second line in Eq. (45)) contains some additional interactions. If sequential information is used as input for { l 1 ,..., l m } ,theFMisvery similar to the Factorized Personalized Markov Chain (FPMC) [Rendle et al. 2010] X  especially if the FM is optimized for ranking (like FPMC) almost all terms that are in the FM model but not in the FPMC model will vanish (see [Rendle 2010] for details). If social information is used as input (e.g., friends), the FM is similar to the Social Trust Ensemble (STE) [Ma et al. 2011]. 4.1.4. BPTF and TimeSVD++. If time should be included, the most simple approach is to treat time as a categorical variable (e.g., each day is a level) and apply the same encoding as in Eq. (42). The FM with this data is similar to the time-aware BPTF model [Xiong et al. 2010]. The differences are that BPTF uses a ternary PARAFAC model over the three categorical variables (user, item, time), whereas FM uses fac-torized pairwise interactions. Moreover, BPTF has an additional regularizer over the time variables. In Freudenthaler et al. [2011], it has been shown that the FM works indeed better than the mo re complex BPTF model.

Another approach is to use a separate time variable per user (i.e., making the user-tors for the user, item, and a user-specific day indicator. With this data, the FM model would be equivalent to. exactly as in the TimeSVD model of Koren [2009b]. Extending the feature vectors of the FM with implicit feedback indicators (see Section 4.1.3) and a linear indicator for the time will result in the TimeSVD++ model (the time group of Figure 1 is an example for a linear time indicator).
 4.1.5. Nearest Neighbor Models. When other numerical measurements are available, for example, other ratings r 1 , r 2 ,..., thesameuserhasgiventoitems l 1 , l 2 ,...  X  I etc., this can be encoded in a feature vector x  X  R | I | + | I | . The FM model with this data would be equivalent to  X  y ( x )=  X  y ( i , { ( r 1 , l 1 ) ,..., ( r m , l m ) } )= w 0 + w i + This model is similar to the factorized nearest neighbor model [Koren 2010].
Another possible approach to encode the data would be to use separate rating indi-indicators in Eq. (47) would be in a separate block for each item. An FM of order d =1 with this data would be equivalent to This approach is identical to the nonfactorized nearest neighbor model of Koren [2008]. It can be combined with the implicit feedback idea which results in the KNN++ model [Koren 2008]. 4.1.6. Attribute-Aware Models. There are several attempts to integrate attribute infor-mation about users and items into recommender systems. It is very simple to use such information in an FM. A straightforward approach is to add the attributes of an item (or user), such as genre, actor, etc., to the input vector x .

Assume the input vector consists of such item attributes and an indicator variable for the user. With this data, the FM would be equivalent to  X  y ( x )=  X  y ( u , a i 1 ,..., a i m )= w 0 + w u + This is almost identical to the attribute-aware approach in Gantner et al. [2010] that uses a linear regression to map item attributes to factors (see the highlighted part  X  attribute mapping  X  of Eq. (51)) X  X he only difference is that the FM contains biases as well as additional interactions between item attributes (e.g., between genre and actors).

If the input vector of standard matrix factorization (Eq. (40)) is extended by at-tribute information of the user (e.g., demographics) and attributes of the item, the FM would correspond to the attribute-aware model proposed in Agarwal and Chen [2009]. Again, the difference is that the FM contains additional interactions within user and item attributes (e.g., interaction between a user X  X  age and gender). There are other attempts for a more generic factorization model. In Agarwal and Chen [2009], a matrix factorization model is extended with regression priors. That is, the mean of the normal distributed priors of factors is a linear regression model. FMs can mimic this approach because for any hierarchical model using normal distributed priors, the mean value of the prior (and thus also a linear regression model for the prior mean) can be added as covariates to the feature vectors. On the other hand, MF with regression priors is much more restricted than FMs, because MF itself is limited to interactions of two categorical variables, and thus, the MF model with regression priors is not appropriate for tasks with interactions over more than two variables, for example, tag recommend ation or context aware recommendation. FMs include (pairwise) interactions between any number of variables (also not limited to categorical ones).
 SVDfeature [Chen et al. 2011] is another generic factorization model. Similar to Agarwal and Chen [2009], in SVDfeature, a matrix factorization model is extended with linear regression terms for the factors and for the bias term. However, compared to FMs, it shares the same shortcomings of Agarwal and Chen [2009]: only interactions between two categorical variables can be factorized. That means it is not able to mimic state-of-the-art context-aware recommenders, tag recommenders, etc. Moreover, for SVDfeature, only SGD learning has been proposed, whereas LIB FM features MCMC inference, which is much more simple to apply, as there is no learning rate, and the regularization values are automatically determined. An advantage of SVDfeature over LIB FM is that due to the more restrictive model equation, it has an improved learning algorithm (following Koren [2008]) for speeding up learning in factor regression terms. In Rendle [2010] it has been shown that FMs can be seen as polynomial regression (or SVM with inhomogeneous polynomial kernel) using factorized parameter matrices. Polynomial regression of order d = 2 can be defined as with model parameters Comparing this to the FM model (Eq. (1)) one can see that FMs use a factorization for pairwise interactions, whereas polynomial regression uses an independent param-eter w j , j per pairwise interaction. This diffe rence is crucial for the success of FMs in sparse settings, for example, recommender systems or other prediction problems in-volving categorical variables of large domain. FMs can estimate pairwise interactions w , j for pairs ( j , j ) even if none or only little observation about the pair is present, because a low-rank assumption is made ( w j , j  X  v j , v j ), that is, it is assumed that the interaction of pair ( j , j )and( j , j  X  ) have something in common. In polynomial re-gression, both pairs are completely independent (a priori).
 LIB FM 1 is an implementation of factorization machines. It includes the SGD, ALS, and MCMC algorithms described in Section 3 3 for regression and classification tasks. FMs of order d = 2 are implemented. The input data format of LIB FM is the same as for SVM light [Joachims 1999] and LIB-SVM [Chang and Lin 2011]. For very large-scale data that does not fit into main mem-ory, LIB FM has a binary data format where only parts have to be kept in main memory. A converter from the standard text format to the binary data format is available. All major options are available over an easy-to-use command line interface. An exam-ple call for learning a dataset with MCMC inference would be ./libFM -method mcmc -task r -dim  X 1;1;8 X  -init_stdev 0.1 -iter 100 -test ml1m-test.libfm -train ml1m-train.libfm -out ml1m-test.pred, where dim specifies the factorization dimensions: 0/1 if w 0 should be included, 0/1 if w should be included, and k  X  N (here k = 8) for the dimensionality of V . init stdev is the standard deviation for initialization, that is,  X  of Algorithm 3. iter is the number of samples that are drawn. In the following, a few practical hints for applying LIB FM to a prediction problem are given. (1) For inexperienced users, it is advisable to use MCMC inference, as it is the most (2) When a predictive model for a new dataset is built, one should start with a low (4) After an appropriate init stdev has been determined, MCMC can be run with a For MCMC, no other hyperparameters have to be specified. Other methods (ALS and SGD) require more hyperparameters to tune (see Section 3 3). LIB FM also contains methods for optimizing an FM model with respect to ranking [Liu and Yang 2008] based on pairwise classification [Rendle et al. 2009]. Ranking is not available over the command line but can be embedded into existing software. An example for embedding LIB FM is provided in the software tool Tag Recommender (also available with source code). In Section 4.1, it was shown that FMs are able to mimic many factorization models. Now this will be substantiated by comparing the LIB FM implementation empirically to several well studied factorization models. The success will be measured by root mean-square error (RMSE) for regression tasks and F1-measure for ranking tasks (see Gunawardana and Shani [2009] for a summary of evaluation metrics for recommender systems). In recommender systems, the most well studied data set is the Netflix challenge 2 which includes about 100,000,000 ratings of about 480,000 users for 17,770 items. All our reported results are obtained on the Netflix quiz set (i.e., the same test set as on the public leaderboard from the Netflix challenge). 6.1.1. Matrix Factorization (MF). The most successful approaches on Netflix are based on matrix factorization (e.g., [Jahrer et al. 2010; Koren 2009a; Tak  X  acs et al. 2009]). For MF, many different variations and learning approaches have been proposed, for exam-ple, ALS [Pil  X  aszy et al. 2010], MCMC [Salakhutdinov and Mnih 2008a], Variational Bayes [Lim and Teh 2007; Stern et al. 2009], but mostly SGD variants (e.g., [Koren 2008; Salakhutdinov and Mnih 2008b]). Thus, even for the simple MF model, the pre-diction quality reported differ largely. We want to investigate how good the learning methods of LIB FM are by setting up an FM with MF indicators (see Eq. (40)), which is equivalent to biased MF. With this setting, all compared approaches share the same model but differ in learning algorithm and implementation.

Figure 3(a) shows a comparison of LIB FM (using SGD and MCMC 3 )totheSGD approaches of PMF [Salakhutdinov and Mnih 2008b] and the MF (SGD) approach of [Koren 2008], as well as the BPMF approach using MCMC inference [Salakhutdinov and Mnih 2008a]. For LIB FM with SGD, we use the regularization values (  X   X  =0 . 04) that have been reported in Koren [2008] for the related SVD++ model. It can be seen that the MCMC approaches have the lowest error, and the MCMC sampler of LIB FM outperforms the sampler of the BPMF model slightly. 6.1.2. Nearest Neighbor Models. Traditionally, nearest neighbor models have attracted a lot of research in the recommender system community (e.g., [Linden et al. 2003; Sarwar et al. 2001; Zheng and Xie 2011]). On the Netflix challenge, the best perform-ing neighborhood approaches are based on treating the similarities between items as model parameters which are learned, that is, KNN (Eq. (49)) [Koren 2008] and factor-ized KNN (Eq. (48)) [Koren 2010]. Again, we want to see how well LIB FM can mimic these models just by feature engineering. We set up the input data for LIB FM such that the FM model corresponds to the KNN and KNN++ (i.e., with additional implicit indicators) as described in Koren [2008]. We use the same pruning protocol to restrict to 256 neighbors, and for SGD, we use the same regularization (  X   X  =0 . 002) as re-ported in Koren [2008]. Figure 3(b) shows that LIB FM with MCMC and SGD achieves comparable quality to the approach of Koren [2008]. Secondly, LIB FM has been studied on the problem of context-aware recommendation [Rendle et al. 2011]. In context-aware recommendation, besides the user and item, there is other information about the rating event available, for example, the location of the user at the time of his rating, the mood, etc. As FMs can work with any number of features, they can be applied easily to this task. Figure 4(a) 4 shows a comparison of LIB FM using ALS and MCMC to the state-of-the art approach Multiverse Recom-mendation [Karatzoglou et al. 2010], which outperforms other context-aware methods, such as item splitting [Baltrunas and Ricci 2009] and the multidimensional approach of Adomavicius et al. [2005]. The last experiment shows the applicability of LIB FM to ranking. We compare LIB FM for the task of tag recommendation (e.g., Lipczak and Milios [2011]) using input data, as in Eq. (42). With this data, LIB FM mimics the PITF model [Rendle and Schmidt-Thieme 2010] which was the best performing approach 5 on Task 2 of the ECML/ PKDD Discovery Challenge 2009. Figure 4(b) shows a comparison of the prediction quality of LIB FM to PITF and the second to fourth best models of the Discovery Challenge: relational classification [Marinho et al. 2009] and the models of the third [Lipczak et al. 2009] and fourth place [Zhang et al. 2009]. Factorization machines (FM) combine the flexibility of feature engineering with fac-torization models. This article summarize s the recent research on FMs and presents three efficient inference methods based on SGD, ALS, and MCMC. Also extensions are presented, among them classification for MCMC and ALS, as well as grouping of variables.

The properties of FMs have been discussed both theoretically in terms of complexity, expressiveness, and relations to other factorization models, as well as with an empiri-cal evaluation. It has been shown that FMs can mimic several specialized factorization models X  X or certain, FMs are not restricted to these examples. Empirical results show that the prediction quality of the describe d inference algorithms for FMs is compara-ble to the best inference approaches for specialized models in the area of recommender systems. In total, that means that the generality of FMs does not come to the prize of a low prediction accuracy or a high computational complexity. All presented algorithms are implemented in the publicly available software tool LIB FM.

There are several directions for future work on FMs. First, due to the generality of FMs, they are supposed to be interesting fo r a wide variety of pre diction problems, especially problems involving categorical variables with large domains might benefit from FMs. Studying FMs using LIB FM on such problems is highly interesting. Second, the complexity of the inference methods for FMs could be reduced, because the algo-rithms proposed so far make no use of repeating patterns in the input data which could be exploited for an additional speed-up. Third, the software implementation LIB FM could be extended by higher-order interactions ( d  X  3).

