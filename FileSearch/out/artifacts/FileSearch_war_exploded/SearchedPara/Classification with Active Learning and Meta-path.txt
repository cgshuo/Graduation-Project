 A heterogeneous information network (HIN) is used to model ob-jects of di ff erent types and their relationships. Meta-paths are se-quences of object types. They are used to represent complex re-lationships between objects beyond what links in a homogeneous network capture. We study the problem of classifying objects in an HIN. We propose class-level meta-paths and study how they can be used to (1) build more accurate classifiers and (2) improve ac-tive learning in identifying objects for which training labels should be obtained. We show that class-level meta-paths and object classi-fication exhibit interesting synergy. Our experimental results show that the use of class-level meta-paths results in very e ff learning and good classification performance in HINs.
 H.2.8 [ Database Management ]: Database Applications X  Data Min-ing heterogeneous information network; classification; meta-path; ac-tive learning
Networks (or graphs) model real world entities and their relation-ships by objects and links 1 . Traditional networks are homogeneous in the sense that objects represent entities of the same type and links represent one type of relationship. For example, Figure 1(a) shows a network of authors (of research papers) with the co-authorship relationships represented by links in the network. [Heterogeneous Information Network (HIN)] Homogeneous networks, although popular, are limited in representing more com-erogeneous information network (HIN), on the other hand, is a net-work whose objects are of di ff erent types and whose links represent ably, and the terms  X  X dge X  and  X  X ink X  interchangeably.
 c di ff erent kinds of relationships. As an example, Figure 1(b) shows a simple bibliographic information network represented by an HIN. In the figure, we see five types of objects: author (A ), organiza-tion (O ), paper (P ), venue (V ) and tag (T ), as well as four types of links (or relationships): authorship (A X  X ), a (A X  X ), description (P X  X ) and publication (P X  X ). Comparing Fig-ures 1(a) and 1(b), we see that the HIN is a much richer source of information  X  not only can the co-authorship relationship (Fig-ure 1(a)) be derived, it also captures information that can hardly be represented by the simple network. For example, the HIN shows that the three co-authorship relationships among A1, A2 and A3 (as shown by the three links in Figure 1(a)) come from one co-authored paper (P1) instead of from three separate papers. A2 A3 A5 A6 
Figure 1: Homogeneous vs. heterogeneous information network [Meta-paths] An interesting feature of an HIN is that meta-paths tionships among objects in the network [21]. A meta-path is a sequence of object types that expresses a binary relationship be-tween two objects in an HIN. For example, the meta-path  X  X uthor-paper-author X  (denoted by APA) expresses the co-authorship rela-the co-authorship relationship if there is some paper, say P1, such that A1 and A2 are connected via a path A1-P1-A2 in the HIN. Sim-ilarly, the meta-path APVPA expresses a relationship between two authors who have published in the same publication venue. Meta-paths are a powerful way to abstract relationships on an HIN and there have been recent works on applying them to improve various data mining tasks. Meta-paths can be obtained either from domain experts or through an automatic extraction process [23]. [Classification] Classification is one of the most important and fundamental data mining tasks. Classifying objects in an HIN has found many interesting applications, especially those related to ob-ject recommendation, link prediction, and community detection. types, and we use numbered capital letters, e.g., A1, P3, to repre-sent objects of the corresponding types. For example, consider an HIN modeling the Facebook network, which consists of various object types such as users, messages, product pages, interest groups, advertisements and photos. If one can classify these objects into their topics of interest (e.g., music (rock, jazz, etc.), sports (football, tennis, etc.), shopping (fashion, electronics, etc.)) accurately, one can perform very e ff mendation (e.g., recommending a relevant product to a user), link prediction (e.g., measuring the likelihood that two users will be-come friends) and community detection (e.g., detecting a commu-nity of users with shared interest for target marketing). The prob-lem of classifying objects in an HIN has been studied [7, 8, 29]. However, many of these classifiers do not consider meta-paths. As we have argued, meta-paths are a powerful tool to express objects X  relationships. They are thus very useful in determining object cor-relation and therefore in object classification. A major objective of this paper is to derive an e ff ective HIN object classifier that uses meta-paths. Our classifier is based on two ideas: (1) we derive a learning model to weigh meta-paths, and (2) we propose the use of class-level meta-paths . Figure 2 helps us to illustrate these ideas in the following discussion. Figure 2: Illustrative examples of meta-path weighing ((a), (b)), class-level meta-paths ((c), (d)), active learning ((e), (f)), and path instances (g). Jumps are shown as dotted lines. [Weighing meta-paths] There could be many meta-paths de-rived for an HIN, especially when the meta-paths are automatically extracted (e.g., using the technique described in [23]). Some meta-paths may be better than others for a given classification task. For example, consider Figure 2(a), which shows a small bibliographic network. (For simplicity, we only show objects of types organi-zation (O), author (A) and paper (P).) In the figure, two objects A1 and A4 are shaded to denote that their class labels are already known. Let X  X  say their labels are and (which are the symbol-s shown next to the objects). Now consider two meta-paths APA and AOA. The former expresses co-authorship and the latter ex-presses co-workership. If we apply the meta-path APA to deduce object correlation then we will classify object A2 to the same class ( ) of A1 because A1 and A2 are connected by an instance of the meta-path APA. (This path instance is highlighted in Figure 2(a) with thick edges.) On the other hand, if we use the meta-path AOA instead, then object A2 will be given A4 X  X  class label ( ) because A2 and A4 are connected by an instance of AOA (path highlight-ed in Figure 2(b)). Which classification result is more likely to be correct depends on the classification task. For example, if the task is to label objects with a research area (say AI, DB), then the co-authorship meta-path APA would be more relevant than the co-workership meta-path AOA. (Two researchers from the same uni-versity can work in di ff erent areas.) This example illustrates that a mechanism for selecting meta-paths is important in constructing a classifier. As we will show later, our algorithm automatically as-signs weights to meta-paths so that those that are more relevant to a given classification task will be given higher weights over the less relevant meta-paths. [Class-level meta-paths] We extend meta-path to class-level meta-path in capturing object correlation to achieve better classification results. We have briefly discussed how meta-paths can help con-struct classifiers. The classifiers, in return, can help us derive class-level meta-paths (or cl-meta-path for short) in the following way. Consider again the co-authorship meta-path APA. Suppose we have the HIN shown in Figure 2(c) in which three objects P1, P2, A1 are already given labels (they are shaded in the figure). If we consider only the meta-path APA, we can only relate author A2 to the labeled author A1; the other authors are not related to A1 via the meta-path. Hence, we can only deduce the label of author A2 and give it the same label ( ) of A1. Now, from the meta-path APA we derive a cl-meta-path AP A. We use the notation X to refer to a jump from an object X1 to an object X2 if (1) both objects are of type X and (2) both are given the same class label. In Figure 2(d), an instance of the jump P is P1 to P2 because they are given the same label. This jump instance is illustrated by a dotted edge (P1 in the figure. Conceptually, two authors A1 and A2 are connect-ed by the cl-meta-path AP A if we can traverse from A1 to A2 in the HIN following edges and jumps specified by the cl-meta-path. For example, from Figure 2(d), we see that A1 is connected to A3 via (A1  X  P1 P2  X  A3), which matches the cl-meta-path AP A. Likewise, A1 is connected to A2 and A4 as well. w.r.t. AP A. Hence, all these three authors can be labeled by the class of A1. Intuitively, if two authors have published papers on the same topic, they are likely to be in the same research area, even if they have not co-authored papers before. This example illustrates the advantage of cl-meta-paths over meta-paths in deriving objects X  correlation. [Active learning] Typical HINs are huge. For example, Face-book has more than 1 billion users and there are more than 2 mil-lion publications on DBLP. Obtaining the class labels of enough objects to form an e ff ective training set is costly. Active learning is about wisely selecting objects for which labels are sought so that accurate classifiers are built. Due to the large sizes of HINs, a good active learning strategy is essential. We remark that cl-meta-paths, which express objects X  correlation, can facilitate active learning on HINs. But how exactly shall cl-meta-paths and active learning be integrated? To illustrate, consider the HIN shown in Figure 2(e). Let us use the cl-meta-path AP A. Four (lightly-shaded) objects, P1, P2, P3, A1 are already labeled. Suppose we want to add one more object to the training set, which object shall we pick so that we can, say, classify more author objects? Note that picking A2 (darkly shaded in Figure 2(e)) to label is a bad choice. This is be-cause the cl-meta-path only connects A2 to A1, and A1 is already labeled. Intuitively, a good choice should be an object that is con-nected to many unlabeled objects and not so to labeled objects. In our example, A3 is a good choice (see Figure 2(f)). Note that A2 can be labeled because it is connected to (an already labeled objec-t) A1 via AP A, and A4, A5, A6 can be labeled because they are connected to the new addition (A3) of the training set via AP A. A major objective of this work is to study how meta-paths and active learning are integrated in constructing HIN classifier. In particular, we investigate (1) how to automatically choose good cl-meta-paths from a cl-meta-path set, and (2) how to design an active learning strategy given a set of cl-meta-paths.

We further show the usefulness of object classification on an HIN edge base of objects and their relationships. It consolidates infor-mation derived from Wikipedia, WordNet and GeoNames. Cur-rently, Yago has knowledge of more than 10 million objects (e.g., people, organizations, cities, locations, etc.) and more than 120 million relationships about these objects. This information on ob-jects and relationships can be conceptually viewed as an HIN.
An interesting kind of relationship of a Yago object is called  X  type  X , which relates the object to other descriptive conceptual ob-jects (e.g., scientist, actor, politician, etc.). Each such conceptual object is expressed either by a word from WordNet or by a cate-gory from Wikipedia. These conceptual objects form hierarchies, e.g.,  X  X merican economist X  is a subconcept of  X  X conomist X , which in turn, is a subconcept of a person. One can consider such con-ceptual objects (e.g., mathematician) as  X  X abels X  of an object (e.g., Knuth). The list of descriptive labels given to an object on Yago is not quite complete. For example, someone working at a university on scientific studies may be given the type  X  X cientist X  but not  X  X ro-fessor X ; the scientist might also not be given a type that describes his area of research (e.g.,  X  X atabase X ,  X  X iscrete geometry X ).
The incompleteness in an object X  X  description weakens the e tiveness of searching and logical deductions on a knowledge base. For example, to identify all database professors of Stanford Uni-versity, we need the labels  X  X atabase X  and  X  X rofessor X  be given to the appropriate person objects on Yago. However, while some pro-fessors at Stanford are given the  X  X rofessor X  type / label on Yago, others are not. Since it is infeasible to exhaustively assign labels to objects on a knowledge base, automatic classification techniques on an HIN is thus highly desirable. Our work therefore would be highly applicable in knowledge base deduction.

Note that in our HIN model links are not labeled. For exam-ple, with the bibliographic HIN, a link between an author object and a paper object is assumed to represent the authorship relation-ship. On Yago, links are labeled with semantic properties (such as  X  X sA X  and  X  X orksAt X ). We remark that our approach can be easily adapted to handle HINs for which multiple links of di ff erent seman-tic between objects are possible. A straightforward way is to add relationship objects . For example, the labeled link Knuth Stanford, can be modeled by a path Knuth  X  worksAt  X  Stanford, where  X  X orksAt X  is considered a vertex in the graph.

In this paper we put forward the idea of meta-path and cl-meta-path to capture complex relationships among objects to facilitate object classification. This idea can be applied to a knowledge base like Yago. E.g., the meta-path X advisedBy  X  X  X  X  X  X  X  X  X  X  X  X  Y advises Z are person objects, captures the relationship that X and Z share the same academic advisor. They are therefore likely to be scholars 3 http: // www.mpi-inf.mpg.de / yago-naga / yago / of the same research area. Hence, knowing the class label of one (e.g., mathematician) allows us to deduce the label of another.
Here we summarize our contributions in this paper:
We propose to use meta-paths of an HIN to derive object corre-lation in the construction of classifiers. Moreover, we propose the idea of cl-meta-path which is an extension of meta-path to further capture object relationships by exploiting objects X  labels.
We study how active learning should be applied to facilitate HIN object classification. In particular, we put forward an algorithm that integrates cl-meta-paths into various active learning strategies.
We propose a method of training a model that automatically as-signs weights to meta-paths such that meta-paths that are more rel-evant to a classification task are given higher weights. Our training model provides an adaptive strategy that leads to more e ff tive learning and more accurate classifiers.
We conduct experiments on real datasets to evaluate our classi-fiers and our active learning algorithm. We show that our classifiers that use cl-meta-paths can achieve high classification accuracy. We also show that our active learning algorithm is highly e ff
The rest of our paper is organized as follows. We review related works in Section 2. In Section 3 we formally define various con-cepts and the problem being studied. In Section 4 we present our classification and active learning algorithms. Experimental results are shown in Section 5. Finally, we conclude our work in Section 6.
There are a number of works studying various data mining tasks on HINs. These works include object clustering [24, 23, 17, 4], link prediction [19, 27, 26, 18], search [20] and object ranking [6, 22, 7, 2]. Some of these works use meta-paths, which were first proposed [14], which models textual data using heterogeneous networks and focuses on text classification.

Some representative studies on active learning techniques for general classifiers include [10, 16, 3]. To select objects for which labels are to be obtained, [10] employs uncertainty sampling (US). US evaluates each object in terms of how confident the classifier is in deducing the object X  X  label. Objects that the classifier is the least confident about are picked. Our HIN active learning method is based on uncertainty sampling. In particular, we study how cl-meta-paths can be integrated nicely with US to achieve very e tive active learning. In [16], an active learning method that employs the idea of query-by-committee (QBC) is given. Given a number of classifiers, objects for which the classifiers disagree most are select-ed. A disadvantage of the QBC approach is that it needs multiple classifiers as well as a  X  X isagreement measure X . In [3], each object is assessed in terms of how much the expected error of a classifi-er would be reduced if the object is assigned a label. Objects that lead to the largest error reductions are picked. Typically, error-reduction-methods are computationally much more expensive than US and QBC methods.

The problem of classifying objects in traditional (homogeneous) networks have been studied extensively. Collective classification [11, 28, 15, 12] is a general technique that classifies an object based on the labels of the object X  X  neighbors in the network. As an example, LLGC [28] builds an a ffi nity matrix A that reflects the  X  X loseness X  (e.g., in terms of path lengths) of any two objects in the network. The a ffi nity matrix derives a classifier F . LLGC follows the intu-ition that objects that are close according to A should be given the same class label. So, if there are objects whose labels assigned by F violate the intuition, F is updated to resolve the conflict between F and A . The updating step repeats until F converges. Although LLGC is designed to classify homogeneous network objects, we can apply it to classify HIN objects by disregarding object types. RankClass [7] integrates object ranking with classification in an HIN. The intuition behind the method is that higher-ranked ob-jects in a network should play a bigger role in classifier construc-tion 4 . For each class label k , RankClass assigns a class-k rank score P ( x p | k ) for each object x p . Also, if there is a link e objects x p and x q , the algorithm assigns a class-k weight S ( e to the link. The algorithm performs an iterative authority propaga-tion on the network to determine the rank scores and link weight-s such that: (1) if an object x p connects to many neighbors with large class-k rank scores through heavyweight links, then P ( x large; (2) if a link e pq connects two objects with large class-k rank scores, then S ( e pq | k ) is large. After the rank scores and link weights converge through the iterative procedure, the algorithm calculates a posterior probability Prob ( k | x p ) to make class prediction.
In [8] a graph-based regularization framework, GNetMine, is proposed. GNetMine first constructs a predictive function f each object type t i and class label k . It then derives an objective function that aims to minimize two values: (1) for any two highly correlated objects x p and x q , the di ff erence between their predic-tive values f k t the di ff erence between its predictive value f k t induced value, which is 1 if x r  X  X  label is k ; 0 otherwise. The predic-tive functions f k t Finally, GNetMine makes class predictions based on the f k
In [29], the AEClass algorithm for classifying objects in an HIN is proposed. Given an HIN G and a set of objects in G for which class labels are to be assigned, AEClass first constructs a collab-oration graph CG . For example, if we want to label authors with their research areas, CG can be constructed by the co-authorship relationship. Then, CG is augmented by considering the various activities that relate the objects. For example, these activities could be conferences that the authors have published in, or the terms that appear in the authors X  papers. By first labeling the activities through classification, objects X  similarities under each label are de-rived. These similarities are then used to assign labels to the ob-jects. A strong requirement of AEClass is that for a given set of objects in an HIN to be labeled, associated activities can be derived from the HIN. Moreover, the labels of such activities need to be of the same kind as those of the objects to be classified.

Our classification algorithm di ff ers from the above methods in that we utilize meta-paths (or cl-meta-paths) to further derive ob-ject relationships. In particular, our algorithm recognizes the dif-ferent link types (e.g., we distinguish a paper-venue link from a paper-author link when extracting instances of a meta-path), while the other methods regard all links as an expression of the simple neighbor relationship.
 In [9], a meta-path-based collective classification algorithm, HC-C, is proposed. While traditional collective classification algo-rithms use links to define an object X  X  neighbors, HCC extends that by using meta-paths. Our algorithm di ff ers from HCC in two as-pects: (1) our algorithm derives cl-meta-paths from meta-paths and use cl-meta-paths to train the classifiers, and (2) our algorithm auto-matically assigns weights to di ff erent meta-paths so that meta-paths that are more relevant to the classification tasks are given higher weights. This avoids the issue of irrelevant meta-paths adversely a ff ecting the quality of the classifiers. Moreover, we integrate ac-tive learning into our HIN object classification, which is not done in other previous works. objects.

In this section we define the problem of active learning in HIN object classification. For simplicity, we use O, A, P to denote var-ious object types and A5, O2, P3, etc. to denote objects of the corresponding types. Table 1 lists the symbols used in the paper. D efinition 1(H eterogeneous I nformation N etwork (HIN)). Let T = { t 1 ,..., t m } be a set of m object types. Let of n objects. Each object x  X  X  has a unique type, denoted by T ( x ) . We use X h to denote the set of all objects of type t X h = { x  X  X | T ( x ) = t h } and V =  X  m h = 1 X h . Let n a set of links, which represents a binary relation between objects in V . An object x i is related to another object x j i ff ( x HIN G = V , E is a set of objects V and a set of links E .
For simplicity, we assume that objects are ordered. We use x denote the i -th object in V . Similarly, we use x t h i to denote the i -th type-t h object.

D efinition 2(M eta -path ). A meta-path P = t i 1 ... t quence of object types. We use P [ j ] to denote the j-th object type (t ) specified in the meta-path P . Given an HIN G and a path p in G , let | p | be the length (as measured by the number of objects) of path p and p [ j ] be the j-th object in p. Path p is an instance of denoted by p P i ff p and P are of the same length and the types of the objects in p match those specified in P . Formally, p (1) | p | = d and (2) T ( p [ j ]) = P [ j ]  X  1  X  j  X  d.
E xample 1. Consider the HIN G shown in Figure 2(g) and the meta-path APA. The path A1-P1-A2 in G is an instance of meta-path APA because objects A1 and A2 are of type author (A) and P1 is of type paper (P).

Each object type t h is associated with a set of class labels CL { l types do not overlap 5 . An object x i of type t h may be assigned a label l h , j  X  CL h . If so, we say x i is labeled and we write L ( x If x i has not been assigned any label, we write L ( x i ) call L () the label function, which is straightforwardly derived from a training set L of labeled objects. Author, AI-Author, ...} while the labels of paper objects could be {Transaction-processing-paper, Planning-paper, ... }. D efinition 3(J ump and E xtended HIN). Given an HIN G = V , E and a training set of labeled objects L (and hence the label function L () ), a jump is a link between two objects x i (1) x i = x j (i.e., they are the same object) or (2) L ( x (i.e., they share the same label). We denote the set of all jumps by J . An extended HIN  X  G = {V , E , J} is obtained from the HIN including J as another type of links in the network.

E xample 2. Figure 2(g) shows an extended HIN  X  G . Jumps are shown as dotted lines in the figure. For clarity, only jumps involv-ing papers (P) are shown. Note that (1) every object is connected to itself by a jump, and (2) P1 and P3 are connected by a jump because they share the same label.

D efinition 4(C lass -level M eta -path ( cl -meta -path length-d meta-path P , we construct a length-d cl-meta-path replacing every object type t mentioned in P by the symbol t ,ex-cept for the beginning and the ending object types in P . That is (1)  X  P [1] = P [1] , (2)  X  P [ d ] = P [ d ] , and (3) (  X  P [ i ] i  X  d  X  1 . For example, from the meta-path APVPA we obtain the cl-meta-path AP V P A.
 D efinition 5(P ath instance ). Given an extended HIN  X  G = {V , E , J} , conceptually, a path p in  X  G is an instance of a cl-meta-path  X  P , denoted by p  X  P , if (1) the objects visited in p follow the se-quence of types specified in  X  P and (2) a jump Xi  X  X j is taken whenever we encounter an X in  X  P . For example, consider the cl-meta-path AP A. The path A1  X  P1 jump  X  X  X  X  X  X  P3  X  A5 shown in Figure 2(g) is an instance because we start with an author object, follow a link to a paper object, take a jump to another paper object, and follow another link to an author object. Formally, p  X  P | p | = ( |  X  P| X  2)  X  2 + 2 , (2) T ( p [ i / 2 + 1]) =  X  P ( p [ i ] , p [ i + 1])  X  X  X  odd 1  X  i  X | p | X  1 , (4) ( p [ i ] even 1  X  i  X | p | X  1 .

Given two objects x i , x j  X   X  G , we say that a length-d path p relates x and x j if p starts from x i and ends at x j , i.e., if ( p [1] ( p [ d ] = x j ) .

Our objective is to integrate meta-paths (and hence cl-meta-paths) into classifier construction and active learning to get more accurate classification results. The goal of the latter is to wisely select ob-jects to be included into the training set to achieve good classifier accuracy despite the training set is limited in size (e.g., by a label-ing budget).

D efinition 6. (Active Learning for HIN Object Classification (ALHOC)): Given an HIN G = V , E of m object types T , the set of class labels CL h for each object type t h  X  X  , a training set of labeled objects L , a set of meta-paths PS , and a budget B, the ALHOC problem is (1) to select B unlabeled objects in V to obtain their labels (and thus to update L ), and (2) to construct m accurate classifiers f h : X h  X  CL h , 1  X  h  X  m.
In this section we describe our algorithm HINAL for classifying objects in HINs using meta-paths and active learning. We assume that a budget B is given, which is the number of objects we can select in active learning for labeling. We first give an overview of HINAL before we explain the technical details later in this section. Figure 3 abstracts the 7 major steps of HINAL.

Given an HIN G and a set of meta-paths PS , we first derive a set of cl-meta-paths cl -PS (Step 1). Given the set of labeled objects L and cl -PS , we determine jumps and construct an extended HIN  X  G (Step 2). From  X  G we determine the relative weights  X   X  P cl-meta-paths  X  P in cl -PS (Step 3). Intuitively, a heavier cl-meta-path is more relevant to the classification task. Since a cl-meta-path expresses a relationship between objects, we derive a correlation matrix W  X  P for each  X  P in cl -PS (Step 4). If two objects are related by many instances of  X  P , then they are highly correlated w.r.t. so the entry in W  X  P corresponding to the object pair will be large. The correlation matrices W  X  P  X  X  and their weights  X   X  P two ways. First, they are used to train m SVM classifiers f for each object type t h  X  X  ) (Step 5). Second, if the budget B is not exhausted, then the matrices, together with the classifiers, are used to perform active learning (Step 6). After objects are selected to label, we update L and  X  G (Step 7). The training process is then repeated until the labeling budget B is exhausted. Next, we give more details on Steps 3-6.
For each cl-meta-path  X  P , its correlation matrix W  X  P matrix ( n = |V| ) that measures how correlated any two objects are w.r.t.  X  P . We measure this correlation between two objects x by the number of path instances of  X  P that relate x i and x Each cl-meta-path  X  P is given a weight  X   X  P . We learn the weights  X = {  X   X  P |  X  P X  cl -PS} by optimizing an objective function on the training data set L . The idea is to maximize the correlations of objects that share the same labels and to minimize the correla-tions of objects that are given di ff erent labels: where  X  is a regularization parameter,  X  2 is the L 2 norm, and sgn() is -1 if the objects have di ff erent labels 6 . To learn derivative w.r.t. each  X   X  P , set those derivatives to 0, and obtain
We train an SVM classifier f h : X h  X  CL h for each object type t  X  X  . We first describe how to construct the feature vector each object x i  X  X  . Then, we describe how we learn the parameters of the SVM classifier f h .

Consider the set of cl-meta-paths cl -PS . Let N clmp be the number of cl-meta-paths. The feature vector  X  x i consists of N clmp each corresponds to a cl-meta-path. Specifically, for each cl-meta-path  X  P , the segment is obtained by extracting the row in the correla-tion matrix W  X  P that corresponds to object x i , weighted with denote this segment of x i  X  X  feature vector  X   X  P x and  X  x i is the concatenation of  X   X  P x constructed the feature vectors of all objects x i  X  X  h , we normalize all the feature vectors X  entries into the range [0,1]. 6 Since we assume di ff erent object types have disjoint label sets, sgn( x i , x j ) = 1if x i and x j are of the same type. One can extend the definition of sgn() to model the similarity of labels of di object types. E.g., the author label DB-author and the paper la-bel Transaction-processing-paper share some similarity. Therefore objects of these two labels could be given a positive sign() value.
Next, we build a multi-class SVM classifier f h 7 . For each label l j  X  CL h , we define the predictive function  X  f h , j by where  X  is a mapping function, and w  X  P h , j (for  X  P X  are parameters to be learned using soft-margin SVM [1]. With the predictive functions of all the labels in CL h , a type-t sified to the label l h , j whose predictive function  X  f value among all labels in CL h . The classifier f h is given by,
We perform iterative active learning to select objects to label. In each iteration, we pick N s objects for which labels are obtained, after which the training set L and hence the extended HIN updated. This is repeated until the budget B is exhausted.
An active learning iteration consists of two parts. First, we select a set of candidate unlabeled objects C . Second, N s objects from C are selected based on uncertainty sampling (see Section 2). Intu-itively, if two objects are highly correlated, labeling one of them is enough for us to infer the label of another. In other words, la-beling both of them is redundant. Hence, we want the objects in the candidate set C not being strongly correlated. To achieve that, we cluster objects so that highly correlated objects are put into the same cluster. By picking our candidates across these clusters, we avoid putting many highly-correlated (and thus redundant) candi-dates in the candidate set C . (see Figure 3 for an illustration).
To perform clustering, we need a distance measure between ob-jects. For that, we use a global correlation matrix W , which is given by the weighted sum of all the cl-meta-paths X  correlation matrices: We perform two touch-ups on W : (1) We normalize the entries in W to the range [0,1]. (2) For any labeled object pair x i we update the entry W [ i , j ]to1if x i and x j are given the same label; or to 0 if they are given di ff erent labels. This update is a well-known method to construct constraints for graph clustering [30]. The objects are clustered using KASP [25], which is a fast approximation algorithm of spectral clustering.

We select one candidate from each cluster. Intuitively, a can-didate x i is good if (1) it is highly correlated to many unlabeled objects (so that labeling x i allows us to infer the labels of many other unlabeled objects), and (2) it is not highly correlated to many labeled objects (so that labeling x i is not redundant). This idea can be quantitatively measured by the following score: The object in each cluster with the best score is then added to the candidate set C 8 .

To finally pick the N S objects to label, we conduct uncertainty sampling [5]. We briefly give an outline of the procedure. For each unlabeled object x u , let t h be its type. Object x u  X  X  label distribu-tion is given by [  X  f h , 1 ( x u ) ,  X  f h , 2 ( x u ) ...,  X  the predictive function of x u being of class label l h , Equation 4). We denote the entropy of the distribution by E ( x (We put the training set L in the entropy notation because the pre-dictive functions are trained based on L .) Now, for each candidate object x  X  C , if we add it to the training set and give it a label y , the training set is expanded. Let L + ( x , y ) denote this expanded train-ing set. We compute the entropies E ( x u |L + ( x , y ) labels y of x , and aggregate them into an expected entropy , denot-ed by E ( x u |L + ( x ,  X  ) ) 9 . The sum over all unlabeled objects x unlabeled objects if x is added to the training set L . The results of the active learning is to select from C , N S candidates that give the smallest overall expected entropies. Algorithm 1 summarizes our algorithm HINAL. In this section we present experimental results to evaluate HI-NAL. Our experiments consist of two parts. First, we study the performance of the SVM classifiers that are trained using cl-meta-paths (see Section 4.2). In particular, we show that our way of learning the cl-meta-paths X  weights is very e ff ective. This allows 8 One can also add the top few objects from each cluster to C , just to allow more candidates to be considered. when we modify L . Algorithm 1 HINAL Input: G = V , E , T , L , PS , B , N S , N C : # of clusters. Output: f h ,1  X  h  X |T| : classifier for object type t h . 1: Derive cl -PS from PS ; Get class-level meta-paths 2: while Budget B &gt; N S do Enough budget? 3: Construct  X  G from G , L , cl -PS ; 4: Compute W  X  P  X  X  based on Eq. 1; 5: Learn  X  based on Eq. 2; Learn  X   X  P 6: Train SVM classifiers f h  X  X  based on Eqs. 4 and 5; 7: Compute correlation matrix W based on Eq. 6; 8: Cluster objects into N C clusters; 9: Pick candidates from clusters to form candidate set C based 10: Select a set A of N S objects from C using uncertainty sam-11: L X  X  X  A ; B  X  B  X  N S ; 12: end while 13: return f h  X  X ; HINAL to automatically use the most e ff ective cl-meta-paths in classifier construction. Second, we evaluate the e our active learning technique. We show that the three steps taken in HINAL X  X  active learning phase, namely, clustering , candidate se-lection , and uncertainty sampling (see Section 4.3) work very well together. This results in much more e ff ective active learning com-pared to other approaches.
We collected two real data sets DBLP and IMDB to conduct our experiments. Both of them represent HINs.
 ed objects related to four research areas: database , data mining , information retrieval , and artificial intelligence to form the dataset. The dataset contains 14,376 papers (P), 20 publication v-total 170,794 direct links of three types: P-V, P-A, and P-T. We have labeled 4,057 authors and all 20 conferences as ground truth. The classification task is to classify authors and venues by research areas. Similar to [23], we consider the meta-path set PS = PV, VPAPV, APA, APTPA, APAPA, APVPA, APV, APAPV}.
 that are related to movies of three genres: sci-fi , comedy , and hor-ror to form the IMDB dataset. The dataset contains 11,430 actors (A), 5,612 movies (M), 3,092 directors (D), and 5,067 keywords (K). There are 53,030 links of three types: M-A, M-D, and M-K. The classification task is to classify actors and directors by their genres. We labeled all actor and director objects as ground truth. We consider the meta-path set PS = {DMKMD, DMAMD, AMA, AMKMA, AMAMA, AMDMA, AMD, AMAMD}.
We study the performance of HINAL X  X  SVM classifiers, which are constructed using the cl-meta-paths derived from the set We turn o ff active learning (Step 6) and assume that all our budget B is spent on getting an initial training set L .
 We compare HINAL with four other classification algorithms: LLGC, GNetMine, HCC, and KEY. We have already described the 10 http: // www.informatik.uni-trier.de / ley / db / 12 http: // www.imdb.com / first three in Section 2. Here, we further describe LLGC and KEY and explain how they are applied to our classification problem.  X 
LLGC is designed for homogeneous networks. To apply it on an HIN G , we transform G into a homogeneous network first.
For example, to build a classifier for authors in DBLP, we need an author network . We get this network by establishing a link between two authors if they are related by the co-authorship re-lationship, which is equivalent to the relationship expressed by the meta-path APA. Figure 1(a) shows an example network de-rived from the HIN in Figure 1(b). Similarly, for the task of classifying venues in DBLP, under LLGC, we construct the v-enue network based on VPAPV. For IMDB, we use meta-paths
AMA and DMAMD for constructing the networks of actors and directors, respectively.  X 
KEY is a keyword-based classifier. KEY uses the same classifier (SVM) as HINAL but it di ff ers at how the feature vector of each node is constructed. Under KEY, the feature vector of an object is composed of the keywords (or terms) that are associated with the object. For example, in DBLP, the feature of an author is all the terms mentioned in the papers written by the author. In other words, authors X  feature vectors are derived from the meta-path APT. Similarly, we use VPT to derive venue X  X  feature vec-tors (the terms mentioned in all the papers of a conference). For
IMDB, we use meta-paths AMK and DMK to extract keywords for classifying actors and directors, respectively.
 Tables 2 and 3 show the classifiers X  accuracies for DBLP and IMDB, respectively, as the size of the training set ( L ) changes We use ( a %, v %) to represent that L contains a % of author objects and v % of venue objects in the DBLP experiment. Since there are much more authors than venues, the author objects in L represen-t a fairly small fraction of the total author population. Similarly, we use ( a %, d %) to represent that L contains a % of actor objects and d % of director objects in the IMDB experiment. From the ta-bles, we observe that (1) a larger L generally gives more accurate classifiers, and (2) HINAL gives much higher accuracies than other algorithms under all cases.

An important feature of HINAL is that it automatically learns the weights  X   X  P  X  X  of the meta-paths. This weighting mechanism is im-portant for HINAL to distinguish meta-paths that are very e to the classification task from those that are not. Before we give 13 We randomly generated 10 L  X  X  for each training set size. The re-ported values are averages of the accuracies obtained over 10 runs. an in-depth comparison study of the classifiers, let us first evaluate how well HINAL learns those weights. As will become clear later, our discussion on meta-path e ff ectiveness can help us understand the relative performance of the various classification algorithms. Consider the task of classifying authors in DBLP. Figure 4 shows HINAL X  X  classifier accuracy vs. L given di ff erent meta-path sets. For simplicity, we consider only four meta-paths as shown in the figure. The curve labeled APA, for example, refers to the case = {APA}, likewise for the other curves labeled APAPA, APTPA, and APVPA. The curve labeled ALL4 refers to the case in which PS consists of all four meta-paths. under various PS
Comparing the individual meta-paths, Figure 4 shows that the meta-path APVPA is highly e ff ective, leading to a high accuracy even when it is the only meta-path HINAL uses. Meta-path APT-PA is less e ff ective but it is better than APAPA and APA. Also, we see that when HINAL is given all four meta-paths to work with, its performance is very close to that of APVPA, which is the most ef-fective of the given four. This indicates that HINAL is able to adjust to the most e ff ective meta-paths. In fact, the relative weights of the meta-paths HINAL learned are APA (0.47), APAPA (0.59), APT-PA (3.71), APVPA (8.33) when |L| = (0.14%, 40%). The weight of APVPA is about 18 times that of APA.

The relative e ff ectiveness of the meta-paths may look counter-intuitive. After all, two authors co-authoring a paper is a strong indicator that they work in the same research area. So, wouldn X  X  A-PA be the best meta-path for classifying authors instead of the worst as shown in Figure 4? To answer that question, let us consider the weighted correlation matrix  X   X  P W  X  P derived from each meta-path Now, for a threshold value  X  and two labeled author objects A1 and A2 such that  X   X  P W  X  P [A1, A2]  X   X  , we say that (A1, A2) form a  X  X rue positive X  if they share the same labels; they form a  X  X alse positive X  otherwise. This true / false positive measures how well the correla-tion derived from P helps predict authors X  labels. In Figure 5, we show the ROC curves of the true / false positive values derived from the various meta-paths. We see that APA (with the largest AUC) indeed is the best relationship for inferring author label. The prob-lem with APA is that the relationship has a very small reach , i.e., the number of co-authors of a given (labeled) author is very limited and so we can infer very few author labels via the APA relation-ship. Consider the case when L = (0.14%,40%), Table 4 shows, on average, the percentage of author objects that can be reached from an author object via a (class-level) meta-path. We see that the reach of APA is too small for the meta-path to build an e ff ective classifier. Combining the reach factor with our discussion based on the ROC curves, one can easily understand the relative e ff ectiveness of the four meta-paths as shown in Figure 4.

Now, let X  X  go back to Table 2. As we have mentioned earlier in this section, LLGC is essentially using the APA meta-path to de-Table 4: Fraction of authors reachable from a given author via a cl-meta-path rive the co-authorship network. As we have explained, APA is a poor meta-path for classifying authors due to its poor reach. This explains why LLGC performance is so poor. HCC uses meta-paths to classify HIN objects. However, it treats all meta-paths equal-ly. Its performance is thus dragged down by the ine ff ective meta-paths (such as APA and APAPA, as illustrated in Figure 4). KEY derives an object X  X  feature by keywords (or terms) through the re-lationship APT. So, two authors are  X  X imilar X  under KEY if they have written papers that share common terms. KEY thus e tively correlates authors through the APTPA meta-paths, which is shown to be mediocre in Figure 4. Although GNetMine does not use meta-paths, its classifier uses all links provided by an HIN. The information GNetMine uses to classify objects is therefore quite rich, which results in its coming second in classifier accuracy. Our algorithm HINAL uses meta-paths, which are information rich. Its ability to learn the weights of meta-paths allows it to adapt to the most e ff ective meta-paths. Our results show that HINAL signifi-cantly outperforms the other classifiers. We have also conducted experiments on HINAL using meta-paths instead of cl-meta-paths. Our results show that using cl-meta-paths (i.e., with jumps) gives better performance than using meta-paths. This shows the signifi-cance of jumps in object classification.
In this section we study the e ff ectiveness of HINAL X  X  active learn-ing component. Recall that HINAL performs active learning with three sub-steps: (1) Clustering  X  which groups objects into clus-ters so as to avoid picking redundant objects to label. (2) GenC  X  which picks candidates from each cluster that give the best good-ness scores (Equation 7) to form the candidate set C . (3) US  X  which performs uncertainty sampling to select objects from C to label. Also, given a budget B , HINAL iterates B / N S times, picking N
S objects to label in each iteration. After each iteration, the clas-sifiers are re-trained (Figure 3). We call this iterative refinement . We study the significance of these steps by modifying HINAL with the following active learning strategies:  X 
Random : Randomly pick N S unlabeled objects to label. This is equivalent to HINAL with all three sub-steps turned o ff . This strategy serves as a baseline for comparison.  X 
Label Selection Based on Clustering (LSC) [13]: Group objects into N C clusters and then randomly select N S / N C unlabeled ob-jects from each cluster to label. This strategy is equivalent to
HINAL with GenC and US turned o ff .  X 
Goodness : Evaluate the goodness score for every unlabeled ob-ject and pick the top-N S of them with the highest scores to label.
This is equivalent to HINAL with clustering and US turned o  X 
Uncertainty Sampling (US) [5]: Perform uncertainty sampling directly on all unlabeled objects to pick N S objects to label. This is equivalent to HINAL with Clustering and GenC turned o ff  X  HINAL without Iterative Refinement (HINAL-noIR) : This is HI-
NAL with N S = B , the whole budget. So all objects to be labeled are selected within one iteration.

Figure 6 shows the classifier accuracy under the various active learners as the budget B varies. Two figures are shown, one for D-BLP and the other for IMDB. Note that the algorithms di ff by the active learning strategy employed. Classifiers are still built based on HINAL X  X  cl-meta-path-based approach. The initial train-ing set for DBLP contains 8 labeled authors and 4 labeled venues. For IMDB, it is 16 actors and 4 directors. We set N S = 1 and 5 for the experiments on DBLP and IMDB, respectively.
From Figure 6(a), we see that, for DBLP, the trivial Random s-trategy performs worst compared with other active learners. LSC, Goodness, and US, which apply one of the three sub-steps of HI-NAL each, show comparable performance and they are clearly bet-ter than Random. This shows that all of the three sub-steps, namely, clustering, goodness measure, and uncertainty sampling are use-ful. Also, we see that HINAL significantly outperforms the others. This shows that the improvements brought about by the three sub-steps are additive. If we compare HINAL and Random, we see that the performance gap between them are wider when B is small-er. Hence, it is very important that we have a good active learning component when we have a tight labeling budget. HINAL-noIR al-so performs very well because it does all three sub-steps. However, by picking objects in one shot, HINAL-noIR misses the opportu-nity to perform active learning with more refined classifiers. This results in a slightly worse performance compared with the iterative HINAL. Similar conclusions can be drawn for the IMDB experi-ment (Figure 6(b)).

Tables 5(a) and (b) show the budget B needed to achieve certain classifier accuracy for DBLP and IMDB, respectively. We see that HINAL requires a much smaller budget than the other active learn-ers, especially when compared against Random. The savings are particularly significant for IMDB. This is because IMDB objects are harder to classify (as reflected by the classifier accuracy val-ues) and so it is even more important that a good training set (as obtained through a good active learning strategy) is used.
We end this section with a brief description of a case study in which we applied HINAL to classify objects on Yago. As we have discussed in the introduction, it is important that objects in a knowl-edge base are given descriptive labels to facilitate e ff tion retrieval and logic deductions. However, such labelings require a lot of manual e ff orts and so are often incomplete. As an example, Yago contains about 60,000 movie objects, some of which are la-beled with their genres (such as comedy , action , sci-fi , etc.). These labelings, however, are incomplete. For example, the movie  X  X he Lord of the Rings: The Return of the King X  is given the label fanta-sy on Yago. However, the movie is categorized by Freebase (anoth-er collaborative knowledge base) as fantasy and adventure . In fact, we inspected a good fraction of Yago X  X  60,000 movie objects. We found that, among them, about 1 / 3 are not labeled with any genre (we marked them as unlabeled ); about 1 / 2 of them are only par-tially labeled , meaning that the genre labels given to each of those movies form only a proper subset of the labels given to the corre-sponding movie on Freebase; and only about 1 / 6 of the movies on Yago are completely labeled . As a further example, we inspected the movie objects on Yago that are categorized as comedy on Free-base. We found that about 47.5% of them are not given the comedy label on Yago.

We investigate how HINAL helps a knowledge base acquire ob-ject labels, especially when only a small fraction of the objects have already been properly labeled. As an illustrative example, we con-sider a very simple binary classification task  X  to label each movie object on Yago as either comedy or non-comedy .

We randomly selected 530 movie objects on Yago as our training data (which is about 0.9% of all movies on Yago). We consulted Freebase and labeled each movie object in the training data as ei-ther comedy or non-comedy . We then applied HINAL on the Yago information network to infer the label ( comedy , non-comedy )ofthe movie objects. We used the meta-path set PS = { MAM, MDM, MPM, MUM, MAMAM, MPMPM, MUMUM }, where the sym-bols denote (M)ovie, (A)ctor / actress, (D)irector, (P)roducer, and m(U)sician. These seven meta-paths connect two movies via the following relationships: (1) they share an actor, (2) they are direct-ed by the same director, (3) they are produced by the same produc-er, (4) they share a music composer, (5) they have actors that have co-acted in a movie, (6) they are produced by producers who have co-produced a movie, (7) they have musicians that have contributed to the music of the same movie.

Note that some of these meta-paths (and the cl-meta-paths they derive) may not be useful in the classification task. Their relative e ff ectiveness is automatically determined by HINAL via its weight-assignment mechanism. Table 6 shows the weights of the meta-paths HINAL assigned. We see that the longer meta-paths (e.g., MAMAM and MPMPM) received higher weights than the shorter ones (e.g., MAM, MPM). This is again due to the reach issue.
Our experiment results show that HINAL is quite e ff ective in i-dentifying comedy movies on Yago. Specifically, the precision and recall for the comedy class is 94.8% and 73.3%, respectively. Note that the classification task is an asymmetric one. That is, while movies on the knowledge base that are identified as comedy should be marked ( comedy ), those that are non-comedy would not be phys-ically given the (non-comedy) label. Instead, other classification tasks could have been done to determine whether those movies are sci-fi or action movies, etc.

We remark that in the experiment, a very small training set was used (which is about 0.9% of all movie objects). Yet, based on this small set of training examples and the meta-paths (and their derived cl-meta-paths), HINAL was able to identify 73.3% of all comedy movies. This number is significantly larger than what Yago currently knows (47.5%). This experiment thus further illustrates the applicability of HINAL in improving knowledge bases.
In this paper we studied the problem of classifying objects in het-erogeneous information networks. We proposed to use meta-paths to derive correlations between objects so as to build better classi-fiers. In particular, we proposed class-level meta-paths and put for-ward a method to determine the relative weights of cl-meta-paths through a learning process. The correlations derived from cl-meta-paths allow us to achieve two objectives. One is to construct object feature vectors for classifiers construction. The other is to perform active learning by which objects are picked to form the training set. We presented a comprehensive study on our algorithm HINAL by experiments. Our results show that HINAL outperforms existing classification algorithms for HINs. Its active learning component is also very e ff ective, beating other approaches. In the future, we plan to apply our method to more networks to evaluate its e tiveness. On the other hand, we can analyze more general classi-fication problem in heterogeneous information networks, in which an object could have more than one class label. This makes sense since a venue could be related to both database and information re-trieval on DBLP data set. It would be more interesting if we could model it as a multi-label classification problem and output a class distribution over each object.
This research is supported by Hong Kong Research Grants Coun-cil GRF grant HKU712712E. [1] C. Cortes and V. Vapnik. Support-vector networks. Machine [2] H. Deng et al. Modeling and exploiting heterogeneous [3] Q. Gu and J. Han. Towards active learning on graphs: An [4] M. Gupta et al. Evolutionary clustering and analysis of [5] A. Holub and P. P. M. Burl. Entropy-based active learning for [6] H. Huang, A. Zubiaga, et al. Tweet ranking based on [7] M. Ji et al. Ranking-based classification of heterogeneous [8] M. Ji, Y. Sun, M. Danilevsky, J. Han, and J. Gao. Graph [9] X. Kong, P. S. Yu, Y. Ding, and D. J. Wild. Meta path-based [10] D. D. Lewis and W. A. Gale. A sequential algorithm for [11] Q. Lu and L. Getoor. Link-based classification. In ICML , [12] J. Neville and D. Jensen. Relational dependency networks. [13] D. Pfe ff ermann and C. Rao. Sample Surveys . Handbook of [14] R. G. Rossi, A. A. Lopes, and S. O. Rezende. A [15] P. Sen, G. Namata, et al. Collective classification in network [16] H. S. Seung, M. Opper, and H. Sompolinsky. Query by [17] Y. Sun, C. C. Aggarwal, and J. Han. Relation strength-aware [18] Y. Sun et al. Co-author relationship prediction in [19] Y. Sun et al. When will it happen?: relationship prediction in [20] Y. Sun and J. Han. Meta-path-based search and mining in [21] Y. Sun, J. Han, X. Yan, P. S. Yu, and T. Wu. Pathsim: Meta [22] Y. Sun, J. Han, P. Zhao, Z. Yin, H. Cheng, and T. Wu. [23] Y. Sun, B. Norick, et al. Integrating meta-path selection with [24] Y. Sun, Y. Yu, and J. Han. Ranking-based clustering of [25] D. Yan, L. Huang, and M. I. Jordan. Fast approximate [26] Y. Yang et al. Predicting links in multi-relational and [27] X. Yu, Q. Gu, M. Zhou, and J. Han. Citation prediction in [28] D. Zhou et al. Learning with local and global consistency. In [29] Y. Zhou and L. Liu. Activity-edge centric multi-label [30] X. Zhu. Semi-supervised learning with graphs . PhD thesis,
