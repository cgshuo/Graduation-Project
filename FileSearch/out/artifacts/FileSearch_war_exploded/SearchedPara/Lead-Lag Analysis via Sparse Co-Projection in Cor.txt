 Correlated topical trend detection is very useful in analyzing public and social media influence. In this paper, we propose an algorithm that can both detect the correlation and dis-cover the corresponding keywords that trigger the correla-tion. To detect the correlation, we use a projection vector to project two text streams onto the same space, and then use a least square cost function to regress one text stream over the other with different time lags. To extract the corresponding keywords, we impose the non-negative sparsity constraints over the projection parameters. In addition, we present an accelerated algorithm based on Nesterov X  X  method to effi-ciently solve the optimization problem. In our experiments, we use both syntehtic and real data sets to demonstrate the advantages and capabilities of the proposed algorithm over CCA on the follower link prediction problem.
 H.2.8 [ Database Management ]: Database Applications X  Data mining ; I.2.7 [ Artificial Intelligence ]: Natural Lan-guage Processing X  Text analysis Algorithm, Experimentation Lead-Lag Analysis; Correlation Detection; Gradient Descent
With the explosive growth of online information such as news articles, blog posts, microblog posts and so on, exten-sive studies have been conducted to help users better under-stand and consume this textual information. One interesting problem is detecting whether the content from different re-sources is correlated and impacts each other. For example, some news providers publish news on certain topics faster than others [9]. Another example is topic influence in a so-cial network. Commonly, the topics of interest of a user in a social network tend to be influenced by their friends X  top-ics [20]. Thus a method to characterize and compare topic trending behaviors based on the text content is very useful for analysts to identify lead topics as well as their dissemi-nation and impact over time.

To help users examine topical lead-lag relationships across corpora, Shi et al. [17] proposed an intuitive and simple so-lution based on LDA [3] and time series analysis. Although this method has achieved a certain amount of success in com-paring topic trending behaviors between two text corpora, there are still two challenges to be addressed.

One challenge is to effectively detect and model the cor-relation between text streams. The text content is usually represented by bag-of-words. Thus, the data is very high-dimensional. Even if we pre-process the text with a topic model or a clustering method, e.g., latent Dirichlet alloca-tion (LDA) [3] or a constrained coclustering approach [18], the space representing the topics is still high-dimensional in the statistical sense. For example, consider that we have 100 topics for two text streams. If we want to analyze their daily correlation for one year, we only have 365 time stamps to do the correlation analysis. Compared to a 100 dimensional space, the 365-timestamped data is still too small.
Another challenge is to find keywords that trigger the cor-relation. For example, we have two text streams discussing the US presidential election. Assume we analyze the correla-tion on the original keyword space instead of the topic space. Typically, keywords such as  X  X bama X ,  X  X omney X ,  X  X conomy X  and  X  X ar X  may trigger the correlation. Other keywords such as  X  X ndonesian X ,  X  X arvard X , and  X  X usinessman X  may under-mine the correlation score. It is not trivial to automatically detect the causal keywords, since there may be a lot of noise.
Recently, a canonical correlation analysis (CCA) based method was proposed to detect the correlation of trends [2]. This method works well in detecting the canonical trend be-tween text streams. However, it still faces some problems. First, CCA uses two projection vectors for each of the two text streams, and analyzes the correlation in the projected one-dimensional time series. As a result, it can detect var-ious kinds of correlations, even when the topics of two text streams are not relevant. In practice, when analyzing the topical lead-lag correlation for the influence on some specif-ic topics, we do not want to include the correlation across topics that are totally irrelevant. Second, CCA will detect both positive and negative correlations. When some of the keywords are positively correlated and others are negatively correlated, CCA cannot distinguish them. However, we do not expect that the lead-lag is caused by a mixture of both positive and negative correlations.

In this paper, we propose an algorithm to both analyze the lead-lag correlation between text streams and extract the keywords that lead to such correlation. To analyze the correlation, we only use one projection vector to project the two candidate text streams onto two time series, and use a least square cost function between two time series to eval-uate the correlation. To extract the leading keywords, we constrain the projection vector to be non-negative, and set the L 1 -norm to one, i.e., making the projection vector lie in the probabilistic simplex. As a consequence, only keywords that are positively correlated have non-zero values of the el-ements in the projection vector. Thus, the projection vector also performs as a feature selection to find the leading key-words. We solve the optimization problem using an accel-erated gradient algorithm based on Nesterov X  X  method [13], which is fast and suitable for large-scale data analysis.
We conducted experiments on two sets of synthetic data to show the difference between our algorithm and CCA, and also use another synthetic dataset with different scales to demonstrate the correctness and efficiency of the accelerat-ed algorithm. Moreover, we present three interesting case studies on social media data based on the application of our algorithm:
The rest of the paper is organized as follows. We intro-duce related work in Section 2. Then the detailed algorithm is presented in Section 3 and the connection to CCA is p-resented in Section 4. Next, we show the experiments that demonstrate the problems with current approaches and the advantages of our approach in Section 5. Finally we con-clude our paper in Section 6.
In this section, we review several papers related to our method. We first briefly discuss the current development of text or topic correlation analysis, then we present some related algorithms.

Topic detection and tracking (TDT) has been investigated for years [1]. Recent research has also studied how to detect topics correlated with each other [22] and how to extract topics from different sources sharing common information over time [23, 26]. In addition to detecting correlated topics over time, it is more interesting to analyze the correlation itself. For example, given two topics, or two sets of text streams under the same topic(s), can we detect whether they are correlated and how they are correlated?
Shi et al . used a simply defined correlation score to ana-lyze the lead-lag behaviors between two topic distribution-s [17]. However, this method can only detect the lead-lag relationships between two text streams in the context of the same topic. It also depends on the results of topic model-s [3]. In addition, it is unclear which topics/keywords trigger the overall correlation of two text streams. Thus, its usage in many real-world applications is limited. Steeg and Gal-styan proposed an entropy based lead-lag analysis for two topic distributions, and applied it to the social network link prediction problem [19]. Their method does not consider long-term correlation between text streams. Instead, it only analyzes the local historical impact. Recently, Bie X mann et al. [2] employed temporal kernel CCA to detect the canon-ical trend between two text streams. As we mentioned in the introduction, this method uses two projection vectors to find the correlation. It can detect correlations when two text streams have similar trends in volume, even if they are talk-ing about totally different topics. Moreover, CCA may mix up positive correlations and negative correlations. Although these correlations may sometimes be interesting, they are probably spurious. Here we focus on detecting the positive correlation relationship between two high-dimensional text streams. We prove that the correlation found by our method is more reliable in the experiment section on both synthetic data and real-world data.

Some variants of the CCA algorithm also consider the s-parsity of the projection vector to improve the interpretabil-ity of results [14, 15, 24, 8]. However, none of them imposes a non-negative constraint, and thus they all mix up the pos-itive and negative correlations. Witten and Tibshirani [25] proposed imposing both non-negative and sparse constraints on the projection vectors. Sparse constraint has gained in-creasing attention and proven to have good generalization and interpretability [21]. However, this method still uses t-wo projection vectors to find the correlation and consequent-ly will confound different topics in the text. Furthermore, these CCA-related methods are not designed for text min-ing related applications. In contrast to all the above CCA-based methods, we use only one projection vector to find the correlation factors. The motivations that led us to choose one projection vector are as follows. First, this will result in more effectiveness when the text is high-dimensional and noisy, since there are fewer parameters to estimate compared with CCA. Second, since text streams consist of homoge-neous data, one projection vector will be more suitable and have a lower risk of finding irrelevant-topic correlation, and thus can reveal a more reliable relationship between the text streams.

Recently, solving a L 1 -norm constraint or L 1 -norm reg-ularized sparse learning problem with the gradient method has been studied a great deal [7, 10, 4]. In [7], Ji and Ye proposed an extended gradient method and an accelerated gradient method based on Nesterov X  X  method to solve the trace norm minimization problem. Liu et al. presented a similar method to handle the l 2 , 1 -norm minimization prob-lem [10]. In [4], Chen et al. applied Nesterov X  X  method to solve a l 1 ,  X  regularized sparse learning problem. When ap-plying Nesterov X  X  method, an important step is Euclidean projection. In [5], Duchi et al. proposed two efficient meth-ods to do projections onto the l 1 -ball in high dimensions. In [11], Liu and Ye reported some approaches for comput-ing the Euclidean projections in linear time. Based on the above existing work, we also propose an accelerated gradi-ent algorithm derived from Nesterov X  X  method to efficiently solve our optimization problem.
In this section, we introduce in detail our algorithm for analyzing the correlation between text streams.

We denote X = [ x 1 , x 2 ,..., x T ]  X  R D  X  T and Y = [ y y ]  X  R D  X  T as two text steams, where x t , y t  X  R D  X  1 D is the dimension of data. For text, it is the vocabulary size. T is the length of the time stamps. In this paper, X and Y are normalized to zero mean and unit variance, P
Our algorithm is inspired by the causality analysis be-tween two time series, which is called co-integration [6]. The basic idea is to use one time series to regress the other. It has been claimed that some spurious relationships caused by correlation will be eliminated in this way [6]. If we have two time series x t and y t where t = 1 ,...,T , the so called  X  X ngle-Granger two-step method X  models the co-integration in following form: The first step is to estimate the parameters  X  and  X  , and the second step is to do a statistic test to judge whether follows a random walk [6].

For high-dimensional text data, we propose projecting the two text streams onto the same one-dimensional space, using a same projection vector. Then we analyze the regression property between the two projected time series as: with a time lead  X  . Since we centralize the data before regression, we do not need the constant parameter  X  shown in Eq. (1).

Note that we constrain every element in w to be non-negative and the sum of these elements to be one. There are several benefits from these constraints:
First, it is easy to verify that the objective function in E-q. (2) is convex, and the constraint region is a closed convex set. Thus, the formulation of our model is a convex opti-mization problem, which is relatively easy to solve. Second, since every element of w is non-negative, we can avoid con-founding positive correlations and negative correlations of different keywords and keep only the positive correlations. Third, according to the constraint conditions, the optimal solution w  X  lies in a probabilistic simplex. Thus, every el-ement in w  X  can be interpreted as a weighting factor or probability of the corresponding keyword. The optimal so-lution w  X  is a probability distribution of different keywords and can be interpreted as a common topic between these two text streams. Fourth, the constraint in our model is a special instance of L 1 -norm constraint. According to Lasso [21], the optimal solution for our model is sparse and in-terpretable, which is useful for extracting the keywords that contribute most to the correlation between two text streams.
In this subsection we present an accelerated algorithm based on Nesterov X  X  method [13] to solve our optimization problem in Eq. (2). For simplicity X  X  sake, we denote A  X  ( Y  X  X  X  )( Y  X  X  X  ) T  X  R D  X  D , and our model in Eq. (2) can be rewritten as: Note that the objective function in our model is a differen-tiable convex function and the constraint region is a closed convex set. Thus, the optimization problem is a constrained smooth convex optimization problem, which can be solved by gradient descent method or subgradient descent method. However, the convergence rate of the gradient descent method and subgradient descent method are O (1 /k ) and O (1 / respectively [10], where k is the iteration steps. Both O (1 /k ) and O (1 / s. Recently, a special kind of gradient descent method called Nesterov X  X  method has gained increasing attention and proven to be effective in solving L 1 -norm regularized optimization problems [10, 4, 11]. Nesterov X  X  method has a convergence rate of O (1 /k 2 ), which is much faster than the gradient de-scent and subgradient descent methods. Therefore, we in-troduce Nesterov X  X  method into our model and propose an accelerated algorithm.

We denote a general constrained smooth convex optimiza-tion problem as: where f ( x ) is a differentiable convex function in Z and Z is a closed convex set.

In contrast to the gradient descent and subgradient de-scent methods, which only utilize the latest point to esti-mate the current point, Nesterov X  X  method utilizes the last two points. It iteratively updates two points, x i which are the approximate point and search point at the i iteration respectively [10].
The search point s i is a linear combination of x i and x where  X  i is a combination coefficient at the i th iteration. The approximate point x i +1 is updated as: where Ep Z (  X  ) represents the Euclidean projection onto the convex set Z , which can be formulated as: So x i +1 is a gradient update of s i and projected onto the constraint convex set Z .
L i in Eq. (6) is the update stepsize at the i th and L i is selected to satisfy the Armijo-Goldstein rule, i.e., where f L i , s i ( x ) = f ( s i ) +  X  f 0 ( s i ) , x  X  s
According to the above description, the accelerated algo-rithm based on Nesterov X  X  method for our model is summa-rized in Algorithm 1: Algorithm 1 An Accelerated Algorithm for Our Model. 2: Output: w . 4: while the convergence condition is not satisfied do 9: L =  X L 11: end while 13: end while In Algorithm 1, f ( w ) = w T A  X  w and f L, s i ( w ) = s various rules to update  X  i in each iteration, here we follow the rule used by Liu et al . in [10].
The remaining question is how to compute the Euclidean projection, i.e. Ep Z ( v ), efficiently. In our model, the closed convex set Z is the probabilistic simplex, and Ep Z ( v ) can be formulated as follows: In order to describe the projection method more clearly, nex-t, we give a simple derivation following the work in [5]. By means of the Lagrange multiplier, the above optimization problem can be reformulated as: L ( w , X  0 , X  ) = 1 2 k w  X  v k 2 2 +  X  0 ( P D i =1 w i  X  1)  X   X  where  X  0  X  R and  X  = [  X  1 , X  2 ,..., X  D ] T  X  R D + are Lagrange multipliers. Then: we have: According to KKT condition,  X  i w i = 0. So if w i &gt; 0, then  X  = 0, and w i = v i  X   X  0 +  X  i = v i  X   X  0 . It has been proven in [16] that if v i &gt; v j and w i = 0, then w j = 0. According to this, we denote w ( i ) as the i th biggest element in w and suppose that there are d positive elements in total, so we have: where v ( i ) is the i th biggest element in v . Therefore, Denote u is v sorted in descending order, then Eq. (14) can be rewritten as: Now the question becomes how to find d . According to [16], the solution for d can be expressed as: In [5], Duchi et al . proposed an elegant ` 1 -ball Euclidean projection method to find d and calculate  X  0 simultaneous-ly, which has an expected time complexity of O ( D ). Here we use this method to do the Euclidean projection onto the probabilistic simplex. This algorithm is summarized in Al-gorithm 2 [5].
 Algorithm 2 Euclidean Projection Algorithm. 4: w = v 5: return 6: else 7: Initialize U = [1 ,...,D ], s = 0, d = 0 8: while U 6 =  X  do 9: select k  X  U at random 11:  X  d = length ( G ) 14: s = s +  X  s 15: d = d +  X  d 16: U = L 17: else 18: U = G \{ k } 19: end if 20: end while 23: end if
Note that the optimization problem derived from our mod-el is an instance of a constrained smooth convex optimiza-tion problem, and it has been proven in [12] that when us-ing Nesterov X  X  method to solve a constrained smooth convex optimization problem such as in Eq. (3), then following in-equality holds: where w  X  is the optimal solution, w 0 is the initial solution and b L g = max { 2 L g ,L 0 } . L g is the Lipschitz continuous gradient of f ( w ) and L 0 is the initial estimation of L Thus, the convergence rate of Algorithm 1 is O ( 1 k 2 ). If we define the desired accuracy as , then the iteration step needed to reach this accuracy is O ( 1  X  ).
The expected time complexity of Algorithm 2 is O ( D ), where D is the dimension of w . In each iteration of Algo-rithm 1, the main computational cost lies in steps 6, 7, 8 and 10. In steps 7 and 10, Algorithm 2 is called upon to do the Euclidean projection, and the expected time complexity is O ( D ). In step 6, search point s i is updated, and the time complexity for this step is also O ( D ). In step 8, we need to calculate f ( w ) and f L, s ( w ), and the time complexity for this is O ( D 2 ). According to the above convergence rate analy-O ( D 2 T ) time to calculate A  X  , which is the input of Algo-rithm 1. So the total time complexity of our approach is
In this section, we first present the general idea and deriva-tion of CCA, and then briefly discuss the difference between our method and CCA.
 CCA uses two projection vectors, w x and w y , to project X and Y respectively. CCA tries to maximize the correla-tion between these two projected series. w x and w y are the solution to the following optimization problem: Since our algorithm first centralizes and normalizes X and Y , the objective function becomes similar to Eq. (2) if we constrain || w || = 1. The only difference is that CCA uses two projection vectors to project different sources while our algorithm only uses one. With this difference, we can better align two sides of text streams. As we discussed, the data from two sides are both text, which is homogeneous data. Therefore, if we use different projection vectors, it is less efficient to find common topics. Moreover, two projection vectors will mix up different leading keywords or topics.
In this section, we first use two toy data sets to demon-strate the advantages of our approach. Then we use a syn-thetic data set to demonstrate the efficiency of the acceler-ated algorithm. After that, we conduct experiments on two real data sets collected from Twitter 1 and Weibo 2 . The first data set is used to show our method can effectively track common trends and simultaneously select the correspond-ing keywords. The second data set is used to illustrate the http://www.twitter.com http://www.weibo.com application of our method on social influence analysis, i.e., follower link prediction.
First, we designed a toy data set to indicate where CCA fails to return the correct correlation when we only want to know the positive correlation factors. We generated two sets of data, each having 5 features and 50 samples, denot-ed as X 0 (no lead-lag) and Y respectively. Thus, we had X 0  X  R 5  X  50 and Y  X  R 5  X  50 . The first feature was positive-ly correlated, and the last feature was negatively correlated, as shown in Figure 1(a). Other features in X 0 and Y were randomly sampled from a Gaussian distribution and inde-pendent from each other. The projection vector generated by our method is shown in Figure 1(b) and the two pro-jection vectors returned by CCA are shown in Figure 1(c). We can see that our algorithm successfully found that the first feature triggered the positive correlation and assigned a high weight to it. However, in this case, CCA assigned the highest values to the last feature, which in fact is negative-ly correlated. The result is quite intuitive since CCA does not consider whether the correlation is positive or negative while ours, with non-negative constraints, can effectively fig-ure out only the positive correlation.

The second toy experiment is used to show that CCA may mix up different features while our method does not. There are three features in this data, which are denoted as f 1 , f 2 and f 3 . Only f 3 is weakly correlated. However, f 1 in Y is strongly correlated with f 2 in X , as shown in Figure 2(a). Figure 2(b) shows that our method successfully found the common feature f 3 , which was weakly correlated, while CCA gave higher weights to features f 1 in w y and f w x and concluded that the two data are strongly correlated, as illustrated in Figure 2(c).

The results of these two experiments imply that CCA can mix up positive and negative correlations as well as detect cross-topic correlations, which is undesirable in many text mining scenarios. In contrast to CCA, our method success-fully detects the correct positive correlation in both experi-ments, thus showing the advantage of our method over CCA in these instances.
In order to validate the correctness and efficiency of the ac-celerated algorithm, we conducted experiments on a synthet-ic data set, where every element of A  X  was set to an indepen-dent gaussian random number. The baseline method used here was the extended gradient method which was proposed in [7]. Both methods were implemented using MATLAB 2009b and all the experiments were performed on a com-puter running Windows 7 with Intel Core 2 Quad CPU(2.66 GHz) and 4G RAM. We ran both methods on synthetic data 10 times and report the average results in Figure 3.
Figure 3(a) shows the time consumed by our accelerated method and gradient method to reach the same accuracy at different dimensions. In this experiment, the length of time series was set to a fixed number (5000). From Figure 3(a), we can see that the time consumed by both methods grew in a way similar to a quadratic function as the dimension increases. However, the time consumed by our accelerat-ed method grew more slowly. In addition, our accelerated by our method and CCA respectively. method clearly took less time than the gradient method in the same dimension, which implies the advantage of the ac-celerated method in large-scale applications. Figure 3(b) shows the objective function values of both methods at each iteration, with 5000 dimensions and 10,000 time points. It is clear that the objective function value of accelerated method decreased much faster than that of the gradient method and needed less iteration steps to achieve the same accuracy, thus once again validating the advantage of our accelerated method over the gradient method.
To validate our method on real-world data, we collected tweets from Twitter that were related to the 2012 US pres-idential election and got 71,446,627 tweets from 9,667,382 users. A standard stopword list was used to remove stop-words. After stemming, we extracted the bag-of-words fea-tures for each tweet and assigned every keyword their TFID-F values. In this experiment, we used our method to detect the common topic trends shared by two different Twitter users and selected the keywords to identify the common top-ic simultaneously.

For example,  X  X OLGOP X  is a popular Twitter user who cares about politics and  X  X OLITICO X  is a professional Twit-ter account which tracks and reports political events. We ex-tracted the bag-of-words features of tweets posted by LOL-GOP and POLITICO during October of 2012 and split them by hour, then we got two multidimensional sequential dataset-s, denoted as X (LOLGOP) and Y (POLITICO) respectively. In order to eliminate the noise caused by keywords that ap-pear very infrequently, we filtered keywords that appear less than one percent of the total time points, based on the in-tuition that very rare keywords are not discussed a lot and have less possibility to trigger trends. The common topi-cal trend returned by our method is shown in Figure 4. It illustrates that they match with each other very well at sev-eral time points. For example, we can see the three highest peaks correspond to three TV debates between Obama and Romney on Oct 3rd, Oct 16th and Oct 22nd. Moreover, the words that the projection vector assigns the highest values to are { think: 0.051, cbs: 0.043, vote: 0.041, love: 0.037 , china: 0.035, debate: 0.034, watch: 0.034 } . These words do have some connections with the presidential election, and can represent common topics.
In this experiment, we used the same dataset as the pre-vious experiment and applied our method to detect trend setters, that is, those whose tweet content can predict what will be discussed later on Twitter. The 2000 most retweeted users and all their tweets posted from Oct 1st to Oct 31th in 2012 were selected. For each user, the bag-of-words features of his/her tweets were extracted and split by hour, denoted as X  X  and those of all the others X  tweets as Y . Similar to the previous experiment, stopwords and words appearing very infrequently were removed.

In order to discover how well a certain user can predict the tweet content of the public afterwards, and how far a-head of time he/she can predict, we traversed  X  from 0 to 5, indicating that our search range was within five hours. For each pair of users X  tweets, we used a five-fold cross validation method to find the best  X  . The data was split into training Topical Trend Figure 4: Common topical trends of users  X  X OLGO-P X  and  X  X OLITICO X . and Y validate } . We applied our method to the training da-ta sets to obtain the projection vector. Then we used this one-dimensional series x and y . We calculated the average correlation score of x and y to select the best  X  , denoted as  X  best . A higher correlation and larger lead-time mean that the user is more influential. It is more likely that this user is the source of some important information or an opinion leader. Thus we defined a new criteria, the Prediction A-bility (PA), to measure the predictability of a user. For a certain user, we selected the lead-time  X  best corresponding to the highest correlation and calculated the PA score ac-cording to  X  best  X  correlation  X  best . Then we ranked all users according to their PA scores. Table 1 shows the top users ranked by PA scores from our data set.

Rank User Name  X  best Corr PA Table 1: Trend setters and their ranking scores.

From Table 1, we can see that the Twitter user with the highest PA score was  X  X all Street Journal, X  a popular Twit-ter account that posts frequently the latest news stories. Moreover,  X  X BC News, X   X  X he Caucus, X  owned by the NY Times, and  X  X ahoo! news X  are already well-known for pub-lic news.  X  X he Associated Press X  is also a news account that posts frequently on the latest news . Thus, the most influ-ential trend setters are news media accounts. This may be because these news outlets are closer to the source of politi-cal news or more sensitive to this news. Moreover, the other users X  profiles that are not news providers are shown in Ta-ble 2. We checked their tweets and found most of their topics were related to political events. The trend setter detection results validate that our algorithm can return meaningful trend setters and is useful for social influence analysis.
From Table 1, we can see that the Twitter user with the highest PA score was  X  X all Street Journal, X  a popular Twit-ter account that posts frequently the latest news stories. Moreover,  X  X BC News, X   X  X he Caucus, X  owned by the NY Times, and  X  X ahoo! news X  are already well-known for pub-lic news.  X  X he Associated Press X  is also a news account that posts frequently on the latest news. Thus, the most influ-ential trend setters are news media accounts. This may be because these news outlets are closer to the source of politi-cal news or more sensitive to this news. Moreover, the other users X  profiles that are not news providers but with higher PA scores are shown in Table 2. We checked their tweet-s and found most of their topics were related to political events. The trend setter detection results validate that our algorithm can return meaningful trend setters and is useful for social influence analysis.
To perform this experiment, we crawled a data set that contains both short messages and user graph information from Weibo.com, a famous social network website in China which is similar to Twitter. We used the user links (i.e., follower and followee links) to verify the correlation analy-sis results. There were a total of 56,290,313 messages from 1,599,795 users. For each Weibo message, the stopword-s were removed according to a standard Chinese stopword list and the bag-of-words features were extrated. Assume that there were two users, A and B, and we wanted to find whether A follows B. We set the temporal lead value of B as one day, which means user B X  X  messages were used to predict A X  X  messages the next day. If B can predict A X  X  messages, then we predict that B influences A, or A is a follower of B.
Some of the example influence relationships found by our method are shown in Figure 5. We anonymized the users X  names here since they are all in Chinese and represent them with numbers. We found the following: All the above examples show that follower links are useful for verifying our correlation results. Beyond that, our method can be further used to predict the possible followers of a user and find the true influence relationship.

To compare our method with CCA, we selected the top 1,162 users ranked by the numbers of their followers and extracted all their messages. We split the messages by day and built text streams for each user. There were 3,355 fol-lower links in total between these users. We also picked 3,355 randomly selected user pairs where the follower rela-tionships did not exist. Then we used the correlation results of our method and CCA to predict the links between users. The prediction was evaluated by ROC curve. We plotted the true positive rate against the false positive rate and got the ROC curves, which are shown in Figure 6. Moreover, the AUC (area under the ROC curve) of our method was 0.7657, higher than that of CCA, which was 0.6236. We can see that our algorithm can significantly better predict the follower links in this data.

In addition, we observed in the experiment that in many cases where CCA assigns high correlation scores, the two users have no link between each other and they tend to talk about obviously different topics. The two projection vectors returned by CCA also differ with each other significantly in these cases. Thus, although the text streams from these t-wo users are not relevant, CCA still detects that they are highly correlated, which is spurious. The high correlation scores may be caused by similar posting patterns, for exam-ple, when both users post a large number of messages sud-denly at adjacent time points. In the results returned by our methods, this kind of case was very rare. This experiment further demonstrates that CCA can find false correlation-s between text streams, while our method can effectively reduce such correlations, since we use only one projection vector and impose sparse non-negative constraints on it.
In this paper, we proposed an approach to analyzing the lead-lag correlation behavior between two text streams. In addition to finding the correlation, we also discovered the keywords that trigger the correlation. To this end, we for-mulated the problem as a least square regression problem over the projected time series of two text streams. To find the correlated keywords, we imposed non-negative and s-parse constraints over the vector elements. To solve this optimization problem, we reported an accelerated gradien-t algorithm based on Nesterov X  X  method, which is able to find the optimal projection vector. We conducted several experiments and case studies on both synthetic data and re-al data to demonstrate the advantage and capabilities of our approach.

Although our approach has the ability to overcome some drawbacks of CCA and is useful in finding common trends, detecting trend setters and predicting follower links, it still has some limitations. First, our approach treats different keywords equally. However, different keywords may have d-ifferent abilities to trigger topic trends, so it would be more beneficial to assign different weights to different keyword-s. This can be solved by introducing a Bayesian model of regression in which weights can be regularized with hyper-Figure 6: ROC curves of our method and CCA for prediction of follower links. parameters. When there are multiple pairs of text streams, the hyper-parameters can be estimated to adjust the corpus. Second, currently we set the lead-lag time as a constant and find a global lead-lag between two text streams. We are al-so interested in finding local lead-lag patterns at individual time points. We will extend our model to automatically op-timize the time-variant lead-lag parameters to align two text streams.
The authors would like to thank Tianyi Wang for helping us crawl the Weibo data. This research work is supported by 863 Program of China under Grant No. 2012AA011004 and Initiative Scientific Research Program of Tsinghua Uni-versity under Grant No. 20111081023. [1] J. Allan. Topic detection and tracking. chapter [2] F. Bie X mann, J.-M. Papaioannou, M. Braun, and [3] D. M. Blei, A. Y. Ng, and M. I. Jordan. Latent [4] X. Chen, W. Pan, J. T. Kwok, and J. G. Carbonell. [5] J. Duchi, S. Shalev-Shwartz, Y. Singer, and [6] R. F. Engle and C. W. J. Granger. Co-integration and [7] S. Ji and J. Ye. An accelerated gradient method for [8] K. L X e Cao, P. Martin, C. Robert-Grani  X e, and P. Besse. [9] J. Leskovec, L. Backstrom, and J. Kleinberg.
 [10] J. Liu, S. Ji, and J. Ye. Multi-task feature learning via [11] J. Liu and J. Ye. Efficient euclidean projections in [12] A. Nemirovski. Efficient methods in convex [13] Y. Nesterov. Gradient methods for minimizing [14] E. Parkhomenko, D. Tritchler, and J. Beyene.
 [15] E. Parkhomenko, D. Tritchler, and J. Beyene. Sparse [16] S. Shalev-Shwartz and Y. Singer. Efficient learning of [17] X. Shi, R. Nallapati, J. Leskovec, D. A. McFarland, [18] Y. Song, S. Pan, S. Liu, F. Wei, M. X. Zhou, and [19] G. Steeg and A. Galstyan. Inferring predictive links in [20] J. Tang, J. Sun, C. Wang, and Z. Yang. Social [21] R. Tibshirani. Regression shrinkage and selection via [22] X. Wang, C. Zhai, X. Hu, and R. Sproat. Mining [23] X. Wang, K. Zhang, X. Jin, and D. Shen. Mining [24] D. M. Witten, T. Hastie, and R. Tibshirani. A [25] D. M. Witten and R. J. Tibshirani. Extensions of [26] J. Zhang, Y. Song, C. Zhang, and S. Liu. Evolutionary
