 S  X ebastien Bubeck sbubeck@princeton.edu Tengyao Wang tengyaow@princeton.edu Department of Mathematics, Princeton University Nitin Viswanathan nviswana@princeton.edu Department of Computer Science, Princeton University We are interested in the following situation: An agent faces K unknown distributions, and he is allowed to do n sequential evaluations of the form ( i,X ) where i  X  X  1 ,...,K } is chosen by the agent and X is a ran-dom variable drawn from the i th distribution and re-vealed to the agent. The goal of the agent after the n evaluations is to identify a subset of the distributions (or arms in the multi-armed bandit terminology) cor-responding to some prespecified criterion. This set-ting was introduced in (Bubeck et al., 2009), where the goal was to identify the distribution with maxi-mal mean. Note that in this formulation of the prob-lem the evaluation budget n is fixed. Another possi-ble formulation is the one of the PAC model studied in (Even-Dar et al., 2002; Mannor &amp; Tsitsiklis, 2004) where an accuracy  X  and a probability of correctness  X  are prespecified, and one wants to minimize the num-ber of evaluations to attain this prespecified accuracy and probability of correctness. This latter formulation has a long history which goes back to the seminal work (Bechhofer, 1954).
 In this paper we focus on the fixed budget setting of (Bubeck et al., 2009). For this fixed budget problem, (Audibert et al., 2010) proposed a new analysis and an optimal algorithm (up to a logarithmic factor). In par-ticular this work introduced a notion of best arm iden-tification complexity , and it was shown that this quan-tity, denoted H , characterizes the hardness of identi-fying the best distribution in a specific set of K dis-tributions. Intuitively, it was shown that the number of evaluations n has to be  X ( H/ log K ) to be able to find the best arm, and the algorithm SR (Successive Rejects) finds it with O ( H log 2 K ) evaluations. Fur-thermore in the latter paper the authors also suggested the open problem of generalizing the analysis and algo-rithms to the identification of the m distributions with the top m means. Our main contribution is to solve this open problem. We suggest a non-trivial extension of the complexity H , denoted H  X  m  X  , to the problem of identifying the top m distributions, and we introduce a new algorithm, called SAR (Successive Accepts and Rejects), that requires only e O H  X  m  X  evaluations 1 to find the top m arms. We also propose a numerical comparison between SAR, SR and uniform sampling for the problem of finding the m top arms. Interest-ingly the experiments show that SR performs badly for m &gt; 1, which shows that the tradeoffs involved in this generalized problem are fundamentally different from the ones for the single best arm identification. Note that this problem and the associated complexity H  X  m  X  have been studied recently in the PAC model in (Kalyanakrishnan et al., 2012). The present paper is a concurrent and independent work with respect to the latter paper. In particular, the algorithm LUCB designed in (Kalyanakrishnan et al., 2012) is based on fundamentally different ideas (confidence intervals and UCB type algorithms) than the one developed in this paper.
 As a by-product of our new analysis we are also able to solve an open problem of (Gabillon et al., 2011). In this paper the authors studied the setting where the agent faces M distinct best arm identification problems. A multi-bandit identification complexity was introduced, that we denote H [ M ] . On the con-trary to the setting of single best arm identification, here the algorithm proposed in (Gabillon et al., 2011) that needs of order of H [ M ] evaluations to find the best arm in each bandit requires to know the complexity H [ M ] to tune its parameters. Using our SAR machinery, we construct a parameter-free algorithm that identifies the best arm in each bandit Both the m -best arms identification and the multi-bandit best arm identification have numerous poten-tial applications. We refer the interested reader to the previously cited papers for several examples. We adopt the terminology of multi-armed bandits. The agent faces K arms and he has a budget of n evaluations (or pulls ). To each arm i  X  { 1 ,...,K } there is an associated probability distribution  X  i , supported 3 on [0 , 1]. These distributions are unknown to the agent. The sequential evaluations protocol goes as follows: at each round t = 1 ,...,n , the agent chooses an arm I t , and observes a reward drawn from  X  I t independently from the past given I t . In the m -best arms identification problem, at the end of the n evaluations, the agent selects m arms denoted J ,...,J m . The objective of the agent is that the set { J 1 ,...,J m } corresponds to the set of arms with the m highest mean rewards.
 Denote by  X  1 ,..., X  K the mean of the arms. In the following we assume that  X  1 &gt; ... &gt;  X  K . The ordering assumption comes without loss of generality, and the assumption that the means are all distinct is made for sake of notation (the complexity measures are slightly different if there is an ambiguity for the top m means). We evaluate the performance of the agent X  X  strategy by the probability of misidentification, that is Finer measures of performance can be proposed, such as it was argued in (Audibert et al., 2010), for a first order analysis it is enough to focus on the quantity e .
 In the (single) best arm identification, (Audibert et al., 2010) introduced the following complexity measures. Let  X  i =  X  1  X   X  i for i 6 = 1,  X  1 =  X  1  X   X  2 , It is easy to see that these two complexity measures are equivalent up to a logarithmic factor since we have (see (Audibert et al., 2010)) [Theorem 4, (Audibert et al., 2010)] shows that the complexity H 1 represents the hardness of the best arm identification problem. While the proof of the latter result is difficult and technical, it is intuitively obvious that H 1 is a lower bound on the number of evaluations necessary to identify the best arm. Indeed, merely to check if an arm has mean  X  i or  X  , one needs to sample it of order of 1 /  X  2 i times. The surprising fact is that of order of H 1 evaluations suffices to identify the best arm. Note that as far as upper bounds are concerned, the quantity H 2 proved to be a useful surrogate for H 1 to express the bounds on e n .
 For the m -best arms identification problem we define the following gaps and the associated complexity mea-sures: where the notation ( i )  X  { 1 ,...,K } is defined such lower bound to [Theorem 4, (Audibert et al., 2010)] with H 1 replaced by H  X  m  X  1 holds true for the m -best arms identification problem. Again such a statement is pretty much obvious. Indeed assume that we know the critical values  X  m and  X  m +1 . Then for each arm we have an hypothesis testing problem which corresponds to deciding whether  X  i &gt;  X  m +1 (i.e., the arm is in the m best) or  X  i &lt;  X  m (i.e., the arm is not in the m best). For this hypothesis testing problem to be feasible, it is easy to see that one needs of order of 1 /  X   X  m  X  i uations of arm i . While this does not prove the lower bound, it strongly suggests that  X  H  X  m  X  1 evaluations are necessary to identify the top m arms.
 In this paper we shall focus on positive results. In particular we will prove an upper bound on e n that gets small when n = e O H  X  m  X  2 (recall that by (1), e O H  X  m  X  2 = e O H  X  m  X  1 ). This result is derived in Section 3, where we introduce our key algorithmic contribution, the SAR (Successive Accepts and Rejects) algorithm. We also present experiments for this setting in Section 5.
 In Section 4 we consider the framework of multi-bandit introduced in (Gabillon et al., 2011), where the agent faces M distinct best arm identification prob-lems. For sake of notation we assume that each prob-lem m  X  X  1 ,...,M } has the same number of arms K . We also restrict our attention to the single best arm identification within each problem, but we could deal with m -best arms identification within each problem. We denote by  X  1 ( m ) ,..., X  K ( m ) the unknown distri-butions of the arms in problem m . We define simi-larly all the relevant quantities for each problem, that is  X  1 ( m ) &gt; ... &gt;  X  K ( m ) ,  X  1 ( m ) ,...,  X  K and H 2 ( m ). Finally we denote by ( i,m ) the arm i in problem m . In the multi-bandit best arm identifica-tion, the forecaster performs n sequential evaluations of the form ( I t ,m t )  X  X  1 ,...,K } X { 1 ,...,M } . At the end of the n evaluations, the agent selects one arm for each problem, denoted ( J 1 , 1) ,..., ( J M ,M ). The ob-jective of the agent is to find the arm with the highest mean reward in each problem, that is in this setting the probability of misidentification can be written as Following (Gabillon et al., 2011) we introduce the fol-lowing complexity measure Again we define a sort of weaker complexity measure by ordering the gaps. Let be a rearrangement of {  X  i ( m ) : 1  X  i  X  K, 1  X  m  X  M } in ascending order, and let Similarly to the m -best arms identification problem we conjecture that the lower bound [Theorem 4, (Audibert et al., 2010)] with H 1 replaced by H [ M ] holds true for the multi-bandit best arm identifi-cation problem. Here we prove, using a variation of SAR, that n = e O H [ M ] 2 (recall that by (1), e O H [ M ] 2 = e O H [ M ] 1 ) is enough to have a small probability of error e n . This result is derived in Section 4. The improvement with respect to (Gabillon et al., 2011) is that our strategy is parameter-free, while the theoretical Gap-E introduced in (Gabillon et al., 2011) requires the knowledge of H [ M ] 1 to tune its parameter. Moreover the analysis of SAR is much simpler than the one of Gap-E.
 For each arm i and all time rounds t  X  1, we denote by T i ( t ) = P t s =1 1 I t = i the number of times arm i was pulled from rounds 1 to t , and by X i, 1 ,X i, 2 ,...,X i,T the sequence of associated rewards. Introduce tions. Denote by X i,s ( m ) and ing quantities in the multi-bandit problem. In this section we describe and analyze a new algo-rithm, called SAR (Sucessive Accepts and Rejects), for the m -best arms identification problem, see Fig-ure 1 for its precise description. The idea behind SAR is similar to the one for SR (Successive Rejects) that was designed for the (single) best arm identification problem, with the additional feature that SAR some-times accepts an arm because it is confident enough that this arm is among the m top arms. Informally SAR proceeds as follows. First the algorithm divides the time (i.e., the n rounds) in K  X  1 phases. At the end of each phase, the algorithm either accepts the arm with the highest empirical mean or dismisses the arm with the lowest empirical mean, and in both cases the corresponding arm is deactivated. During the next phase, it pulls equally often each active arm. The key to decide whether to accept or reject during a cer-tain phase k is to rely on estimates for the gaps  X   X  m  X  More precisely, assume that the algorithm has already is m ( k ) arms left to find. Then, at the end of phase k , SAR computes for the m ( k ) empirical best arms (among the active arms) the distance (in terms of em-pirical mean) to the ( m ( k ) + 1) th empirical best arm among the active arms. On the other hand for the active arms that are not among the m ( k ) empirical best arms, SAR computes the distance to the m ( k ) th empirical best arm. Finally SAR deactivates the arm i k that maximizes these empirical distances. If i k is currently the empirical best arm, then SAR accepts i k and sets m ( k + 1) = m ( k )  X  1, J m  X  m ( k +1) = i and otherwise it simply rejects i k . The length of the phases are chosen similarly to what was done for the SR algorithm.
 Theorem 1 The probability of error of SAR in the m -best arms identification problem satisfies Proof Consider the event  X  defined by By Hoeffding X  X  Inequality and an union bound, the probability of the complementary event  X   X  can be bounded as follows P (  X   X  )  X  where the last inequality comes from the fact that Thus, it suffices to show that on the event  X  , the al-gorithm does not make any error. We prove this by induction on k . Let k  X  1. Assume the algorithm makes no error in all previous k  X  1 stages, that is no bad arm i &gt; m has been accepted and no good arm i  X  m has been rejected. Note that event  X  implies that at the end of stage k , all empirical means are Let A k = { a 1 ,...,a K +1  X  k } be the the set of active arms during phase k . We order the a i  X  X  such that  X  notation we denote m 0 = m ( k ) for the number of arms that are left to find in phase k . The assumption that no error occurs in the first k  X  1 stages implies that and following two types: 1. The algorithm accepts a j at stage k for some j  X  2. The algorithm rejects a j at stage k for some j  X  Again to slightly shorten the notation we denote  X  =  X  k for the bijection (from { 1 ,...,K + 1  X  k } to A k ) such that Suppose Type 1 error occurs. Then a j =  X  (1) since if the algorithm accepts, it must accept the empirical best arm. Furthermore we also have b  X  since otherwise the algorithm would rather reject arm  X  ( K + 1  X  k ). The condition a j =  X  (1) and the event  X  implies that b  X   X   X  a We then look at the condition (2). In the event of  X  , for all i  X  m 0 , we have So there are m + 1 arms in A k (namely a ,a 2 ,...,a m 0 ,a j ) whose empirical means b  X  b  X  Therefore, using those two observations and (2) we deduce  X   X  m  X  Thus so far we proved that if there is a Type 1 error, then But at stage k , only k  X  1 arms have been accepted  X  error does not occur.
 Suppose Type 2 error occurs. The reasoning is symmetric to Type 1. In fact, if we rephrase the problem as finding the K  X  m worst arms instead of the m best arms, this is exactly the same as Type 1 error. Hence Type 2 error cannot occur as well. This completes the induction and consequently the proof of the theorem. In this section we use the idea of SAR for multi-bandit best arm identification. Here at the end of each phase we estimate the gaps  X  i ( m ) within each problem, and we reject the arm with the largest such estimated gap. Moreover if a problem is left with only one active arm, then this arm is accepted and the problem is deacti-vated. The corresponding strategy is described pre-cisely in Figure 2 Theorem 2 The probability of error of SAR in the multi-bandit best arm identification problem satisfies Proof Consider the event  X  defined by  X  =  X  1  X  i  X  K, 1  X  m  X  M, 1  X  k  X  MK  X  1 , Following the same reasoning as in the proof of The-orem 1, it suffices to show that in the event of  X  the algorithm makes no error. We do this by induction on the phase k of the algorithm. Let k  X  1. Assume the algorithm makes no error in all previous k  X  1 stages. Then at phase k , for each active problem m , the arm (1 ,m ) is still active. Moreover, as only k  X  1 arms have been deactivated, one clearly has Suppose the above maximum is achieved for the arm ( i ,m  X  ), so we have Assume now that the algorithm makes an error at the end of phase k , i.e. some arm (1 ,m ) is deactivated and it was not the last active arm in problem m . For this to happen, we necessarily have for some j  X  { 2 ,...,K } (e.g., j = h k ( m )), b Clearly on the event  X  one has On the other hand, using (3) and  X  , one has b Therefore, b  X  1 ,n k ( m ), contradicting (4). This completes the induction and the proof. In this section we revisit the simple experiments of (Audibert et al., 2010) in the setting of multiple iden-tifications. Since our objective is simply to illustrate our theoretical analysis we focus on the m -best arms identification problem, but similar numerical simula-tions could be conducted in the multi-bandit setting and compared to the results of (Gabillon et al., 2011). We compare our proposed strategy SAR to three com-petitors: The uniform sampling strategy that divides evenly the allocation budget n between the K arms, and then return the m arms with the highest empir-ical mean (see (Bubeck et al., 2011) for a discussion of this strategy in the single best arm identification). The SR strategy is the plain Successive Rejects strat-egy of (Audibert et al., 2010) which was designed to find the (single) best arm. We slightly improve it for m -best identification by running only K  X  m  X  1 phases (while still using the full budget n ) and then return-ing the last m surviving arms. Finally we consider the extension of UCB-E to the m -best arms identifi-cation problem, which is based on a similar idea than the extension Gap-E of (Gabillon et al., 2011) for the multi-bandit best arm identification, see Figure 3 for the details. Note that this last algorithm requires to know the complexity H  X  m  X  1 . One could propose an adaptive version, using ideas described in (Audibert et al., 2010), but for sake of simplicity we restrict our attention to the non-adaptive algorithm.
 In our experiments we consider only Bernoulli distri-butions, and the optimal arm always has parameter 1 / 2. Each experiment corresponds to a different sit-uation for the gaps, they are either clustered in few groups, or distributed according to an arithmetic or geometric progression. For each experiment we plot the probability of misidentification for each strategy, varying m between 2 and K  X  1. The allocation budget for each experiment is chosen to be roughly equal to 4. The parameters for the experiments are as follows:  X  Experiment 1: One group of bad arms, K = 20,  X  Experiment 2: Two groups of bad arms, K = 20,  X  Experiment 3: Geometric progression, K = 4,  X  Experiment 4: 6 arms divided in three groups,  X  Experiment 5: Arithmetic progression, K = 15,  X  Experiment 6: Three groups of bad arms, K = 30, It is interesting to note that SR performs badly for m -best arms identification when m &gt; 1, as it has even worse performances than the naive uniform sampling in many cases. This shows that the tradeoffs involved in finding the single best arm and finding the top m arms are fundamentally different. As expected SAR always outperforms uniform sampling, and Gap-E has slightly better performances than SAR (but Gap-E re-quires an extra information to tune its parameter, and the adapative version comes with no provable guaran-tee).
 Audibert, J.-Y., Bubeck, S., and Munos, R. Best arm identification in multi-armed bandits. In Proceedings of the 23rd Annual Conference on Learning Theory (COLT) , 2010.
 Bechhofer, R. E. A single-sample multiple decision procedure for ranking means of normal populations with known variances. Annals of Mathematical Statistics , 25:16 X 39, 1954.
 Bubeck, S., Munos, R., and Stoltz, G. Pure explo-ration in multi-armed bandits problems. In Proceed-ings of the 20th International Conference on Algo-rithmic Learning Theory (ALT) , 2009.
 Bubeck, S., Munos, R., and Stoltz, G. Pure ex-ploration in finitely-armed and continuously-armed bandits. Theoretical Computer Science , 412:1832 X  1852, 2011.
 Even-Dar, E., Mannor, S., and Mansour, Y. Pac bounds for multi-armed bandit and markov deci-sion processes. In Proceedings of the Fifteenth An-nual Conference on Computational Learning Theory (COLT) , 2002.
 Gabillon, V., Ghavamzadeh, M., Lazaric, A., and Bubeck, S. Multi-bandit best arm identification. In
Advances in Neural Information Processing Systems (NIPS) , 2011.
 Kalyanakrishnan, S., Tewari, A., Auer, P., and Stone,
P. Pac subset selection in stochastic multi-armed bandits. In Proceedings of the 29th International Conference on Machine Learning (ICML) , 2012.
 Mannor, S. and Tsitsiklis, J. N. The sample complex-ity of exploration in the multi-armed bandit prob-lem. Journal of Machine Learning Research , 5:623 X  648, 2004.

