 Chien-Lin Tseng 2 Shou-De Lin 1,2 Micro-blogging services such as Twitter 1 , Plurk 2 , and Jaiku 3 , are platforms that allow users to share immediate but short messages with friends. Gener-ally, the micro-blogging services possess some signature properties that differentiate them from conventional weblogs and fo rum. First, microblogs deal with almost real-time messaging, including instant information, expression of feelings, and immediate ideas. It also provides a source of crowd intelligence that can be u sed to investigate com-mon feelings or potential tr ends about certain news or concepts. However, this real-time property can lead to the production of an enormous number of messages that recipients must digest. Second, mi-cro-blogging is time-traceable . The temporal in-formation is crucial because contextual posts that appear close together are, to some extent, correlat-ed. Third, the style of micro-blogging posts tends to be conversation-based with a sequence of re-sponses. This phenomenon indicates that the posts and their responses are highly correlated in many respects. Fourth, micro-blogging is friendship-influenced . Posts from a particular user can also be viewed by his/her friends and might have an im-pact on them (e.g. the empathy effect) implicitly or explicitly. Therefore, posts from friends in the same period may be correlated sentiment-wise as well as content-wise. 
We leverage the above pr operties to develop an automatic and intuitive Web application, Me-meTube, to analyze and display the sentiments be-hind messages in microblogs. Our system can be regarded as a sentiment-driven, music-based sum-marization framework as well as a novel audiovis-ual presentation of art. MemeTube is designed as a search-based tool. The system flow is as shown in Figure 1. Given a query (either a keyword or a user id), the system first extracts a series of relevant posts and replies based on keyword matching. Then sentiment analysis is applied to determine the sentiment of the posts. Next a piece of music is composed to reflect the detected sentiments. Final-ly, the messages and music are fed into the anima-tion generation model, which displays a piano keyboard that plays automatically. 
The contributions of this work can be viewed from three different perspectives.  X  From system perspective of view, we demo a  X  Technically, we integrate a language-model- X  Conceptually, our system demonstrates that, Related works can be divided into two parts: sen-timent classification in microblogs, and sentiment-based audiovisual presentation for social media. For the first part, most of related literatures focus on exploiting different classification methods to separate positive and negative sentiments by a va-riety of textual and linguistics features, as shown in Table 1. Their accuracy ranges from 60%~85% depending on different setups. The major differ-ence between our work and existing approaches is that our model considers three kinds of additional information (i.e., contextual, response and friend-ship information) for sentiment recognition. tigated integrating emotions and music in certain media applications. For example, Ishizuka and Onisawa (2006) generated va riations of theme mu-sic to fit the impressions of story scenes represent-ed by textual content or pictures. Kaminskas (2009) aligned music with user-selected points of interests for recommendation. Li and Shan (2007) produced painting slideshows with musical accompaniment. Hua et al. (2004) proposed a Photo2Video system that allows users to specify incident music that ex-presses their feelings about the photos. To the best of our knowledge, MemeTube is the first attempt to exploit AI techniques to create harmonic audio-visual experiences and interactive emotion-based summarization for microblogs. First, we develop a classification model as our basic sentiment recognition mechanism. Given a training corpus of posts and responses annotated with sentiment labels, we train an n-gram language model for each sentiment. Then, we use such mod-el to calculate the probability that a post expresses the sentiment s associated with that model: where w is the sequence of words in the post. We also use the common Laplace smoothing method. 
For each post p and each sentiment s  X  S , our classifier calculates the probability that such post expresses the sentiment  X  X  X  X  X  X  X  using Bayes rule:  X  X  X  X  X  X  is estimated directly by counting, while  X  X  X  X  X  X  X  can be derived by using the learned lan-guage models. This allow us to produce a distribu-tion of sentiments for a given post p , denoted as  X   X  . 
However, the major challenge in the microblog sentiment detection task is that the length of each post is limited (i.e., pos ts on Twitter are limited to 140 characters). Consequently, there might not be enough information for a sentiment detection sys-tem to exploit. To solve this problem, we propose to utilize the three types of information mentioned earlier. We discuss each type in detail below. 3.1 Response Factor We believe the sentiment of a post is highly corre-lated with (but not necessary similar to) that of re-sponses to the post. For example, an angry post usually triggers angry responses, but a sad post usually solicits supportive responses. We propose to learn the correlation patterns of sentiments from the data and use them to improve the recognition. 
To achieve such goal, from the data, we learn which represents the cond itional probability of a post given responses. Then we use such probability to construct a transition matrix  X   X  , where  X   X  where  X   X  denotes the original sentiment distribu-tion of the post, and  X   X  tion of the  X   X  X  X  response determined by the abovementioned language m odel approach. In ad-dition,  X   X  weight of the response since it is preferable to as-sign higher weights to closer responses. There is also a global parameter  X  that determines how much the system should trust the information de-rived from the responses to the post. If there is no response to a post, we simply assign  X  X   X   X  X   X  . 3.2 Context Factor It is assumed that the sentiment of a microblog post is correlated with the author X  X  previous posts (i.e., the  X  X ontext X  of the post). We also assume that, for each person, there is a sentiment transition matrix  X   X  that represents how his/her sentiments change over time. The  X  X , X  X   X  X  X  element in  X   X  repre-sents the conditional probability from the senti-ment of the previous post to that of the current post:  X   X   X  X  X  X  X  X  X  X  X   X   X   X   X   X  X  |  X  X  X  X  X  X  X  X  X   X   X   X  X  X   X   X  X   X  .
The diagonal elements stand for the consistency of the emotion state of a person. Conceivably, a capricious person X  X  diagnostic  X   X  lower than those of a calm person. The matrix  X  can be learned directly from the annotated data. 
Let  X   X  represent the detected sentiment distribu-tion of an existing post at time t. We want to adjust  X  based on the previous posts from  X  X   X   X  to  X  , where  X   X  is a given temporal threshold. The sys-tem first extracts a set of posts from the same au-thor posted from time  X  X   X   X  to  X  and determines their sentiment distributions  X  X   X  where  X  X   X   X  X  X   X   X ,  X   X ,...,  X   X  X  using the same classifier. Then, the system utilizes the following update equation to obtain an adjusted sentiment distribution  X  X   X  : where  X   X  are defined similar to the previous case. If there is no post in the defined interval, the system will leave  X   X  unchanged. 3.3 Friendship Factor We also assume that the friends X  emotions are cor-related with each other. This is because friends affect each other, and they are more likely to be in the same circumstances, and thus enjoy/suffer sim-ilarly. Our hypothesis is that the sentiment of a post and the sentiments of the author X  X  friends X  recent posts might be correlated. Therefore, we can treat the friends X  recent posts in the same way as the recent posts of the author, and learn the transi-tion matrix  X   X  , where  X   X  nique proposed in the previous section to improve the recognition accuracy. 
However, it is not necessarily true that all friends have similar emot ional patterns. One X  X  sen-timent transition matrix  X   X  might be very different from that of the other, so we need to be careful when using such information to adjust our recogni-tion outcomes. We propose to only consider posts from friends with similar emotional patterns. 
To achieve our goal, we first learn every user X  X  contextual sentiment transition matrix  X   X  from the data. In  X   X  , each row represents a distribution that sums to one; therefore, we can compare two ma-trixes  X   X  KL-divergence of each row. That is, 
Two persons are considered as having similar emotion pattern if their contextual sentiment transi-tion matrixes are similar. After a set of similar friends are identified, their recent posts (i.e., from  X  X   X   X  to  X  ) are treated in the same way as the posts by the author, and we use the method pro-posed previously to fine-tune the recogni-tion outcomes. For each microblog post retrieved according to the query, we can derive its sentiment distribution (as a vector of probabilities) by using the above meth-od. Next, the system transforms every sentiment distribution into an affective vector comprised of a valence value and an arousal value. The valence value represents the positive-to-negative sentiment, while the arousal value re presents the intense-to-silent level. 
We exploit the mapping from each type of sen-timent to a two-dimensional affective vector based on the two-dimensional emotion model of Russell (1980). Using the model we extract the affective score vectors of the six emotions (see Table 2) used in our experiments. The mapping enables us to transform a sentiment distribution  X   X  into an affective score vector by weighted sum approach. For example, given a distribution of (Anger=20%, Surprise=20%,Disgust=10%, Fear=10%, Joy=10%, Sadness=30%), the two-dimensional affective vec-tor can be computed as 0.2*(-0.25, 1) + 0.2*(0.5, 0.75) + 0.1*(-0.75, -0.5) + 0.1*(-0.75, 0.5) + 0.1*(1, 0.25) + 0.3*(-1, -0.25). Finally, the affec-tive vector of each post will be summed to repre-sent the sentiment of the given query in terms of the valence and arousal values. 
Next the system transforms the affective vector into music elements through chord set selection (based on the valence value) and rhythm determi-nation (based on the arousal value). For chord set selection, we design nine basic chord sets as {A, Am, Bm, C, D, Dm, Em, F, G}, where each chord set consists of some basic notes. The chord sets are used to compose twenty chord sequences. Half of the chord sequences are used for weakly positive to strongly positive sentiments and the other half are used for weakly negative to strongly negative sen-timents. The valence value is therefore divided into twenty levels, and gradually shifts from strongly positive to strongly negative. The chord sets ensure that the resulting auditory presentation is in har-mony (Hewitt 2008). For rhythm determination, we divide the arousal values into five levels to de-cide the tempo/speed of the music. Higher arousal values generate music with a faster tempo while lower ones lead to slow and easy-listening music. In this final stage, our system produces real-time animation for visualization. The streams of mes-sages are designed to flow as if they were playing a piece of a piano melody. We associate each mes-sage with a note in the generated music. When a post message flows from right to left and touches a piano key, the key itself blinks once and the corre-sponding tone of the key is produced. The message flow and the chord/rhythm have to be synchro-nized so that it looks as if the messages themselves are playing the piano. The system also allows users to highlight the body of a message by moving the cursor over the flowing message. A snapshot is shown in Figure 2 and the sequential snapshots of the animation are shown in Figure 3. We collect the posts and responses from every ef-fective user, users with mo re than 10 messages, of Plurk from January 31 st to May 23 rd , 2009. In order to create the diversity for the music generation sys-tem, we decide to use six different sentiments, as shown in Table 2, rather than using only three sen-timent types, positive, negative and neutral, as most of the systems in Table 1 have used. The sen-timent of each sentence is labeled automatically using the emoticons. This is similar to what many people have proposed for evaluation (Davidov et al. 2010; Sun et al. 2010; Bifet and Frank 2010; Go et al. 2009; Pak and Paroubek 2010; Chen et al. 2010). We use data from January 31 st to April 30 th the purpose of observing the result of using the three factors, we filter the users without friends, the posts without responses, and the posts without previous post in 24 hour in testing data. We also manually label the sentiments on the testing data (totally 1200 posts, 200 posts for each sentiment). 
We use three metrics to evaluate our model: ac-curacy, Root-Mean-Square Error for valence (de-noted by RMSE( V) ) and RMSE for arousal (denoted by RMSE( A) ). The RMSE values are generated by comparing the affective vector of the predicted sentiment distribution with the affective vector of the answer. Our basic model reaches 33.8% in accuracy, 0.78 in the RMSE(V) and 0.64 in RMSE(A). Note that RMSE  X  0.5 means that there is roughly one quarter (25%) error in the va-lence/arousal values as they range from [-1,1]. 
Note that the main reason the accuracy is not ex-tremely high is that we are dealing with 6 classes. When we combine angry, disgust, fear, and sad-ness into one negative sense and the rest as posi-tive senses, our system reaches 78.7% in accuracy, which is competitive to the state-of-the-art algo-rithms as shown in the related work section. How-ever, doing such generalization will lose the flexibility of producing more fruitful and diverse pieces of music. Therefore we choose more fine-grained classes for our experiment. 
Figure 3 shows the results of exploiting the re-sponse, context, and friendship. Note RMSE  X  0.5 means that there is roughly one quarter (25%) error in the valence/arousal values as they range from [-1,1]. The results show that considering all three additional factors can achieve the best results and decent improvement over the basic LM model. (note that for RMSE, the lower value the better) We create video clips of five different queries for demonstration, which is downloadable from: http://mslab.csie.ntu.edu.tw/memetube/demo/ . This demo page contains the resulting clips of four keyword queries (including football , volcano , Monday , big bang ) and a user id query mstcgeek . Here we briefly describe each case. (1) The video for query term, football , was recorded on February 7 2011, results in a relatively positive and ex-tremely intense atmosphere. It is reasonable be-cause the NFL Super Bowl was played on February 6 th , 2011. The valence value is not as high as the arousal value because some fans might not be very happy to see their favorite team losing the game. (2) The query, volcano , was also record-ed on February 7 th 2011. The resulting video ex-presses negative valence and neutral arousal. After checking the posts, we have learned that it is be-cause the Japanese volcano Mount Asama has con-tinued to erupt. Some users are worried and discussed about the potential follow-up disasters. (3) The query Monday was performed on February 6 2011, which is a Sunday night. The negative valence reflects the  X  X lue Monday X  phenomenon, which leads to some heavy, less smooth melody. (4) The term big bang turns out to be very positive on both valence and arousal, mainly because, besides its relatively neutral meaning in physics, this term also refers to a famous comic show that some peo-ple in Plurk love to watch. We also use one user id as query: the user-id mstcgeek is the official ac-count of Microsoft Taiwan. This user often uses cheery texts to share some videos about their prod-ucts or provide some discounts of their product, which leads to relatively hyped music. Microblog, as a daily journey and social network-ing service, generally captures the dynamics of the change of feelings over time of the authors and their friends. In MemeTube, the affective vector is generated by aggregating the sentiment distribution of each post; thus, it represents the majority X  X  opin-ion (or sentiment) about a topic. In this sense, our system can be regarded as providing users with an audiovisual experience to learn collective opinion of a particular topic. It also shows how NLP tech-niques can be integrated with knowledge about music and visualization to create a piece of inter-esting network art work. Note that MemeTube can be regarded as a flexible framework as well since each component can be further refined inde-pendently. Therefore, our future works are three-fold: For sentiment analysis, we will consider more sophisticated ways to improve the baseline accura-cy and to aggregate individual posts into a collec-tive consensus. For music generation, we plan to add more instruments and exploit learning ap-proaches to improve the selection of chords. For visualization, we plan to add more interactions be-tween music, sentiments, and users. 
