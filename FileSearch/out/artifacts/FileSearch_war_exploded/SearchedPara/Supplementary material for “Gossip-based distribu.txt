 Proof. The lemma can be proved using induction. First of all, note that
E where the last equality follows because of Lemma 3 since N simply from Lemma 3 .
 where ( 11 ) follows from Lemma 4 and the Cauchy-Schwarz inequality. e ff ort. In accordance with (A), assume that t  X  cK/ ( d 2 N ), use t = cK Now, the probability of choosing some arm i in iteration t at peer p is where i  X  = arg max 1  X  i  X  K  X  i . The second term can be decomposed as to apply the separation mentioned at the beginning of proof. Now let C i we bound the first term of ( 10 )by the th reward received on arm i in the whole network, one can show that previous choices of the policy. All that left is to bound P x 0  X   X  i and therefore, applying Bernstein X  X  inequality, one gets T 2 one can use an argument similar to the one applied in Lemma 6 and obtain for to have expected value E z i E [ T 3 ] and E [ T 4 ] can be found in the same way using Lemma 5 . Therefore, Summing up, for an arm i = i  X  , at any peer j and in iteration t , we have Algorithm 2 P2P--greedy.slim at peer j in iteration t 4: else 7: else 10: Pull arm I j,t and receive reward  X  j,t 12: The model to be sent is M j,t +1 13: else 15: With probability 1  X  t let I = I { c j,t d 16: Pull arm I and receive reward  X  j,t 17: if I = I j,t then The arm chosen is based on the model received 20: else 24: The model to be sent is M j,t +1 25: else 26: The model to be sent is M j,t +1 28: c =(1 / 2)( c + c ), d =(1 / 2)( d + d ) 29: r =(1 / 2)( r + r ), q =(1 / 2)( q + q ) 30: f =(1 / 2)( a + a ), g =(1 / 2)( b + b ) 31: I = I = I 32: return M =( I, c, d, r, q, a, b ) 33: function UPDATE(( M =( I, c, d, r, q, a, b ) ,  X  ,t )) 34: if t is power of 2 then 35: c = c + r, d = d + q 36: r = a, q = b 37: a = b =0 38: a = a + N  X  39: b = b + N 40: return M 41: function STEP(( M =( I, c, d, r, q, a, b ) ,t )) 42: if t is power of 2 then 43: c = c + r, d = d + q 44: r = a, q = b 45: a = b =0 46: return M E.1. The cost of information spreading unavoidable in some settings.
 is at least tN/ 2 in iterations t =1 ,..., log pulls.
 E.2. Detereministic algorithms can be suboptimal change significantly.) a regret of size  X  ( NK ) =  X  ( N 2 ) .
 in parallel, leading to an unnecessarily large regret.
 E.3. The necessity of using delays peer j uses s i 1 applied in our P2P--Greedy is really crucial.
 on what causes this really big di ff erence in the performance. the arms have Bernoully distribution with parameter 0 . 8 .  X  previous rewards at any given peer) even after log( N/ 2) iterations. t Thus  X  causes at  X  ( during the following log these rounds, the cumulated regret during these rounds will be  X  ( The e ff ect is thus similar to the one described in Example 8 .
