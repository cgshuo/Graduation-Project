 Clustering algorithms group data items into categories without requiring human supervision or def-inition of categories. They are often the first tool used when exploring new data. A great number of clustering principles have been proposed, most of which can be described as either generative or discriminative in nature. Generative clustering algorithms provide constructive definitions of categories in terms of their geometric properties in a feature space or as statistical processes for generating data. Examples include k-means and Gaussian mixture model clustering. In order for generative clustering to be practical, restrictive assumptions must be made about the underlying category definitions.
 Rather than modeling categories explicitly, discriminative clustering techniques represent the boundaries or distinctions between categories. Fewer assumptions about the nature of categories are made, making these methods powerful and flexible in real world applications. Spectral graph partitioning [1] and maximum margin clustering [2] are example discriminative clustering methods. A disadvantage of existing discriminative approaches is that they lack a probabilistic foundation, making them potentially unsuitable in applications that require reasoning under uncertainty or in data exploration.
 We propose a principled probabilistic approach to discriminative clustering, by formalizing the problem as unsupervised learning of a conditional probabilistic model. We generalize the work of Grandvalet and Bengio [3] and Bridle et al. [4] in order to learn probabilistic classifiers that are appropriate for multi-class discriminative clustering, as explained in Section 2. We identify two fundamental, competing quantities, class balance and class separation, and develop an information theoretic objective function which trades off these quantities. Our approach corresponds to maximizing mutual information between the empirical distribution on the inputs and the induced label distribution, regularized by a complexity penalty. Thus, we call our approach Regularized Information Maximization (RIM).
 In summary, our contribution is RIM, a probabilistic framework for discriminative clustering with a number of attractive properties. Thanks to its probabilistic formulation, RIM is flexible: it is compatible with diverse likelihood functions and allows specification of prior assumptions about expected class proportions. We show how our approach leads to an efficient, scalable optimization procedure that also provides a means of automatic model selection (determination of the number of clusters). RIM is easily extended to semi-supervised classification. Finally, we show that RIM performs better than competing approaches on several real-world data sets. Suppose we are given an unlabeled dataset of N feature vectors (datapoints) X = ( x 1 ,  X  X  X  , x N ) , where x i = ( x i 1 , . . . , x iD ) T  X  R D are D -dimensional vectors with components x id . Our goal is to learn a conditional model p ( y | x , W ) with parameters W which predicts a distribution over label values y  X  X  1 , . . . , K } given an input vector x .
 Our approach is to construct a functional F ( p ( y | x , W ); X ,  X  ) which evaluates the suitability of p ( y | x , W ) as a discriminative clustering model. We then use standard discriminative classifiers such as logistic regression for p ( y | x , W ) , and maximize the resulting function F ( W ; X ,  X  ) over the parameters W .  X  is an additional tuning parameter that is fixed during optimization. criminative model X  X  decision boundaries should not be located in regions of the input space that are densely populated with datapoints. This is often termed the cluster assumption [5], and also corre-sponds to the idea that datapoints should be classified with large margin. Grandvalet &amp; Bengio [3] assumption when training probabilistic classifiers with partial labels. However, in the case of fully unsupervised learning this term alone is not enough to ensure sensible solutions, because conditional entropy may be reduced by simply removing decision boundaries and unlabeled categories tend to be removed. We illustrate this in Figure 1 (left) with an example using the multilogit regression classifier as the conditional model p ( y | x , W ) , which we will develop in Section 3. In order to avoid degenerate solutions, we incorporate the notion of class balance: we prefer con-figurations in which category labels are assigned evenly across the dataset. We define the empirical label distribution which is an estimate of the marginal distribution of y . A natural way to encode our preference towards class balance is to use the entropy H {  X  p ( y ; W ) } , because it is maximized when the labels are uniformly distributed. Combining the two terms, we arrive at which is the empirical estimate of the mutual information between x and y under the conditional model p ( y | x , W ) .
 fiers without supervision. However, they note that I W { y ; x } may be trivially maximized by a con-with this objective tend to fragment the data into a large number of categories, see Figure 1 (center). We therefore introduce a regularizing term R ( W ;  X  ) whose form will depend on the specific choice of p ( y | x , W ) . This term penalizes conditional models with complex decision boundaries in order to yield sensible clustering solutions. Our objective function is and we therefore refer to our approach as Regularized Information Maximization (RIM), see Figure 1 (right). While we motivated this objective with notions of class balance and seperation, our approach may be interpreted as learning a conditional distribution for y that preserves information from the data set, subject to a complexity penalty. Figure 1: Example unsupervised multilogit regression solutions on a simple dataset with three clus-ters. The top and bottom rows show the category label arg max y p ( y | x , W ) and conditional entropy H { p ( y | x , W ) } at each point x , respectively. We find that both class balance and regularization terms are necessary to learn unsupervised classifiers suitable for multi-class clustering. The RIM framework is flexible in the choice of p ( y | x ; W ) and R ( W ;  X  ) . As an example instan-tiation, we here choose multiclass logistic regression as the conditional model. Specifically, if K is the maximum number of classes, we choose bias values b k for each class k . Each weight vector w k  X  R D is D -dimensional with components w kd . The regularizer is the squared L 2 norm of the weight vectors, and may be interpreted as an isotropic normal distribution prior on the weights W . The bias terms are not penalized. In order to optimize Eq. 2 specialized with Eqs. 3, we require the gradients of the objective function. Naive computation of the gradient requires O ( N K 2 D ) , since there are K ( D + 1) parameters and each derivative requires a sum over N K terms. However, the form of the conditional probability derivatives for multi-logit regression are: where  X  kc is equal to one when indices k and c are equal, and zero otherwise. When these expres-sions are substituted into Eq. 4, we find the following expressions: Computing the gradient requires only O ( N KD ) operations since the terms P c p ci log p ci  X  p computed once and reused in each partial derivative expression.
 The above gradients are used in the L-BFGS [6] quasi-Newton optimization algorithm 1 . We find em-pirically that the optimization usually converges within a few hundred iterations. When specialized Figure 2: Demonstration of model selection on the toy problem from Figure 1. The algorithm is initialized with 50 category weight vectors w k . Upon convergence, only three of the categories are populated with data examples. The negative bias terms of the unpopulated categories drive the unpopulated class probabilities  X  p k towards zero. The corresponding weight vectors w k have norms near zero. to multilogit regression, the objective function F ( W ; x ,  X  ) is non-concave. Therefore the algorithm can only be guaranteed to halt at locally optimal stationary points of F . In Section 3.1, we explain how we can obtain an initialization that is robust against local optima. 3.1 Model Selection Setting the derivatives (Eq. 5) equal to zero yields the following condition at stationary points of F : where we have defined The L 2 regularizing function R ( W ;  X  ) in Eq. 3 is additively composed of penalty terms associated of the penalty term w T k w k when datapoints are not assigned to category k ; that is, when  X  p k =
P w unpopulated categories.
 We find empirically that when we initialize with a large number of category weights w k , many de-cay away depending on the value of  X  . Typically as  X  increases, fewer categories are discovered. This may be viewed as model selection (automatic determination of the number of categories) since the regularizing function and parameter  X  may be interpreted as a form of prior on the weight pa-rameters. The bias terms b k are unpenalized and are adjusted during optimization to drive the class probablities  X  p k arbitrarily close to zero for unpopulated classes. This is illustrated in Figure 2. This behavior suggests an effective initialization procedure for our algorithm. We first oversegment the data into a large number of clusters (using k-means or other suitable algorithm) and train a supervised multi-logit classifier using these cluster labels. (This initial classifier may be trained with a small number of L-BFGS iterations since it only serves as a starting point.) We then use this classifier as the starting point for our RIM algorithm and optimize with different values of  X  in order to obtain solutions with different numbers of clusters. The stationary conditions have another interesting consequence. Equation 6 indicates that at sta-tionary points, the weights are located in the span of the input datapoints. We use this insight as justification to define explicit coefficients  X  ki and enforce the constraint w k = P i  X  ki x i during optimization. Substituting this equation into the multilogit regression conditional likelihood allows function that evaluates the inner product x T i x . The conditional model now has the form Substituting the constraint into the regularizing function P k w T k w k yields a natural replacement of w k w k by the Reproducing Hilbert Space (RKHS) norm of the function P i  X  ki K ( x i ,  X  ) : We use the L-BFGS algorithm to optimize the kernelized algorithm over the coefficients  X  ki and biases b k . The partial derivatives for the kernel coefficients are and the derivatives for the biases are unchanged. The gradient of the kernelized algorithm requires O ( KN 2 ) to compute. Kernelized unsupervised multilogit regression exhibits the same model selection behavior as the linear algorithm. We now discuss how RIM can be extended to semi-supervised classification, and to encode prior assumptions about class proportions. 5.1 Semi-supervised Classification In semi-supervised classification, we assume that there are unlabeled examples X U = 1 ,  X  X  X  , x We again use mutual information I W { y ; x } (Eq. 1) to define the relationship between unlabeled points and the model parameters, but we incorporate an additional parameter  X  which will define the tradeoff between labeled and unlabeled examples. The conditional likelihood is incorporated for labeled examples to yield the semi-supervised objective: The gradient is computed and again used in the L-BFGS algorithm in order to optimize this com-bined objective. Our approach is related to the objective in [3], which does not contain the class balance term H ( X  p ( y ; W )) . 5.2 Encoding Prior Beliefs about the Label Distribution So far, we have motivated our choice for the objective function F through the notion of class balance. However, in many classification tasks, different classes have different number of members. In the following, we show how RIM allows flexible expression of prior assumptions about non-uniform class label proportions.
 First, note that the following basic identity holds dropping the constant log( K ) yields another interpretation of the objective The term  X  KL {  X  p ( y ; W ) || U } is maximized when the average label distribution is uniform. We can capture prior beliefs about the average label distribution by substituting a reference distribution D ( y ;  X  ) in place of U (  X  is a parameter that may be fixed or optimized during learning). [7] also use relative entropy as a means of enforcing prior beliefs, although not with respect to class distributions in multi-class classification problems.
 This construction may be used in a clustering task in which we believe that the cluster sizes obey a power law distribution as, for example, considered by [8] who use the Pitman-Yor process for nonparametric language modeling. Simple manipulation yields the following objective: fore find that label distribution priors may be incorporated using an additional cross entropy regu-larization term. Figure 3: Unsupervised Clustering: Adjusted Rand Index (relative to ground truth) versus number of clusters. We empirically evaluate our RIM approach on several real data sets, in both fully unsupervised and semisupervised configurations. 6.1 Unsupervised Learning Kernelized RIM is initialized according to the procedure outlined in Section 3.1, and run until L-BFGS converges. Unlabeled examples are then clustered according to arg max k p ( y = k | x , W ) . We compare RIM against the spectral clustering (SC) algorithm of [1], the fast maximum margin clustering (MMC) algorithm of [9], and kernelized k-means [10]. MMC is a binary clustering algo-rithm. We use the recursive scheme outlined by [9] to extend the approach to multiple categories. The MMC algorithm requires an initial clustering estimate for initialization, and we use SC to pro-vide this.
 We evaluate unsupervised clustering performance in terms of how well the discovered clusters reflect known ground truth labels of the dataset. We report the Adjusted Rand Index (ARI) [11] between an inferred clustering and the ground truth categories. ARI has a maximum value of 1 when two clus-terings are identical. We evaluated a number of other measures for comparing clusterings to ground truth including mutual information, normalized mutual information [12], and cluster impurity [13]. We found that the relative rankings of the algorithms were the same as indicated by ARI. We evaluate the performance of each algorithm while varying the number of clusters that are dis-covered, and we plot ARI for each setting. For SC and k-means the number of clusters is given as an input parameter. MMC is evaluated at { 2 , 4 , 8 ,  X  X  X } clusters (powers of two, due to the recursive scheme.) For RIM, we sweep the regularization parameter  X  and allow the algorithm to discover the final number of clusters.
 Image Clustering. We test the algorithms on an image clustering task with 350 images from four Caltech-256 [14] categories (Faces-Easy, Motorbikes, Airplanes, T-Shirt) for a total of N = 1400 images. We use the Spatial Pyramid Match kernel [15] computed between every pair of images. We sweep RIM X  X   X  parameter across [ 0 . 125 N , 4 N ] . The results are summarized in figure 3. Overall, the clusterings that best match ground truth are given by RIM when it discovers four clusters. We find that RIM outperforms both SC and MMC at all settings. RIM outperforms kernelized k-means when discovering between 4 and 8 clusters. Their performances are comparable for other numbers of clusters. Figure 4 shows example images taken from clusters discovered by RIM. Our RIM implementation takes approximately 110 seconds per run on the Caltech Images datset on a quad core Intel Xeon server. SC requires 38 seconds per run, while MMC requires 44-51 seconds per run depending on the number of clusters specified.
 Molecular Graph Clustering. We further test RIM X  X  unsupervised learning performance on two molecular graph datasets. D&amp;D [16] contains N = 1178 protein structure graphs with binary ground truth labels indicating whether or not they function as enzymes. NCI109 [17] is composed of N = 4127 compounds labeled according to whether or not they are active in an anti-cancer screening. We use the subtree kernel developed by [18] with subtree height of 1. For D&amp;D, we sweep RIM X  X  lambda parameter through the range [ 0 . 001 N , 0 . 05 N ] and for NCI we sweep through the interval [ 0 . 001 N , 1 N ] . Results are summarized in Figures 3 (center and right). We find that of all methods, RIM produces the clusterings that are nearest to ground truth (when discovering 2 clusters Figure 4: Left: Randomly chosen example images from clusters discovered by unsupervised RIM on Caltech Image. Right: Semi-supervised learning on Caltech Images.
 Figure 5: Left, Tetrode dataset average waveform. Right, the waveform with the most uncertain cluster membership according to the classifier learned by RIM. for D&amp;D and 5 clusters for NCI109). RIM outperforms both SC and MMC at all settings. RIM has the advantage over k-means when discovering a small number of clusters and is comparable at other settings. On NCI109, RIM required approximately 10 minutes per run. SC required approximately 13 minutes, while MMC required on average 18 minutes per run.
 Neural Tetrode Recordings. We demonstrate RIM on a large scale data set of 319 , 209 neural activity waveforms recorded from four co-located electrodes implanted in the hippocampus of a behaving rat. The waveforms are composed of 38 samples from each of the four electrodes and are the output of a neural spike detector which aligns signal peaks to the 13 -th sample, see the average waveform in Figure 5 (left). We concatenate the samples into a single 152 -dimensional vector and preprocess by subtracting the mean waveform and divide each vector component by its variance. We use the linear RIM algorithm given in Section 3, initialized with 100 categories. We set  X  to 4 N and RIM discovers 33 clusters and finishes in 12 minutes. There is no ground truth available for this dataset, but we use it to demonstrate RIM X  X  efficacy as a data exploration tool. Figure 6 shows two clusters discovered by RIM. The top row consists of cluster member waveforms superimposed on each other, with the cluster X  X  mean waveform plotted in red. We find that the clustered waveforms have substantial similarity to each other. Taken as a whole, the clusters give an idea of the typical waveform patterns. The bottom row shows the learned classifier X  X  discriminative weights w k for each category, which can be used to gain a sense for how the cluster X  X  members differ from the dataset mean waveform. We can use the probabilistic classifier learned by RIM to discover atypical shows the waveform whose cluster membership is most uncertain.
 Figure 6: Two clusters discovered by RIM on the Tetrode data set. Top row: Superimposed waveform members, with cluster mean in red. Bottom row: The discriminative category weights w k associated with each cluster. 6.2 Semi-supervised Classification We test our semi-supervised classification method described in Section 5.1 against [3] on the Cal-tech Images dataset. The methods were trained using both unlabeled and labeled examples, and classification performance is assessed on the unlabeled portion. As a baseline, a supervised classi-fier was trained on labeled subsets of the data and tested on the remainder. Parameters were selected via cross-validation on a subset of the labeled examples. The results are summarized in Figure 4. We find that both semi-supervised methods significantly improve classification performance rela-tive to the supervised baseline when the number of labeled examples is small. Additionally, we find that RIM outperforms Grandvalet &amp; Bengio. This suggests that incorporating prior knowledge about class size distributions (in this case, we use a uniform prior) may be useful in semi-supervised learning. Our work has connections to existing work in both unsupervised learning and semi-supervised clas-sification.
 Unsupervised Learning. The information bottleneck method [19] learns a conditional model p ( y | x ) where the labels y form a lossy representation of the input space x , while preserving in-formation about a third  X  X elevance X  variable z . The method maximizes I ( y ; z )  X   X I ( x ; y ) , whereas we maximize the information between y and x while constraining complexity with a parametric regularizer. The method of [20] aims to maximize a similarity measure computed between members within the same cluster while penalizing the mutual information between the cluster label y and the input x . Again, mutual information is used to enforce a lossy representation of y | x . Song et al. [22] also view clustering as maximization of the dependence between the input variable and output la-bel variable. They use the Hilbert-Schmidt Independence Criterion as a measure of dependence, whereas we use Mutual Information.
 There is also an unsupervised variant of the Support Vector Machine, called max-margin cluster-ing. Like our approach, the works of [2] and [21] use notions of class balance, seperation, and regularization to learn unsupervised discriminative classifiers. However, they are formulated in the max-margin framework rather than our probabilistic approach. Ours appears more amenable to incorporating prior beliefs about the class labels. Unsupervised SVMs are solutions to a convex relaxation of a non-convex problem, while we directly optimize our non-convex objective. The semidefinite programming methods required are much more expensive than our approach. Semi-supervised Classification. Our semi-supervised objective is related to [3], as discussed in section 5.1. Another semi-supervised method [23] uses mutual information as a regularizing term to be minimized, in contrast to ours which attempts to maximize mutual information. The assumption underlying [23] is that any information between the label variable and unlabeled examples is an artifact of the classifier and should be removed. Our method encodes the opposite assumption: there may be variability (e.g. new class label values) not captured by the labeled data, since it is incomplete. We considered the problem of learning a probabilistic discriminative classifier from an unlabeled data set. We presented Regularized Information Maximization (RIM), a probabilistic framework for tackling this challenge. Our approach consists of optimizing an intuitive information theoretic objective function that incorporates class separation, class balance and classifier complexity, which may be interpreted as maximizing the mutual information between the empirical input and implied label distributions. The approach is flexible, in that it allows consideration of different likelihood functions. It also naturally allows expression of prior assumptions about expected label proportions by means of a cross-entropy with respect to a reference distribution. Our framework allows natural incorporation of partial labels for semi-supervised learning. In particular, we instantiate the framework to unsupervised, multi-class kernelized logistic regression. Our empirical evaluation indicates that RIM outperforms existing methods on several real data sets, and demonstrates that RIM is an effective model selection method.
 Acknowledgements [1] A. Ng, M. I. Jordan, and Y. Weiss. On spectral clustering: Analysis and an algorithm. In NIPS , [2] L. Xu and D. Schuurmans. Unsupervised and semi-supervised multi-class support vector ma-[3] Y. Grandvalet and Y. Bengio. Semi-supervised learning by entropy minimization. In NIPS , [4] John S. Bridle, Anthony J. R. Heading, and David J. C. MacKay. Unsupervised classifiers, [5] Olivier Chapelle and Alexander Zien. Semi-supervised classification by low density separa-[6] D. C. Liu and J. Nocedal. On the limited memory BFGS method for large scale optimization. [7] T. Jaakkola, M. Meila, and T. Jebara. Maximum entropy discrimination. In NIPS , 1999. [8] Y. W. Teh. A hierarchical bayesian language model based on pitman-yor processes. In ACL , [9] K. Zhang, I. W. Tsang, and J. T. Kwok. Maximum margin clustering made practical. In ICML , [10] John Shawe-Taylor and Nello Cristianini. Kernel Methods for Pattern Analysis . Cambridge [11] Lawrence Hubert and Phipps Arabie. Comparing partitions. Journal of Classification , 2:193 X  [12] Alexander Strehl and Joydeep Ghosh. Cluster ensembles  X  A knowledge reuse framework for [13] Y. Chen, J. Ze Wang, and R. Krovetz. CLUE: cluster-based retrieval of images by unsupervised [14] G. Griffin, A. Holub, and P. Perona. Caltech-256 object category dataset. Technical Report [15] S. Lazebnik, C. Schmid, and J. Ponce. Beyond bags of features: Spatial pyramid matching for [16] P. D. Dobson and A. J. Doig. Distinguishing enzyme structures from non-enzymes without [17] Nikil Wale and George Karypis. Comparison of descriptor spaces for chemical compound [18] N. Shervashidze and K. M. Borgwardt. Fast subtree kernels on graphs. In NIPS , 2010. [19] N. Tishby, F. C. Pereira, and W. Bialek. The information bottleneck method. CoRR , [20] N. Slonim, G. S. Atwal, G. Tkacik, and W. Bialek. Information-based clustering. Proc Natl [21] Francis Bach and Za  X   X d Harchaoui. DIFFRAC: a discriminative and flexible framework for [22] Le Song, Alex Smola, Arthur Gretton, and Karsten M. Borgwardt. A dependence maximization [23] A. Corduneanu and T. Jaakkola. On information regularization. In UAI , 2003.
