 Community Question Answering (CQA) services, such as Yahoo! Answers and WikiAnswers, have become popular with users as one of the central paradigms for satisfying users X  information needs. The task of question retrieval in CQA aims to resolve one X  X  query directly by finding the most relevant questions (together with their answers) from an archive of past questions. However, as users can ask any question that they like, a large number of questions in CQA are not about objective (factual) knowledge, but about subjective (sentiment-based) opinions or social interactions. The inhomogeneous nature of CQA leads to reduced perfor-mance of standard retrieval models. To address this prob-lem, we present a hybrid approach that blends several lan-guage modelling techniques for question retrieval, namely, the classic (query-likelihood) language model, the state-of-the-art translation-based language model, and our proposed intent-based language model. The user intent of each can-didate question (objective/subjective/social) is given by a probabilistic classifier which makes use of both textual fea-tures and metadata features. Our experiments on two real-world datasets show that our approach can significantly out-perform existing ones.
 H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval Algorithms, Experimentation, Performance Community Question Answering, Question Retrieval, User Intent, Language Modelling.

In recent years, Community Question Answering (CQA) services, such as Yahoo! Answers, WikiAnswers, Quora, Stack Overflow, and Baidu Zhidao, have become popular knowledge sources for Internet users to access useful infor-mation online. When a user submits a new question (i.e., query ) in CQA, the system would usually check whether similar questions have already been asked and answered be-fore. If so, the user X  X  query could be resolved directly by re-turning those archive questions (i.e., documents ) with their corresponding answers.

Identifying relevant questions in CQA repositories is a dif-ficult task, as the underlying intents of two questions could be very different even if they bear a close lexical resemblance. For example, at the time of writing this paper, when sub-mitting the question  X  X hat does licking your finger before turning a page do? X  to Google, the question  X  X hat would you do if I licked my fingers before turning the pages of your books? X  (with answers like  X  X o, I wouldn X  X  care X ) is deemed as the second best match, since these two questions look similar. However, the intent behind these two questions are substantially different: the former seeks facts while the latter looks for social empathy from other people. In this paper, we aim to develop an approach for question retrieval that can strike the optimal balance between a question X  X  lexical relevance and intent relevance.
The technique of language modelling has proved to be effective for question retrieval in CQA. The state-of-the-art approach utilises the classic (query-likelihood) language model [9] (see Section 3.1) together with the translation-based language model [5,8] (see Section 3.2). Cao et al. have proposed a category-based language model [2] for question retrieval, but it requires users to manually assign one topic category to each of their query questions, which is not al-ways feasible. Furthermore, their category-based language model cannot be applied straightforwardly to incorporate user intent into question retrieval, because it assumes that a question belongs to one and only one category, while we believe that a question can have multiple intents (each to a certain degree).
Using the classic (query-likelihood) language model [9] for information retrieval, we can measure the relevance of an archive question d with respect to the query question q as: assuming that each term w in the query q is generated in-dependently by the unigram model of document d . The probabilities P cla ( w | d ) are estimated from the bag of words in document d with Dirichlet prior smoothing.
There are often lexical gaps between a query question and archive questions in CQA. For example,  X  X here can I see movies for free online X  and  X  X nyone share me a DVD streaming link? X  have the same meaning but are expressed in quite different words. It has been demonstrated that this issue could be addressed by the translation-based language model [5, 8]: where P ( w | t ) represents the probability of a document term t being translated into a query term w . As in [8], we esti-mate such word-to-word translation probabilities P ( w | t ) on a parallel corpus that consists of 200,000 archived question-answer pairs from Yahoo! Answers.
There could be different user intents underlying different questions. For example, many questions in CQA are affected by the users X  individual interests (empathy, support, and affection, etc.) rather than just informational needs. In this paper, we propose to take user intent into account for question retrieval in the language modelling framework: where C k represents a category of user intent, P ( w | C corresponding unigram language model (see Section 3.3.2) and P ( C k | d ) is the probability that the document d belongs to that category (see Section 3.3.1).

Compared to the category-based language model of Cao et al. [2], the intent-based model above is more general and more robust, because, instead of imposing hard mutually-exclusive classifications, it classifies a question into multiple (user intent) categories with certain probabilities.
When computing P ( C k | d ) in the above, intent-based lan-guage model, we adopt the question taxonomy proposed by Chen et al. [4] which classifies the user intent of a question as objective, subjective, and social. A number of popular machine learning algorithms have been tried for question classification, and finally Support Vector Machine (SVM) is chosen due to its consistent outstanding performance. The probabilistic outputs are obtained from SVM via Platt X  X  method [7].

In addition to standard textual features (i.e., the bag of words weighted by TFxIDF), a series of metadata features have been identified and exploited for training the proba-bilistic classifier. We found that question topic, question time, and asker experience are particularly useful for our task of intent-oriented question classification.

Question Topic . Figure 1 shows the distribution of user intent over the top-10 question topic categories in Yahoo! Answers. Objective and subjective questions have similar proportion of presence in most topic categories, except for  X  X rts &amp; Humanities X  (which contains many questions about history and genealogy). The distribution of social questions looks quite different: most social questions are about topics like  X  X amily Relationships, X  X  X ews Events, X  and  X  X ntertain-ment &amp; Music X  on which people may be more inclined to have social interaction. This suggests that question topic features have discriminative power to separate social ques-tions between objective or subjective ones.

Question Time . Figure 2 shows the distribution of user intent over the hour of the day when the question was asked on 2005-05-01. It seems that objective and subjective ques-tions do not have apparent differences in terms of question time. In contrast, social questions show interesting pat-terns: the peak time for social questions is at 18:00 (end of a work day), 15:00 (after lunch), and 03:00 (lonely in the late night). This suggests that question time features are capable of distinguishing social questions from objective or subjective ones.

Question Asker X  X  Experience . Figure 3 shows the dis-tribution of user intent over the question asker X  X  experience, i.e., the number of questions the user has asked before. It
Figure 3: The question asker X  X  experience feature. seems that subjective and social questions are more likely to come from experienced users than new ones, probably because experienced users are relatively familiar with each other. This suggests that the question asker X  X  experience features allow objective questions to separate from subjec-tive or social ones.

It is time-consuming and error-prone to manually label archive questions according to their user intents. Usually, we only have a small set of labelled questions, which would significantly limit the success of supervised learning for ques-tion classification. However, obtaining unlabelled questions is quite easy and cheap. So it is promising to apply semi-supervised learning [3] which can make use of a large amount of unlabelled data in addition to the small set of labelled data.

For this particular problem of user intent oriented ques-tion classification, we argue that the semi-supervised learn-ing framework co-training [1] is particularly suitable. In the co-training process, each example is described by two differ-ent feature sets (views) which provide different and comple-mentary information about it. Ideally, those two views are conditionally independent (given the class) and each view is sufficient (to be used for classification alone). The textual and metadata features mentioned above are both effective in detecting the user intent of questions but with quite differ-ent discriminative powers for different question categories; therefore, they can be considered as the two views for co-training.
Given the probabilistic classification results on all archive questions, we can obtain the unigram language model for each user intent category C k through maximum-likelihood estimation: where tf ( w,d ) is the term frequency of word w in docu-ment d . It is possible to employ more advanced estimation methods, which is left for future work.
To exploit evidences from different perspectives for ques-tion retrieval, we can mix the above language models via Table 1: The performance of supervised learning with different sets of features. Table 2: The performance of supervised learning vs semi-supervised learning (co-training). linear combination: where  X  ,  X  , and  X  are three non-negative weight parame-ters satisfying  X  +  X  +  X  = 1. When  X  = 0, the complete mixture model backs off to the current state-of-the-art ap-proach, i.e., the combination of the classic language model and the translation-based language model only [8]. We conducted experiments on two real-world CQA datasets. The first dataset, YA, comes from Yahoo! Answers. It is part of Yahoo! Labs X  Webscope 1 L6 dataset that consists of 4,483,032 questions with their answers from 2005-01-01 to 2006-01-01. The second dataset, WA, comes from WikiAn-swers. It contains 824,320 questions with their answers col-lected from WikiAnswers 2 from 2012-01-01 to 2012-05-01.
We first experimented with question classification on 1,539 questions that are randomly selected from the YA dataset and manually labelled according to their user intents. Those questions were split into training and testing sets with a proportion of 2:1.

Table 1 shows the performance (mi F 1 ) of [binary] question classification through supervised learning (linear SVM) with different sets of features. One can find that using both tex-tual features and metadata features works better than using either kind of features alone, for all question categories.
Table 2 depicts the performances (mi F 1 and ma F 1 question classification via supervised learning and also semi-supervised learning (co-training) based on both textual and metadata features. It is clear that co-training that regards text features and metadata features as two views works bet-ter than supervised learning that simply pools these two types of features together. The performance gain is proba-bly due to the fact that co-training can make use of a large amount of unlabelled questions in addition to the small set of labelled questions.

We then experimented with question retrieval using a sim-ilar set-up as in [8]: 50 questions were randomly sampled from the YA and WA datasets respectively for testing (which were excluded from the CQA retrieval repositories to en-sure the impartiality of the evaluation), and the top archive http://webscope.sandbox.yahoo.com/ http://wiki.answers.com/ Table 3: The model parameters for different ques-tion retrieval approaches. questions (i.e., search results) returned for each test query question were manually labelled as either relevant or not. In order to see whether user intent relevance can improve question retrieval performance, we compared the following three approaches: The model parameters were tuned on the training data to achieve optimal results, as shown in Table 3. In the mixture models (C+T) and (C+T+I), the ratio between parameter values  X  and  X  was same as that in [8].

The retrieval performances of those approaches, measured by Precision at 10 (P@10) [6] and Mean Average Precision (MAP) [6], are reported in Figure 4. Consistent to the obser-vation in [8], adding the translation-based language model (C+T) brings substantial performance improvement to the classic language model (C). More importantly, it is clear that our proposed hybrid approach incorporating the intent-based language model (C+T+I) outperforms the state-of-the-art approach (C+T) significantly, according to both P@10 and MAP on YA and WA.
The main contribution of this paper is to show the useful-ness of user intent for question retrieval in CQA. Our pro-posed intent-based language model supersedes the existing category-based language model since one question can be-long to multiple (user intent) categories to different degrees, and since a probabilistic question classifier is built automat-ically by taking into consideration of both textual features and metadata features in the co-training framework.
We thank Xiaobing Xue (UMass) who kindly shared his code for the translation-based language model with us. We appreciate the efforts of the three anonymous reviewers and their useful comments and suggestions for improving this work.
