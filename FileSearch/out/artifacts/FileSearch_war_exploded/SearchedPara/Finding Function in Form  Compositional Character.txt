 Good representations of words are important for good generalization in natural language process-ing applications. Of central importance are vec-tor space models that capture functional (i.e., se-mantic and syntactic) similarity in terms of ge-ometric locality. However, when word vectors are learned X  X  practice that is becoming increas-ingly common X  X ost models assume that each word type has its own vector representation that can vary independently of other model compo-nents. This paper argues that this independence assumption is inherently problematic, in particular in morphologically rich languages (e.g., Turkish). In such languages, a more reasonable assumption would be that orthographic (formal) similarity is evidence for functional similarity.

However, it is manifestly clear that similarity in form is neither a necessary nor sufficient condi-tion for similarity in function: small orthographic differences may correspond to large semantic or syntactic differences ( butter vs. batter ), and large orthographic differences may obscure nearly per-fect functional correspondence ( rich vs. affluent ). Thus, any orthographically aware model must be able to capture non-compositional effects in addi-tion to more regular effects due to, e.g., morpho-logical processes. To model the complex form X  function relationship, we turn to long short-term memories (LSTMs), which are designed to be able to capture complex non-linear and non-local dy-namics in sequences (Hochreiter and Schmidhu-ber, 1997). We use bidirectional LSTMs to  X  X ead X  the character sequences that constitute each word and combine them into a vector representation of the word. This model assumes that each charac-ter type is associated with a vector, and the LSTM parameters encode both idiosyncratic lexical and regular morphological knowledge.

To evaluate our model, we use a vector-based model for part-of-speech (POS) tagging and for language modeling, and we report ex-periments on these tasks in several languages comparing to baselines that use more tradi-tional, orthographically-unaware parameteriza-tions. These experiments show: (i) our character-based model is able to generate similar representa-tions for words that are semantically and syntacti-cally similar, even for words are orthographically distant (e.g., October and January ); our model achieves improvements over word lookup tables using only a fraction of the number of parameters in two tasks; (iii) our model obtains state-of-the-art performance on POS tagging (including estab-lishing a new best performance in English); and (iv) performance improvements are especially dra-matic in morphologically rich languages.

The paper is organized as follows: Section 2 presents our character-based model to generate word embeddings. Experiments on Language Modeling and POS tagging are described in Sec-tions 4 and 5. We present related work in Sec-tion 6; and we conclude in Section 7. It is commonplace to represent words as vectors. In contrast to na  X   X ve models in which all word types in a vocabulary V are equally different from each other, vector space models capture the intuition that words may be different or similar along a va-riety of dimensions. Learning vector representa-tions of words by treating them as optimizable pa-rameters in various kinds of language models has been found to be a remarkably effective means for generating vector representations that perform well in other tasks (Collobert et al., 2011; Kalch-brenner and Blunsom, 2013; Liu et al., 2014; Chen and Manning, 2014). Formally, such models de-rameters for each word in the vocabulary V . For a given word type w  X  V , a column is selected by right-multiplying P by a one-hot vector of length | V | , which we write 1 w , that is zero in every di-mension except for the element corresponding to w . Thus, P is often referred to as word lookup ding obtained from a word lookup table for w as w = P  X  1 w . This allows tasks with low amounts of annotated data to be trained jointly with other tasks with large amounts of data and leverage the similarities in these tasks. A common practice to this end is to initialize the word lookup table with the parameters trained on an unsupervised task. Some examples of these include the skip-n -gram and CBOW models of Mikolov et al. (2013). 2.1 Problem: Independent Parameters There are two practical problems with word lookup tables. Firstly, while they can be pre-trained with large amounts of data to learn se-mantic and syntactic similarities between words, each vector is independent. That is, even though models based on word lookup tables are often ob-served to learn that cats , kings and queens exist in roughly the same linear correspondences to each other as cat , king and queen do, the model does not represent the fact that adding an s at the end of the word is evidence for this transformation. This means that word lookup tables cannot gen-erate representations for previously unseen words, such as Frenchification , even if the components, French and -ification , are observed in other con-texts.

Second, even if copious data is available, it is impractical to actually store vectors for all word types. As each word type gets a set of parameters d , the total number of parameters is d  X | V | , where | V | is the size of the vocabulary. Even in rela-tively morphological poor English, the number of word types tends to scale to the order of hundreds of thousands, and in noisier domains, such as on-line data, the number of word types raises con-siderably. For instance, in the English wikipedia dump with 60 million sentences, there are approx-imately 20 million different lowercased and tok-enized word types, each of which would need its own vector. Intuitively, it is not sensible to use the same number of parameters for each word type.
Finally, it is important to remark that it is uncontroversial among cognitive scientists that our lexicon is structured into related forms X  X .e., their parameters are not independent. The well-known  X  X ast tense debate X  between connection-ists and proponents of symbolic accounts con-cerns disagreements about how humans represent knowledge of inflectional processes (e.g., the for-mation of the English past tense), not whether such knowledge exists (Marslen-Wilson and Tyler, 1998). 2.2 Solution: Compositional Models Our solution to these problems is to construct a vector representation of a word by composing smaller pieces into a representation of the larger form. This idea has been explored in prior work by composing morphemes into representations of words (Luong et al., 2013; Botha and Blunsom, 2014; Soricut and Och, 2015). Morphemes are an ideal primitive for such a model since they are X  by definition X  X he minimal meaning-bearing (or syntax-bearing) units of language. The drawback to such approaches is they depend on a morpho-logical analyzer.

In contrast, we would like to compose repre-sentations of characters into representations of words. However, the relationship between words forms and their meanings is non-trivial (de Saus-sure, 1916). While some compositional relation-ships exist, e.g., morphological processes such as adding -ing or -ly to a stem have relatively reg-ular effects, many words with lexical similarities convey different meanings, such as, the word pairs lesson  X  X  X  lessen and coarse  X  X  X  course . Our compositional character to word (C2W) model is based on bidirectional LSTMs (Graves and Schmidhuber, 2005), which are able to learn complex non-local dependencies in sequence models. An illustration is shown in Figure 1. The input of the C2W model (illustrated on bottom) is a single word type w , and we wish to obtain is a d -dimensional vector used to represent w . This model shares the same input and output of a word lookup table (illustrated on top), allowing it to eas-ily replace then in any network.
 As input, we define an alphabet of characters C . For English, this vocabulary would contain an entry for each uppercase and lowercase letter as well as numbers and punctuation. The input word w is decomposed into a sequence of characters c ,...,c m , where m is the length of w . Each c i is defined as a one hot vector 1 c index of c i in vocabulary M . We define a projec-tion layer P C  X  R d C  X | C | , where d C is the number of parameters for each character in the character set C . This of course just a character lookup table, and is used to capture similarities between charac-ters in a language (e.g., vowels vs . consonants). Thus, we write the projection of each input char-acter c i as e c
Given the input vectors x 1 ,..., x m , a LSTM computes the state sequence h 1 ,..., h m +1 by it-eratively applying the following updates: i f c o h t = o t tanh( c t ) , where  X  is the component-wise logistic sig-moid function, and is the component-wise (Hadamard) product. LSTMs define an extra cell memory c t , which is combined linearly at each Figure 1: Illustration of the word lookup tables (top) and the lexical Composition Model (bottom). Square boxes represent vectors of neuron activa-tions. Shaded boxes indicate that a non-linearity. timestamp t . The information that is propagated from c t  X  1 to c t is controlled by the three gates i t , f , and o t , which determine the what to include from the input x t , the what to forget from c t  X  1 and what is relevant to the current state h t . We write W to refer to all parameters the LSTM ( W ix , W f x , b f , . . . ). Thus, given a sequence of charac-ward LSTM, yields the state sequence s f while the backward LSTM receives as input the re-LSTMs use a different set of parameters W f and W b . The representation of the word w is obtained by combining the forward and backward states: where D f , D b and b d are parameters that deter-mine how the states are combined.
 Caching for Efficiency. Relative to e W w , com-puting e C w is computational expensive, as it re-quires two LSTMs traversals of length m . How-ever, e C w only depends on the character sequence of that word, which means that unless the parame-ters are updated, it is possible to cache the value of w for each different w  X  X  that will be used repeat-edly. Thus, the model can keep a list of the most frequently occurring word types in memory and run the compositional model only for rare words. Obviously, caching all words would yield the same also using the same amount of memory. Conse-quently, the number of word types used in cache can be adjusted to satisfy memory vs. perfor-mance requirements of a particular application.
At training time, when parameters are changing, repeated words within the same batch only need to be computed once, and the gradient at the output can be accumulated within the batch so that only one update needs to be done per word type. For this reason, it is preferable to define larger batches. Our proposed model is similar to models used to compute composed representations of sentences from words (Cho et al., 2014; Li et al., 2015). However, the relationship between the meanings of individual words and the composite meaning of a phrase or sentence is arguably more regular than the relationship of representations of charac-ters and the meaning of a word. Is our model capa-ble of learning such an irregular relationship? We now explore this question empirically.

Language modeling is a task with many appli-cations in NLP. An effective LM requires syntactic aspects of language to be modeled, such as word orderings (e.g.,  X  X ohn is smart X  vs .  X  X ohn smart is X ), but also semantic aspects (e.g.,  X  X ohn ate fish X  vs .  X  X ish ate John X ). Thus, if our C2W model only captures regular aspects of words, such as, prefixes and suffixes, the model will yield worse results compared to word lookup tables. 4.1 Language Model Language modeling amounts to learning a func-tion that computes the log probability, log p ( w ) , of a sentence w = ( w 1 ,...,w n ) . This quantity can be decomposed according to the chain rule into the sum of the conditional log probabilities P model computes log p ( w i | w 1 ,...,w i  X  1 ) by composing representations of words w 1 ,...,w i  X  1 using an recurrent LSTM model (Mikolov et al., 2010; Sundermeyer et al., 2012).

The model is illustrated in Figure 2, where we observe on the first level that each word w i is pro-jected into their word representations. This can be case, we will have a regular recurrent language model. To use our C2W model, we can sim-ply replace the word lookup table with the model f ( w i ) = e C w predict word w i +1 . This is performed by project-ing the s i into a vector of size of the vocabulary V and performing a softmax. Figure 2: Illustration of our neural network for Language Modeling.

The softmax is still simply a d  X  V table, which encodes the likelihood of every word type in a given context, which is a closed-vocabulary model. Thus, at test time out-of-vocabulary (OOV) words cannot be addressed. A strategy that is generally applied is to prune the vocabu-lary V by replacing word types with lower fre-quencies as an OOV token. At test time, the prob-ability of words not in vocabulary is estimated as the OOV token. Thus, depending on the number of word types that are pruned, the global perplexi-ties may decrease, since there are fewer outcomes in the softmax, which makes the absolute value of perplexity not informative when comparing mod-els of different vocabulary sizes. Yet, the rela-tive perplexity between different models indicates which models can better predict words based on their contexts.
To address OOV words in the baseline setup, these are replaced by an unknown token, and also associated with a set of embeddings. During train-ing, word types that occur once are replaced with the unknown token stochastically with 0.5 proba-bility. The same process is applied at the character level for the C2W model. 4.2 Experiments Datasets We look at the language model perfor-mance on English, Portuguese, Catalan, German and Turkish, which have a broad range of morpho-logical typologies. While all these languages con-tain inflections, in agglutinative languages affixes tend to be unchanged, while in fusional languages they are not. For each language, Wikipedia articles were randomly extracted until 1 million words are obtained and these were used for training. For de-velopment and testing, we extracted an additional set of 20,000 words.
 Setup We define the size of the word represen-tation d to 50. In the C2W model requires set-ting the dimensionality of characters d C and cur-rent states d CS . We set d C = 50 and d CS = 150 . Each LSTM state used in the language model se-quence s i is set to 150 for both states and cell memories. Training is performed with mini-batch gradient descent with 100 sentences. The learn-ing rate and momentum were set to 0.2 and 0.95. The softmax over words is always performed on lowercased words. We restrict the output vocabu-lary to the most frequent 5000 words. Remaining word types will be replaced by an unknown token, which must also be predicted. The word represen-tation layer is still performed over all word types (i.e., completely open vocabulary). When using word lookup tables, the input words are also low-ercased, as this setup produces the best results. In the C2W, case information is preserved.

Evaluation is performed by computing the per-plexities over the test data, and the parameters that yield the highest perplexity over the development data are used.
 Perplexities Perplexities over the testset are re-ported on Table 4. From these results, we can see that in general, it is clear that C2W always outper-forms word lookup tables (row  X  X ord X ), and that improvements are especially pronounced in Turk-ish, which is a highly morphological language, where word meanings differ radically depending on the suffixes used ( evde  X  in the house vs. ev-den  X  from the house ).
 Number of Parameters As for the number of parameters (illustrated for block  X #Parameters X ), the number of parameters in word lookup tables is V  X  d . If a language contains 80,000 word types (a conservative estimate in morphologically rich lan-guages), 4 million parameters would be necessary. On the other hand, the compositional model con-sists of 8 matrices of dimensions d CS  X  d C +2 d CS . Additionally, there is also the matrix that com-bines the forward and backward states of size d  X  2 d CS . Thus, the number of parameters is roughly 150,000 parameters X  X ubstantially fewer. This model also needs a character lookup table with d C parameters for each entry. For English, there are 618 characters, for an additional 30,900 parameters. So the total number of parameters for English is roughly 180,000 parameters (2 to 3 pa-rameters per word type), which is an order of mag-nitude lower than word lookup tables.
 Performance As for efficiency, both representa-tions can label sentences at a rate of approximately 300 words per second during training. While this is surprising, due to the fact that the C2W model requires a composition over characters, the main bottleneck of the system is the softmax over the vocabulary. Furthermore, caching is used to avoid composing the same word type twice in the same batch. This shows that the C2W model, is rela-tively fast compared operations such as a softmax. Representations of (nonce) words While is is promising that the model is not simply learning lexical features, what is most interesting is that the model can propose embeddings for nonce words, in stark contrast to the situation observed with lookup table models. We show the 5-most-similar in-vocabulary words (measured with cosine simi-larity) as computed by our character model on two Table 2: Most-similar in-vocabular words under the C2W model; the two query words on the left are in the training vocabulary, those on the right are nonce (invented) words. makes our model generalize significantly better than lookup tables that generally use unknown to-kens for OOV words. Furthermore, this ability to generalize is much more similar to that of human beings, who are able to infer meanings for new words based on its form. As a second illustration of the utility of our model, we turn to POS tagging. As morphology is a strong indicator for syntax in many languages, a much effort has been spent engineering fea-tures (Nakagawa et al., 2001; Mueller et al., 2013). We now show that some of these features can be learnt automatically using our model. 5.1 Bi-LSTM Tagging Model Our tagging model is likewise novel, but very straightforward. It builds a Bi-LSTM over words as illustrated in Figure 3. The input of the model is a sequence of features f ( w 1 ) ,...,f ( w n ) . Once again, word vectors can either be generated us-ing the C2W model f ( w i ) = e C w lookup tables f ( w i ) = e W w age of hand-engineered features, in which case f ( w i ) ,...,f n ( w i ) . Then, the sequential fea-tures f ( w 1 ) ,...,f ( w n ) are fed into a bidirec-tional LSTM model, obtaining the forward states s ,..., s f n and the backward states s b Thus, state s f words from 0 to i and s b i from n to i . The for-ward and backward states are combined, for each index from 1 to n , as follows: where L f , L b and b l are parameters defining how the forward and backward states are combined. The size of the forward s f and backward states s b and the combined state l are hyperparameters of the model, denoted as d f spectively. Finally, the output labels for index i are obtained as a softmax over the POS tagset, by projecting the combined state l i . Figure 3: Illustration of our neural network for POS tagging. 5.2 Experiments Datasets For English, we conduct experiments on the Wall Street Journal of the Penn Treebank dataset (Marcus et al., 1993), using the standard splits (sections 1 X 18 for train, 19 X 21 for tuning and 22 X 24 for testing). We also perform tests on 4 other languages, which we obtained from the CoNLL shared tasks (Mart  X   X  et al., 2007; Brants et al., 2002; Afonso et al., 2002; Atalay et al., 2003). While the PTB dataset provides standard train, tuning and test splits, there are no tuning sets in the datasets in other languages, so we withdraw the last 100 sentences from the training dataset and use them for tuning.
 Setup The POS model requires two sets of hy-perparameters. Firstly, words must be converted into continuous representations and the same hy-perparametrization as in language modeling (Sec-tion 4) is used. Additionally, we also compare to the convolutional model of Santos and Zadrozny (2014), which also requires the dimensionality for characters and the word representation size, which are set to 50 and 150, respectively. Sec-ondly, words representations are combined to en-code context. Our POS tagger has three hyperpa-rameters d f to the sizes of LSTM states, and are all set to 50. As for the learning algorithm, use the same setup (learning rate, momentum and mini-batch sizes) as used in language modeling.

Once again, we replace OOV words with an un-known token, in the setup that uses word lookup tables, and the same with OOV characters in the C2W model. In setups using pre-trained word em-beddings, we consider a word an OOV if it was not seen in the labelled training data as well as in the unlabeled data used for pre-training.
 Compositional Model Comparison A compar-ison of different recurrent neural networks for the C2W model is presented in Table 3. We used our proposed tagger tagger in all experiments and re-sults are reported for the English Penn Treebank. Results on label accuracy test set is shown in the column  X  X cc X . The number of parameters in the word composition model is shown in the column  X  X arameters X . Finally, the number of words pro-cessed at test time per second are shown in column  X  X ords/sec X .

We observe that approaches using RNN yield worse results than their LSTM counterparts with a difference of approximately 2%. This suggests that while regular RNNs can learn shorter char-acter sequence dependencies, they are not ideal to learn longer dependencies. LSTMs, on the other hand, seem to effectively obtain relatively higher results, on par with using word look up ta-bles (row  X  X ord Lookup X ), even when using for-ward (row  X  X orward LSTM X ) and backward (row  X  X ackward LSTM X ) LSTMs individually. The best results are obtained using the bidirectional LSTM (row  X  X i-LSTM X ), which achieves an ac-curacy of 97.29% on the test set, surpassing the word lookup table. The convolution model (San-tos and Zadrozny, 2014) obtained slightly lower results (row  X  X onvolutional (S&amp;Z) X ), we think this is because the convolutional model uses a max-pooling layer over series of window convolu-tions. As order is only perserved within windows, longer distance dependences are unobserved.

There are approximately 40k lowercased word types in the training data in the PTB dataset. Thus, a word lookup table with 50 dimensions per type contains approximately 2 million parameters. In the C2W models, the number of characters types (including uppercase and lowercase) is approxi-Table 3: POS accuracy results for the English PTB using word representation models. mately 80. Thus, the character look up table con-sists of only 4k parameters, which is negligible compared to the number of parameters in the com-positional model, which is once again 150k pa-rameters. One could argue that results in the Bi-LSTM model are higher than those achieved by other models as it contains more parameters, so we set the state size d CS = 50 (row  X  X i-LSTM d CS = 50  X ) and obtained similar results.

In terms of computational speed, we can ob-serve that there is a more significant slowdown when applying the C2W models compared to lan-guage modeling. This is because there is no longer a softmax over the whole word vocabulary as the main bottleneck of the network. However, we can observe that while the Bi-LSTM system is 3 times slower, it is does not significantly hurt the perfor-mance of the system.
 Results on Multiple Languages Results on 5 languages are shown in Table 4. In general, we can observe that the model using word lookup tables (row  X  X ord X ) performs consistently worse than the C2W model (row  X  X 2W X ). We also com-pare our results with Stanford X  X  POS tagger, with the default set of features, found in Table 4. Re-sults using these tagger are comparable or bet-ter than state-of-the-art systems. We can observe that in most cases we can slightly outperform the scores obtained using their tagger. This is a promising result, considering that we use the same training data and do not handcraft any features. Furthermore, we can observe that for Turkish, our results are significantly higher ( &gt; 4%). Comparison with Benchmarks Most state-of-the-art POS tagging systems are obtained by ei-ther learning or handcrafting good lexical fea-tures (Manning, 2011; Sun, 2014) or using ad-
Table 4: POS accuracies on different languages ditional raw data to learn features in an unsuper-vised fashion. Generally, optimal results are ob-tained by performing both. Table 5 shows the current Benchmarks in this task for the English PTB. Accuracies on the test set is reported on col-umn  X  X cc X . Columns  X  + feat X  and  X  + data X  de-fine whether hand-crafted features are used and whether additional data was used. We can see that even without feature engineering or unsupervised pretraining, our C2W model (row  X  X 2W X ) is on par with the current state-of-the-art system (row  X  X tructReg X ). However, if we add hand-crafted features, we can obtain further improvements on this dataset (row  X  X 2W + features X ).

However, there are many words that do not con-tain morphological cues to their part-of-speech. For instance, the word snake does not contain any morphological cues that determine its tag. In these cases, if they are not found labelled in the training data, the model would be dependent on context to determine their tags, which could lead to errors in ambiguous contexts. Unsupervised training meth-ods such as the Skip-n -gram model (Mikolov et al., 2013) can be used to pretrain the word rep-resentations on unannotated corpora. If such pre-training places cat , dog and snake near each other in vector space, and the supervised POS data con-tains evidence that cat and dog are nouns, our model will be likely to label snake with the same tag.

We train embeddings using English wikipedia with the dataset used in (Ling et al., 2015), and the Structured Skip-n -gram model. Results using pre-trained word lookup tables and the C2W with the pre-trained word lookup tables as additional parameters are shown in rows  X  X ord(sskip) X  and  X  X 2W + word(sskip) X . We can observe that both systems can obtain improvements over their ran-dom initializations (rows  X  X ord X  and (C2W)).
Finally, we also found that when using the C2W model in conjunction pre-trained word embed-dings, that adding a non-linearity to the repre-proves the results over using a simple linear trans-Table 5: POS accuracy result comparison with state-of-the-art systems for the English PTB. formation (row  X  X 2W(tanh) + word (sskip) X ). This setup, obtains 0.28 points over the current state-of-the-art system(row  X  X CCN X ). 5.3 Discussion It is important to refer here that these results do not imply that our model always outperforms ex-isting benchmarks, in fact in most experiments, results are typically fairly similar to existing sys-tems. Even in Turkish, using morphological anal-ysers in order to extract additional features could also accomplish similar results. The goal of our work is not to overcome existing benchmarks, but show that much of the feature engineering done in the benchmarks can be learnt automatically from the task specific data. More importantly, we wish to show large dimensionality word look tables can be compacted into a lookup table using characters and a compositional model allowing the model scale better with the size of the training data. This is a desirable property of the model as data be-comes more abundant in many NLP tasks. Our work, which learns representations without relying on word lookup tables has not been ex-plored to our knowledge. In essence, our model attempts to learn lexical features automatically while compacting the model by reducing the re-dundancy found in word lookup tables. Individ-ually, these problems have been the focus of re-search in many areas.

Lexical information has been used to augment word lookup tables. Word representation learn-ing can be thought of as a process that takes a string as input representing a word and outputs a set of values that represent a word in vector space. Using word lookup tables is one possi-ble approach to accomplish this. Many meth-ods have been used to augment this model to learn lexical features with an additional model that is jointly maximized with the word lookup table. This is generally accomplished by either performing a component-wise addition of the em-beddings produced by word lookup tables (Chen et al., 2015), and that generated by the additional lexical model, or simply concatenating both rep-resentations (Santos and Zadrozny, 2014). Many models have been proposed, the work in (Col-lobert et al., 2011) refers that additional features sets F i can be added to the one-hot representa-tion and multiple lookup tables I F to project each of the feature sets to the same low-dimensional vector e W w . For instance, the work in (Botha and Blunsom, 2014) shows that us-ing morphological analyzers to generate morpho-logical features, such as stems, prefixes and suf-fixes can be used to learn better representations for words. A problem with this approach is the fact that the model can only learn from what has been defined as feature sets. The models proposed in (Santos and Zadrozny, 2014; Chen et al., 2015) allow the model to arbitrary extract meaningful lexical features from words by defining composi-tional models over characters. The work in (Chen et al., 2015) defines a simple compositional model by summing over all characters in a given word, while the work in (Santos and Zadrozny, 2014) defines a convolutional network, which combines windows of characters and a max-pooling layer to find important morphological features. The main drawback of these methods is that character or-der is often neglected, that is, when summing over all character embeddings, words such as dog and god would have the same representation accord-ing to the lexical model. Convolutional model are less susceptible to these problems as they com-bine windows of characters at each convolution, where the order within the window is preserved. However, the order between extracted windows is not, so the problem still persists for longer words, such as those found in agglutinative languages. Yet, these approaches work in conjunction with a word lookup table, as they compensate for this in-ability. Aside from neural approaches, character-based models have been applied to address mul-tiple lexically oriented tasks, such as translitera-tion (Kang and Choi, 2000) and twitter normaliza-tion (Xu et al., 2013; Ling et al., 2013).

Compacting models has been a focus of re-search in tasks, such as language modeling and machine translation, as extremely large models can be built with the large amounts of training data that are available in these tasks. In language modeling, it is frequent to prune higher order n-grams that do not encode any additional infor-mation (Seymore and Rosenfeld, 1996; Stolcke, 1998; Moore and Quirk, 2009). The same be ap-plied in machine translation (Ling et al., 2012; Zens et al., 2012) by removing longer translation pairs that can be replicated using smaller ones. In essence our model learns regularities at the sub-word level that can be leveraged for building more compact word representations.

Finally, our work has been applied to depen-dency parsing and found similar improvements over word models in morphologically rich lan-guages (Ballesteros et al., 2015). We propose a C2W model that builds word em-beddings for words without an explicit word lookup table. Thus, it benefits from being sen-sitive to lexical aspects within words, as it takes characters as atomic units to derive the embed-dings for the word. On POS tagging, our mod-els using characters alone can still achieve com-parable or better results than state-of-the-art sys-tems, without the need to manually engineer such lexical features. Although both language model-ing and POS tagging both benefit strongly from morphological cues, the success of our models in languages with impoverished morphological cues shows that it is able to learn non-compositional as-pects of how letters fit together.

The code for the C2W model and our language model and POS tagger implementations is avail-able from https://github.com/wlin12/ JNN .
 The PhD thesis of Wang Ling is supported by FCT grant SFRH/BD/51157/2010. This research was supported in part by the U.S. Army Re-search Laboratory, the U.S. Army Research Office under contract/grant number W911NF-10-1-0533 and NSF IIS-1054319 and FCT through the pluri-anual contract UID/CEC/50021/2013 and grant number SFRH/BPD/68428/2010.
