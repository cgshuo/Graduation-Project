 We present a novel framework for the query-performance prediction task. That is, estimating the effectiveness of a search performed in response to a query in lack of relevance judgments. Our approach is based on using statistical deci-sion theory for estimating the utility that a document rank-ing provides with respect to an information need expressed by the query. To address the uncertainty in inferring the information need, we estimate utility by the expected simi-larity between the given ranking and those induced by rel-evance models ; the impact of a relevance model is based on its presumed representativeness of the information need. Specific query-performance predictors instantiated from t he framework substantially outperform state-of-the-art pre dic-tors over five TREC corpora.

The effectiveness of search engines can significantly vary across queries [27, 9]. Thus, the ability to identify which queries are more difficult than others could be of great ben-efit. Indeed, there is a large body of work on predicting query performance , that is, estimating the effectiveness of a search performed in response to a query in lack of relevance-judgments information. (See Section 2 for a survey.)
We present a novel framework for query-performance pre-diction that is based on statistical decision theory. Speci fi-cally, we consider a ranking induced by a retrieval method in response to a query as a decision taken so as to satisfy the underlying information need [15]. The quality of the rankin g  X  i.e., query-performance  X  is then estimated based on the utility it provides with respect to the presumed informatio n need. However, there is often uncertainty about the actual information need, especially when examples of relevant doc -uments are not provided and queries are ambiguous.
To address the uncertainty in inferring the information need, we first assume a  X  X rue X  (latent) model of relevance, e.g., a relevance language model [16] that generates terms in the query and in relevant documents. Then, we estimate the utility of the given ranking by its expected similarity with the rankings induced by estimates of this relevance model.
We instantiate various query-performance predictors from the framework by varying the (i) estimates of the relevance model, (ii) measures for the similarity between the given ranking and that induced by a relevance-model estimate, and (iii) measures for the quality of a relevance-model esti -mate, that is, the extent to which it presumably represents the underlying information need.

A key observation, which enables to derive effective pre-dictors from our framework, concerns the representativene ss of relevance-model estimates for the underlying informati on need. We argue, and empirically show, that relevance-model representativeness can be estimated using various query-performance predictors that were originally devised to pre -dict the quality of the document list from which the rele-vance model is constructed, e.g., the Clarity method [6].
Empirical evaluation performed using five TREC corpora shows that the predictors instantiated from our framework substantially and consistently outperform, in terms of pre -diction quality, four state-of-the-art predictors. A case in point, the average relative prediction improvement over th e Clarity method [6] is above 30%.
Query-performance predictors can be roughly categorized to pre-retrieval and post-retrieval methods [13]. Pre-ret rieval methods analyze the query expression before search is per-formed [6, 13, 21, 19, 11, 31, 10]. Linguistic features and/o r statistical properties of the query-terms distribution ar e of-ten used along with some corpus-based statistics.

Post-retrieval predictors analyze also the result list  X  the list of documents most highly ranked in response to the query. The Clarity method [6], for example, estimates the  X  X ocus X  of the result list with respect to the corpus, as mea-sured by the KL-divergence between their induced (language ) models. Variants of clarity were proposed for improving pre -diction performance [2, 7, 4, 12].

Estimating result-list robustness is another effective post-retrieval prediction paradigm. Intuitively, the more robu st the result list with respect to different factors, the less  X  X  if-ficult X  the query is. For example, the cohesion of the re-sult list, as measured by its clustering patterns, indicate s query performance [26]. The effects on the result list of document perturbations [26, 33], query perturbations [29] , query-model modifications [34], and different retrieval fun c-tions [3], have been used to devise performance predictors. Retrieval scores often reflect document-query similarity. Hence, analyzing retrieval-scores distribution can poten tially help to predict query performance. Indeed, the highest re-trieval score and the mean of top scores were shown to in-dicate query performance [25]. Query performance was also shown to be correlated with the extent to which similar doc-uments in the result list are assigned with similar retrieva l scores [8]. In addition, high retrieval scores at top ranks o f the list with respect to that of the corpus [34], and large variance of retrieval scores in the list [22], were shown to indicate effective performance.

We argue, and empirically show, that some post-retrieval predictors can effectively be used in our framework to esti-mate the extent to which relevance models [16] presumably represent the information need expressed by a query. We use state-of-the-art predictors that represent the differe nt paradigms mentioned above, namely, the Clarity method [6], the query-feedback ( QF ) approach, which is based on ranking robustness [34], and the WIG [34] and NQC [22] measures that utilize retrieval scores. (See Section 3.2.2 for a disc us-sion on these predictors.) The effectiveness of the predicto rs we derive from the framework is substantially better than that of using those four predictors, as originally proposed , to estimate the quality of the result list.

Some work on ranking documents in response to a query [5, 28, 24] addresses the uncertainty in inferring the infor -mation need by using multiple relevance models (or more generally, query-expansion-based models) as we propose in our framework. Furthermore, representativeness measures for relevance models have also been used [28, 24], albeit not QF, WIG, and NQC that we use here. More importantly, the task we pursue  X  predicting the quality of a given docu-ment ranking  X  is different than that of inducing document ranking in response to a query [5, 28, 24].

Our treatment to document ranking as a decision made by the retrieval method in response to a query is inspired by the risk minimization framework [15]. However, while the latter aims at inducing document ranking, our framework is intended to estimate the utility of a given ranking.
Let q , d , and D denote a query, a document and a corpus of documents, respectively. We assume that q is used to express some information need I q .

We use  X  M ( q ; S ) to denote the ranking induced over a set of documents S in response to q using retrieval method M . Our goal is to predict the query-performance of M with respect to q . In other words, we would like to quantify the quality (effectiveness) of the ranking of all documents in th e corpus,  X  M ( q ; D ), with respect to the information need I in lack of relevance-judgments information.
The corpus ranking,  X  M ( q ; D ), could be viewed as a de-cision made by the retrieval method M in response to q so as to satisfy the user X  X  (hidden) information need I q [15]. The ranking effectiveness reflects the utility provided to the user, denoted U (  X  M ( q ; D ); I q ). In what follows we devise estimates for this utility, i.e., query-performance predi ctors.
Suppose that there is an oracle that provides us with a  X  X rue X  model of relevance R I q representing the information need I q . Suppose also that R I q can be used by M for rank-ing  X  e.g., that R I q has a query-model representation to be used by M . A statistical relevance language model that generates the terms in relevant documents is an example for such a query model in the language modeling framework [16]. Then, according to the probability ranking principle [20], using M with R I q yields a ranking  X  M ( R I q ; D ) of max-imal utility (e.g., all relevant documents are positioned a t the highest ranks, and all non-relevant documents are posi-tioned below the relevant documents). Thus, we can use the maximal-utility ranking  X  M ( R I q ; D ) to estimate the utility of the given ranking,  X  M ( q ; D ), based on their  X  X imilarity X , measures of which we discuss in Section 3.2.3:
In practice, we have no explicit knowledge of the under-lying information need I q , except for the information in q , nor do we have an oracle to provide us with a model of rel-evance. Hence, we use estimates  X  R q for R I q that are based on the information in q and in the corpus. Using statistical decision theory principles, we can approximate Equation 1 by the expected similarity between the given ranking and those induced by the estimates for R I q : U (  X  M ( q ; D ); I q )  X  p (  X 
R q | I q ) is the probability that  X  R q represents I q  X  i.e., that  X  R q is the  X  X rue X  relevance model R I q .

In practice, users X  utility is often determined based on the documents most highly ranked. Indeed, evaluation mea-sures of retrieval effectiveness (e.g., MAP and precision at top k ) consider only top-retrieved documents  X  a.k.a., the result list . This is also the case for many post-retrieval query-performance prediction methods (e.g., [6, 29, 33, 34, 4]) th at analyze the result list. Thus, we confine the utility analysi s to the result list D [ k ] M ; q  X  the k most highly ranked docu-ments in the corpus D by M with respect to q : U (  X  M ( q ; D ); I q )  X  (3) That is, we measure the expected inter-ranking similarity between that of the top-k results of the original ranking and their re-ordering as induced by the relevance model esti-mates; if we set k to the number of documents in the corpus, then Equation 3 reduces to Equation 2.

Equation 3 can be instantiated in numerous ways to yield specific query-performance predictors. We have to (i) deriv e estimates  X  R q for the true relevance model (see Section 3.2.1), (ii) estimate the extent to which these estimates represent the hidden information need ( p (  X  R q | I q ))  X  a task which we address in Section 3.2.2, and (iii) select measures of simil ar-ity between ranked lists (see Section 3.2.3).
We use relevance language models (RM3) [16, 1] for the estimates  X  R q . Relevance-model estimation is performed us-ing a pseudo-feedback-based approach, that is, utilizing a list D [  X  ] QL ; q of the  X  highest ranked documents by the query-likelihood (QL) method [23]. Specifically, if p ( w | x ) is the probability assigned to term w by a language model induced from text (or text collection) x , then the query-likelihood approach scores x in response to q = { q i } by A relevance-model estimate,  X  R q ; S , constructed from a set S (  X  X  [  X  ] QL ; q ) of highly ranked documents, is a probability distribution over the vocabulary: is a free parameter. To rank documents using  X  R q ; S , e.g., in Equation 3, the negative cross entropy (CE) is used:
Following some work on addressing the performance ro-bustness issues of pseudo-feedback-based methods [5, 17, 24], we create relevance-model estimates by sampling sets S of documents from D [  X  ] QL ; q . (See Section 4 for specific de-tails.) Then, we construct from each set S a relevance-model estimate  X  R q ; S using Equation 5, that is used to rank D according to Equation 6. Finally, we approximate the utilit y in Equation 3 by: U (  X  M ( q ; D ); I q )  X  (7)
Quantifying the extent to which a relevance-model esti-mate,  X  R q ; S , represents the information need I q (i.e., p ( is a prediction challenge at its own right [7, 28, 24]. It is im -portant to point out that the framework proposed above is not committed to any specific paradigm of quantifying representativeness. Specifically, there is no coupling in t he framework between the way relevance model estimates are devised and the methods used for quantifying their represen -tativeness. Here, we adapt four effective query-performanc e measures that were originally proposed for predicting the quality of a result list as surrogates for estimates  X  p ( of relevance model representativeness. These four measure s represent the different post-retrieval query-performance pre-diction paradigms surveyed in Section 2.
 Clarity. A natural measure for the representativeness of a relevance-model estimate  X  R q ; S is its clarity [6]; that is, the  X  X istance X  between  X  R q ; S and the corpus model, which can be measured by the KL divergence: The larger the KL divergence is, the more distant  X  R q ; S from the corpus model, and hence, is considered more co-herent ( X  X lear X ). Since  X  R q ; S is constructed from documents highly ranked in response to q , we assume, as in some re-cent work on utilizing multiple relevance models for retrie val [28, 24], that higher clarity indicates better representat ive-ness of the information need. We note that originally [6], the clarity of some relevance model was used as an estimate for the quality of the result list from which the model was constructed, rather than as an estimate for the representa-tiveness of the relevance model.
 WIG. The Clarity measure estimates the representativeness of  X 
R q ; S by directly measuring its  X  X uality X  as a language model. An alternative paradigm is based on estimating the presumed percentage of relevant documents in the set S from which  X  R q ; S is constructed. The higher this presumed per-centage, the better representative  X  R q ; S is assumed to be. Recall that S is a set of documents highly ranked by the query-likelihood method. Hence, estimating the relevant-document percentage in S is a form of a query-performance prediction task. To that end, we use the WIG measure [34]  X  p normalization with respect to the query length, which af-fects query-likelihood scores (see Equation 4), is perform ed for inter-query compatibility.

Note that WIG relies on the premise that high retrieval scores with respect to that of the corpus imply to relevance. Thus, WIG was originally [34] computed based on the re-trieval scores of the most highly ranked documents in the result list so as to predict the quality of the result list its elf. Here, we use WIG to measure the presumed representative-ness of a relevance model constructed from S (  X  X  [  X  ] QL ; q we assume that WIG is correlated with the percentage of relevant documents in S . 2 NQC. A recently proposed query-performance predictor [22], NQC, is based on the hypothesis that the standard devia-tion of retrieval scores in the result list is negatively cor re-lated with the potential amount of query drift [18]  X  i.e., non-query-related information manifested in the list. Spe cif-ically, the mean (QL) retrieval score in the list was shown to be the retrieval score of a pseudo non-relevant document, namely, a centroid of the result list. Thus, result lists wit h retrieval scores much higher/lower than the mean were ar-gued, and shown, to be of high quality.
 Hence, NQC can potentially help to estimate the quality of D
QL ; q , and thereby, the extent to which a relevance-model WIG was originally proposed in the Markov Random Field framework [34]. If no term-dependencies are consid-ered, WIG measures the query-performance of the query-likelihood approach, and is effective to this end [32, 22].
There are similar measures used to this end in work on cluster-based retrieval [14]. estimate constructed from D [  X  ] QL ; q is a good representative of the information need. However, NQC does not have a natural implementation for a subset S of D [  X  ] QL ; q . Therefore, we use (as a rough approximation) for each such S the NQC  X  is the mean retrieval score in D [  X  ] QL ; q ; normalization with the corpus score is for inter-query compatibility [22]. QF. The query-feedback (QF) performance predictor mea-sures ranking robustness [34]. Specifically, the quality of the list D [  X  ] QL ; q is presumed to be correlated with the over-lap between the top-n QF ranked documents in D [  X  ] QL ; q the top-n QF ranked documents in the corpus by a search performed using  X  R from D [  X  ] QL ; q . 3 The overlap is simply the number of shared documents. The idea is that a relevance model constructed from a high quality list would not yield a ranking that drifts much from the original ranking. Thus, here we utilize QF as a measure for the representativeness of  X  R less drift the ranking it induces manifests, the more likely it is to represent the information need [7, 28, 24].

In our experiments we use QF only with  X  R with relevance-model estimates that are constructed from subsets of D [  X  ] QL ; q . (See Section 4 for details.)
The remaining task for instantiating Equation 3 is the esti-mation of the similarity Sim (  X  M ( q ; D [ k ] M ; q ) ,  X  between two rankings of the given result list D [ k ] M ; q use three popular measures for similarity between rankings : Pearson X  X  coefficient, which measures the linear correlatio n between the retrieval scores used to induce the two rankings , and Spearman X  X - X  and Kendall X  X - X  that rely only on ranks. All three correlation measures assign values in [  X  1 , +1].
In what follows we evaluate the effectiveness of predictors instantiated from our proposed framework, denoted UEF for utility estimation framework . As noted above, to de-rive a specific predictor from Equation 7 we have to (i) devise a sampling technique for document sets from which relevance-model estimates are created, (ii) select a repre sen-tativeness measure for relevance-model estimates, and (ii i) select a measure of similarity between ranked lists.
As noted above, we use Clarity, WIG, NQC, and QF as measures for the representativeness of relevance model est i-mates; and, Pearson X  X  coefficient, Spearman X  X - X  , and Kendall X  X - X  as inter-ranking similarity measures. We use two strategie s for sampling sets S of documents from D [  X  ] QL ; q  X  the docu-ments most highly ranked by the query likelihood approach  X  so as to define relevance-model estimates. The first, de-noted Single , is using D [  X  ] QL ; q as a single sampled set. As this To maintain consistency with all other predictors, we use RM3 as defined in Equation 5, which is somewhat different than the query model used originally [34]. is the standard, highly effective, approach for selecting do c-uments for relevance-model estimation [16, 1], the resulta nt predictor instantiated from Equation 7 could be regarded as the posterior-mode-based estimate for the integral in Equa -tion 3 [15]. Note that the predictor in this case is the simi-larity between the ranking induced over D [ k ] M ; q by the single relevance model and the original ranking of D [ k ] M ; q the representativeness estimate of the relevance model.
The second document sampling strategy, Multi , is based on using multiple clusters of similar documents from D [  X  ] [17, 24]. Specifically, we employ a simple nearest-neighbor s-based clustering approach wherein each document d (  X  X  [  X  ] and its  X   X  1 nearest neighbors from D [  X  ] QL ; q serve as a cluster [14]; we use the KL divergence between document language models for a similarity measure [14]. Thus, we sample  X  (overlapping) clusters of  X  documents.
We evaluate the prediction quality of a query-performance predictor by measuring Pearson X  X  correlation between the actual average precision (AP at cutoff 1000) for a set of queries  X  as measured by using relevance judgments  X  and the values assigned to the queries by the predictor [4, 34]. All correlation numbers that we report are statistically si g-nificant at a 95% confidence level.

We conducted experiments on several TREC collections that were used in previous query-performance-prediction s tud-ies [29, 33, 32, 8]. Table 1 provides the details of the collec -tions and topics used.
 Table 1: Test collections and topics. The last column reports the average number of relevant documents per topic.

We use titles of TREC topics for queries, except for TREC4 for which no titles are provided, and hence, topic descrip-tions are used. We applied tokenization, Porter stemming, and stopword removal (using the INQUERY list) to all data via the Lemur/Indri toolkit (www.lemurproject.org), whic h was also used for experiments.

The query likelihood (QL) model [23] from Equation 4 serves as the retrieval model M ; k , the size of the result-list ( D
M ; q ) considered for predicting performance, is set to 150. (Experiments with k = 100 yielded slightly worse perfor-mance.) Recall that documents highly ranked by QL (i.e., in the list D [  X  ] QL ; q ) are those used for relevance-model esti-mation. To downplay the effect of parameter tuning, we set  X  = k in all experiments to follow; hence, D [  X  ] QL ; q Thus, the document list for which we predict query per-formance is also the one utilized for devising estimates of relevance models. We come back to this point in Section 4.2.2. For the Multi sampling strategy, we set the cluster size,  X  , to 20; smaller clusters yield less effective prediction.
The representativeness measures of relevance model esti-mates that we use  X  Clarity, WIG, NQC, and QF  X  are state-of-the-art query-performance predictors at their o wn right. Hence, we use optimized versions of these as refer-ence comparisons to our predictors. The Clarity method uses a relevance model constructed from all documents in D
QL ; q ;  X  = 150 indeed yields optimal Clarity performance.  X  = 150 also yields highly effective prediction performance for the NQC measure as previously reported [22]. The QF measure, as Clarity, uses a relevance model constructed fro m D
QL ; q ; optimal QF prediction performance is attained when setting the number of top documents it depends on, n QF , to 50. The WIG measure is highly effective when using the 5 top-ranked documents in D [  X  ] QL ; q as previously reported [34].
To facilitate comparison with the optimized reference com-parisons just described, we use those as relevance-model re p-resentativeness measures in our framework; except, for WIG and Clarity with Multi sampling that use the information within a cluster for devising a representative measure for t he relevance model constructed from it.
 Language models. We use Dirichlet smoothed unigram document language models with the smoothing parameter,  X  , set to 1000 [30]. To construct a relevance model from a document set using Equation 5, we set  X  = 0 for language models of documents in the set, and  X  = 0 (i.e., we use RM1); all relevance models use 100 terms [1]. These param-eter values yield very good performance both for our predic-tors and for Clarity and QF that utilize relevance models.
In Section 4.2.5 we study the effect of varying the inter-ranking-similarity measure, the document-sets sampling s trat-egy, and the representativeness measure on the instantiate d predictors X  performance. We show that the best performing predictors are those that use Pearson correlation as inter-ranking similarity measure, and a single relevance model estimate ( Single ). Hence, in Sections 4.2.1-4.2.4 we present an in-depth analysis of the performance of these predictors .
We note that the computational overhead posted by these predictors on top of computing the representativeness esti -mates they incorporate, which is performed by current pre-dictors adapted to this end, is quite small. That is, a rel-evance model is constructed from documents in the (short) result list and is used to rank this list; then, Pearson cor-relation between the original list ranking and its relevanc e-model-based ranking is computed.
In what follows we fix the inter-ranking similarity measure to Pearson X  X  coefficient, and use a single relevance model estimate constructed from the entire initial list, D [  X  ] prediction quality of the UEF-based predictors, when using the four representativeness measures for the relevance mod el estimate, is presented in Table 2.

Evidently, our predictors consistently and substantially improve over using the representativeness measures as pre-dictors at their own right  X  i.e., to directly predict search effectiveness; recall that these are state-of-the-art pred ic-tors 4 . A case in point, using UEF with Clarity improves prediction quality by more than 30% on average, over the
Somewhat similar relative prediction performance pattern s are observed when using Kendall X  X - X  rather than Pearson X  X  coefficient to measure prediction quality . Specifically, the average improvements over Clarity, WIG, NQC and QF are 5 TREC benchmarks, with respect to direct use of Clarity for performance prediction. Furthermore, the improvement s over all four representative measures for the WT10G bench-mark are quite striking as WT10G is known to post a hard challenge for performance prediction [12]. The relative im -provements for GOV2, on the other hand, are in general smaller than those for the other collections.
As noted above, the representativeness measures that we use were originally shown to be highly effective predictors for the quality of the initial QL-based ranking from which D
QL ; q was created  X  the task that we pursue here as well  X  rather than for the representativeness of a relevance model constructed from D [  X  ] QL ; q . Thus, when using a single rele-vance model estimate ( Single ), our UEF-based predictors could be viewed in this specific setting as combining two predictors for the ranking quality of D [  X  ] QL ; q itself . The first is the representativeness measure and the second is the sim-ilarity between the original ranking and that induced by the relevance model estimate 5 . We hasten to point out, however, that this specific operational consequence does not contra-dict the fundamentals of our framework. On the contrary, highly effective predictors for the ranking quality of D [  X  ] should serve as effective measures for the representativene ss of the single relevance model constructed from D [  X  ] QL ; q was argued in Section 3.2.2. That is, the more relevant doc-uments there are in D [  X  ] QL ; q , and the higher they are ranked, the higher the quality of the relevance model estimate is (refer to Equation 5).

We thus turn to empirically examine the premise just stated that effective predictors of the quality of the rankin g using which D [  X  ] QL ; q is created are indeed effective measures of the representativeness of the relevance-model construc ted from D [  X  ] QL ; q . To that end, we study the performance of the representativeness measures (Clarity, WIG, NQC, QF) when predicting the quality of the ranking induced by the rele-vance model over the entire corpus 6 . Prediction performance is measured, as usual, by the Pearson correlation between the true AP of the relevance-model-based corpus ranking (at cutoff 1000) and that which corresponds to the predicted val-ues. For reference comparison, we report the performance of using the measures to directly predict the quality of the initial QL-based ranking, as originally proposed.

The results in Figure 1 support our premise. That is, the measures are indeed high-quality representativeness e s-timates for the relevance model, as the high correlation num -bers attest. While in general the measures are more effec-tive in directly predicting the quality of the QL-based init ial ranking rather than serving as representativeness estimat es, the reverse holds for the QF measure over most collections. 26%, 24%, 27%, and 21%, respectively. Specific prediction results are omitted due to space considerations.
This ranking-similarity-based predictor yields predicti on performance of . 607, . 579, . 46, . 578 and . 402 for TREC4, TREC5, WT10G, ROBUST, and GOV2, respectively. Hence, while it is an effective predictor at its own right, its integration with the representativeness measure yield s, in general, much better prediction performance.
Similar prediction performance patterns  X  actual numbers are omitted to avoid cluttering the presentation  X  are ob-served with respect to the ranking induced by the relevance model over the inital list D [  X  ] QL ; q . in the first row of a block. Best result in a column is boldfaced . Figure 1: Effectiveness of the performance predic-tors for estimating the representativeness of a rel-evance model ( RelM ) as measured by the predic-tion quality of the ranking it induces over the cor-pus. The prediction quality of the initial query-likelihood-based ranking is presented for reference.
The finding regarding QF sheds some light on its high ef-fectiveness when used in our framework. (See Table 2.) Re-call that UEF(QF) operates as follows: a relevance model is constructed from D [  X  ] QL ; q and is used to rank the corpus. The overlap between top-ranked documents and those at top-ranks of D [  X  ] QL ; q serves for the relevance-model repre-sentativeness estimate; small overlap presumably attests to query-drift manifested by the relevance model. Then, the es -timate is multiplied by the similarity (Pearson X  X  coefficien t) between D [  X  ] QL ; q  X  X  original ranking and that induced over it by the relevance model.
All the predictors that we have studied insofar operate in the language modeling framework. More specifically, we have focused on predicting the effectiveness of the ranking used to create the list D [  X  ] QL ; q using a relevance language model constructed from this list. We now turn to study the effectiveness of our framework in predicting the quality of a ranking produced by another retrieval method. This challenge fits, for example, the scenario of predicting the performance of a retrieval method that may not be known to the predictor, but rather only the induced ranking and/or retrieval scores.

We predict performance for vector-space-based (VS) re-trieval with the cosine measure, and for Okapi X  X  BM25 re-trieval model. In both cases Lemur X  X  implementation with default parameter settings is used.
 Note that the effectiveness of a ranking of the corpus by M (VS or BM25) is estimated by our predictors as follows. We measure the similarity (Pearson X  X  coefficient) between the original ranking of the result list D [ k ] M ; q of top-k retrieved documents by M , and the ranking of D [ k ] M ; q by a single rele-vance model constructed from D [  X  ] QL ; q  X  the QL-based result list; this similarity is then scaled by the estimated repres en-tativeness of the relevance model. Thus, the result list for which we predict performance, D [ k ] M ; q , is different than the list D [  X  ] QL ; q used to construct a relevance model.
As in Table 2, we want to compare our predictor X  X  perfor-mance with that of applying the representativeness measure as a predictor at its own right directly to D [ k ] M ; q the four measures used, NQC is the only predictor that has a non language-model-based implementation; specifically, it was shown to be effective with VS and BM25 [22]. Hence, in Table 3 we compare the performance of our framework us-ing NQC as a representativeness measure, UEF(NQC), with that of using NQC directly to predict performance 7 . We set  X  = k =150 as at the above.

The results in Table 3 clearly attest to the general ef-fectiveness of our framework. Indeed, UEF(NQC) yields better prediction performance for vector-space and Okapi-BM25 retrieval than that of NQC over most collections. Table 3: Using our framework, UEF(NQC), to predict the performance of vector space (VS) and Okapi-BM25 retrieval in comparison to using NQC to directly predict their performance. Boldface marks the best result in a block.
Integrating predictors using linear interpolation was sho wn to be of merit [8, 34]. As such integration yields a predictor for the quality of the initial list from which we construct a relevance model, we can use it in our framework as a repre-
The original reports for NQC have not used GOV2 [22]. sentative measure. Specifically, when integrating WIG and QF [34] the resultant predictor is denoted UEF(WIG+QF), where WIG+QF is the interpolation-based predictor. We also study the performance of UEF(WIG)+UEF(QF) that interpolates UEF(WIG) and UEF(QF). Interpolation with equal weights is performed in all cases upon the min-max normalized values assigned by predictors. The prediction performance numbers are presented in Table 4.

In accordance with previous findings [34], we see in Table 4 that integrating WIG and QF (WIG+QF) results in perfor-mance superior to that of each over most corpora. Using the integrated predictor in our framework (UEF(WIG+QF)) yields further improvements for 3 out of the 5 corpora. Fur-thermore, for all corpora, except for GOV2, it is better to in -tegrate our predictors that are based on WIG and QF  X  i.e., UEF(WIG) and UEF(QF)  X  than to integrate WIG and QF directly. (Compare UEF(WIG)+UEF(QF) and WIG+QF.) Table 4: Integrating predictors using linear interpo-lation (+). Best result in a column is boldfaced.
Heretofore, we have focused on instantiating our frame-work by utilizing the Single sampling strategy (i.e., a single relevance model estimate), and using Pearson X  X  coefficient t o measure inter-ranking similarities. We now turn to exam-ine the effect of varying these factors, along with the mea-sures used for estimating representativeness. To study the effectiveness of the latter when using Multi sampling (i.e., constructing relevance models from clusters), we use as a reference comparison the Uniform measure that assigns all relevance models the same representativeness value. Table 5 presents the performance of all predictors. 8
We can see in Table 5 that all instantiated predictors are effective as the relatively high performance numbers indi-cate. Specifically, almost all of these predictors yield pos -itive average improvements (refer to the last column) over the representativeness measures that they incorporate whe n the latter are used to directly predict performance. A no-table exception is the ROBUST benchmark with the Multi strategy. (See the below for further discussion.) All in all , as the representativeness measures are state-of-the-art p re-dictors at their own right, we find these results gratifying. Among the inter-ranking similarity measures, we see that Pearson X  X  coefficient in general performs best, attesting to the importance of considering the retrieval scores used to induce the rankings.

We can also see in Table 5 that using a single relevance model estimate ( Single ) yields superior performance to that
Results for QF with Multi are not presented as those re-quire running tens of relevance models per query over the corpus. This is computationally demanding, and accord-ingly, does not constitute a realistic prediction scenario . of utilizing multiple relevance models ( Multi ) constructed from clusters. This finding is not surprising as most rep-resentativeness measures that we use are not well suited for estimating representativeness of a relevance model con -structed from a small cluster, which is composed of some top-retrieved documents. This is of utmost importance as potentially very few of the clusters contain a high percenta ge of relevant documents, and identifying these is a hard chal-lenge [14]. Indeed, WIG, which is based on retrieval scores within the clusters, yields performance that is inferior un der Multi sampling to that of using uniform values for represen-tativeness. On the other hand, Clarity, which measures the  X  X uality X  X f the relevance model constructed from the clust er with respect to the corpus, does improve consistently over uniform representativeness scores. This finding attests to the potential of cluster-based sampling.
We presented a novel framework, which is based on sta-tistical decision theory, for predicting query performanc e. The quality of a given document ranking is predicted based on its expected similarity with those induced by estimates of relevance models; the presumed representativeness of a relevance-model estimate of the underlying information ne ed determines its impact. Relevance-model representativene ss is measured using state-of-the-art query-performance pre -dictors that were originally designed to estimate the quali ty of the initial search. Empirical evaluation shows that pred ic-tors instantiated from our framework are substantially mor e effective than current state-of-the-art predictors.
Improving the sampling technique used for relevance-model construction, and devising and adapting [24] better mea-sures of representativeness for relevance models construc ted from clusters, are future directions we intend to explore. Acknowledgments We thank the reviewers for their com-ments. This paper is based upon work supported in part by Israel X  X  Science Foundation under grant no. 890015. and by G. S. Elkin research fund at the Technion. Any opinions, findings and conclusions or recommendations expressed in this material are the authors X  and do not necessarily reflect those of the sponsoring institutions.
