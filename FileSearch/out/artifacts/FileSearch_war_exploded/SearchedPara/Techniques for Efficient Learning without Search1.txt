 The classical classification learning paradigm performs search through a hypoth-esis space to identify a hypothesis that optimizes some objective function with respect to training data. Averaged n-Dependence Estimators (A n DE) [10] is an approach to learning without search or hypothesis selection, which represents a fundamental alternative to the classical learning paradigm.

The new paradigm gives rise to a family of algorithms, of which, Webb et. al. [10] hypothesize, the different members ar e suited for differing quantities of data. The algorithms range from low variance with high bias through to high variance with low bias. Webb et. al. suggest that members with low variance are suited for small datasets whereas members with low bias are suitable for large datasets. They claim that the asymptotic error of the lowest bias variant is Bayes optimal.
The algorithms in the family possess a unique set of features that are suitable for many applications. In particular, they have a training time that is linear with respect to the number of examples and can learn in a single pass through the training data without any need to maintain the training data in memory. Thus, they show great potential for very accurate classification from large data. Further, they have direct capacity for incremental and anytime [6] learning, are robust in the face of noise and directly handle missing values. Importantly, evaluations have shown that their classi fication accuracy is competitive with the state-of-the-art in machine learning [10].

A n DE extends the underlying strategy of Averaged One-Dependence Estima-tors (AODE) [9], which relaxes the Naive Bayes (NB) independence assumption while retaining many of Naive Bayes X  X  desirable computational and theoretical properties. The third member of the A n DE family, A2DE, has been shown to produce strong predictive accuracy over a wide range of data sets [10].
Although evaluations to date support the hypothesis that the predictive accu-racy of A n DE increases for larger data sets with higher orders of n , the expected increase in accuracy comes a t the cost of increased computational requirements. The current implementations further complicate the matter due to their inef-ficiencies. Thus, efficient implementation is critical. Except in cases of lower dimensional data, the computational requirements defeat a straightforward ex-tension of Weka X  X  AODE [11] to handle A3DE.

This paper presents data structures and algorithms that reduce both memory and time required for both training and classification. These improvements have enabled us to evaluate the effectiveness o f A3DE on large datasets. The results provide further evidence that members of the A n DE family with increasing n are increasingly effective at classifying datasets of increasing size.
The remainder of the paper starts by introducing the A n DE family of al-gorithms. Section 3 outlines the memor y representation developed to reduce memory usage. The enhancements made to reduce testing times are outlined in Section 4. Section 5 presen ts the results of evaluating the effectiveness of the enhancements. It also compares the effectiveness of A3DE with A n DE members with lower n . Finally, conclusions are outlined. The classification problem can be stated as estimating, from a training sample  X  of classified objects, the probability P( y | x )thatanexample x = x 1 ,...,x a belongs to class y ,where x i is the value of the i th attribute and y  X  c 1 ,...,c k that are k classes. As P( y | x )  X  P( y, x ), we only need to estimate the latter.
The naive Bayes (NB) algorithm extrapolates to  X  P( x ,y )fromeachtwodi-mensional probability estimate  X  P( x i | y ), by assuming that attributes are inde-pendent given the class. Based on this assumption, Hence we classify using We assume herein that NB and the other A n DE family members are imple-mented by compiling at training time a table of observed low-dimensional prob-abilities. Under this strategy, the complexity of building this model is O ( ta ), where t is the number of training examples and a the number of attributes. As the model simply stores the frequency of each attribute value for each class after scanning the training examples, the space complexity is O ( kav ), where k is the number of classes and v is the average number of attribute values. As the classi-fier only needs to estimate the probability of each class for the attribute values of the test case, the resulting comp lexity at classification time is O ( ka ).
Despite the attribute independence assumption, NB delivers relatively ac-curate results. However, greater accu racy can be achieved if the attribute-independence assumption is relaxed. New algorithms based on NB have been developed, referred to as semi-Naive Bayesian techniques, that achieve greater accuracy by doing this, as real-world problems generally do have relationships among attributes [12].
 Of numerous semi-naive Bayesian techniques, SP-TAN [7], Lazy Bayesian Rules (LBR) [13] and AODE [9] are among the most accurate. However, SP-TAN has very high computational complexity at training time and LBR has high computational complexity for classification. Contrastingly, AODE a more efficient algorithm, avoids some of the undesirable properties of those algorithms to achieve comparable results. 2.1 AODE AODE extends NB X  X  strategy of extrapolating from lower dimensional probabili-ties to make use of three-dimensional probabilities. It averages across over all of a class of three-dimensional models, which are called super-parent one-dependence estimators (SPODE). Each SPODE relaxes the attribute independence assump-tion of NB by making all other attributes independent given the class and one privileged attribute, the super-parent x  X  .

AODE seeks to use In practice, AODE only uses estimates of probabilities for which relevant exam-ples occur in the data. Hence,  X  P where  X  ( x  X  ) is 1 if attribute-value x  X  is present in the data, otherwise 0. In other words, AODE averages over all super-parents whose value occurs in the data, and defaults to NB if there is no such parent.
 2.2 A n DE A n DE [10] generalises AODE X  X  strategy of search free extrapolation from low-dimensional probabilities to high-dimensional probabilities. The first member of the A n DE family (where n = 0) is NB, the second member is AODE and the third is A2DE. Investigation into the accuracy of higher dimensional models with different training set sizes shows that a higher degree model might be susceptible to variance in a small training sample, and consequently that a lower degree model is likely to be more accurate for small data. On the other hand, higher degrees of A n DE may work better for larger training sets as minimizing bias will be of increasing importance as the size of the data increases [3].
For notational convenience we define
A n DE classifies using:  X  P Attributes are assumed independent given the parents and the class. Hence, P( x | y,x s )isestimatedby Given sufficient training data, A2DE has lower error than AODE, but at the cost of significantly more co mputational resources. In order to support incremental learning, A n DE classifiers compile a table of observed joint frequencies of attribute-value combinations during training. The frequencies table is used in testing to calculate posterior probabilities of class membership. The A n DE classifier requires the joint frequencies of n attribute value combinations per class. Additionally, as the classifier defaults to lower orders of n , for super-parents whose values do not occur in the data, the classifier also requires frequencies of all combinations of length up to n per class. As the space requirement for storing these joint frequencies for higher orders of n is undesirable, we developed a new representation that reduces the required space.
The frequency matrix for AODE is a 3-D matrix, where each cell holds the frequency of a (class, parent, child) combination. As an example, consider the frequency matrix for a dataset with two attributes (A and B). Attribute A has two values ( a 1 and a 2 ), while attribute B has three values ( b 1 , b 2 and b 3 ). The parent and child dimensions of the frequency matrix is illustrated in Fig. 1a. It contains cells for each parent-child combination and the (n,n) locations are reserved for frequencies of parents. The 2 -D structure is replic ated for each class to form the 3-D frequency matrix for AODE.

The representation for A2DE is a 4-D matrix that is a collection of tetrahe-dral structures for each class. Each cell contains the frequencies of (class, par-ent1, parent2, child) combinations. The matrix reserves (class, parent1, parent1, parent1) cells for storing frequencies of class-parent1 combinations and (class, parent1, parent2, parent2) cells for storing class-parent1-parent2 combinations.
A n DE requires a matrix of n + 2 dimensions to store frequencies of all at-tribute value combinations. The outer dimension has k elements for each class. The n middle dimensions represent the n parent attribute values and the final dimension represents the child attribute values. The inner dimensions have av elements, where a is the number of attributes and v is the average number of attribute values (including missing values). Consequently, as the size of the fre-quency matrix is determined by figurate numbers ( P n +1 ( av )= av + n n +1 ), resulting in a memory complexity of O ( k av + n n +1 ).

Although this representation allows for straight forward access of the fre-quency of a class-parent-child combination, the matrix has to be implemented as a collection of arrays. This incurs overhead and the does not guarantee that a contiguous block of memory is allocated for the matrix, reducing the possibility that required parts of the matrix are available in the system X  X  cache.
The frequency matrix can be stored com pactly with the elements of each row stored in consecutive positions. This repr esentation minimises the overheads that can occur with multi-dimensional arrays. Taking AODE as an example, the rows in the 2-D matrix, which are all combinations involving the corresponding parent, can be stored sequentially in a 1-D array as shown in Fig. 1b.

Allocating slots for all combinations of attribute values in the frequency ma-trix simplifies access. However, this produces a sparse matrix containing unused slots allocated for impossible combinations. As training and testing cases have only single valued attributes, combinations of attribute values of the same at-tribute are impossible. In the case of t he AODE example, the frequency matrix possible combinations (shaded in black in Fig. 2a). The size of the frequency matrix can be reduced by avoiding the allocation of memory for such impossible combinations. In the AODE example, the size of the 2-combinations matrix can be reduced from 10 to 6. The size of the n combinations matrix is a n +1 v n +1 .
To avoid allocating space for impossible combinations and simplify indexing, the frequency matrix is decomposed into a series of structures for storing at-tribute value combinations of a specific length. Taking the AODE example, the set of 1-D arrays for storing only possible attribute value combinations is shown in Fig. 2b. Array freq1 contains frequencies of each attribute value and Array freq2 contains frequencies of all valid attribute value pairs. A n DE classifies a test instance by calculating posterior probabilities of class membership. They are calculated by iterating through all parent-child permu-tations, resulting in a time complexity of O ( ka ( n +1) ). We reduced the overall testing time by reorganising the frequency matrix and the looping structure to taking advantage of locality of reference.

The CPU cache is a fast but limited memory resource, which stores copies of most frequently used data. It is used t o reduce average time to access memory. We reorganized the memory representat ion and minimized data retrieval from memory to improve the likelihood of availability of data in the CPU cache.
The compact memory representation for the frequency matrix is a 2-D array, which contains k copies of arrays that record n-co mbination attribute value fre-quencies per class. For example, Fig. 3a illustrates the memory representation for a dataset with two attributes (A and B) and two classes ( c 1 and c 2 ). The 2-D representation is poorly suited for accessing all class frequencies of some attribute-value combination. Especially, in the case of datasets with large collec-tion of attributes, this representation reduces the likelihood of all the per-class frequencies of some attribute value combination being available in the system X  X  high-speed access cache.

The locality of reference of attribute-value combinations for all classes can be improved by storing them next to each other. Taking AODE as an example, the 2-D frequency matrix (Fig. 3a) can be rep resented in a 1-D array by interleaving the per class frequencies as shown in Fig . 3b. This representation improves the chances of the frequencies of attribute-value combinations for both the classes being available in high-performance memory. In order to take full advantage of locality of reference of class frequency co mbinations the looping structure of the classifier also had to be rearranged from looping through each class, parent and child to loop through each parent, child and class.

A n DE requires conditional probabilities for all parent child permutations. It-erating through all permutations requires all relevant offsets to be retrieved, indexes to be calculated and the relevant frequencies to be retrieved. Although these retrievals would be loaded into the CPU cache, they are only used once. In order to reuse data and improve the likelihood of data being available in the CPU cache, we modified the implementation to only iterate over unique com-binations. During each iteration conditional probabilities for all permutations of each combination are calculated. This results in reducing the iterations from ka ( n +1) to ka ( n ) and reducing the total number of memory accesses.
The conditional probability of a parent-child attribute permutation is calcu-lated by dividing the frequency of parent-child attributes occurring together by the frequency of parents. The numerator is constant for all permutations of a parent combination. The improved implementation also allows this numerator to be reused, reducing the amount of frequency fetches and the number of index cal-culations. Overall, this reduces the num ber of frequency accesses of parent-child attribute value combinations to 1 2 for AODE, 1 3 for A2DE and 1 4 for A3DE. The effectiveness of the improvements to reduce memory usage and testing times were evaluated on a collection of Datasets from the UCI machine learning repository[1]. The evaluation was focused on three members of the A n DE family of algorithms: AODE, A2DE and A3DE. Although NB is the first member of the A n DE family, it was not evaluated as the improvements are unlikely to have any impact. The improvements were compared against the Weka version of AODE and naive versions of A2DE and A3DE. 5.1 Test Environment We selected nine datasets, described in Table 1, from the UCI machine learning repository for the comparisons. The chosen collection includes small, medium and large datasets with small, medium and high dimensionality. The datasets were split into two sets, with 90% of the data used for training and the remaining 10% used for testing. The experiments were conducted on a single CPU single core virtual Linux machine running on a Dell PowerEdge 1950 with dual quad core Intel Xeon E5410 processor running at 2.33GHz with 8 GB of RAM.
The implementations of the three algorithms of the A n DE family are lim-ited to categorical data. Consequently, al l numerical attribut es are discretized. When MDL discretization [5], a commo n discretization method for NB, was used within each cross-validation fold, we identified that many attributes have only one value. So, we discretized numerical attributes using three-bin equal-frequency discretization prior to cl assification for these experiments.
The memory usage of the classifier was measured by the  X  X lassmexer X  tool [4], which uses Java X  X  instrumentation framework to query the Java virtual machine (JVM). It follows relations of objects, so that the size of the arrays inside arrays are measured, including their object overhead and padding.

Accurately measuring execution time for the Java platform is difficult. There can be interferences due to a number of JVM processes such as garbage col-lection and code profiling. Consequent ly, to make accurate execution time mea-surements, we use a Java benchmarking framework [2] that aims to minimize the noise during measurements. The framework executes the code for a fixed time period (more than 10 seconds) to allow the JVM to complete all dynamic optimizations and forces the JVM to perform garbage collection before mea-surements. All tests are repeated in c ases where the code is modified by the JVM. The code is also executed a number of times with the goal of ensuring the cumulative execution time to be large enough for small errors to be insignificant. 5.2 Optimised Memory Consumption The memory usage for A n DE was reduced by the introduction of a new data structure that avoids the allocation of space for impossible combinations. The reductions in memory usage for the enhanced A n DE implementations were com-pared against the respective versions of A n DE that stores the frequency matrix in a single array. We do not present the memory reductions of compacting the multi-dimensional array into one dimension as they are specific to Java.
The reductions in memory usages are summarised in Fig. 4a. Results show that the memory reduction for AODE ranged from 1% to 14%. The highest percentage in reduction was observed for the adult dataset, which had a reduction of 9.67KB. The main reason for the large reduction is the high average number of attribute values of 8.36 for the adult dataset. In contrast, the other datasets have average number of attribute values of around 3.
The memory usage for A2DE was reduced by a minimum of 9% to a maximum of 53%. The maximum amount of reduction in memory was observed for the high-dimensional Covertype dataset, which had a reduction of around 4.68MB.
The enhanced version of A3DE resulted in the highest reduction in memory usage with reductions ranging from 13% to 64%. The reductions in memory usage for the high dimensional Covertype, Dermatology, Sonar Classification and SPAM E-mail datasets were over 100MB. 5.3 Optimised Testing The total testing times of the algorithms were compared using the test envi-ronment. The proportions of reduction in mean test times for A n DE are given in Fig. 4b. The optimisations result in reductions in average testing times for all three algorithms. The reductions for A3DE were highest, with a 61% (0.83s) mean reduction for the small but high-dimensional Dermatology dataset and 60% (8.89ks) reduction for the large and high-dimensional Covertype dataset. The improvements also reduced the testing times of low dimensional datasets of Abalone (6%) and House Votes (28%).

The reductions in testing times were s ubstantial for A2DE, with reductions ranging from 16% (for Abalone) to 50% (Covertype). The improvements to AODE also resulted in reduced total execution times ranging from 23% to 30%. The highest percentage of reduction was exhibited for the dataset with the largest number of attributes, Sonar Classification. We evaluated the classification accuracy of A n DE algorithms comparing how their performance varies as n increases within the A n DE framework. Previous research [10] has compared the effectiveness of AODE to A2DE, but only limited experimental results were presented fo r A3DE as the Weka implementation failed on high dimensional datasets due to its high memory requirements.

We compared the effectiveness the A n DE members using the enhanced ver-sions implemented in the Weka workbench on the 62 datasets that were used to evaluate the performance of A2DE [10]. Each algorithm was tested on each dataset using the repeated cross-validation bias-variance estimation method [8]. We used two-fold cross validation to maximise variation in the training data be-tween trials. In order to minimise the var iance in our measurements, we report the mean values over 50 cross-validation trials.

The experiments were conducted on the same virtual machine used to evaluate the effectiveness of the improvements. Due to technical issues, including memory leaks in the Weka implementation, increasing amounts of memory is required when multiple trials are conducted. Consequently, we were unable to get bias-variance results for four datasets (Audiology, Census-Income, Covertype and Cylinder bands), that were of high dimensionality. We compared the relative performances of AODE, A2DE and A3DE on the remaining 58 datasets. The lower, equal or higher outcomes when the algorithms are compared to each other is summarised as win/draw/loss records in Tab. 2.

The results show that the bias decreases as n increases at the expense of increased variance. The bias of A3DE is l ower significantly more often than not in comparison to A2DE and AODE. The bias of A2DE is lower significantly more often relative to AODE. In contrast, the variance of AODE is lower significantly more often than A2DE and A3DE. The variance of A2DE is lower significantly more often relative to A3DE.

None of the three algorithms have a significantly lower zero-one loss or RMSE on the evaluated datasets. We believe that this is due to the wide range sizes of datasets used in the evaluation. We hypothesize that members of the A n DE family with lower n , that have a low variance, are best suited for small datasets. In contrast, members with higher degrees of n are best suited for larger datasets. 6.1 A3DE Performance on Large Datasets To assess the hypothesis that increasing values of n within the A n DE family are suited to increasing data quantity, we compared A3DE to lower-order fam-ily members on datasets with over 10,000 cases. Out of the 58 datasets, seven datasets (Adult, Connect-4 Opening, Letter Recognition, MAGIC Gamma Tele-scope, Nursery, Pen Digits and Sign) satisfied this criterion. The number of cases of the chosen datasets ranged from just over 10,000 cases (Pen Digits) to over 60,000 cases (Connect-4 Opening).
 The evaluation results are summarised as win/draw/loss records in Table 3. As expected, the results show A3DE has a lower bias and higher variance than A2DE and AODE. The zero-one loss and the RMSE of A3DE are lower for all the evaluated datasets in comparison to A2DE and AODE (p=0.008). These results confirm that A3DE performs better than its lower-dimensional variants at classifying larger datasets. The A n DE family of algorithms perform search-free learning. The parameter n controls the bias-variance trade-off such that n = a provides a classifier whose asymptotic error is the Bayes optimum. We presented techniques for reducing the memory usage and the testing times of the A n DE implementations that make A3DE feasible to employ for higher-dimensional data. As A3DE is superior to A n DE with lower values of n when applied to large data, and as the linear complexity and single pass learning of A n DE make it particularly attractive for learning from large data, we believe these optimizations have potential for considerable impact.

We developed a new compact memory representation for storing the frequen-cies of attribute-value combinations that stores all frequencies in a 1-D array avoiding the allocation of space for impossible attribute-value combinations. The evaluation results showed that the e nhancements substantially reduced the memory requirements. The enhancem ents reduced the overall A3DE memory requirements ranging from 13% to 64%, including reductions of over 100MB for the high-dimensional datasets.

The classification times of the A n DE algorithms were improved by reorganis-ing the memory representation to maximise locality of reference and minimising memory accesses. These enhancements resu lted in substantial reductions to the total testing times for the A n DE family of algorithms. In the case of A3DE, the maximum reduction in total testing time was 8.89ks, which was a reduction of 60%, for the Covertype dataset.

The enhancements to the A n DE algorithms opened the door for evaluating the performance of A3DE. As expected, t he results showed that A3DE has lower bias in comparison to A2DE and AODE. The results for zero-one error between A3DE, A2DE and AODE did not produce a clear winner. However, A3DE pro-duced the lowest error for large datasets (with over 10,000 cases).

The computational complexity of A n DE algorithms is linear with respect to the number of training examples. Thei r memory requirements are dictated by the number of attribute values in the data. Consequently, the most accurate and feasible member of the A n DE algorithm for a particular dataset will have to be decided based on the dataset X  X  size and its dimensionality.

