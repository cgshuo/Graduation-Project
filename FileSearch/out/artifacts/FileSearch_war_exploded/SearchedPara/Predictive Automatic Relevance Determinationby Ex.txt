 Yuan (Alan) Qi yuanqi@media.mit.edu MIT Media Laboratory, Cambridge, MA, 02139 USA Thomas P. Minka minka@microsoft.com Microsoft Research, 7 J J Thomson Ave, Cambridge, CB3 0FB, UK Rosalind W. Picard picard@media.mit.edu MIT Media Laboratory, Cambridge, MA, 02139 USA Zoubin Ghahramani zoubin@gatsby.ucl.ac.uk In many real-world classification and regression prob-lems the input consists of a large number of features or variables, only some of which are relevant. Inferring which inputs are relevant is an important problem. It has received a great deal of attention in machine learn-ing and statistics over the last few decades (Guyon &amp; Elisseeff, 2003).
 This paper focuses on Bayesian approaches to deter-mining the relevance of input features. One of the most successful methods is called Automatic Relevance De-termination (ARD) (MacKay, 1992; Neal, 1996). This is a hierarchical Bayesian approach where there are hyperparameters which explicitly represent the rele-vance of different input features. These relevance hy-perparameters determine the range of variation for the parameters relating to a particular input, usually by modelling the width of a zero-mean Gaussian prior on those parameters. If the width of that Gaussian is zero, then those parameters are constrained to be zero, and the corresponding input cannot have any effect on the predictions, therefore making it irrelevant. ARD opti-mizes these hyperparameters to discover which inputs are relevant. 1 Automatic Relevance Determination optimizes the model evidence , also known as the marginal likelihood , which is the classic criterion used for Bayesian model selection. In this paper we show that while this is often effective, in cases where there are a very large number of input features it can lead to overfitting. We instead propose a different approach, called predic-tive ARD , which is based on the estimated predictive performance. We show that this estimated predictive performance can be computed efficiently as a side ef-fect of the expectation propagation algorithm for ap-proximate inference and that it performs better that the evidence-based ARD on a variety of classification problems.
 Although the framework we present can be applied to many Bayesian classification and regression models, we focus our presentation and experiments on classifica-tion problems in the presence of irrelevant features as well as in sparse Bayesian learning for kernel methods. Compared to the traditional ARD classification, this paper presents three specific enhancements: (1) an ap-proximation of the integrals via Expectation Propaga-tion, instead of Laplace X  X  method or Monte Carlo; (2) an ARD procedure which minimizes an estimate of the predictive leave-one-out generalization error (obtained directly from EP); (3) a fast sequential update for the hyperparameters based on Faul and Tipping (2002) X  X  recent work. These enhancements improve classifica-tion performance.
 The rest of this paper is organized as follows. Section 2 reviews the ARD approach to classification and its properties. Section 3 presents predictive ARD by EP, followed by experiments and discussions in section 4. A linear classifier classifies a point x according to t = sign( w T x ) for some parameter vector w (the two classes are t =  X  1). Given a training set D = written as tive distribution function for a Gaussian. One can also use the step function or logistic function as  X (  X  ). The basis function  X  T ( x i ) allows the classification bound-ary to be nonlinear in the original features. This is the same likelihood used in logistic regression and in Gaussian process classifiers. Given a new input x N +1 , we approximate the predictive distribution: where  X  w  X  denotes the posterior mean of the weights, called the Bayes Point (Herbrich et al., 1999). The basic idea in ARD is to give the feature weights independent Gaussian priors: where  X  = {  X  i } is a hyperparameter vector that con-trols how far away from zero each weight is allowed to go. The hyperparameters  X  are trained from the data by maximizing the Bayesian  X  X vidence X  p ( t |  X  ), which can be done using a fixed point algorithm or an EM algorithm treating w as a hidden variable (MacKay, 1992). The outcome of this optimization is that many elements of  X  go to infinity such that w would have only a few nonzero weights w j . This naturally prunes irrelevant features in the data. Later we will discuss why ARD favors sparse models (section 2.2). 2.1. ARD-Laplace Both the fixed point and EM algorithms require the posterior moments of w . These moments have been approximated by second-order expansion, i.e. Laplace X  X  method (MacKay, 1992), or approx-imated by Monte Carlo (Neal, 1996). ARD with Laplace X  X  method ( ARD-Laplace ) was used in the Relevance Vector Machine (RVM) (Tipping, 2000). The RVM is a linear classifier using basis functions  X  ( x ) = [ k ( x , x 1 ) ,k ( x , x 2 ) ,  X  X  X  ,k ( x , x Laplace X  X  method approximates the evidence by a Gaussian distribution around the maximum a poste-riori (MP) value of w , w MP , as follows:
If we use a logistic model for  X (  X  ), then the Hessian matrix H has the following form: H =  X  ( X  B  X  T + A ), where  X  =  X  ( x i ) ,..., X  ( x N ) is a d by N matrix, A = diag(  X  ), and B is a diagonal matrix with B ii = Laplace X  X  method is a simple and powerful approach for approximating a posterior distribution. But it does not really try to approximate the posterior mean; in-stead it simply approximates the posterior mean by the posterior mode. The quality of the approximation for the posterior mean can be improved by using EP as shown by Minka (2001). 2.2. Overfitting of ARD Overfitting can be caused not only by over-complicated classifiers, but also by just picking one from many sim-ple classifiers that can correctly classify the data. Con-sider the example plotted in figure 1. The data labelled with  X  X  X  and  X  X  X  are in class 1 and 2 respectively. Every line through the origin with negative slope separates the data. If you apply the regular Bayes Point linear classifier, you get a classifier of angle 135  X  (shown); if you apply ARD to maximize evidence, you end up with a horizontal line which is sparse, becaues it ignores one input feature, but seemingly very dangerous. Both horizontal and vertical sparse classifiers have larger evidence values than the one of 135  X  , though both of them are intuitively more dangerous. Having ef-fectively pruned out one of the two parameter dimen-sions (by taking one of the  X  s to infinity), the evidence for the horizontal and vertical classifiers involves com-puting an integral over only one remaining dimension. However, the evidence for the classifier which retains both input dimensions is an integral over two parame-ter dimensions. In general, a more complex model will have lower evidence than a simpler model if they can both classify the data equally well. Thus ARD using evidence maximization chooses the  X  X impler X  model, which in this case is more dangerous because it uses only one relevant dimension.
 ARD is a Type II maximum likelihood method and thus subject to overfitting as the somewhat dramatic albeit contrived example above illustrates. However, it is important to point out that the overfitting resulting from fitting the relevance hyperparameters  X  is not the same kind of overfitting as one gets from fitting the parameters w as in classical maximum likelihood methods. By optimizing w one can directly fit noise in the data. However, optimizing  X  only corresponds to making decisions about which variables are irrelevant, since w is integrated out. In the simple case where  X  can take only two values, very large or small, and the input is d -dimensional, choosing  X  corresponds to model selection by picking one out of 2 d subsets of input features. This is only d bits of information in the training data that can be overfit, far fewer than what can be achieved by precisely tuning a single real-valued parameter w . However, when d is large, as in the practical examples in our experimental section, we find that even this overfitting can cause problems. This motivates our proposed predictive measure for ARD based on estimating leave-one-out performance. 2.3. Computational Issues To compute the posterior moments of w required by ARD-Laplace, we need to invert the Hessian matrix H for each update. This takes O ( d 3 ) time, which is quite expensive when the dimension d is large. In this section, we improve ARD-Laplace in three ways: replacing Laplace X  X  method by more accurate EP, estimating the predictive performance based on leave-one-out estimate without actually carrying out the expensive cross-validation, and incorporating a fast sequential optimization method into ARD-EP. 3.1. EP for Probit Model The algorithm described in this section is a variant of the one in (Minka, 2001), and we refer the reader to that paper for a detailed derivation. Briefly, Expecta-tion Propagation (EP) exploits the fact that the likeli-hood is a product of simple terms. If we approximate each of these terms well, we can get a good approx-imation to the posterior. Expectation Propagation chooses each approximation such that the posterior us-ing the term exactly and the posterior using the term approximately are close in KL-divergence. This gives a system of coupled equations for the approximations which are iterated to reach a fixed-point.
 Denote the exact terms by g i ( w ) and the approximate terms by  X  g i ( w ):
The approximate terms are chosen to be Gaussian, pa-rameterized by ( m i ,v i ,s i ):  X  g i = s i exp(  X  1 2 v m i ) 2 ) . This makes the approximate posterior distribu-tion also Gaussian: p ( w | t ,  X  )  X  q ( w ) = N ( m w , V To find the best term approximations, proceed as fol-lows: (to save notation, t i  X  i is written as  X  i ) 1. Initialization Step: Set  X  g i = 1: v i =  X  , m i = 0, 2. Loop until all ( m i ,v i ,s i ) converge: 3. Finally, compute the normalizing constant and The time complexity of this algorithm is O ( d 2 ) for pro-cessing each term, and therefore O ( Nd 2 ) per iteration. 3.2. Estimate of Predictive Performance A nice property of EP is that it can easily offer an es-timate of leave-one-out error without any extra com-putation. At each iteration, EP computes in (4) and (5) the parameters of the approximate leave-one-out posterior q \ i ( w ) that does not depend on the i th data point. So we can use the mean m \ i w to approximate a classifier trained on the other ( N  X  1) data points. Thus an estimate of leave-one-out error can be com-puted as where  X (  X  ) is a step function. An equivalent estimate was given by Opper and Winther (2000) using the TAP method for training Gaussian processes, which is equivalent to EP (Minka, 2001).
 Furthermore, we can provide an estimate of leave-one-out error probability. Since Z i in (6) is the posterior probability of the i th data label, we propose the fol-lowing estimator: where Z i and z i is defined in (6) and (7). In (7),  X  i is the product of t i and  X  ( x i ). By contrast, Op-per and Winther estimate the error probability by label t i . Notice that pred utilizes the variance of w given the data, not just the mean. However, it as-sumes that the posterior for w has Gaussian tails. In reality, the tails are lighter so we compensate by pre-scaling z i by 50, a number tuned by simulations. 3.3. Fast Optimization of Evidence This section combines EP with a fast sequential opti-mization method (Faul &amp; Tipping, 2002) to efficiently update the hyperparameters  X  .
 As mentioned before, EP approximates each classi-fication likelihood term g i ( w ) =  X ( w T  X  i ) by  X  g s t  X  ( x i ) as in the previous section. Notice that  X  g i the same form as a regression likelihood term in a regression problem. Therefore, EP actually maps a classification problem into a regression problem where ( m i ,v i ) defines the virtual observation data point with mean m i and variance v i . Based on this interpretation, it is easy to see that for the approximate posterior q ( w ), we have where we define and  X  = (  X  i ,..., X  N ) is a d by N matrix.
 To have a sequential update on  X  j , we can explicitly decompose p ( D |  X  ) into two parts, one part denoted by p ( D |  X  \ j ), that does not depend on  X  j and another that does, i.e.,  X  rows of the data matrix  X  respectively.
 Using the above equation, Faul and Tipping (2002) show p ( D |  X  ) has a maximum with respect to  X  j : where  X  j = u 2 j  X  r j . Thus, in order to maximize the evidence, we introduce the j th feature when  X  j =  X  and  X  j &gt; 0, exclude the j th feature when  X  j &lt;  X  and  X  j  X  0, and reestimate  X  j according to (15) when  X  j &lt;  X  and  X  j &gt; 0. To further save computation, we can exploit the following relations:  X  tains only the features that are currently included in the model, and  X  m w and  X  V w are obtained based on these features. This observation allows us to efficiently compute the EP approximation and update  X  , since in general there are only a small set of the features in the model during the updates. 3.4. Algorithm Summary To summarize the predictive-ARD-EP algorithm: 1. First, initialize the model so that it only contains 2. Then, sequentially update  X  as in section 3.3 and 3. Finally, choose the classifier from the sequential A variant of this algorithm uses the error probability (13) and is called predictive Prob -ARD-EP . Choos-ing the classifier with the maximum evidence (approx-imated by EP) is called evidence-ARD-EP . This section compares evidence-ARD-EP and predictive-ARD-EP on synthetic and real-world data sets. The first experiment has 30 random training points and 5000 random test points with dimension 200. True classifier weights consist of 10 Gaussian weights, sampled from N (  X  0 . 5 , 1) and 190 zero weights. The true classifier is then used as the ground truth to label the data. Thus the data is guaranteed to be separable. The basis functions are simply  X  ( x ) = x . The results over 50 repetitions of this procedure are visualized in figure 3-(a). Both predictive-ARD-EP and predictive Prob -ARD-EP outperform evidence-ARD-EP by picking the model with the smallest estimate of the predictive error, rather than choosing the most probable model.
 Figure 2-(a) shows a typical run. As shown in the fig-ure, the estimates of the predictive performance based on leave-one-out error count (12) and (log) error prob-ability (13) are better correlated with the true test er-ror than evidence and the fraction of features. The ev-idence is computed as in equation (11) and the fraction of features is defined as || w || 0 d where d is the dimension of the classifier w . Also, since the data is designed to be linearly separable, we always get zero training error along the iterations. While the (log) evidence keeps in-creasing, the test error rate first decreases and then in-creases. This demonstrates the overfitting problem as-sociated with maximizing evidence. As to the fraction of features, it first increases by adding new useful fea-tures into the model and then decreases by deleting old features. Notice that the fraction of features converges to a lower value than the true one, 10 200 = 0 . 05, which is plotted as the dashed line in figure2-(a). More-over, choosing the sparsest model, i.e., the one with the smallest fraction of features, leads to overfitting here even though there is zero training error. Next, the algorithms are applied to high-dimensional gene expression datasets: leukaemia and colon cancer. For the leukaemia dataset, the task is to distin-guish acute myeloid leukaemia (AML) from acute lym-phoblastic leukaemia (ALL). The dataset has 47 and 25 samples of type ALL and AML respectively with 7129 features per sample. The dataset was randomly split 100 times into 36 training and 36 test samples, with evidence-ARD-EP and predictive-ARD-EP run on each. Figure 2-(b) shows a typical run that again illustrates the overfitting phenomenon as shown in fig-ure 2-(a). On most of runs including the one shown in figure 2-(b), there is zero training error from the first to the last iterations. The test performance is visualized in figure 3-(b). The error counts of evidence-ARD-EP and predictive-ARD-EP are 3 . 86  X  0 . 14 and 2 . 80  X  0 . 18, respectively. The numbers of the chosen features of these two methods are 2 . 78  X  1 . 65 and 513 . 82  X  4 . 30, respectively.
 For the colon cancer dataset, the task is to discrim-inate tumour from normal tissues using microarray data. The whole dataset has 22 normal and 40 cancer samples with 2000 features per sample. We randomly split the dataset into 50 training and 12 test samples 100 times and run evidence-ARD-EP and predictive-ARD-EP on each partition. The test performance is visualized in figure 4. For comparison, we show the re-sults from Li et al. (2002). The methods tested by Li et al. (2002) include ARD-Laplace with fast sequen-tial updates on a logistic model (Seq-ARD-Laplace), Support Vector Machine (SVM) with recursive fea-ture elimination (SVM-RFE), and SVM with Fisher score feature ranking (SVM-Fisher Score). The er-ror counts of evidence-ARD-EP and predictive-ARD-EP are 2 . 54  X  0 . 13 and 1 . 63  X  0 . 11, respectively. The sizes of chosen feature sets for these two methods are 7 . 92  X  0 . 14 and 156 . 76  X  11 . 68, respectively. As shown in figures 3-(b) and 4, pruning irrelevant fea-tures by maximizing evidence helps to reduce test er-rors. But aggressive pruning will overfit the model and therefore increase the test errors. For both colon can-cer and leukaemia datasets, predictive-ARD-EP with a moderate number of features outperforms all the other methods including EP without feature pruning as well as the evidence-ARD-EP with only a few features left in the model.
 Finally, RBF-type feature expansion can be combined with predictive-ARD-EP to obtain sparse nonlinear Bayesian classifiers. Specifically, we use the following Gaussian basis function  X  ( x i ) feature selection in the previous examples, here ARD is used to choose relevance vectors  X  ( x i ), i.e., to se-lect data points instead of features. The algorithms are tested on two UCI datasets: breast cancer and di-abetes.
 For the breast cancer dataset provided by Zwitter and Soklic (1998), the task is to distinguish no-recurrence-events from recurrence-events. We split the dataset into 100 training and 177 test samples 50 times and run evidence-ARD-EP and predictive-ARD-EP on each partition. The test performance is visualized in fig-ure 5 and summarized in table 1.
 In this experiment, evidence-ARD-EP and predictive-ARD-EP marginally outperform the other alterna-tives, but give much simpler models. They only have about 4 or 10 relevance vectors while SVM uses about 65 support vectors.
 Finally evidence-ARD-EP and predictive-ARD-EP are tested on the UCI diabetes dataset. Each is run on 100 random partitions of 468 training and 300 test samples. The partitions are the same as in R  X atsch et al. (2001), so that we can directly compare our results with theirs. The test performance is sum-marized in figure 4. The error rates of the evidence-ARD-EP and predictive-ARD-EP are 23 . 91  X  0 . 21% and 23 . 96  X  0 . 20%, respectively.
 On the diabetes dataset, predictive-ARD-EP performs comparably or outperforms most of the other state-of-the-art methods; only SVM performs better. Note that the SVM X  X  kernel has been optimized using the test data points, which is not possible in practice (R  X atsch et al., 2001). Predictive-ARD-EP is an efficient algorithm for fea-ture selection and sparse learning. Predictive-ARD-EP chooses the model with the best estimate of the predictive performance instead of choosing the one with the largest marginal likelihood. On high-dimensional micorarray datasets, Predictive-ARD-EP outperforms other state-of-the-art algorithms in test accuracy. On UCI benchmark datasets, it results in sparser classifiers than SVMs with comparable test ac-curacy. The resulting sparse models can be used in applications where classification time is critical. To achieve a desired balance between test accuracy and testing time, one can choose a classifier that minimizes a loss function trading off the leave-one-out error esti-mate and the number of features.
 The success of this algorithm argues against a few popular principles in learning theory. First, it argues against the evidence framework in which the evidence is maximized by tuning hyperparameters. Maximizing evidence is useful for choosing among a small set of dis-tinct models, but can overfit if used with a large con-tinuum of similar models, as in ARD. Second, our find-ings show that larger fraction of nonzero features (or lower sparsity) can lead to better generalization per-formance, even when the training error is zero. This is against the sparsity principles as well as Occam X  X  razor. If what we care about is generalization perfor-mance, then it is better to minimize some measure of predictive performance as predictive ARD does.
