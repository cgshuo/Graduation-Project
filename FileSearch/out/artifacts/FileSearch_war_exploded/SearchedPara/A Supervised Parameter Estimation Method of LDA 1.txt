 Probabilistic topic model are receiving extensive attention in text mining, information proposed by Blei et al. [1] is one of the most notable and most successful probabilistic topic models for unsupervised and supervised learning. Especially for the text classi-fication problem, LDA is a very effective dimension-reduction tool which can dimensional semantic topic space. In text classification, LDA is commonly unsupervised because the parameters of LDA are estimated without reference to category label of the documents in the training corpus. And the terms (or words) in LDA vocabulary are viewed as equally important, but the category discriminating of each term is different, especially for a skewed corpus in which there are many more samples of some categories than others. In other words, LDA ignores the valuable information, that is, its two default assumptions are that both the training corpus is balanced and each terms in vocabulary is equally important. Un-classification.

To address this shortcoming, this paper will propose a novel parameter estimation method based on category and document information which can estimate the parame-LDA model and its classical Gibbs Sampling parameter estimation method. Based on the analysis in section 3, a surpervised parameter estimation method will be presented In section 5, we will present our comparative experiments of this new method. Final-ly, section 6 will give the conclusions. directions: parameters extension, context introduction, and orienting special task [2]. However, a few researchers pay attention to term weight in LDA. In fact, the original LDA [1] didn X  X  mention how to build vocabulary, and in subsequent sLDA (super-vised Latent Dirichlet Allocation) [3] the vocabulary was chosen by TF-IDF which computed the weight of a term using the product of the term frequency (TF) and the Compound Multinomial model using TF-IDF for term weighting. Similarly, Reisinger model. 
Moreover, Zhang et al. [7] proposed a weighted LDA model in which the weight of a term is computed based on Gauss function.Wilson et al. [8] extended the LDA mod-el by accommodate the Pointwise Mutual Information to compute term weight.The LDA. 
However, term weight computed by the above methods can only reflect the docu-corpus, most terms chosen by them may be come from a majority category, which will tend to degrade the performance of classifier directly. But at present few scholars focus on using LDA model for dimensionality reduction of the skewed corpus. most popular solution pursue to improve tr aditional term selection method, which are Document Frequency (DF), Information Gain (IG) and so on, or traditional term weighting method, such as TFIDF. For example, Wu et al. [9] proposed a novel term selection method based on category DF, Xu et al. [10] introduced Inverse Document TF-IDF-IG. Their experiments showed these methods can largely increase the preci-propose a new term weighting method based on TFIDF, and accommodate it to LDA for dimensionality reduction of the skewed corpus. as a collection of correlative words, thus each document can be represented using the latent topics. 
The basic idea of LDA can be thought as be origined from Latent Semantic Index-associations of words and constructs a lower-dimension latent semantic feature space. for probabilistic topic model. After LSI, an alternative to LSI named probabilistic LSI (pLSI) was introduced by Hoffmann. The basic idea of pLSI is a document is a mixture of topics and a topic is a mixture of words. In pLSI the concept  X  X opi c X  appreares clearly, thus pLSI is regarded as the actual origin of probabilistic topic model. However, the two parameters of pLSI the topic distributions for each document an d the word distributions for each topic don X  X  be treated as random variables. For this reason, pLSI is not a complete prob-abilistic topic model. 
From Bayesian School X  X  opinion, every parameter should be random variable and every random varialble should follow a prior distribution. So pLSI model is extended by treating the two parameters of pLSI as random variables and introducing Dirichlet are estimated by Bayesian method. The graphical model of LDA is depicted in Fig. 1. Where w refers to the observed word in a document which contains N words, z refers word distribution for each topic,  X  and  X  are hyperparameters for Dirichlet prior dis-document, and K is the number of latent topics in the corpus. 
The generative process for a corpus under the LDA model is as follows. 1. Choose  X  k ~ Dirichlet (  X  ), k  X  [1,  X  ] 2. For each document m  X  [1,  X  ] 
That is, to make a new document, at first LDA chooses  X  k (k  X  [1,  X  ]) where  X  i, j = w under topic j , after that, for each word in the current document, chooses a topic z m, , and draws a word w m, n from that topic z m, n . In such LDA model, the probability of a word w i within a document is: 
Furthermore, for a corpus consists of M documents and K latent topics, let  X   X  X   X  Based on this, both  X  and  X  are the main objectives of LDA inference where  X  repre-sents a K  X  W (W is the size of the vocabulary) matrix and  X  represents a M  X  K matrix. which topics are important for a particular document, respectively. directly estimating them, another approach is to directly estimate the posterior distri-proach is Gibbs Sampling proposed by Griffiths et al. [13], a specific form of Markov for obtaining samples from complex distributions. 
Gibbs Sampling simulates a high-dimensional distribution by sampling on lower-distribution. The sampling is done sequentially and proceeds until the sampled values approximate the target distribution [14]. In Gibbs Sampling method, parameters do not be estimated directly, but be apporximated using posterior estimation of z. 
Gibbs Sampler for LDA needs to compute the pobability of a topic being assigned to a word, given all other topic assignments to all other words. For an observed word w distribution for z i = k (k  X  [1,  X  ]) is given by including the current word w i . and then assign a new topic index for every word during each iteration of Gibbs Sam-its converged state and two matrices  X  and  X  are estimated from all topic assignment as follows. 
Here, we can see each term (or word) is equally important in calcuating the condi-tional posterior distribution for z i = k. However, in the skewed text classification, the weight must make many mistakes when classifying skewed documents. In order to overcome this limitation of LDA, we will propose an excellent term weighting meth-od to compute term weight, which will be used to estimate the parameters of LDA. The IDF factor of the traditional TFIDF is used to indicate the category discriminating discriminating power the term contributes to text classification. However, a term oc-curred in many documents from a category should be viewed as a strong feature, viewed as a weak feature. The term weight computed by TFIDF can only reflect the document difference, not the category difference. As a result, TFIDF must be im-proved based on the category difference and the document difference. 
Firstly, for the skewed corpus, the absolute category document frequency of term t, which is the number of documents from a category that have at least one occurrence of term t, cannot accurately measure its category discriminating power. For example, stances, and the document frequency of term t is 90 in a minor category that contains 100 instances. Thus term t is more useful to identify this minor category. Therefore, a term that occurs in a minor category should be more valuable than in a major category in case of the same document occurrence number in each category. We will use Rela-tive Category Document Frequency Differen ce (R-CDFD) to measure the difference of documents contain term t between category c  X  and its complement category c  X   X  . The corresponding formula is given by term t, D  X  here D  X   X  the category contains term t, the higher weight term t will achieve. Moreover, another important factor is the relation between term and category which can be measured by higher this conditional probability is, the higher weight term t will achieve. 
The above three factors, i.e., R-CDFD, the category distribution, the relation be-tween term and category, can characterize respectively a profile of term weight, so the three factors should be integrated to compute term weight. Hence, an integrated factor named as Relative Category Difference (RCD) is constructed, which contains the above three sub-factors, and the corresponding formula is as follows. 
Where |C| denotes the total numbers of categories in the corpus, D denotes the to-tal number of documents in the corpus, P  X  c  X  | t  X   X  D  X   X   X  X   X  of category c  X  given term t occurred, and P  X  c  X   X   X  D  X   X  c , here D C notes the total number of documents in the corpus. Then, the RCD is incorporated to replace the IDF of TFIDF . In LDA the new TF -RCD term weighting schema will be used to choose the vocabulary, and estimate parameters which replace the term frequency in Eq.3 and Eq.4 with the sum of term weight as follows. document d i , not including the current word w i . In order to verify the new parameter estimation method of LDA, we construct exper-iments focused on a comparison of TFIDF and TF-RCD in LDA. We run experiments on a subset of WebKB dataset from Ana [17], which have been pre-processed that includes tokenization and stop word removal. The experiment dataset contains 4,199 documents with four categories:  X  X roject X ,  X  X ourse X ,  X  X aculty X  and  X  X tudent X , which is 1,124 to  X  X aculty X , and 1,641 to  X  X tudent X . 84% of all distinct words are observed in gives the category distribution and term distribution of the WebKB dataset used in our experiment. 
On this skewed experiment dataset LDA model is trained. A 5000-term vocabulary of LDA is chosen by TFIDF or TF-RCD. And term weight is incorporated into Gibbs Sampling to assign a proper topic for the term. Then the documents are represented in latent topic space drawn by LDA. We build SVM (Support Vector Machine) classifier with LIBSVM development kit [18], in which linear kernel function is used. The rea-principle. macro-averaged scores are averaged values over the number of categories, and then the performance of classifier is not dominated by major categories. Let P be the precision, R be recall, and m denotes the total number of categories, then macro-averaged precision 1 is R P
Five-fold cross-validation is performed on the experiment dataset. For this purpose, the corpus is initially partitioned into five folds. In each experiment, four fold X  X  data ments results is reported in Table1. 
In Table1, TF-RCD(P) denotes using TF-RCD in both parameter estimation and vocabulary choosing of LDA, TF-RCD(V) denotes only using TF-RCD in vocabulary choosing, TF-IDF(P) denotes using TF-IDF in both parameter estimation and vocabu-lary choosing , and TF-IDF (V) denotes only using TF-IDF in vocabulary choosing. 
The macro-averaged F1 score of TF-RCD(V), compared with TF-IDF (V), is just improved about 3%, and then we can draw a conclusion that term weight only used to build vocabulary will make a little benefit for the performance of the skewed text classifier. But if term weight doesn X  X  pay enough attention to minor category, though formance of the skewed text classifier. This conclusion can be drawn from the com-parison of TF-IDF (V) and TF-IDF(P). 
From Table1 we can see that the macro-averaged F1 score of TF-RCD(P), com-pared with TF-RCD(V), TF-IDF (P) and TF-IDF (V), is the highest and the minority categories benefit most significantly. Fig.3 gets further insights about the comparison of TF-RCD(P), TF-RCD(V), TF-IDF(P), and TF-IDF (V) with chart form. As can be seen from Fig.3, the use of TF-RCD in vocabulary choosing and parameter estimation can greatly improve the whole performance of the skewed text classifier . TF-RCD is a superior term weighting method especially for skewed text categoriza-tion. The term weight computed by TF-RCD can not only reflect the document differ-ence but also the category difference, while TFIDF can only reflect the document category document frequency difference, the category distribution, the relation be-tween term and category, can devote to measure the category discriminating power of a term. 
As a result, TF-RCD can fairly choose more discriminative terms from every cat-egory to build vocabulary for LDA, and the term weights computed by TF-RCD are comparative experiments show that the supervised parameter estimation method is precision of rare category. Acknowledgements. This work was financially supported by National Natural Sci-ence Foundation of China (61272361), also supported by Key Project of National Defense Basic Research Program of China (B11201320), National HeGaoJi Key Pro-ject (2013ZX01039 -002-001-001), National High-Tech Research and Development Program of China (2012AA011002). 
