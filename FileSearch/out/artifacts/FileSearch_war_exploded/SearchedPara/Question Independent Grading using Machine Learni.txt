 Learning supervised models to grade open-ended responses is an expensive process. A model has to be trained for every prompt/question separately, which in turn requires graded samples. In automatic programming evaluation specifically, the focus of this work, this issue is amplified. The mod-els have to be trained not only for every question but also for every language the question is offered in. Moreover, the availability and time taken by experts to create a labeled set of programs for each question is a major bottleneck in scaling such a system. We address this issue by present-ing a method to grade computer programs which requires no manually assigned labeled samples for grading responses to a new, unseen question. We extend our previous work [25] wherein we introduced a grammar of features to learn question specific models. In this work, we propose a method to transform those features into a set of features that main-tain their structural relation with the labels across questions. Using these features we learn one supervised model, across questions for a given language, which can then be applied to an ungraded response to an unseen question. We show that our method rivals the performance of both, question specific models and the consensus among human experts while substantially outperforming extant ways of evaluat-ing codes. We demonstrate the system ' s value by deploying it to grade programs in a high stakes assessment. The learn-ing from this work is transferable to other grading tasks such as math question grading and also provides a new variation to the supervised learning approach.
 Recruitment; Automatic grading; MOOC; Feature engineer-ing; Supervised learning; One-class learning; Question inde-pendent learning
The automatic grading of open-ended responses has be-come the subject of extensive research [9, 27, 10]. Machine learning and other computer science techniques are used to evaluate responses in a variety of domains such as essay writ-ing [11, 24], computer programming [6, 25], spoken English [8, 22, 21] and open response math questions [16]. A number of innovations, in both academia and the industry, are fo-cusing on open-response learning and assessment systems.[4, 2, 5, 1, 3]. MOOCs have a pressing demand for a scalable method of automatic grading of open-ended responses [18] and so does the industry.

Such assessments typically contain questions or prompts regarding a topic (hereafter referred to as question ) to which test-takers must respond with written answers, for example, essay topics are prompts to test English, whereas program-ming problems are questions used to test software engineer-ing skills. For each specific question, responses from dif-ferent test-takers are used to build models that can predict the quality of new, unseen responses. For example, an es-say describing a field trip to New York might include terms such as  X  X tatue of Liberty X ,  X  X iagara X , and  X  X imes Square X  which would qualify the response to be rated highly; on the other hand, a good essay describing a visit to California might include such terms as  X  X ilicon Valley X ,  X  X apa valley X , or  X  X olden Gate Bridge X . For each question, one has to identify such discriminating features and build models using labeled responses, making them specific to the subject matter of the question. The inherent design of such an approach impedes scaling these systems to newer content. Each new question requires a significant number of expert-graded responses to train models. The availability, cost, and time required by the subject matter experts (SMEs) govern the rate at which new questions could be added to the system.

We are interested in the case of automatic programming grading, where the difficulty is two-fold: first, each question needs to have its own predictive model; second, within each question, each specific programming language needs to have separate predictive models. With a programming evaluation platform supporting 5  X  10 modern programming languages, scaling to new questions becomes very expensive. Addition-ally, there are constraints such as the investment of time and resources to train SMEs on the rubrics that they must fol-low in grading responses; the need to have at least two SMEs evaluating each response to ensure consistency in grading; and the scarcity of qualified SMEs. 1
In this work, we address the problem of scalability in au-One cannot make use of a crowdsourcing platform like Amazon Mechanical Turk to source SMEs because evalu-ating programming assignments are not human-intelligence tasks. tomatic program grading. We present a method to grade computer programs which requires no human-graded sam-ples for grading responses to a new, unseen question. For a given programming language, we transform question spe-cific features to features that maintain their relation with the grade/label across questions. We design such a trans-formation by exploiting properties specific to the domain of computer programming. We call these transformed features as expressive structurally invariant features -their structural relation with the label remains invariant across questions. Using the structurally invariant features we learn a question independent supervised model, which can then be applied to (the automatically transformed features of) an ungraded response to an unseen question. In doing so, we extend our previous work where an expressive grammar of features was proposed and used to train question-specific supervised models. We also suggest how the learning from this work can be transferred to other problems of automatic grading. Specifically, this paper makes the following contributions:
The paper is organized as follows - X  2 describes the ques-tion independent learning approach and related work. In  X  3, we present the details of our experiments and results.  X  4 discusses the deployment of our system in a high stakes as-sessment.  X  5 concludes the paper and discusses future work.
We describe in this section how question-independent fea-tures are engineered and used in a machine learning frame-work. We begin by laying out the notation, followed by a summary of the grammar of features proposed in [25]. This provides a context to the discussion in the subsequent sec-tions.
For a given programming language, each programming question is referred to as a question and a code which at-tempts this question, as a response . The set of questions used in this study is denoted by Q all . Each question q  X  Q has a set of responses R ( q )  X  R collected on it, where R is the set of responses across all questions. For any given re-sponse x , its feature vector is referred to as  X  f x . 2 in feature vector  X  f x belongs to a category in set C = { B, All vectors represented by a cap (  X  f x ) are row vectors BC, E, EC, ED, EDC } (see  X  2.2 for details). For a ques-tion q  X  Q all having n responses and m unique features, we define the feature matrix F ( q ) n  X  m as where  X  f x is the feature vector of a particular response x to question q .

We define a good set for each question (see  X  2.3 for details) q  X  Q all containing k good responses as R ( q ) g and the feature matrix of the k good responses G ( q ) k  X  m , as where  X  g y is the feature vector of a particular good response y to question q .
The grammar proposed in [25] generates features that cap-ture the semantic relationships present in a program. The counts and relations of three important properties of a pro-gram are extracted -the tokens used in the program, the expressions and the data dependencies. a) Basic (B) : Examples include counts of all keywords, tokens, operators etc. such as the number of times a  X + X  operator appears or loop tokens such as  X  for  X  and  X  while  X  ap-pear. b) Expressions (E) : Expressions such as y = x %2 are ab-stracted to a notation such as v:2::op:%::c: X 2 X , which denotes an expression having two variables, one modulus operator and the constant  X 2 X . The number of occurrences of such abstract expressions are counted. c) Expression Dependency (ED) : A data dependency is captured when the variable in one expression is used in another expression. For example, the expression x &lt; y , which contains a relational operator ( &lt; ) and two variables, is dependent on the expression y + +, which contains a post-increment operator (++) and one variable. This is denoted using the notation v:1::op:++  X  v:2::op:relation. The oc-currences of each dependency matching such abstractions is counted. This is repeated for each unique pair of dependen-cies that appears in a response. d) Control Context (BC, EC, EDC) : Separate counts are maintained for each of the three properties described above according to the control-context (loops and condi-tional statements) in which they appear. For example, an expression whose abstract notation matches v:2::op:%::c: X 2 X  is counted separately if it appears within an if statement as against in a loop like a for or a while as against in an if statement within a for .

The grammar of features described above is combinatorial in nature; a 10  X  15 line program on an average generates 2000 features. A detailed discussion of the features is avail-able in  X  2.3 of [25].
The features introduced in [25] predict responses ' grades accurately. However, the system requires to build a model for each question separately using labeled responses (see Fig 1a). Since getting responses labeled is a cost intensive pro-cess, we explore the design of a single model that predicts a response ' s label irrespective of the question it solves. De-signing such a model does not fit seamlessly into traditional machine learning approaches since the features which dis-criminate  X  X ood X  responses from  X  X ad X  ones change drasti-cally across questions. For instance, we expect a nested loop in a  X  X ood X  response for bubble sort while one loop and other discriminating expressions is expected in a  X  X ood X  re-sponse to reverse a string. We wish to design features which structurally maintain the same relation with the grades irre-spective of the question. One such relationship is an increase in the value of a feature, irrespective of the question, signal-ing a better grade. 3 We could then learn one model across responses from multiple questions in this invariant feature space and use it to predict grades of responses to unseen questions.
A monotonic relation between a feature and a label is used for a simple illustration; other relations, like a quadratic form, are also acceptable in principle as long as they main-tain this form across questions.
Mathematically expressed, we would want to design a transformation  X  which would transform a question specific feature matrix F ( q ) n  X  m , for a given question q , into a structure invariant feature matrix D ( q ) . Although the transformation  X  takes in question specific parameters, it must be automatic and not require any manually assigned labeled data for a re-sponse to an unseen question. The new invariant feature matrix D ( q ) can then be augmented across questions and be used as one consolidated input to learn a question inde-pendent model QuesInd (see Fig 1b). The new question independent model can then be used to predict the grades of a new question without any human-graded samples.
We describe how we design the transformation  X  which transforms the question specific features into a set of suffi-ciently expressive structurally invariant features. a) Automatic identification of a good set : The good set , which we define as a subset of the responses that solve a question correctly, is vital in transforming question specific features into structurally invariant features (input to  X  in Fig 1b). Although constructed for every question separately, creating this set does not require manually assigned labeled responses for a question. We posit that a carefully designed test-suite for a question automatically identifies function-ally correct responses, which in turn can comprise the good set . This is not to say that responses failing a test suite cannot be good -in our approach, it is sufficient to have a subset of such good responses (corresponding to labels 4 or 5 of the rubric [25]). 4 We require a sufficient number of such responses so that they capture the variation in possible correct approaches to a question. We exploit the ability to automatically identify such a good set for a question to de-velop structurally invariant features. We now describe how we develop structurally invariant features from such an au-tomatically identified good set . b) Distance from a good response : For the sake of simplicity, let us assume the existence of only one possible good response to a question. In such a case, an L tance (in the space of m features) between a given response and the good response will be a structurally invariant fea-ture. A response at a larger distance from the good response will have a larger proportion of different  X  X eywords X ,  X  X on-trol structures X ,  X  X ata dependencies X  etc. as compared to a response with a lower distance. Thus, responses with higher distances should probably have lower grades as compared to ones with lower distances, irrespective of the question the response and the good set belongs to. Such an L 1 distance from an automatically identified good response is hypothe-sized to relate to the grade in a structurally invariant way across questions.

We modify the L 1 distance to a one-sided distance  X  , de-fined as
There could be cases where a response attempt to trick the system by hard-coding the exact test cases being evaluated. In both, our system and the system proposed in [25], this was handled by keeping a good number of test cases in the test-suite hidden from the respondent, making it hard to guess which specific cases a response would be evaluated on. where  X  f x is the feature vector of the response x and  X  g feature vector of a good response y .

We do this guided by the intuition that programming con-structs like keywords, control structures etc. not present in the feature vector (  X  f x ) of response x but present in the fea-ture vector ( X  g y ) of good response y signals a deviation from how response x ought to be written. On the other hand, any excess of such features present in response x might still signal the approach used in good response y 5 .

This still leaves us with two problems. First, there are usually not one but multiple ways to correctly implement a question. The distance  X  defined above can signal near-ness to a specific good response . We would want a metric which could signal nearness to one among many such good responses . Second, the process of calculating  X  transforms the m dimensional feature space, consisting many interest-ing and useful features, to just one monolithic distance. We would want to continue having expressive features to learn models which generalize well. We address these problems below.
 Algorithm 1 Distance from a good set INPUT:  X  f : Feature vector of a response x G ( q ) : Good set feature matrix of question q N : N=1 to normalize distances OUTPUT: Distance of a response x from the good set of a question q 1: Initialize: 2: function goodSetDist (  X  f x , G ( q ) , N ) 3: for y  X  1 to k do . Loop over all good responses 4: d xy  X   X  (  X  f x ,  X  g y ) . See Eq (3) 5: if N==1 then 6: ( d xy )  X   X ( d xy ,  X  g y ) . See Eq (4) 8: d x = meanMin25 ( x all ) . Mean of min 25 9: return d x . Distance from good set c) Distance from a good set : A question typically con-tains multiple good responses in its good set . A given re-sponse has a distance  X  from each of these good responses , which we aim to aggregate meaningfully. Such an aggregate could be calculated in many ways, the mean and the min-imum of the distances from the good set ' s responses being two examples. It was shown in [25] that the mean of the 25% minimum distances correlated moderately with the output and outperformed the mean and the minimum. It discussed
We also conducted experiments, not reported for brevity, whose results validate the intuition empirically. that the mean would dilute the distance metric from match-ing the closest correct response and the minimum would be too sensitive to outlier responses in the good set . We use this mean of the 25% minimum distances across good responses as the aggregate measure. Algorithm 1 describes the dis-tance of a response x belonging to a question q from its good d) Expressive distance scores: In our discussions thus far, we obtain a single monolithic distance from  X  as a fea-ture, for a response, to signal  X  X oodness X . This distance is aggregated across all features ( F ( q ) ) and across the differ-ent responses in the good set . This is in contrast to the question specific approach, wherein we had the advantage of having several varied features to learn a model. We ad-dress this by aggregating the distance across each of the six feature categories one at a time (see  X  2.2 for a description of the categories). We rewrite the features matrices for the responses and the good set as -We then calculate  X  with each of the six input pairs ( F ( q ) G
B ), ( F scores for a response. This results in one distance feature corresponding to only Basic features like counts and tokens, one corresponding to Expression features etc. We hypoth-esize these categories to have different relations (weights) with the grade despite each of them being structurally in-variant across questions. For instance, the Basic feature may relate more to gross differences in the responses and thus help signaling large jumps across rubric levels while the distance in the Expression Dependency may help quan-tify fine differences between responses, differentiating at the upper end of the rubric. e) Transformation,  X  : The distance score calculated for any response x of question q from the good set of q is the transformed feature value which is used to learn question independent models. For a question with n responses, the transformation  X  converts the feature matrix F ( q ) n  X  m vector of distances D ( q ) n  X  1 = d 1 ...d n T where each d distance of response x from the good set G ( q ) . f ) Normalization,  X  : We describe how a distance score is calculated for a response by aggregating its distance from all responses in a good set of a question. Although the aggre-gation ensures that the general relationship between the dis-tances and grades is maintained across questions, the scale of the distances could vary with questions. Specifically, Equa-tion (3) scales up by the number of features which varies across questions. For instance, the set of unique features in responses implementing bubble sort (  X  11 lines of code, O ( N 2 )) would be much more than to swap two numbers (  X  4 lines of code, O (1)). As a consequence, the magnitude of distance for a  X  X earer X  response may turn out to be much larger for bubble sort when compared to swapping two num-bers. This creates an issue when we augment these differ-ently scaled distances for a joint learning task. A simple way to circumvent this is to normalize the aggregate distance by the total number of features in the good response y that it is compared to. We such define a normalization  X  as where d xy is the distance of response x from good response y and  X  y = P m i =1 g yi is the sum of features in the good response y.

Another normalization strategy could be to normalize by the average distance among responses in the good set . One may see this as using distributional properties of an unla-beled sample. We see in the results section  X  3.3 that some of these normalizations are useful. In summary, by automat-ically identifying good responses , we are able to create six structurally invariant features, with a hypothesis that they relate differently and could add incremental value over each other to predict the grade. 6
Recent work in automated programming grading has fo-cused more on providing feedback than a grade. [23] and [20] propose a program synthesis approach and a static analysis approach respectively to provide feedback for erring pro-grams. These approaches find if small modifications could be made to the program to enable it to pass their respec-tive test cases. They provide a list of such modifications as feedback to students. The systems seemingly perform as expected only on very small and non-complex pieces of code (less than 10  X  15 lines implementing simple algorithms) and work only for responses with small deviations from a spec-ified good response. Moreover, they do not focus on evalu-ation against multiple correct responses. [14] and [15] each propose a system which uses unsupervised learning to clus-ter together semantically similar codes. An SME provides a grade to any one representative code from each cluster. This grade is then propagated to the rest of the responses in that cluster. These systems have not been designed to automati-cally grade responses at scale in real time and instead focus on reducing the workload of an SME who is involved in the feedback generation process. This is very different from the business requirements of high-stakes automatic grading of computer programs. In comparison to these systems, ours is the first approach to automatically generate a grade for a program written for a given question without having any manually assigned labeled samples for it. Also, we use su-pervised learning techniques and derive features from a more expressive feature grammar.

Among machine learning techniques, the literature of do-main adaptation [13], which constitutes a sub problem in the field of transfer learning [7, 26], addresses a similar problem as ours. In domain adaptation, a well established model built on data drawn from a source distribution is used to learn a model on data drawn from a target distribution. The task to be accomplished (e.g. tagging natural language) re-mains the same across the domains while some distributional properties of the features change. In our problem, unlike in domain adaptation, the learning task changes significantly
In principle, we could treat every original feature sepa-rately, convert them into a distance, align them across ques-tions and use them to learn a question independent model. This has issues of high sparsity and learning complications because many features are unique to a question.
Even though the task sounds the same, i.e. to grade pro-from one question to another. The feature space and the discriminating features drastically change across questions. From the standpoint of domain adaptation, it is hard to say what is the commonality between the source and the target models for different programming questions -there doesn ' seem to be any connection between the features or the struc-ture of the models for, say, a bubble sort and a tree traversal. However, if one is able to define structurally invariant fea-tures as we do, some learnings from the domain adaptation literature may be useful.
We designed our experiments to address the following questions
To answer these, we analyzed 19 programming questions 8 graded by experts trained on our rubric. The responses to these questions were, on an average, 30  X  35 lines long and required a variety of data-flow dependencies and con-trol structures to be implemented. We experimented with both, linear and non-linear machine learning techniques. For each question, we learned a specific model and compared its performance on a question-independent model learned on a subset of the questions in the data set. We analyze the per-formance of the models aggregated over all the questions as well as separately for each question. We now discuss the details of the experiments.
The experiments were run on a set of programming ques-tions hosted on Automata , our automated programming eval-uation platform [25]. Respondents, who were college seniors majoring in computer science, took a 90 minute assessment in a proctored environment wherein they attempted two pro-gramming questions in a language of their choice. For these questions, the respondents had a choice of C, C++ and Java. A total of 19 questions were used in the study (see Table 1), on which we had an average of 285 responses per ques-tion and 5434 responses in all. The topics covered by these questions spanned iterative/ recursive algorithms, trees and graphs and other algorithms like the shortest job first etc. grams, it actually is different. For instance, the task is to determine whether a response implements bubble sort as against finding whether a number is prime.
Models were built separately on data collected in three lan-guages -C, C++ and Java. Wherever question independent models are mentioned, we refer to the average performance of the models across the three languages.
Due to a paucity of space, we do not list here the details of the question statements.
 Question Name #Codes #Good Set #Features
Qused tr isSameReflection 126 69 2874 waitingTimeSJF 307 31 4513 countCacheMiss 455 93 3682 isTree 420 71 2741 patternPrint 603 211 2945 grayCheck 610 134 1793 transposeMultMatrix 507 183 2728 eliminateVowelString 515 156 2424 Total 3543 948
Qunseen tr cellCompete 215 70 2801 insertSortedList 101 22 1154 isSubTree 104 39 1315 minTreePath 189 76 1255 isPath 71 27 4766 lruCountMiss 244 66 4416 generalizedGCD 248 72 3163 distinctElementCount 256 91 1656 rotatePictureMethod 209 83 2085 reverseLinkedList 41 19 1162 balancedParentheses 213 96 1649 Total 1891 661 Three professional software engineers with 3-5 years ' expe-rience each, who were also seasoned competitive program-mers, shared the task of grading the responses. Each re-sponse was graded by two experts. The experts followed the rubric defined in [25] to grade codes on a scale of 1-5. Before beginning the grading exercise, they underwent a one-week workshop wherein they learned how to interpret the rubric and participated in mock grading exercises. The correlation between the grades of any two experts on an average was 0 . 81 across the questions in the data set.

We note here that grades are required only once for re-sponses to only a subset of these questions. A model built on such responses then predicts grades of responses to any number of unseen questions without manually assigned la-beled responses.
We developed machine learning models on the responses collected on the questions listed in Table 1. We used linear regression, linear regression with L 1 regularization (LASSO), linear regression with L 2 regularization (Ridge regression), decision trees, random forests and SVMs. For LASSO[17] (  X  = 1) we varied  X  from 0 to 4. For ridge regression, we varied  X  from 0 to 100. For random forests[19], we varied the number of estimators from 15 to 100. For SVMs[19, 12], we tested three kernels -linear, polynomial and RBF. We varied the penalty factor C from 0 . 125 to 128, and parame-ters  X  from 0 . 125 to 128 and from 0 . 5 to 16. In all these techniques, the model which gave the best cross-validation correlation was selected. For the question specific models, a feature selection step was followed by a 3-fold cross val-idation. The best cross-validation correlation was used for selecting the final model. Similar to what was seen in [25], linear models worked the best among all these techniques, indicating linearity in the inherent structure of this problem space. We report results only for LASSO in this work which outperformed all other techniques. We trained the following models to answer the questions enumerated at the beginning of the section:
In all the above models, the percentage of test-cases passed was added as a feature.
To learn the QuesSpecAll model, we split the responses to each question into a 67  X  33 train-test set. The selection of the train-test set was nuanced for the QuestIndep models. In addition to testing the model on responses belonging to questions already present in the train set, we ensured that there were responses in the test set from unseen questions -those whose responses were not used to train the models at all. The performance on such an unseen set would then demonstrate how well the QuesIndep models generalized to questions where we did not have any human-graded sam-ples, the use case expected in a real-world scenario. We also ensured that the responses in the good set for each ques-tion were not a part of their respective test-sets. We de-note those questions whose responses are used to train the question independent models as Q used tr . Questions whose responses are not used in training the models are denoted as Q unseen tr . Q all denotes the set of all the questions. In our data-set, we randomly selected 8 questions as part of Q used tr and the remaining 11 as Q unseen tr . We trained the QuesIndep models on 67% of the responses belonging to Q used tr (2384 in total) and tested them on the remaining 33% responses of Q used tr and additionally on all responses of the Q unseen tr questions (3050 in total).
We use the Pearson correlation coefficient ( r ) to quan-tify and measure the similarity between the predicted grades and the experts ' grades. While describing the QuesIndep model building process (see  X  2.3), we suggested how the dis-tances calculated from the good set could have been on dif-ferent scales for different questions. For some questions, this could cause the question independent model to scale and/or displace the distribution of grades linearly as compared to their true distribution. As a consequence, the model may still correlate highly with the grade. We hence consider tion metrics in addition to the correlation coefficient.
We contrast the performance of the QuesIndep and QuesSpec models on the test set (Table 2). To maintain brevity, we tabulate the mean of the models ' performance on each ques-tion. We compare the performance of the models on two sets of questions -Q all and Q unseen tr (see  X  3.2.1). The baseline results for the Q unseen tr are calculated by learning a model on the test case scores of only the Q used tr set and testing on the Q unseen tr set. The QuesSpec results on the Q unseen tr set correspond to the models learned on each of
We first discuss the aggregate results across questions. We find that the question independent model with normaliza-tion ( QuesIndep-N1 ) has an r of 0 . 80 with expert grades on both sets of questions -Q all and Q unseen tr with a bias of 0 . 24 and 0 . 27 on the two sets respectively. This rivals the
The bias in this work does not refer to the bias of learning models. It refers to the observational error. agreement of two human experts ( r = 0 . 81, bias = 0 . 14) and shows that the model may be used in a real world setting. The Q all and Q unseen tr sets follow the same trends and we discuss them together. In comparison to the testcase base-line, the question independent model does much better on each of the metrics -r , bias and MAE. When compared to the question specific models ( QuesSpec-All , QuesSpec ), the question independent model provides comparable per-formance in their r . However, it shows a higher bias and MAE. This shows that some scaling issues do creep in while building question independent models and this aspect may further be improved. However, the difference in MAE for question independent and question specific models is not very large and varies between 0 . 12 and 0 . 25 points. We find that normalization helps both in terms of r (0 . 04) and bias (0 . 04  X  0 . 07). These observations are as expected.
In summary, we find that the question independent mod-els rival expert grades, largely outperform test case mea-sures, and show higher bias as compared to question specific models. Furthermore, normalizing the structurally invariant features does help build better models.

We now discuss the errors on individual questions. We compare the three error metrics on our best question inde-pendent model QuesIndep-N1 with those on the baseline (Table 3). We find that the r and MAE of the question independent model outperform that of the TC baseline by 0 . 15 points on an average. This is also distinctly visible in Fig 2a and Fig 2c (X-axis represents the error metric of Baseline-TC and the Y-axis represents the error metric of the models, QuesIndep-N0 and QuesIndep-N1 ). We see in Fig 2c that the performance on MAE by QuesIndep-N0 on 3 questions does worse than the baseline. However, the performance on these 3 questions improves on using the model with normalization, reinforcing the utility of normal-ization in the learning process.

When we compare biases, we see that the question inde-pendent model performs better than the test case baseline on 13 of the 19 questions (Fig 2b and Table 3). Out of the 6 questions that underperform: 2 are from the Q seen tr set and 4 from the Q unseen tr set (biases marked in bold-face). A reason for this under performance could be the normalization technique not being able to get the distances for these questions on the same scale perfectly well. The simple linear scaling demonstrated through our techniques may be replaced by more sophisticated non-linear scaling, warranting a more detailed study.

To conclude, the question independent model consistently outperforms the baseline on correlation, which is expected. As a result of possible normalization issues, the model un-derperforms in bias in a few cases. However, in no case is the MAE affected and we see it consistently outperform the baseline, demonstrating the utility of the learning technique we introduce in this work.
We deployed the QuesIndep-N1 model on Automata , our online programming evaluation platform. A leading soft-ware product company having more than 10 , 000 employees wanted to use this product for their 2015-2016 hiring cy-cle of software engineers with 0-2 years ' experience. The company wanted to deliver 13 questions developed by them on this platform. Having released the 13 questions in June 2015, we had 68 responses on an average in the good set across the questions within 3 weeks. By the first week of July, we had the question independent models live and eval-uating responses to the questions. This is in stark contrast to the 4  X  5 month process it would typically take to de-velop question specific models per question. Till Decem-ber 2015, a total of 3176 candidates had been evaluated on this platform. The company ' s default shortlisting criterion was to consider candidates passing at least 75% of their test cases, which was followed by an interview where the final hiring decision was made. In this hiring cycle, they short-listed 468 candidates who had scored 4 or 5 as predicted by the QuesIndep-N1 model. Had the hiring been done only based on the test-case cut-off, 391 candidates would have made it through. 384 candidates were common to these two sets. Among the 84 candidates (468  X  384) interviewed by the virtue of QuesIndep-N1 , the interviewers found the grades of 76 respondents match their evaluation. Hence, 16 . 5% candidates ( 76 76+384 ), a sizeable proportion, would not have made it through to the interviews had the criterion been only the test case score. On the other hand, of the 7 candidates who were graded low but had a high testcase score, all were graded 3 by the model, had an average test-case score of 76 . 3% and seemed to be at the cusp of being graded 3 or 4 on visual inspection by the interviewers. At the end of the process, 136 of the shortlisted 468 candidates were offered a job. 46 of these 136 (33%) were signaled by QuesIndep-N1 . This case study demonstrates the success-ful deployment of QuesIndep-N1 models in a high-stakes assessment, helping save man-hours which would have other-wise gone into designing question specific models. In a sepa-rate study done on candidates who have completed 6 months in job, we also found the predictions of QuesIndep-N1 to correlate significantly ( r = 0 . 8) with the manager ratings on job performance.
We present in this work a system to grade computer pro-grams using question-independent supervised machine learn-ing models. Building on the grammar of features we intro-duce in our previous work [25], we engineer semantically invariant features which maintain their structural relation with the labels across questions. This is possible because we identify a set of  X  X ood X  responses to a question automati-cally. We show that a single model learned on such features, derived from the responses to a few questions, is able to predict the grades of responses to questions not seen in the model X  X  training. Further, we show that the performance of such a model clearly outperforms extant evaluation tech-niques, rivals the performance of question-specific models and the consensus of human experts. In doing so, we suc-cessfully demonstrate how to grade responses to questions without any manually assigned labeled samples. Through a case study, we show how practically efficient the system is when deployed to recruit software engineers, saving signifi-cant time and resources in the process.
 We see a window for further research in the normalization techniques we lay out and the transformations to create a larger set of structurally invariant features. The relation-ship of accuracy with the number of train problems, size of data set and the number of good codes for new problems can be studied further. Other learning techniques which model rankings better, like ordinal regression, can also be explored. The automatic identification of functionally cor-rect responses, which we demonstrate in this work, can be extended to other areas as well like open response math-ematical questions and electronic circuit solving, where a numeric comparator judges the final answer of a series of equations. It also applies to evaluations whose outcomes are human intelligence tasks (such as evaluating whether a web-page is aesthetically designed); the identification of correct responses then can be crowd-sourced in real time to produce a high quality good set . These observations promise inter-esting opportunities for research in such areas, all of which will help the holy grail of designing fully automated teaching assistants. We look forward to the learning from this work resulting in similar accurate, scalable grading systems being designed in other domains. [1] Automata. Aspiring Minds [2] E-rater. ETS [3] Intelli metric. Vantage Learning [4] Speechrater. ETS [5] Svar. Aspiring Minds [6] V. Aggarwal, S. Srikant, and V. Shashidhar. Principles [7] J. Baxter. A bayesian/information theoretic model of [8] J. Bernstein, A. Van Moere, and J. Cheng. Validating [9] M. Birenbaum and K. K. Tatsuoka. Open-ended [10] H. M. Breland. The direct assessment of writing skill: [11] J. Burstein, L. Braden-Harder, M. Chodorow, S. Hua, [12] C.-C. Chang and C.-J. Lin. Libsvm: a library for [13] H. Daume III and D. Marcu. Domain adaptation for [14] E. L. Glassman, J. Scott, R. Singh, P. J. Guo, and [15] J. Huang, C. Piech, A. Nguyen, and L. Guibas. [16] A. S. Lan, D. Vats, A. E. Waters, and R. G. Baraniuk. [17] N. Meinshausen and P. B  X  uhlmann. Stability selection. [18] L. Pappano. The year of the mooc. The New York [19] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, [20] K. Rivers and K. R. Koedinger. Automatic generation [21] V. Shashidhar, N. Pandey, and V. Aggarwal.
 [22] V. Shashidhar, N. Pandey, and V. Aggarwal. Spoken [23] R. Singh, S. Gulwani, and A. Solar-Lezama.
 [24] V. Southavilay, K. Yacef, P. Reimann, and R. A. [25] S. Srikant and V. Aggarwal. A system to grade [26] S. Thrun. Is learning the n-th thing any easier than [27] C. Vleuten, G. Norman, and E. Graaff. Pitfalls in the education , 25(2):110 X 118, 1991.
