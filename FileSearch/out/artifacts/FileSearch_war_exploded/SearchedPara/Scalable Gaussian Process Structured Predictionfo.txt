 S  X  ebastien Brati ` eres 2 SEBASTIEN @ CANTAB . NET Zoubin Ghahramani 2 ZOUBIN @ ENG . CAM . AC . UK
SMiLe CLiNiC, Department of Informatics, University of Sussex, UK Microsoft Research, Cambridge, UK Conditional random fields (CRF) (Lafferty et al., 2001) and Markov random fields (MRF) (Blake et al., 2011) are pop-ular models in computer vision, language modelling, and other applications of machine learning, because they al-low specifying rich interactions between multiple random variables using an undirected graph. Despite their success these models remain challenging to work with: both infer-ence and parameter estimation for general graph structures are intractable and require approximations. Furthermore, in order to achieve good predictive performance one needs to specify meaningful features and has to trade-off the result-ing model capacity with the amount of available training data to avoid overfitting.
 This work addresses the specification and estimation prob-lems in a Bayesian framework building on a recent non-parametric model X  X he GPstruct model (Brati ` eres et al., 2013). Our contribution is to present an efficient approx-imate Bayesian learning approach that reliably prevents overfitting yet remains scalable for large general (non-tree structured) graphs. An important limitation preventing the application of GPstruct to larger data sets is the dimension of the kernel matrix, determined by the number of GPstruct latent variables, which is number of training points  X  output dimensionality  X  label space cardinality . While this num-ber may remain reasonable for linear chain factor graphs (Brati ` eres et al., 2013), the output dimensionality in vision problems is vastly higher than in typical language process-ing applications. As an example, this paper tackles an 8 -class segmentation task, training over 572 training images of size 50  X  150 . In the standard GPstruct approach, this yields a kernel matrix of size 4 million by 4 million (  X  10 elements, assuming a block diagonal unary kernel matrix as in (Brati ` eres et al., 2013)), which cannot be inverted us-ing Gaussian process (GP) sparsification techniques such as (Snelson &amp; Ghahramani, 2005) alone. In practice, we find the standard GPstruct model to be applicable to only about one image at a time, which is consistent with the scal-ability results in (Hensman et al., 2013). On the other hand, for these computer vision applications the GPstruct model proposed in this paper provides reliable uncertainty esti-mates that are well calibrated. Because in many real world domains, computer vision is part of a larger autonomous system, we believe that providing reliably quantified un-certainty is an important feature of our method.
 Our paper introduces a scalable approximate Bayesian learning method. The latent variables are divided into sub-sets, each of which is assigned to a GPstruct predictor. The predictors are trained on bootstrap data using the pseudo-likelihood approximation. The above problem is reduced to subsets of 40000 latent variables, distributed over a com-puting cluster, with slave nodes carrying out partial pre-diction and the master node aggregating predictions. Our approach makes it possible to apply the model to prediction on grid factor graphs and potentially other structures. For self-consistency of the paper, we review the Gaus-sian process structured prediction model (Brati ` eres et al., 2013). Assume that we are given a set of N input-output data points or examples D = { ( x 1 , y 1 ) ,..., ( x N , y X  X Y . We use X and Y to denote the input and output space, respectively. We consider structured output: the out-put domain is the product of individual variable domains Y so that Y =  X  of output variables associated with an input. Note impor-tantly that as in other typical structured prediction problems for discrete Y i , the size of Y grows exponentially with I . We describe the relationship between an input variable x and its output variable y by means of an energy function E . The energy function defines a conditional probability distribution Pr ( y | x , E ) as where the log partition function Z ( x , E ) =
P our model in (1) is in the general form of a conditional random field (CRF) (Lafferty et al., 2001; Sutton &amp; McCallum, 2012).
 We will consider the case in which the energy function E decomposes into a sum of energy functions E t F over fac-tors F , where F defines a subset of variables. As an illus-tration, when | F | = 2 , the energy function is: In the above, we use x F to denote the collection ( x i ) and likewise we use y F to denote the parts of y that are in F . While there are many different subsets in F , we as-sume that there are only few distinct types and we use t F to denote the type of the factor F . The function E the same for all factors of that type, but it acts on different variables. This notion of type specifies both repeated struc-ture and parameter tying in the model. As another exam-ple, we can further decompose the energy function in (2). This results in two different types of factors: a) unary 1 tors connecting components of input and output variables ( x i ,y i ) , and b) pairwise factors connecting neighbouring output variables ( y i ,y j ) . We then have The traditional CRF parameterisation (e.g., Sutton &amp; Mc-Callum (2012)) consists of expressing the energy function E ( x , y ) as a weighted sum of features w &gt;  X  ( x , y ) where w  X  R D is the weight vector and  X  ( x , y ) is a feature function. Learning consists of estimating the weight vec-tor under a given prior distribution. This approach is called parametric , since the parameterisation (e.g. the size of the weight vector) is fixed independently of the data. The GP-struct model employs a non-parametric approach by con-sidering the energy terms E a ( x i ,y i ) and E b ( y i ,y tent functions, and imposing a Gaussian process prior (Ras-mussen &amp; Williams, 2006) upon them. In the usual GP terminology, Pr( y | x , E ) defined in equation 1 is the likeli-hood function.
 This requires defining a covariance function over any two such energy functions. Following Brati ` eres et al. (2013), which elaborates on this aspect, we define Cov( E a (  X  ) ,E b (  X  )) to be zero, and further nel function (Sch  X  olkopf &amp; Smola, 2001). By defining that E a ( x i ,y i ) depends on the location i while E b ( y i ,y not, the number of unary latent variables grows with the data, while the number of pairwise latent variables does not. The non-parametric property, which allows model complexity to be determined as part of analysing the data, although appealing, introduces a serious scalability issue in the linear chain factor graph (Brati ` eres et al., 2013). Un-fortunately, this scalability issue is magnified in the vision applications in which we are interested, and which involve grid factor graphs: the number of unary latent variables grows with I , the number of pixels in an image. The same scalability issue affects kernel CRFs (Lafferty et al., 2004), which may be one of the reasons why these models have not become popular.
 Next, we will describe the GPstruct model for grid factor graphs before addressing our approach to scaling up the model. Assume that we are given an observed image x  X  X  where X denotes the set of possible images. We note the pixels x i = 1 ,...,I , so that I is the number of pixels of an image. Our goal is to infer a discrete labelling y  X  Y where the labelling is per-pixel, that is y = ( y i ) i =1 ,...,I , y in practice, we will mostly have a single set Y i := L of labels.
 We assume that the energy functions decompose into sums of energy functions over unary and pairwise factors as in (3). To be specific, consider a toy image of size 3 by 3 . Fig-ure 1 displays the corresponding factor graph model, with a distinct unary factor for each pixel, and shared pairwise factors over inter-pixel edges. All unary factors depend on image data x , and all pairwise factors do not depend on x . Learning Given training data D consisting of N images and their labels, the goal of learning is to infer a posterior distribution over energy functions, that is Pr ( E |D ) . As-suming the images in the dataset are independent and iden-tically distributed given the model, the training data like-lihood can be written as Pr ( D| E ) = Q N n =1 Pr ( y n | where Pr ( y n | x n , E ) is given in (1).
 As for most interesting Bayesian models, analytic or exact computations are generally not possible, and adequate al-gorithms are needed in several places. Generating posterior samples E |D from the GP, given the non-Gaussian likeli-hood, cannot be done analytically and is dealt with by ellip-tical slice sampling (ESS) (Murray et al., 2010), an efficient Markov Chain Monte Carlo method for tightly coupled la-tent variables with a Gaussian prior, as used in Brati ` eres et al. (2013).
 The likelihood computation is intractable due to the nor-malising constant Z ( x , E ) , which involves a summation over the label set Y , whose size is exponential in the num-ber of pixels in an image. Take as a running illustra-tion, a simple foreground-background segmentation task ( L = { 1 , 2 } ) and an image of size 50  X  100 . For this problem, the exact likelihood would be computed using the junction tree algorithm, whose complexity is exponen-tial in the treewidth of the grid graph, i.e. 50 in our ex-ample. To address the intractability of the normaliser, we use a surrogate likelihood, the pseudo-likelihood (Besag, 1975) (PL), as a drop-in for the true likelihood in the ESS procedure. The PL is derived from the per variable con-denote the set of neighbours of variable i according to the underlying factor graph. If a grid factor graph with pair-wise factors is used, |N ( i ) | = 4 except for variable i asso-ciated with a pixel at the boundary. The PL is then de-fined as Pr PL ( D| E ) = Q N n =1 Q i  X  X  Pr ( y n i | y The maximum PL estimator is known to be a consistent estimator (Besag, 1975), but here we use it to approxi-mate the intractable likelihood function itself. The use of PL in MCMC schemes for Bayesian parameter learning in Markov random fields dates back to Wang et al. (2000). A detailed study of MCMC for Bayesian learning in non-trivial undirected graphical models is given in Murray &amp; Ghahramani (2004), which conjectures that for general undirected models, there are no tractable MCMC meth-ods that give the correct equilibrium distribution over pa-rameters. Their pragmatic solution is to explore a variety of approximations for the normalising constant Z ( x , E ) . The method of Murray et al. (2006) is not exactly suited to our setting, since it requires pure Metropolis-Hastings as the sampling algorithm (and works on the acceptance rate), while we use elliptical slice sampling to avoid high sample correlation, in the presence of tightly coupled latent variables. However, earlier, Parise &amp; Welling (2005) have concluded that for fully observed MRFs (as in our case), PL is recommended over perfect sampling due to the com-putational burden of the latter, which is not balanced by a corresponding performance gain. In this paper, we show empirically that PL works well when used as a likelihood approximation in the GPstruct model.
 For GPstruct, the PL computation will still involve 10 , 000 latent variables ( 50  X  100 pixels times 2 classes) in our il-lustrative binary segmentation task. In addition, this high number of latent variables requires large storage for the GP kernel matrix. These two issues are addressed with one strategy, namely reducing the set of latent variables as-signed to a weak learner (WL) by an ensemble method, an approach detailed in section 4.
 To implement this method, as in Nowozin et al. (2011), PL is computed on subsets of pixels, V 0  X  V , where V is the set of pixels of an image (and |V| = I ), while retaining the 4 -connected factor for all i  X  V 0 . Referring to the plate notation in figure 1 (right), the variable y i inside the plate is now repeated |V 0 | times. This gives a subset PL func-produce diversified weak learners, a necessity for ensem-ble methods, we train them on disjoint subsets of pixels. This approach to subset-based ensemble learning is related to the Bayesian committee machine (Tresp, 2000).
 Prediction For a previously unseen test image x  X   X  X , the predictive distribution over the latent structured output y  X   X  X  can be computed as follows: Pr ( y  X  | x  X  , D ) = where Pr ( y  X  | x  X  , E  X  ) is a predictive likelihood, Pr ( E is a conditional multivariate Gaussian (MVG) distribution (due to the GP marginalisation property (Rasmussen &amp; Williams, 2006)), and Pr ( E |D ) is the posterior distribu-tion. In equation (4b) Pr ( E |D ) is obtained from the learn-ing stage, and involves the pixel subset PL.
 We use a Monte Carlo estimate to approximate the above predictive distribution, where posterior samples of E |D are produced by the learning (ESS) part of the algo-rithm discussed above. Since the predictive likelihood Pr ( y  X  | x  X  , E  X  ) is again intractable, we use tree-reweighted (TRW) belief propagation (Wainwright &amp; Jordan, 2008) as an approximation. TRW yields a tractable upper bound on the log partition function, which might give an inconsistent marginal predictive likelihood Pr ( y  X  i | x  X  , E  X  ) in the sense that no joint distribution yields those marginals. Despite this inconsistency, practically, TRW-based inference deliv-ers state-of-the-art predictive performance (see for example Domke (2013)).
 To obtain an optimal point estimate y  X  , we maximise the expected utility y  X  , opt = utility function U . In our experiments, we will use a utility function corresponding to Hamming error, i.e. which counts the number of components of the label vector which are correct. For this utility function, maximising expected utility implies maximising each component independently: y posterior marginal inference (Marroquin et al., 1987). Much evidence shows that ensemble learners can exceed the performance of simple models. Examples of ensem-ble methods are bagging, boosting, random forests and their variants. The bagging algorithm (Breiman, 1996) trains each weak learner from bootstrap data and com-bines individual predictions by uniform averaging or vot-ing over class labels. We use the non-parametric bootstrap of Fushiki et al. (2005) to construct the predictive distribu-tion from Monte Carlo samples. We can now present the full algorithm of our method. Let T be the (application-dependent) size of the ensemble. 1. ( Distributed stage ) For each weak learner t = 2. ( Aggregation stage ) Compute the complete predic-Our bagging algorithm for GPstruct is distributed by na-ture: step 1 can run on slave nodes, step 2 on the master nodes, as our experiments demonstrate. Kernel CRFs (Lafferty et al., 2004) are non-parametric ran-dom field models sharing many of the design goals of the GPstruct model; they also scale in complexity with the amount of training data. A clique selection algorithm al-lows some degree of sparsification. The method we present here may be applicable to speed up kernel CRFs as well as GPstruct.
 As described earlier, our work is closely related to the re-search efforts on GP sparsification , whose goal is to im-prove the runtime from cubic in the number of data points to linear. Almost all sparse GP methods exploit a condi-tional independence assumption between training and test sets, given a set of inducing points (see Qui  X  nonero-Candela &amp; Rasmussen (2005) for a unifying view). However, even linear scaling can be prohibitive for very large data sets. Recent progress in sparsification has led to methods that can potentially process millions of data points (Hensman et al., 2013). Our proposed ensemble method could further benefit from sparse GP methods implemented in each weak learner. This would allow large scale non-parametric struc-tured prediction on high resolution images with millions of pixels. We assess the performance of bagging applied to the GP-struct model on a multiclass image segmentation task using two datasets. We compare GPstruct to a number of other techniques. A further experiment based on an image de-noising task is detailed in the supplementary material. Stanford Background Dataset (Gould et al., 2009) This dataset consists of 715 images of different sizes, resized to 50  X  150 pixels. Each pixel in the image is labelled with one of 8 classes, i.e. { sky, tree, road, grass, water, building, mountain, foreground object } . We keep 80% of the data for the training set (i.e. 572 images), and 20% for the test set (143 images). This split is repeated over 5 folds. LabelMeFacade Image Database (Fr  X  ohlich et al., 2010) Our second dataset contains 100 images for training and 845 images for testing. The images are of different sizes and are resized to 50  X  150 pixels. Each pixel in the image is labelled with one of 9 classes, i.e. { building, car, door, pavement, road, sky, vegetation, window, unlabelled } . For each problem, we will compare the performance of GPstruct to that of other models suited to the same task. Our aim is to assess the contribution of different aspects of our proposed model: the non-parametric property of the latent variables, the choice of learning technique (MCMC vs. margin optimisation), the  X  X tructured output X  property of GPstruct obtained by factors on the inter-pixel edges, and the improvement brought by bagging.
 CRF PL is a CRF model trained with pseudo-likelihood. CRF LBMO (for loss-based marginal optimisation) is the model described in Domke (2013), and is considered to be state-of-the-art for vision CRF applications. While traditionally, CRF parameter learning optimises the like-lihood, Domke (2013) suggests fitting parameters based on the quality of prediction of a given marginal infer-ence algorithm, obtained by TRW or mean-field (we use TRW in our experiments), using truncated univariate lo-gistic loss. Domke (2013) outperforms likelihood-based learning methods such as PL on difficult problems where the model being fit is approximate in nature, such as image denoising and image segmentation tasks. independent is a variant of CRF LBMO based on the same training procedure, but where prediction ignores pair-wise edges, preserving only unary features. This helps ap-preciating the contribution of the edge factors.
 CRF LBMO bag applies the same bagging procedure as the GPstruct model to the CRF LBMO model, i.e. weak learners are trained on the training set, and their predic-tions combined to obtain an overall prediction. Since CRF LBMO is CRF-based it produces probabilistic predic-tions, so that combining predictions consists of averaging marginals produced by each weak learner.
 We train these CRF-based models with a regularisation pa-rameter of 10  X  4 as in Domke (2013). Splitting regulari-sation parameters into unary and pairwise parameters, and giving the pairwise parameter a smaller value did not im-prove performance in our experiments. We use a data inde-pendent pairwise Potts factor. Our unary features are per-pixel posteriors produced by a random forest model, with 11 trees of depth 18 each. Our implementation was done in Matlab 2 , and we use J. Domke X  X  toolbox 3 for the CRF models.
 The GPstruct model is configured as follows. We use a squared exponential kernel between the pixels 4 , i.e. k ( x i , x j ) = exp(  X   X  k x i  X  x j k 2 ) . The kernel width  X  is set to 1 / ( number of features ) . We train 50 weak learners in total. Each weak learner is trained on 5000 pixel posi-tions uniformly chosen in the training set images. Using only a subset of the weak learners, we can explore how to trade performance against runtime.
 Computations for GPstruct were distributed on an Amazon cluster using MIT X  X  Starcluster 5 . Each weak learner trained and issued partial predictions (steps 1a through 1c in sec-tion 4) on a separate slave node, and the final aggregation step (step 2) was carried out on the master node. 6.1. Results The results are summarised in tables 1 and 2. Our primary error metric is the per-pixel error rate, which is consistent with prediction using maximum posterior marginals. Since GPstruct and CRF LBMO produce probabilistic predic-tions, it is also relevant to assess the quality of posterior marginals, e.g. using the negative log marginals of the test data as a metric, which the reader will find discussed in the supplementary material. We also provide a sample visuali-sation of marginals and predicted labels for all methods in figure 2.
 GPstruct outperforms the other models consistently, even with only about 15 weak learners. In all cases, each GPstruct weak learner ever only learns from 5000 pix-els. CRF PL performs weakly, which is consistent with previous studies (Domke, 2013) which found CRF trained with PL suffers from model mis-specification and places too much emphasis on the pairwise factors. However, in-terestingly, PL performs well when used as a likelihood ap-proximation for ESS in the GPstruct model.
 Bagging has an effect on the performance of CRF LBMO , but it is insufficient to bring it to par with GPstruct . Therefore bagging alone does not justify why GPstruct performs better than CRF LBMO , and other properties like being non-parametric and Bayesian come into play. To illustrate configuration options for GPstruct , we plot-ted the error rate against a varying number of weak learners in figure 3, which shows that GPstruct will attain its best performance from 15 weak learners up. Table 1 shows that increasing the number of images seen by the GPstruct weak learners is responsible for a performance increase, since in all cases the total number of pixels learnt from is constant at 5000 . The usefulness of having more inde-pendent training images over having more dependent pixels from the same images has been observed earlier by Shotton et al. (2011) and Nowozin et al. (2011).
 To explore the effect of varying the number of subsampled pixel positions in each image, we started from the set up of GPstruct 15 WL in table 1, with a fixed training set size of 50 images (second column). This corresponds to 100 pixels per images. Reducing the number of subsam-pled pixels in each image to 50 (for a total of 2500 pixels) reduces the error rate to 25 . 18  X  0 . 70 , which is still better than the 26 . 61  X  0 . 65 error rate obtained for 25 training images and 200 pixels per image (for a total of 5000 pix-els), cf. table 1. Reducing to 25 pixels per image instead of the original 100 , the error rates drops again slightly to 25 . 23  X  0 . 68 . This corroborates the finding that more in-dependent images support while keeping the total number of pixels increases performance, and illustrates the robust-ness of GPstruct in the small data regime. More configu-ration options pertaining to the GPstruct model, especially MCMC learning, are discussed in (Brati ` eres et al., 2013). 6.2. Runtimes and complexity Training runtimes on the full training data set are around 360 , 000 sec for CRF PL , 61 , 000 sec for CRF LBMO and independent . The ensemble method we describe here allows trading performance against runtime, since we can choose how many WL to train. Please refer to figure 3 for more details. Each weak learner of GPstruct trains for around 12h ( 43 , 000 sec), the same applies to each weak learner of CRF LBMO bag . GPstruct outperforms the other non-bagging methods with just around 5 weak learn-ers (equivalent runtime 215 , 000 sec). It has equal runtime to CRF LBMO bag and outperforms it from around 15 weak learners.
 The training runtime is dominated by the ESS algorithm X  X  complexity, which in turn is governed by evaluations of the likelihood function for each sample of the latent variables f . Each ESS step has a non-deterministic number of likeli-hood evaluations, as it is essentially a slice sampling algo-rithm.
 Details of the complexity analysis appear in the supplemen-tary material. 6.3. Effect of approximations on GPstruct In order to assess the effect of approximating the likeli-hood with PL, and the prediction with TRW, we conducted a small-scale experiment using the standard GPstruct, in which exact likelihood and prediction are tractable. For this, the Stanford Background Dataset images were resized to 10 by 10 pixels, and the multiclass segmentation prob-lem was turned into a foreground-background (2-class) segmentation task (the foreground label is available in the original dataset). We perform 4 shuffles of the data, and in each select 100 images for training, and 100 for testing. Error rates are averaged over these 4 shuffles and plotted against ESS iterations in figure 4.
 This figure shows that the effect of approximating the pre-diction function has virtually no effect in the error rate. The approximation in parameter estimation using PL appears to be robust in GPstruct, leaving only a 1% gap in accuracy with the exact likelihood. We can therefore conclude that the approximations we use, while efficient enough to make GPstruct scalable, are still robust enough to have a small impact on performance. The results in this paper are a first step in the direction of Bayesian non-parametric structured prediction for large grid factor graphs. Several questions arise from this formu-lation.
 Further scaling  X  In addition to using subsets of pixels to train weak learners, GP sparsification techniques (Snel-son &amp; Ghahramani, 2005; Hensman et al., 2013) applied inside each weak learner should allow substantial scale gains. This will result in hybrid methods that combine sampling and variational methods (see for example Welling et al. (2008)). In addition, the ensemble approach ap-plied to training here can also be applied at test time, al-lowing higher resolution images: we could subdivide the set of test latent variables, before applying the (approxi-mate) marginal computation once all latent variables are collected. There is a balance to be found between the size of pixel subsets per weak learner, and the computation overhead.
 Kernel learning  X  We only explored a joint input-output kernel function that decomposes into a kernel on input space and a kernel on output space (which was simply a scaled indicator function). The input-specific kernel further decomposes into a per-element kernel of the input. Further benefits of GPstruct could come from the use of more ex-pressive kernels over the entire input space, or potentially over the joint input-output space, combined with hyperpa-rameter learning, which we did not explore here. Exten-sions of kernel learning where a rich kernel can be con-structed through a weighted sum of base kernels (for ex-ample Bach (2008)), or even learning the structural form of the kernel itself (Duvenaud et al., 2013; Wilson &amp; Adams, 2013) are open problems. The recently presented GPstruct model (Brati ` eres et al., 2013) has appealing properties which distinguish it among the structured prediction models, but does not scale well due to both its O ( M 2 ) space and O ( M 3 ) time complexi-ties, where M is the number of GP latent variables. This in effect prevented GPstruct from being applied to vision problems involving grid factor graphs.
 Our main contributions are a distributed ensemble method in which weak GPstruct learners produce partial probabilis-tic predictions based on subsets of latent variables, which can be aggregated for a high-accuracy final prediction, and a demonstration that this approach can produce competitive results on two vision tasks. Each individual weak learner benefits from the GPstruct properties: they are kernelised, non-parametric and perform Bayesian inference.
 The resulting method is shown to perform very well on two classic vision tasks, binary denoising and multiclass image segmentation. In the segmentation task, GPstruct consis-tently outperforms state-of-the-art comparisons, and scales well to large data, with M = 2 million latent variables. The authors thank Viktoriia Sharmanska for discussions and Justin Domke for help with his code. NQ is supported by the Newton International Fellowship.
 Bach, Francis. Exploring large feature spaces with hierar-chical multiple kernel learning. In NIPS , 2008.
 Besag, J. Statistical analysis of non-lattice data. Journal of the royal statistical society. Series D (The Statistician) , 24(3):179 X 195, 1975.
 Blake, Andrew, Kohli, Pushmeet, and Rother, Carsten.
Markov Random Fields for Vision and Image Process-ing . MIT Press, 2011.
 Brati ` eres, S  X  ebastien, Quadrianto, Novi, and Ghahramani,
Zoubin. Bayesian structured prediction using Gaussian processes. 2013. http://arxiv.org/abs/1307.3846.
 Breiman, Leo. Bagging predictors. Machine Learning , 24 (2):123 X 140, 1996.
 Domke, Justin. Learning graphical model parameters with approximate marginal inference. IEEE Transactions on Pattern Analysis and Machine Intelligence , 35(10): 2454 X 2467, 2013.
 Duvenaud, David, Lloyd, James Robert, Grosse, Roger,
Tenenbaum, Joshua B., and Ghahramani, Zoubin. Struc-ture discovery in nonparametric regression through com-positional kernel search. In ICML , 2013.
 Fr  X  ohlich, Bj  X  orn, Rodner, Erik, and Denzler, Joachim. A fast approach for pixelwise labeling of facade images. In ICPR , 2010.
 Fushiki, Tadayoshi, Komaki, Fumiyasu, and Aihara, Kazuyuki. Nonparametric bootstrap prediction. Bernoulli , 11(2):293 X 307, 2005.
 Gould, S., Fulton, R., and Koller, D. Decomposing a scene into geometric and semantically consistent regions. In ICCV , 2009.
 Hensman, James, Fusi, Nicolo, and Lawrence, Neil. Gaus-sian processes for big data. In UAI , 2013.
 Lafferty, John, McCallum, Andrew, and Pereira, Fernando.
Conditional random fields: Probabilistic models for seg-menting and labeling sequence data. In ICML , 2001. Lafferty, John D., Zhu, Xiaojin, and Liu, Yan. Kernel con-ditional random fields: representation and clique selec-tion. In ICML , 2004.
 Marroquin, J., Mitter, S., and Poggio, T. Probabilistic solu-tion of ill-posed problems in computational vision. Jour-nal of the American Stat. Assoc. , 82(397):76 X 89, 1987. Murray, Iain and Ghahramani, Zoubin. Bayesian learning in undirected graphical models: approximate MCMC al-gorithms. In UAI , 2004.
 Murray, Iain, Ghahramani, Zoubin, and MacKay, David J. C. MCMC for doubly-intractable distributions. In UAI , 2006.
 Murray, Iain, Adams, Ryan P., and MacKay, David J. C.
Elliptical slice sampling. JMLR -Proceedings Track , 9: 541 X 548, 2010.
 Nowozin, Sebastian, Rother, Carsten, Bagon, Shai, Sharp,
Toby, Yao, Bangpeng, and Kohli, Pushmeet. Decision tree fields. In ICCV , 2011.
 Parise, Sridevi and Welling, Max. Learning in Markov ran-dom fields: An empirical study. In Joint Statistical Meet-ing , 2005.
 Qui  X  nonero-Candela, Joaquin and Rasmussen, Carl Ed-ward. A unifying view of sparse approximate Gaussian process regression. Journal of Machine Learning Re-search , 6:1939 X 1960, 2005.
 Rasmussen, Carl E. and Williams, Christopher K. I. Gaus-sian Processes for Machine Learning . MIT Press, 2006. Sch  X  olkopf, Bernhard and Smola, Alexander J. Learning with Kernels: Support Vector Machines, Regularization, Optimization, and Beyond . MIT Press, 2001.
 Shotton, Jamie, Fitzgibbon, Andrew W., Cook, Mat, Sharp,
Toby, Finocchio, Mark, Moore, Richard, Kipman, Alex, and Blake, Andrew. Real-time human pose recognition in parts from single depth images. In CVPR , 2011. Snelson, Edward and Ghahramani, Zoubin. Sparse Gaus-sian processes using pseudo-inputs. In NIPS , 2005. Sutton, Charles A. and McCallum, Andrew. An intro-duction to conditional random fields. Foundations and Trends in Machine Learning , 4(4):267 X 373, 2012.
 Tresp, Volker. A Bayesian committee machine. Neural Computation , 12(11):2719 X 2741, 2000.
 Wainwright, Martin J. and Jordan, Michael I. Graphical models, exponential families, and variational inference.
Foundations and Trends in Machine Learning , 1(1-2):1 X  305, 2008.
 Wang, Lei, Liu, Jun, and Li, Stan Z. MRF parameter esti-mation by MCMC method. Pattern Recognition , 33(11): 1919 X 1925, 2000.
 Welling, M., Teh, Y.W., and Kappen, B. Hybrid variational-MCMC inference in Bayesian networks. In UAI , 2008. Wilson, Andrew G. and Adams, Ryan P. Gaussian process kernels for pattern discovery and extrapolation. In ICML ,
