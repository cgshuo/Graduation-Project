 Anna Goldenberg ANYA @ CMU . EDU Andrew Moore AWM @ CS . CMU . EDU Robotics Institute, CMU 5000 Forbes Ave, Pittsburgh, PA 15213 USA Keywords: Bayesian networks/graphical models, statistical learning, Bayes Net structure learning Bayesian Networks have been successfully applied in ar-eas such as pharmaceutical research, decision making by doctors, air control and marketing. Structural learning of Bayesian Networks is usually desirable but costly. This paper provides an algorithm for tractable structural learning in Bayes Nets by exploring structures on the local level. We exploit the computational efficiency of Frequent Sets for gathering statistics that are most likely to be use-ful for structure search given the assumption of sparse data. We then give an efficient search algorithm to exploit these statistics for creating the global Bayes Net. 1.1. Why learn large Bayes Nets? Usage of Bayesian Networks to represent expression of genes based on the activity of their regulators (in practice approximated by protein activity levels) is well motivated by Friedman (2004). He suggests that the structure of the network is important in itself, since it may provide infor-mation about gene regulators.
 Another field that has received increasing attention in the last few years is recommender systems. Online systems such as Amazon provide suggestions of what might appeal to the user based on user X  X  other preferences. The use of Bayesian networks in this domain has become established, e.g. (Breese et al., 1998). Often the goal of recommender systems is to predict which are the most likely items that the user would buy next. An example of answering analogous query using Bayes Nets built by our algorithm is presented in Section 6.2.
 The idea of representing social networks as people con-nected by directed arrows has been explored in social sci-ence domain for almost 70 years (Moreno &amp; Jennings, 1938). Initially analyzed networks were on the order of 10s of nodes. However, improvements in data collection and especially the birth of online communities made it necessary to look at much larger networks. For exam-ple, livejournal (an online blog community) contains over 50,000 users (Shklovsky, personal communication). Usage of graphical models in this domain has become increas-ingly popular, due to their robustness to noise. Studies in the gene expression data and social networks in particular suggest that correlations of entities on the local level are very important and in fact they are what makes up the global network (Friedman, 2004; Breiger, 2003). So, along with being computationally practical, Bayesian Net-works created by our algorithm have a very natural motiva-tion stemming from those important domains.
 We provide results on sparse massive datasets showing practical training times, and in many cases superior abil-ity to model the joint distribution in comparison with di-rect extensions of traditional structure search algorithms on large data. We also qualitatively and empirically show that sparse data (particularly data with social net charac-teristics) are modelled better by going beyond information derived from pairwise co-occurrences. Assume our training data is a collection of records with value of the th attribute of the th record where and . We assume sparse data in which the vast majority of values in any dataset row or column are zero. Note we assume no missing values: all are observed. Let the attributes be represented by integers set of attributes be the number of records in which all the attributes in are simultaneously set to 1. Given we say is a Frequent Set of attributes if contains exactly attributes and . Thresh-old is called support in the data mining literature. Given sparse data and a support greater than about 3, it is sur-prisingly easy to compute all Frequent Sets (Agrawal &amp; Srikant, 1994). There is an abundance of literature on Fre-quent Sets as their collection is an essential part of the asso-ciation rules algorithms (Agrawal et al., 1993; Agrawal &amp; Srikant, 1994; Han &amp; Kamber, 2000) widely used in com-mercial data mining.
 There are multiple references to Frequent Sets in the area of modelling sparse datasets as well (Mannila &amp; Toivonen, 1996; Chickering &amp; Heckerman, 1999; Pavlov et al., 2003; Hollmen et al., 2003). This is not surprising, since sparse-ness implies very few co-occurrences between items. In fact, most items do not co-occur with each other, hence we expect the majority of the counts in the pairwise marginals to be . Therefore, it is natural to assume that the Fre-quent Sets contain most of the essential information about the whole dataset. (Chickering &amp; Heckerman, 1999) propose and show how to use an efficient sparse representation for several classes of machine learning algorithms including structure initial-ization for Bayes Nets. We will therefore not focus on rep-resentational aspects of Frequent Sets.
 This paper exploits previous research on the utilization of Frequent Sets for modelling of sparse datasets but takes a new perspective. Assuming that Frequent Sets comprise essential information about our data we propose to exploit them to find Bayes Net structures on the local level. To our knowledge, structures contained within Frequent Sets have not been previously used in order to improve the global model of data. The simplest idea for exploiting Frequent Set information is to use frequent pairs. The only edges which we would consider including in the Bayes Net are those for which the source and destination attributes co-occur more than some support . There are thus far fewer edges to consider than the full possibilities ( is the number of at-tributes).
 There are three problems with this idea.
 Screening the Frequent Sets. We call the set of edges that will eventually be considered for addition into the Bayesian Network the Edgedump . Suppose we have a collection of Frequent Sets . First, we screen the pairs to find positive pairwise correlations. We add an edge between two variables to the if and only if a significant correlation was found between the vari-ables in the pair. We then in turn screen for dependencies in Frequent Sets of size , , etc.
 When does a Frequent Set of size provide new information valuable for building a Bayes Net? It is possi-ble that the dependencies of are already well-explained by interactions of order less than . For example, sup-pose attributes and co-occur frequently, but their co-occurence is well explained by the local Bayesian Net-work DAG structure of . In that case the two-way interactions will already explain all dependencies of . In this case, should not be added to the edge-dump. In fact, only DAGs that contain a node with parents could be missed by considering only lower order interactions.
 We implement a Screening test by searching over all pos-sible DAG structures for and finding whether the best BDeu-scoring structure (see Section 6) has an -parent node (we call it an m-way interaction). We thus allow to pass the screening test if and only if is best explained by a local DAG structure containing an -way interaction. If passes the Screening test, all edges of the highest scoring DAG are added to the Edgedump.
 Once the Edgedump is created, we prioritize the edges ac-cording to their strength, measured by the number of the -way interactions in which they participate. We then cre-ate an empty (edgeless) global Bayesian network and iter-ate through the Edgedump contents, allowing each edge in turn to be added if and only if it improves BDeu and avoids cycles.
 Table 1 contains the full description of the algorithm. In the previous section we pointed out that Frequent Sets bias the algorithm in favor of interactions that cause co-occurrence (and thus positive correlation). Appendix 1 shows why, in the case of sparse data, positive correlations must be stronger than negative correlations, so in general we are not omitting the strongest correlations. There is, however, still a danger that if a few attributes are promiscu-ous (relatively high univariate marginal probability, though still very sparse), they could cause significant negative cor-relations that we could miss. Fortunately, such negative pairwise correlations can be detected cheaply using a tech-nique from (Meila, 1999).
 Let be the mutual information between two attributes. Meila showed that the mutual information can be calcu-lated in a very efficient manner, particularly when dealing with discrete binary data. In fact, if the two variables have not co-occurred in the dataset, the formula simplifies even algorithm SBNS input -max Frequent Set size output -Bayes Net
Also: -Edgedump -a collection of directed edges 10. store all edges in 11. order in decreasing order of edge counts 12. foreach edge 13. if doesn X  X  form a cycle in 14. and improves BDeu 15. add to 16. end foreach 17. return further: is directly proportional to the magnitude of ( ) and ( ) as shown in Equation (2). The full derivation is available in (Meila, 1999). Hence, to add high mutual information edges, we have to check entities that occur with high frequency. We reduce the total number of entities significantly by only consid-ering ones that occurred more than times in the dataset. This step is statistically justified because fewer occurrences mean lower possible mutual information. Table 2 describes the algorithm that augments a given Bayes Net with high mutual information (MI) edges.
 algorithm AugmentWithMutualEdges input -a Bayes Net 10. end while 11. end for 12. return Note that we do not have to search the space of all edges to find edges with the highest mutual information. First of all, we sort entities in descending order of frequency. For each entity , where is the number of entities with support , we only consider , i.e. those entities that have occurred less fre-quently than . If an edge has been rejected, then we move along the list. This step is justified, because en-tities are sorted in descending order of frequencies, hence the mutual information between and is lower than between and . Thus, the edge is even less likely to be added than . Empirical evidence shows that on average only 10% of pairs are considered. 5.1. Second Degree Separation Links It is cheap to do an extra pass of edge-additions in which we iterate over all nodes in the network produced by the previous steps and attempt adding edges directly from the current node to its grandchildren. 5.2. Hillclimbing One of the standard techniques to improve the score is hillclimbing as described in (Cooper &amp; Her-skovits, 1991). This technique improves the score by adding/removing/reversing arcs in a Bayes Net. The set of operations and edge selection procedure may differ between algorithms. Usually hillclimbing is performed in a beam search way: at each step the existing model under-goes a modification/addition of a single edge. In order to pick the best edge we must look at ) possibilities. Since the number of nodes prohibits us to perform even a linear search at each step, we use random hillclimbing in which at each step we choose edges randomly. Specifically, we roll a  X 3-sided X  die with probabilities for addition and for deletion and arc-reversal, and then pick an edge at random to see whether performing the chosen operation improves the global score. The evaluation uses BDeu score described in (Heckerman et al., 1995) and also presented here in equation 3 to com-pare results between different configurations of our algo-rithm and to the randomized hillclimbing as described in Section 5.2. * # where is the th variable, -states of the th parent of , -true/false (in our case of binary variables) states of . The datasets are listed in Table 3. Holdout testsets were used to evaluate overfitting as discussed in Section 6.2.3. 6.1. Datasets The algorithm has been tested on several real life datasets (sizes shown in Table 3).
 6.2. Empirical Results We tested our algorithm in a variety of configurations on the datasets listed in Table 3. The results in Table 4 are reported in terms of the average BDeu score, i.e. the fi-nal BDeu score obtained by the network averaged over the number of records in the dataset. The number of edges in the resulting Bayes Nets is reported in Table 5. It is interest-ing to note that the BDeu scores correspondingto the Bayes Nets obtained by running as described in Table 1 are very close to the ones obtained by random hillclimbing, but have significantly lower number of edges. This sup-ports our claim that the frequent itemsets indeed contain information most relevant to the construction of the highest scoring Bayes Net. It is evident from the results that each of the proposed augmenting algorithms increase the score. We note however that after augmenting the network with highest-mutual-information edges the total number of arcs almost doubled with the highest relative improvement in score when compared to other proposed augmenting tech-niques. The hillclimbing seems to improve the score even further though the number of edges is almost quadrupled compared to . The final score of the DAG produced by depends on user-defined support and maximum Frequent Set size. We have noticed that for Citeseer, IMDB and Institute datasets lowering support and increasing maximum Fre-quent Set size results in higher BDeu scores. Figure 1 shows score fluctuations when varying maximum Frequent Set size given fixed support for the Citeseer dataset. In our experiments we tried different maximum Frequent Set sizes: ( mfss ). The lower bound mfss means that we consider only pairs of items and thus the structure learned is based solely on two-way marginal counts. Figure 1 shows that there is an obvious loss in ac-curacy when high order interactions are not taken into ac-count. Beyond a maximum Frequent Set size of the num-ber of Frequent Sets does not increase substantially in these datasets and hence the behavior of changes little. We have to note here, that there is a natural upper bound on the maximum tuple size due to the sparsity of the datasets. For example, there are publications in the Citeseer database that have authors and only that have ex-actly authors. The potential number of publications that have authors, given the total number of authors in the database is , so the empirical number is only of the total. The exponential drop in the number of occurrences as the size of the tuples increases is shown on Figure 2. Hence, we cannot expect a great im-provement in the score of the Bayes Net when increasing the maximum tuple size, since there is not enough support for larger tuples. 6.2.2. S UPPORT Lowering support greatly increases the number of Frequent Sets to be considered during screening. However, it also introduces quite a few interactions between variables that have low marginal counts. Model fitting in contingency tables in general is sensitive to very low marginal counts even if they are not zero (Bishop et al., 1977). Here we use BDeu, which is less sensitive to low counts. Despite this, it seems to be a good idea to keep support relatively large. In our case, we have tested a few support sizes on smaller datasets and found to be reasonable sup-port choices. The overall score of the model seems to be better with , however it seems to overfit more as is shown in Table 6. We used holdout sets to study overfitting. We withheld roughly a third of the dataset in each case and compared av-erage likelihood per node between the training and testing datasets. The results are summarized in Table 6. The net-works learned using always score higher (better) than those learned by hillclimbing on the testing dataset. This indicates that algorithm learns better fitting models. As can be seen from Table 6, the difference in average loglikelihood score for training and testing is in general smaller for hillclimbing. Also, the average loglike-lihood of the testing set is worse than the training sets, in-dicating some degree of overfitting. We believe that some overfitting occurs due to the multiple hypothesis testing of hundreds of thousands of possible parents. Correction for multiple hypothesis testing problem (similar to corrections used in other learning algorithms such as (Oates &amp; Jensen, 1998)) will be incorporated into in the future. All experiments were conducted on unloaded 2GHz Pentium IV machines with 2GB of RAM. The to-tal times required to run the algorithm and the time it took random hillclimbing to create a Bayes Net by adding/removing/reversing edges are reported in Table 7.
 We also break the total time into segments correspondingto major steps of the algorithm as reported in Table 8. The biggest cost is to obtain the frequencies; the time it takes to perform the remaining operations depends on the num-ber of Frequent Sets that occur more frequently than pre-defined support. Our experiments have shown that num-ber to be only a small fraction of the total number of enti-ties (nodes). It is also interesting to note that random hill-climbing is very fast while the network consists of many small subgraphs, but as soon as the subgraphs are joined together by new edges, the time increases tremendously due to the complexity of cycle detection. For example, it takes random hillclimbing on the order of 10 minutes to add/remove/reverse edges, but it takes over 6 hours to perform those operations given the same number of nodes for edges with relatively small increase in the score. In that sense, the random graphs might not be exactly random as discussed in (Callaway et al., 2001). 6.3. Example application One of the important and growing application fields of large Bayes Nets is recommender systems. A related appli-cation is intelligence: having detected a subset of partici-pants of an adverse event, inferring likely accomplices. The purpose of a recommender service is to provide user with suggestion of products that he/she is likely to buy based on their historical preferences. We simulated a recommender query based on the Citeseer dataset. The mapping is as follows: suppose that the set of co-authors of a paper rep-resents user X  X  preferences of particular products. We then learn a Bayes Net based on the available co-authorship in-formation and query the network with incomplete subsets of authors to predict the most likely selection of entities (or authors in our case) that completes the given set. To answer the query we reported the top most likely completions with the highest loglikelihoods 2 . Our example query is a subset of former or present members of Daphne Koller X  X  group (DAGS): d koller, l getoor, a pfeffer, b taskar . Results are presented in Tables 9 and 10. The suggested completions are in fact people that are either part of or collaborate closely with Daphne Koller X  X  group, completion score koller grove halpern pfeffer getoor taskar -24.065985 koller malik weber pfeffer getoor taskar -24.335174 koller russell parr pfeffer getoor taskar -24.688802 thus by analogy we might expect a set of relevant items to be predicted by the recommender system using this algo-rithm. It is interesting to note that in the example above the one most likely person to complete the given subset is different (Table 9) than the suggestions provided by the al-gorithm under the assumption of 2 missing people (Table 10). This observation suggests that there are more complex interactions that could not be found by systems built on pairwise statistics. The inference took less than a second. Some of the earlier work in this area has concentrated on efficient representation of sparse data and caching of n-way counts (Moore &amp; Lee, 1998). Chickering and Heckerman (1999) and Meila (1999) have noted that computations re-quiring one-way and pairwise counts can be sped up sig-nificantly when dealing with sparse data using caching and such data structures as ADTrees (Moore &amp; Lee, 1998). We believe that this body of work has great potential and thus we build on the ideas introduced in these papers by utilizing the sparse data representation and low overhead efficient calculation of the marginals.
 Using frequent sets when learning Bayes Nets on the lo-cal scale was also explored in (Pavlov et al., 2003). The goal of this work was to answer probabilistic queries on a subset of variables, thus there was no need to combine local information to obtain the joint distribution once the query size was estimated. The performance of Bayes Nets learned from a selection of variables was reported to be worse though close in accuracy to the inferences drawn from a Bayes Net learned on a full dataset. In (Hollmen et al., 2003) it has been proposed to integrate Frequent Sets as a local methodology when modelling joint distributions. This work has shown that mixture models obtained from Frequent Sets using maximum entropy are more accurate, thus supporting our claim that frequent sets contain impor-tant local information when modelling joint distributions. One approach to speed up structural search in Bayes Nets for massive datasets has been to restrict the possible par-ents. The full Sparse Candidate Algorithm is presented in (Friedman et al., 1999). In its original form it is a method to speed up hillclimbing at the cost of lower performance, though in practice the performance loss was shown to be not so great. This work is yet another motivation for us, since structural search on the local scale inadvertently re-stricts the number of parents. However, since on the global scale the number of parents in our Bayesian Network is not limited we perceive it as an improvement on the original Sparse Candidate algorithm.
 Sampling was proposed as one of the techniques to speed up modelling in massive datasets in (Hulten &amp; Domingos, 2002; Pelleg &amp; Moore, 2002). Though an interesting direc-tion it seems to be orthogonal to our approach.
 The idea of augmenting Bayes Nets with high mutual in-formation edges is based on the fact that such dependen-cies could not be accounted for in frequent sets. The fast computation used in this work is based on (Meila, 1999). We have presented a tractable solution to the Bayes Net structure search problem in sparse datasets. Like other researchers, we use Frequent Sets to take advantage of sparseness. Our main new contribution is to perform struc-tural search on the local level in order to produce the global model. We propose several techniques to improve the score of the resulting net. One of the key improvements is aug-mentation by edges with high mutual information for enti-ties that have not co-occurred in the dataset.
 We have performed an empirical study of using two small and two large (over attributes) datasets. We show tractable times while maintaining accuracy better than hill-climbing, which is the only tractable alternative for learn-ing structure in networks of this size.
 We believe that SBNS serves two primary purposes. First, it opens new horizons for modelling joint distributions of massive sparse datasets. Second, it can be viewed as a novel way to postprocess Frequent Sets in commercial data mining. We also raise a question of utilizing dataset prop-erties to improve computational complexity of structural search. We feel that there is an immense potential in ex-ploiting other properties such as frequency distribution to obtain high accuracy models in a fraction of time. To support our claim that the Frequent Sets contain essen-tial information needed to build a Bayes Net from sparse data, we show that in sparse large datasets positive correla-tion between two variables is much stronger than negative Suppose we have 2 binary variables and + with corre-lation coefficient , . Assume our dataset is sparse and has % records, where % is very large. Under the multinomial sampling model the observed correlation coefficient is the maximum likelihood estimate of , (Bishop et al., 1977). Case 1: Two entities have co-occurred ! times and ! times separately elsewhere in the dataset, . Then In fact, only as (which is clearly a violation of the sparseness assumption), the correlation between and + becomes significant.
 Case 2: Two entities have occurred with frequency -! but never with each other. -could be rather large, but still conforming to the sparseness assumption, i.e. -! % , then . Note that when ,wehave the same frequency of occurrence as in Case 1, yet only if We showed that under a sparse assumption, positive corre-lations are much stronger than negative ones. Thus, when learning a Bayes Net we are much more likely to increase the score by screening Frequent Sets first.

