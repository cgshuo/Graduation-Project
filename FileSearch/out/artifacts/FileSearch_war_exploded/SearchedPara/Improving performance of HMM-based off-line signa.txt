 FULL PAPER Luana Batista  X  Eric Granger  X  Robert Sabourin Abstract The neural and statistical classifiers employed in off-line signature verification (SV) systems are often designed from limited and unbalanced training data. In this article,anapproachbasedonthecombinationofdiscreteHid-den Markov Models (HMMs) in the ROC space is proposed to improve the performance of these systems. Inspired by the multiple-hypothesis principle, this approach allows the sys-tem to select, from a set of different HMMs, the most suitable solution for a given input sample. By training an ensemble of user-specific HMMs with different number of states and dif-ferent codebook sizes, and then combining these models in the ROC space, it is possible to construct a composite ROC curve that provides a more accurate estimation of system performance. Moreover, in testing mode, the correspond-ing operating points X  X hich may be selected dynamically according to the risk associated with input samples X  X an sig-nificantly reduce the error rates. Experiments performed by using a real-world off-line SV database, with random, sim-ple and skilled forgeries, indicate that the multi-hypothesis approach can reduce the average error rates by more than 17%, as well as the number of HMM states by 48%.
 Keywords Off-line signature verification  X  ROC curves  X  Codebook selection  X  Hidden Markov Models  X  Pattern recognition 1 Introduction Signature verification (SV) systems seek to authenticate the identity of an individual, based on the analysis of his/her sig-nature, through a process that discriminates a genuine signa-ture from a forgery [ 26 ]. SV systems are relevant in many situations where handwritten signatures are currently used, such as cashing checks, transactions with credit cards, and authenticating documents [ 15 ]. In off-line SV, signatures are available on sheets of paper, which are later scanned in order to obtain a digital representation. Given a digitized signature, an off-line SV system will perform preprocessing, feature extraction, and classification (also called verification). For complete and recent surveys of off-line SV, see [ 1 ] and [ 17 ].
Among several well-known classification methods used in off-line SV, the discrete Hidden Markov Model (HMM) X  X  finite stochastic automata used to model sequences of obser-vations X  X dapts easily to the dynamic characteristics of the western handwriting [ 21 ]. Despite the fact that the HMM is a generative classifier [ 5 ], which generally requires a con-siderable amount of training data to achieve a high level of performance, the HMM-based off-line SV systems are often designed from limited and unbalanced data. This occurs because the dataset used to model a writer generally contains a reduced number of genuine signatures against several ran-dom forgeries [ 25 ]. Another issue with the use of discrete HMMs in off-line SV is the design of codebooks. 1 Typically, the data used to generate codebooks are the same data that are employed to train the HMMs. In the study of Ferrer et al. [ 8 ], all writers share an universal codebook generated with their training data. The main drawback of this strategy is the need to reconstruct the codebook whenever a new writer is added to the system. According to Rigoll and Kosmala [ 28 ], the utilization of user-specific codebooks, generated by using only the training signatures of a particular writer, adds one more personal characteristic to the verification process. However, it was observed that this strategy leads to poor system performance whenever a limited amount of train-ing signatures is available [ 6 ]. Finally, these systems have been evaluated through error rates calculated from a single threshold, assuming that the classification costs are always the same.

Though not fully explored in literature, it has recently been shown that the Receiver Operating Characteristic (ROC) curve X  X here the true positive rates (TPR) are plotted as function of the false positive rates (FPR) X  X rovides a pow-erful tool for evaluating, combining, and comparing off-line SV systems [ 3 , 25 ]. Several interesting properties can be observed from ROC curves. First, the area under the ROC curve (AUC) is equivalent to the probability that the clas-sifier will rank a randomly chosen positive sample higher than a randomly chosen negative sample [ 7 ]. This measure is useful to characterize the system performance by using a single scalar value. In addition, the optimal threshold for a given class distribution lies on the ROC convex hull, which is defined as being the smallest convex set containing the points of the ROC curve. Finally, by taking into account sev-eral operating points (thresholds), the ROC curve allows to analyze these systems under different classification costs [ 7 ]. This property is useful, for instance, for off-line SV systems where the operating points are selected dynamically accord-ing to the risk associated with the amount of a bank check.
In this article, an approach based on the combination of classifiers in the ROC space is proposed to improve perfor-mance of off-line SV systems designed from limited and unbalanced data. By training an ensemble of classifiers with different parameters, and then selecting the best classifier(s) for each operating point, it is possible to construct a compos-ite ROC curve that provides a more accurate estimation of systemperformanceduringtrainingandsignificantlyreduces the error rates during operations. This approach follows the multiple-hypothesis principle [ 11 ], which request the system to propagate several hypothesis throughout the recognition steps, generating a hierarchical tree of possible solutions.
Although the proposed approach may be applied to any type of statistical or neural 2-class classifier, this article in-volves HMMs trained by using different number of states and different codebooks. In order to avoid the problems caused by the use of limited codebooks, two databases are employed by the proposed approach: the development database (DB dev ), which contains the signatures of writers used to generate the candidate codebooks; and the exploitation database (DB exp which contains the signatures of the final users. The latter contains samples used to train, validate and test HMMs.
The rest of this article is organized as follows. The next section presents a brief survey of off-line SV systems based on discrete HMMs. Section 3 describes some key issues that affect the performance of off-line SV systems. Then, Sect. 4 describes the proposed approach and its advantages. Finally, in Sect. 5 , the experimental results are shown and discussed. 2 Off-line SV systems based on local discrete HMMs A traditional off-line SV system based on local discrete HMMs follows the steps shown by Fig. 1 .Atfirst,the signature is scanned in order to obtain a digital represen-tation composed of Q  X  R pixels. Then, the signature image is considered as a discrete 2D function I ( x , y ) x = 0 , 1 , 2 ,..., Q and y = 0 , 1 , 2 ,..., R denote the spatial coordinates, and the value of I in any ( x , y ) corresponds to the grey level (generally a value from 0 to 255) in that point [ 13 ]. After applying some corrections to the signature image (such as noise removal and centering), a set of feature vec-tors V = { V 1 , V 2 ,..., V L } are extracted and quantized in a sequence of discrete observations O = { o 1 , o 2 ,..., where each observation is a symbol provided by the code-book. Finally, the signature is classified as genuine or forgery by using the corresponding user-specific HMM  X  . Although Fig. 1 illustrates only one HMM  X  , the classification sub-system is composed of several local classifiers, where each one is a discrete HMM trained with data corresponding to a specific writer. 2 Since each writer is associated with a sin-gle classifier, these systems are also referred in this article as single-hypothesis systems. Multi-hypothesis systems, on the other hand, allow a same writer to employ different classifiers depending on the input samples.

A common way to report the SV system performance is in terms of error rates. The false negative rate (FNR) is related to genuine signatures which were misclassified as forgeries. Whereas the false positive rate (FPR) is related to forgeries which were misclassified as genuine signatures. FNR and FPR, also known as type 1 and type 2 errors, respectively, can be used to generate a pair { TPR , FPR } in the ROC space, since TPR = 1  X  FNR. Generally, the FPR is calculated regarding three forgery types: random, simple, and skilled. The random forgery is usually a genuine signature sample belonging to a different writer. It is produced when the forger has no access to the genuine samples, not even the writer X  X  name. In the case of simple forgeries, only the writer X  X  name is known. Thus, the forger reproduces the signature in his/her own style [ 4 ]. A simulated forgery represents a reasonable imitation of a genuine signature [ 19 ]. Finally, the average error rate (AER) is related to the total error of the system, where the type 1 and type 2 errors are averaged taking into account the a priori probabilities. On the other hand, if the decision threshold of a system is set to have the percentage of false negatives approximately equal to the percentage of false positives, the equal error rate (EER) is calculated.
Typically, the set of random forgeries of a specific writer is composed of true signatures from the remaining writers in the system. In this article, it is assumed that only random forgeries are used to train and validate classifiers, as well as to generate codebooks. In fact, it is not always possible to obtain samples of forgeries; and, in many real-world appli-cations (e.g., banking), it becomes impracticable [ 24 ]. On the other hand, all types of forgeries are used to test the system performance.

The rest of this section provides additional details on the feature extraction, vector quantization, and classification sub-systems considered in this article. 2.1 Feature extraction Two classes of features are used in off-line SV [ 1 ]: (i) sta-tic, related to the signature shape and (ii) pseudo-dynamic, related to the dynamics of the writing. These features can be extracted locally, if the signature is viewed as a set of segmented regions, or globally, if the signature is viewed as a whole. Since the HMMs are used to model sequence of observations, the local approach is typically employed [ 6 , 19 , 20 , 28 ]. In the grid segmentation scheme of Justino et al. [ 19 , 20 ], all images are aligned to the left and divided in 60 horizontal cells, where each cell is a rectangle composed of 16  X  40 pixels. Then, to absorb the horizontal variability of the signatures, the blank cells in the end of the images are discarded. Therefore, the images may have a variable num-ber of horizontal cells, while the number of vertical cells is always 10. Figure 2 shows an example of a signature image with its final width. Note that the segmentation can be per-formed both in horizontal and vertical directions.
Each column of cells is then converted into a feature vec-tor, where each vector element contains the density of pixels in its respective cell. In other words, each vector element is a value between 0 and 1 which corresponds to the number of black pixels in a cell divided by the total number of pixels of this cell. Other static features, such as pixel distribution and gravity center, as well as pseudo-dynamic features, such as axial slant and stroke curvature, have been extracted through this segmentation scheme [ 19 , 22 ]. 2.2 Vector quantization In order to generate a sequence of discrete observations, each extracted feature vector is quantized as one of the previously computed symbols of the codebook. This codebook may be generated through an iterative clustering algorithm called K -means [ 23 ]. As explained by Algorithm 1 , NV feature vec-tors are separated into NC clusters C i , where each cluster represents a symbol in the codebook.
 Algorithm 1 Description of the K-means algorithm, where C is the i th cluster with centroid c i .
 2.3 Classification Once the sequences of discrete observations are obtained, they are used to train and test the HMMs. A discrete HMM can be defined as  X  = ( N , M , A , B , X  ) , where [ 27 ]:  X  N is the number of distinct states in the model. The set  X  M is the alphabet size, that is, the number of distinct  X  A is the state transition probability distribution, denoted  X  B is the observation symbol probability distribution,  X  and  X  is the initial state distribution, denoted by  X  =
HMMs can be classified in two main types of topologies [ 27 ]: the ergotic and the left-to-right topologies. The ergotic topology is a specific case of a fully connected model in which every state can be reached from any other state in a finite number of steps. With the left-to-right topology, the state indices increase from left to right (that is, a ij = i ), and no transitions are allowed to states whose indices are lower than the current state. As consequence, the sequence of observations must begin in S 1 (that is,  X  i = 0 when i and  X  i = 1 when i = 1) and must end in S N . Finally, given a model initialized according to the constraints described so far, there are three tasks of interest [ 16 ]: 1. The Evaluation Problem . Given a model  X  and a 2. The Decoding Problem . Given a model  X  , what is the 3. The Learning Problem . Given a model  X  and a set
An advantage of the discrete HMMs is that it is not nec-essary to have a priori knowledge about the probability dis-tributions to model a signal [ 2 ]. With enough representative training data, it is possible to adjust the parameters for the HMM. Algorithms based on the expectation X  X aximization (E X  X ) technique (e.g., the Baum X  X elch algorithm) are gen-erally used to perform this task [ 14 ].

Typically, only genuine signatures are used for training a local HMM for SV. In this case, the decision boundary between impostor and genuine spaces is defined by using a validation set that contains samples of both classes. Another particularity of HMM-based SV systems is the use of the left-to-right topology. Indeed, this topology is perfectly adapted to the dynamic characteristics of the occidental handwritten, in which the hand movements are always from left to right. 3 Challenges with the single-hypothesis approach In this article, it is assumed that the performance of the whole SV system is measured by an overall ROC curve obtained from a set of user-specific ROC curves. Averaging methods have been used to group ROC curves produced from different user-specific classifiers in single-hypothesis systems [ 7 , 18 ]. Jain and Ross [ 18 ], for example, proposed a method to gener-ate an averaged ROC curve taking into account user-specific thresholds. 3 At first, the cumulative histogram of random forgery scores of each user i is computed. Then, the simi-larity scores (thresholds) providing a same value of cumula-tive frequency,  X  , are used to compute the operating points {
TPR i ( X  ), FPR i ( X  ) } . Finally, theoperatingpoints associated with a same  X  are averaged. Note that  X  can be viewed as the true negative rate (TNR=negatives (forgeries) correctly classified/total of negatives) and that it may be associated with different thresholds. Figure 3 shows an example where the thresholds associated with  X  = 0 . 3 are different for users 1 and 2, that is t user1 ( 0 . 3 )  X  =  X  5 . 6 and t user2
In off-line SV, where the dataset used to model a writer signature generally contains a reduced number of genuine samples against several random forgeries, it is common to obtain ROC curves with concave areas. In general, a con-cave area indicates that the ranking provided by the classifier in this region is worse than random [ 9 ]. Figure 4 ashows an example of score distribution in an off-line SV system. The positive class, P, contains only 10 genuine samples of a given writer, while the negative class, N, contains 100 sam-ples of forgeries. Due the limited amount of samples in the positive class, the resulting ROC curve (see Fig. 4 b) presents three concave areas, which correspond to low-quality predic-tions [ 9 ]. For example, the similarity scores between  X  and 0 provide TPRs of 90%. The result of averaging the ROC curves related to the models of 100 different writ-ers, by using the Ross X  X  method, is illustrated by Fig. 5 . Note that the imperfections of individual ROC curves are hidden within the average points, which can be observed with any averaging algorithm.

The drawback of using an averaged ROC curve can be observed during the selection of optimal thresholds in the respective convex hull. Given two  X  in the convex hull,  X  and  X  2 ,whereeachoneminimizesadifferentsetofcosts[ 30 ],  X  should provide a TPR higher than  X  1 whenever  X  1 &gt; X  2 However, regarding a user-specific ROC curve,  X  1 and  X  2 may fall in a same concave area, providing identical TPRs. An example is illustrated by Fig. 5 , where TPR ( X  = 0 . 86 is higher than TPR ( X  = 0 . 91 ) in the global convex hull, but, in the user-specific ROC curve, TPR ( X  = 0 . 86 ) is equal to TPR ( X  = 0 . 91 ) . 4 A multi-hypothesis approach Based on the combination of HMMs trained with different number of states, the approach proposed in this section pro-vides a solution to repair concavities of user-specific ROC curves while generating a high quality averaged ROC curve. The utilization of different HMMs is motivated by the fact that the superiority of a classifier over another may not occur on the whole ROC space [ 7 ]. Indeed, in off-line SV systems where the optimal number of states for a HMM is found empirically by a cross-validation process, 4 it is often ob-served that the best HMM is not superior than the other (a) (b) intermediate/ sub-optimal HMMs in all operating points of the ROC space. Three steps are involved in the proposed multi-hypothesis approach: model selection, combination, and averaging. 4.1 Model selection This step allows to select the best classifier for each oper-ating point such that every user X  X  performance is optimized. In this article, a ROC outer boundary is constructed for each user in order to encapsulate the best operating points pro-vided multiple HMMs, each one trained by using a different number of states. Given a set of ROC curves generated from different classifiers associated with a same user, the process consists in splitting the x -axis  X  X  0 , 1 ] into a number of bins, and within each bin finding the pair (FPR, TPR) having the largest value of TPR. While the ROC outer boundary is being generated, the best HMM j , where j is the number of states, is automatically chosen for each operating point. Figure 6 shows an example of a user-specific ROC outer boundary constructed from ROC curves of two different classifiers, HMM 7 and HMM 9 . The corresponding convex hull is com-posed of three vertices, p , q and r , where p and q are asso-ciated with HMM 9 , and r is associated with HMM 7 .
Algorithm 2 presents the approach for generating the outer boundary of ROC curves produced by training L min HMMs on a same dataset but with different number of states; where Algorithm 2 Generating ROC outer boundaries from differ-ent HMMs.
 L min is the number of observations in the shorter training sequence.

Note that this process can also be extended for multi-ple codebooks. In other words, by training an ensemble of HMMs with different codebook sizes and different number of states, each bin can be associated with the pair {codebook, state} providing the highest TPR. Therefore, depending on the operating point, a same individual may use a different model, denoted as HMM NC j , trained with j states and with a codebook of NC clusters. 4.2 Combination This step allows to interpolate between two consecutive clas-sifiers on the ROC curve in order to obtain not yet reached operating points. In this article, the method proposed by Scott et. al [ 29 ] is applied to the ROC outer boundaries in order to repair concavities. Given two vertices A and B on the con-vex hull, it is possible to realize a point C , located between A and B , by randomly choosing A or B . The probability of selecting one of the two operating points is determined by the distance of C regarding A and B . Equations 1 and 2 indicate how a given FPR C can be obtained. The expected operating point (FPR C ,TPR C ) is given by Eqs. 3 and 4 [ 29 ].
By using the Eqs. 1 X 4, Algorithm 3 can realize any FPR C located between two consecutive classifiers, A and B , where each classifier corresponds to a hull vertex of a user-specific ROC outer boundary. Figure 7 presents an example of a max-imum realizable ROC (MRROC) curve. Algorithm 3 Generating MRROC curves.
 4.3 Averaging Finally, a process based on the Jain and Ross X  X  method [ 18 ]is used to group the operating points already computed during the combination step. Given a  X  , the Algorithm 4 searches the pairs { TPR i ( X  ), FPR i ( X  ) } whereFPR i ( X  ) = 1  X   X  (recalling that  X  can be viewed as the TNR, and that FPR = 1  X  TNR). Then, the operating points corresponding to a same  X  are averaged and used to generate the averaged ROC curve. 4.4 Testing Duringoperations,  X  is usedtoretrievetheset of user-specific HMMs/thres-holds which will be applied on input samples. Figure 8 illustrates two possible situations linking the aver-aged ROC curve and a user-specific MRROC curve. In the Algorithm 4 Generating the averaged ROC curve.
 first case,  X  falls directly on HMM 7 . Thus, the user-specific threshold associated to this  X  will be used to classify the input samples. In the second case, the requested  X  is obtained by combining classifiers HMM 7 and HMM 9 . That is, each test sample must be randomly assigned to either HMM 7 or to HMM 9 , according to the probabilities given by Eqs. 1 and 2. Note that a test sample is not assigned to both classifiers at the same time. However, since a fusion strategy is incor-porated to the process, there are no restrictions to the use of combination in this level. 5 Simulation results Two types of codebook have been employed in off-line SV systems based on discrete HMMs: the universal codebook, shared by all writers in the population, and the user-specific codebook, adapted to a particular writer. However, as dis-cussed before, these codebooks have typically been con-structed by using the same data that are used to train the HMMs [ 8 , 28 ]. Instead, this section investigates the use of an independent SV database to generate universal and user-specific codebooks. In both cases, the impact of applying the multi-hypothesis approach to improve the system perfor-mance is analysed.

The Brazilian signature database [ 10 ] is used for proof-of-concept computer simulations. It contains 7,920 samples of signatures that were digitized as 8-bit greyscale images over 400  X  1,000pixels, at resolution of 300dpi. The signatures were provided by 168 writers and are organized as follows:  X  X B dev is composed by 4,320 genuine samples supplied  X  X B exp contains 60 writers, each one with 40 samples of
Given DB dev and DB exp , two experiments are performed: 1. Off-line SV based on an universal codebook. As pro-2. Off-line SV based on user-specific codebooks. By using
Therefore, while a DB dev is used to construct codebooks, as well as to develop SV systems used to evaluate these code-books, a DB exp is employed in training, testing, and valida-tion of the final SV system. As mentioned, the use of DB dev allows to construct codebooks earlier, with sufficient data, independently of the writers in DB exp .

The signature images are represented by means of density of pixels, extracted through the grid segmentation scheme described in Sect. 2 . By varying the number of clusters from 10 to 150, in steps of 5, 29 candidate codebooks are obtained; where each one is constructed by using the first 30 signatures (training set + validation set) of each writer in DB dev .
In the first experiment, in order to select the universal codebook, an averaged ROC curve is produced for each dif-ferent version of the evaluation system. Then, the codebook providing the averaged ROC curve with highest AUC is cho-sen. In the second experiment, the user-specific codebooks are selected before the averaging step; that is, by choosing the codebook providing the MRROC curve with highest AUC for each writer. Then, the individual MRROC curves are aver-aged. Since high values of FPR are rarely useful in real situa-tions, the AUC may be calculated regarding a specific region of interest of the ROC space.

Finally, in the first experiment, a ROC outer boundary is obtained for each writer by using the same universal code-book and a set of HMMs trained with a different number of states. In the second experiment, a ROC outer boundary is obtained for each writer by using his/her own codebook and a set of HMMs trained with a different number of states. Note that, in both experiments, the ROC curves are generated by using a validation set which contains only genuine samples and random forgeries. 5.1 Off-line SV based on an universal codebook In a first step, the multi-hypothesis approach (Algorithms 2  X  4 ) was applied for each one of the candidate codebooks by using the whole validation set. Among the 29 averaged ROC curves, the codebook of 35 clusters (CB 35 ) provided the highest AUC (i.e., 0.997). Figure 9 a shows the averaged ROC curve and AUC associated with CB 35 . Then, in or-der to confirm this result, 10 different averaged ROC curves were generated for each codebook by randomly selecting a subset of signatures in the validation set. Since there are much more random forgeries than genuine samples, only the forgery space was changed for each averaged ROC curve. Figure 9 b presents the relation between the number of clus-ters (NC) used in each codebook and the corresponding AUC, calculated on the entire averaged ROC space. This graphic confirms that the codebook with 35 clusters provides the highest AUCs for this population. Moreover, the Kruskal X  Wallis test [ 12 ] indicates that the AUC values corresponding to CB 35 are significantly different to the data regarding any other codebook in the graphic.
 The next step consisted in applying the codebook CB 35 to DB exp , which is composed of 60 writers. As expected, CB also performed well for this population, providing an AUC of 0.998 in the region of interest (i.e., between FPR=0 and FPR=0.1) of the averaged ROC space. The ROC curve of the multi-hypothesis system is indicated by the star-dashed line in Fig. 10 . Whereas the circle-dashed line represents the baseline or single-hypothesis system used for comparisons. In this system, only the HMM which performs the best in the cross-validation process [ 6 , 21 ] is considered for generat-ing a user-specific ROC curve. This means that all operating points of an individual ROC curve are associated with a same HMM. Moreover, the user-specific ROC curves are directly averaged, without repairing, by using the standard Ross X  X  method [ 18 ] (see the inner graphic in Fig. 10 ). As expected, the multi-hypothesis system provided a higher ROC curve, and, due the Scott X  X  method [ 29 ], a superior number of oper-ating points was obtained.

Depending on the operating point, a same writer could employ a different HMM, as shows Fig. 11 . It is worth noting that the complexity of the HMMs increases with the value of  X  ; which indicates that the operating points in the best region of theROCspace(i.e., theupper-left part) areachieved with a greater number of states. Figure 12 shows the user-specific MRROC curves of two writers. While writer 1 employs different HMMs in the MRROC space, writer 3 uses the same HMM in all operating points. The fact that writer 3 obtained AUC=1 by using an HMM with only 2 states (i.e., HMM 2 ) indicates that his/her corresponding genuine and forgery spaces in the validation set are easily separa-ble. In other words, high inter-class variability demands less complex models.

In the following phase, the operating points of the aver-aged ROC space (given by  X  ) were used to retrieve the user-specific HMMs/thresholds, and apply them to the test set. Table 1 presents the test set error rates for some  X  values in both baseline and proposed systems. In order to observe the impact on system performance at each step of the multi-hypothesis approach, results are first shown without the com-bination step (that is, by performing model selection fol-lowed by averaging), while the last results correspond to the whole multi-hypothesis approach. Since there are three types of forgeries in the test set, the average error rate is calculated as AER ( X  ) = ( FNR ( X  ) + FPR ( X  ) random Which is equivalent to consider equal a priori probabili-ties, that is, P ( genuine ) = P ( random ) = P ( simple ) = P ( skilled ) = 0 . 25, since the test set of each writer is com-posed of 10 samples per category of signature. (b)
In general, the multi-hypothesis system provided smaller error rates. Moreover, the FPR ( X  ) random areclosertothe expected error rates given by 1  X   X  (recalling that  X  can be viewed as the TNR, and that FPR = 1  X  TNR).
 In order to analyze the impact of repairing individual ROC curves, the proposed approach was applied only to the 20 writers having ROC curves with concavities. On aver-age, 75.91% of the problematic writers had their AERs on test enhanced with the multi-hypothesis system in the re-gion between  X  = 0 . 9 and  X  = 1; while 18.64% performed better with the single-hypothesis system. For the remaining 5.45%, both systems performed equally. Figure 13 presents the results for some  X  values, where the improvements ob-tained with the multi-hypothesis system are located in the positive side, that is, below the dotted line. With the single-hypothesis system, the writer indicated by the arrow in (a) had an AER of 22.5%. When using the multi-hypothesis sys-tem, the respective AER was reduced to 4.9%, that is, 17.6% lower.

Finally, the multi-hypothesis system required fewer HMM states than the single-hypothesis system. Figure 14 shows the number of HMM states used by each writer in the (b) (d) (a) single-hypothesis system. Note that these models, applied to all operating points, are more complex than those ones pre-viously shown in Fig. 11 . Regarding the entire ROC space, the proposed approach reduced by 48.09% the number of states employed by the HMMs. This occurs because during the model selection step (see Algorithm 2 ), when two or more HMMs achieve the same TPR, the model with fewer states is chosen. 5.2 Off-line SV based on user-specific codebooks This experiment explored the idea of using user-specific codebooks in order to reduce individual error rates. Since each writer must be evaluated separately, the selection of the best user-specific codebook was performed after the combination step, by choosing that one providing the MRROC curve with greatest AUC in the region of interest (i.e., between FPR=0 and FPR=0.1). Figure 15 a presents the codebook selected for each writer. For some writers, due the high variability inter-class on validation (i.e., genu-ine signatures vs. random forgeries), all codebooks provided AUC = 1. In these cases, the universal codebook CB 35 , found in the previous experiment, was used. Figure 15 b indi-cates that only 13 codebooks (out of 29) were selected by this experiment; where 58% of the writers used the universal codebook CB 35 , and 15% used a codebook with 150 clusters.
After selecting the best codebook per user, the user-specific MRROC curves were averaged by using Algorithm 4 . The dash-dot line in Fig. 16 shows the result-ing averaged ROC curve (AUC = 0 . 9989), which is bet-ter than the curve obtained with the previous version of (b) (d) multi-hypothesis system (i.e., with CB 35 ) in the region be-tween  X  = 0 . 995 and  X  = 1. This improvement was also observed on test, as shows Table 2 ; specially for  X  = 1. The results for  X  = 0 . 96 to 0 . 99 are not shown in Table 2 since both systems performed similarly for  X &lt; 0 . 995. Figure 17 presents the AERs of the 20 problematic writers obtained with the two multi-hypothesis systems. On average, 36.25% of thesewriters hadtheir AERs enhancedwiththeuseof user-specific codebooks. This represents 12.08% of the whole population, which may indicate a considerable amount of users in a real world application. Only 16.25% performed better with CB 35 ; and for the remaining 47.50%, both sys-tems performed equally.

Besides being used to retrieve the set of user-specific classifiers and thresholds, the operating points of the last multi-hypothesis system also stores information about what codebook to use; taking into account that a same codebook may be shared by different writers. In the single-hypothesis system, on the other hand, only the user-specific thresholds are stored, since each writer employs his/her single classifier in all operating points. It is worth noting that another varia-tion of the proposed system could be developed by finding the best codebook at each operating point. However, since the 13 codebooks selected by this experiment already pro-vided composite ROC curves with AUC equal or very close to 1, this approach was not investigated. 6 Conclusions In this article, an approach based on the combination of clas-sifiers in the ROC space was proposed in order to improve performance of off-line SV systems designed from limited and unbalanced data. By training an ensemble of HMMs with different number of states and different codebooks, and then selecting the best model(s) for each operating point, it is possible to construct a composite ROC curve that provides a more accurate estimation of system performance during training and significantly reduces the error rates during oper-ations.

ExperimentscarriedoutontheBrazilianSVdatabase[ 10 ], with random, simple, and skilled forgeries, show that the multi-hypothesis approach leads to a significant improve-ment in overall performance, besides reducing the number of HMM states by up to 48%. The results also show that the use of user-specific codebooks can improve class sep-arability. Moreover, the multi-hypothesis system with uni-versal codebook obtained a considerable reduction in the number of codebook clusters and in the error rates 5 when comparing with another off-line SV system developed with the same SV database [ 21 ]. Therefore, since ROC concavities are observed, the proposed approach is suitable to improve system performance. Of course, given a set of features able to provide adequate class separation, the multi-hypothesis approach would give no advantage.

The multi-hypothesis approach can be used for dynamic selection of the best classification model X  X hat is, the best codebook, HMM and threshold X  X ccording to the risk linked to an input sample. In banking applications, for example, the decision to use a specific operating point may be asso-ciated with the amount of the check. In the simplest case, for a user that rarely signs high value checks, big amounts would require operating points related to low FPRs, such as would be provided by a  X  close to 1; while lower amounts would require operating points related to low FNRs, since the user would not feel comfortable with frequent rejections.

The proposed approach may require greater computa-tional complexity (training time and memory consumption) than a single-hypothesis approach due to the generation of a set of candidate codebooks and HMMs. However, once the multi-hypothesis ROC space is obtained, all sub-optimal solutions can be discarded, and only the HMMs associated with useful operating points should be stored. During the test phase, no additional time is required, since the process consists in the use of one HMM at a time.

Finally, this strategy can be easily adapted to any neu-ral or statistical classifier designed to solve similar two-class problems. It is useful to note that the choice of classifiers should consider the number of training samples. In the cases where a validation set is not available, the ROC curves may be generated by using the training set.
 References
