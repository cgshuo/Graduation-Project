 data sources on deep web available at that time. And CompletePlanet[2] has collected more than 7,000 data sources and classified them into 42 topics. So we can conclude that deep web provides people a great opportunity to get their desired information. 
With proliferation of data sources on deep web, the importance of data integration is growing. These data sources contain a large number of high-quality structured data, through its query interface, and finally returns a response page containing some result widely discussed. structure, which don X  X  care result schema. And many methods are not able to process original extraction methods. Clearly, these issues bring a large number of difficulties to data extraction and new challenges to the data integration researchers. 
To resolve these problems, a complete and effective method supporting data extraction and schema recognition is proposed in this paper. To extract data, a novel algorithm based on clustering is adopted, which is also effective when faced complex data and noise. And a simple extraction rule model is defined to resolve the problem of maintenance. In addition, it does deep mining on result schema recognition. 
The paper is organized as follows. Section 2 presents the related work. In section 3 discussed. The experimental methods and results are reported in Section 6. Section 7 summarizes our contributions and concludes the paper. necessary schema information for data extraction is provided by experts too, such as SG  X  Wrap[3] and DEByE[4]. 
In recent years, more and more automatic methods have been proposed, which can automatically generate wrapper and extract data without human involvement. Several typical methods are MDR[5], MDRII[6], RoadRunner[7][8], EXALG[9], DeLa[10] and ViDRE[11]. 
MDR and MDRII are methods based on Tag Tree structure features, which mine can get very good results. But if the structure is complex, or there is excessive noise nested structure. approach is very novel, but needs an eff ective visualization model[12][13]. Therefore, addition, extraction fully based on visual information is not reliable. When pages have no apparent visual characteristics, the extraction will become very difficult. can identify optional attribute and nested attribute which may repeat many times in a RoadRunner, and has resolved the issue of time cost. But both of them try to generate problem of maintenance very good. and analyzes the semantic of data extracted by assigning a label to a column. But its schema information. simple extraction rule model is defined to resolve the problem of maintenance. (3) In addition, it does deep mining on result schema recognition. building HTML DOM tree model, identifying data region node, identifying data record, alignment and attribute separation. 
The data extraction flow in dashed rectangle is shown in Fig.1 , and the details are as follows. Phase 1: According to the sample page document, we firstly build Html DOM Tree Model, which describe the structure and visual characteristics of tags in the document. The whole data extraction process is based on this model. represents the location of records in the HTML DOM model. method, which will find the largest repetition of sub-node under the data region node. 
Phase 4: We align all of the nodes in a cluster to find a representative tree structure of the data record. labels from the noise. 3.1 HTML DOM Tree Model makes HTML tags representing a nested structure, and we can use a tree structure to Html DOM tree, as shown in Fig.2 . 
Nodes in DOM tree are divided into two kinds: tag node and text node. And the tag information and visual information, on which the system is based. 3.2 Identifying Data Region Node In response page, the region containing data records is called data region by us. And directly, we first identify data region node. 
According to the survey, Deep Web sites mostly focus on a specific domain, such as book, car, air, etc. After the user submits a query, the web site returns a response repetitions. data region. Obviously, both the external representation and the DOM tree of the three books are similar with each other. 
To judge whether a node is a data region node, we need to know whether there are repetitions under the node in the DOM model. We must note that, the repetition may contain one node or several nodes, and repetitions are not always adjacent with each other for there may be noise nodes between them.  X  2 will appears multiple times. We use a clustering-based algorithm to identify data region nodes. the structure of the nodes belong to the same cluster are similar with each other. Then we repeat the process to its child nodes. 
In the clustering process, we use edit distance between trees rooted by the nodes to create a new cluster. 
Finally, we define the only node in a cluster as isolated node. If the proportion of data region node. 3.3 Identifying Data Record When a node is recognized as a data region node, we try to find repetitions under it to identify which nodes are used to describe the data records. As a response page usually there are repetitions containing one node or several nodes in a data region. same attributes, because some of them are necessary attributes and some are optional structure with each other to some degree. To completely describe the record structure, we try to find the largest repetition which contains as much attributes as possible. Before combining clusters, we define the following concepts: 
The distance from node A to node B: the number of node A minus the number of node B. 
The distance from node A to cluster C: the number of node A minus the number of node B, which is the nearest node in A X  X  left belonging to C. to B. A is parallel to B. according their position. belongs. Supposing that data region node N contains child nodes 123456789 and the numbers denote their positions, we obtain CS={A={1,5,9}B={2,6}C={3,7}D={4,8}} after clustering. So the sequence 123456789 transforms to ABCDABCDA. 
Then we discover that B is parallel to A, as well as C and D. So they are able to be combined. Because the distances from BCD to A are 1, 2 and 3, the largest repetition is ABCD. ABCD probably describes a data record. 3.4 Alignment repetition from the clustering model into a DOM Tree model, which is called Pattern ensure that the tree rooted by any other node in the cluster is a sub-tree of it. and obtain a complete Pattern Tree. 3.5 Attribute Separation different. Therefore, if we compare the text in DOM tree of each record, and find out from the attribute value. After analyzing a HTML document, we define a model that contains a set of extraction rules for it to improve extraction efficiency. 4.1 Extraction Rule Model 
P can be represented as the regular expression: ( /tagname ( [tagnum] )? )+, where tagname denotes the name of the tag and tagnum denotes the number of tag under its the tag path  X /html/body/table/tbody/tr[1]/td[2] X , because  X  X ody X  is the only one body  X  X body X , and X  td[2] X  is the second td child of tag  X  X r[1] X . After alignment and attribute separation we obtain a Pattern Tree T that indicate a statistical information of each attribute. 4.2 Extraction Rule Integration some of the rules conflict. Thus, extraction rule integration is discussed. eliminated. frequency of an attribute is too low, it will be eliminated. We have obtained the extraction rules of each data source. But the data extracted has only structure but no semantic. Therefore, we also need to process the data further to understand the meaning of attributes. We assign labels to data to obtain result schema. Label Assignment consists of Local Label Assignment and Global Label Assignment. 5.1 Instance Lib An instance lib for each data source constructed is shown in Table 1 , a row denotes a semantic information. 5.2 Local Label Assignment following five heuristics. query interface, a query condition is usually consisted of attribute name and keyword. interface. After data extraction, we obtain three records as shown in Table 1. Obviously, all the Column A. In other words, we assign a label  X  X itle X  to column A. headers.
 Heuristic 3: search for labels in result data. In response pages, sometimes attribute values are encoded together with attribute names. As shown in Fig.8 ,  X  X uy new X  and prefix and suffix shared by most of the records to obtain a label.
 web site, many tags contain semantic information in record fragment of HTML document, such as tag name and CSS. They also can be used as labels. Fig.9 shows an example. Heuristic 5: some data has conventional format . Some data has conventional format. For example, a date is usually organized as  X  X d-mm-yy X  and  X  X d/mm/yy X . An email usually contain @. 5.3 Global Label Assignment In a domain, every data source shares not only a similar schema but also similar data Label Assignment that searches for labels in other data sources, as shown in Fig.10 . source A. We try to send a query with keyword  X  X ata mining X  to source B. If source B above. Finally labels from source B is assigned to the attribute of source A. 
As there are many the label sources, we a ssign a Label Set to each column instead of only one label. 5.4 Schema Dictionary the labels in schema dictionary, we could obtain the schema information. directory for deep web. During our selection, duplicates under different topics are not response pages containing at least two data records are used. ViDRE, MDRII and our approach DE. Then we compare the results with the correct approach, as shown in Table 3 . Secondly, we experiment on our schema recognition approach and compare it with DeLa. There are two phases in our approach, Local Label Assignment(LLA) and Global Label Assignment(GLA). We compute precision and recall in each phase, and the result are shown in Table 4 . 
Obviously, our method is much better in every domain. In this paper, we propose a full solution for Data Extraction and Schema Recognition for our future works. 
