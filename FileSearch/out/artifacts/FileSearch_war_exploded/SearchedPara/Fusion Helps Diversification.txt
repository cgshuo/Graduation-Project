 A popular strategy for search result diversification is to first retrieve a set of documents utilizing a standard retrieval method and then rerank the results. We adopt a different perspective on the problem, based on data fusion. Starting from the hypothesis that data fusion can improve performance in terms of diversity metrics, we examine the impact of standard data fusion methods on result diversification. We take the output of a set of rankers, optimized for diversity or not, and find that data fusion can significantly improve state-of-the art diversification methods. We also introduce a new data fusion method, called diversified data fusion, which infers latent topics of a query using topic modeling, without leveraging outside informa-tion. Our experiments show that data fusion methods can enhance the performance of diversification and DDF significantly outper-forms existing data fusion methods in terms of diversity metrics. H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval X  retrieval models Data fusion; rank aggregation; diversification; ad hoc retrieval
Search result diversification is widely being studied as a way of tackling query ambiguity. Instead of trying to identify the  X  X orrect X  interpretation behind a query, the idea is to make the search results diversified so that users with different backgrounds will find at least one of these results to be relevant to their information need [2]. In contrast to the traditional assumption of independent document relevance, search result diversification approaches typically con-sider the relevance of a document in light of other retrieved docu-ments [40]. Diversification models try to identify the probable  X  X s-pects X  of the query and return documents for each aspect, thereby making the result list more diverse.

Data fusion approaches, also called rank aggregation approaches, consist in combining result lists in order to produce a new and hope-fully better ranking [16, 42]. Here, results lists can be produced by a wide range of ranking approaches, based, e.g., on different query or document representations. Data fusion methods can improve re-trieval performance in terms of traditional relevance-oriented met-rics like MAP and precision@k over the methods used to generate the individual result lists being fused [17, 26, 27, 49]. One reason is that retrieval approaches often return very different non-relevant documents, but many of the same relevant documents [49].
We examine the hypothesis that data fusion can improve per-formance in terms of diversity metrics by promoting aspects that are found in disparate ranked lists to the top of the fused list . Our first step in testing this hypothesis is to examine the impact of ex-isting data fusion methods in terms of diversity scores when fusing ranked lists. We find that they tend to improve over individual com-ponent runs on nearly all of the diversity metrics that we consider: Prec-IA, MAP-IA,  X  -NDCG, ERR-IA (all at rank 20).

Building on these findings we propose a new data fusion method, called diversified data fusion (DDF). Based on latent Dirichlet al-location (LDA), it operates on documents in the result lists to be fused, whether the result lists have been diversified or not. DDF infers latent topics, their probabilities of being relevant and a multi-nomial distribution of topics over the documents being fused. Thus, it integrates topic structure and rank information. DDF does not as-sume the explicit availability of query aspects, but infers these as well as the latent prior for a given query via the documents be-ing fused. Experimental results show that DDF can aggregate re-sult lists X  X hether produced by diversification or ad hoc retrieval models X  X nd boost the diversity of the final fused list, outperform-ing state-of-the-art diversification methods and established data fu-sion methods, especially in terms of intent-aware precision metrics.
Our contributions in this paper can be summarized as follows: ii. We propose a novel data fusion method that aims at optimiz-iii. We analyze the effectiveness of data fusion for result diver- X 2 discusses related work.  X 3 describes the fusion models that we use (old and new).  X 4 describes our experimental setup.  X 5 is de-voted to our experimental results and we conclude in  X 6.
We distinguish between three directions of related work: search result diversification, data fusion, and latent topic modeling.
Search result diversification is similar to ad hoc search, but dif-fers in its judging criteria and evaluation measures [8, 12]. The basic premise in search result diversification is that the relevance of a set of documents depends not only on the individual relevance of its members, but also on how they relate to one another [2]. Ide-ally, users can find at least one relevant document to the underlying information need. Most previous work on search result diversifica-tion can be classified as either implicit or explicit [39, 41].
Implicit approaches to result diversification promote diversity by selecting a document that differs from the documents appearing before it in terms of vocabulary, as captured by a notion of docu-ment similarity, such as cosine similarity or Kullback-Leibler diver-gence. Carbonell and Goldstein [6] propose the maximal marginal relevance (MMR) method, which reduces redundancy while main-taining query relevance when selecting a document. Chen and Karger [7] describe a retrieval method incorporating negative feed-back in which documents are assumed to be non-relevant once they are included in the result list, with the goal of maximizing diversity. Zhai et al. [51] present a subtopic retrieval model where the utility of a document in a ranking is dependent on other documents in the ranking and documents that cover many different subtopics of a query topic are found. Other implicit work includes, e.g., [1] where set-based recommendation of diverse articles is proposed. We also tackle the problem of search result diversification implicitly, but in a different way, i.e., by data fusion.

Explicit approaches to diversification assume that a set of query aspects is available and return documents for each of them. Past work has shown that explicit approaches are usually somewhat su-perior to implicit diversification techniques. Well-known exam-ples include xQuAD [39], RxQuAD [45], IA-select [2], PM-2 [13], and, more recently, DSPApprox [14]. Instead of modeling a set of aspects implicitly, these algorithms obtain the set of aspects ei-ther manually, e.g., from aspect descriptions [8, 12], or they create them directly from, e.g., suggested queries generated by commer-cial search engines [13, 39] or predefined aspect categories [44]. We propose an implicit fusion-based diversification model where we do not assume that the aspects of the query are available but do assume that we can infer the underlying topics and the prior rele-vance of each topic for search result diversification.
A core concern in data fusion is how to assign a score to a doc-ument that appears in one of the lists to be fused [17, 19, 42, 49]. Most previous work on data fusion focuses on optimizing a tradi-tional evaluation metric, like MAP, p@k and nDCG. Fusion ap-proaches can be categorized into supervised or unsupervised: Su-pervised data fusion approaches, like  X  -Merge [43], first extract a number of features, either from documents or lists, and then utilize a machine learning algorithm to train the fusion model [15, 17, 49].
In contrast, unsupervised data fusion methods mainly use either retrieval scores or ranks of documents in the lists to be merged, with the CombSUM family of fusion methods being the oldest and one of the most successful ones in many information retrieval tasks [26, 42]. State-of-the-art data fusion methods ClustFuseCombSUM and ClustFuseCombMNZ (both cluster-based methods) are proposed in [23]. Methods utilizing retrieval scores take score information from the lists to be fused as input, while those utilizing rank infor-mation only use order information of the documents appearing in the lists to be fused as input. Data fusion methods utilizing rank in-formation have many uses and applications in information retrieval, including, e.g., expert search [30, 35], query reformulations [43], meta-search [4, 17] and microblog search [31, 32].

We do not make the assumption that labeled data is available but integrate standard unsupervised data fusion information into our diversified fusion model for search result diversification via a latent topic model.
Topic models have been proposed for reducing the high dimen-sionality of words appearing in documents into low-dimensional  X  X atent topics. X  From the first work on topic models [21], the Prob-ablistic LSI model, topic models have received significant attention [5, 18, 22] and have proved to be effective in many information retrieval tasks [24, 47, 50]. Latent dirichlet allocation (LDA) [5] represents each document as a finite mixture over  X  X atent X  topics where each topic is represented as a finite mixture over words exist-ing in that document. Based on LDA, many extensions have been proposed, e.g., to handle users X  connections with particular doc-uments and topics [37], to learn relations among different topics [25, 29], for topic over time [46], for dynamic mixture model [48], or tweet summarization [36]. LDA has also been extended to sen-timent analysis [28]. We propose a novel topic model where fusion scores of each document appearing in lists to be fused are used to boost the performance of state-of-the-art diversification methods. Our work adds the following to the work discussed above. We propose a fusion-based approach to the search result diversification task. We find that existing unsupervised fusion methods signifi-cantly outperform state-of-the-art diversification methods. In ad-dition, we propose a novel fusion method, diversified data fusion, that uses the output of a fusion step and a topic modeling step as input to a diversification step. To the best of our knowledge, ours is the first attempt to utilize data fusion for diversification.
We first review our notation and terminology. Then we introduce the task to be addressed, as well as the baseline fusion methods that we use in this paper plus a new fusion method.
We summarize the main notation used in this paper in Table 1. In the remainder, we distinguish between queries, aspects and topics. A query is an expression of an information need; in our experi-mental evaluation below, queries are provided as part of a TREC test collection. An aspect (sometimes called subtopic at the TREC Web track) is an interpretation of an information need. We use topic to refer to latent topics as identified by a topic modeling method, in our case LDA. A component list is a ranked list that serves as input for a data fusion method. A fused list is a list that is the result of applying a fusion method to component lists.
The diversified data fusion task that we address is this: given a query, an index of documents, and a set of ranked lists of docu-ments produced in response to a query, aggregate the lists into a final result list where documents should be diversified. The com-ponent lists may or may not have been diversified themselves or ranked by relevance only.

The underlying data fusion problem consists of running a rank-ing function F X that satisfies: where L is a set of components lists, m = | L | is their number, C the document corpus, q a query, and L f the final fused list.
Let R L i d denote the score of document d based on the rank of d in list L i ; in the literature on data fusion, one often finds R L i d = 0 if d /  X  L i ( d still in the combined set of documents C
L := S m i =1 L i ). In both CombSUM and CombMNZ, R L i often defined as: where | L i | is the length of L i and rank( d,L i )  X  { 1 , . . . , | L the rank of d in L i . The well-known CombSUM fusion method [17, 49], for instance, scores d by the sum of its rank scores in the lists: while CombMNZ [17, 49] rewards d that ranks high in many lists: where |{ L i : d  X  L i }| is the number of lists in which d appears.
We consider CombSUM, CombMNZ and two state-of-the-art data fusion methods, ClustFuseCombSUM and ClustFuseComb-MNZ [23], that integrate cluster information into CombSUM and CombMNZ, respectively, as baseline fusion methods.

In addition, a natural and direct way of diversifying a result list in the setting of data fusion is this: first rank the documents in the component lists by their estimated relevance to the query through a standard data fusion method, such as CombSUM, and then di-versify the ranking through effective search result diversification models, such as MMR [6] and PM-2 [13]. In our experiments, we implement two more baselines, called CombSUMMMR and CombSUMPM-2. They first use CombSUM to obtain a fused list and then use MMR and PM-2, respectively, to diversify the list.
We propose a diversified data fusion (DDF) method that not only inherits the merits of traditional data fusion methods, i.e., it can improve the performance on relevance orientated metrics, but also considers a query as a compound rather than a single representation of an underlying information need, and regards documents appear-ing in the component lists as mixtures of latent topics.
DDF consists of three main parts: (I) perform standard data fu-sion; (II) infer latent topics; (III) perform diversification; see Algo-rithm 1. In the first part ( X  X art I X  in Algorithm 1), DDF computes the fusion scores of the documents in the component lists based on an existing unsupervised data fusion method (steps 1 and 2 in Al-gorithm 1); in this paper we use CombSUM, as our experimental results in  X 5.1 and  X 5.2 show that CombSUM outperforms other plain fusion methods in most cases. In the second part ( X  X art II X  in Algorithm 1), DDF integrates fusion scores into an LDA topic model such that latent topics of the documents, their corresponding estimated relevance scores, and the multinomial distribution of the topics specific to each document can be inferred (steps 3 X 15 in Al-gorithm 1). In the last part ( X  X art III X  in Algorithm 1), DDF uses the outputs of Parts I and II as input for an existing diversification method; in this paper, we use PM-2 [13] because it is a the state-of-the-art search result diversification model. Some concepts in PM-2, such as  X  X uotient X  and  X  X eat, X  play important roles in the definition of the diversification step; they will be discussed in  X 3.4.3.
Below we describe how to infer latent topics ( X  X art II X  in Al-gorithm 1) in  X 3.4.2 and how we utilize the information generated from latent topics and fusion scores ( X  X art III X ) in  X 3.4.3.
Previous work on search result diversification shows that explic-itly computing the probabilities of aspects of a query can improve diversification performance [1, 20, 39]. We do not assume that as-pect information is explicitly available; we infer latent topics and their probabilities of being relevant using topic modeling.
Topic discovery in DDF is influenced not only by token co-oc-currences, but also by the fusion scores of documents in the com-ponent lists. To avoid normalization and because fusion scores of the documents theoretically belong to (0 , +  X  ) , we employ a log-normal distribution for fusion scores to infer latent topics of the query via the documents and their relevance probabilities.
The latent topic model used in DDF is a generative model of relevance and the tokens in the documents that appear in the com-ponent individual lists. The generative process used in Gibbs sam-pling [34] for parameter estimation, is as follows: ii. For each document d  X  C L , draw a multinomial  X  d from a Fig. 1 shows a graphical representation of our model. In the gen-erative process, the fusion scores of tokens observed in the same document are the same and computed by a data fusion method, like Algorithm 1: Diversified data fusion CombSUM, for the document, although a fusion score is generated for each token from the log-normal distribution. We use a fixed number of latent topics, T , although a non-parametric Bayes ver-sion of DDF that automatically integrates over the number of top-ics would certainly be possible. The posterior distribution of topics depends on the information from two modalities X  X oth tokens and the fusion scores of the documents.

Inference is intractable in this model. Following [18, 24, 34, 36, 46, 47, 50], we employ Gibbs sampling to perform approximate in-ference. We adopt a conjugate prior (Dirichlet) for the multinomial distributions, and thus we can easily integrate out  X  and  X  , analyt-ically capturing the uncertainty associated with them. In this way we facilitate the sampling, i.e., we need not sample  X  and  X  at all. Because we use the continuous log-normal distribution rather than discretizing fusion scores, sparsity is not a big concern in fitting the model. For simplicity and speed we estimate these log-normal distributions  X  and  X  by the method of moments, once per iteration of Gibbs sampling (see the Appendix). We find that the sensitivity of the hyper-parameters  X  and  X  is limited. Thus, for simplicity, we use fixed symmetric Dirichlet distributions (  X  = 50 /T and  X  = 0 . 1 ) in all our experiments.

In the Gibbs sampling procedure above, we need to calculate the conditional distribution P ( z di | w , r , z  X  di , X , X , X , X , L ,q ) (step 9 in Algorithm 1), where z  X  di represents the topic assignments for all tokens except w di . We begin with the joint probability of doc-uments to be fused, and using the chain rule, we can obtain the conditional probability conveniently as
P ( z di | w , r , z  X  di , X , X , X , X , L ,q )  X  where n zv is the total number of tokens v that are assigned to topic z , m dz represents the number of tokens in document d that are assigned to topic z . An overview of the Gibbs sampling procedure we use is shown from step 3 to step 12 in Algorithm 1; details are provided in the Appendix.

One merit of our generative model for DDF is that we can predict a fusion score for any document once the tokens in the document have been observed. Given a document, we predict its fusion score by choosing the discretized fusion score that maximizes the poste-rior which is calculated by multiplying the fusion score probability of all tokens from their corresponding topic-wise log-normal dis-tributions, i.e., arg max f Q N d i =1 p ( f |  X  z i , X  z
More importantly, after the Gibbs sampling procedure, we can easily infer the multinomial distribution of topics specific to each document d  X  X  L as (step 13 in Algorithm 1): where n d,z is the number of tokens assigned to latent topic z in document d ; we can also conveniently estimate the probability of a topic being relevant to the query, denoted as v z | q , by (step 15 in Algorithm 1): where E denotes the expectation.
In Part III of our DDF model we propose a modification of PM-2. Before we discuss the details of this modification, we briefly describe PM-2. PM-2 is a probabilistic adaptation of the Sainte-Lagu X  method for assigning seats (positions in the ranked list) to members of competing political parties (aspects) such that the num-ber of seats for each party is proportional to the votes (aspect pop-ularity, also called aspect probabilities, i.e., p ( z | q ) ) they receive. PM-2 starts with a ranked list L f with k empty seats. For each of these seats, it computes the quotient qt [ z | q ] for each topic z given q following the Sainte-Lagu X  formula: where v z | q is the probability of topic z given q , i.e., the weight of topic z . According to the Sainte-Lagu X  method, this seat should be awarded to the topic with the largest quotient in order to best maintain the proportionality of the list. Therefore, PM-2 assigns the current seat to the topic z  X  with the largest quotient. The document to fill this seat is the one that is not only relevant to z topics as well: where P ( d | z,q ) is the probability of d talking about topic z for a given q . After the document d  X  is selected, PM-2 increases the  X  X ortion X  of seats occupied by each of the topics z by its normal-ized relevance to d  X  : This process repeats until we get k documents for L f or we are out of candidate documents. The order in which a document is appended to L f determines its ranking.

We face two challenges in PM-2: it is non-trivial to get the as-and it is non-trivial to compute p ( d | z,q ) , which usually requires explicit access to additional information. To address the first chal-lenge, we compute v z | q by (3), such that (4) can be modified as: qt [ z | q ] = p ( z | q ) For the second challenge, instead of computing P ( d | z,q ) explic-itly, we modify P ( d | z,q ) and apply Bayes X  Theorem so that Then we integrate the fused score generated by CombSUM into our model, i.e., we set in (6). As a result, after applying (6) to (5), DDF selects a candidate document by: d  X  = arg max where p ( z | d ; q ) is the probability of document d belonging to topic z , which can easily be inferred in our DDF model by (2) (i.e., p ( z | d,q ) =  X  d,z ). Therefore, after applying (2) and (3), (7) can be rewritten as: where it should be noted that we ignore the constant term as it has no impact on selecting the candidate document d
In this section, we describe our experimental setup;  X 4.1 lists our research questions;  X 4.2 describes our data set;  X 4.3 lists the met-rics and the baselines;  X 4.4 details the settings of the experiments. The research questions guiding the remainder of the paper are: RQ1 Do fusion methods help improve state-of-the-art search di-RQ2 What is the effect on the diversification performance of DDF RQ3 Does DDF outperform the best diversification and fusion meth-RQ4 How do the rankings of DDF differ from those produced by RQ5 What is the effect on the diversification performance of DDF
In order to answer our research questions we work with the runs submitted to the TREC 2009, 2010, 2011 and 2012 Web tracks, and the billion-page ClueWeb09 collection. 1 There are two tasks in these tracks: an ad hoc search task and a search result diversifi-cation task [8, 10 X 12]. We only focus on the diversification task, where the top-k documents returned should not only be relevant but also cover as many aspects as possible in response to a given query. In total, we have 200 ambiguous queries from the four years, with 2 queries (#95 and #100 in the 2010 edition) not having relevant doc-uments. Typically, each query has 2 to 5 aspects, and some relevant documents are relevant to more than 2 aspects of the query.
Many of the runs submitted to these four years of the Web track for the diversification task were generated by state-of-the-art diver-sification methods. In total, we have 119, 88, 62 and 48 runs from the 2009, 2010, 2011 and 2012 editions, respectively. 2
We evaluate our component runs and fused runs using several standard metrics that are official evaluation metrics in the diver-sification tasks at TREC Web tracks [8, 10 X 12] and are widely used in the literature on search result diversification [2, 3, 13, 14, 38, 40]: Prec-IA@ k [2], MAP-IA@ k [2], ERR-IA@ k [2] and  X  -nDCG@ k [9]. The former two are set-based and indicate, respec-tively, the precision and mean average precision across all aspects of the query in the search results, whereas the remaining ones are cascade measures that penalize redundancy at each position in the ranked list based on how much of that information the user has already seen from documents at earlier ranks. Data/clueweb09 .
We follow published work on search result diversification and mainly compute the metric scores at depth 20. Statistical signifi-cance of observed differences between the performance of two runs is tested using a two-tailed paired t-test and is denoted using ) for significant differences for  X  = . 01 , or M (and O ) for  X  = . 05 .
When assessing a fusion method X we will prefer fusion meth-ods that are safe, where we say that X is safe for metric M if ap-plying X to a set of component runs always yields a fused run that scores at least as high as the highest scoring component run in the set (according to M ).
 We consider several baselines. Two standard fusion methods [26], CombSUM and CombMNZ; two state-of-the-art fusion methods [23], ClustFuseCombSUM and ClustFuseCombMNZ; each year X  X  best performing runs in the diversification tasks at the TREC Web track [8, 10 X 12], and state-of-the-art plain diversification meth-ods, xQuAD [39] and PM-2 [13]. As DDF builds on both fusion and diversification methods, we also consider two fusion methods, CombSUMMMR and CombSUMPM-2, that integrate plain diver-sification methods MMR [6] and PM-2 into CombSUM for diver-sification, respectively.
We report on five main experiments aimed at answering the re-search questions listed in  X 4.1. In our first experiment, aimed at de-termining whether fusion methods help diversification, we fuse the five top performing diversification result lists from the TREC Web 2009, 2010, 2011 and 2012 submitted runs (some lists are gener-ated by the implementation of PM-2) by our baselines, viz., Comb-SUM, CombMNZ, ClustFuseCombSUM, ClustFuseCombMNZ, CombSUMMMR and CombSUMPM-2 (see  X 4.3). The perfor-mance of the baselines is compared against that of DDF.

Our second experiment is aimed at understanding the effect on the diversification performance of DDF and fusion methods of the number of component lists; we randomly sample k  X  { 2 , 4 , . . . , 26 } component runs from the submitted runs in the TREC Web 2012 track and fuse them. We repeat the experiments 20 times and report the average results and the standard deviations. We also show one sample X  X  result when fusing 4 runs.

Next, in order to understand how DDF outperforms the best com-ponent run and the fusion methods per query, our third experiment provides a query-level analysis. Our fourth experiment is aimed at understanding how the runs generated by DDF differ from those produced by other fusion methods; we zoom in on the differences between DDF and the next best performing fusion method, Comb-SUMPM-2, in terms of the documents (and aspects) retrieved by one, but not the other, or by both.

Finally, to understand the influence of the number of latent topics used in DDF, we vary the number of latent topics and assess the performance of DDF. We also use an oracle variant of DDF, called DDF2, where for every test query we consider as many latent topics as there are aspects according to the ground truth. The number of topics used in DDF is set to 10, unless stated otherwise.
In  X 5.1 we examine the performance of baseline fusion methods on the diversification task, which we follow with a section on the performance of DDF in  X 5.2.  X 5.3 details the effect of the number of lists;  X 5.4 provides a query-level analysis;  X 5.5 zooms in on the effect on ranking of DDF compared to the next best fusion method;  X 5.6 examines the effect of the number of latent topics on DDF.
In Table 2 we list the diversity scores of the baseline fusion methods on the diversity task: CombSUM, CombMNZ, ClustFuse-CombSUM, ClustFuseCombMNZ, CombSUMMMR, CombSUM-PM-2, with the 5 best performing component lists from the TREC Web 2009, 2010, 2011 and 2012 tracks, respectively. 3 For all met-rics and in all years, almost all baseline fusion methods outperform the state-of-the-art diversification methods, and in many cases sig-nificantly so. Note, however, that none of the baseline methods is safe in the sense defined in  X 4.3. Additionally, Table 3 shows the diversity scores of the baseline fusion methods when we fuse 4 randomly sampled runs from the 2012 data set, which confirms that fusion does help diversification.
Inspired by the success of baseline fusion methods on the diversi-fication task, we now consider our newly proposed fusion method, DDF. Returning to Tables 2 and 3, two types of conclusion emerge. First, DDF outperforms all component runs (note that component runs in Table 2 are the best runs in the tracks), on all metrics, for all years. In other words, it is safe in the sense defined in Section 4.3. The difference between DDF and the best performing component run is always significant. We believe that the strong performance of DDF is due to the fact that DDF not only focuses on improving the relevance score of fused run but also explicitly tries to diversify the fused run.

Second, DDF outperforms all baseline fusion methods, on all metrics. In many cases, CombSUMPM-2 and CombSUM yield the second and third best performance, respectively, but DDF out-performs them in every case, and often significantly so. DDF can beat CombSUMPM-2 as it tackles two main challenges in PM-2 (see  X 3.4.3), although they build on the same framework. Comb-SUMMMR follows a similar strategy as DDF but its performance is worse than that of DDF. This is due to the fact that MMR models documents as if they are centered around a single topic only. It is clear from Tables 2 and 3 that cluster-based data fusion methods (ClustFuseCombSUM, ClustFuseCombMNZ) sometimes perform a little worse than the standard fusion method they build on (Comb-SUM, CombMNZ). This is because cluster-based fusion focuses on relevance of the documents rather than on diversification.
Next, we zoom in on DDF. In particular, we explore the effect of varying the number of lists to be fused on its performance. Fig. 2 shows the fusion results of randomly sampling k  X  { 2 , 4 ,..., 26 } lists from the 48 runs submitted to the TREC Web 2012 track plus the PM-2 runs (due to space limitations, we only report results us-ing the 2012 runs; the findings on other years are qualitatively sim-ilar). For each k , we repeat the experiment 20 times and report on the average scores and the corresponding standard deviations indi-cated by the error bars in the figure. We use CombSUM as a repre-sentative example for comparison with DDF, as the results of other baseline fusion methods are worse or have qualitatively similar re-sults to those of CombSUM. As shown in Fig. 2, DDF always out-performs CombSUM in terms of the Prec-IA,  X  -nDCG and ERR-IA evaluation metrics and the performance gaps remain almost un-changed, in absolute terms, no matter how many component lists are fused. One reason for this is that as DDF builds on CombSUM, it inherits the merits of the fusion method, and more importantly, at the same time it tries to infer latent topics and rerank the high mation from the ground truth in the PM-2 model and the run  X  X M-2 (engine) X  is produced using information from a commercial search engine. The run  X  X QuAD (uogTr X ) X  is a uogTr X TREC edition run generated using the xQuAD algorithm; see [33]. Table 2: Performance obtained using the 2009 X 2012 editions of the TREC Web tracks. The best performing run per met-ric per year is in boldface. Statistically significant differences between fusion method and the best component run, between DDF and CombSUM, and between DDF and CombSUMPM-2, are marked in the upper right hand corner of the fusion method score, in the upper left hand corner of DDF X  X  score, and in the lower left hand corner of DDF X  X  score, respectively.
 ranked documents in terms of novelty of the documents. For the MAP-IA metric, however, the gaps increase with more component lists being fused. The performance of both DDF and CombSUM increases faster when the number of component lists increases but is  X  10 than when the number of component lists is &gt; 10 , for all the metrics. This seems to be inherent to the underlying CombSUM method and is due to the fact that with smaller numbers of compo-nent lists, there is simply more space available at depth 20 to obtain improvements than with larger numbers of component lists. Table 3: Performance obtained using the 2012 editions of the TREC Web track. The best performing run per metric is in boldface. Other notational conventions as in Table 2.

We take a closer look at per test query improvements of DDF over the best baseline fusion run when fusing the best 5 runs in 2012, viz., CombSUMPM-2, which outperforms the best compo-nent list. Fig. 3 shows the per query performance differences in terms of Prec-IA, MAP-IA,  X  -nDCG and ERR-IA, respectively, of DDF against CombSUMPM-2. DDF achieves performance im-provements for many queries when compared against CombSUM-PM-2, although the differences are sometimes relatively small. In a very small number of cases, DDF performs poorer than CombSUMPM-2. This appears to be due to the fact that DDF  X  X ver-diversifies X  documents in runs produced by CombSUM that have very few relevant document to start with, so that DDF ends up promoting different but non-relevant documents.
Next, we zoom in on one of the metrics that shows the biggest relative differences between DDF and the next best performing fu-sion method, Prec-IA, so as to understand how the runs generated by DDF differ from those by other fusion-based methods. Here, again, we use CombSUMPM-2 as a representative, as it tends to outperform or equal the other fusion methods. Specifically, we re-port changes in the number of relevant documents for DDF against CombSUMPM-2 when fusing the 2012 runs in Table 2 in 2012; see Fig. 4. Red bars indicate the number of relevant documents that appear in the run of DDF but not the run of CombSUMPM-2, white bars indicate the number of relevant documents in both runs, whereas blue bars indicate the number of relevant documents that appear not in DDF but in CombSUMPM-2; topics are ordered first by the size of the red bar, then the size of the white bar, and finally the size of the blue bar.

Clearly, the differences between DDF and CombSUMPM-2 in the top 5 and 10 are more limited than the differences in the top-15 and 20, but in all cases DDF outperforms CombSUMPM-2. E.g., in total there are 45 more relevant documents in the top 20 of the run produced by DDF than those in the CombSUMPM-2 run (49 relevant documents in DDF but not in CombSUMPM-2, 4 relevant documents in CombSUMPM-2 but not in DDF). We examine the matter further by comparing the Prec-AI@5, 10, 15, 20 scores of the DDF and CombSUMPM-2 runs for the 2012 data; see Table 4. The differences at small depths (5, 10) are weakly statistically sig-nificant while those at bigger depths are significant, confirming our observations in Fig. 4; we also find that DDF statistically signifi-cantly outperforms CombSUMPM-2 in terms of Prec-IA scores at depth 5, 10, 15 and 20, which again confirms the above observa-tions based on Fig. 4. Table 4: Prec-IA@5, 10, 15, 20 performance comparison be-tween CombSUMPM-2 and DDF. A statistically significant dif-ference between DDF and CombSUMPM-2 is marked in the upper left hand corner of the DDF score.

Finally, we examine the effect on the overall performance of the number of latent topics used in DDF, and contrast the performance of DDF with varying number of latent topics against DDF2, Comb-SUM and CombSUMPM-2. Here, DDF2 is the same algorithm as DDF except that for every test query it considers as many latent topics as there are aspects according to the ground truth. We use DDF2, DDF, CombSUM and CombSUMPM-2 to fuse the compo-nent result runs listed in Table 2 in 2012 as an example. We vary the number of latent topics in DDF from 2 to 16. See Fig. 5.
When the number of latent topics used in DDF increases from 2 to 6, the performance of DDF increases dramatically. When only 2 latent topics are used, the performance is worse than that of Comb-SUM and CombSUMPM-2; e.g., Prec-IA@20 for DDF is 0.3404, while the scores of CombSUM and CombSUMPM-2 are 0.3592 and 0.3718, respectively. In contrast, when the number of latent topics varies between 8 to 16, the performance of DDF seems to level off. This demonstrates another merit of our fusion model, DDF: it is robust and not sensitive to the number of latent topics once the number of latent topics is  X  X arge enough. X  Another impor-tant finding from Fig. 5 is that DDF2 always enhances the perfor-mance of DDF, CombSUM and CombSUMPM-2, for all metrics, which demonstrates the fact that latent topics can enhance the per-formance. The performance differences between DDF2 and DDF are quite marginal and not statistically significant. We leave it as future work to dynamically estimate the number of aspects (and la-tent topics) of an incoming query and to use this estimate in DDF.
Most previous work on search result diversification focuses on the content of the documents returned by an ad hoc algorithm to diversify the results implicitly or explicitly, i.e., using implicit or explicit representations of aspects. In this paper we have adopted a different perspective on the search result diversification prob-lem, based on data fusion. We proposed to use traditional un-supervised and state-of-the-art data fusion methods, CombSUM, CombMNZ, ClustFuseCombSUM, ClustFuseCombMNZ, Comb-SUMMMR and CombSUMPM-2 to diversify result lists. This led to the insight that fusion does aid diversification. We also pro-posed a fusion-based diversification method, DDF, which infers latent topics from ranked lists of documents produced by a stan-dard fusion method, and combines this with a state-of-the-art re-sult diversification model. We found that data fusion approaches outperform state-of-the-art search result diversification algorithms, with DDF invariably giving rise to the highest scores on all of the metrics that we have considered in this paper. DDF was shown to behave well with different numbers of component lists. We also found that DDF is insensitive to the number of latent topics of a query, once a sufficiently large number was chosen, e.g., 10.
As to future work, we aim to incorporate into DDF methods for automatically estimating the number of aspects, which will be used to set the number of latent topics. The last and third part of DDF is based on a particular choice of method, viz. PM-2, and we only apply rank-based fusion methods for diversification. In future work we plan to compare these choices with alternative choices, and ap-ply other fusion alternatives, e.g., score-based fusion methods. [1] S. Abbar, S. Amer-Yahia, P. Indyk, and S. Mahabadi. depth k (for k = 5 , 10 , 15 , 20 ). Figures should be viewed in color. used in DDF. Note: the figures are not to be the same scale. [2] R. Agrawal, S. Gollapudi, A. Halverson, and S. Ieong. [3] E. Aktolga and J. Allan. Sentiment diversification with [4] J. A. Aslam and M. Montague. Models for metasearch. In [5] D. M. Blei, A. Y. Ng, and M. I. Jordan. Latent dirichlet [6] J. Carbonell and J. Goldstein. The use of MMR, [7] H. Chen and D. R. Karger. Less is more: probabilistic [8] C. L. A. Clarke and N. Craswell. Overview of the TREC [9] C. L. A. Clarke, M. Kolla, G. V. Cormack, O. Vechtomova, [10] C. L. A. Clarke, N. Craswell, and I. Soboroff. Overview of [11] C. L. A. Clarke, N. Craswell, I. Soboroff, and G. V. [12] C. L. A. Clarke, N. Craswell, and E. M. Voorhees. Overview [13] V. Dang and W. B. Croft. Diversity by proportionality: An [14] V. Dang and W. B. Croft. Term level search result [15] M. Efron. Information search and retrieval in microblogs. J. [16] M. Farah and D. Vanderpooten. An outranking approach for [17] E. A. Fox and J. A. Shaw. Combination of multiple searches. [18] T. L. Griffiths and M. Steyvers. Finding scientific topics. [19] D. He and D. Wu. Toward a robust data fusion for document [20] J. He, V. Hollink, and A. de Vries. Combining implicit and [21] T. Hofmann. Probabilistic latent semantic indexing. In [22] O. Jin, N. N. Liu, K. Zhao, Y. Yu, and Q. Yang. Transferring [23] A. K. Kozorovitsky and O. Kurland. Cluster-based fusion of [24] T. Kurashima, T. Iwata, T. Hoshide, N. Takaya, and [25] J. D. Lafferty and D. M. Blei. Correlated topic models. In [26] J. H. Lee. Combining multiple evidence from different [27] J. H. Lee. Analyses of multiple evidence combination. In [28] F. Li, M. Huang, and X. Zhu. Sentiment analysis with global [29] W. Li and A. McCallum. Pachinko allocation: [30] S. Liang and M. de Rijke. Finding knowledgeable groups in [31] S. Liang, M. de Rijke, and M. Tsagkias. Late data fusion for [32] S. Liang, Z. Ren, and M. de Rijke. The impact of semantic [33] N. Limsopatham, R. McCreadie, and M.-D. Albakour.
 [34] J. S. Liu. The collapsed gibbs sampler in bayesian [35] C. Macdonald and I. Ounis. Voting for candidates: Adapting [36] Z. Ren, S. Liang, E. Meij, and M. de Rijke. Personalized [37] M. Rosen-Zvi, T. Griffiths, M. Steyvers, and P. Smyth. The [38] T. Sakai, Z. Dou, and C. L. A. Clarke. The impact of intent [39] R. L. Santos, C. Macdonald, and I. Ounis. Exploiting query [40] R. L. Santos, C. Macdonald, and I. Ounis. Intent-aware [41] R. L. T. Santos, J. Peng, C. Macdonald, and I. Ounis. [42] J. A. Shaw and E. A. Fox. Combination of multiple searches. [43] D. Sheldon, M. Shokouhi, M. Szummer, and N. Craswell. [44] I. Szpektor, Y. Maarek, and D. Pelleg. When relevance is not [45] S. Vargas, P. Castells, and D. Vallet. Explicit relevance [46] X. Wang and A. McCallum. Topics over time: a non-markov [47] X. Wei and W. B. Croft. LDA-based document models for [48] X. Wei, J. Sun, and X. Wang. Dynamic mixture models for [49] S. Wu. Data fusion in information retrieval , volume 13 of [50] Z. Xu, Y. Zhang, Y. Wu, and Q. Yang. Modeling user posting [51] C. Zhai, W. W. Cohen, and J. Lafferty. Beyond independent
