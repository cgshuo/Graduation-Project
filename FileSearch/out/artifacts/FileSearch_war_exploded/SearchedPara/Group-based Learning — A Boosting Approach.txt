 This paper points out that many machine learning problems in IR should be and can be formalized in a novel way, referred to as  X  X roup-based learning X . In group-based learning, it is assumed that training data as well as testing data consist of groups. The clas-sifier is created and utilized across groups. Furthermore, evalua-tion in testing and also in training are conducted at group level, with the use of evaluation measures defined on a group. This pa-per addresses the problem and presents a Boosting algorithm to perform the new learning task. The algorithm, referred to as Ad-aBoost.Group, is proved to be able to improve accuracies in terms of group-based measures during training.
In many IR learning problems, data naturally forms groups and training and testing should be performed on the basis of groups. For example, in email spam filtering, emails are associated with users (receivers), and we can group emails according to users. Emails of di ff erent users may di ff er largely in content and style, while emails of the same user may be quite similar. Furthermore, evaluations in training and testing should be conducted at the user level. For example, F 1 scores should be calculated based on individual users, and we want to enhance the F 1 scores of all users.

In this paper, we aim to (1) give a formal definition to such kind of learning task, which we call group based learning, and (2) pro-pose a learning method that can perform the task. First, group-based learning is formalized in a decision theoretic manner. (1) Training data as well as testing data are assumed to be comprised of di ff erent groups. The groups are supposed to be generated i.i.d. according to a fixed but unknown probability distribution. (2) The goal of learning is to construct a single classifier which can be ap-plied across groups. (3) Evaluation in testing and also in training are conducted at group level. Second, a Boosting algorithm for group-based learning is proposed. The algorithm, named as Ad-aBoost.Group, manages to optimize the total performance in terms of any evaluation measures defined on a group.
We give a formal description of the group-based learning prob-lem. Without loss of generality, we consider binary classifica-tion. Let G denote the space of groups, X the space of instances ( X X  R d ), and Y the space of labels ( Y = { + 1 ,  X  1 } currently). Fur-thermore, let Z denote the space of all possible instance and label pairs: Z =  X   X  i = 1 ( X X Y ) i . Let ( g , ( x , y ))  X  X  X Z be a group g and its associated instance-label pairs ( x , y ), where x = ( x 1 X , y = ( y 1 ,  X  X  X  , y m ( g ) ), and m ( g ) denotes the number of instances.
In training, we are given a set of training data S = { ( g joint probability distribution P G X Z . The goal of learning is to con-struct a classifier f  X  X  to predict y on the basis of x . The learning problem turns out to be that of minimizing the risk function where  X  (( g , ( x , y )); f ) denotes the loss function. Then the empirical risk function is defined as
It is easy to verify that with certain assumptions group-based learning will become equivalent to instance-based learning. Since ( g , ( x , y ))  X  P G X Z = P G  X  P Z|G , Equation (1) can be written as We further assume that the total loss  X  over each group g can be decomposed linearly into a sum of losses  X  over instances within g and that  X  is independent of g , and further assuming that ( x , y ) is generated i.i.d. according to P X X Y|G , (3) can be written as The total loss then becomes an instance-based loss function:
Group-based learning is related to multi-instance learning [1] (groups in this case correspond to bags in multi-instance learn-ing); however, there are also di ff erences between the two. In multi-instance learning, the bags are labeled but instances are not, and the task is to predict the label of a new bag. In group-based learn-ing, the instances are labeled and the task is to predict the labels of instances in a new group.
We propose a novel group-based learning method. The algo-rithm, referred to as  X  X daBoost.Group X , is a natural extension of the AdaBoost [3] algorithm to group-based learning. Figure 1 gives the pseudo code of the AdaBoost.Group algorithm. Input : S = { ( g i , ( x i , y i )) } n i = 1 , performance E , and rounds T Initialize D 1 ( i ) = 1 / n For t = 1 ,  X  X  X  , T End For Output hypothesis f ( x ) = f T ( x ).

AdaBoost.Group takes training data S = { ( g i , ( x i , y put and takes group-based evaluation measure E and number of iterations T as parameters. AdaBoost.Group runs T rounds and in each round it creates a weak classifier h t ( t = 1 ,  X  X  X  , T ). Finally, it outputs a model f by linearly combining the weak classifiers.
The linear combination of weighted weak classifiers is defined as f ( x ) =  X  the corresponding weight, and T the number of weak classifiers.
At each round t , AdaBoost.Group maintains a distribution of weights D t over the groups in the training data. Initially, it sets equal weights to the groups. During the training, it increases the weights of those groups that are not predicted well by f t created so far. As a result, the learning at the next round will be focused on the creation of a weak classifier that can work on the prediction of those  X  X ard X  groups.

At each round, a weak classifier h t is constructed based on train-ing data with weight distribution D t . The  X  X oodness X  of a weak classifier is measured by the group-based evaluation measure E , which is assumed to take values from [0 , 1].

Once a weak classifier h t is built, AdaBoost.Group chooses a weight  X  t &gt; 0 for h t . Intuitively,  X  t measures the importance of h
It can be proved that there exists a lower bound on the total performance of AdaBoost.Group with respect to training data, as shown in Theorem 1.

T  X  X  X  X  X  X  X  X  1. The following bound exists for AdaBoost.Group: where  X  t = Here  X  i t = E (( g i , ( x i , y i )); f t )  X  E (( g i , ( x
We conducted experiments to evaluate the performances of Ad-aBoost.Group on three applications: email spam filtering, acronym interpretation, and protein homology detection. For email spam tation, we created a dataset based on the UCI data  X  X SF Research Awards Abstracts 1990-2003 X . The dataset contains 495 ground truth acronym-expansion pairs; for protein homology detection, we the groups in these datasets into several even subsets and conducted cross validation. The results reported are those averaged over trials. 1 http: // plg.uwaterloo.ca / gvcormac / treccorpus / 2 http: // kodiak.cs.cornell.edu / kddcup / datasets.html AdaBoost.Group (Accuracy) 0.9567 0.9981 0.9970 AdaBoost.Group ( F 1 ) 0.9381 0.9975 0.9966 SVM per f (Accuracy) 0.9442 0.9978 0.9968 SVM per f ( F 1 ) 0.9346 0.9977 0.9953 SVM (cost-sensitive) 0.9481 0.9979 0.9966 AdaBoost (cost-sensitive) 0.9488 0.9974 0.9965 SVM 0.9433 0.9972 0.9965 AdaBoost 0.9436 0.9972 0.9966 AdaBoost.Group (Accuracy) 0.6983 0.9160 0.6708 AdaBoost.Group ( F 1 ) 0.8645 0.8918 0.6848 SVM per f (Accuracy) 0.4804 0.8990 0.6636 SVM per f ( F 1 ) 0.4674 0.9017 0.6712 SVM (cost-sensitive) 0.5612 0.8849 0.6510 AdaBoost (cost-sensitive) 0.5130 0.8867 0.6601 SVM 0.4804 0.7932 0.6354 AdaBoost 0.4885 0.8298 0.6532
We chose several instance-based methods as baselines. State-selected as baseline. Accuracy and F 1 score were adopted as the racy) X  and  X  X VM per f ( F 1 ) X , respectively. We also employed cost-sensitive versions of SVM and AdaBoost, denoted as  X  X VM (cost-sensitive) X  and  X  X daBoost (cost-sensitive) X . For AdaBoost.Group, E can be a group-based evaluation measure. Here, E was defined as Accuracy or F 1 score, denoted as  X  X daBoost.Group (Accuracy) X  or  X  X daBoost.Group ( F 1 ) X , respectively.

The experimental results for the tasks of email spam filtering, acronym interpretation, and protein homology detection in terms of accuracy and F 1 are summarized in Table 1 and Table 2, respec-tively. From the results, we can see that AdaBoost.Group outper-forms all the baselines in terms of the group-based evaluation mea-sures. We conducted significance tests (t-test) on the improvements of AdaBoost.Group (Accuracy) and AdaBoost.Group ( F 1 ) over the baselines. Most of the improvements are statistically significant ( p -value &lt; 0 . 05).
In this paper we have proposed and formalized a new learning problem  X  X roup-based learning X . In group-based learning, all the instances are clustered into groups and the evaluation is conducted at group level. To address the problem we have proposed a novel Boosting algorithm, named AdaBoost.Group. AdaBoost.Group can optimize an upper bound of the total loss function in terms of eval-uation measure defined on a group. [1] T. G. Dietterich, R. H. Lathrop, and T. Lozano-Perez. Solving [2] T. Joachims. A support vector method for multivariate [3] R. E. Schapire and Y. Singer. Improved boosting algorithms
