 When incorporating term dependencies into the retrieval model, one of the important without a theoretically motivated integration model, documents containing dependen-cies may be over-scored if they are weighted in the same way as single words. 
The scoring model is the core of retrieval model. Research had been conducted to study the performance of different scoring models. For example, [4] has studied some heuristics that a scoring model should meet in generative models. [7] has studied dif-introducing some ways to incorporate the dependence model with the independent unigram model in keyword-based short queries. The Dependency Language Model model and dependence model in sentence-based verbose queries. The scoring func-dependence retrieval model in both short queries and verbose queries. 
In this paper, we attempt to solve the over-scored problem using an axiomatic ap-proach through defining several heuristic constraints. The experiment results on three TREC collections show that our model achieves robust improvement both in short queries and verbose queries. In the rest of the paper, Section 2 reviews some previous relevant work; Section 3 presents the definition of several retrieval heuristics and our axiomatic dependence model; a series of experiments on TREC collections is pre-sented in Section 4; some conclusions are summarized in Section 5. ment consists of both the independence score and dependence score, such as Bahadur guage model, the situation is same. In the simplest bi-gram model [3], the probability of bi-gram (q i-1 ,q i ) in document D is smoothed by its unigram: Q={q 1 q 2 ...q m } and document D is: 
In Equation (2), the first score term is the independence unigram score and the sec-ond score term is the smoothed dependence score. Usually  X  is set to 0.9, i.e., the de-pendence score is given less weight than the independence score. 
Dependence Model [5], which can be regarded as the generalization of the bi-gram model, gives the relevance score of a document as: 
In Equation (3), L is the set of term dependencies in query Q. The score function consists of three parts: a unigram score logP(q i |D), a smoothing factor logP(L|D), and a dependence score MI(q i ,q j |L,D). It is the parsing score P(L|D) that serves as a nor-malization factor (or penalty) to balance the impact of single terms and term depend-encies. To estimate P(L|D), the authors use many smoothing strategies. 
MRF [2] combines the score of full independence T, sequential dependence O and independence model. 
Though the above models are derived from different theories, smoothing is an im-portant part when incorporating term dependencies. And it also our belief that smoothing strategy will play a important role when dealing with over-scored problem. The scoring functions in the above models have different forms, which are dedicated smoothing strategies will be used in the definition of these scoring functions. In smoothed unigram language model [9], the RSV formula has the general form of:  X  P
DML (w|D), which is discounted maximum likelihood estimation of word w in D, and P(w|C), which is the weight of word w in collection C. When it comes to the de-pendence model, we define the RSV formula as: document D and Collection C. We follow the spirit of the axiomatic approaches in [4] 3.1 Constraints on Weight of Term Dependency in Collection (q 1 ,q 2 ) and (q 2 ,q 3 ).  X   X  P(q 1 ,q 2 |C)&gt;P(q 2 ,q 3 |C) 
DF(q i ) is the document frequency of term q i , DF(q i , q j ) is the document frequency lated way with  X  as the weight: 3.2 Constraints on Weight of Term Dependency in Document  X  DF(w i ,w j ,R) is the count of documents in which w i and w j have a relation.  X  &gt;C D (q 3 ) , then P(q 1 ,q 2 |D)&gt;P(q 2 ,q 3 |D). 
From the constraints C3 and C4, we define the P(w i ,w j |D) as RSV formula of the dependence model as: 
In equation (8), there are two parameters:  X  and  X  . The relevance score of (Q,D) is the sum of RSV UG (Q,D) and RSV DEP (Q,D). P(w i ,w j |Q) is set to 1/|L| in the follow-ing experiments; |L| stands for the number of term pairs in L. We evaluated our axiomatic dependence model (ADM) using three TREC collections. Table1 shows some statistics of these collections. In the documents X  indexing phase, terms are stemmed using WordNet and stop words are not removed. There are several model using Dirichlet Prior smoothed KL-divergence (KLD) model [10]. BG is a smoothed bigram language model which sets the weight of the empirical bigram probability to 1% and weight of unigram probability to 99% (Differently from [3], we find 99% achieves better performance than 90%). CULM is our implementation of Concept Unigram Language Model described in [6]. In CULM, the estimation of bi-gram is same as BG. ADM is implemented by incorporating above UG model and our axiomatic dependence model. All these models were implemented in Lemur. bose queries (description field). On short queries, the set of term pairs L is treated as adjacent words in the queries (queries that have more than two words are used). Thus, ADM and BG share the same query term dependencies. On verbose queries, Minipar phrases of more than two words are decom posed to sequential relations between adja-cent words. ADM and CULM share the same query term dependencies. (w i ,w j ) appears with the window of size N in document D. We tried the window size length and it is used in the following experiments. The main evaluation metric in this ment over UG is statistically significant accord ing to paired-samples t-test (significant bose queries experiments. This shows the robustness of our ADM model. 
We can see from Table 3 that the improvement of ADM over UG is statistical sig-nificant on verbose queries. While its improvement on short queries is not as signifi-cant, it is better than BG. This shows the effectiveness of our ADM model. In order to deal with the over-scored problem in dependence retrieval model, we have formally define four heuristic constraints that any reasonable retrieval function should satisfy. Secondly, some smoothing strategies are used to derive the score function by some promising improvements over some state-of-art dependence retrieval model. more comparison can be conducted to verify the effectiveness of our ADM model. Acknowledgments. This work is supported by a China 973 Hi-tech project (No. 60603094) and a China 863 Hi-tech project (No. 2006AA010105). 
