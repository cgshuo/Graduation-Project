 ORIGINAL PAPER Ankush Roy 1  X  Biswajit Halder 2  X  Utpal Garain 3  X  David S. Doermann Abstract Automatic authentication of paper money is becominganincreasinglyurgentproblembecauseofnewand improved uses of counterfeits. In this paper, we describe a system developed for discriminating fake notes from genuine ones and apply it to Indian banknotes. Image processing and pattern recognition techniques are used to design the over-all approach. The ability of the embedded security aspects is thoroughly analysed for detecting fake currencies. Real sam-ples are used in the experiments that show a high-precision machine can be developed for authentication of paper money. The system performance is reported for both accuracy and processing speed. The analysis of security features to prevent counterfeiting highlights some of the issues that should be considered in designing of currency notes in the future. The problem of large-scale counterfeiting of paper currency poses a serious threat to our society as large numbers of fake notes causes economic instability. Counterfeiting of currency notes affects the existence of the monetary equilibrium as its value, velocity, output and welfare may be affected. Most countries that use paper currency for transactions are plagued by this problem. Several media reports highlight the alarming rate at which this counterfeits are increasing, the seriousness of the issue, as well as the continuous government efforts to curb this problem [ 1  X  3 ]. Unfortunately, counterfeiters also adapt tothenewsecurityfeatures that areincorporated. Crim-inals continue to find ways to replicate the currency despite new banknote security features in place. There have been leaps and bounds in the technical field of counterfeit curren-cies, and this together with the recent advances in the digital scanning and copying techniques has been an indomitable force. This has moved the European Union legislators to draft new guidelines so that computer and software manu-facturers are forced to introduce new security measures to make it impossible for their products to be used to copy ban-knotes [ 4 ]. However, it is almost impossible to track and stop all of the counterfeiting efforts, and hence, we need to deploy better authentication systems that carefully scrutinize notes before allowing them to circulate.

Bank staff are specially trained to detect counterfeit notes, and so far they have been do it manually as the sorting machines are not able to do this job. Technology on which sorting machines work is limited to image based scanning, which is not foolproof when it comes to the detection of fake currencies. Technology for sorting banknotes does not take into consideration the intricacies involved in security fea-tures,physicalandchemicalpropertiesofpapers,inks,resins, chemical, etc. used to print the notes, and therefore, they are often unable to distinguish subtle discrepancies between fake and genuine notes.

Sometimes the human experts (e.g. the questioned doc-ument examiners) are involved to give their opinions on suspected notes. Existing methods for detecting and confirm-ing the fake notes are too cumbersome as this involves filing a case with the police, sending the document for verifica-tion and then waiting for results to come. On the other hand, the vast traffic actually calls for the introduction of machine authentication of currency notes [ 5 , 6 ]. Forensic agencies are always on the lookout for systems that track these forgeries using automated techniques and give a complete analysis of the error report of the note in question. Furthermore, the speed introduced to the process due to automation will help in handling the huge volume of paper. Machine authentica-tion techniques could also ensure that fraudulent samples are not passed via ATMs and other cash delivery systems [ 7 ]. Even currency note counting or sorting machines may have such equipment installed that looks to trap the counterfeit currency notes that might appear.

Our research is directed to this end. It attempts to provide a complete automated approach for detection of counterfeit currency notes. A thorough analysis is provided that explores the performance of the embedded security features and their sensitivity. This analysis helps the regulatory bodies under-stand which security feature is under what kind of threat of breach and what modifications could be done to improve the design, making it less vulnerable to counterfeiting. Detailed experiments were done with real data to support the claim. A comparative study is also reported, involving forensic doc-ument experts and bank staff to show the applicability and robustness of the system at a grass root level. 1.1 Previous work Research on manufacturing secure currency notes is indeed an old subject. Because of the commercial nature of this research, lots of research studies were patented rather than published in scientific journals. A US Patent issued in 1857 [ 8 ] may be the earliest attempt of an optical method of manu-facturing secure paper money. It involved using paper tinted to absorb light and printing ink that also absorbed light rather than reflecting it so that clear photographic copies could not be made. Many researchers assumed that the embedded watermark and security thread are hard to duplicate and filed patents on how to authenticate the watermark and or secu-rity thread [ 9  X  11 ]. The patent in [ 12 ] described an approach that is based on the reflective property of the banknotes in question. A series of light sources are placed that provides wavelength of varying illumination, which is used to mea-surethereflectiveandrefractiveresponseofthecurrencynote images. These data are then compared with the pre-calculated data on original and fake notes to report the authenticity of the notes. On the other hand, the patent in [ 13 ] proposed a method to verify US currency notes by analysing different aspects of the ink used for printing.

Because of the commercial interest, the patents express lit-tle technical and experimental details and this has restricted the research community to judge the performance of the systems. On the contrary, the researchers reporting details of their methods and experiments have so far dealt with recognition of currency notes. They address the problem of recognizing currency of different countries (US dollars, Euro-notes, etc.) and different denominations for a given currency. For this purpose, neural networks [ 4 , 8 , 9 , 14  X  16 ], genetic algorithms [ 17 ] and hidden Markov model [ 18 ]have been used. These studies have successfully addressed the currency or denomination recognition problem but did not consider whether the input banknote is genuine or fake. There are a few studies which address this problem and give details about their methods and experiments. The paper in [ 19 ] is an example. In this article, the authors proposed a semi-automatic approach for characterizing and distinguish-ing original and fake Euro-notes. Their method is based on the analysis of several areas of the banknotes using a Fourier transformed infrared spectrometer with a microscope with an attenuated total reflectance (ATR) objective. They considered four different regions of a note and observed that fake notes are easily identifiable from the analysis of the spectra corresponding to the four regions. However, the authors did not propose any automated scheme for authenti-cation.

Later, the authors in [ 20 ] describe another system for authenticating Bangladeshi banknotes. They assume that original currencies under test have the bank name printed in micro-letter print. They scan this part (the region where the bank name should be) using a grid scanner, and the textual images are fed in an optical character recognition engine that matches characters with prototypes. Since the fake currencies are assumed not to have the text, they show very low match-ing percentage. The algorithm is heavily dependent on one feature which makes the system very sensitive. The system would fail miserably if the counterfeiters happen to develop a means of duplicating the feature in question.

Very recently, a commercial system [ 21 ] was introduced to the market for detecting fake notes. In order to authen-ticate a paper currency, the system makes use of several hardware components such as light intensity compensation circuits, magnetic level circuits, visual light sensors, infrared ray sensors, ultraviolet ray sensors and many other technical light sensors. These hardware components function simul-taneously and compare crucial security features like ones from the paper, ink, security thread, fluorescent resins, micro-prints, secret watermarks, intaglio patterns, etc. against the data stored in standard memory bank. The memory bank keeps information about the characteristics of genuine and counterfeit notes. For a given note, if the features match those of the real ones, the system passes the note as genuine. If any difference is spotted, the machine saves the features that match those of the counterfeits and these notes are rejected. In case of no match either with a genuine note or with coun-terfeit notes, the notes one identified as genuine but worn out, and hence, the machine passes it as genuine in case of normally worn out notes. 1.2 Contribution of our work After reviewing the literature, we find that there have been few studies reporting technical and experimental details on howtoautomaticallyauthenticatecurrencynotes.Manystud-ies rely on one or two features that also can be duplicated using today X  X  high-end technology. As there are so many security features in currency notes, analysing only a certain aspect of the note security may not be a good choice. A complete integrated framework has been missing that looks into many aspects such as security features in printing, ink, background artwork, watermark and security thread. Another general shortcoming in the existing studies is the use of syn-thetic data. Many authors generate samples at the laboratory to test their algorithms. Therefore, performances of these algorithms on real samples are yet known.

Earlier, we provided conclusive evidence of decision-making using artwork design [ 22 , 23 ] or printing tech-niques [ 24 ], but these approaches looked only on particular security aspect (either the artwork design or the printing process) and dealt with a small database to test the initial system performance. In this paper, we have incorporated sev-eral other security aspects based on ink, security thread and an artwork-based feature set that supplements the printing style-based approach. A large data set comprised of 1000 samples [500 for each genuine (G) and fake (F) class] has been used to test the system performance. Analysis is also done to report the speed of the system. Popular pattern recog-nition tools such as the k-means, neural networks and support vector machines are used. We have also shown the robustness of each feature in tackling the problem and thus pointing out the sensitivity of the features. Feature level analysis brings out important issues that the note designing agencies could keep in mind regarding the susceptibility of the features to duplication. A sequential ordering of security features is also suggested that should be maintained while testing to maxi-mize the performance accuracy. Involvement of real samples is another significant aspect of this study. Indian banknotes are used as a reference. Because of highly heterogeneous concepts involved with Indian currencies, authentication of these notes poses a big technical challenge. The impact of the document and its mobility as a transac-tion medium influences the amount of security in place for its protection. There have been a lot of research on provid-ing security in paper documents [ 25 ]. Banknotes being the principal monetary exchange medium do have a lot of secu-rity features to prevent counterfeiting [ 26 ]. The Apex bank of every country is responsible for using these security fea-tures to prevent forgery. In India, it is the responsibility of the Reserve Bank of India (RBI) [ 27 ]. The security features in a currency note are mainly with its paper, design and print-ing. Authentication of currency notes thus deals with (i) its notepaper, (ii) the note printing technique, (iii) the ink used for, (iv) the note design and (v) other security features (e.g. the thread, the registration mark and many others) that are intentionally incorporated to check the authenticity. These security features provide a tough challenge to the counter-feiters who attempt to replicate them.

Important security features are associated with the cur-rency notepaper itself. Physical features of currency note are based on its length, width, grammage and thickness. The paper has a unique feel and crackling sound, and it is con-structed of high-quality 100 % cotton or wood pulp resulting in a particular colour, a unique fibre length, a uniform surface finish, a typical opacity and its extra strength. The techniques used by forensic experts to analyse paper rely mostly on human perception. Halder et. al. in their work [ 28 , 29 ] study paper as a security feature. Watermarks and security threads areanotherimportantpartsofsecurity.Animportantproperty of watermarks is that this cannot be replicated on scanning or by photocopy equipment. Examination of watermarks check its design, size and thickness using transparent light. The security thread that appears to the left of the Mahatma Gandhi portrait is partially embedded and partially visible. On seeing this thread with an ultraviolet light, the thread appears in a single line. This is also proof of the registration of the note thatboththesidesofthenoteareproperlyaligned.Thisthread has the text of  X  X BI X  and  X  Bharata  X  (India) in Devanagari alternatively written on it. The denomination of a note (e.g. 500, or 1000) is also embedded in the thread. Figure 1 shows the significant security features embedded in both sides of a 500-rupee note (source: Reserve bank of India).

A lot of important security features are involved in the design of currency note. The Guilloche design, portrait design, micro-lettering, type face, font size, colour, see through register, anti-scan lines, Braille mark, rainbow effect, layers of CYMK, bleeding effects, latent image effects, etc. all are involved as part of security design. Another level of distinction of currency notes is the printing process. Among the numerous printing processes that can be used, only a few processes are used for printing of Indian currency notes. Among them, intaglio printing is of prime impor-tance because of the signature it leaves and it being hard to duplicate. The basic components of printing ink are pigment, solventanddrier.Whereinkpigmentisresponsibleforcolour effect on the substrate, a drier is responsible to bind ink pig-ment to the substrate. Examination of the final printing effect is an important aspect for verification of document authen-ticity. The final effects on the currency are defined by unique colour, impression, water resistance, line work (width, thick-ness, sharpness, etc.), halftone effect, digitized patterns and also reflectivity and feel/tactility. The proposed method is based on image processing [ 30 ] and pattern recognition principles [ 31 ]. The feature extraction in this experiment is largely dominated by the input from the forensic experts making sure that every aspect of the security features is considered when choosing features. As not all the features used by the experts [ 32 ] can be captured computa-tionally, a subset of the features is used. Some new features which are effective for detecting fake notes, but are diffi-cult to check manually, are also added. The feature space is analysed and visualized by using clustering technique. The decision-making process is built using two different classi-fiers: (i) artificial neural networks (ANNs) and (ii) support vector machines. Furthermore, a linear discriminate analy-sis is used to measure the performance of each feature. Our feature extraction process considers four different security aspects of the banknote: (i) printing technique, (ii) ink prop-erties, (iii) the thread and (iv) the artwork used in designing the note. The features and rationales behind choosing them are explained below. 3.1 Preprocessing Different security features are available on different parts of the banknote image. So the initial scanned image needs to be divided into distinct ROIs. The image of the currency note is registered using Hough transform on a Canny edge detected image (Fig. 2 a). Template matching of the denomination ( X 500 X / X 1000 X ) and the Mahatma Gandhi portrait generates two fixed positions as reference around which the rest of the ROIs are extracted (Fig. 2 b).
 The horizontal strip just above the registered portrait of Gandhi is used to segment the intaglio fonts. Text extraction from this part is done using the vertical and horizontal pixel projection techniques (Fig. 3 ). The biggest black pixel blob in the image is the area to be focussed on for extracting features usinglatentscan.Averticalstripjustbesidethedenomination is used for the security thread-based measures, and finally, the region between the registered Mahatma Gandhi portrait and the largest black blob is used for the micro-print line features. 3.2 Features 3.2.1 Printing technique Intaglio printing is used for printing currency notes in India. The denomination of the note and  X  X eserve Bank of India X  are printed on the face of the notes and are always printed using the intaglio method. This method of printing leaves several signatures that are hard to replicate [ 28 ]. We have analysed some of the features and tried to differentiate the fake notes from the genuine ones based on printing tech-nique detection. Following are the features used in detecting printing technique.

Dominant intensity ( f 1 p ) is used to capture slight differ-ence in brightness (or glossiness) of banknotes. Mathemati-cally this is represented as follows, f
Thefeature,Holecount( f 2 p ),checksthetexturalsimilarity of the printed character strokes in a note by counting the number of eight-connected white pixel cluster (defined as hole) divided by the area of the character stroke as shown in Eq. ( 2 ), f
Average hue ( f 3 p ) gives an assessment of the quality of colour. This feature is computed in HSV space on the Hue ( H ) stream as follows, f This shows us the degree of separability in colour space of genuine notes compared to fake notes.

R.M.S. contrast ( f 4 p )inEq.( 4 ) measures the varying dif-ference in brightness of the two classes (genuine/fake) of notes. Mathematically, it is expressed as follows (where I and  X  I denote the intensity of the i th pixel and the mean inten-sity, respectively), f In other words, we measure the standard deviation and note the degree of variability in the intensity distribution as a fea-ture.

The key tone ( f 5 p ) value gives us an information about the intensity zone where most of the information is stored by calculating the mean of the intensity profile of the character stroke after masking as given below, Image  X  Masked character  X  f 5 p = Mean ( Intensity ).
Average colour ( f 6 p ) assesses a reconstituted colour matrix, based on a scalar parameter ( p ). Images are trans-formed from RGB colour space to CYMK. While doing this, we observe that in the complemented colour space, black streams have low values in general. However, input from domain experts suggests that black is one of the principle pigments used in itaglio printing. So to improve the separa-bility of the notes in the feature space, we set a higher the black stream. The principal streams in intaglio character stroke are blue and black in 500 denominations. The average colour is computed as below, f where s ( i ) is defined as s ( i ) = pB and B blue and B black correspond to blue and black strokes, respectively.

Along with these six features, three other features are extracted: edge roughness E PBER ,( f 7 p )(Eq. 7 ), area dif-ference ( f 8 p )(Eq. 8 ) and correlation coefficient ( f features are computed based on the work of Breuel et al. [ 33 ]. The edge roughness is computed as f where p a is the perimeter of the original image, p b is perimeter of the filtered (median filter) binary image and E
PBER is the perimeter based edge roughness. In calculating this feature, the character image is first binarized using Otsu threshold value (say, T) ( A otsu ), and then the same image is again binarized using a different threshold value that is cal-culated by adding a normalized parameter sc to T ( A otsu The area difference is computed as f
The correlation coefficient is computed as f where ( i , j )  X  ROI , A is the original grey value image, B is the corresponding binary image and  X  A and  X  B are the mean of A and B , respectively.

A detailed explanation of how these features were engi-neered and the reason why they are used can be explored in [ 24 ]. Based on the above nine features, classifiers are trained to identify fake notes based on printing technique. 3.2.2 Ink properties The reaction of the ink on a particular substrate is different for different inks. This difference actually lends a typical signature indicating the authenticity of a note.
 CCRatio ( f 1 i )Colourcompositionofthecentralzone(Fig. 4 ) of a note is analysed by doing an independent component analysis. This was followed by a filtration method that keeps those pixels ON where the green component index in the RGB colour space is higher than the blue component index which is in turn higher than the red component index to gen-erate a mask. Computationally, it is represented by the colour composition ratio (CCRatio) feature which is defined as f The number of pixels is fixed as the images are registered using a 4-point registration prior to processing.
 Micro-letter ( f 2 i ) This feature appears between the vertical band and Mahatma Gandhi portrait in the notes. In notes of denominations 20 and above, the denominational value and  X  X BI X  constitute the micro-letters. In our study, we have looked into the colour of these micro-letters. The RGB val-ues are first transformed to a specific absolute colour space. This adjustment makes the resulting data device independent. The masked image was changed from RGB to L  X  a  X  b  X  colour space using the CIELAB Illuminant D65 as a reference [ 34 ]. The micro-letters are printed on a yellow background using a variation of yellow hue. Therefore, a difference of the test note X  X  yellow colour variation against the expected value reveals whether the variance is within specified limits.
In CIE La  X  b  X  space, + b movement (shift towards yellow is measured on a scale of + bto  X  b) represents a shift towards yellow along the b  X  axis. So we choose b  X  stream to study the distribution of yellow on the micro-lettering regions. The mean value (  X  b  X  ) is computed from 200 genuine random i.i.d. samples.
 The resultant distribution of the b  X  index is plotted in Fig. 5 . The difference (visually) between genuine and fake notes is seen. Computationally, spread of the index distribu-tion is captured as the feature f 2 i by calculating the standard deviation (spread) values of the b  X  index distribution as for-mulated below, f Ink fluidity ( f 3 i ) It is observed that the ink used to print genuine currency notes blot considerably greater than the counterfeit ink. The study of fluidity of ink as a vision-based feature was done by Franke et al. [ 35 ]. Following this study, we developed a feature that would computationally help in decision-making about the ink authenticity. Edges of the print were taken, and the intensity profile was plotted (Fig. 6 ). We then normalize the curve using an averaging kernel. A steady value is computed in Eq. 12 as follows. Let f ( x ) be the num-ber of pixels having intensity x ,  X  x  X  X ={ x 1 , x 2 ... where x 1 refers to the intensity value corresponding to max-imum pixel count and x k = x k  X  1 + 1. We define f ( x ) as f ( x ) = f ( x k + 1 )  X  f ( x k ), 0 &lt; k &lt; n  X  x the differential histogram. This feature reflects the interac-tion of ink on the substrate, thus checking both the ink and the paper. In other words, tampering the printing process with either of the two would be reflected in this feature. Figure 6 shows visually the difference in edge sharpness.

Note that the f ( x ) vector will always start with a neg-ative quantity as the very first value in this vector is the difference from the highest pixel count. The first positive entry is found ( x p ), and the steady value is generated as fol-lows: steady = where p is the position of the first positive entry.
The percentage overshoot is then recorded as a feature using the steady value in Eq. ( 13 ) and computed as follows: f 3.2.3 Thread Two security thread-related features are considered: the reg-istration of the notes and the text in the security strip. Registration ( f 1 t ) The thread should always appear as a sin-gle line. This is a way to test of the registration of the notes. We check this using a binary feature, f 1 t , which decides whether a note is genuine ( G =1)orfake( D =0).Two sets of thick blobs are found (see Fig. 7 (i)b): One represents the thread parts seen from the front, while the other, rep-resents the thread parts on the back of the note. Two lines, one for the front and the other for the back, are fit through the centroid ( c x , c y ) points of the corresponding blobs. The centroid of a blob is calculated using the following formulae where A be the area of the blob. The blobs are treated as non-intersecting polygons defined by n points. Assume C is the set of all centroids, C ={ c 0 , c 1 ,..., c m } . Each of the m centroids is defined by a tuple ( c x , c y ) . The coordinates c and c y are calculated as shown below. c = c =
This is followed by a distance check of the lines using a threshold distance t , empirically computed from 100 note samples. According to the domain experts, this is a highly sensitive feature and a good point to do an initial check. We observe that the variance was very low on the order of 0.01 when sampled over 100 genuine images. The expected value from this sampling was set as the threshold t . d ( t f , t b )&lt; t , Genuine, where t f = foreground line points, t b = background line points.

The feature f 1 t is a binary feature which generates a deci-sion based on Eq. 14 . Figure 7 (i) shows the registration problem of thread in a fake note.
 Text in thread ( f 2 t ) This is another binary feature that checks whether thread text exists. The texts RBI and  X  X harat X  (Hindi for India X ) in Devanagari script are written on original notes where these two words appear alternatively. We extracted the text portion from the threads and then used conventional pattern matching tools to compare. There were only four such texts patterns as shown in Fig. 7 (ii)a X  X  to compare so the templates were extracted from the original image and then used to as ground truth data for pattern matching. A majority of fake notes showed negligible matching because they do not have any text as shown in Fig. 7 (ii)f. 3.2.4 Artwork This section deals with printing patterns that are intricately introduced in note design. Initially, the image is passed through a median (3  X  3 sub window) filter to remove impul-sive noise. Next, the centroid of each dot is mapped as shown in Fig. 8 . The three features described below are extracted and analysed.
 Dot distribution ( f 1 a ) The distribution of the dot centroids gives us the impression that in the fake note, the distribution of the dots is far less uniform when compared to the genuine notes. Entropy count provides a measure of this randomness. The entropy ( H ) is calculated as a feature, f 1 a , and the fol-lowing equation measures it as f where n is the number of dots.
 Clusterdistributionanddotdensityfeatures ( f 2 a and f 3 a also compute the number of clusters occurring at the char-acter strokes of the letters. An unsupervised agglomerative hierarchical clustering scheme is used with a Euclidean dis-tance check to calculate the number of clusters. Let C 1 and C 2 be the two clusters and d is the distance matrix, then max { d ( x , y ) : x  X  C 1 , y  X  C 2 } . An iterative process contin-ues until the separation of the clusters exceeds a threshold (indicated by a Euclidean distance). The cluster density is defined as f From Fig. 8 , it is evident that the original notes have a more even distribution of dots and far more dots in the character strokes. This difference in the dot count is used as another simple feature which gives the dot density, f 3 a as, f Adaptive Otsu threshold removes most of the background leaving only the character stroke whose area is calculated. Reading latent denomination ( f 4 a ) When viewing the strip lefttotheMahatmaGandhi X  X picture,thedenominationofthe note is seen engraved quite distinctively. The machine read-ability of these denomination digits is measured in Eq. 18 . This zone comprises of two sets of lines X -horizontal and vertical. The vertical lines represent the text part. The sharp-ness of the lines is different in the two categories of notes (genuine vs. fake) resulting in different readability index f which is defined as f
An MLP-NN classifier was trained with digit samples taken from the genuine notes. The vertical lines need to be extracted for the text to be read properly so we use a 3  X  convolution matrix (CM) CM = that reduces the number of horizontal lines in the image and strengthens the vertical lines.

The image that results from this operation has a lot of clutter and noise (Fig. 10 b). A particular component tag is assigned to each eight-connected cluster, and the number of pixels in each cluster is recorded. Figure 9 shows the his-togram of component size vs. component tag. A threshold is selected from this histogram to eliminate most of the clutter. Thismethodbringsoutthelatentinformationfromtheimage. Figure 10 c shows the resultant image after thresholding. Experiments are conducted to check the validity of the pro-posed approach using several real-life samples. This section explains the data set, the capture conditions, our experimental strategies and finally the results. 4.1 Data set The scanning of banknotes involves different medium as explained in the next section. Forensic experts note that a particular group shows a particular kind of proficiency for making fake notes. Two denominations, namely 500 and 1000 (1000 being the highest denomination for Indian ban-knotes), are considered as the fake note-makers mostly target these two denominations. The experts encounter the highest number of fake notes for denomination of 500. In reflec-tion to the real scenario, the data set considers 500 and 1000 denominations in a ratio of 7:3.

The experiment considers 1000 samples (genuine: 500 and fake: 500). All of these are not whole currency notes. Availability of genuine samples is not a problem, whereas getting fake note samples are difficult because of security issues. The forensic experts provided us some fake as well as genuine samples. We extracted multiple samples from a sin-gle currency note. From Fig. 1 , one may understand that in a note, there are many regions correspond to a particular secu-rity feature. For example, samples for intaglio printing can be found in several regions of a banknote. However, regions were selected carefully so that a sample must have all the security features considered in this experiment. Each sample is tagged with its class (genuine or fake). 4.2 Capturing conditions In our experiment, four different scanning techniques: (i) UV scan, (ii) latent, (iii) flood I and (iv) flood II, are used to image a currency note, using a VSC 5000 (Visual Spectral Com-parator) [ 36 ]. It uses a spectrometer and other built-in light sources (ultraviolet, infrared Table 1 on top of visible spec-trum light sources) installed specifically to capture images for the task at hand. This type of machine is specially tuned for scanning banknotes, resulting in properties a simple flat bed scanner or a simple photocopier fail to achieve. Anti-scan lines over the notes corrupts the output image from simple scanners. These anti-scan lines are deliberately incorporated as a design feature, to prevent cheap duplicates from being circulated. An added advantage of using a specialized high-resolution scanner is that it nullifies effects due to ageing or tempering of banknotes, and we need not enhance the image as a part of the preprocessing process.

Different regions of a note are scanned using different techniques. For example, the central design of the note appears prominent under ultraviolet (UV) light; hence, this region is scanned with UV. There are latent marks on the side of a note. The parameters of the cameras used for imaging are giveninTable 1 .AsexplainedinSect. 3 ,theproposedmethod captures several features in order to authenticate a note. Not these features are extracted from every image. Four different scans resulting in different features are listed in Table 2 .Itis noted that while scanning different regions of a note under different capture conditions, we have used only one wave length of the light source. This leads to a relatively low-cost adaptation of our proposed method as most of the commer-cial systems and patents (as discussed in Sect. 3 )haveused high-cost multiple light sources of different wavelengths. 4.3 Experimental protocol The authentication of notes is modelled as a two-class classi-fication problem: discrimination of genuine (denoted by G ) vs. fake (denoted by F ). The reason being that the end users are interested in only knowing whether a note is fake or gen-uine. Different groups of features are taken into account to accomplish this classification task. The feature groups, as discussed in the previous sections, are based on ink, thread, artwork and printing techniques. It may be noted that though the problem is considered as two-class problem, our mail goal is to model the concept of genuineness. Fake samples are considered as negative samples, and as we do not assume any prior knowledge about how the fake notes are generated, we consider them homogeneous instead of heterogeneous samples. Rather, using a prior map of the source to their tar-geted method of forgery would fail to take into account that these groups are quite dynamic and can change from one method to another.

An initial clustering (bi-clustering) is done on the individ-ual feature groups to visualize their degree of separability in feature space. Classification is done using both support vec-tor machines (SVM) and artificial neural network (ANN). Fourfold cross-validation is followed using both the SVM-and ANN-based classification. The data set is divided in a 2:1:1 ratio to generate training, validation and test sets. The relative robustness of the individual features is measured by linear discriminant analysis (LDA) to provide a comparative performance analysis of the feature groups.

Classification with SVM makes use of two kernels (poly-nomial and RBF kernel). Mean squared error (MSE) is computed as follows: MSE = and T are the number of support vectors and test samples, respectively, d ij and y ij are the desired output and the SVM output, respectively, for the i th support vector and the j th test sample.

Three different sets of neural network classifiers are used for each feature group. Input nodes in each of these networks depend on the number of features, i.e. ink work (three nodes), artwork (four nodes), printing (nine nodes). Each of the net-works has one hidden layer and two output nodes. The main goal was to realize a simple architecture that would be fast and reliable. In the present form using one hidden layer, we got impressive accuracies. Therefore, we did not look for higher abstraction of the data using a multi-layer perceptron model. It can be argued that that the query time would not be affected much. However, at the scale of operation which we would operate, Indian economy, small optimizations would result in a huge improvements. The normal back propagation algorithmhasbeenusedfortrainingwithasigmoidactivation function given by f ( x ) = e x 1 + e x .

Next, we use Fisher linear discrimination analysis (LDA) [ 37 ] to study the performance of individual features and their impact on the classification process. The idea is to rank the features based on their degree of separability, being measured by the margin perpendicular to the discriminant hyperplane.
Finally,thecumulativeaccuracytakingallthefeaturesinto consideration is computed and a study is conducted to find out a proper sequence in which the features are to be tested. This is important as this reduces the load of the machine if at various stages the number of notes to be checked could be reduced without sacrificing the accuracy. 4.4 Experimental results At first, results are reported based on individual feature groups. The capacity of these features are then analysed in the context of detecting fake notes. Finally, results are reported considering all the features and a sequence by which fea-tures are to be checked is highlighted in order to authenticate a banknote in question.
 Accuracy of ink-based features There are three ink-based three features are shown in Table 3 . Table 4 reports the classi-fication results when SVM and ANN, respectively, are used as classifiers.
 Accuracy of artwork-based features There are four features which are captured from the note artwork: f 1 a , f 2 a , f f a as discussed in Sect. 3 . Bi-clustering results using these features are shown in Table 5 . Classification accuracies by SVM and ANN are reported in Table 6 .
 Accuracy of printing-based features A comprehensive study of the printing-based features ( f 1 p  X  f 9 p ) is presented in Table 7 .

Inspiteofthehighaccuracyoftheindividualfeaturegroup more than one feature group makes the system more robust. This makes it more immune to the counterfeiter X  X  effort, even if the fake note generators happen to develop means to sur-pass the security measures of any particular feature set. Use of other features would help in maintaining the high perfor-mance of the system.

With currency notes, we are extra careful in making the system conservative so that no fake currency passes through the system. The two thread-based features ( f 1 t , f 2 t fore used upfront that are binary in nature and discriminate notes based on either the presence or absence of two fea-tures [namely registration ( f 1 t ) and text( f 2 2 ) as defined in Sect. 3.2.3 ] into genuine (G) and fake (F). This reduces the processing time too which we have analysed next.
 Recognition accuracy of the complete system The perfor-mance of individual feature group is quite good but not perfect. Now all the features are taken together to test the system. Individual feature groups are selected in different sequence in order to optimize the performance of the sys-tem in terms of accuracy, speed and computational overhead. The Table 8 shows that the system operates fastest if the thread-based features is placed in the beginning. A total of 1000 samples (500:G, 500:F ) were used to compute the speed of the system. The time reported here comprises the time required to scan the notes using the four scanning methods mentioned in Table 1 , for registration to figure out which part of the image should be fed to which module and finally to execute the authentication framework. There is a marked improvement in time by using thread-based features as the very first step in processing. However, observe that the remaining modules are independent of each other, so to further improve time, we parallelize the system as shown in Fig 11 . A final decision is taken by using an AND gate on the decision vectors got from the various classifiers. This ensures that whatever passes as genuine has an uncontested majority vote. Even a single negation would result in the note being rejected.
 Relative performance of the feature group The ability of the three feature groups, namely Ink, Artwork and Print-ing technique-based features, in detecting fake currency notes is analysed using Fisher linear discrimination analy-sis (LDA) [ 37 ]. The previous Sect. 4.3 highlights how we use LDA for this purpose. The projection of the individual feature groups is taken on the best discriminant plane and fur-ther mapped to show the separability of the feature groups in a 2D plot. Figure 12 shows the results.

The ROC curve shows that printing technique-based fea-tures are more robust and reliable among the three feature groups. They are followed by artwork and finally ink-based features. The steep rise in ROC curves depicts that the indi-vidual feature groups by themselves are extremely robust but not perfect. We also observe that the true positive rate quickly shoots to 1 (ideal) with a very low fraction of false positive.
A classifier X  X  performance is judged on the basis of how close the precision recall curve Fig. 13 is compared to the (1, 1) mark which is considered ideal performance of a classifier. We see that all the three feature groups are very close to that mark.

Finally, performance of the individual features on a 0 X  100% accuracy scale is reported in Fig. 14 . However, it may be noted that the study on the individual feature strength presented here strongly depends on the samples that are used during the training process, and hence, the uniformity of the result shown in Fig. 14 is difficult to prove in general. We added this result as the relative performance of the features may give important information to the note designers as to which feature is performing the best and which feature is more susceptible to counterfeiting attack. An automatic method for authentication of currency notes has been described. This research is particularly important whentheproblemoffakebanknotesisconsideredasaserious problem in many countries. The present experiment consid-ers Indian banknotes as a reference. This study investigates how the security features can be computationally captured in order to automate the authentication process. Exhaustive evaluation of the method using real-life samples supports the potential of the approach.

We would also like to point out that common patterns in forgery can be studied to recognize different methods involved in counterfeiting. This would be handy for the law enforcement agencies, and they would be aware of the pres-ence of a certain group of criminals by observing the nature of fake. We do not explicitly address this issue here, as we feel this is outside the scope of the present paper. The complex-ity of the overall system is kept minimal so that a low-cost hardware realization of the proposed method is feasible. A low-cost system is in demand so that a large-scale deploy-ment of such a system becomes possible. For this purpose, we are in touch with a few companies who are interested in prototyping such a system. Some algorithmic optimiza-tion may be needed for embedded realization of the present system.

Another immediate extension of this study is to evalu-ate the method on a different test set. We are in process of collecting a new set of samples from another laboratory (dif-ferent from the one from which we received the current data set) of the Department of Forensic Sciences. Exploiting new features and methods for authentication is indeed needed to make the system robust against future counterfeiting efforts. In fact, the present study does not consider one important security feature, namely the watermark feature of the cur-rencynotes.Thereasonbehindthisreferstothestrangehabits of Indian people scribbling by ink pen over the blank region on the note where watermark is embedded. Such scribbling marks make the use of the watermark feature very sensi-tive [ 38 ]. Our future effort will explore how to get rid of such scribbling marks and use the embedded watermark as one of the security features. Dating banknotes (specially the fake ones) using ink age [ 39 ] or some other features [ 40 ] can also be studied in future.

