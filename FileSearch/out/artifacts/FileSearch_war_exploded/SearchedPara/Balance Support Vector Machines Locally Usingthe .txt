 Measuring the degree of similarity between two examples is a critical task in machine learning. Many positive definite kernels used in the support vector ma-chines (SVM) can be considered as simila rity measures, such as the linear kernel k
LIN ( x 1 ,x 2 )= x T 1 x 2 , the RBF kernel k RBF ( x 1 ,x 2 )=exp( histogram intersection kernel [14] However, these kernels which only tak e into account the two features vectors x 1 and x 2 might provide misleading similarity sc ores, especially when the intra-class variation is large. Fig. 1 illustrates this idea. In the middle row of Fig. 1, the digit  X 7 X  (example x 2 ) is the nearest neighbor of the digit  X 2 X  (example x 1 )when the RBF kernel is used.

A kernel that only compares pairwise similarities is susceptible to such unde-sired mismatches. However, Fig. 1 also illustrates that the structural similarity can greatly help remove the ambiguities. We use the notation x 1  X  x 2 to indi-cate that x 2 is among the nearest neighbors of x 1 . The first row in Fig. 1 shows the other ten nearest neighbors of x 1 ( X 2 X ) excluding x 2 ( X 7 X ) in the decreasing order of similarity. And the third row shows the nearest neighbors of x 2 ( x 1 is not in the neighborhood of x 2 in this example). Since x 1 and x 2 do not share any common neighbor (i.e., x 1 and x 2 are exhibiting completely different link-ing structures in the neighborhood graph), they are not similar in terms of the structural similarity. A combination of the pairwise and structural similarity will generally improve the classification of examples.

In this paper we propose to use bibliographic coupling strength [6] to measure the structural similarity of examples x 1 and x 2 , i.e., counting the number of examples that are nearest neighbors of both x 1 and x 2 . One advantage of this simple structural similarity measure is that we can encode a local graph as a sparse neighborhood vector. By appe nding the neighborhood vector to the original feature vector, existing SVM training and testing techniques can be performed without any change if the linear or histogram intersection kernel is used. The similarity of two examples, though, becomes a combination of both the pairwise similarity and the structural similarity.

We further prove that when the linear or histogram intersection kernel is used, graph structural similarity is equivalent to adding a regularization term that en-courages local balance . Given a set of labeled examples ( x i ,y i ) ,i =1 ,...,n , y  X  X  X  1 , +1 } ,aconstraint dual SVM problem if a bias term is used. We will call this constraint the global balance constraint. Instead, structural similarity leads to the minimization of s 2 ,where s =[ s 1 ,...,s n ] T and s j = i : x ance term because it measures the balance of  X  in the small subset of examples that emit edges toward x j . SVM classifiers usually suffer from imbalanced data distributions. When one class (the minority class) has much less examples than another class (the majority class), the minority class accuracy is generally sig-nificantly lower than that of the majority class. SVM with structural similarity leads to similar performances in both the minor and the major classes because of the locally balanced property. We also show that the local balance term s is a natural supervised learning extension of the authority scores in the unsupervised HITS algorithm [9].

Experimented on 6 benchmark datasets , the proposed locally balanced SVM with structural similarity shows impro ved accuracies in most datasets than nor-mal SVM. In imbalanced problems, the proposed method can increase the minor-ity class accuracy by up to 100%, without harming the majority class accuracy .
The rest of this paper introduces the str uctural similarity kernel (Sec. 2), the local balance property (Sec. 3), and the s igned authority score interpretation (Sec. 4). After discussing related researc h in Sec. 5, experiments (with discussions of limitations of the proposed method) are presented in Sec. 6. Bibliographic coupling was originally proposed for comparing scientific docu-ments [6]. For documents p and q , it is defined as the number of documents cited by both p and q , based on the intuition that p and q are closely related if they share many common bibliography items. A similar intuition applies to structural similarity: if the set of nearest neighbors of x 1 and x 2 share many common members, then it is highly probable that x 1 and x 2 are in the same class.

The nearest neighbor relationship , however, depends on a training set X = { x 1 ,...,x n a data point x i corresponds to a node in  X  X , and a directed edge from x i to x j is created if and only if x j is in the k-nearest-neighbors of x i (denoted as x  X  x in which | X | is size of a set. Note that k STR is a data-dependent kernel  X  it is defined with respect to the training set X .Inordertouse k STR in an SVM, we need to prove that it is a positive definite kernel. Let G be the adjacency matrix of  X  X , i.e., G ij equals 1 if x i  X  x j , and 0 if otherwise. It is easy to see that K
STR = GG T ,where K STR is the kernel matrix satisfying K STR
Alternatively, we can define a neighborhood vector n ( x ) for any sample x . neighbors in X of x , and 0 if otherwise. The neighborhood vector n ( x )and x can be concatenated into an augmented vector a ( x )= augmented vector, we can easily combine the structural similarity and pairwise similarity, because k STR ( x i ,x j )= n ( x i ) T n ( x j ) and consequently It is also possible to trade-off the importance between the two similarity terms by defining a ( x )as a ( x )= Algorithm 1. SVM learning algorithm utilizing the structural similarity
Note that n ( x )equalstoonerowof G if x  X  X . However, n ( x ) is well-defined even if x  X  X .Thus k STR can be easily applied in inductive learning methods. For example, in the SVM learning we first construct a ( x ) for every training example x . We can then train an SVM using a ( x ) instead of x . During the testing phase, any testing example q is converted to a ( q ) and the trained SVM model can be readily applied. The SVM training and testing method that utilizes structural similarity is summarized in Algorithm 1.

It is worth noting that any kernel or distance measure can be used to find the nearest neighbors and to generate the augmented vectors a ( x ) (line 3 and line 6 of Algorithm 1). It is not necessary to use the same kernel to generate the augmented vectors and to train the SVM (line 4 of Algorithm 1).
 k j min( x 1 j ,x 2 j ) is able to combine structural similarity with pairwise similarity. The RBF kernel, however, does not have this property. The structural similarity kernel k STR can play several roles i n SVM classification. Besides adding structural similarity in addition to pairwise similarity (and in-troducing non-linearity into linear SVM classification), it also introduces a local balance heuristic to the dual variables.

Given a set of labeled training examples ( x i ,y i ) ,i =1 ,...,n ,asoftmargin linear SVM with the optimal separating hyperplane w T x + b =0solvesthe following optimization problem [3]: The primal form Eq.4 has a corresponding dual form where y =[ y 1 ,...,y n ] T , e =[1 ,..., 1] T , K ij = x T i x j ,and is the element-wise product operator. We will refer to Eq. 6 as the global balance constraint , as it specifies that the sum of dual variables in the two classes must be equal to each other in the entire training set. As observed in [16], in an imbalanced problem (where the majority/negative class has much more examples than the minority/positive class), Eq. 6 usually leads to more support vectors in the majority class than in the minority one. It is stated in [16] that a positive example close to the boundary are more likely to be classified as negative due to the global balance constraint Eq. 6.

When we train on the augmented examples a ( x i ) instead of x i , the dual objective function now becomes (with the constraints remain unchanged) wherewedefine s = G T (  X  y ). In addition to minimizing the usual SVM objective function f (  X  ), the term s 2 imposes another heuristic that encourages s j to be 0 (or small) for all j =1 ,...,n .

Since it measures the balance in the local neighborhood { x i : x i  X  x j } related to the example x j .Inotherwords, s j = 0 means that the dual variables in this local neighborhood is balanced between the positive and the negative class, which we term as the local balance . Local balance is a desired property when the training set is imbalanced. If the local balance property is strictly satisfied (i.e., s j =0 for all j ), it is guaranteed that for any support vector in the negative class, there will be at least one support vector in the positive class in a local neighborhood. The weights (dual variables  X  i ) of these support vectors are balanced. Thus, even a positive test example near the boundary will not be dominated by negative examples.

The global balance property Eq. 6 can b e safely neglected if local balance is strictly satisfied for all support vectors. These two balance properties are linked by the following relationship: The last equality in Eq. 9 uses the fact that  X  X is a kNN graph and the out-degree of any node x i is k .When s j =0forall j (which implies a less strict condition j s j = 0), global balance is automatically satisfied.

From now on we will only consider support vector machines that do not have the bias term b (and consequently without the global balance constrain Eq. 6). If a bias term is needed, one can append t o each example an additional feature dimension with constant value 1.
 The form of the definition s  X  G T (  X  y ) reminds us of the equation that turns hub scores into authority scores in the HITS algorithm [9]. This resemblance leads to an alternative view of Algorithm 1.

The HITS algorithm finds authoritative sources in a hyperlinked environment (i.e., directed graph). For example, fo r a text-based web search  X  X ar maker X , authorities are those nodes (webpages) that contain useful information about car manufacturers. Authoritative nodes are expected to have a large number of incoming edges. Hubs are defined in [9] as those nodes that have many outgoing edges. The authority score v and hub score u of all nodes are then iteratively updated by the relationship v = G T u and u = Gv ,where G is the adjacency matrix of the d irected graph.

The HITS algorithm is an unsupervised method. In our supervised learning settings, the vector  X  y acts as signed hub scores. Th e optimized dual variables  X  contain the weights or relative importance levels of the training examples. This vector is analogous to the hub score vector u in the HITS algorithm. The vector  X  y is a natural extension of the hub scores into supervised learning. The local balance vector s = G T (  X  y ) is then a good candidate for the supervised extension of the authority score vector v .

Let us consider a simplified problem where we only take into account the structural similarity. An example x i is converted into the neighborhood vector n ( x i )= G i : ,where G i : is the i -th row of the adjacency matrix G . A linear SVM using n ( x i ) as training examples will lead to a classification boundary that is In other words, the local balance vector s equals the classification boundary w if we only consider the structural similarity. In this scenario, the local balance vector s contains in effect the signed authority scores learned through the SVM optimization. When a test example q is given, the neighborhood vector n ( q )isa sparse vector with the value 1 in j -th position only if x j is within the k nearest We just need to find the k nearest neighbors of q , and they will each contribute a local signed authority score s j . The classification of q is then determined by the sum of these k local signed authority scores. This fact exhibits an important property of structural similarity in SVM learning: during the testing time, only the nearest neighbors of the query q will affect the classification result. Support vectors that are far away from q will not affect the decision on q . This property is advantageous in learning imbalanced problems.

When we use the complete augmented vectors a ( x i )=[ x T i n ( x i ) T ] T to train a linear SVM, the last n dimensions of the resulting classification boundary will The second term can also be expressed in ter ms of local signed authority scores. s Local geometry has long been utilized in machine learning, e.g., in (unsupervised) dimensionality reduction (LLE [12]). A neighborhood graph (in which nodes and edges denote examples and similar example pairs respectively) is a common way to utilize the local geometry information (e.g., PageRank [10]). In Kleinberg X  X  HITS algorithm [9], link analysis techniques are applied on the neighborhood graph to find authoritative and hub webpages in an iterative and unsupervised manner (c.f. Sec. 4). In webpage classification, natural link structures exist and bibliographic coupling has been used to extract features [11].

Linking structures are particularly useful when the pairwise similarity com-puted only using the feature vectors of two examples are not accurate. For exam-ple, in computer vision and image analysis, visual features frequently suffer from the fact that their intra-class variation is bigger than the inter-class variation. Graph structural similarity have been successfully used to unsupervisedly rec-ognize objects [7] and localizing regions of interest [8]. The structural similarity in [7] was computed using a method that combines both bibliographic coupling and co-citation (refer to [1]), and is computationally more expensive than Eq. 2. The co-citation count between nodes x i and x j is the number of nodes that have edges pointing to both x i and x j , i.e., reversing the edge directions of the bibliographic couping count in Eq. 2.

Graphs that encode local g eometry also form a majo r thread of methods for semi-supervised learning [19]. In semi-s upervised learning, the nodes are exam-ples (may be unlabeled) and edges connect similar pairs of nodes. A graph is usually coupled with a function f which must be close to the labels of labeled nodes, and is smooth on the entire graph [19]. Thus the graph is acting as a smoothing constraint for the problem.

In particular, a global smoothness functional based on normalized co-citation counts was used in [18]. Another closely related semi-supervised method [13] defines a family of data-dependent norms, which can warp the structure of a reproducing kernel Hilbert space to the g eometry of both labeled and unlabeled data. Thus a supervised kernel learning method can be turned to learn from both labeled and unlabeled data, and can be applied to test unseen examples. The smoothness assumption in [13] is implemented through the graph Laplacian, and requires inverting of a n  X  n matrix where n is the total number of examples. 6.1 Setup Experiments on 6 datasets are used to test the proposed method. These datasets are ijcnn1 , protein , satimage , shuttle , splice ,and vowel .Weusedthe scaled version of these datasets downloaded from the LIBSVM dataset page. 2
Baseline SVM accuracies are provided using the RBF, linear, and histogram intersection kernel (columns rbf , lin ,and hik in Table 1, respectively). SVM-weight with RBF kernel is used as a baseline imbalanced learning SVM method (column rbf-w ), which (although simple) has been shown to be very effective in imbalanced learning [15]. In SVM-wei ght, different costs are associated with where n + and n  X  are the number of examples in the minority and majority classes, respectively.

As the structural similarity kernel can be seamlessly combined with the lin-ear and histogram intersection kernel (Algorithm 1), the proposed method is presented for these two kernels. Since t he kernel (or distance measure) used to generate a kNN graph needs not to be the same as the kernel used in SVM training, we use a notation  X  X +Y X :  X  X  X  represents the kernel for generating the neighborhood kNN graph and augmented feature vectors, and  X  X  X  represents the kernel for subsequent SVM training. Note that the  X  parameter in the RBF ker-nel will not affect the kNN graph when  X  X  X  equals RBF. In this case, we simply ignore the exponential function and the  X  parameter when building a kNN graph for better numerical stability.

The LIBSVM package [2] is used for RBF kernel SVM (which uses the 1-vs-1 strategy for multi-class classification). The LIBLINEAR package [5] is used for linear SVM methods. Although the histogram intersection kernel (HIK) is not a positive definite kernel for real-valued feature vectors. It is proved to be a positive definite kernel when feature values are non-negative numbers, and achieves better performances than the lin ear kernel in most cases [17]. For HIK SVM, we linearly quantize all feature values into the range [0 100], and use the fast HIK SVM method proposed in [17]. The 1-vs-all strategy is used in both LIBLINEAR and [17]. SVM parameters ( C in all methods, and  X  in RBF SVM) are searched using 5-fold cross-validation on the training set, in the range log 2 C  X  [  X  11 15] and log 2  X   X  [  X  11 3], with the search grid size being 2.
In an augmented feature vector a ( x )=[ x T  X n ( x ) T ] T , the parameter  X  con-trols the trade-off between the pairwise similarity and the structural similarity. In linear SVM, we simply set  X  = 1. In HIK SVM, the features x are quantized to [0 100], and we set  X  = 10. The number of nearest neighbors k is set to 10 for all experiments. 6.2 Results Three accuracy numbers are reported for every method. In every cell of Table 1 the first number is the overall accuracy (n umber of correctly predicted examples divided by total number of testing examples, acc ) in percentage. The second number is the arithmetic average ( a-mean ) accuracy of all classes (i.e., average of the diagonal entries of the confusion matrix). The third number is the geometric average ( g-mean ) accuracy of all classes, which i s a commonly used performance measure in imbalanced learning.

Note that g-mean is always less than or equal to a-mean . And we usually ob-serve that g-mean &lt; a-mean acc in imbalanced learning problems. Although there are more sophisticated statistics to measure imbalanced learning perfor-mance (e.g., area under the Prevision-Recall or ROC curve [4]), the two simple average accuracy measures are sufficien t to clearly show the differences in Ta-ble 1. Also note that both the arithmetic and geometric mean apply to problems with multiple classes.

AsshowninTable1, incorporating the structural similarity improves SVM classification accuracies in most cases , no matter whether a linear or histogram intersection kernel is used. In the lin ear SVM, the neighborhood vectors n ( x ) extracts non-linea r features from x and increases the capacity of the classifier. The mean accuracies on the 6 datasets in creases from 76.77% (linear SVM) to 82.16% by adding structural similarity (RBF kNN graph plus linear SVM). The rbf+lin method achieves higher accuraci es than a simple linear SVM in all datasets and all three measures. Using t he linear kernel to generate kNN graph and structural similarity is not as effective as the RBF kernel. However, lin+lin still achieves higher accuracies than lin in 4 out of 6 datasets. It is worth noting that lin+lin usually outperforms lin by a large margin (4-7%). But when lin have higher accuracies than lin+lin , the differences are usually only about 1%.
Similarly, structural similarity also boosts performance of the HIK SVM. Both rbf+hik and hik+hik outperforms hik in all datasets and all performance mea-sures. The absolute level of improvemen ts by structural similarity in HIK SVM, however, is smaller than that in linear SVM. One reason might be that the HIK SVM itself has higher discriminative power than the linear SVM. It is worth noting that rbf+hik has better acc performances than RBF SVM in al-most all datasets, except in the ijcnn1 dataset where rbf+hik is slightly worse. In fact, when structural similarity (RBF kNN graph) is used, even the linear SVM performs closely to the RBF kernel SVM.

Structural simila rity, however, is most effective in dealing imbalanced prob-lems . In plain SVM methods, the a-mean measure is usually much higher than the g-mean measure. After equipped with the structural similarity, the SVM classifiers have similar a-mean and g-mean values. For example, in rbf+hik ,the difference is smaller than 1%. This phenomenon means that the accuracy in different classes are close to each other, based on the inequality of arithmetic and geometric means, i.e., the proposed imbalanced learning strategy is indeed effective . The average a-mean measure is improved from 61.66% (linear SVM) to 78.79% ( rbf+lin ), and from 76.17% (HIK SVM) to 81.91% ( rbf+hik )inthese datasets. We can observe even larger improvements of the g-mean measure in Table 1.

The shuttle dataset is the most imbalanced one, with only 2 training ex-amples in the smallest class and 11478 examples in the largest class. The RBF kernel SVM has an a-mean accuracy of 69.02%, in which the 4 classes with less than 50 training examples are almost completely misclassified (in consequence the g-mean measure is 0.). The linear and histogram intersection kernel are not effective in dealing with this highly imbalanced problem either. Structural simi-larity, however, successfully recognizes examples from these minority classes. For example, rbf+hik has an a-mean of 95.23% and a g-mean of 94.83%. Specifically, lin has 0% accuracies on all these 4 minor ity classes. However, the accuracy of rbf+lin for these 4 classes are 76.92%, 94.87%, 75%, and 100% , respectively.
In Table 1 we also compare the proposed method with SVM-weight, a simple but effective imbalanced SVM learning method. The rbf+hik method outper-forms rbf-w in all three performance measures. Although SVM-weight is also effective in balancing the accuracies betw een the minority and majority classes, it has lower average a-mean and g-mean values than rbf+hik .Wewanttoem-phasize that imbalanced learning methods such as SVM-weight usually increase measures such as g-mean at the cost of a reduced accuracy acc . However, the proposed method carries out effective imbalanced learning without hurting the classification accuracy acc .Infact, rbf+hik has a higher average acc value than that of SVM classifiers with linear, histogram, and RBF kernels, and the SVM-weight classifier.
 We end our discussions about Table 1 with a note about the splice dataset. This dataset is almost balanced (the number of examples in the minority class is close to that of the majority class). rbf-w and rbf achieve exactly the same accuracies on this dataset. However, the similarity kernels can still improve all the three performance measures over the plain SVM in rbf+lin , hik+hik ,and rbf+hik . 6.3 Discussions of Limitations consuming step to construct a kNN graph. At the testing phase, finding the k-nearest neighbor of a query is also c omputationally expensive. On the other hand, because fast SVM training algorithms are available for both the linear and the histogram intersection kernel SVM, the time for cross-validation parameter selection and SVM training can be signifi cantly reduced. For example, the total time for parameter selection, training, and testing is 4524 seconds for RBF ker-nel SVM on the shuttle dataset. The total time for rbf+lin and rbf+hik are only 769 seconds and 1028 seconds, respectively.

Another issue is the choice of  X  . A too small or too large  X  will effectively reduce the augmented vector a ( x ) to either the original feature vector x or the neighborhood vector n ( x ). Although the experiments in Table 1 indicates that the default choice of  X  leads to reasonable results, it is worthwhile to learn a suitable  X  for each dataset. For example, x and n ( x ) can be used to construct two kernel matrices separately. By formulating structural similarity SVM as a multiple kernel learning problem, MKL methods can be used to learn the value of  X  . An additional advantage is that kernels besides the linear kernel and the histogram intersection kernel can be used together with the structural similarity kernel (e.g., X+rbf ). A structural similarity kernel is presented in this paper for SVM learning, and in particular when the problem is imbalanced. Kernels like the RBF kernel com-putes the similarity of two examples using only the feature vectors of them. However, after building a neighborhood graph (kNN graph), the linking patterns of two examples (nodes) convey useful information about how similar they are. The proposed structural similarity kernel captures this structural similarity by computing the bibliographic coupling count, and its feature space representation corresponds to rows of the adjacency matrix of the kNN graph. The structural similarity kernel is a data-dependent kernel.
 The structural similarity kernel can be s eamlessly integrated into the linear SVM or the histogram intersection kernel SVM. We show that it is equivalent to adding a regularization term that encourages balanced weights in all local neighborhoods defined by incoming edges in the kNN graph. Analogous to the unsupervised HITS algorithm, the structural similarity kernel turns hub scores into signed authority scores, and is particularly effective in dealing with imbal-anced learning problems. Experimental results on several datasets show that structural similarity can h elp linear and histogram in tersection kernel to match or surpass the performance of the RBF kernel in terms of classification accuracy. When the problem is imbalanced, structural similarity can significantly improve imbalanced learning performance measures such as g-mean , while at the same time it still maintains high classification accuracy.

Further research that will improve the structural similarity kernel SVM in-cluding at least two directions: a fast (maybe approximate) way to create the kNN graph, and the use of multiple kernel learning to learn an appropriate  X  parameter value.
 Acknowledgement. J. Wu is supported by the NTU SUG grant and AcRF Tier 1 grant RG 34/09.

