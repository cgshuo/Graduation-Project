 XML has become a standard for data exchange in web application development. With the development of web and the wide usage o f XML, search or query in the centralized solution cannot get satisfied result timely. To efficient manage these huge of XML data, distributed processing techniques for storing and managing the XML data in P2P or traditional distributed database techniques are developed [1,2].

Nowadays, cluster computing is broadly applied to the data explosion problem, which brings up an efficient solution by dividing the big data into small parts and processing in parallel on thousands of computers. The basic computing framework of cluster comput-ing is MapReduce, which is a distributed programming model and software framework for rapidly writing app lications that process vast amounts of data in parallel on thou-sands of cheap commodity computers. It has a ttracted much attention from many fields for processing and analyzing huge volumes of data.

Structural similarity search of XML dat a is an important problem in XML data man-agement, which is related to data integration, XML classification and clustering, data cleaning etc. Tree edit distance is used for measuring the structure difference of tree-structured data, however, its complexity is rather expensive even for ordered tree [3]. Recently, Augsten et al.[4] proposes the pq -gram concept for tree structure comparing for the ordered tree. However, these methods are running on single machine, which cannot scale well to large cluster for big corpus.
 In this paper, we propose a filter-and-refine framework on large cluster with the Min-Hashing and locality sensitive hashing techniques for efficient structural similarity search on large XML corpus.

First, pq -gram proposed in [4] is adopted for extracting XML tree-grams, which is flexible for tree data structural similarity comparing. Then Hadoop is used as a plat-form for storing and processing large scale of XML data in parallel. Furthermore Min-Hashing and locality sensitive hashing techniques are implemented on the cluster for efficient searching. Extensive experiments indicate that our framework is efficient and effective for XML structural similarity searching on large cluster and scales well in term of the size of corpus.

To summarize, the main contributions of this paper are outlined as follows:  X  We propose an efficient framework for structural similarity searching of XML data  X  Min-Hashing and locality sensitive hashing are implemented on cluster for efficient  X  Extensive experiments are conducted to demonstrate the effectiveness and effi-The rest of paper is organized as follows. In S ection 2, the preliminaries of problem defi-nition, XML model and MapReduce are introduced. Min-Hashing and locality sensitive hashing are presented in Section 3. Architecture and main algorithms are introduced in Section 4, and experiment evaluations are presented in Section 5. The related work is surveyed in Section 6. Conclusion and future work are presented in Section 7. 2.1 Problem Definition The problem of structural similarity search is to retrieve the top-k most similar doc-uments. Given an XML document corpus D , a query document Q and a similarity function sim , it retrieves the the document set D k = { d i | d i  X  D k , D k  X  X  ,  X  d  X  D\ D
Jaccard similarity is a set based similarity, w hich is a simple and effective similarity measure and defined as sim jacc = | A  X  B | | A  X  B | ,where A and B are two sets. The intuitive meaning of Jaccard sim ilarity is that the more overlapping of A and B , the higher similarity they are. In this paper, Jaccard sim ilarity is adopted as th e similarity measure for the XML structural similarity. 2.2 XML Model and Tree Gram In this paper, an XML document is modeled as a rooted ordered labeled tree. According to this model, element nodes and attributes are considered as structural or internal nodes, and the text value and attribute value nodes are treated as text nodes or leaf node. Only structural nodes are taken into consideration as the structure of XML tree, and the element tag and the attribute node name are considered as the tree node label .
After modeling, the pq -gram proposed in [4] is adopted for extracting tree -gram from XML tree, which is a flexible way to extract the structural information from tree structured data for approximate structural similarity comparing. The pq -gram is a sub-tree of the extended tree for two given integer parameters p and q .
Example of pq -gram of a tree T 1 is shown in Fig. 1. According to [4], for two integers p and q , an extended tree is first built as shown in Fig.1(2), the  X   X   X  nodes represent the extended empty nodes. Parameters of p and q are set to 2 and 3 respectively in the example tree T 1 . The sub-tree enclosed by the dotted line is the first 2,3 -gram of T 1 .
Let g be a pq -gram with the nodes N ( g ) = { n 1 ,  X  X  X  ,n p ,n p +1 ,  X  X  X  ,n p + q } ,where n i the tree node n i [5].
 Example 1. The first 2,3 -gram of tree T 1 is the subtree circled by the dotted line in the Fig. 1(2). After pre-order traversing of the subtree, nodes  X * X ,  X  X  X ,  X * X ,  X * X  and  X  X  X  are visited orderly. The 2,3 -gram tuple is denoted as (*,a,*,*,b).
 An example of pq -gram tuples of T 1 is shown in the Fig. 1(3). The pq -gram tuples of XML Tree T are denoted as G T .The pq -gram tuple universal of the XML corpus is noted as G U , G U = i =1 G Ti .The pq -gram tuple is also called tree -gram or pq -gram if not confused in the following paper. Assume the universe of pq -gram tuples of the corpus are sorted lexicographically. Consequently, the structure of XML tree can be represented by a binary vector with dimension | G U | .
 Definition 1. pq -Gram Binary Vector T , c i =1 iff g i occurs in G T ,otherwise c i =0 .
 Example 2. In the example trees T 1 and T 2 of Fig. 1, vectors (1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0), (1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1) are 2,3 -gram binary vectors respective to T 1 and T 2 . 2.3 Hadoop and MapReduce Hadoop is the core of Apache Hadoop project [6]. It is an open-source implementation of frameworks for cluster computing, which consists of the Hadoop Distributed File System (HDFS) and MapReduce. HDFS is a scal able and reliable data storage system for storing the file in the distributed file system, and MapReduce is a data processing framework for distributed parallel computing.

The MapReduce programming framework is designed for fast developing applica-tions which run on large master-slaves shared-nothing cluster in parallel. It is the ba-sic infrastructure for cluster computing for data-intensive problems. It includes two primitives: map and reduce [7]. The map primitive processes key/value pairs and generates intermediate key/values .The reduce primitive merges all the intermediate key/values of the map output results with the same key . Finally the results are output to the HDFS.

The processing architecture of MapReduce is outlined in Fig. 2. First, files to be processed are divided into equal splits. Each split is assigned to a map task. The inputf ormat of the application parses the data in each split into key/value pairs as the map  X  X  input. The intermediate key value lists are obtained and sorted on each node after map task. Then hash-based partition procedure divides the intermediate key value list into R groups according to the key on each data node. Next, the reduce task is scheduled for merging value with the same key . Finally, the results are output into HDFS.

The advantage of MapReduce is its simple programming interface, which can be run on shared-nothing cluster without worrying about the details of parallel distributed tasks, such as task coordinating, load balancing and data storage problems etc. The structural similarity measure between XML documents is evaluated by tree-gram Min-Hashing. (MH) is introduced in [8] and widely used in duplicate detection [9] etc, which creates compact signatures for sparse binary vectors such that the similarity can be effectively measured by their signature s. Min-Hashing is an easy way to implement min-wise independent permutation [10], which has the property that the probability of two sets have the same value of Min-Hashing is equal to their Jaccard similarity. The formal definition is given as follows.
 Given a random hash function h : x  X  I ,where x is an integer, and I  X  N ,the Min-Hashing function is defined as m h ( v )= argmin { h ( v [ i ]) | v  X  V b , V b is binary vector set, v [ i ] is the i -th component of v and v [ i ]=1 , 0  X  i  X | v | X  1 } . According to the property of Min-Hashing, given two XML documents d 1 , d 2 , the structure of which is represented by the tree-gram sets G T 1 and G T 2 and their tree -gram binary vector are v 1 and v 2 respectively, their structural similarity can be obtained by Eq. 1. Min-Hashing is an approximate procedure for evaluating the set similarity. Thus a ran-dom hash family H : V b  X  I is used for improving the efficiency of retrieval. Given a hash family H , n Min-Hashing signatures are com puted for each document. Thus the binary vector d v of the document d is transformed into Eq. 2 After generating n Min-Hashing value for all the documents, a signature matrix is ob-tained, which is a compact representation of the original binary document-gram vec-tors. Then pairwise similarity can be evaluated by the signature matrix. According to the property of Min-Hashing, Eq. 3 can be obtained.
 Locality Sensitive Hashing. (LSH) is introduced in [11] and widely used in nearest neighbor search for high dimensional data, approximate KNN query etc, which focus on pairs of signature of the underlying similar sets. The scheme of locality sensitive hashing is a distribution on a hash function family H , which operate on a collection of The basic idea of LSH is to hash the objects using H , and ensures that for  X  h  X  X  ,the more similarity of the objects, the higher probability they are hashed to the same bucket. The LSH is a filter-and-refine framework for retrieval. For the retrieval precessing, the candidates of similar objects can be retrieved in the collision buckets, and the non-similarity objects are filtered out. The real similarity objects can be validated in the candidates set, without the need of sequential examining.

In order to improve the searching efficiency, the signature matrix is divided into b bands with r Min-Hashing signatures each band. Signatures in each band are hashed into B collision buckets, where B is a large number. Suppose the similarity of two objects is s , the probability that their LSH signatures agree in all r Min-Hashing signa-tures of in at least one of b bands is 1  X  (1  X  s r ) b , i.e. the probability that they can be retrieved is 1  X  (1  X  s r ) b .

In our system, r Min-Hashing values are concaten ated for the hash key. The band number and the bucket number make up the key jointly of the MapReduce job. The similarity list in the bucket of each band is considered as the value . 4.1 System Architecture The architecture of the system is shown in Fig. 3, which consists of three parts. The first part is the parsing phase . The structure of XML corpus stored in the HDFS are parsed and extracted into tree-gram tuples using the algorithm in [4] with one MapReduce job. The key of the result is file name and the value is the tree-gram set. The key/value pairs are output to HDFS by the reduce task finally. The processing of parsing is shown in the Fig. 2, where g i is the tree-gram ,f i is the file name.

The second part is the vector building phase .The tree -gram universal G U of the corpus is obtained by the U niversal Generator module in one MapReduce job firstly. Then G U is distributed by DistrbutedCache function of Hadoop to each data node. According to G U and the pq -gram tuple set of the XML documents, the binary document-gram vectors are built in one MapReduce job subsequently.
 The third part is the indexing and searching phase . The index is built in one MapReduce job. Given a binary document-gram vector v , r Min-Hashing values of each vector are computed for each band. Then these r Min-Hashing values are concate-nated to make up the hash key jointly and the keys of all the vectors are hashed into B buckets of each band. The band number and the collision bucket number make up the key of the index, and the candidate file list is used as the value . The first three part are computed in parallel on each data node.

Given a query Q , the collision bucket numbers of each band for the query are eval-uated. Next the candidate set is obtained with the LSH index. Then the similarity dis-tances of Q with candidate sets are computed. Fina lly the results are s orted according to their similarity and returned to the user.

After the introduction of the architecture of the system, main algorithms used in the system are presented in the following section. 4.2 Algorithms Vector Building. The tree-grams are extracted from the XML documents residing on each data node in one MapReduce job using the algorithm from [4] in parallel. The tree-Algorithm 1. map Function for Building Binary Vectors gram sets of the XML corpus are output into HDFS. After that, the tree-gram universal G U is generated in another MapReduce job.
 The map function of the binary document-gram vector building is introduced in Algorithm 1. First G U is distributed to each data node by DisributedCache (line 2). Then the binary vector is built for the XML document (line 3-7). Next the vector is emitted with the file name as the key and the vector as the value in the map . Finally the reduce algorithm outputs the binary vectors into HDFS. The reduce algorithm is omitted here due to its simplicity.
 LSH Indexing. LSH Indexing of map function is presented in Algorithm 2. For each vector v ,the computeM inHash () function compute the r Min-Hashing values for each band. These r Min-Hashing values are concatenated to make up the hash key for each band(line 4). Then the band number and the collision bucket number make up the key of the MapReduce, and the file name as the value are emitted to reduce (line 7). Algorithm 2. map Function for LSH Indexing
Subsequently the reduce task of the LSH indexing merges the file list with the same key together, i.e. the same band and the same bucket number.
 Searching Algorithm. The map function for structural similarity search is shown in Algorithm 3. Given a query Q ,the minHashSimilarityQuery () function searches Algorithm 3. map Function for Structural Similarity Searching the structural similarity XML documents and computes their similarity distances (line 1). Finally the similarity distance list is emitted to the reduce task (line 5).
The minHashSimilarityQuery () is introduced in Algorithm 4. Given a query Q , the getV ector () accesses the binary vector firstly (line 4). Subsequently the collision bucket numbers for each band is computed by computeM inHash () for the query vec-tor (line 5). Then for each collision number, the candidate file list is obtained (line 6-8). The Algorithm 4 can be divided into two phrases. The first phrase is the filtering phrase, which filters out the most of the non-similarity objects (line 2-8), and the second phrase is the refining phrase, which affirms the true valid similarity objects in the candidate set. The similarity distances of Q with each file in the candidate set are evaluated (line 9-12). Finally, the result list is sorted and returned (line 13-14).
 Algorithm 4. Min-Hashing Structural Similarity Search on Hadoop 5.1 Experiment Setup All the algorithms are implemented in Java SDK1.6 and run on Hadoop 0.19.1 and Hbase 0.19.1. The cluster is configured with 1 master node and 6 salve nodes. Each node has a duo core intel 2.33GHz processor, 2G main memory and 180GB disk, which runs on Ubuntu 9.04, and with 1G memory allocated to JVM. The cluster is organized in the LAN with 100.0 Mbps. The XML documents are stored on HDFS. Hadoop is configured with the default and the replication of the data is set to 1. Each data node is configured with 2 maps and 1 reduce. The default parameters of b and r are set to 20 and 3 respectively.
 For XML datasets, data from [13,14] are used. Big XML documents such as psd7003, SwissProt, dblp, nasa and treebank in the datasets are split with the XML Twig [15] to smaller ones with size of 100KB, 200KB, 500KB and 1MB. Finally, the size of the XML corpus is 3.4G and consists of 18015 XML documents. The parameters of p and q of the pq -gram are set to 2 and 3 respectively.

The datasets are divided into 4 groups with different size which randomly choose from the 18015 XML documents. The first group D 1 is 667 XML documents with the size about 100MB. The second one D 2 is 4863 XML documents with the size about 900MB. The third group D 3 is 9382 XML documents about 1.9 GB and the forth group D 4 is 18015 XML documents with 3.4 GB size. 5.2 Time Performance In this section, the performance of the system is studied. We mainly studied (1) Parsing of the XML corpus to generate the pq -gram profile of the XML corpus, (2) Vector building for each XML corpus, (3) LSH index b uilding. For the comparing searching performance, 2 searching schemes are studi ed: (1) Sequential scan from HDFS, which evaluates the similarity by sequential scan from HDFS in one MapReduce job, (2) Using LSH index.
Fig. 4 demonstrates the processing time. The time axis is logarithm time and the time unit is second. Fig. 4 indicates that (1) with the increasing of the corpus size, the parsing time increases almost linearly with the cor pus size; (2) the XML parsing takes most of the time and LSH indexing takes about 150s for 18015 XML documents.
 The query performance comparison is sh own in Fig. 5. The time unit is millisecond. For query performance test, the number of bucket B of each band is configured to 7997. Fig. 5 indicates that the query performance with LSH index is far lower than sequential scan from HDFS, which takes time less than 0.2% of scanning from HDFS. 5.3 Precision For precision experiment, the first group XML corpus dataset D 1 is used. The default bucket number B is set to 7997, and the precision , recall and F-measure are tested. average result is used as the final result.
Figures in Fig. 6 show the precision , recall and the F-measure with different param-eters b and r , r =3 ,  X  X  X  , 6 and b =10 , 15 , 2 0 , 2 5 , 30 . Fig. 6(a) shows the precision is decreasing with the increasing of b and r . Fig. 6(b) indicates that the recall is increas-ing the the increasing of b , however, it decreases with the increasing of r . This figure proves the principle of the LSH that the more bucket, the more relevant are retrieved. Fig. 6(c) demonstrates that (1) the F-measure increases with the band number b before 15, it decreases after 25; (2) it increases with the r .When r =3 and b = 2 0 ,the F-measure reaches peak. From Fig. 6(a) to 6(c), when r =3 and b between 15 and 20, the measures of precision , recall and F-measure can get the best search quality. However, when r = 4 , the search quality is stable with different b , but is much lower than r =3 . Thus, conclusions can be drawn that r =3 and b between 15 and 20 is a good choice for Min-Hashing LSH. 5.4 Scalability For scalability performance test, the search time are experimented with 4 group XML corpus and different number of similarity candidates.

Fig. 7 shows the search processing time with different buckets of per band and differ-ent size of corpus. The time scale of is logarithm time and unit is millisecond. From Fig. 7, conclusion can be drawn that (1) with the size of corpus increasing, the processing time increases, (2) with the number incr easing of bucket of each band, the search time decreases. However, with the increasing o f the buckets, the searching time decreases tiny after a sharp decreasing. Thus a tradeoff should be made between space cost and performance.

Fig. 8 demonstrates the searching time with different corpus size and candidate set, and the time unit is millisecond. 4 queri es are issued for this experiment. Q 1 :Ordi-naryIssue.xml, Q 2 : Proceedings.xml, Q 3 : customer.xml, Q 4 : reed.xml. There are 99, 40, 7, 7 similarity documents respectively in each corpus. The bucket number B is set to 14389. Fig. 8 indicates that with the increasing of the size of corpus, the searching time is increasing. The reason is that the larger of the corpus size, the more candidates need to be probed. This experiment also shows that, the more similar documents in the corpus, the more time it costs for searching. However, the query time Q 1 decreases in D 4 than D 3 , the reason is that the data may be distributed unevenly in the cluster. Fig. 5 and Fig. 8 indicate that our system scales well with the size of the corpus. In this section, the literatures on approximate structural similarity computing of XML data, cluster computing and the application of locality sensitive hashing are reviewed.
Approximate tree structural comparison is extensively studied which can give sat-isfied result. Thus researchers have proposed several approximate methods for XML structural comparison. Joshi et al. [16] proposed bag of xpath based structural similar-ity. Recently, tree-gram based methods are proposed to capture the structural similarity for ordered tree [4,17], which is becoming an efficient and flexible way for tree data similarity evaluation, They also give the bound against the tree edit distance. However, they do not take large scale data collection comparison into consideration.
In term of cluster computing, extensive researches have been conducted on large scale distributed storage and computing n owadays. Such as GFS [18], MapReduce [7], Dryad [19], Hbase [20] etc. These techniques have applied in many fields and have attracted lots of attention. The performan ce and efficiency of these system have been shown by empirical research for big data problem in these fields. The MapReduce as the basic computing framework provides easy operations for processing massive data on thousands of machine for distributed computing.

In the last few years, the locality sensitive hashing and Min-Hashing have been well studied and applied in many fields, such as dup licate detection, large rare association rule mining, clustering, information retriev al etc. LSH is widely used for efficient near-est neighbor searching in high dimension objects etc. Google news [21] employs the Min-Hashing and locality sensitive hashing for personal news distribution. Manku et al. [22] make use of Min-Hashing for detecting the duplicate web page in the web crawler. We propose a filter-and-refine framework for searching structural similarity XML doc-uments on the cluster with MapReduce, whic h is efficient and effective with high search quality. Min-Hashing and locality sensitive hashing techniques are implemented with MapReduce in the framework. Our design gives an efficient solution for large scale XML documents management in the web age. Extensive experiments on real datasets show that our framework is efficient and scales well in term of the size of the corpus for structural similarity searching for XML data. In the web age, data update is very com-mon, so how to manage these huge amount of data incrementally is rather important. In the future, optimization o f searching algorithm and incr emental update functionality will be added to the system. Furthermore, data mining and analyzing functions will be incorporated into the framework for large scale XML data management problems. Acknowledgments. This work is supported by NSFC grants (No. 60773075, No.60925008 and No. 60903014), National Hi-Tech 863 program under grant 2009A A01Z149, 973 program (No. 2010CB328106), S hanghai International Cooperation Fund Project (Project No.09530708400) and Shanghai Leading Academic Discipline Project (No. B412).

