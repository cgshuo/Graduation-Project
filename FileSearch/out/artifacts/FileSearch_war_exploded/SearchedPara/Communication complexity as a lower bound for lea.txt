 Vincent Conitzer conitzer@cs.cmu.edu Tuomas Sandholm sandholm@cs.cmu.edu The study of multiagent systems in AI is increas-ingly concerned with settings where the agents are self-interested. Because in such settings, one agent's opti-mal action depends on the actions other agents take, there is no longer a straightforward notion of when the agent is acting optimally. Game theory is concerned with the study of such settings (or games ). It provides formalizations of games, such as a game in normal (or matrix ) form (where players choose actions simulta-neously), or a game in extensive form (where players may choose actions sequentially). It also provides solu-tion concepts , which, given a game, specify which out-comes are sensible. Examples for normal form games are Nash equilibrium (every player should be playing optimally given what the other players are playing); dominance (when one strategy always performs bet-ter than another, the latter should not be played); and iterated dominance (where dominated strategies are sequentially deleted from the game). Examples for extensive form games include all solution concepts for normal form games, and others such as backwards in-duction (solving the game by working backwards from the last actions in the game). Especially the com-plexity of constructing Nash equilibria has recently received a lot of attention in the AI and CS the-ory communities (for example, (Kearns et al., 2001; Papadimitriou, 2001; Leyton-Brown &amp; Tennenholtz, 2003; Blum et al., 2003; Littman &amp; Stone, 2003)); some computational research has also been done on iterated dominance (Gilboa et al., 1993).
 It is not always possible for the players to immedi-ately play according to a solution concept: often the players need to learn how to play, and will only eventu-ally converge to a solution. There are various reasons why learning may be necessary: the players may not know all of the payo s (or other variables) in the game; the players may not be sophisticated enough to com-pute the solution concept; or multiple outcomes may be consistent with the solution concept, and the play-ers need to coordinate. (In this paper, we will focus on the most common variant of the rst case, where each player only knows her own payo s.) Often, con-straints are imposed on the learning algorithm. One type of constraint is that of rationality : a player should try to maximize her own payo s. The rationality con-straint (when present) takes di erent (nonequivalent) forms, such as requiring convergence to a best response when playing against a stationary player (Singh et al., 2000; Bowling &amp; Veloso, 2002; Conitzer &amp; Sandholm, 2003), or regret minimization in the limit (Hannan, 1957; Freund &amp; Schapire, 1999). Sometimes the ra-tionality constraint is \soft", in the sense that algo-rithms that recommend actions that seem to be in the player's best interest are preferred. Another type of constraint that is always soft is that of simplic-ity . Of course simpler algorithms are always preferred, but this is especially so in learning in games; and to economists, who are trying to model human behavior, in particular. 1 Computer scientists usually honor this constraint, but there are some exceptions (Conitzer &amp; Sandholm, 2003). The reinforcement learning ap-proach to learning in games (Tan, 1993; Littman, 1994; Hu &amp; Wellman, 1998; Singh et al., 2000; Bowling &amp; Veloso, 2002; Wang &amp; Sandholm, 2002; Greenwald &amp; Hall, 2003; Stimpson &amp; Goodrich, 2003) arguably sat-is es both properties of rationality and simplicity. In this paper, we study the following question. Two players are playing some game (in either normal (ma-trix) or extensive form). Each player knows its own payo for every outcome of the game, but not the op-ponent's. The players are considering some solution concept (such as Nash equilibrium). How much do the players need to communicate to nd a solution (if it exists)? In general, the players may not want to communicate their payo s truthfully, because this may cause the other player to choose an action in the game that is disadvantageous to the communicating player. If this is the case, a successful communication may not even be feasible. In this paper, we assume that the play-ers are completely cooperative in their communication. That is, they are solely concerned with computing a solution and take no heed of whether this solution is advantageous to them.
 Moreover, it may be the case that communication is restricted. For example, it may be the case that com-munication can only take place through playing the game, by observing the other player's actions (as is usually the case in learning in games). It is possible to implement any communication protocol in this model, by setting up an encoding scheme, encoding bits to be communicated as actions taken in the game. Never-theless, the players may be reluctant to follow such a protocol, because it may force them to play actions in the game that are highly disadvantageous to them-selves while the learning is taking place. In this paper, we do not directly address this, and simply look for the lowest communication complexity in terms of bits. In spite of these simpli cations, there are at least two reasons why the question studied in this paper is im-portant. The rst is straightforward: 1. The communication may in fact not be between the actual players of the game, but rather between two other parties (each of which is an expert on one of the players in the game). If the only interest of these parties is to predict the outcome of the game (according to the solution concept under study), so that they will be completely cooperative; and the cost of their communication is the number of bits sent; then our model is accurate.
 The second reason is (in our opinion) more interesting: 2. The communication necessary to compute the solu-tion is a lower bound on the communication that takes place in any learning algorithm that is guaranteed to converge to this solution concept. (This is assuming that in the learning algorithm, the players also do not have access to each other's payo s.) Combining this with an upper bound on the communication in a sin-gle round of the game, we obtain a lower bound on the (worst-case) number of rounds before convergence. For instance, if in an n n matrix game, the solution con-cept requires the communication of  X ( f ( n )) bits in the worst case (in our cooperative model), then  X ( f ( n ) log( n ) rounds are required in the worst case to converge to the solution concept. This is because each round, the only communication a player receives from the other player is which action she chose, 2 which is a communication of at most log( n ) bits (or 2 log( n ) bits counting both players' communication). Given how di erent the (ra-tionality, simplicity, and other) requirements placed on algorithms in learning in games are, this is arguably the only truly universal lower bound on learning algo-rithms' worst-case time to convergence.
 If the learning cost is not measured in number of rounds (but rather, for instance, in payo s lost), our technique can sometimes still be used to get a lower bound on the cost. For instance, if there is a minimum cost m incurred in every round before convergence, we actions in the game are excessively costly, the learning algorithm should avoid them altogether. Thus, if there are only n 0 &lt;n reasonable (not excessively costly) ac-tions for each player to take, then the lower bound on the number of rounds increases to  X ( f ( n ) log( n 0 ) ) (and the previous lower bound on cost increases to  X ( mf ( n ) log( n We rst review some elementary communication com-plexity. In this paper, we focus on the two-party model introduced by Yao (Yao, 1979). We follow the presen-tation in (Kushilevitz &amp; Nisan, 1997).
 In Yao's two-party communication model, one party holds input x , and the other holds input y . They seek to compute a binary function f ( x;y ). The parties alternatingly 3 send bits, according to some protocol. Once the protocol terminates, it should return a value for f ( x;y ) based on the communicated bits.
 De nition 1 In a deterministic protocol, the next bit sent is a function only of the bits sent so far and the sender's input. D ( f ) is the worst-case number of bits sent in the best correct deterministic protocol for com-puting f .Ina nondeterministic protocol, the commu-nicated bits may additionally depend on nondetermin-istic choices. For z 2f 0 ; 1 g , a nondeterministic pro-tocol for z is correct if it always returns 1  X  z when f ( x;y )=1  X  z , and for any x;y with f ( x;y )= z ,it returns z for at least one sequence of nondeterministic choices. N z ( f ) is the worst-case number of bits sent in the best correct nondeterministic protocol for z . Because any correct deterministic protocol is also a correct nondeterministic protocol, we have for any function f , and for any z , that D ( f ) N z ( f ). To prove lower bounds on communication complexity, there are numerous techniques (Kushilevitz &amp; Nisan, 1997). However, for the purposes of this paper, we will only need one: that of a fooling set . This technique ac-tually proves lower bounds even on nondeterministic communication complexity (and thus also on random-ized communication complexity).
 De nition 2 A fooling set for value z is a set of input pairs f ( x 1 ;y 1 ) ; ( x 2 ;y 2 ) ;:::; ( x m ;y m for any i , f ( x i ;y i )= z , but for any i 6 = j , either f ( x i ;y j ) 6 = z or f ( x j ;y i ) 6 = z . Theorem 1 (Known) If z has a fooling set of size m , then N z ( f ) log( m ) . For the larger part of this paper, we will be concerned with arguably the simplest formalization of games: normal form (also known as matrix ) games.
 De nition 3 In a 2-player n n normal form game , each player i has a set of (pure) strategies N = f 1 ; 2 ;:::;n g , and a utility (or payo ) function u N N ! IR . (We will refer to the players as row ( r ) and column ( c ).) In the problems below, the players seek to compute a binary function of the game (for instance, whether it has a pure-strategy Nash equilibrium). Each player knows only her own payo function. That is, the play-ers' payo functions correspond to the x and y inputs. 4.1. Nash equilibrium We rst study perhaps the best-known solution con-cept: Nash equilibrium.
 De nition 4 A 2-player game has a (pure-strategy) Nash equilibrium if there exist i;j 2 N such that for any i 0 , u r ( i;j ) u r ( i 0 ;j ) and for any j 0 , u u ( i;j 0 ) .
 We now de ne the binary function we seek to compute. De nition 5 The function Na returns 1 if the game has a pure-strategy Nash equilibrium.
 We rst give a simple upper bound on the determinis-tic communication complexity of Na .
 Theorem 2 D ( Na ) is O ( n 2 ) .
 Proof : In the protocol, each player communicates, for every entry of the payo matrix, whether her strategy corresponding to that entry is a best response to the other player's strategy corresponding to that entry| that is, whether she would deviate from that entry. (So, each player communicates one bit per entry.) The game has a pure-strategy equilibrium if and only if for some entry, neither player would deviate from it. We now show a matching lower bound on the nonde-terministic communication complexity of Na .
 Theorem 3 N 0 ( Na ) is  X ( n 2 ) , even if all payo s are either 0 or 1 .
 Proof : We will exhibit a fooling set of size (2 ( n 2 ) Consider the set S of all n n matrix games where ev-ery entry's payo vector is either (0 ; 1) or (1 ; 0)|there are 2 ( n 2 ) such games. Among these, consider the sub-set S 0 that have no row consisting only of (1 ; 0)s and no column consisting only of (0 ; 1)s. j S 0 j is still (2 ( n for the following reason. Suppose we randomly choose a game from S . The probability of any particular row having only (1 ; 0)s (or any particular column having only (0 ; 1)s) is 2  X  n . It follows that the probability of at least one row having only (1 ; 0)s or at least one col-umn having only (0 ; 1)s, is at most 2 n 2  X  n , which is negligible. So only a negligible fraction of the games in S are not in S 0 . None of the games in S 0 have a pure-strategy Nash equilibrium, for the following rea-son. For any entry in the matrix, one of the players receives 0. If it is the row player, there is another en-try in the same column giving her a payo of 1, and she will want to switch to that entry. If it is the col-umn player, there is another entry in the same row giving her a payo of 1, and she will want to switch to that entry. Now consider two games s 1 ;s 2 2 S 0 with s try; say (without loss of generality) that the entry is (1 ; 0) for s 1 and (0 ; 1) for s 2 . Then, if we let s 12 game that has the row player's payo s of s 1 and the column player's payo s of s 2 , the entry under discus-sion is (1 ; 1) in s 12 . Because a payo greater than 1 never occurs in s 12 , this entry is a pure-strategy Nash equilibrium.
 To give an example of how this translates into a bound on learning in games, we can conclude that every mul-tiagent learning algorithm that converges to a pure-strategy Nash equilibrium (if one exists) has a worst-case convergence time of  X ( n 2 log( n ) ) rounds (given that the players do not know each other's payo s).
 Communicating the best response function in Theo-rem 2 is hard because it is set-valued|there can be multiple best responses to a strategy. Next, we in-vestigate what happens if at least one of the players always has a unique best response. Let U be the sub-set of games where the column player has a unique best response against every (pure) strategy for the row player, and let Na j U be the restriction of Na to such games.
 Theorem 4 D ( Na j U ) is O ( n log( n )) .
 Proof : In the protocol, the column player communi-cates her (unique!) best response to each of the row player's pure strategies (a communication of log( n )per strategy). After this, the row player can determine if a pure strategy Nash equilibrium exists (if and only if for one of the row player's pure strategies i , i is a best response to the column player's best reponse to i ), and can communicate this to the column player.
 Theorem 5 N 0 ( Na j U ) is  X ( n log( n )) , even if all pay-o s are either 0 or 1 .
 Proof : We will exhibit a fooling set of size n !. This will prove the theorem, because n log( n ) is (log( n !)). For every permutation : N ! N , consider the fol-lowing game. When the row player plays i and the column player plays j , the row player gets a utility of 1if ( i ) 6 = j , and 0 otherwise; the column player gets a utility of 0 if ( i ) 6 = j , and 1 otherwise. Because this is a zero-sum game, and because for each player, against any opponent strategy, there is a strategy that wins against this opponent strategy, there is no pure-strategy equilibrium. All that remains to show is that if we mix the payo s of two of these games, that is, we de ne the row player's payo s according to 1 , and the column player's payo s according to 2 6 = 1 , there is a pure-strategy Nash equilibrium. Let i be such that ( i ) 6 = 2 ( i ). Then the strategy pair ( i; 2 ( i )) gives the row player utility 1 (because 1 ( i ) 6 = 2 ( i )), and the column player utility 1 also. Because 1 is the high-est utility in the game, this is a Nash equilibrium. Interestingly, slight adaptations of all the proofs in this subsection also work for Stackelberg equilibrium , where the row player moves rst. (Here the compu-tational question is de ned as \Should the row player play her rst action?") Thus, Stackelberg equilibrium has the same communication complexity in each case. We omit the proofs because of space constraint. 4.2. Iterated dominance We now move on to the notions of dominance and it-erated dominance . The idea is that if one strategy al-ways performs better than another, we may eliminate the latter from consideration.
 De nition 6 In a 2-player game, one strategy is said to strictly dominate another strategy for a player if the former gives a strictly higher payo against any oppo-nent strategy. One strategy is said to weakly dominate another strategy if the former gives at least as high a payo against any opponent strategy, and a strictly higher payo against at least one opponent strategy. Sometimes, mixed strategies (probability distributions over pure strategies) are allowed to dominate other strategies. A mixed strategy's payo against an oppo-nent strategy is simply its expected payo .
 It is rarely the case that one strategy dominates all others. However, once we eliminate a strategy from consideration, new dominances may appear. This se-quential process of eliminating strategies is known as iterated dominance .
 De nition 7 With iterated dominance , dominated strategies are sequentially removed from the game. (Here, removing strategies may lead to new dominance relations.) A game is said to be solvable by iterated dominance if there is a sequence of eliminations that eliminates all but one strategy for each player. While the de nition of iterated dominance is concep-tually the same for both strict and weak dominance, the concept technically di ers signi cantly depending on which form of dominance is used. Iterated strict dominance is known to be path-independent : that is, eventually the same strategies will remain regardless of the order in which strategies are eliminated. Iter-ated weak dominance, on the other hand, is known to be path-dependent : which strategies eventually remain depends on the elimination order. In fact, determining whether a game is solvable by iterated weak dominance is NP-complete (Gilboa et al., 1993).
 We rst study iterated strict dominance.
 De nition 8 The function isd returns 1 if the game can be solved using iterated strict dominance. Theorem 6 D ( isd ) is O ( n log( n )) , whether or not elimination by mixed strategies is allowed.
 Proof : In the protocol, the players alternatingly com-municate one of their dominated strategies, which the players then consider removed from the game; or com-municate that no such strategy exists. (We observe that this requires the communication of O (log( n )) bits.) The protocol stops once both players suc-cessively communicate that they have no dominated strategy. We observe that each player can get at most 2 n turns in this protocol. Hence the number of bits communicated is O ( n log( n )).
 Theorem 7 N 1 ( isd ) is  X ( n log( n )) , even if all pay-o s are either 0 or 1 , whether or not elimination by mixed strategies is allowed.
 Proof : We will exhibit a fooling set of size n !. This will prove the theorem, because n log( n ) is (log( n !)). For every permutation : N ! N , consider the follow-ing game. The row player's payo when the row player plays i and the column player plays j is0if ( i ) j , and 1 otherwise|unless ( i )= n , in which case the row player's payo is always 1. The column player's payo when the row player plays i and the column player plays j is 0 if j&lt; ( i ), and 1 otherwise. (We ob-serve that there is a weak dominance relation between any pair of strategies by the same player, and thus al-lowing for dominance by mixed strategies cannot help us|we may as well take the most weakly dominant strategy in the support. So, we can restrict atten-tion to elimination by pure strategies in the rest of the proof.) Because the row player always gets a payo of 1 playing  X  1 ( n ), and the column player always gets a payo of 1 playing n , we can eliminate any strat-egy that always gets a player a payo of 0 against the opponent's remaining strategies. Thus, the row player can eliminate  X  1 (1); then the column player can elim-inate 1; then the row player can eliminate  X  1 (2); then the column player can eliminate 2; etc. , until all but  X  1 ( n ) for the row player and n for the column player have been eliminated. So every one of these games can be solved by iterated strict dominance. All that remains to show is that if we mix the payo s, that is, we de ne the row player's payo s according to 1 , and the column player's payo s according to 2 6 = 1 , the game is not solvable by iterated strict dominance. Let k be the lowest number such that  X  1 1 ( k ) 6 =  X  1 (we observe that k n  X  1). Because iterated strict dominance is path-independent, we may assume that we start eliminating strategies as before for as long as possible. Thus, we will have eliminated strategies 1 (1) = 2 ( k 1 ; 2 ;:::;k  X  1 for the column player. However, at this point,  X  1 2 ( k ) for the row player will not have been eliminated, so playing k (or any other remaining strat-egy) will get the column player 1 against  X  1 2 ( k ), and can thus not be eliminated. Similarly, any remaining strategy will get the row player 1 against k , and can thus not be eliminated. Because k n  X  1, the game cannot be solved by iterated strict dominance. We now move on to iterated weak dominance.
 De nition 9 The function iwd returns 1 if the game can be solved using iterated weak dominance.
 We rst give an upper bound on the nondeterministic communication complexity.
 Theorem 8 N 1 ( iwd ) is O ( n log( n )) , whether or not elimination by mixed strategies is allowed.
 Proof : As in Theorem 6, the players alternatingly communicate an eliminated strategy (they nondeter-ministically choose one from the weakly dominated strategies at that point), and return 1 if they reach a solution. Because iterated weak dominance is path-dependent, whether a solution is reached depends on the nondeterministic choices made; but if the game is solvable, then at least for some sequence of nondeter-ministic choices, they will reach a solution.
 Assuming P 6 =NP, any deterministic communication protocol for determining whether a game is solvable by iterated weak dominance must either have an exponen-tial communication complexity, or require exponen-tial computation per communication step by the play-ers. (Because otherwise, we would have a polynomial-time algorithm for determining whether a game is solvable by iterated weak dominance, which is NP-complete (Gilboa et al., 1993).) We can avoid this by restricting attention to the following subset of games, where path-dependence is partially assumed away. Let I be the subset of games where either no solution by it-erated weak dominance exists, or any elimination path will lead to a solution; and let iwd j I be the restriction of iwd to such games.
 Theorem 9 D ( iwd j I ) is O ( n log( n )) , whether or not elimination by mixed strategies is allowed.
 Proof : Because (by assumption) any elimination path will do, the approach in Theorem 6 is applicable. The following theorem shows that this is the best we can do even with the restriction to the set I (and also that the nondeterministic algorithm given before is the best we can do).
 Theorem 10 N 1 ( iwd j I ) is  X ( n log( n )) , even if all payo s are either 0 or 1 , whether or not elimination by mixed strategies is allowed.
 Proof : We rst observe that when all the payo s are 0 or 1, allowing for weak dominance by mixed strategies does not allow us to perform any more eliminations. (If mixed strategy weakly dominates pure strategy , then all the pure strategies in the support of must get a payo of 1 against any strategy that 0 gets a payo of 1 against. Moreover, at least one pure strategy in the support must receive a strictly better payo than 0 against at least one opponent strategy. But then this pure strategy also weakly dominates 0 .) Thus, we can restrict attention to elimination by pure strategies in the rest of this proof.
 For even n ( n =2 l ), we will exhibit a fooling set of size ( n 2  X  1)!. This will prove the theorem, be-cause n log( n ) is (log(( n 2  X  1)!)). (Note that n 2 log( is ( n log( n ).) For every permutation : L ! L with ( l )= l , consider the following game. When the row player plays i and the column player plays j , both players receive 0 when j&gt;l (half of the row player's strategies are dummy strategies). Oth-erwise, the row player receives a payo of 1 when-ever j 2f ( i ) ; ( i )  X  1 ;l + ( i )  X  1 ; ( i )  X  2 g , and 0 otherwise. The column player receives a payo of 1 whenever j 2f ( i ) ;l + ( i ) g , and 0 otherwise|unless j = n =2 l , in which case the column player always receives 0 (another dummy strategy).
 In each of these games, both players can rst eliminate the dummy strategies; then, the row player can elimi-nate  X  1 (1) using  X  1 (2); then the column player can eliminate 1 and l +1 using (for example) l ; then the row player can eliminate  X  1 (2) using  X  1 (3); then the col-umn player can eliminate 2 and l + 2 using l ; etc. , until only  X  1 ( l )= l is left for the row player, and only l is left for the column player. So every one of these games can be solved by iterated weak dominance. Moreover, any sequence of eliminations will arrive at this solu-tion, for the following reasons.  X  1 ( l )(= l ) for the row player and l for the column player are the unique best responses to each other and hence cannot be elimi-nated. Thus, eventually all the dummy strategies must be deleted. For the same reason, whenever  X  1 ( t ) has been deleted, l + t and t must eventually be deleted. Furthermore, l + t must survive for the column player as long as  X  1 ( t ) survives for the row player, because l + t and t are the only best responses to  X  1 ( t ), and neither can dominate the other. Finally, for the row player,  X  1 ( t + 1) cannot be eliminated before  X  1 ( t ), because as long as  X  1 ( t ) survives, l + t must survive for the column player, and  X  1 ( t + 1) is the unique best response to l + t . Thus, for the smallest t&lt;l such that  X  1 ( t ) survives, eventually l + t  X  1, t  X  1, and t  X  2 must be eliminated for the column player, and then eventually  X  1 ( t + 1) must eliminate  X  1 ( t ) (because it performs better against l + t , which cannot yet have been eliminated). So any elimination path will lead to the solution.
 All that remains to show is that if we mix the payo s, that is, we de ne the row player's payo s according to 1 and the column player's payo s according to dominance. Suppose there is a solution by iterated weak dominance. Because the strategies labeled l are unique best responses against each other (regardless of the permutations), neither can ever be eliminated, so they must constitute the solution.
 We rst claim that the row player's non-dummy strate-gies, and the column player's non-dummy strategies of the form l + k , must be alternatingly eliminated. That is, two non-dummy row player strategies cannot be eliminated without a non-dummy column player strat-egy of the form l + k being eliminated somewhere inbe-tween, and two non-dummy column player strategies of the form l + k cannot be eliminated without a non-dummy row player strategy being eliminated some-where inbetween. Moreover, each non-dummy row player strategy that is eliminated must be the best re-sponse to the last non-dummy column player strategy of the form l + k that was eliminated (with the excep-tion of  X  1 1 (1)); and vice versa, each non-dummy col-umn player strategy of the form l + k that is eliminated must be the best response to the last non-dummy row player strategy that was eliminated. This is so because each non-dummy row player strategy (besides  X  1 1 (1)) is the unique best response against some non-dummy column player strategy of the form l + k ; and each non-dummy column player strategy of the form l + k is the almost unique best response against some non-dummy row player strategy. (\Almost" because k is also a best response, but k can never eliminate l + k because it always performs identically.) Thus the only way of eliminating these strategies is to rst eliminate 1 (1) for the row player, then the best response to that strategy among the column player strategies of the form l + k , then the best response to that strategy, etc. (Other strategies are eliminated inbetween this.) Now, we claim that in the solution, the non-dummy column player strategies of the form l + k must be eliminated in the order l +1 ;l +2 ;:::; 2 l  X  1. For suppose not: then let l + k be the rst eliminated strategy such that l + k  X  1 l + 1 has not yet been eliminated. (We note that k 2.) By the above, the next non-dummy row player strategy to be eliminated should be  X  1 1 ( k + 1). However, l + k  X  1 and l + k + 1 have not yet been eliminated, 4 and thus, k  X  1 and k + 1 cannot be eliminated before  X  1 1 ( k +1) (because if k  X  1 (or k + 1) could be eliminated, then l + k  X  1 (or l + k + 1) could also be eliminated at this point, contradicting the alternation in the elimination proven above). But  X  1 1 ( k + 1) is the only strategy that gets the row player a utility of 1 against both of k  X  1 and k + 1, so it cannot be eliminated. Thus the elimination cannot proceed. This proves the claim. But if this is the elimination order, it follows that 1 = (  X  1 1 (1)) (because l + 1 is the best reponse against 1 (1)), 2 = 2 ( response against  X  1 1 (2), which is the best response against 1), etc. Thus the permutations must be the same, as was to be shown. For the (short) remainder of this paper, we will focus on a di erent formalization of games: extensive form games|games that can be represented in tree form. De nition 10 A 2-player (full information) exten-sive form game is given by a tree with n nodes (one of which is the root), a speci cation of which player moves at each node, and a payo from IR for each player at every leaf. 5.1. Backwards induction The simplest solution concept for games in extensive form is that of backwards induction , where the best ac-tion to take is determined at every node, starting from the bottom nodes and working upwards. To make the \best" action uniquely de ned, we will restrict atten-tion to the subset of extensive form games E in which no player gets the same payo at two di erent leaves. De nition 11 In the backwards induction solution, each node is labeled with one of its children, indicating which action the player at this node should take; under the constraint that for each player, each of her actions should give her the maximal payo given the actions speci ed lower in the tree.
 De nition 12 The function b j E returns 1 if in the backwards induction solution, player 1 chooses her left-most action at the root.
 Theorem 11 D ( b j E ) is O ( n ) .
 Proof : For each choice node in the tree that is not followed by another choice node (that is, the bottom choice nodes), the corresponding player communicates which action she would take at this choice node. As a result, now both players know their valuations for the bottom choice nodes. Thus, in the next stage, for each choice node in the tree that is followed by at most one more choice node (that is, the \second-to-bottom" choice nodes), the corresponding player can communicate which action she would take here. We can continue this process until the players know which action player 1 takes at the root. The communication can be achieved by labeling each edge in the tree as either 0 (for not taken) or 1 (for taken), in the bottom-to-top order just described.
 Theorem 12 N 1 ( b j E ) is  X ( n ) . (Even when the tree has depth 2 .) Proof : Omitted because of space constraint. In learning in games, there are widely varying re-quirements on learning algorithms. We demonstrated how communication complexity can be used as a lower bound on the required learning time or cost. Because this lower bound does not assume any requirements on the learning algorithm, it is universal, applying under any set of requirements on the learning algorithm. We characterized exactly the communication complex-ity of various solution concepts from game theory, giv-ing the tighest lower bounds on learning these concepts that can be obtained with this method. We showed that the communication complexity of nding a pure-strategy Nash equilibrium in an n n game is ( n 2 ) (but only ( n log( n )) when one of the players always has a unique best response to any strategy); the com-munication complexity of iterated strict dominance is ( n log( n )) (whether or not dominance by mixed strategies is allowed); the communication complexity of iterated weak dominance (for games in which solv-ability is path-independent) is ( n log( n )) (whether or not dominance by mixed strategies is allowed); and the communication complexity of backwards induction in a tree with n nodes is ( n ). (Interestingly, the size of the payo s is not a factor in any of these complex-ities.) In each case, we showed the lower bound holds even for nondeterministic communication, and gave a deterministic protocol that achieved the bound. There are various directions for future research. Can the lower bounds presented in this paper be achieved by learning algorithms with additional desirable prop-erties? (The most important such property would be some measure of rationality : the player should attempt to do reasonably well in the game even when still learn-ing. Also, we may wish to add the following constraint: a player should not be able to make the solution that the players eventually converge to more advantageous to herself, by not following the learning algorithm.) If this is not possible, what are minimal restrictions on the learning algorithm that will allow us to strengthen our lower bounds and close the gap? (For instance, is there a weak rationality criterion that all sensible no-tions of rationality should satisfy, and that strength-ens the lower bound signi cantly?) This would consti-tute a new branch of communication complexity the-ory, where communication is nontrivially constrained. Another interesting direction for future research is to investigate whether learning algorithms can do better than the bounds presented in this paper on speci c distributions of games (perhaps drawn from the real world). After all, the lower bounds presented in this paper are worst-case results. We also did not study the communication complexity of computing a mixed-strategy Nash equilibrium. (We do observe that be-cause a mixed-strategy Nash equilibrium always ex-ists, the existence question is trivial in this case.) Yet another possibility is to study solution concepts such as Nash equilibrium for the repeated game rather than for the one-shot game. Finally, one can study whether and how things change if we impose computational constraints on the agents. This material is based upon work supported by the Na-tional Science Foundation under CAREER Award IRI-9703122, Grant IIS-9800994, ITR IIS-0081246, and ITR IIS-0121678.

