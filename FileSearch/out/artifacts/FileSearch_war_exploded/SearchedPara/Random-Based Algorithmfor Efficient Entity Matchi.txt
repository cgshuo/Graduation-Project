 Nowadays, the rapid growth of web da ta and User Generated Content (UGC) changes the way we used to collect and manage information. By the hands of the huge amount of web users, data become much easier to be generated. For instance, in a C2C (Customer to Customer) online business site, it becomes easy to start an online store and generate personalized st ructured/unstructured descriptions for its listed goods. And it is pervasive that different sellers own the same commodity with variety of descriptions together with diverse schemas. This results in the difficulty in product managing which may affect product or price comparison. Additionally, this kind of UGC is becoming large. Then it is urgent to design an efficient distributed entity matching framework by using of this UGC to identify entities that represent the same items.

Though there are already a bunch of entity matching algorithms, we face new challenges which make the traditional methods infeasible. First of all, most of the user generated data are semi-structured or unstructured data, which come from variety data sources and in different formats/schemas. Second, a great quantity of typos occurred in UGC data dramatically reduce the data quality. Third, high computation cost occurs due to the large web data size. Traditional well designed entity matching algorithms are usually for structured data. When they confront with these three challenges including hybrid data structure without uniform schema, low data quality and huge data size, their performance is also challenged.

Lots of works have tried resolving those challenges separately: 1) Document similarity metrics[20] are introduced to measure the similarity between unstruc-tured data, such as online documents. They provide standards of similarity measurements for unstructured data and semi-structured data. 2) Tokenization technique[18] is used to reduce the negative influence to data quality caused by typos and human mistakes. It has become an important step in data cleaning and for improving the accuracy of entity matching. 3) Data blocking strategies are designed to split data into multiple parts for parallelization and lowering computation cost. Inspired by those work, we propose a flexible parallel entity matching framework on MapReduce, which aims to resolve entity matching on unstructured data with higher efficiency and lower cost. That is for these un-structured data, our algorithm shall boost the processing speed while promise load balance and reduce network transmission cost. Those are the
The main contributions can be summarized as follows:  X  We sketch out a random-based framework for entity matching based on  X  We propose a random-based permutation method inspired by PLEB[6]  X  We analyze the cause of redundancy problem in our algorithm, which is  X  We evaluate our approaches and demonstrate their efficiency in comparison The idea of Entity matching (along with Record linkage and deduplication) is first introduced by geneticist Howard Newcombe in [15] who presents odds ratios of frequencies and the decision rules for delineating matches and mismatches. Fellegi and Sunter[4] provide the formal mathematical foundations of record linkage. At the end of 20th century, since the data size grew rapidly, the main problem for entity matching changed from improving calculation accuracy to handling huge amount of data. Blocking strategy was introduced to solve this problem for it can filter majority of the entity pairs with low similarity before similarity comparison. Meanwhile, the proposal of MapReduce[3] also gave us a better platform to solve this problem.

A number of blocking-based entity matching algorithms under MapReduce framework have been presented to help d ealing with big data sets[7,8,19,14]. These works are based on a assumption that there is only one key for an en-tity and use a map/reduce phase to handle the problem. They design different blocking strategies for map phase and then do the matching step in reduce phase. Some of the most influential works includes sorted neighborhood[12] and load-balanced entity matching[11,10]. The sorted neighborhood is a blocking technique by sorting all entities according to blocking keys, assigning a window size w and comparing entities in the window while sliding. This gives us some inspiration of the pair generation method. However, this part of work and its joint works[9] didn X  X  mention the load balancing problem on MapReduce.
Both BlockSplit and PairRange blocking strategies mentioned in [11] focus on solving the imbalance problem. But they rely on a data analysis phase before matching job. This phase scan the input entities to collect a list of all possible pairs, then make a blocking plan to evenly divide those pairs into multiple blocks by the above two strategies. It can perfectly solve the load balancing problem on MapReduce by adding an expensive cost before matching process. Therefore, both of these two strategies are suitable for processing skewed data, but far more slow to deal with regular data or data with enormous size. Another algorithm has been presented for document-similarity computation[1] which have the same background to our work, we will compare our performance with this algorithm. In order to get good matching accuracy for high dimensional data, the distributed entity matching framework is expected to have the following functionalities: a) Fast Entity Similarity Calculation method, b) Efficient Candidate Entity Pair Generation algorithm and c) Redundant pairs removing plan. Figure 1 shows the framework of our algorithm. 3.1 Random-Based Similarity Calculation Cosine similarity is appropriate for measuring similarity of semi-structured and unstructured entities. However, high dimensional vectors may cause the dimensional-curse on entity matching calculation. Locality Sensitive Hashing (LSH) function which keep the property of cosine similarity, proposed by Charikar in [2], provides an option for high dimensional vectors similarity calculation dis-tributively. Theorem: Suppose we have a collection of vectors in a k dimensional vector space (that is N k ). We generate a random vector r of unit length from this k dimensional space, and define a hash function h r as Eqn.1. Then for vectors u and v , we have the corresponding relationship as calculated by Eqn.2. Goemans and Williamson[5] prove that this hash function can promise the cosine similarity in a high probability.

Since high dimensional vectors computation is time-consuming, dimension reducing is generally done for improving calculation performance. Note that the above equation is probabilistic in nature. Hence, we start to generate d numbers of random vectors from N k and get R= { r } , with | R | = dandd k .Foreach hash function h r ( u )for u and each vector r i  X  R . In such a way, we represent each vector u by a d -bit signature S u .Thendimension k is successfully reduced to d whilst it still preserves the cosine similarity, where d k .Thisd-dimension signature keeps features of the original vector. Then the huge deviation between two signatures means big differ ence between two entity vectors.

Cosine similarity between any two vectors is achieved by Eqn. 3. On the other hand, if we use the similarity between signatures to represent the probability in Eqn. 3, we can observe: Pr [ h r ( u )= h r ( v )] = 1  X  ( hamming distance ) / d .
 Thus, it converts the problem of finding cosine similarity between vectors to the problem of calculating hamming distance between signatures. It is more efficient in both processing speed and memory utilization. So in the following descriptions, hamming distance has the same meaning as cosine similarity. 3.2 Entity Pairs Generation As shown above, highly similar entities usually have similar signatures. Then sorting by lexicography will make them cl ose to each other except the deviations occur in the first few bits of their signatures. In order to reduce this kind of deviation and increase the opportunity of getting close for similar signatures, we propose to do random permutation for these signatures as proposed by PLEB (Point Location in Equal Balls), which was first mentioned in [6] and improved in [2]. This algorithm takes random permutations to signatures and sort the permuted signatures. It aims to find vectors with short hamming distances.
A random permutation is considered as a random jumble of the bits of each signature, so the prefix deviation of two similar signatures can be prevented in some of their permutations. We apply several rounds of permutations on sig-nature set. At each round, it generates a set of permuted signatures. Signature pairs with lower hamming distance are expected to get close in some of the sort-ing lists. Accordingly, we can find the top m closest neighbors for each signature and generate our entity pair candidates. Implementation details can be found in the following content. 3.3 Entity Matching on MapReduce We aim at solving the entity matching problem on semi-structured and unstruc-tured data. We first tokenizing the input data and generating high dimensional feature vectors based on the words frequency. We use these vectors as our data input. The goal is to find all matching pairs among these entity vectors. We define that two entities are matched when the hamming distance between their feature vectors is lower than a predefined threshold. We can also output the top N similar vectors by an additional sorting process on the result set.
Figure 1 shows matching framework on MapReduce. Before doing the match-ing process, we carry out three steps of p reprocessing on the source data to get our expected input. Initially, we split the input entities into tokens using the Part-Of-Speech Tagger[17]. Then we generate a dictionary containing all k dif-ferent tokens occurred in the data set. Finally, for each entity u ,a k -dimension vector V u is generated, in which the n th dimension represent the word frequency of the n th token in entity u . The input of our method is a set of ( key , value )pairs made up of entity ID E u and its k -dimension vector V u . In addition, another standard vector set R is introduced into the MapReduce job, which contains d
Figure 2 illustrates the workflow of our MapReduce job. Map phase contains stepsasfollowings: 1. Initially, we apply h r ( u ) to each input entity ( E u , V u )using R as the standard 2. After converting the input vectors into signatures S u , we randomly permute
In reduce phase, we expect to achieve entity pair similarity. After an auto-matic sorting procedure during the shuffle between map and reduce, the reduce phase faces t number of groups represented as ( i, L i ) , in which L i is the sorted list on all signatures in the i th round of permutation. Then we generate match-ing pairs between every entity u and its closest m neighbors in the sorted list. Finally, we calculate the hamming distance of every paired entities and output those with distance below a predefined threshold. The output is formatted as ( E u E v , similarity )( u&lt;v ) , which are the ID concatenation of paired entities X  with its similarity value.

Overall, The map tasks change each k -dimension vector into t number of d -bit signatures. d and t are always far less than k . Generally d and t are between tens to hundreds while k are normally more than tens of thousands determined by the characteristic and size of input data. That gives a significant reduction on data volume, and also a huge cut on the network transmission cost between map and reduce. Unlike most of the blocking-based entity matching methods comprised by multiple MapReduce tasks, our matching algorithm is finished in one MapRe-duce job. Since each MapReduce task spends extra cost on task scheduling and network communication, the cutting on the number of MapReduce jobs can lead to performance promotion. Furthermore, since all permutations of a signature are sent to reducers evenly, which are partitioned by their permutation number i . There are the same number of pairs for each reduce task. Therefore, our model easily solve the load balancing problem. During the pair generation step in reduce phase, all pairs are generated in parallel from different groups. As the same entity pair may be generated from multiple groups, there can be many duplicated pairs, such as the circled pairs E 0 E 4 and E
E 4 in Figure 2. It may cause redundant computation cost, which is a pervasive problem in many MapReduce-based matching algorithms. The reason for the occurrence of these redundancy is that the features of one entity are separated into multiple parts during the map phase, and each part may possibly match any part of the other entity in the reduce phase. It may happen more frequently among these highly similar entity pairs, because of the higher possibility for each part to be matched.

In redundancy-free similarity computation model [13], it adds additional an-notate on each map output to tell the reducer which reducers the rest parts of this entity will be sent to. Though it is efficient, it bases on a strong precondition that all entities sent to the same reducer will definitely be paired among each others. In our algorithm, the permuted signatures of the same entity is sent to all reducers, and for each reducer a signature is only paired with its neighborhoods. So this redundancy-free solution is inapplicable for our random-based method.
We introduce an extra MapReduce job to reduce duplication. Figure 3 shows an example of our method. We modify the original MapReduce job in the reduce phase by cutting off the similarity computation. After generating all pairs, the reduce task terminates and outputs those pairwise information with entity IDs E u ,E v ( u&lt;v ) concatenated as key and their i th permuted signatures P ui ,P vi concatenated as value ,thatis( E u E v ,P ui P vi ).

The map phase of the second MapReduce job is an identity mapper which does nothing. In the following shuffle phase, all pairs with the same entity ID are grouped together. It means that all those duplicated pairs come together. Then we need to pick one pair of permuted signatures in the list and calculate its hamming distance. At last, we output the pair like ( E u E v , similarity ) as final result. We run experiments on a 22-node HP blade cluster. Each node has two Intel Xeon processors E5335 2.00GHz with four cores and one thread per core, 16GB of RAM, and two 1TB hard disks. All nodes run CentOS 6.5, Hadoop 1.2.1, and Java 1.7.0. We evaluate the performance of our algorithm in two aspects: 1) We measure the effect on calculation performance and matching quality by using different parameters. 2) We compare the performance of our algorithm with two other state-of-the-art matching algorithms namely Document Similarity Self-Join (DSSJ)[1] and Dedoop[10]. 5.1 Data Set Description We use CiteSeerX data set. It contains nearly 1.32 Million citations of total size 2.89 GB in XML format. Each citation includes structured attributes such as record ID, author, title, date, page, volume, publisher, etc .Italsohasdocument abstract . We select a few records from Cites eerX and manually make validation sets for accuracy evaluation. 5.2 Parameter Description and Evaluation Metrics There are three parameters t hat may affect the performance:  X  X  : The length of the signature. It directly determines the network transmis- X  X  : The number of permutations. It multiplies the data transmission between  X  X  : The window of selecting neighborhoods. It decides the amount of pairs
We introduce four metrics to evaluate system performance:  X  The network transmission cost is measured by summing up the size of  X  The run-time of MapReduce jobs is recorded to compare the speed of our  X  The redundancy rate is calculated as total num ber of generated pairs /  X  The accuracy is also measured in this part. In order to calculate the ac-
To evaluate the performance, we first use a 200MB subset of CiteSeerX as our input to the effect of changing parameters. Figure 4 to 9 show the perfor-mance variations of our algorithm when changing one of the three parameters. Figures 4, 6, and 8 illustrate the run-time for both of two MapReduce jobs with MR1 for pair generation and MR2 for deduplication, and the transmission cost during MapReduce tasks. We can see from Figure 4 that when we increase the length of signature d , the network transmission cost together with the run-time of MapReduce jobs has a linear growth. Meanwhile, in Figure 5, as the increas-ing of d ,the redundancy decreases steadily and the accuracy increases smoothly. The reason is that as the signature exte nds, the differences between entities can be found more easily. So they may have fewer chances to be paired. Therefore, the redundancy rate decreases. The performance variations with changing t and m are listed in Figure 6 to 9. Figure 6 and 8 shows similar performance with Figure 4, but it seems that in Figure 8 we spend large part of time on MR1 when m =2or m = 4. It is because a smaller m cannot reduce the cost on map phase when generating signatures and doing permutations. Figure 7 clearly shows that the number of permutations t can determine the redundancy rate directly. All these three parameters can strongly influence the performance. Figure4,6,8 show that when d 400 ,t 50 and m 8, we get a better performance on both redundancy rate and accuracy. So we choose d=500, t=50 and m=10 to do the rest of evaluations. Furthermore, the linear growth of run-time shows a good scalability of our matching method, which has a huge advantage in processing bigdataset. 5.3 Different System Comparison The baseline methods for our comparison are Document Similarity Self-Join (DSSJ) and Dedoop.

In order to measure the accuracy, we use the validation set mentioned pre-viously, and compare our matching result with DSSJ and Dedoop results. The standard result tests contain top 10, 20 and 50. Table 1 shows the accuracy of these top N tests. Since Dedoop compares all possible pairs of entities and calculates cosine similarity directly, the similarity result of Dedoop are always correct. However, the transmission cost is problematic that will be analyzed in the following content. When using the parameters (d=500,t=50,m=10), we can get almost the same accuracy as DSSJ method does. At the same time, we eval-uate the processing speed of our method comparing with Dedoop and DSSJ using these parameters. Figure 10 shows run-time between our method and the baseline methods. We just run a small size of data since the run-time of both Dedoop and DSSJ would exceed hours if the size of data is larger than 100MB. The reason is that both of these two methods generate enormous size of pair candidates. We get nearly 100GB map output data when running Dedoop on 200MB data set. It can burden the system drastically on network transmission and is hard to be processed in memory. On the contrary, our random-based matching method shows a good scalability on data set size. The speed of our algorithm is significantly faster than Dedoop, and far more stable even when dealing with gigabytes of input data.
 In this paper, we study the problem of entity matching on unstructured data, which will be formatted as high-dimensional feature vectors. We take the MapRe-duce framework as our programming model and point out the two major challenges met on this model, which are load balancing problem and network transmission cost. We propose a random-based matching method to solve the matching problem which will help to greatly reduce the transmission cost. We use a special LSH function to generate signatures for entities, which helps to reduce entity dimensions. We take PLEB fast search algorithm to generate the candidate pairs efficiently. In addition, we propose our approach to reduce the redundancy during reduce tasks. Given the proposed algorithm, we implement it in Hadoop and analyze its performance on real data sets. Acknowledgements. This work is partially supported by National Basic Re-search Program of Chi-na (Grant No. 2012CB316200), National Science Foun-dation of China (Grant No.61232002, 61402177 and 61332006).

