 This paper evaluates the uptake and efficacy of a unified approach to phrasal query suggestions in the context of a high-precision search engine. The search engine performs ranked extended-Boolean searches with the proximity operator NEAR being the default operation. Suggestions are offered to the searcher when the length of the result list falls outside predefined bounds. If the list is too long, the engine suggests narrowing the query through the use of super phrases; if the list is too short, the engine suggests broadening the query through the use of proximal subphrases. We evaluated uptake of phrasal query suggestions by analyzing search log data from before and after the suggestion feature was added to a commercial version of the search engine. We looked at approximately 1.5 million queries and found that, after they were added, suggestions represented n early 30% of the total queries. We evaluated efficacy thro ugh a controlled study of 24 participants performing nine search es using three different search engines. We found that the engine with phrase suggestions had better high-precision recall than both the same search engine without suggestions and a search engine with a similar interface but using an Okapi BM25 ranking algorithm. H.3.5 [ Information Storage and Retrieval ]: Online Information Services  X  Web-based services Measurement, Human Factors. Proximity Search, Query Log Analysis, User Study, Web Search. A fundamental distinction between queries to a search engine and those to a database is that the former are often ambiguous and are expected to return approximate matches. Searchers resolve this ambiguity via iteration. Accord ing to Henninger and Belkin, a good search engine should aid this process by  X ... not only providing a good query language, but also supporting an iterative dialogue model. X  [Henninger95] Fr om log studies of Web-based search engines, we know that quer ies typically consist of just a matches, thus, iteration is usua lly carried out by scrolling and paging through the long result list. Many alternatives to scrolling have been tried in an effort to improve the iterative process. For example, search engines commonly allow the result list to be sorted by different attributes, such as relevance or date. Some engines attempt to organize the results hierarchically by clus tering [Cutting92, Clusty] or by category [Zamir99, Northernlight]. Richer graph-based interfaces have been created for speciali zed collections and meta-search engines [Rao95, Kartoo], but have not been widely adopted on the Web. Based on the query results, many se arch engines offer suggestions for modifying the query. Common suggestions are alternate spellings or alternate words su ch as synonyms. Some engines offer to create a more complex query based on the original query. For example, relevance feedback is a technique for increasing search precision by expanding th e number of terms in a query [Rocchio71, Salton90]. The  X  X earch within results X  feature found on some engines also narrows a que ry by adding new terms to the original query. A primary challenge confronting any user-interface innovation is innovations have either not proven significantly bene ficial in lab experiments or not proven popular among real users, thus the single-line search box and ranked-lis t results still dominate on the Web. In this study we evaluate sear ch iteration via phr ase suggestions for both narrowing and broadening queries, where the suggested phrases are either super or subphr ases of the original query. We seek to answer two questions: A Boolean search query consists of search terms and the operators AND , OR , and NOT . The operators may be explicit, as in the query  X  X earch AND engine X , or implicit as in  X  X earch engine X . In the implicit case, the operator is chosen by default, typically either AND or OR . For more precision, search engines sometimes include an additional operator NEAR to force a proximity constraint on Most query modifications are aime d at narrowing or broadening a query by either changing the operators connecting the terms or changing the terms. For example, in response to a query that generates few or no results due to a misspelling, a search engine might suggest changing the spelling of a term. For a query that generates too many ma tches, an engine mi ght suggest using a stricter operator such as exact-phrase match. When the terms are held constant , Boolean operators impart an ordering on the sets of results. Let R op represent the result set for a query Q with two terms, x and y , connected by the operator op : Then window of w terms. For a query with k terms, the operator is equivalent to NEAR ( k ). CTVC may be built into the ranking function of a search engine. For example, search engines that default to Boolean disjunction (
OR ) often use a ranking function that gives higher score to documents containing all search terms ( AND ), and may boost documents where the terms appear close together ( NEAR result list is thus roughly ordered by the R op sets, with elements of R
NEAR appearing before elements of R AND appearing before elements of R OR . Ranked list algorithms, like those based on the vector space model, also rewa rd documents that match more query terms, thus they too gene rate results in an order roughly sorted by the R op sets. CTVC may also be the basis fo r a strategy to suggest query modifications. Based on the length of the result list for a query, a search engine may suggest cha nging the query to use the next more restrictive or less restrictive operator. For example, if a query using AND returns too many results, the engine might suggest changing AND to NEAR [Gauch91, Jansen05]. Although easy for searchers to understand, CTVC suffers from a lack of granularity. The more lenient operators especially OR , can generate large result sets that include many irrelevant elements. When the constraints are held c onstant, the effect of adding or deleting terms depends on what ope rators connect the terms. Let R ( Q ) be the result set from the query Q whose terms are connected by the operator op. Consider two queries Q 1 When op is disjunctive, adding a term to a query increases the size of the result set; that is, R OR ( Q 1 )  X  R conjunctive, adding a term redu ces the size of the result set: R and conjunction, this inconsiste ncy can be very confusing. As already pointed out, ranked-list search engines effectively implement both OR and AND , so it is unclear whether adding more terms increases or decreases the size of the result set. By contrast, for a search engine that just implements conjunctive search, CCVT provides a consistent foundation on which to make suggestions that narrow and broaden queries. Proximity-based search engines produce high-precision results at the expense of low recall [Khan04]. Although strict proximity search has not stood out in standard TREC ad hoc experiments, proximity search has intuitive appeal because its operation is easy for searchers to understand. We have implemented a search e ngine that implements proximity search by using NEAR (32) as the default operator between query terms. As proximity search is st rictly conjunctive, adding terms to a query reduces and removing term s expands the result set. We utilize this property to suggest query modifications when a query generates a results set with tw o few (&lt;2) or too many (&gt;14) elements. In the former case, the suggestions are proximal subphrases to broaden the query and in the latter they are super phrases to narrow the query. The order of the terms in a query is not considered when constructing phrases, thus a k -term query Q may be represented candidate super phrases at in dexing time through a lexical analysis of the corpus. Analogously, S is a subphrase of Q if S  X  Q. We call our subphrases proximal because they must satisfy only the constraint rather than PHRASE . Effectively, can didate subphrases are identified at query time by performing a proximity search for each element of 2 Q . For an example of phrasal sugges tions, we ran the query  X  X ncome tax X  against a subset of the wt10g corpus described below in Section 4.2. The search engine output The counts indicate how many tim es the terms in the phrase appear adjacent in the corpus. The word order and capitalization shown are taken from one occurrence of the phrase. By contrast, the query  X  X ncome tax rate in Europe X  returned Following the suggestions is the ranked result list. After documents are selected that meet the search constraints, they are ranked according to a scoring function that considers the number of term occurrences, the location of the terms, and the size and age of the document. In spirit, our super phrases are s imilar to those generated by Anick from top-ranked documents and past queries in the Prisma system [Anick03], and by Gauch from a back-of-the-book index in an expert system for searching [Gauch91]. The hyperindex meta-search engine described by Bruza and Dennis appears to be closest to our approach of na rrowing and widening [Bruza97]. Unlike all these systems, our super phrases are generated directly from the full corpus text and ranked by their length and frequency of occurrence. Gutwin et al. in the Keyphind system had success generating phrases from the full te xt, but they processed queries [Gutwin99]. Although many features have been suggested for improving the search process, search engine users tend to be conservative in their choice of tools. Relevance feedback, for example, has been shown to improve search precision in TREC studies, but is largely ignored when offered as a feature [Jansen00]. Encouragingly, in an experiment adding phrase sugge stions to AltaVista, Anick found that 14% of search sessions u tilized the feature at least once [Anick99]. To evaluate whether searchers w ould use phrasal suggestions, we conducted a study of search logs on a commercial search engine that was modified to include phrase suggestions. Data was collected before and after the modification. Two Web-wide studies, using data from Excite [Jansen00] and characterization of user behavi or during Web search: just over two terms per query, around two queries per session, little use of search operators, and the primacy of the first page of results. When compared to more tradi tional IR settings [Koenemann96, Siegfried93], Web users use much shorter queries and sessions. Chau et al. studied search logs for the Utah State Government website [Chau05]. They found the general metrics of terms per query and queries per session to be similar to Web-wide search, and little explicit use of search operators. Wang et al . analyzed search logs at the University of Tennessee over a four-year period [Wang03]. They too found many similarities with Web-wide search as well as little change in search behavior over the four-year period. The search engine used in this study is operated as a service to websites. The software for crawli ng, indexing, and serving queries is run under contract by the service company. Websites run queries by setting the  X  X ction X  URL in a search form to point to the search server. We analyzed the logs for about 200 sites governmental, such as cities, counties, and state and federal agencies. About 20 percent are mu seums, schools, and historical societies. The rest are commercia l sites. About 95% of the sites are based in the U.S. with the remainder in Canada and the U.K. Nearly all the documents are in English. All but a few of the websites are accessible to the general public. The search indexes vary in size from less than one hundred documents to several hundred t housand. Most cover just one website, but about 10 percent cover a group of sites; the largest being about 100 sites. Weekly ac tivity ranges from a few queries per week to over 6000. For a repr esentative week, the mean was 420 and the median 200 queries. The search forms sit directly on each client website. The design of each form is under the control of the website designer, so there is wide variation in form appearan ce and some variation in default search options. The app earance of the search results is also under the control of the site designer, although the page extract and follow-up operations are nearly the same across all indexes. The baseline and test data were each collected over two-month periods. In the fall of 2005 appr oximately 680K queries were logged. In the fall of 2006 we upgraded the search engine to offer phrasal suggestions and then co llected data for another two months. This time, about 740K queries were logged. Each log entry had the following fields: Table 1 contains the raw data from the search engine logs collected over two months during the fall of 2005. Preliminary analysis showed a significant numb er of searches by Web robots, which we wanted to eliminate. To identify robots we extracted the URL from Web-server requests of the  X  X obots.txt X  file. In addition, a few robots were identi fied by looking for IP addresses with very high query counts over short periods of time. The raw log files were processed to remove queries from robots and to convert all queries to a canonical form. We assigned each query to a session. Two successive queries were considered part of the same session if they came from the same IP address and were separated by less than 15 minutes , as suggested by He and G X ker [He00]. The mean number of terms per query was 1.89 and the mean number of queries per sessi on was 1.93. As can be seen from Table 2, our numbers are similar to those from other studies. These numbers are presented to show that users of our search engine exhibit behavior similar to users of other Web-wide and single-site search engines. Detailed cross-study comparisons are unreliable because the rules fo r counting queries and sessions are not uniform. The search engine was upgraded in the fall of 2006 to include phrasal suggestions. Before making the change, we collected data for one week to compare to the baseline to rule out any temporal effects. Table 3 shows the data from the baseline and test periods. In the table, data from the one -week baseline period is labeled  X  X aseline 06 X . The difference between the two baseline periods is insignificant; searchers exhibited little change in behavior over the year. However, the difference in behavior once phrasal query suggestions were added is striki ng. Both the average query length and session length showed signifi cant increases. The column labeled  X  X erms Ratio X  is the ratio of Terms/Query for the current row to Terms/Query for the baseline.  X  X ueries Ratio X  is the analogous value for Queries/Session. After an initial query, a searcher may issue a follow-up query. Follow-up queries are formed by e ither modifying or replacing the original query, or by followi ng a search engine suggestion. Follow-up queries represented 48% of the total queries before suggestions were added and 61% af ter they were added. Table 4 shows the composition of the follow-up queries. The column  X  X uggest X  shows the percent of follow-up queries that were the result of a search-engine suggesti on. Suggestions in the baseline search engine consisted of spel ling corrections, or if no results were found, offers to search fo r the query terms individually and to relax the search constraints (e.g., NEAR to data, suggestions includ e super and subphrases. 
Table 4. Follow-up actions as a percent of total follow-up We found searcher behavior, as measured by the length of queries and sessions, to be similar to that reported by other studies. Perhaps somewhat surprisingly, the behavior is the same whether searching the whole Web or just a single website. In addition, the behavior does not appear to have changed over the course of the year; log data from Fall 2006 looks very similar to that from Fall 2005. Adding phrasal suggestions did create immediate and striking changes in behavior. The average query length increased nearly 20%, as might be expected becau se super phrases tend to be longer than the two-term mean and subphrases are no shorter. Average session length also increased, by about 30%, as searchers A modified query shares at least one term with the previous query. A replacement query shares no terms with its predecessor. followed more search engine suggestions. Indeed, once phrase suggestions were added to th e search engine, suggestions represented over 50% of follow-up queries. In our baseline, and in prior studies, the most common follow-up query action was a complete replacement of the original query, what we have called a  X  X eplace X  query. There is a high likelihood guessing. From the test data, the increase in choosing suggestions was accompanied by a decrease in query replacement. It appears searchers were following suggestions instead of typing a completely new query. To assess whether phrasal suggestions are helpful to searchers using proximity search, we carried out a controlled between-groups within-subject study. User studies to test search engine features have been notoriously difficult to judge and compare. Variance introduced by search tasks and searcher ability often swamp any effect due to a specif ic feature [Hersh00]. Conclusions the improvements are not statistic ally significant [Brajnik96]. Also, differences in experiment design and user interface often make cross-study comparisons unreliable. In an attempt to make results more comparable, guidelines for running an IR user study were created for the TREC Interactive Track [Hersh02]. Over a four-year period Belkin et al. ran a series of studies assessing relevance fee dback and explicit query term experimental procedure had evolve d to be similar to the TREC Interactive Track protocol. Where appropriate we followed the TREC lead. We were also influenced by a recent study by Jansen and McNeese [Jansen05]. 24 participants were recruited fr om the general student population on campus. Each participant carried out nine search sessions. During a session, the participant searched for one of nine topics using one of the three search engines being tested: Participants were given a standard TREC topic statement in one window and a simple one-line sear ch box in another (See Figure We used the RetEval search engine from the Lemur Toolkit distributed by the Lemur Proj ect (www.lemurproject.org). 1). They were given ten minutes to find as many Web pages from our collection relevant to the topic as they could. In response to a query, the search engines output page extracts that show the query terms in context along with a link to the page. To indicate that a page is relevant, the participant clicked a checkbox below the extract. The pairing of search engine and topic in a session were rotated so that every search engine was paired the same number of times with each topic. We also rotated the starting topic to eliminate ordering effects. All of the participants were fluent in English and comfortable using a Web browser. Counter to many prior studies, the pa rticipants received no training, were not given any tutorial information, and were not alerted to the differences in the search engines. As document collections have grow n in size, many people have pointed out that, for most search es, precision is more important than recall. To test phrasal suggestions in the context of high precision search, we wanted a rich document set with fine-grained relevance judgments. For the TREC Web Track beginning in 2000, Web pages from the .gov domain were collected to creat e a corpus of about 1.6 million Web pages, known as wt10g , with approximately 10 GB of text [Baily03]. Associated with wt10g are 50 query topics with ternary relevance judgments (TREC topics 451-500). It is important to note that the number of judged docum ents is a small percentage of the total, just 70,070 out of the 1.6 million. Of the judged documents, 2371 were judged releva nt for some topic, 246 highly relevant, and the rest not relevant. To keep directory sizes manageable, the documents in wt10g are arranged in a four-level directory hierarchy. The search engine we used has 2 GB limit on text, so we trimmed branches off the hierarchy to create a subset of the collection. Because we are assessing high-precision recall, it is important that the document subset include all documents j udged highly relevant, thus the collection was trimmed as follows: The result was a collection of about 465K documents containing approximately 1.65 GB of text. From the 50 query topics with ternary judgments, we chose the 9 topics that had the most documents judged highly relevant (TREC topics 495, 494, 454, 452, 453, 468, 476, 463, and 457). Table 5 shows a summary of the per session means organized by search engine. The row  X  X iRel X  is the mean number of highly relevant documents found and  X  X iRel Recall X  the mean recall of documents judged highly relevant by TREC (i.e., HiRel/Total documents judged highly relevant). The row  X  X el X  is the mean number of documents judged relevant. 5 The row  X  X ot Judged X  is For example, WT10G1/WTX001/B35/. We don X  X  report  X  X el Recall X  becau se not all relevant documents were in our collection. the mean percent of documents f ound by our participant for where there is no relevance judgment. 
Table 5. Means per user and session. A * indicates the value for SE1 is a significant improvement over SE2 at the 95% We used ANOVA to look for signifi cant differences in the data. Plot 1 shows mean recall for each of the search engines along with their 95% confidence (Tukey/ LSD) intervals. The difference in recall of SE1 is significantly better than that of SE2. As others have pointed out, there is considerable variance in the data due to search topic. Plot 2 shows the mean recall for SE1 and SE2 by topic. Although for half th e topics the performance of SE1 and SE2 are essentially the same, when they differ, SE1 outperforms SE2. The user data confirm the efficacy of phrasal suggestions when added to a proximity-based search engine. SE1 outperformed SE2 in recall of highly relevant docu ments. The difference in mean HiRel recall is significant at the 95% level. From the 13% increase in the number of queries issued with SE1 versus SE2, we conclude that phrasal suggestions made it easier for searchers to create queries. Indeed , suggestions made up 293 of 1019 queries issued to SE1. The study included an Okapi BM25 search engine, SE3, to give a baseline comparable to other studies. BM25 was commonly used in the TREC Web track and performed very well [Craswell04]. In our results, SE1 trended higher than SE3 in recall, but the differences were not statistical ly significant. Three numbers do stand out, however. First are the total number of docum ents marked relevant and the number of queries issued. As judged by our searchers, SE1 outperformed both SE2 and SE3, and SE2 outperformed SE3. The primary interface difference between SE1/SE2 and SE3 is the length of the result lists. SE1/SE2, being proximity-based engines, tend to produce short results lists, while SE3 always returned a list of 100 results. This difference is reflected in the relatively few queries issued to SE3; presumably, searchers were scanning the long result list instead of issuing new queries. Another interesting difference is the number of not judged documents found by SE1 versus SE3. As noted earlier, the TREC documents chosen for judgment are those ranked highly by the search engines participating in TREC. Many people have observed this subset creates a bias against search engines based on novel ranking algorithms [Zobel98, Buckley06]. We thus believe the results compared to SE3 are conservative; complete relevance judgments would benefit SE1 and SE2 more than SE3. We have proposed a unified approach to query refinement suggestions in the context of a s earch engine tuned for precision over recall. Refinements are in th e form of adding terms to and deleting terms from a query, having the effect of narrowing and widening the query. We asked two questions about our approach: Would searchers utilize phrasal query suggestions and would these suggestions help searchers find what they are looking for. Through a log study of a commercial search engine we found that searchers will use phrase suggestions, with a resultan t increase in both the average length of queries and search se ssions. Through a controlled user study we found that phrasal sugge stions significantly increased recall for highly relevant documents. We also found that searchers using proximity-based search engines, both with and without phrasal suggestions, found more total documents than those using a ranked-list engine implementing the Okapi BM25 ranking algorithm. In addition, the difference between total docum ents found and those judged highly relevant by TREC appears to reveal a slight bias of the TREC judgment set in favor of search engines implementing a common ranking algorithm. [Anick99] Anick, P. G. and Tipi rneni, S. The paraphrase search [Anick03] Anick, P. Using te rminological feedback for web [Baily03] Bailey, P., Craswell, N. , and Hawking, D. Engineering [Belkin01] Belkin, N. J., Cool, C., Kelly, D., Lin, S., Park, S. Y., [Bruza97] P.D. Bruza and S. Denni s. Query ReFormulation on the [Buckley06] Buckley, C., Dimmick, D., Soboroff, I., and [Chau05] Chau, M., Fang, X., and Sh eng, O. R. 2005. Analysis of [Clusty] http://www.clusty.com [Craswell04] Nick Craswell and David Hawking. Overview of the [Cutting92] D. R. Cutting, D. R. Ka rger, J. O. Pedersen and J. W. [Gauch91] Gauch, S. and Smith, J. B. Search improvement via [Gutwin99] Gutwin, C., Paynter, G., Witten, I., Nevill-Manning, [He00] Daqing He &amp; Ay  X  e G X ker. Detecting Session Boundaries [Henninger95] Henninger, S. and Be lkin, N. Interface issues and [Hersh02] William Hersh. TREC 2002 Interactive Track Report. [Jansen00] Jansen, B. J., Spink, A., and Saracevic, T. Real life, [Jansen05] Bernard J. Jansen and Michael D. McNeese. [Kartoo] http://www.kartoo.com [Khan04] S. Khan, H. Jameel, A. Sajjad, and H. Iqbal. Evaluation [Koenenmann96] Koenemann, J. a nd Belkin, N. J. 1996. A case [Northernlight] http://www.northernlight.com/ [Rao95] Rao, R., Pedersen, J. O., Hearst, M. A., Mackinlay, J. D., [Robertson94 ] S E Robertson, S Walker, S Jones, M M Hancock-[Salton90] Salton, G. and Buck ley, C. Improving retrieval [Siegfried93] Siegfried, Susan, Ma rcia J. Bates, and Deborah N. [Silverstein99] Silverstein, C., Marais, H., Henzinger, M., and [Wang03] Wang, P., Berry, M. W., and Yang, Y. Mining [Zamir99] Oren Eli Zamir. Zamir, O. and Etzioni, O. Grouper: a [Zobel98] Zobel, J. How reliable are the results of large-scale 
