 Search engines receive queries with a broad range of different search intents. However, they do not perform equally well for all queries. Understanding where search engines perform poorly is critical for improving their performance. In this paper, we present a method for automatically identifying poorly-performing query groups where a search engine may not meet searcher needs. This allows us to create coherent query clusters that help system design-ers generate actionable insights a bout necessary changes and helps learning-to-rank algorithms better learn relevance signals via spe-cialized rankers. The result is a framework capable of estimating dissatisfaction from Web search l ogs and learning to improve per-formance for dissatisfied queries. Through experimentation, we show that our method yields good quality groups that align with established retrieval performance metrics. We also show that we can significantly improve retrieval effectiveness via specialized rankers, and that coherent grouping of underperforming queries generated by our method is important in improving each group. H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval  X  selection process, search process. Dissatisfied query groups; Search satisfaction; Specialized rankers. Search engines are emerging as the primary resource through which people find information. As a result, they are expected to handle a broad range of different types of requests. However, search engines may only apply a single ranking function over all queries they re-ceive [35]. Inevitably in such circ umstances, there are queries that search engines do not handle well, leading to user dissatisfaction (DSAT). Automatically identifying groups of underperforming DSAT queries could help search designers decide where to focus resources to improve performance. There has been work on studying searcher success and frustration as well as measuring query performance through human labels or via automatic methods [13][14][20]. Those methods have focused on identifying individual instances of user dissatisfaction from log data. However, from the search engine X  X  perspective it is not cost effec-tive to simply identify particular queries where the engine performs poorly. Commercial search engines are trained over very large sets of queries. When a poorly performin g query is identified, this signal can be difficult to use for improving ranking quality. First, it is only a single example, among millions of others, so even if it is added to training model. Moreover, machine learned models are difficult to debug or interpret and hence we will most likely be unable to de-termine why a particular query fails. Finally, human intervention, either for the generation of new interface, for ranking features, or for manually labeling more examples of queries, is costly and it is not prudent to make these investments on a per-query basis. There-fore, we must be able to combine the queries into coherent groups to take action and improve them. In this paper we present an approach for identifying groups of que-ries where search engines perform poorly and develop methods to automatically improve retrieval qua lity for groups of failing queries. Dissatisfied query groups are identified automatically from search engine logs. The resulting groups have a strong correlation with two established measures of search performance: search-result click-through rate (CTR) and normalized discounted cumulative gain (NDCG) [26]. The query groups that our method identifies therefore comprise queries that are likely to result in searcher dissatisfaction. The queries in these groups share common attributes and are coher-ent in a way that makes them susceptible to ranking improvements. As such, we also trained a specialized ranker that is optimized to a particular dissatisfaction group that yielded significant relevance gains over our baselines. This paper makes the following research contributions:  X  Proposes the automatic identific ation of underperforming (dis- X  Introduces a novel method for automatically identifying such  X  Validates that the groups that our method identifies are indeed  X  Develops specialized rankers that leverage the coherence of the  X  Demonstrates through experimentation that: (i) our learned The remainder of this paper is structured as follows. Section 2 de-scribes relevant related work in the areas of search success and query performance. Section 3 defines our problem and Section 4 describes how we extract instances of dissatisfaction from log data. We describe our method for automatically grouping dissatisfaction instances in Section 5. In Section 6 we describe the training of a specialized ranker to target a typical DSAT group identified using our method. Experiments and associated results are presented in Section 7. We discuss the findings and their implications in Section 8 and conclude in Section 9. There are two areas of work related to the research presented in this paper: (i) search satisfaction, succe ss, and frustration, and (ii) query performance. We cover each of the two areas in turn. Extensive literature exists on deriving indicators of task success or failure from online user behavior. In search specifically, several fruitful approaches have been tried. One approach is to correlate behavior with either self-reported success [14] or labels of success provided by expert judges [13][20]. Early investigations correlated self-reported measures of search satisfaction with implicit signals, such as search-result clicks a nd dwell time for clicks [14]. Fox et al. [14] used an instrumented browser to determine whether there was an association between explicit ratings of satisfaction and implicit measures of searcher in terest and identified the measures that were most strongly associated with user satisfaction. They found that there was a link between user activity and satisfaction ratings, and that clickthrough, dwell time, and session termination activity combined to make good predictors of satisfaction for Web pages. Fox et al. found that short dwell times and clicking many (four or more) search results for a query were both indicators of dissatisfaction. Behavioral patterns were also used to predict user satisfaction for search sessions. Feild et al. [13] developed methods to predict user frustration. They assigned users difficult information seeking tasks and monitored their degree of frustration via query logs and physical sensors. One behavior that can be associated with dissatisfaction is search engine switching  X  the vo luntary transition between different engines. Guo et al. [16] characterized the reason that searchers switch between search engines. Using a browser plugin, they captured switching rationales in-situ. They showed that one of the primary reasons that searchers changed engine was dis-satisfaction with the results provided by the pre-switch engine. Us-ing the labeled switching events gathered from the plugin, they also showed that there was sufficient consistency in searchers X  interac-tion behaviors before and after the switching event to accurately predict switching rationales. This pr edictor can be used to dynami-cally adapt the search experience and derive more accurate competi-tive metrics. Indeed, Feild et al. showed that features capable of accurately predicting switching events were also highly predictive of searcher frustration. Huffman and Hochster [25] found a relatively strong correlation with session satisfaction using a linear model encompassing the search task, whether the information need was navigational, and the number of events in the session. Hassan et al. [20] developed mod-els of user behavior to accurately estimate search success on a ses-sion level, independent of the relevance of documents retrieved by the search engine. Ageev et al. [1] propose a formalization of differ-ent types of success for informational search, and presented a scala-ble game-like infrastructure for crowdsourcing search behavior studies, specifically targeted towards capturing and evaluating suc-cessful search strategies on inform ational tasks with known intent. They show that their model can predict search success effectively on their data and on a separate set of log data comprising search engine sessions. Going beyond individual search sessions to study the effects of satisfaction over time, Hu et al. [23] performed a lon-gitudinal study of the relationship between search satisfaction and search engine re-use. They showed that satisfaction with a search engine can contribute to users X  propensity to use and re-use its ser-vice over time. Search engine performance for a particular query is typically meas-ured using relevance metrics such as NDCG [26]. NDCG can be calculated based on manual judgments on the relevance of docu-ments in the result list, or estimated using models derived from user click behavior (e.g., [2]). Session discounted cumulative gain (SDCG) [27] applies a discount to relevant results found in response to queries later in the session. Th is considers that multiple queries can be part of a search goal, but still requires manual relevance judgments, which can be costly to obtain and do not scale to unseen queries. Al-Maskari et al. [4] found a reasonable correlation be-tween many information retrieval (IR) metrics and user satisfaction with result rankings (although surprisingly not NDCG, given the small numbers of judgments availa ble per query). They advocated for a combination of different measures in evaluating the effective-ness of IR systems. Research on predicting query performance has been conducted to understand differences in the quality of search results provided by search systems for different queries. Measures such as Jensen-Shannon divergence [8], query clarity [10], and weighted infor-mation gain 0 have been developed to predict the retrieval perfor-mance on a query (as measured by average precision, for example). He and Ounis [23], and Hauff et al. [22] also developed automatic methods for predicting query difficulty (i.e., how good the search results are for a query). Leskovec et al. [29] used graphical proper-ties of the link structure of the result set to predict the quality of the result set and the likelihood of qu ery reformulation. Teevan et al. [31] developed methods to predict which queries could most benefit from personalization. Research has also been conducted on predict-ing query performance using searcher interactions. Carterette and Jones [9] used clickthrough behavi or to evaluate the quality of search advertising results, but they did not study other interaction features, and their focus was on search advertising not general Web search. Guo et al. [14] used inte raction features, including switching features, to predict query performance. However, that was on a per-query basis rather than on the basis of query groups that lead to searcher dissatisfaction. The research presented in this paper extends this previous work in a number of ways: (i) rather than predicting the performance of indi-vidual queries, we focus on identifying underperforming query groups that share attributes such as length or domain; (ii) we vali-date that the groups represent likely searcher dissatisfaction via established performance measures such as clickthrough rate and NDCG, and; (iii) we train a specia lized ranker for groups identified and show that (a) we can obtain relevance gains by targeting specif-ic DSAT groups, and (b) training a specialized ranker tailored to the coherent DSAT groups identified by our method is better than simp-ly targeting the broad set of all underperforming queries (e.g., only those queries with low CTR). We start by defining some terms used throughout the paper: DEFINITION. A query group is a set of queries sharing a set of common factors and can be characterized using binary attributes. For example  X  X ueries about movies with 5 or more words X . DEFINITION. A DSAT query group is a set of queries for which the search engine performs poorly in comparison to its average performance over all queries. DEFINITION. A specialized ranker is a ranking model that is built to handle a specific query group. The specialized ranker has a local effect because it only provides the search results for queries belong-ing to a specific query group. Given a stream of queries submitte d by search engine users, we seek to: (i) automatically identify poorly-performing groups of que-ries suggestive of searcher dissat isfaction (DSAT), and (ii) train a specialized local ranker capable of addressing the DSAT through relevance gains. To identify the underperforming query groups we construct attribute sets that co ver DSAT instances and correlate highly with dissatisfaction. Commercial search engines rely on features of log data. They also need humans to define these features and to generate relevance labels from which the ranking algorithms can learn [2]. In theory, a search engine that adopts a framework capable of doing both (i) and (ii) above could improve with little need for human involvement in generating additional features which could result in a self-correcting search engine. The development of such a framework motivates our research. We now describe its implementation in our context. The input to our method is the sear ch log interaction data gathered from consenting users of a toolbar deployed by a commercial search engine. Search logs are usually or ganized in the form of search sessions. A search session is a sequence of user activities that begin with a query, includes subsequent queries and URL visits, and ends more than 30 minutes on a page. The 30-minute cutoff to determine session boundaries has been common ly used in previous work, e.g., [12]. In our case, sear ch sessions can span multiple search engines since we are interested in transitions between engines as evidence of searcher dissatisfaction. More discussion on this issue is provided in the next section of the paper. We use the data from search sessi ons to automatically identify que-ries that resulted in searcher dissatisfaction. This step is the first step in our process. This module uses log data (from the search engine or in our case a toolbar from the search provider) to generate a large number of individual dissatisfaction instances. For each instance, we collect information about the query, time and market where it was submitted, and the search engine results page (SERP). Individ-ual dissatisfaction instances were used to identify poorly performing groups of queries. Specialized rankers specifically targeting the identified groups are then trained to improve the search experience. One specialized ranker is trained for each dissatisfaction group. The specialized ranker can use the same features used by the original ranker, but trained using a group-specific data set. Alternatively, additional investment may be involved to fu rther improve these groups. For example, once the groups have been identified, additional features may need to be generated to help the ranker learn how to perform better for this query group. This activity is labor intensive and time consuming, meaning that an automated alternative such as the method proposed in this paper can be extremely valuable. Finally the specialized ranker is combined with the general ranker and the resultant ranker can be used to replace the original. The specialized ranker has a local effect because it will be used for que-ries that belong to the corresponding group only. The general ranker will be used for the remainder of the query traffic. If gains are achieved through the specialized ranker and all other queries are treated in the same way as before, then overall relevance gains will be observed. The primary challenge is th erefore in identifying dis-satisfaction groups because enumerating all possible ways of com-bining attributes to form groups is prohibitively expensive. Address-ing this challenge is ou r focus in this paper.
 We begin by describing how we identify the individual DSAT in-stances that are used to construct the DSAT query groups. We mine instances of searcher dissatisfaction based on search en-gine switching events. Search engine switching is the process de-scribing a user X  X  voluntary transition from one Web search engine to another. A search engine switching event is a pair of consecutive queries that are issued on different search engines within a single session. Note that in identifying pre-switch queries, if the user is-sued a navigational query for a target search engine (e.g., search for  X  X ahoo X  on Google, or  X  X oogle X  on Bing), this query is regarded as part of the switching action and the preceding query in the pre-switch engine is used as the  X  X re-switch X  query. Search logs contain a large and varied set of search behaviors that could be associated with searcher dissatisfaction (e.g., clickthrough rate, abandonment, short dwell time or quick back clicks). We use search engine switching to identify user dissatisfaction instances for two main reasons: (i) search engine switching is a rare but important event that has been shown to correlate with dissatisfaction in prior research [16] [32], and we have a reliable classifier that can distin-guish switches caused by dissatisfaction from others; (ii) other que-ry performance measures such as clickthrough data or editorial judgments could be used, but we abstained from using them in this phase in order to be able to use them later to validate our results. After the validity of our dissatisfied query groups is established through our study, these query performance measures could be used to generate more dissatisfaction instances. Users switch from one search engine to another for various reasons. One of the most common reasons is dissatisfaction with the results they received from the pre-switch engine [33], which accounts for around 60% of engine switching events. Guo et al. [16] proposed a method for estimating the cause behind an observed switch given recent search behavior such as queries and SERP clicks on the source and destination search engines. They gathered ground truth data through the deployment of a plugin to around 200 Web search-ers. The plugin captured switching rationales in-situ when a switch occurred and also logged search behavior before and after the switch. The cause of the switch could be one of: (i) dissatisfaction (the searcher was unhappy with the pre-switch engine), (ii) coverage (the searcher wanted to check the information they had found on the other search engine), (iii) preferen ces (they usually used the target engine or the target engine was bette r for the current task type), (iv) unintentional (browser defaults or homepage settings), and (v) oth-er. They found that they could accurately predict when the reason for an engine switch was dissatisfaction-related .
 Inspired by the work of Guo and co lleagues, we built a classifier to distinguish dissatisfaction-related engine switches. The dissatisfac-tion engine switching classifier learns to predict the switch cause from training data of switching instances from [16]. The dataset contained 562 labeled switches from 107 different users. We merged all switching causes into two classes: dissatisfaction and everything else, and trained a logistic regression classifier to identi-fy dissatisfaction instances. DEFINITION. A dissatisfaction instance in our study is a switch between search engines where the pre-and post-switch queries are the same and has been labeled by our classifier as DSAT-related. We assume that a dissatisfaction instance represents dissatisfaction with the pre-switch engine. We represent search behavior by adapting and extending a subset of the features presented in previous studies that we believed were particularly likely to be associated with dissatisfaction [16]. We also added features describing the action transition of users (e.g., query-click, query-query, etc.), and the time difference between every pair of actions. It has been shown in previous work that action transi-tions are a very good descriptor of user behavior when modeling satisfaction [20][21]. An action could be a query submission, a click on a SERP, or a click on any other SERP feature. For example, submitting a query followed by reformulating and resubmitting a related query is a sign of dissatisfaction. The features used in our classifier are summarized in Table 1. We only considered switching instances that had the same query before and after the engine switch since these have been shown to be more strongly associated with dissatisfaction in pr evious work [16]. Later in our experiments, we wi ll use only query-related features and post-switch features to identi fy switches resulting from DSAT. Excluding pre-switch features allows us to validate the results of our DSAT group identification usi ng measures such as SERP click-through rate without introducing any bias. We evaluated the performance using the F-score, with  X  set to 0.5. This gives twice as much weight to precision than to recall. We are interested in a highly precise set of dissatisfaction cases that we can reliably use as input for the following steps. There are a large num-ber of switching instances in the l ogs. We need to precisely identify some of them, rather than finding many of them with lower preci-sion. We evaluated the performance using 10-fold cross validation which resulted in an F 0.5 score of 81.2 when we use query and post-switch features. This exceeds the 79. 0 reported by Guo et al. [16], attributable to the extra features we added. Many previous studies have addressed the problem of measuring query performance [10][26][29]. Individual instances where users are dissatisfied with search result quality can be identified using a number of methods. One such method was presented in the previous section. Given a set of DSAT instances, the more challenging prob-lem is how to take actions to improve search relevance based on these observations. This is challenging because it may be difficult to tweak the search engine ranking algorithm to handle a dissatisfac-tion case in an isolated context. To provide richer context to dissat-isfaction cases, we seek to find frequently-occurring patterns that define dissatisfaction groups . In the remainder of this section we describe the methods that we use to identify dissatisfaction groups. We begin with how we represent the individual DSAT instances, th en describe the frequent pattern mining approach that we used to identify frequent attribute sets, and conclude by describing the method used to create the dissatisfaction groups from these attribute sets. We describe every dissatisfaction in stance with a vector of binary attributes. Each attribute describes a specific characteristic of the dissatisfaction instance. Let = X  {  X   X   X ,  X   X ,...,  X  } be a set of  X  binary attributes. Let  X {= X   X   X ,  X   X ,...,  X  } be a set of dissatisfaction in-stances. Every DSAT instance in  X  has a subset of attributes in  X  . We use several types of attributes to describe each dissatisfaction instance. We summarize them below: Query Attributes: This is a set of attributes that describe the query itself. Several categories are used to describe the intent behind the query. It is impractical to use the text in Web pages to draw conclu-sions about the topicality of the query. Conversely, we could look at URLs or domains but that would be very limited due to data sparse-ness. Instead, we used the Open Directory Project (ODP), also re-ferred to as dmoz.org. ODP is an open Web directory maintained by a community of volunteer editors. It uses a hierarchical scheme for organizing URLs into categories and subcategories. We use the top two ODP categories and assign labels to URLs automatically using an approach similar to [34]. URLs that exist in the directory were classified according to the corresponding categories. Missing URLs were incrementally pruned one path level at a time until a match was found or a miss declared. A query is assigned the plurality label of the labels of its top 10 results. In addition to these categories, we also include the query length, language, query phrase type (noun phrase, verb phrase or a question) as determined by the Stanford parser [28]. SERP Attributes: We also added a set of attributes to describe the SERP that was displayed to the us er as a response to submitting the query. Examples of these attributes include whether a direct answer was displayed for that query, and if so, then what type of answer (e.g. Weather, Stocks, etc.), whether a query suggestion was shown, and whether a spelling correction was shown. Impression Attributes: An impression is a single instance of a particular query. We used another set of attributes to describe each dissatisfaction impression. We use the market from which the query was issued (e.g., US, UK, etc.), the vertical used to issue the query (e.g., Web, News, Images, etc.) and the search engine used to issue the query (Bing, Google or Yahoo). This allows us to identify DSAT groups for specific markets, verticals, and engines as well as DSAT groups that span multiple markets, verticals, or engines. We also added some temporal attributes such as day of the week, time of day (morning, afternoon, evening, night), and month of the year. This yields a set  X  with 140 attributes. Every dissatisfaction in-stance  X  is represented with a subset of attributes in  X  . Table 2. Features to identify dissatisfied engine switches.  X  X  X _ X  X  X  X  Num. characters in query  X  X  X _ X  X  X  X  Num. words in query  X  X  X  X _ X  X  X  X  Time in seconds between pre-switch and post switch queries  X  X  X  X  X  X  X _ X  X  X  Num. queries in session  X  X  X  X  X  X  X _ X  X  X  X  Num. unique queries in session  X  X  X  X  X  X _ X  X  X  Num. query reformulations  X  X  X  X  X  X _ X  X  X  Num. clicks  X  X  X  X  X  X _ X  X  X _ X  X  X  Num. clicks with dwell time &gt; 30s  X  X  X  X  X _ X  X  X  X  X _ X  X  X  Num. clicks with dwell time &lt; 15s  X  X  X  X  X  X _ X  X  X  X _ X  X  X  Num. clicks on URLs containing a query term in their title  X  X  X _ X  X  X  X  X   X   X  X  X  X  X _  X   X  X  X  X _ Avg. dwell time fo r every action pair  X  Identifying frequent patterns in transactional or relational data sets is a well-studied problem in the data mining literature [18]. The problem was motivated by the massive amounts of data continuous-ly collected and stored by many businesses. The discovery of fre-quent patterns in this huge amount of data was found useful for many decision making processes. A typical example of this scenario is the market basket analysis problem. The objective of this analysis is to find associations between the different items that customers purchase. This information is valuable for designing marketing plans and product catalogs. The problem of mining dissatisfaction groups can be approached from a similar perspective. To find dissatisfaction groups, we look for frequent patterns of attributes that correlate with user dissatisfac-tion. The representation we described in the previous subsection aligns well with this approach. In the proposed representation, every dissatisfaction instance is represented by a row, and every attribute is presented by a column. All values are binary; indicating whether the corresponding attribute is present in a particular row or not. Multi-valued attributes are replac ed with multiple variables corre-sponding to the different values. An attribute in our case is analo-gous to an item in the market basket analysis case. Note that we use the terms  X  X ttribute X  and  X  X tem X  interchangeably throughout the remainder of the paper. There are several algorithms for discovering frequent patterns from relational databases. For example, Apriori [5] is a seminal algorithm for finding frequent itemsets. It employs a level-wise search process where frequent itemsets of size  X  are used to discover frequent item-sets of size 1+ X  . It starts by finding the counts of single items, then itemsets with size 2 and so on. Finding each set of itemsets with a certain size requires a full scan of the dataset. To reduce the search space, it discards all itemsets that are supersets of previously found infrequent itemsets. The main disadvantage of Apriori is that it employs a generate-and-test method which may result in the generation of a huge number of candidate sets. Additionally, it may need to repeatedly scan the database for every itemset size. The FP-Growth algorithm [19] overcomes these problems via a divide-and-conquer strategy. FP-Growth only needs to scan the data set twice, and unlike Apriori, it does not involve any candidate items et generation. We use the We-ka [17] implementation of FP-Growth [19]. There are also variants of FP-Growth that were designed to be run in parallel on a distribut-ed cluster of machines, allowing la rge volumes of log data to be processed using this algorithm [11][30]. The FP-Growth algorithm consists of two main steps. In the first step, a compact data structure, the FP-tree, is built to represent the data. In the second step, frequent patterns are extracted from the FP-tree. To build the FP-tree, the data are scanned twice. In the first scan, it counts the frequencies of every attribute and discards infre-quent attributes. The attributes are then sorted in decreasing order based on their frequencies. This fixed order is used so paths can overlap when records share items . So, paths overlap when they share the same prefix. In the second scan, the FP-tree is constructed. The FP-tree is a com-pact way to retain information ab out itemset association [19]. Nodes in the tree represent attributes and each node has a counter. The algorithm reads one row at a time and maps it to a path. Paths over-lap when rows share attributes and node counters are incremented. Special pointers are used to link nodes representing the same attrib-ute. An example illustrating the FP-tree construction is shown in Figure 1. The algorithm starts by creating the root of the tree and labeling it  X  X ull X . For each row in the dataset, the items in that row are sorted according to their frequency and a branch is created to represent the row. Every node in the branch corresponds to an item and is labeled with the item identifier and its count; which is initial-ized to 1 at creation. This process is repeated for every row. A pointer is created to connect any two nodes with the same identifier and the node count is updated accordingly. Figure 1 shows the tree after reading the one row, two rows and all rows. After reading the first row, where only a and b are set to 1, two nodes are added to the cases. The second row has b, c, and d se t to 1. The second row does not share the same prefix with the first one, hence a new path is created and the counters of all nodes are set to 1 . When the algo-rithm reads the third row, it finds out that the prefix {a} overlaps process continues until all rows are scanned. Pointers are main-tained between nodes representing the same item (dotted lines in the figure). As explained earlier, the algorithm needs to scan the dataset only twice. In the first scan, the number of occurrences of every {English Query, Local, Na vigational, NumWords &gt;10} {English Query, Locations, Questions, NumWords &gt;10} {English Query, US Market} {NumWords &gt;10} Table 3. Dataset contains sati sfaction and dissatisfaction. Attribute_1 Attribute_2 ... Attribute_n (D)SAT attribute is computed and infrequent attributes are discarded. The FP-tree is created in the second scan. After constructing the FP-tree, we proceed to the second step. To extract frequent patterns, FP-Gro wth applies a bottom-up algorithm using a divide-and-conquer strategy starting at the leaves and mov-ing toward the root, building freque nt patterns as it ascends the tree. For example, it starts with frequent attribute sets ending in e and then ending in de and so on so forth. We apply the FP-Growth algorithm to the dataset described in the previous section. Table 2 presents a few examples of some of the identified frequent attribute sets. Some sets consist of a single at-tribute, while others have many a ttributes. In the next subsection, we describe how dissatisfaction groups can be identified using these frequent attribute sets. Frequent attribute sets discovered from a set of dissatisfaction in-stances do not necessarily define a dissatisfaction group. It could be the case that a set of attributes defines a large query group, and even though the percentage of dissatisfaction cases resulting from this group is very small, it could still be overrepresented in the dissatis-faction dataset. For example, FP-Growth reported that the group {  X  X  X  X  X  X   X  X , X  X  X  X  X   X  X  X  X  X  X  X  } is frequent. This occurred because most of the queries in the dataset are English queries coming from the US market, not because this is a dissatisfaction group. To address this problem, we built a new data set, the satisfaction dataset , and define a measure to identify groups correlated with dissatisfaction. We build this dataset by randomly sampling a num-ber of queries from  X  where: (i) the query received many clicks and the last one had dwell time greater than 30 seconds, or (ii) the query received a single click with dwell time greater than 30 seconds. Dwell time greater than 30 seconds has been widely used to denote satisfaction as in previous work [14]. Table 3 shows an example illustrating the appearance of the com-bined dataset. The dataset contains both satisfaction (SAT) and DSAT instances. Every instance is represented by a row in the da-taset. The same set of attributes is used to describe both types of instances. An additional attribute is added to show whether the in-stance is associated with satisfaction or dissatisfaction. We apply the FP-Growth algorithm to the data to generate sets of attributes that co-occur frequently with one another. Unlike the market basket analysis, we are not concerned with the correlation between all kinds of attributes. We ar e mainly interested in attribute sets that correlate highly with dissatisfaction. As a result, we restrict the generated sets to those that include the attribute  X  X SAT X  and ignore all other patterns. To distinguish attribute sets that are highly correlated with dissatis-faction and attribute sets that are highly represented in both satisfac-tion and dissatisfaction instances, we define  X  DSAT correlation  X . DSAT correlation is a simple measure that estimates the correla-tion/dependence between an attribute set and dissatisfaction. It is defined as follows for any attribute set = X  {  X ,..., X   X  } : where  X  (  X  ) is the probability of observing the set of attributes  X  (  X  ) .  X  (  X  X  X  X , X  ) is estimated by dividing the number of instanc-es that belong to both the group defined by  X  and the dissatisfaction set by the total number of SAT and DSAT instances.  X  (  X  ) is esti-mated by dividing the number of instances that belong to the group defined by  X  by the total number instances. Finally  X  (  X  X  X  X  ) is the proportion of dissatisfaction instan ces in the dataset. The number of occurrences of  X  and  X   X   X  X  X  X  are computed using the FP-Growth algorithm as explained earlier. If the resulting value of the DSAT correlation is less than 1, then the occurrence of  X  is negatively correlated with dissatisfaction. If the resulting value is greater than 1, then  X  is positively correlated with are independent. This value is sometimes referred to as the lift in the data mining literature [18]. In our case, it implies that the occur-faction. Now let us revisit the group that was defined by the attribute set {  X  X  X  X  X  X   X  X , X  X  X  X  X   X  X  X  X  X  X  X  } . Even though this set frequently co-occurs with dissatisfaction, its DSAT correlation is 0.98 showing that it is nearly independent of SAT or DSAT. This allows us to identify more interesting groups a nd gives us a metric to evaluate how every group correlates with dissatisfaction. Other measures of correlation could be used as well for the same purpose (e.g.,  X   X  ). Correlation measures are better than confidence (i.e., the number of occurrences of  X   X   X  X  X  X  divided by the num-ber of occurrences of  X  ). Confidence could be misleading if the dataset is unbalanced with respect to the proportion of satisfaction and dissatisfaction instances. This imbalance is typical in any ran-dom sample of query impressions because most users achieve their search goals and dissatisfaction is usually the exception. For exam-ple, Ageev et al. [1] performed a study where users were asked to find answers to specific questions using Web search. In 87% of the tasks, users reported success, and in 75% of these cases, the answer was correct. Hassan et al. [21] collected firsthand success labels from users using several search engines and reported a 80% satis-faction rate. Notice that the DSAT correlation is also insensitive to the distribution of SAT vs. DSAT cases in the dataset. Hence, it will find attribute sets statistically co rrelated with dissatisfaction regard-less of whether the sample follows the original underlying distribu-tion of satisfaction or not. A specialized ranker can be trained for each dissatisfaction group. DSAT groups are easier to learn than individual DSAT instances because they provide more context for the learning algorithm than single instances. Additionally, and importantly, the coherence of the DSAT groups we identify makes them more susceptible to ranking improvements than a potentially broad and heterogeneous set of underperforming queries. We show later in this section that the coherence is important for training specialized rankers. Attributes of the group can be used to determine for each query whether to apply the method. In pr actice, the specialized ranker is used for all queries where the group X  X  membership criteria applies and the general ranker is used for all other traffic. If we observe gains through applying the specialized ranker and do not change the general ranker used for all other queries, we will see the overall search effectiveness of the search engine improve. In the remainder of this section we describe how we train a specialized ranker for one such DSAT group identified through the analysis in the previous section and show that we can obtain significant relevance gains over strong baselines by targeting that group. In a learning-to-rank framework, each query document pair is repre-sented by a feature vector. The performance of any ranking function depends on feature selection and tr aining data. We fix the set of features used for both the genera l and the specialized rankers. Our feature set contains several hundred features including query, doc-ument, and query-document features. It includes many content-based features, click-based features, static rank-based features, and many others. Note that the features that were used to characterize the DSAT groups (i.e., the attributes used by the DSAT group iden-tification step) were also available to the general ranker, giving it an opportunity to learn to use DSAT information on its own and poten-tially removing the need to iden tify DSAT groups. The general ranker therefore represents a strong baseline against which to com-pare our specialized ranking approach. We employ a re-ranking framework to create a specialized ranker optimized to perform well on the DSAT group. We use a cascade approach where the output of the G eneral Ranker is used as a fea-ture for training a Specialized Ranker. The specialized ranker is trained on the DSAT group data only using the same features as the general ranker. The output of the general ranker is added to the training and testing data of the specialized ranker as an additional feature. This allows the specialized ranker to benefit from the sig-nals in the general data, and at the mean time optimize the perfor-mance over the specialized data. Notice that we cannot simply combine the general dataset with the specialized dataset and train a single ranker. By doing this, we are altering the true underlying distribu tion of queries and the general traffic will be under-represented compared to the natural query distribution. Even though, this ranker may have a better perfor-mance on the DSAT group, it will alter the performance of the rest to improve the performance on the DSAT group without affecting the general traffic. By changing the training data for the specialized ranker to only use in-group queries, we show that identifying an underperforming group is a fundamental step toward improving the performance of search engines on queries belonging to this group. We will also show that we can achieve relevance gains (perhaps attributable to group coherence) by simply targeting that group even when the feature set is held constant. Note that the training and testing data does not overlap with the data used to learn the underperforming query groups. In this subsection, we describe the data we used to identify individ-ual DSAT instances and DSAT gr oups. We obtained millions of records of interaction logs (from July to September 2011) for hun-dreds of thousands of consenting users through a widely-distributed Web browser toolbar. Log entries include a unique identifier for the user, a timestamp for each page view, and the URL of the Web page visited. Intranet and secure (http s) URL and any personally identifi-able information were removed fro m the logs prior to analysis. From these logs, we identified sw itching instances where the same query has been issued on different search engines. This set of logs is referred to as  X  . We applied the DSAT classifier de scribed earlier to a random set of switches observed in  X  , setting recall low to boost precision. The classifier predicted that approximately 100,000 of the instances were caused by dissatisfaction with the pre-switch engine results. We randomly sampled a set of another 100,000 satisfaction instanc-es as described in Section 5 to co nstruct the satisf action dataset and applied the method in Section 5 to generate DSAT groups. We gen-erated around 200 groups; 50% of them were either negatively cor-related or uncorrelated with DSAT and were discarded. and DSAT correlation for every DSAT group. (b) Relation between average NDCG@1 and DSAT correlation for eve-and DSAT correlation for every DSAT group. Note that in The average number of attributes describing every group was 3.4. The average fraction of dissatisfaction instances in a group was 5.5% of the total number of DSAT instances. We divided the groups into three bins according to DSAT correlation: less than 0.8, 0.8 to 1.2, and greater than 1.2. These correspond to groups with negative correlation, no correlation, and positive correlation to dis-satisfaction. We noticed that groups with no correlation to dissatis-faction have lower average number of attributes (3.2), and higher average number of instances (7.6 %) compared to the other two groups that had a number of attributes of 3.4 and 3.6 and a percent-age of instances of 3.8% and 3.7% for the negative correlation and the positive correlation groups respectively. Once we had the groups generated, we wanted to establish whether they were in fact related to searcher dissatisfaction. To validate that the groups that we identify do act ually represent likely searcher dissatisfaction, we used establishe d measures of search engine per-formance, namely result clickthrough rate (CTR) and NDCG. For every identified group, we co mpute the average clickthrough rate for queries belonging to this group. Average clickthrough rate is computed using a separate set of queries that does not overlap with the datasets described in the previous section. Every group is defined using a set of attributes. To compute the average click-through rate, we identify all queries that have all those attributes, compute the clickthrough rate for every query and then compute the average over all such queries. We abstained from using any pre-switch features (i.e., only used qu ery and post-switch features; see Section 4) while generating dissa tisfaction switches to avoid any bias in the clickthrough rate experiment. Figure 3a shows the relation between average clickthrough rate and DSAT correlation for every group. We included all frequent groups even those who were found to be uncorrelated or negatively corre-lated with dissatisfaction. Every group is represented by a bubble. The size of the bubble represents the number of dissatisfaction in-stances that belong to this group. Groups are not orthogonal and hence the same dissatisfaction instance may belong to more than one group. The size of the attribute set defining every group can range from 1 to arbitrary large nu mbers. In practice, the largest attribute set had no more than six attributes. Groups with size less than 0.5% of the size of the entire dissatisfaction set are ignored. We have to set a lower limit on the size of a group; otherwise every dissatisfaction instance will result in a group. We can observe from Figure 3a that groups with high DSAT corre-lation have low clickthrough rate, while groups with low DSAT correlation have higher clickthrough rate resulting in a high correla-tion between the two measures. We also notice that most of the large groups are either negatively correlated or uncorrelated with dissatisfaction. Many of those groups are defined by a single attrib-ute such as long queries or informational queries. Similarly, we compared DSAT correlation to NDCG@1 and NDCG@3 in Figures 3b and 3c, respectively. NDCG at a particular rank position  X  is defined as: where ) X ( X  X  X  is the relevance of the  X   X  X  X  document in the ranked list. NDCG is computed by normalized DCG using the DCG of the ideal ranking (IDCG). We used a large query set with editorial relevance judgments labeled as part of a se parate search engine assessment effort to identify a set of queries that belong to each group. Every query-document pair was judged on a five-point grade scale: Bad , each result. The dataset was constructed using pooled relevance judgments and query-document pairs were judged by trained human assessors. Comparison against NDCG yields very similar results to comparison against CTR. To quantify the correlation with established query level metrics, we computed the Pearson correlation coefficient between DSAT corre-lation and: (i) average clickthrough rate, (ii) average NDCG@1, and (iii) average NDCG@3. Correla tions were measured using the Pearson X  X  correlation coefficient. Th e results are shown in Table 4. The table shows a strong negative correlation between DSAT Corre-lation and the three other metrics validating that the groups do seem to represent likely searcher dissatisfaction. In summary, we defined a process and a metric for identifying que-ry groups associated with dissatisfaction. The process uses search interaction logs and does not involve any human intervention. The identified groups show high correlation with established query-level performance metrics such as CTR or NDCG. One important ad-vantage of these groups is that they are coherent in a way that pro-vides a rich context for isolated queries. This coherence allows us to use these groups to improve Web search performance, as we will demonstrate in the next section. We used the learning-to-rank algo rithm presented in [35] and com-pared different ranking functions using NDCG (defined earlier). In this subsection we present the results of two comparisons: (i) a spe-cialized ranker trained on one of our groups versus general ranker trained over all queries, and (ii) specialized ranker trained on one of our groups versus a ranker trained on a potentially-diverse set of underperforming queries. The group that we selected repr esented approximately 2% of the dissatisfaction instances, and had a DSAT correlation of 1.3. We selected this group because we were able to use existing query-document relevance judgments to quickly build a dataset with que-ries with document relevance judgments that belong to this group. Collecting query-document relevance judgments for thousands of queries and hundreds of documents per query is a costly and time 
NDCG@
Table 6. NDCG improvements for low CTR group using a 
NDCG@ consuming process. Crowdsourcing could be used to create more labeled datasets to target other DSAT groups. We compare the results of three experiments. First, we use a general queryset to train a learning-to-rank algorithm. The general queryset contains queries that belong to the specific group among many other queries. Second, we train a specialized ranker using a queryset with queries that only belong to the selected group. If targeting specific underperforming groups is a good strategy, then the ranking func-tion trained using a specialized quer y set will be better than the one using a general query set. Finally, to measure the value of coherence in our groups, we compare the pe rformance of our specialized rank-er with a ranker trained specifically for a set of poorly-performing queries, with low CTR. The general ranker baseline is trained using a general queryset with 10,000 queries; some of them belong to the selected group. As not-ed above, the general ranker had access to attributes used in identi-fying DSAT groups, allowing it to learn to use DSAT information. Note that the general ranker is a state-of-the-art heavily optimized ranker using hundreds of features including clickthrough rate, link based features and content based features. The specialized ranker is trained using a specialized dataset. The dataset contains queries that belong to the selected group only. The size of the specialized dataset is 2,000 queries. The size of the spe-cialized dataset is much less than the size of the general dataset. This is not intentional and we believe that increasing the size of the specialized dataset could even lead to larger gains. The data used to train both rankers does not overlap w ith the data used to identify the underperforming query groups. We evaluated both the general and the specialized rankers using the specialized data using 10-fold cross validation. We evaluate the performance of both rankers using specialized data only because we are only altering the ranking on queries belonging to the selected group. The performance on all other queries will remain unchanged. The rationale behind this is that the all queries will go to the general ranker except for queries belonging to the selected group which will go to the specialized ranker. Note that these are not the same under-performing queries we identified earlier. Rather, they are new que-ries that belong to the underperforming group (i.e. all the attributes defining the group fire for them). In Table 5, we report NDCG@1 through NDCG@5 improvements over the general ranker. Improvement s are statistically significant at the 0.05 level according to the Wilcoxon p-value . The table shows that we observe a 1.52% NDCG@1 improvement and NDCG@5 improvement of 0.77%. We performed another experiment to measure the effect that the coherence of the identified dissatisfac tion groups has in improving relevance when a specialized ranker is trained. We wanted to under-stand whether the relevance gains we achieved were specific to dissatisfaction groups, or if they can be obtained if we train a ranker on any set of poorly-performing queries. We construct a new data set with poorly performing queries (clickthrough rate less than 0.2), and with the same number of quer ies as the specialized dataset de-scribed earlier. We train a ranker using the same set of features on the low CTR dataset. We measure the performance using NDCG gain compared to the general ranker. The results are shown in Table 6. The results show that the gain is much smaller than the special-ized ranker case and often not significant. The NDCG@1 gain is the only gain that is significant at the 0.05 level. This shows the value of the dissatisfaction groups and supports our hypothesis that group coherence is important for ranking. Identifying groups of queries where search engines underperform is an important research challenge. We have described a novel method to automatically identify dissatisf action groups comprising queries where a search engine appears to be failing. For one such group identified by our method, we trained a specialized ranking algo-rithm and showed significant relevan ce gains over a state-of-the-art general ranker. We validated our DSAT group identification approach by showing that groups identified were correlated with established measures of search engine performance, name ly clickthrough rate and NDCG. However, unlike subsets of the queries identified by those metrics, queries were grouped so that they shared common attributes. As we showed through an experiment, such coherence is important for ranking. In particular, we showed that the specialized ranker per-formed better for our coherent groups than groups containing a broad range of queries that were just filtered based on a measure of retrieval effectiveness (low CTR in our case). The methods presented combine to form an end-to-end framework capable of automatically identifying dissatisfied query groups and the adapting the ranking algorithm to them. However, the frame-work does not have to be used in its entirety. It may be desirable to first identify the groups of queries on which the search engine is underperforming and then triage those groups to make a determina-tion regarding how to proceed. Deploying a specialized ranker may not always be appropriate depending on the nature of the group identified. For example, in DSAT groups where the searcher may benefit from interactive support rather than ranking improvements (e.g., a specialized direct answer on the SERP), interface modifica-tions may be more appropriate. We should also acknowledge some limitations of our study. The framework is limited by the need to gather dissatisfaction data be-fore specialized rankers can be trained. New rankers that are re-leased based on this data then need to wait some time (days or weeks) for new data to arrive. The method can only work on groups of sufficient size to provide a sufficient number of queries to train the ranker. This approach does not work for the smaller groups. One solution is to create group hierarchies to combine multiple smaller groups into a single large group based on shared attributes. There are a number of important areas of future work. For the train-ing and evaluation of our specialized ranking algorithms we used human relevance judgments. Howe ver, obtaining these judgments may be costly and we may not have such judgments readily availa-ble for the query groups identified by our algorithm. Prior work has shown that judgments may be obtai ned implicitly from search inter-action [3][7] and we will explore the use of such judgments to fur-ther lessen the any remaining dependencies on human involvement in our framework (reducing additional costs where possible). We will also investigate the cost-benefit tradeoffs of adding more fea-tures tailored to the group of interest, rather than simply re-training the ranker for the current set of features. To handle smaller query groups for which we may lack sufficient training data to train a dedicated ranker, we will investigate how to combine multiple groups in a way that still maintains the group coherence that con-tributes to the success of our approach. Search engines try to satisfy all users X  needs and inevitably do not perform well for all queries. In this paper, we described and validat-ed a method for automatic identi fication of dissatisfaction groups comprising queries where an engine is underperforming. Knowledge of these DSAT groups can help search engines since it can offer insights that inform engine design decisions and resource allocation. Importantly, rather than only identifying underperform-ing groups, we closed the loop and investigated automatically learn-ing a specialized ranking algorithm that outperforms a general rank-er with access to the same features . Our findings also show the ben-efit of group coherence in training specialized rankers. The method we describe in this paper can help search engines learn from evi-dence of searcher dissatisfaction and directly improve their search performance for troublesome queries. Future work will expand this research to target learning new ranking algorithms for multiple DSAT groups simultaneously and develop new methods to combine many smaller query groups to improve coverage. [1] Ageev, M., Guo, Q., Lagun, D., and Agichtein, E. (2011). Find [2] Agichtein, E., Brill, E., and Dumais, S.T. (2006). Improving [3] Agichtein, E., Brill, E., Dumais, S.T., and Ragno, R. (2006). [4] Agrawal, R., Imielinski, T., and Swami, A. (1993). Mining [5] Agrawal, R., and Srikant, R. (1994). Fast algorithms for min-[6] Al-Maskari, A., Sanderson, M., and Clough, P. (2007). The [7] Bennett, P.N., Radlinski, F., Yilmaz, E. and White, R.W. [8] Carmel, D., Yom-Tov, E., Darlow, A., and Pelleg, D. (2006). [9] Carterette, B. and Jones, R. (2 007). Evaluating search engines [10] Cronen-Townsend, S., Zhou, Y., and Croft, W. B. (2002). [11] Dan, O., Dmitriev, P., and White, R.W. (2012). Mining for [12] Downey, D., Dumais, S., and Horvitz, E. (2007). Model of [13] Feild, H., Allan, J., and Jones, R. (2010). Predicting searcher [14] Fox, S., Karnawat, K., Mydland, M., Dumais, S.T., and White, [15] Guo, Q., White, R.W., Dumais, S.T., Wang, J., and Anderson, [16] Guo, Q., White, R.W., Zhang, Y., Anderson, B., and Dumais, [17] Hall, M., Frank, E., Holmes, G., Pfahringer, B., Reutemann, [18] Han, J. and Kamber, M. (2006). Data Mining: Concepts and [19] Han, J., Pei, J., Yin, Y., and Mao, R. (2004). Mining Frequent [20] Hassan, A., Jones, R., and Klinkner, K.L. (2010). Beyond [21] Hassan, A., Song, Y., and He, L. (2011). A task level user [22] Hauff, C., Murdock, V., and Baeza-Yates, R. (2008). Im-[23] He, B. and Ounis, I. (2004). Inferring query performance using [24] Hu, V., Stone, M., Pedersen, J., and White, R.W. (2011). Ef-[25] Huffman, S. and Hochster, M. (2007). How well does result [26] K. J X rvelin and Kekalainen. J. (2002). Cumulated gain-based [27] K. J X rvelin, S.L. Price, L.M.L. Delcambre, and M.L. Nielsen. [28] Dan Klein and Christopher D. Ma nning. (2003). Accurate [29] Leskovec, J., Dumais, S., and Horvitz, E. (2007). Web projec-[30] Li, H., Wang, Y., Zhang, D., Zh ang, M., and Chang, E. (2008). [31] Teevan, J., Dumais, S., and Liebling, D. (2008). To personal-[32] White, R.W. and Drucker, S.M. (2007). Investigating behav-[33] White, R.W. and Dumais, S. (2009). Characterizing and pre-[34] White, R.W. and Huang, J. (2010) . Assessing the scenic route: [35] Xu, J., and Li, H. (2007). AdaRank: A boosting algorithm for [36] Zhou, Y. and Croft, W.B. (2007). Query performance predic-
