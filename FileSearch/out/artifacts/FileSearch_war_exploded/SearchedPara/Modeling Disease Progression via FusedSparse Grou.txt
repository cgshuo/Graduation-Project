 Alzheimer X  X  Disease (AD) is the most common neurodegen-erative disorder associated with aging. Understanding how the disease progresses and identifying related pathological biomarkers for the progression is of primary importance in the clinical diagnosis and prognosis of Alzheimer X  X  disease. In this paper, we develop novel multi-task learning tech-niques to predict the disease progression measured by cog-nitive scores and select biomarkers predictive of the pro-gression. In multi-task learning, the prediction of cognitive scores at each time point is considered as a task, and mul-tiple prediction tasks at different time points are performed simultaneously to capture the temporal smoothness of the prediction models across different time points. Specifically, we propose a novel convex fused sparse group Lasso (cFSGL) formulation that allows the simultaneous selection of a com-mon set of biomarkers for multiple time points and specific sets of biomarkers for different time points using the sparse group Lasso penalty and in the meantime incorporates the temporal smoothness using the fused Lasso penalty. The proposed formulation is challenging to solve due to the use of several non-smooth penalties. One of the main techni-cal contributions of this paper is to show that the proximal operator associated with the proposed formulation exhibits a certain decomposition property and can be computed effi-ciently; thus cFSGL can be solved efficiently using the accel-erated gradient method. To further improve the model, we propose two non-convex formulations to reduce the shrink-age bias inherent in the convex formulation. We employ the difference of convex (DC) programming technique to solve the non-convex formulations. We have performed extensive experiments using data from the Alzheimer X  X  Disease Neu-roimaging Initiative (ADNI). Results demonstrate the effec-tiveness of the proposed progression models in comparison with existing methods for disease progression. We also per-form longitudinal stability selection to identify and analyze the temporal patterns of biomarkers in disease progression. H.2.8 [ Database Management ]: Database Applications X  Data Mining ;J.3[ Life and Medical Sciences ]: Health, Medical information systems Algorithms Alzheimer X  X  Disease, regression, multi-task learning, fused Lasso, sparse group Lasso, cognitive score
Alzheimer X  X  disease (AD), accounting for 60-70% of age-related dementia, is a severe neurodegenerative disorder. AD is characterized by loss of memory and declination of cognitive function due to progressive impairment of neurons and their connections, leading directly to death [21]. In 2011 there are approximately 30 million individuals afflicted with dementia and the number is projected to be over 114 mil-lion by 2050 [36]. Currently there is no cure for Alzheimer X  X  and efforts are underway to develop sensitive and consistent biomarkers for AD.

In order to better understand the disease, an important area that has recently received increasing attention is to understand how the disease progresses and identify related pathological biomarkers for the progression. Realizing its importance, NIH in 2003 funded the Alzheimer X  X  Disease Neuroimaging Initiative (ADNI). The initiative is facilitat-ing the scientific evaluation of neuroimaging data includ-ing magnetic resonance imaging (MRI), positron emission tomography (PET), other biomarkers, and clinical and neu-ropsychological assessments for predicting the onset and pro-gression of MCI (Mild Cognitive Impairment) and AD. The identification of sensitive and specific markers of very early AD progression will facilitate the diagnosis of early AD and the development, assessment, and monitoring of new treat-ments. There are two types of progression models that have been commonly used in the literature: the regression model [10, 31] and the survival model [30, 34]. Many exist-ing work consider a small number of input features, and the model building involves an iterative process in which each feature is evaluated individually by adding to the model and testing the performance of predicting the target represent-ing the disease status [18, 35]. The disease status can be measured by a clinical score such as Mini Mental State Ex-amination (MMSE) or Alzheimer X  X  Disease Assessment Scale cognitive subscale (ADAS-Cog) [10, 31], or the volume of a certain brain region [16], or clinically defined categories [9, 26]. When high-dimensional data, such as neuroimages (i.e., MRI and/or PET) are used as input features, the methods of sequentially evaluating individual features are suboptimal. In such cases, dimension reduction techniques such as prin-ciple component analysis are commonly applied to project the data into a lower-dimensional space [10]. One disadvan-tage of using dimension reduction is that the models are no longer interpretable. A better alternative is to use feature selection in modeling the disease progression [31]. Most ex-isting work focus on the prediction of target at a single time point (baseline [31], or one year [10]); however, a joint analy-sis of data from multiple time points is expected to improve the performance especially when the number of subjects is small and the number of input features is large.

To address the aforementioned challenges, multi-task learn-ing techniques have recently been proposed to model the dis-ease progression [39, 43]. The idea of multi-task learning is to utilize the intrinsic relationships among multiple related tasks in order to improve the generalization performance; it is most effective when the number of samples for each task is small. One of the key issues in multi-task learning is to identify how the tasks are related and build learning models to capture such task relatedness. One way of model-ing multi-task relationship is to assume all tasks are related and task models are closed to each other [11], or tasks are clustered into groups [4, 20, 32, 41]. Alternatively, one can assume that the tasks share a common subspace [2, 7], or a common set of features [3, 29]. In [39], the prediction of different types of targets such as MMSE and ADAS-Cog is modeled as a multi-task learning problem and all models are constrained to share a common set of features. In [43], multi-task learning is used to model the longitudinal disease pro-gression. Given the set of baseline features of a patient, the prediction of the patient X  X  disease status at each time point can be considered as a regression task. Multiple prediction tasks at different time points are performed simultaneously to capture the temporal smoothness of the prediction mod-els across different time points. However, similar to [39], the formulation in [43] constrains the models at all time points to select a common set of features, thus failing to capture the temporal patterns of the biomarkers in disease progression [6, 19]. It is thus desirable to develop formulations that allow the simultaneous selection of a common set of biomarkers for multiple time points and specific sets of biomarkers for different time points.

In this paper, we propose novel multi-task learning formu-lations for predicting the disease progression measured by the clinical scores (ADAS-Cog and MMSE). Specifically, we propose a convex fused sparse group Lasso (cFSGL) formu-lation that simultaneously selects a common set of biomark-ers for all time points and selects a specific set of biomark-ers at different time points using the sparse group Lasso penalty [14], and in the meantime incorporates the tempo-ral smoothness using the fused Lasso penalty [33]. The pro-posed formulation is, however, challenging to solve due to the use of several non-smooth penalties including the sparse group Lasso and fused Lasso penalties. We show that the proximal operator associated with the optimization prob-lem in cFSGL exhibits a certain decomposition property and can be solved efficiently. Therefore cFSGL can be effi-ciently solved using the accelerated gradient method [27, 28]. The convex sparsity-inducing penalties are known to intro-duce shrinkage bias [12]. To further improve the progression model and reduce the shrinkage bias in cFSGL, we propose two non-convex progression formulations. We employ the difference of convex (DC) programming technique to solve the non-convex formulations, which iteratively solves a se-quence of convex relaxed optimization problems. We show that at each step the convex relaxed problems are equivalent to reweighted sparse learning problems [5].

We have performed extensive experiments to demonstrate the effectiveness of the proposed models using data from the Alzheimer X  X  Disease Neuroimaging Initiative (ADNI). We have also performed longitudinal stability selection [43] us-ing our proposed formulations to identify and analyze the temporal patterns of biomarkers in disease progression.
In the longitudinal AD study, cognitive scores of selected patients are repeatedly measured at multiple time points. The prediction of cognitive scores at each time point can be considered as a regression problem, and the prediction of cognitive scores at multiple time points can be treated as a multi-task regression problem. By employing multi-task regression, the temporal information among different tasks can be incorporated into the model to improve the prediction performance.

Consider a multi-task regression problem of t tasks with n samples of d features. Let { x 1 ,  X  X  X  , x n } be the input data at the baseline, and let { y 1 ,  X  X  X  , y n } be the targets, where each x i  X  R d represents a sample (patient), and y i  X  R t the corresponding targets (clinical scores) at different time points. We collectively denote X =[ x 1 ,  X  X  X  , x n ] T  X  R n as the data matrix, Y =[ y 1 ,  X  X  X  , y n ] T  X  R n  X  t as the target matrix, and W = w 1 ,  X  X  X  , w t  X  R d  X  t as the weight matrix. To consider the missing values from the target, we denote the loss function as: where matrix S  X  R n  X  t indicates missing target values: S i,j = 0 if the target value of sample i is missing at the j th time point, and S i,j = 1 otherwise. The component-wise operator is defined as follows: Z = A B denotes Z i,j = A i,j B i,j ,forall i , j . The multi-task regression solves the following optimization problem: min W L ( W )+ X ( W ), where  X ( W ) is a regularization term that captures the task relatedness.

In the multi-task setting for modeling disease progression, each task is to predict a specific target (e.g., MMSE) for a set of subjects at different time points. It is thus reason-able to assume that the difference of the predictions between immediate time points is small, i.e., the temporal smooth-ness [43]. It is also well believed in the literature that a small subset of biomarkers are related to the disease pro-gression, and biomarkers involved at different stages may be different [19]. To this end, we propose a novel multi-task learning formulation for modeling disease progression which allows simultaneous joint feature selection for multiple tasks and task-specific feature selection, and in the meantime in-corporates the temporal smoothness. Mathematically, the proposed formulation solves the following convex optimiza-tion problem: where W 1 is the Lasso penalty, the group Lasso penalty Lasso penalty, R is an ( t  X  1)  X  t sparse matrix in which R i,i =1and R i,i +1 =  X  1, and  X  1 ,  X  2 and  X  3 are regulariza-tion parameters. The combination of Lasso and group Lasso penalties is also known as the sparse group Lasso penalty, which allows simultaneous joint feature selection for all tasks and selection of a specific set of features for each task. The fused Lasso penalty is employed to incorporate the tempo-ral smoothness. We call the formulation in Eq. (2)  X  X onvex fused sparse group Lasso X (cFSGL). The cFSGL formulation involves three non-smooth terms, and is thus challenging to solve. We propose to solve the optimization problem by the accelerated gradient method (AGM) [27, 28]. One of the key steps in using AGM is the computation of the proxi-mal operator associated with the composite of non-smooth penalties defined as follows: It is clear that each row of W is decoupled in Eq. (3). Thus for obtaining the i th row w i , we only need to solve the fol-lowing optimization problem: where v i is the i th row of V . The proximal operator in Eq. (4) is challenging to compute due to the presence of three non-smooth terms. One of the key technical contributions of this paper is to show that the proximal operator exhibits a certain decomposition property, based on which we can efficiently compute the proximal operator in two stages, as summarized in the following theorem:
Theorem 1. Define  X  FL ( v )=argmin w 1  X  GL ( v )=argmin w 1 Then the following holds: Proof: The necessary and sufficient optimality conditions for (4), (5), and (6) can be written as: 0  X   X  GL (  X  FL ( v ))  X   X  FL ( v )+  X  3  X  X  (  X  GL (  X  FL where SGN( x ) is a set defined in a componentwise manner as: and It follows from (10) and (12) that: 1) if  X  FL ( v ) 2  X   X  then  X  GL (  X  FL ( v )) = 0 ;and2)if  X  FL ( v ) 2 &gt; X  3  X 
It is easy to observe that, 1) if the i -th entry of  X  FL is zero, so is the i -th entry of  X  GL (  X  FL ( v )); 2) if the i -th entry of  X  FL ( v ) is positive (or negative), so is the i -th entry of  X  GL (  X  FL ( v )). Therefore, we have:
Meanwhile, 1) if the i -th and the ( i + 1)-th entries of  X 
FL ( v ) are identical, so are those of  X  GL (  X  FL ( v )); 2) if the i -th entry is larger (or smaller) than the ( i + 1)-th entry in  X  FL ( v ), so is in  X  GL (  X  FL ( v )). Therefore, we have:
It follows from (9), (10), (13), and (14) that: 0  X   X  GL (  X  FL ( v ))  X  v +  X  1 SGN(  X  GL (  X  FL ( v ))) +  X  2 R T SGN( R X  GL (  X  FL ( v ))) +  X  3  X  X  (  X  GL (  X  FL Since (4) has a unique solution, we can get (7) from (8) and (15).
 Note that the fused Lasso signal approximator [13] in Eq.(5) can be effectively solved using [24]. The complete algorithm for computing the proximal operator associated with cFSGL is given in Algorithm 1.
 Algorithm 1 Proximal operator associated with the Con-vex Fused Sparse Group Lasso (cFSGL) Output: W  X  R d  X  t 1: for i=1:d do 2: u i = arg min w 1 2 w  X  v i 2 2 +  X  1 w 1 +  X  2 R w 1 3: w i = arg min w 1 2 w  X  u i 2 2 +  X  3 w 2 4: end for
In cFSGL, we aim to select task-shared and task-specific features using the sparse group Lasso penalty. However, the decomposition property shown in Theorem 1 implies that a simple composition of the 1 -norm penalty and 2 , 1 -norm penalty may be sub-optimal. Besides, the sparsity-inducing penalties are known to lead to biased estimates [12]. To this end, we propose the following non-convex multi-task regression formulation for modeling disease progression: where the second term is the summation of the squared root of 1 -norm of w i ( w i is the i th row of W ), and is called the composite (0 . 5 , 1) -norm regularization. Note that it is in fact not a valid norm due to its non-convexity. It is known that the 0 . 5 penalty leads to a sparse solution, thus many of the rows of W will be zero, i.e., the features corresponding to the zero rows will be removed from all tasks. In addition, for the nonzero rows, due to the use of the 1 penalty for the rows, many features within these nonzero rows will be zero, resulting in task-specific features. Thus, the use of . 5 , 1) penalty leads to a tight coupling of between-task and within-task feature selection. In addition, the 0 . 5 penalty is expected to reduce the estimation bias associated with the convex sparsity-inducing penalties.

We also consider an alternative non-convex formulation which includes the fused Lasso term of each row within the square root, resulting in a composite (0 . 5 , 1) -like penalty: A good merit of using non-convex penalties is that they are closer to the optimal 0 - X  X orm X  (minimizing which is NP-hard) and give better sparsity [15]. In addition, a practical advantage of the non-convex progression models presented in Eqs. (16) and (17) is that there are only 2 regulariza-tion parameters to be estimated, compared to 3 parameters in the convex formulation in Eq. (2). However, one disad-vantage of the non-convex penalties is that the associated optimization problems are non-convex and global solutions are not guaranteed. A well-known method for solving non-convex problems is to approximate the non-convex formula-tion by a convex relaxation via the difference of convex (DC) programming techniques [17]. Next, we show how the non-convex problems can be solved using DC programming and then relate the relaxed formulations to reweighted convex formulations.
The formulations in Eq. (16) and Eq. (17) can be ex-pressed in the following general form: where ( W )and h ( W ) are convex. Since x  X  we decompose the objective function in Eq. (18) into the following form: Denote the two functions as f ( W )= ( W )+ h ( W )and g ( h ( W )) = h ( W )  X  h ( W ). We can express the formulation in Eq. (18) in the form of difference of two functions: Using the convex-concave procedure (CCCP) algorithm [38], we can linearize g ( h ( W )) using the 1st-order Taylor expan-sion at the current point W as: f ( W )  X  g ( h ( W )) = f ( W )  X  g ( h ( W )) which is the convex upper bound of the non-convex problem. In every iteration of the CCCP algorithm, we minimize the upper bound: and the objective function is guaranteed to decrease. We obtain a local optimal W  X  of Eq. (18) by iteratively solving Eq. (20). The CCCP algorithm has been applied successfully to solve many non-convex problems [8, 22, 37].
We first consider the non-convex optimization problem in Eq. (16), whose convex relaxed form corresponding to Eq. (20) is given by: where  X  i =1 / w i ( k ) 1 + and is a small number in-cluded to avoid singularity. It is clear that the convex re-laxed problem in each iteration is a fused Lasso problem with a reweighted 1 -norm term. If we omit the fused term, the general (1 , 0 . 5) -regularized optimization problem is of the following form: which, under DC programming, involves solving a series of reweighted Lasso [5, 23] problems: It is known that the reweighted Lasso reduces the estimation bias of Lasso, thus leading to a better solution. Similarly, for the non-convex optimization problem in Eq. (17), we iteratively solve the following convex problem: where  X  i =1 / R w i T ( k ) 1 +  X  w i ( k ) 1 + .Inthiscase, in each iteration, we solve a fused Lasso problem with a reweighted 1 -term and a reweighted fused term.

The non-convex optimization problems may be sensitive to the starting point. In our algorithm in Eq. (21), for ex-ample, if all elements in row i of the model w i are initialized to be close to 0, then in the next iteration  X  i will be set to a very large number. The large penalty forces the row to stay at 0 in later iterations. Therefore, in our convex relaxed algorithms in Eq. (21) and Eq. (22), we propose to use the solution of a problem similar to fused Lasso as the starting point. For example, the starting point we use in Eq. (21) is: This is equivalent to setting  X  i / 2 = 1. Similarly, in Eq. (22) we set  X  i / 2=1.
We propose to employ longitudinal stability selection to quantify the importance of the features selected by the pro-posed formulations for disease progression. The idea of lon-gitudinal stability selection is to apply stability selection [25] to multi-task learning models for longitudinal study. The stability score (between 0 and 1) of each feature is indicative of the importance of the specific feature for disease progres-sion. In this paper, we propose to use longitudinal stability selection with cFSGL and nFSGL to analyze the tempo-ral patterns of biomarkers. The temporal pattern of sta-bility scores of the features selected at different time points can potentially reveal how disease progresses temporally and spatially.

The longitudinal stability selection algorithm with cFSGL and nFSGL is given as follows. Let F be the index set of features, and let f  X  F denote the index of a particular feature. Let  X  be the regularization parameter space and let the stability iteration number be denoted as  X  . For cFSGL an element  X   X   X isatriple  X  1 , X  2 , X  3 , and for nFSGL is a tuple of the corresponding parameter pairs. Let B ( i ) { X ( i ) ,Y ( i ) } be a random subsample from input data { of size n/ 2 without replacement. For a given  X   X   X , let  X  W ( i ) be the optimal solution of cFSGL or nFSGL on B ( The set of features selected by the model  X  W ( i ) of the task at time point p is denoted by: We repeat this process for  X  times and obtain the selection probability  X   X   X  f,p of each feature f at time point p : where I ( . ) is the indicator function defined as: I ( c )=1if c is true and I ( c ) = 0 otherwise. Repeat the above procedure for all  X   X   X , we obtain the stability score for each feature f at time point p : The stability vector of a feature f at all t time points is given by: S ( f )=[ S 1 ( f ) ... S t ( f )], which reveals the change of the importance of feature f at different time points. We define the stable features at time point p as: and choose  X  = 20 in our experiments. We are interested in the stable features at all time points, i.e., f  X   X  U =  X  t Note that S ( f ) is dependent on the progression model used.
We emphasize here that unlike the previous work which gives a list of features common for all time points [43], our proposed approaches yield a different list of features at dif-ferent time points. Note that in the above stability selec-tion we use temporal information via fused Lasso. Conse-quently the distribution of stability scores also has temporal smoothness property: for each feature the stability scores are smooth across different time points (as shown in exper-imental results in Section 6.3). If simply using Lasso in stability selection, then we obtain independent probability lists at each time point, and therefore such temporal smooth pattern cannot be captured.
In our previous work [43], we proposed to use the temporal group Lasso (TGL) regularization to capture task related-ness, which involves the following optimization problem: min where  X  1 ,  X  2 and  X  3 are regularization parameters. The TGL formulation in Eq. (24) contains three penalty terms. The first term penalize the 2 -norm of the model to prevent over-fitting; the second term enforces temporal smoothness using -norm, which is equivalent to a Laplacian term, and the last 2 , 1 -norm introduces joint feature selection. We argue that it is more natural to incorporate the within-task fea-ture selection and temporal smoothness using a composite penalty as in our proposed cFSGL formulation in Eq. (2).
For example, the only sparsity-inducing term in TGL for-mulation in Eq. (24) is the 2 , 1 -norm regularized joint fea-ture selection. Therefore an obvious disadvantage of this formulation is that it restricts all models from different time points to select a common set of features; however, differ-ent features may be involved at different time points. In addition, one key advantage of fused Lasso compared with the Laplacian-based smoothing used in [43] is that under the fused Lasso penalty the selected features across differ-ent time points are smooth, i.e., nearby time points tend to select similar features, while the Laplacian-based penalty focuses on the smoothing of the prediction models across different time points. Thus, the fused Lasso penalty better captures the temporal smoothness of the selected features, which is closer to the real-world disease progression mecha-nism.

In the TGL formulation, the temporal smoothness is en-forced using a smooth Laplacian term, though fused Lasso in cFSGL indeed has better properties such as sparsity con-tinuity. We have used this restrictive model in TGL, in order to avoid the computational difficulties introduced by the composite of non-smooth terms ( 2 , 1 -norm and fused Lasso). We show in this paper that the proximal operator associated with the optimization problem in cFSGL exhibits a certain decomposition property and can be computed ef-ficiently (Theorem 1); thus cFSGL can be solved efficiently using accelerated gradient method. Another contribution of this paper is that we extend our progression model using a composite of non-convex sparsity-inducing terms, and we further propose to employ the DC programming to solve the non-convex formulations.
In this section we evaluate the proposed progression mod-els on the data sets from the Alzheimer X  X  Disease Neuroimag-ing Initiative (ADNI) 1 . The source codes can be found in the Muli-tAsk Learning via StructurAl Regularization (MAL-SAR) package [42].
The ADNI project is a longitudinal study, where a variety of measurements are collected from selected subjects includ-ing Alzheimer X  X  disease patients (AD), mild cognitive im-pairment patients (MCI) and normal controls (NL), repeat-edly over a 6-month or 1-year interval. The measurements include MRI scans (M), PET scans (P), CSF measurements (C), and cognitive scores such as MMSE and ADAS-Cog. We denote all measurements other than the three types of www.loni.ucla.edu/ADNI Table 1: The sample size and feature dimensional-ity of different data sets used in the experiments. M denotes baseline MMSE features and E denotes baseline META features.
 Table 2: Features included in the META dataset.
 In META, we include baseline cognitive scores as features to predict the future cognitive scores. A detailed explanation of each cognitive score and lab test can be found at [1].
 biomarkers (M, P, C) as META (E). A detailed list of the META data is given in Table 2. The date when the patient performs the screening in the hospital for the first time is called baseline , and the time point for the follow-up visits is denoted by the duration starting from the baseline. For in-stance, we use the notation  X  X 06 X  to denote the time point half year after the first visit. Currently ADNI has up to 48 months X  follow-up data for some patients. However, many patients drop out from the study for many reasons (e.g. de-ceased). In our experiments, we predict future MMSE and ADAS-Cog scores using various measurements at the base-line. For each target we build a prediction model using a data set that only contains baseline MRI features (M), and another data set that contains both MRI and META fea-tures (M+E). In the current study, CSF and PET are not used due to the small sample size. The MRI features are extractedinthesamewayasin[43]. Thereare5types of MRI features used: white matter parcellation volume (Vol.WM.), cortical parcellation volume (Vol.C.), surface area (Surf. Area), cortical thickness average (CTA), cortical thickness standard deviation (CTStd). The sample size and dimensionality for each time point and feature combination is given in Table 1.
In the first experiment, we compare the proposed meth-ods including Convex Fused Sparse Group Lasso (cFSGL) and the two Non-Convex Fused Group Lasso: nFSGL1 in Eq. (16) and nFSGL2 in Eq. (17) with ridge regression (Ridge) and Temporal Group Lasso (TGL) on the prediction of MMSE and ADAS-Cog using selected types of feature combinations, namely M and M+E. Note that Lasso is a special case of cFSGL when both  X  2 and  X  3 are set to 0. For each feature combination, we randomly split the data into training and testing sets using a ratio 9 : 1. The 5-fold cross validation is used to select model parameters. For the regression per-formance measures, we use Normalized Mean Squared Error (nMSE) as used in the multi-task learning literature [40, 3] and weighted correlation coefficient (R-value) as employed in the medical literature addressing AD progression prob-lems [10, 31, 18]. We report the mean and standard devia-tion based on 20 iterations of experiments on different splits of data. To investigate the effects of the fused Lasso term, in cFSGL we fix the value of  X  2 in Eq.(2) to be 20 , 50 , 100, and perform cross validation to select  X  1 and  X  3 . The three configurations are labeled as cFSGL1, cFSGL2 and cFSGL3 respectively.

The experimental results using 90% training data on MRI and MRI+META are presented in Table 3 and Table 4. Overall our proposed approaches outperform Ridge and TGL, in terms of both nMSE and correlation coefficient. We have the following observations: 1) The fused Lasso term is effec-tive. We witness significant improvement in cFSGL when changing the parameter value for the fused Lasso term. 2) The proposed cFSGL and nFSGL formulations witness sig-nificant improvement for later time points. This may be due to the data sparseness at later time points (see Table 1), as the proposed sparsity-inducing models are expected to achieve better generalization performance in this case. 3) The non-convex nFSGL formulations are better than cFSGL in many tasks. One practical strength of the non-convex nFSGL formulations is that they have fewer parameters to be estimated (only 2 parameters).
One of the strengthens of the proposed formulations is that they facilitate the identification of temporal patterns of biomarkers. In this experiment we study the temporal patterns of biomarkers using longitudinal stability selection with cFSGL and nFSGL. Note that because the sample size at the M48 time point is too small, we perform stability selection for M06, M12, M24, and M36 only.

The stability vectors of MRI stable features using cFSGL nFSGL1 and nFSGL2 formulations are given in Figure 1, Figure 2 and Figure 3 respectively. In the figures, we collec-tively list the stable features (  X  = 20) at the 4 time points. The total number of features may be less than 80 because one feature may be identified as a stable feature at multi-ple time points. In Figure 1(a), we observe that cortical thickness average of left middle temporal, cortical thickness average of left and right Entorhinal, and white matter vol-ume of left Hippocampus are important biomarkers for all time points, which agrees with the previous findings [43]. Cortical volume of left Entorhinal provides significant infor-mation in later stages than in the first 6 months. Several biomarkers including white matter volume of left and right Amygdala, and surface area of right Bankssts provide use-ful information only in later time points. On the contrary, some biomarkers have a large stability score during the first 2 years after baseline screening, such as cortical thickness average of left inferior temporal, left inferior parietal, and cortical thickness standard deviation of left isthmus cingu-late, right lingual, left inferior parietal, and cortical volume of right precentral, right isthmus cingulate, and left middle temporal cortex.

The stability vector of stable MRI features for MMSE are given in Figure 1(b). We obtain very different patterns from ADAS-Cog. We find that most biomarkers provide signifi-cant information for the first 2 years and very few of them point. 90 percent of data is used as training data. for each time point. 90 percent of data is used as training data. contain information about the progression in later stages. The lacking of predictable MRI biomarkers in later stages is a potential factor that contributes to the lower predictive performance of MMSE than that of ADAS-Cog in our study and other related studies [39]. These results suggest that ADAS-Cog may be a better cognitive measurement for lon-gitudinal study. The different temporal patterns of biomark-ers for these two scores also suggest that restricting the two models for predicting these two scores to share a common set of features as in [39] may lead to sub-optimal performance.
We also perform stability selection of nFSGL1 and nFSGL2 using only MRI biomarkers. The results are given in Figure 2 and Figure 3. We observe that most biomarkers identified in cFSGL are also included in the top feature lists in nFSGL. This demonstrates the consistency between these two ap-proaches. We also observe that the patterns of temporal selection stability differ from that of cFSGL in that fewer features have high probability. In nFSGL2 there is only one feature, namely cortical thickness average of right En-torhinal cortex, that has high probability at all time points, compared to 5 in cFSGL longitudinal stability selection. In nFSGL2 we observe that white matter volume of left Hip-pocampus also maintains a high stability vector. The higher temporal sparsity observed in nFSGL may be due to the non-convex (0 . 5 , 1) -norm penalty. In this paper, we propose a convex fused sparse group Lasso (cFSGL) formulation for modeling disease progres-sion. cFSGL allows the simultaneous selection of a com-mon set of biomarkers for multiple time points and specific sets of biomarkers for different time points using the sparse group Lasso penalty and at the same time incorporates the temporal smoothness using the fused Lasso penalty. We show that the proximal operator associated with the opti-mization problem exhibits a certain decomposition property and thus can be solved effectively. To further improve the model, we propose two non-convex formulations, which are expected to reduce the shrinkage bias in the convex formu-lation. We employ the difference of convex (DC) program-ming technique to solve the non-convex formulations. The effectiveness of the proposed progression models is evalu-ated by extensive experimental studies on data sets from the Alzheimer X  X  Disease Neuroimaging Initiative (ADNI) data sets. Results show that the proposed progression models are more effective than an existing multi-task learning formula-tion for disease progression. We also perform longitudinal stability selection to identify and analyze the temporal pat-terns of biomarkers for MMSE and ADAS-Cog respectively. The presented analysis can potentially provide novel insights into the AD progression.

Our proposed formulations for disease progression assume that the training data is complete, i.e., there are no missing values in the feature matrix X .Weplantoextendour formulations to deal with missing data.
 This work was supported in part by NIH R01 LM010730, NSF IIS-0812551, IIS-0953662, MCB-1026710, and CCF-1025177.
