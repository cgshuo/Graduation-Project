 Hypergraph partitioning has been considered as a promising method to address the challenges of high dimensionality in document clustering. With documents modeled as vertices and the relationship among documents captured by the hy-peredges, the goal of graph partitioning is to minimize the edge cut. Therefore, the definition of hyperedges is vital to the clustering performance. While several definitions of hyperedges have been proposed, a systematic understand-ing of desired characteristics of hyperedges is still missing. To that end, in this paper, we first provide a unified clique perspective of the definition of hyperedges, which serves as a guide to define hyperedges. With this perspective, based on the concepts of hypercliques and shared (reverse) nearest neighbors, we propose three new types of clique hyperedges and analyze their properties regarding purity and size issues. Finally, we present an extensive evaluation using real-world document datasets. The experimental results show that, with shared (reverse) nearest neighbor based hyperedges, the clustering performance can be improved significantly in terms of various external validation measures without the need for fine tuning of parameters.
 H.3.3 [ Information Search and Retrieval ]: Clustering Algorithm, Experimentation Document Clustering, Shared Nearest Neighbor,Clique, Hypergraph Partitioning
Document clustering is often addressed with probabilistic methods, which encounters a number of problems due to the challenges of high dimensionality. An alternative is graph-theoretic techniques. One previous work along this line is Association Rule Hypergraph Partitioning (ARHP) [3]. [1, 5] offered comprehensive evaluations that involved hyper-graph and other models such as hierarchical approaches and k-means. Specifically, ARHP first constructs a hypergraph with individual items as vertices and frequent itemsets as hyperedges. Then a hypergraph partitioning algorithm is applied to obtain the item clusters. Because the goal of graph partitioning is to minimize the sum of weights of cut
T reating words (with size m ) as transactions and doc-uments as items, the frequent itemsets mined in ARHP with a support threshold s for hyperedges are examples of C-edges, if we define the adjacency test as AT ( d 1 ,d 2 )= supp ( { d 1 ,d 2 } )  X  sm . That is, we say two documents are adjacent if they share at least sm words.

With binary document vector representation, the cosine similarity between two documents is cos ( d 1 ,d 2 )= supp ( { d 1 ,d 2 } ) / any two documents in the frequent itemset is guaranteed to share at least sm words, their cosine similarity may be very low if they are supported by two large but different sets of words, respectively. To circumvent this problem, we propose to use hypercliques as hyperedges, with correspond-ing h-confidences as edge weights. They are also examples of C-edges if we define the adjacency test as AT ( d 1 ,d 2 )= cos ( d 1 ,d 2 )  X  h c , where h c is the h-confidence threshold. If we generalize the cosine for itemset P = { d 1 , ..., d m } as cos ( P )= supp ( P ) / ( for P is also lower bounded by h c .
We define a new type of clique hyperedge based on shared nearest neighbors (SNN), with the following adjacency test: AT ( d 1 ,d 2 )= | kNN ( d 1 )  X  kNN ( d 2 ) | X  s c . That is, ev-ery two documents in the clique share at least s c kNNs. Similarly, we can define a clique hyperedge based on shared reverse nearest neighbors (SRNN). Such cliques can again be found via association mining. The first step is to per-form kNN queries for every document and store the results in a binary matrix NN . Because RkNN and kNN are com-plementary, by transposing NN , we get matrix RN N that stores the RkNN relationship.

For simplicity, we assume every document has exactly k +1 kNNs: k other documents plus itself. Then some inter-esting observations can be made about the corresponding RkNN transaction database for RN N , where each transac-tion corresponds to a document/item p and stores p  X  X  RkNNs ( p has at least one RkNN: itself). 1) For any itemset P , hconf ( P )= supp ( P ) n/ ( k + 1), where n is the number of transactions in the database. 2) The support count of item-set P is equal to the number of kNNs shared by all items in P . 3) Given an itemset P = { i 1 , ..., i t } that are contained in s transactions corresponding to Q = { j 1 , ..., j s } , then, a) P  X  X  SNN set is exactly Q ;b) Q  X  X  SRNN set is exactly P if P is closed, i.e., no supersets of P keep the same support as P .
Similarly, to construct SRNN based supercliques, we can mine frequent itemsets from the kNN transaction database for matrix NN , with the following properties: 1) The sup-port count of a single item is equal to the number of its RkNNs, which may vary considerably, starting from 1. 2) The support count of itemset P is equal to the number of RkNNs shared by all items in P . In this case, h-confidence is used as hyperedge weight.
In our experiments, we used 10 datasets [7] from differ-ent sources and evaluated the clustering results with: nor-malized mutual information( NMI ), error rate( ERR ) and F-measure. Because the true class sizes are highly imbal-anced in our datasets, we tried the number of partitions N with N = K and N =2 K respectively, where K denotes the number of true classes. Since the clustering results depend on the particular support/h-confidence threshold used, for
