 We study discriminative clustering for market segmentation tasks. The underlying problem setting resembles discrimi-native clustering, however, existing approaches focus on the prediction of univariate cluster labels. By contrast, market segments encode complex (future) behavior of the individ-uals which cannot be represented by a single variable. In this paper, we generalize discriminative clustering to struc-tured and complex output variables that can be represented as graphical models. We devise two novel methods to jointly learn the classifier and the clustering using alternating op-timization and collapsed inference, respectively. The two approaches jointly learn a discriminative segmentation of the input space and a generative output prediction model for each segment. We evaluate our methods on segmenting user navigation sequences from Yahoo! News. The proposed collapsed algorithm is observed to outperform baseline ap-proaches such as mixture of experts. We showcase exem-plary projections of the resulting segments to display the interpretability of the solutions.
 I.5.3 [ Clustering ]: Algorithms Algorithms, Experimentation Discriminative Clustering, Market Segmentation
Market segmentation reveals divisions in a given market, where a market refers to a population of interest such as  X  T his work was performed during an internship at Yahoo! Research, Barcelona, Spain.  X  Also Yahoo! Research, Barcelona, Spain.
 people, customers, or organizations. A market segment is a subset of a market that is characterized by similar demands and/or needs based on qualities of a given product such as price or function.

Every segment is required to meet the following criteria. (i) It is homogeneous within the segment, so that individuals within the same segment exhibit common needs and can be targeted jointly with the same marketing strategy. (ii) It is easily distinguishable from other segments to guarantee that different segments have different demands and (iii) serves as a blueprint for distinct targeting strategies. The require-ments are often summarized as homogeneity , identifiability , and interpretability (19).

Besides frequently deployed self-organizing maps (SOMs) (13; 7), market segmentation matches the problem setting of model-based clustering approaches. Clustering techniques either minimize the within-cluster similarity (1; 14), maxi-mize the between-cluster similarity (7), or optimize a combi-nation of both (18), and thus aim to produce homogeneous and identifiable solutions. Once segments have been com-puted, new customers need to be assigned to one of the subgroups to advertise the product accordingly.

Unfortunately, mapping new instances to an existing clus-tering is often difficult in practice. Intuitively, the new cus-tomer should be grouped to the closest segment with respect to some similarity measure. Closeness can for instance be computed as the distance in feature space to the median or the nearest member of a segment (11; 4). However, often at the time of classifying a new instance, not all features are known. Even using the similarity measure that is used by the clustering method itself on all features is frequently ob-served to perform surprisingly inaccurate, see e.g., (20) and Section 4. Other difficulties are for instance distribution-based clusterings, such as Expectation Maximization (6), which assign probabilities for cluster-memberships. By do-ing so, customers are probabilistically related to every seg-ment. Converting this soft assignment into a hard assign-ment by taking a maximum a posteriori or winner-takes-all decision is often suboptimal if the memberships deviate from a point distribution, in which case more than one segments are likely candidates (14).

Optimally, the segmentation is therefore learned together with a classifier that discriminatively maps new instances to clusters; a problem setting which is also known as dis-criminative clustering (5; 20; 24; 8). The idea is to have the clustering provide the labels for the classifier which is trained in a supervised manner. The joint optimization al-ters the clustering so that the segments can be easily dis-criminated from each other by the classifier. Combining the t wo criteria thus guarantees concise clusterings and accu-rate classifiers. Existing approaches focus on clustering a population and predicting a cluster label for a new instance. By contrast, market segmentation is more complex. In mar-ket segmentation tasks, we need to differentiate between the data that characterizes individuals and the data that charac-terizes their future behavior. The clustering clearly needs to take all available information into account to generate mean-ingful segments. However, the classifier does not have access to future events and needs to take a decision on the available information such as gender, income, etc. This observation renders existing approaches to discriminative clustering too restrictive for market segmentation tasks.

In this paper we generalize discriminative clustering for market segmentation tasks using the structured prediction framework. We differentiate between attributes of a cus-tomer and her interests/behavior . Attributes are a priori available features of individuals of the population such as gender or income. Her behavior is a collection of interact-ing variables describing a segment. As segments need to be interpretable, we model the output data as a complex and structured variable which can be represented as a graphical model. The distinction allows for learning a classifier only on the attributes, computing the clustering on both attributes and behavior, and finally summarizing the segments only in terms of the behavior.

We devise two solutions which are based on the regular-ized empirical risk minimization framework. The first is a straightforward adaptation of mixtures of experts. Classi-fier and clustering are optimized using an alternating strat-egy where we fix one component while optimizing the other. The second solution uses approximations and integrates out parameters of the classifier using collapsed inference for effi-ciency. Both approaches use generative models for the out-put structure and, in contrast to conventional discriminative clustering approaches, do not involve trade-off parameters for classification accuracy and cluster consistency (class bal-ance) because the optimization problems are not prone to trivial and degenerate solutions.

Use cases of our methods contain traditional market seg-mentation tasks. Consider for instance a company that aims at promoting a new product or a hotel chain that intends to lure visitors with special offers. Our methods not only com-pute a meaningful segmentation of the customers but also allow for devising appropriate targeting strategies from the graphical models. Moreover, our method serves as discrim-inative clustering for structured variables, where the task is not to output a single class/cluster label but the average structure for every segment. The differentiation between attributes and behavior increases the range of applications that can be addressed. A special  X  but still novel  X  case is obtained when attributes and behavior partially overlap.
Empirically, we study our methods on another interesting use case: Segmenting user navigation sessions on the Web for displaying segment-specific website layouts. We experi-ment on a large click log from Yahoo! News. The attribute data is assembled from meta-information about the session such as the timestamp, the referrer domain, and the first page request. The behavior consists of subsequent naviga-tion actions given by click sequences. The generative rep-resentation of the behavior data is interpretable and can be easily transformed into segment-specific layouts.
The remainder of the paper is structured as follows. Sec-tion 2 discusses the relationship of our problem setting with previously studied settings and methods. In Section 3 we derive two algorithms to optimize the empirical counterpart of the expected segmented log-likelihood. Section 4 reports on empirical results using a large click log from a commercial news provider and Section 5 concludes.
Market segmentation tasks are often solved using neural networks such as self-organizing maps (13; 7). Kiang et al. (13) for instance extend self-organizing maps to group cus-tomers according to their attitude towards different commu-nications modes. D X  X rso and de Giovanni (7) use the natu-ral clustering property of self-organizing maps together with dissimilarity measures which capture temporal structure of the data. In general, clustering data with self-organizing maps and variants thereof inherently implements the homo-geneity assumption of market segmentation. However, clas-sifying new instances into the clustering is often difficult and it is not possible to output generative models to summarize the resulting clusters. Additionally, the optimization crite-rion of self-organizing maps is highly sensitive to the actual initialization and usually converges to different local optima.
Related to market segmentation is the task of estimating a mixture model for observations (6). Introducing selector variables encoding probabilistic cluster-memberships, maxi-mizing the log-likelihood by marginalizing over the selector is usually straightforward. The selector can be modeled in a data-dependent or data-independent fashion but the prob-abilistic nature of the cluster-memberships render a direct application for market segmentation tasks impossible.
Discriminative clustering simultaneously computes a seg-mentation of the data at hand and a classifier that discrim-inates the resulting clusters well. Existing approaches in-clude projections into lower-dimensional subspaces (5), joint optimization of max-margin classifiers and clusterings (20; 24), the optimization of scatter metrics (21), and the max-imization of an information theoretic criterion to balance class separation and classifier complexity (8). Sinkkonen et al. (17) aim to find clusters that are homogeneous in auxiliary data given by additional discrete variables. The above mentioned approaches do not predict any output vari-able but focus on the discrete cluster variable. Moreover, in our generalized problem setting, instances are represented as input-output pairs. The classifier discriminates the clusters given only the input, whereas the cluster parameters need to accurately estimate the outputs of the contained instances. Previous work on discriminative clustering does not split in-stances into two parts. They represent instances as a single input which consequently allows the classifiers to access the whole example at decision time. The same assumption is commonly being made in market segmentation studies that involve model-based clustering approaches, (9; 16; 22) but prohibits a natural solution for market segmentation tasks.
This problem setting can be seen as an alteration of the setting which the Mixture of Experts approach (10; 12) aims to solve, where the behavior y is predicted given the at-tributes x as a mixture model where the mixture component weights depend again on x . In our case, mixture component weights have to be always point distributions as demanded by the application. Framing the distribution of y given the mixture component as a pure generative model allows us to derive a more efficient algorithm than that of the Mixture o f Experts approach.

Zhao et al. (23) proposed a maximum-margin cluster-ing for multivariate loss functions. Minimizing the complex losses allows for capturing structural differences that cannot be expressed in terms of standard misclassification rates. In principle, by defining a loss function that captures the differences of two clusterings, one could possibly solve mar-ket segmentation tasks as their approach implicitly favors clusterings that are easily discriminable. However, the loss function cannot be expressed in terms of the contingency ta-ble of the two clusterings, and the decoding problem in the inner loop of the algorithm, that is finding the most violated constraint, becomes intractable in practice.

Also related to our problem setting are multi-view clus-tering approaches, where the data is split into two disjoint feature sets, which are sometimes also called views. Bickel and Scheffer (2) present an intertwined Expectation Maxi-mization algorithm to compute the maximum likelihood so-lution for one view using the expectations provided by its peer view. The two data views are modeled generatively and the algorithm maximizes the joint marginal likelihood. By contrast, we aim to find a discriminative classifier on the input view and instead of maximizing the marginal likeli-hood of the output view we seek to maximize the likelihood conditioned on a hard cluster assignment.
We now present our main contribution, the generalization of discriminative clustering for structured output variables to solve market segmentation problems. We introduce the problem setting in the next section and present a straightfor-ward solution in terms of mixtures of experts in Section 3.2. An efficient approximation is devised in Section 3.3 and Sec-tion 3.3.3 discusses scalability issues.
We are given a sample M from a market where individuals are represented by tuples ( x, y )  X  X  X Y encoding attributes x and behavior y . Attributes x may encompass individual features like gender, income, etc while the expressed historic behavior is captured by y  X  Y and represented as a graph-ical model. The behaviors y are governed by a family of distributions denoted by P ( y |  X  ) with natural parameter  X  .
In our running example on segmenting user navigation on the Web, attributes x encode meta-information about the session such as the timestamp, the referrer domain, and the first page request and is represented as a feature vector. The behavior y encodes sequences of the subsequent Web navigation and can for instance be represented as a Markov-chain where nodes correspond to pageviews and connecting edges visualize clicks.

We aim to find an appropriate segmentation of the mar-ket M . Formally, the goal is to find a classifier h : X  X  { 1 , . . . , k } that maps attributes x to one of k clusters, pa-rameterized by  X  = (  X  1 , . . . ,  X  k ), where the  X  j are chosen to maximize the likelihood of the observed behaviors y of the respective segment. The number of clusters k is assumed to be given by the application at hand, because it consti-tutes a trade-off between predictive power and effort spent for developing multiple market strategies. As h and  X  are not independent they need to be optimized jointly. Hence, over all classifiers h and parameter collections  X  , we aim at maximizing the expected risk functional R that is defined in terms of the segmented log-likelihood Since the true joint distribution P ( x, y ) is unknown, we re-place Equation (1) by its empirical counterpart on the finite market sample of size n given by M = { ( x i , y i ) } n i =1 Directly maximizing Equation (2) in terms of the compo-nent parameters  X  and the classifier h is infeasible, since the objective function is not only highly non-convex and non-continuous but an NP-hard problem because of combinato-rial assignments. However, if the classifier h was fixed, the  X  could be optimized directly, as h provides the segmentation and for each segment j optimal parameters  X   X  j are trivially computed by For many common distribution families, the maximum like-lihood estimates P ( y |  X  ) can be computed easily by counting or averaging over the observations in the segment, respec-tively. Vice versa, keeping the segment parameters  X  1 , . . . ,  X  fixed, learning the classifier h results in a standard multi-class classification scenario. Using linear models, h can be written as where each segment has its own weight vector w j . In the re-mainder, we will use h and w = ( w 1 , . . . , w k )  X  interchange-ably. The next section exploits this observation and presents a joint alternating optimization scheme.
A straightforward approach to solve market segmentation problems is to alternate the optimization of the classifier and the clustering while fixing the other, respectively. As shown in Equation (3), keeping the classifier fixed allows to apply standard maximum likelihood techniques to compute the natural parameters of the segments. We thus focus on deriving the classifier h for a fixed clustering. We make use of the maximum-margin framework and deploy a re-scaled variant of the hinge loss to deal with uncertain cluster-memberships (or class labels).

The idea is as follows. Intuitively, an individual ( x, y ) should be assigned to the segment that realizes the high-est log-likelihood with respect to y . However, two or more segments might be competing for the instance and realize Algorithm 1 A lternating Optimization 2: Initialize  X  randomly 3: repeat 5: s.t. ( w  X  j  X  7: until convergence similar log-likelihoods, in which case a winner-takes-all d e-cision is prohibitive. We thus treat the difference of the log-likelihoods between the most likely segment j  X  and cluster j 6 = j  X  as a misclassification score , given by These scores can be incorporated in a support vector ma-chine by re-scaling the hinge loss and act like example-de-pendent costs (3). The re-scaled hinge loss becomes a convex upper bound of the difference of the log-likelihoods, Stacking up w = ( w 1 , . . . , w k )  X  and  X  = (  X  1 , . . . ,  X  arrive at the following maximum-margin optimization prob-lem f or 1  X  i  X  n , j  X  X  1 , . . . , k } , j 6 = j  X  , j  X  i and regularizaion parameter  X  &gt; 0.

Hence, Equation (2) can be optimized by randomly initial-izing  X  and subsequently alternating between optimizing w and  X  j while fixing the respective peer. Algorithm 1 instan-tiates the pseudo-code as a member of the Expectation Max-imization framework. The alternating optimization scheme can also be interpreted as an adaptation of the EM algorithm for a mixture of experts (12). The classifier h , given as the weight vectors w j , corresponds to the gating networks. In contrast to the conventional mixture of experts model, it is trained using an SVM to output deterministic decisions, instead of soft decisions. The generative distribution over behaviors differs from the expert networks only in the fact that in our problem setting the prediction of the behavior y is not allowed to depend directly on the attributes x , only via the cluster assignment.

A major drawback of the alternating approach is however the discrete assignment of individuals to only a single seg-ment, even during the intermediate optimization steps. As a consequence, the algorithm is prone to degenerate solu-tions and poor local optima. Additionally, the optimization is expensive in terms of computational time as it requires the computation of a multi-class SVM until convergence in every step.
We now present an efficient approximation of the discrim-inative segmentation problem by using a continuous relax-ation to the original problem formulation. We first show that the parameters of the classifier can be computed in closed-form so that the joint optimization problem depends only on the segment parameters. Second, we devise an EM-like algorithm (6) to optimize the remaining objective.
As the discrete cluster assignments cause many difficul-ties, we now replacing them by soft assignments using an adjustable soft-max function. The parameter  X  controls the degree of reproducing the maximum, that is for  X   X   X  we precisely obtain the maximum operator. Incorporating the soft-max in Equation (2) yields the optimization problem which still contains the mutually dependent variables  X  and w . To obtain an efficiently solvable optimization problem, we express the objective as a continuous function of w so that w can be eliminated using collapsed inference. Instead of the hinge loss in Equation (6), we employ another tight convex upper bound in terms of the squared loss, Implicitly, introducing the squared loss converts the classi-fier into a regressor that aims at predicting the log-likelihood for an individual ( x, y ) and the j -th segment as accurate as possible. Assuming the log-likelihoods were predicted perfectly, the parameters w would not only be optimal for the regression but also for Equation (2) as the classifier h in Equation (4) would still return the most likely segment. Changing the loss function also has the advantage that now the optimal solution for w can be computed analytically. The corresponding regularized optimization problem is also known as regularized least squares regression (RLSR) or ridge regression and is given by for  X  &gt; 0. Setting the derivative with respect to w to zero and solving for w j , we obtain the optimal solution that can be computed in closed-form where X  X  R n  X  d contains the stacked attribute vectors and  X  (  X  j ) = (log P ( y 1 |  X  j ) , . . . , log P ( y n |  X  likelihoods for the j -th segment. The computation of the inner product can effectively be sped-up by precomputing the linear trans-formation of the attributes. Introducing auxiliary variables  X  x given by allows to efficiently rewrite the inner product as w  X  j x =  X  (  X  j )  X   X  x . Further note that  X  x depends only on inner prod-ucts of individual attributes. Hence, the kernel trick can be applied to incorporate non-linearity. Introducing a Mercer kernel  X  , the auxiliary matrix  X  X can be written in terms of the corresponding kernel matrix K with elements K ij = expressed in terms of a set of dual multipliers  X  and the ker-nel function as h ( x ) = arg max j P i  X  ji  X  ( x i , x j multipliers can be obtained explicitly as a function of the component parameters as
Substituting the obtained observations in Equation (7) results in a simplified optimization problem that does no longer depend on w a nd that has only the  X  j as free param-eters,
The optimization problem in Equation (11) can be solved with an EM-like algorithm (6) using auxiliary variables z 0, with P j z i,j = 1, encoding the belief that the i -th exam-ple belongs to the j -th cluster. EM-like algorithms consist of two phases which assign instances to generating segments (E-step) and maximize the segment parameters with respect to the associated instances (M-step), respectively. In the E-step, it therefore suffices to identify the auxiliary variables with the true posterior probabilities given the current  X  , Following the general EM framework, we express the empir-ical risk functional in Equation (11) in terms of the expec-tations z i,j . This allows us to effectively pull the logarithm into the sum over segments for the M-step; we arrive at the optimization problem which can be rewritten as max The above Equation is to be maximized with respect to  X  . Compared to conventional M-steps for mixture model esti-mation problems,  X  appears not only in P ( y i |  X  j ), but also in what are usually the segment weights for each example. This renders the objective function non-concave and, conse-quentially, there is no exact analytical solution.
As a remedy, we derive an approximation that is linear and non-decreasing in the  X  (  X  j ), rendering the objective concave in  X  j and thus analytically solvable for common choices of P ( y |  X  j ). We begin with approximating the normalization lor expansion around the current  X  old which is given by cients and C is a constant. Substituting Equation (12) into the objective function of the M-step and collecting the coef-ficients gives us argmax Algorithm 2 C ollapsed Optimization Algorithm 1: I nput: ( x 1 , y 1 ) , . . . , ( x n , y n ) ,  X  2:  X   X  1, t  X  1, initialize  X  (0) randomly 3: repeat 4: E-step: Q ( z i = j )  X  P ( z i = j | x i , y i ,  X ,  X ,  X  5: M-step:  X  ( t )  X  arg max  X  P i P j Q ( z i = j )  X  6: log P ( y i , z i = j | x i ,  X ,  X ,  X  ) 7:  X   X   X   X  1 . 1, t  X  t + 1 8: until convergence 9:  X   X  = argmax  X   X  R (  X  ,  X  (  X  )) 10:  X   X   X  (  X   X  ) which is a linear function of log P ( y i |  X  j ) . For increasing  X  , the Taylor coefficients for each instance approach a point dis-The same holds for the auxiliary variables z i,j , which ap-proach t ij . Thus, the second summands of the coefficients in Equation (13) approach zero, and hence for large  X  all the log P ( y i |  X  j ) have either positive or very small negative coef-ficients. We clip coefficients below zero to guarantee that the objective function is non-decreasing in the log P ( y i |  X  obtain an approximation that approaches the exact solution with increasing  X  .

The softmax factor  X  therefore constitutes a trade-off be-tween accurately approximating the original optimization problem of maximizing  X  R (  X  , h ) and smoothing the objective function to facilitate finding good local optima.
We deal with this tradeoff by starting the EM-algorithm with  X  = 1, and multiplying it by a constant factor each iter-ation. Preliminary experiments have shown the factor 1 . 1 to work well. Due to the approximation of the hard decisions with a soft-max, the algorithm is not guaranteed to mono-tonically increase the true objective value. We thus select the intermediate result with the highest objective value as the final solution for the cluster parameters.

Algorithm 2 shows the collapsed optimization algorithm in pseudo code. In line 2, the cluster parameters  X  are ini-tialized randomly. Line 4 performs the expectation step of the EM-algorithm, computing the current posterior es-timates, given the soft-max factor  X  , the cluster parame-ters, and implicitly also the classifier h . The maximization step in line 5 boils down to an optimization problem of the form P i P j c ij log P ( y i |  X  j ), which for non-negative coeffi-cients c ij can be solved analytically for many choices of dis-tribution families. For example if P ( y |  X  ) is a multinomial distribution over y  X  { 1 , . . . , m } with P ( y = q |  X  the maximum is attained for  X  j,q = P i c ij [[ y i = q ]] / P for all j, q , where [[  X  ]] is the indicator function. Finally, lines 9 and 10 select the best intermediate solution in terms of the true objective, and re-obtain the explicit classifier using Equation (10).
The computational bottlenecks of the collapsed optimiza-tion algorithm are the computation of the  X  x i , involving ma-trix inversions and multiplications, and the computation of the coefficients in Equation (13), where we have to sum over all pairs of examples, leading to an overall complexity of the algorithm of O ( n 2 . 376 + n 2 kT ).

For applications with a large number of examples the super-quadratic dependence on the sample size n makes the algorithm effectively intractable. We can alleviate this by r andomly partitioning the examples in the least-squares es-timation in Equation (8) into s disjoint subsets S (1) , . . . , S of size m . For each subset the weight vectors w ( l ) are esti-mated separately, and thus within each subset the vectors  X  (  X  j ) and the transformed examples  X  x have only m compo-nents. Consequently, in Equation (13) the inner summation over the examples only runs over the m examples in the subset to which example ( x i , y i ) belongs. Finally, we obtain the parameters of the classifier h by averaging the weight vectors over all subsets, w j = 1 s P l w ( l ) j .
M ixing the separately learned weight vectors is identical to the mixture weight method by Mann et al. (15) that has been shown, theoretically and empirically, to yield results close to learning a weight vector using all examples at once. Note however that  X  is still learned from the whole, unparti-tioned training sample. Using the partitioned estimation for the weight vectors, the overall complexity of the algorithm becomes O ( nm 1 . 376 + nmkT ). For m  X  n , the computation becomes tractable even for very large sample sizes n .
In this section, we evaluate the proposed algorithm using a large data sample from Yahoo! News United Kingdom 1 . The click log contains browsing session logs, where events from the same user are identified by their browser cookies and sessions are split after 25 minutes of inactivity. We use all sessions from June 2011 for training and the first week of July 2011 for testing. Figure 1 shows the categories, such as politics/world , politics/uk , science/space , in the training set, averaged over the four weeks where different colors cor-respond to different categories 2 . Figure 1: Click volume of categories over time.

T he goal of our study is threefold: Firstly, we aim to seg-ment the user sessions of Yahoo! News according to their interests expressed by clicks. Secondly, new sessions need to be classified to one of the segments so that every seg-ment accurately predicts subsequent clicks of the respective user. Finally, the segments need to be interpretable to allow for devising target strategies from the segment description.
A ll processing is anonymous and aggregated
Colors occur more than once due to the large number of categories.
 A typical targeting strategy in our Yahoo! News example could for instance be a dynamic layout of the Web site to advertise news articles of categories that the respective user is probably interested in.

From a data perspective, modeling sequences of clicked categories by Markov processes is straightforward. How-ever, Markov processes, e.g., visualized by transition matri-ces, are difficult to interpret as the entries encode interests with respect to the previous category. Taking the inferred Markov model properly into account would imply changing the website layout within a session depending on the previ-ous category. A simpler way to obtain interpretable clusters is to use multinomial distributions for the output variables of interest. We use the sequences of user clicks enriched with the respective locations of the clicks. That is, the behavior y consists of the multi-set of subsequently clicked categories c and link sections s . The distribution P ( y |  X  j ) is defined as the product of multinomial distributions where  X  and  X  are the parameter vectors governing the dis-tributions over categories and link sections, respectively.
The attributes x of a session is represented as a binary feature vector encoding the most common referrer domains, the respective category of the first pageview, as well as fea-tures encoding the timestamp; we use binary indicators for every day of the week, for each hour of the day, and for each hour of the week. For the collapsed algorithm, we use a linear kernel and randomly partition the training data into disjoint subsets of size 1,000 for computing the predicted log-likelihoods.
We compare the collapsed algorithm with three baselines, the alternating optimization scheme in Section 3.2, a mix-ture of experts model and a k -means based solution. The mixture of experts model (12) minimizes the squared error in terms of the within-cluster log-likelihoods and optimizes the marginal likelihood Instead of the prior P ( z = j ) we have a conditional distribu-tion P ( z = j | x ) which is defined in analogy to the collapsed algorithm as The mixture of experts model is optimized with a standard EM-algorithm and therefore provides only probabilistic clus-ter assignments and does not take into account that sessions need to be assigned to only a single cluster.

The third baseline is derived from the straightforward, yet somewhat na  X   X ve, approach to segment the input space first and only then optimize the generative model in each cluster. The drawback of this non-iterative approach is that it does generally not lead to homogeneous behavior within clusters because the segments are fixed when estimating the genera-tive models. We use k -means for finding the clustering, and estimate segment paramters  X  by maximum likelihood based on the hard cluster assignments. The classifier h classifies a new instance into the cluster with the nearest centroid.
In each setting, every algorithm is deployed 10 times with r andom parameter initializations and in the remainder we only report the results of the run with highest training like-lihood.
In this section, we evaluate the convergence behavior of the collapsed algorithm. Recall that the collapsed algorithm optimizes an approximate objective, where the hard clus-ter assignments are replaced by a soft-max controlled by an increasing factor  X  . To cancel out effects caused by the approximation, we substitute the resulting  X  into the ex-act optimization criterion in Equation (2) and measure the respective objective value. Note that the results do not nec-essarily increase monotonically. Figure 2: Objective values for the collapsed algo-r ithm (solid) and the mixture of experts baseline (dashed), for different numbers of clusters k .

Figure 2 shows the results for different numbers of clus-ters for the collapsed algorithm (solid curves). For compar-ison, we also added the mixture of experts baseline (dashed curves). As expected, the true objective value is not mono-tonic, since both algorithms optimize an approximation to the exact optimization criterion. The figure also shows that the best values are obtained after at most 20 iterations.
To evaluate the performance of the collapsed algorithm, we measure its predictive accuracy in terms of how well fu-ture behavior can be predicted. The classifier and the seg-mentation are learned jointly as described in Section 3 us-ing the training set and then deployed to the test set. The sessions in the test set are first classified by the classifier in one of the segments which is then used to predict the future clicks of the user. Since the final prediction is a com-plex variable, we refrain from expressing the performance in terms of error rates and measure the predictive log-likelihood log P ( y |  X  h ( x ) ) instead. We compare the collapsed algorithm to the alternating optimization scheme, the mixture of ex-perts model, and the k -means based solution. We report on averages and standard errors over 10 repetitions with differ-ent random initializations.

Figure 3 shows the predictive performance for varying numbers of clusters. Not surprisingly, all methods perform Figure 3: Averaged predictive performance and s tandard error. equally worse for only a single cluster. For only a few clus-ters, the mixture of experts baseline performs about as well as the collapsed algorithm. We credit this finding to the existence of easy-to-reach solutions that do not necessarily require hard cluster assignments in the  X  -steps. However, when the number of clusters grows, the performance of the mixture of experts approach decreases slightly while that of the collapsed model increases. Here it becomes more and more important to select the parameters in a way that al-lows to discriminate well between the clusters, and thus the collapsed algorithm outperforms the baselines significantly. The alternating algorithm and the k-means baseline perform significantly worse than the collapsed algorithm. Only for 20 and more clusters the alternating algorithm produces bet-ter results than the mixture of experts model. Note that the k-means performs worst as it does not use an alternat-ing update schema but first learns the clustering and then estimates the generative models using the fixed segments.
It is apparent that the predictive performance levels off after increasing the number of clusters beyond 10. Intu-itively, this observation can be explained by a trade-off be-tween classification and segmentation: even if a more fine-grained clustering would be able to predict the future behav-ior more accurately, the classifier cannot discriminate well between a larger number of similar clusters to identify the best-matching segment. We observe a natural trade-off be-tween predictive power and the effort that has to be spent for developing and maintaining target strategies for a large number of market segments.

The execution time of the collapsed algorithm for a solu-tion with 10 clusters is within the range of 3 hours, compared to about an hour each for the mixture of experts and the k -means baselines. The alternating optimization however takes about 11 hours which renders its application infeasi-ble in practice.
Market segmentation aims at grouping similar individu-als of a population together that share the same needs or that have similar demands. The goal is to target individu-als within the same segment jointly e.g., to advertise a new product. To this end, the segments need to be interpretable F igure 4: Visualization of click frequencies for the five most frequent link locations using four clusters. to derive a concise description of the segments that can be converted into a segment-specific targeting strategy.
In our collapsed algorithm, generative models in each seg-ment encode the contained behavior and interest. The flex-ibility of the probabilistic inference machinery allows us to project the behavior onto discriminative variables to visu-alize different characteristics of the clusters. In this section we give two examples for such projections to visualize dif-ferently distributed user behavior across the clustering. For simplicity, we use a solution with four clusters.
The first example shows a visualization of segment-specific user clicks in terms of their location on the Web page. In-cluding the location of clicks is necessary for altering the lay-out dynamically as changes in frequently clicked areas will have impact the behavior more than substituting a redun-dant and less clicked widget. We focus on the five modules of the Web site that receive the highest number of clicks in the data.

Figure 4 shows the results. Segments 2, 3, and 4 exhibit very similar click behavior in terms of the clicked modules. By contrast, cluster 1 differs significantly in the usage of the Web components. On average, users in cluster 1 prefer the location visualized in black over the alternatives compared to users in the other segments. This observation could be ex-ploited to directly devise target strategies. While members of cluster 2 X 4 should be addressed by changing the content of the modules visualized in gray or dark blue, users in the first segment could also be triggered by the module encoded in black.

Analogously, the behavior could be projected on the cat-egories to visualize the respective distribution of categories for each segment. However, we choose to show a more inter-esting projection for lack of space. The incorporation of the timestamps of the sessions allows us to visualize the clus-ters in time. As the feature representation of timestamps encompasses one week, Figure 5 shows the average category distribution across the days of the week where different col-ors correspond to different categories. 3
Apparently, the clusters do not only differ in terms of the categories but also specialize on certain periods in time be-cause the segments are optimized using all available data, that is, attribute and behavior encoding variables. The first cluster clearly specializes on Sundays and is characterized by a clean topic distribution. The three other cluster also pos-sess dominant categories but focuses more on working days than on weekends. Cluster 4 contains the most diverse set of categories and acts like a basin for categories that are not as easy to discriminate. Here it becomes obvious that a solu-tion with only four clusters may not be optimal for the task at hand. When we increase the maximal number clusters, the category distribution of clusters becomes cleaner that is less categories are likely. Additionally, clusters adapt better to specialized periods such as working days or weekends for larger k .

Taking various such projections into account describes segments from different angles and helps to find a concise targeting strategy. For instance, knowing the articles that are likely to be read in an ongoing session helps to address the respective user in various ways including displaying ads. Incorporating context informations such as the click behav-ior of the segments, finally allows for tailoring web pages to each segment and to increase the overall user experience.
We studied discriminative clustering for structured and complex response variables that can be represented as gen-erative models. The problem setting matches market seg-mentation tasks where populations are to be segmented into disjoint groups. Solving market segmentation-like problems appropriately not only involves a clustering of the individu-als but also learning a classifier that discriminates well be-
C olors are again reused due to the large number of cate-gories. tween the segments, for instance to allow for classifying new c ustomers to one of the groups. The two components need to be learned jointly and have access to different pieces of information about the individuals: the classifier needs to group individuals on the basis of a priori available infor-mations while the clustering aims at grouping people with similar (future) needs or behavior.

We devised two algorithms based on alternating optimiza-tion and collapsed inference, respectively. Empirical results showed that the collapsed variant is not only more efficient but also predicts accurately the click behavior of users for Yahoo! News. The generative nature of the clustering led to interpretable clusters. We showed how projections of the clustering on only a few variables allowed for targeting the detected segments individually and contributed to user un-derstanding.

Our approach is not restricted to Yahoo! News and can generally be applied to arbitrary market segmentation tasks and other Web sites to improve the overall user experience. As our approach is orthogonal to personalized approaches, future work will study the integration of both frameworks. Part of this work was supported by the German Science Foundation under the reference number GA 1615/1-1. [1] N. Bansal, A. Blum, and S. Chawla. Correlation [2] S. Bickel and T. Scheffer. Multi-view clustering. In [3] U. Brefeld, P. Geibel, and F. Wysotzki. Support [4] R. D X  X ndrade. U-statistic hierarchical clustering. [5] F. De la Torre and T. Kanade. Discriminative cluster [6] A. Dempster, N. Laird, and D. Rubin. Maximum [7] P. D X  X rso and L. D. Giovanni. Temporal [8] R. Gomes, A. Krause, and P. Perona. Discriminative [9] J. Huang, G. Tzeng, and C. Ong. Marketing [10] R. Jacobs, M. Jordan, S. Nowlan, and G. Hinton. [11] S. C. Johnson. Hierarchical clustering schemes. [12] M. Jordan and R. Jacobs. Hierarchical mixtures of [13] M. Y. Kiang, M. Y. Hu, and D. M. Fisher. An [14] S. P. Lloyd. Least square quantization in pcm. IEEE [15] G. Mann, R. McDonald, M. Mohri, N. Silberman, and [16] M. Namvar, M. Gholamian, and S. KhakAbi. A two [17] J. Sinkkonen, S. Kaski, and J. Nikkil  X  a. Discriminative [18] K. Wagstaff, C. Cardie, S. Rogers, and S. Schr  X  odl. [19] M. Wedel and W. Kamakura. Market segmentation: [20] L. Xu, J. Neufeld, B. Larson, and D. Schuurmans. [21] J. Ye, Z. Zhao, and M. Wu. Discriminative k-means [22] W. Yu and G. Qiang. Customer segmentation of port [23] B. Zhao, J. Kwok, and C. Zhang. Maximum margin [24] B. Zhao, F. Wang, and C. Zhang. Efficient multiclass
