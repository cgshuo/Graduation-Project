 1. Introduction
An audio spectrogram can be decomposed as a linear combina-tion of spectral basis functions. In such a model, the short-term magnitude (or power) spectrum of the signal x  X  f ; t  X  in frame t and frequency f is modeled as a weighted sum of the basis functions as ^ x  X  f ; t  X  X   X  N
 X  where g n ( t ) is the gain of the basis function n at frame t , and b n  X  1 ; ... ; N are the bases. When dealing with musical instrument sounds in the context of automatic music transcription, ideally, each basis function represents a single pitch, and the correspond-ing gains contain information about the onset and offset times of notes having that pitch.

There are several methods in the literature that are used to estimate this type of decomposition, such as independent component 2004 ), atomic decompositions ( Gribonval and Bacry, 2003 ) and non-negative matrix factorization (NMF) ( Lee and Seung, 1999 ). for audio applications such as polyphonic audio transcription ( Abdallah and Plumbley, 2004 , 2006 ; Cont, 2006 ), speech recogni-tion ( Gemmeke et al., 2011 ; Hurmalainen et al., 2011 ) and sound source separation ( Virtanen, 2007 ; Ozerov and F X votte, 2010 ). the gains are sparse. Speci fi cally, the probability densities have peaks at zero and have heavy tails ( Olshausen and Field, 1997 ), or in other words, most observations can be encoded with only a few signi fi cant nonzero gain values. This assumption fi ts well with the notion that, in music, only a relatively small fraction of the available notes will be sounded at each frame ( Abdallah and Plumbley, 2004 ).
 non-negativity of the parameters is a natural restriction. As a result, it is possible to use the projected steepest descent algo-rithm, or it is possible to combine NMF and sparse coding, which leads to non-negative sparse coding (NNSC) ( Abdallah and Plumbley, 2004 ; Hoyer, 2004 ).
 adaptive. In a fi xed basis, the basis is learned by training the system on isolated notes, while in an adaptive basis, the basis is learned directly from the signal to be analyzed. As demonstrated provide a good generalization of the model parameters when the music scene in the training and test signals do not differ too much.
In this paper, we propose a signal decomposition method that can be used for monophonic music transcription and as a candidate selection technique in polyphonic transcriptors. This approach is based on NNSC which is constrained to explicitly assume the signal to be monophonic with only one possible state (note) at each frame.
This extreme sparsity constraint has been used before by other signal decomposition methods within a probabilistic framework.
For example, Benaroya et al. (2006) proposed a method for sound source separation in which each source Short Time Fourier
Transform (STFT) is modeled by a Gaussian Mixture Model (GMM) modulated by a frame-dependent amplitude parameter accounting for nonstationarity, which leads to the Gaussian Scaled Mixture
Model (GSMM) where the source is implicitly assumed to be monophonic with many possible states. This method has also been used in Durrieu et al. (2010) for main melody tracking in polyphonic audio signals. Ozerov et al. (2009) proposed a method called the Factorial Scaled Hidden Markov Model (FS-HMM) that generalized
GSMM and NMF with Itakura Saito divergence (IS-NMF) and incorporates temporal continuity using Markov modeling.
Conversely, we propose a novel method that enforces single-pitch and harmonicity constrains in a deterministic manner, performs the decomposition based on NNSC with Beta-divergence ( F X votte et al., 2009 ), and uses instrument speci information, which is learned in a supervised way (i.e. using a training stage). For the testing stage, the simplicity of the method allows for the direct computation of the factorization, which leads to very ef fi cient runtimes in comparison with other signal decomposition methods found in the literature, such as GSMM or FS-HMM. In fact, the runtimes obtained and the possibility to analyze each frame independently makes our method more suitable for realtime applications where very low latency is required. Moreover, we propose to use our method as a candidate selection stage in combination with a realtime signal decomposi-tion method to address polyphonic music transcription, which is a more complex scenario. To evaluate the reliability of our method, we have applied it to the music transcription of monaural mono-phonic and polyphonic signals, and we have obtained satisfactory results in comparison with other signal decomposition methods and selected state-of-the-art transcription methods.

The structure of the rest of the paper is as follows. In Section 2 , we review the harmonicity and sparsity constrained signal decom-position methods proposed in previous studies. In Section 3 ,a novel theoretical approach to constrain a signal model to be harmonic and have a single nonzero gain is explained, and the algorithm to perform the decomposition for music transcription is detailed. In Section 4 , we propose a realtime polyphonic transcrip-tion system in which the novel method explained in the previous section is used as a candidate selection technique. In Section 5 , the proposed method is applied to music transcription of monophonic and polyphonic signals and is compared with other state-of-the-art transcription methods. Finally, we summarize the work and discuss future perspectives in Section 6 . 2. Theoretical background 2.1. Basic Harmonic Constrained (BHC) method
As automatic music transcription is the application of the method used in this work, this method is constrained to be harmonic. This restriction has been used in other works devoted Vincent et al., 2010 ; Raczy  X  ski et al., 2007 ). The harmonicity in this way each basis can de fi ne a single fundamental frequency. This constraint is introduced in the model presented in Eq. (1) requiring that a distinct basis function represents each note of the instrument. b  X  f  X  X   X  M where b n ( f ), n  X  1 ; ... ; N are the bases for each note n , m the number of harmonics, a n  X  m is the amplitude of harmonic m for magnitude spectrum of the window function, and the spectrum of a harmonic component at frequency mf 0  X  n  X  is approximated by G  X  f  X  mf 0  X  n  X  X  .

The model for the magnitude spectra of a music signal is then obtained as ^ x  X  f ; t  X  X   X  N where the time gains g n ( t ) and the harmonic amplitudes a the parameters of the method to be estimated.

To obtain the factorization of Eq. (3) , the reconstruction error between the observed spectrogram x  X  f ; t  X  and the modeled spec-2010 ; F X votte et al., 2009 ; F X votte and Idier, 2011 ), the cost function to be minimized is the Beta-divergence,
The Beta-divergence includes in its de fi nition the most popular cost functions. When  X   X  2, the Beta-divergence is equivalent to the Euclidean (EUC) distance. The Kullback  X  Leibler (KL) diver-gence is obtained when  X   X  1, and the Itakura  X  Saito (IS) diver-gence is computed when  X   X  0. 2.2. BHC with Sparse Constraint (BHC-SC) method
Sparsity is a natural restriction applied to the gains that enforces the signal model to have only a few nonzero gains g n ( t ) at each frame t .Thisassumption fi ts well with the concept that, in music, only a relatively small fraction of the available notes will be sounded at each frame. In the special case of monophonic tran-scription, there should be only a single nonzero gain at each frame. Other works devoted to transcription and source separation have used sparseness in their signal models ( Abdallah and Plumbley, 2004 ; Gemmeke et al., 2011 ; Virtanen, 2007 ; Hoyer, 2004 ).
For signal models that minimize a divergence, the sparsity is typically introduced as a regularization penalty term ( Gemmeke et al., 2011 ). This penalty term helps to discard those solutions in which most of their gains are set to nonzero values. The cost function is then de fi ned as D  X  x  X  f ; t  X j ^ x  X  f ; t  X  X  X  D  X   X  x  X  f ; t  X j ^ x  X  f ; where  X  is a function that penalizes nonzero gains and  X  is a parameter that controls the importance of the regularized term. Although there are several de fi nitions for the penalty terms in the literature, in the experimental setup we have used the L1 norm  X   X  x  X  X   X  x  X  1 as proposed in Virtanen (2007) , Olshausen and Field (1997) , Cand X s and Wakin (2008) because it has proven to be less sensitive to variations in parameter  X  ( Virtanen, 2007 ).
This sparsity constraint computes the L 1 norm of the time gains, which is an effective method to determine sparse solutions ( Cand X s and Wakin, 2008 ; Chen et al., 1998 ). The introduction of this regularized term when using the KL divergence in a statistical framework can be seen as a Bayesian extension of a statistical model ( Virtanen et al., 2008 ). 2.3. Augmented NMF for parameter estimation
Non-negativity of the parameters is widely used in music transcription ( Carabias-Orti et al., 2011 ; Bertin et al., 2010 ; Vincent et al., 2010 ) and source separation ( Ozerov et al., 2011 ;
Virtanen and Klapuri, 2006 ). Under the non-negativity restriction, the parameters can be estimated by minimizing the reconstruction error between the observed x  X  f ; t  X  and the modeled ^ x
 X  spectrogram.

To obtain the amplitudes that minimize the cost function, Lee and Seung (2000) proposed an iterative algorithm based on multi-plicative update (MU) rules. Under these rules, D  X  x  X  f shown to be non-increasing at each iteration while ensuring the non-negativity of the bases and the gains. These MU rules are obtained applying diagonal rescaling to the step size of the gradient descent algorithm (see Lee and Seung, 2000 for further details). The
MU rule for each scalar parameter  X  l is given by expressing the two positive terms  X   X   X   X   X   X  l  X 
The main advantage of the MU in Eq. (6) is that this rule ensures the non-negativity of the bases and the gains, which results in an augmented non-negative matrix factorization (NMF) algorithm. For the harmonically constrained model of Eq. (3) , the
MU that minimize the Beta-divergence for the amplitudes of the model are computed from F X votte and Idier (2011) a  X  m  X  a n  X  m  X  Furthermore, when using the regularized penalty term in
MU ( Gemmeke et al., 2011 ): g  X  t  X   X  g n  X  t  X 
 X  where  X  is the regularization term. The sparsity constraint is not imposed for  X   X  0. 3. Proposed monophonic constrained signal method
When dealing with the monophonic signals, the sparseness should be enforced in such a way that only one gain is active at each frame. Here, we propose to compute, as solution of the method at frame t , the gain g n opt ; t and basis b n opt the Beta-divergence supposing that the other gains at the same frame are zero. The signal model is then de fi ned as n
 X  mum note n opt at frame t .

The idea of constraining the gains to a single nonzero entry at each frame has previously been proposed for statistical models ( Ozerov et al., 2011 ). In a statistical framework, this type of restriction is introduced by means of GSMM ( Benaroya et al., 2006 ) or FS-HMM ( Ozerov et al., 2009 ) under Gaussianity assump-tions and using Itakura Saito (IS) divergence. However, in signal models that minimize Beta-divergence, to the best of our knowl-edge, such a model has not been proposed.
 a live scenario, some reverberation is present. However, the cost for the note produced by the direct signal is commonly smaller than the cost of the reverberation echoes of past notes, which makes Eq. (9) valid for live settings. 3.1. Parameter estimation for monophonic music transcription training data are available, it is advantageous to learn the instrument-dependent bases in advance and fi x them during the analysis of the signals. In fact, this approach has been shown to perform well when the conditions of the music scene do not differ too much between the training and the test data. Here, we have used an approach similar to Carabias-Orti et al. (2011) . Speci the amplitudes of each note of a musical instrument a n  X  learned with databases of isolated sounds during the training stage. 3.1.1. Training stage use a pitch range where each f 0  X  n  X  corresponds to a single note in the semitone scale. Because the pitches in the training data are known in advance, the gains in the training stage are initialized so that the gain representing the pitch that is present in a signal frame is set to unity whereas the rest of the gains are set to zero.
Gains initialized to zero will remain zero, and therefore, the frame is represented by the correct pitch. Using this initialization, the application of the sparsity constraint is unnecessary in the training stage. The training procedure is summarized in Algorithm 1 .
Algorithm 1. Training Algorithm Description. 3.1.2. Evaluation stage transcribed, the amplitudes for all notes and harmonics a estimated for the active instrument in the training stage are used. We do not change these amplitudes during the evaluation stage.
Eq. (9) , the gains can be computed directly from the data and the trained amplitudes without the need for an iterative algorithm. The optimum nonzero gain at each frame g n opt ; t minimizes the Beta-divergence of the single-pitch constrained signal model.
To obtain the optimum note at each frame, we must fi rst compute the distortion obtained for each note at each frame and then select the note that achieves the minimum distortion as the optimum note at each frame.

D  X   X  x t  X  f  X j g n ; t b n  X  f  X  X  X   X 
The value of the gain for note n and frame t is obtained by minimizing Eq. (10) . This minimization has a unique nonzero solution due to the scalar nature of the gain for note n and frame t . g
Finally, the optimum note at each frame is selected as the note that minimizes the Beta-divergence at each frame n  X  t
 X  X  arg min
The proposed solution is valid for  X   X   X  0 ; 2 . 4. Application for polyphonic music transcription
In the state-of-the-art of music transcription, relatively suc-cessful methods have been proposed for monophonic signals.
However, the transcription of polyphonic signals is a more complicated task due to the presence of multiples notes and, possibly, instruments at a time.

In the case of polyphonic music, the signal model can be de fi ned as ^ x  X  f ; t
 X  X   X  J where n  X  1 ; ... ; N and j  X  1 ; ... ; J are the note and instrument indices, respectively. The signal model now includes different basis functions and gains, b n ; j  X  f  X  and g n ; j  X  t  X  instrument.

Although there are several methods in the literature that perform the decomposition, NMF has shown to be the most suitable in the case of music transcription ( Smaragdis and most of the state-of-the-art NMF methods used iterative algo-rithms, such as the gradient descent or the Expectation Maximiza-tion (EM) among others. There are several works devoted to increasing the convergence speed of these algorithms for several signal processing applications ( Zhou et al., 2012 ; Wang and Li, 2010 ; Kim and Park, 2008 ), but the iterative nature of NMF makes it unsuitable for realtime applications. Cont (2006) proposed a method for realtime multi-F0 estimation using instrument learned patterns, sparsity constraint and a gradient descend algorithm optimized to converge using a reduced number of iterations, however, as explained in Section 5.4 , results are poor in compar-ison with our proposal. 4.1. Pseudoinverse method for the gain estimation of polyphonic signals
As in the MBHC case, let us consider that the basis functions b  X  f
 X  for each instrument j are learned in advance and then held fi xed. In this method, information about the instruments being played is required to select the appropriate basis functions.
In the case of  X   X  2 (Euclidean distance), Eq. (4) can be expressed in matrix notation as
D  X   X  X  X  B G  X  2  X  14  X  where X is the signal input matrix in time and frequency, B the basis matrix and G the gain matrix.

Then, we can examine this factorization as a reduced-rank basis decomposition so that X  X  B G , and subsequently, the gains can be estimated in just one step
G  X 
A X  X  15  X  where A  X  R  X  0 ; NxF  X  B  X  and the  X  operator is the Moore matrix inverse.

This method is analogous to the classical Euclidean-NMF using the gradient descent algorithm, but this method allows for the application to realtime problems. As a side effect, the Euclidean distance relies more heavily on the largest coef fi cients, and less precision is to be expected in the estimation of the low-power components than using the smaller values of  X  ( F X votte et al., 2009 ), which leads to worse spectrogram modeling and poor transcription results, as seen in the experimental section.
To improve the Pseudoinverse method performance, we pro-pose to constrain the method to be sparse. To this end, a MBHC-based candidate selection stage will be applied restricting the method to have only a few gains that are not 0 for each instrument at each frame. 4.2. Candidates selection stage In this work, we propose to obtain a list of candidates using the MBHC method from Section 3 . Although the method is designed for monophonic signals, it is adapted to polyphonic signals by assuming that only one instrument is being played. The distortion caused by this monophonic solution is then computed using Eqs. (10) and (11) . The C notes that cause the lower distortion rate are the selected as candidates for instrument j . Because the MBHC method has very low computational cost, the selection of candidates can be performed in real time.
 The key here is to determine the optimal number of candidates C that reduce the computational cost while not being so restrictive such that the correct notes are lost. The performance of the candidate selector has been tested using the Bach Chorals database ( Duan and Pardo, 2011 ) to determine the number of candidates per instrument. The results are shown in Table 1 . Fifteen candi-dates per instrument are needed to maintain an accuracy of at least 5% in selecting the correct note from the note candidates. 4.3. Proposed realtime method for polyphonic signals The proposed algorithm is composed using two steps. First, the MBHC-based candidate selection method is applied. Then, sparsity is enforced to the Pseudoinverse method by restricting the nonzero gains to only the selected candidates. Similar to the Pseudoinverse, the proposed method is able to obtain a realtime factorization while considerably improving the transcription results, as we will see in the evaluation results. This method will be referred to as Pseudoinverse-CS for the remainder of the paper.
Algorithm 2 describes the computational procedure for the proposed Pseudoinverse-CS method.
 Algorithm 2. Proposed Pseudoinverse-CS method. 1 Initialize b n ; j  X  f  X  to the trained instrument models 2 for t  X  1 to number of frames do 3 j  X  1to J do 4 Compute MBHC with Eqs. (11) and (10) 5 Select the C notes that causes the lowest Beta-6 end for 7 Generate basis dictionary B with the candidate basis 8 Estimate the candidate gains using Eq. (15) . The rest of 9 end for 5. Evaluation
To evaluate the performance of our method, we propose two different scenarios. First, we apply the proposed algorithm in
Section 3 to a monophonic music transcription task. Then, we evaluate the two-stage method proposed in Section 4 for polyphonic music transcription, which is a more complicated task because the signals from individual notes overlap in time and in frequency. A comparison with other signal decomposition and state-of-the-art music transcription methods is performed, which shows the robustness and reliability of the proposed method. 5.1. Training and testing data
For the training stage, the full pitch range of isolated notes for each instrument from RWC musical instrument sound database ( Goto et al., 2002 ; Goto, 2004 ) is used. Seven instruments are considered for the experiments (violin, clarinet, tenor saxophone, bassoon, fl ute, horn and oboe). For each instrument, individual sounds are available at semitone intervals over the entire range of notes that could be produced by that instrument. The audio signals are monaural with 44.1 kHz sampling frequency and 16 bits per sample. From the RWC database we select the fi les with normal playing style and mezzo dynamic level. Training with different con fi gurations will lead to different models. However, as demon-good generalization.

To evaluate the monophonic music transcription application, we have used the database proposed in Duan and Pardo (2011) , which consists of 10 human played Bach four-part chorales.
The audio fi les are 30 s of monaural real music performances sampled at 44.1 kHz with 16 bits per sample. Each piece is performed by a quartet of instruments: violin, clarinet, tenor saxophone and bassoon played in an isolated way. Therefore, there are 40 audio fi les (10 from each instrument) with a total of 1200 s of testing material. Aligned MIDI data for each audio also provided.

Finally, for the polyphonic scenario, we use the same poly-phonic woodwind database as in Vincent et al. (2010) . The signals are generated by mixing the recordings of the individual human-played instrument excerpts from the woodwind database for the
MIREX 1 Multiple Fundamental Frequency Estimation &amp; Tracking task. This subset is composed of 5 solo instruments (bassoon, clarinet, fl ute, horn and oboe) each having 54 s. The mixing is performed using only the fi rst 30 s of the monophonic woodwind excerpts, as in Vincent et al. (2010) . In this way, the polyphony ranges from 2 to 5. In total, 26 monaural polyphonic signals are created with a sampling rate of 44.1 kHz and 16 bits per sample. 5.2. Experimental setup (a) Time  X  frequency representation: Transcription applications (b) Amplitude-based pitch salience measure: Given the time-(c) Evaluation metrics: There are two groups of metrics used to 5.3. Results for the monophonic signals from the database proposed in Duan and Pardo (2011) are evaluated. The method in Section 3 (MBCH) is compared to the following algorithms: 1. Deterministic NMF-based approaches with Beta-divergence: 2. Statistical signal decomposition approaches:
As mentioned in the introduction, the last two models are restricted to have a single nonzero entry for each frame.
Although FASST was originally designed for sound source separation, we have adapted it to address automatic music transcription. Different FASST con fi gurations have been tested, but they are omitted here to keep the presentation compact.
The best performance has been obtained using the QERB time frequency representation and computing the decompositions with the Generalized Expectation Maximization (GEM) ( Dempster et al. ) algorithm where the generative model has been modi fi ed to be Poisson distributed (originally FASST utilizes
Gaussian distribution with IS divergence). Using this distribution is equivalent to performing the factorization using the Kullback
Leibler divergence (  X   X  1) ( Virtanen et al., 2008 ). The number of bases K has been fi xed at 104 (i.e., notes from 24 to 127 in the semitone scale) and is independent of the modeled instrument.
Finally, the transcription procedure is exactly the same as the method proposed in Section 3.1 .Speci fi cally, we learn the parameters fi rst using training data and then the factorization is performed using the stored characteristic spectral patterns for the active instrument of each excerpt. 3. Other state-of-the-art monophonic transcription systems: (6) Autocorrelation Function (ACF) and the (7) YIN algorithm ( de Cheveigne and Kawahara, 2002 ). Although these methods were designed for single-F0 estimation, they are adapted to monophonic music transcription. To obtain a transcriptor, the energy of the signal every 10 ms is used to determine the pitch presence by means of Eq. (16) . Additionally, in the case of the
YIN algorithm no pitch is considered for those frames with an aperiodic energy greater than 0.15 from the total energy.
It must be stressed that F0 estimation algorithms can be applied to tasks other than music transcription because they obtain better resolution in frequency than the proposed algo-rithm for monophonic transcription that has a resolution of a semitone. As an advantage, F0 estimation methods do not require a training stage or information about the instruments that is being played in the signal to complete analysis. To provide a fair comparison with the non-NMF state-of-the-art methods, the signal power of the periodic frames is used to compute the pitch estimation using the same optimal detection threshold T (dB) as in the NMF method case.

All the signal decomposition based algorithms provide amplitude-based pitch salience measures, which are then inter-polated over a 10-ms grid and used to compute the pitch estima-tion as explained in Section 4.2.b.

For the methods in Section 2 (BHC and BHC-SC) and the proposed one (MBHC), the optimal parameter values of the divergence  X  and the threshold T for each algorithm are not
Instead, we analyze the different performances of the studied algorithms as a function of the parameter value, which allows us to obtain a deeper understanding of the effects caused by the variation of each parameter. As proposed in Bertin et al. (2010) ,
Vincent et al. (2010) , the distortion measure  X  was varied between 0 and 2 in 0.1 steps and the detection threshold T over the range between  X  40 and 0 dB in 1 dB steps. Moreover, the use of the optimal threshold value is extended to all the compared methods to allow a fair comparison.

The measures presented here are averaged between all the fi and are presented separately for each tested method and training databases used. Moreover, as proposed in Carabias-Orti et al. (2011) , the NMF models with free parameters are randomly initialized, and the average accuracy (Acc) for each transcribed fi le is computed after a set of 30 executions. In our experiments, the 95% con fi dence intervals for the average accuracy (Acc) were less than 1.6% for all the algorithms, which means that the differences between most of the algorithms are statistically signi fi cant.
 Numerical results of the Acc transcription are displayed in Table 2 for optimum con fi guration values. As observed, the proposed MBHC achieves the best results within the compared methods. Regarding the studied NMF methods in Section 2 , we can see how the proposed MBHC method clearly outperforms the BHC and the BHC-SC methods. Therefore, enforcing the method to have a single-nonzero entry appears to be very suitable when dealing with monophonic signals, and these results are independent of the instrument that is being played. The use of the sparsity constraint improves the results especially in the case of the bassoon (more than 10%). The reason for this outperformance is due to, in the case of the bassoon, the amplitude variations over the time line of the note provoke a mismatch with the window transform in certain time frequency locations (i.e., blurred regions in the spectrogram). Therefore, the sparsity constraints appears to minimize this effect because only a few nonzero gain values are allowed.

Regarding the results obtained for the other signal decomposi-tion methods, it is observed that the BNMF, GSMM and FS-HMM are clearly outperformed (approximately 8%). A reason for this underperformance is because these methods are computed using the KL divergence while BHC, BHC-SC and MBHC used the optimum value of  X  from the Beta-divergence. Moreover, the number of parameters to be estimated in the case of the BHC, BHC-SC and MBHC models is considerably less because the basis functions are de fi ned by a vector of m amplitudes, while in the case of the BNMF, GSMM and FS-HMM models, all the amplitudes in the frequency range are considered.

As observed, the proposed MBHC algorithm always results in better results compared with the analyzed state-of-the-art mono-phonic transcriptors. Additionally, the unsupervised ACF and the YIN algorithms obtain very competitive results and slightly out-perform other supervised signal decomposition methods except for our proposed method and the BHC-SC method.
 The performance variation as a function of  X  and T is shown in Fig. 1 . Regarding the evolution of the accuracy in the  X  it can be seen how the best results are obtained in the range between 0.8 and 1.2 for the methods described in Section 2 (BHC and BHC-SC) and between 0.8 and 1.6 for the proposed MBHC method. Although lower values of  X  have been proven to be very suitable in many signal processing application ( Bertin et al., 2010 ; F X votte et al., 2009 ), the performance here is very poor. The reason for this underperformance is due to the harmonicity constraint imposed on these methods. Actually, the musical instruments are not perfectly harmonic and they generate low energy values beyond the boundaries that cannot be modeled in the basis functions. The same problem occurs for the background noise present in the signal. These differences do not affect the greater values of  X  because they more heavily rely on the largest primarily affect the divergence because both the highest and lowest coef fi cients are weighted in a similar way (scale invariance in the case of  X   X  0).

Regarding the results of the function of the detection threshold T ,it isveryinterestingtoseethattheproposedMBHCmethodprovides very suitable results even with very low threshold values. Actually, inthecaseoftheBHCandtheBHC-SCmethods,greatervaluesof T (around  X  13 dB) are required to obtain the best performance. Fig. 2 illustrates a comparison of the performance between the
BHC, the BHC-SC and the proposed MBHC methods when dealing with a monophonic signal of a bassoon. As observed, the proposed method ( Fig. 2 (h)) results in a more reliable representation of the ground truth transcription than the BHC and BHC-SC methods ( Fig. 2 (d) and (f)). Moreover, in this example we can see how the estimation of the BHC method is clearly improved with the introduction of the sparseness constraint, which we have pre-viously addressed. Performance of the methods of the function of the value of parameter T can be understood regarding the estimated time varying gains ( Fig. 2 (c), (e) and (g) for the BHC, the BHC-SC and the MBHC methods, respectively). Actually, the differences between the gains and the optimum transcription of the proposed MBHC method is almost insigni fi cant, which rein-forces the idea that our method performance is more robustly independent of the value of T than the BHC and BHC-SC methods. In Table 3 , the runtimes of the studied methods are shown.
All the experiments have been performed with Matlab running on a 2.66 GHz Intel Core 2 Quad processor. Conversely, both the BHC and the BHC-SC methods are based on the NMF and use the gradient descent iterative algorithm to perform the factorization.
This algorithm is repeated until convergence or the maximum number of iterations reached. Conversely, the proposed MBHC method is based on non-negative sparse coding and allows direct computation of the factorization, which performs 10 times faster than BHC and BHC-SC methods.
 Regarding the other analyzed signal decomposition methods,
BNMF with the GEM algorithm obtains a shorter runtime com-pared with the BHC and the BHC-SC methods, but it is far from the MBHC method due to the iterative process. Otherwise, GSMM and
FS-HMM result in the fastest runtimes compared with the ana-lyzed methods. These methods are discrete state-based models that estimate the best state sequence at each iteration, which requires a long computational time.

Temporal continuity is an interesting constraint that has demonstrated to reduce the number of note insertion errors caused by interferences in the gains that are produced by other onsetting notes, which results in better transcription results ( Carabias-Orti et al., 2011 ; Bertin et al., 2010 ; Virtanen et al., 2008 ). In fact, the FS-HMM algorithm is implicitly de fi decisions using the information from several frames.
 are less suitable when dealing with realtime applications (i.e., interactive music tracking) that require very low latency. However, the proposed MBCH method analyzes each frame independently, which together with the simplicity and ef fi ciency of the direct algorithm makes our method more adequate for realtime applications.
 scriptors, we can see how the ACF method is the fastest and obtains almost instantaneous transcription. Conversely, MBHC is two times faster than the YIN, although it is also realtime. 5.4. Results on polyphonic signals second column indicates which methods require information about the instruments played in the scene in advance. Note that some of the evaluated methods are trained or adjust their para-meters using datasets of musical instruments. For example, the method in Reis et al. (2012) used an  X  internal database  X  samples which are adapted online while the noise is dynamically estimated. Also, Benetos and Dixon (2012) used a dictionary of spectral templates extracted for various instruments (using the whole note range for each one). Finally, in the case of Zhou and
Reiss (2008) , parameters are  X  tuned  X  using a piano and guitar dataset. However, none of these algorithms require information about the played instruments for the testing procedure. In the case of BHC, BHC.SC, Pseudoinverse and Pseudoinverse-CS, parameter j from Eq. (13) is de fi ned according to the number of instruments. music transcription (Pseudoinverse-CS), we have tested this method with same polyphonic woodwind database as in Vincent et al. (2010) , which is also used to evaluate the MultiF0 Estimation and Tracking methods in MIREX, but we randomly cut and mix the different sources. Moreover, we have compared our proposal with other polyphonic transcription methods. In fact, some of these methods perform Multipitch Estimation, which is a much more complex task than pitched transcription, but these methods have been adapted to the latter scenario.
 The list of methods 2 compared in Table 4 is detailed below:
Non-realtime approaches: NMF based method including (1) BHC, (2) BHC-SC methods explained in Section 2 and (3) NMF with the spectral basis vectors modeled as a weighted sum of smoothed narrowband spectra harmonically constrained ( Vincent et al., 2010 ). Other approaches including the well-known state-of-the-art multiF0 estimators and music transcription systems such as (4) the Correlogram-based algorithm in Tolonen and Karjalainen (2000) , (5) the event modeling algorithm in Ryyn X nen and
Klapuri (2005) which won the 2007 MIREX MultiF0 Estimation and Tracking task, (6) the Harmonic sum algorithm ( Klapuri, 2006 ) and (7) the Spectral peak clustering algorithm in Pertusa (2008) . Moreover some of the latest algorithms from the litera-ture has been evaluated such as (8) the Genetic algorithm based approach in Reis et al. (2012) and the two more accurate algorithm from the 2012 MIREX MultiF0 Estimation and Tracking task (9) ( Benetos and Dixon, 2012 )and(10)( Dressler, 2012 ).
Realtime approaches: (11) Pseudoinverse and (12) Pseudo-inverse-CS methods explained in Section 4 .(13)ANMFbased realtime Multi-F0 approach presented in Cont (2006) ,whichis optimized for realtime using learned instrument spectral pat-terns, sparsity constraints and gradient descent updates instead of the original NMF multiplicative updates. (14) A realtime music transcription system presented in Zhou and Reiss (2008) that combines an onset detector and a MultiF0 estimator and analyzes the input signal using the Resonator Time  X  Frequency Image (RTFI).

As observed, the proposed Pseudoinverse-CS method is the best realtime method among the compared methods and only 2% worse in terms of accuracy than the most reliable of the studied non-realtime methods.

The Pseudoinverse method performs poorly mainly because it is based on the Euclidean distance, which relies more heavily on the largest coef fi cients of the trained bases and leads to a less precise spectrogram representation than using smaller values of ( F X votte et al., 2009 ). Enforcing sparsity in the Pseudoinverse method using the MBHC-based candidate selection stage appears to solve this problem because the energy is distributed only among a few notes.

In the case of the BHC and the BHC-SC, their performance is mainly due to the use of different values of  X  , which allows a better spectrogram modeling leading to more reliable results, as explained in Section 5.3 .

Comparing with the referenced methods, the best results are obtained by Dressler (2012) which indeed was the most reliable method in the last year MIREX MultiF0 Estimation and Tracking task. Vincent et al. (2010) performed very similar than the best method. This method is very interesting because it is the only NMF based compared methods in which both, training and testing stages are performed online which very convenient when no training data is available. Unfortunately, it is an iterative method so it is not suitable for realtime applications. Other non-realtime approaches such as Klapuri (2006) , Ryyn X nen and Klapuri (2005) , and Benetos and Dixon (2012) also obtain reliable results. In fact, none of these referenced methods are tested using timbral information, that is, they can be considered as  X  generic tion algorithms. Despite the method in Reis et al. (2012) can be tested without information about the played instruments in the scene, it is designed for piano and therefore, worse results are expected with other instruments.

Regarding the analyzed state-of-the-art realtime methods, on the one hand, the method in Cont (2006) obtains poor results in comparison with our proposal. Note that this method requires a compromise between speed of computation and precision. In fact, reducing the number of iterations drastically to meet realtime usually leads to local minima and consequently, worse transcrip-tion results. On the other hand, the method proposed by Zhou and
Reiss (2008) obtains the worst results of the compared realtime methods. One reason for the poor performance could be that the method parameters were tuned using a piano and guitar dataset, while the analyze dataset is composed by different instruments.
This behavior can also be observed by analyzing the non-piano individual results obtained in the 2008 MIREX MultiF0 Estimation and Tracking task.
 respect to the function of the polyphonic level is shown in Fig. 3 .
As observed, the method is realtime although the runtime increases with respect to the function of the polyphony. Note that all the experiments have been performed using Matlab, so quicker runtimes are expected by optimizing the code using the mex function. Therefore, our proposal has demonstrated to be the most suitable of the analyzed methods when training data is available but the transcription process has to be performed in realtime. 6. Conclusions to address a monophonic constraint is proposed. Moreover, to demonstrate the usability of this method for other tasks, we propose to use it as a candidate selection technique within a polyphonic transcription system. Harmonicity and single-nonzero gain constraints are enforced in a deterministic manner. The method is trained using a database composed of isolated notes for each instrument of the RWC database. Moreover, the proposed method has been tested in two scenarios: monophonic and polyphonic music transcription. In the fi rst scenario, the method is evaluated using a monophonic database composed of 40 fi bassoon, clarinet, tenor saxophone and violin and is compared with other signal decomposition and state-of-the-art monophonic transcription methods obtaining the most reliable results in terms of accuracy and runtime.
 as a candidate selection technique in combination with a realtime signal decomposition-based method to address polyphonic signals. In fact, we have evaluated our method with a polyphonic dataset composed by mixtures from the woodwind quintet by
Beethoven from the MIREX database. The results obtained demon-strate the usability and the reliability of the proposed method as being a suitable alternative to address realtime problems.
In the future, we aim to extend our method to address sound source separation. Moreover, we aim to adapt the parameters to the concrete music scene. A possible method to achieve a reliable adaptation is to provide a proper initialization of the gains, and an additional method is to introduce some constraints to adapt the parameters only when we have a high level of assurance that a note is active and played in an isolated way.
 Acknowledgments
The authors would like to thank the anonymous reviewers whose comments greatly helped improve the original manuscript as well as Z. Duan for kindly sharing his annotated real-world music database.
 References
