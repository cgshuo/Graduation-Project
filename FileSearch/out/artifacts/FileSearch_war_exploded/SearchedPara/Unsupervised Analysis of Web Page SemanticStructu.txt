 Minoru Yoshida 1 , Kazuyuki Matsumoto 1 ,KenjiKita 1 , and Hiroshi Nakagawa 2 In this study, we propose a new model for HTML documents that can extract document structures from them. Document structures are hierarchical structures of documents that decompose documents in to smaller parts recursively. For ex-ample, scientific papers typically consist of several sections ,eachofwhichcan be decomposed into subsections . In addition, titles , abstracts ,andsoon,are included in the document structure.

Web document analysis is a challenge to extract such document structures from HTML documents . Web documents can be decomposed into subdocuments , typically with their headings representing titles of ea ch subdocuments. Figure 1 shows an example of subdocuments found in the web page shown in Figure 2, where each subdocument is presented as a heading. For example, in this document,  X  X ge: 25 X  is a subdocument with the heading  X  X ge X  and content  X 25 X . Our purpose is to extract such lists of subdocuments such that those in the same list have parallel relations (such as  X  X EL:... X  and  X  X AX:... X  in Figure 1.) Note that there are  X  X ested X  lists  X  the element starting with  X  X ontact: X  contains another list  X  X EL:... X  and  X  X AX:... X .

We assume generative models for documents. The most basic way to col-lect parallel subdocuments is to use clustering algorithms such as K-means. The model we propose in this study is somewhat similar; however, it uses a hier-archical Bayesian framework to not only model similarities of visual effects, but also model  X  hierarchical structures by using dep endency trees as constraints to prior  X  simultaneously model local tag similarity and general visual effect usage, by Research on extracting repeated patterns from Web documents has a long his-tory. The most popular approach is to make DOM trees and find frequency patterns on them[1, 2]. The problem with this approach is that it does not work for repeated patterns indicated by non-DOM patterns including patterns with symbols as shown in Figure 2.

Several studies have addressed the problem of extracting logical structures from general HTML pages without labeled training examples . One of these stud-ies used domain-specific knowledge to extract information used to organize log-ical structures [3]. However, the approach in these studies cannot be applied to domains without any knowledge. Another study employed algorithms to de-tect repeated patterns in a list of HTML tags and texts [4, 5] or more struc-tured forms [6 X 8] such as DOM trees. This approach might be useful for certain types of Web documents, particularly those with highly regular formats such as www.yahoo.com and www.amazon.com . However, there are also many cases in which HTML tag usage does not have significant regularity, or the HTML tag patterns do not reflect semantic structures (whereas symbol patterns do.) Therefore, this type of algorithm may be inadequate for the task of heading ex-traction from arbitrary Web documents. Nguyen [9] proposed a method for web document analysis using supervised machine learning. However, our proposal is to use probabilistic modeling for Web documents to obtain their structures in an unsupervised manner.

Some studies on extracting titles or headlines have been reported in [10, 11]. Our task differs from these, in that their methods focus only on titles (and headlines) and ignore the other parts of Web documents, while our al-gorithm handles all parts of the Web documents and provides a tree structure of the entire document; this algorithm enables the system to extract various types of heading other than titles and headlines, such as attributes. In particular, our approach has the advantage that it can handle symbols as well as HTML tags, making the system applicable to many private (less formal) Web documents. Cai et al. [12] proposed VIPS that can e xtract content structure without DOM trees. Their algorithm depends on several heuristic rules for visual repre-sentation of each block in Web documents. However, we propose unsupervised analysis based on a Bayesian probabilistic model for Web document structures. This has several advantages, including easy adaptation to some specific docu-ment layouts, easiness of tuning its parameters (because we only have to change hyperparameters), and ability of obtaining probabilities for words and symbols that may be used for other type of documents such as text files.

Weninger et al.[13] compared several list extraction algorithm. One of the contributions of this study is that we propose a model for nested structures of lists, which has not been tried in most previous studies. 3.1 Problem Setting and Terms Definition We model an HTML document as a list of rows .Eachrow r i is a list of blocks , i.e, r i = &lt;b i 1 , ..., b i | blocks in the row. A block b j is a pair ( w j , s j )ofthe representative word and symbol list s = &lt;s j 1 , ..., s j | s
Because our experiments currently use Japanese documents, which do not contain word breaking symbols, it is not a trivial task to extract the representa-tive word for each document. In our current system, we extract the predefined length of suffix from each block 1 and call them representative words .Wedid not use word breaker tools, partly because they are not for short strings such as those that frequently appear in Web documents, partly because we do not need human-comprehensive features (because our purpose is not information ex-traction but document structure recognition), and partly because simple suffix extraction rules contribute to stability of extraction results.

We assume several hidden variables asso ciated with the above observed vari-ables. First, each block b j is labeled with label l j , which can have one of two values { H,C } . Here, H means heading and C means contents . Headings are ti-tles of subdocuments, and we assume that the heading is presented in the first few blocks of the subdocument, followed by other blocks that we call content blocks. (See Figure 2.) Blocks labeled with H are called heading blocks ,and blocks labeled with C are called content blocks . Heading rows are the rows that contain one or more heading blocks, while content rows are the rows that con-tain no heading blocks. In addition a hidden variable p k is associated with each symbol s k . It indicates whether the symbol is a linefeed-related one, used in the Gibbs sampling step described later.

Next, we assume the dependency structures in documents. Here a dependency relation between two rows means that one row is modified by another. In Figure 2, the row  X  X ge: 25 X  ( depending row ) depends on the row  X  X ohn Smith X  ( de-pended row ). We assume that a pair of hidden variables ( dep i ,bound i )forthe i -th row depends on. Note that the structure is augmented with an additional variable bound i , which denotes the position of the boundary between heading blocks and content blocks in the i -th row. If bound i = 0, it means that there is no heading block in the row (and dep i =  X  1 in this case), and if bound i = | r i | , it means that there is no content block in the line. The sisters of row r i de-note the list of all the rows r i 1 , r i 2 , ... depending on the same row as r i (i.e., dep ( r i 1 )= dep ( r i 2 )= ... = dep ( i )).
 Dependency Structures. Our definition of dependency structures in this study is slightly different from that used in natural language processing communities. One main difference is that it allows isolated rows that do not depend on any other rows. We consider that isolated rows link to a special row called the null row , indicated by the id number  X  1. We consider that two dependency structures are different if at least one row has differen t links. Note that we prohibit crossings in the dependency structures, and their probability is set to 0 (See Figure 3). Dependency structures with no crossing links are called possible dependency structures . We define the probability of generating the whole document as P ( d, T )= P in document d .

Our idea is to divide the process of generating blocks into vertical and horizon-tal generation processes. The former generates rows under the constraints of row dependency structures. Currently, the probability of row dependency structures is defined as uniform distribution among all possible dependency structures. Af-ter each row is generated, all blocks in the row are generated horizontally with probabilities induced by CFG rules. Dividing the generation process in this way reduces the size of the search space. One of the merits of using CFG for prior probability calculation is that it can naturally model the ratio of headings and contents in each row, regardless of how ma ny blocks are in the line. For example, if we directly model the probability of the value of bound i , different lengths of rows require different models, which mak es the model complicated. Instead, in our model, the ratio can be modeled by generation probabilities of a few of rules.
Figure 4 shows our PCFG rules used in our model. H means headings and C means contents . HL and CL mean heading list and contents list , which generates a list of heading blocks and content blocks, respectively. R means rows ,which consist of one heading list, optionally followed by a content list. Here R_nolink is a nonterminal that indicates content (isolated) rows, and R_haslink is a non-terminal for heading rows. Note that this model prohibits headings from starting inthemiddleofeachrow.

Then, probabilities are calculated on the basis of the resulting CFG tree struc-tures, using the PCFG rules shown in Figure 4.
 Probability for Heading Rows. A heading row needs a probability of p4 or p5 before generating its heading and content lists. However, content rows do not need such probability (because they generate content lists with a probability of 1byrule [1] .) Probability by Heading Blocks. As rule [3] shows, for each heading row, the last heading block needs a probability of p7 , and other heading blocks need a probability of p6 , to be generated. We define heading probability of the row as P Probability by Content Blocks. From rule [4], it is easily shown that each content block needs a probability of 0.5 to be generated. We define content probability of the row as P c =0 . 5 n c ,where n c is the number of content blocks in the document. 4.1 Block Probability The remaining part of the probability is the probability for blocks P block ( D | T ). First, note that each block is labeled H or C , according to the CFG tree in the horizontal generation. Each word in the block is generated from a distribution selected according to this lab el. We assign one of the labels { B,N,E,L } to each symbol in the document using the following rules. Intuitively, B denotes boundary between heading and contents, N denotes non-boundary , E denotes end of subdocuments ,and L denotes line symbols that are used in most of the linefeeds, which are not likely to have any semantic meaning, such as heading-associated tags such as &lt;h1&gt; .  X  If the separator s i k is in the last block of row r i , and the value of p i k is 1,  X  If the separator s i k is in the last block of row r i , the value of p i k is 0, and  X  Otherwise, if bound i = i k , separators in block b i k are labeled B .  X  Otherwise, separators are labeled N .

Based on these labels, P block ( d | T ) is defined as a multinomial distribution of where w H is a list of words labeled H , w C is a list of words labeled C , s B is a list of symbols labeled B , s N is a list of symbols labeled N , s L is a list of symbols labeled E , s L is a list of symbols labeled L ,indocument d .
Our word/symbol generation model is a hierarchical Bayesian model. We as-sume the following generative process for words assigned nonterminal H . 1. Each word w in a document labeled H (i.e, w  X  w H )isdrawnfromthe 2. H d , the heading distribution for document d , is drawn from the Dirichlet 3. The base distribution H base is drawn from the Dirichlet distribution with
Words labeled C , and separators labeled N , E ,or L are distributed in the same manner. Base distributions and concentration parameters for them are
Sampling from B is slightly different, because distributions are generated not for each document, but for each sister . Here sisters are a group of rows that depends on the same row. 1. Each separator s in the class labeled B (i.e., s  X  s B )isdrawnfrom B i : 2. B i , the bound distribution for sisters class i , is drawn from the Dirichlet 3. The base distribution B base is drawn from the Dirichlet distribution with
Parallel subdocuments tend to have similar layouts, and such similarity typ-ically appears in the boundary between headings and contents. We intend to model similar layouts by modeling boundary separators in the same list as that drawn from the same distribution.
 We collapse the distribution for each document drawn from base distributions. For example, assume that w 1 ,w 2 , ..., andw n  X  1 have been drawn from H d . Then, distribution for w n is obtained by integrating out the multinomial distribution H d , which results in the following equation.
 where n w is the number of times w occurs, and n. is the number of all word occurrences, in the list w 1 , ..., w n  X  1 . This equation can be obtained using the one for Pitman-Yor process [14] by assigning the discount parameter to be zero. By using this backoff-smoothing style equation, we can model the locality of the usage of words/separators by the first term, which corresponds to the number of occurrences of w in the same context, and global usage by the second term. 4.2 Sampling of Dependency Relations Gibbs sampling is executed by looking at each row r i and sampling the pair Here d  X  i means the document without current row r i . We calculate the relative probability of the document for all possible values for ( dep i ,bound i ) by taking all possible values for ( dep i ,bound i ), calculating the probability P ( d, T )foreach dependency value, and normalizing them by the sum of all calculated values. 4.3 Sampling of Base Distributions Another important part of our Gibbs sampling is sampling distributions given the document structures, which model the general tendency of usage of words and symbols. We use the fast sampling scheme described in [14] which omit time-consuming  X  X ookkeeping X  operations for sampling base distributions. First, the parameter m , which indicates  X  X ow many times each word w was drawn from the second term of the equation 1, is drawn from the following distribu-unsigned Stirling numbers of the first kind. (Note that the factor k in Teh X  X  representation corresponds to word w in our representation.) After drawing m , the base parameter  X  is drawn from the following Dirichlet distribution: (  X  1 , ...,  X  K ) parameter and the base distribution for the distribution for drawing H , respec-tively.

The following base distributions are sampled by using this scheme: H base is sampled from w H , C base is sampled from w C , N base is sampled from s N , B base is sampled from s N ,and L base is sampled from s L . 5.1 Sentence Row Finder In HTML layout structure detection, sentence blocks are critical performance bottlenecks. For example, it is relatively easy to detect the suffixes of the rows that indicate sentences. However, it is difficult to decide whether the row starts with headings, especially when the sen tences are decorated with HTML tags or symbols. (e.g.,  X  X obby: I like to hear music! X )
Ourideaistouse prefixes to decide whether the row contains headings. We assume that rows starting with sentences contain no headers , and the algorithm finds sentences by using the ratio of obvious sentences in all rows, starting with the prefix. The obvious sentences are det ected by using simple heuristics that  X  X f symbols in the row are only commas and periods, then the row surely consists of only sentences. X  Currently, if the ratio exceeds the threshold value 0 . 3, the row is determined as a sentence. Note that the sentence row finder is also applied to the baseline algorithm described later. 5.2 Hyperparameter Settings We assume a Dirichlet prior for each rule of probability where Dirichlet hyper-parameters are set  X  1 =  X  2=5 . 0 for rules [2.1] and [2.2] ,and(  X  1 , X  2 )= (10 . 0 , 90 . 0) for rules [3.1] and [3.2] heuristically. The latter parameters sug-gest that our observation that the length of heading lists is not so large, giving high-probability values to short heading lists. This sampling of parameters helps to stabilize sampled dependency relations. 5.3 Parallelization Parallelization of the above Gibbs sampling is straightforward because each sam-pling of tuples ( dep, rel, bound ) only uses the state of other tuples in the same document, along with the base distributions such as H base and B base ,whichare not changed in tuple sampling. The task of sampling tuples is therefore divided into several groups as each group consists of one or more whole documents, and the sampling of tuples for each group is executed in parallel. (Sampling base distributions is not easily parallelized.) 5.4 Dependency Structure Estimation Gibbs sampling can be used as a scheme for sampling the latent variables; how-ever, it is not obvious how to extract highly probable states using this sampling scheme. Plausible base distributions can be obtained by taking several samples and averaging them. However, dependenc y structures are so complicated that it is almost impossible to see the same sample of structure two times or more. We thus use the following heuristic steps to obtain highly probable structures.  X  Run the Gibbs sampling for some burn-in period.  X  Take several samples for base distributions, and average them as an estima- X  Initialize the latent categorical variables.  X  Run the Gibbs sampling again, but only for categorical variables for some  X  Take the structures with the maximum marginal likelihood so far.  X  Greedy finalization: for each line, fix the state to the one with the highest Our corpus consists of 1,012 personal web pages found in the Japanese web site @nifty . We randomly selected 50 Web documents from them. We excluded 10 documents that contain &lt;table&gt; tags because table st ructures need special treatment for proper analysis and including them into the corpus harms the reliability of the evaluation. We extracted all repeated subdocuments in the remaining 40 documents manually. Amo ng them, 14 documents contained no repeated subdocuments. For each algor ithm, we extracted each set of sisters from dependency structures and regarde d them as resulting sets of lists. We used purity, inverse purity, and their f-measure for evaluation, which is a popular measure for clustering evaluation. 6.1 Evaluation Measure To evaluate the quality of extracted lists, we use purity and inverse purity measures[15], which are popular for cluster evaluation. We regard each extracted list as a cluster of subdocuments and represent it with the pair ( i, j ), where i is the start position and the j is the end position of the subdocuments. The end position is set just before the start position of the next subdocument in the list. 2 Subdocument extraction is evaluated by comparing this cluster to manually constructed subdocument clusters.

Assume that C i is a cluster in the algorithm results, and L i is a cluster in the manual annotation. Purity is computed by P = i | C i | N max j Precision ( C i ,L j ) as IP = i | L i | N max j Precision ( L i ,C j )where N = i | L i | .
Quality of the output lists is evaluated by the F-measure, which is the har-monic mean of purity and inverse purity: F = 1 1 /P +1 /IP .

WedidnotusedB-cubedevaluationm easures[16] because B-cubed is an element-wise definition, which calculates correctness of all rows in the corpus, in-dicating that we would have to consider rows that have no headings, for which no clusters are generated. B-cubed measur es are developed as a metric that works for soft-clustering, whereas our task can be regarded as hard clustering, in which P-IP measures work well.
 We used micro-averaged and macro-averaged f-measures for cluster evaluation. Macro-averaged f-measure compute f-value for each document that has any re-peated patterns (i.e., 26 documents in the test set) and average all the f-values. However, micro-averaged f-measures regard all 40 documents in the test set as one document, and calculate P, IP, and F on this one large document. Thus, we can evaluate how each method does not extract unnecessary lists from documents with no repeated lists by using a micro-averaged f-measure. 6.2 Baseline We use the baseline algorithm that uses some heuristic rules to extract sub-documents. We test several configurations (e.g., what header tags are used for extraction, whether rows with | r | = 1 are extracted as headings, etc.) and select the one that performed the best on the test set. This baseline algorithm selects heading rows among all rows (except the ones discarded by the sentence row finder) using following heuristic rules.

First, it uses  X  X eader tag heuristics X . For example, if the row is in an &lt;h2&gt; tag, we assume that the row is a heading that modifies the following blocks until &lt;h2&gt; , &lt;h3&gt; ,and &lt;h4&gt; are used in this heuristics. 3
Second, it uses the block number heuristics which showed good performance in our preliminary experiments. Assume that | r | is the number of blocks in the row | r | .If | r | X  2, the algorithms regard r as heading row (we assume that this is not a sentence row, we assume r is bracketed by &lt;h7&gt; , which indicates that it will be the heading of the next rows (if the next row has more than one blocks.)
Note that this simple heuristics can extract many sub-documents in Figure 2 including  X  X ge:25 X  and  X  X EL:+01-234-5678 X . 6.3 Results We run our Gibbs sampling with 1000 initial iterations and 500 final iterations. Values of parameters (  X  B , X  E , X  L , X  N ) were set to (10 , 100 , 100 , 1000) heuristi-cally. We use the uniform distribution for each base distribution. Results were obtained by running Gibbs sampling 5 times and averaging all the averaged f-measure values.

Table 1 shows the results. Our algorithm outperformed the baseline algo-rithm by about 3.3  X  7.6 points. The performance gain of our algorithm in micro-averaged f-measure increased from 3.3 to 5.3 by using 14  X  X o-repeat X  doc-uments. This result suggests that our me thod works well in detecting  X  X o-repeat X  documents to avoid incorrect repeated lists.

Performance gain was mainly obtained by detection of heading blocks that could not be found by the baseline algorithm and detection of content blocks that could not be found by sentence row finder heuristics. However, the performance of our algorithm for documents with heading blocks that were easily detected by the baseline algorithm tended to be lower. We need an algorithm that takes the strength of both our method and the baseline method for better performance. In this study, we proposed a probabilistic model for document structures for HTML documents that uses Bayesian hie rarchical modeling. Our model can si-multaneously manage both local coherence and global tendencies of layout usage, thanks to hierarchical modeling and cache effects obtained by integrating out of distributions. Experimental results sh owed that document structures obtained by our model were better than those obtained by the heuristic baseline method. For future study, we are keen to improve the performance of our method by, for example, using larger data sets to obtain more reliable knowledge about layout usage, or using more sophisticated methods to obtain maximum-likelihood states for our model.

