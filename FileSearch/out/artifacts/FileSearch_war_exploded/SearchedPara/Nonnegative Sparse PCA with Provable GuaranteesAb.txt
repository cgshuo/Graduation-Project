 Megasthenis Asteris MEGAS @ UTEXAS . EDU Dimitris S. Papailiopoulos DIMITRIS @ UTEXAS . EDU Given a data matrix S  X  R n  X  m comprising m zero-mean vectors on n features, the first principal component (PC) is where A = 1 / m  X  SS T is the n  X  n positive semidefinite (PSD) empirical covariance matrix. Subsequent PCs can be computed after A has been appropriately deflated to re-move the first eigenvector. PCA is arguably the workhorse of high dimensional data analysis and achieves dimension-ality reduction by computing the directions of maximum variance. Typically, all n features affect positively or neg-atively these directions resulting in dense PCs, which ex-plain the largest possible data variance, but are often not interpretable.
 It has been shown that enforcing nonnegativity on the com-puted principal components can aid interpretability. This is particularly true in applications where features inter-act only in an additive manner. For instance, in bioinfor-matics, chemical concentrations are nonnegative (Kim &amp; Park, 2007), or the expression level of genes is typically attributed to positive or negative influences of those genes, but not both (Badea &amp; Tilivea, 2005). Here, enforcing non-negativity, in conjunction with sparsity on the computed components can assist the discovery of local patterns in the data. In computer vision, where features may coin-cide with non negatively valued image pixels, nonnegative sparse PCA pertains to the extraction of the most informa-tive image parts (Lee &amp; Seung, 1999). In other applica-tions, nonnegative weights admit a meaningful probabilis-tic interpretation.
 Sparsity emerges as an additional desirable trait of the com-puted components because it further helps interpretabil-ity (Zou et al., 2006; d X  X spremont et al., 2007b), even in-dependently of nonnegativity. From a machine learning perspective, enforcing sparsity serves as an unsupervised feature selection method: the active coordinates in an opti-mal l 0 -norm constrained PC should correspond to the most informative subset of features. Although nonnegativity in-herently promotes sparsity, an explicit sparsity constraint enables precise control on the number of selected features. Nonnegative Sparse PC. Nonnegativity and sparsity can be directly enforced on the principal component optimiza-tion by adding constraints to (1). The k -sparse nonnegative principal component of A is where S n k = { x  X  R n : k x k 2 = 1 , k x k 0  X  k, x  X  0 } , for a desired sparsity parameter k  X  [ n ] .
 The problem of computing the first eigenvector (1) is eas-ily solvable, but with the additional sparsity and nonnega-tivity constraints problem (2) becomes computationally in-tractable. The cardinality constraint alone renders sparse PCA NP-hard (Moghaddam et al., 2006b). Even if the l 0 -norm constraint is dropped, we show that problem (2) re-mains computationally intractable by reducing it to check-ing matrix copositivity, a well known co-NP complete de-cision problem (Murty &amp; Kabadi, 1987; Parrilo, 2000). Therefore, each of the constraints x  X  0 and k x k 0  X  k individually makes the problem intractable.
 Our Contribution: We introduce a novel algorithm for approximating the nonnegative k -sparse principal compo-nent with provable approximation guarantees.
 Given any PSD matrix A  X  R n  X  n , sparsity parameter k , and accuracy parameter d  X  [ n ] , our algorithm outputs a nonnegative, k -sparse, unit norm vector x d that achieves at least  X  d fraction of the maximum objective value in (2), i.e. , where Here,  X  i is the i th largest eigenvalue of A , and the accuracy parameter d specifies the rank of the approximation used and controls the running time. Specifically, our algorithm runs in time O ( n d k d + n d +1 ) . As can be seen our result de-pends on the spectral profile of A : the faster the eigenvalue decay, the tighter the approximation.
 Near-Linear time approximation. Our algorithm has a running time O ( n d k d + n d +1 ) , which in the linear sparsity regime can be as high as O ( n 2 d ) . This can be non-practical for large data sets, even if we set the rank parameter d to be two or three. We present a modification of our algorithm that can provably approximate the result of the first in near-linear time. Specifically, for any desired accuracy  X  (0 , 1] it computes a nonnegative, k -sparse, unit norm vector such that where  X  d is as described in (4). We show that the running time of our approximate algorithm is O  X  d  X  n log n , which is near-linear in n for any fixed accuracy parameters d and .
 Our approximation theorem has several implications. Exact solution for low-rank matrices. Observe that if the matrix A has rank d , our algorithm returns the optimal k -sparse PC for any target sparsity k . The same holds in the case of the rank-d update matrix A =  X  I + C , with rank ( C ) = d and arbitrary constant  X  , since the algorithm can be equivalently applied on C .
 PTAS for any spectral decay. Consider the linear sparsity regime k = c  X  n and assume that the eigenvalues follow a decay law  X  i  X   X  1  X  f ( i ) for any decay function f ( i ) which vanishes: f ( i )  X  0 as i  X  X  X  . Special cases include power law decay f ( i ) = 1 /i  X  or even very slow decay functions like f ( i ) = 1 / log log i . For all these cases, we can solve nonnegative sparse PCA for any desired accuracy in time polynomial in n and k , but not in 1 / . Therefore, we obtain a polynomial-time approximation scheme (PTAS) for any spectral decay behavior.
 Computable upper bounds. In addition to these theoret-ical guarantees, our method yields a data dependent upper bound on the maximum value of (2), that can be computed by running our algorithm. As it can be seen in Fig. 4-6, the obtained upper bound, combined with our achievable point, sandwiches the unknown optimum within a narrow region. Using this upper bound we are able to show that our solutions are within 40  X  90% from the optimal in all the datasets that we examine. To the best of our knowledge, this framework of data dependent bounds has not been con-sidered in the previous literature. 1.1. Related Work There is a substantial volume of work on sparse PCA, spanning a rich variety of approaches: from early heuris-tics in (Jolliffe, 1995), to the LASSO based techniques (Zou et al., 2006), a greedy branch-and-bound technique in (Moghaddam et al., 2006a), or semidefinite programming approaches (d X  X spremont et al., 2008; Zhang et al., 2012; d X  X spremont et al., 2007a). This line of work does not con-sider or enforce nonnegativity constraints.
 When nonnegative components are desired, fundamentally different approaches have been used. Nonnegative matrix factorization (Lee &amp; Seung, 1999) and its sparse variants (Hoyer, 2004; Kim &amp; Park, 2007) fall within that scope: data is expressed as (sparse) nonnegative linear combina-tions of (sparse) nonnegative parts. These approaches are interested in finding a lower dimensionality representation of the data that reveals latent structure and minimizes a re-construction error, but are not explicitly concerned with the statistical significance of individual output vectors. Nonnegativity as an additional constraint on (sparse) PCA first appeared in (Zass &amp; Shashua, 2007). The authors sug-gested a coordinate-descent scheme that jointly computes a set of nonnegative sparse principal components, maxi-mizing the cumulative explained variance. An l 1 -penalty promotes sparsity of computed components on average, but not on each component individually. A second convex penalty is incorporated to favor orthogonal components. Similar convex optimization approaches for nonnegative PCA have been subsequently proposed in the literature. In (Allen &amp; Maleti  X  c-Savati  X  c, 2011) for instance, the authors suggest an alternating maximization scheme for the com-putation of the first nonnegative PC, allowing the incorpo-ration of known structural dependencies.
 A competitive algorithm for nonnegative sparse PCA was established in (Sigg &amp; Buhmann, 2008), with the de-velopment of a framework stemming from Expectation-Maximization (EM) for a probabilistic generative model of PCA. The proposed algorithm, which enforces hard spar-sity, or nonnegativity, or both constraints simultaneously, computes the first approximate PC in O ( n 2 ) , i.e. , time quadratic in the number of features.
 To the best of our knowledge, no prior works provide prov-able approximation guarantees for the nonnegative sparse PCA optimization problem. Further, no data dependent up-per bounds have been present in the previous literature. Differences from SPCA work. Our work is closely related to (Karystinos &amp; Liavas, 2010; Asteris et al., 2011; Papail-iopoulos et al., 2013) that introduced the ideas of solving low-rank quadratic combinatorial optimization problems on low-rank PSD matrices using hyperspectral transforma-tions. Such transformations are called spannograms and follow a similar architecture. In this paper, we extend the spannogram framework to nonnegative sparse PCA. The most important technical issue compared to (Asteris et al., 2011; Papailiopoulos et al., 2013) is introducing nonnega-tivity constraints in spannogram algorithms.
 To understand how this changes the problem, notice that in the original sparse PCA problem without nonnegativity constraints, if the support is known, the optimal principal component supported on that set can be easily found. How-ever, under nonnegativity constraints, the problem is hard even if the optimal support is known. This is the funda-mental technical problem that we address in this paper. We show that if the involved subspace is low-dimensional, it is possible to solve this problem. Given an n  X  n PSD matrix A , the desired sparsity k , and an accuracy parameter d  X  [ n ] , our algorithm computes a nonnegative, k -sparse, unit norm vector x d approximating the nonnegative, k -sparse PC of A . We begin with a high-level description of the main steps of the algorithm. Step 1. Compute A d , the rank-d approximation of A . We compute A d , the best rank-d approximation of A , zeroing Algorithm 1 Spannogram Nonnegative Sparse PCA out the n  X  d trailing eigenvalues of A , that is, where  X  i is the i th largest eigenvalue of A and u i the cor-responding eigenvector.
 Step 2. Compute S d , a set of O ( n d ) candidate supports. Enumerating the n k possible supports for k -sparse vectors in
R n is computationally intractable. Using our Spanno-gram technique described in Section 4, we efficiently de-termine a collection S d of support sets, with cardinality |S d |  X  2 d n +1 d , that provably contains the support of the nonnegative, k -sparse PC of A d .
 Step 3. Compute X d , a set of candidate solutions. For each candidate support set I  X  X  d , we compute a candidate solution x supported only in I : The constant rank of A d is essential in solving (6): the constrained quadratic maximization is in general NP-hard, even for a given support.
 Step 4. Output the best candidate solution in X d , i.e. , the candidate that maximizes the quadratic form.
 If multiple components are desired, the procedure is re-peated after an appropriate deflation has been applied on A d (Mackey, 2008). The steps are formally presented in Algorithm 1. A detailed description is the subject of subse-quent sections. 2.1. Approximation Guarantees Instead of the nonnegative, k -sparse, principal component x ? of A , which attains the optimal value OPT = x ? T Ax ? our algorithm outputs a nonnegative, k -sparse, unit norm vector x d . We measure the quality of x d as a surrogate of x ? by the approximation factor x d T Ax d / OPT . Clearly, the approximation factor takes values in (0 , 1] , with higher values implying tighter approximation.
 Theorem 1. For any n  X  n PSD matrix A , sparsity param-eter k , and accuracy parameter d  X  [ n ] , Alg. 1 outputs a nonnegative, k -sparse, unit norm vector x d such that where in time O ( n d +1 + n d k d ) .
 The approximation guarantee of Theorem 1 relies on estab-lishing connections among the eigenvalues of A , and the quadratic forms x d T Ax d and x d T A d x d . The proof can be found in the supplemental material. The complexity of Algorithm 1 follows upon its detailed description. Our algorithm approximates the nonnegative, k -sparse PC of a PSD matrix A by computing the corresponding PC of A d , a rank-d surrogate of the input argument A : where v i = to the i th largest eigenvalue of A , and V = [ v 1  X  X  X  v R n  X  d . In this section, we delve into the details of our al-gorithmic developments and describe how the low rank of A d unlocks the computation of the desired PC. 3.1. Rank-1 : A simple case We begin with the rank-1 case because, besides its moti-vational simplicity, it is a fundamental component of the algorithmic developments for the rank-d case.
 In the rank-1 case, V reduces to a single vector in R n and x , the nonnegative k -sparse PC of A 1 , is the solution to That is, x 1 is the nonnegative, k -sparse, unit length vector that maximizes ( v T x ) 2 . Let I = supp ( x 1 ) , |I| X  k , be the unknown support of x 1 . Then, ( v T x ) 2 = P i  X  X  v i  X  x Since x 1  X  0 , it should not be hard to see that the active en-tries of x 1 must correspond to nonnegative or nonpositive entries of v , but not a combination of both. In other words, v , the entries of v indexed by I , must satisfy v I  X  0 or v
I  X  0 . In either case, by the Cauchy-Schwarz inequality, Equality in (9) can always be achieved by setting x v / k v I k 2 if v I  X  0 , and x I =  X  v I / k v I k 2 if v The support of the optimal solution x 1 is the set I for which k v
I k 2 2 in (9) is maximized under the restriction that the entries of v I do not have mixed signs.
 Def. 1. Let I + k ( v ) , 1  X  k  X  n denote the set of indices of the (at most) k largest nonnegative entries in v  X  R n . Proposition 3.1. Let x 1 be the solution to problem (8) . Then, supp ( x 1 )  X  X  1 = I + k ( v ) , I + k (  X  v ) . The collection S 1 and the associated candidate vectors via (9) are constructed in O ( n ) . The solution x 1 is the candi-date that maximizes the quadratic. 3.2. Rank-d case In the rank-d case, x d , the nonnegative, k -sparse PC of A is the solution to the following problem: Consider an auxiliary vector c  X  R d , with k c k 2 = 1 . From the Cauchy-Schwarz inequality, Equality in (11) is achieved if and only if c is colinear to V
T x . Since c spans the entire unit sphere, such a c ex-ists for every x , yielding an alternative description for the objective function in (10): where S d = c  X  R d : k c k 2 = 1 is the d -dimensional unit sphere. The maximization in (10) becomes The set of candidate supports. A first key observation is that for fixed c , the product ( Vc ) is a vector in R n . Maxi-mizing | ( Vc ) T x | 2 over all vectors x  X  S n k is a rank-1 in-stance of the optimization problem, as in (8). Let ( c d be the optimal solution of (10). By Proposition 3.1, the sup-port of x d coincides with either I + k ( Vc d ) or I + k Hence, we can safely claim that supp ( x d ) appears in Naively, one might think that S d can contain as many as k distinct support sets. In Section 4, we show that |S d 2 for efficiently constructing S d in O ( n d +1 ) . Each support in S d corresponds to a candidate principal component. Solving for a given support. We seek a pair ( x , c ) that maximizes (13) under the additional constraint that x is supported only on a given set I . By the Cauchy-Schwarz inequality, the objective in (13) satisfies where V I is the matrix formed by the rows of V indexed by I . Equality in (15) is achieved if and only if x I is col-inear to V I c . However, it is not achievable for arbitrary c , as x I must be nonnegative. From Proposition 3.1, we infer that x being supported in I implies that all entries of V have the same sign. Further, whenever the last condition holds, a nonnegative x I colinear to V I c exists and equal-ity in (15) can be achieved. Under the additional constraint that supp ( x ) = I  X  X  d , the maximization in (13) becomes The constraint V I c  X  0 in (16), is equivalent to requiring that all entries in V I c have the same sign, since c and  X  c achieve the same objective value.
 The optimization problem in (16) is NP-hard. In fact, it en-compasses the original nonnegative PCA problem as a spe-cial case. Here, however, the constant dimension d =  X (1) of the unknown variable c permits otherwise intractable op-erations. In Section 5, we outline an O ( k d ) algorithm for solving this constrained quadratic maximization.
 The algorithm. The previous discussion suggests a two-step algorithm for solving the rank-d optimization prob-lem in (10). First, run the Spannogram algorithm to con-struct S d , the collection of O ( n d ) candidate supports for x , in O ( n d +1 ) . For each I  X  S d , solve (16) in O ( k to obtain a candidate solution x ( I ) supported on I . Out-put the candidate solution that maximizes the quadratic x
T A d x . Efficiently combining the previous steps yields an O ( n d +1 + n d k d ) procedure for approximating the non-negative sparse PC, outlined in Alg. 1. In this section, we describe how to construct S d , the collec-tion of candidate supports, defined in (14) as for a given V  X  R n  X  d . S d comprises all support sets in-duced by vectors in the range of V . The Spannogram of V is a visualization of its range, and a valuable tool in effi-ciently collecting those supports.
 4.1. Constructing S 2 We describe the d = 2 case, the simplest nontrivial case, to facilitate a gentle exposure to the Spannogram technique. The core ideas generalize to arbitrary d and a detailed de-scription is provided in the supplemental material. Spherical variables. Up to scaling, all vectors v in the range of V  X  R n  X  2 , R ( V ) , can be written as v = Vc for some c  X  R 2 : k c k = 1 . We introduce a variable  X   X   X  = (  X   X / 2 , X / 2] , and set c to be the following function of  X  : The range of V , R ( V ) = { X  v (  X  ) =  X  Vc (  X  ) , X   X   X  } , is also a function of  X  , and in turn S 2 can be expressed as Spannogram. The i th entry of v (  X  ) is a continuous func-tion of  X  generated by the i th row of V : [ v (  X  )] V i, 1 sin(  X  ) + V i, 2 cos(  X  ) . Fig. 1 depicts the functions cor-responding to the rows of an arbitrary matrix V  X  R 4  X  2 We call this a spannogram , because at each  X  , the values of the curves coincide with the entries of a vector in the range of V . A key observation is that the sorting of the curves at some  X  is locally invariant for most points in  X  . In fact, due to the continuity of the curves, as we move along the  X  -axis, the set I + k ( v (  X  )) can only change at points where a curve intersects with (i) another curve, or (ii) the zero axis; der of two curves is necessary, although not sufficient, for I k ( v (  X  )) to change.
 Appending a zero ( n + 1) th row to V , the two aforemen-tioned conditions can be merged into one: I + k ( v (  X  )) can change only at the points where two of the n + 1 curves in-tersect. Finding the unique intersection point of two curves covering all possible candidate support sets. There are ex-actly n +1 2 such points partitioning  X  into n +1 2 + 1 inter-vals within which the set of largest k nonnegative entries of v (  X  ) and  X  v (  X  ) are invariant.
 Constructing S 2 . The point  X  ij where the i th and j th curves intersect, corresponds to a vector v (  X  ij )  X  R ( V ) whose i th and j th entries are equal. To find it, it suffices to compute a c 6 = 0 such that ( e i  X  e j ) T Vc = 0 , i.e. , a unit norm vector c ij in the one-dimensional nullspace of ( e i  X  e j ) T V . Then, v (  X  ij ) = Vc ij .
 We compute the candidate support I + k ( v (  X  ij )) at the in-tersection. Assuming for simplicity that only the i th and j th curves intersect at  X  ij , the sorting of all curves is un-changed in a small neighborhood of  X  ij , except the i th j th curves whose order changes over  X  ij . If both the i th j th entries of v (  X  ij ) or none of them is included in the k largest nonnegative entries, then the set I + k ( v (  X  )) in the two intervals incident to  X  ij is identical. Otherwise, the i and j th curve occupy the k th and ( k + 1) th order at  X  the change in their relative order implies that one leaves and one joins the set of k largest nonnegative curves at  X  The support sets associated with the two adjacent intervals differ only in one element (one contains index i and the other contains index j instead), while the remaining k  X  1 common indices correspond to the k  X  1 largest curves at the intersection point  X  ij . We include both in S 2 and repeat the above procedure for I + k (  X  v (  X  ij )) .
 Each pairwise intersection is computed in O (1) and the at most 4 associated candidate supports in O ( n ) . In total, the collection S 2 comprises |S 2 | X  4 n +1 2 = O ( n 2 ) candidate supports and can be constructed in O ( n 3 ) .
 The generalized Spannogram algorithm for constructing S d runs in O ( n d +1 ) and is formally presented in Alg. 2. A de-tailed description is provided in the supplemental material. Each support set I in S d yields a candidate nonnegative, k -sparse PC, which can be obtained by solving (16), a quadratic maximization over the intersection of halfspaces and the unit sphere: where Q = V T I V I is a d  X  d matrix and R is a k  X  d ma-trix. Problem ( P d ) is NP-hard: for Q PSD and R = I d  X  d it reduces to the original problem in (2). Here, however, we are interested in the case where the dimension d is a constant. We outline an O ( k d ) algorithm, i.e. , polynomial in the number of linear constraints, for solving ( P detailed proof is available in the supplemental material. Algorithm 2 Spannogram algorithm for constructing S d The objective of ( P d ) is maximized by u 1  X  R d , the lead-ing eigenvector of Q . If u 1 or  X  u 1 is feasible, i.e. , if it satisfies all linear constraints, then c ? =  X  u 1 . It can be shown that if none of  X  u 1 is feasible, at least one of the k linear constraints is active at the optimal solution c ? , that is, there exists 1  X  i  X  k such that R i, : c ? = 0 . Fig. 2 depicts an example for d = 2 . The leading eigen-vector of Q lies outside the feasible region, an arc of the unit-circle in the intersection of k halfspaces. The optimal solution coincides with one of the two endpoints of the fea-sible region, where a linear inequality is active, motivating a simple algorithm for solving ( P 2 ) : ( i ) for each linear in-equality determine a unit length point where the inequality becomes active, and ( ii ) output the point that is feasible and maximizes the objective.
 Back to the general ( P d ) problem, if a linear inequality R i, : c  X  0 for some i  X  [ k ] is enforced with equality, the modified problem can be written as a quadratic maximiza-tion in the form of ( P d ) , with dimension reduced to d  X  1 and k  X  1 linear constraints. This observation suggests a re-cursive algorithm for solving ( P d ) : If  X  u 1 is feasible, it is also the optimal solution. Otherwise, for i = 1 ,...,k , set the i th inequality constraint active, solve recursively, and collect candidate solutions. Finally, output the candidate that maximizes the objective. The O ( k d ) recursive algo-rithm is formally presented in the supplemental material. Alg. 1 approximates the nonnegative, k -sparse PC of a PSD matrix A by solving the nonnegative sparse PCA problem exactly on A d , the best rank-d approximation of A . Albeit polynomial in n , the running time of Alg. 1 can be imprac-tical even for moderate values of n .
 Instead of pursuing the exact solution to the low-rank non-negative sparse PCA problem max x  X  S n compute an approximate solution in near-linear time, with performance arbitrarily close to optimal. The suggested procedure is outlined in Algorithm 3, and a detailed dis-cussion is provided in the supplemental material. Alg. 3 relies on randomly sampling points from the range of A d and efficiently solving rank-1 instances of the nonnegative sparse PCA problem as described in Section 3.1.
 Theorem 2. For any n  X  n PSD matrix A , sparsity param-eter k , and accuracy parameters d  X  [ n ] and  X  (0 , 1] , Alg. 3 outputs a nonnegative, k -sparse, unit norm vector b x d such that with probability at least 1  X  1 / n , in time O  X  d  X  n log n plus the time to compute the d leading eigenvectors of A . We empirically evaluate the performance of our algorithm on various datasets and compare it to the EM algorithm 1 for sparse and nonnegative PCA of (Sigg &amp; Buhmann, 2008) which is known to outperform previous algorithms.
 CBCL Face Dataset. The CBCL face image dataset (Sung, 1996), with 2429 gray scale images of size 19  X  19 pixels, has been used in the performance evaluation of both the NSPCA (Zass &amp; Shashua, 2007) and EM (Sigg &amp; Buh-mann, 2008) algorithms.
 Fig. 3 depicts samples from the dataset, as well as six or-thogonal, nonnegative, k -sparse components ( k = 40 ) suc-cessively computed by ( i) Alg. 3 ( d = 3 , = 0 . 1 ) and Algorithm 3 Approximate Spannogram NSPCA ( -net) ( ii) the EM algorithm. Features active in one component are removed from the dataset prior to computing subse-quent PCs to ensure orthogonality. Fig. 3 reveals the ability of nonnegative sparse PCA to extract significant parts. In Fig. 4, we plot the variance explained by the computed approximate nonnegative, k -sparse PC (normalized by the leading eigenvalue) versus the sparsity parameter k . Alg. 3 for d = 3 and = 0 . 1 , and the EM algorithm exhibit nearly identical performance. For this dataset, we also compute the leading component using the NSPCA algo-rithm of (Zass &amp; Shashua, 2007). Note that NSPCA does not allow for a precise control of the sparsity of its output; an appropriate sparsity penalty  X  was determined via bi-nary search for each target sparsity k . We plot the explained variance only for those values of k for which a k -sparse component was successfully extracted. Finally, note that both the EM and NSPCA algorithms are randomly initial-ized. All depicted values are the best results over multiple random restarts.
 Our theory allows us to obtain provable approximation guarantees: based on Theorem 2 and the output of Alg. 3, we compute a data dependent upper bound on the maxi-mum variance, which provably lies in the shaded area. For instance, for k = 180 , the extracted component explains at least 58% of the variance explained by the true nonneg-ative, k -sparse PC. The quality of the bound depends on the accuracy parameters d and , and the eigenvalue decay of the empirical covariance matrix of the data. There exist datasets on which our algorithm provably achieves 70% or even 90% of the optimal.
 Leukemia Dataset. The Leukemia dataset (Armstrong et al., 2001) contains 72 samples, each consisting of ex-pression values for 12582 probe sets. The dataset was used in the evaluation of (Sigg &amp; Buhmann, 2008). In Fig. 5, we plot the normalized variance explained by the computed nonnegative, k -sparse PC versus the sparsity parameter k . For low values of k , Alg. 3 outperforms the EM algorithm in terms of explained variance. For larger values, the two algorithms exhibit similar performance.
 The approximation guarantees accompanying our algo-rithm allow us to upper bound the optimal performance. For k as small as 50 , which roughly amounts to 0 . 4% of the features, the extracted component captures at least 44 . 6% of the variance corresponding to the true nonnegative k -sparse PC. The obtained upper bound is a significant im-provement compared to the trivial bound given by  X  1 . Low Resolution Spectrometer Dataset. The Low Reso-lution Spectrometer (LRS) dataset, available in (Bache &amp; Lichman, 2013), originates from the Infra-Red Astronomy Satellite Project. It contains 531 high quality spectra (sam-ples) measured in 93 bands. Fig. 6 depicts the normalized variance explained by the computed nonnegative, k -sparse PC versus the sparsity parameter k . The empirical covari-ance matrix of this dataset exhibits sharper decay in the spectrum than the previous examples, yielding tighter ap-proximation guarantees according to our theory. For in-stance, for k = 20 , the extracted nonnegative component captures at least 86% of the maximum variance. For values closer to k = 90 , where the computed PC is nonnegative but no longer sparse, this value climbs to nearly 93% . We introduced a novel algorithm for nonnegative sparse PCA, expanding the spannogram theory to nonnegative quadratic optimization. We observe that the performance of our algorithm often matches and sometimes outperforms the previous state of the art (Sigg &amp; Buhmann, 2008). Even though the theoretical running time of Alg. 3 scales better than EM, in practice we observed similar speed, both in the order of a few seconds. Our approach has the benefit of provable approximation, giving both theoretical a-priori guarantees and data dependent bounds that can be used to estimate the variance explained by nonnegative sparse PCs, as shown in our experiments. The authors would like to acknowledge support from NSF grants CCF-1344364, CCF-1344179, DARPA XDATA and research gifts by Google and Docomo.
