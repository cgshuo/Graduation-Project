 Matrix completion (MC) with additional information has found wide applicability in several machine learning appli-cations. Among algorithms for solving such problems, In-ductive Matrix Completion(IMC) has drawn a considerable amount of attention, not only for its well established the-oretical guarantees but also for its superior performance in various real-world applications. However, IMC based meth-ods usually place very strong constraints on the quality of the features(side information) to ensure accurate recovery, which might not be met in practice. In this paper, we pro-pose Goal-directed Inductive Matrix Completion(GIMC) to learn a nonlinear mapping of the features so that they satisfy the required properties. A key distinction between GIMC and IMC is that the feature mapping is learnt in a super-vised manner, deviating from the traditional approach of un-supervised feature learning followed by model training. We establish the superiority of our method on several popular machine learning applications including multi-label learning, multi-class classification, and semi-supervised clustering.
Matrix completion methods are widely used in several applications, ranging from collaborative filtering for recom-mender systems [8], to social network analysis [4] and clus-tering [25]. In these applications, a particular entry of the observed matrix is modeled as a linear interaction between factors corresponding to the row and column variables. For example, a rating provided by user i for movie j in a recom-mender system is modeled as r ij = w T i h j , where w i , h low dimensional user and movie embeddings.

Modern applications typically involve large amounts of data (often in the millions or more), and several scalable algorithms have been proposed to solve matrix completion problems in such large data regimes [27, 21]. As a conse-quence of large-scale data acquisition methods, these ap-plications involving large amounts of data also involve side information. Fast and scalable methods have been proposed in [30, 19] when the side information is in the form of pair-wise relationships, such as product co-purchasing networks or social networks for recommender systems, or gene X  X ene interaction networks in computational biology.

In several applications of interest, one is not aware of the relational information in the form of graphs, but is given direct information in the form of features. These might cor-respond to demographic information (gender, occupation) for users and product information (genre, year of release) in a movie recommender system for example. When such features are provided, one can model an observation as a specific interaction between the features. For example in the recommender system case considered above, we would have r ij = x T i Z y j where Z is the learnt model and x are the user (movie) features [5] 1 . These methods two ad-vantages over traditional matrix completion applications:
While such methods have been considered before in [24, 5], the authors in [2] showed that a significant drawback of feature-based matrix completion methods X  X nductive Matrix Completion(IMC) X  is that there need to be extremely strict constraints placed on the features to ensure accurate recov-ery of the underlying matrix. Specifically, the space spanned by the row (column) features must correspond to the row (column) space of the matrix to be factorized. In prac-tice, such constraints are seldom met, which might result in poor performance of IMC. One way to solve this prob-lem is by constructing mappings of the features so that the mapped features are more aligned to the subspaces in question. Moreover, the mappings should be constructed so that the methods are scalable, to be applicable to modern datasets. This brings us to the questions we wish to answer in this paper:
We expound on this in more detail in the sequel 1. Can a (possibly nonlinear) mapping be constructed for 2. Can the resulting method be scaled to large datasets? We answer both the above questions in the affirmative. To answer the first question, we aim to construct (nonlinear) embeddings of the row (or column) features so that they are better aligned to the task at hand. Specifically, we develop a minimization scheme called Goal-Directed Inductive Ma-trix Completion (GIMC) that alternates between learning the low-rank factors of the target matrix, and the aforemen-tioned embeddings. A key contribution of this work is that the embeddings are learned in a s upervised manner. This is a significant deviation from several other feature learning methodologies that learn the features in an unsupervised setting and use the learned features to solve the machine learning task.

To answer the second question, we resort to performing linear approximations of the nonlinear mappings that we aim to learn. Linear approximations of kernel-based meth-ods have spawned large amounts of interest, either by us-ing random Fourier features [17] or via Nystr  X  om approxima-tions [23]. Such methods are attractive since they facilitate the use of highly scalable existing methods for learning with linear models. However, the dimension of these approximate kernel mappings could be very high, which induces a signifi-cant computational burden. We show that since we can learn the features based on the task at hand, we can use signif-icantly fewer Fourier features or Nystr  X  om landmark points to achieve the same error as classical, unsupervised kernel approximation methods.
 Viewed from a different perspective, our proposed Goal-Directed Inductive Matrix Completion framework could be viewed as a three-layer neural network. In this shallow neural network, the objective is a least squares loss and cos (  X  ) and sin (  X  ) are the activation functions. Interestingly, with such a shallow neural network, we achieve state-of-the-art performance in various matrix completion applications. Note that instead of using gradient descent in our algorithm to optimize the loss function, similar to neural networks, we could use back-propagation and SGD as an optimization scheme, and also use other activation functions such as the sigmoid function or hyperbolic tangent.
 We consider three applications to test our proposed method: Multi-Label learning, Multi-Class classification, and Semi-supervised Clustering. In these three cases, we show that GIMC achieves state of the art results, while using far fewer features than what would be required to achieve the same performance using unsupervised feature learning methods. We verify that we achieve orders of magnitude speedups over existing methods.
 The rest of the paper is organized as follows: in the next Section, we summarize related work. In Section 3 we detail the Inductive Matrix Factorization formulation and summa-rize known results. We propose our model in Section 4 and show how to apply our framework for solving three popular machine learning applications: Multi-Label learning, Multi-Class classification, and Semi-supervised Clustering in Sec-tion 5. We provide extensive empirical results on several real-world datasets in Section 6 before concluding our paper in Section 7.
We categorize related work into three categories, based on the core contribution of the corresponding works:
Stochastic gradient descent(SGD) is widely used in matrix factorization due to its efficiency and ease of implementa-tion. Various update schemes to parallelize SGD have been proposed: HogWild [21] uses lock free approach and DSGD [20] partitions the rating matrix into independent blocks to avoid conflict in the distributed system. [15] takes I/O cost and the CPU utility into consideration and proposed a fast and robust parallel SGD matrix factorization algorithm to deal with the situation when data cannot fit into memory. On a different note, [27][26] proposed to use coordinate de-scent instead of SGD to optimize the loss function, and the corresponding parallel solvers are shown to have superior performance in both multi-core and distributed settings.
Matrix completion with side information has drawn much attention for improving the performance of traditional ma-trix completion. For instance, [19, 30] incorporates graph information as a regularization term into matrix completion framework, where the graph encodes pairwise relationships among variables. Along similar lines, [13] uses a social net-work among users to bias the recommender system. Instead of these pairwise relationships, in scenarios where one is pro-vided with features for the rows and/or columns of the ma-trix, [5, 24, 10] proposed an Inductive Matrix Completion formulation. The method is  X  X nductive X , in that it general-izes to previously unobserved data points, which is a draw-back in traditional recommender systems. These methods have since been applied to predict gene-disease relationships [14] and semi-supervised clustering [25].
To overcome the computational barrier of applying kernel methods to very large datasets, linear approximations have become popular, common among them being Nystr  X  om based methods [23] [29] [11] and random feature based meth-ods [18] [12] [3, 7]. It has been shown that these approximate kernel mappings can speed up the training and prediction for kernel machines such as kernel SVM and Gaussian pro-cesses. To achieve a  X  X ood X  approximation to the underlying kernel, these methods require the generation of many ran-dom features or landmark points. Thus, while such meth-ods can in principle alleviate the computational costs associ-ated with kernel machines, achieving a good approximation requires generation of many random features or landmark points which contributes to increased costs.
In this section, we first introduce traditional matrix com-pletion and inductive matrix completion, and then formally set up the problem we are interested in solving. Consider a standard matrix completion setup, where we observe entries from an n x  X  n y matrix A . Let  X  : |  X  | n x n y be the set of observed entries in A . Traditional matrix completion models A to be low rank, meaning that the row and column variables of A share a low dimensional latent space.
Standard matrix completion tries to recover the low-rank matrix by solving one of the following optimization prob-lems: where W  X  R n x  X  k and H  X  R n y  X  k , and P  X  (  X  ) is the projec-tion operator that only retains those entries of the matrix that lie in the set  X , It has been shown that these two prob-lems are in fact equivalent when the chosen k  X  rank( A ) [22]. In addition, although the factorization objective (2) is non-convex, it turns out that the (local optimal) solution given by this non-convex form is usually comparable to the global optimum in (1). To solve (2), note that when either W or H is fixed, the above optimization becomes convex with respect to the other variable, so that one can solve (2) by alternating minimization. Various optimization techniques, for example, coordinate descent and stochastic gradient de-scent, have been proposed to solve (2) efficiently.
One challenge for traditional matrix completion is that it cannot directly use side information and apply it to predict new and unseen data, or to simplify computation. Induc-tive Matrix Completion [5] alleviates this issue. Specifically, suppose we are given a set of features X  X  R n x  X  d x for the rows of A , and similarly for the columns, Y  X  R n y  X  d y . Each row of X (denoted as x i ) and each row of Y (denoted as y are features for the i -th row and j -th column entity respec-tively. Then, IMC incorporates the feature information into matrix completion by solving the following problem: min min where now W  X  R d x  X  k and H  X  R d y  X  k .

Note that the number of unknown parameters are k ( d x + d ) which can be significantly smaller than k ( n x + n y ). This has the potential to speed up computations by non-trivial amounts. Furthermore, besides recovering unseen entries in A , (3) can also be used to deal with cold start problem in recommendation systems. For example, for a movie recom-mendation problem, given a new user where only its fea-tures x new are available, IMC could predict the score for the movies to be x T new ZY T . We refer the interested reader to [5] for details.

We consider a comparison between MC and IMC on a synthetic example. We generate the underlying matrix A as XY T + N , where X,Y  X  R 200  X  50 are two rank-50 random matrices with each entry drawn from Gaussian distribution N (0 , 1), and N  X  R 200  X  200 is the noise matrix with each en-try drawn from N (0 , 0 . 1). X and Y are provided as features to IMC, and rank k is set to be 20 for both MF and IMC. We vary the number of observed entries from 5% to 30% and recover the remaining entries of the target matrix. The approximately recovered matrix  X  A will be WH T in MF and XWH T Y T in IMC. We then evaluate the recovered matrix  X  in Figure 1, IMC results in lower error than MC, suggest-ing that incorporating side information is useful for better recovery given a small perturbed observations.
 While Figure 1 shows that IMC could outperform MC, the Figure 1: Toy example comparing MC, IMC, IMC-RFF, and GIMC-LFF. MC is the traditional model (Eq (2) ); IMC is the model in Eq (3) ; IMC-RFF is a modification of IMC with random Fourier fea-ture based side information as in Eq (5) . GIMC-LFF learns the Fourier projections and the model simul-taneously by solving Eq (9) side information is the random Fourier features from X and Y as in Eq (5) ; GIMC-LFF is to learn the projections of Fourier fea-ture and model in IMC simultaneously. Using side information (IMC and IMC-RFF) improves perfor-mance, but not as significantly as learning both the projections and the model (GIMC-LFF) quality of the features plays a significant role in determining the accuracy of the recovered matrix. Indeed, the authors in [24] showed that to guarantee exact recovery in IMC, the features X,Y should be perfectly aligned with the row and column spaces of the target matrix A . Specifically, we require that where Col (  X  ) and Row (  X  ) correspond to the column and row space respectively. That is, to achieve good performance, the features X and Y should be strongly related to the un-derlying problem, which might not be satisfied in practice. This motivates us to ask if, given noisy features X and Y, one can learn an embedding of the features so that we can retain the gains provided by IMC given noisy features. In this section, we introduce our framework X  X oal-directed Inductive Matrix Completion (GIMC). We first show how to employ nonlinear mappings into IMC, and then introduce the proposed GIMC model where we add supervision into nonlinear mappings throughout the learning process.
As discussed previously, performance of IMC may suffer if condition (4) is not satisfied. One way to address this issue is to generate features based on X and Y ; that is, mapping X to  X  X ( X ) and Y to  X  Y ( Y ) via two mappings  X  overcome the violation of (4), such mapping has to be non-linear , since for any linear mapping L , Col ( L ( X ))  X  Col ( X ) and thus cannot help to relieve the violation of (4). By doing so, the IMC problem can modified to be: where  X  X ( X ) and  X  Y ( Y ) are new set of features generated by the non-linear mapping of X and Y respectively. For simplicity, here we again consider Z to be low rank, but in general, the term k Z k  X  could be further replaced by other regularizations for different applications. From now on, we will focus our discussion on the mapping of X , and use n,d to represent n x ,n d . All subsequent discussions could be ap-plied to Y as well with n = n y ,d = d y .

Indeed, the specific mapping to be used can be varied, and the choices are many. A popular choice of such mappings are kernel mappings where we map x to  X  ( x ) which could be infinite dimensional. Recently, scalable alternatives to using exact kernels (either directly in the primal or via the Gram matrix in the dual) have been proposed where the ex-act mapping is replaced by an approximation. For instance, for shift-invariant kernels, based on Bochner X  X  theorem[17], the feature mapping  X  (  X  ) can be written as (with high prob-ability): where U = { u 1 ,  X  X  X  , u m } are the m projection directions sam-pled according to the distribution from the Fourier trans-form of the kernel function. For the RBF kernel ( k ( x i e distribution is a Gaussian distribution p ( u ) = N (0 , 2  X I ). Since there are infinite number of data points from that dis-tribution, the feature mapping is infinite dimensional, and cannot be used in practice. [17] proposed to randomly sam-ple a few projections from the distribution, and used the approximate feature mapping to speed up kernel machines.
Another popular kernel mapping is Nystr  X  om features that approximate the kernel matrix G by sampling m n land-mark points { u r } m r =1 , and forming two matrices C  X  and E  X  R m  X  m based on the kernel function. Here C ir = K ( x i , u r ) and E ij = K ( u i , u j ), and the kernel matrix can be approximated as where E  X  denotes the pseudo-inverse of E . From the fea-ture point of view, the Nystr  X  om method may be viewed as constructing features for x i ; with m landmark points u ,  X  X  X  , u m , the feature mapping is x i  X   X  U ( x i ) = [ k ( x i , u 1 ) ,k ( x i , u 2 ) ,  X  X  X  ,k ( x where M = ( E  X  ) 1 2 and U = { u 1 ,  X  X  X  , u m ,M } .
However, when using the above feature mappings for solv-ing IMC, as shown in the experiment, we need large num-ber of projections or landmark points (large m ), to achieve good performance. The memory requirement for storing these new set of features  X  U ( x i ) for all x i is O ( nm ). This can be infeasible in many applications, for example, when m &gt; 10 , 000 and n approaches 1 million. A natural question to ask is if we can reduce the number of samples needed to perform the approximation.
To overcome the issue with large number of nonlinear fea-tures, we propose Goal-directed Inductive Matrix Comple-tion(GIMC), which aims to learn the feature mapping au-tomatically to benefit the model. Instead of constructing a mapping  X  U (  X  ) in an unsupervised setting which ignores the task at hand, we propose to learn the feature mappings and the model parameters simultaneously in a supervised man-ner. This can be attempted by solving the following joint optimization problem: min where ` is the loss function, Z represents the model, and the feature mapping of X and Y are  X  U (  X  ) , X  V (  X  ) which are parameterized by U and V respectively. For simplicity, in this paper we focus on squared loss ` ( b,a ) = 1 2 ( b  X  a ) in general we can extend the algorithm to any loss function. The main difference between problems (5) and (9) is that the former only considers the model parameters Z given a fixed feature mapping since U,V are predetermined, while the latter considers to learn Z and U,V simultaneously. As a result, we have the flexibility to optimize the kernel map-ping according to the objective function that is defined by different machine learning applications with IMC.

Again, we can define the feature mapping  X  U (  X  ) in (9) us-ing various functions. In our first algorithm, Goal-directed IMC with Learned Fourier Features (GIMC-LFF) approach, we define the feature mapping according to (6), and the parameters U = { u r } m r =1 are the projection directions in Fourier Features. Compared to the original RFF that sam-ples { u r } m r =1 randomly from a distribution, we can learn the projection directions { u r } m r =1 by minimizing the final objective function in IMC.
 Our second proposed algorithm, Goal-directed IMC with Learned Nystr  X  om approximation (GIMC-LNYS), uses  X  U (  X  ) defined in (8). In this case, the parameters U = { u 1 ,..., u where each u i is a landmark point, and M  X  R m  X  m is a lin-ear transformation. Although gradient descent updates dis-cussed below can be applied to two set of variables ( { u and M ), we observe no improvement by learning two sets of parameters, so in the following we assume M = I in GIMC-LNYS. Thus, U = { u r } m r =1 for both GIMC-LNYS and GIMC-LFF, where each u r  X  R d .

Problem (9) is non-convex and can be solved via alternat-ing minimization, where we alternatively optimize the model parameters Z and feature mapping parameters U and V . The update of model Z varies through applications, and we will discuss how to update model parameters under specific applications shortly in Section 5. To update nonlinear map-ping parameters u 1 ,..., u m , we use gradient descent and update the parameters as: where  X  is the step size and we adopt the Armijo-rule based step size selection to perform line search. Assume B  X 
U ( x i ) T Z X  V ( y j ) is the current prediction on ( i,j ) element, the gradient for each u r can be computed by chain rule: each u r only correlates with two features (see (6)), so the Jacobian matrix has only two nonzero columns; where for GIMC-LNYS each Jacobian matrix has only one nonzero column. The update of V could be conducted similarly.
The time complexity of computing (11) can be analyzed as follows. (1) Computing  X  V ( y j ) for all j : Assume the computation of each element in  X  V ( y j ) requires O ( d ) time (one inner product for both GIMC-LNYS and GIMC-LFF), so constructing  X  V ( y j ) for all j requires O ( dmn ) time. This can be further reduced to O (nnz( Y ) m ) if the original feature matrix Y is sparse. (2) Computing  X  U ( x i ) for all i : similarly this requires O (nnz( X ) m ) time. (3) Computing eq (11): Since only one or two columns of  X  X  U ( x i )  X  u term in eq (11) requires O ( m ) time. In summary, the overall time complexity for evaluating  X  u r f ( Z,U,V ) is which is similar to the original matrix completion if m is small.

Details of the method are given in Algorithm 1. Since we employ alternating minimization scheme for solving (9), one can show that with the same number of landmark points or projection directions m , our method can achieve lower objective function values compared to the one achieved by using IMC with unsupervised Nystr  X  om (IMC-NYS) or ran-dom Fourier features(IMC-RFF). Specifically, let U 0 and V be the initial set of mapping directions, Z 0 be the model trained using IMC-RFF or IMC-NYS, and U t be the up-dated directions at iteration t of Algorithm 1, we will have which potentially implies a lower generalization error. In the toy example in Figure 1, we can clearly see that GIMC-LFF performs better than IMC-RFF, because we learn the fea-ture and model together and thus construct a better set of non-linear features to benefit IMC. In our real-world experi-ments, we will also show that for a fixed number of features, our method outperforms IMC using both traditional Nys-tr  X  om and random Fourier feature mappings.
We now state how to apply our proposed GIMC model to three machine learning applications: multi-label/multi-class learning problems, and semi-supervised clustering problem.
Modern multi-label learning algorithms have to deal with problems with a very large number of samples, features and labels 2 For example in our experiments, the largest dataset called  X  X xtreme multi-label learning X  http://research. microsoft.com/en-us/um/people/manik/events/xc15/
Algorithm 1: Goal-Directed Inductive Matrix Factor-ization (GIMC)
Input : Partially observed set { A ij | ( i,j )  X   X  } ,
Output : The model Z , U = { u r } m x r =1 and V = { v r [For GIMC-LFF]: Initialize { u r } m x r =1 , { v r } m y original random Fourier features [For GIMC-LNYS]: Initialize { u r } m x r =1 , { v r } m y kmeans sampling or random sampling from training data for t = 1 ,..., maxiter do 4 Update Z by existing IMC with {  X  U ( x i ) } n x i =1 and 5 Compute g r  X  X  X  u r f ( Z,U,V ),  X  r = 1 ,...,m x 6 Line search to find a step size  X  7 Update u r  X  u r  X   X  g r ,  X  r = 1 ,...,m x 8 Compute g r  X  X  X  v r f ( Z,U,V ),  X  r = 1 ,...,m y 9 Line search to find a step size  X  10 Update v r  X  v r  X   X  g r ,  X  r = 1 ,...,m y we used has more than a hundred thousand samples, fea-tures, and labels.

Traditional multi-label learning approaches usually only consider linear models due to computational concerns, espe-cially when the number of labels is huge (see [28, 16]). Using our goal-directed kernel approximation framework, we can not only learn the non-linear features, but also make com-putation efficient in both training and prediction phases.
Given training data X = { x i } n i =1 , we use A  X  R n  X  L denote the 0/1 label matrix, where each row of A represents the L labels associated with x i . We can formulate a simple model that assumes no correlation among labels, and solve for each label independently: where each w j is the parameters for predicting whether the data has label j . This can be reduced to L binary classifi-cation problems, but this approach is very time consuming. For example, the Delicious dataset with 983 labels requires more than 1 day for training. Generally speaking, a method cannot scale to large n and large L if the time complexity grows with O ( nL ).
 We now show that we can apply GIMC-LFF and GIMC-LNYS to solve this problem. As ` is chosen to be the squared loss, the objective function can be rewritten as and W  X  R m  X  L are model parameters. We can see that (12) is a special case of (9), where  X  V ( Y ) = I and  X  is all the nL elements in A . 3 Therefore, we can solve it by the alternating minimization algorithm. Unfortunately, as analyzed in Section 4.2, directly computing (11) will lead to O ( |  X  | m ) = O ( Lmn ), which will not scale to problems with L,n  X  100 , 000.

Interestingly, in the following we show that by carefully arranging the computation, the computational complexity can be reduced to O ( m 2 ( n + L ) + m k X k 0 + m k A k leads to a scalable algorithm that only depends on the num-ber of nonzero elements in feature and label matrices.
When U is fixed, the subproblem with respect to W is a standard linear regression problem, which can be solved by a linear system solver: W  X  (  X  X  U ( X ) T  X  U ( X )+  X I )  X  1 This costs O ( m 3 + m k A k 0 + m 2 L ) time complexity.
When W is fixed, for GIMC-LFF the subproblem with respect to U can be written as  X  u r f ( w ,U ) = which can be written in a compact form as  X 
U f ( w ,U ) = where U  X  R d  X  m , W T = [ W T (1) W T (2) ] and  X  is the element-wise product. This can be computed in O ( m 2 n + m k X k m k A k 0 ) time where k  X  k 0 is number of nonzeroes in the matrix.

In the line search step, we use the following formulation to compute the objective function value, k A  X   X  U ( X ) W k 2 F = X which only requires O ( m k A k 0 + nm 2 + Lm 2 ) time. The complexity will not grow as O ( nL ), which means it can use large number of samples and labels when A is sparse. A similar update rule can be derived for GIMC-LNYS.

As a final remark, we can also use (12) for solving multi-class classification problem, where A is a n  X  L matrix with L to be the number of classes. A ij = 1 when x i belong to j -th class. The algorithm and analysis remain the same.
We now show that we can apply our GIMC framework for semi-supervised clustering as well. The semi-supervised clustering problem can be stated as follows: Suppose we are given n items and a feature matrix X  X  R n  X  d where the i -th row x i is the feature of i -th item, and a set of pairwise constraints C = { S ij | ( i,j )  X   X  } where each constraint S is in the following form: Rigorously speaking, it is a special case for the  X  X eneralized X  GIMC model where regularization of model parameter is replaced with k X k 2 F .
 Then the goal is to find a clustering of items such that most similar items are within the same cluster. A state-of-the-art approach MCCC [25] models this problem using IMC framework. Specifically, let S = P k i =1 c i c T i be the rank-k similarity matrix, where c i is the indicator of the i -th cluster. Then the constraint set C could be thought of as partial observations of S . MCCC first tries to complete the low rank matrix S using IMC (3), and then uses its top-k singular vectors, which ideally will be the cluster indicators, to derive a partition.

Although it has been shown that MCCC outperforms tra-ditional semi-supervised clustering algorithms [25], one issue is that it only considers linear mapping of features in ma-trix completion step, while the clustering may be revealed by some non-linear mapping of features. Thus, to overcome this problem, we propose to apply our GIMC framework to solve the semi-supervised clustering by learning both non-linear mapping of features and the underlying similarity S by solving the following optimization problem: min which is a special case of the GIMC framework (9) with Y = X and A = S 4 . To solve above optimization prob-lem, we alternate between learning the nonlinear mapping (i.e. solve U,V with fixed W,H ) and the model (i.e. solve W,H with fixed U,V ). Once converged, we can derive a clustering by running k -means on top-k singular vectors of  X  U ( X ) WH T  X  V ( X ) T .

We now briefly discuss how to solve each subproblem ef-ficiently using GIMC under semi-supervised clustering con-text. First, fixing U,V , solving W,H becomes a standard IMC problem, and one can use existing efficient algorithms (e.g. alternating minimization with conjugate gradient [28] as used in our experiment) to solve for W and H . The method addressed in [28] only requires O (( |  X  | + nm ) k ) for each update of W or H .

Fixing W,H , we could solve U,V alternatively by gradi-ent descent with line search. For example, let us first con-sider (13) to be GIMC-LFF where U , V are parameters for projections in Fourier Features, then the learning process of U is similar to the one in multilabel learning, except here we only consider the loss on a subset  X . The gradient of each projection u r could be written as:  X  u r f ( U,V,W,H ) = X where Q =  X  V ( X ) HW T . The gradient of U could be com-puted in O (( |  X  | + nd ) m ) as discussed in Section 4.2. In addition, for each line search iteration, evaluating objective function also takes O (( |  X  | + nd ) m ) since only |  X  | inner prod-ucts have to be computed. Similar update rule and time complexity analysis could also be derived if we consider Nys-tr  X  om features for U and V . Typically, m,d n, |  X  | n the overall time complexity for learning mappings grows at a rate much smaller than O ( n 2 ), especially if the available con-straints are sparse. Using GIMC-LFF for semi-supervised
Here we consider the equivalent nonconvex form of IMC objective (see Section 3). clustering is shown in Algorithm 2. We can derive similar algorithm using GIMC-LNYS.
 Algorithm 2: Semi-supervised Clustering with GIMC-LFF Input : feature matrix X , constraint set
Output : Clustering result  X  . // Solve for GIMC-LFF model (13)
Initialize random projections U,V  X  R d  X  m , model parameter W,H  X  R 2 m  X  k . for t = 1 ,..., maxiter do 3 [ W,H ]  X  min W,H 1 2 k P  X  S  X  4 Update U with gradient descent (using (14)). 5 Update V with gradient descent (similar to (14)). // Use left side of matrix to approximate top-k singular vectors
T  X  svds(  X  U ( X ) W,k )  X   X  kmeans( T,k )
We consider the three applications mentioned in the pre-ceding section. We compare the following methods: 1. IMC-NYS: IMC with Nystr  X  om features. Here the land-2. IMC-RFF: IMC with random Fourier features [18]. 3. LEML[28]: state-of-the-art multi-label solver using IMC 4. FastXML[16]: state-of-the-art multi-label solver using 5. SLEEC[1]: state-of-the-art multi-label solver which learns 6. k-means: traditional k-means clustering, which only 7. MCCC[25]: a semi-supervised clustering method using 8. GIMC-LNYS: our proposed framework GIMC which 9. GIMC-LFF: our proposed framework GIMC which learns Note that we compare with SLEEC, FastXML, and LEML for the multi-label learning task, and k-means and MCCC for semi-supervised clustering problem. SLEEC, FastXML, and LEML are highly optimized C++ implementation pub-lished along with the original papers. We use the parame-ters along with the released code for these three methods. We use Gaussian kernel as the basis kernel function used in Nystr  X  om feature and random Fourier features. The kernel width  X  in Gaussian kernel and the regularization term  X  in IMC are chosen by cross-validation. All experiments are conducted on a machine with an Intel Xeon X5440 2.83GHz CPU and 32G RAM.
First, we show that our proposed algorithms can be ap-plied to large-scale multi-label and multi-class classification problems. The datasets are listed in Table 1. The four multi-label datasets are from [1]. We evaluate the results by averaged precision at top 1, 3, and 5.
 For multi-label learning problem, firstly we compare with IMC using Nystr  X  om features (IMC-NYS) and random Fourier features(IMC-RFF) in Figure 2. Here we vary m , i.e., the number of projections or landmark points in the feature mapping, which is directly related to the memory usage and prediction time of our method, and show the top-3 accu-racy. It shows that GIMC-LFF and GIMC-LNYS are much better than IMC-NYS and IMC-RFF. This shows that with the same number of features, learning feature mapping and models together will benefit IMC. Furthermore, we compare with three state-of-the-art multi-label learning approaches: SLEEC [1], FastXML [16] and LEML [28] in the same fig-ures. For these three algorithms, we compare with the best accuracy using the best parameters reported in their pa-pers. Also, GIMC-LFF achieves better test accuracy com-pared to state-of-the-art multi-label solvers on all these three datasets.

The detailed comparison in terms of precision at 1 , 3 , 5 are shown in Table 2. In addition, the efficiency of our algo-rithms is competitive to state-of-the-art algorithms, SLEEC and FastXML. On the NUS-WIDE dataset, using 500 fea-tures with 100 iterations, our GIMC-LFF achieves a 17.27% accuracy taking 6,482 seconds, while FastXML achieves a 16.32% using 17,774 seconds and SLEEC takes 68,232 sec-onds and achieves 17.67% accuracy.

We can also use our proposed methods for multi-class clas-sification. We treat it as a special case of multi-label learn-ing problem where the label matrix Y has only one 1 in each row. Here we test on one popular multi-class classification dataset: CIFAR-10.

CIFAR-10: We benchmark our method on the CIFAR-10[9], a popular multi-class classification dataset with 10 classes of natural images including 50,000 training samples and 10,000 testing samples. Each image is an 32  X  32 RGB image. For each image, we extracted 12,288-dimensional fea-tures using CAFFE, a popular deep learning framework[6]. We then apply our proposed method GIMC-LFF and GIMC-LNYS on these new set of features for classification. The result is in Figure 2. In our experiment, linear SVMs with the new set of features achieves 81.73% accuracy, while our proposed methods achieve around 84% accuracy using 2000 features which is also much better than IMC with random Fourier features and Nystr  X  om features. with SLEEC, but takes much less time: 4,724 vs 25,289 seconds.
Since GIMC-LFF and GIMC-LNYS perform similarly well, we mainly compare our proposed GIMC-LFF for semi-supervised clustering with three popular clustering methods. We con-sider using simple k -means on data points X  features (and ignore pairwise constraints) as the baseline, and also com-pare with state-of-the-art clustering method MCCC [25], and IMC-RFF which first generates random Fourier fea-tures, and then applies MCCC with these nonlinear fea-tures. We take m = 500 for both IMC-RFF and GIMC-LFF. Note that (1) MCCC is based on the IMC framework; (2) since MCCC has been shown to outperform many traditional methods (see [25] for details), we thus focus on comparing with MCCC to demonstrate the effectiveness of our model. In this experiment, we consider three real-world datasets: Mushroom, Segment and Covtype 5 from classification bench-marks. Items with the same label are regarded as in the same cluster which is the general setting for semi-supervised clustering. We randomly sample |  X  | pairs of items and gen-erate the pairwise constraints based on these pairs, with points. For each choice of |  X  | , we apply each method sev-eral times with different regularization parameters  X  chosen the corresponding clustering  X  ) that achieves the smallest empirical clustering error on the validation set: |  X  | Then we use the parameter  X  to train the model and gen-erate the clustering  X  , and evaluate  X  using the clustering error rate to the ground-truth clustering defined by: n ( n  X  1)
We subsample from the entire dataset to make clusters have balanced size. where  X   X  i is the ground-truth cluster of item i . The result is shown in Table 3. First, we observe that MCCC, IMC-RFF, and GIMC-LFF all perform well on Mush-room as perfect clustering could be (almost) derived. This confirms that MCCC is indeed effective when clustering is linearly dependent on features (100% training classification accuracy could be attained by a linear SVM on this dataset). However, in Segment and Covtype where features are not linearly separable for clustering, MCCC is trapped and both IMC-RFF and GIMC-LFF perform better than MCCC on these two datasets. This shows that considering nonlinear mapping of features improves the clustering result when data points are not linearly separable. Furthermore, we see that GIMC-LFF can further improve IMC-RFF with sufficient constraints. This is because when constraints are sufficient, GIMC-LFF could reliably learn a better nonlinear mapping based on constraints, and thus a better clustering could be obtained. The results show that our framework outperforms both state-of-the-art MCCC algorithm and IMC-RFF by learning a better nonlinear mapping of features for cluster-ing. We propose a family of Goal-directed Inductive Matrix Completion (GIMC) algorithms. Instead of constructing non-linear features without any supervision to the final goal, we formulate the non-linear mapping finding stage and the model learning stage together as a unified function. As a re-sult, out algorithms find the non-linear mapping that is good for Inductive Matrix Completion. We apply our two algo-rithms, GIMC based on Fourier features (GIMC-LFF) and GIMC based on Nystr  X  om features(GIMC-LNYS), to multi-label learning, multi-class classification, and semi-supervised clustering problems. Experimental results show that our al-gorithms significantly reduce the number of non-linear fea-tures needed, and we obtain a better prediction accuracy and lower clustering error rate compared to state-of-the-art methods. class problems such as the CIFAR-10 dataset.

As future work, we are planning to exploit (1) different non-linear features mapping, e.g., polynomial kernel feature mapping; (2) different optimization techniques in our frame-work to speed up the computation, e.g., using coordinate descent and stochastic gradient descent; (3) other machine learning applications, such as classification and regression. Acknowledgments
This research was supported by NSF grants CCF-1320746 and IIS-1546459. We also thank Dinesh Jayaraman for help on CAFFE installation, Hsiang-Fu Yu for help on LEML code, Prateek Jain and Manik Varma for sharing the code and parameters for FastXML and SLEEC.
 [1] K. Bhatia, H. Jain, P. Kar, M. Varma, and P. Jain. [2] K.-Y. Chiang, C.-J. Hsieh, and I. S. Dhillon. Matrix [3] B. Dai, B. Xie, N. He, Y. Liang, A. Raj, M. Balcan, and [4] C. Hsieh, K. Chiang, and I. S. Dhillon. Low rank mod-[5] P. Jain and I. S. Dhillon. Provable inductive matrix [6] Y. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, [7] P. Kar and H. Karnick. Random feature maps for dot [8] Y. Koren, R. Bell, and C. Volinsky. Matrix factorization non-linear mappings and models jointly will benefit IMC. [9] A. Krizhevsky. Learning Multiple Layers of Features [10] R. Kueng, H. Rauhut, and U. Terstiege. Low rank ma-[11] S. Kumar, M. Mohri, and A. Talwalkar. Ensemble Nys-[12] Q. V. Le, T. Sarlos, and A. J. Smola. Fastfood  X  approx-[13] H. Ma, D. Zhou, C. Liu, M. R. Lyu, and I. King. In [14] N. Natarajan and I. S. Dhillon. Inductive matrix com-[15] J. Oh, W.-S. Han, H. Yu, and X. Jiang. Fast and robust [16] Y. Prabhu and M. Varma. Fastxml: A fast, accurate [17] A. Rahimi and B. Recht. Random features for large-[18] A. Rahimi and B. Recht. Weighted sums of random [19] N. Rao, H.-F. Yu, P. Ravikumar, and I. S. Dhillon. [20] B. Recht and C. R  X e. Parallel stochastic gradient algo-[21] B. Recht, C. Re, S. Wright, and F. Niu. Hogwild: A [22] N. Srebro and A. Shraibman. Rank, trace-norm and [23] C. K. I. Williams and M. Seeger. Using the Nystr  X  om [24] M. Xu, R. Jin, and Z.-H. Zhou. Speedup matrix comple-[25] J. Yi, L. Zhang, R. Jin, Q. Qian, and A. Jain. Semi-[26] H.-F. Yu, C.-J. Hsieh, S. Si, and I. S. Dhillon. Scalable [27] H.-F. Yu, C.-J. Hsieh, S. Si, and I. S. Dhillon. Parallel [28] H. F. Yu, P. Jain, P. Kar, and I. S. Dhillon. Large-scale [29] K. Zhang, I. W. Tsang, and J. T. Kwok. Improved [30] T. Zhou, H. Shan, A. Banerjee, and G. Sapiro. Ker-
