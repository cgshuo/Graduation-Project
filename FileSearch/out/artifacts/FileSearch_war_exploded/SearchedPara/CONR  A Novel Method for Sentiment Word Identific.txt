 Sentiment word identification (SWI) is of high relevance to sentiment analysis technologies and applications. Current-ly most SWI methods heavily rely on sentiment seed words that have limited sentiment information. Even though there emerge non-seed approaches based on sentiment labels of documents, but in which the context information has not been fully considered. In this paper, based on matrix fac-torization with co-occurrence neighbor regularization which is derived from context, we propose a novel non-seed model called CONR for SWI. Instead of seed words, CONR exploit-s two important factors: sentiment matching and sentiment consistency for sentiment word identification. Experimental results on four publicly available datasets show that CONR can outperform the state of-the-art methods.
 I.2 [ Natural Language Processing ]: Text analysis Theory sentiment word identification; sentiment lexicon; sentiment analysis; matrix factorization
In recent years, sentiment analysis has become a hot is-sue and has been used across a wide range of domains. The task is to predict the sentiment polarities (also known as semantic orientations) of opinions by analyzing sentiment words and expressions in sentences and documents [1]. Sen-timent words are words that express a positive or negative sentiment polarity. Therefore, sentiment word identification (SWI) is a critical and necessary initial procedure with re-spect to the majority of tasks of sentiment analysis such as subjectivity detection, appraisal expression recognition and sentiment polarity classification. So we focus on SWI in this paper.

The main existing approaches infer the polarities of sen-timent words from the labels of seed words. The semantic orientation of a given word is calculated from the strength of its association with a set of positive words, minus the strength of its association with a set of negative words [1, 2, 3]. But the outstanding problem among these models is that they rely heavily on seed words with sentiment labels which are usually manually selected. These models are very sensitive to seed words. Any missing key word could lead to poor performance. In fact, subjective documents often pro-vide additional information other than content information [4, 7, 8] as shown in Figure 1(a). Specifically, the polarities of subjective documents and their most component sentiment words are the same. This phenomenon is called sentiment matching which is represented via a document-word contri-bution matrix as shown in Figure 1(b). Motivated by this observation, Yu et al. first propose an optimization-based method for SWI [4]. It utilizes the sentiment labels of doc-uments instead of seed words, but it ignores the semantic association in context.

Intuitively, two frequently co-occur sentiment words are more likely to have similar sentiment than those of two ran-domly selected words. In principle, a positive sentimen-t word occurs more frequently alongside positive words in positive documents, whereas negative sentiment words will occur most often in the vicinity of negative words in neg-ative documents. We call the above semantic association phenomenons as sentiment consistency. Inspired by this ob-servation, we explore the utilization of sentiment consistency information to facilitate SWI. In Figure 1(c), these informa-tion can be represented via a word-word influence matrix. Traditional methods do not utilize it.

In this paper, we propose a matrix factorization frame-work called CONR without seed words for SWI. In par-ticular, we first construct the document-word contribution matrix and word-word influence matrix. Then we discuss how these relations could be modeled and utilized for SWI. Finally, we conduct extensive experiments on four publicly available datasets to verify the proposed model.
To the best of our knowledge, this paper is the first work that identifies sentiment words using matrix factorization without seed words.
In this section, we present the notations and then formal-Figure 1: Subjective document Representation and Sentiment Word Influence Information ly define the problem of sentiment words identification in the context of matrix factorization. We use D = { d 1 ,d 2 ,  X  X  X  to denote a set of m subjective documents. L = { l i } m i denoted as the corresponding sentiment label set. If d i is a positive document, then l i = +1; otherwise l i =  X  1. The vocabulary index is denoted by W = { w 1 ,w 2 ,  X  X  X  ,w n } define an m  X  n matrix I to indicate the associations between documents and words. I ij isequalto1if w j  X  d i and equal to0otherwise. R  X  R m,n is defined as the document-word contribution matrix that describes n candidate words X  X  nu-merical contributions on m documents in sentiment view. Influence matrix is denoted by X  X  R n,n which depicts the relations between words.

With the notations above, our task in this paper can be described as: given a corpus G = { D, W,L } ,weaimto recognize and assign sentiment labels for candidate words based on R and X .
In this section, we convert sentiment word identification problem to predict the unobserved entries in R based on the observed statistics with matrix factorization algorithm. We use the labeled documents to label candidate words with semantic association information.

Intuitively, words with high frequencies are more impor-tant than the ones with low frequencies in a subjective doc-ument; and if a word only occurs in positive or negative documents, it tends to hold a high sentiment strength and contributes a lot to subjective documents. Based on these intuitions, we define R as follows: where F ( i ) ( j ) is the word frequency of w j in d i , h pus and F ( neg ) ( j ) is the frequency that w j occurs in negative corpus.

A low-rank matrix factorization approach seeks to an ap-proximate R ( R  X  R ) by minimizing where U  X  R k,m is latent document feature matrix, V  X  R k,n is latent candidate word feature matrix, k&lt;min ( m, n )and  X , X  &gt; 0. Sigular value decomposition (SVD) [5] and non-negative matrix factorization (NMF) [6] are often used to find a local minimum.

As mentioned in Section 1, two frequently co-occur can-didate sentiment words tend to hold the same polarity. In-spired by this observation, We define co-occurrence neighbor as follows.
 Definition 1: Co-Occurrence Neighbor : Two words in a document are considered as co-occurrence neighbor. For w i K ( i ) is denoted to its neighbors.

By Definition 1, X ij =0 indicates that w i has never co-occurred with w j in the corpus. A larger value corresponds to more frequent co-occurrence and stronger influence from w i to w j . So, we calculate X ij by where S ( i, j ) is the similarity function to indicate the sim-ilarity between w i and w j . Obviously, the more similar w to w j , the bigger influence on w j . We use point mutual information to depict the similarity between w i and w j .
Next, we explore the utilization of co-occurrence neigh-borhood information to regularize the learned model. For computational convenience, we assume that all these neigh-bors are linear i.e. each candidate word can be optimally approximated using a linear combination of its neighbors. Then we change the optimization function to where  X &gt; 0 and the second term is a co-occurrence neigh-borhood regularization which is used to fit the influences from neighbors to V j . Factually, V j is more influenced by the neighbors with which frequently co-occur. So we can change this regularization term to where X jk allows the term to treat co-occurrence neighbors differently. Thus, our matrix factorization model with co-occurrence neighbor regularization can be formulated as: We use gradient descent method to search the solution. After that, we can get the semantic orientation of w j by Here, N (+) represents the positive documents in the corpus while N (  X  ) represents the negative ones. w j is classified as having a positive semantic orientation when  X  j &gt; 0anda negative orientation when  X  j &lt; 0. The magnitude (absolute value) considers the strength of the semantic orientation. Larger values correspond to stronger semantic orientation.
In this section, subsets of four publicly available dataset-s are employed. The first dataset is the Internet Movie Database (IMDB) 1 . The second dataset we employ for e-valuation is the movie reviews 2 . The third one is DVD reviews from NLP&amp;CC2013 3 . And the last one is com-puter reviews 4 in Chinese. Sentiment words are generated based on MPQA subjective lexicon 5 and HowNet 6 .Toin-crease the difficulty, a comparable amount of non-sentiment words are added for testing. So our first task is subjec-tivity detection which distinguishes sentiment words from http://ai.stanfor.edu/amaas/data/sentiment/ http://www.cs.cornell.edu/people/pabo/movie-review-data/ http://www.datatang.com/data/44115/ http://www.searchforum.org.cn/tansongbo/ http://mpqa.cs.pitt.edu/ http://www.keenage.com/ non-sentiment ones and the second task is polarity classifi-cation that assigns a polarity label to each sentiment word. Detailed statistics of datasets are summarized in Table 1.
In face of the long lists of recommended polarity words, people are only concerned about the top-ranked words with the highest sentiment values [4]. So we consider the precision of the top K ranked sentiment words in our experiments:
For a ranked sequence of words, it is desirable to consider the order. For this purpose, we introduce mean average precision (MAP) which are commonly used in information retrieval to evaluate the results. Average precision (AP) is defined as: where f(k) is the indicator function that is equal to 1 if the k th word is a right word and equal to 0 otherwise. We average the AP@K values of the positive sequence and the negative sequence to get MAP@K.

We consider P 1 @ K and MAP 1 @ K as the measurements for subjectivity detection in which right words are the words that identified as sentiment words. And we use P 2 @ K and MAP 2 @ K for polarity classification where rights words are these assigned correct sentiment labels. For example, if the top 3 words for the positive sequence are { positive, non-sentiment, negative } , P 1 @3, P 2 @3, AP 1 @3, AP 2 @3 values are { 2/3, 1/3, (1/1+2/3)/3, (1/1)/3 } respectively.
In order to show the effectiveness of our proposed senti-ment words identification model CONR, we compare the i-dentification results with four baselines, SO-PMI[2], WEED[4], SVD[5] and NMF[6]. SO-PMI needs seed words, so we ran-domly select 20% seeds for it.

The P 1 @ K and P 2 @ K results are shown in Table 2, which are the average scores of the top ranked positive and negative words lists with k=10. By comparing the results of different methods, we can draw the following conclusions: (1)Compared with the optimization-based method WEED, CONR achieves consistently better performances on all four datasets with different K. In particular, CONR is 2.65%-17.83% higher than the state-of-the-art method WEED on Computer dataset. (2)The matrix factorization based methods SVD and N-MF outperform SO-PMI. This shows that the sentiment matching factor has positive impacts on the sentiment word identification. It is also noted that CONR outperforms SVD and NMF baselines. The highest improvement with respect to SVD and NMF are obtained on the Moive dataset when the K is 10. The experiment results demonstrate that, by ex-ploiting co-occurrence neighbor information, CONR is able to achieve significant improvement. (3) We also notice totally different characteristics of these four datasets. The results generated by all the methods on IMDB and DVD datasets are much higher than those on Moive and Computer dataset. This indicates that maybe any two sentiment words frequently occur together in the former two datasets.
 Figure 2 shows the final results of MAP 1 @ K and MAP 2 @ K . We can draw some similar conclusions as above. It should be also noted that MAP @ K values are lower than P @ K . It is because that the order is considered in MAP @ K .
In addition, as the size of K increases, P @ K and MAP @ K of all methods falls accordingly. It means that all methods can rank the the most probable sentiment words in the front of the word list.
In this paper, we introduce a matrix factorization mod-el CONR for SWI. In CONR, we not only consider the document-word contribution matrix but also incorporate co-occurrence neighbor information of the sentiment words. To the best of our knowledge, this paper is the first work that identifies sentiment words using matrix factorization frame-work without seed words. Experimental results shows that CONR outperforms the state-of-the-art optimization-based model.
 This work was supported by Strategic Priority Research Pro-gram of Chinese Academy of Sciences (XDA06030600) and National Nature Science Fo undati on of China (No. 61202226). [1] G.Qiu,B.Liu,J.JBu,Ch.Chen.Expandingdomain sentiment lexicon through double propagation. In
Proceedings of IJCAI, pages 1199-1204,2009. [2] P.D. Turney, M.L. Littman. Measuring praise and criticism: Inference of semantic orientation from association. ACM Transactions on Information Systems, 21(4): 315-346, 2003. [3] A.L. Mass. R.E. Daly, et al.. Learning word vectors for sentiment analysis. In Proceedings of ACL, 2011. [4] H. Yu, Z. Deng, S. Li. Identifying sentiment words using an optimization-based model without seed words.
In Proceedings of ACL, pages 855-859, 2013. [5] G.H. Golub, C. Reinsch. Singular value decomposition and least squares solutions. Numerische Mathematik, 14(5): 403-420, 1970. [6] D.D. Lee, H.S. Seung. Learning the parts of objects by non-negative matrix factorization. Nature, 401(6755): 788-791, 1999. [7] J. Bross, H. Ehrig. Automatic construction of domain and aspect specific sentiment lexicons for customer review mining. In Proceedings of CIKM, pages 1077-1086, 2013. [8] Y. Lu, M. Castellanos et al.. Automatic construction of a context-aware sentiment lexicon: An optimization approach. In Proceedings of WWW, pages 347-356, 2011.
