 @cs.columb ia.edu The goal of single document text or speech sum-marization is to identify information from a text or spok en document that summarizes, or con veys the essence of a document. E XTRACTIVE SUM -MARIZATION identifies portions of the original doc-ument and concatenates these segments to form a summary . Ho w these segments are selected is thus critical to the summarization adequac y.

Man y classifier -based methods have been exam-ined for extracti ve summarization of text and of speech (Mask ey and Hirschber g, 2005; Christensen et. al., 2004; Kupiec et. al., 1995). These ap-proaches attempt to classify segments as to whether the y should or should not be included in a summary . Ho we ver, the classifiers used in these methods im-plicitly assume that the posterior probability for the inclusion of a sentence in the summary is only de-pendent on the observ ations for that sentence, and is not affected by pre vious decisions. Some of these (K upiec et. al., 1995; Mask ey and Hirschber g, 2005) also assume that the features themselv es are inde-pendent. Such an independence assumption simpli-fies the training procedure of the models, but it does not appear to model the factors human beings appear to use in generating summaries. In particular , human summarizers seem to tak e pre vious decisions into account when deciding if a sentence in the source document should be in the document X  s summary . In this paper , we examine a Hidden Mark ov Model (HMM) approach to the selection of seg-ments to be included in a summary that we belie ve better models the interaction between extracted seg-ments and their features, for the domain of Broad-cast Ne ws (BN). In Section 2 we describe related work on the use of HMMs in summarization. We present our own approach in Section 3 and discuss our results in Section 3.1. We conclude in Section 5 and discuss future research. Most speech summarization systems (Christensen et. al., 2004; Hori et. al., 2002; Zechner , 2001) use lexical features deri ved from human or Automatic Speech Recognition (ASR) transcripts as features to select words or sentences to be included in a sum-mary . Ho we ver, human transcripts are not gener -ally available for spok en documents, and ASR tran-scripts are errorful. So, lexical features have prac-tical limits as a means of choosing important seg-ments for summarization. Other research efforts have focussed on text-independent approaches to ex-tracti ve summarization (Ohtak e et. al., 2003), which rely upon acoustic/prosodic cues. Ho we ver, none of these efforts allo w for the conte xt-dependence of extracti ve summarization, such that the inclusion of one word or sentence in a summary depends upon prior selection decisions. While HMMs are used in man y language processing tasks, the y have not been emplo yed frequently in summarization. A signifi-cant exception is the work of Conro y and O X  X eary (2001), which emplo ys an HMM model with pivoted QR decomposition for text summarization. Ho w-ever, the structure of their model is constrained by identifying a fix ed number of  X  X ead X  sentences to be extracted for a summary . In the work we present belo w, we introduce a new HMM approach to ex-tracti ve summarization which addresses some of the deficiencies of work done to date. We define our HMM by the follo wing parameters:  X  = 1 ..N : The state space, representing a set of states where N is the total number of states in the model; O = o vation vectors, where each vector is of size k ; A = { a is the probability of transition from state i to state j ; b ( o jk ) : The observ ation probability density func-tion, estimated by  X  M o notes a single Gaussian density function with mean of  X  with M the number of mixture components and c the weight of the k th mixture component;  X  =  X  The initial state probability distrib ution. For con ve-nience, we define the parameters for our HMM by a set  X  that represents A , B and  X  . We can use the parameter set  X  to evaluate P ( O |  X  ) , i.e. to measure the maximum lik elihood performance of the output observ ables O . In order to evaluate P ( O |  X  ) , how-ever, we first need to compute the probabilities in the matrices in the parameter set  X 
The Mark ov assumption that state durations have a geometric distrib ution defined by the probability of self transitions mak es it dif ficult to model dura-tions in an HMM. If we introduce an explicit du-ration probability to replace self transition proba-bilities, the Mark ov assumption no longer holds. Yet, HMMs have been extended by defining state duration distrib utions called Hidden Semi-Mark ov Model (HSMM) that has been succesfully used (Tweed et. al., 2005). Similar to (Tweed et. al., 2005) X  s use of HSMMs, we want to model the po-sition of a sentence in the source document explic-itly . But instead of building an HSMM, we model this positional information by building our position-sensiti ve HMM in the follo wing way:
We first discretize the position feature into L num-ber of bins, where the number of sentences in each bin is proportional to the length of the document. We build 2 states for each bin where the second state models the probability of the sentence being included in the document X  s summary and the other models the exclusion probability . Hence, for L bins we have 2 L states. For any bin lth where 2 l and 2 l  X  1 are the corresponding states, we remo ve all transitions from these states to other states except 2( l +1) and 2( l +1)  X  1 . This con verts our ergodic L state HMM to an almost Left-to-Right HMM though l states can go back to l  X  1 . This models sentence position in that decisions at the lth state can be ar-rived at only after decisions at the ( l  X  1) th state have been made. For example, if we discretize sen-tence position in document into 10 bins, such that 10% of sentences in the document fall into each bin, then states 13 and 14, corresponding to the seventh bin (.i.e. all positions between 0.6 to 0.7 of the text) can be reached only from states 11, 12, 13 and 14.
The topology of our HMM is sho wn in Figure 1. 3.1 Featur es and Training We trained and tested our model on a portion of the TDT -2 corpus pre viously used in (Mask ey and Hirschber g, 2005). This subset includes 216 stories from 20 CNN sho ws, comprising 10 hours of audio data and corresponding manual transcript. An an-notator generated a summary for each story by ex-tracting sentences. While we thus rely upon human-identified sentence boundaries, automatic sentence detection procedures have been found to perform with reasonable accurac y compared to human per -formance (Shriber g et. al., 2000).

For these experiments, we extracted only acous-tic/prosodic features from the corpus. The intu-ition behind using acoustic/prosodic features for speech summarization is based on research in speech prosody (Hirschber g, 2002) that humans use acous-tic/prosodic variation  X  expanded pitch range, greater intensity , and timing variation  X  to indi-cate the importance of particular segments of their speech. In BN, we note that a change in pitch, am-plitude or speaking rate may signal dif ferences in the relati ve importance of the speech segments pro-duced by anchors and reporters  X  the professional speak ers in our corpus. There is also considerable evidence that topic shift is mark ed by changes in pitch, intensity , speaking rate and duration of pause (Shriber g et. al., 2000), and new topics or stories in BN are often introduced with content-laden sen-tences which, in turn, often are included in story summaries.

Our acoustic feature-set consists of 12 features, similar to those used in (Inoue et. al., 2004; Chris-tensen et. al., 2004; Mask ey and Hirschber g, 2005). It includes speaking rate (the ratio of voiced / total frames); F0 minimum , maximum , and mean ; F0 range and slope ; minimum, maximum , and mean RMS ener gy (minDB, maxDB, meanDB); RMS slope (slopeDB); sentence duration (timeLen = endtime -starttime). We extract these features by automatically aligning the annotated manual tran-scripts with the audio source. We then emplo y Praat (Boersma, 2001) to extract the features from the audio and produce normalized and raw versions of each. Normalized features were produced by divid-ing each feature by the average of the feature values for each speak er, where speak er identify was deter -mined from the Dragon speak er segmentation of the TDT -2 corpus. In general, the normalized acoustic features performed better than the raw values.
We used 197 stories from this labeled corpus to train our HMM. We computed the transition proba-bilities for the matrix A ative frequenc y of the transitions made from each state to the other valid states. We had to compute four transition probabilities for each state, i.e. a where j = i, i + 1 , i + 2 , i + 3 if i is odd and j = i  X  1 , i, i + 1 , i + 2 if i is even. Odd states signify that the sentence should not be included in the summary , while even states signify sentence in-clusion. Observ ation probabilities were estimated using a mixture of Gaussians where the number of mixtures was 12. We computed a 12 X 1 matrix for the mean  X  and 12 X 12 matrices for the covariance matrix  X  for each state. We then computed the max-imum lik elihood estimates and found the optimal sequence of states to predict the selection of docu-ment summaries using the Viterbi algorithm. This approach maximizes the probability of inclusion of sentences at each stage incrementally . We tested our resulting model on a held-out test set of 19 stories. For each sentence in the test set we ex-tracted the 12 acoustic/prosodic features. We built a 12 XN matrix using these features for N sentences in the story where N was the total length of the story . We then computed the optimal sequence of sentences to include in the summary by decoding our sentence state lattice using the Viterbi algorithm. For all the even states in this sequence we extracted the corresponding segments and concatenated them to produce the summary .

Ev aluating summarizers is a dif ficult problem, since there is great disagreement between humans over what should be included in a summary . Speech summaries are even harder to evaluate because most objecti ve evaluation metrics are based on word over-lap. The metric we will use here is the standard information retrie val measure of Precision, Recall and F-measure on sentences. This is a strict met-ric, since it requires exact matching with sentences in the human summary; we are penalized if we iden-tify sentences similar in meaning but not identical to the gold standard.

We first computed the F-measure of a baseline system which randomly extracts sentences for the summary; this method produces an F-measure of 0.24. To determine whether the positional informa-tion captured in our position-sensiti ve HMM model was useful, we first built a 2-state HMM that models only inclusion/e xclusion of sentences from a sum-mary , without modeling sentence position in the document. We trained this HMM on the train-ing corpus described abo ve. We then trained a position-sensiti ve HMM by first discretizing posi-tion into 4 bins, such that each bin includes one-quarter of the sentences in the story . We built an 8-state HMM that captures this positional informa-tion. We tested both on our held-out test set. Re-sults are sho wn in Table 1. Note that recall for the 8-state position-sensiti ve HMM is 16% better than recall for the 2-state HMM, although precision for the 2-state model is slightly (1%) better than for the 8-state model. The F-measure for the 8-state position-sensiti ve model represents a slight im-pro vement over the 2-state model, of 1%. These re-sults are encouraging, since, in skewed datasets lik e documents with their summaries, only a few sen-tences from a document are usually included in the summary; thus, recall is generally more important than precision in extracti ve summarization. And, compared to the baseline, the position-sensiti ve 8-state HMM obtains an F-measure of 0.41, which is 17% higher than the baseline.
 ModelT ype Precision Recall F-Meas HMM-8state 0.26 0.95 0.41 HMM-2state 0.27 0.79 0.40 Baseline 0.23 0.24 0.24 We have sho wn a novel way of using continuous HMMs for summarizing speech documents without using any lexical information. Our model generates an optimal summary by decoding the state lattice, where states represent whether a sentence should be included in the summary or not. This model is able to tak e the conte xt and the pre vious decisions into account generating better summaries. Our re-sults also sho w that speech can be summarized fairly well using acoustic/prosodic features alone, without lexical features, suggesting that the effect of ASR transcription errors on summarization may be mini-mized by techniques such as ours. We would lik e to thank Yang Liu, Michel Galle y and Fadi Biadsy for helpful comments. This work was funded in part by the DARP A GALE program under a subcontract to SRI International.

