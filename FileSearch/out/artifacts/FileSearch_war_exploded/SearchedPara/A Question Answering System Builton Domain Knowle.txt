 Question answering is a long-standing problem. Nowadays, with the development of intelligent devices, many business question answering systems are in great use in our everyday life. Siri, a widely-used business question answering system, can handle many types of questions such as  X  X ow is the weather like today? X  and  X  X all [ somebody ] X . Although these business question answering systems perform well in such short and single questions, they have the following two major limita-tions: First, these systems can only deal with simple or pattern-fixed questions. Once a complex question is raised, they will lead user to a search engine instead of providing any answers. Second, dialog management has received less attention in real-world QA systems. They treat input questions as stand-alone ones while questions are often related to dialogs.
 ing on a wide variety of domain questions and being able to hold the dialog history. In this paper, we developed a general framework for question answering systems which takes only a domain knowledge base as input. For a given domain knowledge base, a semantic parser is generated from it to parse the user ques-tions to logical forms, and answer candidates are extracted from the knowledge base by the queries translated from the logical forms. Dialog management, a key component of the framework, collects all user-computer conversations and generates dialog strategies for more accurate answer prompt.
 Accurate question analysis and smart dialog management are two challenging technique issues in our framework development. Where, a good question analyzer is expected to parse entities and their relations from users questions accurately, And it should have a good capability in extension to handle complex questions such as those questions with reasoning. A smart dialog manager is expected to use the conversation contexts for quick identification of users concerns when handling the ambiguous questions.
 In summary, we made the following contributions in this study:  X  We proposed a novel framework for question answering system with dialog  X  We developed a method to build a domain-specific semantic parser from  X  We developed a general dialog management method with capability of The rest of this paper is organized as follows: Section 2 reviews related work. Section 3 provides an overview of our proposed framework. Section 4 and Section 5 describe more details on semantic parsing and dialog management in our frame-work. Section 6 analyzes the performance of a health domain QA system built in our framework. Section 7 concludes. 2.1 Semantic Parsing Semantic parsing targets on parsing natural language questions into logical forms. There are usually two major levels for a semantic parser. At the lexi-cal level, a semantic parser maps natural language phrases to logical predicates in knowledge base. In a limited domain, a lexicon can be learned from annotated corpus [ 1 , 2 ]. At the compositional level, the predicates are combined into a logic form of the question. Pattern based approaches use dependency pattern [ 3 ]or question pattern generated from paraphrase[ 4 ]. Rule-based approaches requires combination rules which are specified manually [ 5 , 6 ] or learned from annotated logical forms [ 7 ]. 2.2 Dialog Management The main task of dialog manager is to give the user reasonable responses accord-based methods to manage the dialog. In these approaches, tasks are described as frames or topic trees which have to be written manually. Latest approaches we implement general strategies by introducing a history graph and domain-independent inference algorithms and then leave the high-level strategies to the business logic which will be implemented system-specific. 3.1 Knowledge Base The main input of our framework is a knowledge base K =( E, R, C,  X  ), in which E represents the set of entities, R the relations, C the categories and  X  the knowledge instances which are triples of two entities and the relations between them. In our approach, we require that every entity in labeled. Figure 1 shows the schema and some sample instances of our knowledge base. 3.2 Logical Form Following the study of [ 12 ], we use another graph QG q called question graph to represent the semantic of a given question q . Every node (or edge) in QG represents an entity (or relation) mentioned in q and it must match with an concept in the underlying knowledge base. QG q also has to meet the type con-straints of the ontology: every two nodes n and n in QG q an edge e only if type n / type n satisfies the domain/range of r . 3.3 Problem Statement Given a knowledge base K , our goal is to generate a semantic parser which can map natural language questions to their logical forms which are semantically equivalent to them. And within every session, we maintain the dialog history in our framework. When we find that there are missing or ambiguous conditions in a question, we use the history to complete or disambiguate it before generating the knowledge base query. 3.4 Framework The architecture of our framework is shown as Figure 2 . A question q in natural language is fed into the following pipeline: Firstly, semantic parser maps q to its logical form (i.e., a question graph). Then in graph injection step, history graph and other constraints from business logic are injected if the question graph is unsolvable with its conditions. After that query generation translates the logical form to SPARQL query. Knowledge base executes the query and returns the answer as an answer graph. Finally, the answer graph is pushed back into the history graph and all nodes in it are re-weighted for future injection. Business logic translates the answer graph into a natural language sentence with some rules. Business logic also executes the domain specific rules in dialog management, e.g. asking for more information when there are too many candidate answers , handling the user X  X  response after a system-ask round or providing other constraints for the next round of dialog. The main goals of semantic parsing are finding the best logical form for a natural language question and generating the knowledge base query. In this approach, we generate a context-free grammar from the knowledge base (Section 4.1 ). By using a context-free grammar, we can parse questions with reasoning which are incapable for pattern-based methods that are widely used in real-world dialog systems. With the grammar, we can generate a derivation d question q . Then the derivation d q is converted to the question graph QG a SPARQL query which can be executed by knowledge base (Section 4.2 ). Figure 3 shows an example parsing of a question contains an one-step reasoning. 4.1 Parser Generation In this section, we discuss the construction of a context-free grammar With this grammar, we can generate derivations for questions. Figure 3a shows the derivation of a question with an one-step reasoning.
 Entities and Categories. For each entity e  X  E K , it generates production rules { URI e  X  l | X  l  X  label ( e ) } . And for each category c a set of production rules as { URI c  X  URI e | X  e : type e also has labels, which are phrases in natural language that represent an object in this category. For example, the category Medicine may be represented by  X  X edicine X  or  X  X rug X . So for a category c  X  C K , { URI c added to G . In Figure 3a , for example, Type:Disease produces the non-terminal character URI:Cold , URI:Cold produces the terminal character  X  (Cold) X  and Type:Medicine produces its label phrase  X  (Medicine) X .
 Relations. For a binary relation r  X  R K , if label of r occurs in the sentence it combines two part which are adjacent to it and generates a higher-level semantic category. Table 1 shows all production rules generated for a given relation r . domain ( r )and range ( r ) are both categories in C K . Since we cannot know which one between domain ( r )and range ( r ) is the semantic subject, so rules for both domain ( r )and range ( r ) are generated. This strategy will not cause disambigua-tion in derivation because higher level reduction will accept the right category. expressed weakly or implicitly [ 6 ]. For example, the relation is omitted in  X  by phrases in these questions, we add bridging rules of relations to ing components which are actually implicit references to previous conversation. For example, in a medical diagnose system, user may input the question  X  Relation disease:medication appears in the question but one of its argument Disease is missing. Slot rules to G to handle these incomplete questions. Lexicon Extension. In grammar G , we use labels of concepts to map natural language phrases to knowledge base concepts. To avoid ambiguity, we only use entities X  labels and aliases in knowledge base. But to cover various utterances, we extend the lexicon of relations by alignment of a large text corpus to the knowledge base. Intuitively, a phrase and a relation align if they share many of the same entity arguments.
 We use 25 million questions from Baidu Zhidao 1 and replace the name phrases in questions by simple string matching with knowledge base entities. For those questions which have exactly 2 entities in it, we add the entity pair to the context set of the text between them. For example, the sentence is  X  (Cold use aspirin) X , ( Cold , Aspirin ) is added to Context ( X  (use) X ). The context of a relation r is defined by the triples in knowledge base, Context ( r )= { ( s, o ) | ( s, r, o )  X  X } . Then we calculate the overlap between the contexts of phrases and relations. The phrase p is added to the label set of relation r if |
Context ( p )  X  Context ( r ) | &gt; X  , where  X  is a manual-specified threshold. 4.2 Question Graph and SPARQL Construction We apply our parser on a question and the derivation which has the most cov-erage on question words is selected as the best derivation. With this derivation, a question graph that represents the semantic of the question can be gener-ated. Specificlly, we make a bottom-up traverse on the derivation to generate the question graph.
 node with a label is created. And for the category rule URI property is added to the blank node in graph. Until now there is only one node in each graph, so the subject node of the graph is the blank node itself. A subject node is the semantical center of the graph, it will be connected with other graphs in future.
 from relation r is applied, we connect the graphs generated by the sub-trees with the relation r on their subject nodes. A slot rule have only one sub-tree, so we will generate a blank node which marked as  X  X lot X  to the omitted side of the relation. The subject node of the new graph will be chosen from two old ones whose type matches the left-hand side of the rule.
 which is semantically equivalent to the question. Figure 3b shows the question graph generated from the derivation shown in Figure 3a .
 In final step, every edge in question graph and two nodes it connects will be mapped to one triple condition in the SPARQL query. Blank nodes are mapped to variables while nodes with URIs are added as is. Figure 3c shows the SPARQL query generated from the question graph shown in Figure 3b . In a dialog system, users X  questions are related to the contexts, so a major challenge of dialog system is to infer implicit references in questions. history graph is built totally from question graphs and answer graphs, the infer-ence algorithms are domain-independent. This framework can be used directly in any domain without extra efforts on transferring. 5.1 Answer Graph The results returned by knowledge base contains all possible solutions in which the variables are solved to entities in knowledge base. Since variables in SPARQL query are generated from blank node in question graph, we replace every blank node in the question graph with its corresponding solution to generate a new graph we call answer graph . Note that there might be more than one group of solution for a query. One answer graph will be generated for each group of solution. 5.2 History Graph and Push Back History graph maintains the history of the dialog. Every answer graph will be merged into history graph, and we call this procedure push back .
 Timeliness is important in the history, latest contexts are likely to be refer-enced while far contexts should be forgotten. In history graph, we add an extra property weight for every concept (entities and relations). Concepts that are just mentioned in answer graph will be given the highest weight (e.g. 1) while the weight of concepts not mentioned will decline by a coefficient. This weight will be used as the priority in graph injection step. 5.3 Graph Injection After at least one round of dialog, the history graph gets non-empty and can be used to infer implicit references or missing conditions in following questions. For the question graph has only one node b , these questions have no enough conditions to be solved. If the single node b does not have a label (Q2 in Figure 4a , its question graph shown as 4b ), we pick a relation  X  r whose domain (or range) matches type b with highest weight. Then we choose an entity  X  e that matches the range (or domain) of  X  r from the history graph to build a triple in question graph. In the worst case, there is no available  X  r meets the type requirement, we only pick an entity  X  e and traverse all relations in knowledge base relation whose domain and range match type  X  e and type b a label (Q3 in Figure 4a , question graph shown as 4c ), user may want to change a condition. So we pick a relation  X  r whose domain (or range) matches type highest weight. Then we complete the question graph by adding b to domain (or range) of  X  r and creating a blank node of range (or domain) of  X  r .Nowthe question graph have a complete triple as the condition.
 an entity which matches the type constraints of the slot node with the highest weight and replace the slot node in question graph with it.
 able query. Note that this graph injection algorithm needs neither hand-written frames or scripts nor extra knowledge about the task of the system. in a health diagnose system, when statements given by patient are not enough, the system can ask patient for more statements and send the answer of last round to the dialog manager as candidates. In graph injection step, answer candidates are injected to question graph as a filter. This interface is also useful in deal with user profile since user profile can also be represented as a profile graph and injected to the question graph. In this section we describe our experimental evaluation. For the semantic parser, natural language questions sampled from Internet are used as the input. We measure the precision of our semantic parser by compare the question graphs generated by the parser with the manually annotated question graph. For the dialog management, we test the behavior of it by some test cases. 6.1 Datasets and Setup As the test bed for evaluating our framework, we use a medical domain knowledge base in which entities are labeled in Chinese. It contains 16155 instances with 1093 diseases, 236 symptoms and 903 medicines.
 natural language questions from Baidu Zhidao. These questions are all about the categories and relations covered by our knowledge base, including disease, symptom, medicine, treatment and so on. The equivalent conjunctive SPARQL queries written manually are used as the golden standard of the semantic parser. whether the actions taken by the framework are correct. 6.2 Results and Discussion Semantic Parsing. Table 2 shows the results of semantic parsing on the test questions. In Table 2 we can see 56% (74 questions) are parsed correctly. Note that lexicon extension for relations significantly improves the performance by 8%. 28% (37 questions) are not parsed because the missing entities (ME). Entities are labeled by their scientific name in knowledge base while people usually using the popular names of them. There are also some misspelled names or traditional chinese medicine, which are totally not contained in our knowledge base. 4% (5 questions) have unrecognized categories or relations (UCR). For exam-ple, category Medicine has label  X  X edicine X  so it can parse the questions like  X  (What medicines to take for cold) X  but will failure on  X  4% (5 questions) are wrongly parsed (WP). Those caused by the ambitious parsing in derivations. For example,  X  (The cure effect to pinkeye by clarithromycin) X , our parser combined the  X  X ure X  and  X  X inkeye X  firstly and then the  X  X larithromycin X . These questions are described in an inverted order. This linguistic phenomenon is not covered by our rules so a wrong derivation is generated. 8% (11 questions) have structures that are not covered by the grammar (Out-of-Grammar). Grammar are generated strictly by the schema of knowledge base. But people often use implicit reasoning when they think the it is self-evident. For example, user may ask  X  (What are the medicines for headache) X .
 Headache is a Type:Symptom and it has no property medication . Type:Disease should be the bridge between Type:Symptom and Type:Medicine , but is omitted in the given question. Questions with aggregation functions are also not covered in our grammar.
 Dialog Management. In this section, we will use some test cases to evaluate our dialog management algorithm.
 The first case in Figure 5 shows a typical scenario that user X  X  question con-tains implicit references to the previous dialog: Q2 asks about medicines in A2; Q3 has the same question as Q1 but the subject changed; Q4 shares the same subject with Q3. Note that when Q2 is raised, there are two entities that can be injected to question graph ( X  X eserpine X  and  X  X igoxin X ). They both have the highest weight so both of them are injected. When Q4 is raised, there are also two entities that can be injected to question graph ( X  X ypertension X  and  X  X iabetes X ). As  X  X iabetes X  is mentioned recently so it has higher weight than  X  X ypertension X  and is chosen to be injected.
 Figure 6 shows a case when business logic takes the control. In the diagnose mode, users give their symptoms and the system tells the possible diseases. When there are too many results, the business logic will not show a long list but ask for further information and save the answer of this round (A1) for the candidate of next round (Q2). When there are just several answers left (A2), the business logic will pick up a decidable symptom and give yes-no questions for choosing. We presented a general framework for domain-specific question answering sys-tems with dialog management. With this framework, one can generate a domain-specific dialog system with only a domain knowledge base as input. In the frame-work, we designed a logical form to represent the semantic of questions and the history of conversations. And we proposed a method to generate a semantic parser from the knowledge base and extended it with web corpus. Then we achieve a general dialog management algorithm about inferring implicit refer-ences in questions. Our experiments showed that our framework has good per-formance in handling complex questions and dealing with multi-round dialog. question patterns in our grammar to handle more linguistic phenomenon such as aggregation; (2) collecting more labels or alias for entities and categories to increase the coverage of our parser; (3) creating more rules from the knowledge base to handle variety of questions. For dialog management, we will design more interface for business logic to implement more complex dialog control strategies that are useful in business systems.
