 ppantel@microsoft.com The process of using a pointing device to select a span of text has a long history dating back to the invention of the mouse. It serves to access functions on text spans, such as copying/pasting, looking up a word in a dictionary, searching the Web, or accessing other accelerators. As con-sumers move from traditional PCs to mobile de-vices (e.g., tablets and smartphones), touch inter-action is replacing the pointing devices of yore. Although more intuitive and arguably a more natu-ral form of interaction, touch offers much less acu-ity (colloquially referred to as the fat finger prob-lem). To select multi-word spans today, mobile devices require an intricate series of gestures that quently, there is an opportunity to reinvent the way users select text in such devices.

Our task is, given a single user touch, to pre-dict the span that the user likely intended to se-lect. We call this task smart selection . We re-strict our prediction task to cases where a user in-tends to perform research on a text span (dictio-nary/thesaurus lookup, translation, searching). We specifically consider operations on text spans that do not form a single unit (i.e., an entity, a concept, a topic, etc.) to be out of scope. For example, full sentences, paragraph and page fragments are out of scope.

Smart selection, as far as we know, is a new re-search problem. Yet there are many threads of re-search in the NLP community that identify multi-word sequences, which have coherent properties. For example, named-entity recognizers identify entities such as people/places/organizations, chun-kers and parsers identify syntactic constituents such as noun phrases, key phrase detectors or term segmentors identify term boundaries. While each of these techniques retrieve meaningful linguistic units, our problem is a semantic one of recovering a user X  X  intent , and as such none alone solves the entire smart selection problem.

In this paper, we model the problem of smart selection using an ensemble learning approach. We leverage various linguistic techniques, such as those discussed above, and augment them with other sources of information from a knowledge base and a web graph. We evaluate our meth-ods using a novel dataset constructed for our task. We construct our dataset of true user-intended selections by crowdsourcing the task of a user selecting spans of text in a researching task. We obtain 13,681 data points. For each in-tended selection, we construct test cases for each individual sub-word, simulating the user select-ing via touch. The resulting testset consists of 33,912  X  simulated selection , intended selection  X  -pairs, which we further stratify into head, torso, and tail subsets. We release the full dataset and testset to the academic community for further re-search on this new NLP task. Finally, we empir-ically show that our ensemble model significantly improves upon various baseline systems.

In summary, the major contributions of our re-search are:  X  We introduce a new natural language pro- X  We conduct a large crowd-sourced user study  X  We propose a machine-learned ensemble  X  We empirically show that our model can ef-Related work falls into three broad categories: lin-guistic unit detection, human computer interaction (HCI), and intent detection. 2.1 Linguistic Unit Detection Smart selection is closely related to the detection of syntactic and semantic units: user selections are often entities, noun phrases, or concepts. A first approach to solving smart selection is to select an entity, noun phrase, or concept that subsumes the user selection. However, no single approach alone can cover the entire smart selection problem. For example, consider an approach that uses a state-of-the-art named-entity recognizer (NER) (Chinchor, 1998; Tjong Kim Sang and De Meulder, 2003; Finkel et al., 2005; Ratinov and Roth, 2009). We found in our dataset (see Section 3.2) that only a quarter of what users intend to select consists in fact of named entities. Although an NER ap-proach can be very useful, it is certainly not suf-ficient. The remainder of the data can be partially addressed with noun phrase (NP) detectors (Ab-ney, 1991; Ramshaw and Marcus, 1995; Mu  X  noz et al., 1999; Kudo and Matsumoto, 2001) and lists of items in a knowledge base (KB), but again, each is not alone sufficient. NP detectors and KB-based methods are further very susceptible to the gen-eration of false positives (i.e., text contains many nested noun phrases and knowledge base items in-clude highly ambiguous terms).

In our work, we leverage all three techniques in order to benefit from their complementary cover-age of user selections. We further create a novel unit detector, called the hyperlink intent model. Based on the assumption that Wikipedia anchor texts are similar in nature to what users would se-lect in a researching task, it models the problem of recovering Wikipedia anchor texts from partial selections. 2.2 Human Computer Interaction There is a substantial amount of research in the HCI community on how to facilitate interaction of a user with touch and speech enabled devices. To give but a few examples of trends in this field, Gunawardana et al. (2010) address the fat finger problem in the use of soft keyboards on mobile de-vices, Kumar et al. (2012) explore a novel speech interaction paradigm for text entry, and Sakamoto et al. (2013) introduce a technique that combines touch and voice input on a mobile device for im-proved navigation of user interface elements such as commands and controls. To the best of our knowledge, however, the problem of smart selec-tion as we defined it has not been addressed. 2.3 Intent detection There is a long line of research in the web lit-erature on understanding user intent. The clos-est to smart selection is query recommendation (Baeza-Yates et al., 2005; Zhang and Nasraoui, 2006; Boldi et al., 2008), where the goal is to sug-gest queries that may be related to a user X  X  intent. Query recommendation techniques are based ei-ther on clustering queries by their co-clicked URL patterns (Baeza-Yates et al., 2005) or on leverag-ing co-occurrences of sequential queries in web search sessions (Zhang and Nasraoui, 2006; Boldi et al., 2008; Sadikov et al., 2010). The key dif-ference from smart selection is that in our task the output is a selection that is relevant to the context of the document where the original selection ap-pears (e.g., by adding terms neighboring the selec-tion). In query recommendation, however, there is no notion of a document being read by the user and, instead, the recommendations are based ex-clusively on the aggregation of behavior of multi-ple users. 3.1 Smart Selection Definition Let D be the set of all documents. We define a selection to be a character  X  offset , length  X  -tuple in a document d  X  D . Let S be the set of all possible selections in D and let S d be the set of all possible selections in d .

We define a scored smart selection,  X  , in a doc-ument d , as a pair  X  =  X  x,y  X  where x  X  S d is a selection and y  X  R + is a score for the selection.
We formally define the smart selection function  X  as producing a ranked scored list of all possi-ble selections from a document and user selection  X  : D  X  S  X  (  X  1 ,..., X  | S Consider a user who selects s in a document d . Let  X  be the target selection that best captures what the user intended to select. We define the smart selection task as recovering  X  given the pair  X  d,s  X  . Our problem then is to learn a function  X  that best recovers the target selection from any user selec-tion.

Note that even for a human, reconstructing an intended selection from a single word selection is not trivial. While there are some fairly clear cut cases such as expanding the selection  X  X bama X  to Barack Obama in the sentence  X  While in DC, Barack Obama met with...  X , there are cases where the user intention depends on extrinsic fac-tors such as the user X  X  interests. For example, in a phrase  X  University of California at Santa Cruz  X  with a selection  X  X alifornia X , some (albeit proba-bly few) users may indeed be interested in the state of California , others in the University of California system of universities, and yet others specifically in the University of California at Santa Cruz . In the next section, we describe how we obtained a dataset of true intended user selections. 3.2 Data In order to obtain a representative dataset for the smart selection task, we focus on a real-world ap-plication of users interacting with a touch-enabled e-reader device. In this application, a user is read-ing a book and chooses phrases for which she would like to get information from resources such as a dictionary, Wikipedia, or web search. Yet, be-cause of the touch interface, she may only touch on a single word. 3.2.1 Crowdsourced Intended Selections We obtain the intended selections through the fol-lowing crowdsourcing exercise. We use the en-tire collection of textbooks in English from Wik-books. The corpus consists of 2,696 textbooks that span a large variety of categories such as Comput-ing, Humanities, Science, etc. We first produce a uniform random sample of 100 books, and then sample one paragraph from each book. The result-ing set of 100 paragraphs is then sent to the crowd-sourcing system. Each paragraph is evaluated by 100 judges, using a pool of 152 judges. For each paragraph, we request the judges to select com-plete phrases for which they would like to  X  X earn more in resources such as Wikipedia, search en-gines and dictionaries X , i.e., our true user intended selections. As a result of this exercise, we obtain 13,681 judgments, corresponding to 4,067 unique intended selections. The distribution of number of unique judges who selected each unique intended selection, in a log-log scale, is shown in Figure 1. Notice that this is a Zipfian distribution since it follows a linear trend in the log-log scale.
Intuitively, the likelihood that a phrase is of interest to a user correlates with the number of judges who select that phrase. We thus use the number of judges who selected each phrase as a proxy for the likelihood that the phrase will be chosen by users.

The resulting dataset consists of 4,067  X  d, X   X  -pairs where d is a Wikibook document paragraph and  X  is an intended selection, along with the num-ber of judges who selected it. We further assigned Figure 1: Zipfian distribution of unique intended selections vs. the number of judges who selected them, in log-log scale. each pair to one of five randomly chosen folds, which are used for cross-validation experiments. 3.2.2 Testset Construction We define a test case as a triple  X  d,s, X   X  where s is a simulated user selection. For each  X  d, X   X  -pair in our dataset we construct n correspond-ing test cases by simulating the user selections { X  d, X ,s 1  X  ,...,  X  d, X ,s n  X  X  where s 1 ,...,s n corre-spond to the individual words in  X  . In other words, each word in  X  is considered as a candidate user selection.

We discard all target selections that only a sin-gle judge annotated since we observed that these mostly contained errors and noise, such as full sen-tences or nonsensical long sentence fragments.
Our first testset, labeled T ALL , is the resulting traffic-weighted multiset. That is, each test case  X  d,s, X   X  appears k times, where k is the number of judges who selected  X  in d . T ALL consists of 33,913 test cases.

We further utilize the distribution of judgments in the creation of three other testsets. Following the stratified sampling methodology commonly employed in the IR community, we construct testsets for the frequently, less frequently, and rarely annotated intended selections, which we call HEAD , TORSO , and TAIL , respectively. We obtain these testsets by first sorting each unique selection according to their frequency of occur-rence, and then partitioning the set so that HEAD corresponds to the elements at the top of the list that account for 20% of the judgments; TAIL cor-responds to the elements at the bottom also ac-counting for 20% of the judgments; and TORSO corresponds to the remaining elements. The re-sulting test sets, T HEAD , T TORSO , T TAIL consist of
Test sets along with fold assignments and annotation guidelines are avail-able at http://research.microsoft.com/en-us/downloads/eb42522c-068e-404c-b63f-cf632bd27344/. 3.3 Discussion Our focus on single word selections is motivated by the touchscreen scenario presented in Sec-tion 1. Although our touch simulation assumes that each word in a target selection is equally likely to be selected by a user, in fact we expect this dis-tribution to be non-uniform. For example, users may tend to select the first or last word more fre-quently than words in the middle of the target se-lection. Or perhaps users tend to select nouns and verbs more frequently than function words. We consider this out of scope for our paper, but view it as an important avenue of future investigation. Fi-nally, for non-touchscreen environments, such as the desktop case, it would also be interesting to study the problem on multi-word user selections.
To get an idea of the kind of intended selections that comprise our dataset, we broke them down ac-cording to whether they referred to named entities or not. Perhaps surprisingly, the fraction of named rest of the intended selections mostly correspond to concepts and topics such as embouchure forma-tion , vocal fold relaxation , NHS voucher values , time-domain graphs , etc. As argued in Section 1, existing techniques, such as NER taggers, chunkers, Knowledge Base lookup, etc., are geared towards aspects of the task (i.e., NEs, concepts, KB entries), but not the task as a whole. We can, however, combine the outputs of these systems with a learned  X  X eta-model X . The meta-model ranks the combined can-didates according to a criterion that is derived from data that resembles real usage of smart selection as closely as possible. This technique is known in the machine learning community as ensemble learning (Dietterich, 1997).

Our ensemble approach, described in this sec-tion, serves as our main implementation of the smart selection function  X  of Equation 1. Each of the ensemble members are themselves a separate implementation of  X  and will be used as a point of comparison in our experiments. Below, we de-scribe the ensemble members before turning to the ensemble learner. 4.1 Ensemble Members 4.1.1 Hyperlink Intent Model The Hyperlink Intent Model ( HIM ), which lever-ages web graph information, is a machine-learned system based on the intuition that anchor texts in Wikipedia are good representations of what users might want to learn about. We build upon the fact that Wikipedia editors write anchor texts for enti-ties, concepts, and things of potential interest for follow-up to other content. HIM learns to recover anchor texts from their single word subselections.
Specifically, HIM iteratively decides whether to expand the current selection (initially a single word) one word to the left or right via greedy bi-nary decisions, until a stopping condition is met. At each step, two binary classifiers are consulted. The first one scores the left expansion decision and the second one scores the right expansion de-cision. In addition, we use the same two classi-fiers to evaluate the expansion decision  X  X rom the outside in X , i.e., from the word next to the current selection (left and right, respectively) to the clos-est word in the current selection. If the probabil-ity for expansion of any model exceeds a prede-fined threshold, then the most probable expansion is chosen and we continue the iteration with the newly expanded selection as input. The algorithm is illustrated in Figure 2.

We automatically create our training set for HIM by first taking a random sample of 8K Wikipedia anchor texts. We treat each anchor text as an in-tended selection, and each word in the anchor text as a simulated user selection. For each word to the left (or the right) of the user selection that is part of the anchor text, we create a positive training ex-ample. Similarly, for each word to the left (or the right) that is outside of the anchor text, we create a negative training example. We include additional negative examples using random word selections from Wikipedia content. For this purpose we sam-Figure 2: Hyperlink Intent Model ( HIM ) decoding flow for smart selection. ple random words that are not part of an anchor text. Our final data consists of 2.6M data points,
We use logistic regression as the classification algorithm for our binary classifiers. The fea-tures used by each model are computed over three strings: the current selection s (initially the single-word simulated user selection), the candidate ex-pansion word w , and one word over from the right or left of s . The features fall into five fea-ture families: (1) character-level features , includ-ing capitalization, all-cap formatting, character length, presence of opening/closing parentheses, presence and position of digits and non-alphabetic characters, and minimum and average character uni/bi/trigram frequencies (based on frequency ta-bles computed offline from Wikipedia article con-tent); (2) stopword features , which indicate the presence of a stop word (from a stop word list); (3) tf.idf scores precomputed from Wikipedia con-tent statistics; (4) knowledge base features , which indicate whether a string matches an item or a sub-string of an item in the knowledge base described in Section 4.1.2 below; and (5) lexical features , which capture the actual string of the current se-lection and the candidate expansion word. 4.1.2 Unit Spotting Our second qualitative class of ensemble members use notions of unit that are either based on linguis-tic constituency or knowledge base presence. The general process is that any unit that subsumes the user selection is treated as a smart selection can-didate. Scoring of candidates is by normalized length, under the assumption that in general the most specific (longest) unit is more likely to be the intended selection.
Our first unit spotter, labeled NER is geared towards recognizing named entities. We use a commercial and proprietary state-of-the-art NER system, trained using the perceptron algo-rithm (Collins, 2002) over more than a million hand-annotated labels.

Our second approach uses purely syntactic in-formation and treats noun phrases as units. We la-bel this model as NP . For this purpose we parse the sentence containing the user selection with a syn-tactic parser following (Ratnaparkhi, 1999). We then treat every noun phrase that subsumes the user selection as a candidate smart selection.
Finally, our third unit spotter, labeled KB , is based on the assumption that concepts and other entries in a knowledge base are, by nature, things that can be of interest to people. For our knowl-edge base lookup, we use a proprietary graph con-sisting of knowledge from Wikipedia, Freebase, and paid feeds from various providers from do-mains such as entertainment, local, and finance. 4.1.3 Heuristics Our third family of ensemble members imple-ments simple heuristics, which tend to be high pre-cision especially in the HEAD of our data.

The first heuristic, representing the current touch-enabled selection paradigm seen in many of today X  X  tablets and smartphones, is labeled CUR . It simply assumes that the intended selection is al-ways the user-selected word.

The second is a capitalization-based heuristic ( CAP ), which simply expands every selected capi-talized word selection to the longest uninterrupted sequence of capitalized words. 4.2 Ensemble Learning In this section, we describe how we train our meta-learner, labeled ENS , which takes as input the can-didate lists produced by the ensemble members from Section 4.1, and scores each candidate, pro-ducing a final scored ranked list.

We use logistic regression as a classification al-gorithm to address this task. Our 22 features in ENS consist of three main classes: (1) features related to the individual ensemble members; (2) features related to the user selection; and (3) fea-tures related to the candidate smart selection. For (1), the features consist of whether a particular ensemble member generated the candidate smart selection and its score for that candidate. If the candidate smart selection is not in the candidate list of an ensemble member, its score is set to zero. For both (2) and (3), features account for length and capitalization properties of the user se-lection and the candidate smart selection (e.g., to-ken length, ratio of capitalized tokens, ratio of cap-italized characters, whether or not the first and last tokens are capitalized.)
Although training data for the HIM model was automatically generated from Wikipedia, for ENS we desire training data that reflects the true ex-pected user experience. For this, we use five-fold cross-validation over our data collection de-scribed in Section 3.2. That is, to decode a fold with our meta-learner, we train ENS with the other four folds. Note that every candidate selection for a  X  document , user selection  X  -pair,  X  d,s  X  , for the same d and s , are assigned to a single fold, hence the training process does not see any user selection from the test set. 5.1 Experimental Setup from Section 3.2.2, where a test case is defined as a triple  X  d,s, X   X  , and where d is a document, s is a user selection, and  X  is the intended user selection. In this section, we describe our evaluation metric and summarize the system configurations that we evaluate. 5.1.1 Metric In our evaluation, we apply the smart selection function  X  ( d,s ) (see Eq. 1) to each test case and measure how well it recovers  X  .

Let A be the set of  X  d, X   X  -pairs from our dataset described in Section 3.2.1 that corresponds to a testset T . Let T  X  d, X   X  be the set of all test cases in T with a fixed d and  X  . We define the macro precision of a smart selection function, P  X  , as fol-lows:
P  X  ( d, X  ) = Table 1: Smart selection performance, as a func-tion of CP, on T ALL .  X  and  X  indicate statistical significance with p = 0 . 01 and 0 . 05 , respectively. An oracle ensemble would achieve an upper bound CP of 87.3%.

We report cumulative macro precision at rank ( CP @ k ) in our experiments since our testsets contain a single true user-intended selection for each test case 7 . However, this is an overly conservative metric since in many cases an alternative smart selection might equally please the user. For example, if our testset contains a user intended selec-tion  X  = The University of Southern California , then given the simulated selec-tion  X  X alifornia X , both  X  and University of Southern California would most likely equally satisfy the user intent (whereas the latter would be considered incorrect in our evaluation). In fact, the ideal testset would further evaluate the distance or relevance of the smart selection to the intended user selection. We would then find per-haps that Southern California is a more reasonable smart selection than of Southern California . However, precisely defining such a relevance function and designing the guidelines for a user study is non-trivial and left for future work. 5.1.2 Systems In our experiments, we evaluate the follow-ing systems, each described in detail in Sec-tion 4: Passthrough ( CUR ), Capitalization ( CAP ), Named-Entity Recognizer ( NER ), Noun Phrase ( NP ), Knowledge Base ( KB ), Hyperlink Intent Model ( HIM ), Ensemble ( ENS ). 5.2 Results Table 1 reports the smart selection performance on the full traffic weighted testset T ALL , as a func-tion of CP@ k . Our ensemble approach recovers the true user-intended selection in 56 . 8% of the cases. In its top-2 and top-3 ranked smart selec-tions, the true user-intended selection is retrieved 76 . 0% and 82 . 6% of the time, respectively. In po-sition 1, ENS significantly outperforms all other systems with 95% confidence. Moreover, we no-tice that the divergence between ENS and the other systems greatly increases for K  X  2 , where the significance is now at the 99% level.

The CUR system models the selection paradigm of today X  X  consumer touch-enabled devices (i.e., it assumes that the intented selection is always the touched word). Without changing the user inter-face, we report a 45% improvement in predicting what the user intended to select over this baseline. If we changed the user interface to allow two or three options to be displayed to the user, then we would improve by 93% and 110%, respectively. For CUR and NER , we report results only at K = 1 since these systems only ever return a sin-gle smart selection. Note also that when no named entity is found by NER , or no noun phrase is found by NP or no knowledge base entry is found by KB , the corresponding systems return the original user selection as their smart selection.

CAP does not vary much across K : when the intended selection is a capitalized multi-word, the longest string tends to be the intended selection. The same holds for KB .

Whereas Table 1 reports the aggregate expected traffic performance, we further explore the per-formance against the stratified T HEAD , T TORSO , and T
TAIL testsets. The results are summarized in Ta-ble 2. As outlined in Section 3.2, the HEAD se-lections tend to be disproportionately entities and capitalized terms when compared to the TORSO and TAIL . Hence CAP , NER and KB perform much better on the HEAD . In fact, on the HEAD , CAP per-forms statistically as well as the ENS model. This means that at position 1, for systems that need to focus only on the HEAD , a very simple solution is adequate. For TORSO and TAIL , however, ENS performs better. At positions 2 and 3, across all strata, the ENS model significantly outperforms all other systems (with 99% confidence).

Next, we studied the relative contribution of each ensemble member to the ENS model. Fig-ure 3 illustrates the results of the ablation study. The ensemble member that results in the biggest performance drop when removed is HIM . Perhaps achieve an upper bound CP of 98.5%, 86.8% and 64.8% for T Figure 3: Ablation of ensemble model members over T ALL . Each consecutive model removes one member specified in the series name. surprisingly, a first ablation of either the CAP or KB model, two of the better individual performing models from Table 1, leads to an ablated-ENS per-formance that is nearly identical to the full ENS model. One possible reason is that both tend to generate similar candidates (i.e., many entities in our KB are capitalized). Although the HIM model as a standalone system does not outperform sim-ple linguistic unit selection models, it appears to be the most important contributor to the overall ensemble. 5.3 Error Analysis: Oracle Ensemble We begin by assessing an upper bound for our en-semble, i.e., an oracle ensemble , by assuming that if a correct candidate is generated by any ensem-ble member, the oracle ensemble model places it in first position. For T ALL the oracle performance is 87.3%. In other words, our choice of ensemble members was able to recover a correct smart se-lection as a candidate in 87.3% of the user study cases. For T HEAD , T TORSO , and T TAIL , the oracle performance is 98.5%, 86.8%, and 64.8%, respec-tively.

Although our ENS model X  X  CP@3 is within 2-6 points of the oracle, there is room to significantly improve our CP@1, see Table 1 and Table 2. We analyze this opportunity by inspecting a random sample of 200 test cases where ENS produced an incorrect smart selection in position 1. The break-down of these cases is: 1 case from T HEAD ; 50 cases from T TORSO ; 149 cases from T TAIL , i.e., most errors occur in the TAIL .

For 146 of these cases (73%), not a single en-semble member produced the correct target selec-tion  X  as a candidate. We analyze these cases in detail in Section 5.4. Of the remaining cases, 25, 10, 9, 4, 4, and 2 were correct in positions 2, 3, 4, 5, 6, 7, respectively. Table 3 lists some examples.
In 18 cases (33%), the result in position 1 is very reasonable given the context and user selec-tion (see lines 1-4 in Table 3 for examples). Often the target selection was also found in second po-sition. These cases highlight the need for a more relaxed, relevance-based user study, as pointed out at the end of Section 5.1.1.

We attributed 7 (13%) of the cases to data prob-lems: some cases had a punctuation as a sole char-acter user selection, some had a mishandled es-caped quotation character, and some had a UTF-8 encoding error.

The remaining 29 (54%) were truly model er-rors. Some examples are shown in lines 5-8 in Ta-ble 3. We found three categories of errors here. First, our model has learned a strong prior on pre-ferring the original user selection (see example line 5). From a user experience point of view, when the model is unsure of itself, it is in fact better not to alter her selection. Second, we also learned a strong capitalization prior, i.e., to trust the CAP member (see example line 6). Finally, we noticed that we have difficulty handling user selec-tions consisting of a stopword (we noted determin-ers, prepositions, and the word  X  X nd X ). Adding a few simple features to ENS based on a stopwords list or a list of closed-class words should address this problem. 5.4 Error Analysis: Ensemble Members Over all test cases, the distribution of cases with-out a correct candidate generated by an ensem-ble member in the HEAD , TORSO , TAIL is 0.3%, 34.6%, and 65.1%, respectively. We manually in-spected a random sample of 100 such test cases.
The majority of them, 83%, were large sentence fragments, which we consider out of scope ac-cording to our prediction task definition outlined in Section 1. The average token length of the tar-get selection  X  for these was 15.3. In compari-son, we estimate the average token length of the task-admissable cases to be 2.7 tokens. Although most of these long fragment selections seem to be noise, a few cases are statements that a user would reasonably want to know more about, such as: (i)  X  X alks of a merger between the NHL and the WHA were growing X  or (ii)  X  X aN + NaN * 1.0i X .

In 10% of the cases, we face a punctuation-handling issue, and in each case our ensemble was able to generate a correct candidate when fixing the punctuation. For example, for the book title  X  = What is life? , our ensemble found the candidate What is life , dropping the ques-tion mark. For  X  = Near Earth Asteroid. our ensemble found Near Earth Asteroid , dropping the period. Similar problems occurred with parentheses and quotation marks.

In two cases, our ensemble members dropped a leading  X  X he X  token, e.g., for  X  = the Hume Highway , we found Hume Highway .

Finally, 2 cases were UTF-8 encoding mistakes, leaving five  X  X rue error X  cases. We introduced a new paradigm, smart selection , to address the cumbersome text selection capabil-ities of today X  X  touch-enabled mobile devices. We report 45% improvement in predicting what the user intended to select over current touch-enabled consumer platforms, such as iOS, Android and Windows. We release to the community a dataset of 33 , 912 crowdsourced true intended user selec-tions and corresponding simulated user touches.
There are many avenues for future work, includ-ing understanding the distribution of user touches on their intended selection, other interesting sce-narios (e.g., going beyond the e-reader towards document editors and web browsers may show dif-ferent distributions in what users select), leverag-ing other sources of signal such as a user X  X  profile, her interests and her local session context, and ex-ploring user interfaces that leverage n-best smart selection prediction lists, for example by provid-ing selection options to the user after her touch.
With the release of our 33 , 912 -crowdsourced dataset and our model analyses, it is our hope that the research community can help accelerate the progress towards reinventing the way text selec-tion occurs today, the initial steps for which we have taken in this paper. The authors thank Aitao Chen for sharing his NER tagger for our experiments, and Bernhard Kohlmeier, Pradeep Chilakamarri, Ashok Chan-dra, David Hamilton, and Bo Zhao for their guid-ance and valuable discussions.
