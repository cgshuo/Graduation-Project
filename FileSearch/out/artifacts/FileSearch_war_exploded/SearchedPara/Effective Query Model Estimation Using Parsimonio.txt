 In the recent past, the language modeling approach has become popular IR model based on its sound theoretical basis and good empirical success [3], [4], [5], [8], [9], language modeling had a trouble with incorporation of the relevance feedback or query expansion. Relevance feedback (or pseudo relevance feedback) is the well-model, relevance feedback can be well explained in its framework, while language does not explicitly assume a relevance document set [15]. Risk minimization framework and query model concept, suggested by Lafferty and Zhai [8], extend the language modeling approach to incorporate relevance feedback or query expansion. In risk minimization framework, language modeling is re-designed by KL(Kullback-Leiber) divergence between query model and document model. Query model is probabilistic version of user X  X  query sample, which encodes knowl-edge about a user X  X  information need. 
Obtaining high initial retrieval performance is very important problem, since post processing such as pseudo relevance feedback is highly dependent on initial retrieval performance. To improve an initial retrieval performance, query expansion based on word co-occurrence can be one of good strategies. In language modeling approach, word co-occurrence was formulated by translation model [1], [8]. First translation model, suggested by Berger and Lafferty, is document-query translation model [1]. And, this model was expanded with Markov Chain Translation Model (MCTM) [8]. Both translation models as expanded language modeling approaches, showed im-provements over baseline performance. However, MCTM yields high time complex-ity. Especially in offline construction, its time complexity is O ( NK ), where N is num-ber of documents and K is average number of terms in document. More seriously, non-topical terms are left in the model and their selection as expansion terms causes unstable retrieval results, so retrieval performance cannot be guaranteed. 
To resolve these problems, we propose to use Parsimonious Translation Model (PTM) . It conceptually corresponds to MCTM, but there is a difference in using document language model. In general Markov chain translation model (called original translation model, OTM), document language model is regarded as a mixture model of MLE document model and global collection language model. On the other hand, in PTM, document language model is regarded as is a mixture model of document spe-cific topic model and global collection language model. Document specific topic model is obtained by eliminating global common portions and using document topic portions from MLE document model. 
The paper is organized as follows. In Section 2 we briefly review KL divergence framework of the language modeling approaches and query model estimation prob-lem. In Section 3 we examine our query model estimation method, including con-struction method of parsimonious translation model, in more detail. A series of ex-Section 6 concludes and points out possible directions for future work. 2.1 Kullback-Leiber Divergence Framework Basically, the language modeling approach ranks documents in the collection with the query-likelihood (formula 1) that a given query Q would be observed during repeated random sampling from each document model [3], [4], [13], [19], [23]. 1 is document language model for D . 
Laffery and Zhai [8], proposed Kullback-Leiber divergence framework for lan-guage modeling so that allows modeling of both queries and documents and incorpo-rates relevance feedback or query expansion. The risk between documents and query is defined as follows. where p ( w|  X  Q ) is query model, and documents are ranked in inverse proportion to its risk. 2.2 Query Model Estimation Problem Laffery and Zhai [8] suggested Markov chain word translation model, where word translation events occur by random work processes on Markov chain [8], so that train-ing and application costs are significantly reduced without harming performance. In translation model based on this Markov chain, model construction has high time com-plexity. For given term q, translation model on Markov chain (using one step) is cal-culated as follows. 
Translation model t ( w | q ) means the probability to generate w from document topi-are suggested in relevance model of Lavrenko and Croft [10]. 
We can rewrite formula (3). where p ( w|  X  D ) p ( q|  X  D ) corresponds to co-occurrence probability in document between two terms q and w . 2.3 Problems of Translation Model To obtain single translation probability, co-occurrence probabilities must be summed across whole documents. And time complexity for summation is O ( N ) for given pair w and q , where N is the number of documents. This translation model has two prob-lems. The first problem is its complexity. At retrieval time, it is not practical to calcu-calculation quickly, a well known strategy is to restrict extraction of term pairs within local context : small windows such as few words or phrase level or sentence level [7], almost all NLP applications, such as word sense disambiguation [7]. Therefore co-occurrence from only local context cannot completely substitute co-occurrence from global context. Essentially, in query expansion problem,  X  X opically X  related terms are more desirable than locally related terms, because topically related terms can encode more correctly user X  X  information need. 
The second problem is in retrieval performance. Retrieval performance by the translation model is not guaranteed reliably, because non-topical terms of documents selected, their negative effects on retrieval performance may be serious. Therefore, a methodology is needed to prevent it from expanding non-topical terms. 
In these regards, we propose query expansion by Parsimonious Translation Model (PTM). In the model, highly topical terms are first selected in each document and co-occurrence statistics are constructed by only these terms among whole documents, ignoring non-topical terms. New probabilities of highly topical terms in document are modeled by document specific topic model , which is a kind of parsimonious language model [3]. In the document specific topic m odel, there are only few terms having non-zero probabilities. Parsimonious translation model is derived by applying Markov chain process among parsimonious document models. In this section, we describe our method to construct PTM and to estimate query model first. Next, PTM is acquired from these document specific topic models. Pseudo query constructing two-dimensional features such as bi-gram and tri-gram. 3.1 Estimating Document Specific Topic Model As noted in Section 2, document language models are constructed by mixing MLE document language model and global collection language model. MLE for document is far from document specific model because it contains global common words. To construct document specific topic model, we assume that documents are generated from mixture model with document specifi c model and global collection model. For given document D , the likelihood of document is as follows. where is p ( w |  X  D ) document specific topic model for estimation. To maximize the document likelihood, we apply EM algorithm [2]. 
E-step: 
M-step: indicates the number of EM iterations. As iterations increase, global collection model simplicity, let us denote  X  D* to convergent document specific topic model . 
Next, selection process is performed, where only highly topical terms are selected, between 50 and 100). Another method is select_ratio ( P ), where top terms are selected ( P is about between 0.6 and 0.9). After now, we will further explain with se-lect_ratio ( P ). 
Let us define parsimonious document language model s D  X  consisting of topical terms selected by select_ratio ( P ). Let D s be a set of topical terms of the document D selected by select_ratio ( P ). D s is a maximal subset of words of document D that satis-fies the constraint w Ds p ( w|  X  D* ) &lt; P. where  X  D is a normalization factor with value 1/ w Ds p ( w|  X  D* ). 3.2 Parsimonious Translation Model As mentioned in Section 2, translation probability t ( w | q ) is the probability generating w in the document that includes given term q . Since word translation model is mixture model of different document models, it is one of document language model. As sub-stituting document language model of formula (4) into summation of document spe-cific model and global coll ection model, we further derive translation model. where  X  is a smoothing parameter for mixing document specific model, and collection language model. Conceptually, although  X  corresponds to the smoothing parameter  X  for initial retrieval, we treat  X  differently to  X  . 
Translation model consists of three summation parts: Document specific co-document specific co-occurrence model by global likelihood p ( q ). 
At offline indexing stage, of these quantities, we need to pre-calculate only docu-calculated easily from information of basic language modeling. 
When using select_ratio ( P ) method for document specific model, time complexity 0.01. In this case, reduction ratio of time complexity is about 100 times. 3.3 Estimation of Query Model Given query terms Q , we can infer query model from translation model as following, similar to [8]. where  X  Q is inferred query model directly from translation model. 
Final query model  X  Q X  is acquired by mixing MLE query model and above inferred relevance document model using parameter  X  . Our experimental database consists of two collections for Korean, and five TREC4 average number of unique terms of documents and  X # Q X  is the number of topics and  X # R X  is the number of relevant documents in each test set. 
In Korean documents, it is well known that bi-character (n-Gram) indexing units are highly reliable [12]. Thus the bi-character indexing unit is used in this experimen-where stop words are removed and then Poster stemming is applied. 
For baseline language modeling approach, we use Jelinek smoothing, setting the smoothing parameter  X  into 0.25. This smoothing parameter value is acquired empiri-cally, by performing several experimentations across different parameters. 5.1 Retrieval Effectiveness of Parsimonious Translation Model collections by changing parsimony level from 0.1 to 1.0. (Results of combining meth-guage model using MLE of query sample, query model estimated from OTM (for-mula (3)), and query model estimated from PTM (formula (8)). Interpolation parame-parameter  X  is fixed at 1.0 because of full parsimony. 
As shown in Figure 1, in almost all parsimony levels, PTM significantly improves baseline in six data collections. Remarkably, performance of PTM is better than per-formance of OTM at low parsimony level. In OTM, some noise can occur because common words of query can be expanded by common terms of document. Therefore, compared with baseline, high accuracy of PTM implies that PTM can effectively eliminate noise of term expansion in OTM, and select good expansion terms in terms of retrieval performance. 
Concerning optimal parsimony level, while for Korean collection optimal parsi-and 0.4. However, performance in TREC4-FR test collection is relatively exceptional. As shown in Figure 1, PTM does not good in parsimony levels not in 0.2~0.4 but in 0.6 ~0.8. It seems that in TREC4-FR good expansion terms have relatively small probabilities value. In Section 5.2, the reason will be discussed in detail. Table 2 summarizes the best performance of parsimonious models (PTM*) and OTM in seven different test collections. The last column with symbol  X %chg X  indi-cates improvement ratio of PTM* over OTM. P* is the parsimony level at the best. From this table, we know that PTM highly improves baseline from 5% to 25%, and especially 25.55% improvement is achieved at TREC4-ZIFF test collection. 
From these experiments, we can conclude that PTM highly improves retrieval per-formance at the optimal parsimony levels. 5.2 Storage Size of Parsimonious Translation Model Table 3 shows the storage size of PTM across various parsimony levels (0.1 ~ 0.6 and cell; numbers in top are storage size, and numbers of in bottom is the ratio of the size to OTM. Bold numbers express the best performance in each collection. In Korean test sets, reduction ratio of size at optimal parsimony level is lower than 1%. In Eng-TREC4-ZIFF to 13.14% at TREC4-AP. At this time, exceptional phenomenon of optimal parsimony level in TREC4-FR can be interpreted in terms of the size reduc-tion, since degree of size reduction is slowly increased according to parsimony level. It requires parsimony level of 0.6 to achieve size reduction of about 10%. 
From Table 3, we can see that PTM remarkably reduce space complexity. This also implies that PTM reduces time complexity, because in constructing co-occurrence statistics time complexity is proportional to space complexity Summing up, we propose effective construction method for co-occurrence statistics using parsimonious translation model. Parsimonious translation model involves an elegant method for selecting highly topi cal terms in documents, by document specific topic model. Basically, our idea is to use the several states of art methods in language modeling approach for information retrieval. From our experimentation on two different collections, Parsimonious translation model not only drastically reduces time-and space-complexity but also highly improves retrieval performance at the optimal parsimony levels. This work was supported by the KOSEF through the Advanced Information Technol-ogy Research Center(AITrc) and by the BK21 Project. 
