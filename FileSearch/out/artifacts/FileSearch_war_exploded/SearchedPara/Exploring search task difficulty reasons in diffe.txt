 1. Introduction
Online information searching has become an everyday activity in the lives of people around the world. With today X  X  search systems, people can often easily find the information they need. However, at times, people may have a hard time the literature on a topic. These difficult tasks can lead to users not finding desired information and accordingly cause frus-tration and/or system switch behaviors. Search systems aimed at helping people locate information effectively, efficiently, and enjoyably should make it an important goal to reduce task difficulty, assist users with difficult tasks, and increase users X  satisfaction.

Task difficulty has been attracting more and more research attention in the field of information retrieval (IR). Some researchers studied query performance and query difficulty from the language model perspective (e.g., Carmel, Yom-Tov,  X  subjective perception assessed by task doers. Studies along this line have examined users X  search behaviors in difficult vs. easy tasks (e.g., Aula, Khan, &amp; Guan, 2010; Gwizdka, 2008 ), users X  perception of task difficulty before and after working such as user characteristics (e.g., Arguello, 2014; Gwizdka, 2008; Liu, Belkin, &amp; Cole, 2012 ).

Despite the above research directions of task difficulty, little has been known about why users feel certain tasks are dif-ficult and what makes them feel this difficulty. Even though a system can predict, from monitoring the users X  behaviors, that they are having difficulty in their search, the system cannot help users overcome the difficulty without a further understand-to address this issue and developed a task difficulty reason scheme based on a lab experiment with 32 participants searching for 4 tasks. It would be helpful to test this scheme with more participants. In addition, it would be interesting to explore if the difficulty reasons vary across task types and across users with different backgrounds. In order to better understand the nature of search task difficulty and eventually benefit search system design, the current research attempts to explore the following research questions: 1. What are the reasons users perceive a search task is difficult? 2. Do the task difficulty reasons vary across different tasks and task types? 3. Do the task difficulty reasons vary between searchers with different levels of task topic knowledge? 2. Literature review 2.1. Search task difficulty and user behaviors
Search task difficulty has attracted quite some research attention in IR, especially in recent years. There is not a consensus is the task doer X  X  perception of task complexity. Li and Belkin (2008) noted in their comprehensive task classification scheme that task difficulty can only be subjective, as assessed by task doers. Some other researchers define task difficulty based on certain objective criteria and/or measurements. For example, Aula et al. (2010) employed a user X  X  task answer being correct difficulty levels based on the retrieved results X  precision@10 using the topic terms as search keywords.

One line of studies explored the relationship between task difficulty and searchers X  behaviors. Kim (2006) examined the effects of task difficulty on user behaviors in three types of tasks: factual, interpretive, and exploratory. Task difficulty was elicited through users X  ratings on a Likert scale based on their subjective judgments. It was found that in factual tasks, post-task difficulty was significantly associated with task completion time, and the numbers of queries and documents viewed; in exploratory tasks, user behaviors were significantly correlated with pre-task difficulty; but in interpretive tasks, most cor-relations between behaviors and task difficulty were not significant. Liu, Gwizdka, Liu, and Belkin (2010) also used users X  subjective perceptions on task difficulty as elicited by their ratings on task difficulty questions. They examined through a lab experiment how user behaviors vary in tasks with different difficulty levels, as well as different types. They found that in difficult tasks, users had longer task completion time, issued more queries, viewed more content pages, and had longer dwell time on content pages.

Aula et al. X  X  (2010) took a more objective way to define task difficulty. It was determined by user success or failure in finding the answers to their task questions, which were closed information tasks that had a single, unambiguous answer. The authors conducted two studies to research task difficulty, one being a lab experiment and the other a large-scale study.
Results showed that in difficult tasks, users formulated more diverse queries, used advanced operators more, and spent longer time on the search result pages.
 Another line of studies has been attempting to predict search task difficulty from searchers X  behaviors. Gwizdka and
Spence (2006) examined how searchers X  behaviors could indicate the difficulty of a factual information-seeking task. Task difficulty was self-assessed by users after each task. Their results indicated that higher search effort, lower navigational speed, and lower search efficiency were good predictors of task difficulty tested by regression models. In Liu, Gwizdka, et al. (2010) study, tasks were categorized as easy and difficult based on searchers X  post-task judgment on tasks X  difficulty levels. Searchers X  behaviors were grouped at two levels depending on the time point when the behavioral factor value can be captured: the whole-task-session level factors whose values can only be obtained after a search session is done, and the within-task-session level factors whose values can be obtained while a search session is ongoing. They found that both whole-session level and within-session level user behaviors could serve as task difficulty predictors in logistic regres-sion models. Whole-session level variables showed higher prediction accuracy, but do not enable real-time prediction. On the other hand, while within-session level factors can ensure real-time prediction, the prediction accuracy in general was mediocre, especially in certain types of tasks, possibly because of the limited number of within-session factors that were considered and used in their model.

Liu, Liu, et al. (2012) investigated users X  behavioral differences between difficult and easy search tasks and built predic-tion models based on users X  behavioral factors. The behaviors they used were at three levels, distinguished by the time point when the measurements can be done: first-round level at the beginning of the search, accumulated level during the search, and whole-session level by the end of the search. They found that a number of user behaviors at all three levels showed dif-ferences between easy and difficult tasks. Models predicting task difficulty at all three levels were developed and evaluated.
A real-time model incorporating first-round and accumulated levels of behaviors obtained fairly good prediction perfor-mance that was comparable with the model using the whole-session level behaviors that are not real-time. A similar study conducted by Arguello (2014) also attempted to predict task difficulty using logistic regression models. The study also con-sidered first-round and whole-session level search behaviors and collected difficulty ratings in post-search questionnaires.
Other factors considered in the study included searchers X  interest in and knowledge of search topics. The study found that whole-session prediction was more effective than first-round prediction. The best approach for either level prediction was to combine a wide-range of features, and for different level prediction, the most predictive features were different. The study also found that users X  level of interest and prior knowledge did not help with prediction performance.

Liu et al. (2011) examined how users X  perception of task difficulty changes before and after searching for information to solve tasks using two user studies which employed carefully designed tasks of different types along several dimensions: task structure (subtasks being dependent upon or parallel with each other), task goal in quality (being specific or amorphous), and naming (being named or unnamed). It was found that while in some types of tasks, users X  perceptions of task difficulty did not change before and after working on the tasks, in others, they did, either increasing or decreasing. It was also found that users X  background factors, e.g., previous topic knowledge and search experience, do not necessarily correlate with their perceived task difficulty, or perceived difficulty change. 2.2. Task type in information retrieval
Much attention has been drawn on examining the effects of different tasks on information searchers X  behaviors and per-formance. The basis of studying task type is to classify user tasks into different types along some task feature(s), for example, and Belkin (2008) constructed a comprehensive classification scheme that includes a number of dimensions of task features/ attributes: task product, complexity, and difficulty, to name a few.

One frequently examined task feature is task complexity, which was related to, but not the same as task difficulty. Vakkari (1999) developed a model integrating task complexity and user actions. Bystr X m and colleagues conducted a series of studies ( Bystr X m, 2002; Bystr X m &amp; J X rvelin, 1995 ) examining the relationships between task complexity, information types, and information sources. They defined task complexity as the users X   X  X  X  priori determinability of, or uncertainty about, task out-comes, process, and information requirements X  X  ( Bystr X m &amp; J X rvelin, 1995, p. 194 ). They found that task complexity was related to information type and information source selection. As task complexity increased, users needed more sources of information, more domain information and more problem solving information; they were less likely to predict the types of information they needed; and they were more dependent upon experts to provide useful information. Li (2008) defined and measured task complexity by the number of activities in a work task, or the number of search sources needed in a search task. She found that objective task complexity affected many aspects of searching behavior, including: the number of search systems consulted, portals visited, web result pages and items viewed, users X  interaction with library resources, query-related interactive behavior, success, satisfaction, and the total task completion time. Following the same definition of task complexity as Li (2008), Liu, Cole, et al. (2010) found that high complexity tasks led to longer task completion time, more pages visited, more sources, and more queries.

Task types have also been classified by other features and have been studied regarding their effects on users X  information behaviors. Qiu (1993) found that when engaging in specific tasks rather than in general tasks, users tended to adopt more structured search patterns. Moreover, users preferred to use browsing features for completing general tasks, but analytical searching in specific tasks. Kim (2006) looked at three types of tasks: factual, interpretive, and exploratory tasks, and found that task type significantly influenced number of pages saved and the ratio of pages viewed to pages saved. Kellar et al. (2007) examined how users navigated and interacted with web browser across four types of tasks: fact-finding, information gathering, browsing, and transactions. They found the information gathering task to be the most complex: users spent more time completing it, viewed more pages, and used the web browser functions most heavily. Toms et al. (2007) examined how search was affected by tasks with different information goals (decision making, fact finding, and information gathering tasks) and task structures (having hierarchical or parallel concepts in the tasks). They found that in the hierarchical tasks, users performed fewer queries but took more time to process the search results than in the parallel tasks. In tasks with different goals, users did not show notable differences in the number of queries and time spent to formulate queries. Li (2008) studied tasks along the  X  X  X ask product X  X  feature, in which tasks included intellectual and decision/solution types. She found that, com-pared with decision/solution tasks, intellectual tasks involved more IR systems consulted and result pages viewed, longer queries, and higher self-ratings on task success. Also along the  X  X  X ask product X  X  feature to classify task types, Liu, Cole, et al. (2010) found that mixed product tasks, compared with factual tasks, led to longer task completion time, more pages visited, and more sources referred to. Liu, Cole, et al. (2010) also found that along the  X  X  X ask goal (quality) X  X  feature, amorphous tasks, compared with specific tasks, led to more sources referred to.
 2.3. Task topic knowledge in information retrieval
In the rich literature examining the effects of user knowledge on information searches, researchers have examined two different types of knowledge: (1) subject domain knowledge, which is a person X  X  knowledge of a general subject or domain, for example, medicine, law, or psychology; and (2) task topic knowledge, which refers to the user X  X  knowledge with the topic of his/her specific task at hand. Zhang, Liu, and Cole (2013) found different effects of domain knowledge and topic knowledge on users X  behaviors. Topic knowledge was significantly correlated with performance measures for individual tasks, and domain knowledge was significantly correlated with performance at more general levels, over multiple task sessions. Because of the differences, the following reviewed these two types of knowledge respectively.

The majority of previous studies on knowledge have focused on domain knowledge. Hsieh-Yee (1993) found that domain knowledge affected search tactics. As compared to a familiar subject area, when users worked with a search task outside of their field, they used the thesaurus more for term suggestion, made more effort in preparing for the search, monitored the search more closely, included more synonyms, and tried more term combinations. Vakkari, Pennanen, and Serola (2003) found that medical major students began to use a wider and more specific vocabulary in their development of research proposals at the end of a 3-month seminar compared to the beginning. Sihvonen and Vakkari (2004) found that the number and type of terms selected from the thesaurus for expansion by domain experts in the medical area improved search effec-tiveness, whereas the use of the thesaurus by domain novices had no impact. Wildemuth (2004) found that low domain knowledge, before the users took a microbiology course, was associated with less efficient selection of concepts to include in the search and with more errors in the reformulation of search tactics. Zhang, Anghelescu, and Yuan (2005) found that a high-level knowledge group of users in the Heat and Thermodynamics engineering domain were found to have better per-formance (retrieved slightly more relevant documents), issued longer queries, and had more queries per task. Duggan and
Payne (2008) found that knowledge of the music domain had little effect on search performance, but that of the football domain had much effect on search performance: it was positively correlated with search accuracy, and negatively correlated with time spent on web pages and mean query length. White, Dumais, and Teevan (2009) found that within their domain of expertise, experts search differently than non-experts in terms of the sites they visit, the query vocabulary they use, their patterns of search behavior, and their search success.

Meanwhile, some studies investigated the effects of search task topic knowledge in information searches. Topic knowl-paper, topic knowledge and topic familiarity are used interchangeably.

Allen (1991) defined  X  X  X opical knowledge X  X  as specific factual knowledge of a topic. He found people with high topical knowledge used more search expressions than those with low knowledge. Kelly and Cool (2002) found that reading time tends to decrease and efficacy (measured by the ratio of the number of saved documents to the total number of viewed doc-uments) increases with increasing search task topic familiarity. Hembrooke, Granka, Gay, and Liddy (2005) found that topic experts issued longer and more complex queries than topic novices. Experts also used elaborations as a reformulation strat-egy more often as compared to simple stemming and backtracking modifications used by novices. Liu and Belkin (2010) found the total dwell time that users spent on a retrieved document was positively correlated with document usefulness regardless of the user X  X  topic knowledge level, but the first dwell time (the unobtrusive duration from opening a document to the user leaving it for the first time) was not significantly correlated with document usefulness, and topic knowledge affected their correlation. 3. Method 3.1. Experimental design
To answer the research questions, a user experiment was conducted. A total of 48 college students participated in the experiment, coming from two universities, 16 at the University of South Carolina (USC) Columbia campus and 32 at the
Southern Connecticut State University (SCSU) campus. The experiment asked participants to work with four assigned infor-mation search tasks, and provide their subjective judgments of task difficulty and reasons. Task type was the pre-controlled factor, with different features along various dimensions in a task classification scheme (details below). Task difficulty was obtained post hoc through user ratings on a Likert scale, as was users X  topic knowledge levels. 3.2. Participants
Among the 48 undergraduate students, 30 were females and 18 were males. Their average age was 22.3 years. Partici-pants were randomly recruited by posting recruitment advertisements on community notice boards in the University student centers, dormitories, libraries, and dining rooms, as well as sending out emails to student listserv at randomly selected departments. Each participant was compensated $15 upon completion of the experiment. They were also told in the beginning of each experiment session that the top 20% participants who saved the best sets of documents for the tasks would get an additional $10. The reason for using such an incentive mechanism was to encourage the participants to work seriously on search tasks. 3.3. Tasks
Four tasks were designed according to the faceted classification scheme developed by Li and Belkin (2008) , which clas-sifies task features on different dimensions. In the current study, four task features/facets were controlled, which were pre-viously found to have significant influences on task difficulty in related studies. The following lists the four task facets and their definitions: Product: an intellectual task produces new ideas or findings; a factual task locates facts, data, etc.

Task goal (quality): a specific task has a goal that is explicit and measurable; the goal of an amorphous task is not mea-surable; a combined task has both concrete and amorphous goals.

Complexity: low complexity tasks involve 1 X 2 activities and searching in one type of information source; moderate com-plexity tasks involve 3 X 4 activities and searching in 2 types of sources; high complexity tasks involve 5 and more activ-ities and searching at least 3 types of sources.

Naming: a named task has a specified search target; an unnamed task does not have a specified search target. (Note that this feature was not in the original Li and Belkin (2008) study, but was proposed and used in Liu et al. (2011) .) The four tasks are described as below, with Table 1 listing the facet values of each task.

Task 1 (T1): Suppose you are preparing to apply for a graduate school in your major. You want to do some research online, narrow down the target schools that you think best match with your background and interest. You also want to consider their financial aid opportunities and locations when choosing the schools. After you decide on the target schools, you want to learn about their admissions requirements. Your search task : Find two schools that you want to apply for, and save the webpages that show their admissions requirements.

This task was an intellectual task because identifying two schools generated  X  X  X indings X  X ; its goal was combined because the two schools were amorphous, but the admissions requirements were specific; its target was unnamed, and it is a high complexity task because users would need to narrow down and identify the schools and also locate the admissions require-ment pages.

Task 2 (T2): You like and admire someone so much that you want to create a Wikipedia entry for this person. Your search task : Please collect and save all the webpages and other online sources that are helpful for you to write such a Wikipedia entry. Note: for the selection of the person, you want to make sure that there is not an existing entry in Wikipedia for him/her. (The note was in the task description, and the Italic and bold font was as well.)
This task was factual because only facts (or information sources with facts) about the person were required; its goal was amorphous because the person for whom the entry was created was not specified, as were the facts/information to be included in the entry; its target was unnamed, and it is a high complexity task because users would need to try locating as many as possible sources/facts for the person.

Task 3 (T3): Your friend is a soccer fan and he is participating in a contest about soccer knowledge. For one question, he could not answer, but he had a chance to call you and ask you for information. The question is: Name at least one soccer player who had done a hat-trick in the 2010 X 2011 Italian Soccer League Series A. Your search task : Search online for the answer for the above question. Save the webpage(s) with the answer.

This task was factual because the answer was a fact; its goal was specific because there was only the right answer to look for; its target was named, and it is a low complexity task because users would just need to locate the soccer player who meets the requirements.

Task 4 (T4): You have read that Alfred North Whitehead wrote a book, Science and Modern World, and you are interested in learning more in the concept of  X  X  X imple location. X  X  You would like to learn about the various interpretations and views on
Whitehead X  X  simple location as a philosophical concept. Your search task : Search and save 5 articles, in full-text (.pdf or .html), that you think are helpful.

This task produced an intellectual task because learning about the various interpretations and views of the concept gen-erated  X  X  X deas X  X ; its goal was combined because specifically 5 articles were required, but the helpfulness of these articles did not have a single measurement; its target was named, and it is a high complexity task because users would need to under-stand the concept and locate 5 articles that were helpful to understand the concept.

Each participant finished all 4 tasks. In order to minimum the order effect, the task order for all 32 sessions in the whole experiment followed a Latin square design.
 3.4. Procedure and data collection
Participants came individually to a computer lab to take part in the experiment. Upon arrival, they first completed a con-sent form and a background questionnaire eliciting their demographic information and search experience. After a training task, they worked with the 4 tasks in the assigned order, each for up to 15 min. Each participant was allowed to freely search online and save the documents that they thought could help them solve the assigned tasks. Users X  interaction with comput-ers was logged by Morae usability software ( http://www.techsmith.com/morae.html ), although the current study did not use this data, but instead relied on the questionnaire responses.

Questionnaires were administered both before and after each task to elicit users X  perceived task difficulty levels, their self-assessed knowledge levels with the task topic, their interest in the topic, and so on. All ratings were based on a 7-point scale (1 = not, 4 = somewhat, 7 = extremely). They were also asked why they gave the difficulty ratings, and what made them think the task would be/was difficult. After they finished all four tasks, a final questionnaire asked if they have further com-ments on the experiment. They were then given the compensation. 4. Results
Difficulty ratings and reasons were collected both before and after tasks, but the current analysis considered only the post-task perceptions since they represented users X  judgments based on the actual task experience. Task difficulty in this study follows the notion of Li and Belkin (2008) that it was users X  subjective judgments. Users X  responses to both questions about why and what made them feel the task was difficult were examined, and then classified into different categories fol-lowing a coding scheme (see below). 4.1. Task difficulty rating overview
There were a total of 192 user X  X ask sessions (48 users by 4 tasks). First examined were user ratings for the 4 tasks as an overview. A one-way ANOVA test showed that Task 4 was rated as the most difficult, which was significantly more difficult than Task 1 ( Table 2 ). The ratings of T2 and T3 were descriptively but not statistically significantly higher than T1 and lower than T4.

It is noted that T3 is a factual, specific, named, and low complexity task ( Table 1 ), which may have seemed to be low dif-ination, we speculate that task difficulty levels may be caused by various factors, such as users X  familiarity with task topics and task type, and it may be possible that users X  low topic familiarity with T3 (the most frequently given difficulty reason in
T3, as shown in Table 4 ) made them feel that it was difficult, despite the fact that it was a low complexity task. Since the current study focuses on difficulty reasons, more examination on difficulty ratings will be conducted in future studies. 4.2. Coding scheme
Based on the task difficulty reasons given by the 32 participants at SCSU in both the pre-and the post-task questionnaires, a scheme was developed that classified different reasons into categories. An examination of the 16 participants at USC did not detect new reasons for task difficulty. In general, the various reasons spread across different aspects, including task fea-ture, user aspect, and the interaction between the user and the task. The coding scheme is listed in Table 3 . More details about this coding scheme can be found in Liu and Kim (2013) .

An inter-coder reliability test was conducted by two authors using a 10% sample randomly selected in the SCSU data pool with 32 participants. The Cohen X  X  Kappa value between the two coders was 0.87, which was satisfactory. One author coded all the 48 participants X  responses to difficulty reasons. 4.3. Reasons for task difficulty perception
Table 4 lists the frequency of each difficulty reason in each task. As can be seen, some people rated the tasks as not dif-not difficult correspond to the task difficulty ratings in Table 2 . T1 was about locating information of the graduate schools, which could have seemed to be a familiar topic to many participants, and in fact, none of the participants listed low topic high ( Table 6 ).

For other cases, they gave their reasons why the tasks felt difficult, and the top reasons are listed by tasks ( Table 4 ). The cutoff frequency for each task was selected as the median value of difficulty reason frequency in that task (4, 4.5, 2, and 3.5 respectively in the four tasks). Those reasons with a frequency greater than the cutoff frequency were selected as top reasons.

Table 5 listed the top reasons for the 4 tasks. For Task 1, there were four top reasons: specific requirements, uncertainty about information need, too much (unrelated) information, and task complexity. For Task 2, the five top reasons were not enough information, specific requirements, uncertainty about information need, too much (unrelated) information, and complexity. The three top difficulty reasons for Task 3 were specific requirements, low topic knowledge, and too much (unrelated) information. For Task 4, there were 8 top difficulty reasons: low topic knowledge, too much (unrelated) informa-tion, specific requirements, need to read/comprehend information, resource credibility/quality, time limitation, not enough information, and complexity.

There were common reasons in the four tasks that made the users perceive the tasks as difficult: specific requirements, and too much (unrelated) information ( Table 6 ). Other reasons leading to task difficulty in different tasks varied. T1, T2 and
T4 had the same common reason of task complexity. T2 and T4 had the same common reason of not enough information. T1 and T2 had the common reason of uncertainty about information need. T3 and T4 had one common reason: low topic knowledge. T4 also had some unique reasons that were not identified as top difficulty reasons in other tasks, and they were: time limitation, need to read/comprehend information, and resource credibility/quality. 4.4. Relationships between users X  background and difficulty ratings
As a frequently examined factor reflecting information searchers X  background, task topic knowledge was examined rience in the post-task questionnaire, the topic knowledge in the pre-task questionnaire better reflects one X  X  base knowledge and background with the task topic. These relationships were used in the current study.

First analyzed was the relationship between task difficulty ratings and users X  topic knowledge. A significant and positive correlation was found between them ( r (128) = .187, p = .034).

In order to analyze the differences between the lower and higher topic knowledge users, they were divided into groups by their knowledge levels before they worked with the task, i.e., their pre-task topic knowledge ratings. According to the rating scale meanings, ratings 1 X 3 were put in a low knowledge group, 4 a mid-knowledge group, and 5 X 7 a high knowledge group.
Table 6 shows the number of participants in each group in each task. 4.5. Task difficulty by topic knowledge group in each task
Since task topic knowledge was a factor significantly correlated with task difficulty, task difficulty reasons were also examined with different user groups that had different levels of topic knowledge. This subsection presents the main reasons why a task was difficult in different tasks for different knowledge groups of users. For each group of users, the cutoff thresh-old for each task was again the median frequency of difficulty reasons in that task. In the low knowledge group, the median frequency values were 1.5, 2, 1.5, and 3 respectively for the four tasks, and in the high knowledge users, those were 1, 3.5, and 1 in Tasks 1 X 3, while no users had high knowledge in Task 4. Difficulty reasons with a frequency greater than the cutoff values were selected as top reasons.

In these analyses, only the low and high knowledge groups were selected and compared, which would possibly reduce the chance that mid-knowledge group users could have the features of both the low and the high groups.

Table 7 shows the top difficulty reasons in the low and high knowledge groups for different tasks. As can be seen, for each task, there were some common reasons identified in the two groups, but there were also variations.
 For Task 1, both low and high knowledge users identified specific requirements as a common top reason for task difficulty.
Low knowledge users also identified complexity and little experience as the top reasons. High knowledge users also men-tioned the uncertainty about information need, too much (unrelated) information, and system performance as top difficulty reasons.

For Task 2, two common top reasons in the two groups were not enough information and task complexity. Low knowl-edge users also identified the uncertainty about information need as a difficulty reason. High knowledge users mentioned task complexity as another top reason for task difficulty.

For Task 3, one common reason in both groups was specific requirements. Low knowledge users mentioned three other top reasons: low topic knowledge, too much (unrelated) information, and hard to formulate queries.

For Task 4, there was only one user who rated the knowledge level as high. One self-assessed the knowledge level as 4 (medium knowledge level), and all others ( N = 46) rated themselves as low knowledge with the task topic. Low knowledge users mentioned many reasons for task difficulty: Low topic knowledge, too much (unrelated) information, specific require-ments, need to read/comprehend information, time limitation, not enough information, and complexity. The one high knowledge user mentioned the resource credibility/quality as the task difficulty reason. 5. Discussion 5.1. Task difficulty and task type
Section 4 compared task difficulty reasons for each task. Since each task was special in topic, it would be more beneficial and valuable to compare the difficulty reasons along task types, which builds the basis, at least to some extent, for generalization of the findings. Table 8 lists the top reasons appearing in the four tasks.

It was found that two common reasons were identified as top reasons in all tasks: specific requirements and too much (unrelated) information. The reason of  X  X  X pecific requirements X  X  seems to reflect the users X  wish to find the requested infor-did not offer information to meet the  X  X  X pecific requirements X  X . Similarly, having too much unrelated information made the users not easily find the needed information, which could have accordingly made them feel the task was difficult.
Since these two reasons appeared in all task types, they would not help in detecting task type or generalizing difficulty reasons to specific task types. What is more helpful here is to examine the top reasons that appeared in some but not all tasks, which can possibly be related with task types (features).

One of such task reasons was the uncertainty about information need. This appeared in T1 and T2. This reason of the uncertainty about information need could be due to the feature of those two tasks being unnamed in target, as well as amor-phous in task goal (quality). In the graduate school application task (T1), the two schools were not named, and the task goal cannot be measured because of this unnamed nature. In the Wikipedia entry task (T2), the person for whom the entry will be created was unnamed, and the task goal was also amorphous. These features may have made the users feel uncertain about the information need, and it is indicated that tasks that are unnamed (in target) or amorphous (in task goal) would possibly make the users feel they were difficult because of the uncertainty of the information need.

Another of the common reasons appearing in some but not all tasks was task complexity, which was identified as a top difficulty reason in T1, T2 and T4. This matched the fact that these three tasks were all designed as high-complexity tasks. A task being complex in nature is likely to make the users feel it is difficult.

For both T3 and T4, users identified low topic knowledge as a top reason for task difficulty, but this was not a top reason for T1 and T2. That T3 and T4 were both  X  X  X amed X  X  in target might have allowed users to assess how much knowledge they had with the  X  X  X pecific X  X  topics; on the other hand, T1 and T2 did not have their named targets, which did not provide users  X  X  X pecific targets X  X  on which to assess their topic knowledge. In this case, although users can still assess their knowledge on the task topic in general, the fact that they would have to determine their specific target from this Unnamed task (i.e., the reason of  X  X  X ncertain about information need X  X , which was one of the top reasons in T1 and T2 as above mentioned) may add another layer of reason for task difficulty, which may also shadow the role of topic knowledge in leading to task difficulty.
The reason  X  X  X ot enough information X  X  was identified as a top reason in T2 and T4. There was not a common task facet for these two tasks only, so it is somewhat tricky to attribute or connect this reason with the available task facets addressed in this study. A further examination of these two tasks may explain why  X  X  X ot enough information X  X  was identified as a top rea-son. These two tasks were both high-complexity tasks, which may be a sufficient but not necessary condition to  X  X  X ot enough information X  X  (e.g., T1 was also a highly complex task, but users did not have problems with finding enough information for that task). T2 asked users to collect as much information as possible for a person not having a Wikipedia page, which was a recall-based task. Not having a Wikipedia page most likely indicate that the person is not famous enough, which is also an indication of not having much information on the Internet. So it is mostly the requirements of this task, not the task facets cific concept of a philosopher. Despite the fact that many users were not familiar with the concept, there are a good amount of relevant articles available in library databases. However, many users relied only on general search engines, which did not return many relevant results. The underlying reason for this task being perceived difficult by users was actually not using the best search sources. The relationship between the reason of not enough information and other reasons such as not using the right search sources can be further examined in future studies, but it can be seen that  X  X  X ot enough information X  X  does not seem to connect with the addressed task facets in the current study. 5.2. Task difficulty reasons in different knowledge groups
The two groups of users with different levels of topic knowledge showed quite some differences, in addition to some com-common top reason in T1, specific requirements and not enough information in T2, and specific requirements in T3. As dis-cussed above, the reason of specific requirements appeared in all four tasks, and it was also found to appear in both groups in all three tasks (Task 4 did not have high knowledge group users). In Task 2 (Wikipedia entry), the common reason of not having enough information matched the task being a recall-based one, asking users to collect all webpages and other sources (i.e., as much information as they can) for them to create a Wikipedia entry for someone.

When personalizing search results for users, it would be more interesting and beneficial to look at the differences in dif-ficulty reasons between the two groups. This discussion is divided by tasks and continued below.

For Task 1 (graduate school application), low knowledge users identified two top reasons that high knowledge users did not have: complexity and little experience. On the other hand, high knowledge users identified three top reasons that low knowledge users did not have: the uncertainty about information need, too much (unrelated) information, and system per-formance. For the low knowledge users, task complexity and little experience with the task were both general perceptions with the task as a whole, as compared with the reasons identified by the high knowledge users, uncertain about information need, too much (unrelated) information and system performance, which are all more specific and address the target schools in mind as well as the search results. This may be because the high knowledge users knew better what to do with the task than the low knowledge users, who may not have clear ideas how to deal with such tasks.

For the high knowledge users, the identified top reason of uncertainty about information need seems to indicate that, in this case, the more the user knows about a topic, the more likely that he/she feels uncertain about the information need. Those who know little about the topic may not even realize the information needs they could have had. Considering the task facets aspect, this task was intellectual, with combined goal (quality) (both amorphous and specific aspects appeared in the task), the target was unnamed, and it was a highly complex task. The amorphous goal and unnamed target of this task could have made those who had some topic knowledge feel uncertain about the information need. As for the reasons of  X  X  X oo much (unre-lated) information X  X  and  X  X  X ystem performance X  X , similar discussions can be made. That the users have a certain level of knowl-edge may allow them to judge the results as not relevant. On the other hand, the low knowledge users may not have advanced this far yet, and so they were only feeling the task complexity in a general sense and did not have enough experience in dealing with this kind of task. Further examination and confirmation on these speculations could be performed in future studies.
One implication of the differences in findings from user groups in T1 is that for low knowledge users, when they work with the tasks that have amorphous goals, an unnamed target, and call for an intellectual product, the system can try to make them feel experienced, or have some background with this topic, for example, by suggesting what others usually look for and collect information. The system can also try to decompose the tasks for the users, making them feel the task is less complex, for example, by organizing the information for users while they search. For high knowledge users, when they work with these types of tasks, the system can help them to identify or narrow down their information need. There could be var-ious approaches to this goal such as offering options in categories, and providing filtering functions to narrowing down.
Meanwhile, it is acknowledged that the system will need to somehow know the task type beforehand. This is an issue that has attracted research attention, for example, Liu , Belkin, et al. (2012) tried to predict task type by users X  behaviors.
For Task 2, low knowledge users identified uncertainty about the information need (to identify the person for whom the entry will be created) as a top reason, but high knowledge users did not. On the other hand, high knowledge users identified task complexity as a top reason, but the low knowledge users did not. The pattern was seemingly opposite than that in T1, in which low knowledge users felt the task was complex but high knowledge users felt uncertain about the information need. However, a closer look at the two tasks and the task difficulty reasons indicates that the patterns are not opposite for no reason. Different than T1 which was an intellectual task and asked users to collect two schools X  admissions pages (specific and amorphous), T2 was factual and asked users to collect all the information needed to create the Wikipedia entry (amor-phous). Looking at the two tasks, what made them complex was a bit different: for T1, users had to identify the schools and also collect information, for T2, users had to identify the person, collect information, and more importantly, collect as many sources as possible. In this task, the low knowledge users perceived the task difficulty because they had to identify the per-son (uncertain about information need), but high knowledge users showed a further understanding of the task requirement by giving task complexity as a top difficulty reason (collect as much information as possible). In summary of both T1 and T2, high knowledge users seemed to be able to identify the more specific reasons why a task was difficult than low knowledge users. This also seems to require considering the task features/types when discussing the task difficulty reasons, which con-firmed the importance of task type in task difficulty research.

The implication for this on system design is that for amorphous and unnamed factual tasks that are recall-based (different than the intellectual tasks such as Task 1), it would be helpful to have features/functions to help users save the sources, in addition to providing as many related sources about the topic as possible. This would be done in hopes to reduce users X  per-ception of task difficulty caused by task complexity, especially for those high knowledge users.

For Task 3, high knowledge users did not have other top reasons for task difficulty besides the common reason of  X  X  X pecific requirements X  X . Low knowledge users identified three other top reasons: low topic knowledge, too much (unrelated) infor-mation, and hard to formulate queries. The low knowledge reason matches their low knowledge background. For low knowl-edge users, it seems reasonable that they may feel difficulty in formulating queries, and accordingly, the results may not be ideal or relevant, leading them to feel there were  X  X  X oo much unrelated information X  X . Based on the findings in T3, the impli-cation for system design could be to provide easily read results (or results matching with users X  knowledge gain in the task accomplishment process), in addition to query suggestions, especially to the low knowledge users. 5.3. Contributions, limitations, and future directions This paper reports an examination of task difficulty reasons as seen in different task types and user knowledge groups.
The research confirmed a task difficulty reason scheme developed by Liu and Kim (2013) . Furthermore, the original merits of this paper are the analyses of task difficulty reason in different task types and user groups. These contribute to personalize task difficulty reasons by task and user groups.

One limitation of the current research is the nature of the controlled lab experiment. The findings are important in that they explored the relationships between task difficulty reasons and task type. Meanwhile, it is acknowledged that task dif-ficulty reasons may have been limited, to some extent, to the controlled lab experiment and the assigned task nature in this experiment, although we think, on the other hand, that the design of the various task types following the task facet scheme could have taken care of most frequently seen task difficulty reasons. Future studies will collect task difficulty reasons in naturalistic settings.

Besides the above future plan and the implications that have been discussed about the task difficulty scheme, which can be linked with future research, another future direction is to examine if there are relationships between task difficulty rea-sons, users X  search behaviors, and contextual factors including tasks and knowledge background. This can provide us with a better understanding of user behaviors, as well as ways to predict task difficulty reasons based on user behaviors. 6. Conclusions
Based on the data collected in a controlled lab experiment, the paper explored why information searchers felt an infor-tion scheme previously developed by Liu and Kim (2013) was confirmed. It involved several aspects: users, tasks, and user X  X ask interaction. Following this scheme, the difficulty reasons that users gave for each task session were categorized.
Difficulty reasons were compared in different tasks and task types. It was found that the common reasons leading to task difficulty in all four tasks were specific requirements and too much (unrelated) information, meaning that users wanted the system to provide the right, exact, and relevant information in the search result pages. Various difficulty reasons could be linked with the different task types. The study also examined the commonalities and differences between the identified top difficulty reasons between users with different levels of topic knowledge. The possible reasons that led to the different top reasons were discussed, and some implications were provided for changes in system design to personalize searches for different groups of users, besides helping reduce task difficulty for all users in general. Future research can examine the rela-tionships between task difficulty reasons, users X  search behaviors, and contextual factors (tasks, knowledge background, etc.). Future studies can also attempt to collect data and identify the task difficulty reasons in a naturalistic setting, which all help improve our understanding of task difficulty and building IR systems that help reduce users X  perception of difficulty. Acknowledgments graduate students Alison Weber and Chelsea Stone for their assistance with data collection. References
