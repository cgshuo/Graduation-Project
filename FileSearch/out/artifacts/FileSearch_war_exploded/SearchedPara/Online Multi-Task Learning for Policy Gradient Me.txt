 Olin College of Engineering, Needham, MA 02492 USA Matthew E. Taylor TAYLORM @ EECS . WSU . EDU Sequential decision making (SDM) is an essential compo-nent of autonomous systems. Although significant progress has been made on developing algorithms for learning iso-lated SDM tasks, these algorithms often require a large amount of experience before achieving acceptable per-formance. This is particularly true in the case of high-dimensional SDM tasks that arise in robot control prob-lems. The cost of this experience can be prohibitively ex-pensive (in terms of both time and fatigue of the robot X  X  components), especially in scenarios where an agent will face multiple tasks and must be able to quickly acquire control policies for each new task. Another failure mode of conventional methods is that when the production en-vironment differs significantly from the training environ-ment, previously learned policies may no longer be correct. When data is in limited supply, learning task models jointly through multi-task learning (MTL) rather than in-dependently can significantly improve model performance (Thrun &amp; O X  X ullivan, 1996; Zhang et al., 2008; Rai &amp; Daum  X  e, 2010; Kumar &amp; Daum  X  e, 2012). However, MTL X  X  performance gain comes at a high computational cost when learning new tasks or when updating previously learned models. Recent work (Ruvolo &amp; Eaton, 2013) in the super-vised setting has shown that nearly identical performance to batch MTL can be achieved in online learning with large computational speedups. Building upon this work, we in-troduce an online MTL approach to learn a sequence of SDM tasks with low computational overhead. Specifically, we develop an online MTL formulation of policy gradient reinforcement learning that enables an autonomous agent to accumulate knowledge over its lifetime and efficiently share this knowledge between SDM tasks to accelerate learning. We call this approach the Policy Gradient Effi-cient Lifelong Learning Algorithm (PG-ELLA) X  X he first (to our knowledge) online MTL policy gradient method. Instead of learning a control policy for an SDM task from scratch, as in standard policy gradient methods, our approach rapidly learns a high-performance control pol-icy based on the agent X  X  previously learned knowledge. Knowledge is shared between SDM tasks via a latent ba-sis that captures reusable components of the learned poli-cies. The latent basis is then updated with newly acquired knowledge, enabling a ) accelerated learning of new task models and b ) improvement in the performance of existing models without retraining on their respective tasks. The latter capability is especially important in ensuring that the agent can accumulate knowledge over its lifetime across numerous tasks without exhibiting negative transfer. We show that this process is highly efficient with robust theo-retical guarantees. We evaluate PG-ELLA on four dynam-ical systems, including an application to quadrotor control, and show that PG-ELLA outperforms standard policy gra-dients both in the initial and final performance.
 Due to its empirical success, there is a growing body of work on transfer learning approaches to reinforcement learning (RL) (Taylor &amp; Stone, 2009). By contrast, rela-tively few methods for multi-task RL have been proposed. One class of algorithms for multi-task RL use nonpara-metric Bayesian models to share knowledge between tasks. For instance, Wilson et al. (2007) developed a hierarchi-cal Bayesian approach that models the distribution over Markov decision processes (MDPs) and uses this distri-bution as a prior for learning each new task, enabling it to learn tasks consecutively. In contrast to our work, Wil-son et al. focused on environments with discrete states and actions. Additionally, their method requires the ability to compute an optimal policy given an MDP. This process can be expensive for even moderately large discrete envi-ronments, but is computationally intractable for the types of continuous, high-dimensional control problems consid-ered here. Another example is by Li et al. (2009), who developed a model-free multi-task RL method for partially observable environments. Unlike our problem setting, their method focuses on off-policy batch MTL. Finally, Lazaric &amp; Ghavamzadeh (2010) exploit shared structure in the value functions between related MDPs. However, their ap-proach is designed for on-policy multi-task policy evalua-tion, rather than computing optimal policies.
 A second approach to multi-task RL is based on Policy Reuse (Fern  X  andez &amp; Veloso, 2013), in which policies from previously learned tasks are probabilistically reused to bias the learning of new tasks. One drawback of Policy Reuse is that it requires that tasks share common states, actions, and transition functions (but allows different reward functions), while our approach only requires that tasks share a com-mon state and action space. This restriction precludes the application of Policy Reuse to the scenarios considered in Section 7, where the systems have related but not identical transition functions. Also, in contrast to PG-ELLA, Policy Reuse does not support reverse transfer, where subsequent learning improves previously learned policies.
 Perhaps the approach most similar to ours is by Deisenroth et al. (2014), which uses policy gradients to learn a sin-gle controller that is optimal on average over all training tasks. By appropriately parameterizing the policy, the con-troller can be customized to particular tasks. However, this method requires that tasks differ only in their reward func-tion, and thus is inapplicable to our experimental scenarios. We first describe our framework for policy gradient RL and lifelong learning. The next section uses this framework to present our approach to online MTL for policy gradients. 3.1. Policy Gradient Reinforcement Learning We frame each SDM task as an RL problem, in which an agent must sequentially select actions to maximize its ex-pected return. Such problems are typically formalized as a Markov decision process (MDP)  X  X  , A ,P,R, X   X  , where X  X  R d is the (potentially infinite) set of states, A  X  R is the set of possible actions, P : X  X A X X 7 X  [0 , 1] is a state transition probability function describing the sys-tem X  X  dynamics, R : X  X A 7 X  R is the reward function measuring the agent X  X  performance, and  X   X  [0 , 1) speci-fies the degree to which rewards are discounted over time. At each time step h , the agent is in state x h  X  X  and must choose an action a h  X  A , transitioning it to a new state x h +1  X  p ( x h +1 | x h , a h ) as given by P and yielding re-ward r h +1 = R ( x h , a h ) . A policy  X  : X  X A 7 X  [0 , 1] is defined as a probability distribution over state-action pairs, where  X  ( a | x ) represents the probability of se-lecting action a in state x . The goal of an RL agent is to find an optimal policy  X  ? that maximizes the expected re-turn. The sequence of state-action pairs forms a trajectory  X  = [ x 0: H , a 0: H ] over a possibly infinite horizon H . Policy gradient methods (Sutton et al., 1999; Peters &amp; Schaal, 2008; Peters &amp; Bagnell, 2010) have shown success in solving high-dimensional problems, such as robotic con-trol (Peters &amp; Schaal, 2007). These methods represent the policy  X   X  ( a | x ) using a vector  X   X  R d of control param-eters. The goal is to determine the optimal parameters  X  that maximize the expected average return: where T is the set of all possible trajectories. The trajectory distribution p  X  (  X  ) and average per time step return R (  X  ) are defined as: with an initial state distribution P 0 : X 7 X  [0 , 1] . Most policy gradient algorithms, such as episodic REIN-FORCE (Williams, 1992), PoWER (Kober &amp; Peters, 2011), and Natural Actor Critic (Peters &amp; Schaal, 2008), employ supervised function approximators to learn the control pa-rameters  X  by maximizing a lower bound on the expected return of J (  X  ) (Eq. 1). To achieve this, one generates tra-jectories using the current policy  X   X  , and then compares the result with a new policy parameterized by  X   X  . As de-scribed by Kober &amp; Peters (2011), the lower bound on the expected return can be attained using Jensen X  X  inequality and the concavity of the logarithm: log J  X   X  = log where D KL p (  X  ) || q (  X  ) = see that this is equivalent to minimizing the KL divergence between the reward-weighted trajectory distribution of  X   X  and the trajectory distribution p  X   X  of the new policy  X  3.2. The Lifelong Learning Problem In contrast to most previous work on policy gradients, which focus on single-task learning, this paper focuses on the online MTL setting in which the agent is re-quired to learn a series of SDM tasks Z (1) ,..., Z over its lifetime. Each task t is an MDP Z ( t ) = P 0 . The agent will learn the tasks consecutively, acquir-ing multiple trajectories within each task before moving to the next. The tasks may be interleaved, providing the agent the opportunity to revisit earlier tasks for further experi-ence, but the agent has no control over the task order. We assume that a priori the agent does not know the total num-ber of tasks T max , their distribution, or the task order. The agent X  X  goal is to learn a set of optimal policies may be evaluated on any previously seen task, and so must strive to optimize its learned policies for all tasks so far ( 1  X  T  X  T max ). This section develops the Policy Gradient Efficient Life-long Learning Algorithm (PG-ELLA). 4.1. Learning Objective To share knowledge between tasks, we assume that each task X  X  control parameters can be modeled as a linear com-bination of latent components from a shared knowledge base. A number of supervised MTL algorithms (Kumar &amp; Daum  X  e, 2012; Ruvolo &amp; Eaton, 2013; Maurer et al., 2013) have shown this approach to be successful. Our approach incorporates the use of a shared latent basis into policy gra-dient learning to enable transfer between SDM tasks. PG-ELLA maintains a library of k latent components L  X  R d  X  k that is shared among all tasks, forming a basis for the control policies. We can then represent each task X  X  con-trol parameters as a linear combination of this latent basis  X  ( t ) = Ls ( t ) , where s ( t )  X  R k is a task-specific vector of coefficients. The task-specific coefficients s ( t ) are encour-aged to be sparse to ensure that each learned basis compo-nent captures a maximal reusable chunk of knowledge. We can then represent our objective of learning T station-ary policies while maximizing the amount of transfer be-tween task models by: e ( L )= where  X  ( t ) = Ls ( t ) , the L 1 norm of s ( t ) is used to approx-imate the true vector sparsity, and k X k F is the Frobenius norm. The form of this objective function is closely related to other supervised MTL methods (Ruvolo &amp; Eaton, 2013; Maurer et al., 2013), with important differences through the incorporation of J (  X  ) as we will examine shortly. Our approach to optimizing Eq. (2) is based upon the Ef-ficient Lifelong Learning Algorithm (ELLA) (Ruvolo &amp; Eaton, 2013), which provides a computationally efficient method for learning L and the s ( t )  X  X  online over multiple tasks in the case of supervised MTL. The objective solved by ELLA is closely related to Eq. (2), with the exception that the  X  X  (  X  ) term is replaced with a measure of each task model X  X  average loss over the training data in ELLA. Since Eq. (2) is not jointly convex in L and the s ( t )  X  X , most su-pervised MTL methods use an expensive alternating op-timization procedure to train the task models simultane-ously. Ruvolo &amp; Eaton provide an efficient alternative to this procedure that can train task models consecutively, en-abling Eq. (2) to be used effectively for online MTL. In the next section, we adapt this approach to the policy gradient framework, and show that the resulting algorithm provides an efficient method for learning consecutive SDM tasks. 4.2. Multi-Task Policy Gradients Policy gradient methods maximize the lower bound of J (  X  ) (Eq. 1). In order to use Eq. (2) for MTL with policy gradients, we must first incorporate this lower bound into our objective function. Rewriting the error term in Eq. (2) in terms of the lower bound yields e ( L )= where  X   X  ( t ) = Ls ( t ) . However, we can note that J Therefore, maximizing the lower bound of J L ,  X   X   X  ( t ) equivalent to the following minimization problem: min Substituting the above result with  X   X  ( t ) = Ls ( t ) into Eq. (2) leads to the following total cost function for MTL with pol-icy gradients: e T ( L ) = While Eq. (3) enables batch MTL using policy gradi-ents, it is computationally expensive due to two ineffi-ciencies that make it inappropriate for online MTL: a ) the explicit dependence on all available trajectories through haustive evaluation of a single candidate L that requires the optimization of all s ( t )  X  X  through the outer summation. To-gether, these aspects cause Eq. (3) (and similarly Eq. (2)) to have a computational cost that depends on the total number of trajectories and total number of tasks T , complicating its direct use in the lifelong learning setting.
 We next describe methods for resolving each of these inef-ficiencies while minimizing Eq. (3), yielding PG-ELLA as an efficient method for multi-task policy gradient learning. In fact, we show that the complexity of PG-ELLA in learn-ing a single task policy is independent of a ) the number of tasks seen so far and b ) the number of trajectories for all other tasks, allowing our approach to be highly efficient. As mentioned above, one of the inefficiencies in minimiz-ing e T ( L ) is its dependence on all available trajectories for all tasks. To remedy this problem, as in ELLA, we approx-imate e T ( L ) by performing a second-order Taylor expan-sion of J L ,  X  (  X   X  ( t ) ) around the optimal solution:  X  As shown by Ruvolo &amp; Eaton (2013), the second-order Taylor expansion can be substituted into the MTL objec-tive function to provide a point estimate around the optimal solution, eliminating the dependence on other tasks. To compute the second-order Taylor representation, the first and second derivatives of J L ,  X   X   X  ( t ) w.r.t. with: Therefore:  X  =  X  E Policy gradient algorithms determine  X  ( t ) =  X   X  ( t ) ? following the above gradient. The second derivative of J  X  evaluated at  X  ( t ) :  X  Substituting the second-order Taylor approximation into Eq. (3) yields the following:  X  e T ( L )= where k v k 2 A = v T Av , the constant term was suppressed since it has no effect on the minimization, and the linear term was ignored since by construction  X  ( t ) is a minimizer. Most importantly, the dependence on all available trajecto-ries has been eliminated, remedying the first inefficiency. The second inefficiency in Eq. (3) arises from the proce-dure used to compute the objective function for a single candidate L . Namely, to determine how effective a given value of L serves as a common basis for all learned tasks, an optimization problem must be solved to recompute each of the s ( t )  X  X , which becomes increasingly expensive as T grows large. To remedy this problem, we modify Eq. (3) (or equivalently, Eq. (4)) to eliminate the minimization over all s ( t )  X  X . Following the approach used in ELLA, we opti-mize each task-specific projection s ( t ) only when training on task t , without updating them when training on other tasks. Consequently, any changes to  X  ( t ) when learning on other tasks will only be through updates to the shared basis L . As shown by Ruvolo &amp; Eaton (2013), this choice to up-date s ( t ) only when training on task t does not significantly affect the quality of model fit as T grows large.
 With this simplification, we can rewrite Eq. (4) in terms of two update equations: L where L m refers to the value of the latent basis at the start of the m th training session, t corresponds to the particular task for which data was just received, and To compute L m , we null the gradient of Eq. (6) and solve the resulting equation to yield the updated column-wise vectorization of L as A  X  1 b , where: For efficiency, we can compute A and b incrementally as new tasks arrive, avoiding the need to sum over all tasks. 4.3. Data Generation &amp; Model Update Using the incremental form (Eqs. 5 X 6) of the policy gra-dient MTL objective function (Eq. 3), we can now con-struct an online MTL algorithm that can operate in a life-long learning setting. In typical policy gradient methods, trajectories are generated in batch mode by first initializ-ing the policy and sampling trajectories from the system (Kober &amp; Peters, 2011; Peters &amp; Bagnell, 2010). Given these trajectories, the policy parameters are updated, new Algorithm 1 PG-ELLA ( k, X , X  ) while some task t is available do end while trajectories are sampled from the system using the updated policy, and the procedure is then repeated. In this work, we adopt a slightly modified version of policy gradients to op-erate in the lifelong learning setting. The first time a new task is observed, we use a random policy for sampling; each subsequent time the task is observed, we sample trajecto-ries using the previously learned  X  ( t ) . Additionally, instead of looping until the policy parameters have converged, we perform only one run over the trajectories.
 Upon receiving data for a specific task t , PG-ELLA per-forms two steps to update the model: it first computes the task-specific projections s ( t ) , and then refines the shared latent space L . To compute s ( t ) , we first determine  X  and  X  ( t ) using only data from task t . The details of this step depend on the form chosen for the policy, as described in Section 5. We can then solve the L 1 -regularized regres-sion problem given in Eq. (5) X  X n instance of the Lasso X  to yield s ( t ) . In the second step, we update L by first reini-tializing any zero-columns of L and then following Eq. (6). The complete PG-ELLA is given as Algorithm 1. PG-ELLA supports a variety of policy forms and base learners, enabling it to be used in a number of policy gradi-ent settings. This section describes how two popular policy gradient methods can be used as the base learner in PG-ELLA. In theory, any policy gradient learner that can pro-vide an estimate of the Hessian can be incorporated. 5.1. Episodic REINFORCE In episodic REINFORCE (Williams, 1992), the stochastic policy for task t is chosen according a is used to minimize the KL-divergence, equivalently maximizing the total discounted pay-off. The sec-ond derivative for episodic REINFORCE is given by  X  5.2. Natural Actor Critic In episodic Natural Actor Critic (eNAC), the stochastic policy for task t is chosen in a similar fashion to that of change in the probability distribution is measured by a KL-divergence that is approximated using a second-order expansion to incorporate the Fisher information matrix. Accordingly, the gradient follows:  X   X   X  J = G  X  1  X   X  J (  X  ) , where G denotes the Fisher information matrix. The Hes-sian can be computed in a similar manner to the previous section. For details, see Peters &amp; Schaal (2008). Here, we provide theoretical results that establish that PG-ELLA converges and that the cost (in terms of model per-formance) for making the simplification from Section 4.2.1 is asymptotically negligible. We proceed by first stating theoretical results from Ruvolo &amp; Eaton (2013), and then show that these theoretical results apply directly to PG-ELLA with minimal modifications. First, we define: Recall from Section 4.2.1, that the lefthand side of the pre-ceding equation specifies the cost of basis L if we leave the s ( t )  X  X  fixed (i.e., we only update them when we receive training data for that particular task). We are now ready to state the two results from Ruvolo &amp; Eaton (2013): Proposition 1: The latent basis becomes more stable over time at a rate of L T +1  X  L T = O 1 T .
 Proposition 2: 1.  X  g T ( L T ) converges almost surely; 2.  X  g T ( L T )  X  e T ( L T ) converges almost surely to 0. Proposition 2 establishes that the algorithm converges to a fixed per-task loss on the approximate objective function  X  g
T and the objective function that does not contain the sim-plification from Section 4.2.1. Further, Prop. 2 establishes that these two functions converge to the same value. The consequence of this last point is that PG-ELLA does not incur any penalty (in terms of average per-task loss) for making the simplification from Section 4.2.1.
 The two propositions require the following assumptions: 1. The tuples  X  ( t ) ,  X  ( t ) are drawn i.i.d. from a distri-2. For all L ,  X  ( t ) , and  X  ( t ) , the smallest eigenvalue of The second assumption is a mild condition on the unique-ness of the sparse coding solution. The first assumption can be verified by assuming that there is no sequential de-pendency of one task to the next. Additionally, the fact that  X  ( t ) is contained in a compact region can be verified for the episodic REINFORCE algorithm by looking at the form of the Hessian and requiring that the time horizon H ( t ) is finite. Using a similar argument we can see that the magnitude of the gradient for episodic REINFORCE is also bounded when H ( t ) is finite. If we then assume that we make a finite number of updates for each task model we can ensure that the sum of all gradient updates is finite, thus guaranteeing that  X  ( t ) is contained in a compact region. Computational Complexity : Each update begins by run-ning a step of policy gradient to update  X  ( t ) and  X  We assume that the cost of the policy gradient update is O (  X  ( d,n t )) , where the specific cost depends on the par-ticular policy algorithm employed and n t is the number of trajectories obtained for task t at the current iteration. To complete the analysis, we use a result from Ruvolo &amp; Eaton (2013) that the cost of updating L and s ( t ) is O ( k 2 gives an overall cost of O ( k 2 d 3 +  X  ( d,n t )) for each update. We applied PG-ELLA to learn control policies for the four dynamical systems shown in Figure 1, including three me-chanical systems and an application to quadrotor control. We generated multiple tasks by varying the parameteriza-tion of each system, yielding a set of tasks from each do-main with varying dynamics. For example, the simple mass spring damper system exhibits significantly higher oscilla-tions as the spring constant increases. Notably, the opti-mal policies for controlling these systems vary significantly even for only slight variations in the system parameters. 7.1. Benchmark Dynamical Systems We evaluated PG-ELLA on three benchmark dynamical systems. In each domain, the distance between the current state and the goal position was used as the reward function. Simple Mass Spring Damper: The simple mass (SM) system is characterized by three parameters: the spring constant k in N/m, the damping constant d in Ns/m, and the mass m in kg. The system X  X  state is given by the position x and velocity  X  x of the mass, which vary according to a lin-ear force F . The goal is to design a policy for controlling the mass to be in a specific state g ref =  X  x ref ,  X  x experiments, the goal state varied from being g ref =  X  0 , 0  X  to g ( i ) ref =  X  i, 0  X  , where i  X  X  1 , 2 ,..., 5 } . Cart-Pole: The cart-pole (CP) system has been used extensively as a benchmark for evaluating RL algo-rithms (Bus  X oniu et al., 2010). CP dynamics are charac-terized by the cart X  X  mass m c in kg, the pole X  X  mass m p kg, the pole X  X  length l in meters, and a damping parameter d in Ns/m. The state is characterized by the position x and velocity  X  x of the cart, as well as the angle  X  and angular ve-locity  X   X  of the pole. The goal is to design a policy capable of controlling the pole in an upright position.
 Three-Link Inverted Pendulum: The three-link CP (3CP) is a highly nonlinear and difficult system to control. The goal is to balance three connected rods in an upright position by moving the cart. The dynamics are parameter-ized by the mass of the cart m c , rod mass m p,i , length l inertia I i , and damping parameters d i , where i  X  { 1 , 2 , 3 } represents the index for each of the three rods. The sys-tem X  X  state is characterized by an eight-dimensional vector, consisting of the position x and velocity  X  x of the cart, and the angle {  X  i } 3 i =1 and angular velocity {  X   X  i } 3 We first generated 30 tasks for each domain by varying the system parameters over the ranges given in Table 1. These parameter ranges were chosen to ensure a variety of tasks, including those that were difficult to control with highly chaotic dynamics. We then randomized the task order with repetition and PG-ELLA acquired a limited amount of ex-perience in each task consecutively, updating L and the s ( t )  X  X  after each session. At each learning session, PG-ELLA was limited to 50 trajectories (for SM &amp; CP) or 20 trajectories (for 3CP) with 150 time steps each to perform the update. Learning ceased once PG-ELLA had experi-enced at least one session with each task.
 To configure PG-ELLA, we used eNAC (Peters &amp; Schaal, 2008) as the base policy gradient learner. The dimension-ality k of the latent basis L was chosen independently for each domain via cross-validation over 10 tasks. The step-size for each task domain was determined by a line search after gathering 10 trajectories of length 150 .
 To evaluate the learned basis at any point in time, we initialized policies for each task using  X  ( t ) = Ls t = { 1 ,...,T } . Starting from these initializations, learn-ing on each task commenced using eNAC. The number of trajectories varied among the domains from a minimum of 20 on the simple mass system to a maximum of 50 on the quadrotors. The length of each of these trajectories was set to 150 time steps across all domains. We measured performance using the average reward computed over 50 episodes of 150 time steps, and compared this to standard eNAC running independently with the same settings. Figure 2 compares PG-ELLA to standard policy gradient learning using eNAC, showing the average performance on all tasks versus the number of learning iterations. PG-ELLA clearly outperforms standard eNAC in both the ini-tial and final performance on all task domains, demonstrat-ing significantly improved performance from MTL.
 We evaluated PG-ELLA X  X  performance on all tasks using the basis L learned after observing various subsets of tasks, from observing only three tasks (10%) to observing all 30 tasks (100%). These experiments assessed the quality of the learned basis L on both known as well as unknown tasks, showing that performance increases as PG-ELLA learns more tasks. When a particular task was not observed, the recent L with a zero initialization of s ( t ) was used. To assess the difference in total number of trajectories be-tween PG-ELLA and eNAC, we also tried giving eNAC an additional 50 trajectories of length 150 time steps at each iteration. However, its overall performance did not change. 7.2. Quadrotor Control We also evaluated PG-ELLA on an application to quadro-tor control, providing a more challenging domain. The quadrotor system is illustrated in Figure 1, with dynamics influenced by inertial constants around e 1 , B , e 2 , B thrust factors influencing how the rotor X  X  speed affects the overall variation of the system X  X  state, and the length of the rods supporting the rotors. Although the overall state of the system can be described by a nine-dimensional vector, we focus on stability and so consider only six of these state variables. The quadrotor system has a high-dimensional action space, where the goal is control the four rotational velocities { w i } 4 i =1 of the rotors to stabilize the system. To ensure realistic dynamics, we used the simulated model de-scribed by Bouabdallah (2007), which has been verified on and used in the control of a physical quadrotor.
 To produce multiple tasks, we generated 15 quadrotor systems by varying each of: the inertia around the x-I yy  X  [4 . 2 e  X  3 , 5 . 2 e  X  3 ] , inertia around the z-axis I In each case, these parameter values have been used by Bouabdallah (2007) to describe physical quadrotors. We used a linear quadratic regulator, as described by Bouabdal-lah, to initialize the policies in both the learning (i.e., deter-mining L and s ( t ) ) and testing (i.e., comparing to standard policy gradients) phases. We followed a similar experimen-tal procedure to evaluate PG-ELLA on quadrotor control, where we used 50 trajectories of 150 time steps to perform an eNAC policy gradient update each learning session. Figure 3 compares PG-ELLA to standard policy gradients (eNAC) on quadrotor control. As on the benchmark sys-tems, we see that PG-ELLA clearly outperforms standard eNAC in both the initial and final performance, and this performance increases as PG-ELLA learns more tasks. The final performance of the policy learned by PG-ELLA af-ter observing all tasks is significantly better than the policy learned using standard policy gradients, showing the bene-fits of knowledge transfer between tasks. Most importantly for practical applications, by using the basis L learned over previous tasks, PG-ELLA can achieve high performance in a new task much more quickly (with fewer trajectories) than standard policy gradient methods. PG-ELLA provides an efficient mechanism for online MTL of SDM tasks while providing improved performance over standard policy gradient methods. By supporting knowl-edge transfer between tasks via a shared latent basis, PG-ELLA is also able to rapidly learn policies for new tasks, providing the ability for an agent to rapidly adapt to new situations. In future work, we intend to explore the poten-tial for cross-domain transfer with PG-ELLA.
 This work was partially supported by ONR N00014-11-1-0139, AFOSR FA8750-14-1-0069, and NSF IIS-1149917. We thank the reviewers for their helpful suggestions.
