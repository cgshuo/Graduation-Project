 Emotion theories and e motion p sychology are currently concerned with the universa l-ity and cultural relativity of emotional expression, which, in fact, is a key issue e x-ploring  X  wh at is the essence and function of emotion?  X  It is generally acknowledged that emotional expression is both psychobiologically and cultural ly controlled, but the respective effect imposed by psychobiology and culture on em o tional expression remains unexplor ed. The earliest predecessors studying cross -cultural emotion include Charles Darwin [1], Ekman [2] and Izard[3]. They notice that listener s from one cu l-ture have the ability to decode the facial expression of an actor from another culture. They claim that l ike decoding facial e x pression of emotion, people from different culture can decode vocal expression of em o tion. Therefore, from the psychobiological perspective, emotion decoding is universal. Cross -cultural studies on emotion enco d-ing and decoding are ne eded to supply speech technology with culturally -relative emotional expressions. Erickson [1 4 ] made a review on cross -linguistic studies , r e-cen t ly, lot of research have been carrying on cross -cultural research on emotional speech[4 -11], some of the results are consistent such as the p erception of em o tional expressions is more succes s ful when the stimuli are multimodal , facial expression plays a major role in the correct decoding of emotion ; some are inconsistent such as the recognition of the emotions was not influenced by cu l tural differences [ 11 ] , while some thought there exist cross -cultural difference[4 -6]. Besides, they suggest  X  A nger joy and sad  X  may constitute three basic em o tions [10], and listener s from different cultures show different sensitive de grees to the acou s tic parameters when decoding emotions .

Speech communication is a physiological process, conveying both audio and visual information. Human  X  s perception bases on the information transmitted by both cha n-nels. Generally, in speech communicat ion, the information from two channels is co m-plementary and coherent. But when the inform a tion is conflicting and nevertheless integrated then the percept in one of the modalities might be changed by the other modality. [12]
Fagel [12] claims that the stim ulus with conflicting audio and visual content can be perceived as an emotion which is neither the emotion indicated by the audio info r-mation nor the emotion indicated by the visual i n formation, which is called emotional McGurk Effect. It is assumed that t he valence (positive or negative emotion) is pr i-marily conveyed by the visual channel while the degree of arousal is reflected by the audio channel. A match of a positive facial expression with a negative voice will be perceived as joy. Ho w ever, the identi fication of content emotion will be derived from the combination of sad voice with happy facial expression. Other mismatches b e-tween audio and visual information are only perceived as either the emotion indicated by audio channel or the em o tion indicated b y visual channel.

Since the encoding and decoding of emotion may depend on multiple modalities and language backgrounds, the purpose of the present study is to clarify the process of the e n coding and decoding of emotion through a cross -cultural perceptual experiment for multi -modal emotions. The preliminary analysis on Chinese and Japanese liste n-ing to em o tional speech of a Chinese speaker in three conditions of congruent audio -video, audio -only and video -only, reveals that language and culture will impose an influence on the identific a tion of emotion. In the present paper, we will continue to explore the emotional speech communication but modulated in conflicting AV cha n-nels. Here, the issues concerned are as: (i) what is the interplay between the two co n-fl icting AV channels in conve y ing emotional information? (ii) Does the emotional McGurk effect exist when the emotions are conveyed in conflic t ing channels? (iii) Are there any culture effects on perception on the conflicting AV emotions?
The assumptions are : (i) When listeners decode the conflicting AV stimuli, they might rely on some modality more than others across different emotions, i.e. one m o-dality should have stronger em o tional modulation for some emotions than that in another modality. (ii) Although the common ps y chological factor contributes to the emotional communication, the decoding of conflicting AV information will be affec t-ed by linguistic and cu l tural background and (iii) the emotional McGurk effect may also be related to culture norms of the encoder/listener. Table 1 lists the Chinese and the correspon d ing Japanese prompts. In order to control the time spent in the experiment, the prompts were divided into two sets. The se n-tences were matched in the number of syllables, from 1 to 5, with different tonal combinations, in different grammatical structures. The contents of the texts were em o-tionally neutral. The speech data used in the present paper is from a Chinese female student from Beijing Film Academy, who speak s Standard Chinese . Her emotion speech was vid e-otaped with Canon Power Shot TX1 in the sound -proof room. She u ttered the prompts in Table 1 in seven emotional states. T he seven em o tions are c lassified by valence ( positive or negative emot ions) : H a ppiness is positive and  X  SAdness, ANger, DI s-the degree of arousal,  X  S a dness and F ear  X  are being low arousal , while  X  H appiness, A nger and D isgust  X  are being hi gh arousal.

In order to explore the conflicting channel and the McGurk phenomenon, conflic t-ing AV stimuli were obtained through dubbing a vis u al emotion with anot her vocal emotion for the same sentence. Then 5*7*7=245 dubbed stimuli were obtained for each set i ncluding 35 congruent AV t o kens .

Listener s were 10 Chinese college students not knowing Japanese and 10 Japanese college st u dents not knowing Chinese . T hey w ere recruited to identify the emotional states for all the dubbed stimuli and rate the expressive degrees on a 5 -point scale (0 -4 ) , multiple choices are allowed . T he higher the score the more expressive the stim u-lus is. The perceived scores were averaged for each intended emotions to obtain the confu s-ing pattern for Chinese and Japanese listeners respectively. T o depict the perceptual patterns clearly, a kind of spi d er graph s representing the average perceptual scores from these 10 Chin ese and 10 Japanese listeners are plotted in Figure s 1 to 4. Each ring in the graph represents the distribution of the rating scores of one perceived em o-tion for combinations of one facial (/vocal) expression (modality 1) and seven vocal (/ facial) express ions (moda l ity two). These rings are called here Emotion Rings . The change s in the shape and radius reflect the change of the perceptual pattern s . If the ring symmetrically distributes in all direction like a circle, then the perceived em o tion is not relat ed to the second modality. However, if the ring is in an unsymmetrical distribution, it m eans that the facial -vocal combination with higher scoring has a stronger tendency to be perceived as th at emotion, on the contrary, the lower scor e the small er chance . The variation of diameter size corr e lates with rating scores i n various facial -v ocal combinations. 3.1 Comparison on percept ual patterns 3.3.1 Perceptual Results for Chin ese liste n ers (1) Fig . 1 (A) indicates that when  X  X eutral X  facial expression is dubbed with the seven vocal emotions, the two primarily perceived emotions are  X  X eutral X  and  X  S u r-prise X  . And the distribution patterns of the two emotion rings show a tendency to complement each other. The combinations of  X  X eutral X  face with  X  X eutral X ,  X  X appy X ,  X  F ear X ,  X  X ad X  and  X  D isgust X  voices tend to be perceived as  X  X eutral X  while the comb i-nations of  X  X eutral X  face with  X  A ngry X  and  X  Surprise  X  voices tend to be perceived as  X  S urprise X  . Fig . 2 (A) shows that the combinations of  X  X eutral X  voice with varied fac i-al expressions are mainly perceived as  X  X eutral X  . Except for the combination of a  X  X eutral X  voice with a  X  X appy X  face, which is perceived as  X  X appy X  , almost all co m-binations are perceived as  X  X eutral X  . The comb i nation of  X  X eutral X  voice with  X  A ngry X  face is perceived as either  X  D isgust X  or  X  X eutral X  in equal pro b ability, which is another exception. (2) Fig . 1 (B) indicates that the combinations of  X  X appy X  facial expression with va r-ied emotional voices tend to be perceived as  X  X appy X  , which is illustrated by an eve n-ly distr i buted emotion ring. It means that the perception of  X  X appy X  depends more on visual information than audio information, although the facially  X  X appy X  emotion also initiates the percept of  X  S urpris e  X  and  X  X e u tral X  as shown by the two small ring s in the center. Fig . 2 (B) reveals that the combinations of  X  X appy X  voice with varied facial expressions (except for  X  X appy X  face) could not be correctly pe r ceived as  X  X appy X  . The combin a tions of  X  X appy X  voice with  X  X ad X ,  X  S urprise X , or  X  X eutral X  faces are perceived as  X  X eutral X  emotion. (3) Fig . 1 (C) displays the complicated perceptual patterns activated by dubbing  X  A ngry X  face with varied emotional voices. The integrations of  X  A ngry X  face with  X  X appy X ,  X  D i s gust X  and  X  A ngry X  voices can be perceived as  X  A n ger X ,  X  S urprise X  or  X  D isgust X  with almost equal scores. Fig . 2 (C) shows that the combinations of  X  A ngry X  voice with varied facial expressions are primarily perceived as  X  S u r prise X  . Only the combination of  X  A ngry X  voice with  X  X appy X  face is perceived as  X  X a ppy X  . (4) Fig. 1 (D) shows that the combinations of  X  D isgust X  face with varied emotional voices could not be correctly perceived as  X  D isgust X  . When  X  D isgust X  face goes with  X  D isgust X ,  X  S urprise X ,  X  A ngry X  and  X  X appy X  voice, the percept of  X  S urprise X  is induc ed. When  X  D isgust X  facial expression is combined with  X  N eutral X  voice, the percept of  X  X eutral X  emotion is initiated. The combination of  X  D isgust X  f a cial expression with  X  X ad X  voice is perceived as  X  X ad X  with very low rating scores. Fig. 2 (D) specifies tha t the perceptual pattern of combinations of  X  D isgust X  voice with varied facial expre s-sions is sim i lar to that shown in Fig. 1 (D), with most combinations being perceived as  X  S urprise X  except that the combination of  X  D isgust X  voice with  X  X appy X  facial expre s-sion is perceived as  X  X appy X  and the co m bination of  X  D isgust X  voice with  X  X eutral X  face is pe r ceived as  X  X eutral X  . (5) Fig. 1 (E) reveals that most combinations of  X  F ear X  expression with varied em o-tional voices could not be correctly perceived as  X  F ear X  ; i nstead, two obvious rings of  X  S u r prise X  and  X  X eutral X  emotion are displayed. The percept of  X  S urprise is induced when  X  F ear X  expression is combined with  X  Di sgusted X ,  X  S urprise X  or  X  A ngry X  voices. The percept of  X  X e u tral X  emotion is initiated when  X  F ear X  ex pression is dubbed with  X  X eutral X  voice. However, the result is vague when  X  F ear X  expression is dubbed with  X  X ad X  ,  X  X a p py X  or  X  F ear X  voices. Fig. 2 (E) shows that the perception of  X  F ear X  voice with varied facial expressions is ambiguous with rating scores lower than two points except that the co m bination of  X  F ear X  voice with  X  X appy X  face brings a percept of  X  X appy X  . (6) Fig. 1 (F) shows that when  X  X ad X  face is dubbed with varied emotional voices, two emotion rings of  X  S urprise X  and  X  X eutral X  are exhibited in a symmetrical pattern. Sp e cifically, the combinations of  X  X ad X  face with  X  A ngry X ,  X  S urprise X  and  X  D isgust X  voices lead to the percept of  X  S urprise X  and the combinations of  X  X ad X  face with  X  X e u-tral X  and  X  X a p py X  voices are perceived as  X  X eutral X  emotion. Wh en  X  X ad X  face goes along with  X  F ear X  or  X  X ad X  voice, either  X  X eutral X  or  X  X ad X  is pe r ceived with almost equal scores. Fig. 2 (F) reveals that two overlapped emotion rings are formed when  X  X ad X  voice is combined with varied facial expressions (except for  X  X a ppy X  face), namely,  X  X ad X  and  X  X e u tral X  emotion rings. However, their rating scores are very low, which are less than 2 points. The co m bination of  X  X ad X  voice with  X  X appy X  face is more likely to be perceived as  X  X a p py X  . (7) The two symmetrically distribut ed emotion rings in Fig. 1 (G) display the pa t-terns of the  X  S urprise X  face dubbed with varied em o tional voices: a  X  S urprise X  ring derived from the combinations of  X  S urprise X  face with  X  A ngry X ,  X  S urprise X  and  X  D i s-gust X  voices; and a  X  X eutral X  emotion ring d erived from the combinations of  X  S u r-prise X  face with  X  X eutral X ,  X  X appy X ,  X  X ad X  and  X  F ear X  voices. 
In Fig. 2 (G), the perceptual pattern of surprised voice with varied facial expre s-sions is represented by a do m inant  X  S urprise X  emotion ring. But the combination of  X  S urprise X  voice with  X  X appy X  face is i n clin ed to be perceived as  X  X appy X  . 3.1.2 Perceptual Results for Japanese li s teners (1) Fig.3 (A) indicates that when  X  X eutral X  face is dubbed with non - X  Neutral X  voi c-es, the emotion stimuli are primarily perceived as  X  X eutral X  . Fig.4 (A) shows that the combinat ions of  X  X eutral X  voice with varied facial expre s sions could not lead to the dominance of any emotion ring, except that the combin a tion of  X  X eutral X  voice with  X  X appy X  face is r e garded as  X  X appy X  ; and neutral voice with  X  N eutral X  face is regarded as neutra l. The combination of  X  X eutral X  voice with an  X  Angry X  or  X  D isgust  X  face is perceived as either  X  D isgust X  or  X  A n ger X  , with almost equal scores less than 2 points. (2) In Fig .3 (B), the large and symmetrically distributed emotion ring shows that the combinat ions of  X  X appy X  face with varied emotional voices are perceived as  X  X appy X  , sign i fying that the visual modality contributes more than the audio modality in the identif i cation of  X  X appy X  . In other words, the perception of  X  X appy X  can be independent of the a udio modality. Fig.4 (B) reveals that the co m binations of  X  X appy X  voice with varied f a cial expressions (except for  X  X appy X  face) could not be perceived as  X  X appy X  . The co m binations of  X  X appy X  voice with  X  Angry X  or  X  X isgust X  faces are most likely to be perc eived as  X  X ngry X  and then as  X  X isgust X  . The combination of  X  X appy X  voice with  X  X e u tral X  face is mainly perceived as  X  X eutral X  . (3) Fig.3 (C) presents two dominant rings, with the  X  X isgust X  emotion ring embe d-ded in the  X  X ngry X  emotion ring. This perceptual pattern is triggered by int e grating  X  X ngry X  facial expression with varied emotional voices. Fig.4 (C) shows that the int e-grations of  X  X ngry X  voice with varied facial expre s sions are primarily perceived as  X  X ngry X  . Only the integration of  X  X ngry X  voice with  X  X appy X  facial expression is pe r-ceived as  X  X appy X  . and  X  X ngry X  ring, which are resulted from the combinations of  X  X isgust X  facial expre s-sion with varied emotional voices . Fear can be perceived when  X  X isgust X  facial e x-pression is dubbed with  X  Fear X  or  X  X ad X  voice. Fig.4 (D) specifies that the combin a-tions of  X  X isgust X  voice with  X  Angry X  ,  X  X appy X  and  X  X eutral X  facial expre s sions are perceived as  X  X ngry X  ,  X  X appy X  and  X  X eutra l X  respectively, while the perce p tual scores of other combinations are very low and show no obvious tendencies. face dubbed with varied emotional voices (scores &lt; 2 points) . Fig.4 (E) reveals that except that the perception of the combination of  X  X ear X  voice with  X  X a p py X  face is identified as  X  X appy X  and the combination of  X  X ear X  voice with  X  X eutral X  face is ide n-tified as  X  X eutral X  , all the scores of the combinations of  X  X ea r X  voice with other facial expressions are lower than 2 points. al expression with varied emotional voices. Fig.4 (F) also reveals that there is no o b-vious perceptual tendenc y (scores &lt; 2 points) when  X  X ear X  voice is co m bined with varied facial expressions except for  X  H appy X  and  X  N eutral X  facial expressions. The perception scores for  X  X ear X  and  X  X ad X  are equal. The combin a tions of  X  X ear X  voice with  X  X appy X  and  X  X eutral X  facial expressions tend to be perceived as the em o tion implied in the facial expression. (7) Fig.3 (G) shows no obvious perceptual tendencies under conditions of  X  X u r-prise X  face with varied emotional voices. Each pe r ceptual score is less than 2 points. It can b e concluded form Fig.4 (G) that the combinations of surprised voice with varied facial expressions cannot be recognized as  X  S urprise X  ; the combination of surprised voice with  X  X appy X  facial expression is inclined to be perceived as  X  X appy X  ; the combinations of surprised voice with  X  X ngry X  and  X  X isgust X  facial expressions t end to be perceived as  X  X ngry X  ; and the combination of surprised voice with  X  X e u-tral X  facial expression is perceived as  X  X eutral X  . 3.2 Comparison of perceptual patterns between Chinese and Japanese for Fig ures 5 ~ 8 show the average perceptua l score as a function of the intended emotion by vocal and facial expressions for Chinese and Japanese listener s in AV and CAV conditions. F rom the perspective of psychological dimension of emotion, Ch i nese and Japanese have similar perceptual patterns: fo r emotions with high arousal, the visual modality makes a major contribution to emotion decoding; while for emotions with low arousal, the audio modality makes a major contribution. In the AV -congruent setting, the perceptual scores of the Chinese for  X  X eu tral X  ,  X  X appy X  , and  X  X u r prise X  are higher than those of the Japanese, signifying higher confidence for the Chinese; while the scores for  X  X ngry X  ,  X  X isgust X  ,  X  X ad X  and  X  X ear X  are lower than those of the Jap a-nese, signifying lower confidence for the Ch i nese. The comparison of Fig . 5 with Fig . 7 indicates that there is a sha r per drop in the scores of the Japanese than the Chinese according to vocal emo tions. Fig. 6 and Fig. 8 reveal that the degree of falling accor d-ing to facial emotions between the Japanese a nd the Chinese is similar except  X  S u r-prise X  and  X  X eutral X  emotion. The results may imply that, for Japanese listeners, d e-coding Chinese emotion counts more on the visual modality than the audio modality, and their decoding for  X  X e u tral X  and  X  S urprise X  faci al expression is better than the Chinese. It confirms the results in the previous study that in cross -cultural commun i-cation the facial information could help non -native listeners in emotion decoding than only vocal information. By further comparing Fig .5 with Fig . 7, Fig . 6 with Fig . 8, we find the tendency that Japanese listeners are consistent with Chinese li s teners where visual mo d ality exists, while they are discrepant for vocal modality conditions. The result supports the assumption that cross -cultural effect also exists when decoding information transmitted in incongruent channels, and that this effect is greater in the vocal channel than the f a cial channel. 3.3 Emotional M c Gurk effect Table 2 show s the cases of Emotional McGurk effect o b tained for th ose cases with listener ;  X  H appy voice + A ngry face -&gt; D isgust  X  for Jap a nese listener .
It was shown that Emotional McGurk e f fect distributes differently between Chinese and Japanese. T here is only one set of combin a tion (surprise voice with various facial expressions) where McGurk effect is not observed. But there are four sets of comb i-n a tions (neutral, fear, happy facial expression with various voice and angry voice with var i ous facial expressions) where McGurk effect is not obser ved. The  X  X hird emotion X  in McGurk effect is most likely to be surprise and more likely to be neutral for Ch i-nese but most likely to be anger and more likely to be disgust for Japanese. These results demonstrate that culture has effect on the decoding of e motion. Taking a fu r-ther look at the Ch i nese data, we find that the case where McGurk effect is observed is normally related to either a negative vocal or a negative visual expression (here ambiguous emotion state concerning intended negative or positive one, here it is e x-pressed more negative). Visual expression of  X  S urprise, F ear and S adness  X  tend to be perceived as  X  N eutral  X  em o tion. For Japanese, the combination where M cGurk effect is observed is also the one where either vocal expression or visual expression of em o-tion is negative. Of these combinations, as long as visual modality indicates the em o-tion of anger, the combination will be reco g nized as  X  D isgust  X  , most of t he other cases will be d e coded as  X  A ngry  X  .
 The main conclusion is that cultural bac k ground poses difference in the perceptual patterns between the Chinese and the Japanese. From the perspective of psychological dime n sion of emotion, in the AV -conflicting setting, the Chinese and the Japanese have similar patterns: for em otions with high arousal, the visual modality makes a major co n tribution to emotion decoding; while for emotions with low arousal, the audio modality makes a major contribution. Due to linguistic and cultural di f ference, the Chinese liste n ers make more use of the audio modality to decode emotion; for those Japanese who don X  X  know Chinese, the emotion recognition counts more on the visual modality and their decoding of  X  X eutral X  and  X  X urprise X  facial expression is better than that of the Chinese. Regarding t o the rating confidence, the Ch i nese give higher scores than the Jap a nese. The findings here are different from those in [ 12 ], which assumed that the visual modality mainly transmits valence (positive or negative emotion) and the audio modality mainly tran smits arousal (the degree of e x citement). One explanation for this discrepancy lies in the difference in the number of emotions: seven in our research and only four in [ 12 ].

The emotional McGurk effect is found in the AV  X  conflicting experiment. Though the occurrence of the McGurk effect relates highly to negative emotions, the perce p-tion patterns are different due to the culture effect. Inconsistent with Fagel [56], no cases of the emotional McGurk effect relating to positive emotions are found. To some ext ent, the o c currence frequency of the McGurk effect shows that the Chinese listeners have a te n dency to jump to a conclusion (  X  X urprise X  ) while the Japanese favor ambiguity (  X  X  n ger X ,  X  X isgust X  or  X  X eutral X  ). 
The results support the assumptions that (1) Whe n listeners decoding the conflic t-ing AV stimuli, they might rely on some modality more than another across different emotions, as shown in Table 3 -7. (2) Although common psychological factor contri b-utes to the emotional communication, the decoding of confl icting AV information will be a f fected by culture background, and (3) the emotional McGurk effect exists, and it may also be r e lated to cultural norms of the encoder/listener.

Future research will focus on more speakers from various cultures to verify the emotional McGurk effect patterns .
 This work was supported by the National Basic Research Program (973Program) of China (No. 2013CB329301) , NSFC Project with No. 60975081 and CASS innovation project.

