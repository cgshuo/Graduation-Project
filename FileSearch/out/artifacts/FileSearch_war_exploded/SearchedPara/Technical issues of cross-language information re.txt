 1. Introduction
Cross-language information retrieval (CLIR) is the circumstance in which a user tries to search a set of documents written in one language for a query in another language. The issues of CLIR have been dis-cussed for several decades. As widely recognized, research efforts for developing CLIR techniques can be traced back to Gerard Salton  X  s articles in the early 1970s (e.g., Salton, 1970 ).

Especially after the advent of the World Wide Web in the 1990s, CLIR has become more important, the research community of IR has begun to tackle problems of CLIR extensively and intensively. The
Workshop on Cross-Linguistic Information Retrieval held in August 1996 during the SIGIR  X  96 Conference is frequently cited as an epochal event for promoting research on CLIR. Currently, CLIR issues are addressed in workshops of large-scale retrieval experiments such as TREC,
CLEF and NTCIR. As described in the introductory paper to this issue, each workshop has been concerned with languages other than English as follows: TREC: Spanish, Chinese, German, French, Italian, and Arabic.
 CLEF: French, German, Italian, Swedish, Spanish, Dutch, Finnish, and Russian so far.
 NTCIR: Japanese, Chinese and Korean.

Various research findings on CLIR have been reported at the meetings of TREC, CLEF and NTCIR, and many papers have been published in scientific journals and proceedings.

This article aims at reviewing techniques and methods for enhancing performance of CLIR. We already have a comprehensive review on this topic ( Oard &amp; Diekema, 1998 ). In addition, Peters and Sheridan (2001) cover a wide range of literature and topics on CLIR. The main purpose of this article is to examine literature subsequent to the review by Oard and Diekema and to attempt to organize research results since and Diekema (1998) will be referred to again in this article.

However, it should be noted that this review cannot be completely comprehensive because of the large number of papers on CLIR published in various research areas. The purpose here is to provide a useful map of technical issues of CLIR, rather than extensively enumerating research papers on CLIR. This paper dia data is outside our scope.

The rest of the paper is organized as follows. First, in Section 2, we discuss techniques to match query terms with document representations in the CLIR. More specifically, various methods of translation are described. Section 3 is dedicated to explaining some techniques for solving the problem of term ambiguity, which may occur in the process of translation. Some formal models for CLIR are introduced in Section 4.
In particular, we describe the application of the language model (LM), which enables us to combine the retrieval model and the translation model. In Section 5, other important CLIR research topics are dis-cussed: the pivot language approach, search of multilingual document collections, combination of language
CLIR. Finally, Section 6 briefly discusses the future direction of CLIR research. 2. Matching strategies and translation 2.1. Matching strategies 2.1.1. Types of matching strategies
The most basic approach to CLIR is to automatically translate the query into an equivalent in the lan-guage of the target documents. The translation makes it possible to execute matching operations between the query and each document, and subsequently, compute document scores according to a standard retrie-val model such as the vector space or probabilistic model.

However, this is only the starting point. Oard and Diekema (1998) have identified four types of strategies for matching a query with a set of documents in the context of CLIR ( Oard &amp; Diekema, 1998, pp. 230 X  232 ):  X  No translation (1) Cognate matching  X  Translation (2) Query translation (3) Document translation (4) Interlingual techniques 2.1.2. Cognate matching terminology are left unchanged through the stage of translation. The unchanged term can be expected to tic relationship.

Some researchers have tried to add devices for matching cognates more effectively. For example, Davis (1997) introduced fuzzy matching based on edit distance between Spanish cognates and English words.
Interestingly, for CLIR from English to French, Buckley, Walz, Mitra, and Cardie (1998) pointed out that  X  X  X nglish query words are treated as potentially misspelled French words, X  X  and attempted to treat English words as variations of French words according to lexicographical rules. Similarly, Hiemstra and Kraaij (1999) applied fuzzy matching to every query term X  X  X ot just untranslatable terms.

An alternative approach to cognate matching may be to decompose words in both the query and doc-ument into n -grams (more specifically, character-based overlapping n -grams), and to perform matching ployed the n -gram technique for dealing with untranslatable query words, and McNamee and Mayfield (2002b) similarly applied n -grams to cognate matching.

When two languages are very different, e.g., English and Japanese, the techniques of edit distance and n -grams may not work well. However, in such cases, we can utilize phonetic transliteration from English words for cognate matching. Gey (2001) stated that  X  X  ... we can often find that many words, particularly in technology areas, have been borrowed phonetically from English and are pronounced similarly, yet with phonetic customization in the borrowing language. X  X  By exploring a method for measuring similarity be-tween transliteration and its original word, we may make cognate matching feasible. For example, Knight and Graehl (1998) studied automatic recognition of Japanese transliteration. The problem of Arabic trans-literation was tackled by Stalls and Knight (1998) . In general, the problem of matching between a source word and its transliteration has been frequently discussed for the cases of English X  X apanese, English X  X hi-nese, English X  X orean, and so on (see Fujii &amp; Ishikawa, 2001 ). 2.1.3. Query translation
Query translation is the most widely used matching strategy for CLIR due to its tractability. That is, the uments (although it should be noted that, if we focus on only real-time online settings, query translation may take more time because the query must always be translated after it is entered by a user). disambiguation X  X  ( Oard &amp; Diekema, 1998, p. 231 ). Term disambiguation will be discussed later. 2.1.4. Document translation
Document translation has opposite advantages and disadvantages from query translation. In CLIR experiments, this approach is not usually utilized, and query translation is dominant. However, some available for translation, which can improve translation quality.

Oard and Hackett (1998) reported that automatic machine translation of a set of documents using a commercial MT system outperforms query translation in an experiment of CLIR from German to English. similarly translated a document collection by MT software. On the other hand, Franz et al. (2000) proposed a  X  X  X ast Document Translation X  X  algorithm for dealing with a large set of documents within a reasonable amount of processing time. The algorithm is based on a statistical approach to machine translation devel-oped by the IBM group ( Brown, Della Pietra, Della Pietra, &amp; Mercer, 1993 ). 2.1.5. Interlingual techniques
Finally, in interlingual techniques , an intermediate space of subject representation into which both the query and the documents are converted is used to compare them. Oard and Diekema (1998) categorized latent semantic indexing (LSI) and controlled-vocabulary techniques based on multilingual thesauri as interlingual techniques. In an early work, Landauer and Littman (1990) used the LSI method to create a multidimensional indexing space for a parallel corpus of English and French documents.
Suppose that a parallel corpus includes N documents and each document has a pair of equivalent texts in two languages. We denote an M 1  X  N term-document frequency matrix in one language by X the distinct number of terms in the language, and similarly assume that X frequency matrix for another language. Each element of X 1 of a term within the text of a document. Then we constitute an M  X  N matrix such that where M = M 1 + M 2 . If a singular value decomposition (SVD) such that X = U K V allows us to calculate each document score for a query in both of the two languages as a value of inner Landauer, 1998 ).

A similar approach was also employed by Berry and Young (1995) , Dumais, Landauer, and Littman (1996) , Littman, Dumais, and Landauer (1998) , Mori, Kokubu, and Tanaka (2001) , etc. It should be noted that, in order to apply LSI to CLIR, a parallel (or comparable) corpus is needed to construct a language-independent indexing space.

Another type of interlingual approach is to use the  X  X  X ynsets X  X  provided in WordNet, which is a well-known machine-readable thesaurus. For example, Diekema, Oroumchian, Sheridan, and Liddy (1999) em-ployed the WordNet synset numbers as language-independent representations for CLIR. Since a synset number (label) representing a concept is corresponded to a set of concrete words in each of languages sup-lish X  X hinese IR. 2.2. Translation techniques
It is widely recognized that there are three main approaches to translation in CLIR:  X  Machine translation (MT).  X  Translation by bilingual machine-readable dictionary (MRD).  X  Parallel or comparable corpora-based methods.
In addition some researchers have recently attempted to make use of WWW resources for obtaining translation equivalents. 2.2.1. Machine translation techniques
Intuitively, the MT system seems to be a fine tool for CLIR, and actually, if good MT software is avail-able, the CLIR task becomes easier. However, in the case of query translation, the MT approach has not always shown better performance than that of dictionary-based translation. For example, Ballesteros and Croft (1998) reported that dictionary-based techniques outperformed a popular commercial MT system.
One of the reasons is that, as mentioned above, queries are often short and do not provide sufficient con-ally try to select only one translation from the many candidates that the source words may have. Nie, Sim-ard, Isabelle, and Durand (1999) pointed out that  X  X  X y limiting the selection to only one word, the MT process prevents the IR system from expanding the original query by synonyms or related words. X  X  2.2.2. Dictionary-based methods
Using a bilingual MRD is the general approach for CLIR when no commercial MT system with an words X  X  architectures, in which both query statements and document texts are decomposed into a set of words (or phrases) through a process of indexing. Thus we can translate a query easily by replacing each query term with its translation equivalents appearing in a bilingual dictionary or a bilingual term list.
Ballesteros and Croft (1997) first pointed out problems with this method as follows:  X  Specialized vocabulary not contained in the dictionary will not be translated.  X  Dictionary translations are inherently ambiguous and add extraneous information.  X  Failure to translate multiterm concepts such as phrases reduces effectiveness.

These defects can be considered the main reasons for the degradation of CLIR performance in compar-tette (1996) stated that  X  X  ... we learn that translation ambiguity and missing terminology are the two primary sources of error ...  X  X  Also, they reported that manual translation of multiword noun phrases con-tributes to improvement of retrieval performance. This suggests the importance of translation of multiterm concepts.

Many methods have been proposed for solving problems of term ambiguity and phrasal translation as degree by combining multiple resources such as other dictionaries or term lists generated from parallel (or comparable) corpora (see also Section 5.3). 2.2.3. Parallel corpora-based method Parallel or comparable corpora are useful resources enabling us to extract beneficial information for
CLIR. As mentioned already, the cross-language LSI approach makes use of this kind of corpus for con-allel or comparable corpus. For example, in order to translate English queries into Spanish, Davis and Dunning (1995) extracted moderately frequent Spanish terms from Spanish documents aligned with
English documents which had been searched using an English query (source query). A similar technique was tested by Yang, Carbonell, Brown, and Frederking (1998) in which they applied a technique of pseudo-relevance feedback to enhance the effectiveness of the search of the parallel corpus. Sheridan and Ballerini (1996) , Braschler and Scha  X  uble (2000, 2001) and Molina-Salgado, Moulinier,
Knudson, Lund, and Sekhon (2002) attempted to generate a similarity thesaurus from a comparable or par-allel corpus based on associations between each pair of terms, which are computed from statistics on the number of documents in which both of the terms appear. The similarity thesaurus was used for obtaining translation equivalents of source query terms through a kind of query expansion. Similarly, Yang et al. (1998) empirically investigated the search performance of a method using a co-occurrence matrix of source and target terms for the translation of query terms.

Similar approaches of generating a bilingual term list from a parallel or comparable corpus have also been utilized by other researchers. For example, Chen, Gey, Kishida, Jiang, and Liang (1999) and Chen (2002) employed the logarithm of the likelihood ratio, 2log k ( Dunning, 1993 ) for measuring the associ-ation between a source term and a target term. McNamee, Mayfield, and Piatko (2001) seem to use a meas-ure similar to mutual information (MI) for generating a bilingual term list from a parallel corpus. In
On the other hand, some researchers in the CLIR field have attempted to estimate translation probability from a parallel corpus according to a well-known algorithm developed by a research group at IBM ( Brown allel corpus. The IBM algorithm includes five models, Model 1 through Model 5, of which Model 1 is the simplest and is often used for CLIR. The fundamental idea of Model 1 is to estimate each translation prob-ability so that the probability represented such that is maximized, where t is a sequence of terms t 1 , ... , t s 1993 ).

Franz, Scott McCarley, and Roukos (1999) employed a similar algorithm for estimating translation probabilities between a pair of terms. Nie (1999) attempted to compare performance between the probabi-the WWW. In addition, research groups employing the language model approach to CLIR (see below) have 2001 ; Kraaij, 2002 , etc.).
 Some issues on the use of the IBM model for CLIR were described extensively with actual examples in
Nie, Simard, and Foster (2001) and Nie and Simard (2002) , e.g., translation of ambiguous words or com-lation X  X  for solving the problem that some common words are often included as higher-ranked translations common words (however, the technique did not show good performance). 2.2.4. Use of WWW resources
As mentioned above, Nie et al. (1999) and Nie (2000) have employed WWW resources effectively for allel corpus from the Web. The WWW can provide rich and ubiquitous machine-readable resources, from which we may be able to automatically extract information useful for CLIR. For example, Chen (2002) and
Chen and Gey (2003) made use of a general search engine on the Internet and tried to find English trans-lation equivalents of Chinese or Japanese terms (mainly proper nouns) by analyzing contexts of these terms in Chinese and Japanese Web documents returned by the engine. This method employs the fact that these that there is room for applying a more sophisticated technique of natural language understanding in order to extract useful information on translation from texts of Web pages. 3. Term disambiguation techniques 3.1. Translation ambiguity
Disambiguation from among multiple alternative term translations, for which many researchers have at-word sense disambiguation (WSD) plays an important role in various applications such as machine trans-ambiguate translations enumerated under each headword in a bilingual MRD or a term list generated from a parallel corpus.

If all translations listed in bilingual resources are straightforwardly employed as search terms, extrane-
Thus it is desirable that only relevant terms will be automatically or semi-automatically selected from a set of translations.
 ambiguity of words having essentially different senses.

Several more sophisticated methods have been explored in the field of CLIR: (1) Use of part-of-speech (POS) tags. (2) Use of parallel corpus. (3) Use of co-occurrence statistics in the target corpus. (4) Use of the query expansion technique.

It should be noted that the term  X  X  X orpus-based disambiguation X  X  is often used in literature when collec-tionary (see Section 3.6). 3.2. Use of part-of-speech tags
The basic idea of using part-of-speech (POS) tags for translation disambiguation is to select only trans-lations having the same POS with that of the source query term. Davis (1997) and Davis and Ogden (1997) applied a part-of-speech tagger to English queries for solving translation ambiguity in CLIR tasks from method was tested by Ballesteros and Croft (1998) . This method requires that POS tagging software is available for both languages.
 3.3. Parallel corpus-based disambiguation
A parallel corpus was used for determining the  X  X  X est X  X  translation or set of translations by Davis (1997,
MRD according to the result of searching a parallel corpus. The procedure is as follows: (1) Pick a set of translations for each term in the source (English) query from an MRD. (2) Search a part each set of Spanish documents. (3) Search the English part of the parallel corpus for the source query. (4) Select a single translation of which set of documents is closest to the set of documents searched by the source query.
 This procedure is repeated for each query term so as to obtain a final set of the  X  X  X est X  X  translations.
Boughanem and Nassr (2001) and Boughanem, Chrisment, and Nassr (2002) employed a parallel corpus in a similar way for disambiguating translations obtained by dictionary search.
 some top-ranked documents were determined by searching the part of the corpus written in the source lan-guage for the original query. Next, some top-ranked terms were extracted directly from the target language was selected according to the scores in the ranked term list. 3.4. Disambiguation based on co-occurrence statistics
Ballesteros and Croft (1998) pointed out that  X  X  X he correct translations of query terms should co-occur in target language documents and incorrect translations should tend not to co-occur. X  X  Suppose that two terms,  X  X  X ercury X  X  and  X  X  X lanet, X  X  are included in the source language (English) query. Although the term sonably expected that a correct translation equivalent of  X  X  X ercury X  X  will tend to co-occur with a correct from the target collection, Ballesteros and Croft (1998) employed a variation of mutual information (MI) computed from co-occurrence of a pair of two terms. According to the values for all possible pairs of two translation equivalents, we may choose a correct combination of translations. In Ballesteros and Croft (1998) , this technique was actually applied to the disambiguation of phrasal translation (see below).
In order to disambiguate translation equivalents using MI or other similarity measures, we first have to two translations that have the maximum value. After that, the source query terms for which translations were determined are excluded, and the same procedure is repeated until translations of all source terms are selected. Similar techniques with some modifications have been described by many subsequent research-Uemura (2000) , Adriani (2001) , Sadat, Maeda, Yoshikawa, and Uemura (2002) , Qu, Grefenstette, and exceeds a threshold.
 included in the source query because a large number of possible combinations of translation candidates must be considered. To alleviate this problem, Gao et al. (2001a, 2001b) proposed an approximate algo-rithm using  X  X  X ohesion X  X  of a term t with a set T of other terms such that lations for the other source terms. A similar approach with modification was also employed by Seo et al. of terms such as  X  X (have, sub-verb, I). X  X 
However, it is possible to unexpectedly generate false combinations by simply using co-occurrence sta-tistics because translation equivalents of two unrelated terms in the context of a given source query may reduce false combinations. First, the two most related terms in the query were determined based on co-occurrence statistics in the source language corpus, and then the  X  X  X est X  X  translations were selected from pus. It should be noted that these two corpora do not have to be parallel or comparable.
Federico and Bertoldi (2002) proposed a query translation model incorporating a hidden Markov model (HMM) for estimating joint probability of two sequences of source terms and translations. In the model, a rences in the target corpus. We can expect that this probability works as a device for translation disambiguation.

Finally, we should pay attention to other sources from which co-occurrence statistics can be gener-ated. For example, Maeda et al. (2000) and Qu et al. (2003) have attempted to make use of a Web search engine in order to obtain co-occurrence frequencies. They sent a combination of possible translations to a
Web portal (e.g., AltaVista), and employed the number of pages it returned as a score for selecting translations. 3.5. Query expansion for disambiguation
Pseudo relevance feedback (PRF), also known as blind feedback, is widely recognized as an effective technique for enhancing performance of information retrieval. Usually, a set of search terms in a query given by a user is expanded by adding terms automatically selected from top-ranked documents that are searched for the original query. Ballesteros and Croft (1997, 1998) have empirically shown that PRF also works effectively for CLIR tasks.

In the case of CLIR, two kinds of PRF are feasible:  X  Pre-translation feedback and  X  Post-translation feedback
First, documents from a corpus in the source language can be retrieved prior to translation in order to back ). Ballesteros and Croft (1997) suggested that pre-translation feedback may contribute to improvement term respectively. That is, synonyms or related terms corresponding to the  X  X  X orrect X  X  meaning of each source term within a context of the query are expected to be automatically added through the PRF process. subsequent process of translation. Recently, McNamee and Mayfield (2002a) reported that pre-translation query expansion is very useful when lexical coverage of translation resources is poor. On the other hand, iments of monolingual retrieval.

In CLIR, two well-known methods for weighting terms in the top-ranked documents are often utilized irrelevant documents. It should be noted that in PRF, some number of top-ranked documents are presup-posed to be relevant.

On the other hand, based on the probabilistic model of Robertson and Sparck Jones (1976) , a weight w of term t j is computed such that and where r j is the number of relevant documents including t the number of all relevant documents, and N is the total number of documents included in the database. Some attempts have made to improve performance of the probabilistic feedback method. For example,
Sakai, Koyama, Suzuki, and Manabe (2003) used an alternative form such that and also proposed other variations incorporating scores of relevant documents. 3.6. Structured query model
Another idea for dealing with multiple translations for each source query term is to consider them as a set of synonyms. From this point of view, conventional Boolean logic may be useful. Hull (1998) suggested without dramatically increasing the weight of the underlying concept. X  X  Hull (1998) actually proposed a probabilistic indexing model enabling us to rank documents based on a query in which Boolean operators are incorporated.

Meanwhile, Pirkola (1998) attempted to use the synonym operator #SYN( ) provided in the INQUERY source query terms. Use of #SYN( ) can prevent overweighting certain source query terms with multiple the #SYN operator and the #UWN operator, a kind of proximity operator for treating compound terms, od ) has been subsequently used by other research groups ( Darwish &amp; Oard, 2003 ; Hedlund, Keskustalo, 2000 ). Darwish and Oard (2003) developed a method for incorporating translation probability into Pir-kola  X  s method.

To be precise, the structured query model should not be classified with disambiguation techniques, but it has the same effect with that of disambiguation in that the model can enhance performance of search for a query including many ambiguous translations. 3.7. Another method for disambiguation
Boughanem et al. (2002) , explored a  X  X  X i-directional translation X  X  technique in which a form of backward translation is used for ranking translation candidates. Suppose that we need to translate English query is found in an English X  X rench dictionary. Next, using a French X  X nglish dictionary, each French equivalent
French translation equivalent is chosen as a preferred translation. 3.8. Phrasal translation techniques
As Ballesteros and Croft (1997) pointed out that  X  X  ... failure to translate multiterm concepts as phrases a bilingual dictionary or a term list including phrases or compound words as headwords. Phrases or com-pound words can be automatically identified in the source query by matching operations against headwords of an MRD, or by using part-of-speech tags. For example, we can assume a word combination of  X  X  X oun X  noun X  X  or  X  X  X djective X  X oun X  X  to be a compound word. Thus the identified compound word can be replaced with the corresponding term in the target language by searching in a bilingual dictionary.
However, it is inevitable that we will be confronted with insufficient coverage of lexical resources to be used. When an untranslatable phrase is found in the source query, we are forced to execute word-by-word translation, which, incidentally, causes a term ambiguity problem. Suppose that a phrase consisting of two words appears in the source query where one has two translations and the other has three from an MRD. In example of the problem of ambiguity in phrasal translation.

A simple way is to automatically adopt all combinations as multiword expressions. Alternatively, Bal-lesteros and Croft (1997, 1998) proposed a sophisticated disambiguation method based on co-occurrence statistics mentioned above and reported that the statistical method shows good performance. Theoretically, phrases or compounds can be categorized as compositional or non-compositional. According to Pirkola et al. (2001) ,  X  X  X ompounds whose meaning can be derived from the meanings of component words are called compositional compounds, X  X  and  X  X  X  compound whose meaning cannot be deduced on the basis of its com-ponent is called a non-compositional compound. X  X  Thus, clearly the co-occurrence statistics-based method has limitations for detecting translations of non-compositional phrases. It would be promising to combine both dictionary-based and statistical methods in order to compensate for the drawbacks of each method. 4. Formal model for CLIR 4.1. Retrieval model for estimating document scores
If the query is translated into the language used in the document collection and the weight of each query hoc retrieval. Typically, document scores for ranked output are calculated using inverted files, and docu-ments are sorted in decreasing order of scores.

Thus, in CLIR, standard retrieval models or algorithms for estimating document scores such as the vector
Gey, 1994 ), etc., have been employed. In particular, as mentioned above, the INQUERY system may hold a unique position in CLIR because the structured query approach can be implemented by using special functions of the INQUERY system (#SYN or #UWN operators). PIRCS ( Kwok, 1996 ), a kind of probabi-2001 ).
 Recently, some researchers have attempted to apply the so-called language model (LM) to CLIR tasks. guage processing of query statements and document texts, it may be natural to apply the LM to problems of IR. 4.2. Language model for CLIR ing the query given a language model of a document, X  X  and to use the value as a document score ( Hiemstra, 1998 ; Ponte &amp; Croft, 1998 ). A simple form of the probability can be written such that porated in order to prevent the probability of a query term not appearing in the document from becoming zero. When weights of each query term are incorporated, the above formula is slightly changed (see Miller, Leek, &amp; Schwartz, 1999b for details).

One of the advantages of the language model approach to CLIR tasks is to enable us to put translation ways to incorporate the translation probability: The formula (I) has been used in Xu et al. (2001) , Fraser, Xu, and Weischedel (2003) and Franz and Scott den Markov Model (HMM) to an IR problem ( Miller, Leek, &amp; Schwartz, 1999a, 1999b ). Since then, for-mula (II) has been employed by Hiemstra and Kraaij (1999) , Kraaij, Pohlmann, and Hiemstra (2000) , etc.
With respect to (II) , Hiemstra, Kraaij, Pohlmann, and Westerveld (2001) proposed a new relevance feed-back method for CLIR using the language model. The main difference between the two formulas is that in formula (I) , a corpus in the language of the query is needed for estimating probability P ( t ).
Lavrenko, Choquette, and Croft (2002) explored another formal method for applying language modeling in specifies how often we expect to see any given word in the documents relevant to the query.
The translation probabilities can be estimated by the following methods:  X  EM algorithm developed by IBM group ( Brown et al., 1993 ).  X  Use of information in a bilingual dictionary.
A simple method of using information in a dictionary is to count the number of translations for each source term. For example, if a source term s has n translations, t weighting translations, Hiemstra and Kraaij (1999) seem to use a more complicated procedure based on the number of distinct  X  X  X enses X  X  of the source term that each translation covers. 5. Other research topics in CLIR 5.1. Pivot language approach
So many languages are spoken in the world, and it is not always possible to obtain the bilingual re-sources we need for a particular pair of languages. A promising technique to circumvent the problem of language acts as a mediator between two languages for which no bilingual resource is available. Suppose that a CLIR task between Japanese and Dutch is requested by a user. In this case, machine-readable resources of Japanese X  X utch pairs may be unavailable, and it would be easier to find Japanese X  X nglish and
Dutch X  X nglish resources since English is such a widely used language. Thus CLIR between Japanese and Dutch can be performed via English (as an intermediary) without direct bilingual resources of Japanese and Dutch.

The pivot language approach may also alleviate the problem of explosive combinations of languages, i.e., if we have to perform CLIR between each pair of n languages, O( n
A basic way of using the pivot language approach would be a transitive translation of a query using two anese X  X nglish and English X  X utch dictionaries are available, CLIR can be performed by replacing Japanese query terms with the corresponding English equivalents and successively substituting the English equiva-lents with the Dutch equivalents. Of course, if Japanese X  X nglish and English X  X utch MT systems can be used, a similar transitive translation is also feasible.

With the case of dictionary-based transitive translation, translation ambiguity can become a more seri-ous problem. It is possible that resulting translations become doubly ambiguous if each replacement stage yields ambiguity: (1) from the source language to the intermediate language and (2) from the intermediate alents, simple replacements are going to produce 64 (=4 3 which would inevitably contain some irrelevant translations. To solve this problem, Ballesteros (2000) at-tempted to apply the disambiguation methods mentioned above (co-occurrence-based method, query expansion, etc.) to transitive translation and attained a substantial improvement in search performance. lation ambiguity problem in which two pivot languages are used independently and removal of erroneous translations is attempted by taking only translations in common from two ways of transitive translation using two pivot languages.

The pivot language approach has been utilized in TREC, NTCIR, and CLEF due to unavailability of bilingual resources. For example, the following transitive combinations of languages have been explored:  X  English&gt;French&gt;German ( Franz et al., 1999 )  X  French&gt;English&gt;German, etc. ( Gey, Jiang, Chen, &amp; Larson, 1999 )  X  German&gt;English&gt;Italian ( Hiemstra &amp; Kraaij, 1999 )  X  Japanese&gt;English&gt;Chinese ( Lin &amp; Chen, 2003 )  X  Chinese&gt;English&gt;Japanese ( Chen &amp; Gey, 2003 )
In particular, Franz et al. (1999) proposed some interesting techniques for searching German documents with English queries: (1) Convolution of translation probability: Estimating translation probability from an English term e to a (2) Automatic query generation from the intermediate language corpus: Generating French queries 5.2. Merging strategy for multilingual information retrieval
Suppose that we have a multilingual document collection in which two or more languages are mixed (not task is more complicated than simple bilingual CLIR. In CLEF and NTCIR, multilingual CLIR has been adopted as a research task, and many research groups have worked on the issue.

Basically, there are two approaches for multilingual IR ( Lin &amp; Chen, 2003 ):  X  Distributed architecture in which the document collection is separated by language, and each part is indexed and retrieved independently. ument collection and is indexed in one huge index file.

In distributed architectures, a standard bilingual search is repeatedly performed for each separate language sub-collection respectively, and several ranked document lists are generated by each run. documents in any language are successfully ranked. Essentially, the merging strategy is a general research merge ranked lists obtained from each resource. In CLIR, the following merging strategies have been investigated:  X  Raw score : straightforwardly using document scores estimated in each run.  X  Round robin : interleaving each document list in a round robin fashion by assuming that distribution of relevant documents is identical among the lists.  X  Normalized score : normalizing document scores by each run in order to remove effects of collection-dependent statistics on estimation of the scores.  X  Rank-based score : mathematically converting ranks in each run into scores by assuming a relationship between the rank and probability of relevance.  X  Modified score : modifying raw scores in each run so as to reduce effects of collection-size dependency, translation ambiguity, etc.
If the retrieval model employed for each run can estimate relevance probability of each document cor-raw scores). For example, Chen and Gey (2003) simply merged the results from Chinese, Japanese and Eng-
However, in most cases, it would be difficult to consider each document score to be a pure probability of evant documents are distributed in the same way in every separate language sub-collection, a simple strat-egy is round robin-based merging, in which only the rank of each document is taken into account.
Otherwise, an alternative method is to use normalized document scores such that where v is a raw score, and v min and v max are the minimum and maximum in each run respectively ance among the four strategies of round robin, raw score, normalized score and the CORI approach (see
Callan et al., 1995 for details) using the CLEF test collection and reported that normalized score is dom-inant among them. Similarly, Moulinier and Molina-Salgado (2002) tried to conduct comparisons among round robin, raw score, CORI, normalized score and collection-weighted normalized score (a variation of normalized score), and reported that collection-weighted normalized score showed higher mean average precision.

Other techniques for estimating optimal scores for merging ranked lists have been proposed. Franz et al. mating raw scores. Meanwhile, Lin and Chen (2003) proposed a method of modifying raw scores based on the degree of ambiguity when each source query was translated, according to an assumption that a good translation may give much more relevant documents. Savoy (2003a) tested a logistic regression formula for predicting a relevance probability of a document given a rank and a score of the document.
On the other hand, for the centralized architecture, the set of multilingual documents is not divided into sub-collections for each language. In order to search such a heterogeneous collection, we need either (1) to translate the source query into all languages included in the document collection and to merge all translations into a single query, or (2) to translate the documents into a single language used in the query.

Gey et al. (1999) , Chen (2002) and Nie and Jin (2003) employed the first method for searching the CLEF having fewer documents may take advantage of weighting by document frequency ( Lin &amp; Chen, 2003 ). 5.3. Combination of some language resources formance of CLIR. Specifically, in the case of searches between two unrelated languages in which cognate to deal with untranslatable terms. Actually, McNamee and Mayfield (2002a) experimentally confirmed a conjecture that retrieval performance drops with decreased lexical coverage when using the CLEF test collection.

A promising approach to the problem of poor lexical coverage is to merge results of translations from multiple language resources. Xu et al. (2001) and Darwish and Oard (2003) combined three dis-ity of each pair of terms was calculated as an average of three values of probability obtained from each resource.
 Jones and Lam-Adesina (2002) also explored techniques of data fusion and query combination ( Belkin, the case of data fusion, two document scores computed from outputs by two MT systems were simply summed for each document. In the case of query combination, before the estimation of document scores, a single query was formed by taking the unique terms from two outputs. The authors discuss the mathe-matical characteristics of the two techniques. 5.4. Language processing issues 5.4.1. Text processing for languages other than English Traditionally, research articles on IR in international journals have largely concentrated on searching have been mainly published in the countries in which the language is spoken as the mother tongue. How-various languages other than English. Thus papers on this topic have been published in international con-ferences or journals, and many language resources (e.g., MRDs, stopword lists, stemmers, morphological analyzers, etc.) for various languages have become available on the Internet.

A typical procedure for processing the text of queries and documents in each language is as follows. (1) Tokenization of text into a set of terms (2) Assignment of part-of-speech tag (3) Stopword removal (4) Lemmatization (5) Stemming (6) Noun phrase extraction 5.4.2. Tokenization
Complexity of text processing depends on which language is targeted. For example, as well-known in the case of the Chinese and Japanese languages, there is no explicit boundary between words in each sentence. Thus tokenization of text in such languages is more complicated than that of European languages.
However, it might be necessary to solve similar problems in some European languages. For example, decomposition of compound words is significant for mono-or cross-language retrieval of German or Swed-compound words have been explored by many researchers. 5.4.3. Stopword list
There are two methods for creating a stopword list in a new language: statistical and linguistic. The sta-ument collection in that language and chooses the top n words by frequency to be the stop list. The list into the target language and use that list in processing the new language.
A general guideline for creating a stopword list was given by Fox (1990) , and English and French stop-ating German and Italian stopword lists in line with the guidelines by Fox (1990) . 5.4.4. Stemming
Porter  X  s algorithm ( Porter, 1980 ) is widely used to stem English words in IR. Although effectiveness of normalization by stemming for English monolingual IR has not yet been shown explicitly seems that the use of stemmers brings about greater improvement of retrieval performance ( Savoy, languages are needed at various stages of processing, the development of effective stemmers is certainly important to enhance search performance. We can obtain a number of rule-based stemmers for European languages from Porter  X  s SNOWBALL project ( http://snowball.tartarus.org/ ), in which Porter (2001) pro-vides an excellent description of the components which are essential in the creation of a good rule-based stemmer.

Oard, Levow, and Cabezas (2001) employed a four-stage  X  X  X ackoff translation X  X  for locating a term with-in a translation lexicon, in which four matching operations are performed: (1) matching of the surface form of a term to surface forms of headwords in the lexicon, (2) matching of the stem of a term to surface forms of headwords, (3) matching of the surface form of a term to stems of headwords, (4) matching of the stem of a term to stems of headwords.

In addition, Oard et al. (2001) have proposed a  X  X  X tatistical stemming X  X  approach in order to automati-unsupervised acquisition of morphology in the field of computational linguistics ( Oard et al., 2001 ). 5.5. User interfaces for interactive CLIR
Although most of the research literature on CLIR implicitly treats the search as a task to be performed automatically by a machine, a practical approach for providing better search results would be to develop tive CLIR , a well-designed user interface would play an important role. An early interactive CLIR system for English X  X panish, QUILT, accommodates some functions and GUI for supporting query translation ( Davis &amp; Ogden, 1997 ):  X  Options for displaying the query translation terms from a bilingual lexicon.  X  Pop-up windows that show the retrieved Spanish document with translated Spanish query terms highlighted.  X  A Pop-up window that shows the variant translations of each English term.

QUILT also seems to have a feature that displays English gloss translations of Spanish documents re-trieved by the system. A more recent version of QUILT was described by Davis and Ogden (2000) . Some other systems have been developed such as FromTo-CLIR ( Kim et al., 1999 ), MULINEX ( Capstick et al., 2000 ) and so on. Peters and Sheridan (2001) also listed several working CLIR systems. In CLEF-2001, a challenging track for exploring interactive applications of CLIR, iCLEF, was included.
The track was concerned with current technology for supporting interactive relevance assessment, and sev-in following CLEF campaigns.
 5.6. Evaluation of CLIR
In order to develop better techniques or methods for automatic or interactive CLIR, a continuing se-quence of experimental evaluations is indispensable. From this viewpoint, we must admire the efforts of TREC, CLEF and NTCIR and their huge contributions toward significantly promoting and enhancing
CLIR research. Descriptions of the systems and findings of these retrieval experiments can be found in the working notes and proceedings of these activities.

Many techniques described in this article were proposed and experimentally tested in the campaigns organized by these three initiatives. Very useful research findings on the performance of the CLIR tech-niques have been cumulated through the evaluation process. Unfortunately, the provision of details of per-formance levels is outside the scope of this review.

Methodology used for evaluation is also an important topic for CLIR research. Standard Cranfield-type methods have been basically used to assess CLIR experiments in TREC, CLEF and NTCIR. However, it should be noted that CLIR experiments have a unique characteristic, that is, the performance of search runs executed using queries in the language of document collections can be employed as a baseline for the evaluation. For example, results of English to Japanese CLIR runs can be evaluated by comparing them with those of Japanese monolingual runs if corresponding Japanese queries are correctly prepared by human translators. We can usually assume that the monolingual runs give us an upper limit of perform-ance. The overviews of the TREC, CLEF and NTCIR, and the introductory paper of this special issue provide us with more useful information on evaluation methodologies for CLIR tasks. 6. Concluding remarks: future directions for research
Through a review of the literature, this paper has described research issues on CLIR and discussed various techniques that can be adopted. As mentioned in the introduction, this review does not cover all research research fields and communities. We can cite Oard and Dorr (1996) , Oard and Diekema (1998) , Peters and Sheridan (2001) , and Fujii and Ishikawa (2001) as sources for identifying additional papers. The last issue we should discuss is the future direction of CLIR research. What is the goal of CLIR?
What should the next steps be to achieve the goal? In the workshop on CLIR held at SIGIR 2002, the organizers presented three challenges ( Gey, Kando, &amp; Peters, 2002 ): 1. Where to get resources for resource-poor languages? 2. Why do we not have a sizeable Web corpus in multiple languages? 3. Why aren  X  t search engines using our research? documents that they cannot read, what is the utility? X  X  This is a crucial point for considering the future direction of CLIR research. That is, we may need to make a plan after having a clear grasp of information needs of users on CLIR and explicitly delineating realistic utility when applications of CLIR are employed by the actual users.

Meanwhile, various interesting areas for CLIR research seem to remain, e.g., CLIR for multimedia data, cross-language question answering, cross-language filtering, cross-language topic detection and tracking, cross-language summarization, cross-language document clustering, and so on. This review article cannot ble the actual users to effectively and efficiently satisfy their information needs. Acknowledgement The author would like to thank the referees and the editors for useful suggestions.
 References
