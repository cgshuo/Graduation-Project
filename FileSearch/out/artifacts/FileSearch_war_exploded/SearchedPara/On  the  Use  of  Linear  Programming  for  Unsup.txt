 We propose a new algorithm for dimensionality reduction and un-supervised text classification. We use mixture models as underly-ing process of generating corpus and utilize a novel, L1-norm based approach introduced by Kleinberg and Sandler [19]. We show that our algorithm performs extremely well on large datasets, with peak accuracy approaching that of supervised learning based on Sup-port Vector Machines with large training sets. The method is based on the same idea that underlies Latent Semantic Indexing (LSI). We find a good low-dimensional subspace of a feature space and project all documents into it. However our projection minimizes different error, and unlike LSI we build a basis, that in many cases corresponds to the actual topics. We present results of testing of our algorithm on the abstracts of arXiv-an electronic repository of scientific papers, and the 20 Newsgroup dataset -a small snapshot of 20 specific newsgroups.
 H.3.3 [ Information Storage and Retrieval ]: Clustering, Infor-mation Filtering; F.2.2 [ Analysis of Algorithms and Problem Complexity ]: Non-numerical Algorithms and Problems; H.1.2 [ Information Systems ]: Models and PrinciplesUser/Machine sys-temsHuman information processing Algorithms, theory, experimentation.
 Dimensionality reduction, generative models, mixture models, un-supervised learning, L1 norm, latent class models, linear program-ming, LSI, singular value decomposition, text classification.
With the growing availability of large text collections contain-ing documents about every possible topic, the following question Copyright 2005 ACM 1-59593-135-X/05/0008 ... $ 5.00. often arises: is it possible to sort the documents into different cate-gories without (or with minimal) human intervention? A significant understanding of supervised learning -e.g. the setting where an al-gorithm has some pre-classified data as a part of the input, has been achieved in the last decade with plethora of very fast and accurate methods available [9, 17]. However, the problem of unsupervised text classification in most cases still remains a challenge. To the best of our knowledge, all existing text clustering algorithms being used for text classification suffer from practicality problems. Most notably, inefficiency -they can not operate on collections larger than few hundred documents, and/or inadequate classification ac-curacy.

In this work, we believe we take an important step towards an efficient and accurate algorithm for unsupervised text classifica-tion. Our technique is based on a previous work of the author with Kleinberg [19], and the idea is similar in spirit to the one that un-derlies Latent Semantic Indexing( LSI ) [7, 25]: Each document is represented as a vector of term frequencies in the space of all possible terms. Following the LSI approach, we find a good low-dimensional subspace in this space and project each document vec-tor into it, while trying to preserve as much of the inherent structure as possible. However, in contrast to LSI , our projection operator is based on minimizing L 1 error and we show that it works much better than SVD projection. In particular the basis of the topical space in many cases actually corresponds to underlying topics, and na  X  X ve clustering according to highest coefficient is very efficient.
One important feature of our method is that, within underlying model we reconstruct the precise underlying term distribution for each document with high probability. While spectral techniques do provide the same type of guarantees [1, 25, 24]. the choice of a model based on minimization of L 2 -norm of the error is not neces-sarily justified [14]. Furthermore, recent work [15, 22, 10], suggest, that spectral techniques might be not well suited for text classifica-tion and other learning problems where Zipf X  X  law distributions are involved. While proper normalization might help in specific cases [5, 6], to the best of our knowledge, no general remedy is known. Our approach is based on L 1 norm that seem to work well for ar-bitrary distributions. Preferability of L 1 over L 2 -norm for partic-ular learning problems has also been observed by Ng [23] and Ke and Kanade [18]. Our explanation is that L 2 norm puts too much weight on heavy components that constitute only a small part of the system. For example, in most text documents, stop-words have much higher frequency of appearance than topic-specific terms, and yet in most cases, they possess no information about topics. While the stop-words themselves could be eliminated using stop-words list, they illustrate a more general problem introduced by frequent terms for L 2 based methods. Overview of the model. We use the following model to de-scribe the process of generating corpus. Intuitively, each term in every document is sampled from a mixture of few underlying dis-tributions (e.g. topics). The mixture coefficients might be different for every document, however the underlying distributions are the same for the whole corpus. In contrast to Latent Dirichlet Allo-cation [7, 4], we don X  X  require our topic distributions to be of any specific shape, but our bounds depend on a specific measure of in-dependence of topics, which we will define below.
 Overview of the algorithm. The algorithm starts by building a  X  X ixture model X , which is equivalent to the underlying model. Here we relax the definition of mixture model (and hence the quotes) to allow mixtures coefficients describing particular document to be negative. However we still require the resulting mixture to remain a probability distribution. The algorithm then computes approximate coefficients using linear programming, and finally uses the learned model to compute underlying term distributions. It can be shown [19] that within the mixture model framework, the recovered term distribution is very close to the true hidden term distributions. In other words, for each document, we can fully recover its underly-ing term distribution. While this could be used as an independent term-smoothing pre-processing step, and/or as a query enhance-ment mechanism, that automatically includes synonymy search, we leave this as a foundation for future work. In this paper we concen-trate on the mixture model found by our algorithm, and each doc-ument X  X  mixture coefficients. It turns out that the model we build, in many cases closely resembles true model, and thus mixture co-efficients could be used to estimate relevance of each document to the actual topics. Furthermore with relatively few topics (e.g. 5 or less) our algorithm performance is approaching the performance of supervised Support Vector Machines with large training dataset. To the best of our knowledge, this is the first unsupervised algorithm capable of operating on large datasets, and to achieve such perfor-mance. All our experiments were conducted on the collection of
In Section 3.1, we show that when there are only 2 topics, such behavior is provable within the model. For multi-topic problem, our empirical evaluation suggest that found basis is still closely connected to actual topics and later we present some additional in-tuition that favors this conjecture. However larger datasets and/or additional analysis are needed to confirm or reject this statement in its full generality.

We also evaluate performance of our algorithm as an intermedi-ate dimensionality reduction. In other words, while the clustering according to the highest coefficient is not effective, we still might be able to show that most of this information is still preserved after we have projected down all the documents, and hence potentially allowing to run more accurate, but less efficient algorithm. We show that the accuracy loss due to our method is much smaller than the one of obtained from singular value decomposition, that is the only method known to us that would allow to do fully 3 unsuper-vised large scale dimensionality reduction. 2 Paper organization. In the next section we describe standard generative mixture model, and introduce all necessary definitions and notation. In Section 3 we describe the algorithm along with the 1 See http://www.arxiv.org , www/data/news20.html , seeing some labelled data [2] intuition behind it. The forth section contains experimental results, and finally we conclude the paper with open problems and further directions.
 Remark 1 . Linear Programming has previously been used for clus-tering previously. Particularly, correlation clustering introduced by Bansal, Blum and Chawla [3], uses linear and semidefinite pro-grams to produce approximations for a specific graph clustering problem [28, 8]. However, their application is very different from ours in the sense that the programs were obtained as relaxations of corresponding integer program. Whereas for our method, as we will show below, it is a natural (and optimal) setting.
We use Multiple Cause Mixture Model [19, 4, 25, 26, 27, 20, 13, 14] to describe the process of text corpus generation. Each document is modelled as a sample of fixed size from a mixture of possibly overlapping distributions over a set of all possible terms. The coefficients for the mixture are possibly unique for every doc-ument, but the underlying distributions are the same for the whole corpus.

Formally, there is a collection of documents C of the size | C | . All words used in the collection, form a dictionary D of the size n = | D | . Finally, there are k topics and each topic probability distribution W c over the whole dictionary D . Each document d is a sample of fixed size from a distribution defined by the mixture with coefficients P d 1 ,...P dk (E.g. WP d ). Vector P d of hidden mixing coefficients is called relevance vector , and D d is called the term distribution vector. Naturally, we require that P d 1 = D d 1 =1 . The normalized vector  X  term frequencies as they occur in d is called a signature vector , and
We use the following naming conventions. Latin letters c , always denote topics, w and v to denote terms and d to denote documents. Specifically, when we use these letters to denote matrix indices they will  X  X ype-check X  with the semantic meaning of the index. For example, W cw would denote the probability of word w to appear in the document purely on topic c .

For a particular document d , we will use d and  X  d to denote its hidden term distribution and observed term frequencies(e.g. the signature vector) respectively, and p for its hidden relevance vector. In other words p is a column P d , and d is equal to W p .
By slightly abusing notation, when it is clear from the context, we will be using  X  x , to denote both random variable, and a particular observation of  X  x .
While our algorithm is very similar to the algorithm presented in [19], we have to recast it to text classification problem. Addition-ally the original algorithm was not practical enough to be run on large collections. Therefore for the sake of completeness, we de-scribe our algorithm in full and present some basic intuition behind it. For a more formal exposition we refer the reader to the original work.

The algorithm runs in two stages. First it finds some  X  X uit-able X  mixture model for a given collection C . This will define our topical subspace. We do this via analyzing a matrix of word co-occurrences. Second, for each document it recovers mixture coef-ficients in the built mixture model. For this it uses linear program-ming to build optimal projection operator. In order to maintain the property of  X  X ag of words X  formulation in general. clarity, these steps are presented in the reverse order, since the sec-ond part can be viewed as an independent learning technique, while the first relies on the ideas used in the second step.
Suppose we know the mixture model, how do we find mixture coefficients? A traditional approach would be to use EM-based methods [21], however, that in general might converge to a sub-optimal solution. Out method is based on the use of general-ized pseudoinverses [19], and it provably recovers coefficients with small additive error, with high probability.
 We begin with a simple definition: For arbitrary rectangular n  X  k ( n  X  k ) matrix W , matrix called a generalized pseudoinverse of W if W  X  W = I .
 If possible generalized pseudoinverse matrix is obtained from the sin-gular value decomposition [11]. However we use a pseudoinverse which minimizes its maximal element.

The reason why we need the pseudoinverse with this property is as follows. Consider an arbitrary document d of length s with term distribution and relevance vectors d and p respectively. Obviously d = W p , and thus This implies that if we knew underlying the term distribution then we can find p and thus solve the classification problem.
However, instead of d , we only have a signature vector  X  while E  X  d = d , the vector  X  d is very sparse and thus is a bad approximation for an underlying term distribution d . Neverthe-less, recall that we express mixture coefficients in equation (1), as a weighted sum of the elements of d . Thus, even though each indi-vidual  X  d i is a bad approximation for d i , one can apply tail inequal-ities to the weighted sum W  X   X  d i . Therefore, we can write: This fact is formulated in the following lemma.

L EMMA 3.1 ([19]). Let  X  d be a document signature with at least s words in it, and let V be an arbitrary k  X  n matrix, with maximal element bounded by B , then if s  X  B 2 k P ROOF . Note that  X  d can be represented as a sum independent vectors, where the i -th term in this sum is an indicator vector for i -th word in the document. The rest is a simple corollary of Chebyshev inequality and the union bound.
 This lemma implies that if a pseudoinverse with bounded maximal element exists, then it immediately gives us an algorithm to find a document relevance vector: multiply pseudoinverse W  X  by a sig-nature vector  X  d and the result is a good approximation to relevance vector. Obviously the smaller is the maximal element of W better error bound we obtain.

Now we again use the result of [19], which states that there is always a pseudoinverse such that its maximal element is bounded by 1  X  , where  X  is a quantity defined as and is called independence coefficient . Intuitively, this number re-flects how independent are these distributions. For example, if  X =0 , then W has linearly dependent columns and no general-ized pseudoinverse exists. A somewhat simplistic example here is to imagine that one large topic such as  X  X omputers X , almost purely consists of a few subtopics such as  X  X omputer software X  and  X  X om-puter hardware X . The underlying term frequency distribution for  X  X omputers X  would be a normalized sum of term distributions for  X  X oftware X  and  X  X ardware X  topics, and thus independence coeffi-cient would be close to zero for such system. Obviously, it is impossible to classify whether document is relevant to computers  X  X nly X , or it also related to hardware and software, based on only its term histogram. Conversely if  X =1 , then the term distributions for different topics are disjoint, and the corresponding pseudoin-verse is simply an indicator matrix for these topics. In our setting, this corresponds to the case when documents on different topics use non overlapping vocabulary. For example, one can think of a collection of documents written on different languages, and where topics correspond to different languages.

Obviously, the actual value of this coefficient is defined by the collection of documents. In our experiments we have very rarely observed the value of  X  to be below 0.3, and have never encoun-tered it below 0.1.

T HEOREM 3.2 ([19]). For any n  X  k matrix W = { W wc } such that  X =min pseudoinverse W  X  = { x cw } such that max | b pseudoinverse can be found in polynomial time.

P ROOF . The desired generalized pseudoinverse (if it exists) can be found by solving the following linear program, 8 for the existential part of the proof we refer to [19] This theorem gives us a way to do supervised learning, by using training data to learn topic distributions  X  W , to learn underlying term distribution for each topic. Then we compute the pseudoin-ture coefficients of unseen document d by applying pseudoinverse  X  W Input: Collection of documents C , with a pre-labeled subset C C Output: Classification c d for each document d Description: 1. For each topic c , compute  X  W c as a word distribution in doc-2. Compute pseudoinverse  X  W  X  using linear program from the-3. For each unlabeled document d , compute  X  p = W  X   X  We would like to reemphasize here that the number of words in a document, needed to learn its mixture coefficients is independent of the size of the dictionary. It only depends on the desired confidence  X  , accuracy  X  and the independence coefficient  X  .
 Remark 1 . It is also possible to use for projection a pseudoinverse obtained from Singular Value Decomposition (or any other). How-ever the value of the maximal element might be dependent on the dictionary size, thus making tail inequalities not immediately ap-plicable.
In this section, our goal is to build a model, which would allow us to apply the learning algorithm of the previous section. Recall that we have shown that if the following conditions are satisfied: Then where  X  hides a small additive error. This in turns implies [19]: VV  X   X  d  X  d . Thus it would be sufficient to find a model with underlying distributions that span the same, or approximately the same subspace as the true topics.

While choosing just random vectors that span the same subspace as the topics, would immediately allow us to perform dimensional-ity reduction of the text data, nevertheless in general it is not enough for the text clustering per se. However, it turns out that the basis we build seem to be closely connected to the actual underlying top-ics, thus suggesting to do a na  X   X ve classification according to the coordinate with the largest value. This method can be shown to be correct when only two topics are involved, and at the end of this section we present some additional retrospective intuition on why the model we build is related to the hidden topic distributions.
One source of vectors that are linear combinations of W co-occurrence matrix R .
 the random variable which indicates, how many times words and v have occurred together in the same document, across the whole corpus. Then the matrix of expectations R = E  X  R is called the c o-occurrence matrix. The actual observation of  X  R which we see in the text (and also denote as  X  R ) is called the o bserved co-occurrence matrix.
 In other words a column of co-occurrence matrix, corresponding to term w , is an aggregate term distribution in all the documents containing term w .

Let L be a diagonal m  X  m matrix, such that L dd contains the length of document d . Then it is easy to see that R ( WP ) L ( L  X  I ) 2 ( WP ) T , and thus columns of R are indeed lin-ear combinations of topical distributions. Obviously true co-occurrence matrix is hidden, and we only know  X  R . Nevertheless it can be shown that as the number of documents | C | grows rel-atively to the size of the dictionary | D | , the normalized observed co-occurrence matrix uniformly approaches R , and they span ap-proximately the same subspace. In [19] authors argued that an al-gorithm, that tries to maintain a large independence coefficient, by greedily choosing columns of normalized  X  R , will result in that  X ( V )  X  f ( X ( W ) ,k ) , for some function f . As a corollary, they showed that using V as a topic distributions results in accurate recovery of underlying distributions.

However, while being feasible to run, their algorithm would still take a very long time to run on a large dataset.

We use heuristics to speed up the algorithm and to reduce the required sample complexity. First, we don X  X  compute the full co-occurrence matrix, but only the columns which correspond to the words which have occurred at least t times and are not in the stop-words list. Parameter t is an empirical value and is set to approximately 1/10 of the dictionary size. This serves as both a speeding up and an error reduction measure: i.e. we don X  X  ana-lyze words, if there is not enough statistics for them. The resulting matrix is called truncated co-occurrence matrix 5 . Second, our in-dependence coefficient is heuristically approximated as a minimal L 1 distance between the columns.

Now we are ready to present the algorithm which finds the model.
 Input: Observed co-occurrence matrix  X  R , number of topics parameter t .
 Output: Topical space V .
 Description: 1. Remove columns from R corresponding to stop-words, or 2. Let ( w 1 ,w 2 )=argmax R w 1  X  X  w 2 1 , set V 1 to R 3. For each c in between 3 and k :
Our algorithm creates matrix V , which (in the limit) spans the same subspace as W , and that is sufficient for our LP algorithm to learn coefficients.

While our algorithm potentially might fail to find k sufficiently independent vectors due to the heuristics used, however if it suc-ceeds (and this is checked during the final pseudoinverse lookup step), it still enjoys the same type of guarantees as original algo-rithm.
We start with presenting an algorithm, and then show simple it-erative modification which allows even further decrease required sample complexity.
 Input: Documents C , parameter t .
 Output: Topical subspace H . D d and P d -estimated term distribu-tion vectors and mixing coefficients.
 Description: 1. Compute the matrix  X  D of documents signatures  X  D . 2. Compute the observed co-occurrence matrix  X  R (  X  D ) 3. Find the matrix V -our alternative mixture model, using al-5. For each document d , with signature vector  X  d : Because of the choice of V , it has a pseudoinverse with bounded maximal element, following the analysis of [19] this algorithm will provably recover term distributions as the number of documents goes to infinity. We refer reader to [19] for the details of the proof. While this algorithm works well on large collections, the following modification allows to reduce the required amount of data signifi-cantly. Observe that our algorithm in fact uses only few columns of the co-occurrence matrix to build the topical subspace, which, effectively means that we use only small fraction of documents to construct probability distributions, and hence significantly increase our sampling error. To address this we use the following iterative modification of the algorithm. Classification results of the first it-eration, are used as a pre-classified data, to train Algorithm 1. In other words, we use the whole corpus to rebuild and refine our top-ical subspace.
 Input: Collection of documents C .
 Output: Classification c d for every document d .
 Description: 1. Using unsupervised algorithm, compute intermediate classi-2. Run supervised algorithm as if intermediate classification is 3. Iterate the previous step l more times 4. Report current intermediate classification as the final. Parameter l in this algorithm is a small constant, and we found that value l =4 , works well in all cases. Also we note that while this procedure allows to greatly reduce the amount of data needed to produce good classification. However, as the amount of available data grows, the effect of applying second and consecutive iterations of Algorithm 4 declines.
For simplicity, we assume that all documents have the same size s , each document is relevant to only one topic, and each topic is two topic distributions are far apart e.g. W 1  X  W 2 1  X   X  is possible to do fully unsupervised classification of the collection, with the error approaching zero as the number of documents grows. pus is generated from a mixture W of two topics, such that
W 1  X  W 2 1  X   X  0 , and let  X  ,  X  are constants. Then if the size s of each document is at least g ( X  0 , X , X  ) , and the total number of documents | C | exceeds f ( X  0 , X , X , | D | ) , for some appropriate polynomial functions f and g , then algorithm 3 with probability 1  X   X  will correctly classify at least 1  X   X  fraction of all the documents.

P ROOF . First, since  X  R uniformly approaches R as the number of documents grows and since the number of documents can be a function of the size of the dictionary, we can assume without loss of generality that R =  X  R . maintain clarity.
 Figure 1: Each point corresponds to a column of co-occurrence matrix. The most distant columns of the co-occurrence matrix, tend to be the ones that are close to the underlying topics
Now, condition W 1  X  W 2 1  X   X  0 implies that there are two words w 1 and w 2 , such that W 1 w 1  X  W 2 w 1  X   X  W distance between columns of the co-occurrence matrix chosen by the algorithm 2, and that in turn lower bounds  X ( H ) call that all documents have underlying vectors either W 1 and thus their representation in the space H would be of the form (  X  1 , X  2 ) , where one of the components is at most 0 and another at least 1. The rest is a simple corollary of the lemma 3.1.
Recall that each column w of co-occurrence matrix  X  R d is a term histogram for documents containing word w . In other words, our algorithm tries to locate terms, such that corresponding terms his-togram are as different as possible. The hope is that each such term would be only relevant to one topic, and corresponding term his-togram would actually correspond to topic distributions.
 Geometrically this corresponds to the following interpretation. The set of topical distributions forms a k -dimensional polyhedron X ,in n dimensional feature space. Obviously all normalized columns of co-occurrence matrix would be lying in X . Further-more, if we assume that each document is relevant to only one topic, and each topic has at least one word relevant to only that particular topic, then we can expect corresponding column of co-occurrence matrix to be close to the actual topics, see Figure 1. Intuitively our heuristic then should produce points close to the ver-tices of X which correspond to the actual topic distributions. Remark 2 . Replacing our greedy search by an algorithm which builds a convex hull for co-occurrence matrix might help to do classification better in the instances with large number of topics. However computing convex hull in n -dimensional space is a com-putationally expensive operation, and also it is an open question if an analogue of 3.3 could be proved for k&gt; 2 .
Note that if a particular word w has appeared in a document, then it seems natural that it is somewhat more likely to appear again than other word also relevant to the same topic. In the mixture model all consequent appearances have the same probability as the first one.Especially this is noticeable in small documents like paper ab-stracts. To accommodate this, in our experiments we ignore second and all consequent appearances of the same word within one docu-
For our experiments, we have used two datasets. The first one is a subset of abstracts from arXiv , which approximately 250,000 scientific abstracts on 12 different categories, which we used as our primary dataset. Our second collection is the 20 Newsgroup, which contains 20,000 messages posted to one of the 20 usenet groups.
With arXiv, Algorithm 4 was tested as both unsupervised learn-ing algorithm and the dimensionality reduction step. In both cases it has shown outstanding results. Particularly on 4 topic classifi-cation problem, algorithm outperforms LSI , and performed better than SVM with 400 (100x4) training documents.

In the second dataset (20 Newsgroup), we do binary classifica-tion, and compare it with results of [29]. Due the scarcity of data (recall that we need to build huge co-occurrence matrix), we were not able to run our algorithm on 5-topic classification problems. However we did experiments with those sets in a setting where for each topic our algorithm was given a  X  X int X -a descriptive keyword, as an additional noise reduction measure.

For our tests, we have used SVM struct implementation of sup-port vector machines due Joachims [16]. Our algorithm was imple-
For the dictionary, we have used words which have occurred at least 5 times and we have used neither stemming (e.g. words  X  X est X  and  X  X ests X  are considered different words), nor stop-words (e.g. words such as  X  X  X ,  X  X he X ) removal.
Due to the large amount of computer resources required to per-form LSI , we perform our comparison tests with Latent Seman-tic Indexing, on randomly selected 1/10th of documents from each category. Later we also run our algorithm on the selected full arXiv categories.

As our first experiment, we classify 3780 documents on hep-ph (high energy physics -phenomenology) against 4370 documents on astro-ph (astrophysics). For each document, we produce a 2-dimensional vector of document relevancies to semantic dimen-sions. Illustrative representation of both algorithms outcome is pre-sented in Figure 2. For LSI 2-nd and 3-rd singular vectors were early separable, as one can see clusters are very distinct for our method, whereas in LSI they are barely distinguishable, and addi-tional clustering is needed.

Second, we do four topic classification between astro-ph, hep-ph, cond-mat (condensed matter, 4690 documents) and math (math-ematics, 2300 documents). Our algorithm fully succeeds in recov-ering the topics, and performs almost as well as trained SVM. For LSI, further classification is needed. In order to minimize an ad-ditional error introduced by extra unsupervised method, we have trained SVM to do final discrimination between topics. Compari-son results are presented in Table 1. We also included classification results for trained on a full dictionary SVM as a reference. Note ingly to their actual frequencies or tf.idf [12] measures, those pro-duced comparable, yet slightly worse results. We don X  X  present these results in the current paper. 100 61.0% 90.6% 87.7% -300 68.5% 90.8% 91.2% -1000 67.9% 90.8% 93.3% -Table 1: SVM vs. LSI+SVM vs I-LP. accuracy of 4-way clas-sification on a subset of arXiv of  X  15,000 documents. First column contains number of labeled samples per topic, supplied to SVM step. The last column is performance of our method, which does not need any training data. Rows 2-4 contain aver-aged over 10 runs accuracy. Table 2: Effect of iterations as amount of available data grows. In both cases classification is done on the same topics (hep-ph, cond-mat, math, astro-ph). that our method performs almost as good as SVM with 300 train-ing documents per topic (e.g. 1200 total).

Now we switch to the full arXiv. In the first experiment, we do unsupervised clustering for 4 largest categories, and compare per-formance a gain from consequent iterations. As expected the effect for full arXiv is nearly not as dramatic as for 16,000 document sub-set. This supports our claim that iterations are helpful to reduce the required amount of data, but they don X  X  give much improvement when plenty of input data is available.

Next, we do classification in the datasets with 5 and 6 topics, for which we add category gr-qc (general relativity -quantum cosmol-ogy) and nucl-th (nuclear theory) to the test set. While accuracy has declined it is still remained very high, especially, considering unsupervised nature of the method and closely related categories (e.g. quantum cosmology vs. astro-physics). Detailed classifica-tion results are presented in Table 3
Our last experiment on arXiv is a test of our iterative algorithm as a dimensionality reduction step. To measure the quality of our pro-jections we compare the performance of supervised method (SVM) on a full feature set with SVM trained on projection subspace. For that we try 4-, 6-and 10-categories test cases. The 4-and 6-cat-egory datasets contains the same categories as above. For the last one we add four more: quant-ph, nlin, physics and hep-th. The largest dataset contains  X  232 , 000 documents. Results are pre-sented in Table 4.
 Remark 1 . Unsupervised classification of 6 and more topics proved to be difficult for our algorithm. The classification had less than 50% accuracy on one or more topics. One such example is presented in Table 3. While we suspect insufficient amount of data to be the major reason, it might also be the point where our heuris-tic algorithm starts to break, and/or the mixture model becomes less accurate description of a textual data. Future exploration of those reasons is a very interesting next step. itself has no knowledge of true cluster order. Figure 3: LSI +SVM vs. I-LP vs. SVM, 4-way classification on a subset of arXiv of  X  15,000 documents. X -axis contains num-ber of labeled examples per category supplied to SVM (note that there is no SVM step for I-LP, so its performance is a con-stant). 20 Newsgroup collection consists of 20 groups, each containing approximately 1000 messages. Below is the list of groups, where we have used the same numbering as in [29] NG1: alt.atheism, NG2:comp.graphics, NG3: comp.os.ms-windows.misc, NG4: comp.sys.ibm.pc.hardware, NG5: comp.sys.mac.hardware, NG6: comp.windows.x, NG7: misc.forsale, NG8:rec.autos, NG9: rec.motorcycles, NG10: rec.sport.baseball, NG11: rec.sport.hockey, NG12: sci.crypt, NG13: sci.electronics, NG14: sci.med, NG15: sci.space, NG16: soc.religion.christian, NG17: talk.politics.guns, NG18: talk.politics.mideast, NG19: talk.politics.misc, NG20: talk.religion.misc While 1,000 per group might seem to be a lot of documents, it is not quite enough to build n  X  n co-occurrence matrix, especially if more than two topics are involved. Therefore, we only compare our algorithm on binary classification with results of [29] for p-QR, p-k-means and k-means methods. We also note that in [29] experiments were run on 100 messages subsets, which render the test results to be not directly comparable with ours.

In our experiments we have removed the headers from all the messages, and all words that have occurred less than 5 times.
For non-related groups (NG1/NG2 and NG1/NG15) our algo-rithm gives 96% and 93% accuracy, respectively (best in [29] is 89% and 73% respectively). For related groups accuracy has dropped. For NG2/NG3 and NG8/NG9 we have 75.5% and 87% (vs. 62.3 and 75.9 in [29]), and for the final two examples our multiple reasons for this drop, we suspect the primary one to be the insufficient amount of data -with high probability noise influence our choice of columns in co-occurrence matrix. To support this claim, we do the following experiment. For each group, a single descriptive word is picked, and these words are given as a part of the input. The algorithm then chooses columns of co-occurrence matrix corresponding to these words as a first approximation to the topic subspace (instead of using greedy search). After that, it runs the iterations in the usual mode. Here is the list of words we have used as hints: NG1: ATHEISM, NG2:GRAPHICS, NG3: WINDOWS NG4: IBM, NG5: MAC, NG6: SUN NG8:CAR, NG9: BIKE, NG10: BASEBALL, NG11: HOCKEY, NG15: SPACE, NG18: ISRAEL NG19: FBI The classification results for this experiment are given in the last column of Table 5. Quite expectedly, all problematic (NG10/NG11 and NG18/NG19) binary cases were resolved, and the rest observed only a slight increase in the accuracy. For hinted-I-LP, in addition, ran 5-way classification tests. The algorithm certainly succeeds on the first test (unrelated topics, 88% accuracy). For the second test, while it outperforms all other algorithms -the results are not as impressive (computer related topics, 54% accuracy), although still significantly better than the best in [29]. This again confirms the intuition that for closely related topics our algorithm needs more statistical data in order to discriminate between them.
We presented a new technique for the large scale unsupervised text classification, which to the best of our knowledge outperforms all unsupervised methods. While this is a very applicable result by itself, it suggests that there is a lot of structure still hidden in the high-dimensional textual data. We believe that our algorithm is an important first step towards exploring and understanding such structure.

Another distinctive feature of our algorithm and analysis of [19] is that they provide guarantees about underlying term distributions and possibly about classification accuracy within the model and thus potentially could be used to measure suitability of the model for a given task. Particularly, the success of our method shows that MCMM is indeed a good approximation for textual data.

Our approach poses many new questions of both technical and theoretical nature. First, we used heuristic to build a mixture model. While it has worked surprisingly well for the classification, it would be very interesting to see if our mixture is indeed related to the un-derlying topics in the general case. Alternatively, can one find in some sense  X  X he best X  mixture model in feasible time? It is our be-lief that for the single-label classification problem, there is a suit-able definition of the  X  X est X  model, where it could be proven to be close to underlying model. However perhaps, we might need additional assumptions about the model.

Another interesting direction is to see if one can apply our method to linearly dependent topics, such as simultaneous clus-tering by authors and content, and/or hierarchical clustering.
From the theoretical point of view, improving actual bounds on the sample complexity presented is [19] and particularly matching them with our experimental results, is a very important open ques-tion. One approach is to assume that distributions have a particular shape -for example power-law like. Finally, as it was mentioned in [19],  X  is similar to the smallest singular value. Can one con-struct analogues of other singular values and build something simi-lar to SVD, but with respect to L 1 norm? Would it describe textual data better than traditional approach? What could one say about relevant word for each topic it has to classify. data partitioning when similarity is measured using L the general case?
Author would like to thank Jon Kleinberg and Thorsten Joachims for useful discussions and recommendations, Paul Ginsparg and Paul Houle for providing collection of arXiv abstracts, and also Aleksandrs Slivkins and Mey Khalili for their help in preparation of this manuscript. [1] Y. Azar, A. Fiat, A. Karlin, F. McSherry, and J. Saia. Spectral [2] L. Baker and A. McCallum. Distributional clustering of [3] N. Bansal, A. Blum, and S. Chawla. Correlation clustering. [4] D. Blei, A. Ng, and M. Jordan. Latent dirichlet allocation. J. [5] F. R. Chung, L. Lu, and V. Vu. Spectra of random graphs [6] A. Dasgupta, J. Hopcroft, and F. McSherry. Spectral analysis [7] S. C. Deerwester, S. T. Dumais, T. K. Landauer, G. W. [8] E. Demaine and N. Immorlica. Correlation clustering with [9] I. S. Dhillon, S. Mallela, and R. Kumar. Enhanced word [10] T. Fenner, A. Flaxman, and A. Frieze. High degree vertices [11] G. H. Golub and C. V. Loan. Matrix Computations(3rd [12] G.Salton and M. McGill. Introduction to modern information [13] T. Hofmann. Probabilistic latent semantic analysis. In Proc. [14] T. Hofmann. Unsupervised learning by probabilistic latent [15] P. Husbands, H. Simons, and C. Ding. On the use of singular [16] T. Joachims. Making large-scale svm learning practical. In [17] T. Joachims. Text categorization with support vector [18] Q. Ke and T. Kanade. Robust subspace computation using [19] J. Kleinberg and M. Sandler. Using mixture models for [20] A. K. McCallum. Multi-label text classification with a [21] G. McLachlan and K. Basford. Mixture Models, inference [22] M. Mihail and C. Papadimitriu. On the eigenvalue power [23] A. Ng. Feature selection, l 1 vs l 2 regularization and [24] A. Ng, M. Jordan, and Y. Weiss. On spectral clustering: [25] C. Papadimitriou, P. Raghavan, H. Tamaki, and S. Vempala. [26] M. Sahami, M. A. Hearst, and E. Saund. Applying the [27] E. Saund. A multiple cause mixture model for unsupervised [28] C. Swamy. Correlation clustering: Maximizing agreements [29] H. Zha, X. He, C. Dong, and H. Simons. Spectral relaxation
