 In real world use of test collection methods, it is essential that the query test set be representative of the work load expected in the actual application. Using a random sample of queries from a media company X  X  query log as a  X  X old standard X  test set we demonstrate that biases in sitemap-derived and top n query sets can lead to significant perturbations in engine rankings and big differences in estimated performance levels. H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval; H.3.4 [ Information Storage and Retrieval ]: Systems and Software X  Performance evaluation (efficiency and effectiveness) Performance, Measurement
Test collections, e.g. those of TREC [6], are well estab-lished as the orthodox tool for evaluating, tuning and com-paring retrieval systems. Test collections typically consist of documents, queries and, for each query, a set of known useful answers.

Previous research has addressed the choice of appropriate effectiveness measures for particular purposes. Buckley and Voorhees [2] have also studied the size of query sets needed to achieve stable rankings of alternative retrieval systems on those measures.

Various authors have addressed the issue of reducing the cost of obtaining judgments, e.g. Clarke and Cormack [3]. Sanderson and Zobel [5] argue that judging effort is better expended on shallower judging of a larger range of topics than on deeper judging of a small set. Hawking and Zobel [4] compare the value of different types of ranking evidence in the search of an enterprise web using a  X  X est page X  evaluation. In this type of evaluation, retrieval systems are rated solely on their ability to retrieve the most useful document at or near the top of the ranking. Other relevant but less useful documents are ignored.

Here, we consider the choice of the queries comprising the test set. We are interested in methods for evaluating retrieval effectiveness in real search applications such as an enterprise search facility, a library search engine or a newswire retrieval system. In this context, it is important that effectiveness results obtained by testing should accurately reflect the experience of people actually using the application. Ideally, we would measure and average across every query instance submitted (the workload ), but this is clearly impractical. We need to sample from the population.

Early Web search engine companies are believed to have focused tuning efforts on the n most frequently submitted queries. 1 This approach makes intuitive sense as the more popular the query, the more benefit gained by improving its results. However, it is likely that the n most popular queries constitute a biased sample of the workload. For example, they are typically shorter than average. More recently, some large scale studies based on Microsoft X  X  web search engine [1] have used random sampling in order to better reflect the end-user  X  X xperience X .

Hawking and Zobel evaluated using highly popular, medium popularity, randomly chosen and sitemap-derived query sets in order to confirm that conclusions about the value of topic metadata in retrieval were not dependent upon the query set. They found that while the pattern of results from sitemap-derived query sets were very similar to those from the other sets, there was some exaggeration of the performance of anchortext-based evidence.

Here, we use a methodology similar to Hawking and Zobel but look at how well popular and sitemap test sets predict the  X  X old standard X  measures obtained from an unbiased sample of the workload of the search facility provided on the external website of a media company. We use best page evaluation as, anecdotally, users and purchasers of enterprise search services expect simple queries to return the obvious answer at rank one. For example, a search for a product name on the manufacturer X  X  website should return that product X  X  home page at rank one. Further, best page evaluation allows sitemap queries to be used and permits a larger sample of queries within the bounds of judging effort.
Anonymous Media Organisation X  X  web site (7 . 6  X  10 5 pages) and six months of query log data (2005 X 6) were used for the experiments.

The test sets are listed in Table 1. A random sample of three hundred queries was taken from Anonymous Media Organisation X  X  query log. The query log contained 2 . 9  X  10 6 casefolded queries recorded between December 2005 and personal communication Table 1: Test sets: n is the number of queries for which best answers were found. terms is the mean number of words in those queries. Workload pro-portion is the portion of the search engine X  X  total workload represented by the test set.
 May 2006. Judgements were made by an author who was familiar with the organisation but not a domain expert. The judge found the  X  X est page X  available to answer the query. This page was, in the judge X  X  opinion, the page most likely to be useful to an issuer of the query. In a few cases, there was more than one  X  X est X  page. The  X  X opular X  test set is derived from the most popular 132 queries for the site. In both the sample and popular test sets, queries for which answers could not be found were not considered.

The  X  X itemap X  test uses the text of each link on the or-ganisation X  X  sitemaps as a query and the target as the best answer. This test has the advantage that the best answers are chosen by the organisation itself (thereby avoiding exper-imenter bias and/or ignorance) with no need for additional judging. The proportion of overall workload represented by the sitemap testset is low despite the large number of queries. In all cases, redirections have been taken into account. Only the top ten results from each result set were examined. In the sitemap case, the site X  X  main sitemap was removed be-fore indexing, but sub-site sitemaps, from which the majority of the sitemap test X  X  queries were derived, were not.
Each test set was run against four different retrieval en-gines, labelled E1 X  X 4 and chosen to span a wide variety of different retrieval methods. Precise details of differences between engines are not important to the central issue of this paper. However, E3 relies entirely on anchortext ev-idence and E4 relies entirely on click data. Engines were compared using mean reciprocal rank of the first correct answer (MRR1) as MRR1 is the obvious choice for best page evaluation.

Figure 1 illustrates that the estimated performance varies substantially depending upon the test set. Compared to the unbiased sample estimate, the popular query set strongly over-estimates the effectiveness of E4 (click data based) and under-estimates the performance of E1 and E2. Similarly, the sitemap test over-estimates the performance of E3 (anchor text based) and under-estimates that of E2 and E4.
The ranking of the engines also varies considerably de-pending upon the test set as shown here. A  X - X  indicates a gap of more than 0.1 in MRR1. sample: E1 E2 E3 --E4 popular: E1 E2 E3 E4 sitemap: E3 -E1 -E2 ---E4
Sitemap tests are very appealing from an experimental point of view but our study has demonstrated appreciable biases. Popular queries, on first inspection, sound like a reasonable way of tuning a search engine to the needs of Figure 1: Deviations of performance estimates de-rived from popular (white) and sitemap (black) sets from the unbiased sample estimate, represented by the horizontal axis. The deviations are obtained by subtracting MRR1 scores. The unbiased sample es-timate for each engine is shown in parentheses. many users, but like the sitemap test show significant bias. The popular tests did not, in this case, change the rankings of the search engines, but may in other cases.

In this work, the population we have studied is actually of queries received by the search engine which have an easily identifiable best answer, rather than the total workload.
Traditional test collections, such as those used in TREC, facilitate the direct and reproducible comparison of search methods. However, unless the queries in a test collection form an unbiased sample of a real search workload, engine rankings and performance estimates are not likely to reflect real world performance. We suspect that there are substantial biases in the selection of the made-up queries used in many TREC and INEX evaluations, but we did not have access to a set of such queries coupled with a matching collection and query logs.

However, we have shown that, despite their attractions, neither sitemap nor top n query sets provide unbiased esti-mates of performance across an actual workload. [1] E. Agichtein, E. Brill, S. Dumais, and R. Ragno. [2] C. Buckley and E. M. Voorhees. Evaluating evaluation [3] G. V. Cormack, C. R. Palmer, and C. L. A. Clarke. [4] D. Hawking and J. Zobel. Does topic metadata help [5] M. Sanderson and J. Zobel. Information retrieval system [6] E. Voorhees and D. Harman, editors. TREC:
