 Discourse planning is a subtask of Natural Lan-guage Generation (NLG), concerned with deter-mining the ordering of messages in a document and the discourse relations that hold among them (Reiter and Dale, 2000). Early approaches to dis-course planning used manually written rules, often based on schemas (McKeown, 1985) or on Rhetor-ical Structure Theory (RST) (Mann and Thomp-son, 1987; Hovy, 1993; Power, 2000). In the past decade, various statistical approaches have emerged (Duboue and McKeown, 2001; Dimitro-manolaki and Androutsopoulos, 2003; Soricut and Marcu, 2006; Konstas and Lapata, 2013). Other relevant statistical approaches to content ordering can also be found in the summarization literature (Barzilay et al., 2001; Lapata, 2003; Bollegala et al., 2005). These approaches overwhelmingly focus on determining the best order of messages using semantic conent, while discourse relations are in most cases either determined by manually-written derivation rules or completely ignored.
Meanwhile, researchers working on discourse relation disambiguation have observed that the se-quence of discourse relations itself, independently of content, helps in disambiguating adjacent re-lations (Wellner et al., 2006; Pitler et al., 2008). Sequential discourse information has been used successfully in discourse parsing (Ghosh et al., 2011; Feng and Hirst, 2014), and discourse struc-ture was shown to be as important for text co-herence as entity-based content structure (Lin et al., 2011; Feng et al., 2014). Surprisingly, so far, discourse sequential information from exist-ing discourse-annotated corpora, such as the Penn Discourse Treebank (PDTB) (Prasad et al., 2008) has not been used in generation.

In this paper, we present an NLG framework that generates texts from existing semantic web ontologies. We use an n-gram model of discourse relations to perform discourse planning for these stories. Through a crowd-sourced human evalua-tion, we show that the ordering of our documents and the choice of discourse relations is signifi-cantly better when using this model. In concept-to-text generation pipelines, discourse planning typically occurs after the content selec-tion stage. The input, therefore, is an unordered set of messages that are not yet realized: instead of being represented as text, the messages have a structured semantic representation.

In this paper, we generate comparison stories , describing and comparing two similar entities, from an RDF ontology. The RDF semantic rep-resentation is commonly used in semantic web re-sources and free ontologies. An RDF message (called a triple ) has three parts: a subject, a pred-icate and an object. For each story, we consider any triple whose subject is one of the participating entities as a potential message to be generated. We do only minimal processing on these messages: where two triples have the same subject and pred-icate but different objects, we merge them into a single message with multiple objects; and where two triples have the same subject and object but different predicates, we merge them into a single message with multiple predicates.

Next, we build the set of potential discourse re-lations between all messages. We use the PDTB class-level relations, of which there are four: ex-pansion , comparison , contingency and temporal . Each is an abstraction of a family of more specific relations, such as cause , concession , etc. We do not differentiate between explicit and implicit re-lations, and treat entrel as a type of expansion .
Potential discourse relations are implied in the semantics of the triples: messages that contain the same predicate and object may have an expansion relation among them (e.g.  X  X ohn has a ball. Mary also has a ball X ). Messages that contain the same predicate but different subjects and objects may have a comparison relation (e.g.  X  X ohn likes ap-ples but Mary likes oranges X ).

Specific predicate pairs will also have specific potential relations among them -for example,  X  X irth place X  and  X  X esidence X  have a temporal re-lation (when applied to the same subject). The same is true for contingency relations (e.g.,  X  X ity X  and  X  X ountry X  for the same subject -if the sub-ject is in a city, it implies which country it is in). We manually annotated the 59 predicate pairs that had potential temporal and contingency relations, as well as 8 pairs with special potential compari-son relations (e.g.,  X  X irth place X  and  X  X esidence X  if the subject is the same but the object is not).
Once the potential relations are identified, we have a directed multigraph where each vertex is a message and each edge is a potential relation. There can be multiple edges between any two ver-tices, since messages may have more than one po-tential relation among them.

Once the graph is ready, we perform content se-lection. Given a desired number of messages to generate, we choose the set of messages that max-imizes the number of edges in the resulting sub-graph (thus ensuring that the selected messages are discourse-coherent). If there are multiple such sets, we choose one at random.

The task we are focused on in this paper is dis-course planning, which in our formulation is the task of finding a Hamiltonian path through the se-lected subgraph, thus simultaneously selecting the order of the messages (nodes) as well as the rela-tions (edges) that connect them. Our approach for choosing the best path is discussed in the next sec-tion. For the remainder of this section, we describe our simple implementations of the next stages of generation: sentence planning and realization.
For each of the four discourse relations we use, we selected a few explicit connectives from the PDTB that are often used to convey them. We specifically chose connectives that apply to the en-tire range of class-level relations (e.g., for compar-ison we chose  X  X hile X  -since it applies to both contrast and concession in the PDTB, but not  X  X n contrast X  which applies only to the former). We also chose only those connectives which have the structure [ARG1 connective ARG2] or [ARG1. connective, ARG2]. During realization, we arbi-trarily choose a connective to realize the relation.
Since the ordering and relations between mes-sages is determined by the discourse plan, sen-tence planning falls naturally out of it: sentence breaks occur where the connective pattern creates them, or where there is no relation between adja-cent messages.

To realize the messages themselves, we follow a single pattern:  X  X he [predicate(s)] of [subject] (is/are) [object(s)] X . Simple rules are used to plu-ralize the predicate when there are multiple objects and to create lists of multiples objects or predi-cates where needed.
 These basic solutions for the various stages of NLG produce texts that are rich enough to be ac-ceptable for human readers, but which have rel-atively little variation in grammatical and lexical quality. This crucial combination allows us to per-form a human study to specifically evaluate the discourse planning component. As explained in the previous section, we formu-late the discourse planning task as finding a path through a multigraph of potential relations be-tween messages. One major component of what makes a good path is the sequence of content: some content is more central and should appear earlier, for example; and some predicates and ob-jects are semantically related and should appear near one another. In this paper we focus on a com-ponent that has so far been neglected in generation -the sequence of discourse relations -while try-ing to minimize the effect that content semantics have on the evaluation (other than the semantics implicit in the relations). In order to quantify the likelihood of a sequence of relations, we build an n-gram model from a discourse-annotated corpus.
An n-gram model measures the transitional probabilities for sequences of the units that the n-grams are composed of. In this case, the units are discourse relations. The probability of a particu-lar sequence of relations of length n + 1 given an existing subsequence of length n is computed as a fraction of the number of times it appears in the corpus and the number of times the subsequence appears in the corpus, i.e.
 Where C ( s ) is the number of times sequence s ap-pears in the corpus. Using this model to generate a discourse plan given a potential relation multi-graph is a stochastic process: at each stage, we choose the next relation edge out of the last cho-sen message vertex (the first vertex is chosen at random) based on the selected sequence of rela-tion edges and the probabilities for the next rela-tion in the model. Once a vertex is added to the path edges leading to it can no longer be selected. One method for evaluating a discourse plan in-dependently of content is to produce pairs of generated short text documents, each containing the same content, but with different ordering and discourse relations (as dictated by the discourse plan). The only obvious way to decide which text is better is to have human judges make that de-cision. It is important to minimize the effects of other qualities of the texts (differences in content, word choice, grammatical style, etc.) as much as possible, so that the judgement is based only on the differences in order and discourse.

We used DBPedia (Auer et al., 2007) -an RDF ontology extracted from Wikipedia -to generate content. Each document generated was a compar-ison story of two entities in a single category (i.e., the messages in the stories were selected, as de-scribed in Section 2, from the set of triples where one of the entities was the subject). In order to experiment with different domains, we used four different categories: Office Holder (i.e., a per-son holding office such as a President or a Judge); River ; Television Show ; and Military Unit . The The birth place of Allen J. Ellender is Montegut,
Louisiana, while the death place of Allen J. El-lender is Maryland. The birth place of Robert E.
Quinn is Phoenix, Rhode Island. Subsequently, the death place of Robert E. Quinn is Rhode Island. The birth place of Allen J. Ellender is Montegut, Louisiana. In comparison, the birth place of Robert
E. Quinn is Phoenix, Rhode Island. The death place of Robert E. Quinn is Rhode Island, but the death place of Allen J. Ellender is Maryland. entity pairs from each category were chosen at ran-dom but were required to have at least 8 predicates and 3 objects in common, so that they were some-what semantically related.

To ensure that human judges can easily tell the differences between the stories on a sentential level, we limited the size of each story to 4 mes-sages. For each pair of stories, everything but the discourse plan (i.e. the content selection, the real-ization of messages and the lexical choice of con-nectives) was identical. Figure 1 shows an exam-ple pair of stories from the Office Holder category. 4.1 Experiments We conducted two crowd sourced experiments on the CrowdFlower platform. Each question con-sisted of two short stories that are completely iden-tical in content, but each generated with a different discourse planner. The human judge was asked to decide which of the stories has a better flow (or whether they are equally good), and then to give each of the stories a score from 1 to 5, paying spe-cific attention to the ordering of the prepositions and the relations between them. The stories were presented in a random order and were not given labels, to avoid bias. We generated 125 pairs of stories from each category -a total of 500 -for each experiment.
 Each question was presented to three judges. In each experiment, there was complete disagree-ment among the three annotators in approximately 15% of the questions, and those were discarded. In approximately 20% there was complete agree-ment, and in the rest of the questions there were two judges who agreed and one who disagreed. We also computed inter-annotator agreement us-ing Cohen X  X  Kappa for 217 pairs of judges who Table 1: Results for the comparison between the PDTB n-gram model and the baseline Table 2: Results for the comparison between the Wikipedia model and the PDTB model both answered at least 10 of the same questions. The average kappa value was 0 . 5 , suggesting rea-sonable agreement.

In the first experiment, we compared stories generated by a planner using an n-gram model ex-tracted from the PDTB with stories generated by a baseline planner, where all edges have identical probabilities. The results are shown in Table 1.
In the second experiment, we used a PDTB shallow discourse parser we developed (Biran and McKeown, 2015) to create a discourse-annotated version of the English Wikipedia. We then com-pared stories generated by a planner using an n-gram model extracted from the parsed Wikipedia corpus with those generated by a planner using the PDTB model. The results are shown in Table 2.
The total results in both tables are statistically significant ( p &lt; 0 . 05 ). 4.2 Discussion The results in Table 1 show that the judges signif-icantly preferred the stories created by the n-gram model-based planner to those created by the base-line planner, both in terms of the three-way deci-sion and in terms of the numeric score. This is true for the total set as well as every specific topic, ex-cept for River . This may be because the predicates in the River category are much more cohesive than in other categories: virtually all predicates related to rivers describe an aspect of the location of the river. That fact may make it easier for a random planner to produce a story that seems coherent. Note, however, that while the judges preferred the baseline story more often in the River questions, the average score is higher for the model, which suggests that when the baseline was better it was only mildly so, while when the model was better is was significantly so.

The results in Table 2 show that the Wikipedia-based model produces better results than the PDTB-based model. We hypothesize that it is for two reasons. First, Wikipedia contains definitional texts and is closer in style and content to the stories we produce than the PDTB, which contains WSJ articles. Temporal relations constitute about 10% of both corpora, but contingency and comparison relations each make up almost 20% of the PDTB, while in Wikipedia they span only 10% and 12% of the corpus, respectively, making the share of ex-pansion relations much larger. Second, since the PDTB is small, higher-order n-grams are sparsely found, which can add noise to the model. The Wikipedia corpus is significantly larger and does not suffer from this problem.

The differences in average scores seen in the ex-periments are relatively small. That is expected, since we have eliminated the content coherence factor, which is known to be significant. In addi-tion, while judges were specifically asked to fo-cus on the order of messages and relations be-tween them, there is inevitably some noise due to accidental lexical or syntactic mismatches, order-ing that is awkward content-wise, and other side-effects of the generation framework we employed. We introduced an approach to discourse planning that relies on a potential discourse multigraph, al-lowing for an n-gram model of relations to drive the discourse plan and efficiently determine both the ordering and the relations between messages.
We conducted two experiments, comparing sto-ries generated with different discourse planners. The first shows that an n-gram model-based plan-ner significantly outperforms the random baseline. The second suggests that using an n-gram model derived from a corpus that is larger and closer in style and content, though less accurately anno-tated, can further improve results.

In future work, we intend to combine this discourse-based view of coherence with a content-based view to create a unified statistical discourse planner. In addition, we will explore additional stochastic models of discourse that look at other, non-sequential collocational information.
