 SHORT PAPER Tianming Hu  X  Sam Yuan Sung Abstract We investigate the following problem: Given a set of candidate cluster-ings for a common set of objects, find a centroid clustering that is most compatible to the input set. First, we propose a series of entropy-based distance functions for comparing various clusterings. Such functions enable us to directly select the lo-cal centroid from the candidate set. Second, we present two combining methods for the global centroid. The selected/combined centroid clustering is likely to be a good choice, i.e., top or middle ranked in terms of closeness to the true clustering. Finally, we evaluate their effectiveness on both artificial and real data sets. Keywords Cluster analysis  X  Centroid clustering  X  Consensus clustering  X  Conditional entropy  X  Metric distance function 1 Introduction Given a set of N data indexed with { 1 , 2 ,..., N } , with a pre-specified number of clusters K &lt; N , the aim of clustering is to assign each datum to one and ex-actly one cluster. The assignment can be characterized by a many-to-one mapping, k = C ( i ) , which assigns datum i to the k th cluster. Among all these distinct clus-terings, one seeks an optimal clustering C  X  to achieve the required goal. Unfortu-nately, one cannot exhaust all possible clusterings to find the optimal one because the number of different clusterings grows very fast with the increase of N .So, practical clustering algorithms only examine a very small fraction of all possible clusterings, with the goal to identify a small subset that is likely to contain the optimal clustering.
 introduced earlier, clustering is a difficult problem combinatorially. Although a number of clustering methods have been proposed, none of them are universal enough to perform equally well in all cases. Differences in assumptions and con-texts in different communities have made the transfer of useful generic concepts and methodologies slow to occur [ 11 ]. Since almost all clustering algorithms can only find a sub-optimal solution in practice, a natural question arises if we can ob-tain a better one by consensus clustering, i.e., combining outcomes from different clustering algorithms to form a consolidated one.
 some reasons such as privacy, the whole data set may be vertically partitioned into different sites. Every site contains all entities but with a fraction of attributes. The clustering has to be performed in the subspace. This is called attribute-distributed clustering and the usefulness of having multiple views of data for better clustering is addressed in [ 12 , 13 ]. With one candidate clustering from every subspace, we need to combine them to form a consolidated one, which is expected to be better than any candidate.
 work and the preliminaries. The entropy-based definitions of clustering distances are developed in Sect. 3 . Section 4 proposes two combining methods, which are evaluated in Sect. 5 . Finally, we summarize this paper in Sect. 6 . 2 Background 2.1 Related work Clustering can be regarded as an unsupervised classification problem. For super-vised classification, there is an extensive body of work on combining multiple unknown, so the major problem is how to weigh and combine the obtained candi-date clusterings. Without any knowledge about the quality of the candidates, we can only assume that they are equally good and then search for a centroid clus-tering that is closest to the whole candidate set. The centroid one is likely to be a good choice, i.e., top or middle ranked no matter what the true clustering criteria are.
 tering algorithms [2 X 4]. That is, multiple clusterings are created and evaluated as intermediate steps in the process of attaining a single, higher quality clustering. In [ 6 ], a boost-clustering algorithm is proposed to exploit the general principles of boosting. In this paper, consensus clustering refers to a more general problem. We just combine any given set of clusterings to produce a consolidated one with-out accessing the original data or any clustering algorithms that generated them. Neither do we impose any constraint on them.
 objects. There is much work in the literature focusing on such distance measures [ 8 , 9 , 15 ]. Comparatively, there is little work for comparing distinct clusterings. Proposed for comparing the true partition and the obtained clustering, Rand Index assigned to the same cluster or not. Apparently, we can use one minus Rand Index as a distance measure. We will elaborate on this idea later in the paper. 2.2 Basics of entropy Since we concentrate on clustering where each cluster can be labeled a discrete value, we only consider discrete random variables. Let X and Y be two dis-respectively. The entropy of X , denoted by H ( X ) ,isdefinedas H ( X ) = of uncertainty or information in X and is maximized when all of p ( x i ) are equal [ 16 ].
 Representing the average of uncertainty remaining in X after knowing Y , the con-ditional entropy H ( X | Y ) can also be defined using the conditional distribution P ( X | Y ) . It can be proved that H ( X , Y ) = H ( Y ) + H ( X | Y ) . Besides, it can be shown that H ( X )  X  H ( X | Y ) , where the equality holds iff X and Y are independent. 2.3 Mapping from clusterings to distributions Obviously, every clustering can be uniquely mapped to a random variable. For a clustering represented by X , each group of data can be labeled by a distinct value x i the corresponding random variable X takes on. Hence, we can denote we can define a new joint clustering denoted by ( X , Y ) , where each of nm clus-ters is uniquely labeled by a pair ( x i , y j ) and P ( X = x i , Y = y j ) interpreted as the fraction of data in the intersection of clusters x i and y j .Wesaytwo clusterings are independent if their respective distributions are independent, i.e., P ( X , Y ) = P ( X ) P ( Y ) . Similarly, conditional clustering ( Y | X ) refers to a set into m groups according to y . Each final group, labeled ( y j | x i ) , consists of data fraction of data of cluster x i that reside in y j . 3 Entropy-based clustering distance 3.1 Definition tance between two clusterings X and Y over the same data set. Suppose X is the true clustering, then H ( X | Y ) measures Y  X  X  within cluster scatter by computing the entropy of the distribution of each cluster of Y in X .If Y  X  X  within cluster scat-ter is small, each of its clusters must be contained at most in a couple of clusters of X , which means H ( X | Y ) is small. Similarly, H ( Y | X ) is related to Y  X  X  between cluster scatter. If clusters in Y are well separated, each of X  X  X  compact clusters must be contained at most in a couple of clusters of Y , which means H ( Y | X ) is small.
 minimized to zero iff X = Y . (2) It is maximized to H ( X ) + H ( Y ) iff X and Y are independent. (3) In any other cases, the result is between 0 and H ( X ) + H ( Y ) . To be a metric distance function, it must also fulfill the triangle inequality, whose proof is omitted due to lack of space.
 a partition X to a set of M partitions ={ X m } M m = 1 as D ( X ,)  X  is agreed by . When we examine partitions within this set, we can find the lo-cal optimal/centroid clustering X  X  l , defined as the one with the smallest distance, i.e., X  X  l  X  argmin X  X  D ( X ,) . If this constraint is dropped, we can search for X 3.2 Normalized distances can be obtained in several ways. The simplest one is d n 0 ( X , Y ) defined in Eq. ( 2 ) and d n 0 ( X , Y )  X  1 can be easily proved.
 generally grows as the number of clusters increases. So it may favor those with a small number of clusters. Although d n 0 ( X , Y ) preserves the triangle inequality, it inherits the weakness of d ( X , Y ) and does not change relative ranking. We use 0 in the subscript of d n 0 to show that it is a trivial normalization. Besides, it will be far less than 1 for many pairs, for ln N is only reachable by the finest partition. entropies, two alternatives are defined in Eqs. ( 3 )and( 4 ). For consistency, we assume that 0 / 0 = 0. It is not hard to show that the former is less than or equal to the latter. Both are equal to 0 iff X = Y and1iff X and Y are independent, regardless of their individual entropy sizes. However, it is unknown if the triangle inequality holds for them.
 D 4 Toward the global centroid Given two clusterings ={ X , Y } , it is easy to show that there are at least three clusterings, X , Y ,and ( X , Y ) , that have the smallest distance D to . Neverthe-less, it becomes more complicated when contains three clusterings or more. Now we present two methods that search for a solution compatible to in a gen-eral sense in that they do not explicitly check the distance values. 4.1 Rand index based graph partitioning Rand Index considers pairwise relation of objects, i.e., if two objects are assigned to the same cluster or not. Thus, for a clustering that partitions N objects, an N  X  N similarity matrix S can be constructed, with entry ( i , j ) equal to 1 when objects i and j are assigned to the same cluster, 0 otherwise. We can generalize this idea to M candidate clusterings ={ H m } M m = 1 . In this case, an N  X  N matrix S can be similarly constructed, with entry ( i , j ) equal to the fraction of clusterings that with H m ( i , j ) = 1 if objects i and j are assigned together in clustering H m ,0 otherwise. Using distance D , we can also develop a weighted version. First, we set the weight w m for H m as the similarity between H m and , which is obtained with additive inversion, w m = 1  X  D ( H m ,) (for those normalized distances). Then the weighted version is obtained with S ( i , j ) = M m = 1 w m H m ( i , j )/ M . sonable similarity-based partitioning method. The unweighted method mentioned earlier is similar to the one proposed in [ 19 ], which is derived using hypergraph representation for clusterings. They employed METIS [ 14 ], a graph partitioning algorithm, for its robust and scalable properties. We also employ METIS here for the subsequent partitioning. 4.2 Joint-cluster graph partitioning In the aforementioned method, only pairwise relation is considered and we still re-cluster at the resolution of the original data. Why not consider higher order relation of multiple objects? H m | ( | H m | denotes the number of clusters in H m ) joint-clusters in the joint clustering ( H 1 ,..., H M ) . If the candidates are similar, many joint-clusters will be empty and the sample size will be far less than M m = 1 | H m | . As stated before, every joint-cluster x can be denoted by ( h 1 x candidate H m . The weight is just the number of objects in that joint-cluster. Note contained by a cluster in every candidate clustering. Since all candidate clusterings agree that all data in every joint-cluster must stay together, we can re-cluster at the resolution of joint-clusters. To use METIS, what remains to be determined is the similarity S ( x , y ) between two joint-clusters x = ( h 1 x ( h total number of data in x and y , | h m x candidate H m .
 5 Experimental evaluation 5.1 Local centroid candidate We first demonstrate the properties and applications of the local centroid candi-date. Perhaps these entropy-based metrics can be most useful when we, without any additional knowledge, need to select a best one from a set of candidate parti-tions. It enables us to find the local centroid that is not too bad, regardless of the data structure and the corresponding true clustering criteria.
 erated with five bivariate normal distributions, 100 each, with common diagonal spectively. Setting the number of clusters to 5, we iteratively run K -means algo-rithm with random initial centers until five distinct partitions are generated. Then they are ranked in terms of ascending order of distance to the true clustering. The aforementioned experiment is repeated 100 times and the results are reported in Ta b l e 1 . One can see that the local centroid X  X  l is top ranked at  X  = 1 most of the time. This confidence declines as  X  increases, for the overlap between individual classes gets more significant. However, even at  X  = 3 with considerable overlap between classes, the heaviest frequency consistently concentrates on the first or second rank for X  X  l . 5.2 Combined clustering In the following experiments, the two combining methods, Weighted Rand Index based Graph Partitioning (WRGP), and Joint-Cluster Graph Partitioning (JCGP), achieve varying success. Because METIS tries to produce the balanced partition (all clusters are of equal size), we only consider clustering of this type. Let us take a look at the worst time complexity for them. Suppose we have M candidate clusterings, each partitioning N data into K clusters. Assuming linear complexity for METIS, then the major computation is spent in constructing the similarity matrix, which is O ( KM N 2 ) for WRGP and O ( KM ( K M ) 2 ) for JCGP. As we will see later, the more similar those candidates get, the fewer the non-empty joint-clusters we will have, which is the actual similarity matrix size in JCGP. distributions with the common diagonal covariance matrix  X  2 I and means ( 0 sets are generated: S1 with  X  = 0 . 1 and S2 with  X  = 0 . 3. For the real data, two labeled data sets at the UCI repository are used: iris (150 data in three classes) and Cleveland heart disease (303 data in two classes collected by Dr. Robert Detrano). K -means/expectation-maximization with random initialization are applied to the artificial/real data to generate candidate clusterings. Because distance types n 1and n 2 always yield similar results, we only report the results of n 0and n 1 thereafter. 5.2.1 Subspace clustering As shown in Table 2 , four candidate clusterings are generated in four different subspaces, which are then fed into WRGP and JCGP to produce the combined clusterings. Two sets of subspaces are tried for the heart data. The experiments are run 10 times and the median distance values are given in Tables 3 and 4 for JCGP, WRGP, the local centroid candidate X  X  l ,andthe local worst candidate X + l (whose distance to the set is the largest). One can see that both JCGP and WRGP perform best on S1 and S2 in terms of d ( X , T ) . The improvement in other cases is less significant. In general, compared to JCGP, WRGP always leads to a better or comparable result. However, as for computational complexity, the similarity matrix size is much smaller for JCGP (equal to the number of non-empty joint-clusters), which is given in Table 5 . Note that the corresponding size for WRGP is just the data size. 5.2.2 Full space clustering We also evaluate the combining methods when candidates are from the full space. Data S1 is not used because it is too easy for K -means to find the true clustering in the full space. Each time 10 distinct candidate clusterings are generated for each data set. The median distance values of 10 runs are given by Tables 6 and 7 .In terms of D ( X ,) , both methods lead to a smaller distance than X  X  l onlyondata S2. They fail on data iris and heart. In terms of d ( X , T ) , which is our ultimate goal, both methods succeed on data S2 and iris. On data heart, only WRGP leads to a slightly smaller distance than X  X  l . On all data sets, however, X  X  l is consis-candidates than the combined results. 6Conclusion In this paper, we addressed two basic problems in finding the centroid clustering from a set of candidate clusterings. First, we proposed a series of entropy-based distance measures for comparing clusterings. It only involves set intersection op-eration and is independent of the data type and structure in question, since the input to our problem is a set of cluster labelings rather than the original data them-selves. We showed that they satisfy some of basic properties a legal distance func-tion requires. Given a set of candidate clusterings, they enable us to find the local centroid candidate defined as the one with the smallest distance to the candidate set. We also discussed combining methods for the global centroid clustering. The centroid clustering is likely to be closer to the true partition than other candidates. This assertion was demonstrated on both artificial and real data sets, with candi-date clusterings from either the full space or the subspace.
 References
