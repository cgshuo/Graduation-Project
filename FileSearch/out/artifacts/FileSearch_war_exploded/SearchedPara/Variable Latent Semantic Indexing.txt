 Latent Semantic Indexing is a classical method to produce optimal low-rank approximations of a term-document ma-trix. However, in the context of a particular query distribu-tion, the approximation thus produced need not be optimal. We propose VLSI, a new query-dependent (or  X  X ariable X ) low-rank approximation that minimizes approximation er-ror for any specified query distribution. With this tool, it is possible to tailor the LSI technique to particular settings, often resulting in vastly improved approximations at much lower dimensionality. We validate this method via a series of experiments on classical corpora, showing that VLSI typ-ically performs similarly to LSI with an order of magnitude fewer dimensions.
 G.1.3 [ Numerical Analysis ]: Numerical Linear Algebra X  Singular value decomposition ; G.1.3 [ Numerical Analy-sis ]: Numerical Linear Algebra X  Sparse, structured, and very large systems (direct and iterative methods) ; H.3.3 [ Infor-mation Storage and Retrieval ]: Information Search and Retrieval X  Miscellaneous Algorithms, Experimentation, Measurement, Theory Linear algebra, SVD, Matrix approximation, LSI, VLSI Work done at Verity, Inc.

Dimensionality reduction is a classic technique in data analysis and mining. Here one has a number of entities , each of which is a vector in a space of features . For instance in text retrieval the entities are documents and the features (axes of the vector space) are usually terms occurring in the documents. We can then view the set of entities as a matrix A in the space of features, where each feature is a column of A . In text analysis for instance, the number of axes in this vector space can thus be in the tens of thousands, cor-responding to tens of thousands of terms in the lexicon. An entry in the matrix connotes some measure of the strength of a term in a document, usually derived from occurrence statistics (e.g., the frequency of the term in the document). Other examples of entities studied in this form include im-ages (where the features include color, hue etc.), audio, face recognition, OCR, human fingerprints, etc.

Dimensionality reduction recognizes that despite the large number of axes in the space of features, most data sets arising in practical applications result in the matrix A hav-ing a good low-dimensional approximation A 0 : a matrix A with the same number of rows/columns as A , but with rank considerably smaller than the number of axes in the vector space. Intuitively, A 0 captures the most salient features of A but enjoys a representation in a subspace of very low dimen-sion. In text analysis, for instance, it is widely reported that good approximations of rank 200-300 exist for typical doc-ument collections. Computationally such approximations are typically found using the linear-algebraic technique of singular value decompositions (SVD X  X ) , a method rooted in statistical analysis [13]. SVD X  X  have become a  X  X orkhorse in machine learning, data mining, signal processing, computer vision, ... X  [10]. Eckart and Young [8] proved that in a spe-cific technical sense (made precise below), the SVD yields the best possible approximation to any matrix A , given any target rank for the approximation A 0 . As a result of the SVD, each document can be viewed as a vector in a low-dimensional space of a few hundred dimensions; the axes in the new space do not in general correspond to terms in the lexicon.
The classic application of SVD X  X  to text analysis stems from work of Dumais et al. [3, 7]. The authors adapted SVD to the term-document matrix, characterizing their method as latent semantic indexing (LSI) . The principal application of LSI was to respond to text queries: following standard practice in text retrieval, each query (expressed as a set of terms) is viewed as a unit vector in the space in which each document is represented (whether the original set of terms as in A , or in the low-dimensional approximate space). Given a query, the system would identify the documents with the highest cosine similarity to the query (in the primary or approximate space) and return these as the best matches to the query.

Experimentally, these and subsequent papers [4, 5, 6] showed that latent semantic indexing was effective not only in that it found a good approximation A 0 of rank about 300, but further that for retrieval on queries the approximation A ten yielded better (rather than almost as good) results than A . Qualitatively, this was explained by Dumais et al. and others by the following intuition: the approximation A 0 , in being forced to  X  X queeze X  the primary vector space (spanned by A ), collapses synonymous terms (axes) such as car and automobile . Additionally, it was argued that LSI separated multiple meanings of a single term (such as charge ), into different axes of A 0 based on co-occurrences of charge with disparate groups of other terms (say, electron and proton , as opposed to brigade and cannon ).

Somewhat surprisingly, the entire premise of LSI  X  the computation of the approximation A 0 as well as its empiri-cal success in retrieval  X  are oblivious to the characteristics of queries. Given that the motivation of Dumais et al. was to respond to queries, it is surprising that the approxima-tion pays no attention to the types of queries. For a simple example, consider querying a collection of news stories. The approximation constructed by LSI would faithfully represent all the topics in the news. Suppose now that the distribu-tion of queries is, however, focused heavily on the subject of Finance. Could it be that there are better low-rank approxi-mations to the matrix A that are especially tuned to queries focused on Finance, ignoring terms (axes) that epitomize other subjects?
One approach might be to remove terms commonplace in non-Finance categories from the matrix A , then perform LSI on the resulting matrix with fewer axes to begin with. This raises the question: is there a principled way to com-pute such a query-dependent LSI, and establish its opti-mality via an analog of the Eckart-Young theorem? Could such a query-dependent LSI significantly outperform query-oblivious LSI in retrieval performance? We answer these questions in the affirmative: we devise a novel form of LSI that takes the query distribution into account, prove its op-timality, and establish experimentally that it dramatically outperforms LSI on retrieval, for any target rank for the approximation.

Our results are more general than the particular applica-tion to text. Indeed, one could apply the same technique to querying images, fingerprints or other entities represented in a vector space. From a pragmatic standpoint, the query distribution could be  X  X earned X  over time, so that one could periodically recompute an approximation tuned to the cur-rent query distribution. More generally, dimensionality re-duction for data analysis is never performed in a vacuum; rather, it is performed with a context in mind. To the ex-tent that this context can be formulated as a certain type of co-occurrence matrix (made precise below), our technique is applicable.
In addition to the research mentioned above, SVD X  X  have been applied to a variety of settings in data analysis in-cluding face recognition [18], collaborative filtering [12], de-noising [17] and object analysis [14]. All of these applica-tions are  X  in the sense outlined above  X  query-oblivious. Weighted generalizations of SVD have been considered be-fore [23, 25]; however these minimize a weighted Frobenius norm instead of the usual norm and are not applicable to our case. Probabilistic latent semantic indexing [11] and its cousins from statistics [24] use a generative probabilistic model for the entries of the matrix A , rather than for the query distribution.
Section 2 gives the mathematical development of our new approximation method, and proves its optimality. By char-acterizing the queries likely to arise through a probability distribution (in fact, we use a general model based on where there is a co-occurrence matrix on pairs of query terms), we derive a form of query-dependent, or variable LSI, which we denote VLSI. A nice feature of our approximation is that it reduces to the standard LSI approximation for the special case when the co-occurrence matrix is the scaled identity matrix.
 Section 3 details experiments on a collection of documents. We study various query distributions, including ones that we tailor to be topic-focused (in the sense of the Finance example from Section 1.1 above). We study the retrieval ef-fectiveness as a function of the number of dimensions in the low-dimensional approximation A 0 . In all cases, we find that VLSI dramatically outperforms LSI on retrieval effectiveness for any given number of dimensions in the low-dimensional approximation. An alternative way of viewing these results: for any quantitative level of retrieval effectiveness, the num-ber of dimensions in the low-rank approximation is dramat-ically lower for VLSI than for LSI. As an example, whereas LSI on text corpora appears to require hundreds of dimen-sions in the approximation, a few tens of dimensions often suffice for VLSI.
Let A  X &lt; m  X  n be the term X  X ocument matrix over m terms (the rows) and n documents (the columns); in this section we do not address the issue of how this matrix is constructed. The singular value decomposition (SVD) of a matrix is the most commonly used orthogonal decomposi-tion of the matrix, expressing it as a product of two orthogo-nal matrices and a diagonal matrix. For a matrix A  X &lt; m  X  n the singular value decomposition of A is written as where U = [ u 1 , . . . , u n ] and V = [ v 1 , . . . , v thogonal matrices, and  X  = (  X  1 , . . . ,  X  n ) is a diagonal matrix of nonnegative entries. The columns of U and V are referred to as the left and right singular vectors of A and the diago-nal entries in  X  as the singular values of A . It is well known that every real matrix has an SVD decomposition and if in addition the matrix is symmetric and positive semidefinite, then it has a decomposition (the eigenvalue decomposition) of the form Y  X  Y T in which all entries of  X  are non-negative. Notation. For a matrix A , we use rk( A ) to denote its rank and Tr ( A ) to denote its trace , i.e., the sum of its diagonal entries. We use k A k 2 F to denote the Frobenius norm , where k A k 2 F = norm is k A k 2 F = Tr A T A .
 Definition 1 (SVD rank-k approximation). If A = U  X  V T is the singular value decomposition of A , then the SVD rank-k approximation of A is defined as We write A k = U k  X  k V T k , where U k = [ u 1 , . . . , u (  X  1 , . . . ,  X  k ) and V k = [ v 1 , . . . , v k ]. Note also that A A well-known property of the SVD is that it optimizes the Frobenius norm (cf. [9]).

Theorem 2. For any matrix A  X &lt; m  X  n , We now show that the SVD rank-k approximation of A can also be interpreted as the rank-k matrix that best ap-proximates the average distortion that A imparts to a unit random vector. Suppose that q = ( q 1 , . . . , q n ) is a ran-dom (query) vector such that E [ q i ] = 0 for all coordinates i = 1 , . . . , n . Consider the average distortion that occurs on multiplication of q by A . The rank-k matrix that best approximates the average distortion is given by We now motivate our approach by showing a relationship between the SVD of A and optimizing this average distor-tion.

Definition 3 (Co-occurrence matrix). Let Q be any distribution on &lt; m . The co-occurrence matrix C Q  X &lt; is defined to be C Q = E q  X  Q [ qq T ] .

Sometimes it is useful to think of Q as the probability distribution from which queries are drawn: the i -th coordi-nate being 1 corresponds to the i -th term appearing in the query. Note that if Q is the product distribution obtained by taking, say, the Gaussian N (0 ,  X  2 ) distribution in each coordinate, then C Q =  X  2 I . In general, there is a useful con-nection between optimizing the average distortion and the SVD when the co-occurrence matrix is the scaled identity matrix.

Lemma 4. If for a distribution Q over &lt; m , the co-occurrence matrix is C Q =  X  2 I for some  X  , then min
Proof. To prove this correspondence we just need to simplify the given expression. Using the fact that for two vectors u and v , u T v = Tr vu T , we have Thus optimizing the expected distortion can be done by tak-ing X to be A k , the SVD rank-k approximation of the matrix A .
We now extend the above result to compute the best rank-k approximation to a given matrix when the distribution Q is arbitrary. A random query generated from this distribu-tion Q is again denoted by q .

First note that C Q is positive semidefinite, as for any vec-tor v , This also means that any such C Q has an eigenvalue de-composition C Q = Y  X  Y T where  X  = (  X  1 , . . . ,  X  n ) with  X  1  X  X  X  X  X  X   X  n  X  0. If rk( C Q ) = r , we write this eigen-value decomposition as C Q = Y r  X  r Y T r .

Motivated by Lemma 4, the natural generalization of SVD to arbitrary query distributions Q is to find a rank-k approx-imation A Q,k to A such that
Definition 5 (Square root and pseudoinverse). For a distribution Q , let C Q = Y r  X  r Y T r be the co-occurrence matrix of rank r . The square-root of C Q is defined to be C We show the following.

Theorem 6. Suppose V k  X &lt; n  X  k contains the top k right singular vectors of C 1 / 2 Q A in its columns. Then the matrix A Q,k minimizing (2) is the matrix A Q,k = AV k V T k . Proof. Suppose for now C Q is a full-rank matrix. Then, C Define the random vector w = C  X  1 / 2 Q q . Then, Using this,
E Since the co-occurrence matrix of w is I , we can apply Lemma 4. Thus, the above expression is minimized when C
Q X is the rank-k approximation of the matrix C V k are the top-k right singular vectors of C 1 / 2 Q A , then the rank-k approximation of C 1 / 2 Q A is given by ( C 1 / 2 Therefore, we need as C Q is a full rank matrix.
 jection onto the space spanned by the query vectors. Intu-itively, the only change that we need to handle this case is to work in the r -dimensional space spanned by the query distribution. Below we work out the details for a rigorous is in &lt; r where r is the rank of the query distribution. Also, It follows that Substituting the value of q T Y r Y T r = w T Y T r C 1 / 2 expression, we have So, arguing as before, we need to compute the rank-k ap-proximation to the matrix Y T r C 1 / 2 Q A . Now, for any vector v , where the last equality follows from the column orthogonal-ity of Y r . Thus the right singular vectors of Y T r C 1 / 2 the same as those of C 1 / 2 Q A . So if V k are the top k right singular vectors of C 1 / 2 Q A , then the rank-k approximation 2, (3) is minimized when and it suffices to choose X = AV k V T k .

And so, in both the cases, A Q,k = AV k V T k is the minimiz-ing choice for (2).
 Note that the above approximation collapses to the usual SVD when Q is the uniform distribution on unit vectors. In fact, it does so even for isotropic single term query dis-tributions, i.e., distributions where the probability of any term appearing is p , say for all terms, and no two terms appear together in the query, i.e., E [ q i q j ] = 0. Then the co-occurrence matrix of such distributions is C Q = pI . Thus Figure 1: Histogram of document size for Reuters-21578 collection. for this case, our optimization is the same as that of Lemma (4), and therefore, just computing the SVD of A would serve our goals.

Finally, the computational requirements of VLSI are simi-lar to that of standard LSI. One benefit of the rank-k approx-imation is that instead of storing the matrix A that poten-tially requires O ( mn ) space, one could store the rank-k de-composition ( U k ,  X  k , V k ) that needs only O ( mk + nk ) space. In our case, it is sufficient to store the matrices ( AV k which needs O ( mk + nk ) space as well.
We selected the Reuters-21578 document set Distribution 1.0 [16] for our experiments. This corpus has been widely used in machine learning because it contains detailed topic labels for each document. We chose the corpus because we may employ these topic labels in order to be able to gen-erate controlled query distributions. The corpus contains 21,578 documents that appeared in the Reuters news feed in 1987. Documents are labeled with membership in five sets of categories, one of which is TOPICS, which represent a broad range of 135 economic subject categories such as  X  X old X  or (the commodity)  X  X oconuts. X  Figure 1 shows the distribution (in words) of the sizes of the documents in the collection. There are 112,356 distinct words in the corpus, with an average document length of 134 words.

The lexicon is built as follows. First, the text is ex-tracted from the XML files in which the Reuters-21578 cor-pus is delivered. Next, tokens are extracted by splitting at whitespace boundaries. As is commonplace in text process-ing, tokens are then Porter-stemmed and case-folded, punc-tuation is removed, and a standard 416-word stopword list is employed to remove stopwords. Finally, certain html forma-tions and tokens that appear only once in the entire corpus are removed. This results in a final cleaned dictionary of 33,749 terms.

Based on this dictionary, the corpus is scanned to produce a term X  X ocument matrix of counts, A c , for which A c ij is the number of occurrences of term i in document j . From this matrix, we derive two other matrices on which we perform our experiments. First, the Boolean matrix A b is produced by setting A b ij to 1 if A c ij is non-zero, and to zero otherwise.
In addition, we study a weighted version of the term X  document matrix. In classic text analysis, various tech-niques are known for the derivation of the weight of a term in a document based on term and corpus statistics. Of the many such mappings (from term/corpus statistics to weights) known, one that has proven particularly success-ful in text retrieval is the so-called Okapi weighting, defined for a particular query Q and a particular document D as follows:
X Here, N is the total number of documents, df is the number of documents containing term t , tf is the number of occur-rences of t in document D , dl is the length of D , adl is the average length of documents in the corpus, and qtf is the number of occurrences of t in the query.

Accordingly,the Okapi weighting matrix A O is produced by applying the standard Okapi term weighting algorithm to the entries of A c ; we note in passing that we use the Okapi formula parameters k 1 = 1 . 2, b = 0 . 75, and k 3 = 7 common in prior text analysis [21, 19].

We then used an external memory version of SVDPACKC 1.0 package [2] to perform a 1000-dimensional SVD on the Boolean and Okapi matrices A b and A O as a baseline. We also apply VLSI to each of these matrices with a family of query distributions defined below. We show results com-paring the two low-rank approximations using two different metrics, which we now describe.
Our evaluation is always in the context of a particular query distribution Q , where a query in the distribution is a T -element vector of real numbers representing the weighting of each of the T terms. For most of our experiments, we consider single-word queries.

We compare results using two metrics. For any particular query vector q , the score assigned by some matrix A (taken generically to represent A b or A O ) to each document is sim-ply q T A . We will write LSI( A, k ) to represent the rank-k approximation of A produced by LSI, and VLSI( A, Q, k ) to represent the rank-k approximation of A produced by VLSI with respect to query distribution Q . The L 2 error of an approximation  X  A to A for a query q is simply || q T ( A  X  and the error of  X  A with respect to distribution Q is
Second, we also consider an evaluation metric based on in-formation retrieval that compares the rankings induced by A versus  X  A . Following the standard vector space ranking algorithm, for query vector q , consider ranking the columns of M by their score in q T M (breaking ties by ranking the smaller index first). Let S be the top k documents as ranked by  X  A , the approximation to A . The competitive precision at d [22] is defined as 1 /d times the number of those documents that appear in the top d documents as ranked by A . Thus, a competitive precision approaching 1 means that most of the highly-ranked documents in our approximation would also have been ranked highly by the original matrix. We consider competitive precision at 10 unless specified other-wise. For brevity, we name one minus competitive precision as competitive error (CE) .
There is evidence to suggest that query distributions fol-low a power law: the probability that a particular query occurs x times is proportional to x  X   X  for some  X  . We build on this evidence to generate some of our query distributions, in a manner described below. We found three explicit refer-ences to particular power law exponents over query distribu-tions in the literature. Baeza-Yates [1] reports an exponent of 1.7 in the context of a Chilean search engine; Lempel and Moran [15] give 2.4 for a log of 7M queries submitted to the Altavista search engine; and Saraiva et al. [20] report 2.7 for a Brazilian search engine. Given this variation in exponents, we adopt a power law exponent of 2.4 as being a middle ground.

We consider the following query distributions. Consider the lexicon of all terms in the analysis (corresponding to all rows in the matrix A ). Now, rank this lexicon by total num-ber of occurrences of each term across the entire corpus. Let t be the i -th term in this ranking, and let p i be the occur-rence probability of term t i . We consider three distributions over single-term queries. The first distribution simply mir-rors the distribution of terms in the corpus, while the other three follow a power law. We then consider two cases of planted power laws, all with exponent 2.4. First, the power law is placed over the terms in their order of frequency in the corpus; second, the power law is placed over terms in random order. More formally, the three query distributions are as follows: D1 : The probability of t i is p i .
 D2 : The probability of t i is ci 0 . 714 .
 D3 : The probability of t i is c (  X  ( i )) 0 . 714 where  X  is a ran-
In addition to these three distributions, we also consider distributions D1 and D2 in which the terms t i are ranked, not according to their frequency in the corpus, but according to their frequency in the subset of documents that discuss a particular topic family. We consider two such topic fam-ilies: money , and commodities , which cover 2615 and 1849 documents respectively. Both topics contain about 12,000 unique terms. Terms that do not appear in the documents covering the topic have zero probability in the query dis-tribution, and all other terms have probability determined by scheme D1 X  X 3. This yields a further six query distrib-utions, which we will refer to as  X  X oney.(D1,D2,D3) X  and  X  X ommodity.(D1,D2,D3), X  in addition to the original 3.
In all our result graphs, we use two measures for the qual-ity of the approximations: (1) L 2 error normalized so that the error with a single dimension in the approximation is 1, and (2) competitive error, i.e., one minus competitive preci-sion. Note that both measures are query-dependent, unlike the Frobenius norm in the theory of SVD.
In both D2 and D3, the power law with exponent 2.4 is equivalent to a Zipf exponent of 0.714 on the rank. c is a normalizing constant. Comparing results on query distributions D1 X  X 3.
 Figure 2 shows the results for query distributions D1 X  X 3 for the matrix A O using approximations of rank 1 through 1000. For query distribution D1, which mirrors the term distribution in the original corpus, the trends of LSI and VLSI are quite similar but at different magnitudes of error. VLSI shows a 10% improvement at 10 dimensions, a 27% improvement at 50 dimensions, a 50% improvement at 125 dimensions, and an 80% improvement at 1000 dimensions. Stated alternatively, VLSI with just 40 dimensions is about equivalent in performance to LSI with 250 dimensions.
When a power law is introduced to the query log in query distribution D2, matching the significant skew found in real-world queries, the results are more significant. Error rates drop more rapidly for both LSI and VLSI. At 50 dimensions, the error of LSI is about 60% of the rank-1 error, while the error of VLSI plunges to about 7%. At 125 dimensions, LSI remains at 56% the initial error while VLSI has dropped to under 3%. Viewed alternatively, the error rate of LSI with 250 dimensions is matched by the error of VLSI using only 10 dimensions.

As expected, when the power law is planted over a random permutation of the terms in the corpus, the results are more dramatic. At 50 dimensions, the error of LSI has dropped by barely 5% from a rank-1 approximation. At 22 dimensions, the error of VLSI is only 7% as great (i.e., down by 93%), and by 50 dimensions, it is between 1 and 2%. Comparing results at 1000 dimensions, the error of VLSI is four orders of magnitude smaller. This is an extreme instance, and not representative of actual query logs.
 Boolean versus Okapi weighting. We may ask whether this significant improvement in approximation for a specific query distribution is an artifact of the Okapi weighting intro-duced into the matrix. Fixing upon distribution D2, which contains a planted power law over the terms with the same rank order as shown in the corpus distribution of term fre-quencies, we compare the Okapi-weighted matrix A O to the Boolean matrix A b . Figure 3 shows the results. Dropoffs are much faster for the Boolean matrix for both LSI and VLSI. The error rate for VLSI begins at about half that of LSI, and by 100-150 dimensions, it has dropped to about 10% of the error rate of LSI for a similar number of dimensions. Topic-specific query distributions. Turning to topic-specific keyword distributions, we would expect that such distributions focus on specific types of keywords and are thus an appropriate arena for VLSI. Figure 4 shows the re-sults for the money and commodity topics. The combination of topic focus and planted power law results in low-rank highly-accurate approximations in the VLSI case: the ap-proximation shows only 10% of the initial error at as few as 15 dimensions, and only 1% by 100 dimensions. LSI X  X  performance at 250 dimensions is approximately similar to that of VLSI at 25 dimensions. In money.D1, in which the query distribution follows the corpus distribution within the money topic, the improvements are slower. VLSI with 25 di-mensions again is comparable to LSI with 250 dimensions, but in this case LSI with 1000 dimensions (and VLSI with 100 dimensions) still show 30% of the original error. Figure 2: LSI and VLSI for query distributions D1 X  D3. The first plot shows results for 1000 dimensions while the second plot show a more detailed view of the first 200 dimensions.
 Competitive precision. So far, we have considered the L 2 error of the approximation. However, we may also consider competitive precision, which is our rank-oriented measure: how do various matrix approximations modify the rank or-der in which documents are returned? Figure 5 shows the results for query distributions D1 and D2, reporting com-petitive error, i.e., one minus competitive precision. The competitive error measure does not tend quickly to zero as tiny differences in the approximation may have significant impact on the rank order. The queries we study tend to match a relatively large number of documents, and the or-dering of those documents in the original and approximate matrices is largely random until the number of dimensions becomes extremely high. The two cases show different be-haviors: D2 drops off extremely quickly due to the skewed nature of the query distribution, but for each competitive error parameter, flattens to a particular value dependent on the size of the result set for different queries weighted by the query probability. In D1, VLSI at 100 dimensions at-tains a competitive error comparable to LSI at 1000 dimen-sions. The same condition holds in D2, but in this case it is Figure 3: LSI and VLSI for query distribution D2 over both Okapi-weighted matrix and Boolean ma-trix. The first plot shows results for 1000 dimensions while the second plot show a more detailed view of the first 200 dimensions. more instructive to note that both schemes have flattened by around 100 dimensions, and the difference in competitive error is about a factor of 2 in favor of VLSI.
 Uniform measurements. The results so far make heavy use of the significant skew encountered in query logs. How-ever, VLSI also shows an improvement when measured in the following manner. Imagine drawing 100 queries from the query distribution without replacement, thus significantly reducing the skew. The results for distribution D2 are shown in Figure 6. The quality of approximation in either metric for LSI at 1000 dimensions is about equivalent to that of VLSI at 100 or fewer dimensions.
 Multi-word queries. All experiments so far have been performed on single-term query distributions. In this case, the formulation in Section 2 has a clean form and is straight-forward to implement. However the formulation is more gen-eral, and allows the query distribution to be over arbitrary vectors. Therefore, we consider a small experiment over Figure 4: LSI and VLSI over 1000 dimensions.
 Top figure shows query distributions money.D1 and money.D2; bottom figure shows commodity.D1 and commodity.D2. two-word queries. The distribution is computed as follows. First, the documents for the commodity topic are scanned, and the counts of all bigrams are kept. The top 25 bigrams are dropped as they are very frequent without being mean-ingful queries. The remaining bigrams are ranked according to frequency, and a power law with exponent 2.4 is planted on this ranking, as we did in the single-term case for query distribution D2. The results of this experiment are shown in Figure 7 for both L 2 distance and competitive error at 10. Due to computational constraints, we completed the ex-periment for 100 dimensions only. About ten dimensions of VLSI produced the same L 2 error and competitive error as 100 dimensions of LSI.
Beginning with the observation that latent semantic in-dexing (and its precursors) are data-dependent but query-oblivious, we developed a new form of low-rank approxima-tion that is query-dependent. Experiments with our new ap-proximation are extremely encouraging, suggesting an order of magnitude improvement in the quality of approximation Figure 5: LSI and VLSI over 1000 dimensions with query distribution D1 (above) and D2 (below) mea-sured by competitive error at 10 through 1000 doc-uments. Plots with points correspond to LSI; those without points correspond to VLSI. for the purposes of serving queries . A number of further directions arise: Figure 6: LSI and VLSI over 1000 dimensions with query distribution D2 measured via 100 queries drawn from the distribution without replacement.
 Figure 7: LSI and VLSI over 100 dimensions with two-word query distribution. [1] R. Baeza-Yates. Web usage mining in search engines. [2] M. Berry, T. Do, G. O X  X rien, V. Krishna, and [3] S. Deerwester, S. T. Dumais, T. K. Landauer, G. W. [4] S. T. Dumais. LSI meets TREC: A status report. In [5] S. T. Dumais. Latent semantic indexing (LSI) and [6] S. T. Dumais. Latent semantic indexing (LSI): [7] S. T. Dumais, G. Furnas, T. Landauer, and [8] C. Eckart and G. Young. The approximation of a [9] G. H. Golub and C. F. V. Loan. Matrix Computation . [10] T. Hoffmann. Matrix decomposition techniques in [11] T. Hofmann. Probabilistic latent semantic indexing. [12] T. Hofmann. Latent semantic models for collaborative [13] I. T. Jolliffe. Principal Component Analysis . Springer, [14] D. D. Lee and H. S. Seung. Learning the parts of [15] R. Lempel and S. Moran. Predictive caching and [16] D. D. Lewis. http://www.daviddlewis.com/ [17] S. Mika, B. Sch  X olkopf, A. Smola, K.-R. M  X uller, [18] B. Moghaddam and A. Pentland. Face recognition [19] S. E. Robertson and S. Walker. Okapi/Keenbow at [20] P. C. Saraiva, E. S. de Moura, N. Ziviani, W. Meira, [21] A. Singhal. Modern information retrieval: A brief [22] P. K. C. Singitham, M. S. Mahabhashyam, and [23] N. Srebro and T. Jaakkola. Weighted low-rank [24] M. Tipping and C. Bishop. Probabilistic principal [25] J. Ye. Generalized low-rank approximations of
