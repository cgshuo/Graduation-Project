 Sanjay Chawla 1,2 , Kiran Garimella 3( In this paper we propose a novel formulation for discovering the backbone of traffic networks. We are given the topology of a network (its structure) G = ( V, E ) and a traffic log (functional activity) L = { ( s amount of traffic w i that incurs between source s i and destination t also given a budget B that accounts for a total edge cost. The goal is to discover a sparse subnetwork R of G , of cost at most B ,which summarizes as well as possible the recorded traffic L .
 network design . An example application of our algorithm is shown in Fig. 1 . Here, we consider a traffic log (Fig. 1 , left), which consists of the most popular routes used on the London tube. The backbone produced by our algorithm takes into account this demand (based on the traffic log) and summarizes the underlying network, thus presenting us with insights about usage pattern of the London tube (Fig. 1 , right). This representation of the  X  X ackbone X  of the network could be very useful to identify the important edges to upgrade or to keep better maintained in order to minimize the total traffic disruptions.
 We only consider source-destination pairs in the traffic log, and not full tra-jectories, as source-destination information captures true mobility demand in a network. For example, data about the daily commute from home (source) to office (destination) is more resilient than trajectory information, which is often determined by local and transient constraints, like traffic conditions on the road, time of day, etc. Furthermore, in communication networks, only the source-ip and destination-ip information is encoded in TCP-IP packets. Similarly, in a city metro, check-in and check-out information is captured while the intervening movement is not logged.
 Example. To understand the key aspects of BackboneDiscovery consider the example shown in Fig. 2 . In this example, there are four groups of nodes: (i) group A consists of n nodes, a 1 ,...,a n , (ii) group B consists of n nodes, b 1 ,...,b n , (iii) group C consists of 2 nodes, c D consists of m nodes, d 1 ,...,d m . Assume that m is smaller then n , and thus much smaller than n 2 . All edges shown in the figure have cost 1, except the edges between c 1 and c 2 , which has cost 2. Further assume that there is one unit of traffic between each a i and each b j ,for i, j =1 ,...,n , resulting in n destination pairs (the majority of the traffic), and one unit of traffic between d and d i +1 ,for i =1 ,...,m additional marginal traffic).
 centers (commercial, residential, entertainment, etc.) with some heavily-used links connecting them (group C ), and some peripheral ways around, that serve additional traffic (group D ).
  X  We do not need to guarantee short paths for all pairs of nodes, but only for  X  Due to the budget constraint, it may not be possible to guarantee connectivity  X  Certain high cost edges may be an essential part of the backbone that other the BackboneDiscovery problem. In Sect. 3 we survey related work and dis-tinguish our problem from other relevant approaches. Section 4 introduces our algorithm based on the greedy approach, while Sect. 5 provides details of our experimental evaluation, results and discussion. Section 6 is a short conclusion. Let G =( V, E ) be a network, with | V | = n and | E | = m . For each edge e there is a cost c ( e ). Additionally, we consider a traffic log triples ( s i ,t i ,w i ), with s i ,t i  X  V , i =1 ,...,k . A triple ( s fact that w i units of traffic have been recorded between nodes s We aim at discovering the backbone of traffic networks. A backbone R is a subset of the edges of the network G ,thatis, R  X  E that provides a good sum-marization for the whole traffic in L . In particular, we require that if the available traffic had used only edges in the backbone R , it should have been almost as efficient as using all the edges in the network. We formalize this intuition below. Given two nodes s, t  X  V and a subset of edges A  X  E , we consider the shortest path d A ( s, t )from s to t that uses only edges in the set A . In this shortest-path definition, edges are counted according to their cost c . If there is no path from s to t using only edges in A , we define d A d ( s, t ) is the shortest path from s to t using all the edges in the network, and d ( s, t ) is the shortest path from s to t using only edges in the backbone R . To measure the quality of a backbone R , with respect to some traffic log L = { ( s consider shortest paths from s i to t i , and evaluate how much longer are those paths on the backbone R , than on the original network. The idea of using stretch factor for evaluating the quality of a subgraph has been used extensively in the past in the context of spanner graphs [ 8 ].
 In order to aggregate shortest-path information for all source X  X estination pairs in our log in a meaningful way, we need to address two issues. The first issue is that not all source X  X estination pairs have the same volume in the traffic log. This can be easily addressed by weighting the contribution of each pair ( s ,t ) by its corresponding volume w i .
 The second issue is that since we aim at discovering very sparse backbones, many source X  X estination pairs in the log could be disconnected in the backbone. To address this problem we aggregate shortest-path distances using the harmonic mean . This idea, proposed by Marchiori and Latora [ 5 ] and recently used by Boldi and Vigna [ 1 ] in measuring centrality in networks, provides a very clean way to deal with infinite distances: if a source X  X estination pair is not connected, their distance is infinity, so the harmonic mean accounts for this by just adding a zero term in the summation. Using the arithmetic mean is problematic, as we would need to add an infinite term with other finite numbers.
 Overall, given a set of edges A  X  E , we measure the connectivity of the traffic log
L = { ( s i ,t i ,w i ) } , |L| = k by The stretch factor of a backbone R is then defined as The stretch factor is always greater or equal than 1. The closer it is to 1, the better the connectivity that it offers to the traffic log factor provides a principled objective to optimize connectivity while allowing to leave disconnected pairs, when there is insufficient budget.
 Problem 1 ( BackboneDiscovery ). Consider a network G =( V, E ) and a a backbone network R  X  E of total cost B that minimizes the stretch factor  X  ( R ) or report that no such solution exists.
 lack of space, uses a reduction from the set cover problem. The BackboneDiscovery is related to the k -spanner and the Steiner-forest problem [ 8 , 13 ]. In the k -spanner problem the goal is to find a minimum-cost subnetwork R of G , such that for each pair of nodes u and v , the shortest path between u and v on R is at most k times longer than the shortest path between u and v on G . In our problem, we are not necessarily interested in preserving the k -factor distance between all nodes but for only a subset of them. and the goal is to find a minimum-cost forest on which each source s to the corresponding destination t i . Our problem is different from the Steiner-forest problem because we do not need all { ( s i ,t i ) } to optimize a stretch factor so that the structural aspect of the network are also taken into account. The Prize collecting Steiner-forest problem (PCSF) [ 4 ] is a version of the Steiner-forest problem that allows for disconnected source X  destination pairs, by imposing a penalty for disconnected pairs.
 fiers, simplifying graphs and subgraph extraction [ 2 , 6 , 7 , 11 , 14 ]. However these approaches do not consider budget constraints in the context of structural and functional information.
 from larger subgraphs subject to constraints [ 3 , 10 ]. The main focus of most of these approaches is on the trade-off between the level of network reduction and the amount of relevant information to be preserved either for visualization or community detection. The algorithm we propose for the BackboneDiscovery problem is a greedy heuristic that connects one-by-one the source X  X estination pairs of the traffic log L . A distinguishing feature of our algorithm is that it utilizes a notion of edge benefit . In particular, we assume that for each edge e include the edge e in the final solution. The benefit measure is computed using the traffic log L and it takes into account the global structure of the network G . The more central an edge is with respect to a traffic log, the more beneficial it is to include it in the solution, as it can be used to serve many source X  X estination pairs. In this paper we use edge-betweenness as a centrality measure, adapted to take into account the traffic log. We also experimented with commute-time centrality , but edge-betweenness was found to be more effective.
 Our algorithm relies on the notion of effective distance ( e ), defined as ( e )= c ( e ) /b ( e ), where c ( e ) is the cost of an edge e  X  ness of e . The intuition is that by dividing the cost of each edge by its benefit, we are biasing the algorithm towards selecting edges with high benefit. We now present our algorithm in more detail. 4.1 The Greedy Algorithm As discussed above, our algorithm operates with effective distances ( e )= a cost/benefit trade-off: edges with small cost and large benefit are favored to be included in the backbone. In the description of the greedy algorithm that follows, we assume that the effective distance ( e ) of each edge is given as input. The algorithm works in an iterative fashion, maintaining and growing the backbone, starting from the empty set. In the i -th iteration the algorithm picks a source X  X estination pair ( s i ,t i ) from the traffic log a pair ( s i ,t i ) means computing a shortest path p i from s its edges in the current R , if they are not already there. For the shortest-path computation the algorithm uses the effective distances ( e ). When an edge is newly added to the backbone its cost is subtracted from the available budget. distance is reset to zero, since it can be used for free in subsequent iterations of the algorithm. The source X  X estination pair that is chosen to be served in each iteration is the one that reduces the stretch factor the most at that iteration; and hence the greedy nature of the algorithm. The algorithm proceeds until it exhausts all its budget or until the stretch factor becomes equal to 1 (which means that all pairs in the log are served via a shortest path). The pseudo-code for the greedy algorithm is shown as Algorithm 1 .
 We experiment with two variants of this greedy scheme, depending on the benefit score we use. (i) Greedy : we use uniform benefit scores, b ( e )=1; (ii) GreedyEB : the benefit score of an edge is set equal to its weighted edge-betweenness centrality , weighted by the traffic log L . 4.2 Speeding up the Greedy Algorithm As we show in the experimental section the greedy algorithm provides solutions of good quality, in particularly the variant with the edge-betweenness weighting scheme. As the greedy algorithm is expensive, in this section we discuss a number of optimizations. We start by analyzing the running time of the greedy. Algorithm 1. The greedy algorithm Running Time. Assume that the benefit scores b ( e ) are given for all edges e  X  E , and that the algorithm performs I iterations until it exhausts its budget. In each iteration we need to perform O ( k 2 ) shortest-path computations, where k is the size of the traffic log L . A shortest path computation is and thus the overall complexity of the algorithm is O ( Ik number of iterations I depends on the available budget and in the worst case it can be as large as k . However, since we aim at finding sparse backbones, the number of iterations is typically smaller.
 tion techniques: (i) We maintain the connected components built by greedy. We use this information to avoid re-computing shortest paths for all ( s which s i and t i belong to different connected components. (ii) When computing the decrease in the stretch factor due to a candidate shortest path to be added in the backbone, for pairs for which we have to recompute a shortest-path dis-tance, we first compute an optimistic lower bound, based on the shortest path on the whole network (which we compute once in a preprocessing step). If this opti-mistic lower bound is not better than the current best stretch factor then we can skip the computation of the shortest path on the backbone. In practice, these two optimizations lead to 20 X 35% improvement in performance (details in Sect. 5 ). (iii) We use landmarks [ 9 ] to approximate the computation of shortest paths. This reduces the complexity of our greedy algorithm to O ( I ( k + m + n log n )), where is the number of landmarks. As we show in Fig. 4 , this optimization provides an improvement of up to 4 times in terms of runtime. The aim of the experimental section is to evaluate the performance of the pro-posed algorithm, the optimizations, and the edge-betweenness measure. We also compare our algorithm with other state-of-the-art methods which attempt to solve a similar problem.
 Datasets. We experiment with six real datasets: four transportation networks, one web network and one internet-traffic network. For five of the datasets we also obtain real traffic, while for one we use synthetically-generated traffic. The characteristics and description of our datasets are provided in Table 1 . Traffic Log for UKRoad . Since we were not able to obtain real-world traffic data for the UKRoad network, we generate synthetic traffic logs ent scenarios. In particular we generate traffic logs according to four different distributions: (i) power-law traffic volume, power-law s -t pairs; (ii) power-law traffic volume, uniformly random s -t pairs; (iii) uniformly random traffic vol-ume, power-law s -t pairs; and (iv) uniformly random traffic volume, uniformly random s -t pairs. The goal is to understand the behavior of the algorithm with respect to the characteristics of the traffic log L .
 Baseline. To obtain better intuition for the performance of our methods we define a simple baseline, where a backbone is created by adding edges in increas-betweenness; this was the best-performing baseline among other baselines we tried, such as adding source X  X estination pairs one by one (i) randomly, (ii) in decreasing order of volume ( w i ), (iii) in increasing order of effective distance defined using closeness centrality, etc. 5.1 Quantitative Results We focus our evaluation on three main criteria: (i) Comparison of the perfor-mance with and without the edge-betweenness measure; (ii) effect of the opti-mizations, in terms of quality and speedup; and (iii) effect of allocating more budget on the stretch factor.
 Effect of Edge-Betweenness. We study the effect of using edge-betweenness in the Greedy algorithm. The results are presented in Fig. 3 .
 Effect of Landmarks. Landmarks provide faster computation with a trade off for quality. Figure 4 shows the speedup achieved when using landmarks. In the figures, BasicGreedyEB indicates the greedy algorithm that doesn X  X  use any optimizations. GreedyEBCC makes use of the optimizations proposed in Sect. 4.2 which do not use approximation. GreedyEBLandmarks* makes use of the land-marks optimatization and the * indicates the number of landmarks we tried. Figure 5 shows the performance of GreedyEB algorithm with and without using landmarks.
 Budget vs. Stretch Factor. We examine the trade-off between budget and stretch factor for our algorithm and its variants. A lower stretch factor for the same budget indicates that the algorithm is able to pick better edges for the backbone. Figure 3 shows the trade-off between budget and stretch factor for all our datasets. In all figures the budget used by the algorithms, shown in the x -axis, is expressed as a percentage of the total edge cost.
 Key Findings. Our key findings are the following.  X  The greedy algorithm and its variants performs much better than the baseline  X  The backbones discovered by our algorithms are sparse and summarize well the given traffic (Figs. 3 , 5 ). In all cases, with about 15 % of the edge cost in the network it is possible to summarize the traffic with stretch factor close to 1. In some cases, even smaller budget (than 15 %) is sufficient to reach a lower stretch-factor value.  X  Incorporating edge-betweenness as an edge-weighting scheme in the algorithm improves the performance, in certain cases there is an improvement of at least 50 % (see Fig. 3 ; in most cases, even though there is a significant improvement, the plot is overshadowed by a worse performing baseline). This is because, using edges of high centrality will make sure that these edges are included in many shortest paths, leading to re-using many edges.  X  The optimizations we propose in Sect. 4.2 help in reducing the running time of our algorithm (Fig. 4 ). For the optimizations not using landmarks, we see around 30 % improvement in running time. Using landmarks substantially decreases the time taken by the algorithms (3 X 4 times). While there is a compromise in the quality of the solution, we can observe from Fig. 5 that the performance drop is small in most cases and can be controlled by choosing the number of landmarks accordingly. Our algorithms, using the optimizations we propose, scale to large, real-world networks with tens of thousands of nodes, which is the typical size of a road/traffic network.
 5.2 Comparison to Existing Approaches In this section, we compare the performance of BackboneDiscovery other related work in literature. The comparison is done based on two factors (i) stretch factor and (ii) percentage of edges covered by the solution. Intuitively, a good backbone should try to minimize both, i.e., produce a sparse backbone, which also preserves the shortest paths between vertices as well as possible. Comparison with Prize Collecting Steiner-Forest (PCSF). Prize Col-lecting Steiner-forest [ 4 ] is a variant of the classic Steiner Forest problem, which allows for disconnected source X  X estination pairs, by paying a penalty. The goal is to minimize the total cost of the solution by  X  X uying X  a set of edges (to connect the s  X  t pairs) and paying the penalty for those pairs which are not connected. We compare the performance of GreedyEB with PCSF, based on two factors (i) stretch factor (Fig. 6 a), and (ii) percentage of edges covered by the solution (Fig. 6 b). We use the same ( s , t ) pairs that we use in GreedyEB and set the traffic volume w i as the penalty score in PCSF. We first run PCSF on our datasets and compute the budget of the solution produced. Using the budget as input to GreedyEB , we compute our backbone.
 better stretch factor than PCSF. In most datasets, our algorithm produces a backbone which is at least 2 times better in terms of stretch factor. observe that the fraction of edges covered by our algorithm is lower than that of PCSF. This could be because GreedyEB re-uses edges belonging to multiple paths. Figure 6 (a,b) show that even though our solution is much better in terms of stretch factor, we produce sparse backbones (in terms of the percentage of edges covered).
 Comparison with k -spanner. As described in Sect. 3 , our problem is similar to k -spanner [ 8 ] in the sense that we try to minimize the stretch factor. A k -spanner of a graph is a subgraph in which any two vertices are at most k times far apart than on the original graph. One of the main advantages of GreedyEB compared to spanners is that spanners can not handle disconnected vertices. We also propose and optimize a modified version of stretch factor in order to handle disconnected vertices. Similar to PCSF, we first compute a 2-spanner using a 2 approximation greedy algorithm and compute the budget used. We then run GreedyEB for the same budget. Figure 6 (c,d) show the performance of GreedyEB in terms of stretch factor and percentage of edges covered. Our objective here is to compare the cost GreedyEB pays in terms of stretch factor for allowing disconnected vertices. We can clearly observe that even though we allow for disconnected pairs, Greedy-EB performs slightly better in terms of stretch factor and also produces a signifi-cantly sparser backbone.
 Comparison with the Algorithm of Toivonen et al. (T-IGA). Next, we compare GreedyEB with the Iterative Global Algorithm proposed in Toivonen, et al. [ 11 ] (T-IGA), a framework for path-oriented graph simplification, in which edges are pruned while keeping the original quality of the paths between all pairs of nodes. The objective here is to check how well we perform in terms of graph sparsification. Figure 6 (e,f) shows the comparison in terms of stretch factor and percentage of edges covered. Similar to the above approaches, we use the same budget as that used by T-IGA. We observe that for most of the datasets, their algorithm works poorly in terms of sparsification, pruning less than 20 % of the edges (Fig. 6 (f)). Our algorithm performs better both in terms of the stretch of the final solution as well as sparseness of the backbone.
 the power of our algoritm in finding a concise representation of the graph, at the same time maintaining a low stretch factor. In all the three cases, GreedyEB performs considerably better than the related work.
 Fairness. Though we claim that our approach performs better, we need to keep in mind that there might be differences between these algorithms. PCSF does not optimize for stretch factor. Spanners and T-IGA do not have a traffic log (( s , t ) pairs). They also do not try to optimize stretch factor. For this section, we were just interested in contrasting the performance of our approach with existing state of the art methods and show how our approach is different and better at what we do. We introduced a new problem, BackboneDiscovery , to address a modern phe-nomenon: these days not only is the structural information of a network available but increasingly, highly granular functional (activity) information related to net-work usage is accessible. For example, the aggregate traffic usage of the London Subway between all stations is available from a public website. The Discovery problem allowed us to efficiently combine structural and functional information to obtain a highly sophisticated understanding of how the Tube is used (See Fig. 1 ) making it an important tool for network and traffic planning.
