 Localized search engines are small-scale systems that index a particular community on the web. They offer several ben-efits over their large-scale counterparts in that they are rel-atively inexpensive to build, and can provide more precise and complete search capability over their relevant domains. One disadvantage such systems have over large-scale search engines is the lack of global PageRank values. Such informa-tion is needed to assess the value of pages in the localized search domain within the context of the web as a whole. In this paper, we present well-motivated algorithms to es-timate the global PageRank values of a local domain. The algorithms are all highly scalable in that, given a local do-main of size n ,theyuse O ( n ) resources that include compu-tation time, bandwidth, and storage. We test our methods across a variety of localized domains, including site-specific domains and topic-specific domains. We demonstrate that by crawling as few as n or 2 n additional pages, our methods can give excellent global PageRank estimates.
 H.3.3 [ Information Storage and Retrieval ]: Informa-tion Search and Retrieval; G.1.3 [ Numerical Analysis ]: Numerical Linear Algebra; G.3 [ Probability and Statis-tics ]: Markov Processes PageRank, Markov Chain, Stochastic Complementation Algorithms, Experimentation
Localized search engines are small-scale search engines that index only a single community of the web. Such com-munities can be site-specific domains, such as pages within Copyright 2006 ACM 1-59593-339-5/06/0008 ... $ 5.00. the cs.utexas.edu domain, or topic-related communities X  for example, political websites. Compared to the web graph crawled and indexed by large-scale search engines, the size of such local communities is typically orders of magnitude smaller. Consequently, the computational resources needed to build such a search engine are also similarly lighter. By restricting themselves to smaller, more manageable sections of the web, localized search engines can also provide more precise and complete search capabilities over their respective domains.

One drawback of localized indexes is the lack of global information needed to compute link-based rankings. The PageRank algorithm [3], has proven to be an effective such measure. In general, the PageRank of a given page is de-pendent on pages throughout the entire web graph. In the context of a localized search engine, if the PageRanks are computed using only the local subgraph, then we would ex-pect the resulting PageRanks to reflect the perceived popu-larity within the local community and not of the web as a whole. For example, consider a localized search engine that indexes political pages with conservative views. A person wishing to research the opinions on global warming within the conservative political community may encounter numer-ous such opinions across various websites. If only local Page-Rank values are available, then the search results will reflect only strongly held beliefs within the community. However, if global PageRanks are also available, then the results can ad-ditionally reflect outsiders X  views of the conservative commu-nity (those documents that liberals most often access within the conservative community).

Thus, for many localized search engines, incorporating global PageRanks can improve the quality of search results. However, the number of pages a local search engine indexes is typically orders of magnitude smaller than the number of pages indexed by their large-scale counterparts. Localized search engines do not have the bandwidth, storage capacity, or computational power to crawl, download, and compute the global PageRanks of the entire web. In this work, we present a method of approximating the global PageRanks of a local domain while only using resources of the same or-der as those needed to compute the PageRanks of the local subgraph.

Our proposed method looks for a supergraph of our local subgraph such that the local PageRanks within this super-graph are close to the true global PageRanks. We construct this supergraph by iteratively crawling global pages on the current web frontier X  X .e., global pages with inlinks from pages that have already been crawled. In order to provide a good approximation to the global PageRanks, care must be taken when choosing which pages to crawl next; in this paper, we present a well-motivated page selection algorithm that also performs well empirically. This algorithm is de-rived from a well-defined problem objective and has a run-ning time linear in the number of local nodes.

We experiment across several types of local subgraphs, including four topic related communities and several site-specific domains. To evaluate performance, we measure the difference between the current global PageRank estimate and the global PageRank, as a function of the number of pages crawled. We compare our algorithm against several heuristics and also against a baseline algorithm that chooses pages at random, and we show that our method outperforms these other methods. Finally, we empirically demonstrate that, given a local domain of size n , we can provide good approximations to the global PageRank values by crawling at most n or 2 n additional pages.

The paper is organized as follows. Section 2 gives an overview of localized search engines and outlines their ad-vantages over global search. Section 3 provides background on the PageRank algorithm. Section 4 formally defines our problem, and section 5 presents our page selection criteria and derives our algorithms. Section 6 provides experimen-tal results, section 7 gives an overview of related work, and, finally, conclusions are given in section 8.
Localized search engines index a single community of the web, typically either a site-specific community, or a topic-specific community. Localized search engines enjoy three major advantages over their large-scale counterparts: they are relatively inexpensive to build, they can offer more pre-cise search capability over their local domain, and they can provide a more complete index.

The resources needed to build a global search engine are enormous. A 2003 study by Lyman et al. [13] found that the  X  X urface web X  (publicly available static sites) consists of 8.9 billion pages, and that the average size of these pages is approximately 18.7 kilobytes. To download a crawl of this size, approximately 167 terabytes of space is needed. For a researcher who wishes to build a search engine with access to a couple of workstations or a small server, storage of this magnitude is simply not available. However, building a lo-calized search engine over a web community of a hundred thousand pages would only require a few gigabytes of stor-age. The computational burden required to support search queries over a database this size is more manageable as well. We note that, for topic-specific search engines, the relevant community can be efficiently identified and downloaded by using a focused crawler [21, 4].

For site-specific domains, the local domain is readily avail-able on their own web server. This obviates the need for crawling or spidering, and a complete and up-to-date in-dex of the domain can thus be guaranteed. This is in con-trast to their large-scale counterparts, which suffer from sev-eral shortcomings. First, crawling dynamically generated pages X  X ages in the  X  X idden web X  X  X as been the subject of research [20] and is a non-trivial task for an external crawler. Second, site-specific domains can enable the robots exclu-sion policy. This prohibits external search engines X  crawlers from downloading content from the domain, and an external search engine must instead rely on outside links and anchor text to index these restricted pages.

By restricting itself to only a specific domain of the in-ternet, a localized search engine can provide more precise search results. Consider the canonical ambiguous search query,  X  X aguar X , which can refer to either the car manufac-turer or the animal. A scientist trying to research the habi-tat and evolutionary history of a jaguar may have better success using a finely tuned zoology-specific search engine than querying Google with multiple keyword searches and wading through irrelevant results. A method to learn bet-ter ranking functions for retrieval was recently proposed by Radlinski and Joachims [19] and has been applied to various local domains, including Cornell University X  X  website [8].
The PageRank algorithm defines the importance of web pages by analyzing the underlying hyperlink structure of a web graph. The algorithm works by building a Markov chain from the link structure of the web graph and computing its stationary distribution. One way to compute the station-ary distribution of a Markov chain is to find the limiting distribution of a random walk over the chain. Thus, the PageRank algorithm uses what is sometimes referred to as the  X  X andom surfer X  model. In each step of the random walk, the  X  X urfer X  either follows an outlink from the current page (i.e. the current node in the chain), or jumps to a random page on the web.

We now precisely define the PageRank problem. Let U be an m  X  m adjacency matrix for a given web graph such that U ji =1ifpage i links to page j and U ji = 0 otherwise. We define the PageRank matrix P U to be: where D U is the (unique) diagonal matrix such that UD  X  1 is column stochastic,  X  is a given scalar such that 0  X   X  e is the vector of all ones, and v is a non-negative, L 1 normalized vector, sometimes called the  X  X andom surfer X  vec-tor. Note that the matrix D  X  1 U is well-defined only if each column of U has at least one non-zero entry X  X .e., each page in the webgraph has at least one outlink. In the presence of such  X  X angling nodes X  that have no outlinks, one commonly used solution, proposed by Brin et al. [3], is to replace each zero column of U by a non-negative, L 1 -normalized vector. The PageRank vector r is the dominant eigenvector of the PageRank matrix, r = P U r . We will assume, without loss of generality, that r has an L 1 -norm of one. Computationally, r can be computed using the power method. This method first chooses a random starting vector r (0) , and iteratively multiplies the current vector by the PageRank matrix P U ; see Algorithm 1. In general, each iteration of the power method can take O ( m 2 )operationswhen P U isadensema-trix. However, in practice, the number of links in a web graph will be of the order of the number of pages. By ex-ploiting the sparsity of the PageRank matrix, the work per iteration can be reduced to O ( km ), where k is the average number of links per web page. It has also been shown that the total number of iterations needed for convergence is pro-portional to  X  anddoesnotdependonthesizeoftheweb graph [11, 7]. Finally, the total space needed is also O ( km ), mainly to store the matrix U .
Algorithm 1: A linear time (per iteration) algorithm for ComputePR ( U ) Input: U : Adjacency matrix.
 Output: r : PageRank vector.
 Choose (randomly) an initial non-negative vector r (0) such that r (0) 1 =1. i  X  0 repeat ity } until r ( i )  X  r ( i  X  1) &lt; X  {  X  is the convergence threshold. r
Given a local domain L , let G be an N  X  N adjacency matrix for the entire connected component of the web that contains L , such that G ji = 1 if page i links to page j and G ji = 0 otherwise. Without loss of generality, we will partition G as: where L is the n  X  n local subgraph corresponding to links inside the local domain, L out is the subgraph that corre-sponds to links from the local domain pointing out to the global domain, G out is the subgraph containing links from the global domain into the local domain, and G within con-tains links within the global domain. We assume that when building a localized search engine, only pages inside the lo-cal domain are crawled, and the links between these pages are represented by the subgraph L . The links in L out are also known, as these point from crawled pages in the local domain to uncrawled pages in the global domain.

As defined in equation (1), P G is the PageRank matrix formed from the global graph G , and we define the global PageRank vector of this graph to be g .Letthe n -length vector p  X  be the L 1 -normalized vector corresponding to the global PageRank of the pages in the local domain L : where E L =[ I | 0 ] is the restriction matrix that selects the components from g corresponding to nodes in L .Let p denote the PageRank vector constructed from the local do-main subgraph L . In practice, the observed local PageRank p and the global PageRank p  X  will be quite different. One would expect that as the size of local matrix L approaches the size of global matrix G , the global PageRank and the ob-served local PageRank will become more similar. Thus, one approach to estimating the global PageRank is to crawl the entire global domain, compute its PageRank, and extract the PageRanks of the local domain.

Typically, however, n N , i.e., the number of global pages is much larger than the number of local pages. There-fore, crawling all global pages will quickly exhaust all local resources (computational, storage, and bandwidth) available to create the local search engine. We instead seek a super-graph  X  F of our local subgraph L with size O ( n ). Our goal FindGlobalPR ( L , L out , T , k ) Input: L : zero-one adjacency matrix for the local do-main, L out : zero-one outlink matrix from L to global subgraph as in (2), T : number of iterations, k :numberof pages to crawl per iteration.
 Output:  X  p : an improved estimate of the global Page-Rank of L .
 F  X  L F f  X  ComputePR ( F ) for ( i =1to T ) end {
Extract PageRanks of original local domain &amp; normalize  X  p istofindsuchasupergraph  X  F with PageRank  X  f ,sothat  X  f when restricted to L is close to p  X  . Formally, we seek to minimize We cho ose the L 1 norm for measuring the error as it does not place excessive weight on outliers (as the L 2 norm does, for example), and also because it is the most commonly used distance measure in the literature for comparing PageRank vectors, as well as for detecting convergence of the algo-rithm [3].

We propose a greedy framework, given in Algorithm 2, for constructing  X  F . Initially, F is set to the local subgraph L , and the PageRank f of this graph is computed. The al-gorithm then proceeds as follows. First, the SelectNodes algorithm (which we discuss in the next section) is called and it returns a set of k nodes to crawl next from the set of nodes in the current crawl frontier, F out . These selected nodes are then crawled to expand the local subgraph, F ,and the PageRanks of this expanded graph are then recomputed. These steps are repeated for each of T iterations. Finally, the PageRank vector  X  p , which is restricted to pages within the original local domain, is returned. Given our computa-tion, bandwidth, and memory restrictions, we will assume that the algorithm will crawl at most O ( n ) pages. Since the PageRanks are computed in each iteration of the algorithm, which is an O ( n ) operation, we will also assume that the number of iterations T is a constant. Of course, the main challenge here is in selecting which set of k nodes to crawl next. In the next section, we formally define the problem and give efficient algorithms.
In this section, we present node selection algorithms that operate within the greedy framework presented in the pre-vious section. We first give a well-defined criteria for the page selection problem and provide experimental evidence that this criteria can effectively identify pages that optimize our problem objective (3). We then present our main al-gorithmic contribution of the paper, a method with linear running time that is derived from this page selection crite-ria. Finally, we give an intuitive analysis of our algorithm in terms of  X  X eaks X  and  X  X lows X . We show that if only the  X  X low X  is considered, then the resulting method is very similar to a widely used page selection heuristic [6].
For a given page j in the global domain, we define the expanded local graph F j : where u j is the zero-one vector containing the outlinks from F into page j ,and s contains the inlinks from page j into the local domain. Note that we do not allow self-links in this framework. In practice, self-links are often removed, as they only serve to inflate a given page X  X  PageRank.
Observe that the inlinks into F from node j are not known until after node j is crawled. Therefore, we estimate this inlink vector as the expectation over inlink counts among the set of already crawled pages, In practice, for any given page, this estimate may not reflect the true inlinks from that page. Furthermore, this expec-tation is sampled from the set of links within the crawled domain, whereas a better estimate would also use links from the global domain. However, the latter distribution is not known to a localized search engine, and we contend that the above estimate will, on average, be a better estimate than the uniform distribution, for example.

Let the PageRank of F be f . We express the PageRank f j of the expanded local graph F j as where x j is the PageRank of the candidate global node j , and f j is the L 1 -normalized PageRank vector restricted to the pages in F .

Since directly optimizing our problem goal requires know-ing the global PageRank p  X  , we instead propose to crawl those nodes that will have the greatest influence on the Page-Ranks of pages in the original local domain L : Experimentally, the influence score is a very good predictor of our problem objective (3). For each candidate global node j , figure 1(a) shows the objective function value Global Diff ( f as a function of the influence of page j . The local domain used here is a crawl of conservative political pages (we will provide more details about this dataset in section 6); we observed similar results in other domains. The correlation is quite strong, implying that the influence criteria can ef-fectively identify pages that improve the global PageRank estimate. As a baseline, figure 1(b) compares our objec-tive with an alternative criteria, outlink count. The outlink count is defined as the number of outlinks from the local domain to page j . The correlation here is much weaker. Figure 1: (a) The correlation between our influence page selection criteria (7) and the actual objective function (3) value is quite strong. (b) This is in con-trast to other criteria, such as outlink count, which exhibit a much weaker correlation.
As described, for each candidate global page j ,the in-fluence score (7) must be computed. If f j is computed exactly for each global page j , then the PageRank algo-rithm would need to be run for each of the O ( n ) such global pages j we consider, resulting in an O ( n 2 ) computational cost for the node selection method. Thus, computing the exact value of f j will lead to a quadratic algorithm, and we must instead turn to methods of approximating this vector. The algorithm we present works by performing one power method iteration used by the PageRank algorithm (Algo-rithm 1). The convergence rate for the PageRank algorithm has been shown to equal the random surfer probability  X  [7, 11]. Given a starting vector x (0) ,if k PageRank iterations are performed, the current PageRank solution x ( k ) satisfies: where x  X  is the desired PageRank vector. Therefore, if only one iteration is performed, choosing a good starting vector is necessary to achieve an accurate approximation.
We partition the PageRank matrix P F j , corresponding to the  X  subgraph F j as: where  X  F =  X  F ( D F + diag ( u j ))  X  1 +(1  X   X  ) and diag ( u j ) is the diagonal matrix with the ( i, i ) equal to one if the i th element of u j equals one, and is zero otherwise. We have assumed here that the random surfer vector is the uniform vector, and that L has no  X  X angling links X . These assumptions are not necessary and serve only to simplify discussion and analysis.

A simple approach for estimating f j is the following. First, estimate the PageRank f + j of F j by computing one Page-Rank iteration over the matrix P F j , using the starting vec-tor  X  = component from our estimate of f + j (i.e., the component corresponding to the added node j ), and renormalizing. The problem with this approach is in the starting vector. Recall from (6) that x j is the PageRank of the added node j . The difference between the actual PageRank f + j of P and the starting vector  X  is Thus, by (8), after one PageRank iteration, we expect our estimate of f + j to still have an error of about 2  X x j .Inpar-ticular, for candidate nodes j with relatively high PageRank x , this method will yield more inaccurate results. We will next present a method that eliminates this bias and runs in O ( n ) time. Since f + j , as given in (6) is the PageRank of the matrix P Solving the above system for f j can be shown to yield complement of the column stochastic matrix P F j with re-spect to the sub matrix  X  F . The theory of stochastic comple-mentation is well studied, and it can be shown the stochastic complement of an irreducible matrix (such as the PageRank matrix) is unique. Furthermore, the stochastic complement is also irreducible and therefore has a unique stationary dis-tribution as well. For an extensive study, see [15].
It can be easily shown that the sub-dominant eigenvalue of S is at most +1  X  ,where is the size of F . For sufficiently large , this value will be very close to  X  . This is important, as other properties of the PageRank algorithm, notably the algorithm X  X  sensitivity, are dependent on this value [11].
In this method, we estimate the length vector f j by computing one PageRank iteration over the  X  stochastic complement S ,startingatthevector f : This is in contrast to the simple method outlined in the pre-vious section, which first iterates over the ( +1)  X  ( +1) matrix P F j to estimate f + j , and then removes the last com-ponent from the estimate and renormalizes to approximate f . The problem with the latter method is in the choice of the ( + 1) length starting vector,  X  . Consequently, the PageRank estimate given by the simple method differs from the true PageRank by at least 2  X x j ,where x j is the Page-Rank of page j . By using the stochastic complement, we can establish a tight lower bound of zero for this difference. To see this, consider the case in which a node k is added to F to form the augmented local subgraph F k ,andthat the PageRank of this new graph is (1 cally, the addition of page k does not change the PageRanks of the pages in F , and thus f k = f . By construction of the stochastic complement, f k = S f k , so the approximation given in equation (11) will yield the exact solution.
Next, we present the computational details needed to effi-ciently compute the quantity f j  X  f 1 over all known global pages j . We begin by expanding the difference f j  X  f ,where the vector f j is estimated as in (11), f j  X  f  X  Sf  X  f Note that the matrix ( D F + diag ( u j ))  X  1 is diagonal. Letting o [ k ] be the outlink count for page k in F , we can express the k th diagonal element as: Noting that ( o [ k ]+1)  X  1 = o [ k ]  X  1  X  ( o [ k ]( o [ k ]+1)) rewriting this in matrix form yields ( D F + diag ( u j ))  X  1 = D  X  1 F  X  D  X  1 F ( D F + diag ( u We use the same identity to express Recall that, by definition, we have P F =  X  FD  X  1 F +(1  X  Substituting (13) and (14) in (12) yields noting that by definition, f = P F f , and defining the vectors x , y ,and z to be The first term x is a sparse vector, and takes non-zero values only for local pages k that are siblings of the global page j .Wedefine( i, j )  X  F if and only if F [ j, i ]=1(equiva-lently, page i links to page j ) and express the value of the component x [ k ]as: where o [ k ], as before, is the number of outlinks from page k in the local domain. Note that the last two terms, y and z are not dependent on the current global node j . Given the function h j ( f )= y +(  X  u T j f ) z 1 ,thequantity f j can be expressed as If we can compute the function h j in linear time, then we can compute each value of f j  X  f 1 using an additional amount of time that is proportional to the number of non-zero components in x . These optimizations are carried out in Algorithm 3. Note that (20) computes the difference be-tween all components of f and f j , whereas our node selec-tion criteria, given in (7), is restricted to the components corresponding to nodes in the original local domain L .
Let us examine Algorithm 3 in more detail. First, the algorithm computes the outlink counts for each page in the local domain. The algorithm then computes the quantity  X  u j f for each known global page j . This inner product can be written as where the second term sums over the set of local pages that link to page j . Since the total number of edges in F out assumed to have size O ( )(recallthat isthenumberof pages in F ), the running time of this step is also O ( ).
The algorithm then computes the vectors y and z ,as given in (17) and (18), respectively. The L 1 NormDiff method is called on the components of these vectors which correspond to the pages in L , and it estimates the value of
E L ( y +(  X  u T j f ) z ) 1 for each page j . The estimation works as follows. First, the values of  X  u T j f are discretized uniformly into c values { a 1 , ..., a c } .Thequantity E L ( y + a then computed for each discretized value of a i andstoredin atable. Toevaluate E L ( y + a z ) 1 for some a  X  [ a 1 ,a the closest discretized value a i is determined, and the corre-sponding entry in the table is used. The total running time for this method is linear in and the discretization parame-ter c (which we take to be a constant). We note that if exact values are desired, we have also developed an algorithm that runs in O ( log ) time that is not described here.
In the main loop, we compute the vector x ,asdefined in equation (16). The nested loops iterate over the set of pages in F that are siblings of page j . Typically, the size of this set is bounded by a constant. Finally, for each page j ,the scores vector is updated over the set of non-zero components k of the vector x with k  X  L . This set has size equal to the number of local siblings of page j ,andis a subset of the total number of siblings of page j .Thus, each iteration of the main loop takes constant time, and the total running time of the main loop is O ( ). Since we have assumed that the size of F will not grow larger than O ( n ), the total running time for the algorithm is O ( n ). SC-Select ( F , F out , f , k )
Input: F : zero-one adjacency matrix of size corre-sponding to the current local subgraph, F out : zero-one outlink matrix from F to global subgraph, f :Page-Rank of F , k : number of pages to return
Output: pages: set of k pages to crawl next { Compute outlink sums for local subgraph } foreach (page j  X  F ) end { Compute scalar  X  u T j f for each global node j } foreach (page j  X  F out ) end { Compute vectors y and z as in (17) and (18) } z  X  (1  X  w )  X  1  X  s { Approximate y + g [ j ]  X  z 1 for all values g [ j ] } norm diffs  X  L 1 NormDiffs ( g , E L y , E L z ) foreach (page j  X  F out ) end
Return k pages with highest scores
We now present an intuitive analysis of the stochastic complementation method by decomposing the change in Page-Rank in terms of  X  X eaks X  and  X  X lows X . This analysis is moti-vated by the decomposition given in (15). PageRank  X  X low X  is the increase in the local PageRanks originating from global page j . The flows are represented by the non-negative vector (  X  u j f ) z (equations (15) and (18)). The scalar  X  u T j f can be thought of as the total amount of PageRank flow that page j has available to distribute. The vector z dictates how the flow is allocated to the local domain; the flow that local page k receives is proportional to (within a constant factor due to the random surfer vector) the expected number of its inlinks.

The PageRank  X  X eaks X  represent the decrease in PageRank resulting from the addition of page j . The leakage can be quantified in terms of the non-positive vectors x and y (equations (16) and (17)). For vector x , we can see from equation (19) that the amount of PageRank leaked by a local page is proportional to the weighted sum of the Page-Ranks of its siblings. Thus, pages that have siblings with higher PageRanks (and low outlink counts) will experience more leakage. The leakage caused by y is an artifact of the random surfer vector.

We will next show that if only the  X  X low X  term, (  X  u T j is considered, then the resulting method is very similar to a heuristic proposed by Cho et al. [6] that has been widely used for the  X  X rawling Through URL Ordering X  problem. This heuristic is computationally cheaper, but as we will see later, not as effective as the Stochastic Complementation method.

Our node selection strategy chooses global nodes that have the largest influence (equation (7)). If this influence is approximated using only  X  X lows X , the optimal node j  X  is: j  X  =argmax The resulting page selection score can be expressed as a sum of the PageRanks of each local page k that links to j ,where each PageRank value is normalized by o [ k ]+1. Interestingly, the normalization that arises in our method differs from the heuristic given in [6], which normalizes by o [ k ]. The al-gorithm PF-Select , which is omitted due to lack of space, first computes the quantity f T ( D F + diag ( u j ))  X  1 global page j , and then returns the pages with the k largest scores. To see that the running time for this algorithm is O ( n ), note that the computation involved in this method is a subset of that needed for the SC-Select method (Algo-rithm 3), which was shown to have a running time of O ( n ).
In this section, we provide experimental evidence to ver-ify the effectiveness of our algorithms. We first outline our experimental methodology and then provide results across a variety of local domains.
Given the limited resources available at an academic in-stitution, crawling a section of the web that is of the same magnitude as that indexed by Google or Yahoo! is clearly infeasible. Thus, for a given local domain, we approximate the global graph by crawling a local neighborhood around the domain that is several orders of magnitude larger than the local subgraph. Even though such a graph is still orders of magnitude smaller than the  X  X rue X  global graph, we con-tend that, even if there exist some highly influential pages that are very far away from our local domain, it is unrealis-tic for any local node selection algorithm to find them. Such pages also tend to be highly unrelated to pages within the local domain.

When explaining our node selection strategies in section 5, we made the simplifying assumption that our local graph contained no dangling nodes. This assumption was only made to ease our analysis. Our implementation efficiently handles dangling links by replacing each zero column of our adjacency matrix with the uniform vector. We evaluate the algorithm using the two node selection strategies given in Section 5.2, and also against the following baseline methods: At each iteration of the FindGlobalPR algorithm, we eval-uate performance by computing the difference between the current PageRank estimate of the local domain, E L f E L f 1 the global PageRank of the local domain E L g E L g 1 .AllPage-Rank calculations were performed using the uniform ran-dom surfer vector. Across all experiments, we set the ran-dom surfer parameter  X  ,tobe . 85, and used a convergence threshold of 10  X  6 . We evaluate the difference between the local and global PageRank vectors using three different met-rics: the L 1 and L  X  norms, and Kendall X  X  tau. The L 1 norm measures the sum of the absolute value of the differences be-tween the two vectors, and the L  X  norm measures the ab-solute value of the largest difference. Kendall X  X  tau metric is a popular rank correlation measure used to compare Page-Ranks [2, 11]. This metric can be computed by counting the number of pairs of pairs that agree in ranking, and sub-tracting from that the number of pairs of pairs that disagree in ranking. The final value is then normalized by the total number of n 2 such pairs, resulting in a [  X  1 , 1] range, where a negative score signifies anti-correlation among rankings, and values near one correspond to strong rank correlation.
Our experiments are based on two large web crawls and were downloaded using the web crawler that is part of the Nutch open source search engine project [18]. All crawls were restricted to only  X  X ttp X  pages, and to limit the num-ber of dynamically generated pages that we crawl, we ig-nored all pages with urls containing any of the characters as the  X  X du X  dataset, was seeded by homepages of the top 100 graduate computer science departments in the USA, as rated by the US News and World Report [16], and also by the home pages of their respective institutions. A crawl of depth 5 was performed, restricted to pages within the  X .edu X  domain, resulting in a graph with approximately 4.7 million pages and 22.9 million links. The second crawl was seeded by the set of pages under the  X  X olitics X  hierarchy in the dmoz open directory project[17]. We crawled all pages up to four links away, which yielded a graph with 4.4 million pages and 17.3 million links.

Within the  X  X du X  crawl, we identified the five site-specific domains corresponding to the websites of the top five grad-uate computer science departments, as ranked by the US News and World Report. This yielded local domains of vari-ous sizes, from 10,626 (UIUC) to 59,895 (Berkeley). For each of these site-specific domains with size n , we performed 50 iterations of the FindGlobalPR algorithm to crawl a total of 2 n additional nodes. Figure 2(a) gives the ( L 1 ) difference from the PageRank estimate at each iteration to the global PageRank, for the Berkeley local domain.

The performance of this dataset was representative of the typical performance across the five computer science site-specific local domains. Initially, the L 1 difference between the global and local PageRanks ranged from . 0469 (Stan-ford) to . 149 (MIT). For the first several iterations, the three link-based methods all outperform the random selec-tion heuristic. After these initial iterations, the random heuristic tended to be more competitive with (or even out-perform, as in the Berkeley local domain) the outlink count and PageRank flow heuristics. In all tests, the stochastic complementation method either outperformed, or was com-petitive with, the other methods. Table 1 gives the average difference between the final estimated global PageRanks and the true global PageRanks for various distance measures. Table 1: Average final performance of various node selection strategies for the five site-specific com-puter science local domains. Note that Kendall X  X  Tau measures similarity, while the other metrics are dissimilarity measures. Stochastic Complementa-tion clearly outperforms the other methods in all metrics.

Within the  X  X olitics X  dataset, we also performed two site-specific tests for the largest websites in the crawl: www.adam-smith.org , the website for the London based Adam Smith Institute, and www.enterstageright.com , an online conser-vative journal. As with the  X  X du X  local domains, we ran our algorithm for 50 iterations, crawling a total of 2 n nodes. Fig-ure 2 (b) plots the results for the www.enterstageright.com domain. In contrast to the  X  X du X  local domains, the Random and OutlinkCount methods were not competitive with ei-ther the SC-Select or the PF-Select methods. Among all datasets and all node selection methods, the stochastic com-plementation method was most impressive in this dataset, realizing a final estimate that differed only . 0279 from the global PageRank, a ten-fold improvement over the initial lo-cal PageRank difference of . 299. For the Adam Smith local domain, the initial difference between the local and global PageRanks was . 148, and the final estimates given by the SC-Select , PF-Select , OutlinkCount ,and Random methods were . 0208, . 0193, . 0222, and . 0356, respectively.
Within the  X  X olitics X  dataset, we constructed four topic-specific local domains. The first domain consisted of all pages in the dmoz politics category, and also all pages within each of these sites up to two links away. This yielded a local domain of 90,811 pages, and the results are given in figure 2 (c). Because of the larger size of the topic-specific domains, we ran our algorithm for only 25 iterations to crawl a total of n nodes.

We also created topic-specific domains from three politi-cal sub-topics: liberalism, conservatism, and socialism. The pages in these domains were identified by their correspond-ing dmoz categories. For each sub-topic, we set the local domain to be all pages within three links from the corre-sponding dmoz category pages. Table 2 summarizes the performance of these three topic-specific domains, and also the larger political domain.
 To quantify a global page j  X  X  effect on the global Page-Rank values of pages in the local domain, we define page j  X  X  impact to be its PageRank value, g [ j ], normalized by the fraction of its outlinks pointing to the local domain: where, o L [ j ] is the number of outlinks from page j to pages in the local domain L ,and o [ j ] is the total number of j  X  X  outlinks. In terms of the random surfer model, the impact of page j is the probability that the random surfer (1) is currently at global page j in her random walk and (2) takes an outlink to a local page, given that she has already decided not to jump to a random page.

For the politics local domain, we found that many of the pageswithhigh impact were in fact political pages that should have been included in the dmoz politics topic, but were not. For example, the two most influential global pages were the political search engine www.askhenry.com ,andthe home page of the online political magazine, www.policy-review.com . Among non-political pages, the home page of the journal  X  X ducation Next X  was most influential. The journal is freely available online and contains articles regard-ing various aspect of K-12 education in America. To provide some anecdotal evidence for the effectiveness of our page se-lection methods, we note that the SC-Select method chose 11 pages within the www.educationnext.org domain, the PF-Select method discovered 7 such pages, while the Out-linkCount and Random methods found only 6 pages each.
For the conservative political local domain, the socialist website www.ornery.org had a very high impact score. This Table 2: Final performance among node selection strategies for the four political topic-specific crawls. Note that Kendall X  X  Tau measures similarity, while the other metrics are dissimilarity measures. was largely due to a link from the front page of this site to an article regarding global warming published by the National Center for Public Policy Research, a conservative research group in Washington, DC. Not surprisingly, the global PageRank of this article (which happens to be on the home page of the NCCPR, www.nationalresearch.com ), was approximately . 002, whereas the local PageRank of this page was only . 00158. The SC-Select method yielded a global PageRank estimate of approximately . 00182, the PF-Select method estimated a value of . 00167, and the Ran-dom and OutlinkCount methods yielded values of . 01522 and . 00171, respectively.
The node selection framework we have proposed is similar to the url ordering for crawling problem proposed by Cho et al. in [6]. Whereas our framework seeks to minimize the difference between the global and local PageRank, the objec-tive used in [6] is to crawl the most highly (globally) ranked pages first. They propose several node selection algorithms, including the outlink count heuristic, as well as a variant of our PF-Select algorithm which they refer to as the  X  X age-Rank ordering metric X . They found this method to be most effective in optimizing their objective, as did a recent survey of these methods by Baeza-Yates et al. [1]. Boldi et al. also experiment within a similar crawling framework in [2], but quantify their results by comparing Kendall X  X  rank correla-tion between the PageRanks of the current set of crawled pages and those of the entire global graph. They found that node selection strategies that crawled pages with the high-est global PageRank first actually performed worse (with respect to Kendall X  X  Tau correlation between the local and global PageRanks) than basic depth first or breadth first strategies. However, their experiments differ from our work in that our node selection algorithms do not use (or have access to) global PageRank values.
 Many algorithmic improvements for computing exact Page-Rank values have been proposed [9, 10, 14]. If such algo-rithms are used to compute the global PageRanks of our local domain, they would all require O ( N ) computation, storage, and bandwidth, where N is the size of the global domain. This is in contrast to our method, which approxi-mates the global PageRank and scales linearly with the size of the local domain.

Wang and Dewitt [22] propose a system where the set of web servers that comprise the global domain communicate with each other to compute their respective global Page-Ranks. For a given web server hosting n pages, the com-putational, bandwidth, and storage requirements are also linear in n . One drawback of this system is that the num-ber of distinct web servers that comprise the global domain can be very large. For example, our  X  X du X  dataset contains websites from over 3,200 different universities; coordinating such a system among a large number of sites can be very difficult.
 Gan, Chen, and Suel propose a method for estimating the PageRank of a single page [5] which uses only constant band-width, computation, and space. Their approach relies on the availability of a remote connectivity server that can supply the set of inlinks to a given page, an assumption not used in our framework. They experimentally show that a reasonable estimate of the node X  X  PageRank can be obtained by visiting at most a few hundred nodes. Using their algorithm for our problem would require that either the entire global domain first be downloaded or a connectivity server be used, both of which would lead to very large web graphs.
The internet is growing exponentially, and in order to nav-igate such a large repository as the web, global search en-gines have established themselves as a necessity. Along with the ubiquity of these large-scale search engines comes an in-crease in search users X  expectations. By providing complete and isolated coverage of a particular web domain, localized search engines are an effective outlet to quickly locate con-tent that could otherwise be difficult to find. In this work, we contend that the use of global PageRank in a localized search engine can improve performance.

To estimate the global PageRank, we have proposed an iterative node selection framework where we select which pages from the global frontier to crawl next. Our primary contribution is our stochastic complementation page selec-tion algorithm. This method crawls nodes that will most significantly impact the local domain and has running time linear in the number of nodes in the local domain. Experi-mentally, we validate these methods across a diverse set of local domains, including seven site-specific domains and four topic-specific domains. We conclude that by crawling an ad-ditional n or 2 n pages, our methods find an estimate of the global PageRanks that is up to ten times better than just using the local PageRanks. Furthermore, we demonstrate that our algorithm consistently outperforms other existing heuristics.
Often times, topic-specific domains are discovered using a focused web crawler which considers a page X  X  content in conjunction with link anchor text to decide which pages to crawl next [4]. Although such crawlers have proven to be quite effective in discovering topic-related content, many ir-relevant pages are also crawled in the process. Typically, these pages are deleted and not indexed by the localized search engine. These pages can of course provide valuable information regarding the global PageRank of the local do-main. One way to integrate these pages into our framework is to start the FindGlobalPR algorithm with the current subgraph F equal to the set of pages that were crawled by the focused crawler.

The global PageRank estimation framework, along with the node selection algorithms presented, all require O ( n ) computation per iteration and bandwidth proportional to thenumberofpagescrawled, Tk . If the number of itera-tions T is relatively small compared to the number of pages crawled per iteration, k , then the bottleneck of the algorithm will be the crawling phase. However, as the number of iter-ations increases (relative to k ), the bottleneck will reside in the node selection computation. In this case, our algorithms would benefit from constant factor optimizations. Recall that the FindGlobalPR algorithm (Algorithm 2) requires that the PageRanks of the current expanded local domain be recomputed in each iteration. Recent work by Langville and Meyer [12] gives an algorithm to quickly recompute Page-Ranks of a given webgraph if a small number of nodes are added. This algorithm was shown to give speedup of five to ten times on some datasets. We plan to investigate this and other such optimizations as future work.

In this paper, we have objectively evaluated our methods by measuring how close our global PageRank estimates are to the actual global PageRanks. To determine the bene-fit of using global PageRanks in a localized search engine, we suggest a user study in which users are asked to rate the quality of search results for various search queries. For some queries, only the local PageRanks are used in rank-ing, and for the remaining queries, local PageRanks and the approximate global PageRanks, as computed by our algo-rithms, are used. The results of such a study can then be analyzed to determine the added benefit of using the global PageRanks computed by our methods, over just using the local PageRanks.
 Acknowledgements. This research was supported by NSF grant CCF-0431257, NSF Career Award ACI-0093404, and a grant from Sabre, Inc. [1] R. Baeza-Yates, M. Marin, C. Castillo, and [2] P. Boldi, M. Santini, and S. Vigna. Do your worst to [3] S. Brin and L. Page. The anatomy of a large-scale [4] S. Chakrabarti, M. van den Berg, and B. Dom.
 [5] Y. Chen, Q. Gan, and T. Suel. Local methods for [6] J. Cho, H. Garcia-Molina, and L. Page. Efficient [7] T. H. Haveliwala and S. D. Kamvar. The second [8] T. Joachims, F. Radlinski, L. Granka, A. Cheng, [9] S. D. Kamvar, T. H. Haveliwala, C. D. Manning, and [10] S. D. Kamvar, T. H. Haveliwala, C. D. Manning, and [11] A. N. Langville and C. D. Meyer. Deeper inside [12] A. N. Langville and C. D. Meyer. Updating the [13] P. Lyman, H. R. Varian, K. Swearingen, P. Charles, [14] F. McSherry. A uniform approach to accelerated [15] C. D. Meyer. Stochastic complementation, uncoupling [16] US News and World Report. http://www.usnews.com. [17] Dmoz open directory project. http://www.dmoz.org. [18] Nutch open source search engine. [19] F. Radlinski and T. Joachims. Query chains: learning [20] S. Raghavan and H. Garcia-Molina. Crawling the [21] T. Tin Tang, D. Hawking, N. Craswell, and [22] Y. Wang and D. J. DeWitt. Computing pagerank in a
