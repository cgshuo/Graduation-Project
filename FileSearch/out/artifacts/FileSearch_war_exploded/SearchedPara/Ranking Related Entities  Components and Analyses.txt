 Related entity finding is the task of returning a ranked list of home-pages of relevant entities of a specified type that need to engage in a given relationship with a given source entity. We propose a framework for addressing this task and perform a detailed analy-sis of four core components; co-occurrence models, type filtering, context modeling and homepage finding. Our initial focus is on recall. We analyze the performance of a model that only uses co-occurrence statistics. While it identifies a set of related entities, it fails to rank them effectively. Two types of error emerge: (1) en-tities of the wrong type pollute the ranking and (2) while some-how associated to the source entity, some retrieved entities do not engage in the right relation with it. To address (1), we add type filtering based on category information available in Wikipedia. To correct for (2), we add contextual information, represented as lan-guage models derived from documents in which source and target entities co-occur. To complete the pipeline, we find homepages of top ranked entities by combining a language modeling approach with heuristics based on Wikipedia X  X  external links. Our method achieves very high recall scores on the end-to-end task, providing a solid starting point for expanding our focus to improve precision; additional heuristics lead to state-of-the-art performance. H.3 [ Information Storage and Retrieval ]: H.3.3 Information Search and Retrieval Algorithms, Experimentation, Measurement, Performance Entity search, Language modeling, Wikipedia
Over the past decade, increasing attention has been devoted to retrieval technology aimed at identifying entities relevant to an in-formation need. The area received a big boost with the arrival of the TREC Question Answering track in 1999, where much research has focused on fact-based questions such as  X  X ho invented the pa-per clip? X  Such questions can be answered by named entities such as locations, dates, etc. [35]. The expert finding task, studied at the TREC Enterprise track (2005 X 2008), focused on a single type of entity: people [7]. The INEX Entity Ranking task (2007 X 2009) broadened the task to include other types and required systems to return ranked lists of entities given a textual description ( X  X ountries where one can pay with the euro X ) and type information ( X  X oun-tries X ) [9]. Next, the TREC 2009 Entity track introduced the related entity finding (REF) task: given a source entity, a relation and a tar-get type, identify homepages of target entities that enjoy the spec-ified relation with the source entity and that satisfy the target type constraint [3]. E.g., for a source entity ( X  X ichael Schumacher X ), a relation ( X  X is teammates while he was racing in Formula 1 X ) and a target type ( X  X eople X ) return entities such as  X  X ddie Irvine X  and  X  X elipe Massa. X  REF aims at making arbitrary relations between entities searchable; it provides a way of searching for information through entities, previously only possible by (implicitly) manually annotated links such as those in social networks.
 Figure 1: Components of an idealized entity finding system. Solid arrows indicate control flow, dashed arrows data flow. We start with an idealized entity retrieval architecture, see Fig. 1. Computations take place at two levels: the entity repository is built off-line, using tools and techniques for named entity recognition and normalization. Queries are processed online, through a re-trieval pipeline. This pipeline resembles a question answering ar-chitecture, where first candidate answers are generated, followed by type filtering and the final ranking (scoring) steps. Candidate generation is a recall-oriented step, while the subsequent two blocks aim to improve precision. Our work sets out the challenge of adopt-ing this general architecture to the REF task, and addresses the is-sue of balancing precision and recall when executing a search.
When building a system to perform a task such as REF, the most important evaluation is on the end-to-end task. The TREC Entity track will play an important role in advancing REF technology, but its end-to-end focus means that it is difficult to disentangle the per-formance contributions of individual components. This effect is reinforced in the case of a new task such as REF where a canonical architecture has yet to emerge. In this paper we go through a series of ablation studies and contrastive runs so as to obtain an under-standing of each of the components that play a role and the impact they have on precision and recall.
 Specifically, we address the REF task as defined at TREC 2009 and consider a particular instantiation of the idealized entity find-ing system, shown in Fig. 2. Our focus is on retrieval and rank-ing rather than on named entity recognition and normalization; to simplify matters we use Wikipedia as a repository of (normalized) known entities. While the restriction to entities in Wikipedia is a limitation in terms of the number of entities considered, it provides us with high-quality data, including names, unique identifiers and type information, for millions of entities. Our framework is generic and conceptually independent of this particular usage of Wikipedia.
Given our focus on entities in Wikipedia, it is natural to ad-dress the REF task in two phases. In the first we build up our re-trieval pipeline (the blocks at the bottom of Fig. 2) working only with Wikipedia pages of entities; in the second we map entities to their homepages. In phase one we use a generative framework to combine the components. The first component is a co-occurrence-based model that selects candidate entities. While, by itself, a co-occurrence-based model can be effective in identifying the poten-tial set of related entities, it fails to rank them effectively. Our fail-ure analysis reveals two types of error that affect precision: (1) en-tities of the wrong type pollute the ranking and (2) entities are re-trieved that are associated with the source entity without engaging in the right relation with it. To address (1), we add a type filtering component based on category information in Wikipedia. To correct for (2), we complement the pipeline with contextual information, represented as statistical language models derived from documents in which the source and target entities co-occur. The addition of context proves beneficial for both recall and precision. A final im-provement in this phase is obtained by employing a large corpus to correct for sparseness issues.
 In phase two, we conform to the official TREC definition of the REF task by adding a homepage finding component that maps en-tities represented by Wikipedia pages to homepages. We show that our approach achieves competitive performance on the official task, especially in terms of recall. We demonstrate the generalizability of our framework by expanding it with two heuristics: one aimed at improving type filtering, the other at co-occurrence. We find that these methods have a very positive impact on all measures.
The main contribution of this paper is two-fold. First, we pro-pose a transparent architecture for addressing the REF task. Sec-ond, we provide a detailed analysis of the effectiveness of its com-ponents and estimation methods, shedding light on the balance be-tween precision and recall in the context of the REF task.
Below, we discuss related work in  X 2. In  X 3 we detail our ap-proach to the REF task. Our experimental setup is described in  X 4. In  X 5 we analyze the effectiveness of a pure co-occurrence model. Type filtering is considered in  X 6 and contextual informa-tion is added in  X 7. Improved estimations of co-occurrence and context models are considered in  X 8. We address the (sub)task of homepage finding (mapping entities to their homepages) in  X 9. In  X 10 we discuss TREC Entity results as well as additional heuristics that can be incorporated into our framework. We conclude in  X 11. Entity retrieval. The roots of entity retrieval go back to natural language processing, specifically to information extraction (IE). The goal in IE is to find all entities for a certain class, for exam-ple  X  X ities. X  The general approach taken uses context based pat-terns to extract entities; e.g.,  X  X ities such as * and * X , either learned from examples [28] or created manually [16]. At the intersection of natural language processing and IR lies question answering (QA), which combines IE and IR, investigated at the TREC QA track [35]. What sets QA apart from Entity Retrieval? One is a matter of tech-nology: many QA systems considered at TREC have a knowledge-intensive pipeline that simply does not comply with the wishes of efficient processing on very large volumes of data. Another is the difference in task; while the  X  X ist X  subtask at the QA track does in-deed resemble the REF task, the two differ in important ways: (i) QA list queries do not always contain an entity [34], e.g.,  X  X hat are 12 types of clams X  and (ii) REF queries impose a more specific (elaborate) relation between the source entity and the target entities, e.g.,  X  X irlines that currently use Boeing 747 planes. X 
A particularly relevant paper on the interface of QA and REF is [26] wherein entity language models are processed using a prob-abilistic representation of the language used to describe a named entity (person, organization or location). The model is constructed from snippets of text surrounding mentions of an entity. Unsurpris-ingly, we model the language model of an individual entity in the same manner, but have more complex models of pairs of entities.
Our main concern is with precision and recall aspects of our ap-proach to the REF task. We initially focus on recall and then ap-ply techniques to boost precision: one of these techniques is type filtering , aimed at demoting entities that are not of the required type. Previously, type filtering has been considered in the setting of QA, where candidate answers are filtered by employing surface pat-terns [27] or by a mix of top-down and bottom-up approaches [29]. We apply type filtering based on Wikipedia category assignments and category structure in the context of the REF task.

The expert finding task, which was run at the TREC Enterprise track [7], focuses on a single type ( X  X erson X ) and relation ( X  X xpert in X ). In a language modeling approach to the task experts are found either by modeling an expert X  X  knowledge by its associated docu-ments ( X  X odel 1 X ) or first collecting topic related documents and then modeling experts ( X  X odel 2 X ) [1, 2]. Additionally, kernels have been used to emphasize terms occurring in close proximity to experts (entities) [25]. A two stage language modeling approach, consisting of a relevance and a co-occurrence model, has been con-sidered in [4]; the relevance model determines if a document is rel-evant to a query, while the co-occurrence model determines the as-sociation between an expert and query in a document. A generative probabilistic framework was proposed in [11], with two families of approaches: candidate and topic generation models.

The novelty of our approach is that we use co-occurrence and context to model entity-entity associations, instead of entity-docu-ment and document-query associations as seen in most expert find-ing systems. Co-occurrence-based methods are widely used to de-termine the strength of association between terms. These methods come in many flavors, e.g., as global co-occurrence [18] (deter-mined on an entire corpus) or local co-occurrence [41] (determined on a relevant set of documents) for query expansion. Hasegawa et al. [15] use the context of entity co-occurrence pairs for relation extraction; entity pairs that occur in the same sentence are clus-tered based on terms between them, and relations are character-ized by frequent words in the cluster. Maximum likelihood estima-tion (MLE) is an obvious choice for determining co-occurrence; a problem is that some words co-occur frequently just by chance. In [21] a number of hypothesis testing methods are listed that deter-mine whether the co-occurrence of two entities is more than mere chance, these include statistical tests, likelihood ratio and point wise mutual information. In  X 5 we determine their value for REF.
The INEX Entity Ranking track [9] broadened expert search by moving from searching for a single type of entity (people) to any type using Wikipedia categories. The corpus also changed to Wi-kipedia, so that each entity corresponds to a Wikipedia page. Two tasks were introduced, entity ranking: return a list of Wikipedia pages (entities) for a query and category, and list completion: re-turn a list of Wikipedia pages for a query, category and example entities. The fact that each entity is represented by a Wikipedia page allows for using standard document retrieval to obtain a list of relevant entities for a query. The approaches differ in the way they combine this with category and example entity information; using a language modeling framework to combine initial retrieval scores with category information [17, 39], or a linear combination of doc-ument, category and link based component scores [8, 33, 45]. To derive a score for the category component most approaches use set overlap between entity categories and topic target categories; others use the topic category label as a query to an index of category la-bels [8, 45]. Another commonly used technique is category expan-sion, based either on the Wikipedia category structure [32, 39] or on lexical similarity between category labels and the query topic [33]. TREC 2009 Entity track. A recent development in evaluating entity-oriented search was the introduction of the Entity track at TREC in 2009 with the aim to perform entity-oriented search tasks on the web [3]. The first edition featured the related entity finding (REF) task. One approach to this task is to directly obtain home-pages by submitting the REF query (source entity and relation) to a search engine [24]. Other approaches first collect text snippets from documents relevant to the REF query, next obtain entities by performing named entity recognition on the snippets, implement some sort of ranking step and finally find homepages, usually by using entity names as queries [37]. Several language modeling ap-proaches were employed to rank entities, where the entity model is constructed from snippets containing the entity and the relation is used as a query [40, 42, 44]. The two stage retrieval model from the Enterprise track is adapted in [38]. Fang et al. [12] use a hi-erarchical relevance retrieval model and improve their model by exploiting list structures, training regression models for type fil-tering and applying heuristic filtering and pattern matching rules. Zhai et al. [43] propose a probabilistic framework to estimate the probability of an entity given a REF query, with two components: the probability of the relation given an entity and source entity, and the probability of an entity given the source entity and target type. While this model is the closest to our approach, it differs in the assumptions made about the dependencies between the query com-ponents, see  X 3. The approaches further differ in the way they es-timate co-occurrence and construct entity and relation models. A number of approaches rely heavily on Wikipedia; as a repository of entity names, to perform entity type filtering based on categories and to find homepages through external links [19, 22, 30].
The goal of the REF task is to return a ranked list of relevant entities e for a query, where a query consists of a source entity ( E ), target type ( T ) and a relation ( R ) [3]. We formalize REF as the task of estimating the probability P ( e | E,T,R ) . This probability is dif-ficult to estimate, due to the lack of training material, exacerbated by the fact that relations do not come from a closed vocabulary. Also, the model should capture a particular relation conditioned on the two entities involved. To address these concerns we turn to a generative model. First, we apply Bayes X  Theorem and rewrite P ( e | E,T,R ) to: Next, we drop the denominator as it does not influence the ranking of entities, and derive our final ranking formula as follows:
P ( E,T,R | e )  X  P ( e ) In (2) we assume that type T is independent of source entity E and relation R . We rewrite P ( E,R | e ) to P ( R | E,e ) so that it expresses the probability that R is generated by two (co-occurring) entities ( e and E ). Finally, we rewrite P ( E,e ) to P ( e | E )  X  P ( E ) in (3) as the latter is more convenient for estimation. We drop P ( E ) in (4) as it is assumed to be uniform, thus does not influence the ranking. The generative model, shown above, functions as follows. The in-put entity E is chosen with probability P ( E ) , which generates a target entity e with probability P ( e | E ) . The input and target enti-ties together generate a relation R with probability P ( R | E,e ) . Fi-nally, the target entity generates a type T with probability P ( T | e ) .
Assuming that input entities are chosen from a uniform distri-bution, we are left with the following components: (i) pure co-occurrence model ( P ( e | E ) ), (ii) type filtering ( P ( T | e ) ) and (iii) contextual information ( P ( R | E,e ) ). We summarize the develop-ments to come in the figure below; we analyze a pure co-occurrence model and its performance on the REF task in  X 5. We then add type filtering and contextual information to the pipeline; these are intro-duced and examined in  X 6 and  X 7, respectively. The components are combined using Eq. 4.
 Research questions. We address the official REF task, REF on a web corpus, by first solving it on a smaller less noisy corpus: Wiki-pedia, where entities are identified by their Wikipedia page. In this setting we consider three research questions. (1) How do different measures for computing co-occurrence affect the recall of a pure co-occurrence based REF model? (2) Can a basic category based type filtering approach successfully be applied to REF to improve precision without hurting recall? (3) Can recall and precision be en-hanced by adding context to co-occurrences, to ensure that source and target entities engage in the right relation? We then look at the REF task in the setting of a large web corpus. To conform to the of-ficial REF task we map the Wikipedia entity representation to a rep-resentation that identifies entities by homepages and consider three additional research questions. (4) Does the use of a larger corpus improve estimations of co-occurrence and context models? (5) Is the initial focus on Wikipedia a sensible approach; can it achieve comparable performance to other approaches? (6) Can our basic Table 1: Description of our 15 test topics. Target entity types are ORG=organization, PER=person and PROD=product. framework effectively incorporate additional heuristics in order to be competitive with other state-of-the-art approaches? Document collection. Our document collection is the ClueWeb09 Category B subset [5] ( X  X W-B X  for short), with about 50 million documents, including English Wikipedia. We use the Wikipedia part of ClueWeb09 and filtered out duplicate pages, page not found errors and non-English pages. This left us with about 5M doc-uments, 2.6M of which correspond to unique entities. The total number of unique entity occurrences in Wikipedia documents (i.e., each unique entity occurring in a document counts only once, inde-pendent of the actual number of occurrences) is 373M.
 Entity recognition and normalization. While named entity recog-nition and normalization are not our focus, they are key pre-pro-cessing steps. We use Wikipedia as a repository of known (nor-malized) entities. We handle named entity recognition (NER) in Wikipedia by considering only anchor texts as entity occurrences. We obtain an entity X  X  name by removing the Wikipedia prefix from the anchor URL. For named entity normalization (NEN) we map URLs to a single entity variant. Here we make use of Wikipedia redirects that map common alternative spellings or references (e.g.,  X  X chumacher, X   X  X chumi X  and  X  X . Schumacher X ) to the main vari-ant of an entity ( X  X ichael Schumacher X ). Below, when using the full CW-B subset, we use the entity names as queries to an index of this collection. This effectively bypasses NER as the resulting doc-ument lists identify in which documents the entities occur. In this case, we do not perform NEN; while potentially introducing noise, we believe that the amount of data partly compensates for this. Test topics. We base our test set on the TREC 2009 Entity topics. A topic consists of a source entity ( E ), a target entity type ( T ) and the desired relation ( R ) described in free text. Since we are restrict-ing ourselves to entities in Wikipedia, we are not able to use all 20 TREC Entity topics, but only 15 of them. Specifically, we exclude three topics (#3, #8 and #13) without relevant results in Wikipedia and another two (#2 and #16) with source entities without a Wi-kipedia page. For the remaining topics we manually mapped the source entity to a Wikipedia page, this is the only manual interven-tion in the pipeline; the topics are listed in Table 1.

We perform two types of evaluation. First, throughout  X 5 X  X 8 we focus on finding entities as represented by their Wikipedia page. We establish ground truth by extracting all primary Wikipedia pages from the TREC 2009 Entity qrels. We handle Wikipedia redirects and duplicates in our evaluation; a Wikipedia page returned for any of the variants of a relevant entity is considered to be correct, but once found, other variants of that page are ignored. This setup con-stitutes a change to the original TREC REF task, arguably making it easier, therefore the reported numbers are not directly compara-ble with those of the TREC 2009 Entity track [3]. Our second type of evaluation, on the original TREC REF task, is performed in  X 10, where we compare our scores with those of TREC Entity partici-pants; based on their original submissions, we also compute their Wikipedia-based evaluation scores. 1 Evaluation metrics. We focus on two measures: precision and recall. Specifically, we use R-Precision (R-prec), where R is the number of relevant entities for a topic, and recall at rank N (R@N), where we take N to be 100, 2000 and  X  X ll X  (i.e., considering all returned entities). In Table 10 we also report on the metrics used at the TREC 2009 Entity track: P@10, NDCG@R, and the number of primary and relevant entity homepages retrieved. We forego sig-nificance testing as we do not have the minimal number of topics (25) recommended [36].
The pure co-occurrence component is the first building block of our retrieval pipeline. It can produce a ranking of entities on its own.
 Since we are planning on expanding this pipeline with additional components (that will build on the set of entities identified in this step), our main focus throughout this section will be on recall. Estimation. The pure co-occurrence component ( P ( e | E ) ) ex-presses the association between entities based on occurrences in documents, independent of context (i.e., the actual content of doc-uments). To express the strength of co-occurrence between e and E we use a function cooc ( e,E ) and estimate P ( e | E ) as follows: We consider four settings of cooc ( e,E ) : (i) as maximum likelihood estimate, (ii)  X  2 hypothesis test, (iii) pointwise mutual information and (iv) log likelihood ratio; we briefly recall their details [21]. (i) Maximum likelihood estimate (MLE) uses the relative fre-quency of co-occurrences between e and E to determine the strength of their association: where c ( e,E ) is the number of documents in which e and E co-occur and c ( E ) is the number of documents in which E occurs. (ii) The  X  2 hypothesis test determines if the co-occurrence of two entities is more likely than just by chance. A  X  2 test is given by: cooc  X  2 ( e,E ) = N  X  ( c ( e,E )  X  c ( e, E )  X  c ( e, E )  X  c ( e,E ) ) where N is the total number of documents, and e , E indicate that e , E do not occur, respectively (i.e., c ( e, E ) is the number of doc-uments in which neither e or E occurs). (iii) Pointwise mutual information (PMI) determines the amount of information we gain if we observe e and E together. It is useful
Evaluation script, qrels and additional resources are made publicly available at http://ilps.science.uva.nl/ resources/cikm2010-entity . to determine independence between entities, but of less value to determine how dependent two entities are. PMI is given by: (iv) Log likelihood ratio is another measure that determines de-pendence and is more reliable than PMI [10]. It is defined as: where k 1 = c ( e,E ) , k 2 = c ( e, E ) , n 1 = c ( E ) , n p 1 = k 1 /n 1 , p 2 = k 2 /n 2 and p = ( k 1 + k 2 ) / ( n L ( p,k,n ) = k log p + ( n  X  k ) log(1  X  p ) .
 Results. Table 2 shows the results of the different estimation meth-ods for the pure co-occurrence model. Out of the four methods,  X  is a clear winner while PMI performs worst on all metrics. MLE and LLR deliver very similar scores; their recall is comparable to that of  X  2 , but they achieve much lower R-precision. All estima-tors return entities that co-occur at least once with the source entity, hence R@All is the same for all, just over 93%.
 Analysis. The numbers presented in Table 2 demonstrate that sim-ple co-occurrence statistics can achieve reasonable recall and can be used to obtain a candidate set of entities (e.g., top 2000) that can then be further examined by subsequent components in the pipeline. Fig. 3 (Top) shows the R@2000 scores of the methods per topic. For most topics at least one of the methods achieves high recall, with the exception of topic #4.

Unlike recall, R-precision scores are very low, suggesting that pure co-occurrence is not enough to solve the REF task. Fig. 3 (Bot-tom) shows that all methods score zero on R-precision on all but 4 topics. To identify the types of errors made, we take topic #17 (cf. Table 1) as an example and list the top 10 entities produced by our co-occurrence methods in Table 3. 2 Clearly,  X  2 finds relevant enti-ties (bold) mixed with non-relevant entities that are not of the target type T (normal font). The other methods suffer more heavily from this type of error and fail to return any relevant entities in the top 10. We also see another type of error: entities, that are of the right type, but do not satisfy the target relation with the source entity. Note that one of the entities (indicated by  X  ) is relevant, but not identified as such, as its Wikipedia page does not occur in the qrels.
Different co-occurrence methods display distinct characteristics in what they consider as strongly associated. MLE and LLR focus on popular entities; the top ranking entity,  X  X haritable Organiza-tion X , occurs 5,271,075 times. The other extreme is demonstrated by PMI, which favors rare entities: the top ranking entity occurs 2 times and exclusively with the source entity. Finally,  X  2 well when entities co-occur frequently with the source entity and less with others; the top ranked entity occurs in 327 documents, in 187 cases together with the source entity, for the second best entity these numbers are 148 and 106, respectively.

As all methods and topics suffer from entities of the wrong type polluting the rankings, we address this next.
We use topic #17 as a running example throughout the paper, to illustrate the impact of additional ranking and filtering components. Figure 3: Per topic R@2000 (Top) and R-precision (Bottom) scores for the pure co-occurrence estimation methods.
 Table 3: Top 10 entities for topic #17. Relevant entities in bold, entities of the wrong type in roman, and entities of the right type but in the wrong relation in italics. MLE and LLR have the same top 10 ranking and are not displayed separately.
To combat the problem that results produced by the pure co-occurrence model are polluted by entities of the wrong type, we add a type filtering component on top of the pure co-occurrence model; this is indicated by the thick box in the figure below. The challenge will be to maintain the high recall levels attained by the pure co-occurrence model while improving precision. Recall from (4), that type filtering is formalized as P ( T | e ) . The entity type filtering component P ( T | e ) expresses the probability that an entity e is of the target type. Combined with the pure co-occurrence model, it yields the following model for ranking entities (we omit details of the derivation for brevity; it goes analogously to  X 3): Estimation. In order to perform type filtering we exploit category information available in Wikipedia. We map each of the (input) entity types ( T  X  X  PER , ORG , PROD } ) to a set of Wikipedia categories ( cat( T ) ) and we create a similar mapping from entities to categories ( cat( e ) ). The former is created manually, while the latter is granted to us in the form of page-category assignments in Wikipedia (recall that Wikipedia pages correspond to entities). With these two mappings we estimate P ( T | e ) as follows: Since the Wikipedia category structure is not a strict hierarchy and the category assignments are imperfect [9], we (optionally) expand Figure 4: R-precision and R@2000 at increasing levels of cate-gory expansion. the set of categories assigned to each target entity type T , hence write cat L n ( T ) , where L n is the chosen level of expansion.
For the initial mapping of types to categories, cat L 1 ( T ) , we manually assign a number of categories to each type as in [19]. To the person type we map categories that end with  X  X irth, X   X  X eath, X  start with  X  X eople X  and the category  X  X iving People. X  To the or-ganization type we map categories that start with  X  X rganizations X  or  X  X ompanies X  and to the product type we map categories starting with  X  X roducts X  or ending with  X  X ntroductions. X  Next, we use the Wikipedia category hierarchy to expand this set by adding all direct child categories of the categories in L 1 , to obtain our first expan-sion set L 2 . We continue expanding the categories this way, one level at a time until no new categories are added.

While this particular form of type filtering is specific and tailored to Wikipedia, it is reasonable to assume that a named entity recog-nizer would provide us with high-level type information; therefore, it is not a limitation of the generalizability of our framework. Results. By varying the expansion levels, we can optimize type filtering in two ways: for (R-)precision and for recall (R@2000). We first investigate the optimal levels of expansion for R-precision. Fig. 4 (Left) shows that R-precision increases when moving from level 0 (no filtering, shown on the right end of the plot) to level 2 expansion, but drops as the level of category expansion is further increased. This is in line with our expectation that an increasing number of categories allow more entities of the wrong type; be-cause of the imperfection of the Wikipedia category structure, ex-pansion results in the addition of many irrelevant categories. As to recall, Fig. 4 (Right) shows R@2000 vs. level of expansion. R@2000 first increases and then decreases (PMI) or remains the same (MLE, LLR, and  X  2 ) as categories are expanded. At level 6 or beyond, the number of non-relevant entities allowed into the ranking is large enough to push relevant entities out of the top 2000. Uniformly applying category expansion down to the same level for all types is not necessarily optimal; some relevant entities of type organization are removed at expansion levels smaller than 6, while those of type person are only filtered out at level 1.

Table 4 shows the results of applying type filtering to the pure co-occurrence model optimized for precision (top rows) and for recall (bottom rows); relative changes are given w.r.t. the results in Ta-ble 2. We see an increase in R-precision for all methods. The best results when optimized for R-precision are achieved with  X  we see large relative improvements for all methods in R-precision. Type filtering causes recall to drop sharply at low ranks; achiev-ing max 60% R@2000 as opposed to 83% without filtering (cf. Table 2). The best R-precision scores averaged over all topics are achieved with type filtering at level 2 ( L 2 ); this is the setting we will use when reporting scores optimized for R-precision.
As to the results optimized for recall, we find, again, that all methods improve both R-precision and R@100. The R@100 and R@2000 results suggest that  X  2 ranks relevant entities closer to the top 100 than the other methods. We find 79% of the relevant entities in the top 2000 and in total only 7% of the relevant entities Co-occ. R-Prec R@100 R@2000 R@All Optimized for Precision
MLE .1196 (+200%) .3827 (+29%) .5924 (-27%) .5977 (-56%)  X  2 .1753 (+60%) .3976 (+22%) .5977 (-38%) .5977 (-56%) PMI .0316 (+30%) .1920 (+96%) .5910 (+21%) .5977 (-56%) LLR .1196 (+200%) .3827 (+29%) .5857 (-23%) .5977 (-56%) Optimized for Recall
MLE .0791 (+98%) .3915 (+32%) .7740 (+3%) .8667 (-7%)  X  2 .1338 (+22%) .5012 (+53%) .7881 (-5%) .8667 (-7%) PMI .0298 (+22%) .1344 (+37%) .7065 (+45%) .8667 (-7%) LLR .0791 (+98%) .3915 (+32%) .7740 (+8%) .8667 (-7%) Table 4: Results of type filtering with optimal level of filtering.
Table 5: Top 10 entities for topic #17 with type filtering ( L are lost by type filtering. We achieve the best recall scores with type filtering at level 6 ( L 6 ); this is the value used for recall-optimized settings reported in the remainder of the paper.

By varying the level of expansion we can effectively aim either for R-precision or for R@2000, without hurting the other. This decision is likely to be made depending on whether this is the last component of the pipeline or results will be passed along for down-stream processing. Optimizing category expansion levels for pre-cision and recall carry the risk of overfitting, especially on a small topic set. Our aim with this tuning, however, is not to squeeze out the last bit of performance, but to demonstrate that type filtering can effectively be used to balance precision and recall. Two rea-sons reduce the risk of overfitting: (i) the target types are of a high level causing the granularity of category expansion to be of a coarse nature and (ii) the level of expansion is the same for all types. Analysis. Table 5 shows the top 10 results for topic #17 after type filtering. We see that type filtering effectively removes enti-ties of the wrong type from the ranking: all remaining entities are of type PER and no relevant entities were removed. Another type of error X  X ntities of the right type but not engaged in the required relation R to the source entity E ( X  X hefs with a show on the Food Network X ) X , is now more prominent (see, e.g., Oprah Winfrey and George W. Bush ). In  X 7 we address this type of error by adding con-text to the co-occurrence model and only admitting co-occurrences in contexts that display evidence of the required relation.
To suppress entities that are of the right type T but that do not engage in the required relation R , we add an additional component: modeling contextual information (the thick box below): Recall from (4) that the context of a co-occurrence model is cap-tured as P ( R | E,e ) . Putting things, this is how we rank ( X 3): Co-occ. R-Prec R@100 R@2000 R@All Optimized for Precision
MLE .2099 (+76%) .4929 (+29%) .5950 (0%) .5977 (0%)  X  2 .2094 (+19%) .4631 (+16%) .5977 (0%) .5977 (0%) PMI .0678 (+115%) .2715 (+41%) .5889 (-1%) .5977 (0%) LLR .2032 (+70%) .4955 (+29%) .5950 (+2%) .5977 (0%) Optimized for Recall
MLE .1905 (+140%) .6221 (+60%) .8344 (+8%) .8667 (0%)  X  2 .1798 (+34%) .5708 (+14%) .8459 (+7%) .8667 (0%) PMI .0678 (+127%) .3313 (+147%) .8315 (+18%) .8667 (0%) LLR .1705 (+115%) .5997 (+53%) .8316 (+7%) .8667 (0%) Estimation. The P ( R | E,e ) component is the probability that a relation is generated from ( X  X bservable in X ) the context of a source and candidate entity pair. We represent the relation between a pair of entities by a co-occurrence language model (  X  Ee ), a distribution over terms taken from documents in which the source and candidate entities co-occur. By assuming independence between the terms in the relation R we arrive at the following estimation: where n ( t,R ) is the number of times t occurs in R . To estimate the co-occurrence language model  X  Ee we aggregate term probabilities from documents in which the two entities co-occur: where D Ee denotes the set of documents in which E and e co-occur and | D Ee | is the number of these documents. P ( t |  X  probability of term t within the language model of document d : where n ( t,d ) is the number of times t appears in document d , P ( t ) is the collection language model, and  X  is the Dirichlet smoothing parameter, set to the average document length in the collection [20]. Results. Table 6 shows the results of the context dependent model (including type filtering), optimized for precision (Top) and recall (Bottom); relative changes are w.r.t. the corresponding cells in Ta-ble 4. In both cases, R-precision and R@100 are substantially im-proved, while R@2000 and R@All remain the same or slightly im-prove. The best performing method across the board is MLE, but there is only a slight difference with the LLR and  X  2 scores. PMI achieved the largest relative improvements, but it still lags behind the other three methods for both R-precision and R@100.
 Analysis. Looking at Table 7 we see that several entities have been replaced with others,  X  X resh X  ones. Some that were in the  X  X rong X  relation (i.e., Oprah and Bush , cf. Table 5) have been removed. For both MLE and LLR Chef and Celebrity are now returned at the top ranks; these entities are observed very frequently together with re-lation terms (and type filtering erroneously recognizes them as peo-ple). Some entities occur only in a handful of documents ( &lt; 10), as a consequence of which very little evidence of the relation R can be found in their contexts (examples from the qrels include Alexandra Guarnaschelli , Aida Mollenkamp , Daisy Martinez ). We observe a larger performance gain for the MLE and LLR based models than for  X  2 . By introducing context, the result lists X  X onsisting of fre-quent entities, favored by these models X  X re supplemented with entities that occur in suitable contexts. The entities found by the  X  2 model show a large overlap with those identified on the basis of context, hence limiting the performance gain.
 Table 7: Top 10 entities for topic #17 after adding context.
These observations point to two issues with using Wikipedia as a corpus: (1) estimates for the pure co-occurrence models are un-reliable and (2) the corpus is too small for constructing accurate context models, i.e., there is simply not enough textual material for certain entities. In  X 8 we address these problems by considering a larger corpus to improve our estimations of the pure co-occurrence model and to gather contexts for more robust context models.
We investigate how using a large corpus (CW-B,  X 4) for estimat-ing our models can overcome the issue that for some entities their co-occurrences are limited to a small set of pages and that for some there is not enough context to be able to derive a robust language model. These changes affect two components of our pipeline: Estimation. Using a large corpus for REF presents two challenges: NER on the entire corpus is time consuming and the shear number of entities becomes prohibitively large for any but the simplest of methods. To deal with these issues, we limit ourselves to a  X  X ork-ing entity set X  consisting of the top 2000 entities produced by the context dependent co-occurrence model (estimated on Wikipedia). We chose the entities returned for PMI without filtering as this pro-duced the highest R@2000 (i.e., 87%). For our pure co-occurrence model we need, for each source-candidate entity pair, the number of documents in which they occur separately and the number of doc-uments in which they co-occur ( X 5). We estimate these numbers by submitting the top 2000 entities as queries to an indexed ver-sion of CW-B, which returns the document IDs. We do the same for the source entities and then compare the document ID lists to find documents with co-occurrences. In order to estimate the con-text dependent model we consider only documents containing the source entity. We then create the co-occurrence model for a source-candidate entity pair by using the candidate as a query, effectively collecting all documents in which they co-occur.
 Results. Table 8 shows the results for the co-occurrence models with estimates obtained from CW-B; relative changes in columns 2 and 3 are w.r.t. Table 4; those in columns 4 and 5 are w.r.t. Ta-ble 6. In the top left quadrant R-precision and R@100 of the pure co-occurrence model (optimized for precision) both improve over the same model using Wikipedia-based estimates for all methods: adding data solves the issue of sparse co-occurrences.

In the top right quadrant we see that the addition of context, us-ing CW-B documents, further improves the  X  2 results, similar to what we saw when adding context in  X 7. In this case however, R-precision is worse than that achieved by the Wikipedia-based model for MLE, PMI, and LLR. In contrast,  X  2 shows a 25% improvement when adding CW-B documents. The models optimized for recall demonstrate a similar behavior; the pure co-occurrence model (bot-tom left) improves over the Wikipedia-based model, while the con-text dependent one does not, except for  X  2 . For the  X  2 Co-occ. Pure Co-Occurrence Context Dependent Optimized for Precision MLE .1512 (+26%) .5423 (+42%) .1898 (-11%) .5423 (+10%)  X  2 .2382 (+36%) .4891 (+23%) .2623 (+25%) .4747 (+3%) PMI .1363 (+331%) .3545 (+85%) .0649 (-8%) .3137 (+16%) LLR .1540 (+29%) .4947 (+29%) .1767 (-15%) .4873 (-2%) Optimized for Recall MLE .0799 (+1%) .5821 (+49%) .0966 (-97%) .6982 (+12%)  X  2 .2281 (+70%) .5474 (+9%) .2399 (+33%) .5418 (-5%) PMI .0966 (+224%) .3748 (+179%) .0577 (-18%) .3308 (0%) LLR .0793 (0%) .5655 (+44%) .0988 (-73%) .6469 (+8%) Table 8: Results for the context dependent model with filtering and estimations using the CW-B corpus. Figure 5: Differences in R-precision per topic; context depen-dent model using CW-B vs. Wikipedia. A negative score indi-cates greater precision for the Wikipedia-based model. seem to have reached a good balance between precision and recall, continuing to improve R-precision with improvements or little ef-fect on R@100. For the other methods, the picture is more diverse, especially for recall-optimized type filtering.
 Analysis. Fig. 5 shows the difference per topic in R-precision of the context dependent model using either CW-B or Wikipedia; a negative score indicates higher R-precision for the Wikipedia-based model. Using Wikipedia documents greatly improves preci-sion scores for three of the topics for MLE and LLR. As we look into these topics we see that the Wikipedia page of each source en-tity contains a full list of all the relevant entities (e.g.,  X  X embers of the Beaux Arts Trio X  and  X  X embers of Jefferson Airplane X ), mak-ing them relatively easy, with external evidence likely to gener-ate noise. However, the CW-B based model improves R-precision scores on a number of topics, which suggests that we can effec-tively use a larger corpus to handle a more diverse set of topics. In our running example (cf. Table 9) we now achieve a near perfect ranking for  X  2 , MLE and LLR; PMI still finds only rare entities. Table 9: Top 10 entities with improved estimations for topic #17; some names truncated for layout reasons.
Up to this point in the retrieval pipeline entities have been identi-fied by their Wikipedia page. However, according the TREC Entity track, an entity is uniquely identified by its homepage, therefore we now focus on the homepage finding component in our architecture: Approach. The 2009 Entity track allows up to three homepages and a Wikipedia page to be returned for each entity and judges pages as either primary 3 , relevant or non-relevant. In this paper, we define homepage finding as the task of returning the primary homepage for an entity. Our approach combines language mod-eling based homepage finding and link-based approaches (see be-low), as a linear mixture with equal weights on the components. Ranking homepages. We address homepage finding as a docu-ment retrieval problem, and employ a standard language modeling approach with uniform priors [31]; it ranks homepages according to the query likelihood: here, we use the name of the entity e a query, P ( q = e n | d ) . Successful approaches to named page and homepage finding use a combination of multiple document fields to represent documents [6, 23]. Following [23], we estimate P ( e as a linear mixture of four components, constructed from the body, title, header and inlink fields. The parameters of the model are es-timated empirically, see below.
 Ranking links. Since our REF system identifies entities by their Wikipedia pages, it is natural to use the information on those pages for homepage finding; external links often contain a link to the entity X  X  homepage [19, 30]. We, again, view this as a ranking problem and estimate the probability that document d is the home-page of entity e given a link e wl on the entity X  X  Wikipedia page: P ( d | e wl ) . We set this probability proportional to the position of the link among all external links on the Wikipedia page (pos ( e Since we have to return  X  X alid" homepages (i.e., that are present in CW-B), we perform an additional filtering step, and exlude URLs from our ranking which do not exist in CW-B.

We also employ a method based on DBpedia, which provides a list of entities with the URL of their homepage. 4 While these home-pages may be more reliable than those found through the earlier ex-ternal links strategy, the coverage of this method is limited. We set the probability of a homepage given a DBpedia URL, P ( d | e 1 if the URL exists in CW-B, and to 0 otherwise. To take advantage of the high quality, but sparse, data in DBpedia, while maintaining high coverage through external links in Wikipedia, we combine the external link and DBpedia strategies using a mixture model; for the sake of simplicity, we set equal weights to both components. Evaluation. Before incorporating the homepage finding compo-nent into the end-to-end retrieval process, we evaluate its perfor-mance on its own. For this purpose we created a test set of home-page finding topics from TREC 2009 Entity qrels; we consider each entity with a primary homepage as a topic, and take the homepage as relevant document; topics and qrels are made available, see Fn. 1.
Parameter estimation (for the weights of the document fields in the mixture model) is done in two ways. The first uses the TREC 2002 Web track data [6]; while performance on the Web track X  X  topics (MRR 0 . 69 ) is comparable to the best approaches at TREC
A primary homepage is the main page about, and in control of, the entity, whereas a relevant page merely mentions the entity .
Available at http://wiki.dbpedia.org/Downloads33 . Table 10: Comparison of our best runs and TREC results. Wi-kipedia pages are counted as primary homepages. 2002, this setting does not perform very well on CW-B (MRR 0 . 35 ). Our second parameter estimation method utilizes Wikipedia, using the page title as a name for the entity and considering exter-nal links with  X  X fficial website X  in their anchor text as homepages of that entity; this leads to a more acceptable performance of the mixture model on CW-B (MRR 0 . 47 ).

Turning to an evaluation of the homepage finding method, using the dedicated topics and judgments derived from TREC 2009 En-tity qrels (Fn. 1), we find that, by itself, the DBpedia-based method results in very low scores (MRR 0 . 08 ), due its low coverage. Using external links from Wikipedia pages of entities we achieve a more acceptable score (MRR 0 . 44 ). Combining links from DBPedia and Wikipedia results only in minor improvements (MRR 0 . 45 ); this is not surprising given that DBPedia is extracted from Wikipedia. The best performance overall is achieved when the language modeling-based approach is combined with the two link-based approaches (MRR 0 . 62 ); this is the method that we use in the rest of the paper.
We now perform an end-to-end evaluation on the task specified at the TREC Entity track. According to the track X  X  definition, up to 3 homepages and a Wikipedia page may be returned for each entity; each is judged on a 3-point scale (non-relevant, relevant or primary). We combine the pipeline developed in  X 5 X  X 8 with the homepage finding component developed in  X 9. Table 10 presents the results. The Baseline row corresponds to our best performing run on CW-B (cf.  X 8). Note that we still only consider the 15 topics described in  X 4. Observe that our recall-oriented model (r1) out-performs other Entity track approaches in terms of the total number of primary pages found (#pri), while the precision-oriented model (p1) is in the top 6 in terms of precision ( P 10 ).

Next we take a look at how competitive our results are when we apply heuristic methods that were popular at the Entity track to our model. We experiment with two additional techniques.
 Improved type filtering. Serdyukov and de Vries [30] use the high quality type definitions provided by the DBpedia ontology form type filtering. We follow this approach and map the ontology categories  X  X erson X  and  X  X rganization X  to their respective topic target types ( PER and ORG ). We associate the class  X  X esource X 
Obtained from http://wiki.dbpedia.org/Ontology . with the product target type ( PROD ), as there is no specific prod-uct category in the ontology. In case an entity does not occur in the ontology, we fall back to our Wikipedia-based filtering (either precision-or recall-oriented), as described in  X 6. We incorporate this in the type filtering component of our model as follows:
P ( T | e ) = where T  X  X  PER , ORG , PROD } and ont ( e ) returns the set of types for an entity in the DBpedia ontology.
 The combination of our category based filtering approach with DBpedia based filtering has a positive effect on both precision and recall, Table 10 (p2 and r2). The two approaches complement each other as the category based filtering covers all entities, but is im-precise, while filtering based on the DBpedia ontology is precise, but only covers some of the entities in Wikipedia.
 Anchor-based co-occurrence. Another approach employed at the Entity track [30, 40] is to only consider entities that link to, or are linked from, the Wikipedia page of the source entity. We view this as a special case of co-occurrence; its strength is proportional to the number of times source and target entities cross-link to each other on their corresponding Wikipedia pages. We estimate this anchor based co-occurrence as follows:
P anc ( e | E ) =  X  a  X  where c ( e,E a ) is the number of times the candidate entity e occurs in the anchor text on the Wikipedia page of the input entity E , and c ( E,e a ) is the other way around. We incorporate this into the pure co-occurrence component ( X 5) as a sum with equal weights.
With the addition of the anchor-based co-occurrence we further improve our precision and recall scores; see Table 10 (p3 and r3). Anchor-based co-occurrence works well in this setting as for most topics the relevant entities occur as anchor texts on the page of the source entity and vice versa (e.g., topics #9 and #20).
 Wikipedia-based evaluation. Another way to compare our model and those of other TREC participants is to use the Wikipedia based evaluation employed throughout the paper. From each participant X  X  run we extract the Wikipedia fields and evaluate the number of pri-mary Wikipedia pages for each topic in terms of R-precision and Recall@100. We observe that we outperform all but one of the other approaches in terms of R-precision (p3) and all approaches in terms of Recall@100 (r1, r2 and r3). The high precision achieved by the best performing team is due to their extensive use of heuris-tics, e.g., using a web search engine to collect relevant pages, craft-ing extraction patterns and exploiting lists and tables [12]. Adjusted judgements. Finally, the runs produced by our models are not official TREC runs and as such were not included in the assessment procedure; this might leave us with sparse judgments. Following standard TREC practice, non-judged documents are con-sidered non-relevant X  X he resulting scores could therefore be an underestimation of our actual retrieval performance. To investi-gate how this affects our results we remove all entities for which there is no judgment available at all (neither primary, relevant or non-relevant, for neither the homepage or Wikipedia fields). We observe that only considering judged entities has a big affect on the precision and recall of our model (extended with anchor based co-occurrence and improved type filtering), see Table 10 (p4 and r4). In the precision oriented model 763 of the 6184 pages are judged (186 primary, 78 relevant). In the recall oriented model 1119 of the 6172 pages are judged (214 primary, 156 relevant). These num-bers show that many of the returned entities have not been judged, impeding an assessment of the full potential of our models.
We examined an architecture for addressing the related entity finding (REF) task on a web corpus, where we focused on four core components: pure co-occurrence, type filtering, contextual in-formation, and homepage finding. Initially we investigated the task on a smaller, less noisy corpus, using Wikipedia pages to uniquely identify entities. To identify a potential set of related entities we looked at four measures for computing co-occurrence and found that  X  2 performed best. An analysis showed that rankings of all methods were polluted by entities of the wrong type. We found that even a basic category based type filtering approach is very ef-fective and that the level of category expansion can be tuned to-wards precision or recall. Furthermore, adding context improves both recall and precision by ensuring that source and target entities engage in the right relation. We then looked at the REF task in the setting of a web corpus and found that using a larger corpus im-proves the estimations of both co-occurrence and context models. To conform to the official REF task we used a homepage finding component to map the Wikipedia entity representation to a home-page and found that our framework achieves decent precision and very high recall scores compared to other approaches on the official task. Finally, we found that our model can effectively incorporate additional heuristics that lead to state-of-the-art performance. Acknowledgements. This research was supported by the European Union X  X  ICT Policy Support Programme as part of the Competitive-ness and Innovation Framework Programme, CIP ICT-PSP under grant agreement nr 250430, by the DuOMAn project carried out within the STEVIN programme which is funded by the Dutch and Flemish Governments under project nr STE-09-12, by the Nether-lands Organisation for Scientific Research (NWO) under project nrs 612.066.512, 612.061.814, 612.061.815, 640.004.802, and par-tially by the Center for Creation, Content and Technology (CCCT).
