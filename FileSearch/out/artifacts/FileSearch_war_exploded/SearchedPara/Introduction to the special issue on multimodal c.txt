 Jean-Claude Martin  X  Patrizia Paggio  X  Peter Kuehnlein  X  Rainer Stiefelhagen  X  Fabio Pianesi 1 Why a special issue on multimodal corpora? There is an increasing interest in multimodal communication as suggested by several national and international projects (ISLE, HUMAINE, SIMILAR, CHIL, AMI, CALO, VACE, CALLAS), the attention devoted to the topic by well-known institutions and organizations (the National Institute of Standards and Technology, the Linguistic Data Consortium), and the success of conferences related to multimodal communication (ICMI, IVA, Gesture, Measuring Behavior, Nordic Symposium on Multimodal Communication, LREC Workshops on Multimodal Corpora).

As Dutoit et al. ( 2006 ) lament, however,  X  there is a lack of multimodal corpora suitable for the evaluation of recognition/synthesis approaches and interaction strategies ... one must admit that most corpora available today target the study of a limited number of modalities, if not one  X . Corpora are not only relevant to evaluation purposes, their importance extending to all the stages of design and development of multimodal systems. Moreover, established practices and guidelines are also missing concerning their design, the number and types of levels they should contain, etc. Indeed, multimodal corpora are expensive to collect and to annotate: they require multiple levels of annotations (various properties of speech and gestures, facial expressions, location of people, body posture, etc.) possibly at different levels of abstractions (actions and events, relational behavior). Despite these problems, however, several interesting attempts at developing multimodal corpora have been recently conducted, and the important insights and experience gained in these projects deserve systematization and dissemination. It seems, therefore, that the time is ripe to offer an overview of these efforts in this special issue.

Our focus is on multimodal corpora and their use for representing and modeling human behavior. We have also included communication studies that contribute to the definition of collection protocols, coding schemes, and reliable models of multimodal human behavior that can be built from corpora and assessed against previous non-digital experimental approaches from the social sciences.

Out of 28 submitted papers, 11 were selected for publication. They illustrate the need of a wide-angle approach for tackling multimodal research corpora: theory and applied aspects, formal and informal annotations, large corpus versus small corpus, manual versus automatic annotation are only some of the issues involved. The papers cover three main topics: the first five provide insights on multimodal communication and its phenomena (emotion, irony, explanation, feedback, sequence and turn-taking). The following four papers describe corpus-based approaches to embodied conversational agents (facial expressions, gestures and their combinations). Finally, the last two papers deal with an area of multimodal interpersonal communication which has been recently receiving much attention: meetings. For each of these three topics, we provide below a short overview of the state of the art as well as short summaries of the papers published in this special issue. 2 Multimodal communication phenomena Multimodal communication has been the topic of many studies in the social sciences (see, e.g., Collier 1985 ; Siegman and Feldstein 1985 ; Feldman and Rim 1991 ; Argyle 2004 ; Harrigan et al. 2005 ; Knapp and Hall 2006 ). Issues that were previously attacked from the perspective of verbal interaction gain new complexity from having to account for the interaction between different communication modalities. Hence, we find polysemous signals (an eyebrow raise might mean surprise, emphasis or suggestion) but also cross-modal synonyms (the meaning  X  X  X mphasis X  X  can be displayed via an eyebrow raise, a head nod, or both). Poggi ( 1996 , 2003 ) defines a framework for communication systems characterizing both meaning types and the types of signals used to express them.

Much research has focused on the single modalities, considering various settings and domains. The papers of this special issue concern mainly three modalities which have received much attention in multimodal corpora research: gestures, facial expressions, and their relation with speech.
Gestures have been studied in the domains of teaching (Goldin-Meadow et al. 1999 ; Kress et al. 2001 ), route description (Kita 2003 ; Tepper et al. 2004 ), encounters (Pentland 2005 ), conversations (Loehr 2004 ), weather forecast (Kett-ebekov et al. 2002 ), map tasks (van der Sluis and Krahmer 2004 ), collaborative building tasks (Beun and Cremers 2001 ), pointing games (Kranstedt et al. 2004 ), dialogue games (Piwek and Beun 2001 ), etc. Kendon defines a gesture as a  X  X  X isible action [...] used as an utterance or as part of an utterance X  X  and  X  X  X ctions that have the features of manifest deliberate expressiveness X  X  (Kendon 2004 ); here the term utterance maintains much of its meaning as when used for traditional verbal communication:  X  X  X ny unit of activity that is treated by those co-present as a communicative move, turn or contribution X  X . McNeill considers as gestures  X  X  X he movements of the hands and arms that we see when people talk X  X  (McNeill 1992 ), and  X  X  X he everyday occurrences of spontaneous, unwitting, and regular accompa-niments of speech that we see in our moving fingers, hands and arms X  X  (McNeill 2005 ).

Facial expressions have been especially studied by Ekman (Ekman and Friesen 1975 ; Ekman 1999 , 2003 ). He described how  X  X  X apid signals (seconds or fractions of seconds) are produced by the contractions of the facial muscles, resulting in temporary changes in facial appearance, shifts in the location and shape of the facial features, and temporary wrinkles X  X . Facial signals have emotional meaning, but also send emblematic messages (the meaning of which is very specific, an emblem being the non-verbal equivalent of a common word or phrase such as a head nod for  X  X  X es X  X  and  X  X  X o X  X ). Raising the brows and holding them while keeping the rest of the face blank is an example of facial emblem signaling questioning. If the brow raise is done together with a head movement, it might be an exclamation. Some facial emblems are conventionally taken to make reference to a specific feeling. Facial signals, however, are also used as conversational punctuators (e.g., to emphasize a particular word). Thus, any movement of a given facial area may have several meanings. For example, lowered, drawn-together brows typically express anger, but they are also an emblem (for determination, concentration and perplexity) and a punctuator (Ekman and Friesen 1975 ). The Facial Action Coding System (FACS) (Ekman et al. 2002 ) is a physically based coding scheme which, however, does not include behavioral interpretation. It explains how to classify facial movements as a function of the muscles which are involved. A comparison of different schemes for coding facial expressions can be found in (Cohn and Ekman 2005 ).

An important issue in the study of multimodal communication is how to capture the behaviors of interest. Analog video has been used for a long time for observing and manually annotating gestures and facial expressions, and several procedures and protocols have been proposed (Harrigan et al. 2005 ), often depending on the type of modality or mix of modalities being considered. For instance, in his the study of conversational gestures, (Kendon 2004 ) used video recordings collected in various countries (Italy and US) during dinner parties, committee meetings, casual card games, interactions between customers and vendors at market stalls, semi-public presentations by tour guides, and informal conversations. McNeill explored communicative gestures via narrative and descriptive protocols: retelling a story from a cartoon or comics, describing a house, describing video clips showing small dolls interacting with simple objects (McNeill 1992 ; McNeill et al. 2001 ; McNeill 2005 ). Butterworth and Beattie ( 1978 ) examined recordings of tutorial sessions. Krauss had subjects describe pictures and actors portraying transcribed monologues (Krauss 1998 ).

During the last ten years, however, several studies of cross-modal relations have been conducted in a variety of contexts and data (laboratory, meeting, TV material, field studies) using digital video and computer-based annotations. Compared to the works mentioned above, digital corpus-based studies mostly aim at producing computational models of multimodal behavior for purposes such as the design of human X  X omputer interfaces, intelligent monitoring, etc. The ISLE project surveyed several such corpora of human multimodal communication built before 2002 (Wegener Knudsen et al. 2002 a, 2002 b). Useful overview of the work done can be found in the proceedings of the workshops on multimodal corpora organized at the last three LREC conferences (Maybury and Martin 2002 ; Martin et al. 2004 , 2006 ), where discussion about methodology and guidelines for multimodal corpora, coding schemes, tools for manual and automatic annotation, and the use of corpora for the design and evaluation of human X  X omputer interfaces can be found.

The first five papers of this special issue provide insights on multimodal expression of different communication phenomena (emotion, irony, explanation, feedback, sequence and turn-taking). They all deal with multimodality in interpersonal communication, and some of them also touch on how to use the knowledge gained in the design of multimodal interfaces.

Irony is a communicative act in which the sender X  X  literal goal is to communicate a meaning x, but through this meaning the sender has the goal of communicating another meaning, y, which contrasts with meaning x. In their paper entitled  X  X  X rony in a judicial debate: analyzing the subtleties of irony while testing the subtleties of an annotation scheme X  X , Isabella Poggi et al . propose an annotation scheme and illustrate its application to a judicial debate. For example, they propose three ways in which an addressee can be alerted to the presence of irony: metacommunication, paracommunication, and parody. Once understood that a sentence is to be interpreted as ironic, the addressee has to understand the real meaning intended by the sender. They demonstrate how a corpus-based approach makes it possible to capture the contrast between signals in parallel modalities in order to improve our understanding of irony.

The paper by Giorgio Merola on  X  X  X motional gestures in sport X  X  is concerned with spontaneous emotions and their expression in gestures. It introduces an original study about gesture activity of athletes during the telling of their best and worst performances. A manual annotation scheme was defined to classify each gesture in terms of handshape, motoric structure, meaning, goal, and type. This scheme enables qualitative and quantitative analyses of the athletes X  gestures and supports the hypothesis that the mental images expressed by the gestures performed while re-living positive and negative experience contain not only visual and propositional, but also sensory-motor and emotional components.

Communicative feedback refers to unobtrusive (usually short) vocal or bodily expressions whereby a recipient of information can inform a contributor of information about whether they are able and willing to communicate, perceive the information, and understand the information. In their contribution  X  X  X he analysis of embodied communicative feedback in multimodal corpora X  X  prerequisite for behavior simulation X  X , Jens Allwood et al . describe categories for setting up a corpus and suggest an account of communicative feedback based upon it that is intended to improve the behavior of a virtual character.

In their paper entitled  X  X  X he MUMIN coding scheme for the annotation of feedback, turn management and sequencing phenomena X  X , Jens Allwood et al . propose a coding scheme for the study of gesture in interpersonal communication, focusing on expressions for feedback, but also turn management and sequencing. Gestures are coded according to their function, shape and dynamics, as well as according to their cross-modal relationships. The results of an evaluation study conducted on three different video samples in Swedish, Finnish and Danish are reported. The preliminary results obtained in these studies show that the reliability of the categories defined in the scheme is acceptable, and that the scheme as a whole constitutes a versatile analysis tool for the study of multimodal interaction.
Kristine Lund , in  X  X  X he importance of gaze and gesture in interactive multimodal explanation X  X , analyzes two corpora to substantiate the claim that gesture and gaze play relevant roles in explanatory dialogues and thus are to be considered as multimodal phenomena. The two corpora that are analyzed are: (1) a group of teacher novices and experts and (2) a student teacher dyad, both of whom construct explanations of students X  reasoning after viewing videos of student dyads solving physics problems. She lays out the relevance of these phenomena for the construction of multimodal interfaces that support explanation between a human and a computer. 3 Multimodal communication and virtual characters By leveraging knowledge on how various modalities are used in human commu-nication, multimodal human X  X omputer interfaces aim at intuitive human-machine interaction systems, allowing the user to exploit modalities such as speech and gestures e.g. to query about graphical objects displayed on the screen. The ultimate goal is an intuitive, robust, and efficient human X  X omputer interaction which can be deployed in many different settings: kiosks (Wahlster 2006 ), mobile devices (Oviatt 2003 ), etc. Proper evaluation methodologies need to be brought into play to assess how these goals are achieved by a given interface (Almeida et al. 2002 ; Bernsen and Dybkj X r 2004 ; Holzapfel et al. 2004 ; van der Sluis and Krahmer 2004 ).

In this setting, a multimodal output option that is attracting much research effort consists of virtual characters (Embodied Conversational Agents X  X CAs) displayed on the screen and endowed with human-like communication capabilities based on speech, gesture and facial expressions (Cassell et al. 1994 , 2000 ). ECAs are expected to increase the intuitiveness and naturalness of the interaction, e.g., via the display of emotional expressions that can be useful for educational or medical applications, by providing motivating cues (Rist et al. 2003 ; Kipp 2004 ; Pelachaud et al. 2004 ; Buisine 2005 ; Vinayagamoorthy et al. 2006 ), etc. Designing ECAs is a long-term research challenge raising many questions concerning, e.g., the architecture of such systems, the computation and selection of verbal and non-verbal behaviors, their synchronization, their representation, and finally the evaluation at multiple levels of the usefulness, efficiency and friendliness of such interfaces (Ruttkay and Pelachaud 2004 ). One major issue is to find out which combinations of modalities are best suited for specific behaviors, e.g. deictic or emotional behaviors.

As summarized in the previous section, psychological and social sciences provide important experimental results about the way people use modalities for the purpose of communication. These contributions, however, are largely insufficient for building a well functioning multimodal interface. More often than not, they lack the specificity and richness of details that are needed to allow, e.g., an ECA to appropriately interact with a human in a given setting. To this end, detailed descriptions of actual human behavior in specific domains are needed to model the ECA X  X  (Martin 2006 ).

Corpus-based approaches to ECA specifications seem to provide the natural answer. They have a long history, indeed, going back to the Disney cartoon designers using live actions to inspire the manual drawings of believable cartoon characters (Johnston and Thomas 1995 ). This approach has been recently revived by the movie industry through the use of movement capture techniques to inspire body or facial animations (cf. DVD Bonus edition of The Lord of the rings, the two towers). Motion capture techniques are also applied to ECA design.

In order to capture communication in a non-intrusive context, manual annotation procedures are also used. (Cassell et al. 2001 ) manually analyzed human monologues and dialogues and observed that postural shifts could be predicted from discourse and conversation state. They implemented this knowledge in the Collagen dialogue manager so that posture shifts were combined with other modalities such as gaze or hand gestures. An experimental study about gaze lead to the specification of gazing in relation to both turn-taking and propositional content (Cassell et al. 1999 ). Capitalizing on these, and other, studies, Cassell et al. were able to propose an iterative methodology cycle consisting of successive steps of: acquiring data, studying them, implement the insights as formal models, test the results, acknowledge gap in the data, acquire new data, etc. (Andre  X  2006 ) reviewed similar attempts and proposals, systematizing them into a number of classes such as the  X  X  X nderstanding via learning to generate X  X  (e.g. Wachsmuth),  X  X  X nalysis by observation and synthesis X  X  (e.g. Krahmer),  X  X  X tudy, model, build, test cycle X  X  (e.g. Cassell).

Four papers of this special issue describe corpus-based approaches to embodied conversational agents.
 In their paper about  X  X  X orpus-Based Generation of Conversational Facial Displays X  X , Mary Ellen Foster and Jon Oberlander present a domain-specific corpus of facial displays, where head and eyebrow motions were manually annotated. Then they use this corpus to select head and eyebrow motions for an embodied conversational agent, and compare two different motion selection strategies: one that chooses the majority option in all cases, and one that makes a weighted choice among all of the options. They compare these methods to each other in two ways: through cross-validation against the corpus, and by asking human judges to rate the output. The results of the two evaluation studies differ: the cross-validation study favored the majority strategy, while the human judges preferred schedules generated using weighted choice.
 In their paper entitled  X  X  X n Annotation Scheme for Conversational Gestures: How to economically capture timing and form X  X , Michael Kipp et al . present a gesture annotation scheme for the purpose of automatically generating and animating character-specific hand and arm gestures. They focus on how to capture temporal structure and location information with relatively little annotation effort. The scheme is evaluated in terms of how accurately it captures the original gestures by re-creating those gestures on an animated character using the annotated data. It is also explained how future extensions can be implemented would the need for more precision arise (encoding hand shape, encoding the gestures for each hand on separate tracks and encoding dynamics).
 In their paper on  X  X  X  case study of gesture expressivity breaks X  X , Nicolas Ech Chafai et al . target the use of gesture expressivity in traditional cartoons. They define an annotation schema and use it to annotate two cartoons. From the resulting material Chafai et al. extract expressivity patterns and rules to manipulate the pragmatic value of a discourse, and then assess their effectiveness through an empirical study testing the perception of annotated behaviors replayed by an expressive agent. This study supports the role of irregularities and discontinuities of gesture expressivity in rhetorical functions of the discourse.
 In their paper on  X  X  X irtual agent multimodal mimicry of humans X  X , George Caridakis et al . present a system to synthesize expressive behavior of virtual agents, based on automatic perception and video analysis of actions performed by human subjects. In their work, facial feature movements, as well as head and hand movements are automatically extracted from video. These cues are then used to animate a virtual agent. This mimicry includes perception, interpretation, planning and animation of the expressions shown by the human, resulting not in an exact duplicate rather than an expressive model of the user X  X  original behavior. A possible future application is that of perceiving visual attention cues from the user. 4 Multimodal communication during meetings The last two papers describe the application of corpus based approaches to meetings, an area of multimodal interpersonal communication which has been receiving much attention via national and international efforts these last years: (e.g., the ARDA VACE-II program, EU-funded projects such as CHIL-FP6, AMI-FP6, etc.).

The importance attributed to the multimodal analysis of meetings comes from the formidable challenges they raise to multimodal analysis as well as the need for automatic support that can improve meeting effectiveness and participant satisfac-tion. Concerning the scientific challenges, meetings involve several people working and discussing together towards some common goal(s); multiparty interaction is the norm, a fact that gives rise to complex communicative patterns involving both verbal and non-verbal channels. The latter, in turn, require extensive analysis of the acoustic and visual scene through several cameras and microphones. Moreover, because of their very nature, meetings are the scene of complex interpersonal psychological phenomena that can have important bearings on the effectiveness of the meeting itself and on the well-being of the participating individuals, as well as on the groups as a psycho-social unit. The ultimate goal of many efforts, therefore, is to enable machines to analyze various facets of meetings as they go on, and use the gathered knowledge and information to support meeting productivity (e.g., the right information at the right time), improve the quality of the interaction (e.g., through automatic coaching and facilitation), provide easy access to relevant portions of meetings (meeting browsing and summarization), etc. An important feature of this scenario is the dramatic change of perspective at the interaction level; no longer a single user confronted with a machine, but a machine and several interacting users.

Obviously, all these challenges and goals can be pursued only if suitable corpora are available to develop and train algorithms, test service functionality hypotheses, etc. Up to now, several such corpora have been produced: the AMI Meeting Corpus (McCowan et al. 2005 ), the VACE Multimodal Meeting Corpus (Chen et al. 2006 ), the NIST Meeting Room Pilot Corpus (Garofolo et al. 2004 ), the ICSI Meeting Corpus (Janin et al. 2003 ).

The two papers included in this section cover some of the most important issues in this area.

The paper by Djamel Mostefa et al . deals with the collection of multimodal corpora of meetings and lectures developed within the EU-funded project CHIL. The annotation consists of the orthographic transcription of speech, 2D position coordinates of facial features, multi-person head locations in 3D space, as well as coarse-grained head pose labels. The paper describes the technical requirements placed by the consortium on the data collection setup, the quality standards enforced on the data and the validation procedures followed. Furthermore, it gives an overview of technology evaluation experiments where the CHIL corpus has been used to address a number of important technological issues in the area of pervasive computing and ambient intelligence, from person tracking and identification, to speaker diarization (the  X  X  X ho spoke when X  X  problem).

The paper by Fabio Pianesi et al . presents an annotated multimodal corpus of audio-visual recordings of group meeting interactions. The corpus is based on the so-called Survival Task, in which a group is asked to reach consensus about how to survive in a disaster scenario. The task is often used in psychological studies on group dynamics, and is well-suited for the authors X  goal of providing material to facilitate understanding of group interaction and ultimately support more efficient decision making in meetings. In keeping with this goal, the annotation scheme focuses on the functional annotation of the interaction, and deals with task-oriented roles such as  X  X rienteer X  or  X  X eeker X  as well as socio-emotional roles such as  X  X ttacker X  or  X  X upporter X . The paper presents the first encouraging results obtained by a classifier in assigning functional roles on the basis of simple features relating to speech and body activity. 5 Conclusion We believe the papers collected in this special issue are valuable contributions to a deeper knowledge of the potential of models of human multimodal behavior for the specification and evaluation of multimodal input and output interfaces, and to a better understanding of the challenging issues connected with the development and usage of multimodal corpora. We also hope this work will help fostering a strong and broad multidisciplinary community of multimodal researchers and multimodal interface developers. The availability of the rich experimental data contained in multimodal corpora is, in fact, a necessary prerequisite for the development and validation of theories of multimodal communication as well as models of multimodal interaction between humans and machines.
 References
